{"id": "2506.11064", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11064", "abs": "https://arxiv.org/abs/2506.11064", "authors": ["Jiajun He", "Tomoki Toda"], "title": "PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding", "comment": "Accepted by IEEE TASLP 2025", "summary": "End-to-end automatic speech recognition (ASR) models often struggle to\naccurately recognize rare words. Previously, we introduced an ASR\npostprocessing method called error detection and context-aware error correction\n(ED-CEC), which leverages contextual information such as named entities and\ntechnical terms to improve the accuracy of ASR transcripts. Although ED-CEC\nachieves a notable success in correcting rare words, its accuracy remains low\nwhen dealing with rare words that have similar pronunciations but different\nspellings. To address this issue, we proposed a phoneme-augmented multimodal\nfusion method for context-aware error correction (PMF-CEC) method on the basis\nof ED-CEC, which allowed for better differentiation between target rare words\nand homophones. Additionally, we observed that the previous ASR error detection\nmodule suffers from overdetection. To mitigate this, we introduced a retention\nprobability mechanism to filter out editing operations with confidence scores\nbelow a set threshold, preserving the original operation to improve error\ndetection accuracy. Experiments conducted on five datasets demonstrated that\nour proposed PMF-CEC maintains reasonable inference speed while further\nreducing the biased word error rate compared with ED-CEC, showing a stronger\nadvantage in correcting homophones. Moreover, our method outperforms other\ncontextual biasing methods, and remains valuable compared with LLM-based\nmethods in terms of faster inference and better robustness under large biasing\nlists."}
{"id": "2506.11069", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11069", "abs": "https://arxiv.org/abs/2506.11069", "authors": ["Tao Zhong", "Mengzhe Geng", "Shujie Hu", "Guinan Li", "Xunying Liu"], "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition", "comment": null, "summary": "Accurate recognition of dysarthric and elderly speech remains challenging to\ndate. While privacy concerns have driven a shift from centralized approaches to\nfederated learning (FL) to ensure data confidentiality, this further\nexacerbates the challenges of data scarcity, imbalanced data distribution and\nspeaker heterogeneity. To this end, this paper conducts a systematic\ninvestigation of regularized FL techniques for privacy-preserving dysarthric\nand elderly speech recognition, addressing different levels of the FL process\nby 1) parameter-based, 2) embedding-based and 3) novel loss-based\nregularization. Experiments on the benchmark UASpeech dysarthric and\nDementiaBank Pitt elderly speech corpora suggest that regularized FL systems\nconsistently outperform the baseline FedAvg system by statistically significant\nWER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing\ncommunication frequency to one exchange per batch approaches centralized\ntraining performance."}
{"id": "2506.11071", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11071", "abs": "https://arxiv.org/abs/2506.11071", "authors": ["Renjith Rajagopal", "Peter Winzell", "Sladjana Strbac", "Konstantin Lindström", "Petter Hörling", "Faisal Kohestani", "Niloofar Mehrzad"], "title": "Embedded Acoustic Intelligence for Automotive Systems", "comment": null, "summary": "Transforming sound insights into actionable streams of data, this abstract\nleverages findings from degree thesis research to enhance automotive system\nintelligence, enabling us to address road type [1].By extracting and\ninterpreting acoustic signatures from microphones installed within the\nwheelbase of a car, we focus on classifying road type.Utilizing deep neural\nnetworks and feature extraction powered by pre-trained models from the Open AI\necosystem (via Hugging Face [2]), our approach enables Autonomous Driving and\nAdvanced Driver- Assistance Systems (AD/ADAS) to anticipate road surfaces,\nsupport adaptive learning for active road noise cancellation, and generate\nvaluable insights for urban planning. The results of this study were\nspecifically captured to support a compelling business case for next-generation\nautomotive systems. This forward-looking approach not only promises to redefine\npassenger comfort and improve vehicle safety, but also paves the way for\nintelligent, data-driven urban road management, making the future of mobility\nboth achievable and sustainable."}
{"id": "2506.11072", "categories": ["eess.AS", "cs.CL", "cs.CY", "cs.SD", "stat.AP", "K.4; J.4; I.2"], "pdf": "https://arxiv.org/pdf/2506.11072", "abs": "https://arxiv.org/abs/2506.11072", "authors": ["Tahiya Chowdhury", "Veronica Romero"], "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling", "comment": "5 pages, 1 figure, 3 tables", "summary": "Machine learning-based behavioral models rely on features extracted from\naudio-visual recordings. The recordings are processed using open-source tools\nto extract speech features for classification models. These tools often lack\nvalidation to ensure reliability in capturing behaviorally relevant\ninformation. This gap raises concerns about reproducibility and fairness across\ndiverse populations and contexts. Speech processing tools, when used outside of\ntheir design context, can fail to capture behavioral variations equitably and\ncan then contribute to bias. We evaluate speech features extracted from two\nwidely used speech analysis tools, OpenSMILE and Praat, to assess their\nreliability when considering adolescents with autism. We observed considerable\nvariation in features across tools, which influenced model performance across\ncontext and demographic groups. We encourage domain-relevant verification to\nenhance the reliability of machine learning models in clinical applications."}
{"id": "2506.11090", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11090", "abs": "https://arxiv.org/abs/2506.11090", "authors": ["David Palzer", "Matthew Maciejewski", "Eric Fosler-Lussier"], "title": "End-to-End Diarization utilizing Attractor Deep Clustering", "comment": "To appear at INTERSPEECH 2025", "summary": "Speaker diarization remains challenging due to the need for structured\nspeaker representations, efficient modeling, and robustness to varying\nconditions. We propose a performant, compact diarization framework that\nintegrates conformer decoders, transformer-updated attractors, and a deep\nclustering style angle loss. Our approach refines speaker representations with\nan enhanced conformer structure, incorporating cross-attention to attractors\nand an additional convolution module. To enforce structured embeddings, we\nextend deep clustering by constructing label-attractor vectors, aligning their\ndirectional structure with audio embeddings. We also impose orthogonality\nconstraints on active attractors for better speaker separation while\nsuppressing non-active attractors to prevent false activations. Finally, a\npermutation invariant training binary cross-entropy loss refines speaker\ndetection. Experiments show that our method achieves low diarization error\nwhile maintaining parameter count."}
{"id": "2506.11179", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.11179", "abs": "https://arxiv.org/abs/2506.11179", "authors": ["Md Mynoddin", "Troyee Dev", "Rishita Chakma"], "title": "Brain2Vec: A Deep Learning Framework for EEG-Based Stress Detection Using CNN-LSTM-Attention", "comment": null, "summary": "Mental stress has become a pervasive factor affecting cognitive health and\noverall well-being, necessitating the development of robust, non-invasive\ndiagnostic tools. Electroencephalogram (EEG) signals provide a direct window\ninto neural activity, yet their non-stationary and high-dimensional nature\nposes significant modeling challenges. Here we introduce Brain2Vec, a new deep\nlearning tool that classifies stress states from raw EEG recordings using a\nhybrid architecture of convolutional, recurrent, and attention mechanisms. The\nmodel begins with a series of convolutional layers to capture localized spatial\ndependencies, followed by an LSTM layer to model sequential temporal patterns,\nand concludes with an attention mechanism to emphasize informative temporal\nregions. We evaluate Brain2Vec on the DEAP dataset, applying bandpass\nfiltering, z-score normalization, and epoch segmentation as part of a\ncomprehensive preprocessing pipeline. Compared to traditional CNN-LSTM\nbaselines, our proposed model achieves an AUC score of 0.68 and a validation\naccuracy of 81.25%. These findings demonstrate Brain2Vec's potential for\nintegration into wearable stress monitoring platforms and personalized\nhealthcare systems."}
{"id": "2506.11074", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11074", "abs": "https://arxiv.org/abs/2506.11074", "authors": ["Tarek Kunze", "Marianne Métais", "Hadrien Titeux", "Lucas Elbert", "Joseph Coffey", "Emmanuel Dupoux", "Alejandrina Cristia", "Marvin Lavechin"], "title": "Challenges in Automated Processing of Speech from Child Wearables: The Case of Voice Type Classifier", "comment": "5 pages, 3 figures", "summary": "Recordings gathered with child-worn devices promised to revolutionize both\nfundamental and applied speech sciences by allowing the effortless capture of\nchildren's naturalistic speech environment and language production. This\npromise hinges on speech technologies that can transform the sheer mounds of\ndata thus collected into usable information. This paper demonstrates several\nobstacles blocking progress by summarizing three years' worth of experiments\naimed at improving one fundamental task: Voice Type Classification. Our\nexperiments suggest that improvements in representation features, architecture,\nand parameter search contribute to only marginal gains in performance. More\nprogress is made by focusing on data relevance and quantity, which highlights\nthe importance of collecting data with appropriate permissions to allow\nsharing."}
{"id": "2506.11096", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11096", "abs": "https://arxiv.org/abs/2506.11096", "authors": ["Guillaume Wisniewski", "Séverine Guillaume", "Clara Rosina Fernández"], "title": "Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting", "comment": null, "summary": "Pretrained speech representations like wav2vec2 and HuBERT exhibit strong\nanisotropy, leading to high similarity between random embeddings. While widely\nobserved, the impact of this property on downstream tasks remains unclear. This\nwork evaluates anisotropy in keyword spotting for computational documentary\nlinguistics. Using Dynamic Time Warping, we show that despite anisotropy,\nwav2vec2 similarity measures effectively identify words without transcription.\nOur results highlight the robustness of these representations, which capture\nphonetic structures and generalize across speakers. Our results underscore the\nimportance of pretraining in learning rich and invariant speech\nrepresentations."}
{"id": "2506.11294", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11294", "abs": "https://arxiv.org/abs/2506.11294", "authors": ["Xue Zhang", "Bang Huang", "Mohamed-Slim Alouini"], "title": "Design of 3D Beamforming and Deployment Strategies for ISAC-based HAPS Systems", "comment": null, "summary": "This paper explores high-altitude platform station (HAPS) systems enabled by\nintegrated sensing and communication (ISAC), in which a HAPS simultaneously\ntransmits communication signals and synthetic aperture radar (SAR) imaging\nsignals to support multi-user communication while performing ground target\nsensing. Taking into account the operational characteristics of SAR imaging, we\nconsider two HAPS deployment strategies: (i) a quasi-stationary HAPS that\nremains fixed at an optimized location during SAR operation, following the\nstop-and-go scanning model; and (ii) a dynamic HAPS that continuously adjusts\nits flight trajectory along a circular path. For each strategy, we aim at\nmaximizing the weighted sum-rate throughput for communication users while\nensuring that SAR imaging requirements, such as beampattern gain and\nsignal-to-noise ratio (SNR), are satisfied. This is achieved by jointly\noptimizing the HAPS deployment strategy, i.e., its placement or trajectory,\nalong with three-dimensional (3D) transmit beamforming, under practical\nconstraints including transmit power limits, energy consumption, and flight\ndynamics. Nevertheless, the formulated optimization problems corresponding to\nthe two deployment strategies are inherently non-convex. To address the issue,\nwe propose efficient algorithms that leverage both convex and non-convex\noptimization techniques to obtain high-quality suboptimal solutions. Numerical\nresults demonstrate the effectiveness and advantages of the proposed approaches\nover benchmark schemes."}
{"id": "2506.11075", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11075", "abs": "https://arxiv.org/abs/2506.11075", "authors": ["Loann Peurey", "Marvin Lavechin", "Tarek Kunze", "Manel Khentout", "Lucas Gautheron", "Emmanuel Dupoux", "Alejandrina Cristia"], "title": "Fifteen Years of Child-Centered Long-Form Recordings: Promises, Resources, and Remaining Challenges to Validity", "comment": "5 pages, 3 figures", "summary": "Audio-recordings collected with a child-worn device are a fundamental tool in\nchild language research. Long-form recordings collected over whole days promise\nto capture children's input and production with minimal observer bias, and\ntherefore high validity. The sheer volume of resulting data necessitates\nautomated analysis to extract relevant metrics for researchers and clinicians.\nThis paper summarizes collective knowledge on this technique, providing entry\npoints to existing resources. We also highlight various sources of error that\nthreaten the accuracy of automated annotations and the interpretation of\nresulting metrics. To address this, we propose potential troubleshooting\nmetrics to help users assess data quality. While a fully automated quality\ncontrol system is not feasible, we outline practical strategies for researchers\nto improve data collection and contextualize their analyses."}
{"id": "2506.11350", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11350", "abs": "https://arxiv.org/abs/2506.11350", "authors": ["Heinrich Dinkel", "Zhiyong Yan", "Tianzi Wang", "Yongqing Wang", "Xingwei Sun", "Yadong Niu", "Jizhong Liu", "Gang Li", "Junbo Zhang", "Jian Luan"], "title": "GLAP: General contrastive audio-text pretraining across domains and languages", "comment": null, "summary": "Contrastive Language Audio Pretraining (CLAP) is a widely-used method to\nbridge the gap between audio and text domains. Current CLAP methods enable\nsound and music retrieval in English, ignoring multilingual spoken content. To\naddress this, we introduce general language audio pretraining (GLAP), which\nexpands CLAP with multilingual and multi-domain abilities. GLAP demonstrates\nits versatility by achieving competitive performance on standard audio-text\nretrieval benchmarks like Clotho and AudioCaps, while significantly surpassing\nexisting methods in speech retrieval and classification tasks. Additionally,\nGLAP achieves strong results on widely used sound-event zero-shot benchmarks,\nwhile simultaneously outperforming previous methods on speech content\nbenchmarks. Further keyword spotting evaluations across 50 languages emphasize\nGLAP's advanced multilingual capabilities. Finally, multilingual sound and\nmusic understanding is evaluated across four languages. Checkpoints and Source:\nhttps://github.com/xiaomi-research/dasheng-glap."}
{"id": "2506.11351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11351", "abs": "https://arxiv.org/abs/2506.11351", "authors": ["Sheng Huang", "Jacob R. Randall", "Cory Hilton", "Jeffrey A. Nanzer"], "title": "A Compact Dynamic Omnidirectional Antenna", "comment": null, "summary": "We propose a novel omnidirectional antenna design incorporating directional\nmodulation for secure narrow planar information transmission. The proposed\nantenna features a compact size and stable omnidirectional radiation\nperformance by employing two tightly spaced, printed meander line monopole\nantennas, acting as a single radiating element. To achieve a narrow information\nsecure region, the proposed antenna is fed by differential power excitation of\ntwo ports with real-time dynamic switching. This leads to phase pattern\nmodulation only along the electrical polarization, resulting in directionally\nconfined information recoverable region in the E-plane, while maintaining\nhighly constant or static omnidirectional H-plane pattern, inducing a\n$360^\\circ$ information recoverable region. The dynamic antenna is designed and\nfabricated on a single layer of Rogers RO4350B which provides a miniaturized\nplanar size of $0.36 \\times 0.5 , \\lambda_0^2$ at 2.7 GHz and easy integration.\nTo validate the wireless communication performance, the fabricated antenna is\ndirectly fed with a 10 dB power ratio by a radio frequency (RF) switching\nsystem and evaluated for 16-QAM and 256-QAM transmission in a high\nsignal-to-noise ratio (SNR) environment. Experimental results demonstrate that\nfor 16-QAM transmission, a narrow E-plane information beam (IB) of\napproximately $34^\\circ$ and omnidirectional H-plane IB are obtained, and a\nnarrower E-plane IB is achieved around $15^\\circ$ for 256-QAM. These results\nconfirm that the proposed antenna offers a simple yet effective approach to\nenhance planar physical information security with a compact dynamic antenna\nsystem."}
{"id": "2506.11079", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11079", "abs": "https://arxiv.org/abs/2506.11079", "authors": ["Lingyun Gao", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts", "comment": "This paper is accepted to Interspeech 2025. This publication is part\n  of the project Responsible AI for Voice Diagnostics (RAIVD) with file number\n  NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which\n  is financed by the Dutch Research Council (NWO)", "summary": "Automatic reading aloud evaluation can provide valuable support to teachers\nby enabling more efficient scoring of reading exercises. However, research on\nreading evaluation systems and applications remains limited. We present a novel\nmultimodal approach that leverages audio and knowledge from text resources. In\nparticular, we explored the potential of using Whisper and instruction-tuned\nlarge language models (LLMs) with prompts to improve transcriptions for child\nspeech recognition, as well as their effectiveness in downstream reading\nmistake detection. Our results demonstrate the effectiveness of prompting\nWhisper and prompting LLM, compared to the baseline Whisper model without\nprompting. The best performing system achieved state-of-the-art recognition\nperformance in Dutch child read speech, with a word error rate (WER) of 5.1%,\nimproving the baseline WER of 9.4%. Furthermore, it significantly improved\nreading mistake detection, increasing the F1 score from 0.39 to 0.73."}
{"id": "2506.11403", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11403", "abs": "https://arxiv.org/abs/2506.11403", "authors": ["Fabian Ritter-Gutierrez", "Yi-Cheng Lin", "Jeremy H. M Wong", "Hung-yi Lee", "Eng Siong Chng", "Nancy F. Chen"], "title": "A correlation-permutation approach for speech-music encoders model merging", "comment": "Under review", "summary": "Creating a unified speech and music model requires expensive pre-training.\nModel merging can instead create an unified audio model with minimal\ncomputational expense. However, direct merging is challenging when the models\nare not aligned in the weight space. Motivated by Git Re-Basin, we introduce a\ncorrelation-permutation approach that aligns a music encoder's internal layers\nwith a speech encoder. We extend previous work to the case of merging\ntransformer layers. The method computes a permutation matrix that maximizes the\nmodel's features-wise cross-correlations layer by layer, enabling effective\nfusion of these otherwise disjoint models. The merged model retains speech\ncapabilities through this method while significantly enhancing music\nperformance, achieving an improvement of 14.83 points in average score compared\nto linear interpolation model merging. This work allows the creation of unified\naudio models from independently trained encoders."}
{"id": "2506.11438", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11438", "abs": "https://arxiv.org/abs/2506.11438", "authors": ["Nianzu Li", "Peiran Wu", "Lipeng Zhu", "Derrick Wing Kwan Ng"], "title": "Movable-Antenna Array Enhanced Downlink NOMA", "comment": "Accepted in 2025 IEEE ICC Workshops", "summary": "Movable antenna (MA) has gained increasing attention in the field of wireless\ncommunications due to its exceptional capability to proactively reconfigure\nwireless channels via localized antenna movements. In this paper, we\ninvestigate the resource allocation design for an MA array-enabled base station\nserving multiple single-antenna users in a downlink non-orthogonal multiple\naccess (NOMA) system. We aim to maximize the sum rate of all users by jointly\noptimizing the transmit beamforming and the positions of all MAs at the BS,\nsubject to the constraints of transmit power budget, finite antenna moving\nregion, and the conditions for successive interference cancellation decoding\nrate. The formulated problem, inherently highly non-convex, is addressed by\nsuccessive convex approximation (SCA) and alternating optimization methods to\nobtain a high-quality suboptimal solution. Simulation results unveil that the\nproposed MA-enhanced downlink NOMA system can significantly improve the sum\nrate performance compared to both the fixed-position antenna (FPA) system and\nthe traditional orthogonal multiple access (OMA) system."}
{"id": "2506.11086", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11086", "abs": "https://arxiv.org/abs/2506.11086", "authors": ["Sujoy Roychowdhury", "H. G. Ranjani", "Sumit Soman", "Nishtha Paul", "Subhadip Bandyopadhyay", "Siddhanth Iyengar"], "title": "Intelligibility of Text-to-Speech Systems for Mathematical Expressions", "comment": "Accepted at Interspeech 2025", "summary": "There has been limited evaluation of advanced Text-to-Speech (TTS) models\nwith Mathematical eXpressions (MX) as inputs. In this work, we design\nexperiments to evaluate quality and intelligibility of five TTS models through\nlistening and transcribing tests for various categories of MX. We use two Large\nLanguage Models (LLMs) to generate English pronunciation from LaTeX MX as TTS\nmodels cannot process LaTeX directly. We use Mean Opinion Score from user\nratings and quantify intelligibility through transcription correctness using\nthree metrics. We also compare listener preference of TTS outputs with respect\nto human expert rendition of same MX. Results establish that output of TTS\nmodels for MX is not necessarily intelligible, the gap in intelligibility\nvaries across TTS models and MX category. For most categories, performance of\nTTS models is significantly worse than that of expert rendition. The effect of\nchoice of LLM is limited. This establishes the need to improve TTS models for\nMX."}
{"id": "2506.11476", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11476", "abs": "https://arxiv.org/abs/2506.11476", "authors": ["Tom Baker", "Javier Nistal"], "title": "LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation", "comment": "Accepted at ISMIR 2025", "summary": "Text-to-audio diffusion models produce high-quality and diverse music but\nmany, if not most, of the SOTA models lack the fine-grained, time-varying\ncontrols essential for music production. ControlNet enables attaching external\ncontrols to a pre-trained generative model by cloning and fine-tuning its\nencoder on new conditionings. However, this approach incurs a large memory\nfootprint and restricts users to a fixed set of controls. We propose a\nlightweight, modular architecture that considerably reduces parameter count\nwhile matching ControlNet in audio quality and condition adherence. Our method\noffers greater flexibility and significantly lower memory usage, enabling more\nefficient training and deployment of independent controls. We conduct extensive\nobjective and subjective evaluations and provide numerous audio examples on the\naccompanying website at https://lightlatentcontrol.github.io"}
{"id": "2506.11497", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11497", "abs": "https://arxiv.org/abs/2506.11497", "authors": ["Priyanka Maity", "Suraj Srivastava", "Aditya K. Jagannatham", "Lajos Hanzo"], "title": "Joint Angle and Velocity-Estimation for Target Localization in Bistatic mmWave MIMO Radar in the Presence of Clutter", "comment": null, "summary": "Sparse Bayesian learning (SBL)-aided target localization is conceived for a\nbistatic mmWave MIMO radar system in the presence of unknown clutter, followed\nby the development of an angle-Doppler (AD)-domain representation of the\ntarget-plus-clutter echo model for accurate target parameter estimation. The\nproposed algorithm exploits the three-dimensional (3D) sparsity arising in the\nAD domain of the scattering scene and employs the powerful SBL framework for\nthe estimation of target parameters, such as the angle-of-departure (AoD),\nangle-of-arrival (AoA) and velocity. To handle a practical scenario where the\nactual target parameters typically deviate from their finite-resolution grid, a\nsuper-resolution-based improved off-grid SBL framework is developed for\nrecursively updating the parameter grid, thereby progressively refining the\nestimates. We also determine the Cram\\'er-Rao bound (CRB) and Bayesian CRB for\ntarget parameter estimation in order to benchmark the estimation performance.\nOur simulation results corroborate the superior performance of the proposed\napproach in comparison to the existing algorithms, and also their ability to\napproach the bounds derived."}
{"id": "2506.11089", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11089", "abs": "https://arxiv.org/abs/2506.11089", "authors": ["Jeena Prakash", "Blessingh Kumar", "Kadri Hacioglu", "Bidisha Sharma", "Sindhuja Gopalan", "Malolan Chetlur", "Shankar Venkatesan", "Andreas Stolcke"], "title": "Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM", "comment": null, "summary": "Automatic speech recognition (ASR) models rely on high-quality transcribed\ndata for effective training. Generating pseudo-labels for large unlabeled audio\ndatasets often relies on complex pipelines that combine multiple ASR outputs\nthrough multi-stage processing, leading to error propagation, information loss\nand disjoint optimization. We propose a unified multi-ASR prompt-driven\nframework using postprocessing by either textual or speech-based large language\nmodels (LLMs), replacing voting or other arbitration logic for reconciling the\nensemble outputs. We perform a comparative study of multiple architectures with\nand without LLMs, showing significant improvements in transcription accuracy\ncompared to traditional methods. Furthermore, we use the pseudo-labels\ngenerated by the various approaches to train semi-supervised ASR models for\ndifferent datasets, again showing improved performance with textual and\nspeechLLM transcriptions compared to baselines."}
{"id": "2506.11542", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11542", "abs": "https://arxiv.org/abs/2506.11542", "authors": ["Thanapat Trachu", "Thanathai Lertpetchpun", "Ekapol Chuangsuwanich"], "title": "Amplifying Artifacts with Speech Enhancement in Voice Anti-spoofing", "comment": "Accepted to Interspeech2025", "summary": "Spoofed utterances always contain artifacts introduced by generative models.\nWhile several countermeasures have been proposed to detect spoofed utterances,\nmost primarily focus on architectural improvements. In this work, we\ninvestigate how artifacts remain hidden in spoofed speech and how to enhance\ntheir presence. We propose a model-agnostic pipeline that amplifies artifacts\nusing speech enhancement and various types of noise. Our approach consists of\nthree key steps: noise addition, noise extraction, and noise amplification.\nFirst, we introduce noise into the raw speech. Then, we apply speech\nenhancement to extract the entangled noise and artifacts. Finally, we amplify\nthese extracted features. Moreover, our pipeline is compatible with different\nspeech enhancement models and countermeasure architectures. Our method improves\nspoof detection performance by up to 44.44\\% on ASVspoof2019 and 26.34\\% on\nASVspoof2021."}
{"id": "2506.11540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11540", "abs": "https://arxiv.org/abs/2506.11540", "authors": ["Wenbo Ding", "Yang Li", "Dongsheng Wang", "Bin Zhao", "Yunrong Zhu", "Yibo Zhang", "Yumeng Miao"], "title": "MMWiLoc: A Multi-Sensor Dataset and Robust Device-Free Localization Method Using Commercial Off-The-Shelf Millimeter Wave Wi-Fi Devices", "comment": "8 pages, 8 figures", "summary": "Device-free Wi-Fi sensing has numerous benefits in practical settings, as it\neliminates the requirement for dedicated sensing devices and can be\naccomplished using current low-cost Wi-Fi devices. With the development of\nWi-Fi standards, millimeter wave Wi-Fi devices with 60GHz operating frequency\nand up to 4GHz bandwidth have become commercially available. Although\nmillimeter wave Wi-Fi presents great promise for Device-Free Wi-Fi sensing with\nincreased bandwidth and beam-forming ability, there still lacks a method for\nlocalization using millimeter wave Wi-Fi. Here, we present two major\ncontributions: First, we provide a comprehensive multi-sensor dataset that\nsynchronously captures human movement data from millimeter wave Wi-Fi, 2.4GHz\nWi-Fi, and millimeter wave radar sensors. This dataset enables direct\nperformance comparisons across different sensing modalities and facilitates\nreproducible researches in indoor localization. Second, we introduce MMWiLoc, a\nnovel localization method that achieves centimeter-level precision with low\ncomputational cost. MMWiLoc incorporates two components: beam pattern\ncalibration using Expectation Maximization and target localization through\nMulti-Scale Compression Sensing. The system processes beam Signal-to-Noise\nRatio (beamSNR) information from the beam-forming process to determine target\nAngle of Arrival (AoA), which is then fused across devices for localization.\nOur extensive evaluation demonstrates that MMWiLoc achieves centimeter-level\nprecision, outperforming 2.4GHz Wi-Fi systems while maintaining competitive\nperformance with high-precision radar systems. The dataset and examples\nprocessing code will be released after this paper is accepted at\nhttps://github.com/wowoyoho/MMWiLoc."}
{"id": "2506.11145", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11145", "abs": "https://arxiv.org/abs/2506.11145", "authors": ["Taous Iatariene", "Alexandre Guérin", "Romain Serizel"], "title": "Tracking of Intermittent and Moving Speakers : Dataset and Metrics", "comment": null, "summary": "This paper presents the problem of tracking intermittent and moving sources,\ni.e, sources that may change position when they are inactive. This issue is\nseldom explored, and most current tracking methods rely on spatial observations\nfor track identity management. They are either based on a previous localization\nstep, or designed to perform joint localization and tracking by predicting\nordered position estimates. This raises concerns about whether such methods can\nmaintain reliable track identity assignment performance when dealing with\ndiscontinuous spatial tracks, which may be caused by a change of direction\nduring silence. We introduce LibriJump, a novel dataset of acoustic scenes in\nthe First Order Ambisonics format focusing on speaker tracking. The dataset\ncontains speakers with changing positions during inactivity periods, thus\nsimulating discontinuous tracks. To measure the identity assignment\nperformance, we propose to use tracking association metrics adapted from the\ncomputer vision community. We provide experiments showing the complementarity\nof association metrics with previously used tracking metrics, given continuous\nand discontinuous spatial tracks."}
{"id": "2506.11605", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11605", "abs": "https://arxiv.org/abs/2506.11605", "authors": ["Alexis Plaquet", "Naohiro Tawara", "Marc Delcroix", "Shota Horiguchi", "Atsushi Ando", "Shoko Araki", "Hervé Bredin"], "title": "Dissecting the Segmentation Model of End-to-End Diarization with Vector Clustering", "comment": "37 pages, 18 figures. Submitted to Computer Speech & Language", "summary": "End-to-End Neural Diarization with Vector Clustering is a powerful and\npractical approach to perform Speaker Diarization. Multiple enhancements have\nbeen proposed for the segmentation model of these pipelines, but their synergy\nhad not been thoroughly evaluated. In this work, we provide an in-depth\nanalysis on the impact of major architecture choices on the performance of the\npipeline. We investigate different encoders (SincNet, pretrained and finetuned\nWavLM), different decoders (LSTM, Mamba, and Conformer), different losses\n(multilabel and multiclass powerset), and different chunk sizes. Through\nin-depth experiments covering nine datasets, we found that the finetuned\nWavLM-based encoder always results in the best systems by a wide margin. The\nLSTM decoder is outclassed by Mamba- and Conformer-based decoders, and while we\nfound Mamba more robust to other architecture choices, it is slightly inferior\nto our best architecture, which uses a Conformer encoder. We found that\nmultilabel and multiclass powerset losses do not have the same distribution of\nerrors. We confirmed that the multiclass loss helps almost all models attain\nsuperior performance, except when finetuning WavLM, in which case, multilabel\nis the superior choice. We also evaluated the impact of the chunk size on all\naforementioned architecture choices and found that newer architectures tend to\nbetter handle long chunk sizes, which can greatly improve pipeline performance.\nOur best system achieved state-of-the-art results on five widely used speaker\ndiarization datasets."}
{"id": "2506.11594", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11594", "abs": "https://arxiv.org/abs/2506.11594", "authors": ["Mohammad Soleymani", "Ignacio Santamaria", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "title": "Energy Efficiency Optimization of Finite Block Length STAR-RIS-aided MU-MIMO Broadcast Channels", "comment": "Accepted at IEEE SPAWC 2025", "summary": "Energy-efficient designs are proposed for multi-user (MU) multiple-input\nmultiple-output (MIMO) broadcast channels (BC), assisted by simultaneously\ntransmitting and reflecting (STAR) reconfigurable intelligent surfaces (RIS)\noperating at finite block length (FBL). In particular, we maximize the sum\nenergy efficiency (EE), showing that STAR-RIS can substantially enhance it. Our\nfindings demonstrate that the gains of employing STAR-RIS increase when the\ncodeword length and the maximum tolerable bit error rate decrease, meaning that\na STAR-RIS is more energy efficient in a system with more stringent latency and\nreliability requirements."}
{"id": "2506.11157", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11157", "abs": "https://arxiv.org/abs/2506.11157", "authors": ["Juhi Khalid", "Martin Bouchard"], "title": "Improved in-car sound pick-up using multichannel Wiener filter", "comment": "6 pages", "summary": "With advancements in automotive electronics and sensors, the sound pick-up\nusing multiple microphones has become feasible for hands-free telephony and\nvoice command in-car applications. However, challenges remain in effectively\nprocessing multiple microphone signals due to bandwidth or processing\nlimitations. This work explores the use of the Multichannel Wiener Filter\nalgorithm with a two-microphone in-car system, to enhance speech quality for\ndriver and passenger voice, i.e., to mitigate notch-filtering effects caused by\nechoes and improve background noise reduction. We evaluate its performance\nunder various noise conditions using modern objective metrics like Deep Noise\nSuppression Mean Opinion Score. The effect of head movements of\ndriver/passenger is also investigated. The proposed method is shown to provide\nsignificant improvements over a simple mixing of microphone signals."}
{"id": "2506.11620", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11620", "abs": "https://arxiv.org/abs/2506.11620", "authors": ["Stefan Bleeck"], "title": "(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test", "comment": null, "summary": "Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials."}
{"id": "2506.11629", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11629", "abs": "https://arxiv.org/abs/2506.11629", "authors": ["Panqi Chen", "Siyuan Li", "Lei Cheng", "Xiao Fu", "Yik-Chung Wu", "Sergios Theodoridis"], "title": "FieldFormer: Self-supervised Reconstruction of Physical Fields via Tensor Attention Prior", "comment": null, "summary": "Reconstructing physical field tensors from \\textit{in situ} observations,\nsuch as radio maps and ocean sound speed fields, is crucial for enabling\nenvironment-aware decision making in various applications, e.g., wireless\ncommunications and underwater acoustics. Field data reconstruction is often\nchallenging, due to the limited and noisy nature of the observations,\nnecessitating the incorporation of prior information to aid the reconstruction\nprocess. Deep neural network-based data-driven structural constraints (e.g.,\n``deeply learned priors'') have showed promising performance. However, this\nfamily of techniques faces challenges such as model mismatches between training\nand testing phases. This work introduces FieldFormer, a self-supervised neural\nprior learned solely from the limited {\\it in situ} observations without the\nneed of offline training. Specifically, the proposed framework starts with\nmodeling the fields of interest using the tensor Tucker model of a high\nmultilinear rank, which ensures a universal approximation property for all\nfields. In the sequel, an attention mechanism is incorporated to learn the\nsparsity pattern that underlies the core tensor in order to reduce the solution\nspace.\n  In this way, a ``complexity-adaptive'' neural representation, grounded in the\nTucker decomposition, is obtained that can flexibly represent\n  various types of fields. A theoretical analysis is provided to support the\nrecoverability of the proposed design. Moreover, extensive experiments, using\nvarious physical field tensors, demonstrate the superiority of the proposed\napproach compared to state-of-the-art baselines."}
{"id": "2506.11160", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11160", "abs": "https://arxiv.org/abs/2506.11160", "authors": ["Yu Pan", "Yuguang Yang", "Yanni Hu", "Jianhao Ye", "Xiang Zhang", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "title": "S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech Translation Framework via Seamlessly Speech-Text Alignment and Streaming Speech Decoder", "comment": "Working in progress", "summary": "Multilingual speech-to-speech translation (S2ST) aims to directly convert\nspoken utterances from multiple source languages into natural and intelligible\nspeech in a target language. Despite recent progress, significant challenges\nremain: (1) achieving high-quality and low-latency S2ST remains a critical\nhurdle; (2) existing S2ST approaches heavily rely on large-scale parallel\nspeech corpora, which are extremely difficult to collect. To address these\nissues, we propose S2ST-Omni, an efficient and scalable framework for\nmultilingual speech-to-speech translation. Specifically, we decompose the S2ST\ntask into speech-to-text translation (S2TT) and text-to-speech synthesis (TTS),\nunifying them within a single end-to-end speech-language model. To achieve\nhigh-quality S2TT while reducing dependence on parallel corpora, we leverage\nlarge-scale pretrained models -- Whisper for audio understanding and Qwen 3.0\nfor text understanding. A lightweight speech adapter is introduced to align\nspeech and text representations, enabling effective use of pretrained\nmultimodal knowledge. To ensure both translation quality and real-time\nperformance, we adopt a pretrained streaming speech decoder in the TTS stage to\ngenerate target speech in an autoregressive manner. Extensive experiments on\nthe CVSS benchmark demonstrate that S2ST-Omni outperforms state-of-the-art S2ST\nbaselines while maintaining comparable latency, highlighting its effectiveness\nand practical potential for real-world deployment."}
{"id": "2506.11747", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11747", "abs": "https://arxiv.org/abs/2506.11747", "authors": ["Daniil Kocharov", "Okko Räsänen"], "title": "Enabling automatic transcription of child-centered audio recordings from real-world environments", "comment": "pre-print", "summary": "Longform audio recordings obtained with microphones worn by children-also\nknown as child-centered daylong recordings-have become a standard method for\nstudying children's language experiences and their impact on subsequent\nlanguage development. Transcripts of longform speech audio would enable rich\nanalyses at various linguistic levels, yet the massive scale of typical\nlongform corpora prohibits comprehensive manual annotation. At the same time,\nautomatic speech recognition (ASR)-based transcription faces significant\nchallenges due to the noisy, unconstrained nature of real-world audio, and no\nexisting study has successfully applied ASR to transcribe such data. However,\nprevious attempts have assumed that ASR must process each longform recording in\nits entirety. In this work, we present an approach to automatically detect\nthose utterances in longform audio that can be reliably transcribed with modern\nASR systems, allowing automatic and relatively accurate transcription of a\nnotable proportion of all speech in typical longform data. We validate the\napproach on four English longform audio corpora, showing that it achieves a\nmedian word error rate (WER) of 0% and a mean WER of 18% when transcribing 13%\nof the total speech in the dataset. In contrast, transcribing all speech\nwithout any filtering yields a median WER of 52% and a mean WER of 51%. We also\ncompare word log-frequencies derived from the automatic transcripts with those\nfrom manual annotations and show that the frequencies correlate at r = 0.92\n(Pearson) for all transcribed words and r = 0.98 for words that appear at least\nfive times in the automatic transcripts. Overall, the work provides a concrete\nstep toward increasingly detailed automated linguistic analyses of\nchild-centered longform audio."}
{"id": "2506.11639", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.11639", "abs": "https://arxiv.org/abs/2506.11639", "authors": ["Hassan Mortada", "Cyril Falcon", "Yanis Kahil", "Mathéo Clavaud", "Jean-Philippe Michel"], "title": "Recursive KalmanNet: Deep Learning-Augmented Kalman Filtering for State Estimation with Consistent Uncertainty Quantification", "comment": "5 pages, 3 figures. Accepted for publication in EUSIPCO 2025\n  proceedings", "summary": "State estimation in stochastic dynamical systems with noisy measurements is a\nchallenge. While the Kalman filter is optimal for linear systems with\nindependent Gaussian white noise, real-world conditions often deviate from\nthese assumptions, prompting the rise of data-driven filtering techniques. This\npaper introduces Recursive KalmanNet, a Kalman-filter-informed recurrent neural\nnetwork designed for accurate state estimation with consistent error covariance\nquantification. Our approach propagates error covariance using the recursive\nJoseph's formula and optimizes the Gaussian negative log-likelihood.\nExperiments with non-Gaussian measurement white noise demonstrate that our\nmodel outperforms both the conventional Kalman filter and an existing\nstate-of-the-art deep learning based estimator."}
{"id": "2506.11169", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11169", "abs": "https://arxiv.org/abs/2506.11169", "authors": ["Soumen Garai", "Suman Samui"], "title": "Advances in Small-Footprint Keyword Spotting: A Comprehensive Review of Efficient Models and Algorithms", "comment": "61 pages, 21 figures", "summary": "Small-Footprint Keyword Spotting (SF-KWS) has gained popularity in today's\nlandscape of smart voice-activated devices, smartphones, and Internet of Things\n(IoT) applications. This surge is attributed to the advancements in Deep\nLearning, enabling the identification of predefined words or keywords from a\ncontinuous stream of words. To implement the SF-KWS model on edge devices with\nlow power and limited memory in real-world scenarios, a efficient Tiny Machine\nLearning (TinyML) framework is essential. In this study, we explore seven\ndistinct categories of techniques namely, Model Architecture, Learning\nTechniques, Model Compression, Attention Awareness Architecture, Feature\nOptimization, Neural Network Search, and Hybrid Approaches, which are suitable\nfor developing an SF-KWS system. This comprehensive overview will serve as a\nvaluable resource for those looking to understand, utilize, or contribute to\nthe field of SF-KWS. The analysis conducted in this work enables the\nidentification of numerous potential research directions, encompassing insights\nfrom automatic speech recognition research and those specifically pertinent to\nthe realm of spoken SF-KWS."}
{"id": "2506.11811", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11811", "abs": "https://arxiv.org/abs/2506.11811", "authors": ["Jing Liu", "EnQi Lian"], "title": "Abstract Sound Fusion with Unconditioned Inversion Model", "comment": null, "summary": "An abstract sound is defined as a sound that does not disclose identifiable\nreal-world sound events to a listener. Sound fusion aims to synthesize an\noriginal sound and a reference sound to generate a novel sound that exhibits\nauditory features beyond mere additive superposition of the sound constituents.\nTo achieve this fusion, we employ inversion techniques that preserve essential\nfeatures of the original sample while enabling controllable synthesis. We\npropose novel SDE and ODE inversion models based on DPMSolver++ samplers that\nreverse the sampling process by configuring model outputs as constants,\neliminating circular dependencies incurred by noise prediction terms. Our\ninversion approach requires no prompt conditioning while maintaining flexible\nguidance during sampling."}
{"id": "2506.11714", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.11714", "abs": "https://arxiv.org/abs/2506.11714", "authors": ["Faruk Pasic", "Lukas Eller", "Stefan Schwarz", "Markus Rupp", "Christoph F. Mecklenbräuker"], "title": "Deep Learning-based mmWave MIMO Channel Estimation using sub-6 GHz Channel Information: CNN and UNet Approaches", "comment": "Submitted to IEEE Conference on Computer Communications Workshops\n  (INFOCOM WKSHPS), 2025", "summary": "Future wireless multiple-input multiple-output (MIMO) systems will integrate\nboth sub-6 GHz and millimeter wave (mmWave) frequency bands to meet the growing\ndemands for high data rates. MIMO link establishment typically requires\naccurate channel estimation, which is particularly challenging at mmWave\nfrequencies due to the low signal-to-noise ratio (SNR). In this paper, we\npropose two novel deep learning-based methods for estimating mmWave MIMO\nchannels by leveraging out-of-band information from the sub-6 GHz band. The\nfirst method employs a convolutional neural network (CNN), while the second\nmethod utilizes a UNet architecture. We compare these proposed methods against\ndeep-learning methods that rely solely on in-band information and with other\nstate-of-the-art out-of-band aided methods. Simulation results show that our\nproposed out-of-band aided deep-learning methods outperform existing\nalternatives in terms of achievable spectral efficiency."}
{"id": "2506.11514", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11514", "abs": "https://arxiv.org/abs/2506.11514", "authors": ["Xingwei Sun", "Heinrich Dinkel", "Yadong Niu", "Linzhang Wang", "Junbo Zhang", "Jian Luan"], "title": "Efficient Speech Enhancement via Embeddings from Pre-trained Generative Audioencoders", "comment": "Accepted by Interspeech 2025", "summary": "Recent research has delved into speech enhancement (SE) approaches that\nleverage audio embeddings from pre-trained models, diverging from\ntime-frequency masking or signal prediction techniques. This paper introduces\nan efficient and extensible SE method. Our approach involves initially\nextracting audio embeddings from noisy speech using a pre-trained audioencoder,\nwhich are then denoised by a compact encoder network. Subsequently, a vocoder\nsynthesizes the clean speech from denoised embeddings. An ablation study\nsubstantiates the parameter efficiency of the denoise encoder with a\npre-trained audioencoder and vocoder. Experimental results on both speech\nenhancement and speaker fidelity demonstrate that our generative\naudioencoder-based SE system outperforms models utilizing discriminative\naudioencoders. Furthermore, subjective listening tests validate that our\nproposed system surpasses an existing state-of-the-art SE model in terms of\nperceptual quality."}
{"id": "2506.11862", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11862", "abs": "https://arxiv.org/abs/2506.11862", "authors": ["Xiaodan Chen", "Xiaoxue Gao", "Mathias Quoy", "Alexandre Pitti", "Nancy F. Chen"], "title": "Confidence-Based Self-Training for EMG-to-Speech: Leveraging Synthetic EMG for Robust Modeling", "comment": null, "summary": "Voiced Electromyography (EMG)-to-Speech (V-ETS) models reconstruct speech\nfrom muscle activity signals, facilitating applications such as\nneurolaryngologic diagnostics. Despite its potential, the advancement of V-ETS\nis hindered by a scarcity of paired EMG-speech data. To address this, we\npropose a novel Confidence-based Multi-Speaker Self-training (CoM2S) approach,\nalong with a newly curated Libri-EMG dataset. This approach leverages synthetic\nEMG data generated by a pre-trained model, followed by a proposed filtering\nmechanism based on phoneme-level confidence to enhance the ETS model through\nthe proposed self-training techniques. Experiments demonstrate our method\nimproves phoneme accuracy, reduces phonological confusion, and lowers word\nerror rate, confirming the effectiveness of our CoM2S approach for V-ETS. In\nsupport of future research, we will release the codes and the proposed\nLibri-EMG dataset-an open-access, time-aligned, multi-speaker voiced EMG and\nspeech recordings."}
{"id": "2506.11779", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11779", "abs": "https://arxiv.org/abs/2506.11779", "authors": ["Ishtiaque Ahmed", "Yingzhuo Sun", "Jingwen Fu", "Alper Kose", "Leila Musavian", "Ming Xiao", "Berna Ozbek"], "title": "Semantic Communications in 6G: Coexistence, Multiple Access, and Satellite Networks", "comment": null, "summary": "The exponential growth of wireless users and bandwidth constraints\nnecessitates innovative communication paradigms for next-generation networks.\nSemantic Communication (SemCom) emerges as a promising solution by transmitting\nextracted meaning rather than raw bits, enhancing spectral efficiency and\nenabling intelligent resource allocation. This paper explores the integration\nof SemCom with conventional Bit-based Communication (BitCom) in heterogeneous\nnetworks, highlighting key challenges and opportunities. We analyze multiple\naccess techniques, including Non-Orthogonal Multiple Access (NOMA), to support\ncoexisting SemCom and BitCom users. Furthermore, we examine multi-modal SemCom\nframeworks for handling diverse data types and discuss their applications in\nsatellite networks, where semantic techniques mitigate bandwidth limitations\nand harsh channel conditions. Finally, we identify future directions for\ndeploying semantic-aware systems in 6G and beyond."}
{"id": "2506.11532", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11532", "abs": "https://arxiv.org/abs/2506.11532", "authors": ["Wen Huang", "Xuechen Liu", "Xin Wang", "Junichi Yamagishi", "Yanmin Qian"], "title": "From Sharpness to Better Generalization for Speech Deepfake Detection", "comment": "Accepted to Interspeech 2025", "summary": "Generalization remains a critical challenge in speech deepfake detection\n(SDD). While various approaches aim to improve robustness, generalization is\ntypically assessed through performance metrics like equal error rate without a\ntheoretical framework to explain model performance. This work investigates\nsharpness as a theoretical proxy for generalization in SDD. We analyze how\nsharpness responds to domain shifts and find it increases in unseen conditions,\nindicating higher model sensitivity. Based on this, we apply Sharpness-Aware\nMinimization (SAM) to reduce sharpness explicitly, leading to better and more\nstable performance across diverse unseen test sets. Furthermore, correlation\nanalysis confirms a statistically significant relationship between sharpness\nand generalization in most test settings. These findings suggest that sharpness\ncan serve as a theoretical indicator for generalization in SDD and that\nsharpness-aware training offers a promising strategy for improving robustness."}
{"id": "2506.12008", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12008", "abs": "https://arxiv.org/abs/2506.12008", "authors": ["Olga Vechtomova", "Jeff Bos"], "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI", "comment": "Accepted for publication at ICCC 2025 (International Conference on\n  Computational Creativity)", "summary": "Dance performance traditionally follows a unidirectional relationship where\nmovement responds to music. While AI has advanced in various creative domains,\nits application in dance has primarily focused on generating choreography from\nmusical input. We present a system that enables dancers to dynamically shape\nmusical environments through their movements. Our multi-modal architecture\ncreates a coherent musical composition by intelligently combining pre-recorded\nmusical clips in response to dance movements, establishing a bidirectional\ncreative partnership where dancers function as both performers and composers.\nThrough correlation analysis of performance data, we demonstrate emergent\ncommunication patterns between movement qualities and audio features. This\napproach reconceptualizes the role of AI in performing arts as a responsive\ncollaborator that expands possibilities for both professional dance performance\nand improvisational artistic expression across broader populations."}
{"id": "2506.11815", "categories": ["eess.SP", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.11815", "abs": "https://arxiv.org/abs/2506.11815", "authors": ["Tae-Seong Han", "Jae-Wook Heo", "Hakseung Kim", "Cheol-Hui Lee", "Hyub Huh", "Eue-Keun Choi", "Dong-Joo Kim"], "title": "Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection", "comment": "This manuscript contains 17 pages, 10 figures, and 3 tables", "summary": "Electrocardiography (ECG) signals are often degraded by noise, which\ncomplicates diagnosis in clinical and wearable settings. This study proposes a\ndiffusion-based framework for ECG noise quantification via reconstruction-based\nanomaly detection, addressing annotation inconsistencies and the limited\ngeneralizability of conventional methods. We introduce a distributional\nevaluation using the Wasserstein-1 distance ($W_1$), comparing the\nreconstruction error distributions between clean and noisy ECGs to mitigate\ninconsistent annotations. Our final model achieved robust noise quantification\nusing only three reverse diffusion steps. The model recorded a macro-average\n$W_1$ score of 1.308 across the benchmarks, outperforming the next-best method\nby over 48%. External validations demonstrated strong generalizability,\nsupporting the exclusion of low-quality segments to enhance diagnostic accuracy\nand enable timely clinical responses to signal degradation. The proposed method\nenhances clinical decision-making, diagnostic accuracy, and real-time ECG\nmonitoring capabilities, supporting future advancements in clinical and\nwearable ECG applications."}
{"id": "2506.11630", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11630", "abs": "https://arxiv.org/abs/2506.11630", "authors": ["Xiangzhu Kong", "Huang Hao", "Zhijian Ou"], "title": "Lightweight and Robust Multi-Channel End-to-End Speech Recognition with Spherical Harmonic Transform", "comment": "Interspeech 2025", "summary": "This paper presents SHTNet, a lightweight spherical harmonic transform (SHT)\nbased framework, which is designed to address cross-array generalization\nchallenges in multi-channel automatic speech recognition (ASR) through three\nkey innovations. First, SHT based spatial sound field decomposition converts\nmicrophone signals into geometry-invariant spherical harmonic coefficients,\nisolating signal processing from array geometry. Second, the Spatio-Spectral\nAttention Fusion Network (SSAFN) combines coordinate-aware spatial modeling,\nrefined self-attention channel combinator, and spectral noise suppression\nwithout conventional beamforming. Third, Rand-SHT training enhances robustness\nthrough random channel selection and array geometry reconstruction. The system\nachieves 39.26\\% average CER across heterogeneous arrays (e.g., circular,\nsquare, and binaural) on datasets including Aishell-4, Alimeeting, and XMOS,\nwith 97.1\\% fewer computations than conventional neural beamformers."}
{"id": "2506.11064", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11064", "abs": "https://arxiv.org/abs/2506.11064", "authors": ["Jiajun He", "Tomoki Toda"], "title": "PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding", "comment": "Accepted by IEEE TASLP 2025", "summary": "End-to-end automatic speech recognition (ASR) models often struggle to\naccurately recognize rare words. Previously, we introduced an ASR\npostprocessing method called error detection and context-aware error correction\n(ED-CEC), which leverages contextual information such as named entities and\ntechnical terms to improve the accuracy of ASR transcripts. Although ED-CEC\nachieves a notable success in correcting rare words, its accuracy remains low\nwhen dealing with rare words that have similar pronunciations but different\nspellings. To address this issue, we proposed a phoneme-augmented multimodal\nfusion method for context-aware error correction (PMF-CEC) method on the basis\nof ED-CEC, which allowed for better differentiation between target rare words\nand homophones. Additionally, we observed that the previous ASR error detection\nmodule suffers from overdetection. To mitigate this, we introduced a retention\nprobability mechanism to filter out editing operations with confidence scores\nbelow a set threshold, preserving the original operation to improve error\ndetection accuracy. Experiments conducted on five datasets demonstrated that\nour proposed PMF-CEC maintains reasonable inference speed while further\nreducing the biased word error rate compared with ED-CEC, showing a stronger\nadvantage in correcting homophones. Moreover, our method outperforms other\ncontextual biasing methods, and remains valuable compared with LLM-based\nmethods in terms of faster inference and better robustness under large biasing\nlists."}
{"id": "2506.11851", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11851", "abs": "https://arxiv.org/abs/2506.11851", "authors": ["Wenjing Cao", "Yafei Wang", "Tianxiang Ji", "Tianyang Cao", "Wenjin Wang", "Symeon Chatzinotas", "Björn Ottersten"], "title": "Interference in Spectrum-Sharing Integrated Terrestrial and Satellite Networks: Modeling, Approximation, and Robust Transmit Beamforming", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper investigates robust transmit (TX) beamforming from the satellite\nto user terminals (UTs), based on statistical channel state information (CSI).\nThe proposed design specifically targets the mitigation of\nsatellite-to-terrestrial interference in spectrum-sharing integrated\nterrestrial and satellite networks. By leveraging the distribution information\nof terrestrial UTs, we first establish an interference model from the satellite\nto terrestrial systems without shared CSI. Based on this, robust TX beamforming\nschemes are developed under both the interference threshold and the power\nbudget. Two optimization criteria are considered: satellite weighted sum rate\nmaximization and mean square error minimization. The former achieves a superior\nachievable rate performance through an iterative optimization framework,\nwhereas the latter enables a low-complexity closed-form solution at the expense\nof reduced rate, with interference constraints satisfied via a bisection\nmethod. To avoid complex integral calculations and the dependence on user\ndistribution information in inter-system interference evaluations, we propose a\nterrestrial base station position-aided approximation method, and the\napproximation errors are subsequently analyzed. Numerical simulations validate\nthe effectiveness of our proposed schemes."}
{"id": "2506.11703", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11703", "abs": "https://arxiv.org/abs/2506.11703", "authors": ["Kathleen MacWilliam", "Thomas Dietzen", "Toon van Waterschoot"], "title": "Tracking of Spatially Dynamic Room Impulse Responses Along Locally Linearized Trajectories", "comment": "8 pages, 6 figures. Accepted paper for conference: Forum Acousticum\n  Euronoise 2025 (fa-euronoise2025)", "summary": "Measuring room impulse responses (RIRs) at multiple spatial points is a\ntime-consuming task, while simulations require detailed knowledge of the room's\nacoustic environment. In prior work, we proposed a method for estimating the\nearly part of RIRs along a linear trajectory in a time-varying acoustic\nscenario involving a static sound source and a microphone moving at constant\nvelocity. This approach relies on measured RIRs at the start and end points of\nthe trajectory and assumes that the time intervals occupied by the direct sound\nand individual reflections along the trajectory are non-overlapping. The\nmethod's applicability is therefore restricted to relatively small areas within\na room, and its performance has yet to be validated with real-world data. In\nthis paper, we propose a practical extension of the method to more realistic\nscenarios by segmenting longer trajectories into smaller linear intervals where\nthe assumptions approximately hold. Applying the method piecewise along these\nsegments extends its applicability to more complex room environments. We\ndemonstrate its effectiveness using the trajectoRIR database, which includes\nmoving microphone recordings and RIR measurements at discrete points along a\ncontrolled L-shaped trajectory in a real room."}
{"id": "2506.11069", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11069", "abs": "https://arxiv.org/abs/2506.11069", "authors": ["Tao Zhong", "Mengzhe Geng", "Shujie Hu", "Guinan Li", "Xunying Liu"], "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition", "comment": null, "summary": "Accurate recognition of dysarthric and elderly speech remains challenging to\ndate. While privacy concerns have driven a shift from centralized approaches to\nfederated learning (FL) to ensure data confidentiality, this further\nexacerbates the challenges of data scarcity, imbalanced data distribution and\nspeaker heterogeneity. To this end, this paper conducts a systematic\ninvestigation of regularized FL techniques for privacy-preserving dysarthric\nand elderly speech recognition, addressing different levels of the FL process\nby 1) parameter-based, 2) embedding-based and 3) novel loss-based\nregularization. Experiments on the benchmark UASpeech dysarthric and\nDementiaBank Pitt elderly speech corpora suggest that regularized FL systems\nconsistently outperform the baseline FedAvg system by statistically significant\nWER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing\ncommunication frequency to one exchange per batch approaches centralized\ntraining performance."}
{"id": "2506.11899", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11899", "abs": "https://arxiv.org/abs/2506.11899", "authors": ["Jiawei Zhuang", "Hongwei Hou", "Minjie Tang", "Wenjin Wang", "Shi Jin", "Vincent K. N. Lau"], "title": "DMRS-Based Uplink Channel Estimation for MU-MIMO Systems with Location-Specific SCSI Acquisition", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "With the growing number of users in multi-user multiple-input multiple-output\n(MU-MIMO) systems, demodulation reference signals (DMRSs) are efficiently\nmultiplexed in the code domain via orthogonal cover codes (OCC) to ensure\northogonality and minimize pilot interference. In this paper, we investigate\nuplink DMRS-based channel estimation for MU-MIMO systems with Type II OCC\npattern standardized in 3GPP Release 18, leveraging location-specific\nstatistical channel state information (SCSI) to enhance performance.\nSpecifically, we propose a SCSI-assisted Bayesian channel estimator (SA-BCE)\nbased on the minimum mean square error criterion to suppress the pilot\ninterference and noise, albeit at the cost of cubic computational complexity\ndue to matrix inversions. To reduce this complexity while maintaining\nperformance, we extend the scheme to a windowed version (SA-WBCE), which\nincorporates antenna-frequency domain windowing and beam-delay domain\nprocessing to exploit asymptotic sparsity and mitigate energy leakage in\npractical systems. To avoid the frequent real-time SCSI acquisition, we\nconstruct a grid-based location-specific SCSI database based on the principle\nof spatial consistency, and subsequently leverage the uplink received signals\nwithin each grid to extract the SCSI. Facilitated by the multilinear structure\nof wireless channels, we formulate the SCSI acquisition problem within each\ngrid as a tensor decomposition problem, where the factor matrices are\nparameterized by the multi-path powers, delays, and angles. The computational\ncomplexity of SCSI acquisition can be significantly reduced by exploiting the\nVandermonde structure of the factor matrices. Simulation results demonstrate\nthat the proposed location-specific SCSI database construction method achieves\nhigh accuracy, while the SA-BCE and SA-WBCE significantly outperform\nstate-of-the-art benchmarks in MU-MIMO systems."}
{"id": "2506.11090", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11090", "abs": "https://arxiv.org/abs/2506.11090", "authors": ["David Palzer", "Matthew Maciejewski", "Eric Fosler-Lussier"], "title": "End-to-End Diarization utilizing Attractor Deep Clustering", "comment": "To appear at INTERSPEECH 2025", "summary": "Speaker diarization remains challenging due to the need for structured\nspeaker representations, efficient modeling, and robustness to varying\nconditions. We propose a performant, compact diarization framework that\nintegrates conformer decoders, transformer-updated attractors, and a deep\nclustering style angle loss. Our approach refines speaker representations with\nan enhanced conformer structure, incorporating cross-attention to attractors\nand an additional convolution module. To enforce structured embeddings, we\nextend deep clustering by constructing label-attractor vectors, aligning their\ndirectional structure with audio embeddings. We also impose orthogonality\nconstraints on active attractors for better speaker separation while\nsuppressing non-active attractors to prevent false activations. Finally, a\npermutation invariant training binary cross-entropy loss refines speaker\ndetection. Experiments show that our method achieves low diarization error\nwhile maintaining parameter count."}
{"id": "2506.11072", "categories": ["eess.AS", "cs.CL", "cs.CY", "cs.SD", "stat.AP", "K.4; J.4; I.2"], "pdf": "https://arxiv.org/pdf/2506.11072", "abs": "https://arxiv.org/abs/2506.11072", "authors": ["Tahiya Chowdhury", "Veronica Romero"], "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling", "comment": "5 pages, 1 figure, 3 tables", "summary": "Machine learning-based behavioral models rely on features extracted from\naudio-visual recordings. The recordings are processed using open-source tools\nto extract speech features for classification models. These tools often lack\nvalidation to ensure reliability in capturing behaviorally relevant\ninformation. This gap raises concerns about reproducibility and fairness across\ndiverse populations and contexts. Speech processing tools, when used outside of\ntheir design context, can fail to capture behavioral variations equitably and\ncan then contribute to bias. We evaluate speech features extracted from two\nwidely used speech analysis tools, OpenSMILE and Praat, to assess their\nreliability when considering adolescents with autism. We observed considerable\nvariation in features across tools, which influenced model performance across\ncontext and demographic groups. We encourage domain-relevant verification to\nenhance the reliability of machine learning models in clinical applications."}
{"id": "2506.11145", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11145", "abs": "https://arxiv.org/abs/2506.11145", "authors": ["Taous Iatariene", "Alexandre Guérin", "Romain Serizel"], "title": "Tracking of Intermittent and Moving Speakers : Dataset and Metrics", "comment": null, "summary": "This paper presents the problem of tracking intermittent and moving sources,\ni.e, sources that may change position when they are inactive. This issue is\nseldom explored, and most current tracking methods rely on spatial observations\nfor track identity management. They are either based on a previous localization\nstep, or designed to perform joint localization and tracking by predicting\nordered position estimates. This raises concerns about whether such methods can\nmaintain reliable track identity assignment performance when dealing with\ndiscontinuous spatial tracks, which may be caused by a change of direction\nduring silence. We introduce LibriJump, a novel dataset of acoustic scenes in\nthe First Order Ambisonics format focusing on speaker tracking. The dataset\ncontains speakers with changing positions during inactivity periods, thus\nsimulating discontinuous tracks. To measure the identity assignment\nperformance, we propose to use tracking association metrics adapted from the\ncomputer vision community. We provide experiments showing the complementarity\nof association metrics with previously used tracking metrics, given continuous\nand discontinuous spatial tracks."}
{"id": "2506.11096", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11096", "abs": "https://arxiv.org/abs/2506.11096", "authors": ["Guillaume Wisniewski", "Séverine Guillaume", "Clara Rosina Fernández"], "title": "Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting", "comment": null, "summary": "Pretrained speech representations like wav2vec2 and HuBERT exhibit strong\nanisotropy, leading to high similarity between random embeddings. While widely\nobserved, the impact of this property on downstream tasks remains unclear. This\nwork evaluates anisotropy in keyword spotting for computational documentary\nlinguistics. Using Dynamic Time Warping, we show that despite anisotropy,\nwav2vec2 similarity measures effectively identify words without transcription.\nOur results highlight the robustness of these representations, which capture\nphonetic structures and generalize across speakers. Our results underscore the\nimportance of pretraining in learning rich and invariant speech\nrepresentations."}
{"id": "2506.11074", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11074", "abs": "https://arxiv.org/abs/2506.11074", "authors": ["Tarek Kunze", "Marianne Métais", "Hadrien Titeux", "Lucas Elbert", "Joseph Coffey", "Emmanuel Dupoux", "Alejandrina Cristia", "Marvin Lavechin"], "title": "Challenges in Automated Processing of Speech from Child Wearables: The Case of Voice Type Classifier", "comment": "5 pages, 3 figures", "summary": "Recordings gathered with child-worn devices promised to revolutionize both\nfundamental and applied speech sciences by allowing the effortless capture of\nchildren's naturalistic speech environment and language production. This\npromise hinges on speech technologies that can transform the sheer mounds of\ndata thus collected into usable information. This paper demonstrates several\nobstacles blocking progress by summarizing three years' worth of experiments\naimed at improving one fundamental task: Voice Type Classification. Our\nexperiments suggest that improvements in representation features, architecture,\nand parameter search contribute to only marginal gains in performance. More\nprogress is made by focusing on data relevance and quantity, which highlights\nthe importance of collecting data with appropriate permissions to allow\nsharing."}
{"id": "2506.11157", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11157", "abs": "https://arxiv.org/abs/2506.11157", "authors": ["Juhi Khalid", "Martin Bouchard"], "title": "Improved in-car sound pick-up using multichannel Wiener filter", "comment": "6 pages", "summary": "With advancements in automotive electronics and sensors, the sound pick-up\nusing multiple microphones has become feasible for hands-free telephony and\nvoice command in-car applications. However, challenges remain in effectively\nprocessing multiple microphone signals due to bandwidth or processing\nlimitations. This work explores the use of the Multichannel Wiener Filter\nalgorithm with a two-microphone in-car system, to enhance speech quality for\ndriver and passenger voice, i.e., to mitigate notch-filtering effects caused by\nechoes and improve background noise reduction. We evaluate its performance\nunder various noise conditions using modern objective metrics like Deep Noise\nSuppression Mean Opinion Score. The effect of head movements of\ndriver/passenger is also investigated. The proposed method is shown to provide\nsignificant improvements over a simple mixing of microphone signals."}
{"id": "2506.11350", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11350", "abs": "https://arxiv.org/abs/2506.11350", "authors": ["Heinrich Dinkel", "Zhiyong Yan", "Tianzi Wang", "Yongqing Wang", "Xingwei Sun", "Yadong Niu", "Jizhong Liu", "Gang Li", "Junbo Zhang", "Jian Luan"], "title": "GLAP: General contrastive audio-text pretraining across domains and languages", "comment": null, "summary": "Contrastive Language Audio Pretraining (CLAP) is a widely-used method to\nbridge the gap between audio and text domains. Current CLAP methods enable\nsound and music retrieval in English, ignoring multilingual spoken content. To\naddress this, we introduce general language audio pretraining (GLAP), which\nexpands CLAP with multilingual and multi-domain abilities. GLAP demonstrates\nits versatility by achieving competitive performance on standard audio-text\nretrieval benchmarks like Clotho and AudioCaps, while significantly surpassing\nexisting methods in speech retrieval and classification tasks. Additionally,\nGLAP achieves strong results on widely used sound-event zero-shot benchmarks,\nwhile simultaneously outperforming previous methods on speech content\nbenchmarks. Further keyword spotting evaluations across 50 languages emphasize\nGLAP's advanced multilingual capabilities. Finally, multilingual sound and\nmusic understanding is evaluated across four languages. Checkpoints and Source:\nhttps://github.com/xiaomi-research/dasheng-glap."}
{"id": "2506.11075", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11075", "abs": "https://arxiv.org/abs/2506.11075", "authors": ["Loann Peurey", "Marvin Lavechin", "Tarek Kunze", "Manel Khentout", "Lucas Gautheron", "Emmanuel Dupoux", "Alejandrina Cristia"], "title": "Fifteen Years of Child-Centered Long-Form Recordings: Promises, Resources, and Remaining Challenges to Validity", "comment": "5 pages, 3 figures", "summary": "Audio-recordings collected with a child-worn device are a fundamental tool in\nchild language research. Long-form recordings collected over whole days promise\nto capture children's input and production with minimal observer bias, and\ntherefore high validity. The sheer volume of resulting data necessitates\nautomated analysis to extract relevant metrics for researchers and clinicians.\nThis paper summarizes collective knowledge on this technique, providing entry\npoints to existing resources. We also highlight various sources of error that\nthreaten the accuracy of automated annotations and the interpretation of\nresulting metrics. To address this, we propose potential troubleshooting\nmetrics to help users assess data quality. While a fully automated quality\ncontrol system is not feasible, we outline practical strategies for researchers\nto improve data collection and contextualize their analyses."}
{"id": "2506.11703", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11703", "abs": "https://arxiv.org/abs/2506.11703", "authors": ["Kathleen MacWilliam", "Thomas Dietzen", "Toon van Waterschoot"], "title": "Tracking of Spatially Dynamic Room Impulse Responses Along Locally Linearized Trajectories", "comment": "8 pages, 6 figures. Accepted paper for conference: Forum Acousticum\n  Euronoise 2025 (fa-euronoise2025)", "summary": "Measuring room impulse responses (RIRs) at multiple spatial points is a\ntime-consuming task, while simulations require detailed knowledge of the room's\nacoustic environment. In prior work, we proposed a method for estimating the\nearly part of RIRs along a linear trajectory in a time-varying acoustic\nscenario involving a static sound source and a microphone moving at constant\nvelocity. This approach relies on measured RIRs at the start and end points of\nthe trajectory and assumes that the time intervals occupied by the direct sound\nand individual reflections along the trajectory are non-overlapping. The\nmethod's applicability is therefore restricted to relatively small areas within\na room, and its performance has yet to be validated with real-world data. In\nthis paper, we propose a practical extension of the method to more realistic\nscenarios by segmenting longer trajectories into smaller linear intervals where\nthe assumptions approximately hold. Applying the method piecewise along these\nsegments extends its applicability to more complex room environments. We\ndemonstrate its effectiveness using the trajectoRIR database, which includes\nmoving microphone recordings and RIR measurements at discrete points along a\ncontrolled L-shaped trajectory in a real room."}
{"id": "2506.11403", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11403", "abs": "https://arxiv.org/abs/2506.11403", "authors": ["Fabian Ritter-Gutierrez", "Yi-Cheng Lin", "Jeremy H. M Wong", "Hung-yi Lee", "Eng Siong Chng", "Nancy F. Chen"], "title": "A correlation-permutation approach for speech-music encoders model merging", "comment": "Under review", "summary": "Creating a unified speech and music model requires expensive pre-training.\nModel merging can instead create an unified audio model with minimal\ncomputational expense. However, direct merging is challenging when the models\nare not aligned in the weight space. Motivated by Git Re-Basin, we introduce a\ncorrelation-permutation approach that aligns a music encoder's internal layers\nwith a speech encoder. We extend previous work to the case of merging\ntransformer layers. The method computes a permutation matrix that maximizes the\nmodel's features-wise cross-correlations layer by layer, enabling effective\nfusion of these otherwise disjoint models. The merged model retains speech\ncapabilities through this method while significantly enhancing music\nperformance, achieving an improvement of 14.83 points in average score compared\nto linear interpolation model merging. This work allows the creation of unified\naudio models from independently trained encoders."}
{"id": "2506.11079", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11079", "abs": "https://arxiv.org/abs/2506.11079", "authors": ["Lingyun Gao", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts", "comment": "This paper is accepted to Interspeech 2025. This publication is part\n  of the project Responsible AI for Voice Diagnostics (RAIVD) with file number\n  NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which\n  is financed by the Dutch Research Council (NWO)", "summary": "Automatic reading aloud evaluation can provide valuable support to teachers\nby enabling more efficient scoring of reading exercises. However, research on\nreading evaluation systems and applications remains limited. We present a novel\nmultimodal approach that leverages audio and knowledge from text resources. In\nparticular, we explored the potential of using Whisper and instruction-tuned\nlarge language models (LLMs) with prompts to improve transcriptions for child\nspeech recognition, as well as their effectiveness in downstream reading\nmistake detection. Our results demonstrate the effectiveness of prompting\nWhisper and prompting LLM, compared to the baseline Whisper model without\nprompting. The best performing system achieved state-of-the-art recognition\nperformance in Dutch child read speech, with a word error rate (WER) of 5.1%,\nimproving the baseline WER of 9.4%. Furthermore, it significantly improved\nreading mistake detection, increasing the F1 score from 0.39 to 0.73."}
{"id": "2506.11862", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11862", "abs": "https://arxiv.org/abs/2506.11862", "authors": ["Xiaodan Chen", "Xiaoxue Gao", "Mathias Quoy", "Alexandre Pitti", "Nancy F. Chen"], "title": "Confidence-Based Self-Training for EMG-to-Speech: Leveraging Synthetic EMG for Robust Modeling", "comment": null, "summary": "Voiced Electromyography (EMG)-to-Speech (V-ETS) models reconstruct speech\nfrom muscle activity signals, facilitating applications such as\nneurolaryngologic diagnostics. Despite its potential, the advancement of V-ETS\nis hindered by a scarcity of paired EMG-speech data. To address this, we\npropose a novel Confidence-based Multi-Speaker Self-training (CoM2S) approach,\nalong with a newly curated Libri-EMG dataset. This approach leverages synthetic\nEMG data generated by a pre-trained model, followed by a proposed filtering\nmechanism based on phoneme-level confidence to enhance the ETS model through\nthe proposed self-training techniques. Experiments demonstrate our method\nimproves phoneme accuracy, reduces phonological confusion, and lowers word\nerror rate, confirming the effectiveness of our CoM2S approach for V-ETS. In\nsupport of future research, we will release the codes and the proposed\nLibri-EMG dataset-an open-access, time-aligned, multi-speaker voiced EMG and\nspeech recordings."}
{"id": "2506.11476", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11476", "abs": "https://arxiv.org/abs/2506.11476", "authors": ["Tom Baker", "Javier Nistal"], "title": "LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation", "comment": "Accepted at ISMIR 2025", "summary": "Text-to-audio diffusion models produce high-quality and diverse music but\nmany, if not most, of the SOTA models lack the fine-grained, time-varying\ncontrols essential for music production. ControlNet enables attaching external\ncontrols to a pre-trained generative model by cloning and fine-tuning its\nencoder on new conditionings. However, this approach incurs a large memory\nfootprint and restricts users to a fixed set of controls. We propose a\nlightweight, modular architecture that considerably reduces parameter count\nwhile matching ControlNet in audio quality and condition adherence. Our method\noffers greater flexibility and significantly lower memory usage, enabling more\nefficient training and deployment of independent controls. We conduct extensive\nobjective and subjective evaluations and provide numerous audio examples on the\naccompanying website at https://lightlatentcontrol.github.io"}
{"id": "2506.11145", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11145", "abs": "https://arxiv.org/abs/2506.11145", "authors": ["Taous Iatariene", "Alexandre Guérin", "Romain Serizel"], "title": "Tracking of Intermittent and Moving Speakers : Dataset and Metrics", "comment": null, "summary": "This paper presents the problem of tracking intermittent and moving sources,\ni.e, sources that may change position when they are inactive. This issue is\nseldom explored, and most current tracking methods rely on spatial observations\nfor track identity management. They are either based on a previous localization\nstep, or designed to perform joint localization and tracking by predicting\nordered position estimates. This raises concerns about whether such methods can\nmaintain reliable track identity assignment performance when dealing with\ndiscontinuous spatial tracks, which may be caused by a change of direction\nduring silence. We introduce LibriJump, a novel dataset of acoustic scenes in\nthe First Order Ambisonics format focusing on speaker tracking. The dataset\ncontains speakers with changing positions during inactivity periods, thus\nsimulating discontinuous tracks. To measure the identity assignment\nperformance, we propose to use tracking association metrics adapted from the\ncomputer vision community. We provide experiments showing the complementarity\nof association metrics with previously used tracking metrics, given continuous\nand discontinuous spatial tracks."}
{"id": "2506.11542", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11542", "abs": "https://arxiv.org/abs/2506.11542", "authors": ["Thanapat Trachu", "Thanathai Lertpetchpun", "Ekapol Chuangsuwanich"], "title": "Amplifying Artifacts with Speech Enhancement in Voice Anti-spoofing", "comment": "Accepted to Interspeech2025", "summary": "Spoofed utterances always contain artifacts introduced by generative models.\nWhile several countermeasures have been proposed to detect spoofed utterances,\nmost primarily focus on architectural improvements. In this work, we\ninvestigate how artifacts remain hidden in spoofed speech and how to enhance\ntheir presence. We propose a model-agnostic pipeline that amplifies artifacts\nusing speech enhancement and various types of noise. Our approach consists of\nthree key steps: noise addition, noise extraction, and noise amplification.\nFirst, we introduce noise into the raw speech. Then, we apply speech\nenhancement to extract the entangled noise and artifacts. Finally, we amplify\nthese extracted features. Moreover, our pipeline is compatible with different\nspeech enhancement models and countermeasure architectures. Our method improves\nspoof detection performance by up to 44.44\\% on ASVspoof2019 and 26.34\\% on\nASVspoof2021."}
{"id": "2506.11160", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11160", "abs": "https://arxiv.org/abs/2506.11160", "authors": ["Yu Pan", "Yuguang Yang", "Yanni Hu", "Jianhao Ye", "Xiang Zhang", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "title": "S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech Translation Framework via Seamlessly Speech-Text Alignment and Streaming Speech Decoder", "comment": "Working in progress", "summary": "Multilingual speech-to-speech translation (S2ST) aims to directly convert\nspoken utterances from multiple source languages into natural and intelligible\nspeech in a target language. Despite recent progress, significant challenges\nremain: (1) achieving high-quality and low-latency S2ST remains a critical\nhurdle; (2) existing S2ST approaches heavily rely on large-scale parallel\nspeech corpora, which are extremely difficult to collect. To address these\nissues, we propose S2ST-Omni, an efficient and scalable framework for\nmultilingual speech-to-speech translation. Specifically, we decompose the S2ST\ntask into speech-to-text translation (S2TT) and text-to-speech synthesis (TTS),\nunifying them within a single end-to-end speech-language model. To achieve\nhigh-quality S2TT while reducing dependence on parallel corpora, we leverage\nlarge-scale pretrained models -- Whisper for audio understanding and Qwen 3.0\nfor text understanding. A lightweight speech adapter is introduced to align\nspeech and text representations, enabling effective use of pretrained\nmultimodal knowledge. To ensure both translation quality and real-time\nperformance, we adopt a pretrained streaming speech decoder in the TTS stage to\ngenerate target speech in an autoregressive manner. Extensive experiments on\nthe CVSS benchmark demonstrate that S2ST-Omni outperforms state-of-the-art S2ST\nbaselines while maintaining comparable latency, highlighting its effectiveness\nand practical potential for real-world deployment."}
{"id": "2506.11605", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11605", "abs": "https://arxiv.org/abs/2506.11605", "authors": ["Alexis Plaquet", "Naohiro Tawara", "Marc Delcroix", "Shota Horiguchi", "Atsushi Ando", "Shoko Araki", "Hervé Bredin"], "title": "Dissecting the Segmentation Model of End-to-End Diarization with Vector Clustering", "comment": "37 pages, 18 figures. Submitted to Computer Speech & Language", "summary": "End-to-End Neural Diarization with Vector Clustering is a powerful and\npractical approach to perform Speaker Diarization. Multiple enhancements have\nbeen proposed for the segmentation model of these pipelines, but their synergy\nhad not been thoroughly evaluated. In this work, we provide an in-depth\nanalysis on the impact of major architecture choices on the performance of the\npipeline. We investigate different encoders (SincNet, pretrained and finetuned\nWavLM), different decoders (LSTM, Mamba, and Conformer), different losses\n(multilabel and multiclass powerset), and different chunk sizes. Through\nin-depth experiments covering nine datasets, we found that the finetuned\nWavLM-based encoder always results in the best systems by a wide margin. The\nLSTM decoder is outclassed by Mamba- and Conformer-based decoders, and while we\nfound Mamba more robust to other architecture choices, it is slightly inferior\nto our best architecture, which uses a Conformer encoder. We found that\nmultilabel and multiclass powerset losses do not have the same distribution of\nerrors. We confirmed that the multiclass loss helps almost all models attain\nsuperior performance, except when finetuning WavLM, in which case, multilabel\nis the superior choice. We also evaluated the impact of the chunk size on all\naforementioned architecture choices and found that newer architectures tend to\nbetter handle long chunk sizes, which can greatly improve pipeline performance.\nOur best system achieved state-of-the-art results on five widely used speaker\ndiarization datasets."}
{"id": "2506.11169", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11169", "abs": "https://arxiv.org/abs/2506.11169", "authors": ["Soumen Garai", "Suman Samui"], "title": "Advances in Small-Footprint Keyword Spotting: A Comprehensive Review of Efficient Models and Algorithms", "comment": "61 pages, 21 figures", "summary": "Small-Footprint Keyword Spotting (SF-KWS) has gained popularity in today's\nlandscape of smart voice-activated devices, smartphones, and Internet of Things\n(IoT) applications. This surge is attributed to the advancements in Deep\nLearning, enabling the identification of predefined words or keywords from a\ncontinuous stream of words. To implement the SF-KWS model on edge devices with\nlow power and limited memory in real-world scenarios, a efficient Tiny Machine\nLearning (TinyML) framework is essential. In this study, we explore seven\ndistinct categories of techniques namely, Model Architecture, Learning\nTechniques, Model Compression, Attention Awareness Architecture, Feature\nOptimization, Neural Network Search, and Hybrid Approaches, which are suitable\nfor developing an SF-KWS system. This comprehensive overview will serve as a\nvaluable resource for those looking to understand, utilize, or contribute to\nthe field of SF-KWS. The analysis conducted in this work enables the\nidentification of numerous potential research directions, encompassing insights\nfrom automatic speech recognition research and those specifically pertinent to\nthe realm of spoken SF-KWS."}
{"id": "2506.11620", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11620", "abs": "https://arxiv.org/abs/2506.11620", "authors": ["Stefan Bleeck"], "title": "(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test", "comment": null, "summary": "Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials."}
{"id": "2506.11514", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11514", "abs": "https://arxiv.org/abs/2506.11514", "authors": ["Xingwei Sun", "Heinrich Dinkel", "Yadong Niu", "Linzhang Wang", "Junbo Zhang", "Jian Luan"], "title": "Efficient Speech Enhancement via Embeddings from Pre-trained Generative Audioencoders", "comment": "Accepted by Interspeech 2025", "summary": "Recent research has delved into speech enhancement (SE) approaches that\nleverage audio embeddings from pre-trained models, diverging from\ntime-frequency masking or signal prediction techniques. This paper introduces\nan efficient and extensible SE method. Our approach involves initially\nextracting audio embeddings from noisy speech using a pre-trained audioencoder,\nwhich are then denoised by a compact encoder network. Subsequently, a vocoder\nsynthesizes the clean speech from denoised embeddings. An ablation study\nsubstantiates the parameter efficiency of the denoise encoder with a\npre-trained audioencoder and vocoder. Experimental results on both speech\nenhancement and speaker fidelity demonstrate that our generative\naudioencoder-based SE system outperforms models utilizing discriminative\naudioencoders. Furthermore, subjective listening tests validate that our\nproposed system surpasses an existing state-of-the-art SE model in terms of\nperceptual quality."}
{"id": "2506.11811", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.11811", "abs": "https://arxiv.org/abs/2506.11811", "authors": ["Jing Liu", "EnQi Lian"], "title": "Abstract Sound Fusion with Unconditioned Inversion Model", "comment": null, "summary": "An abstract sound is defined as a sound that does not disclose identifiable\nreal-world sound events to a listener. Sound fusion aims to synthesize an\noriginal sound and a reference sound to generate a novel sound that exhibits\nauditory features beyond mere additive superposition of the sound constituents.\nTo achieve this fusion, we employ inversion techniques that preserve essential\nfeatures of the original sample while enabling controllable synthesis. We\npropose novel SDE and ODE inversion models based on DPMSolver++ samplers that\nreverse the sampling process by configuring model outputs as constants,\neliminating circular dependencies incurred by noise prediction terms. Our\ninversion approach requires no prompt conditioning while maintaining flexible\nguidance during sampling."}
{"id": "2506.11532", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.11532", "abs": "https://arxiv.org/abs/2506.11532", "authors": ["Wen Huang", "Xuechen Liu", "Xin Wang", "Junichi Yamagishi", "Yanmin Qian"], "title": "From Sharpness to Better Generalization for Speech Deepfake Detection", "comment": "Accepted to Interspeech 2025", "summary": "Generalization remains a critical challenge in speech deepfake detection\n(SDD). While various approaches aim to improve robustness, generalization is\ntypically assessed through performance metrics like equal error rate without a\ntheoretical framework to explain model performance. This work investigates\nsharpness as a theoretical proxy for generalization in SDD. We analyze how\nsharpness responds to domain shifts and find it increases in unseen conditions,\nindicating higher model sensitivity. Based on this, we apply Sharpness-Aware\nMinimization (SAM) to reduce sharpness explicitly, leading to better and more\nstable performance across diverse unseen test sets. Furthermore, correlation\nanalysis confirms a statistically significant relationship between sharpness\nand generalization in most test settings. These findings suggest that sharpness\ncan serve as a theoretical indicator for generalization in SDD and that\nsharpness-aware training offers a promising strategy for improving robustness."}
{"id": "2506.11862", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.11862", "abs": "https://arxiv.org/abs/2506.11862", "authors": ["Xiaodan Chen", "Xiaoxue Gao", "Mathias Quoy", "Alexandre Pitti", "Nancy F. Chen"], "title": "Confidence-Based Self-Training for EMG-to-Speech: Leveraging Synthetic EMG for Robust Modeling", "comment": null, "summary": "Voiced Electromyography (EMG)-to-Speech (V-ETS) models reconstruct speech\nfrom muscle activity signals, facilitating applications such as\nneurolaryngologic diagnostics. Despite its potential, the advancement of V-ETS\nis hindered by a scarcity of paired EMG-speech data. To address this, we\npropose a novel Confidence-based Multi-Speaker Self-training (CoM2S) approach,\nalong with a newly curated Libri-EMG dataset. This approach leverages synthetic\nEMG data generated by a pre-trained model, followed by a proposed filtering\nmechanism based on phoneme-level confidence to enhance the ETS model through\nthe proposed self-training techniques. Experiments demonstrate our method\nimproves phoneme accuracy, reduces phonological confusion, and lowers word\nerror rate, confirming the effectiveness of our CoM2S approach for V-ETS. In\nsupport of future research, we will release the codes and the proposed\nLibri-EMG dataset-an open-access, time-aligned, multi-speaker voiced EMG and\nspeech recordings."}
{"id": "2506.12008", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12008", "abs": "https://arxiv.org/abs/2506.12008", "authors": ["Olga Vechtomova", "Jeff Bos"], "title": "Reimagining Dance: Real-time Music Co-creation between Dancers and AI", "comment": "Accepted for publication at ICCC 2025 (International Conference on\n  Computational Creativity)", "summary": "Dance performance traditionally follows a unidirectional relationship where\nmovement responds to music. While AI has advanced in various creative domains,\nits application in dance has primarily focused on generating choreography from\nmusical input. We present a system that enables dancers to dynamically shape\nmusical environments through their movements. Our multi-modal architecture\ncreates a coherent musical composition by intelligently combining pre-recorded\nmusical clips in response to dance movements, establishing a bidirectional\ncreative partnership where dancers function as both performers and composers.\nThrough correlation analysis of performance data, we demonstrate emergent\ncommunication patterns between movement qualities and audio features. This\napproach reconceptualizes the role of AI in performing arts as a responsive\ncollaborator that expands possibilities for both professional dance performance\nand improvisational artistic expression across broader populations."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [eess.AS](#eess.AS) [Total: 17]
- [cs.SD](#cs.SD) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Brain2Vec: A Deep Learning Framework for EEG-Based Stress Detection Using CNN-LSTM-Attention](https://arxiv.org/abs/2506.11179)
*Md Mynoddin,Troyee Dev,Rishita Chakma*

Main category: eess.SP

TL;DR: Brain2Vec是一种基于EEG信号的深度学习工具，通过混合架构（卷积、循环和注意力机制）分类压力状态，在DEAP数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题日益普遍，需要非侵入性诊断工具。EEG信号虽直接反映神经活动，但其非平稳性和高维度特性带来建模挑战。

Method: 采用混合架构：卷积层捕捉空间依赖，LSTM层建模时序模式，注意力机制突出关键时间区域。预处理包括带通滤波、z-score归一化和分段。

Result: 在DEAP数据集上，AUC得分为0.68，验证准确率为81.25%，优于传统CNN-LSTM基线。

Conclusion: Brain2Vec有望集成到可穿戴压力监测平台和个性化医疗系统中。

Abstract: Mental stress has become a pervasive factor affecting cognitive health and
overall well-being, necessitating the development of robust, non-invasive
diagnostic tools. Electroencephalogram (EEG) signals provide a direct window
into neural activity, yet their non-stationary and high-dimensional nature
poses significant modeling challenges. Here we introduce Brain2Vec, a new deep
learning tool that classifies stress states from raw EEG recordings using a
hybrid architecture of convolutional, recurrent, and attention mechanisms. The
model begins with a series of convolutional layers to capture localized spatial
dependencies, followed by an LSTM layer to model sequential temporal patterns,
and concludes with an attention mechanism to emphasize informative temporal
regions. We evaluate Brain2Vec on the DEAP dataset, applying bandpass
filtering, z-score normalization, and epoch segmentation as part of a
comprehensive preprocessing pipeline. Compared to traditional CNN-LSTM
baselines, our proposed model achieves an AUC score of 0.68 and a validation
accuracy of 81.25%. These findings demonstrate Brain2Vec's potential for
integration into wearable stress monitoring platforms and personalized
healthcare systems.

</details>


### [2] [Design of 3D Beamforming and Deployment Strategies for ISAC-based HAPS Systems](https://arxiv.org/abs/2506.11294)
*Xue Zhang,Bang Huang,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 本文研究了基于集成感知与通信（ISAC）的高空平台站（HAPS）系统，通过同时传输通信信号和合成孔径雷达（SAR）成像信号，实现多用户通信与地面目标感知。提出了两种HAPS部署策略，并通过联合优化部署策略和三维波束成形，最大化通信用户加权和速率，同时满足SAR成像要求。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用HAPS系统结合ISAC技术，实现通信与感知的双重功能，解决传统系统中资源分配和性能平衡的挑战。

Method: 方法包括两种HAPS部署策略（准静态和动态），并通过非凸优化问题建模，提出高效的凸与非凸优化算法求解。

Result: 数值结果表明，所提方法在通信速率和SAR成像性能上优于基准方案。

Conclusion: 结论表明，所提出的HAPS部署策略和优化算法能有效平衡通信与感知需求，为未来ISAC系统设计提供了实用方案。

Abstract: This paper explores high-altitude platform station (HAPS) systems enabled by
integrated sensing and communication (ISAC), in which a HAPS simultaneously
transmits communication signals and synthetic aperture radar (SAR) imaging
signals to support multi-user communication while performing ground target
sensing. Taking into account the operational characteristics of SAR imaging, we
consider two HAPS deployment strategies: (i) a quasi-stationary HAPS that
remains fixed at an optimized location during SAR operation, following the
stop-and-go scanning model; and (ii) a dynamic HAPS that continuously adjusts
its flight trajectory along a circular path. For each strategy, we aim at
maximizing the weighted sum-rate throughput for communication users while
ensuring that SAR imaging requirements, such as beampattern gain and
signal-to-noise ratio (SNR), are satisfied. This is achieved by jointly
optimizing the HAPS deployment strategy, i.e., its placement or trajectory,
along with three-dimensional (3D) transmit beamforming, under practical
constraints including transmit power limits, energy consumption, and flight
dynamics. Nevertheless, the formulated optimization problems corresponding to
the two deployment strategies are inherently non-convex. To address the issue,
we propose efficient algorithms that leverage both convex and non-convex
optimization techniques to obtain high-quality suboptimal solutions. Numerical
results demonstrate the effectiveness and advantages of the proposed approaches
over benchmark schemes.

</details>


### [3] [A Compact Dynamic Omnidirectional Antenna](https://arxiv.org/abs/2506.11351)
*Sheng Huang,Jacob R. Randall,Cory Hilton,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 提出一种新型全向天线设计，结合定向调制实现安全的窄平面信息传输，具有紧凑尺寸和稳定性能。


<details>
  <summary>Details</summary>
Motivation: 增强平面物理信息安全性，同时保持全向辐射性能。

Method: 采用两个紧密排列的印刷曲折线单极天线，通过差分功率激励和实时动态切换实现窄信息安全区域。

Result: 实验验证了16-QAM和256-QAM传输下的窄E平面信息束（约34°和15°）和全向H平面信息束。

Conclusion: 该天线设计简单有效，可提升信息安全性，适用于紧凑动态天线系统。

Abstract: We propose a novel omnidirectional antenna design incorporating directional
modulation for secure narrow planar information transmission. The proposed
antenna features a compact size and stable omnidirectional radiation
performance by employing two tightly spaced, printed meander line monopole
antennas, acting as a single radiating element. To achieve a narrow information
secure region, the proposed antenna is fed by differential power excitation of
two ports with real-time dynamic switching. This leads to phase pattern
modulation only along the electrical polarization, resulting in directionally
confined information recoverable region in the E-plane, while maintaining
highly constant or static omnidirectional H-plane pattern, inducing a
$360^\circ$ information recoverable region. The dynamic antenna is designed and
fabricated on a single layer of Rogers RO4350B which provides a miniaturized
planar size of $0.36 \times 0.5 , \lambda_0^2$ at 2.7 GHz and easy integration.
To validate the wireless communication performance, the fabricated antenna is
directly fed with a 10 dB power ratio by a radio frequency (RF) switching
system and evaluated for 16-QAM and 256-QAM transmission in a high
signal-to-noise ratio (SNR) environment. Experimental results demonstrate that
for 16-QAM transmission, a narrow E-plane information beam (IB) of
approximately $34^\circ$ and omnidirectional H-plane IB are obtained, and a
narrower E-plane IB is achieved around $15^\circ$ for 256-QAM. These results
confirm that the proposed antenna offers a simple yet effective approach to
enhance planar physical information security with a compact dynamic antenna
system.

</details>


### [4] [Movable-Antenna Array Enhanced Downlink NOMA](https://arxiv.org/abs/2506.11438)
*Nianzu Li,Peiran Wu,Lipeng Zhu,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 论文研究了移动天线（MA）阵列在非正交多址（NOMA）下行链路系统中的资源分配设计，通过联合优化波束成形和天线位置，显著提升了系统总速率。


<details>
  <summary>Details</summary>
Motivation: 移动天线因其能主动重构无线信道的特性受到关注，本文旨在探索其在NOMA系统中提升性能的潜力。

Method: 采用逐次凸近似（SCA）和交替优化方法，解决高度非凸的优化问题。

Result: 仿真结果表明，MA增强的NOMA系统在总速率上优于固定天线（FPA）系统和传统正交多址（OMA）系统。

Conclusion: 移动天线在NOMA系统中具有显著性能优势，为未来无线通信提供了新思路。

Abstract: Movable antenna (MA) has gained increasing attention in the field of wireless
communications due to its exceptional capability to proactively reconfigure
wireless channels via localized antenna movements. In this paper, we
investigate the resource allocation design for an MA array-enabled base station
serving multiple single-antenna users in a downlink non-orthogonal multiple
access (NOMA) system. We aim to maximize the sum rate of all users by jointly
optimizing the transmit beamforming and the positions of all MAs at the BS,
subject to the constraints of transmit power budget, finite antenna moving
region, and the conditions for successive interference cancellation decoding
rate. The formulated problem, inherently highly non-convex, is addressed by
successive convex approximation (SCA) and alternating optimization methods to
obtain a high-quality suboptimal solution. Simulation results unveil that the
proposed MA-enhanced downlink NOMA system can significantly improve the sum
rate performance compared to both the fixed-position antenna (FPA) system and
the traditional orthogonal multiple access (OMA) system.

</details>


### [5] [Joint Angle and Velocity-Estimation for Target Localization in Bistatic mmWave MIMO Radar in the Presence of Clutter](https://arxiv.org/abs/2506.11497)
*Priyanka Maity,Suraj Srivastava,Aditya K. Jagannatham,Lajos Hanzo*

Main category: eess.SP

TL;DR: 提出一种基于稀疏贝叶斯学习（SBL）的双基地毫米波MIMO雷达目标定位方法，用于未知杂波环境，并通过角度-多普勒（AD）域模型实现高精度参数估计。


<details>
  <summary>Details</summary>
Motivation: 解决在未知杂波环境下，双基地毫米波MIMO雷达系统中目标参数（如AoD、AoA和速度）的高精度估计问题。

Method: 利用AD域的三维稀疏性，结合SBL框架估计目标参数，并开发超分辨率离网格SBL框架以处理参数偏离网格的情况。

Result: 仿真结果表明，所提算法性能优于现有方法，并能接近推导的Cramér-Rao界和贝叶斯CRB。

Conclusion: 该方法在目标参数估计中表现出色，尤其在复杂杂波环境下具有显著优势。

Abstract: Sparse Bayesian learning (SBL)-aided target localization is conceived for a
bistatic mmWave MIMO radar system in the presence of unknown clutter, followed
by the development of an angle-Doppler (AD)-domain representation of the
target-plus-clutter echo model for accurate target parameter estimation. The
proposed algorithm exploits the three-dimensional (3D) sparsity arising in the
AD domain of the scattering scene and employs the powerful SBL framework for
the estimation of target parameters, such as the angle-of-departure (AoD),
angle-of-arrival (AoA) and velocity. To handle a practical scenario where the
actual target parameters typically deviate from their finite-resolution grid, a
super-resolution-based improved off-grid SBL framework is developed for
recursively updating the parameter grid, thereby progressively refining the
estimates. We also determine the Cram\'er-Rao bound (CRB) and Bayesian CRB for
target parameter estimation in order to benchmark the estimation performance.
Our simulation results corroborate the superior performance of the proposed
approach in comparison to the existing algorithms, and also their ability to
approach the bounds derived.

</details>


### [6] [MMWiLoc: A Multi-Sensor Dataset and Robust Device-Free Localization Method Using Commercial Off-The-Shelf Millimeter Wave Wi-Fi Devices](https://arxiv.org/abs/2506.11540)
*Wenbo Ding,Yang Li,Dongsheng Wang,Bin Zhao,Yunrong Zhu,Yibo Zhang,Yumeng Miao*

Main category: eess.SP

TL;DR: 论文提出了一种基于毫米波Wi-Fi的无设备定位方法MMWiLoc，并发布了一个多传感器数据集，支持厘米级精确定位。


<details>
  <summary>Details</summary>
Motivation: 毫米波Wi-Fi在无设备感知中具有潜力，但缺乏定位方法，因此需要开发高效且精确的解决方案。

Method: MMWiLoc结合了波束模式校准（使用期望最大化）和目标定位（多尺度压缩感知），通过处理波束信噪比信息确定目标到达角。

Result: MMWiLoc实现了厘米级定位精度，优于2.4GHz Wi-Fi系统，与高精度雷达系统性能相当。

Conclusion: MMWiLoc为毫米波Wi-Fi定位提供了高效解决方案，数据集和代码将开源以促进研究。

Abstract: Device-free Wi-Fi sensing has numerous benefits in practical settings, as it
eliminates the requirement for dedicated sensing devices and can be
accomplished using current low-cost Wi-Fi devices. With the development of
Wi-Fi standards, millimeter wave Wi-Fi devices with 60GHz operating frequency
and up to 4GHz bandwidth have become commercially available. Although
millimeter wave Wi-Fi presents great promise for Device-Free Wi-Fi sensing with
increased bandwidth and beam-forming ability, there still lacks a method for
localization using millimeter wave Wi-Fi. Here, we present two major
contributions: First, we provide a comprehensive multi-sensor dataset that
synchronously captures human movement data from millimeter wave Wi-Fi, 2.4GHz
Wi-Fi, and millimeter wave radar sensors. This dataset enables direct
performance comparisons across different sensing modalities and facilitates
reproducible researches in indoor localization. Second, we introduce MMWiLoc, a
novel localization method that achieves centimeter-level precision with low
computational cost. MMWiLoc incorporates two components: beam pattern
calibration using Expectation Maximization and target localization through
Multi-Scale Compression Sensing. The system processes beam Signal-to-Noise
Ratio (beamSNR) information from the beam-forming process to determine target
Angle of Arrival (AoA), which is then fused across devices for localization.
Our extensive evaluation demonstrates that MMWiLoc achieves centimeter-level
precision, outperforming 2.4GHz Wi-Fi systems while maintaining competitive
performance with high-precision radar systems. The dataset and examples
processing code will be released after this paper is accepted at
https://github.com/wowoyoho/MMWiLoc.

</details>


### [7] [Energy Efficiency Optimization of Finite Block Length STAR-RIS-aided MU-MIMO Broadcast Channels](https://arxiv.org/abs/2506.11594)
*Mohammad Soleymani,Ignacio Santamaria,Eduard Jorswieck,Robert Schober,Lajos Hanzo*

Main category: eess.SP

TL;DR: 提出了一种基于STAR-RIS的多用户MIMO广播信道的能效设计，证明其在有限块长度下能显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 研究如何在多用户MIMO广播信道中利用STAR-RIS提升能效，尤其是在严格延迟和可靠性要求的场景下。

Method: 通过最大化总能量效率（EE），分析STAR-RIS在不同块长度和误码率条件下的性能。

Result: STAR-RIS能显著提升能效，且在更短的码字长度和更低的误码率要求下效果更明显。

Conclusion: STAR-RIS在严格延迟和可靠性要求的系统中更具能效优势。

Abstract: Energy-efficient designs are proposed for multi-user (MU) multiple-input
multiple-output (MIMO) broadcast channels (BC), assisted by simultaneously
transmitting and reflecting (STAR) reconfigurable intelligent surfaces (RIS)
operating at finite block length (FBL). In particular, we maximize the sum
energy efficiency (EE), showing that STAR-RIS can substantially enhance it. Our
findings demonstrate that the gains of employing STAR-RIS increase when the
codeword length and the maximum tolerable bit error rate decrease, meaning that
a STAR-RIS is more energy efficient in a system with more stringent latency and
reliability requirements.

</details>


### [8] [FieldFormer: Self-supervised Reconstruction of Physical Fields via Tensor Attention Prior](https://arxiv.org/abs/2506.11629)
*Panqi Chen,Siyuan Li,Lei Cheng,Xiao Fu,Yik-Chung Wu,Sergios Theodoridis*

Main category: eess.SP

TL;DR: 该论文提出了一种名为FieldFormer的自监督神经网络方法，用于从有限的现场观测数据中重建物理场张量，无需离线训练，通过Tucker分解和注意力机制实现高效表示。


<details>
  <summary>Details</summary>
Motivation: 物理场张量重建在无线通信和水下声学等应用中至关重要，但现有深度学习方法存在训练与测试阶段模型不匹配的问题。

Method: 采用Tucker分解建模高多线性秩张量，结合注意力机制学习核心张量的稀疏模式，实现自监督学习。

Result: 理论分析和实验表明，FieldFormer在多种物理场张量重建任务中优于现有方法。

Conclusion: FieldFormer通过自监督学习和Tucker分解提供了一种灵活且高效的物理场重建方法。

Abstract: Reconstructing physical field tensors from \textit{in situ} observations,
such as radio maps and ocean sound speed fields, is crucial for enabling
environment-aware decision making in various applications, e.g., wireless
communications and underwater acoustics. Field data reconstruction is often
challenging, due to the limited and noisy nature of the observations,
necessitating the incorporation of prior information to aid the reconstruction
process. Deep neural network-based data-driven structural constraints (e.g.,
``deeply learned priors'') have showed promising performance. However, this
family of techniques faces challenges such as model mismatches between training
and testing phases. This work introduces FieldFormer, a self-supervised neural
prior learned solely from the limited {\it in situ} observations without the
need of offline training. Specifically, the proposed framework starts with
modeling the fields of interest using the tensor Tucker model of a high
multilinear rank, which ensures a universal approximation property for all
fields. In the sequel, an attention mechanism is incorporated to learn the
sparsity pattern that underlies the core tensor in order to reduce the solution
space.
  In this way, a ``complexity-adaptive'' neural representation, grounded in the
Tucker decomposition, is obtained that can flexibly represent
  various types of fields. A theoretical analysis is provided to support the
recoverability of the proposed design. Moreover, extensive experiments, using
various physical field tensors, demonstrate the superiority of the proposed
approach compared to state-of-the-art baselines.

</details>


### [9] [Recursive KalmanNet: Deep Learning-Augmented Kalman Filtering for State Estimation with Consistent Uncertainty Quantification](https://arxiv.org/abs/2506.11639)
*Hassan Mortada,Cyril Falcon,Yanis Kahil,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: 论文提出了一种基于卡尔曼滤波的递归神经网络Recursive KalmanNet，用于在非高斯噪声条件下实现精确状态估计和误差协方差量化。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的状态估计常偏离卡尔曼滤波的线性高斯假设，需要数据驱动的滤波技术。

Method: 采用递归Joseph公式传播误差协方差，并优化高斯负对数似然。

Result: 在非高斯测量白噪声实验中，模型优于传统卡尔曼滤波和现有深度学习估计器。

Conclusion: Recursive KalmanNet在复杂噪声条件下表现出色，为状态估计提供了新思路。

Abstract: State estimation in stochastic dynamical systems with noisy measurements is a
challenge. While the Kalman filter is optimal for linear systems with
independent Gaussian white noise, real-world conditions often deviate from
these assumptions, prompting the rise of data-driven filtering techniques. This
paper introduces Recursive KalmanNet, a Kalman-filter-informed recurrent neural
network designed for accurate state estimation with consistent error covariance
quantification. Our approach propagates error covariance using the recursive
Joseph's formula and optimizes the Gaussian negative log-likelihood.
Experiments with non-Gaussian measurement white noise demonstrate that our
model outperforms both the conventional Kalman filter and an existing
state-of-the-art deep learning based estimator.

</details>


### [10] [Deep Learning-based mmWave MIMO Channel Estimation using sub-6 GHz Channel Information: CNN and UNet Approaches](https://arxiv.org/abs/2506.11714)
*Faruk Pasic,Lukas Eller,Stefan Schwarz,Markus Rupp,Christoph F. Mecklenbräuker*

Main category: eess.SP

TL;DR: 论文提出两种基于深度学习的毫米波MIMO信道估计方法，利用sub-6 GHz频段的带外信息，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 未来无线MIMO系统需整合sub-6 GHz和毫米波频段以满足高数据速率需求，但毫米波频段的信道估计因低信噪比而具挑战性。

Method: 提出两种深度学习方法：卷积神经网络（CNN）和UNet架构，利用sub-6 GHz频段的带外信息进行毫米波MIMO信道估计。

Result: 仿真结果表明，所提方法在频谱效率上优于仅依赖带内信息的深度学习方法和现有带外辅助方法。

Conclusion: 利用带外信息的深度学习方法能有效提升毫米波MIMO信道估计性能。

Abstract: Future wireless multiple-input multiple-output (MIMO) systems will integrate
both sub-6 GHz and millimeter wave (mmWave) frequency bands to meet the growing
demands for high data rates. MIMO link establishment typically requires
accurate channel estimation, which is particularly challenging at mmWave
frequencies due to the low signal-to-noise ratio (SNR). In this paper, we
propose two novel deep learning-based methods for estimating mmWave MIMO
channels by leveraging out-of-band information from the sub-6 GHz band. The
first method employs a convolutional neural network (CNN), while the second
method utilizes a UNet architecture. We compare these proposed methods against
deep-learning methods that rely solely on in-band information and with other
state-of-the-art out-of-band aided methods. Simulation results show that our
proposed out-of-band aided deep-learning methods outperform existing
alternatives in terms of achievable spectral efficiency.

</details>


### [11] [Semantic Communications in 6G: Coexistence, Multiple Access, and Satellite Networks](https://arxiv.org/abs/2506.11779)
*Ishtiaque Ahmed,Yingzhuo Sun,Jingwen Fu,Alper Kose,Leila Musavian,Ming Xiao,Berna Ozbek*

Main category: eess.SP

TL;DR: 论文探讨了语义通信（SemCom）与传统比特通信（BitCom）在异构网络中的集成，分析了多址技术和多模态框架，并展望了6G中的应用。


<details>
  <summary>Details</summary>
Motivation: 无线用户和带宽限制的指数增长需要下一代网络的创新通信范式，语义通信通过传输提取的语义而非原始比特，提高了频谱效率。

Method: 研究了语义通信与传统比特通信的集成，分析了非正交多址（NOMA）等技术，并探讨了多模态语义通信框架。

Result: 语义通信在卫星网络等场景中能缓解带宽限制和恶劣信道条件，展示了其潜力。

Conclusion: 语义通信是未来6G及更远网络的重要方向，但仍需解决共存技术和应用挑战。

Abstract: The exponential growth of wireless users and bandwidth constraints
necessitates innovative communication paradigms for next-generation networks.
Semantic Communication (SemCom) emerges as a promising solution by transmitting
extracted meaning rather than raw bits, enhancing spectral efficiency and
enabling intelligent resource allocation. This paper explores the integration
of SemCom with conventional Bit-based Communication (BitCom) in heterogeneous
networks, highlighting key challenges and opportunities. We analyze multiple
access techniques, including Non-Orthogonal Multiple Access (NOMA), to support
coexisting SemCom and BitCom users. Furthermore, we examine multi-modal SemCom
frameworks for handling diverse data types and discuss their applications in
satellite networks, where semantic techniques mitigate bandwidth limitations
and harsh channel conditions. Finally, we identify future directions for
deploying semantic-aware systems in 6G and beyond.

</details>


### [12] [Diffusion-Based Electrocardiography Noise Quantification via Anomaly Detection](https://arxiv.org/abs/2506.11815)
*Tae-Seong Han,Jae-Wook Heo,Hakseung Kim,Cheol-Hui Lee,Hyub Huh,Eue-Keun Choi,Dong-Joo Kim*

Main category: eess.SP

TL;DR: 提出了一种基于扩散模型的ECG噪声量化方法，通过重建异常检测解决标注不一致和传统方法泛化性差的问题，显著提升了噪声量化性能。


<details>
  <summary>Details</summary>
Motivation: ECG信号常受噪声干扰，影响临床和可穿戴设备的诊断准确性，传统方法存在标注不一致和泛化性差的问题。

Method: 采用扩散模型框架，通过Wasserstein-1距离（$W_1$）比较干净和噪声ECG的重建误差分布，仅需三步反向扩散即可实现噪声量化。

Result: 模型在基准测试中取得1.308的宏观平均$W_1$分数，优于次优方法48%以上，外部验证显示强泛化性。

Conclusion: 该方法提升了临床决策、诊断准确性和实时ECG监测能力，为临床和可穿戴ECG应用提供了支持。

Abstract: Electrocardiography (ECG) signals are often degraded by noise, which
complicates diagnosis in clinical and wearable settings. This study proposes a
diffusion-based framework for ECG noise quantification via reconstruction-based
anomaly detection, addressing annotation inconsistencies and the limited
generalizability of conventional methods. We introduce a distributional
evaluation using the Wasserstein-1 distance ($W_1$), comparing the
reconstruction error distributions between clean and noisy ECGs to mitigate
inconsistent annotations. Our final model achieved robust noise quantification
using only three reverse diffusion steps. The model recorded a macro-average
$W_1$ score of 1.308 across the benchmarks, outperforming the next-best method
by over 48%. External validations demonstrated strong generalizability,
supporting the exclusion of low-quality segments to enhance diagnostic accuracy
and enable timely clinical responses to signal degradation. The proposed method
enhances clinical decision-making, diagnostic accuracy, and real-time ECG
monitoring capabilities, supporting future advancements in clinical and
wearable ECG applications.

</details>


### [13] [Interference in Spectrum-Sharing Integrated Terrestrial and Satellite Networks: Modeling, Approximation, and Robust Transmit Beamforming](https://arxiv.org/abs/2506.11851)
*Wenjing Cao,Yafei Wang,Tianxiang Ji,Tianyang Cao,Wenjin Wang,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: 论文研究了基于统计信道状态信息的卫星到用户终端的鲁棒发射波束成形设计，旨在减轻频谱共享的卫星-地面网络中的干扰。


<details>
  <summary>Details</summary>
Motivation: 解决卫星与地面网络共享频谱时的干扰问题，尤其是在缺乏共享信道状态信息的情况下。

Method: 利用地面用户终端的分布信息建立干扰模型，开发了在干扰阈值和功率预算下的鲁棒波束成形方案，包括卫星加权和速率最大化和均方误差最小化两种优化标准。

Result: 通过迭代优化框架实现了较高的可达速率性能，同时通过低复杂度闭式解满足干扰约束。数值仿真验证了方案的有效性。

Conclusion: 提出的方法在减少干扰的同时优化了性能，适用于卫星-地面频谱共享网络。

Abstract: This paper investigates robust transmit (TX) beamforming from the satellite
to user terminals (UTs), based on statistical channel state information (CSI).
The proposed design specifically targets the mitigation of
satellite-to-terrestrial interference in spectrum-sharing integrated
terrestrial and satellite networks. By leveraging the distribution information
of terrestrial UTs, we first establish an interference model from the satellite
to terrestrial systems without shared CSI. Based on this, robust TX beamforming
schemes are developed under both the interference threshold and the power
budget. Two optimization criteria are considered: satellite weighted sum rate
maximization and mean square error minimization. The former achieves a superior
achievable rate performance through an iterative optimization framework,
whereas the latter enables a low-complexity closed-form solution at the expense
of reduced rate, with interference constraints satisfied via a bisection
method. To avoid complex integral calculations and the dependence on user
distribution information in inter-system interference evaluations, we propose a
terrestrial base station position-aided approximation method, and the
approximation errors are subsequently analyzed. Numerical simulations validate
the effectiveness of our proposed schemes.

</details>


### [14] [DMRS-Based Uplink Channel Estimation for MU-MIMO Systems with Location-Specific SCSI Acquisition](https://arxiv.org/abs/2506.11899)
*Jiawei Zhuang,Hongwei Hou,Minjie Tang,Wenjin Wang,Shi Jin,Vincent K. N. Lau*

Main category: eess.SP

TL;DR: 本文研究了基于上行DMRS的多用户MIMO系统信道估计，提出了一种利用位置特定统计信道信息（SCSI）的贝叶斯估计器（SA-BCE）及其窗口化版本（SA-WBCE），以降低计算复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着多用户MIMO系统用户数量的增加，如何高效利用正交覆盖码（OCC）减少导频干扰并提升信道估计性能成为关键问题。

Method: 提出基于最小均方误差准则的SCSI辅助贝叶斯估计器（SA-BCE），并通过窗口化处理（SA-WBCE）降低计算复杂度。同时，构建基于网格的位置特定SCSI数据库以减少实时获取需求。

Result: 仿真结果表明，所提出的SCSI数据库构建方法精度高，SA-BCE和SA-WBCE在多用户MIMO系统中性能显著优于现有方法。

Conclusion: 通过结合SCSI和贝叶斯估计，本文方法在降低复杂度的同时显著提升了信道估计性能，适用于实际系统。

Abstract: With the growing number of users in multi-user multiple-input multiple-output
(MU-MIMO) systems, demodulation reference signals (DMRSs) are efficiently
multiplexed in the code domain via orthogonal cover codes (OCC) to ensure
orthogonality and minimize pilot interference. In this paper, we investigate
uplink DMRS-based channel estimation for MU-MIMO systems with Type II OCC
pattern standardized in 3GPP Release 18, leveraging location-specific
statistical channel state information (SCSI) to enhance performance.
Specifically, we propose a SCSI-assisted Bayesian channel estimator (SA-BCE)
based on the minimum mean square error criterion to suppress the pilot
interference and noise, albeit at the cost of cubic computational complexity
due to matrix inversions. To reduce this complexity while maintaining
performance, we extend the scheme to a windowed version (SA-WBCE), which
incorporates antenna-frequency domain windowing and beam-delay domain
processing to exploit asymptotic sparsity and mitigate energy leakage in
practical systems. To avoid the frequent real-time SCSI acquisition, we
construct a grid-based location-specific SCSI database based on the principle
of spatial consistency, and subsequently leverage the uplink received signals
within each grid to extract the SCSI. Facilitated by the multilinear structure
of wireless channels, we formulate the SCSI acquisition problem within each
grid as a tensor decomposition problem, where the factor matrices are
parameterized by the multi-path powers, delays, and angles. The computational
complexity of SCSI acquisition can be significantly reduced by exploiting the
Vandermonde structure of the factor matrices. Simulation results demonstrate
that the proposed location-specific SCSI database construction method achieves
high accuracy, while the SA-BCE and SA-WBCE significantly outperform
state-of-the-art benchmarks in MU-MIMO systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [15] [PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding](https://arxiv.org/abs/2506.11064)
*Jiajun He,Tomoki Toda*

Main category: eess.AS

TL;DR: 论文提出了一种基于音素增强的多模态融合方法（PMF-CEC），用于改进ASR中罕见词和同音词的识别准确性，并通过保留概率机制优化错误检测。


<details>
  <summary>Details</summary>
Motivation: 解决ASR模型在识别罕见词和同音词时准确率低的问题，尤其是针对发音相似但拼写不同的词。

Method: 在ED-CEC基础上引入音素增强的多模态融合方法（PMF-CEC），并加入保留概率机制以过滤低置信度的编辑操作。

Result: 在五个数据集上，PMF-CEC在保持合理推理速度的同时，进一步降低了偏置词错误率，尤其在同音词纠正上表现更优。

Conclusion: PMF-CEC在罕见词和同音词纠正上优于其他上下文偏置方法，且比基于LLM的方法具有更快的推理速度和更好的鲁棒性。

Abstract: End-to-end automatic speech recognition (ASR) models often struggle to
accurately recognize rare words. Previously, we introduced an ASR
postprocessing method called error detection and context-aware error correction
(ED-CEC), which leverages contextual information such as named entities and
technical terms to improve the accuracy of ASR transcripts. Although ED-CEC
achieves a notable success in correcting rare words, its accuracy remains low
when dealing with rare words that have similar pronunciations but different
spellings. To address this issue, we proposed a phoneme-augmented multimodal
fusion method for context-aware error correction (PMF-CEC) method on the basis
of ED-CEC, which allowed for better differentiation between target rare words
and homophones. Additionally, we observed that the previous ASR error detection
module suffers from overdetection. To mitigate this, we introduced a retention
probability mechanism to filter out editing operations with confidence scores
below a set threshold, preserving the original operation to improve error
detection accuracy. Experiments conducted on five datasets demonstrated that
our proposed PMF-CEC maintains reasonable inference speed while further
reducing the biased word error rate compared with ED-CEC, showing a stronger
advantage in correcting homophones. Moreover, our method outperforms other
contextual biasing methods, and remains valuable compared with LLM-based
methods in terms of faster inference and better robustness under large biasing
lists.

</details>


### [16] [Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition](https://arxiv.org/abs/2506.11069)
*Tao Zhong,Mengzhe Geng,Shujie Hu,Guinan Li,Xunying Liu*

Main category: eess.AS

TL;DR: 本文研究了正则化联邦学习技术在隐私保护的构音障碍和老年语音识别中的应用，通过参数、嵌入和新型损失正则化方法，显著降低了词错误率。


<details>
  <summary>Details</summary>
Motivation: 构音障碍和老年语音识别的准确性仍具挑战性，而联邦学习在保护隐私的同时加剧了数据稀缺和不平衡问题。

Method: 采用参数、嵌入和新型损失正则化方法，优化联邦学习过程。

Result: 在UASpeech和DementiaBank数据集上，正则化联邦学习系统显著优于基线FedAvg系统，词错误率降低0.55%（相对2.13%）。

Conclusion: 正则化联邦学习能有效提升隐私保护的语音识别性能，接近集中式训练效果。

Abstract: Accurate recognition of dysarthric and elderly speech remains challenging to
date. While privacy concerns have driven a shift from centralized approaches to
federated learning (FL) to ensure data confidentiality, this further
exacerbates the challenges of data scarcity, imbalanced data distribution and
speaker heterogeneity. To this end, this paper conducts a systematic
investigation of regularized FL techniques for privacy-preserving dysarthric
and elderly speech recognition, addressing different levels of the FL process
by 1) parameter-based, 2) embedding-based and 3) novel loss-based
regularization. Experiments on the benchmark UASpeech dysarthric and
DementiaBank Pitt elderly speech corpora suggest that regularized FL systems
consistently outperform the baseline FedAvg system by statistically significant
WER reductions of up to 0.55\% absolute (2.13\% relative). Further increasing
communication frequency to one exchange per batch approaches centralized
training performance.

</details>


### [17] [Embedded Acoustic Intelligence for Automotive Systems](https://arxiv.org/abs/2506.11071)
*Renjith Rajagopal,Peter Winzell,Sladjana Strbac,Konstantin Lindström,Petter Hörling,Faisal Kohestani,Niloofar Mehrzad*

Main category: eess.AS

TL;DR: 利用声音数据分类道路类型，提升自动驾驶系统智能性。


<details>
  <summary>Details</summary>
Motivation: 通过声音数据增强汽车系统智能，改善驾驶舒适性和安全性，并为城市规划提供数据支持。

Method: 使用深度神经网络和预训练模型（如Hugging Face）分析车轮基座麦克风采集的声学特征。

Result: 成功分类道路类型，支持自适应学习和主动降噪，为下一代汽车系统提供商业案例。

Conclusion: 该方法有望提升乘客舒适度、车辆安全性，并推动智能城市道路管理。

Abstract: Transforming sound insights into actionable streams of data, this abstract
leverages findings from degree thesis research to enhance automotive system
intelligence, enabling us to address road type [1].By extracting and
interpreting acoustic signatures from microphones installed within the
wheelbase of a car, we focus on classifying road type.Utilizing deep neural
networks and feature extraction powered by pre-trained models from the Open AI
ecosystem (via Hugging Face [2]), our approach enables Autonomous Driving and
Advanced Driver- Assistance Systems (AD/ADAS) to anticipate road surfaces,
support adaptive learning for active road noise cancellation, and generate
valuable insights for urban planning. The results of this study were
specifically captured to support a compelling business case for next-generation
automotive systems. This forward-looking approach not only promises to redefine
passenger comfort and improve vehicle safety, but also paves the way for
intelligent, data-driven urban road management, making the future of mobility
both achievable and sustainable.

</details>


### [18] [Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling](https://arxiv.org/abs/2506.11072)
*Tahiya Chowdhury,Veronica Romero*

Main category: eess.AS

TL;DR: 论文评估了OpenSMILE和Praat两种语音分析工具在提取自闭症青少年语音特征时的可靠性，发现工具间存在显著差异，影响了模型性能，并建议进行领域相关验证以提高临床应用的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有语音处理工具缺乏验证，可能导致行为相关信息的捕捉不可靠，影响模型的可重复性和公平性，尤其是在不同人群和情境下。

Method: 通过评估OpenSMILE和Praat提取的语音特征，分析其在自闭症青少年中的可靠性及其对模型性能的影响。

Result: 不同工具提取的特征存在显著差异，这些差异影响了模型在不同情境和人口群体中的表现。

Conclusion: 建议进行领域相关的验证，以提高机器学习模型在临床应用中的可靠性。

Abstract: Machine learning-based behavioral models rely on features extracted from
audio-visual recordings. The recordings are processed using open-source tools
to extract speech features for classification models. These tools often lack
validation to ensure reliability in capturing behaviorally relevant
information. This gap raises concerns about reproducibility and fairness across
diverse populations and contexts. Speech processing tools, when used outside of
their design context, can fail to capture behavioral variations equitably and
can then contribute to bias. We evaluate speech features extracted from two
widely used speech analysis tools, OpenSMILE and Praat, to assess their
reliability when considering adolescents with autism. We observed considerable
variation in features across tools, which influenced model performance across
context and demographic groups. We encourage domain-relevant verification to
enhance the reliability of machine learning models in clinical applications.

</details>


### [19] [Challenges in Automated Processing of Speech from Child Wearables: The Case of Voice Type Classifier](https://arxiv.org/abs/2506.11074)
*Tarek Kunze,Marianne Métais,Hadrien Titeux,Lucas Elbert,Joseph Coffey,Emmanuel Dupoux,Alejandrina Cristia,Marvin Lavechin*

Main category: eess.AS

TL;DR: 论文探讨了儿童佩戴设备录音数据在语音科学中的应用障碍，指出改进特征、架构和参数搜索对性能提升有限，而数据相关性和数量更为关键。


<details>
  <summary>Details</summary>
Motivation: 利用儿童佩戴设备收集自然语音数据，以推动基础和应用语音科学的发展。

Method: 通过三年实验改进语音类型分类任务，测试了特征表示、架构和参数搜索等方法。

Result: 特征、架构和参数搜索的改进仅带来边际性能提升，数据相关性和数量对进展更为重要。

Conclusion: 收集和共享适当许可的数据是提升语音技术性能的关键。

Abstract: Recordings gathered with child-worn devices promised to revolutionize both
fundamental and applied speech sciences by allowing the effortless capture of
children's naturalistic speech environment and language production. This
promise hinges on speech technologies that can transform the sheer mounds of
data thus collected into usable information. This paper demonstrates several
obstacles blocking progress by summarizing three years' worth of experiments
aimed at improving one fundamental task: Voice Type Classification. Our
experiments suggest that improvements in representation features, architecture,
and parameter search contribute to only marginal gains in performance. More
progress is made by focusing on data relevance and quantity, which highlights
the importance of collecting data with appropriate permissions to allow
sharing.

</details>


### [20] [Fifteen Years of Child-Centered Long-Form Recordings: Promises, Resources, and Remaining Challenges to Validity](https://arxiv.org/abs/2506.11075)
*Loann Peurey,Marvin Lavechin,Tarek Kunze,Manel Khentout,Lucas Gautheron,Emmanuel Dupoux,Alejandrina Cristia*

Main category: eess.AS

TL;DR: 论文总结了儿童语言研究中使用穿戴设备收集音频数据的方法，强调自动化分析的必要性，并提出了数据质量评估的指标。


<details>
  <summary>Details</summary>
Motivation: 通过长时录音捕捉儿童语言输入和输出，减少观察者偏差，提高数据有效性。

Method: 总结现有技术资源，提出自动化分析中的误差来源及解决策略。

Result: 指出完全自动化的质量控制不可行，但提供了改进数据收集和分析的实际策略。

Conclusion: 研究者需结合自动化工具和人工策略，以确保数据质量和分析准确性。

Abstract: Audio-recordings collected with a child-worn device are a fundamental tool in
child language research. Long-form recordings collected over whole days promise
to capture children's input and production with minimal observer bias, and
therefore high validity. The sheer volume of resulting data necessitates
automated analysis to extract relevant metrics for researchers and clinicians.
This paper summarizes collective knowledge on this technique, providing entry
points to existing resources. We also highlight various sources of error that
threaten the accuracy of automated annotations and the interpretation of
resulting metrics. To address this, we propose potential troubleshooting
metrics to help users assess data quality. While a fully automated quality
control system is not feasible, we outline practical strategies for researchers
to improve data collection and contextualize their analyses.

</details>


### [21] [Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts](https://arxiv.org/abs/2506.11079)
*Lingyun Gao,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

TL;DR: 论文提出了一种多模态方法，结合音频和文本资源，利用Whisper和指令调优的大语言模型（LLMs）提升儿童语音识别和阅读错误检测的效果。


<details>
  <summary>Details</summary>
Motivation: 自动朗读评估可为教师提供高效评分支持，但相关研究有限。

Method: 采用Whisper和指令调优的LLMs，通过提示改进儿童语音识别转录，并评估其在阅读错误检测中的效果。

Result: 最佳系统在荷兰儿童朗读语音识别中达到5.1%的词错误率（WER），优于基线9.4%；阅读错误检测的F1分数从0.39提升至0.73。

Conclusion: 提示Whisper和LLMs显著提升了语音识别和阅读错误检测性能。

Abstract: Automatic reading aloud evaluation can provide valuable support to teachers
by enabling more efficient scoring of reading exercises. However, research on
reading evaluation systems and applications remains limited. We present a novel
multimodal approach that leverages audio and knowledge from text resources. In
particular, we explored the potential of using Whisper and instruction-tuned
large language models (LLMs) with prompts to improve transcriptions for child
speech recognition, as well as their effectiveness in downstream reading
mistake detection. Our results demonstrate the effectiveness of prompting
Whisper and prompting LLM, compared to the baseline Whisper model without
prompting. The best performing system achieved state-of-the-art recognition
performance in Dutch child read speech, with a word error rate (WER) of 5.1%,
improving the baseline WER of 9.4%. Furthermore, it significantly improved
reading mistake detection, increasing the F1 score from 0.39 to 0.73.

</details>


### [22] [Intelligibility of Text-to-Speech Systems for Mathematical Expressions](https://arxiv.org/abs/2506.11086)
*Sujoy Roychowdhury,H. G. Ranjani,Sumit Soman,Nishtha Paul,Subhadip Bandyopadhyay,Siddhanth Iyengar*

Main category: eess.AS

TL;DR: 评估五种TTS模型处理数学表达式（MX）的质量与可理解性，发现其表现普遍不如专家朗读，且可理解性因模型和MX类别而异。


<details>
  <summary>Details</summary>
Motivation: 现有研究对高级TTS模型处理MX的评估有限，需填补这一空白。

Method: 通过听力和转录测试评估五种TTS模型，使用LLM生成LaTeX MX的英文发音，并采用用户评分和转录正确性指标。

Result: TTS模型对MX的输出可理解性不足，且表现显著差于专家朗读；LLM选择影响有限。

Conclusion: 需改进TTS模型以更好地处理MX。

Abstract: There has been limited evaluation of advanced Text-to-Speech (TTS) models
with Mathematical eXpressions (MX) as inputs. In this work, we design
experiments to evaluate quality and intelligibility of five TTS models through
listening and transcribing tests for various categories of MX. We use two Large
Language Models (LLMs) to generate English pronunciation from LaTeX MX as TTS
models cannot process LaTeX directly. We use Mean Opinion Score from user
ratings and quantify intelligibility through transcription correctness using
three metrics. We also compare listener preference of TTS outputs with respect
to human expert rendition of same MX. Results establish that output of TTS
models for MX is not necessarily intelligible, the gap in intelligibility
varies across TTS models and MX category. For most categories, performance of
TTS models is significantly worse than that of expert rendition. The effect of
choice of LLM is limited. This establishes the need to improve TTS models for
MX.

</details>


### [23] [Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM](https://arxiv.org/abs/2506.11089)
*Jeena Prakash,Blessingh Kumar,Kadri Hacioglu,Bidisha Sharma,Sindhuja Gopalan,Malolan Chetlur,Shankar Venkatesan,Andreas Stolcke*

Main category: eess.AS

TL;DR: 提出了一种基于多ASR提示驱动的统一框架，利用文本或语音大语言模型（LLM）进行后处理，显著提升了转录准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂多阶段处理，导致错误传播和信息丢失，需要更高效的解决方案。

Method: 采用多ASR提示驱动框架，结合文本或语音LLM进行后处理，替代传统投票机制。

Result: 实验表明，该方法在转录准确性和半监督ASR模型性能上均优于传统方法。

Conclusion: 统一框架结合LLM后处理能有效提升ASR模型的训练效果和转录质量。

Abstract: Automatic speech recognition (ASR) models rely on high-quality transcribed
data for effective training. Generating pseudo-labels for large unlabeled audio
datasets often relies on complex pipelines that combine multiple ASR outputs
through multi-stage processing, leading to error propagation, information loss
and disjoint optimization. We propose a unified multi-ASR prompt-driven
framework using postprocessing by either textual or speech-based large language
models (LLMs), replacing voting or other arbitration logic for reconciling the
ensemble outputs. We perform a comparative study of multiple architectures with
and without LLMs, showing significant improvements in transcription accuracy
compared to traditional methods. Furthermore, we use the pseudo-labels
generated by the various approaches to train semi-supervised ASR models for
different datasets, again showing improved performance with textual and
speechLLM transcriptions compared to baselines.

</details>


### [24] [Tracking of Intermittent and Moving Speakers : Dataset and Metrics](https://arxiv.org/abs/2506.11145)
*Taous Iatariene,Alexandre Guérin,Romain Serizel*

Main category: eess.AS

TL;DR: 论文提出了一种针对间歇性和移动声源的跟踪问题，并引入LibriJump数据集和新的跟踪关联度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪方法依赖空间观测，难以处理因静默期间方向变化导致的不连续轨迹，因此需要新的解决方案。

Method: 提出LibriJump数据集（First Order Ambisonics格式）模拟不连续轨迹，并采用计算机视觉领域的跟踪关联度量方法。

Result: 实验表明关联度量与传统跟踪指标在连续和不连续轨迹场景下具有互补性。

Conclusion: 论文为解决不连续轨迹的跟踪问题提供了新数据集和度量方法，展示了其有效性。

Abstract: This paper presents the problem of tracking intermittent and moving sources,
i.e, sources that may change position when they are inactive. This issue is
seldom explored, and most current tracking methods rely on spatial observations
for track identity management. They are either based on a previous localization
step, or designed to perform joint localization and tracking by predicting
ordered position estimates. This raises concerns about whether such methods can
maintain reliable track identity assignment performance when dealing with
discontinuous spatial tracks, which may be caused by a change of direction
during silence. We introduce LibriJump, a novel dataset of acoustic scenes in
the First Order Ambisonics format focusing on speaker tracking. The dataset
contains speakers with changing positions during inactivity periods, thus
simulating discontinuous tracks. To measure the identity assignment
performance, we propose to use tracking association metrics adapted from the
computer vision community. We provide experiments showing the complementarity
of association metrics with previously used tracking metrics, given continuous
and discontinuous spatial tracks.

</details>


### [25] [Improved in-car sound pick-up using multichannel Wiener filter](https://arxiv.org/abs/2506.11157)
*Juhi Khalid,Martin Bouchard*

Main category: eess.AS

TL;DR: 论文探讨了在车内双麦克风系统中使用多通道维纳滤波算法，以提升语音质量并减少背景噪声。


<details>
  <summary>Details</summary>
Motivation: 随着汽车电子和传感器的发展，多麦克风拾音技术成为可能，但信号处理仍面临带宽和计算限制的挑战。

Method: 采用多通道维纳滤波算法，针对车内双麦克风系统优化语音质量，减少回声和背景噪声。

Result: 实验表明，该方法在不同噪声条件下表现优于简单麦克风信号混合，显著提升了语音质量。

Conclusion: 多通道维纳滤波算法在车内语音处理中具有显著优势，能够有效改善语音质量和噪声抑制。

Abstract: With advancements in automotive electronics and sensors, the sound pick-up
using multiple microphones has become feasible for hands-free telephony and
voice command in-car applications. However, challenges remain in effectively
processing multiple microphone signals due to bandwidth or processing
limitations. This work explores the use of the Multichannel Wiener Filter
algorithm with a two-microphone in-car system, to enhance speech quality for
driver and passenger voice, i.e., to mitigate notch-filtering effects caused by
echoes and improve background noise reduction. We evaluate its performance
under various noise conditions using modern objective metrics like Deep Noise
Suppression Mean Opinion Score. The effect of head movements of
driver/passenger is also investigated. The proposed method is shown to provide
significant improvements over a simple mixing of microphone signals.

</details>


### [26] [S2ST-Omni: An Efficient and Scalable Multilingual Speech-to-Speech Translation Framework via Seamlessly Speech-Text Alignment and Streaming Speech Decoder](https://arxiv.org/abs/2506.11160)
*Yu Pan,Yuguang Yang,Yanni Hu,Jianhao Ye,Xiang Zhang,Hongbin Zhou,Lei Ma,Jianjun Zhao*

Main category: eess.AS

TL;DR: S2ST-Omni框架通过分解语音到语音翻译任务为语音到文本翻译和文本到语音合成，结合预训练模型，解决了多语言语音翻译的高质量和低延迟需求。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音到语音翻译（S2ST）的高质量、低延迟需求以及对大规模平行语音语料库的依赖问题。

Method: 将S2ST任务分解为S2TT和TTS，利用Whisper和Qwen 3.0预训练模型，引入轻量级语音适配器对齐语音和文本表示，采用流式语音解码器实现实时性能。

Result: 在CVSS基准测试中，S2ST-Omni优于现有S2ST基线，同时保持低延迟。

Conclusion: S2ST-Omni展示了高效、可扩展的解决方案，具有实际部署潜力。

Abstract: Multilingual speech-to-speech translation (S2ST) aims to directly convert
spoken utterances from multiple source languages into natural and intelligible
speech in a target language. Despite recent progress, significant challenges
remain: (1) achieving high-quality and low-latency S2ST remains a critical
hurdle; (2) existing S2ST approaches heavily rely on large-scale parallel
speech corpora, which are extremely difficult to collect. To address these
issues, we propose S2ST-Omni, an efficient and scalable framework for
multilingual speech-to-speech translation. Specifically, we decompose the S2ST
task into speech-to-text translation (S2TT) and text-to-speech synthesis (TTS),
unifying them within a single end-to-end speech-language model. To achieve
high-quality S2TT while reducing dependence on parallel corpora, we leverage
large-scale pretrained models -- Whisper for audio understanding and Qwen 3.0
for text understanding. A lightweight speech adapter is introduced to align
speech and text representations, enabling effective use of pretrained
multimodal knowledge. To ensure both translation quality and real-time
performance, we adopt a pretrained streaming speech decoder in the TTS stage to
generate target speech in an autoregressive manner. Extensive experiments on
the CVSS benchmark demonstrate that S2ST-Omni outperforms state-of-the-art S2ST
baselines while maintaining comparable latency, highlighting its effectiveness
and practical potential for real-world deployment.

</details>


### [27] [Advances in Small-Footprint Keyword Spotting: A Comprehensive Review of Efficient Models and Algorithms](https://arxiv.org/abs/2506.11169)
*Soumen Garai,Suman Samui*

Main category: eess.AS

TL;DR: 该论文综述了小型足迹关键词检测（SF-KWS）的七类技术，旨在为低功耗边缘设备提供高效的TinyML框架。


<details>
  <summary>Details</summary>
Motivation: 随着智能语音设备和物联网应用的普及，SF-KWS需求增长，但边缘设备的资源限制需要高效解决方案。

Method: 研究了七类技术：模型架构、学习技术、模型压缩、注意力感知架构、特征优化、神经网络搜索和混合方法。

Result: 为SF-KWS领域提供了全面的技术概述，并指出了潜在研究方向。

Conclusion: 该研究为理解和推动SF-KWS领域发展提供了宝贵资源。

Abstract: Small-Footprint Keyword Spotting (SF-KWS) has gained popularity in today's
landscape of smart voice-activated devices, smartphones, and Internet of Things
(IoT) applications. This surge is attributed to the advancements in Deep
Learning, enabling the identification of predefined words or keywords from a
continuous stream of words. To implement the SF-KWS model on edge devices with
low power and limited memory in real-world scenarios, a efficient Tiny Machine
Learning (TinyML) framework is essential. In this study, we explore seven
distinct categories of techniques namely, Model Architecture, Learning
Techniques, Model Compression, Attention Awareness Architecture, Feature
Optimization, Neural Network Search, and Hybrid Approaches, which are suitable
for developing an SF-KWS system. This comprehensive overview will serve as a
valuable resource for those looking to understand, utilize, or contribute to
the field of SF-KWS. The analysis conducted in this work enables the
identification of numerous potential research directions, encompassing insights
from automatic speech recognition research and those specifically pertinent to
the realm of spoken SF-KWS.

</details>


### [28] [Efficient Speech Enhancement via Embeddings from Pre-trained Generative Audioencoders](https://arxiv.org/abs/2506.11514)
*Xingwei Sun,Heinrich Dinkel,Yadong Niu,Linzhang Wang,Junbo Zhang,Jian Luan*

Main category: eess.AS

TL;DR: 本文提出了一种基于预训练音频编码器的语音增强方法，通过提取并去噪音频嵌入，再通过声码器合成清晰语音，实验证明其高效且优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索利用预训练模型的音频嵌入进行语音增强，以替代传统的时频掩蔽或信号预测技术。

Method: 使用预训练音频编码器提取噪声语音的嵌入，通过紧凑编码网络去噪，再用声码器合成清晰语音。

Result: 实验表明该方法在语音增强和说话人保真度上优于判别性音频编码器模型，主观测试验证其感知质量更优。

Conclusion: 提出的生成式音频编码器语音增强系统高效且性能优越，优于现有先进模型。

Abstract: Recent research has delved into speech enhancement (SE) approaches that
leverage audio embeddings from pre-trained models, diverging from
time-frequency masking or signal prediction techniques. This paper introduces
an efficient and extensible SE method. Our approach involves initially
extracting audio embeddings from noisy speech using a pre-trained audioencoder,
which are then denoised by a compact encoder network. Subsequently, a vocoder
synthesizes the clean speech from denoised embeddings. An ablation study
substantiates the parameter efficiency of the denoise encoder with a
pre-trained audioencoder and vocoder. Experimental results on both speech
enhancement and speaker fidelity demonstrate that our generative
audioencoder-based SE system outperforms models utilizing discriminative
audioencoders. Furthermore, subjective listening tests validate that our
proposed system surpasses an existing state-of-the-art SE model in terms of
perceptual quality.

</details>


### [29] [From Sharpness to Better Generalization for Speech Deepfake Detection](https://arxiv.org/abs/2506.11532)
*Wen Huang,Xuechen Liu,Xin Wang,Junichi Yamagishi,Yanmin Qian*

Main category: eess.AS

TL;DR: 该论文研究了语音深度伪造检测（SDD）中泛化能力的挑战，提出通过锐度作为理论代理指标，并应用锐度感知最小化（SAM）提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 泛化能力是语音深度伪造检测的关键挑战，但缺乏理论框架解释模型性能。

Method: 通过分析锐度对领域变化的响应，并应用SAM显式降低锐度。

Result: 实验表明锐度与泛化能力显著相关，SAM能提升模型在未见测试集上的性能。

Conclusion: 锐度可作为SDD中泛化的理论指标，SAM是提升鲁棒性的有效策略。

Abstract: Generalization remains a critical challenge in speech deepfake detection
(SDD). While various approaches aim to improve robustness, generalization is
typically assessed through performance metrics like equal error rate without a
theoretical framework to explain model performance. This work investigates
sharpness as a theoretical proxy for generalization in SDD. We analyze how
sharpness responds to domain shifts and find it increases in unseen conditions,
indicating higher model sensitivity. Based on this, we apply Sharpness-Aware
Minimization (SAM) to reduce sharpness explicitly, leading to better and more
stable performance across diverse unseen test sets. Furthermore, correlation
analysis confirms a statistically significant relationship between sharpness
and generalization in most test settings. These findings suggest that sharpness
can serve as a theoretical indicator for generalization in SDD and that
sharpness-aware training offers a promising strategy for improving robustness.

</details>


### [30] [Lightweight and Robust Multi-Channel End-to-End Speech Recognition with Spherical Harmonic Transform](https://arxiv.org/abs/2506.11630)
*Xiangzhu Kong,Huang Hao,Zhijian Ou*

Main category: eess.AS

TL;DR: SHTNet是一个轻量级的球谐变换框架，通过三项创新解决多通道语音识别的跨阵列泛化问题，包括几何不变的信号处理、空间-频谱注意力融合网络和随机SHT训练，计算量减少97.1%。


<details>
  <summary>Details</summary>
Motivation: 解决多通道语音识别中不同麦克风阵列几何形状导致的泛化挑战。

Method: 1. 球谐变换分解信号为几何不变的系数；2. 空间-频谱注意力融合网络结合空间建模和噪声抑制；3. 随机SHT训练增强鲁棒性。

Result: 在异构阵列上平均CER为39.26%，计算量比传统方法减少97.1%。

Conclusion: SHTNet有效解决了跨阵列泛化问题，计算效率高。

Abstract: This paper presents SHTNet, a lightweight spherical harmonic transform (SHT)
based framework, which is designed to address cross-array generalization
challenges in multi-channel automatic speech recognition (ASR) through three
key innovations. First, SHT based spatial sound field decomposition converts
microphone signals into geometry-invariant spherical harmonic coefficients,
isolating signal processing from array geometry. Second, the Spatio-Spectral
Attention Fusion Network (SSAFN) combines coordinate-aware spatial modeling,
refined self-attention channel combinator, and spectral noise suppression
without conventional beamforming. Third, Rand-SHT training enhances robustness
through random channel selection and array geometry reconstruction. The system
achieves 39.26\% average CER across heterogeneous arrays (e.g., circular,
square, and binaural) on datasets including Aishell-4, Alimeeting, and XMOS,
with 97.1\% fewer computations than conventional neural beamformers.

</details>


### [31] [Tracking of Spatially Dynamic Room Impulse Responses Along Locally Linearized Trajectories](https://arxiv.org/abs/2506.11703)
*Kathleen MacWilliam,Thomas Dietzen,Toon van Waterschoot*

Main category: eess.AS

TL;DR: 提出了一种扩展方法，通过分段线性轨迹来估计房间脉冲响应（RIRs），适用于更复杂的真实场景。


<details>
  <summary>Details</summary>
Motivation: 测量多个空间点的RIRs耗时且模拟需要详细声学环境知识，现有方法仅适用于小范围且假设严格。

Method: 将长轨迹分段为小线性区间，假设近似成立，逐段应用原方法。

Result: 使用trajectoRIR数据库验证，扩展方法在真实L形轨迹中有效。

Conclusion: 分段方法扩展了原方法的适用范围，适用于更复杂的房间环境。

Abstract: Measuring room impulse responses (RIRs) at multiple spatial points is a
time-consuming task, while simulations require detailed knowledge of the room's
acoustic environment. In prior work, we proposed a method for estimating the
early part of RIRs along a linear trajectory in a time-varying acoustic
scenario involving a static sound source and a microphone moving at constant
velocity. This approach relies on measured RIRs at the start and end points of
the trajectory and assumes that the time intervals occupied by the direct sound
and individual reflections along the trajectory are non-overlapping. The
method's applicability is therefore restricted to relatively small areas within
a room, and its performance has yet to be validated with real-world data. In
this paper, we propose a practical extension of the method to more realistic
scenarios by segmenting longer trajectories into smaller linear intervals where
the assumptions approximately hold. Applying the method piecewise along these
segments extends its applicability to more complex room environments. We
demonstrate its effectiveness using the trajectoRIR database, which includes
moving microphone recordings and RIR measurements at discrete points along a
controlled L-shaped trajectory in a real room.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [32] [End-to-End Diarization utilizing Attractor Deep Clustering](https://arxiv.org/abs/2506.11090)
*David Palzer,Matthew Maciejewski,Eric Fosler-Lussier*

Main category: cs.SD

TL;DR: 提出了一种高效的说话人分割框架，结合了Conformer解码器、Transformer更新的吸引子和深度聚类角度损失，显著降低了分割错误率。


<details>
  <summary>Details</summary>
Motivation: 说话人分割因需要结构化说话人表示、高效建模和适应多变条件而具有挑战性。

Method: 采用Conformer结构增强说话人表示，结合交叉注意力机制和卷积模块，扩展深度聚类以对齐标签吸引子向量与音频嵌入方向，并施加正交约束优化说话人分离。

Result: 实验表明，该方法在保持参数数量的同时实现了较低的分割错误率。

Conclusion: 提出的框架通过结构化嵌入和优化损失函数，显著提升了说话人分割的性能。

Abstract: Speaker diarization remains challenging due to the need for structured
speaker representations, efficient modeling, and robustness to varying
conditions. We propose a performant, compact diarization framework that
integrates conformer decoders, transformer-updated attractors, and a deep
clustering style angle loss. Our approach refines speaker representations with
an enhanced conformer structure, incorporating cross-attention to attractors
and an additional convolution module. To enforce structured embeddings, we
extend deep clustering by constructing label-attractor vectors, aligning their
directional structure with audio embeddings. We also impose orthogonality
constraints on active attractors for better speaker separation while
suppressing non-active attractors to prevent false activations. Finally, a
permutation invariant training binary cross-entropy loss refines speaker
detection. Experiments show that our method achieves low diarization error
while maintaining parameter count.

</details>


### [33] [Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting](https://arxiv.org/abs/2506.11096)
*Guillaume Wisniewski,Séverine Guillaume,Clara Rosina Fernández*

Main category: cs.SD

TL;DR: 研究表明，尽管预训练语音表示（如wav2vec2和HuBERT）存在各向异性，但其相似性度量在关键词识别任务中仍有效。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练语音表示的各向异性对下游任务的影响，特别是在计算纪录片语言学的关键词识别中。

Method: 使用动态时间规整（Dynamic Time Warping）评估wav2vec2的相似性度量。

Result: 结果显示，尽管存在各向异性，wav2vec2仍能有效识别未转录的单词，并捕捉语音结构，跨说话者泛化能力强。

Conclusion: 预训练在学习丰富且不变的语音表示中至关重要。

Abstract: Pretrained speech representations like wav2vec2 and HuBERT exhibit strong
anisotropy, leading to high similarity between random embeddings. While widely
observed, the impact of this property on downstream tasks remains unclear. This
work evaluates anisotropy in keyword spotting for computational documentary
linguistics. Using Dynamic Time Warping, we show that despite anisotropy,
wav2vec2 similarity measures effectively identify words without transcription.
Our results highlight the robustness of these representations, which capture
phonetic structures and generalize across speakers. Our results underscore the
importance of pretraining in learning rich and invariant speech
representations.

</details>


### [34] [GLAP: General contrastive audio-text pretraining across domains and languages](https://arxiv.org/abs/2506.11350)
*Heinrich Dinkel,Zhiyong Yan,Tianzi Wang,Yongqing Wang,Xingwei Sun,Yadong Niu,Jizhong Liu,Gang Li,Junbo Zhang,Jian Luan*

Main category: cs.SD

TL;DR: GLAP扩展了CLAP，增加了多语言和多领域能力，在音频-文本检索、语音检索和分类任务中表现优异，并在多语言关键词识别和音乐理解方面展现出强大能力。


<details>
  <summary>Details</summary>
Motivation: 当前CLAP方法仅支持英语，忽略了多语言语音内容，因此需要一种更通用的方法。

Method: 提出GLAP，扩展CLAP以支持多语言和多领域能力，并在多个基准测试中验证其性能。

Result: GLAP在标准音频-文本检索任务中表现优异，显著超越现有方法，同时在多语言关键词识别和音乐理解任务中取得强结果。

Conclusion: GLAP是一种多功能且强大的方法，适用于多语言和多领域的音频-文本任务。

Abstract: Contrastive Language Audio Pretraining (CLAP) is a widely-used method to
bridge the gap between audio and text domains. Current CLAP methods enable
sound and music retrieval in English, ignoring multilingual spoken content. To
address this, we introduce general language audio pretraining (GLAP), which
expands CLAP with multilingual and multi-domain abilities. GLAP demonstrates
its versatility by achieving competitive performance on standard audio-text
retrieval benchmarks like Clotho and AudioCaps, while significantly surpassing
existing methods in speech retrieval and classification tasks. Additionally,
GLAP achieves strong results on widely used sound-event zero-shot benchmarks,
while simultaneously outperforming previous methods on speech content
benchmarks. Further keyword spotting evaluations across 50 languages emphasize
GLAP's advanced multilingual capabilities. Finally, multilingual sound and
music understanding is evaluated across four languages. Checkpoints and Source:
https://github.com/xiaomi-research/dasheng-glap.

</details>


### [35] [A correlation-permutation approach for speech-music encoders model merging](https://arxiv.org/abs/2506.11403)
*Fabian Ritter-Gutierrez,Yi-Cheng Lin,Jeremy H. M Wong,Hung-yi Lee,Eng Siong Chng,Nancy F. Chen*

Main category: cs.SD

TL;DR: 提出了一种基于相关置换的方法，用于合并语音和音乐编码器，显著提升了音乐性能。


<details>
  <summary>Details</summary>
Motivation: 统一语音和音乐模型需要昂贵的预训练，而直接合并模型在权重空间未对齐时具有挑战性。

Method: 采用相关置换方法，逐层计算最大化特征交叉相关的置换矩阵，以合并Transformer层。

Result: 合并后的模型在保留语音能力的同时，音乐性能显著提升，平均得分比线性插值模型高14.83分。

Conclusion: 该方法可从独立训练的编码器中创建统一的音频模型。

Abstract: Creating a unified speech and music model requires expensive pre-training.
Model merging can instead create an unified audio model with minimal
computational expense. However, direct merging is challenging when the models
are not aligned in the weight space. Motivated by Git Re-Basin, we introduce a
correlation-permutation approach that aligns a music encoder's internal layers
with a speech encoder. We extend previous work to the case of merging
transformer layers. The method computes a permutation matrix that maximizes the
model's features-wise cross-correlations layer by layer, enabling effective
fusion of these otherwise disjoint models. The merged model retains speech
capabilities through this method while significantly enhancing music
performance, achieving an improvement of 14.83 points in average score compared
to linear interpolation model merging. This work allows the creation of unified
audio models from independently trained encoders.

</details>


### [36] [LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation](https://arxiv.org/abs/2506.11476)
*Tom Baker,Javier Nistal*

Main category: cs.SD

TL;DR: 论文提出了一种轻量级模块化架构，用于改进文本到音频扩散模型的控制能力，减少参数数量并提高灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频扩散模型缺乏细粒度的时间变化控制，且ControlNet方法内存占用大、灵活性不足。

Method: 提出轻量级模块化架构，减少参数数量，同时保持音频质量和条件遵循能力。

Result: 新方法在音频质量和条件遵循上与ControlNet相当，但内存占用更低，训练和部署更高效。

Conclusion: 该方法显著提升了控制的灵活性和效率，适用于独立控制的训练和部署。

Abstract: Text-to-audio diffusion models produce high-quality and diverse music but
many, if not most, of the SOTA models lack the fine-grained, time-varying
controls essential for music production. ControlNet enables attaching external
controls to a pre-trained generative model by cloning and fine-tuning its
encoder on new conditionings. However, this approach incurs a large memory
footprint and restricts users to a fixed set of controls. We propose a
lightweight, modular architecture that considerably reduces parameter count
while matching ControlNet in audio quality and condition adherence. Our method
offers greater flexibility and significantly lower memory usage, enabling more
efficient training and deployment of independent controls. We conduct extensive
objective and subjective evaluations and provide numerous audio examples on the
accompanying website at https://lightlatentcontrol.github.io

</details>


### [37] [Amplifying Artifacts with Speech Enhancement in Voice Anti-spoofing](https://arxiv.org/abs/2506.11542)
*Thanapat Trachu,Thanathai Lertpetchpun,Ekapol Chuangsuwanich*

Main category: cs.SD

TL;DR: 提出了一种模型无关的流程，通过语音增强和噪声放大隐藏的伪造语音特征，显著提高了伪造检测性能。


<details>
  <summary>Details</summary>
Motivation: 伪造语音通常包含生成模型引入的伪影，现有方法主要关注架构改进，而本研究探索如何增强这些伪影的可见性。

Method: 采用三步流程：噪声添加、噪声提取和噪声放大，结合语音增强技术提取并放大伪造特征。

Result: 在ASVspoof2019和ASVspoof2021数据集上，伪造检测性能分别提高了44.44%和26.34%。

Conclusion: 该方法通过增强伪造特征，显著提升了检测性能，且兼容多种语音增强模型和检测架构。

Abstract: Spoofed utterances always contain artifacts introduced by generative models.
While several countermeasures have been proposed to detect spoofed utterances,
most primarily focus on architectural improvements. In this work, we
investigate how artifacts remain hidden in spoofed speech and how to enhance
their presence. We propose a model-agnostic pipeline that amplifies artifacts
using speech enhancement and various types of noise. Our approach consists of
three key steps: noise addition, noise extraction, and noise amplification.
First, we introduce noise into the raw speech. Then, we apply speech
enhancement to extract the entangled noise and artifacts. Finally, we amplify
these extracted features. Moreover, our pipeline is compatible with different
speech enhancement models and countermeasure architectures. Our method improves
spoof detection performance by up to 44.44\% on ASVspoof2019 and 26.34\% on
ASVspoof2021.

</details>


### [38] [Dissecting the Segmentation Model of End-to-End Diarization with Vector Clustering](https://arxiv.org/abs/2506.11605)
*Alexis Plaquet,Naohiro Tawara,Marc Delcroix,Shota Horiguchi,Atsushi Ando,Shoko Araki,Hervé Bredin*

Main category: cs.SD

TL;DR: 本文深入分析了端到端神经聚类在说话人日志中的架构选择，发现微调WavLM编码器和Conformer解码器表现最佳，并在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 评估说话人日志中不同架构选择的协同效应，以提升性能。

Method: 研究了多种编码器（SincNet、WavLM）、解码器（LSTM、Mamba、Conformer）、损失函数（多标签、多类）和块大小的影响。

Result: 微调WavLM编码器和Conformer解码器表现最佳，多类损失函数在多数情况下更优，但微调WavLM时多标签更佳。

Conclusion: 最佳系统在多个数据集上达到SOTA，新架构能更好处理长块大小，提升性能。

Abstract: End-to-End Neural Diarization with Vector Clustering is a powerful and
practical approach to perform Speaker Diarization. Multiple enhancements have
been proposed for the segmentation model of these pipelines, but their synergy
had not been thoroughly evaluated. In this work, we provide an in-depth
analysis on the impact of major architecture choices on the performance of the
pipeline. We investigate different encoders (SincNet, pretrained and finetuned
WavLM), different decoders (LSTM, Mamba, and Conformer), different losses
(multilabel and multiclass powerset), and different chunk sizes. Through
in-depth experiments covering nine datasets, we found that the finetuned
WavLM-based encoder always results in the best systems by a wide margin. The
LSTM decoder is outclassed by Mamba- and Conformer-based decoders, and while we
found Mamba more robust to other architecture choices, it is slightly inferior
to our best architecture, which uses a Conformer encoder. We found that
multilabel and multiclass powerset losses do not have the same distribution of
errors. We confirmed that the multiclass loss helps almost all models attain
superior performance, except when finetuning WavLM, in which case, multilabel
is the superior choice. We also evaluated the impact of the chunk size on all
aforementioned architecture choices and found that newer architectures tend to
better handle long chunk sizes, which can greatly improve pipeline performance.
Our best system achieved state-of-the-art results on five widely used speaker
diarization datasets.

</details>


### [39] [(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test](https://arxiv.org/abs/2506.11620)
*Stefan Bleeck*

Main category: cs.SD

TL;DR: 论文提出了一种名为SimPhon Speech Test-25的新型语音测试方法，通过计算模拟和优化，解决了传统听力测试在评估老年性耳聋中语音理解功能缺陷的不足。


<details>
  <summary>Details</summary>
Motivation: 传统听力测试对老年性耳聋中语音理解功能的评估不足，尤其是超阈值缺陷。这促使开发更具诊断特异性的语音感知测试。

Method: 采用多阶段计算流程，利用自动语音识别系统模拟听力损失，通过声学退化和数据分析优化出25对单词对。

Result: SimPhon Speech Test-25的诊断性能与标准语音可懂度指数无显著相关性，表明其能捕捉超出简单可听性的感知缺陷。

Conclusion: 该方法显著提高了听力测试开发的效率，并已准备好进行初步人体试验。

Abstract: Traditional audiometry often provides an incomplete characterization of the
functional impact of hearing loss on speech understanding, particularly for
supra-threshold deficits common in presbycusis. This motivates the development
of more diagnostically specific speech perception tests. We introduce the
Simulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,
multi-stage computational pipeline for the in silico design and validation of a
phonetically balanced minimal-pair speech test. This methodology leverages a
modern Automatic Speech Recognition (ASR) system as a proxy for a human
listener to simulate the perceptual effects of sensorineural hearing loss. By
processing speech stimuli under controlled acoustic degradation, we first
identify the most common phoneme confusion patterns. These patterns then guide
the data-driven curation of a large set of candidate word pairs derived from a
comprehensive linguistic corpus. Subsequent phases involving simulated
diagnostic testing, expert human curation, and a final, targeted sensitivity
analysis systematically reduce the candidates to a final, optimized set of 25
pairs (the SimPhon Speech Test-25). A key finding is that the diagnostic
performance of the SimPhon Speech Test-25 test items shows no significant
correlation with predictions from the standard Speech Intelligibility Index
(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond
simple audibility. This computationally optimized test set offers a significant
increase in efficiency for audiological test development, ready for initial
human trials.

</details>


### [40] [Enabling automatic transcription of child-centered audio recordings from real-world environments](https://arxiv.org/abs/2506.11747)
*Daniil Kocharov,Okko Räsänen*

Main category: cs.SD

TL;DR: 提出一种方法，通过自动检测长音频中可被现代ASR系统可靠转录的部分，显著提高转录准确性。


<details>
  <summary>Details</summary>
Motivation: 长音频转录对研究儿童语言发展至关重要，但手动标注不现实，现有ASR技术因噪声问题难以直接应用。

Method: 自动筛选长音频中可被ASR准确转录的片段，而非处理全部内容。

Result: 在四个英语长音频语料库中，该方法转录13%的语音，中位WER为0%，平均WER为18%，显著优于全量转录（中位WER 52%）。

Conclusion: 该方法为儿童长音频的自动化语言分析提供了可行方案。

Abstract: Longform audio recordings obtained with microphones worn by children-also
known as child-centered daylong recordings-have become a standard method for
studying children's language experiences and their impact on subsequent
language development. Transcripts of longform speech audio would enable rich
analyses at various linguistic levels, yet the massive scale of typical
longform corpora prohibits comprehensive manual annotation. At the same time,
automatic speech recognition (ASR)-based transcription faces significant
challenges due to the noisy, unconstrained nature of real-world audio, and no
existing study has successfully applied ASR to transcribe such data. However,
previous attempts have assumed that ASR must process each longform recording in
its entirety. In this work, we present an approach to automatically detect
those utterances in longform audio that can be reliably transcribed with modern
ASR systems, allowing automatic and relatively accurate transcription of a
notable proportion of all speech in typical longform data. We validate the
approach on four English longform audio corpora, showing that it achieves a
median word error rate (WER) of 0% and a mean WER of 18% when transcribing 13%
of the total speech in the dataset. In contrast, transcribing all speech
without any filtering yields a median WER of 52% and a mean WER of 51%. We also
compare word log-frequencies derived from the automatic transcripts with those
from manual annotations and show that the frequencies correlate at r = 0.92
(Pearson) for all transcribed words and r = 0.98 for words that appear at least
five times in the automatic transcripts. Overall, the work provides a concrete
step toward increasingly detailed automated linguistic analyses of
child-centered longform audio.

</details>


### [41] [Abstract Sound Fusion with Unconditioned Inversion Model](https://arxiv.org/abs/2506.11811)
*Jing Liu,EnQi Lian*

Main category: cs.SD

TL;DR: 论文提出了一种声音融合方法，通过反转技术合成抽象声音，结合原始声音和参考声音生成新声音，并提出了基于DPMSolver++采样器的SDE和ODE反转模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是实现声音融合，生成超越简单叠加的新声音，同时保留原始声音的关键特征。

Method: 采用反转技术，提出基于DPMSolver++采样器的SDE和ODE反转模型，消除噪声预测项的循环依赖。

Result: 方法无需提示条件，同时保持采样过程中的灵活引导。

Conclusion: 该研究为声音合成提供了一种可控且灵活的新方法。

Abstract: An abstract sound is defined as a sound that does not disclose identifiable
real-world sound events to a listener. Sound fusion aims to synthesize an
original sound and a reference sound to generate a novel sound that exhibits
auditory features beyond mere additive superposition of the sound constituents.
To achieve this fusion, we employ inversion techniques that preserve essential
features of the original sample while enabling controllable synthesis. We
propose novel SDE and ODE inversion models based on DPMSolver++ samplers that
reverse the sampling process by configuring model outputs as constants,
eliminating circular dependencies incurred by noise prediction terms. Our
inversion approach requires no prompt conditioning while maintaining flexible
guidance during sampling.

</details>


### [42] [Confidence-Based Self-Training for EMG-to-Speech: Leveraging Synthetic EMG for Robust Modeling](https://arxiv.org/abs/2506.11862)
*Xiaodan Chen,Xiaoxue Gao,Mathias Quoy,Alexandre Pitti,Nancy F. Chen*

Main category: cs.SD

TL;DR: 论文提出了一种基于置信度的多说话人自训练方法（CoM2S）和新的Libri-EMG数据集，用于解决V-ETS模型因数据稀缺而受限的问题，实验证明该方法提升了语音重建效果。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏配对的EMG-语音数据，V-ETS模型的进展受限，因此需要一种有效的方法来利用有限数据提升模型性能。

Method: 提出CoM2S方法，利用预训练模型生成合成EMG数据，并通过基于音素级置信度的过滤机制增强自训练效果。

Result: 实验表明，该方法提高了音素准确性，减少了音素混淆，并降低了词错误率。

Conclusion: CoM2S方法有效提升了V-ETS模型的性能，未来将公开代码和Libri-EMG数据集以支持进一步研究。

Abstract: Voiced Electromyography (EMG)-to-Speech (V-ETS) models reconstruct speech
from muscle activity signals, facilitating applications such as
neurolaryngologic diagnostics. Despite its potential, the advancement of V-ETS
is hindered by a scarcity of paired EMG-speech data. To address this, we
propose a novel Confidence-based Multi-Speaker Self-training (CoM2S) approach,
along with a newly curated Libri-EMG dataset. This approach leverages synthetic
EMG data generated by a pre-trained model, followed by a proposed filtering
mechanism based on phoneme-level confidence to enhance the ETS model through
the proposed self-training techniques. Experiments demonstrate our method
improves phoneme accuracy, reduces phonological confusion, and lowers word
error rate, confirming the effectiveness of our CoM2S approach for V-ETS. In
support of future research, we will release the codes and the proposed
Libri-EMG dataset-an open-access, time-aligned, multi-speaker voiced EMG and
speech recordings.

</details>


### [43] [Reimagining Dance: Real-time Music Co-creation between Dancers and AI](https://arxiv.org/abs/2506.12008)
*Olga Vechtomova,Jeff Bos*

Main category: cs.SD

TL;DR: 论文提出了一种双向系统，舞者通过动作动态塑造音乐环境，AI作为响应式合作者。


<details>
  <summary>Details</summary>
Motivation: 传统舞蹈表演中动作单向响应音乐，AI在舞蹈中的应用多限于从音乐生成编舞。本文旨在建立舞者与音乐的双向创作关系。

Method: 采用多模态架构，通过智能组合预录音乐片段响应舞蹈动作，生成连贯音乐作品。

Result: 通过表演数据相关性分析，展示了动作特性与音频特征之间的新兴沟通模式。

Conclusion: 系统重新定义了AI在表演艺术中的角色，为专业舞蹈和即兴艺术表达提供了新可能。

Abstract: Dance performance traditionally follows a unidirectional relationship where
movement responds to music. While AI has advanced in various creative domains,
its application in dance has primarily focused on generating choreography from
musical input. We present a system that enables dancers to dynamically shape
musical environments through their movements. Our multi-modal architecture
creates a coherent musical composition by intelligently combining pre-recorded
musical clips in response to dance movements, establishing a bidirectional
creative partnership where dancers function as both performers and composers.
Through correlation analysis of performance data, we demonstrate emergent
communication patterns between movement qualities and audio features. This
approach reconceptualizes the role of AI in performing arts as a responsive
collaborator that expands possibilities for both professional dance performance
and improvisational artistic expression across broader populations.

</details>

{"id": "2506.12083", "categories": ["cs.SD", "cs.MA", "eess.AS", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.12083", "abs": "https://arxiv.org/abs/2506.12083", "authors": ["Amitesh Pandey", "Jafarbek Arifdjanov", "Ansh Tiwari"], "title": "TuneGenie: Reasoning-based LLM agents for preferential music generation", "comment": "15 pages", "summary": "Recently, Large language models (LLMs) have shown great promise across a\ndiversity of tasks, ranging from generating images to reasoning spatially.\nConsidering their remarkable (and growing) textual reasoning capabilities, we\ninvestigate LLMs' potency in conducting analyses of an individual's preferences\nin music (based on playlist metadata, personal write-ups, etc.) and producing\neffective prompts (based on these analyses) to be passed to Suno AI (a\ngenerative AI tool for music production). Our proposition of a novel LLM-based\ntextual representation to music model (which we call TuneGenie) and the various\nmethods we develop to evaluate & benchmark similar models add to the increasing\n(and increasingly controversial) corpus of research on the use of AI in\ngenerating art."}
{"id": "2506.12154", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12154", "abs": "https://arxiv.org/abs/2506.12154", "authors": ["Haoran Zhou", "Xingchen Song", "Brendan Fahy", "Qiaochu Song", "Binbin Zhang", "Zhendong Peng", "Anshul Wadhawan", "Denglin Jiang", "Apurv Verma", "Vinay Ramesh", "Srivas Prasad", "Michele M. Franceschini"], "title": "Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding", "comment": "Accepted to INTERSPEECH 2025", "summary": "OpenAI Whisper is a family of robust Automatic Speech Recognition (ASR)\nmodels trained on 680,000 hours of audio. However, its encoder-decoder\narchitecture, trained with a sequence-to-sequence objective, lacks native\nsupport for streaming ASR. In this paper, we fine-tune Whisper for streaming\nASR using the WeNet toolkit by adopting a Unified Two-pass (U2) structure. We\nintroduce an additional Connectionist Temporal Classification (CTC) decoder\ntrained with causal attention masks to generate streaming partial transcripts,\nwhile the original Whisper decoder reranks these partial outputs. Our\nexperiments on LibriSpeech and an earnings call dataset demonstrate that, with\nadequate fine-tuning data, Whisper can be adapted into a capable streaming ASR\nmodel. We also introduce a hybrid tokenizer approach, which uses a smaller\ntoken space for the CTC decoder while retaining Whisper's original token space\nfor the attention decoder, resulting in improved data efficiency and\ngeneralization."}
{"id": "2506.12199", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12199", "abs": "https://arxiv.org/abs/2506.12199", "authors": ["Jaeyeon Kim", "Heeseung Yun", "Gunhee Kim"], "title": "ViSAGe: Video-to-Spatial Audio Generation", "comment": "ICLR 2025. Project page: https://jaeyeonkim99.github.io/visage/", "summary": "Spatial audio is essential for enhancing the immersiveness of audio-visual\nexperiences, yet its production typically demands complex recording systems and\nspecialized expertise. In this work, we address a novel problem of generating\nfirst-order ambisonics, a widely used spatial audio format, directly from\nsilent videos. To support this task, we introduce YT-Ambigen, a dataset\ncomprising 102K 5-second YouTube video clips paired with corresponding\nfirst-order ambisonics. We also propose new evaluation metrics to assess the\nspatial aspect of generated audio based on audio energy maps and saliency\nmetrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an\nend-to-end framework that generates first-order ambisonics from silent video\nframes by leveraging CLIP visual features, autoregressive neural audio codec\nmodeling with both directional and visual guidance. Experimental results\ndemonstrate that ViSAGe produces plausible and coherent first-order ambisonics,\noutperforming two-stage approaches consisting of video-to-audio generation and\naudio spatialization. Qualitative examples further illustrate that ViSAGe\ngenerates temporally aligned high-quality spatial audio that adapts to\nviewpoint changes."}
{"id": "2506.12222", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12222", "abs": "https://arxiv.org/abs/2506.12222", "authors": ["Tony Alex", "Sara Ahmed", "Armin Mustafa", "Muhammad Awais", "Philip JB Jackson"], "title": "SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes", "comment": "Accepted at ICLR 2025. Code and pre-trained models are available at\n  \\url{https://github.com/ta012/SSLAM}", "summary": "Self-supervised pre-trained audio networks have seen widespread adoption in\nreal-world systems, particularly in multi-modal large language models. These\nnetworks are often employed in a frozen state, under the assumption that the\nSSL pre-training has sufficiently equipped them to handle real-world audio.\nHowever, a critical question remains: how well do these models actually perform\nin real-world conditions, where audio is typically polyphonic and complex,\ninvolving multiple overlapping sound sources? Current audio SSL methods are\noften benchmarked on datasets predominantly featuring monophonic audio, such as\nenvironmental sounds, and speech. As a result, the ability of SSL models to\ngeneralize to polyphonic audio, a common characteristic in natural scenarios,\nremains underexplored. This limitation raises concerns about the practical\nrobustness of SSL models in more realistic audio settings. To address this gap,\nwe introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel\ndirection in audio SSL research, designed to improve, designed to improve the\nmodel's ability to learn from polyphonic data while maintaining strong\nperformance on monophonic data. We thoroughly evaluate SSLAM on standard audio\nSSL benchmark datasets which are predominantly monophonic and conduct a\ncomprehensive comparative analysis against SOTA methods using a range of\nhigh-quality, publicly available polyphonic datasets. SSLAM not only improves\nmodel performance on polyphonic audio, but also maintains or exceeds\nperformance on standard audio SSL benchmarks. Notably, it achieves up to a\n3.9\\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision\n(mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear\nevaluation and fine-tuning regimes with performance improvements of up to 9.1\\%\n(mAP)."}
{"id": "2506.12059", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12059", "abs": "https://arxiv.org/abs/2506.12059", "authors": ["Jiajun He", "Naoki Sawada", "Koichi Miyazaki", "Tomoki Toda"], "title": "CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models", "comment": "Accepted by INTERSPEECH 2025", "summary": "In real-world applications, automatic speech recognition (ASR) systems must\nhandle overlapping speech from multiple speakers and recognize rare words like\ntechnical terms. Traditional methods address multi-talker ASR and contextual\nbiasing separately, limiting performance in complex scenarios. We propose a\nunified framework that combines multi-talker overlapping speech recognition and\ncontextual biasing into a single task. Our ASR method integrates pretrained\nspeech encoders and large language models (LLMs), using optimized finetuning\nstrategies. We also introduce a two-stage filtering algorithm to efficiently\nidentify relevant rare words from large biasing lists and incorporate them into\nthe LLM's prompt input, enhancing rare word recognition. Experiments show that\nour approach outperforms traditional contextual biasing methods, achieving a\nWER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000,\ndemonstrating its effectiveness in complex speech scenarios."}
{"id": "2506.12052", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12052", "abs": "https://arxiv.org/abs/2506.12052", "authors": ["Ahmed Y. Radwan", "Mustafa Yildirim", "Navid Hasanzadeh", "Hina Tabassum", "Shahrokh Valaee"], "title": "A Tutorial-cum-Survey on Self-Supervised Learning for Wi-Fi Sensing: Trends, Challenges, and Outlook", "comment": null, "summary": "Wi-Fi technology has evolved from simple communication routers to sensing\ndevices. Wi-Fi sensing leverages conventional Wi-Fi transmissions to extract\nand analyze channel state information (CSI) for applications like proximity\ndetection, occupancy detection, activity recognition, and health monitoring. By\nleveraging existing infrastructure, Wi-Fi sensing offers a privacy-preserving,\nnon-intrusive, and cost-effective solution which, unlike cameras, is not\nsensitive to lighting conditions. Beginning with a comprehensive review of the\nWi-Fi standardization activities, this tutorial-cum-survey first introduces\nfundamental concepts related to Wi-Fi CSI, outlines the CSI measurement\nmethods, and examines the impact of mobile objects on CSI. The mechanics of a\nsimplified testbed for CSI extraction are also described. Then, we present a\nqualitative comparison of the existing Wi-Fi sensing datasets, their\nspecifications, and pin-point their shortcomings. Next, a variety of\npreprocessing techniques are discussed that are beneficial for feature\nextraction and explainability of machine learning (ML) algorithms. We then\nprovide a qualitative review of recent ML approaches in the domain of Wi-Fi\nsensing and present the significance of self-supervised learning (SSL) in that\ncontext. Specifically, the mechanics of contrastive and non-contrastive\nlearning solutions is elaborated in detail and a quantitative comparative\nanalysis is presented in terms of classification accuracy. Finally, the article\nconcludes by highlighting emerging technologies that can be leveraged to\nenhance the performance of Wi-Fi sensing and opportunities for further research\nin this domain"}
{"id": "2506.12260", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12260", "abs": "https://arxiv.org/abs/2506.12260", "authors": ["Wei Wang", "Wangyou Zhang", "Chenda Li", "Jiatong Shi", "Shinji Watanabe", "Yanmin Qian"], "title": "Improving Speech Enhancement with Multi-Metric Supervision from Learned Quality Assessment", "comment": "Submitted to ASRU 2025", "summary": "Speech quality assessment (SQA) aims to predict the perceived quality of\nspeech signals under a wide range of distortions. It is inherently connected to\nspeech enhancement (SE), which seeks to improve speech quality by removing\nunwanted signal components. While SQA models are widely used to evaluate SE\nperformance, their potential to guide SE training remains underexplored. In\nthis work, we investigate a training framework that leverages a SQA model,\ntrained to predict multiple evaluation metrics from a public SE leaderboard, as\na supervisory signal for SE. This approach addresses a key limitation of\nconventional SE objectives, such as SI-SNR, which often fail to align with\nperceptual quality and generalize poorly across evaluation metrics. Moreover,\nit enables training on real-world data where clean references are unavailable.\nExperiments on both simulated and real-world test sets show that SQA-guided\ntraining consistently improves performance across a range of quality metrics."}
{"id": "2506.12067", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12067", "abs": "https://arxiv.org/abs/2506.12067", "authors": ["Aditya Kamlesh Parikh", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Evaluating Logit-Based GOP Scores for Mispronunciation Detection", "comment": "Accepted to Interspeech 2025. This publication is part of the project\n  Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013\n  of the research programme NGF AiNed Fellowship Grants which is financed by\n  the Dutch Research Council (NWO)", "summary": "Pronunciation assessment relies on goodness of pronunciation (GOP) scores,\ntraditionally derived from softmax-based posterior probabilities. However,\nposterior probabilities may suffer from overconfidence and poor phoneme\nseparation, limiting their effectiveness. This study compares logit-based GOP\nscores with probability-based GOP scores for mispronunciation detection. We\nconducted our experiment on two L2 English speech datasets spoken by Dutch and\nMandarin speakers, assessing classification performance and correlation with\nhuman ratings. Logit-based methods outperform probability-based GOP in\nclassification, but their effectiveness depends on dataset characteristics. The\nmaximum logit GOP shows the strongest alignment with human perception, while a\ncombination of different GOP scores balances probability and logit features.\nThe findings suggest that hybrid GOP methods incorporating uncertainty modeling\nand phoneme-specific weighting improve pronunciation assessment."}
{"id": "2506.12165", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12165", "abs": "https://arxiv.org/abs/2506.12165", "authors": ["Huanqiang Duan", "Manno Versluis", "Qinyu Chen", "Leo C. N. de Vreede", "Chang Gao"], "title": "TCN-DPD: Parameter-Efficient Temporal Convolutional Networks for Wideband Digital Predistortion", "comment": "Accepted to IEEE MTT-S International Microwave Symposium (IMS) 2025", "summary": "Digital predistortion (DPD) is essential for mitigating nonlinearity in RF\npower amplifiers, particularly for wideband applications. This paper presents\nTCN-DPD, a parameter-efficient architecture based on temporal convolutional\nnetworks, integrating noncausal dilated convolutions with optimized activation\nfunctions. Evaluated on the OpenDPD framework with the DPA_200MHz dataset,\nTCN-DPD achieves simulated ACPRs of -51.58/-49.26 dBc (L/R), EVM of -47.52 dB,\nand NMSE of -44.61 dB with 500 parameters and maintains superior linearization\nthan prior models down to 200 parameters, making it promising for efficient\nwideband PA linearization."}
{"id": "2506.12325", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12325", "abs": "https://arxiv.org/abs/2506.12325", "authors": ["Yuntao Shou", "Jun Yao", "Tao Meng", "Wei Ai", "Cen Chen", "Keqin Li"], "title": "GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition", "comment": null, "summary": "Multimodal emotion recognition in conversations (MERC) aims to infer the\nspeaker's emotional state by analyzing utterance information from multiple\nsources (i.e., video, audio, and text). Compared with unimodality, a more\nrobust utterance representation can be obtained by fusing complementary\nsemantic information from different modalities. However, the modality missing\nproblem severely limits the performance of MERC in practical scenarios. Recent\nwork has achieved impressive performance on modality completion using graph\nneural networks and diffusion models, respectively. This inspires us to combine\nthese two dimensions through the graph diffusion model to obtain more powerful\nmodal recovery capabilities. Unfortunately, existing graph diffusion models may\ndestroy the connectivity and local structure of the graph by directly adding\nGaussian noise to the adjacency matrix, resulting in the generated graph data\nbeing unable to retain the semantic and topological information of the original\ngraph. To this end, we propose a novel Graph Spectral Diffusion Network\n(GSDNet), which maps Gaussian noise to the graph spectral space of missing\nmodalities and recovers the missing data according to its original\ndistribution. Compared with previous graph diffusion methods, GSDNet only\naffects the eigenvalues of the adjacency matrix instead of destroying the\nadjacency matrix directly, which can maintain the global topological\ninformation and important spectral features during the diffusion process.\nExtensive experiments have demonstrated that GSDNet achieves state-of-the-art\nemotion recognition performance in various modality loss scenarios."}
{"id": "2506.12073", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12073", "abs": "https://arxiv.org/abs/2506.12073", "authors": ["Zongli Ye", "Jiachen Lian", "Xuanru Zhou", "Jinming Zhang", "Haodong Li", "Shuhe Li", "Chenxu Guo", "Anaisha Das", "Peter Park", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis", "comment": "Accepted for Interspeech2025", "summary": "Accurate alignment of dysfluent speech with intended text is crucial for\nautomating the diagnosis of neurodegenerative speech disorders. Traditional\nmethods often fail to model phoneme similarities effectively, limiting their\nperformance. In this work, we propose Neural LCS, a novel approach for\ndysfluent text-text and speech-text alignment. Neural LCS addresses key\nchallenges, including partial alignment and context-aware similarity mapping,\nby leveraging robust phoneme-level modeling. We evaluate our method on a\nlarge-scale simulated dataset, generated using advanced data simulation\ntechniques, and real PPA data. Neural LCS significantly outperforms\nstate-of-the-art models in both alignment accuracy and dysfluent speech\nsegmentation. Our results demonstrate the potential of Neural LCS to enhance\nautomated systems for diagnosing and analyzing speech disorders, offering a\nmore accurate and linguistically grounded solution for dysfluent speech\nalignment."}
{"id": "2506.12218", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12218", "abs": "https://arxiv.org/abs/2506.12218", "authors": ["Samuel Rey", "Hamed Ajorlou", "Gonzalo Mateos"], "title": "Directed Acyclic Graph Convolutional Networks", "comment": null, "summary": "Directed acyclic graphs (DAGs) are central to science and engineering\napplications including causal inference, scheduling, and neural architecture\nsearch. In this work, we introduce the DAG Convolutional Network (DCN), a novel\ngraph neural network (GNN) architecture designed specifically for convolutional\nlearning from signals supported on DAGs. The DCN leverages causal graph filters\nto learn nodal representations that account for the partial ordering inherent\nto DAGs, a strong inductive bias does not present in conventional GNNs. Unlike\nprior art in machine learning over DAGs, DCN builds on formal convolutional\noperations that admit spectral-domain representations. We further propose the\nParallel DCN (PDCN), a model that feeds input DAG signals to a parallel bank of\ncausal graph-shift operators and processes these DAG-aware features using a\nshared multilayer perceptron. This way, PDCN decouples model complexity from\ngraph size while maintaining satisfactory predictive performance. The\narchitectures' permutation equivariance and expressive power properties are\nalso established. Comprehensive numerical tests across several tasks, datasets,\nand experimental conditions demonstrate that (P)DCN compares favorably with\nstate-of-the-art baselines in terms of accuracy, robustness, and computational\nefficiency. These results position (P)DCN as a viable framework for deep\nlearning from DAG-structured data that is designed from first (graph) signal\nprocessing principles."}
{"id": "2506.12405", "categories": ["cs.SD", "00A65", "J.5"], "pdf": "https://arxiv.org/pdf/2506.12405", "abs": "https://arxiv.org/abs/2506.12405", "authors": ["Emmanuel Deruty", "David Meredith", "Maarten Grachten", "Pascal Arbez-Nicolas", "Andreas Hasselholt Jørgensen", "Oliver Søndermølle Hansen", "Magnus Stensli", "Christian Nørkær Petersen"], "title": "Methods for pitch analysis in contemporary popular music: multiple pitches from harmonic tones in Vitalic's music", "comment": "Pending review, Journal of the Audio Engineering Society", "summary": "Aims. This study suggests that the use of multiple perceived pitches arising\nfrom a single harmonic complex tone is an active and intentional feature of\ncontemporary popular music. The phenomenon is illustrated through examples\ndrawn from the work of electronic artist Vitalic and others.\n  Methods. Two listening tests were conducted: (1) evaluation of the number of\nsimultaneous pitches perceived from single harmonic tones, and (2) manual pitch\ntranscription of sequences of harmonic tones. Relationships between signal\ncharacteristics and pitch perception were then analyzed.\n  Results. The synthetic harmonic tones found in the musical sequences under\nstudy were observed to transmit more perceived pitches than their acoustic\ncounterparts, with significant variation across listeners. Multiple ambiguous\npitches were associated with tone properties such as prominent upper partials\nand particular autocorrelation profiles.\n  Conclusions. Harmonic tones in a context of contemporary popular music can,\nin general, convey several ambiguous pitches. The set of perceived pitches\ndepends on both the listener and the listening conditions."}
{"id": "2506.12285", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12285", "abs": "https://arxiv.org/abs/2506.12285", "authors": ["Yinghao Ma", "Siyou Li", "Juntao Yu", "Emmanouil Benetos", "Akira Maezawa"], "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following", "comment": "Accepted by ISMIR 2025", "summary": "Recent advances in audio-text large language models (LLMs) have opened new\npossibilities for music understanding and generation. However, existing\nbenchmarks are limited in scope, often relying on simplified tasks or\nmulti-choice evaluations that fail to reflect the complexity of real-world\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\ninstruction following benchmark designed to evaluate audio-text LLMs on a\ndiverse set of music information retrieval (MIR) tasks. These include genre\nclassification, emotion regression, emotion tagging, instrument classification,\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\ntechnique recognition, instrument performance technique detection, music\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\nevaluation metrics consistent with previous state-of-the-art MIR models,\nensuring direct comparability with supervised approaches. We provide an\nevaluation toolkit supporting all open-source audio-textual LLMs, including\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\nperformance gaps between LLMs and supervised models, along with their culture,\nchronological and gender bias, highlighting the potential and limitations of\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\nfoundation for evaluating music instruction following, driving progress in\nmusic-aware LLMs."}
{"id": "2506.12308", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.12308", "abs": "https://arxiv.org/abs/2506.12308", "authors": ["Weijie Yuan", "Yuanhao Cui", "Jiacheng Wang", "Fan Liu", "Geng Sun", "Tao Xiang", "Jie Xu", "Shi Jin", "Dusit Niyato", "Sinem Coleri", "Sumei Sun", "Shiwen Mao", "Abbas Jamalipour", "Dong In Kim", "Mohamed-Slim Alouini", "Xuemin Shen"], "title": "From Ground to Sky: Architectures, Applications, and Challenges Shaping Low-Altitude Wireless Networks", "comment": "10 pages, 5 figures", "summary": "In this article, we introduce a novel low-altitude wireless network (LAWN),\nwhich is a reconfigurable, three-dimensional (3D) layered architecture. In\nparticular, the LAWN integrates connectivity, sensing, control, and computing\nacross aerial and terrestrial nodes that enable seamless operation in complex,\ndynamic, and mission-critical environments. In this article, we introduce a\nnovel low-altitude wireless network (LAWN), which is a reconfigurable,\nthree-dimensional (3D) layered architecture. Different from the conventional\naerial communication systems, LAWN's distinctive feature is its tight\nintegration of functional planes in which multiple functionalities continually\nreshape themselves to operate safely and efficiently in the low-altitude sky.\nWith the LAWN, we discuss several enabling technologies, such as integrated\nsensing and communication (ISAC), semantic communication, and fully-actuated\ncontrol systems. Finally, we identify potential applications and key\ncross-layer challenges. This article offers a comprehensive roadmap for future\nresearch and development in the low-altitude airspace."}
{"id": "2506.12440", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.DL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12440", "abs": "https://arxiv.org/abs/2506.12440", "authors": ["Federico Simonetta"], "title": "Style-based Composer Identification and Attribution of Symbolic Music Scores: a Systematic Survey", "comment": "Accepted at the TISMIR", "summary": "This paper presents the first comprehensive systematic review of literature\non style-based composer identification and authorship attribution in symbolic\nmusic scores. Addressing the critical need for improved reliability and\nreproducibility in this field, the review rigorously analyzes 58 peer-reviewed\npapers published across various historical periods, with the search adapted to\nevolving terminology. The analysis critically assesses prevailing repertoires,\ncomputational approaches, and evaluation methodologies, highlighting\nsignificant challenges. It reveals that a substantial portion of existing\nresearch suffers from inadequate validation protocols and an over-reliance on\nsimple accuracy metrics for often imbalanced datasets, which can undermine the\ncredibility of attribution claims. The crucial role of robust metrics like\nBalanced Accuracy and rigorous cross-validation in ensuring trustworthy results\nis emphasized. The survey also details diverse feature representations and the\nevolution of machine learning models employed. Notable real-world authorship\nattribution cases, such as those involving works attributed to Bach, Josquin\nDesprez, and Lennon-McCartney, are specifically discussed, illustrating the\nopportunities and pitfalls of applying computational techniques to resolve\ndisputed musical provenance. Based on these insights, a set of actionable\nguidelines for future research are proposed. These recommendations are designed\nto significantly enhance the reliability, reproducibility, and musicological\nvalidity of composer identification and authorship attribution studies,\nfostering more robust and interpretable computational stylistic analysis."}
{"id": "2506.12500", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12500", "abs": "https://arxiv.org/abs/2506.12500", "authors": ["Shota Horiguchi", "Takanori Ashihara", "Marc Delcroix", "Atsushi Ando", "Naohiro Tawara"], "title": "Mitigating Non-Target Speaker Bias in Guided Speaker Embedding", "comment": "Accepted to Interspeech 2025", "summary": "Obtaining high-quality speaker embeddings in multi-speaker conditions is\ncrucial for many applications. A recently proposed guided speaker embedding\nframework, which utilizes speech activities of target and non-target speakers\nas clues, drastically improved embeddings under severe overlap with small\ndegradation in low-overlap cases. However, since extreme overlaps are rare in\nnatural conversations, this degradation cannot be overlooked. This paper first\nreveals that the degradation is caused by the global-statistics-based modules,\nwidely used in speaker embedding extractors, being overly sensitive to\nintervals containing only non-target speakers. As a countermeasure, we propose\nan extension of such modules that exploit the target speaker activity clues, to\ncompute statistics from intervals where the target is active. The proposed\nmethod improves speaker verification performance in both low and high overlap\nratios, and diarization performance on multiple datasets."}
{"id": "2506.12599", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12599", "abs": "https://arxiv.org/abs/2506.12599", "authors": ["Anthony Joseph Perre", "Parker Huggins", "Alphan Sahin"], "title": "A Smooshed BMOCZ Zero Constellation for CFO Estimation Without Channel Coding", "comment": "This work has been accepted for presentation at IEEE PIMRC 2025", "summary": "In this study, we propose a new binary modulation on conjugate-reciprocal\nzeros (BMOCZ) zero constellation, which we call smooshed binary modulation on\nconjugate-reciprocal zeros (SBMOCZ), to address carrier frequency offset\n(CFO)-induced zero rotation without depending on channel coding. In our\napproach, we modify the phase mapping of Huffman BMOCZ by shrinking the angle\nbetween adjacent zeros, except for the first and last, to introduce a gap in\nthe zero constellation. By discerning the gap location in the received\npolynomial, the receiver can estimate and correct the phase rotation. We\ndemonstrate the error rate performance of SBMOCZ relative to Huffman BMOCZ,\nshowing that SBMOCZ addresses a CFO-induced rotation at the cost of a modest\nperformance reduction compared to Huffman BMOCZ in the absence of a CFO.\nFinally, we compare SBMOCZ to Huffman BMOCZ using a cyclically permutable code\n(CPC), showing a 4 dB bit error rate (BER) improvement in a fading channel,\nwhile demonstrating comparable performance across other simulations."}
{"id": "2506.12570", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12570", "abs": "https://arxiv.org/abs/2506.12570", "authors": ["Hui Wang", "Yifan Yang", "Shujie Liu", "Jinyu Li", "Lingwei Meng", "Yanqing Liu", "Jiaming Zhou", "Haoqin Sun", "Yan Lu", "Yong Qin"], "title": "StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling", "comment": null, "summary": "Recent advances in zero-shot text-to-speech (TTS) synthesis have achieved\nhigh-quality speech generation for unseen speakers, but most systems remain\nunsuitable for real-time applications because of their offline design. Current\nstreaming TTS paradigms often rely on multi-stage pipelines and discrete\nrepresentations, leading to increased computational cost and suboptimal system\nperformance. In this work, we propose StreamMel, a pioneering single-stage\nstreaming TTS framework that models continuous mel-spectrograms. By\ninterleaving text tokens with acoustic frames, StreamMel enables low-latency,\nautoregressive synthesis while preserving high speaker similarity and\nnaturalness. Experiments on LibriSpeech demonstrate that StreamMel outperforms\nexisting streaming TTS baselines in both quality and latency. It even achieves\nperformance comparable to offline systems while supporting efficient real-time\ngeneration, showcasing broad prospects for integration with real-time speech\nlarge language models. Audio samples are available at:\nhttps://aka.ms/StreamMel."}
{"id": "2506.12627", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12627", "abs": "https://arxiv.org/abs/2506.12627", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "Towards Neural Audio Codec Source Parsing", "comment": null, "summary": "A new class of audio deepfakes-codecfakes (CFs)-has recently caught\nattention, synthesized by Audio Language Models that leverage neural audio\ncodecs (NACs) in the backend. In response, the community has introduced\ndedicated benchmarks and tailored detection strategies. As the field advances,\nefforts have moved beyond binary detection toward source attribution, including\nopen-set attribution, which aims to identify the NAC responsible for generation\nand flag novel, unseen ones during inference. This shift toward source\nattribution improves forensic interpretability and accountability. However,\nopen-set attribution remains fundamentally limited: while it can detect that a\nNAC is unfamiliar, it cannot characterize or identify individual unseen codecs.\nIt treats such inputs as generic ``unknowns'', lacking insight into their\ninternal configuration. This leads to major shortcomings: limited\ngeneralization to new NACs and inability to resolve fine-grained variations\nwithin NAC families. To address these gaps, we propose Neural Audio Codec\nSource Parsing (NACSP) - a paradigm shift that reframes source attribution for\nCFs as structured regression over generative NAC parameters such as quantizers,\nbandwidth, and sampling rate. We formulate NACSP as a multi-task regression\ntask for predicting these NAC parameters and establish the first comprehensive\nbenchmark using various state-of-the-art speech pre-trained models (PTMs). To\nthis end, we propose HYDRA, a novel framework that leverages hyperbolic\ngeometry to disentangle complex latent properties from PTM representations. By\nemploying task-specific attention over multiple curvature-aware hyperbolic\nsubspaces, HYDRA enables superior multi-task generalization. Our extensive\nexperiments show HYDRA achieves top results on benchmark CFs datasets compared\nto baselines operating in Euclidean space."}
{"id": "2506.12639", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12639", "abs": "https://arxiv.org/abs/2506.12639", "authors": ["Amarilton L. Magalhães", "André L. F. de Almeida", "A. Lee Swindlehurst"], "title": "Semi-Blind Channel Estimation for Downlink Communications Based on Dynamic Metasurface Antennas", "comment": "3 pages, 4 figures, LateX", "summary": "Dynamic metasurface antennas (DMAs) are emerging as a promising technology to\nenable energy-efficient, large array-based multi-antenna systems. This paper\npresents a simple channel estimation scheme for the downlink of a\nmultiple-input single-output orthogonal frequency division multiplexing\n(MISO-OFDM) communication system exploiting DMAs. The proposed scheme extracts\nseparate estimates of the wireless channel and the unknown waveguide\npropagation vector using a simple iterative algorithm based on the parallel\nfactor (PARAFAC) decomposition. Obtaining decoupled estimates of the wireless\nchannel and inner waveguide vector enables the isolation and compensation for\nits effect when designing the DMA beamformer, regardless of the wireless\nchannel state, which evolves much faster due to its shorter coherence time and\nbandwidth. Additionally, our solution operates in a data-aided manner,\ndelivering estimates of useful data symbols jointly with channel estimates,\nwithout requiring sequential pilot and data stages. To the best of our\nknowledge, this is the first work to explore this CE approach. Numerical\nresults corroborate the notable performance of the proposed scheme."}
{"id": "2506.12573", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12573", "abs": "https://arxiv.org/abs/2506.12573", "authors": ["Haven Kim", "Zachary Novack", "Weihan Xu", "Julian McAuley", "Hao-Wen Dong"], "title": "Video-Guided Text-to-Music Generation Using Public Domain Movie Collections", "comment": "ISMIR 2025 regular paper. Dataset and code available at\n  https://havenpersona.github.io/ossl-v1", "summary": "Despite recent advancements in music generation systems, their application in\nfilm production remains limited, as they struggle to capture the nuances of\nreal-world filmmaking, where filmmakers consider multiple factors-such as\nvisual content, dialogue, and emotional tone-when selecting or composing music\nfor a scene. This limitation primarily stems from the absence of comprehensive\ndatasets that integrate these elements. To address this gap, we introduce Open\nScreen Sound Library (OSSL), a dataset consisting of movie clips from public\ndomain films, totaling approximately 36.5 hours, paired with high-quality\nsoundtracks and human-annotated mood information. To demonstrate the\neffectiveness of our dataset in improving the performance of pre-trained models\non film music generation tasks, we introduce a new video adapter that enhances\nan autoregressive transformer-based text-to-music model by adding video-based\nconditioning. Our experimental results demonstrate that our proposed approach\neffectively enhances MusicGen-Medium in terms of both objective measures of\ndistributional and paired fidelity, and subjective compatibility in mood and\ngenre. The dataset and code are available at\nhttps://havenpersona.github.io/ossl-v1."}
{"id": "2506.12705", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12705", "abs": "https://arxiv.org/abs/2506.12705", "authors": ["Ahsan J. Cheema", "Sunil Puria"], "title": "Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration", "comment": "Accepted for presentation at INTERSPEECH 2025", "summary": "Trouble hearing in noisy situations remains a common complaint for both\nindividuals with hearing loss and individuals with normal hearing. This is\nhypothesized to arise due to condition called: cochlear neural degeneration\n(CND) which can also result in significant variabilities in hearing aids\noutcomes. This paper uses computational models of auditory periphery to\nsimulate various hearing tasks. We present an objective method to quantify\nhearing loss and CND by comparing auditory nerve fiber responses using a\nNeurogram Similarity Index Measure (NSIM). Specifically study 1, shows that\nNSIM can be used to map performance of individuals with hearing loss on phoneme\nrecognition task with reasonable accuracy. In the study 2, we show that NSIM is\na sensitive measure that can also be used to capture the deficits resulting\nfrom CND and can be a candidate for noninvasive biomarker of auditory\nsynaptopathy."}
{"id": "2506.12682", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12682", "abs": "https://arxiv.org/abs/2506.12682", "authors": ["Yiyang Ni", "Qi Zhang", "Guangji Chen", "Yan Cai", "Jun Li", "Shi Jin"], "title": "Conditional Diffusion Model-Driven Generative Channels for Double RIS-Aided Wireless Systems", "comment": "5 pages, 4 figures", "summary": "With the development of the upcoming sixth-generation networks (6G),\nreconfigurable intelligent surfaces (RISs) have gained significant attention\ndue to its ability of reconfiguring wireless channels via smart reflections.\nHowever, traditional channel state information (CSI) acquisition techniques for\ndouble-RIS systems face challenges (e.g., high pilot overhead or multipath\ninterference). This paper proposes a new channel generation method in\ndouble-RIS communication systems based on the tool of conditional diffusion\nmodel (CDM). The CDM is trained on synthetic channel data to capture channel\ncharacteristics. It addresses the limitations of traditional CSI generation\nmethods, such as insufficient model understanding capability and poor\nenvironmental adaptability. We provide a detailed analysis of the diffusion\nprocess for channel generation, and it is validated through simulations. The\nsimulation results demonstrate that the proposed CDM based method outperforms\ntraditional channel acquisition methods in terms of normalized mean squared\nerror (NMSE). This method offers a new paradigm for channel acquisition in\ndouble-RIS systems, which is expected to improve the quality of channel\nacquisition with low pilot overhead."}
{"id": "2506.12665", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12665", "abs": "https://arxiv.org/abs/2506.12665", "authors": ["Valentin Ackva", "Fares Schulz"], "title": "ANIRA: An Architecture for Neural Network Inference in Real-Time Audio Applications", "comment": "8 pages, accepted to the Proceedings of the 5th IEEE International\n  Symposium on the Internet of Sounds (2024) - repository:\n  github.com/anira-project/anira", "summary": "Numerous tools for neural network inference are currently available, yet many\ndo not meet the requirements of real-time audio applications. In response, we\nintroduce anira, an efficient cross-platform library. To ensure compatibility\nwith a broad range of neural network architectures and frameworks, anira\nsupports ONNX Runtime, LibTorch, and TensorFlow Lite as backends. Each\ninference engine exhibits real-time violations, which anira mitigates by\ndecoupling the inference from the audio callback to a static thread pool. The\nlibrary incorporates built-in latency management and extensive benchmarking\ncapabilities, both crucial to ensure a continuous signal flow. Three different\nneural network architectures for audio effect emulation are then subjected to\nbenchmarking across various configurations. Statistical modeling is employed to\nidentify the influence of various factors on performance. The findings indicate\nthat for stateless models, ONNX Runtime exhibits the lowest runtimes. For\nstateful models, LibTorch demonstrates the fastest performance. Our results\nalso indicate that for certain model-engine combinations, the initial\ninferences take longer, particularly when these inferences exhibit a higher\nincidence of real-time violations."}
{"id": "2506.12785", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12785", "abs": "https://arxiv.org/abs/2506.12785", "authors": ["Hyeonuk Nam"], "title": "Frequency Dynamic Convolutions for Sound Event Detection", "comment": "Ph. D. Dissertation in English(KAIST)", "summary": "Recent research in deep learning-based Sound Event Detection (SED) has\nprimarily focused on Convolutional Recurrent Neural Networks (CRNNs) and\nTransformer models. However, conventional 2D convolution-based models assume\nshift invariance along both the temporal and frequency axes, leadin to\ninconsistencies when dealing with frequency-dependent characteristics of\nacoustic signals. To address this issue, this study proposes Frequency Dynamic\nConvolution (FDY conv), which dynamically adjusts convolutional kernels based\non the frequency composition of the input signal to enhance SED performance.\nFDY conv constructs an optimal frequency response by adaptively weighting\nmultiple basis kernels based on frequency-specific attention weights.\nExperimental results show that applying FDY conv to CRNNs improves performance\non the DESED dataset by 7.56% compared to the baseline CRNN. However, FDY conv\nhas limitations in that it combines basis kernels of the same shape across all\nfrequencies, restricting its ability to capture diverse frequency-specific\ncharacteristics. Additionally, the $3\\times3$ basis kernel size is insufficient\nto capture a broader frequency range. To overcome these limitations, this study\nintroduces an extended family of FDY conv models. Dilated FDY conv (DFD conv)\napplies convolutional kernels with various dilation rates to expand the\nreceptive field along the frequency axis and enhance frequency-specific feature\nrepresentation. Experimental results show that DFD conv improves performance by\n9.27% over the baseline. Partial FDY conv (PFD conv) addresses the high\ncomputational cost of FDY conv, which results from performing all convolution\noperations with dynamic kernels. Since FDY conv may introduce unnecessary\nadaptivity for quasi-stationary sound events, PFD conv integrates standard 2D\nconvolutions with frequency-adaptive kernels to reduce computational complexity\nwhile maintaining performance. Experimental results demonstrate that PFD conv\nimproves performance by 7.80% over the baseline while reducing the number of\nparameters by 54.4% compared to FDY conv. Multi-Dilated FDY conv (MDFD conv)\nextends DFD conv by addressing its structural limitation of applying the same\ndilation across all frequencies. By utilizing multiple convolutional kernels\nwith different dilation rates, MDFD conv effectively captures diverse\nfrequency-dependent patterns. Experimental results indicate that MDFD conv\nachieves the highest performance, improving the baseline CRNN performance by\n10.98%. Furthermore, standard FDY conv employs Temporal Average Pooling, which\nassigns equal weight to all frames along the time axis, limiting its ability to\neffectively capture transient events. To overcome this, this study proposes\nTAP-FDY conv (TFD conv), which integrates Temporal Attention Pooling (TA) that\nfocuses on salient features, Velocity Attention Pooling (VA) that emphasizes\ntransient characteristics, and Average Pooling (AP) that captures stationary\nproperties. TAP-FDY conv achieves the same performance as MDFD conv but reduces\nthe number of parameters by approximately 30.01% (12.703M vs. 18.157M),\nachieving equivalent accuracy with lower computational complexity. Class-wise\nperformance analysis reveals that FDY conv improves detection of non-stationary\nevents, DFD conv is particularly effective for events with broad spectral\nfeatures, and PFD conv enhances the detection of quasi-stationary events.\nAdditionally, TFD conv (TFD-CRNN) demonstrates strong performance in detecting\ntransient events. In the case studies, PFD conv effectively captures stable\nsignal patterns in tank powertrain fault recognition, DFD conv recognizes wide\nharmonic spectral patterns on speed-varying motor fault recognition, while TFD\nconv outperforms other models in detecting transient signals in offshore arc\ndetection. These results suggest that frequency-adaptive convolutions and their\nextended variants provide a robust alternative to conventional 2D convolutions\nin deep learning-based audio processing."}
{"id": "2506.12778", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12778", "abs": "https://arxiv.org/abs/2506.12778", "authors": ["Yomali Lokugama", "Saman Atapattu", "Nathan Ross", "Sithamparanathan Kandeepan", "Chintha Tellambura"], "title": "Dynamic Scheduling for Enhanced Performance in RIS-assisted Cooperative Network with Interference", "comment": "6 pages, IEEE 102nd Vehicular Technology Conference (VTC2025-Fall),\n  Chengdu, China", "summary": "Reconfigurable Intelligent Surfaces (RIS) have emerged as transformative\ntechnologies, enhancing spectral efficiency and improving interference\nmanagement in multi-user cooperative communications. This paper investigates\nthe integration of RIS with Flexible-Duplex (FlexD) communication, featuring\ndynamic scheduling capabilities, to mitigate unintended external interference\nin multi-user wireless networks. By leveraging the reconfigurability of RIS and\ndynamic scheduling, we propose a user-pair selection scheme to maximize system\nthroughput when full channel state information (CSI) of interference is\nunavailable. We develop a mathematical framework to evaluate the throughput\noutage probability when RIS introduces spatial correlation. The derived\nanalytical results are used for asymptotic analysis, providing insights into\ndynamic user scheduling under interference based on statistical channel\nknowledge. Finally, we compare FlexD with traditional Full Duplex (FD) and Half\nDuplex (HD) systems against RIS-assisted FlexD. Our results show FlexD's\nsuperior throughput enhancement, energy efficiency and data management\ncapability in interference-affected networks, typical in current and\nnext-generation cooperative wireless applications like cellular and vehicular\ncommunications."}
{"id": "2506.12672", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12672", "abs": "https://arxiv.org/abs/2506.12672", "authors": ["Yuta Hirano", "Sakriani Sakti"], "title": "SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition", "comment": "Accepted by Interspeech 2025", "summary": "We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an\nenhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT\nhandles overlapped speech, and we found the decoder performs implicit speaker\nseparation. We hypothesize this implicit separation is often insufficient due\nto ambiguous acoustic cues in overlapping regions. To address this, SC-SOT\nexplicitly conditions the decoder on speaker information, providing detailed\ninformation about \"who spoke when\". Specifically, we enhance the decoder by\nincorporating: (1) speaker embeddings, which allow the model to focus on the\nacoustic characteristics of the target speaker, and (2) speaker activity\ninformation, which guides the model to suppress non-target speakers. The\nspeaker embeddings are derived from a jointly trained E2E speaker diarization\nmodel, mitigating the need for speaker enrollment. Experimental results\ndemonstrate the effectiveness of our conditioning approach on overlapped\nspeech."}
{"id": "2506.12817", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12817", "abs": "https://arxiv.org/abs/2506.12817", "authors": ["Zhihong Jia", "Hongbin Wang", "Yuanzhong Shen", "Feng Hu", "Jiayu An", "Kai Shu", "Dongrui Wu"], "title": "Magnetoencephalography (MEG) Based Non-Invasive Chinese Speech Decoding", "comment": null, "summary": "As an emerging paradigm of brain-computer interfaces (BCIs), speech BCI has\nthe potential to directly reflect auditory perception and thoughts, offering a\npromising communication alternative for patients with aphasia. Chinese is one\nof the most widely spoken languages in the world, whereas there is very limited\nresearch on speech BCIs for Chinese language. This paper reports a\ntext-magnetoencephalography (MEG) dataset for non-invasive Chinese speech BCIs.\nIt also proposes a multi-modality assisted speech decoding (MASD) algorithm to\ncapture both text and acoustic information embedded in brain signals during\nspeech activities. Experiment results demonstrated the effectiveness of both\nour text-MEG dataset and our proposed MASD algorithm. To our knowledge, this is\nthe first study on modality-assisted decoding for non-invasive speech BCIs."}
{"id": "2506.12831", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12831", "abs": "https://arxiv.org/abs/2506.12831", "authors": ["Zonghui Yang", "Shijian Gao", "Xiang Cheng", "Liuqing Yang"], "title": "Synesthesia of Machines (SoM)-Enhanced Sub-THz ISAC Transmission for Air-Ground Network", "comment": null, "summary": "Integrated sensing and communication (ISAC) within sub-THz frequencies is\ncrucial for future air-ground networks, but unique propagation characteristics\nand hardware limitations present challenges in optimizing ISAC performance\nwhile increasing operational latency. This paper introduces a multi-modal\nsensing fusion framework inspired by synesthesia of machine (SoM) to enhance\nsub-THz ISAC transmission. By exploiting inherent degrees of freedom in sub-THz\nhardware and channels, the framework optimizes the radio-frequency environment.\nSquint-aware beam management is developed to improve air-ground network\nadaptability, enabling three-dimensional dynamic ISAC links. Leveraging\nmulti-modal information, the framework enhances ISAC performance and reduces\nlatency. Visual data rapidly localizes users and targets, while a customized\nmulti-modal learning algorithm optimizes the hybrid precoder. A new metric\nprovides comprehensive performance evaluation, and extensive experiments\ndemonstrate that the proposed scheme significantly improves ISAC efficiency."}
{"id": "2506.13001", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "I.2.1; I.2.6; H.5.5; J.5"], "pdf": "https://arxiv.org/pdf/2506.13001", "abs": "https://arxiv.org/abs/2506.13001", "authors": ["Christian Zhou-Zheng", "Philippe Pasquier"], "title": "Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV", "comment": null, "summary": "Existing work in automatic music generation has primarily focused on\nend-to-end systems that produce complete compositions or continuations.\nHowever, because musical composition is typically an iterative process, such\nsystems make it difficult to engage in the back-and-forth between human and\nmachine that is essential to computer-assisted creativity. In this study, we\naddress the task of personalizable, multi-track, long-context, and controllable\nsymbolic music infilling to enhance the process of computer-assisted\ncomposition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear\narchitecture, to enable efficient and coherent musical cocreation on edge\ndevices. We also demonstrate that MIDI-RWKV admits an effective method of\nfinetuning its initial state for personalization in the very-low-sample regime.\nWe evaluate MIDI-RWKV and its state tuning on several quantitative and\nqualitative metrics, and release model weights and code at\nhttps://github.com/christianazinn/MIDI-RWKV."}
{"id": "2506.13053", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13053", "abs": "https://arxiv.org/abs/2506.13053", "authors": ["Han Zhu", "Wei Kang", "Zengwei Yao", "Liyong Guo", "Fangjun Kuang", "Zhaoqing Li", "Weiji Zhuang", "Long Lin", "Daniel Povey"], "title": "ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching", "comment": null, "summary": "Existing large-scale zero-shot text-to-speech (TTS) models deliver high\nspeech quality but suffer from slow inference speeds due to massive parameters.\nTo address this issue, this paper introduces ZipVoice, a high-quality\nflow-matching-based zero-shot TTS model with a compact model size and fast\ninference speed. Key designs include: 1) a Zipformer-based flow-matching\ndecoder to maintain adequate modeling capabilities under constrained size; 2)\nAverage upsampling-based initial speech-text alignment and Zipformer-based text\nencoder to improve speech intelligibility; 3) A flow distillation method to\nreduce sampling steps and eliminate the inference overhead associated with\nclassifier-free guidance. Experiments on 100k hours multilingual datasets show\nthat ZipVoice matches state-of-the-art models in speech quality, while being 3\ntimes smaller and up to 30 times faster than a DiT-based flow-matching\nbaseline. Codes, model checkpoints and demo samples are publicly available."}
{"id": "2506.12908", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12908", "abs": "https://arxiv.org/abs/2506.12908", "authors": ["Runnan Liu", "Weifeng Zhu", "Shu Sun", "Wenjun Zhang"], "title": "Low-Latency Terrestrial Interference Detection for Satellite-to-Device Communications", "comment": "6 pages", "summary": "Direct satellite-to-device communication is a promising future direction due\nto its lower latency and enhanced efficiency. However, intermittent and\nunpredictable terrestrial interference significantly affects system reliability\nand performance. Continuously employing sophisticated interference mitigation\ntechniques is practically inefficient. Motivated by the periodic idle intervals\ncharacteristic of burst-mode satellite transmissions, this paper investigates\nonline interference detection frameworks specifically tailored for\nsatellite-to-device scenarios. We first rigorously formulate interference\ndetection as a binary hypothesis testing problem, leveraging differences\nbetween Rayleigh (no interference) and Rice (interference present)\ndistributions. Then, we propose a cumulative sum (CUSUM)-based online detector\nfor scenarios with known interference directions, explicitly characterizing the\ntrade-off between detection latency and false alarm rate, and establish its\nasymptotic optimality. For practical scenarios involving unknown interference\ndirection, we further propose a generalized likelihood ratio (GLR)-based\ndetection method, jointly estimating interference direction via the Root-MUSIC\nalgorithm. Numerical results validate our theoretical findings and demonstrate\nthat our proposed methods achieve high detection accuracy with remarkably low\nlatency, highlighting their practical applicability in future\nsatellite-to-device communication systems."}
{"id": "2506.13127", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13127", "abs": "https://arxiv.org/abs/2506.13127", "authors": ["Jiaming Cheng", "Ruiyu Liang", "Chao Xu", "Ye Ni", "Wei Zhou", "Björn W. Schuller", "Xiaoshuai Hao"], "title": "I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency Calibration for Speech Enhancement", "comment": "submitted to IEEE Transactions on Neural Networks and Learning\n  Systems", "summary": "In recent years, complexity compression of neural network (NN)-based speech\nenhancement (SE) models has gradually attracted the attention of researchers,\nespecially in scenarios with limited hardware resources or strict latency\nrequirements. The main difficulties and challenges lie in achieving a balance\nbetween complexity and performance according to the characteristics of the\ntask. In this paper, we propose an intra-inter set knowledge distillation (KD)\nframework with time-frequency calibration (I$^2$S-TFCKD) for SE. Different from\nprevious distillation strategies for SE, the proposed framework fully utilizes\nthe time-frequency differential information of speech while promoting global\nknowledge flow. Firstly, we propose a multi-layer interactive distillation\nbased on dual-stream time-frequency cross-calibration, which calculates the\nteacher-student similarity calibration weights in the time and frequency\ndomains respectively and performs cross-weighting, thus enabling refined\nallocation of distillation contributions across different layers according to\nspeech characteristics. Secondly, we construct a collaborative distillation\nparadigm for intra-set and inter-set correlations. Within a correlated set,\nmulti-layer teacher-student features are pairwise matched for calibrated\ndistillation. Subsequently, we generate representative features from each\ncorrelated set through residual fusion to form the fused feature set that\nenables inter-set knowledge interaction. The proposed distillation strategy is\napplied to the dual-path dilated convolutional recurrent network (DPDCRN) that\nranked first in the SE track of the L3DAS23 challenge. Objective evaluations\ndemonstrate that the proposed KD strategy consistently and effectively improves\nthe performance of the low-complexity student model and outperforms other\ndistillation schemes."}
{"id": "2506.13279", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13279", "abs": "https://arxiv.org/abs/2506.13279", "authors": ["David Sundström", "Filip Elvander", "Andreas Jakobsson"], "title": "Boundary-Informed Sound Field Reconstruction", "comment": "Accepted for publication at EUSIPCO 2025", "summary": "We consider the problem of reconstructing the sound field in a room using\nprior information of the boundary geometry, represented as a point cloud. In\ngeneral, when no boundary information is available, an accurate sound field\nreconstruction over a large spatial region and at high frequencies requires\nnumerous microphone measurements. On the other hand, if all geometrical and\nacoustical aspects of the boundaries are known, the sound field could, in\ntheory, be simulated without any measurements. In this work, we address the\nintermediate case, where only partial or uncertain boundary information is\navailable. This setting is similar to one studied in virtual reality\napplications, where the goal is to create a perceptually convincing audio\nexperience. In this work, we focus on spatial sound control applications, which\nin contrast require an accurate sound field reconstruction. Therefore, we\nformulate the problem within a linear Bayesian framework, incorporating a\nboundary-informed prior derived from impedance boundary conditions. The\nformulation allows for joint optimization of the unknown hyperparameters,\nincluding the noise and signal variances and the impedance boundary conditions.\nUsing numerical experiments, we show that incorporating the boundary-informed\nprior significantly enhances the reconstruction, notably even when only a few\nhundreds of boundary points are available or when the boundary positions are\ncalibrated with an uncertainty up to 1 dm."}
{"id": "2506.12964", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12964", "abs": "https://arxiv.org/abs/2506.12964", "authors": ["Abuzar B. M. Adam", "Mohammed A. M. Elhassan", "Elhadj Moustapha Diallo", "Mohamed Amine Ouamri"], "title": "Interference Mitigation in STAR-RIS-Aided Multi-User Networks with Statistical CSI", "comment": "6 pages, 3 figures", "summary": "In this paper, we investigate real-time interference mitigation in multiuser\nwireless networks assisted by simultaneously transmitting and reflecting\nreconfigurable intelligent surfaces (STAR-RISs). Unlike conventional methods\nthat rely on instantaneous channel state information (CSI), we consider a\npractical scenario where only statistical CSI is available, and the STAR-RIS\nphase shifts are impaired by random phase errors modeled via the Von Mises\ndistribution. To tackle the resulting nonconvex optimization problem induced by\nunit-modulus constraints and stochastic interference, we derive a closed-form\napproximation of the effective channel matrix using statistical expectations.\nWe then reformulate the interference minimization problem as an unconstrained\noptimization over a Riemannian manifold and propose a conjugate gradient\nalgorithm tailored to the complex circle manifold. The proposed solution\nenables efficient real-time computation of optimal phase shifts while\naccounting for hardware imperfections and limited CSI. Simulation results\nconfirm that our method significantly suppresses inter-user interference and\nachieves superior SINR performance and convergence speed compared to\nconventional baselines."}
{"id": "2506.13272", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13272", "abs": "https://arxiv.org/abs/2506.13272", "authors": ["Pranav M N", "Gandham Sai Santhosh", "Tejas Joshi", "S Sriniketh Desikan", "Eswar Gupta"], "title": "SONIC: Sound Optimization for Noise In Crowds", "comment": null, "summary": "This paper presents SONIC, an embedded real-time noise suppression system\nimplemented on the ARM Cortex-M7-based STM32H753ZI microcontroller. Using\nadaptive filtering (LMS), the system improves speech intelligibility in noisy\nenvironments. SONIC focuses on a novel approach to noise suppression in audio\nsignals, specifically addressing the limitations of traditional Active Noise\nCancellation (ANC) systems. The paper explores various signal processing\nalgorithms in a micro-controller point of view, highlighting various\nperformance factors and which were considered optimal in our embedded system.\nAdditionally we also discussed the system architecture, explaining how the\nMCU's efficiency was harnessed, along with an in-depth overview of how the\naudio signals were translated within the processor. The results demonstrate\nimproved speech clarity and practical real-time performance, showing low-power\nDSP as an alternative to complex AI denoising methods."}
{"id": "2506.13295", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13295", "abs": "https://arxiv.org/abs/2506.13295", "authors": ["Taewoo Kim", "Uijong Lee", "Hayoung Park", "Choongsang Cho", "Nam In Park", "Young Han Lee"], "title": "Instance-Specific Test-Time Training for Speech Editing in the Wild", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "Speech editing systems aim to naturally modify speech content while\npreserving acoustic consistency and speaker identity. However, previous studies\noften struggle to adapt to unseen and diverse acoustic conditions, resulting in\ndegraded editing performance in real-world scenarios. To address this, we\npropose an instance-specific test-time training method for speech editing in\nthe wild. Our approach employs direct supervision from ground-truth acoustic\nfeatures in unedited regions, and indirect supervision in edited regions via\nauxiliary losses based on duration constraints and phoneme prediction. This\nstrategy mitigates the bandwidth discontinuity problem in speech editing,\nensuring smooth acoustic transitions between unedited and edited regions.\nAdditionally, it enables precise control over speech rate by adapting the model\nto target durations via mask length adjustment during test-time training.\nExperiments on in-the-wild benchmark datasets demonstrate that our method\noutperforms existing speech editing systems in both objective and subjective\nevaluations."}
{"id": "2506.12997", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12997", "abs": "https://arxiv.org/abs/2506.12997", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "MORIC: CSI Delay-Doppler Decomposition for Robust Wi-Fi-based Human Activity Recognition", "comment": null, "summary": "The newly established IEEE 802.11bf Task Group aims to amend the WLAN\nstandard to support advanced sensing applications such as human activity\nrecognition (HAR). Although studies have demonstrated the potential of sub-7\nGHz Wi-Fi Channel State Information (CSI) for HAR, no method currently performs\nreliably in real-world scenarios. This work tackles the poor generalization of\nWi-Fi-based HAR by introducing an innovative approach to extracting and\nutilizing movement-related representations, which makes it robust to noise and\nstatic environmental properties. This is achieved by transforming CSI signals\ninto the delay profile space and decomposing them into various Doppler\nvelocities, which serve as informative projections of a mobile point's velocity\nfrom different unknown random angles. To mitigate the impact of this\nrandomness, MORIC is introduced as a novel time series classification model\nbased on random convolutional kernels, designed to be invariant to the random\norder and repetition of input representations, thereby enabling robust Wi-Fi\nCSI-based activity classification. Experimental results on the collected\ndataset demonstrate that the proposed method outperforms state-of-the-art\napproaches in terms of generalization accuracy for hand motion recognition,\nparticularly for challenging gestures. Furthermore, incorporating a small\nnumber of calibration samples leads to a significant improvement in accuracy,\nenhancing the practicality of the method for real-world deployment."}
{"id": "2506.13595", "categories": ["cs.SD", "cs.CG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13595", "abs": "https://arxiv.org/abs/2506.13595", "authors": ["Eunwoo Heo", "Byeongchan Choi", "Myung ock Kim", "Mai Lan Tran", "Jae-Hun Jung"], "title": "Persistent Homology of Music Network with Three Different Distances", "comment": null, "summary": "Persistent homology has been widely used to discover hidden topological\nstructures in data across various applications, including music data. To apply\npersistent homology, a distance or metric must be defined between points in a\npoint cloud or between nodes in a graph network. These definitions are not\nunique and depend on the specific objectives of a given problem. In other\nwords, selecting different metric definitions allows for multiple topological\ninferences. In this work, we focus on applying persistent homology to music\ngraph with predefined weights. We examine three distinct distance definitions\nbased on edge-wise pathways and demonstrate how these definitions affect\npersistent barcodes, persistence diagrams, and birth/death edges. We found that\nthere exist inclusion relations in one-dimensional persistent homology\nreflected on persistence barcode and diagram among these three distance\ndefinitions. We verified these findings using real music data."}
{"id": "2506.13414", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13414", "abs": "https://arxiv.org/abs/2506.13414", "authors": ["Alexander Polok", "Jiangyu Han", "Dominik Klement", "Samuele Cornell", "Jan Černocký", "Lukáš Burget"], "title": "BUT System for the MLC-SLM Challenge", "comment": null, "summary": "We present a two-speaker automatic speech recognition (ASR) system that\ncombines DiCoW -- a diarization-conditioned variant of Whisper -- with\nDiariZen, a diarization pipeline built on top of Pyannote. We first evaluate\nboth systems in out-of-domain (OOD) multilingual scenarios without any\nfine-tuning. In this scenario, DiariZen consistently outperforms the baseline\nPyannote diarization model, demonstrating strong generalization. Despite being\nfine-tuned on English-only data for target-speaker ASR, DiCoW retains solid\nmultilingual performance, indicating that encoder modifications preserve\nWhisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen\non the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform\nthe fine-tuned Pyannote baseline, while DiCoW sees further gains from domain\nadaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and\nranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several\nlabeling inconsistencies in the training data -- such as missing speech\nsegments and incorrect silence annotations -- which can hinder diarization\nfine-tuning. We propose simple mitigation strategies to address these issues\nand improve system robustness."}
{"id": "2506.13008", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13008", "abs": "https://arxiv.org/abs/2506.13008", "authors": ["Minwoo Kim", "Youngchol Choi", "Yeongjun Kim", "Eojin Seo", "Hyun Jong Yang"], "title": "Joint Spectrum Sensing and Resource Allocation for OFDMA-based Underwater Acoustic Communications", "comment": "14 pages, 4 figures", "summary": "Underwater acoustic (UWA) communications generally rely on cognitive radio\n(CR)-based ad-hoc networks due to challenges such as long propagation delay,\nlimited channel resources, and high attenuation. To address the constraints of\nlimited frequency resources, UWA communications have recently incorporated\northogonal frequency division multiple access (OFDMA), significantly enhancing\nspectral efficiency (SE) through multiplexing gains. Still, {the} low\npropagation speed of UWA signals, combined with {the} dynamic underwater\nenvironment, creates asynchrony in multiple access scenarios. This causes\ninaccurate spectrum sensing as inter-carrier interference (ICI) increases,\nwhich leads to difficulties in resource allocation. As efficient resource\nallocation is essential for achieving high-quality communication in OFDMA-based\nCR networks, these challenges degrade communication reliability in UWA systems.\nTo resolve the issue, we propose an end-to-end sensing and resource\noptimization method using deep reinforcement learning (DRL) in an OFDMA-based\nUWA-CR network. Through extensive simulations, we confirm that the proposed\nmethod is superior to baseline schemes, outperforming other methods by 42.9 %\nin SE and 4.4 % in communication success rate."}
{"id": "2506.12059", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12059", "abs": "https://arxiv.org/abs/2506.12059", "authors": ["Jiajun He", "Naoki Sawada", "Koichi Miyazaki", "Tomoki Toda"], "title": "CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models", "comment": "Accepted by INTERSPEECH 2025", "summary": "In real-world applications, automatic speech recognition (ASR) systems must\nhandle overlapping speech from multiple speakers and recognize rare words like\ntechnical terms. Traditional methods address multi-talker ASR and contextual\nbiasing separately, limiting performance in complex scenarios. We propose a\nunified framework that combines multi-talker overlapping speech recognition and\ncontextual biasing into a single task. Our ASR method integrates pretrained\nspeech encoders and large language models (LLMs), using optimized finetuning\nstrategies. We also introduce a two-stage filtering algorithm to efficiently\nidentify relevant rare words from large biasing lists and incorporate them into\nthe LLM's prompt input, enhancing rare word recognition. Experiments show that\nour approach outperforms traditional contextual biasing methods, achieving a\nWER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000,\ndemonstrating its effectiveness in complex speech scenarios."}
{"id": "2506.13455", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13455", "abs": "https://arxiv.org/abs/2506.13455", "authors": ["Wenmiao Gao", "Yang Xiao"], "title": "Stereo sound event localization and detection based on PSELDnet pretraining and BiMamba sequence modeling", "comment": "Technical report for DCASE 2025 Challenge Task 3", "summary": "Pre-training methods have achieved significant performance improvements in\nsound event localization and detection (SELD) tasks, but existing\nTransformer-based models suffer from high computational complexity. In this\nwork, we propose a stereo sound event localization and detection system based\non pre-trained PSELDnet and bidirectional Mamba sequence modeling. We replace\nthe Conformer module with a BiMamba module and introduce asymmetric\nconvolutions to more effectively model the spatiotemporal relationships between\ntime and frequency dimensions. Experimental results demonstrate that the\nproposed method achieves significantly better performance than the baseline and\nthe original PSELDnet with Conformer decoder architecture on the DCASE2025 Task\n3 development dataset, while also reducing computational complexity. These\nfindings highlight the effectiveness of the BiMamba architecture in addressing\nthe challenges of the SELD task."}
{"id": "2506.13014", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13014", "abs": "https://arxiv.org/abs/2506.13014", "authors": ["Jason M. Merlo", "Jeffrey A. Nanzer"], "title": "Collaborative Beamforming for Communication Applications Using a Two-Element Fully-Wireless Open-Loop Coherent Distributed Array", "comment": null, "summary": "In this work we demonstrate a proof of concept of a fully-wireless two-node\nopen-loop coherent distributed communication system and evaluate its\nperformance by transmitting QPSK , 64-, and 256-QAM constellations at a symbol\nrate of 2 MBd over a 58 m link in an urban environment. The system is\nimplemented in a distributed manner with on-node processing using\nsoftware-defined radios (SDRs) and wireless internode communication to share\ncoordination information and does not rely on external time or frequency\nreferences such as the global navigation satellite system (GNSS). In each\nexperiment ~100 messages were transmitted and a mean coherent gain of 0.936 was\nachieved across all measurements with a mean symbol error ratio of below\n$1.4\\times 10^{-4}$ achieved up to 64-QAM, demonstrating a reliable bandwidth\nof up to 12 Mbps."}
{"id": "2506.12067", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12067", "abs": "https://arxiv.org/abs/2506.12067", "authors": ["Aditya Kamlesh Parikh", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Evaluating Logit-Based GOP Scores for Mispronunciation Detection", "comment": "Accepted to Interspeech 2025. This publication is part of the project\n  Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013\n  of the research programme NGF AiNed Fellowship Grants which is financed by\n  the Dutch Research Council (NWO)", "summary": "Pronunciation assessment relies on goodness of pronunciation (GOP) scores,\ntraditionally derived from softmax-based posterior probabilities. However,\nposterior probabilities may suffer from overconfidence and poor phoneme\nseparation, limiting their effectiveness. This study compares logit-based GOP\nscores with probability-based GOP scores for mispronunciation detection. We\nconducted our experiment on two L2 English speech datasets spoken by Dutch and\nMandarin speakers, assessing classification performance and correlation with\nhuman ratings. Logit-based methods outperform probability-based GOP in\nclassification, but their effectiveness depends on dataset characteristics. The\nmaximum logit GOP shows the strongest alignment with human perception, while a\ncombination of different GOP scores balances probability and logit features.\nThe findings suggest that hybrid GOP methods incorporating uncertainty modeling\nand phoneme-specific weighting improve pronunciation assessment."}
{"id": "2506.13709", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13709", "abs": "https://arxiv.org/abs/2506.13709", "authors": ["Sirui Li", "Shuai Wang", "Zhijun Liu", "Zhongjie Jiang", "Yannan Wang", "Haizhou Li"], "title": "SpeechRefiner: Towards Perceptual Quality Refinement for Front-End Algorithms", "comment": "Accepted by Interspeech 2025", "summary": "Speech pre-processing techniques such as denoising, de-reverberation, and\nseparation, are commonly employed as front-ends for various downstream speech\nprocessing tasks. However, these methods can sometimes be inadequate, resulting\nin residual noise or the introduction of new artifacts. Such deficiencies are\ntypically not captured by metrics like SI-SNR but are noticeable to human\nlisteners. To address this, we introduce SpeechRefiner, a post-processing tool\nthat utilizes Conditional Flow Matching (CFM) to improve the perceptual quality\nof speech. In this study, we benchmark SpeechRefiner against recent\ntask-specific refinement methods and evaluate its performance within our\ninternal processing pipeline, which integrates multiple front-end algorithms.\nExperiments show that SpeechRefiner exhibits strong generalization across\ndiverse impairment sources, significantly enhancing speech perceptual quality.\nAudio demos can be found at https://speechrefiner.github.io/SpeechRefiner/."}
{"id": "2506.13258", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.13258", "abs": "https://arxiv.org/abs/2506.13258", "authors": ["Murat Babek Salman", "Emil Björnson"], "title": "DoA Estimation using MUSIC with Range/Doppler Multiplexing for MIMO-OFDM Radar", "comment": "The paper was presented at IEEE ICC2025", "summary": "Sensing emerges as a critical challenge in 6G networks, which require\nsimultaneous communication and target sensing capabilities. State-of-the-art\nsuper-resolution techniques for the direction of arrival (DoA) estimation\nencounter significant performance limitations when the number of targets\nexceeds antenna array dimensions. This paper introduces a novel sensing\nparameter estimation algorithm for orthogonal frequency-division multiplexing\n(OFDM) multiple-input multiple-output (MIMO) radar systems. The proposed\napproach implements a strategic two-stage methodology: first, discriminating\ntargets through delay and Doppler domain filtering to reduce the number of\neffective targets for super-resolution DoA estimation, and second, introducing\na fusion technique to mitigate sidelobe interferences. The algorithm enables\nrobust DoA estimation, particularly in high-density target environments with\nlimited-size antenna arrays. Numerical simulations validate the superior\nperformance of the proposed method compared to conventional DoA estimation\napproaches."}
{"id": "2506.12073", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12073", "abs": "https://arxiv.org/abs/2506.12073", "authors": ["Zongli Ye", "Jiachen Lian", "Xuanru Zhou", "Jinming Zhang", "Haodong Li", "Shuhe Li", "Chenxu Guo", "Anaisha Das", "Peter Park", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis", "comment": "Accepted for Interspeech2025", "summary": "Accurate alignment of dysfluent speech with intended text is crucial for\nautomating the diagnosis of neurodegenerative speech disorders. Traditional\nmethods often fail to model phoneme similarities effectively, limiting their\nperformance. In this work, we propose Neural LCS, a novel approach for\ndysfluent text-text and speech-text alignment. Neural LCS addresses key\nchallenges, including partial alignment and context-aware similarity mapping,\nby leveraging robust phoneme-level modeling. We evaluate our method on a\nlarge-scale simulated dataset, generated using advanced data simulation\ntechniques, and real PPA data. Neural LCS significantly outperforms\nstate-of-the-art models in both alignment accuracy and dysfluent speech\nsegmentation. Our results demonstrate the potential of Neural LCS to enhance\nautomated systems for diagnosing and analyzing speech disorders, offering a\nmore accurate and linguistically grounded solution for dysfluent speech\nalignment."}
{"id": "2506.12083", "categories": ["cs.SD", "cs.MA", "eess.AS", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.12083", "abs": "https://arxiv.org/abs/2506.12083", "authors": ["Amitesh Pandey", "Jafarbek Arifdjanov", "Ansh Tiwari"], "title": "TuneGenie: Reasoning-based LLM agents for preferential music generation", "comment": "15 pages", "summary": "Recently, Large language models (LLMs) have shown great promise across a\ndiversity of tasks, ranging from generating images to reasoning spatially.\nConsidering their remarkable (and growing) textual reasoning capabilities, we\ninvestigate LLMs' potency in conducting analyses of an individual's preferences\nin music (based on playlist metadata, personal write-ups, etc.) and producing\neffective prompts (based on these analyses) to be passed to Suno AI (a\ngenerative AI tool for music production). Our proposition of a novel LLM-based\ntextual representation to music model (which we call TuneGenie) and the various\nmethods we develop to evaluate & benchmark similar models add to the increasing\n(and increasingly controversial) corpus of research on the use of AI in\ngenerating art."}
{"id": "2506.13330", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13330", "abs": "https://arxiv.org/abs/2506.13330", "authors": ["Ashwani Koul", "Gustaf Hendeby", "Isaac Skog"], "title": "Performance Analysis of Communication Signals for Localization in Underwater Sensor Networks", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Fusion of passive and active measurements from sensor nodes becomes critical\nin localizing underwater objects and is traditionally achieved by communicating\ninformation to a central node. This causes significant inefficiencies in\nbandwidth, energy, and processing time, which are critical in marine\napplications. With integrated sensing and communication (ISAC) systems, the\nprocess of sensing, localization, and communication can be achieved jointly,\nand the inefficiencies can be minimized. Thus, the primary objective of this\nstudy is to analyse the efficacy of such communication signals in localizing a\nmoving target in given underwater conditions. The Cram\\'er-Rao Lower Bound\n(CRLB) is a performance metric used to determine the theoretical lower bound on\nlocalization errors. Simulation results illustrate the contours of localization\nerror across various scenarios, offering valuable insights into system\nperformance under different target dynamics and sea state conditions,\nshowcasing their potential for efficient and reliable underwater localization\napplications."}
{"id": "2506.12285", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12285", "abs": "https://arxiv.org/abs/2506.12285", "authors": ["Yinghao Ma", "Siyou Li", "Juntao Yu", "Emmanouil Benetos", "Akira Maezawa"], "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following", "comment": "Accepted by ISMIR 2025", "summary": "Recent advances in audio-text large language models (LLMs) have opened new\npossibilities for music understanding and generation. However, existing\nbenchmarks are limited in scope, often relying on simplified tasks or\nmulti-choice evaluations that fail to reflect the complexity of real-world\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\ninstruction following benchmark designed to evaluate audio-text LLMs on a\ndiverse set of music information retrieval (MIR) tasks. These include genre\nclassification, emotion regression, emotion tagging, instrument classification,\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\ntechnique recognition, instrument performance technique detection, music\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\nevaluation metrics consistent with previous state-of-the-art MIR models,\nensuring direct comparability with supervised approaches. We provide an\nevaluation toolkit supporting all open-source audio-textual LLMs, including\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\nperformance gaps between LLMs and supervised models, along with their culture,\nchronological and gender bias, highlighting the potential and limitations of\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\nfoundation for evaluating music instruction following, driving progress in\nmusic-aware LLMs."}
{"id": "2506.12154", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12154", "abs": "https://arxiv.org/abs/2506.12154", "authors": ["Haoran Zhou", "Xingchen Song", "Brendan Fahy", "Qiaochu Song", "Binbin Zhang", "Zhendong Peng", "Anshul Wadhawan", "Denglin Jiang", "Apurv Verma", "Vinay Ramesh", "Srivas Prasad", "Michele M. Franceschini"], "title": "Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding", "comment": "Accepted to INTERSPEECH 2025", "summary": "OpenAI Whisper is a family of robust Automatic Speech Recognition (ASR)\nmodels trained on 680,000 hours of audio. However, its encoder-decoder\narchitecture, trained with a sequence-to-sequence objective, lacks native\nsupport for streaming ASR. In this paper, we fine-tune Whisper for streaming\nASR using the WeNet toolkit by adopting a Unified Two-pass (U2) structure. We\nintroduce an additional Connectionist Temporal Classification (CTC) decoder\ntrained with causal attention masks to generate streaming partial transcripts,\nwhile the original Whisper decoder reranks these partial outputs. Our\nexperiments on LibriSpeech and an earnings call dataset demonstrate that, with\nadequate fine-tuning data, Whisper can be adapted into a capable streaming ASR\nmodel. We also introduce a hybrid tokenizer approach, which uses a smaller\ntoken space for the CTC decoder while retaining Whisper's original token space\nfor the attention decoder, resulting in improved data efficiency and\ngeneralization."}
{"id": "2506.13408", "categories": ["eess.SP", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.13408", "abs": "https://arxiv.org/abs/2506.13408", "authors": ["Miguel Camelo Botero", "Esra Aycan Beyazit", "Nina Slamnik-Kriještorac", "Johann M. Marquez-Barja"], "title": "HELENA: High-Efficiency Learning-based channel Estimation using dual Neural Attention", "comment": null, "summary": "Accurate channel estimation is critical for high-performance Orthogonal\nFrequency-Division Multiplexing systems such as 5G New Radio, particularly\nunder low signal-to-noise ratio and stringent latency constraints. This letter\npresents HELENA, a compact deep learning model that combines a lightweight\nconvolutional backbone with two efficient attention mechanisms: patch-wise\nmulti-head self-attention for capturing global dependencies and a\nsqueeze-and-excitation block for local feature refinement. Compared to CEViT, a\nstate-of-the-art vision transformer-based estimator, HELENA reduces inference\ntime by 45.0\\% (0.175\\,ms vs.\\ 0.318\\,ms), achieves comparable accuracy\n($-16.78$\\,dB vs.\\ $-17.30$\\,dB), and requires $8\\times$ fewer parameters\n(0.11M vs.\\ 0.88M), demonstrating its suitability for low-latency, real-time\ndeployment."}
{"id": "2506.12500", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12500", "abs": "https://arxiv.org/abs/2506.12500", "authors": ["Shota Horiguchi", "Takanori Ashihara", "Marc Delcroix", "Atsushi Ando", "Naohiro Tawara"], "title": "Mitigating Non-Target Speaker Bias in Guided Speaker Embedding", "comment": "Accepted to Interspeech 2025", "summary": "Obtaining high-quality speaker embeddings in multi-speaker conditions is\ncrucial for many applications. A recently proposed guided speaker embedding\nframework, which utilizes speech activities of target and non-target speakers\nas clues, drastically improved embeddings under severe overlap with small\ndegradation in low-overlap cases. However, since extreme overlaps are rare in\nnatural conversations, this degradation cannot be overlooked. This paper first\nreveals that the degradation is caused by the global-statistics-based modules,\nwidely used in speaker embedding extractors, being overly sensitive to\nintervals containing only non-target speakers. As a countermeasure, we propose\nan extension of such modules that exploit the target speaker activity clues, to\ncompute statistics from intervals where the target is active. The proposed\nmethod improves speaker verification performance in both low and high overlap\nratios, and diarization performance on multiple datasets."}
{"id": "2506.12199", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12199", "abs": "https://arxiv.org/abs/2506.12199", "authors": ["Jaeyeon Kim", "Heeseung Yun", "Gunhee Kim"], "title": "ViSAGe: Video-to-Spatial Audio Generation", "comment": "ICLR 2025. Project page: https://jaeyeonkim99.github.io/visage/", "summary": "Spatial audio is essential for enhancing the immersiveness of audio-visual\nexperiences, yet its production typically demands complex recording systems and\nspecialized expertise. In this work, we address a novel problem of generating\nfirst-order ambisonics, a widely used spatial audio format, directly from\nsilent videos. To support this task, we introduce YT-Ambigen, a dataset\ncomprising 102K 5-second YouTube video clips paired with corresponding\nfirst-order ambisonics. We also propose new evaluation metrics to assess the\nspatial aspect of generated audio based on audio energy maps and saliency\nmetrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an\nend-to-end framework that generates first-order ambisonics from silent video\nframes by leveraging CLIP visual features, autoregressive neural audio codec\nmodeling with both directional and visual guidance. Experimental results\ndemonstrate that ViSAGe produces plausible and coherent first-order ambisonics,\noutperforming two-stage approaches consisting of video-to-audio generation and\naudio spatialization. Qualitative examples further illustrate that ViSAGe\ngenerates temporally aligned high-quality spatial audio that adapts to\nviewpoint changes."}
{"id": "2506.13490", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13490", "abs": "https://arxiv.org/abs/2506.13490", "authors": ["Qiao Ren", "Xidong Mu", "Siyu Lin", "Yuanwei Liu"], "title": "Pinching-Antenna Systems (PASS) Meet Multiple Access: NOMA or OMA?", "comment": null, "summary": "A fundamental two-user PASS-based communication system is considered under\nthree MA schemes, namely non-orthogonal multiple access (NOMA), frequency\ndivision multiple access (FDMA), and time division multiple access (TDMA). For\neach MA scheme, a pinching beamforming optimization problem is formulated to\nminimize the required transmit power for satisfying users' rate requirements.\nFor NOMA and FDMA, a two-stage algorithm is proposed, where the locations of\nPAs are derived sequentially by using the successive convex approximation (SCA)\nmethod and fine-turning phase adjustment. For TDMA, by leveraging the\ntime-switching feature of PASS, the optimal pinching beamforming of each time\nslot is derived to maximize the served user channel gain. Numerical results are\nprovided to show that: 1) PASS can achieve a significant performance gain over\nconventional antenna systems, and 2) NOMA consistently outperforms FDMA, while\nTDMA provides superior performance than NOMA for symmetric user rate\nrequirements."}
{"id": "2506.12627", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12627", "abs": "https://arxiv.org/abs/2506.12627", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "Towards Neural Audio Codec Source Parsing", "comment": null, "summary": "A new class of audio deepfakes-codecfakes (CFs)-has recently caught\nattention, synthesized by Audio Language Models that leverage neural audio\ncodecs (NACs) in the backend. In response, the community has introduced\ndedicated benchmarks and tailored detection strategies. As the field advances,\nefforts have moved beyond binary detection toward source attribution, including\nopen-set attribution, which aims to identify the NAC responsible for generation\nand flag novel, unseen ones during inference. This shift toward source\nattribution improves forensic interpretability and accountability. However,\nopen-set attribution remains fundamentally limited: while it can detect that a\nNAC is unfamiliar, it cannot characterize or identify individual unseen codecs.\nIt treats such inputs as generic ``unknowns'', lacking insight into their\ninternal configuration. This leads to major shortcomings: limited\ngeneralization to new NACs and inability to resolve fine-grained variations\nwithin NAC families. To address these gaps, we propose Neural Audio Codec\nSource Parsing (NACSP) - a paradigm shift that reframes source attribution for\nCFs as structured regression over generative NAC parameters such as quantizers,\nbandwidth, and sampling rate. We formulate NACSP as a multi-task regression\ntask for predicting these NAC parameters and establish the first comprehensive\nbenchmark using various state-of-the-art speech pre-trained models (PTMs). To\nthis end, we propose HYDRA, a novel framework that leverages hyperbolic\ngeometry to disentangle complex latent properties from PTM representations. By\nemploying task-specific attention over multiple curvature-aware hyperbolic\nsubspaces, HYDRA enables superior multi-task generalization. Our extensive\nexperiments show HYDRA achieves top results on benchmark CFs datasets compared\nto baselines operating in Euclidean space."}
{"id": "2506.12222", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12222", "abs": "https://arxiv.org/abs/2506.12222", "authors": ["Tony Alex", "Sara Ahmed", "Armin Mustafa", "Muhammad Awais", "Philip JB Jackson"], "title": "SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes", "comment": "Accepted at ICLR 2025. Code and pre-trained models are available at\n  \\url{https://github.com/ta012/SSLAM}", "summary": "Self-supervised pre-trained audio networks have seen widespread adoption in\nreal-world systems, particularly in multi-modal large language models. These\nnetworks are often employed in a frozen state, under the assumption that the\nSSL pre-training has sufficiently equipped them to handle real-world audio.\nHowever, a critical question remains: how well do these models actually perform\nin real-world conditions, where audio is typically polyphonic and complex,\ninvolving multiple overlapping sound sources? Current audio SSL methods are\noften benchmarked on datasets predominantly featuring monophonic audio, such as\nenvironmental sounds, and speech. As a result, the ability of SSL models to\ngeneralize to polyphonic audio, a common characteristic in natural scenarios,\nremains underexplored. This limitation raises concerns about the practical\nrobustness of SSL models in more realistic audio settings. To address this gap,\nwe introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel\ndirection in audio SSL research, designed to improve, designed to improve the\nmodel's ability to learn from polyphonic data while maintaining strong\nperformance on monophonic data. We thoroughly evaluate SSLAM on standard audio\nSSL benchmark datasets which are predominantly monophonic and conduct a\ncomprehensive comparative analysis against SOTA methods using a range of\nhigh-quality, publicly available polyphonic datasets. SSLAM not only improves\nmodel performance on polyphonic audio, but also maintains or exceeds\nperformance on standard audio SSL benchmarks. Notably, it achieves up to a\n3.9\\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision\n(mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear\nevaluation and fine-tuning regimes with performance improvements of up to 9.1\\%\n(mAP)."}
{"id": "2506.13713", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13713", "abs": "https://arxiv.org/abs/2506.13713", "authors": ["Shunyu Li", "Tianqi Mao", "Guangyao Liu", "Fan Zhang", "Ruiqi Liu", "Meng Hua", "Zhen Gao", "Qingqing Wu", "George K. Karagiannidis"], "title": "Intelligent Metasurface-Enabled Integrated Sensing and Communication: Unified Framework and Key Technologies", "comment": "This work has been submitted to IEEE Wireless Communications for\n  possible publication", "summary": "As the demand for ubiquitous connectivity and high-precision environmental\nawareness grows, integrated sensing and communication (ISAC) has emerged as a\nkey technology for sixth-generation (6G) wireless networks. Intelligent\nmetasurfaces (IMs) have also been widely adopted in ISAC scenarios due to their\nefficient, programmable control over electromagnetic waves. This provides a\nversatile solution that meets the dual-function requirements of next-generation\nnetworks. Although reconfigurable intelligent surfaces (RISs) have been\nextensively studied for manipulating the propagation channel between base and\nmobile stations, the full potential of IMs in ISAC transceiver design remains\nunder-explored. Against this backdrop, this article explores emerging\nIM-enabled transceiver designs for ISAC systems. It begins with an overview of\nrepresentative IM architectures, their unique principles, and their inherent\nadvantages in EM wave manipulation. Next, a unified ISAC framework is\nestablished to systematically model the design and derivation of diverse\nIM-enabled transceiver structures. This lays the foundation for performance\noptimization, trade-offs, and analysis. The paper then discusses several\ncritical technologies for IM-enabled ISAC transceivers, including dedicated\nchannel modeling, effective channel estimation, tailored beamforming\nstrategies, and dual-functional waveform design."}
{"id": "2506.12705", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12705", "abs": "https://arxiv.org/abs/2506.12705", "authors": ["Ahsan J. Cheema", "Sunil Puria"], "title": "Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration", "comment": "Accepted for presentation at INTERSPEECH 2025", "summary": "Trouble hearing in noisy situations remains a common complaint for both\nindividuals with hearing loss and individuals with normal hearing. This is\nhypothesized to arise due to condition called: cochlear neural degeneration\n(CND) which can also result in significant variabilities in hearing aids\noutcomes. This paper uses computational models of auditory periphery to\nsimulate various hearing tasks. We present an objective method to quantify\nhearing loss and CND by comparing auditory nerve fiber responses using a\nNeurogram Similarity Index Measure (NSIM). Specifically study 1, shows that\nNSIM can be used to map performance of individuals with hearing loss on phoneme\nrecognition task with reasonable accuracy. In the study 2, we show that NSIM is\na sensitive measure that can also be used to capture the deficits resulting\nfrom CND and can be a candidate for noninvasive biomarker of auditory\nsynaptopathy."}
{"id": "2506.12260", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12260", "abs": "https://arxiv.org/abs/2506.12260", "authors": ["Wei Wang", "Wangyou Zhang", "Chenda Li", "Jiatong Shi", "Shinji Watanabe", "Yanmin Qian"], "title": "Improving Speech Enhancement with Multi-Metric Supervision from Learned Quality Assessment", "comment": "Submitted to ASRU 2025", "summary": "Speech quality assessment (SQA) aims to predict the perceived quality of\nspeech signals under a wide range of distortions. It is inherently connected to\nspeech enhancement (SE), which seeks to improve speech quality by removing\nunwanted signal components. While SQA models are widely used to evaluate SE\nperformance, their potential to guide SE training remains underexplored. In\nthis work, we investigate a training framework that leverages a SQA model,\ntrained to predict multiple evaluation metrics from a public SE leaderboard, as\na supervisory signal for SE. This approach addresses a key limitation of\nconventional SE objectives, such as SI-SNR, which often fail to align with\nperceptual quality and generalize poorly across evaluation metrics. Moreover,\nit enables training on real-world data where clean references are unavailable.\nExperiments on both simulated and real-world test sets show that SQA-guided\ntraining consistently improves performance across a range of quality metrics."}
{"id": "2506.12665", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12665", "abs": "https://arxiv.org/abs/2506.12665", "authors": ["Valentin Ackva", "Fares Schulz"], "title": "ANIRA: An Architecture for Neural Network Inference in Real-Time Audio Applications", "comment": "8 pages, accepted to the Proceedings of the 5th IEEE International\n  Symposium on the Internet of Sounds (2024) - repository:\n  github.com/anira-project/anira", "summary": "Numerous tools for neural network inference are currently available, yet many\ndo not meet the requirements of real-time audio applications. In response, we\nintroduce anira, an efficient cross-platform library. To ensure compatibility\nwith a broad range of neural network architectures and frameworks, anira\nsupports ONNX Runtime, LibTorch, and TensorFlow Lite as backends. Each\ninference engine exhibits real-time violations, which anira mitigates by\ndecoupling the inference from the audio callback to a static thread pool. The\nlibrary incorporates built-in latency management and extensive benchmarking\ncapabilities, both crucial to ensure a continuous signal flow. Three different\nneural network architectures for audio effect emulation are then subjected to\nbenchmarking across various configurations. Statistical modeling is employed to\nidentify the influence of various factors on performance. The findings indicate\nthat for stateless models, ONNX Runtime exhibits the lowest runtimes. For\nstateful models, LibTorch demonstrates the fastest performance. Our results\nalso indicate that for certain model-engine combinations, the initial\ninferences take longer, particularly when these inferences exhibit a higher\nincidence of real-time violations."}
{"id": "2506.12785", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12785", "abs": "https://arxiv.org/abs/2506.12785", "authors": ["Hyeonuk Nam"], "title": "Frequency Dynamic Convolutions for Sound Event Detection", "comment": "Ph. D. Dissertation in English(KAIST)", "summary": "Recent research in deep learning-based Sound Event Detection (SED) has\nprimarily focused on Convolutional Recurrent Neural Networks (CRNNs) and\nTransformer models. However, conventional 2D convolution-based models assume\nshift invariance along both the temporal and frequency axes, leadin to\ninconsistencies when dealing with frequency-dependent characteristics of\nacoustic signals. To address this issue, this study proposes Frequency Dynamic\nConvolution (FDY conv), which dynamically adjusts convolutional kernels based\non the frequency composition of the input signal to enhance SED performance.\nFDY conv constructs an optimal frequency response by adaptively weighting\nmultiple basis kernels based on frequency-specific attention weights.\nExperimental results show that applying FDY conv to CRNNs improves performance\non the DESED dataset by 7.56% compared to the baseline CRNN. However, FDY conv\nhas limitations in that it combines basis kernels of the same shape across all\nfrequencies, restricting its ability to capture diverse frequency-specific\ncharacteristics. Additionally, the $3\\times3$ basis kernel size is insufficient\nto capture a broader frequency range. To overcome these limitations, this study\nintroduces an extended family of FDY conv models. Dilated FDY conv (DFD conv)\napplies convolutional kernels with various dilation rates to expand the\nreceptive field along the frequency axis and enhance frequency-specific feature\nrepresentation. Experimental results show that DFD conv improves performance by\n9.27% over the baseline. Partial FDY conv (PFD conv) addresses the high\ncomputational cost of FDY conv, which results from performing all convolution\noperations with dynamic kernels. Since FDY conv may introduce unnecessary\nadaptivity for quasi-stationary sound events, PFD conv integrates standard 2D\nconvolutions with frequency-adaptive kernels to reduce computational complexity\nwhile maintaining performance. Experimental results demonstrate that PFD conv\nimproves performance by 7.80% over the baseline while reducing the number of\nparameters by 54.4% compared to FDY conv. Multi-Dilated FDY conv (MDFD conv)\nextends DFD conv by addressing its structural limitation of applying the same\ndilation across all frequencies. By utilizing multiple convolutional kernels\nwith different dilation rates, MDFD conv effectively captures diverse\nfrequency-dependent patterns. Experimental results indicate that MDFD conv\nachieves the highest performance, improving the baseline CRNN performance by\n10.98%. Furthermore, standard FDY conv employs Temporal Average Pooling, which\nassigns equal weight to all frames along the time axis, limiting its ability to\neffectively capture transient events. To overcome this, this study proposes\nTAP-FDY conv (TFD conv), which integrates Temporal Attention Pooling (TA) that\nfocuses on salient features, Velocity Attention Pooling (VA) that emphasizes\ntransient characteristics, and Average Pooling (AP) that captures stationary\nproperties. TAP-FDY conv achieves the same performance as MDFD conv but reduces\nthe number of parameters by approximately 30.01% (12.703M vs. 18.157M),\nachieving equivalent accuracy with lower computational complexity. Class-wise\nperformance analysis reveals that FDY conv improves detection of non-stationary\nevents, DFD conv is particularly effective for events with broad spectral\nfeatures, and PFD conv enhances the detection of quasi-stationary events.\nAdditionally, TFD conv (TFD-CRNN) demonstrates strong performance in detecting\ntransient events. In the case studies, PFD conv effectively captures stable\nsignal patterns in tank powertrain fault recognition, DFD conv recognizes wide\nharmonic spectral patterns on speed-varying motor fault recognition, while TFD\nconv outperforms other models in detecting transient signals in offshore arc\ndetection. These results suggest that frequency-adaptive convolutions and their\nextended variants provide a robust alternative to conventional 2D convolutions\nin deep learning-based audio processing."}
{"id": "2506.12325", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12325", "abs": "https://arxiv.org/abs/2506.12325", "authors": ["Yuntao Shou", "Jun Yao", "Tao Meng", "Wei Ai", "Cen Chen", "Keqin Li"], "title": "GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition", "comment": null, "summary": "Multimodal emotion recognition in conversations (MERC) aims to infer the\nspeaker's emotional state by analyzing utterance information from multiple\nsources (i.e., video, audio, and text). Compared with unimodality, a more\nrobust utterance representation can be obtained by fusing complementary\nsemantic information from different modalities. However, the modality missing\nproblem severely limits the performance of MERC in practical scenarios. Recent\nwork has achieved impressive performance on modality completion using graph\nneural networks and diffusion models, respectively. This inspires us to combine\nthese two dimensions through the graph diffusion model to obtain more powerful\nmodal recovery capabilities. Unfortunately, existing graph diffusion models may\ndestroy the connectivity and local structure of the graph by directly adding\nGaussian noise to the adjacency matrix, resulting in the generated graph data\nbeing unable to retain the semantic and topological information of the original\ngraph. To this end, we propose a novel Graph Spectral Diffusion Network\n(GSDNet), which maps Gaussian noise to the graph spectral space of missing\nmodalities and recovers the missing data according to its original\ndistribution. Compared with previous graph diffusion methods, GSDNet only\naffects the eigenvalues of the adjacency matrix instead of destroying the\nadjacency matrix directly, which can maintain the global topological\ninformation and important spectral features during the diffusion process.\nExtensive experiments have demonstrated that GSDNet achieves state-of-the-art\nemotion recognition performance in various modality loss scenarios."}
{"id": "2506.12705", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12705", "abs": "https://arxiv.org/abs/2506.12705", "authors": ["Ahsan J. Cheema", "Sunil Puria"], "title": "Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration", "comment": "Accepted for presentation at INTERSPEECH 2025", "summary": "Trouble hearing in noisy situations remains a common complaint for both\nindividuals with hearing loss and individuals with normal hearing. This is\nhypothesized to arise due to condition called: cochlear neural degeneration\n(CND) which can also result in significant variabilities in hearing aids\noutcomes. This paper uses computational models of auditory periphery to\nsimulate various hearing tasks. We present an objective method to quantify\nhearing loss and CND by comparing auditory nerve fiber responses using a\nNeurogram Similarity Index Measure (NSIM). Specifically study 1, shows that\nNSIM can be used to map performance of individuals with hearing loss on phoneme\nrecognition task with reasonable accuracy. In the study 2, we show that NSIM is\na sensitive measure that can also be used to capture the deficits resulting\nfrom CND and can be a candidate for noninvasive biomarker of auditory\nsynaptopathy."}
{"id": "2506.12817", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.12817", "abs": "https://arxiv.org/abs/2506.12817", "authors": ["Zhihong Jia", "Hongbin Wang", "Yuanzhong Shen", "Feng Hu", "Jiayu An", "Kai Shu", "Dongrui Wu"], "title": "Magnetoencephalography (MEG) Based Non-Invasive Chinese Speech Decoding", "comment": null, "summary": "As an emerging paradigm of brain-computer interfaces (BCIs), speech BCI has\nthe potential to directly reflect auditory perception and thoughts, offering a\npromising communication alternative for patients with aphasia. Chinese is one\nof the most widely spoken languages in the world, whereas there is very limited\nresearch on speech BCIs for Chinese language. This paper reports a\ntext-magnetoencephalography (MEG) dataset for non-invasive Chinese speech BCIs.\nIt also proposes a multi-modality assisted speech decoding (MASD) algorithm to\ncapture both text and acoustic information embedded in brain signals during\nspeech activities. Experiment results demonstrated the effectiveness of both\nour text-MEG dataset and our proposed MASD algorithm. To our knowledge, this is\nthe first study on modality-assisted decoding for non-invasive speech BCIs."}
{"id": "2506.12440", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.DL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12440", "abs": "https://arxiv.org/abs/2506.12440", "authors": ["Federico Simonetta"], "title": "Style-based Composer Identification and Attribution of Symbolic Music Scores: a Systematic Survey", "comment": "Accepted at the TISMIR", "summary": "This paper presents the first comprehensive systematic review of literature\non style-based composer identification and authorship attribution in symbolic\nmusic scores. Addressing the critical need for improved reliability and\nreproducibility in this field, the review rigorously analyzes 58 peer-reviewed\npapers published across various historical periods, with the search adapted to\nevolving terminology. The analysis critically assesses prevailing repertoires,\ncomputational approaches, and evaluation methodologies, highlighting\nsignificant challenges. It reveals that a substantial portion of existing\nresearch suffers from inadequate validation protocols and an over-reliance on\nsimple accuracy metrics for often imbalanced datasets, which can undermine the\ncredibility of attribution claims. The crucial role of robust metrics like\nBalanced Accuracy and rigorous cross-validation in ensuring trustworthy results\nis emphasized. The survey also details diverse feature representations and the\nevolution of machine learning models employed. Notable real-world authorship\nattribution cases, such as those involving works attributed to Bach, Josquin\nDesprez, and Lennon-McCartney, are specifically discussed, illustrating the\nopportunities and pitfalls of applying computational techniques to resolve\ndisputed musical provenance. Based on these insights, a set of actionable\nguidelines for future research are proposed. These recommendations are designed\nto significantly enhance the reliability, reproducibility, and musicological\nvalidity of composer identification and authorship attribution studies,\nfostering more robust and interpretable computational stylistic analysis."}
{"id": "2506.13272", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.13272", "abs": "https://arxiv.org/abs/2506.13272", "authors": ["Pranav M N", "Gandham Sai Santhosh", "Tejas Joshi", "S Sriniketh Desikan", "Eswar Gupta"], "title": "SONIC: Sound Optimization for Noise In Crowds", "comment": null, "summary": "This paper presents SONIC, an embedded real-time noise suppression system\nimplemented on the ARM Cortex-M7-based STM32H753ZI microcontroller. Using\nadaptive filtering (LMS), the system improves speech intelligibility in noisy\nenvironments. SONIC focuses on a novel approach to noise suppression in audio\nsignals, specifically addressing the limitations of traditional Active Noise\nCancellation (ANC) systems. The paper explores various signal processing\nalgorithms in a micro-controller point of view, highlighting various\nperformance factors and which were considered optimal in our embedded system.\nAdditionally we also discussed the system architecture, explaining how the\nMCU's efficiency was harnessed, along with an in-depth overview of how the\naudio signals were translated within the processor. The results demonstrate\nimproved speech clarity and practical real-time performance, showing low-power\nDSP as an alternative to complex AI denoising methods."}
{"id": "2506.13053", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13053", "abs": "https://arxiv.org/abs/2506.13053", "authors": ["Han Zhu", "Wei Kang", "Zengwei Yao", "Liyong Guo", "Fangjun Kuang", "Zhaoqing Li", "Weiji Zhuang", "Long Lin", "Daniel Povey"], "title": "ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching", "comment": null, "summary": "Existing large-scale zero-shot text-to-speech (TTS) models deliver high\nspeech quality but suffer from slow inference speeds due to massive parameters.\nTo address this issue, this paper introduces ZipVoice, a high-quality\nflow-matching-based zero-shot TTS model with a compact model size and fast\ninference speed. Key designs include: 1) a Zipformer-based flow-matching\ndecoder to maintain adequate modeling capabilities under constrained size; 2)\nAverage upsampling-based initial speech-text alignment and Zipformer-based text\nencoder to improve speech intelligibility; 3) A flow distillation method to\nreduce sampling steps and eliminate the inference overhead associated with\nclassifier-free guidance. Experiments on 100k hours multilingual datasets show\nthat ZipVoice matches state-of-the-art models in speech quality, while being 3\ntimes smaller and up to 30 times faster than a DiT-based flow-matching\nbaseline. Codes, model checkpoints and demo samples are publicly available."}
{"id": "2506.12570", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12570", "abs": "https://arxiv.org/abs/2506.12570", "authors": ["Hui Wang", "Yifan Yang", "Shujie Liu", "Jinyu Li", "Lingwei Meng", "Yanqing Liu", "Jiaming Zhou", "Haoqin Sun", "Yan Lu", "Yong Qin"], "title": "StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling", "comment": null, "summary": "Recent advances in zero-shot text-to-speech (TTS) synthesis have achieved\nhigh-quality speech generation for unseen speakers, but most systems remain\nunsuitable for real-time applications because of their offline design. Current\nstreaming TTS paradigms often rely on multi-stage pipelines and discrete\nrepresentations, leading to increased computational cost and suboptimal system\nperformance. In this work, we propose StreamMel, a pioneering single-stage\nstreaming TTS framework that models continuous mel-spectrograms. By\ninterleaving text tokens with acoustic frames, StreamMel enables low-latency,\nautoregressive synthesis while preserving high speaker similarity and\nnaturalness. Experiments on LibriSpeech demonstrate that StreamMel outperforms\nexisting streaming TTS baselines in both quality and latency. It even achieves\nperformance comparable to offline systems while supporting efficient real-time\ngeneration, showcasing broad prospects for integration with real-time speech\nlarge language models. Audio samples are available at:\nhttps://aka.ms/StreamMel."}
{"id": "2506.13295", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13295", "abs": "https://arxiv.org/abs/2506.13295", "authors": ["Taewoo Kim", "Uijong Lee", "Hayoung Park", "Choongsang Cho", "Nam In Park", "Young Han Lee"], "title": "Instance-Specific Test-Time Training for Speech Editing in the Wild", "comment": "Submitted to IEEE Signal Processing Letters", "summary": "Speech editing systems aim to naturally modify speech content while\npreserving acoustic consistency and speaker identity. However, previous studies\noften struggle to adapt to unseen and diverse acoustic conditions, resulting in\ndegraded editing performance in real-world scenarios. To address this, we\npropose an instance-specific test-time training method for speech editing in\nthe wild. Our approach employs direct supervision from ground-truth acoustic\nfeatures in unedited regions, and indirect supervision in edited regions via\nauxiliary losses based on duration constraints and phoneme prediction. This\nstrategy mitigates the bandwidth discontinuity problem in speech editing,\nensuring smooth acoustic transitions between unedited and edited regions.\nAdditionally, it enables precise control over speech rate by adapting the model\nto target durations via mask length adjustment during test-time training.\nExperiments on in-the-wild benchmark datasets demonstrate that our method\noutperforms existing speech editing systems in both objective and subjective\nevaluations."}
{"id": "2506.12573", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12573", "abs": "https://arxiv.org/abs/2506.12573", "authors": ["Haven Kim", "Zachary Novack", "Weihan Xu", "Julian McAuley", "Hao-Wen Dong"], "title": "Video-Guided Text-to-Music Generation Using Public Domain Movie Collections", "comment": "ISMIR 2025 regular paper. Dataset and code available at\n  https://havenpersona.github.io/ossl-v1", "summary": "Despite recent advancements in music generation systems, their application in\nfilm production remains limited, as they struggle to capture the nuances of\nreal-world filmmaking, where filmmakers consider multiple factors-such as\nvisual content, dialogue, and emotional tone-when selecting or composing music\nfor a scene. This limitation primarily stems from the absence of comprehensive\ndatasets that integrate these elements. To address this gap, we introduce Open\nScreen Sound Library (OSSL), a dataset consisting of movie clips from public\ndomain films, totaling approximately 36.5 hours, paired with high-quality\nsoundtracks and human-annotated mood information. To demonstrate the\neffectiveness of our dataset in improving the performance of pre-trained models\non film music generation tasks, we introduce a new video adapter that enhances\nan autoregressive transformer-based text-to-music model by adding video-based\nconditioning. Our experimental results demonstrate that our proposed approach\neffectively enhances MusicGen-Medium in terms of both objective measures of\ndistributional and paired fidelity, and subjective compatibility in mood and\ngenre. The dataset and code are available at\nhttps://havenpersona.github.io/ossl-v1."}
{"id": "2506.13455", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13455", "abs": "https://arxiv.org/abs/2506.13455", "authors": ["Wenmiao Gao", "Yang Xiao"], "title": "Stereo sound event localization and detection based on PSELDnet pretraining and BiMamba sequence modeling", "comment": "Technical report for DCASE 2025 Challenge Task 3", "summary": "Pre-training methods have achieved significant performance improvements in\nsound event localization and detection (SELD) tasks, but existing\nTransformer-based models suffer from high computational complexity. In this\nwork, we propose a stereo sound event localization and detection system based\non pre-trained PSELDnet and bidirectional Mamba sequence modeling. We replace\nthe Conformer module with a BiMamba module and introduce asymmetric\nconvolutions to more effectively model the spatiotemporal relationships between\ntime and frequency dimensions. Experimental results demonstrate that the\nproposed method achieves significantly better performance than the baseline and\nthe original PSELDnet with Conformer decoder architecture on the DCASE2025 Task\n3 development dataset, while also reducing computational complexity. These\nfindings highlight the effectiveness of the BiMamba architecture in addressing\nthe challenges of the SELD task."}
{"id": "2506.12665", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.12665", "abs": "https://arxiv.org/abs/2506.12665", "authors": ["Valentin Ackva", "Fares Schulz"], "title": "ANIRA: An Architecture for Neural Network Inference in Real-Time Audio Applications", "comment": "8 pages, accepted to the Proceedings of the 5th IEEE International\n  Symposium on the Internet of Sounds (2024) - repository:\n  github.com/anira-project/anira", "summary": "Numerous tools for neural network inference are currently available, yet many\ndo not meet the requirements of real-time audio applications. In response, we\nintroduce anira, an efficient cross-platform library. To ensure compatibility\nwith a broad range of neural network architectures and frameworks, anira\nsupports ONNX Runtime, LibTorch, and TensorFlow Lite as backends. Each\ninference engine exhibits real-time violations, which anira mitigates by\ndecoupling the inference from the audio callback to a static thread pool. The\nlibrary incorporates built-in latency management and extensive benchmarking\ncapabilities, both crucial to ensure a continuous signal flow. Three different\nneural network architectures for audio effect emulation are then subjected to\nbenchmarking across various configurations. Statistical modeling is employed to\nidentify the influence of various factors on performance. The findings indicate\nthat for stateless models, ONNX Runtime exhibits the lowest runtimes. For\nstateful models, LibTorch demonstrates the fastest performance. Our results\nalso indicate that for certain model-engine combinations, the initial\ninferences take longer, particularly when these inferences exhibit a higher\nincidence of real-time violations."}
{"id": "2506.13709", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.13709", "abs": "https://arxiv.org/abs/2506.13709", "authors": ["Sirui Li", "Shuai Wang", "Zhijun Liu", "Zhongjie Jiang", "Yannan Wang", "Haizhou Li"], "title": "SpeechRefiner: Towards Perceptual Quality Refinement for Front-End Algorithms", "comment": "Accepted by Interspeech 2025", "summary": "Speech pre-processing techniques such as denoising, de-reverberation, and\nseparation, are commonly employed as front-ends for various downstream speech\nprocessing tasks. However, these methods can sometimes be inadequate, resulting\nin residual noise or the introduction of new artifacts. Such deficiencies are\ntypically not captured by metrics like SI-SNR but are noticeable to human\nlisteners. To address this, we introduce SpeechRefiner, a post-processing tool\nthat utilizes Conditional Flow Matching (CFM) to improve the perceptual quality\nof speech. In this study, we benchmark SpeechRefiner against recent\ntask-specific refinement methods and evaluate its performance within our\ninternal processing pipeline, which integrates multiple front-end algorithms.\nExperiments show that SpeechRefiner exhibits strong generalization across\ndiverse impairment sources, significantly enhancing speech perceptual quality.\nAudio demos can be found at https://speechrefiner.github.io/SpeechRefiner/."}
{"id": "2506.12672", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.12672", "abs": "https://arxiv.org/abs/2506.12672", "authors": ["Yuta Hirano", "Sakriani Sakti"], "title": "SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition", "comment": "Accepted by Interspeech 2025", "summary": "We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an\nenhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT\nhandles overlapped speech, and we found the decoder performs implicit speaker\nseparation. We hypothesize this implicit separation is often insufficient due\nto ambiguous acoustic cues in overlapping regions. To address this, SC-SOT\nexplicitly conditions the decoder on speaker information, providing detailed\ninformation about \"who spoke when\". Specifically, we enhance the decoder by\nincorporating: (1) speaker embeddings, which allow the model to focus on the\nacoustic characteristics of the target speaker, and (2) speaker activity\ninformation, which guides the model to suppress non-target speakers. The\nspeaker embeddings are derived from a jointly trained E2E speaker diarization\nmodel, mitigating the need for speaker enrollment. Experimental results\ndemonstrate the effectiveness of our conditioning approach on overlapped\nspeech."}
{"id": "2506.13001", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "I.2.1; I.2.6; H.5.5; J.5"], "pdf": "https://arxiv.org/pdf/2506.13001", "abs": "https://arxiv.org/abs/2506.13001", "authors": ["Christian Zhou-Zheng", "Philippe Pasquier"], "title": "Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV", "comment": null, "summary": "Existing work in automatic music generation has primarily focused on\nend-to-end systems that produce complete compositions or continuations.\nHowever, because musical composition is typically an iterative process, such\nsystems make it difficult to engage in the back-and-forth between human and\nmachine that is essential to computer-assisted creativity. In this study, we\naddress the task of personalizable, multi-track, long-context, and controllable\nsymbolic music infilling to enhance the process of computer-assisted\ncomposition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear\narchitecture, to enable efficient and coherent musical cocreation on edge\ndevices. We also demonstrate that MIDI-RWKV admits an effective method of\nfinetuning its initial state for personalization in the very-low-sample regime.\nWe evaluate MIDI-RWKV and its state tuning on several quantitative and\nqualitative metrics, and release model weights and code at\nhttps://github.com/christianazinn/MIDI-RWKV."}
{"id": "2506.13127", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13127", "abs": "https://arxiv.org/abs/2506.13127", "authors": ["Jiaming Cheng", "Ruiyu Liang", "Chao Xu", "Ye Ni", "Wei Zhou", "Björn W. Schuller", "Xiaoshuai Hao"], "title": "I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency Calibration for Speech Enhancement", "comment": "submitted to IEEE Transactions on Neural Networks and Learning\n  Systems", "summary": "In recent years, complexity compression of neural network (NN)-based speech\nenhancement (SE) models has gradually attracted the attention of researchers,\nespecially in scenarios with limited hardware resources or strict latency\nrequirements. The main difficulties and challenges lie in achieving a balance\nbetween complexity and performance according to the characteristics of the\ntask. In this paper, we propose an intra-inter set knowledge distillation (KD)\nframework with time-frequency calibration (I$^2$S-TFCKD) for SE. Different from\nprevious distillation strategies for SE, the proposed framework fully utilizes\nthe time-frequency differential information of speech while promoting global\nknowledge flow. Firstly, we propose a multi-layer interactive distillation\nbased on dual-stream time-frequency cross-calibration, which calculates the\nteacher-student similarity calibration weights in the time and frequency\ndomains respectively and performs cross-weighting, thus enabling refined\nallocation of distillation contributions across different layers according to\nspeech characteristics. Secondly, we construct a collaborative distillation\nparadigm for intra-set and inter-set correlations. Within a correlated set,\nmulti-layer teacher-student features are pairwise matched for calibrated\ndistillation. Subsequently, we generate representative features from each\ncorrelated set through residual fusion to form the fused feature set that\nenables inter-set knowledge interaction. The proposed distillation strategy is\napplied to the dual-path dilated convolutional recurrent network (DPDCRN) that\nranked first in the SE track of the L3DAS23 challenge. Objective evaluations\ndemonstrate that the proposed KD strategy consistently and effectively improves\nthe performance of the low-complexity student model and outperforms other\ndistillation schemes."}
{"id": "2506.13595", "categories": ["cs.SD", "cs.CG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13595", "abs": "https://arxiv.org/abs/2506.13595", "authors": ["Eunwoo Heo", "Byeongchan Choi", "Myung ock Kim", "Mai Lan Tran", "Jae-Hun Jung"], "title": "Persistent Homology of Music Network with Three Different Distances", "comment": null, "summary": "Persistent homology has been widely used to discover hidden topological\nstructures in data across various applications, including music data. To apply\npersistent homology, a distance or metric must be defined between points in a\npoint cloud or between nodes in a graph network. These definitions are not\nunique and depend on the specific objectives of a given problem. In other\nwords, selecting different metric definitions allows for multiple topological\ninferences. In this work, we focus on applying persistent homology to music\ngraph with predefined weights. We examine three distinct distance definitions\nbased on edge-wise pathways and demonstrate how these definitions affect\npersistent barcodes, persistence diagrams, and birth/death edges. We found that\nthere exist inclusion relations in one-dimensional persistent homology\nreflected on persistence barcode and diagram among these three distance\ndefinitions. We verified these findings using real music data."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [eess.AS](#eess.AS) [Total: 15]
- [cs.SD](#cs.SD) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Tutorial-cum-Survey on Self-Supervised Learning for Wi-Fi Sensing: Trends, Challenges, and Outlook](https://arxiv.org/abs/2506.12052)
*Ahmed Y. Radwan,Mustafa Yildirim,Navid Hasanzadeh,Hina Tabassum,Shahrokh Valaee*

Main category: eess.SP

TL;DR: Wi-Fi sensing利用现有Wi-Fi基础设施提取和分析信道状态信息（CSI），用于多种应用，如活动识别和健康监测。本文综述了Wi-Fi标准化、CSI测量方法、数据集比较、预处理技术及机器学习方法，并探讨了自监督学习的潜力。


<details>
  <summary>Details</summary>
Motivation: 利用Wi-Fi基础设施实现隐私保护、非侵入式且成本低廉的感知技术，弥补摄像头在光照条件下的不足。

Method: 综述Wi-Fi标准化和CSI测量方法，比较现有数据集，讨论预处理技术，分析机器学习（尤其是自监督学习）在Wi-Fi感知中的应用。

Result: 展示了自监督学习（如对比学习）在分类准确性上的优势，并指出未来技术可进一步提升Wi-Fi感知性能。

Conclusion: Wi-Fi感知技术潜力巨大，未来可通过新兴技术进一步优化，并存在广阔的研究空间。

Abstract: Wi-Fi technology has evolved from simple communication routers to sensing
devices. Wi-Fi sensing leverages conventional Wi-Fi transmissions to extract
and analyze channel state information (CSI) for applications like proximity
detection, occupancy detection, activity recognition, and health monitoring. By
leveraging existing infrastructure, Wi-Fi sensing offers a privacy-preserving,
non-intrusive, and cost-effective solution which, unlike cameras, is not
sensitive to lighting conditions. Beginning with a comprehensive review of the
Wi-Fi standardization activities, this tutorial-cum-survey first introduces
fundamental concepts related to Wi-Fi CSI, outlines the CSI measurement
methods, and examines the impact of mobile objects on CSI. The mechanics of a
simplified testbed for CSI extraction are also described. Then, we present a
qualitative comparison of the existing Wi-Fi sensing datasets, their
specifications, and pin-point their shortcomings. Next, a variety of
preprocessing techniques are discussed that are beneficial for feature
extraction and explainability of machine learning (ML) algorithms. We then
provide a qualitative review of recent ML approaches in the domain of Wi-Fi
sensing and present the significance of self-supervised learning (SSL) in that
context. Specifically, the mechanics of contrastive and non-contrastive
learning solutions is elaborated in detail and a quantitative comparative
analysis is presented in terms of classification accuracy. Finally, the article
concludes by highlighting emerging technologies that can be leveraged to
enhance the performance of Wi-Fi sensing and opportunities for further research
in this domain

</details>


### [2] [TCN-DPD: Parameter-Efficient Temporal Convolutional Networks for Wideband Digital Predistortion](https://arxiv.org/abs/2506.12165)
*Huanqiang Duan,Manno Versluis,Qinyu Chen,Leo C. N. de Vreede,Chang Gao*

Main category: eess.SP

TL;DR: TCN-DPD是一种基于时间卷积网络的高效数字预失真架构，用于减轻射频功率放大器的非线性问题，尤其在宽带应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 射频功率放大器的非线性问题在宽带应用中尤为突出，需要高效的预失真技术来改善性能。

Method: 提出TCN-DPD架构，结合非因果扩张卷积和优化的激活函数，参数效率高。

Result: 在OpenDPD框架和DPA_200MHz数据集上，TCN-DPD实现了优异的线性化性能（ACPRs -51.58/-49.26 dBc，EVM -47.52 dB，NMSE -44.61 dB），且参数效率高（低至200参数）。

Conclusion: TCN-DPD在宽带功率放大器线性化中表现出色，具有高效性和参数效率的优势。

Abstract: Digital predistortion (DPD) is essential for mitigating nonlinearity in RF
power amplifiers, particularly for wideband applications. This paper presents
TCN-DPD, a parameter-efficient architecture based on temporal convolutional
networks, integrating noncausal dilated convolutions with optimized activation
functions. Evaluated on the OpenDPD framework with the DPA_200MHz dataset,
TCN-DPD achieves simulated ACPRs of -51.58/-49.26 dBc (L/R), EVM of -47.52 dB,
and NMSE of -44.61 dB with 500 parameters and maintains superior linearization
than prior models down to 200 parameters, making it promising for efficient
wideband PA linearization.

</details>


### [3] [Directed Acyclic Graph Convolutional Networks](https://arxiv.org/abs/2506.12218)
*Samuel Rey,Hamed Ajorlou,Gonzalo Mateos*

Main category: eess.SP

TL;DR: 论文提出了一种针对有向无环图（DAG）的卷积网络（DCN）及其并行版本（PDCN），通过因果图滤波器和并行处理提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNN）未考虑DAG的偏序特性，DCN旨在填补这一空白，提供更高效的DAG信号处理框架。

Method: DCN利用因果图滤波器学习节点表示，PDCN通过并行处理解耦模型复杂度与图大小。

Result: 实验表明，(P)DCN在准确性、鲁棒性和计算效率上优于现有方法。

Conclusion: (P)DCN为基于DAG的深度学习提供了高效且理论支持的框架。

Abstract: Directed acyclic graphs (DAGs) are central to science and engineering
applications including causal inference, scheduling, and neural architecture
search. In this work, we introduce the DAG Convolutional Network (DCN), a novel
graph neural network (GNN) architecture designed specifically for convolutional
learning from signals supported on DAGs. The DCN leverages causal graph filters
to learn nodal representations that account for the partial ordering inherent
to DAGs, a strong inductive bias does not present in conventional GNNs. Unlike
prior art in machine learning over DAGs, DCN builds on formal convolutional
operations that admit spectral-domain representations. We further propose the
Parallel DCN (PDCN), a model that feeds input DAG signals to a parallel bank of
causal graph-shift operators and processes these DAG-aware features using a
shared multilayer perceptron. This way, PDCN decouples model complexity from
graph size while maintaining satisfactory predictive performance. The
architectures' permutation equivariance and expressive power properties are
also established. Comprehensive numerical tests across several tasks, datasets,
and experimental conditions demonstrate that (P)DCN compares favorably with
state-of-the-art baselines in terms of accuracy, robustness, and computational
efficiency. These results position (P)DCN as a viable framework for deep
learning from DAG-structured data that is designed from first (graph) signal
processing principles.

</details>


### [4] [From Ground to Sky: Architectures, Applications, and Challenges Shaping Low-Altitude Wireless Networks](https://arxiv.org/abs/2506.12308)
*Weijie Yuan,Yuanhao Cui,Jiacheng Wang,Fan Liu,Geng Sun,Tao Xiang,Jie Xu,Shi Jin,Dusit Niyato,Sinem Coleri,Sumei Sun,Shiwen Mao,Abbas Jamalipour,Dong In Kim,Mohamed-Slim Alouini,Xuemin Shen*

Main category: eess.SP

TL;DR: 本文提出了一种新型低空无线网络（LAWN），采用可重构的三维分层架构，整合了连接、感知、控制和计算功能，适用于复杂动态的关键任务环境。


<details>
  <summary>Details</summary>
Motivation: 传统空中通信系统功能单一，无法满足低空复杂环境的需求，因此需要一种能够动态整合多种功能的新型网络架构。

Method: LAWN通过集成感知与通信（ISAC）、语义通信和全驱动控制系统等技术，实现功能平面的紧密整合。

Result: LAWN能够安全高效地在低空环境中运行，并支持多种潜在应用。

Conclusion: 本文为低空空域的未来研究和开发提供了全面的路线图，并指出了跨层挑战。

Abstract: In this article, we introduce a novel low-altitude wireless network (LAWN),
which is a reconfigurable, three-dimensional (3D) layered architecture. In
particular, the LAWN integrates connectivity, sensing, control, and computing
across aerial and terrestrial nodes that enable seamless operation in complex,
dynamic, and mission-critical environments. In this article, we introduce a
novel low-altitude wireless network (LAWN), which is a reconfigurable,
three-dimensional (3D) layered architecture. Different from the conventional
aerial communication systems, LAWN's distinctive feature is its tight
integration of functional planes in which multiple functionalities continually
reshape themselves to operate safely and efficiently in the low-altitude sky.
With the LAWN, we discuss several enabling technologies, such as integrated
sensing and communication (ISAC), semantic communication, and fully-actuated
control systems. Finally, we identify potential applications and key
cross-layer challenges. This article offers a comprehensive roadmap for future
research and development in the low-altitude airspace.

</details>


### [5] [A Smooshed BMOCZ Zero Constellation for CFO Estimation Without Channel Coding](https://arxiv.org/abs/2506.12599)
*Anthony Joseph Perre,Parker Huggins,Alphan Sahin*

Main category: eess.SP

TL;DR: 提出了一种新的零星座调制方法SBMOCZ，用于解决载波频率偏移（CFO）引起的零旋转问题，不依赖信道编码。通过调整相位映射引入零星座间隙，接收端通过识别间隙位置估计并校正相位旋转。


<details>
  <summary>Details</summary>
Motivation: 解决CFO引起的零旋转问题，同时避免依赖复杂的信道编码。

Method: 修改Huffman BMOCZ的相位映射，缩小相邻零点间的角度（除首尾外），引入零星座间隙。接收端通过间隙位置估计和校正相位旋转。

Result: SBMOCZ在存在CFO时表现优于Huffman BMOCZ，但在无CFO时性能略有下降。与CPC结合的Huffman BMOCZ相比，SBMOCZ在衰落信道中BER提升4 dB。

Conclusion: SBMOCZ有效解决了CFO引起的零旋转问题，性能在特定场景下优于现有方法，但需权衡无CFO时的性能损失。

Abstract: In this study, we propose a new binary modulation on conjugate-reciprocal
zeros (BMOCZ) zero constellation, which we call smooshed binary modulation on
conjugate-reciprocal zeros (SBMOCZ), to address carrier frequency offset
(CFO)-induced zero rotation without depending on channel coding. In our
approach, we modify the phase mapping of Huffman BMOCZ by shrinking the angle
between adjacent zeros, except for the first and last, to introduce a gap in
the zero constellation. By discerning the gap location in the received
polynomial, the receiver can estimate and correct the phase rotation. We
demonstrate the error rate performance of SBMOCZ relative to Huffman BMOCZ,
showing that SBMOCZ addresses a CFO-induced rotation at the cost of a modest
performance reduction compared to Huffman BMOCZ in the absence of a CFO.
Finally, we compare SBMOCZ to Huffman BMOCZ using a cyclically permutable code
(CPC), showing a 4 dB bit error rate (BER) improvement in a fading channel,
while demonstrating comparable performance across other simulations.

</details>


### [6] [Semi-Blind Channel Estimation for Downlink Communications Based on Dynamic Metasurface Antennas](https://arxiv.org/abs/2506.12639)
*Amarilton L. Magalhães,André L. F. de Almeida,A. Lee Swindlehurst*

Main category: eess.SP

TL;DR: 提出了一种基于动态超表面天线（DMA）的MISO-OFDM系统下行链路信道估计方案，通过PARAFAC分解迭代算法分离无线信道和波导传播向量的估计。


<details>
  <summary>Details</summary>
Motivation: 动态超表面天线（DMA）技术为高效能大规模多天线系统提供了潜力，但现有信道估计方法未充分利用其特性。

Method: 采用基于PARAFAC分解的迭代算法，分离估计无线信道和未知波导传播向量，并联合估计有用数据符号。

Result: 数值结果表明，所提方案性能显著，能够独立补偿波导传播效应。

Conclusion: 该方案首次探索了基于DMA的信道估计方法，为高效多天线系统设计提供了新思路。

Abstract: Dynamic metasurface antennas (DMAs) are emerging as a promising technology to
enable energy-efficient, large array-based multi-antenna systems. This paper
presents a simple channel estimation scheme for the downlink of a
multiple-input single-output orthogonal frequency division multiplexing
(MISO-OFDM) communication system exploiting DMAs. The proposed scheme extracts
separate estimates of the wireless channel and the unknown waveguide
propagation vector using a simple iterative algorithm based on the parallel
factor (PARAFAC) decomposition. Obtaining decoupled estimates of the wireless
channel and inner waveguide vector enables the isolation and compensation for
its effect when designing the DMA beamformer, regardless of the wireless
channel state, which evolves much faster due to its shorter coherence time and
bandwidth. Additionally, our solution operates in a data-aided manner,
delivering estimates of useful data symbols jointly with channel estimates,
without requiring sequential pilot and data stages. To the best of our
knowledge, this is the first work to explore this CE approach. Numerical
results corroborate the notable performance of the proposed scheme.

</details>


### [7] [Conditional Diffusion Model-Driven Generative Channels for Double RIS-Aided Wireless Systems](https://arxiv.org/abs/2506.12682)
*Yiyang Ni,Qi Zhang,Guangji Chen,Yan Cai,Jun Li,Shi Jin*

Main category: eess.SP

TL;DR: 本文提出了一种基于条件扩散模型（CDM）的双RIS通信系统信道生成方法，解决了传统CSI获取技术的高开销和多径干扰问题。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，双RIS系统的信道状态信息获取面临高开销和多径干扰等挑战，需要新的解决方案。

Method: 使用条件扩散模型（CDM）在合成信道数据上进行训练，以捕捉信道特性，并详细分析了扩散过程。

Result: 仿真结果表明，基于CDM的方法在归一化均方误差（NMSE）上优于传统方法。

Conclusion: 该方法为双RIS系统信道获取提供了新范式，有望以低开销提升信道获取质量。

Abstract: With the development of the upcoming sixth-generation networks (6G),
reconfigurable intelligent surfaces (RISs) have gained significant attention
due to its ability of reconfiguring wireless channels via smart reflections.
However, traditional channel state information (CSI) acquisition techniques for
double-RIS systems face challenges (e.g., high pilot overhead or multipath
interference). This paper proposes a new channel generation method in
double-RIS communication systems based on the tool of conditional diffusion
model (CDM). The CDM is trained on synthetic channel data to capture channel
characteristics. It addresses the limitations of traditional CSI generation
methods, such as insufficient model understanding capability and poor
environmental adaptability. We provide a detailed analysis of the diffusion
process for channel generation, and it is validated through simulations. The
simulation results demonstrate that the proposed CDM based method outperforms
traditional channel acquisition methods in terms of normalized mean squared
error (NMSE). This method offers a new paradigm for channel acquisition in
double-RIS systems, which is expected to improve the quality of channel
acquisition with low pilot overhead.

</details>


### [8] [Dynamic Scheduling for Enhanced Performance in RIS-assisted Cooperative Network with Interference](https://arxiv.org/abs/2506.12778)
*Yomali Lokugama,Saman Atapattu,Nathan Ross,Sithamparanathan Kandeepan,Chintha Tellambura*

Main category: eess.SP

TL;DR: 论文研究了RIS与FlexD通信的结合，通过动态调度和用户配对选择提升系统吞吐量，并在干扰网络中展示了FlexD的优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究RIS与FlexD通信的集成，以解决多用户无线网络中外部干扰问题，提升系统性能。

Method: 提出基于RIS可重构性和动态调度的用户配对选择方案，建立数学模型分析吞吐量中断概率，并进行渐近分析。

Result: FlexD在吞吐量、能效和数据管理方面优于传统FD和HD系统。

Conclusion: RIS辅助的FlexD在干扰网络中具有显著优势，适用于蜂窝和车联网等未来无线应用。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as transformative
technologies, enhancing spectral efficiency and improving interference
management in multi-user cooperative communications. This paper investigates
the integration of RIS with Flexible-Duplex (FlexD) communication, featuring
dynamic scheduling capabilities, to mitigate unintended external interference
in multi-user wireless networks. By leveraging the reconfigurability of RIS and
dynamic scheduling, we propose a user-pair selection scheme to maximize system
throughput when full channel state information (CSI) of interference is
unavailable. We develop a mathematical framework to evaluate the throughput
outage probability when RIS introduces spatial correlation. The derived
analytical results are used for asymptotic analysis, providing insights into
dynamic user scheduling under interference based on statistical channel
knowledge. Finally, we compare FlexD with traditional Full Duplex (FD) and Half
Duplex (HD) systems against RIS-assisted FlexD. Our results show FlexD's
superior throughput enhancement, energy efficiency and data management
capability in interference-affected networks, typical in current and
next-generation cooperative wireless applications like cellular and vehicular
communications.

</details>


### [9] [Synesthesia of Machines (SoM)-Enhanced Sub-THz ISAC Transmission for Air-Ground Network](https://arxiv.org/abs/2506.12831)
*Zonghui Yang,Shijian Gao,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: 提出了一种基于机器联觉的多模态感知融合框架，用于优化亚太赫兹频段的集成传感与通信（ISAC）性能，降低延迟。


<details>
  <summary>Details</summary>
Motivation: 亚太赫兹频段的独特传播特性和硬件限制对优化ISAC性能提出了挑战，同时增加了操作延迟。

Method: 通过利用亚太赫兹硬件和信道的自由度，开发了斜视感知波束管理，结合多模态信息（如视觉数据）优化混合预编码器。

Result: 实验表明，该方案显著提高了ISAC效率，并实现了三维动态ISAC链路。

Conclusion: 提出的框架通过多模态感知融合和斜视感知波束管理，有效提升了亚太赫兹ISAC的性能和适应性。

Abstract: Integrated sensing and communication (ISAC) within sub-THz frequencies is
crucial for future air-ground networks, but unique propagation characteristics
and hardware limitations present challenges in optimizing ISAC performance
while increasing operational latency. This paper introduces a multi-modal
sensing fusion framework inspired by synesthesia of machine (SoM) to enhance
sub-THz ISAC transmission. By exploiting inherent degrees of freedom in sub-THz
hardware and channels, the framework optimizes the radio-frequency environment.
Squint-aware beam management is developed to improve air-ground network
adaptability, enabling three-dimensional dynamic ISAC links. Leveraging
multi-modal information, the framework enhances ISAC performance and reduces
latency. Visual data rapidly localizes users and targets, while a customized
multi-modal learning algorithm optimizes the hybrid precoder. A new metric
provides comprehensive performance evaluation, and extensive experiments
demonstrate that the proposed scheme significantly improves ISAC efficiency.

</details>


### [10] [Low-Latency Terrestrial Interference Detection for Satellite-to-Device Communications](https://arxiv.org/abs/2506.12908)
*Runnan Liu,Weifeng Zhu,Shu Sun,Wenjun Zhang*

Main category: eess.SP

TL;DR: 该论文研究了卫星到设备通信中的在线干扰检测框架，针对突发模式传输的空闲间隔，提出了基于CUSUM和GLR的检测方法，显著提高了检测准确性和低延迟。


<details>
  <summary>Details</summary>
Motivation: 由于地面干扰的不连续性和不可预测性影响系统可靠性，而持续使用复杂干扰抑制技术效率低下，因此研究针对卫星到设备通信的在线干扰检测方法。

Method: 将干扰检测建模为二元假设检验问题，利用Rayleigh和Rice分布的差异；提出基于CUSUM的在线检测器（已知干扰方向）和基于GLR的检测方法（未知干扰方向），后者结合Root-MUSIC算法估计干扰方向。

Result: 数值结果验证了理论分析，显示所提方法在检测准确性和低延迟方面表现优异。

Conclusion: 所提出的方法在未来卫星到设备通信系统中具有实际应用潜力，能够高效检测干扰并降低延迟。

Abstract: Direct satellite-to-device communication is a promising future direction due
to its lower latency and enhanced efficiency. However, intermittent and
unpredictable terrestrial interference significantly affects system reliability
and performance. Continuously employing sophisticated interference mitigation
techniques is practically inefficient. Motivated by the periodic idle intervals
characteristic of burst-mode satellite transmissions, this paper investigates
online interference detection frameworks specifically tailored for
satellite-to-device scenarios. We first rigorously formulate interference
detection as a binary hypothesis testing problem, leveraging differences
between Rayleigh (no interference) and Rice (interference present)
distributions. Then, we propose a cumulative sum (CUSUM)-based online detector
for scenarios with known interference directions, explicitly characterizing the
trade-off between detection latency and false alarm rate, and establish its
asymptotic optimality. For practical scenarios involving unknown interference
direction, we further propose a generalized likelihood ratio (GLR)-based
detection method, jointly estimating interference direction via the Root-MUSIC
algorithm. Numerical results validate our theoretical findings and demonstrate
that our proposed methods achieve high detection accuracy with remarkably low
latency, highlighting their practical applicability in future
satellite-to-device communication systems.

</details>


### [11] [Interference Mitigation in STAR-RIS-Aided Multi-User Networks with Statistical CSI](https://arxiv.org/abs/2506.12964)
*Abuzar B. M. Adam,Mohammed A. M. Elhassan,Elhadj Moustapha Diallo,Mohamed Amine Ouamri*

Main category: eess.SP

TL;DR: 论文研究了在多用户无线网络中利用STAR-RIS实现实时干扰抑制的问题，提出了一种基于统计CSI和考虑相位误差的优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖瞬时CSI，但在实际场景中仅统计CSI可用且存在相位误差，因此需要更实用的解决方案。

Method: 通过统计期望推导有效信道矩阵的闭式近似，将干扰最小化问题转化为黎曼流形上的无约束优化，并提出针对复数圆流形的共轭梯度算法。

Result: 仿真结果表明，该方法显著抑制用户间干扰，在SINR性能和收敛速度上优于传统基线。

Conclusion: 所提方法在硬件不完美和有限CSI条件下实现了高效的实时相位偏移计算，具有实际应用价值。

Abstract: In this paper, we investigate real-time interference mitigation in multiuser
wireless networks assisted by simultaneously transmitting and reflecting
reconfigurable intelligent surfaces (STAR-RISs). Unlike conventional methods
that rely on instantaneous channel state information (CSI), we consider a
practical scenario where only statistical CSI is available, and the STAR-RIS
phase shifts are impaired by random phase errors modeled via the Von Mises
distribution. To tackle the resulting nonconvex optimization problem induced by
unit-modulus constraints and stochastic interference, we derive a closed-form
approximation of the effective channel matrix using statistical expectations.
We then reformulate the interference minimization problem as an unconstrained
optimization over a Riemannian manifold and propose a conjugate gradient
algorithm tailored to the complex circle manifold. The proposed solution
enables efficient real-time computation of optimal phase shifts while
accounting for hardware imperfections and limited CSI. Simulation results
confirm that our method significantly suppresses inter-user interference and
achieves superior SINR performance and convergence speed compared to
conventional baselines.

</details>


### [12] [MORIC: CSI Delay-Doppler Decomposition for Robust Wi-Fi-based Human Activity Recognition](https://arxiv.org/abs/2506.12997)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 论文提出了一种基于Wi-Fi CSI的鲁棒性人类活动识别方法，通过延迟剖面空间转换和随机卷积核模型MORIC，显著提升了手势识别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Wi-Fi CSI的人类活动识别方法在真实场景中泛化能力不足，亟需一种鲁棒性更强的解决方案。

Method: 将CSI信号转换为延迟剖面空间，分解为多普勒速度，并设计随机卷积核模型MORIC以应对输入表示的随机性。

Result: 实验表明，该方法在手势识别任务中优于现有技术，尤其是对复杂手势，且少量校准样本可显著提升精度。

Conclusion: 提出的方法在真实场景中具有更高的实用性和鲁棒性，为Wi-Fi感知应用提供了新思路。

Abstract: The newly established IEEE 802.11bf Task Group aims to amend the WLAN
standard to support advanced sensing applications such as human activity
recognition (HAR). Although studies have demonstrated the potential of sub-7
GHz Wi-Fi Channel State Information (CSI) for HAR, no method currently performs
reliably in real-world scenarios. This work tackles the poor generalization of
Wi-Fi-based HAR by introducing an innovative approach to extracting and
utilizing movement-related representations, which makes it robust to noise and
static environmental properties. This is achieved by transforming CSI signals
into the delay profile space and decomposing them into various Doppler
velocities, which serve as informative projections of a mobile point's velocity
from different unknown random angles. To mitigate the impact of this
randomness, MORIC is introduced as a novel time series classification model
based on random convolutional kernels, designed to be invariant to the random
order and repetition of input representations, thereby enabling robust Wi-Fi
CSI-based activity classification. Experimental results on the collected
dataset demonstrate that the proposed method outperforms state-of-the-art
approaches in terms of generalization accuracy for hand motion recognition,
particularly for challenging gestures. Furthermore, incorporating a small
number of calibration samples leads to a significant improvement in accuracy,
enhancing the practicality of the method for real-world deployment.

</details>


### [13] [Joint Spectrum Sensing and Resource Allocation for OFDMA-based Underwater Acoustic Communications](https://arxiv.org/abs/2506.13008)
*Minwoo Kim,Youngchol Choi,Yeongjun Kim,Eojin Seo,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于深度强化学习（DRL）的端到端感知与资源优化方法，用于解决OFDMA水下认知无线电网络中的异步多址问题，显著提升了频谱效率和通信成功率。


<details>
  <summary>Details</summary>
Motivation: 水下声学通信面临长传播延迟、有限信道资源和高衰减等挑战，OFDMA虽提升了频谱效率，但异步多址问题导致频谱感知不准确和资源分配困难。

Method: 采用深度强化学习（DRL）设计端到端的感知与资源优化方法，以解决异步多址问题。

Result: 仿真结果表明，该方法在频谱效率上优于基线方案42.9%，通信成功率提升4.4%。

Conclusion: 提出的DRL方法有效解决了水下OFDMA-CR网络的资源分配问题，显著提升了通信性能。

Abstract: Underwater acoustic (UWA) communications generally rely on cognitive radio
(CR)-based ad-hoc networks due to challenges such as long propagation delay,
limited channel resources, and high attenuation. To address the constraints of
limited frequency resources, UWA communications have recently incorporated
orthogonal frequency division multiple access (OFDMA), significantly enhancing
spectral efficiency (SE) through multiplexing gains. Still, {the} low
propagation speed of UWA signals, combined with {the} dynamic underwater
environment, creates asynchrony in multiple access scenarios. This causes
inaccurate spectrum sensing as inter-carrier interference (ICI) increases,
which leads to difficulties in resource allocation. As efficient resource
allocation is essential for achieving high-quality communication in OFDMA-based
CR networks, these challenges degrade communication reliability in UWA systems.
To resolve the issue, we propose an end-to-end sensing and resource
optimization method using deep reinforcement learning (DRL) in an OFDMA-based
UWA-CR network. Through extensive simulations, we confirm that the proposed
method is superior to baseline schemes, outperforming other methods by 42.9 %
in SE and 4.4 % in communication success rate.

</details>


### [14] [Collaborative Beamforming for Communication Applications Using a Two-Element Fully-Wireless Open-Loop Coherent Distributed Array](https://arxiv.org/abs/2506.13014)
*Jason M. Merlo,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 论文展示了一种全无线两节点开环相干分布式通信系统的概念验证，并在城市环境中通过传输QPSK、64-QAM和256-QAM信号评估其性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证无需依赖外部时间或频率参考（如GNSS）的分布式通信系统的可行性。

Method: 使用软件定义无线电（SDR）实现分布式处理，并通过无线节点间通信共享协调信息。

Result: 在58米链路上传输信号，实现了0.936的平均相干增益和低于1.4×10⁻⁴的符号错误率（64-QAM以下），带宽可达12 Mbps。

Conclusion: 系统展示了高可靠性和带宽潜力，适用于分布式通信场景。

Abstract: In this work we demonstrate a proof of concept of a fully-wireless two-node
open-loop coherent distributed communication system and evaluate its
performance by transmitting QPSK , 64-, and 256-QAM constellations at a symbol
rate of 2 MBd over a 58 m link in an urban environment. The system is
implemented in a distributed manner with on-node processing using
software-defined radios (SDRs) and wireless internode communication to share
coordination information and does not rely on external time or frequency
references such as the global navigation satellite system (GNSS). In each
experiment ~100 messages were transmitted and a mean coherent gain of 0.936 was
achieved across all measurements with a mean symbol error ratio of below
$1.4\times 10^{-4}$ achieved up to 64-QAM, demonstrating a reliable bandwidth
of up to 12 Mbps.

</details>


### [15] [DoA Estimation using MUSIC with Range/Doppler Multiplexing for MIMO-OFDM Radar](https://arxiv.org/abs/2506.13258)
*Murat Babek Salman,Emil Björnson*

Main category: eess.SP

TL;DR: 提出了一种用于OFDM MIMO雷达系统的新型感知参数估计算法，通过两阶段方法实现高密度目标环境下的稳健DoA估计。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要同时具备通信和目标感知能力，而现有超分辨率DoA估计技术在目标数量超过天线阵列维度时性能受限。

Method: 采用两阶段方法：1）通过延迟和多普勒域滤波减少目标数量；2）引入融合技术抑制旁瓣干扰。

Result: 数值模拟验证了该方法在高密度目标环境中的优越性能。

Conclusion: 该算法在有限尺寸天线阵列下实现了稳健的DoA估计，优于传统方法。

Abstract: Sensing emerges as a critical challenge in 6G networks, which require
simultaneous communication and target sensing capabilities. State-of-the-art
super-resolution techniques for the direction of arrival (DoA) estimation
encounter significant performance limitations when the number of targets
exceeds antenna array dimensions. This paper introduces a novel sensing
parameter estimation algorithm for orthogonal frequency-division multiplexing
(OFDM) multiple-input multiple-output (MIMO) radar systems. The proposed
approach implements a strategic two-stage methodology: first, discriminating
targets through delay and Doppler domain filtering to reduce the number of
effective targets for super-resolution DoA estimation, and second, introducing
a fusion technique to mitigate sidelobe interferences. The algorithm enables
robust DoA estimation, particularly in high-density target environments with
limited-size antenna arrays. Numerical simulations validate the superior
performance of the proposed method compared to conventional DoA estimation
approaches.

</details>


### [16] [Performance Analysis of Communication Signals for Localization in Underwater Sensor Networks](https://arxiv.org/abs/2506.13330)
*Ashwani Koul,Gustaf Hendeby,Isaac Skog*

Main category: eess.SP

TL;DR: 研究探讨了集成传感与通信（ISAC）系统在水下移动目标定位中的效能，通过被动和主动测量融合减少传统方法的带宽、能量和处理时间低效问题。


<details>
  <summary>Details</summary>
Motivation: 传统水下目标定位方法因信息集中处理导致带宽、能量和处理时间低效，ISAC系统能联合完成传感、定位和通信，减少这些低效问题。

Method: 使用Cramér-Rao Lower Bound（CRLB）作为性能指标，分析不同目标动态和海洋状态下的定位误差。

Result: 仿真结果显示，ISAC系统在不同场景下能有效减少定位误差，提升水下定位的效率和可靠性。

Conclusion: ISAC系统在水下移动目标定位中具有高效和可靠的潜力，适用于多种目标动态和海洋条件。

Abstract: Fusion of passive and active measurements from sensor nodes becomes critical
in localizing underwater objects and is traditionally achieved by communicating
information to a central node. This causes significant inefficiencies in
bandwidth, energy, and processing time, which are critical in marine
applications. With integrated sensing and communication (ISAC) systems, the
process of sensing, localization, and communication can be achieved jointly,
and the inefficiencies can be minimized. Thus, the primary objective of this
study is to analyse the efficacy of such communication signals in localizing a
moving target in given underwater conditions. The Cram\'er-Rao Lower Bound
(CRLB) is a performance metric used to determine the theoretical lower bound on
localization errors. Simulation results illustrate the contours of localization
error across various scenarios, offering valuable insights into system
performance under different target dynamics and sea state conditions,
showcasing their potential for efficient and reliable underwater localization
applications.

</details>


### [17] [HELENA: High-Efficiency Learning-based channel Estimation using dual Neural Attention](https://arxiv.org/abs/2506.13408)
*Miguel Camelo Botero,Esra Aycan Beyazit,Nina Slamnik-Kriještorac,Johann M. Marquez-Barja*

Main category: eess.SP

TL;DR: HELENA是一种紧凑的深度学习模型，用于高效信道估计，在低延迟和实时部署中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高精度信道估计对5G新无线电等OFDM系统至关重要，尤其是在低信噪比和严格延迟约束下。

Method: HELENA结合轻量卷积主干和两种高效注意力机制：全局依赖的块状多头自注意力和局部特征细化的挤压-激励块。

Result: 相比CEViT，HELENA推理时间减少45%，精度相当，参数减少8倍。

Conclusion: HELENA适用于低延迟、实时部署场景。

Abstract: Accurate channel estimation is critical for high-performance Orthogonal
Frequency-Division Multiplexing systems such as 5G New Radio, particularly
under low signal-to-noise ratio and stringent latency constraints. This letter
presents HELENA, a compact deep learning model that combines a lightweight
convolutional backbone with two efficient attention mechanisms: patch-wise
multi-head self-attention for capturing global dependencies and a
squeeze-and-excitation block for local feature refinement. Compared to CEViT, a
state-of-the-art vision transformer-based estimator, HELENA reduces inference
time by 45.0\% (0.175\,ms vs.\ 0.318\,ms), achieves comparable accuracy
($-16.78$\,dB vs.\ $-17.30$\,dB), and requires $8\times$ fewer parameters
(0.11M vs.\ 0.88M), demonstrating its suitability for low-latency, real-time
deployment.

</details>


### [18] [Pinching-Antenna Systems (PASS) Meet Multiple Access: NOMA or OMA?](https://arxiv.org/abs/2506.13490)
*Qiao Ren,Xidong Mu,Siyu Lin,Yuanwei Liu*

Main category: eess.SP

TL;DR: 论文研究了基于PASS的两用户通信系统在NOMA、FDMA和TDMA三种多址接入方案下的性能，通过优化波束成形以最小化发射功率，并提出了两阶段算法。结果表明PASS优于传统天线系统，且NOMA和TDMA在不同场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究PASS系统在多址接入方案下的性能优化，以最小化发射功率并满足用户速率需求。

Method: 针对NOMA和FDMA提出两阶段算法（SCA方法和相位调整），针对TDMA利用时间切换特性优化波束成形。

Result: PASS性能优于传统天线系统；NOMA在非对称需求下表现最佳，TDMA在对称需求下表现最佳。

Conclusion: PASS系统在多址接入方案中具有显著优势，NOMA和TDMA在不同场景下各有优劣。

Abstract: A fundamental two-user PASS-based communication system is considered under
three MA schemes, namely non-orthogonal multiple access (NOMA), frequency
division multiple access (FDMA), and time division multiple access (TDMA). For
each MA scheme, a pinching beamforming optimization problem is formulated to
minimize the required transmit power for satisfying users' rate requirements.
For NOMA and FDMA, a two-stage algorithm is proposed, where the locations of
PAs are derived sequentially by using the successive convex approximation (SCA)
method and fine-turning phase adjustment. For TDMA, by leveraging the
time-switching feature of PASS, the optimal pinching beamforming of each time
slot is derived to maximize the served user channel gain. Numerical results are
provided to show that: 1) PASS can achieve a significant performance gain over
conventional antenna systems, and 2) NOMA consistently outperforms FDMA, while
TDMA provides superior performance than NOMA for symmetric user rate
requirements.

</details>


### [19] [Intelligent Metasurface-Enabled Integrated Sensing and Communication: Unified Framework and Key Technologies](https://arxiv.org/abs/2506.13713)
*Shunyu Li,Tianqi Mao,Guangyao Liu,Fan Zhang,Ruiqi Liu,Meng Hua,Zhen Gao,Qingqing Wu,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 本文探讨了智能超表面（IM）在集成感知与通信（ISAC）系统中的应用，提出了统一的ISAC框架，并讨论了关键技术如信道建模、波束成形和波形设计。


<details>
  <summary>Details</summary>
Motivation: 随着对无处不在的连接和高精度环境感知需求的增长，ISAC成为6G网络的关键技术，而IM因其对电磁波的高效可编程控制能力，在ISAC中展现出巨大潜力。

Method: 文章首先概述了IM架构及其优势，建立了统一的ISAC框架，并系统化建模了IM支持的收发器设计，随后讨论了信道建模、信道估计、波束成形和波形设计等关键技术。

Result: 研究为IM在ISAC系统中的性能优化和权衡提供了理论基础，并提出了多种关键技术方案。

Conclusion: IM在ISAC系统中具有广阔的应用前景，未来研究可进一步探索其实际部署和性能优化。

Abstract: As the demand for ubiquitous connectivity and high-precision environmental
awareness grows, integrated sensing and communication (ISAC) has emerged as a
key technology for sixth-generation (6G) wireless networks. Intelligent
metasurfaces (IMs) have also been widely adopted in ISAC scenarios due to their
efficient, programmable control over electromagnetic waves. This provides a
versatile solution that meets the dual-function requirements of next-generation
networks. Although reconfigurable intelligent surfaces (RISs) have been
extensively studied for manipulating the propagation channel between base and
mobile stations, the full potential of IMs in ISAC transceiver design remains
under-explored. Against this backdrop, this article explores emerging
IM-enabled transceiver designs for ISAC systems. It begins with an overview of
representative IM architectures, their unique principles, and their inherent
advantages in EM wave manipulation. Next, a unified ISAC framework is
established to systematically model the design and derivation of diverse
IM-enabled transceiver structures. This lays the foundation for performance
optimization, trade-offs, and analysis. The paper then discusses several
critical technologies for IM-enabled ISAC transceivers, including dedicated
channel modeling, effective channel estimation, tailored beamforming
strategies, and dual-functional waveform design.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [20] [CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models](https://arxiv.org/abs/2506.12059)
*Jiajun He,Naoki Sawada,Koichi Miyazaki,Tomoki Toda*

Main category: eess.AS

TL;DR: 提出了一种结合多说话人重叠语音识别和上下文偏置的统一框架，通过预训练语音编码器和大型语言模型优化性能，显著提升了复杂场景下的识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法分别处理多说话人ASR和上下文偏置，性能受限。本文旨在统一这两项任务以提升复杂场景下的识别能力。

Method: 整合预训练语音编码器和大型语言模型，采用优化的微调策略，并引入两阶段过滤算法高效识别罕见词。

Result: 在LibriMix和AMI SDM数据集上分别达到7.9%和32.9%的WER，优于传统方法。

Conclusion: 该框架在复杂语音场景中表现出色，为多说话人和罕见词识别提供了有效解决方案。

Abstract: In real-world applications, automatic speech recognition (ASR) systems must
handle overlapping speech from multiple speakers and recognize rare words like
technical terms. Traditional methods address multi-talker ASR and contextual
biasing separately, limiting performance in complex scenarios. We propose a
unified framework that combines multi-talker overlapping speech recognition and
contextual biasing into a single task. Our ASR method integrates pretrained
speech encoders and large language models (LLMs), using optimized finetuning
strategies. We also introduce a two-stage filtering algorithm to efficiently
identify relevant rare words from large biasing lists and incorporate them into
the LLM's prompt input, enhancing rare word recognition. Experiments show that
our approach outperforms traditional contextual biasing methods, achieving a
WER of 7.9% on LibriMix and 32.9% on AMI SDM when the biasing size is 1,000,
demonstrating its effectiveness in complex speech scenarios.

</details>


### [21] [Evaluating Logit-Based GOP Scores for Mispronunciation Detection](https://arxiv.org/abs/2506.12067)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

TL;DR: 比较基于logit和基于概率的GOP评分在发音评估中的表现，发现logit方法在分类上更优，但效果因数据集而异。混合方法结合两者特征可提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于softmax后验概率的GOP评分存在过度自信和音素分离不足的问题，限制了发音评估的效果。

Method: 在荷兰和汉语母语者的英语发音数据集上，比较基于logit和概率的GOP评分，评估分类性能和与人工评分的相关性。

Result: 基于logit的GOP在分类上优于基于概率的方法，最大logit GOP与人类感知最一致。混合方法结合两者特征表现最佳。

Conclusion: 结合不确定性建模和音素特定权重的混合GOP方法可改进发音评估。

Abstract: Pronunciation assessment relies on goodness of pronunciation (GOP) scores,
traditionally derived from softmax-based posterior probabilities. However,
posterior probabilities may suffer from overconfidence and poor phoneme
separation, limiting their effectiveness. This study compares logit-based GOP
scores with probability-based GOP scores for mispronunciation detection. We
conducted our experiment on two L2 English speech datasets spoken by Dutch and
Mandarin speakers, assessing classification performance and correlation with
human ratings. Logit-based methods outperform probability-based GOP in
classification, but their effectiveness depends on dataset characteristics. The
maximum logit GOP shows the strongest alignment with human perception, while a
combination of different GOP scores balances probability and logit features.
The findings suggest that hybrid GOP methods incorporating uncertainty modeling
and phoneme-specific weighting improve pronunciation assessment.

</details>


### [22] [Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis](https://arxiv.org/abs/2506.12073)
*Zongli Ye,Jiachen Lian,Xuanru Zhou,Jinming Zhang,Haodong Li,Shuhe Li,Chenxu Guo,Anaisha Das,Peter Park,Zoe Ezzes,Jet Vonk,Brittany Morin,Rian Bogley,Lisa Wauters,Zachary Miller,Maria Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: Neural LCS是一种新方法，用于解决不流畅语音与文本对齐问题，通过音素级建模显著提升了对齐准确性和分割效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法在音素相似性建模上表现不佳，限制了其在神经退行性语音障碍诊断中的应用。

Method: 提出Neural LCS，利用音素级建模解决部分对齐和上下文感知相似性映射问题。

Result: 在模拟数据集和真实PPA数据上，Neural LCS显著优于现有模型。

Conclusion: Neural LCS为语音障碍诊断提供了更准确、基于语言学的方法。

Abstract: Accurate alignment of dysfluent speech with intended text is crucial for
automating the diagnosis of neurodegenerative speech disorders. Traditional
methods often fail to model phoneme similarities effectively, limiting their
performance. In this work, we propose Neural LCS, a novel approach for
dysfluent text-text and speech-text alignment. Neural LCS addresses key
challenges, including partial alignment and context-aware similarity mapping,
by leveraging robust phoneme-level modeling. We evaluate our method on a
large-scale simulated dataset, generated using advanced data simulation
techniques, and real PPA data. Neural LCS significantly outperforms
state-of-the-art models in both alignment accuracy and dysfluent speech
segmentation. Our results demonstrate the potential of Neural LCS to enhance
automated systems for diagnosing and analyzing speech disorders, offering a
more accurate and linguistically grounded solution for dysfluent speech
alignment.

</details>


### [23] [CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following](https://arxiv.org/abs/2506.12285)
*Yinghao Ma,Siyou Li,Juntao Yu,Emmanouil Benetos,Akira Maezawa*

Main category: eess.AS

TL;DR: CMI-Bench是一个全面的音乐指令跟随基准测试，用于评估音频-文本大语言模型（LLMs）在多样化音乐信息检索（MIR）任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围有限，未能反映真实音乐分析的复杂性，因此需要更全面的评估工具。

Method: 重新解释传统MIR注释为指令跟随格式，并引入CMI-Bench，支持标准化评估指标和开源模型。

Result: 实验显示LLMs与监督模型之间存在显著性能差距，并揭示了文化、年代和性别偏见。

Conclusion: CMI-Bench为音乐指令跟随评估提供了统一基础，推动了音乐感知LLMs的发展。

Abstract: Recent advances in audio-text large language models (LLMs) have opened new
possibilities for music understanding and generation. However, existing
benchmarks are limited in scope, often relying on simplified tasks or
multi-choice evaluations that fail to reflect the complexity of real-world
music analysis. We reinterpret a broad range of traditional MIR annotations as
instruction-following formats and introduce CMI-Bench, a comprehensive music
instruction following benchmark designed to evaluate audio-text LLMs on a
diverse set of music information retrieval (MIR) tasks. These include genre
classification, emotion regression, emotion tagging, instrument classification,
pitch estimation, key detection, lyrics transcription, melody extraction, vocal
technique recognition, instrument performance technique detection, music
tagging, music captioning, and (down)beat tracking: reflecting core challenges
in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized
evaluation metrics consistent with previous state-of-the-art MIR models,
ensuring direct comparability with supervised approaches. We provide an
evaluation toolkit supporting all open-source audio-textual LLMs, including
LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant
performance gaps between LLMs and supervised models, along with their culture,
chronological and gender bias, highlighting the potential and limitations of
current models in addressing MIR tasks. CMI-Bench establishes a unified
foundation for evaluating music instruction following, driving progress in
music-aware LLMs.

</details>


### [24] [Mitigating Non-Target Speaker Bias in Guided Speaker Embedding](https://arxiv.org/abs/2506.12500)
*Shota Horiguchi,Takanori Ashihara,Marc Delcroix,Atsushi Ando,Naohiro Tawara*

Main category: eess.AS

TL;DR: 论文提出了一种改进的说话人嵌入方法，通过利用目标说话人活动线索优化全局统计模块，提升了在多说话人条件下的嵌入质量。


<details>
  <summary>Details</summary>
Motivation: 在多说话人条件下获取高质量的说话人嵌入对许多应用至关重要。现有方法在严重重叠情况下表现良好，但在低重叠情况下性能下降。

Method: 提出了一种扩展的全局统计模块，利用目标说话人活动线索，仅从目标说话人活跃的区间计算统计信息。

Result: 该方法在低和高重叠比例下均提升了说话人验证性能，并在多个数据集上改进了说话人分离性能。

Conclusion: 通过优化全局统计模块对非目标说话人区间的敏感性，新方法显著提升了嵌入质量，适用于多种重叠场景。

Abstract: Obtaining high-quality speaker embeddings in multi-speaker conditions is
crucial for many applications. A recently proposed guided speaker embedding
framework, which utilizes speech activities of target and non-target speakers
as clues, drastically improved embeddings under severe overlap with small
degradation in low-overlap cases. However, since extreme overlaps are rare in
natural conversations, this degradation cannot be overlooked. This paper first
reveals that the degradation is caused by the global-statistics-based modules,
widely used in speaker embedding extractors, being overly sensitive to
intervals containing only non-target speakers. As a countermeasure, we propose
an extension of such modules that exploit the target speaker activity clues, to
compute statistics from intervals where the target is active. The proposed
method improves speaker verification performance in both low and high overlap
ratios, and diarization performance on multiple datasets.

</details>


### [25] [Towards Neural Audio Codec Source Parsing](https://arxiv.org/abs/2506.12627)
*Orchid Chetia Phukan,Girish,Mohd Mujtaba Akhtar,Arun Balaji Buduru,Rajesh Sharma*

Main category: eess.AS

TL;DR: 论文提出了一种名为NACSP的新方法，用于解决音频深度伪造（codecfakes）中的源属性问题，通过结构化回归预测生成参数，并提出了HYDRA框架以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前的开集属性方法在识别未知神经音频编解码器（NAC）时存在局限性，无法具体描述或区分其内部配置，导致泛化能力不足。

Method: 提出NACSP方法，将源属性问题转化为生成NAC参数的结构化回归任务，并设计HYDRA框架，利用双曲几何和多任务注意力机制提升性能。

Result: HYDRA在基准数据集上表现优于欧几里得空间的基线方法，实现了多任务泛化的最佳结果。

Conclusion: NACSP和HYDRA为音频深度伪造的源属性问题提供了更精细和可泛化的解决方案。

Abstract: A new class of audio deepfakes-codecfakes (CFs)-has recently caught
attention, synthesized by Audio Language Models that leverage neural audio
codecs (NACs) in the backend. In response, the community has introduced
dedicated benchmarks and tailored detection strategies. As the field advances,
efforts have moved beyond binary detection toward source attribution, including
open-set attribution, which aims to identify the NAC responsible for generation
and flag novel, unseen ones during inference. This shift toward source
attribution improves forensic interpretability and accountability. However,
open-set attribution remains fundamentally limited: while it can detect that a
NAC is unfamiliar, it cannot characterize or identify individual unseen codecs.
It treats such inputs as generic ``unknowns'', lacking insight into their
internal configuration. This leads to major shortcomings: limited
generalization to new NACs and inability to resolve fine-grained variations
within NAC families. To address these gaps, we propose Neural Audio Codec
Source Parsing (NACSP) - a paradigm shift that reframes source attribution for
CFs as structured regression over generative NAC parameters such as quantizers,
bandwidth, and sampling rate. We formulate NACSP as a multi-task regression
task for predicting these NAC parameters and establish the first comprehensive
benchmark using various state-of-the-art speech pre-trained models (PTMs). To
this end, we propose HYDRA, a novel framework that leverages hyperbolic
geometry to disentangle complex latent properties from PTM representations. By
employing task-specific attention over multiple curvature-aware hyperbolic
subspaces, HYDRA enables superior multi-task generalization. Our extensive
experiments show HYDRA achieves top results on benchmark CFs datasets compared
to baselines operating in Euclidean space.

</details>


### [26] [Using Neurogram Similarity Index Measure (NSIM) to Model Hearing Loss and Cochlear Neural Degeneration](https://arxiv.org/abs/2506.12705)
*Ahsan J. Cheema,Sunil Puria*

Main category: eess.AS

TL;DR: 本文提出了一种通过神经图相似性指数（NSIM）量化听力损失和耳蜗神经退行性变（CND）的方法，并在两项研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 嘈杂环境中的听力问题普遍存在，且与CND相关，但缺乏客观的量化方法。

Method: 使用听觉外周计算模型模拟听力任务，通过NSIM比较听觉神经纤维响应。

Result: 研究1表明NSIM能准确映射听力损失者的音素识别表现；研究2显示NSIM能敏感捕捉CND缺陷，可作为非侵入性生物标志物。

Conclusion: NSIM是一种有效的工具，可用于量化听力损失和CND，并有望应用于临床诊断。

Abstract: Trouble hearing in noisy situations remains a common complaint for both
individuals with hearing loss and individuals with normal hearing. This is
hypothesized to arise due to condition called: cochlear neural degeneration
(CND) which can also result in significant variabilities in hearing aids
outcomes. This paper uses computational models of auditory periphery to
simulate various hearing tasks. We present an objective method to quantify
hearing loss and CND by comparing auditory nerve fiber responses using a
Neurogram Similarity Index Measure (NSIM). Specifically study 1, shows that
NSIM can be used to map performance of individuals with hearing loss on phoneme
recognition task with reasonable accuracy. In the study 2, we show that NSIM is
a sensitive measure that can also be used to capture the deficits resulting
from CND and can be a candidate for noninvasive biomarker of auditory
synaptopathy.

</details>


### [27] [Frequency Dynamic Convolutions for Sound Event Detection](https://arxiv.org/abs/2506.12785)
*Hyeonuk Nam*

Main category: eess.AS

TL;DR: 该研究提出了频率动态卷积（FDY conv）及其扩展模型（DFD conv、PFD conv、MDFD conv、TFD conv），以解决传统2D卷积在声音事件检测（SED）中频率依赖性不足的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统2D卷积假设时间和频率轴的平移不变性，无法有效处理声音信号的频率依赖性特征，因此需要动态调整卷积核以优化性能。

Method: 提出FDY conv，通过频率特定注意力权重动态调整卷积核；扩展模型包括DFD conv（扩张卷积）、PFD conv（部分动态卷积）、MDFD conv（多扩张卷积）和TFD conv（时间注意力池化）。

Result: FDY conv提升基线性能7.56%，DFD conv提升9.27%，PFD conv提升7.80%并减少参数54.4%，MDFD conv提升10.98%，TFD conv性能与MDFD conv相当但参数减少30.01%。

Conclusion: 频率自适应卷积及其扩展模型为基于深度学习的音频处理提供了更优的解决方案，适用于不同类型的声音事件检测。

Abstract: Recent research in deep learning-based Sound Event Detection (SED) has
primarily focused on Convolutional Recurrent Neural Networks (CRNNs) and
Transformer models. However, conventional 2D convolution-based models assume
shift invariance along both the temporal and frequency axes, leadin to
inconsistencies when dealing with frequency-dependent characteristics of
acoustic signals. To address this issue, this study proposes Frequency Dynamic
Convolution (FDY conv), which dynamically adjusts convolutional kernels based
on the frequency composition of the input signal to enhance SED performance.
FDY conv constructs an optimal frequency response by adaptively weighting
multiple basis kernels based on frequency-specific attention weights.
Experimental results show that applying FDY conv to CRNNs improves performance
on the DESED dataset by 7.56% compared to the baseline CRNN. However, FDY conv
has limitations in that it combines basis kernels of the same shape across all
frequencies, restricting its ability to capture diverse frequency-specific
characteristics. Additionally, the $3\times3$ basis kernel size is insufficient
to capture a broader frequency range. To overcome these limitations, this study
introduces an extended family of FDY conv models. Dilated FDY conv (DFD conv)
applies convolutional kernels with various dilation rates to expand the
receptive field along the frequency axis and enhance frequency-specific feature
representation. Experimental results show that DFD conv improves performance by
9.27% over the baseline. Partial FDY conv (PFD conv) addresses the high
computational cost of FDY conv, which results from performing all convolution
operations with dynamic kernels. Since FDY conv may introduce unnecessary
adaptivity for quasi-stationary sound events, PFD conv integrates standard 2D
convolutions with frequency-adaptive kernels to reduce computational complexity
while maintaining performance. Experimental results demonstrate that PFD conv
improves performance by 7.80% over the baseline while reducing the number of
parameters by 54.4% compared to FDY conv. Multi-Dilated FDY conv (MDFD conv)
extends DFD conv by addressing its structural limitation of applying the same
dilation across all frequencies. By utilizing multiple convolutional kernels
with different dilation rates, MDFD conv effectively captures diverse
frequency-dependent patterns. Experimental results indicate that MDFD conv
achieves the highest performance, improving the baseline CRNN performance by
10.98%. Furthermore, standard FDY conv employs Temporal Average Pooling, which
assigns equal weight to all frames along the time axis, limiting its ability to
effectively capture transient events. To overcome this, this study proposes
TAP-FDY conv (TFD conv), which integrates Temporal Attention Pooling (TA) that
focuses on salient features, Velocity Attention Pooling (VA) that emphasizes
transient characteristics, and Average Pooling (AP) that captures stationary
properties. TAP-FDY conv achieves the same performance as MDFD conv but reduces
the number of parameters by approximately 30.01% (12.703M vs. 18.157M),
achieving equivalent accuracy with lower computational complexity. Class-wise
performance analysis reveals that FDY conv improves detection of non-stationary
events, DFD conv is particularly effective for events with broad spectral
features, and PFD conv enhances the detection of quasi-stationary events.
Additionally, TFD conv (TFD-CRNN) demonstrates strong performance in detecting
transient events. In the case studies, PFD conv effectively captures stable
signal patterns in tank powertrain fault recognition, DFD conv recognizes wide
harmonic spectral patterns on speed-varying motor fault recognition, while TFD
conv outperforms other models in detecting transient signals in offshore arc
detection. These results suggest that frequency-adaptive convolutions and their
extended variants provide a robust alternative to conventional 2D convolutions
in deep learning-based audio processing.

</details>


### [28] [Magnetoencephalography (MEG) Based Non-Invasive Chinese Speech Decoding](https://arxiv.org/abs/2506.12817)
*Zhihong Jia,Hongbin Wang,Yuanzhong Shen,Feng Hu,Jiayu An,Kai Shu,Dongrui Wu*

Main category: eess.AS

TL;DR: 该论文提出了一种多模态辅助语音解码（MASD）算法，用于非侵入性中文语音脑机接口（BCI），并发布了首个中文文本-脑磁图（MEG）数据集。


<details>
  <summary>Details</summary>
Motivation: 中文作为全球广泛使用的语言，其语音BCI研究稀缺，亟需填补这一空白。

Method: 提出MASD算法，结合文本和声学信息解码脑信号，并构建中文文本-MEG数据集。

Result: 实验验证了文本-MEG数据集和MASD算法的有效性。

Conclusion: 这是首个关于非侵入性语音BCI多模态辅助解码的研究。

Abstract: As an emerging paradigm of brain-computer interfaces (BCIs), speech BCI has
the potential to directly reflect auditory perception and thoughts, offering a
promising communication alternative for patients with aphasia. Chinese is one
of the most widely spoken languages in the world, whereas there is very limited
research on speech BCIs for Chinese language. This paper reports a
text-magnetoencephalography (MEG) dataset for non-invasive Chinese speech BCIs.
It also proposes a multi-modality assisted speech decoding (MASD) algorithm to
capture both text and acoustic information embedded in brain signals during
speech activities. Experiment results demonstrated the effectiveness of both
our text-MEG dataset and our proposed MASD algorithm. To our knowledge, this is
the first study on modality-assisted decoding for non-invasive speech BCIs.

</details>


### [29] [ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching](https://arxiv.org/abs/2506.13053)
*Han Zhu,Wei Kang,Zengwei Yao,Liyong Guo,Fangjun Kuang,Zhaoqing Li,Weiji Zhuang,Long Lin,Daniel Povey*

Main category: eess.AS

TL;DR: ZipVoice是一种基于流匹配的零样本TTS模型，具有小模型尺寸和快速推理速度，同时保持高质量语音输出。


<details>
  <summary>Details</summary>
Motivation: 解决现有大规模零样本TTS模型因参数过多导致的推理速度慢的问题。

Method: 1) 基于Zipformer的流匹配解码器；2) 平均上采样初始语音-文本对齐和Zipformer文本编码器；3) 流蒸馏方法减少采样步骤。

Result: 在100k小时多语言数据集上，ZipVoice语音质量与SOTA模型相当，模型尺寸小3倍，推理速度快30倍。

Conclusion: ZipVoice在保持高质量的同时显著提升了推理效率，适用于实际应用。

Abstract: Existing large-scale zero-shot text-to-speech (TTS) models deliver high
speech quality but suffer from slow inference speeds due to massive parameters.
To address this issue, this paper introduces ZipVoice, a high-quality
flow-matching-based zero-shot TTS model with a compact model size and fast
inference speed. Key designs include: 1) a Zipformer-based flow-matching
decoder to maintain adequate modeling capabilities under constrained size; 2)
Average upsampling-based initial speech-text alignment and Zipformer-based text
encoder to improve speech intelligibility; 3) A flow distillation method to
reduce sampling steps and eliminate the inference overhead associated with
classifier-free guidance. Experiments on 100k hours multilingual datasets show
that ZipVoice matches state-of-the-art models in speech quality, while being 3
times smaller and up to 30 times faster than a DiT-based flow-matching
baseline. Codes, model checkpoints and demo samples are publicly available.

</details>


### [30] [Boundary-Informed Sound Field Reconstruction](https://arxiv.org/abs/2506.13279)
*David Sundström,Filip Elvander,Andreas Jakobsson*

Main category: eess.AS

TL;DR: 论文研究了在部分边界信息可用的情况下，通过贝叶斯线性框架结合边界先验信息，显著提升了声场重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏完整边界信息时，传统方法需要大量麦克风测量才能实现高精度声场重建。本文旨在解决部分或不确定边界信息下的声场重建问题。

Method: 采用线性贝叶斯框架，结合阻抗边界条件的边界先验信息，并联合优化未知超参数（如噪声、信号方差和阻抗边界条件）。

Result: 数值实验表明，即使边界点数量有限或边界位置存在一定不确定性，边界先验信息仍能显著提升重建效果。

Conclusion: 边界先验信息在部分边界信息可用时对声场重建具有重要价值，尤其适用于空间声音控制应用。

Abstract: We consider the problem of reconstructing the sound field in a room using
prior information of the boundary geometry, represented as a point cloud. In
general, when no boundary information is available, an accurate sound field
reconstruction over a large spatial region and at high frequencies requires
numerous microphone measurements. On the other hand, if all geometrical and
acoustical aspects of the boundaries are known, the sound field could, in
theory, be simulated without any measurements. In this work, we address the
intermediate case, where only partial or uncertain boundary information is
available. This setting is similar to one studied in virtual reality
applications, where the goal is to create a perceptually convincing audio
experience. In this work, we focus on spatial sound control applications, which
in contrast require an accurate sound field reconstruction. Therefore, we
formulate the problem within a linear Bayesian framework, incorporating a
boundary-informed prior derived from impedance boundary conditions. The
formulation allows for joint optimization of the unknown hyperparameters,
including the noise and signal variances and the impedance boundary conditions.
Using numerical experiments, we show that incorporating the boundary-informed
prior significantly enhances the reconstruction, notably even when only a few
hundreds of boundary points are available or when the boundary positions are
calibrated with an uncertainty up to 1 dm.

</details>


### [31] [Instance-Specific Test-Time Training for Speech Editing in the Wild](https://arxiv.org/abs/2506.13295)
*Taewoo Kim,Uijong Lee,Hayoung Park,Choongsang Cho,Nam In Park,Young Han Lee*

Main category: eess.AS

TL;DR: 提出了一种实例特定的测试时训练方法，用于野外语音编辑，通过直接和间接监督解决声学一致性和带宽不连续问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音编辑系统难以适应多样化的声学条件，导致实际场景中编辑性能下降。

Method: 采用测试时训练方法，结合未编辑区域的真实声学特征直接监督和编辑区域的间接监督（基于时长约束和音素预测的辅助损失）。

Result: 在野外基准数据集上，该方法在客观和主观评估中均优于现有语音编辑系统。

Conclusion: 该方法通过实例特定的训练策略，有效提升了语音编辑的适应性和性能。

Abstract: Speech editing systems aim to naturally modify speech content while
preserving acoustic consistency and speaker identity. However, previous studies
often struggle to adapt to unseen and diverse acoustic conditions, resulting in
degraded editing performance in real-world scenarios. To address this, we
propose an instance-specific test-time training method for speech editing in
the wild. Our approach employs direct supervision from ground-truth acoustic
features in unedited regions, and indirect supervision in edited regions via
auxiliary losses based on duration constraints and phoneme prediction. This
strategy mitigates the bandwidth discontinuity problem in speech editing,
ensuring smooth acoustic transitions between unedited and edited regions.
Additionally, it enables precise control over speech rate by adapting the model
to target durations via mask length adjustment during test-time training.
Experiments on in-the-wild benchmark datasets demonstrate that our method
outperforms existing speech editing systems in both objective and subjective
evaluations.

</details>


### [32] [BUT System for the MLC-SLM Challenge](https://arxiv.org/abs/2506.13414)
*Alexander Polok,Jiangyu Han,Dominik Klement,Samuele Cornell,Jan Černocký,Lukáš Burget*

Main category: eess.AS

TL;DR: 论文提出了一种结合DiCoW和DiariZen的双说话人自动语音识别系统，在未微调的多语言场景中表现优异，微调后性能进一步提升，并在MLC-SLM挑战赛中排名第二。


<details>
  <summary>Details</summary>
Motivation: 解决多语言场景下双说话人语音识别的挑战，提升自动语音识别系统的泛化能力和性能。

Method: 结合DiCoW（基于Whisper的说话人条件化变体）和DiariZen（基于Pyannote的说话人分割流程），在未微调和微调场景下进行测试。

Result: DiariZen在未微调场景下优于基线模型，DiCoW保留多语言能力；微调后系统性能进一步提升，最终在MLC-SLM挑战赛中排名第二。

Conclusion: 系统在多语言和微调场景中表现优异，同时提出数据标注问题的简单缓解策略，提升系统鲁棒性。

Abstract: We present a two-speaker automatic speech recognition (ASR) system that
combines DiCoW -- a diarization-conditioned variant of Whisper -- with
DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate
both systems in out-of-domain (OOD) multilingual scenarios without any
fine-tuning. In this scenario, DiariZen consistently outperforms the baseline
Pyannote diarization model, demonstrating strong generalization. Despite being
fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid
multilingual performance, indicating that encoder modifications preserve
Whisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen
on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform
the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain
adaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and
ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several
labeling inconsistencies in the training data -- such as missing speech
segments and incorrect silence annotations -- which can hinder diarization
fine-tuning. We propose simple mitigation strategies to address these issues
and improve system robustness.

</details>


### [33] [Stereo sound event localization and detection based on PSELDnet pretraining and BiMamba sequence modeling](https://arxiv.org/abs/2506.13455)
*Wenmiao Gao,Yang Xiao*

Main category: eess.AS

TL;DR: 提出了一种基于预训练PSELDnet和双向Mamba序列建模的立体声事件定位与检测系统，显著降低了计算复杂度并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型在声音事件定位与检测任务中计算复杂度高，需要改进。

Method: 用BiMamba模块替代Conformer模块，并引入非对称卷积以更有效建模时空关系。

Result: 在DCASE2025 Task 3开发数据集上，性能显著优于基线及原始PSELDnet，同时降低了计算复杂度。

Conclusion: BiMamba架构能有效解决SELD任务的挑战。

Abstract: Pre-training methods have achieved significant performance improvements in
sound event localization and detection (SELD) tasks, but existing
Transformer-based models suffer from high computational complexity. In this
work, we propose a stereo sound event localization and detection system based
on pre-trained PSELDnet and bidirectional Mamba sequence modeling. We replace
the Conformer module with a BiMamba module and introduce asymmetric
convolutions to more effectively model the spatiotemporal relationships between
time and frequency dimensions. Experimental results demonstrate that the
proposed method achieves significantly better performance than the baseline and
the original PSELDnet with Conformer decoder architecture on the DCASE2025 Task
3 development dataset, while also reducing computational complexity. These
findings highlight the effectiveness of the BiMamba architecture in addressing
the challenges of the SELD task.

</details>


### [34] [SpeechRefiner: Towards Perceptual Quality Refinement for Front-End Algorithms](https://arxiv.org/abs/2506.13709)
*Sirui Li,Shuai Wang,Zhijun Liu,Zhongjie Jiang,Yannan Wang,Haizhou Li*

Main category: eess.AS

TL;DR: SpeechRefiner是一种后处理工具，利用条件流匹配（CFM）提升语音感知质量，弥补现有前端技术的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语音预处理技术（如去噪、去混响和分离）可能产生残留噪声或新伪影，影响感知质量，而传统指标（如SI-SNR）无法捕捉这些缺陷。

Method: 提出SpeechRefiner，基于条件流匹配（CFM）进行后处理，提升语音质量。

Result: 实验表明，SpeechRefiner能泛化处理多种语音损伤源，显著提升感知质量。

Conclusion: SpeechRefiner是一种有效的后处理工具，可弥补前端技术的不足，提升语音质量。

Abstract: Speech pre-processing techniques such as denoising, de-reverberation, and
separation, are commonly employed as front-ends for various downstream speech
processing tasks. However, these methods can sometimes be inadequate, resulting
in residual noise or the introduction of new artifacts. Such deficiencies are
typically not captured by metrics like SI-SNR but are noticeable to human
listeners. To address this, we introduce SpeechRefiner, a post-processing tool
that utilizes Conditional Flow Matching (CFM) to improve the perceptual quality
of speech. In this study, we benchmark SpeechRefiner against recent
task-specific refinement methods and evaluate its performance within our
internal processing pipeline, which integrates multiple front-end algorithms.
Experiments show that SpeechRefiner exhibits strong generalization across
diverse impairment sources, significantly enhancing speech perceptual quality.
Audio demos can be found at https://speechrefiner.github.io/SpeechRefiner/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [35] [TuneGenie: Reasoning-based LLM agents for preferential music generation](https://arxiv.org/abs/2506.12083)
*Amitesh Pandey,Jafarbek Arifdjanov,Ansh Tiwari*

Main category: cs.SD

TL;DR: 论文提出了一种基于大语言模型（LLMs）的音乐偏好分析与生成系统TuneGenie，用于生成音乐创作提示。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs强大的文本推理能力，分析个人音乐偏好并生成有效的音乐创作提示。

Method: 开发了TuneGenie模型，结合音乐元数据和个人描述，生成音乐创作提示，并提出了评估和基准测试方法。

Result: 展示了LLMs在音乐生成领域的潜力，并贡献了相关研究。

Conclusion: TuneGenie为AI在艺术生成中的应用提供了新视角，但也引发了争议。

Abstract: Recently, Large language models (LLMs) have shown great promise across a
diversity of tasks, ranging from generating images to reasoning spatially.
Considering their remarkable (and growing) textual reasoning capabilities, we
investigate LLMs' potency in conducting analyses of an individual's preferences
in music (based on playlist metadata, personal write-ups, etc.) and producing
effective prompts (based on these analyses) to be passed to Suno AI (a
generative AI tool for music production). Our proposition of a novel LLM-based
textual representation to music model (which we call TuneGenie) and the various
methods we develop to evaluate & benchmark similar models add to the increasing
(and increasingly controversial) corpus of research on the use of AI in
generating art.

</details>


### [36] [Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding](https://arxiv.org/abs/2506.12154)
*Haoran Zhou,Xingchen Song,Brendan Fahy,Qiaochu Song,Binbin Zhang,Zhendong Peng,Anshul Wadhawan,Denglin Jiang,Apurv Verma,Vinay Ramesh,Srivas Prasad,Michele M. Franceschini*

Main category: cs.SD

TL;DR: 论文通过微调Whisper模型，结合WeNet工具包和U2结构，实现了流式自动语音识别（ASR），并引入混合分词器提升效率。


<details>
  <summary>Details</summary>
Motivation: Whisper模型缺乏对流式ASR的原生支持，需要改进以适应实时场景。

Method: 采用WeNet工具包的U2结构，引入CTC解码器和因果注意力掩码生成流式部分转录，Whisper解码器重新排序输出。

Result: 实验表明，经过微调的Whisper在流式ASR任务中表现良好，混合分词器提升了数据效率和泛化能力。

Conclusion: Whisper可通过微调适应流式ASR，混合分词器设计有效。

Abstract: OpenAI Whisper is a family of robust Automatic Speech Recognition (ASR)
models trained on 680,000 hours of audio. However, its encoder-decoder
architecture, trained with a sequence-to-sequence objective, lacks native
support for streaming ASR. In this paper, we fine-tune Whisper for streaming
ASR using the WeNet toolkit by adopting a Unified Two-pass (U2) structure. We
introduce an additional Connectionist Temporal Classification (CTC) decoder
trained with causal attention masks to generate streaming partial transcripts,
while the original Whisper decoder reranks these partial outputs. Our
experiments on LibriSpeech and an earnings call dataset demonstrate that, with
adequate fine-tuning data, Whisper can be adapted into a capable streaming ASR
model. We also introduce a hybrid tokenizer approach, which uses a smaller
token space for the CTC decoder while retaining Whisper's original token space
for the attention decoder, resulting in improved data efficiency and
generalization.

</details>


### [37] [ViSAGe: Video-to-Spatial Audio Generation](https://arxiv.org/abs/2506.12199)
*Jaeyeon Kim,Heeseung Yun,Gunhee Kim*

Main category: cs.SD

TL;DR: 论文提出了一种从无声视频直接生成一阶Ambisonics空间音频的方法，并引入了YT-Ambigen数据集和新评估指标。ViSAGe框架通过结合CLIP视觉特征和自回归神经音频编解码模型，生成高质量的空间音频。


<details>
  <summary>Details</summary>
Motivation: 空间音频能提升视听体验的沉浸感，但传统制作方法复杂且需要专业知识。本文旨在简化这一过程。

Method: 提出ViSAGe框架，利用CLIP视觉特征和自回归神经音频编解码模型，结合方向性和视觉引导，从无声视频生成一阶Ambisonics。

Result: ViSAGe生成的空间音频质量高且与视频时间对齐，优于两阶段方法。

Conclusion: ViSAGe为从视频生成空间音频提供了一种高效且高质量的解决方案。

Abstract: Spatial audio is essential for enhancing the immersiveness of audio-visual
experiences, yet its production typically demands complex recording systems and
specialized expertise. In this work, we address a novel problem of generating
first-order ambisonics, a widely used spatial audio format, directly from
silent videos. To support this task, we introduce YT-Ambigen, a dataset
comprising 102K 5-second YouTube video clips paired with corresponding
first-order ambisonics. We also propose new evaluation metrics to assess the
spatial aspect of generated audio based on audio energy maps and saliency
metrics. Furthermore, we present Video-to-Spatial Audio Generation (ViSAGe), an
end-to-end framework that generates first-order ambisonics from silent video
frames by leveraging CLIP visual features, autoregressive neural audio codec
modeling with both directional and visual guidance. Experimental results
demonstrate that ViSAGe produces plausible and coherent first-order ambisonics,
outperforming two-stage approaches consisting of video-to-audio generation and
audio spatialization. Qualitative examples further illustrate that ViSAGe
generates temporally aligned high-quality spatial audio that adapts to
viewpoint changes.

</details>


### [38] [SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes](https://arxiv.org/abs/2506.12222)
*Tony Alex,Sara Ahmed,Armin Mustafa,Muhammad Awais,Philip JB Jackson*

Main category: cs.SD

TL;DR: 论文提出了一种名为SSLAM的自监督学习方法，旨在提升模型处理多音源音频的能力，同时在单音源音频上保持或超越现有性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督音频模型通常在单音源数据上表现良好，但在实际多音源复杂音频中的泛化能力未被充分探索，限制了其实际应用。

Method: 引入SSLAM方法，专注于从多音源音频中学习，同时保持对单音源数据的性能。

Result: SSLAM在标准单音源数据集上表现优异（如AudioSet-2M提升3.9%），并在多音源数据集上创下新SOTA（提升达9.1%）。

Conclusion: SSLAM有效提升了自监督音频模型在多音源场景下的性能，为实际应用提供了更强的鲁棒性。

Abstract: Self-supervised pre-trained audio networks have seen widespread adoption in
real-world systems, particularly in multi-modal large language models. These
networks are often employed in a frozen state, under the assumption that the
SSL pre-training has sufficiently equipped them to handle real-world audio.
However, a critical question remains: how well do these models actually perform
in real-world conditions, where audio is typically polyphonic and complex,
involving multiple overlapping sound sources? Current audio SSL methods are
often benchmarked on datasets predominantly featuring monophonic audio, such as
environmental sounds, and speech. As a result, the ability of SSL models to
generalize to polyphonic audio, a common characteristic in natural scenarios,
remains underexplored. This limitation raises concerns about the practical
robustness of SSL models in more realistic audio settings. To address this gap,
we introduce Self-Supervised Learning from Audio Mixtures (SSLAM), a novel
direction in audio SSL research, designed to improve, designed to improve the
model's ability to learn from polyphonic data while maintaining strong
performance on monophonic data. We thoroughly evaluate SSLAM on standard audio
SSL benchmark datasets which are predominantly monophonic and conduct a
comprehensive comparative analysis against SOTA methods using a range of
high-quality, publicly available polyphonic datasets. SSLAM not only improves
model performance on polyphonic audio, but also maintains or exceeds
performance on standard audio SSL benchmarks. Notably, it achieves up to a
3.9\% improvement on the AudioSet-2M (AS-2M), reaching a mean average precision
(mAP) of 50.2. For polyphonic datasets, SSLAM sets new SOTA in both linear
evaluation and fine-tuning regimes with performance improvements of up to 9.1\%
(mAP).

</details>


### [39] [Improving Speech Enhancement with Multi-Metric Supervision from Learned Quality Assessment](https://arxiv.org/abs/2506.12260)
*Wei Wang,Wangyou Zhang,Chenda Li,Jiatong Shi,Shinji Watanabe,Yanmin Qian*

Main category: cs.SD

TL;DR: 该论文提出了一种利用语音质量评估（SQA）模型指导语音增强（SE）训练的方法，解决了传统目标函数与感知质量不一致的问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强（SE）的目标函数（如SI-SNR）与感知质量不一致，且难以适应多种评估指标。此外，真实数据中缺乏干净的参考信号。因此，研究如何利用SQA模型指导SE训练具有重要意义。

Method: 提出了一种训练框架，利用SQA模型（能够预测多种评估指标）作为SE的监督信号。该方法适用于无干净参考的真实数据。

Result: 实验表明，SQA引导的训练在多种质量指标上均表现更优，适用于模拟和真实数据集。

Conclusion: SQA模型可以有效指导SE训练，提升性能并解决传统方法的局限性。

Abstract: Speech quality assessment (SQA) aims to predict the perceived quality of
speech signals under a wide range of distortions. It is inherently connected to
speech enhancement (SE), which seeks to improve speech quality by removing
unwanted signal components. While SQA models are widely used to evaluate SE
performance, their potential to guide SE training remains underexplored. In
this work, we investigate a training framework that leverages a SQA model,
trained to predict multiple evaluation metrics from a public SE leaderboard, as
a supervisory signal for SE. This approach addresses a key limitation of
conventional SE objectives, such as SI-SNR, which often fail to align with
perceptual quality and generalize poorly across evaluation metrics. Moreover,
it enables training on real-world data where clean references are unavailable.
Experiments on both simulated and real-world test sets show that SQA-guided
training consistently improves performance across a range of quality metrics.

</details>


### [40] [GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition](https://arxiv.org/abs/2506.12325)
*Yuntao Shou,Jun Yao,Tao Meng,Wei Ai,Cen Chen,Keqin Li*

Main category: cs.SD

TL;DR: 论文提出了一种新颖的图谱扩散网络（GSDNet），用于解决多模态情感识别中的模态缺失问题，通过将高斯噪声映射到缺失模态的图谱空间，保留原始数据的语义和拓扑信息。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别（MERC）在实际场景中因模态缺失问题性能受限，现有图扩散模型可能破坏图的连通性和局部结构。

Method: 提出GSDNet，将高斯噪声映射到图谱空间，恢复缺失数据，保留全局拓扑信息和重要谱特征。

Result: 实验表明GSDNet在各种模态缺失场景下实现了最先进的情感识别性能。

Conclusion: GSDNet通过图谱扩散方法有效解决了模态缺失问题，提升了情感识别的鲁棒性。

Abstract: Multimodal emotion recognition in conversations (MERC) aims to infer the
speaker's emotional state by analyzing utterance information from multiple
sources (i.e., video, audio, and text). Compared with unimodality, a more
robust utterance representation can be obtained by fusing complementary
semantic information from different modalities. However, the modality missing
problem severely limits the performance of MERC in practical scenarios. Recent
work has achieved impressive performance on modality completion using graph
neural networks and diffusion models, respectively. This inspires us to combine
these two dimensions through the graph diffusion model to obtain more powerful
modal recovery capabilities. Unfortunately, existing graph diffusion models may
destroy the connectivity and local structure of the graph by directly adding
Gaussian noise to the adjacency matrix, resulting in the generated graph data
being unable to retain the semantic and topological information of the original
graph. To this end, we propose a novel Graph Spectral Diffusion Network
(GSDNet), which maps Gaussian noise to the graph spectral space of missing
modalities and recovers the missing data according to its original
distribution. Compared with previous graph diffusion methods, GSDNet only
affects the eigenvalues of the adjacency matrix instead of destroying the
adjacency matrix directly, which can maintain the global topological
information and important spectral features during the diffusion process.
Extensive experiments have demonstrated that GSDNet achieves state-of-the-art
emotion recognition performance in various modality loss scenarios.

</details>


### [41] [Methods for pitch analysis in contemporary popular music: multiple pitches from harmonic tones in Vitalic's music](https://arxiv.org/abs/2506.12405)
*Emmanuel Deruty,David Meredith,Maarten Grachten,Pascal Arbez-Nicolas,Andreas Hasselholt Jørgensen,Oliver Søndermølle Hansen,Magnus Stensli,Christian Nørkær Petersen*

Main category: cs.SD

TL;DR: 研究发现，当代流行音乐中单音谐波复合音产生的多音感知是主动且有意的特征，通过实验和信号分析验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 探讨当代流行音乐中单音谐波复合音的多音感知现象及其在音乐创作中的意义。

Method: 通过两项听力测试（多音感知评估和手动音高转录）及信号特性分析。

Result: 合成谐波音比自然音传递更多感知音高，且感知音高与信号特性（如上部分音和自相关特征）相关。

Conclusion: 当代流行音乐中的谐波音可传递多个模糊音高，感知音高取决于听者和听音条件。

Abstract: Aims. This study suggests that the use of multiple perceived pitches arising
from a single harmonic complex tone is an active and intentional feature of
contemporary popular music. The phenomenon is illustrated through examples
drawn from the work of electronic artist Vitalic and others.
  Methods. Two listening tests were conducted: (1) evaluation of the number of
simultaneous pitches perceived from single harmonic tones, and (2) manual pitch
transcription of sequences of harmonic tones. Relationships between signal
characteristics and pitch perception were then analyzed.
  Results. The synthetic harmonic tones found in the musical sequences under
study were observed to transmit more perceived pitches than their acoustic
counterparts, with significant variation across listeners. Multiple ambiguous
pitches were associated with tone properties such as prominent upper partials
and particular autocorrelation profiles.
  Conclusions. Harmonic tones in a context of contemporary popular music can,
in general, convey several ambiguous pitches. The set of perceived pitches
depends on both the listener and the listening conditions.

</details>


### [42] [Style-based Composer Identification and Attribution of Symbolic Music Scores: a Systematic Survey](https://arxiv.org/abs/2506.12440)
*Federico Simonetta*

Main category: cs.SD

TL;DR: 本文首次对基于风格的音乐符号作曲者识别和作者归属文献进行了全面系统综述，分析了58篇论文，指出了现有研究的不足并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决该领域可靠性和可重复性不足的问题，推动更可信的计算风格分析。

Method: 系统综述58篇同行评审论文，分析其曲目、计算方法和评估方法，强调平衡准确率和交叉验证的重要性。

Result: 发现现有研究普遍存在验证不足和依赖简单准确率的问题，提出了改进指南。

Conclusion: 建议未来研究采用更可靠的指标和方法，以提升作曲者识别和作者归属研究的可信度和音乐学有效性。

Abstract: This paper presents the first comprehensive systematic review of literature
on style-based composer identification and authorship attribution in symbolic
music scores. Addressing the critical need for improved reliability and
reproducibility in this field, the review rigorously analyzes 58 peer-reviewed
papers published across various historical periods, with the search adapted to
evolving terminology. The analysis critically assesses prevailing repertoires,
computational approaches, and evaluation methodologies, highlighting
significant challenges. It reveals that a substantial portion of existing
research suffers from inadequate validation protocols and an over-reliance on
simple accuracy metrics for often imbalanced datasets, which can undermine the
credibility of attribution claims. The crucial role of robust metrics like
Balanced Accuracy and rigorous cross-validation in ensuring trustworthy results
is emphasized. The survey also details diverse feature representations and the
evolution of machine learning models employed. Notable real-world authorship
attribution cases, such as those involving works attributed to Bach, Josquin
Desprez, and Lennon-McCartney, are specifically discussed, illustrating the
opportunities and pitfalls of applying computational techniques to resolve
disputed musical provenance. Based on these insights, a set of actionable
guidelines for future research are proposed. These recommendations are designed
to significantly enhance the reliability, reproducibility, and musicological
validity of composer identification and authorship attribution studies,
fostering more robust and interpretable computational stylistic analysis.

</details>


### [43] [StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling](https://arxiv.org/abs/2506.12570)
*Hui Wang,Yifan Yang,Shujie Liu,Jinyu Li,Lingwei Meng,Yanqing Liu,Jiaming Zhou,Haoqin Sun,Yan Lu,Yong Qin*

Main category: cs.SD

TL;DR: StreamMel是一种单阶段流式文本到语音（TTS）框架，通过连续mel频谱图建模，实现低延迟、高质量语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有流式TTS系统多采用多阶段流水线和离散表示，导致计算成本高且性能不佳，而离线TTS系统难以实时应用。

Method: StreamMel通过将文本标记与声学帧交错，实现单阶段流式合成，建模连续mel频谱图。

Result: 在LibriSpeech上的实验表明，StreamMel在质量和延迟上优于现有流式TTS基线，甚至接近离线系统性能。

Conclusion: StreamMel展示了与实时语音大语言模型集成的广阔前景，支持高效实时生成。

Abstract: Recent advances in zero-shot text-to-speech (TTS) synthesis have achieved
high-quality speech generation for unseen speakers, but most systems remain
unsuitable for real-time applications because of their offline design. Current
streaming TTS paradigms often rely on multi-stage pipelines and discrete
representations, leading to increased computational cost and suboptimal system
performance. In this work, we propose StreamMel, a pioneering single-stage
streaming TTS framework that models continuous mel-spectrograms. By
interleaving text tokens with acoustic frames, StreamMel enables low-latency,
autoregressive synthesis while preserving high speaker similarity and
naturalness. Experiments on LibriSpeech demonstrate that StreamMel outperforms
existing streaming TTS baselines in both quality and latency. It even achieves
performance comparable to offline systems while supporting efficient real-time
generation, showcasing broad prospects for integration with real-time speech
large language models. Audio samples are available at:
https://aka.ms/StreamMel.

</details>


### [44] [Video-Guided Text-to-Music Generation Using Public Domain Movie Collections](https://arxiv.org/abs/2506.12573)
*Haven Kim,Zachary Novack,Weihan Xu,Julian McAuley,Hao-Wen Dong*

Main category: cs.SD

TL;DR: 论文提出了Open Screen Sound Library (OSSL)数据集，结合电影片段、高质量配乐和情绪标注，用于改进电影音乐生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成系统在电影制作中应用有限，因缺乏综合考虑视觉内容、对话和情绪等多因素的数据集。

Method: 引入OSSL数据集，并提出视频适配器增强基于自回归Transformer的文本到音乐模型。

Result: 实验表明，该方法显著提升了MusicGen-Medium在分布和配对保真度上的表现，以及主观情绪和类型兼容性。

Conclusion: OSSL数据集和视频适配器有效改进了电影音乐生成任务。

Abstract: Despite recent advancements in music generation systems, their application in
film production remains limited, as they struggle to capture the nuances of
real-world filmmaking, where filmmakers consider multiple factors-such as
visual content, dialogue, and emotional tone-when selecting or composing music
for a scene. This limitation primarily stems from the absence of comprehensive
datasets that integrate these elements. To address this gap, we introduce Open
Screen Sound Library (OSSL), a dataset consisting of movie clips from public
domain films, totaling approximately 36.5 hours, paired with high-quality
soundtracks and human-annotated mood information. To demonstrate the
effectiveness of our dataset in improving the performance of pre-trained models
on film music generation tasks, we introduce a new video adapter that enhances
an autoregressive transformer-based text-to-music model by adding video-based
conditioning. Our experimental results demonstrate that our proposed approach
effectively enhances MusicGen-Medium in terms of both objective measures of
distributional and paired fidelity, and subjective compatibility in mood and
genre. The dataset and code are available at
https://havenpersona.github.io/ossl-v1.

</details>


### [45] [ANIRA: An Architecture for Neural Network Inference in Real-Time Audio Applications](https://arxiv.org/abs/2506.12665)
*Valentin Ackva,Fares Schulz*

Main category: cs.SD

TL;DR: anira是一个高效的跨平台库，专为满足实时音频应用的神经网络推理需求而设计，支持多种后端和架构，并通过线程池和延迟管理优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络推理工具难以满足实时音频应用的需求，因此开发了anira库以提供高效、兼容性强的解决方案。

Method: anira支持ONNX Runtime、LibTorch和TensorFlow Lite作为后端，通过线程池解耦推理与音频回调，并内置延迟管理和基准测试功能。

Result: 实验表明，对于无状态模型，ONNX Runtime运行最快；对于有状态模型，LibTorch性能最佳。某些组合的初始推理时间较长。

Conclusion: anira通过优化后端选择和线程管理，显著提升了实时音频应用中神经网络推理的性能和稳定性。

Abstract: Numerous tools for neural network inference are currently available, yet many
do not meet the requirements of real-time audio applications. In response, we
introduce anira, an efficient cross-platform library. To ensure compatibility
with a broad range of neural network architectures and frameworks, anira
supports ONNX Runtime, LibTorch, and TensorFlow Lite as backends. Each
inference engine exhibits real-time violations, which anira mitigates by
decoupling the inference from the audio callback to a static thread pool. The
library incorporates built-in latency management and extensive benchmarking
capabilities, both crucial to ensure a continuous signal flow. Three different
neural network architectures for audio effect emulation are then subjected to
benchmarking across various configurations. Statistical modeling is employed to
identify the influence of various factors on performance. The findings indicate
that for stateless models, ONNX Runtime exhibits the lowest runtimes. For
stateful models, LibTorch demonstrates the fastest performance. Our results
also indicate that for certain model-engine combinations, the initial
inferences take longer, particularly when these inferences exhibit a higher
incidence of real-time violations.

</details>


### [46] [SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition](https://arxiv.org/abs/2506.12672)
*Yuta Hirano,Sakriani Sakti*

Main category: cs.SD

TL;DR: SC-SOT是一种改进的SOT训练方法，用于端到端多说话人语音识别，通过显式引入说话人信息提升重叠语音处理能力。


<details>
  <summary>Details</summary>
Motivation: SOT在重叠语音处理中存在隐式说话人分离不足的问题，SC-SOT旨在通过显式条件化解码器来改善这一点。

Method: SC-SOT通过引入说话人嵌入和说话人活动信息，显式条件化解码器，并结合端到端说话人日志模型生成说话人嵌入。

Result: 实验证明SC-SOT在重叠语音处理中表现更优。

Conclusion: SC-SOT通过显式说话人信息条件化，有效提升了多说话人语音识别的性能。

Abstract: We propose Speaker-Conditioned Serialized Output Training (SC-SOT), an
enhanced SOT-based training for E2E multi-talker ASR. We first probe how SOT
handles overlapped speech, and we found the decoder performs implicit speaker
separation. We hypothesize this implicit separation is often insufficient due
to ambiguous acoustic cues in overlapping regions. To address this, SC-SOT
explicitly conditions the decoder on speaker information, providing detailed
information about "who spoke when". Specifically, we enhance the decoder by
incorporating: (1) speaker embeddings, which allow the model to focus on the
acoustic characteristics of the target speaker, and (2) speaker activity
information, which guides the model to suppress non-target speakers. The
speaker embeddings are derived from a jointly trained E2E speaker diarization
model, mitigating the need for speaker enrollment. Experimental results
demonstrate the effectiveness of our conditioning approach on overlapped
speech.

</details>


### [47] [Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV](https://arxiv.org/abs/2506.13001)
*Christian Zhou-Zheng,Philippe Pasquier*

Main category: cs.SD

TL;DR: 该论文提出了一种基于RWKV-7线性架构的MIDI-RWKV模型，用于个性化、多轨道、长上下文和可控的音乐填充任务，以增强计算机辅助作曲过程。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成系统多为端到端，难以支持人机交互的迭代创作过程，因此需要一种更灵活的方法。

Method: 采用RWKV-7线性架构，支持高效、连贯的音乐协同创作，并提出了低样本量下的状态微调方法。

Result: MIDI-RWKV在定量和定性指标上表现良好，并支持个性化调整。

Conclusion: MIDI-RWKV为计算机辅助音乐创作提供了一种高效且个性化的解决方案。

Abstract: Existing work in automatic music generation has primarily focused on
end-to-end systems that produce complete compositions or continuations.
However, because musical composition is typically an iterative process, such
systems make it difficult to engage in the back-and-forth between human and
machine that is essential to computer-assisted creativity. In this study, we
address the task of personalizable, multi-track, long-context, and controllable
symbolic music infilling to enhance the process of computer-assisted
composition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear
architecture, to enable efficient and coherent musical cocreation on edge
devices. We also demonstrate that MIDI-RWKV admits an effective method of
finetuning its initial state for personalization in the very-low-sample regime.
We evaluate MIDI-RWKV and its state tuning on several quantitative and
qualitative metrics, and release model weights and code at
https://github.com/christianazinn/MIDI-RWKV.

</details>


### [48] [I$^2$S-TFCKD: Intra-Inter Set Knowledge Distillation with Time-Frequency Calibration for Speech Enhancement](https://arxiv.org/abs/2506.13127)
*Jiaming Cheng,Ruiyu Liang,Chao Xu,Ye Ni,Wei Zhou,Björn W. Schuller,Xiaoshuai Hao*

Main category: cs.SD

TL;DR: 本文提出了一种基于时频校准的帧内-帧间知识蒸馏框架（I²S-TFCKD），用于语音增强任务，旨在平衡模型复杂性与性能。


<details>
  <summary>Details</summary>
Motivation: 在硬件资源有限或延迟要求严格的场景下，如何平衡神经网络语音增强模型的复杂性与性能是主要挑战。

Method: 提出双流时频交叉校准的多层交互蒸馏方法，并通过帧内-帧间相关性构建协作蒸馏范式。

Result: 实验表明，该策略显著提升了低复杂度学生模型的性能，优于其他蒸馏方案。

Conclusion: I²S-TFCKD框架有效解决了语音增强任务中复杂性与性能的平衡问题。

Abstract: In recent years, complexity compression of neural network (NN)-based speech
enhancement (SE) models has gradually attracted the attention of researchers,
especially in scenarios with limited hardware resources or strict latency
requirements. The main difficulties and challenges lie in achieving a balance
between complexity and performance according to the characteristics of the
task. In this paper, we propose an intra-inter set knowledge distillation (KD)
framework with time-frequency calibration (I$^2$S-TFCKD) for SE. Different from
previous distillation strategies for SE, the proposed framework fully utilizes
the time-frequency differential information of speech while promoting global
knowledge flow. Firstly, we propose a multi-layer interactive distillation
based on dual-stream time-frequency cross-calibration, which calculates the
teacher-student similarity calibration weights in the time and frequency
domains respectively and performs cross-weighting, thus enabling refined
allocation of distillation contributions across different layers according to
speech characteristics. Secondly, we construct a collaborative distillation
paradigm for intra-set and inter-set correlations. Within a correlated set,
multi-layer teacher-student features are pairwise matched for calibrated
distillation. Subsequently, we generate representative features from each
correlated set through residual fusion to form the fused feature set that
enables inter-set knowledge interaction. The proposed distillation strategy is
applied to the dual-path dilated convolutional recurrent network (DPDCRN) that
ranked first in the SE track of the L3DAS23 challenge. Objective evaluations
demonstrate that the proposed KD strategy consistently and effectively improves
the performance of the low-complexity student model and outperforms other
distillation schemes.

</details>


### [49] [SONIC: Sound Optimization for Noise In Crowds](https://arxiv.org/abs/2506.13272)
*Pranav M N,Gandham Sai Santhosh,Tejas Joshi,S Sriniketh Desikan,Eswar Gupta*

Main category: cs.SD

TL;DR: SONIC是一种基于ARM Cortex-M7的嵌入式实时噪声抑制系统，采用自适应滤波（LMS）提升嘈杂环境中的语音清晰度。


<details>
  <summary>Details</summary>
Motivation: 解决传统主动噪声消除（ANC）系统的局限性，探索嵌入式系统中的高效噪声抑制方法。

Method: 使用自适应滤波（LMS）算法，优化MCU性能，分析音频信号处理架构。

Result: 系统显著提升语音清晰度，实现实时低功耗运行。

Conclusion: 低功耗DSP可作为复杂AI去噪方法的替代方案。

Abstract: This paper presents SONIC, an embedded real-time noise suppression system
implemented on the ARM Cortex-M7-based STM32H753ZI microcontroller. Using
adaptive filtering (LMS), the system improves speech intelligibility in noisy
environments. SONIC focuses on a novel approach to noise suppression in audio
signals, specifically addressing the limitations of traditional Active Noise
Cancellation (ANC) systems. The paper explores various signal processing
algorithms in a micro-controller point of view, highlighting various
performance factors and which were considered optimal in our embedded system.
Additionally we also discussed the system architecture, explaining how the
MCU's efficiency was harnessed, along with an in-depth overview of how the
audio signals were translated within the processor. The results demonstrate
improved speech clarity and practical real-time performance, showing low-power
DSP as an alternative to complex AI denoising methods.

</details>


### [50] [Persistent Homology of Music Network with Three Different Distances](https://arxiv.org/abs/2506.13595)
*Eunwoo Heo,Byeongchan Choi,Myung ock Kim,Mai Lan Tran,Jae-Hun Jung*

Main category: cs.SD

TL;DR: 论文研究了在音乐图中应用持久同调，通过三种不同的距离定义分析其对拓扑结构的影响，并验证了这些定义在一维持久同调中的包含关系。


<details>
  <summary>Details</summary>
Motivation: 持久同调在数据分析中广泛应用，但距离定义的选择会影响拓扑推断结果。本文旨在探索不同距离定义对音乐图持久同调的影响。

Method: 在预定义权重的音乐图中，基于边路径定义了三种距离，并分析了它们对持久条形码、持久图以及出生/死亡边的影响。

Result: 发现三种距离定义在一维持久同调中存在包含关系，并通过实际音乐数据验证了这一结果。

Conclusion: 不同距离定义对持久同调结果有显著影响，选择适合的距离定义对音乐数据的拓扑分析至关重要。

Abstract: Persistent homology has been widely used to discover hidden topological
structures in data across various applications, including music data. To apply
persistent homology, a distance or metric must be defined between points in a
point cloud or between nodes in a graph network. These definitions are not
unique and depend on the specific objectives of a given problem. In other
words, selecting different metric definitions allows for multiple topological
inferences. In this work, we focus on applying persistent homology to music
graph with predefined weights. We examine three distinct distance definitions
based on edge-wise pathways and demonstrate how these definitions affect
persistent barcodes, persistence diagrams, and birth/death edges. We found that
there exist inclusion relations in one-dimensional persistent homology
reflected on persistence barcode and diagram among these three distance
definitions. We verified these findings using real music data.

</details>

{"id": "2506.13971", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.13971", "abs": "https://arxiv.org/abs/2506.13971", "authors": ["Andrew Chang", "Chenkai Hu", "Ji Qi", "Zhuojian Wei", "Kexin Zhang", "Viswadruth Akkaraju", "David Poeppel", "Dustin Freeman"], "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience", "comment": "Interspeech 2025", "summary": "Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience."}
{"id": "2506.14204", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.14204", "abs": "https://arxiv.org/abs/2506.14204", "authors": ["Aswin Shanmugam Subramanian", "Amit Das", "Naoyuki Kanda", "Jinyu Li", "Xiaofei Wang", "Yifan Gong"], "title": "Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios", "comment": "Accepted to Interspeech 2025", "summary": "We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions."}
{"id": "2506.14427", "categories": ["eess.AS", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.14427", "abs": "https://arxiv.org/abs/2506.14427", "authors": ["Shilong Wu", "Hang Chen", "Jun Du"], "title": "M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization Dataset", "comment": "11 pages, 5 figures", "summary": "In the field of speaker diarization, the development of technology is constrained by two problems: insufficient data resources and poor generalization ability of deep learning models. To address these two problems, firstly, we propose an automated method for constructing speaker diarization datasets, which generates more accurate pseudo-labels for massive data through the combination of audio and video. Relying on this method, we have released Multi-modal, Multi-scenario and Multi-language Speaker Diarization (M3SD) datasets. This dataset is derived from real network videos and is highly diverse. In addition, we further propose a scenario-related model fine-tuning strategy. Based on the general model pre-trained using the above dataset, we combine the specific data of the target scenario (e.g., meetings) and achieve targeted optimization by using Adapter and LoRA joint fine-tuning, thus achieving the model's domain adaptation. Our dataset and code have been open-sourced at https://huggingface.co/spaces/OldDragon/m3sd."}
{"id": "2506.14657", "categories": ["eess.AS", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.14657", "abs": "https://arxiv.org/abs/2506.14657", "authors": ["Jongin Choi", "Jina Park", "Woojoo Lee", "Jae-Jin Lee", "Massoud Pedram"], "title": "ASAP-FE: Energy-Efficient Feature Extraction Enabling Multi-Channel Keyword Spotting on Edge Processors", "comment": "7 pages, 11 figures, ISLPED 2025", "summary": "Multi-channel keyword spotting (KWS) has become crucial for voice-based applications in edge environments. However, its substantial computational and energy requirements pose significant challenges. We introduce ASAP-FE (Agile Sparsity-Aware Parallelized-Feature Extractor), a hardware-oriented front-end designed to address these challenges. Our framework incorporates three key innovations: (1) Half-overlapped Infinite Impulse Response (IIR) Framing: This reduces redundant data by approximately 25% while maintaining essential phoneme transition cues. (2) Sparsity-aware Data Reduction: We exploit frame-level sparsity to achieve an additional 50% data reduction by combining frame skipping with stride-based filtering. (3) Dynamic Parallel Processing: We introduce a parameterizable filter cluster and a priority-based scheduling algorithm that allows parallel execution of IIR filtering tasks, reducing latency and optimizing energy efficiency. ASAP-FE is implemented with various filter cluster sizes on edge processors, with functionality verified on FPGA prototypes and designs synthesized at 45 nm. Experimental results using TC-ResNet8, DS-CNN, and KWT-1 demonstrate that ASAP-FE reduces the average workload by 62.73% while supporting real-time processing for up to 32 channels. Compared to a conventional fully overlapped baseline, ASAP-FE achieves less than a 1% accuracy drop (e.g., 96.22% vs. 97.13% for DS-CNN), which is well within acceptable limits for edge AI. By adjusting the number of filter modules, our design optimizes the trade-off between performance and energy, with 15 parallel filters providing optimal performance for up to 25 channels. Overall, ASAP-FE offers a practical and efficient solution for multi-channel KWS on energy-constrained edge devices."}
{"id": "2506.13833", "categories": ["cs.SD", "cs.AI", "cs.RO", "eess.AS", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2506.13833", "abs": "https://arxiv.org/abs/2506.13833", "authors": ["Xiaoliang Chen", "Le Chang", "Xin Yu", "Yunhe Huang", "Xianling Tu"], "title": "A Survey on World Models Grounded in Acoustic Physical Information", "comment": "28 pages,11 equations", "summary": "This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal \"intuitive physics\" engine through sound."}
{"id": "2506.14065", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14065", "abs": "https://arxiv.org/abs/2506.14065", "authors": ["Ethan Chien", "Jan Steckel"], "title": "Dynamically Tunable Helical Antenna", "comment": "7 pages, 5 figures", "summary": "Unmanned aerial FPV systems demand ultra-low latency, high-reliability communication links. At high speeds and in cluttered environments, Doppler shifts and rapid multipath changes can dramatically raise packet error rates. This paper investigates these phenomena in the context of ExpressLRS (ELRS) long-range FPV control links and demonstrates a novel solution: real-time geometry tuning of a circularly polarized helical antenna array. This study integrates Maxwell-equation-based full-wave simulations (via Ansys HFSS) with controlled, blind field trials to validate performance. A new analysis framework incorporates Doppler-induced frequency offset into the antenna's radiation pattern and the system's error model. Compared to a conventional fixed antenna, the adaptive helical array shows a 20-30% PER reduction when drones exceed 150 mph. The adaptive system automatically adjusts coil pitch and diameter to retune the antenna as flight parameters (velocity, attitude) change. Measured VSWR stays near unity, preventing transmitter reflection spikes. RSSI variation is reduced by half, indicating stronger link stability in urban multi-path. A regression analysis confirms that the reduction in PER due to tuning is highly statistically significant. Calibration data and error analyses are provided to validate our methodology. These findings advance the understanding of high-mobility UAV communication channels and demonstrate that reconfigurable hardware-here, mechanically tunable helices-can effectively counter Doppler and multi-path impairments. The findings inform new design principles for UAV antenna arrays and suggest a path toward AI-integrated adaptive RF systems for drone swarms and racing platforms."}
{"id": "2506.13833", "categories": ["cs.SD", "cs.AI", "cs.RO", "eess.AS", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2506.13833", "abs": "https://arxiv.org/abs/2506.13833", "authors": ["Xiaoliang Chen", "Le Chang", "Xin Yu", "Yunhe Huang", "Xianling Tu"], "title": "A Survey on World Models Grounded in Acoustic Physical Information", "comment": "28 pages,11 equations", "summary": "This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal \"intuitive physics\" engine through sound."}
{"id": "2506.13969", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13969", "abs": "https://arxiv.org/abs/2506.13969", "authors": ["Vsevolod Vladimirovich Deriushkin"], "title": "Set theoretic solution for the tuning problem", "comment": null, "summary": "In this paper I want to suggest a new solution to the problem of musical tuning. On one hand, I see it as a generalization of Just Intonation (JI) to inharmonic timbers, on another, as a unification of spectral interference and harmonicity contributions to consonance within a single framework. The main achievement of the work is the ability to mathematically quantify the phenomenon of musical consonance using set theory. That quantification is done by defining two measures of consonance: affinity and harmonicity. These measures naturally generate sets of intervals that can be used as dynamic tuning systems. The paper is aimed at a broad audience of people who may not be skilled in music and tuning theory or mathematics. Thus, I attempt to give as much details and explanations as I can, while keeping the number of pages as low as possible."}
{"id": "2506.14165", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14165", "abs": "https://arxiv.org/abs/2506.14165", "authors": ["Zhong Yang", "Zhengqiu Zhu", "Yong Zhao", "Yonglin Tian", "Changjun Fan", "Runkang Guo", "Wenhao Lu", "Jingwei Ge", "Bin Chen", "Yin Zhang", "Guohua Wu", "Rui Wang", "Gyorgy Eigner", "Guangquan Cheng", "Jincai Huang", "Zhong Liu", "Jun Zhang", "Imre J. Rudas", "Fei-Yue Wang"], "title": "A Comprehensive Survey on Underwater Acoustic Target Positioning and Tracking: Progress, Challenges, and Perspectives", "comment": null, "summary": "Underwater target tracking technology plays a pivotal role in marine resource exploration, environmental monitoring, and national defense security. Given that acoustic waves represent an effective medium for long-distance transmission in aquatic environments, underwater acoustic target tracking has become a prominent research area of underwater communications and networking. Existing literature reviews often offer a narrow perspective or inadequately address the paradigm shifts driven by emerging technologies like deep learning and reinforcement learning. To address these gaps, this work presents a systematic survey of this field and introduces an innovative multidimensional taxonomy framework based on target scale, sensor perception modes, and sensor collaboration patterns. Within this framework, we comprehensively survey the literature (more than 180 publications) over the period 2016-2025, spanning from the theoretical foundations to diverse algorithmic approaches in underwater acoustic target tracking. Particularly, we emphasize the transformative potential and recent advancements of machine learning techniques, including deep learning and reinforcement learning, in enhancing the performance and adaptability of underwater tracking systems. Finally, this survey concludes by identifying key challenges in the field and proposing future avenues based on emerging technologies such as federated learning, blockchain, embodied intelligence, and large models."}
{"id": "2506.13969", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13969", "abs": "https://arxiv.org/abs/2506.13969", "authors": ["Vsevolod Vladimirovich Deriushkin"], "title": "Set theoretic solution for the tuning problem", "comment": null, "summary": "In this paper I want to suggest a new solution to the problem of musical tuning. On one hand, I see it as a generalization of Just Intonation (JI) to inharmonic timbers, on another, as a unification of spectral interference and harmonicity contributions to consonance within a single framework. The main achievement of the work is the ability to mathematically quantify the phenomenon of musical consonance using set theory. That quantification is done by defining two measures of consonance: affinity and harmonicity. These measures naturally generate sets of intervals that can be used as dynamic tuning systems. The paper is aimed at a broad audience of people who may not be skilled in music and tuning theory or mathematics. Thus, I attempt to give as much details and explanations as I can, while keeping the number of pages as low as possible."}
{"id": "2506.13970", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13970", "abs": "https://arxiv.org/abs/2506.13970", "authors": ["Charles C Onu"], "title": "Making deep neural networks work for medical audio: representation, compression and domain adaptation", "comment": "PhD Thesis", "summary": "This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.\n  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.\n  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare."}
{"id": "2506.14254", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14254", "abs": "https://arxiv.org/abs/2506.14254", "authors": ["Jingreng Lei", "Yang Li", "Zeyi Ren", "Qingfeng Lin", "Ziyue Wang", "Ya-Feng Liu", "Yik-Chung Wu"], "title": "Distributed Activity Detection for Cell-Free Hybrid Near-Far Field Communications", "comment": null, "summary": "A great amount of endeavor has recently been devoted to activity detection for massive machine-type communications in cell-free massive MIMO. However, in practice, as the number of antennas at the access points (APs) increases, the Rayleigh distance that separates the near-field and far-field regions also expands, rendering the conventional assumption of far-field propagation alone impractical. To address this challenge, this paper considers a hybrid near-far field activity detection in cell-free massive MIMO, and establishes a covariance-based formulation, which facilitates the development of a distributed algorithm to alleviate the computational burden at the central processing unit (CPU). Specifically, each AP performs local activity detection for the devices and then transmits the detection result to the CPU for further processing. In particular, a novel coordinate descent algorithm based on the Sherman-Morrison-Woodbury update with Taylor expansion is proposed to handle the local detection problem at each AP. Moreover, we theoretically analyze how the hybrid near-far field channels affect the detection performance. Simulation results validate the theoretical analysis and demonstrate the superior performance of the proposed approach compared with existing approaches."}
{"id": "2506.13970", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.13970", "abs": "https://arxiv.org/abs/2506.13970", "authors": ["Charles C Onu"], "title": "Making deep neural networks work for medical audio: representation, compression and domain adaptation", "comment": "PhD Thesis", "summary": "This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.\n  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.\n  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare."}
{"id": "2506.14148", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14148", "abs": "https://arxiv.org/abs/2506.14148", "authors": ["Long-Vu Hoang", "Tuan Nguyen", "Tran Huy Dat"], "title": "Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment", "comment": "Accepted to Interspeech 2025", "summary": "This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries."}
{"id": "2506.14311", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14311", "abs": "https://arxiv.org/abs/2506.14311", "authors": ["Zexin Fang", "Bin Han", "Wenwen Chen", "Hans D. Schotten"], "title": "Lightweight Node Selection in Hexagonal Grid Topology for TDoA-Based UAV Localization", "comment": "Submitted to GLOBECOM 2025 WKSHPS", "summary": "This paper investigates the optimization problem for TDoA-based UAV localization in low-altitude urban environments with hexagonal grid node deployment. We derive a lightweight optimized node selection strategy based on only RSSI measurements, to pre-select optimal nodes, avoiding extensive TDoA measurements in energy-constrained UAV scenarios. Theoretical and simulation results demonstrate that dynamically selecting the number of reference nodes improves localization performance while minimizing resource overhead."}
{"id": "2506.14148", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14148", "abs": "https://arxiv.org/abs/2506.14148", "authors": ["Long-Vu Hoang", "Tuan Nguyen", "Tran Huy Dat"], "title": "Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment", "comment": "Accepted to Interspeech 2025", "summary": "This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries."}
{"id": "2506.14153", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14153", "abs": "https://arxiv.org/abs/2506.14153", "authors": ["Tuan Dat Phuong", "Long-Vu Hoang", "Huy Dat Tran"], "title": "Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models", "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection."}
{"id": "2506.14385", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14385", "abs": "https://arxiv.org/abs/2506.14385", "authors": ["Amy S. Inwood", "Peter J. Smith", "Mahmoud AlaaEldin", "Michail Matthaiou"], "title": "Performance Characterization of Continuous Reconfigurable Intelligent Surfaces", "comment": null, "summary": "We consider a reconfigurable intelligent surface (RIS) that can implement a phase rotation continuously over the whole surface rather than via a finite number of discrete elements. Such an RIS can be considered a design for future systems where advances in metamaterials make such an implementation feasible or as the limiting case where the number of elements in a traditional RIS increases in a given area. We derive the optimal RIS design for the single-user (SU) scenario assuming a line-of-sight (LoS) from the RIS to the base station (BS) and correlated Rayleigh fading for the other links. We also derive the associated optimal signal-to-noise ratio (SNR) and its mean, a bound on the mean spectral efficiency (SE), an approximation to the SNR outage probability and an approximation to the coefficient of variation for the investigation of channel hardening."}
{"id": "2506.14153", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14153", "abs": "https://arxiv.org/abs/2506.14153", "authors": ["Tuan Dat Phuong", "Long-Vu Hoang", "Huy Dat Tran"], "title": "Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models", "comment": "Accepted to Interspeech 2025", "summary": "Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection."}
{"id": "2506.14223", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14223", "abs": "https://arxiv.org/abs/2506.14223", "authors": ["Anna Hamberger", "Sebastian Murgul", "Jochen Schmidt", "Michael Heizmann"], "title": "Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription", "comment": "Accepted to the 50th International Computer Music Conference (ICMC), 2025", "summary": "Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription."}
{"id": "2506.14557", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14557", "abs": "https://arxiv.org/abs/2506.14557", "authors": ["Yang Luo", "Arunprakash Jayaprakash", "Gaojie Chen", "Chong Huang", "Qu Luo", "De Mi", "Pei Xiao"], "title": "Widely Linear Augmented Extreme Learning Machine Based Impairments Compensation for Satellite Communications", "comment": "12 pages, accepted for pulication in IEEE Transactions on Vehicular Technology", "summary": "Satellite communications are crucial for the evolution beyond fifth-generation networks. However, the dynamic nature of satellite channels and their inherent impairments present significant challenges. In this paper, a novel post-compensation scheme that combines the complex-valued extreme learning machine with augmented hidden layer (CELMAH) architecture and widely linear processing (WLP) is developed to address these issues by exploiting signal impropriety in satellite communications. Although CELMAH shares structural similarities with WLP, it employs a different core algorithm and does not fully exploit the signal impropriety. By incorporating WLP principles, we derive a tailored formulation suited to the network structure and propose the CELM augmented by widely linear least squares (CELM-WLLS) for post-distortion. The proposed approach offers enhanced communication robustness and is highly effective for satellite communication scenarios characterized by dynamic channel conditions and non-linear impairments. CELM-WLLS is designed to improve signal recovery performance and outperform traditional methods such as least square (LS) and minimum mean square error (MMSE). Compared to CELMAH, CELM-WLLS demonstrates approximately 0.8 dB gain in BER performance, and also achieves a two-thirds reduction in computational complexity, making it a more efficient solution."}
{"id": "2506.14223", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14223", "abs": "https://arxiv.org/abs/2506.14223", "authors": ["Anna Hamberger", "Sebastian Murgul", "Jochen Schmidt", "Michael Heizmann"], "title": "Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription", "comment": "Accepted to the 50th International Computer Music Conference (ICMC), 2025", "summary": "Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription."}
{"id": "2506.14226", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.14226", "abs": "https://arxiv.org/abs/2506.14226", "authors": ["Yiyang Zhao", "Shuai Wang", "Guangzhi Sun", "Zehua Chen", "Chao Zhang", "Mingxing Xu", "Thomas Fang Zheng"], "title": "Investigation of Zero-shot Text-to-Speech Models for Enhancing Short-Utterance Speaker Verification", "comment": null, "summary": "Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research."}
{"id": "2506.14571", "categories": ["eess.SP", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14571", "abs": "https://arxiv.org/abs/2506.14571", "authors": ["Venkatakrishnan Vaidyanathapuram Krishnan", "Nathaniel Condit-Schultz"], "title": "The Perception of Phase Intercept Distortion and its Application in Data Augmentation", "comment": "Submitted to the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "summary": "Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks."}
{"id": "2506.14293", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14293", "abs": "https://arxiv.org/abs/2506.14293", "authors": ["Tawsif Ahmed", "Andrej Radonjic", "Gollam Rabby"], "title": "SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling", "comment": null, "summary": "We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists."}
{"id": "2506.14293", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14293", "abs": "https://arxiv.org/abs/2506.14293", "authors": ["Tawsif Ahmed", "Andrej Radonjic", "Gollam Rabby"], "title": "SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling", "comment": null, "summary": "We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists."}
{"id": "2506.14636", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14636", "abs": "https://arxiv.org/abs/2506.14636", "authors": ["Qingqing Wu", "Ziyuan Zheng", "Ying Gao", "Weidong Mei", "Xin Wei", "Wen Chen", "Boyu Ning"], "title": "Integrating Movable Antennas and Intelligent Reflecting Surfaces (MA-IRS): Fundamentals, Practical Solutions, and Opportunities", "comment": "8 pages, 6 figures, submitted for submitted to IEEE Magazines for possible publications", "summary": "Movable antennas (MAs) and intelligent reflecting surfaces (IRSs) enable active antenna repositioning and passive phase-shift tuning for channel reconfiguration, respectively. Integrating MAs and IRSs boosts spatial degrees of freedom, significantly enhancing wireless network capacity, coverage, and reliability. In this article, we first present the fundamentals of MA-IRS integration, involving clarifying the key design issues, revealing performance gain, and identifying the conditions where MA-IRS synergy persists. Then, we examine practical challenges and propose pragmatic design solutions, including optimization schemes, hardware architectures, deployment strategies, and robust designs for hardware impairments and mobility management. In addition, we highlight how MA-IRS architectures uniquely support advanced integrated sensing and communication, enhancing sensing performance and dual-functional flexibility. Overall, MA-IRS integration emerges as a compelling approach toward next-generation reconfigurable wireless systems."}
{"id": "2506.14434", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14434", "abs": "https://arxiv.org/abs/2506.14434", "authors": ["Bidisha Sharma", "Karthik Pandia Durai", "Shankar Venkatesan", "Jeena J Prakash", "Shashi Kumar", "Malolan Chetlur", "Andreas Stolcke"], "title": "Unifying Streaming and Non-streaming Zipformer-based ASR", "comment": "Accepted in ACL2025 Industry track", "summary": "There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements."}
{"id": "2506.14396", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.14396", "abs": "https://arxiv.org/abs/2506.14396", "authors": ["Jiayi He", "Jiangyan Yi", "Jianhua Tao", "Siding Zeng", "Hao Gu"], "title": "Manipulated Regions Localization For Partially Deepfake Audio: A Survey", "comment": null, "summary": "With the development of audio deepfake techniques, attacks with partially deepfake audio are beginning to rise. Compared to fully deepfake, it is much harder to be identified by the detector due to the partially cryptic manipulation, resulting in higher security risks. Although some studies have been launched, there is no comprehensive review to systematically introduce the current situations and development trends for addressing this issue. Thus, in this survey, we are the first to outline a systematic introduction for partially deepfake audio manipulated region localization tasks, including the fundamentals, branches of existing methods, current limitations and potential trends, providing a revealing insight into this scope."}
{"id": "2506.14722", "categories": ["eess.SP", "cs.IT", "math.PR", "physics.app-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2506.14722", "abs": "https://arxiv.org/abs/2506.14722", "authors": ["Ankitha E Bangera"], "title": "A stochastic noise model based excess noise factor expressions for staircase avalanche photodiodes", "comment": "This article derives the excess noise factor expressions for staircase APDs using a stochastic noise model (18 pages, 4 figures, preprint under submission) and can be used as a replacement to Capasso's expressions. Capasso's formula and its equivalents are incorrect, detailed in our article titled \"Mathematical proof of errors in Capasso's excess noise factor formula... .\"", "summary": "Multistep staircase avalanche photodiodes (APDs) are the solid-state analogue of photomultiplier tubes, owing to their deterministic amplification with twofold stepwise gain via impact ionization. Yet, the stepwise impact ionization irregularities worsen with increasing step counts, which are a major source of internal noise in these APDs. Some noise models for staircase APDs have been previously reported, where the excess noise factor expressions are based on Friis' noise factor formula for cascade networks, erroneously considering the power gains as the gains. Excess noise factor being a key component in staircase APDs' noise models, we formulate generalized excess noise factor expressions for multilayer graded-bandgap APDs in terms of their layer-wise ionization probabilities, applicable for all operating biases, which include the sub-threshold, staircase, and tunnelling breakdown regimes. We further derive simplified expressions for staircase APDs and prove that these expressions match Bangera's corrections to Friis' noise factor formulas for cascade networks."}
{"id": "2506.14503", "categories": ["cs.SD", "cs.DL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14503", "abs": "https://arxiv.org/abs/2506.14503", "authors": ["Baris Bozkurt"], "title": "An Open Research Dataset of the 1932 Cairo Congress of Arab Music", "comment": "14 pages, 4 figures, 4 tables", "summary": "This paper introduces ORD-CC32 , an open research dataset derived from the 1932 Cairo Congress of Arab Music recordings, a historically significant collection representing diverse Arab musical traditions. The dataset includes structured metadata, melodic and rhythmic mode tags (maqam and iqa), manually labeled tonic information, and acoustic features extracted using state-of-the-art pitch detection methods. These resources support computational studies of tuning, temperament, and regional variations in Arab music. A case study using pitch histograms demonstrates the potential for data-driven analysis of microtonal differences across regions. By making this dataset openly available, we aim to enable interdisciplinary research in computational ethnomusicology, music information retrieval (MIR), cultural studies, and digital heritage preservation. ORD-CC32 is shared on Zenodo with tools for feature extraction and metadata retrieval."}
{"id": "2506.14398", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.14398", "abs": "https://arxiv.org/abs/2506.14398", "authors": ["Chia-Hua Wu", "Wanying Ge", "Xin Wang", "Junichi Yamagishi", "Yu Tsao", "Hsin-Min Wang"], "title": "A Comparative Study on Proactive and Passive Detection of Deepfake Speech", "comment": null, "summary": "Solutions for defending against deepfake speech fall into two categories: proactive watermarking models and passive conventional deepfake detectors. While both address common threats, their differences in training, optimization, and evaluation prevent a unified protocol for joint evaluation and selecting the best solutions for different cases. This work proposes a framework to evaluate both model types in deepfake speech detection. To ensure fair comparison and minimize discrepancies, all models were trained and tested on common datasets, with performance evaluated using a shared metric. We also analyze their robustness against various adversarial attacks, showing that different models exhibit distinct vulnerabilities to different speech attribute distortions. Our training and evaluation code is available at Github."}
{"id": "2506.14571", "categories": ["eess.SP", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14571", "abs": "https://arxiv.org/abs/2506.14571", "authors": ["Venkatakrishnan Vaidyanathapuram Krishnan", "Nathaniel Condit-Schultz"], "title": "The Perception of Phase Intercept Distortion and its Application in Data Augmentation", "comment": "Submitted to the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "summary": "Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks."}
{"id": "2506.14434", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14434", "abs": "https://arxiv.org/abs/2506.14434", "authors": ["Bidisha Sharma", "Karthik Pandia Durai", "Shankar Venkatesan", "Jeena J Prakash", "Shashi Kumar", "Malolan Chetlur", "Andreas Stolcke"], "title": "Unifying Streaming and Non-streaming Zipformer-based ASR", "comment": "Accepted in ACL2025 Industry track", "summary": "There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements."}
{"id": "2506.14503", "categories": ["cs.SD", "cs.DL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14503", "abs": "https://arxiv.org/abs/2506.14503", "authors": ["Baris Bozkurt"], "title": "An Open Research Dataset of the 1932 Cairo Congress of Arab Music", "comment": "14 pages, 4 figures, 4 tables", "summary": "This paper introduces ORD-CC32 , an open research dataset derived from the 1932 Cairo Congress of Arab Music recordings, a historically significant collection representing diverse Arab musical traditions. The dataset includes structured metadata, melodic and rhythmic mode tags (maqam and iqa), manually labeled tonic information, and acoustic features extracted using state-of-the-art pitch detection methods. These resources support computational studies of tuning, temperament, and regional variations in Arab music. A case study using pitch histograms demonstrates the potential for data-driven analysis of microtonal differences across regions. By making this dataset openly available, we aim to enable interdisciplinary research in computational ethnomusicology, music information retrieval (MIR), cultural studies, and digital heritage preservation. ORD-CC32 is shared on Zenodo with tools for feature extraction and metadata retrieval."}
{"id": "2506.14504", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.14504", "abs": "https://arxiv.org/abs/2506.14504", "authors": ["Emmanuel Deruty"], "title": "Evolving music theory for emerging musical languages", "comment": "In Music 2025, Innovation in Music Conference. 20-22 June, 2025, Bath Spa University, Bath, UK", "summary": "This chapter reconsiders the concept of pitch in contemporary popular music (CPM), particularly in electronic contexts where traditional assumptions may fail. Drawing on phenomenological and inductive methods, it argues that pitch is not an ontologically objective property but a perceptual construct shaped by listeners and conditions. Analyses of quasi-harmonic tones reveal that a single tone can convey multiple pitches, giving rise to tonal fission. The perception of pitch may also be multistable, varying for the same listener over time. In this framework, the tuning system may emerge from a tone's internal structure. A parallel with the coastline paradox supports a model of pitch grounded in perceptual variability, challenging inherited theoretical norms."}
{"id": "2506.14684", "categories": ["cs.SD", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.14684", "abs": "https://arxiv.org/abs/2506.14684", "authors": ["Aditya Bhattacharjee", "Ivan Meresman Higgs", "Mark Sandler", "Emmanouil Benetos"], "title": "Refining music sample identification with a self-supervised graph neural network", "comment": "Accepted at International Conference for Music Information Retrieval (ISMIR) 2025", "summary": "Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.\n  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.\n  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work."}
{"id": "2506.14723", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14723", "abs": "https://arxiv.org/abs/2506.14723", "authors": ["Yusong Wu", "Tim Cooijmans", "Kyle Kastner", "Adam Roberts", "Ian Simon", "Alexander Scarlatos", "Chris Donahue", "Cassie Tarakajian", "Shayegan Omidshafiei", "Aaron Courville", "Pablo Samuel Castro", "Natasha Jaques", "Cheng-Zhi Anna Huang"], "title": "Adaptive Accompaniment with ReaLchords", "comment": "Accepted by ICML 2024", "summary": "Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \\emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities."}
{"id": "2506.14750", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14750", "abs": "https://arxiv.org/abs/2506.14750", "authors": ["Gaobin Yang", "Maokui He", "Shutong Niu", "Ruoyu Wang", "Hang Chen", "Jun Du"], "title": "Exploring Speaker Diarization with Mixture of Experts", "comment": null, "summary": "In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios."}
{"id": "2506.14204", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.14204", "abs": "https://arxiv.org/abs/2506.14204", "authors": ["Aswin Shanmugam Subramanian", "Amit Das", "Naoyuki Kanda", "Jinyu Li", "Xiaofei Wang", "Yifan Gong"], "title": "Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios", "comment": "Accepted to Interspeech 2025", "summary": "We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions."}

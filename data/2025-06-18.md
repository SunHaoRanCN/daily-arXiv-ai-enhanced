<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Dynamically Tunable Helical Antenna](https://arxiv.org/abs/2506.14065)
*Ethan Chien,Jan Steckel*

Main category: eess.SP

TL;DR: 本文研究了高速无人机FPV系统中的通信问题，提出了一种实时几何调谐的圆极化螺旋天线阵列解决方案，显著降低了包错误率。


<details>
  <summary>Details</summary>
Motivation: 高速无人机在复杂环境中飞行时，多普勒频移和多径效应会导致通信链路的高包错误率，需要一种新的解决方案来提升通信可靠性。

Method: 结合Maxwell方程的全波仿真（Ansys HFSS）和盲场试验，开发了一种实时调谐螺旋天线阵列的方法，并分析了多普勒频移对天线辐射模式的影响。

Result: 与传统固定天线相比，自适应螺旋天线阵列在无人机速度超过150 mph时，包错误率降低了20-30%，且VSWR接近1，RSSI波动减少一半。

Conclusion: 研究表明，可重构硬件（如机械调谐螺旋天线）能有效应对多普勒和多径效应，为无人机天线阵列设计提供了新思路，并为AI集成的自适应射频系统奠定了基础。

Abstract: Unmanned aerial FPV systems demand ultra-low latency, high-reliability communication links. At high speeds and in cluttered environments, Doppler shifts and rapid multipath changes can dramatically raise packet error rates. This paper investigates these phenomena in the context of ExpressLRS (ELRS) long-range FPV control links and demonstrates a novel solution: real-time geometry tuning of a circularly polarized helical antenna array. This study integrates Maxwell-equation-based full-wave simulations (via Ansys HFSS) with controlled, blind field trials to validate performance. A new analysis framework incorporates Doppler-induced frequency offset into the antenna's radiation pattern and the system's error model. Compared to a conventional fixed antenna, the adaptive helical array shows a 20-30% PER reduction when drones exceed 150 mph. The adaptive system automatically adjusts coil pitch and diameter to retune the antenna as flight parameters (velocity, attitude) change. Measured VSWR stays near unity, preventing transmitter reflection spikes. RSSI variation is reduced by half, indicating stronger link stability in urban multi-path. A regression analysis confirms that the reduction in PER due to tuning is highly statistically significant. Calibration data and error analyses are provided to validate our methodology. These findings advance the understanding of high-mobility UAV communication channels and demonstrate that reconfigurable hardware-here, mechanically tunable helices-can effectively counter Doppler and multi-path impairments. The findings inform new design principles for UAV antenna arrays and suggest a path toward AI-integrated adaptive RF systems for drone swarms and racing platforms.

</details>


### [2] [A Comprehensive Survey on Underwater Acoustic Target Positioning and Tracking: Progress, Challenges, and Perspectives](https://arxiv.org/abs/2506.14165)
*Zhong Yang,Zhengqiu Zhu,Yong Zhao,Yonglin Tian,Changjun Fan,Runkang Guo,Wenhao Lu,Jingwei Ge,Bin Chen,Yin Zhang,Guohua Wu,Rui Wang,Gyorgy Eigner,Guangquan Cheng,Jincai Huang,Zhong Liu,Jun Zhang,Imre J. Rudas,Fei-Yue Wang*

Main category: eess.SP

TL;DR: 本文系统综述了水下声学目标跟踪技术，提出了基于目标尺度、传感器感知模式和协作模式的多维分类框架，并重点探讨了深度学习与强化学习等新兴技术的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋资源开发、环境监测和国防安全至关重要，但现有综述视角狭窄，未能充分涵盖新兴技术带来的范式转变。

Method: 通过多维分类框架系统梳理2016-2025年180多篇文献，涵盖理论基础与算法方法，特别关注机器学习技术的应用。

Result: 研究发现深度学习与强化学习显著提升了水下跟踪系统的性能与适应性。

Conclusion: 文章总结了领域关键挑战，并基于联邦学习、区块链等新兴技术提出了未来研究方向。

Abstract: Underwater target tracking technology plays a pivotal role in marine resource exploration, environmental monitoring, and national defense security. Given that acoustic waves represent an effective medium for long-distance transmission in aquatic environments, underwater acoustic target tracking has become a prominent research area of underwater communications and networking. Existing literature reviews often offer a narrow perspective or inadequately address the paradigm shifts driven by emerging technologies like deep learning and reinforcement learning. To address these gaps, this work presents a systematic survey of this field and introduces an innovative multidimensional taxonomy framework based on target scale, sensor perception modes, and sensor collaboration patterns. Within this framework, we comprehensively survey the literature (more than 180 publications) over the period 2016-2025, spanning from the theoretical foundations to diverse algorithmic approaches in underwater acoustic target tracking. Particularly, we emphasize the transformative potential and recent advancements of machine learning techniques, including deep learning and reinforcement learning, in enhancing the performance and adaptability of underwater tracking systems. Finally, this survey concludes by identifying key challenges in the field and proposing future avenues based on emerging technologies such as federated learning, blockchain, embodied intelligence, and large models.

</details>


### [3] [Distributed Activity Detection for Cell-Free Hybrid Near-Far Field Communications](https://arxiv.org/abs/2506.14254)
*Jingreng Lei,Yang Li,Zeyi Ren,Qingfeng Lin,Ziyue Wang,Ya-Feng Liu,Yik-Chung Wu*

Main category: eess.SP

TL;DR: 本文提出了一种基于协方差的分布式算法，用于解决大规模MIMO中混合近远场活动检测问题，并通过仿真验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着天线数量增加，传统的远场传播假设不再适用，需要解决混合近远场活动检测的挑战。

Method: 采用协方差建模，提出基于Sherman-Morrison-Woodbury更新和泰勒展开的坐标下降算法，实现分布式处理。

Result: 理论分析了混合近远场信道对检测性能的影响，仿真显示所提方法优于现有方法。

Conclusion: 所提出的分布式算法在混合近远场活动检测中表现出色，适用于大规模MIMO系统。

Abstract: A great amount of endeavor has recently been devoted to activity detection for massive machine-type communications in cell-free massive MIMO. However, in practice, as the number of antennas at the access points (APs) increases, the Rayleigh distance that separates the near-field and far-field regions also expands, rendering the conventional assumption of far-field propagation alone impractical. To address this challenge, this paper considers a hybrid near-far field activity detection in cell-free massive MIMO, and establishes a covariance-based formulation, which facilitates the development of a distributed algorithm to alleviate the computational burden at the central processing unit (CPU). Specifically, each AP performs local activity detection for the devices and then transmits the detection result to the CPU for further processing. In particular, a novel coordinate descent algorithm based on the Sherman-Morrison-Woodbury update with Taylor expansion is proposed to handle the local detection problem at each AP. Moreover, we theoretically analyze how the hybrid near-far field channels affect the detection performance. Simulation results validate the theoretical analysis and demonstrate the superior performance of the proposed approach compared with existing approaches.

</details>


### [4] [Lightweight Node Selection in Hexagonal Grid Topology for TDoA-Based UAV Localization](https://arxiv.org/abs/2506.14311)
*Zexin Fang,Bin Han,Wenwen Chen,Hans D. Schotten*

Main category: eess.SP

TL;DR: 论文研究了低空城市环境中基于TDoA的无人机定位优化问题，提出了一种轻量级节点选择策略，仅依赖RSSI测量预选最优节点，以减少能量受限场景下的TDoA测量开销。


<details>
  <summary>Details</summary>
Motivation: 在低空城市环境中，无人机定位面临能量和资源限制的挑战，需要一种高效且轻量化的节点选择方法。

Method: 提出基于RSSI测量的轻量级优化节点选择策略，动态选择参考节点数量。

Result: 理论和仿真结果表明，动态选择参考节点数量能提升定位性能，同时最小化资源开销。

Conclusion: 该策略为能量受限的无人机定位提供了一种高效解决方案。

Abstract: This paper investigates the optimization problem for TDoA-based UAV localization in low-altitude urban environments with hexagonal grid node deployment. We derive a lightweight optimized node selection strategy based on only RSSI measurements, to pre-select optimal nodes, avoiding extensive TDoA measurements in energy-constrained UAV scenarios. Theoretical and simulation results demonstrate that dynamically selecting the number of reference nodes improves localization performance while minimizing resource overhead.

</details>


### [5] [Performance Characterization of Continuous Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2506.14385)
*Amy S. Inwood,Peter J. Smith,Mahmoud AlaaEldin,Michail Matthaiou*

Main category: eess.SP

TL;DR: 论文研究了连续相位旋转的可重构智能表面（RIS），推导了单用户场景下的最优设计及相关性能指标。


<details>
  <summary>Details</summary>
Motivation: 探索未来系统中基于超材料的连续相位旋转RIS设计，以及传统RIS元素数量增加的极限情况。

Method: 假设RIS到基站为视距链路，其他链路为相关瑞利衰落，推导了最优RIS设计及相关性能指标。

Result: 得到了最优信噪比（SNR）、平均SNR、频谱效率（SE）的界限、SNR中断概率近似值以及信道硬化研究的变异系数近似值。

Conclusion: 连续相位旋转的RIS设计在单用户场景下具有潜在优势，为未来系统提供了理论支持。

Abstract: We consider a reconfigurable intelligent surface (RIS) that can implement a phase rotation continuously over the whole surface rather than via a finite number of discrete elements. Such an RIS can be considered a design for future systems where advances in metamaterials make such an implementation feasible or as the limiting case where the number of elements in a traditional RIS increases in a given area. We derive the optimal RIS design for the single-user (SU) scenario assuming a line-of-sight (LoS) from the RIS to the base station (BS) and correlated Rayleigh fading for the other links. We also derive the associated optimal signal-to-noise ratio (SNR) and its mean, a bound on the mean spectral efficiency (SE), an approximation to the SNR outage probability and an approximation to the coefficient of variation for the investigation of channel hardening.

</details>


### [6] [Widely Linear Augmented Extreme Learning Machine Based Impairments Compensation for Satellite Communications](https://arxiv.org/abs/2506.14557)
*Yang Luo,Arunprakash Jayaprakash,Gaojie Chen,Chong Huang,Qu Luo,De Mi,Pei Xiao*

Main category: eess.SP

TL;DR: 本文提出了一种结合CELMAH架构和WLP的新型后补偿方案CELM-WLLS，用于解决卫星通信中的动态信道和非线性损伤问题，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星通信的动态性和固有损伤对5G后网络发展构成挑战，需开发更高效的信号恢复方法。

Method: 结合CELMAH和WLP，提出CELM-WLLS方案，利用信号非纯性进行后失真处理。

Result: CELM-WLLS在BER性能上比CELMAH提升约0.8 dB，计算复杂度降低三分之二。

Conclusion: CELM-WLLS是一种高效且稳健的卫星通信解决方案，优于传统方法。

Abstract: Satellite communications are crucial for the evolution beyond fifth-generation networks. However, the dynamic nature of satellite channels and their inherent impairments present significant challenges. In this paper, a novel post-compensation scheme that combines the complex-valued extreme learning machine with augmented hidden layer (CELMAH) architecture and widely linear processing (WLP) is developed to address these issues by exploiting signal impropriety in satellite communications. Although CELMAH shares structural similarities with WLP, it employs a different core algorithm and does not fully exploit the signal impropriety. By incorporating WLP principles, we derive a tailored formulation suited to the network structure and propose the CELM augmented by widely linear least squares (CELM-WLLS) for post-distortion. The proposed approach offers enhanced communication robustness and is highly effective for satellite communication scenarios characterized by dynamic channel conditions and non-linear impairments. CELM-WLLS is designed to improve signal recovery performance and outperform traditional methods such as least square (LS) and minimum mean square error (MMSE). Compared to CELMAH, CELM-WLLS demonstrates approximately 0.8 dB gain in BER performance, and also achieves a two-thirds reduction in computational complexity, making it a more efficient solution.

</details>


### [7] [The Perception of Phase Intercept Distortion and its Application in Data Augmentation](https://arxiv.org/abs/2506.14571)
*Venkatakrishnan Vaidyanathapuram Krishnan,Nathaniel Condit-Schultz*

Main category: eess.SP

TL;DR: 本文研究了相位截断失真，一种频率无关的相位偏移引起的特殊相位失真，假设其虽显著改变信号波形但不可感知。实验支持这一假设，并探讨了其在机器学习数据增强中的应用，实验结果显示该方法能提升音频机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 探讨相位截断失真的可感知性及其在机器学习中的应用潜力。

Method: 通过人类被试实验验证相位截断失真的不可感知性，并将其作为数据增强方法应用于音频机器学习任务。

Result: 实验证实相位截断失真不可感知，且作为数据增强方法能提升机器学习性能。

Conclusion: 相位截断失真虽改变波形但不可感知，可用于机器学习数据增强，提升任务效果。

Abstract: Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks.

</details>


### [8] [Integrating Movable Antennas and Intelligent Reflecting Surfaces (MA-IRS): Fundamentals, Practical Solutions, and Opportunities](https://arxiv.org/abs/2506.14636)
*Qingqing Wu,Ziyuan Zheng,Ying Gao,Weidong Mei,Xin Wei,Wen Chen,Boyu Ning*

Main category: eess.SP

TL;DR: 论文探讨了可移动天线（MAs）和智能反射面（IRSs）的集成，通过主动天线重定位和被动相位调整增强无线网络性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合MAs和IRSs，提升空间自由度，显著增强无线网络的容量、覆盖范围和可靠性。

Method: 分析了MA-IRS集成的关键设计问题、性能增益及协同条件，并提出了优化方案、硬件架构、部署策略和鲁棒设计。

Result: MA-IRS集成显著提升了无线系统性能，并支持高级集成感知与通信功能。

Conclusion: MA-IRS集成是下一代可重构无线系统的有前景的解决方案。

Abstract: Movable antennas (MAs) and intelligent reflecting surfaces (IRSs) enable active antenna repositioning and passive phase-shift tuning for channel reconfiguration, respectively. Integrating MAs and IRSs boosts spatial degrees of freedom, significantly enhancing wireless network capacity, coverage, and reliability. In this article, we first present the fundamentals of MA-IRS integration, involving clarifying the key design issues, revealing performance gain, and identifying the conditions where MA-IRS synergy persists. Then, we examine practical challenges and propose pragmatic design solutions, including optimization schemes, hardware architectures, deployment strategies, and robust designs for hardware impairments and mobility management. In addition, we highlight how MA-IRS architectures uniquely support advanced integrated sensing and communication, enhancing sensing performance and dual-functional flexibility. Overall, MA-IRS integration emerges as a compelling approach toward next-generation reconfigurable wireless systems.

</details>


### [9] [A stochastic noise model based excess noise factor expressions for staircase avalanche photodiodes](https://arxiv.org/abs/2506.14722)
*Ankitha E Bangera*

Main category: eess.SP

TL;DR: 本文分析了多层阶梯雪崩光电二极管（APD）的噪声模型，提出了适用于所有偏置条件的广义超额噪声因子表达式，并简化了阶梯APD的表达式。


<details>
  <summary>Details</summary>
Motivation: 阶梯APD的逐步电离不规则性随步数增加而加剧，是内部噪声的主要来源。现有噪声模型基于Friis公式，但存在错误。

Method: 提出广义超额噪声因子表达式，基于层间电离概率，适用于所有偏置条件（亚阈值、阶梯和隧穿击穿）。

Result: 推导了阶梯APD的简化表达式，并证明其与Bangera对Friis公式的修正一致。

Conclusion: 新表达式更准确地描述了阶梯APD的噪声特性，为噪声模型提供了更通用的理论基础。

Abstract: Multistep staircase avalanche photodiodes (APDs) are the solid-state analogue of photomultiplier tubes, owing to their deterministic amplification with twofold stepwise gain via impact ionization. Yet, the stepwise impact ionization irregularities worsen with increasing step counts, which are a major source of internal noise in these APDs. Some noise models for staircase APDs have been previously reported, where the excess noise factor expressions are based on Friis' noise factor formula for cascade networks, erroneously considering the power gains as the gains. Excess noise factor being a key component in staircase APDs' noise models, we formulate generalized excess noise factor expressions for multilayer graded-bandgap APDs in terms of their layer-wise ionization probabilities, applicable for all operating biases, which include the sub-threshold, staircase, and tunnelling breakdown regimes. We further derive simplified expressions for staircase APDs and prove that these expressions match Bangera's corrections to Friis' noise factor formulas for cascade networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
*Andrew Chang,Chenkai Hu,Ji Qi,Zhuojian Wei,Kexin Zhang,Viswadruth Akkaraju,David Poeppel,Dustin Freeman*

Main category: eess.AS

TL;DR: 论文提出了一种半监督学习方法，用于预测视频会议中的负面体验时刻，显著减少了标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: 视频会议中的负面体验（如流畅性或愉悦感丧失）研究不足，且自然数据中这些时刻稀少，标注成本高。

Method: 采用半监督学习（SSL），结合标注和未标注的多模态（音频、面部、文本）数据训练深度特征。

Result: 模态融合的协同训练SSL在ROC-AUC和F1分数上优于监督学习模型，仅需8%标注数据即可达到96%的全数据性能。

Conclusion: 该方法为视频会议体验建模提供了一种高效的标注框架。

Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.

</details>


### [11] [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/abs/2506.14204)
*Aswin Shanmugam Subramanian,Amit Das,Naoyuki Kanda,Jinyu Li,Xiaofei Wang,Yifan Gong*

Main category: eess.AS

TL;DR: 论文扩展了SOT框架，以平衡流式和离线ASR应用的延迟与准确性，提出CSS前端、双模型和segSOT改进。


<details>
  <summary>Details</summary>
Motivation: 解决流式和离线ASR应用中延迟与准确性的平衡问题，满足实时字幕和摘要需求。

Method: 1. 使用CSS单通道前端与E2E系统结合处理重叠语音；2. 实现流式（Conformer Transducer）与离线（Sequence-to-Sequence）双模型或两阶段模型；3. 探索segSOT以优化离线场景和多说话者转录。

Result: CSS框架提升ASR系统准确性，双模型和segSOT分别优化流式与离线性能。

Conclusion: 提出的方法有效平衡了ASR应用的延迟与准确性，适用于多种场景。

Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.

</details>


### [12] [M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization Dataset](https://arxiv.org/abs/2506.14427)
*Shilong Wu,Hang Chen,Jun Du*

Main category: eess.AS

TL;DR: 论文提出了一种自动化构建说话人日志数据集的方法，并发布了一个多模态、多场景、多语言的M3SD数据集。同时，提出了一种场景相关的模型微调策略，通过Adapter和LoRA联合微调实现模型领域适应。


<details>
  <summary>Details</summary>
Motivation: 解决说话人日志领域数据资源不足和深度学习模型泛化能力差的问题。

Method: 1. 通过音视频结合生成伪标签，自动化构建数据集；2. 提出场景相关微调策略，结合目标场景数据，使用Adapter和LoRA联合微调。

Result: 发布了M3SD数据集，并通过微调策略实现了模型的领域适应。

Conclusion: 提出的方法和数据集有效解决了数据不足和模型泛化问题，为说话人日志领域提供了实用工具。

Abstract: In the field of speaker diarization, the development of technology is constrained by two problems: insufficient data resources and poor generalization ability of deep learning models. To address these two problems, firstly, we propose an automated method for constructing speaker diarization datasets, which generates more accurate pseudo-labels for massive data through the combination of audio and video. Relying on this method, we have released Multi-modal, Multi-scenario and Multi-language Speaker Diarization (M3SD) datasets. This dataset is derived from real network videos and is highly diverse. In addition, we further propose a scenario-related model fine-tuning strategy. Based on the general model pre-trained using the above dataset, we combine the specific data of the target scenario (e.g., meetings) and achieve targeted optimization by using Adapter and LoRA joint fine-tuning, thus achieving the model's domain adaptation. Our dataset and code have been open-sourced at https://huggingface.co/spaces/OldDragon/m3sd.

</details>


### [13] [ASAP-FE: Energy-Efficient Feature Extraction Enabling Multi-Channel Keyword Spotting on Edge Processors](https://arxiv.org/abs/2506.14657)
*Jongin Choi,Jina Park,Woojoo Lee,Jae-Jin Lee,Massoud Pedram*

Main category: eess.AS

TL;DR: ASAP-FE是一种面向硬件的多通道关键词检测前端，通过半重叠IIR帧、稀疏感知数据缩减和动态并行处理，显著降低计算和能耗需求，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 多通道关键词检测在边缘环境中面临高计算和能耗挑战，需要高效解决方案。

Method: 采用半重叠IIR帧减少冗余数据，稀疏感知数据缩减进一步降低数据量，动态并行处理优化执行效率。

Result: ASAP-FE平均减少62.73%工作量，支持32通道实时处理，精度下降小于1%。

Conclusion: ASAP-FE为边缘设备提供高效、实用的多通道关键词检测解决方案。

Abstract: Multi-channel keyword spotting (KWS) has become crucial for voice-based applications in edge environments. However, its substantial computational and energy requirements pose significant challenges. We introduce ASAP-FE (Agile Sparsity-Aware Parallelized-Feature Extractor), a hardware-oriented front-end designed to address these challenges. Our framework incorporates three key innovations: (1) Half-overlapped Infinite Impulse Response (IIR) Framing: This reduces redundant data by approximately 25% while maintaining essential phoneme transition cues. (2) Sparsity-aware Data Reduction: We exploit frame-level sparsity to achieve an additional 50% data reduction by combining frame skipping with stride-based filtering. (3) Dynamic Parallel Processing: We introduce a parameterizable filter cluster and a priority-based scheduling algorithm that allows parallel execution of IIR filtering tasks, reducing latency and optimizing energy efficiency. ASAP-FE is implemented with various filter cluster sizes on edge processors, with functionality verified on FPGA prototypes and designs synthesized at 45 nm. Experimental results using TC-ResNet8, DS-CNN, and KWT-1 demonstrate that ASAP-FE reduces the average workload by 62.73% while supporting real-time processing for up to 32 channels. Compared to a conventional fully overlapped baseline, ASAP-FE achieves less than a 1% accuracy drop (e.g., 96.22% vs. 97.13% for DS-CNN), which is well within acceptable limits for edge AI. By adjusting the number of filter modules, our design optimizes the trade-off between performance and energy, with 15 parallel filters providing optimal performance for up to 25 channels. Overall, ASAP-FE offers a practical and efficient solution for multi-channel KWS on energy-constrained edge devices.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [14] [A Survey on World Models Grounded in Acoustic Physical Information](https://arxiv.org/abs/2506.13833)
*Xiaoliang Chen,Le Chang,Xin Yu,Yunhe Huang,Xianling Tu*

Main category: cs.SD

TL;DR: 本文综述了基于声学物理信息的世界模型研究，涵盖理论、方法、技术应用及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索声学信号在高保真环境感知、因果物理推理和动态事件预测模拟中的潜力。

Method: 结合物理信息神经网络（PINNs）、生成模型和自监督多模态学习框架。

Result: 声学世界模型在机器人、自动驾驶、医疗和金融等领域有重要应用。

Conclusion: 未来需解决技术和伦理挑战，发展稳健、因果、不确定性感知的声学智能。

Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.

</details>


### [15] [Set theoretic solution for the tuning problem](https://arxiv.org/abs/2506.13969)
*Vsevolod Vladimirovich Deriushkin*

Main category: cs.SD

TL;DR: 提出一种新的音乐调音解决方案，将Just Intonation推广到非谐波音色，并统一频谱干扰与和谐性对协和度的贡献。


<details>
  <summary>Details</summary>
Motivation: 解决音乐调音问题，特别是为非谐波音色提供理论支持，同时简化协和度的量化方法。

Method: 通过集合论定义两种协和度度量：亲和性与和谐性，生成动态调音系统的音程集合。

Result: 成功用数学方法量化音乐协和度，并生成可用于动态调音的音程集合。

Conclusion: 该框架为非专业人士提供了易于理解的音乐协和度量化方法，同时保持简洁性。

Abstract: In this paper I want to suggest a new solution to the problem of musical tuning. On one hand, I see it as a generalization of Just Intonation (JI) to inharmonic timbers, on another, as a unification of spectral interference and harmonicity contributions to consonance within a single framework. The main achievement of the work is the ability to mathematically quantify the phenomenon of musical consonance using set theory. That quantification is done by defining two measures of consonance: affinity and harmonicity. These measures naturally generate sets of intervals that can be used as dynamic tuning systems. The paper is aimed at a broad audience of people who may not be skilled in music and tuning theory or mathematics. Thus, I attempt to give as much details and explanations as I can, while keeping the number of pages as low as possible.

</details>


### [16] [Making deep neural networks work for medical audio: representation, compression and domain adaptation](https://arxiv.org/abs/2506.13970)
*Charles C Onu*

Main category: cs.SD

TL;DR: 论文探讨了机器学习在医学音频信号分析中的技术挑战，重点研究了婴儿哭声预测医疗状况的方法，包括低数据环境下的迁移学习、模型压缩、领域适应技术，并发布了一个开源数据集。


<details>
  <summary>Details</summary>
Motivation: 医学音频信号（如肺音、心音和语音）蕴含重要健康信息，但目前主要依赖专家听觉分析。自动化分析可标准化处理、支持低资源环境筛查，并发现人类难以察觉的细微模式，助力早期诊断。

Method: 1. 利用成人语音数据库通过迁移学习提升婴儿哭声分析模型；2. 提出基于张量分解的端到端循环网络压缩方法；3. 开发音频领域适应的新技术，结合计算机视觉方法；4. 发布开源婴儿哭声数据集。

Result: 实现了高压缩率（数百倍）的轻量级模型，提升了跨领域泛化能力，并提供了首个开源婴儿哭声数据集。

Conclusion: 研究为将婴儿哭声视为生命体征奠定了基础，展示了AI音频监测在推动普惠医疗中的潜力。

Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.

</details>


### [17] [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/abs/2506.14148)
*Long-Vu Hoang,Tuan Nguyen,Tran Huy Dat*

Main category: cs.SD

TL;DR: 本文提出了一种基于声散射的非侵入式物体分类方法，并通过头发评估案例展示了其有效性。利用AI驱动的深度学习声音分类技术，实现了头发类型和湿度的分类，最高准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 探索一种隐私保护、非接触式的分类方法，以替代传统的视觉分类，适用于多种行业。

Method: 通过发射声波并捕捉散射信号，结合四种深度学习策略（全监督、嵌入分类、监督基础模型微调、自监督模型微调）进行分类。

Result: 最佳策略（自监督模型微调）实现了近90%的分类准确率。

Conclusion: 声散射技术为非接触式分类提供了高效、隐私保护的解决方案，具有广泛的应用潜力。

Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.

</details>


### [18] [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/abs/2506.14153)
*Tuan Dat Phuong,Long-Vu Hoang,Huy Dat Tran*

Main category: cs.SD

TL;DR: 提出了一种在XLSR-Conformer模型中用Kolmogorov-Arnold Network（KAN）替代传统MLP的方法，显著提升了合成语音检测性能。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术的进步导致伪造攻击日益复杂，现有基于自监督学习（SSL）的检测系统仍有改进空间。

Method: 在XLSR-Conformer模型中用KAN替代传统MLP，利用Kolmogorov-Arnold表示定理改进架构。

Result: 在ASVspoof2021上，相对性能提升60.55%（LA和DF集），21LA集EER降至0.70%。

Conclusion: 将KAN整合到SSL模型中，是提升合成语音检测性能的有前景方向。

Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.

</details>


### [19] [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/abs/2506.14223)
*Anna Hamberger,Sebastian Murgul,Jochen Schmidt,Michael Heizmann*

Main category: cs.SD

TL;DR: Fretting-Transformer利用T5架构，将MIDI自动转录为吉他谱，解决了弦-品模糊性和可演奏性问题，性能优于基线方法和商业应用。


<details>
  <summary>Details</summary>
Motivation: MIDI符号缺乏吉他演奏的关键信息，音乐转录在MIR中尤为重要，尤其是弦乐器如吉他。

Method: 采用T5编码器-解码器模型，将任务视为符号翻译问题，结合数据预处理和标记化策略，使用DadaGP等数据集。

Result: 实验表明，Fretting-Transformer在准确性和可演奏性上优于A*和Guitar Pro等基线方法。

Conclusion: 模型通过上下文敏感处理和调弦/变调夹条件，为未来吉他自动转录奠定了坚实基础。

Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.

</details>


### [20] [Investigation of Zero-shot Text-to-Speech Models for Enhancing Short-Utterance Speaker Verification](https://arxiv.org/abs/2506.14226)
*Yiyang Zhao,Shuai Wang,Guangzhi Sun,Zehua Chen,Chao Zhang,Mingxing Xu,Thomas Fang Zheng*

Main category: cs.SD

TL;DR: 研究探讨了利用零样本文本转语音（ZS-TTS）系统进行说话人验证的数据增强，实验表明合成语音与真实语音结合可显著降低错误率，但长合成语音效果不如长真实语音。


<details>
  <summary>Details</summary>
Motivation: 短语音说话人验证因信息有限而准确性低，ZS-TTS系统在保留说话人身份方面有进展，因此研究其用于数据增强的潜力。

Method: 评估了三种预训练ZS-TTS系统（NatureSpeech 3、CosyVoice、MaskGCT）在VoxCeleb 1数据集上的表现，结合真实与合成语音进行测试。

Result: 结合真实与合成语音样本使相对等错误率（EER）降低10%-16%，尤其对短语音效果显著，但长合成语音效果不如长真实语音。

Conclusion: ZS-TTS在说话人验证中具有潜力，但也面临挑战，为未来研究提供了方向。

Abstract: Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research.

</details>


### [21] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/abs/2506.14293)
*Tawsif Ahmed,Andrej Radonjic,Gollam Rabby*

Main category: cs.SD

TL;DR: Sleeping-DISCO 9M是一个用于音乐生成任务的大规模预训练数据集，填补了开源高质量流行音乐数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开源的高质量流行音乐数据集，现有数据集多为合成或重录音乐，未能反映真实音乐风格。

Method: 数据集基于实际流行音乐和世界知名艺术家的作品构建。

Result: 提供了更符合真实音乐风格的数据集。

Conclusion: Sleeping-DISCO 9M有望推动生成音乐社区的发展。

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.

</details>


### [22] [Unifying Streaming and Non-streaming Zipformer-based ASR](https://arxiv.org/abs/2506.14434)
*Bidisha Sharma,Karthik Pandia Durai,Shankar Venkatesan,Jeena J Prakash,Shashi Kumar,Malolan Chetlur,Andreas Stolcke*

Main category: cs.SD

TL;DR: 提出了一种统一流式和非流式ASR模型的框架，通过动态右上下文和分块注意力掩码训练zipformer模型，显著降低开发成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少流式和非流式ASR模型的开发和部署成本，同时提升模型性能。

Method: 使用动态右上下文和分块注意力掩码训练zipformer模型，分析不同右上下文帧数对精度和延迟的影响。

Result: 在Librispeech和内部数据集上测试，相对词错误率降低7.9%，流式性能接近非流式模型。

Conclusion: 该方法有效统一了流式和非流式ASR模型，提供了灵活的延迟-精度权衡控制。

Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.

</details>


### [23] [Manipulated Regions Localization For Partially Deepfake Audio: A Survey](https://arxiv.org/abs/2506.14396)
*Jiayi He,Jiangyan Yi,Jianhua Tao,Siding Zeng,Hao Gu*

Main category: cs.SD

TL;DR: 本文首次系统综述了部分深度伪造音频的区域定位任务，包括基础、现有方法分支、当前局限和潜在趋势。


<details>
  <summary>Details</summary>
Motivation: 随着音频深度伪造技术的发展，部分伪造音频攻击逐渐增多，因其隐蔽性更难被检测，带来更高安全风险。目前缺乏对此问题的全面综述。

Method: 通过系统梳理部分深度伪造音频区域定位任务的基础、现有方法分支、当前局限和潜在趋势。

Result: 提供了对该领域的深入见解，填补了研究空白。

Conclusion: 本文为部分深度伪造音频的区域定位任务提供了系统性综述，揭示了当前研究的局限和未来发展方向。

Abstract: With the development of audio deepfake techniques, attacks with partially deepfake audio are beginning to rise. Compared to fully deepfake, it is much harder to be identified by the detector due to the partially cryptic manipulation, resulting in higher security risks. Although some studies have been launched, there is no comprehensive review to systematically introduce the current situations and development trends for addressing this issue. Thus, in this survey, we are the first to outline a systematic introduction for partially deepfake audio manipulated region localization tasks, including the fundamentals, branches of existing methods, current limitations and potential trends, providing a revealing insight into this scope.

</details>


### [24] [An Open Research Dataset of the 1932 Cairo Congress of Arab Music](https://arxiv.org/abs/2506.14503)
*Baris Bozkurt*

Main category: cs.SD

TL;DR: ORD-CC32是一个基于1932年开罗阿拉伯音乐大会录音的开放研究数据集，包含结构化元数据、旋律和节奏模式标签、手动标注的主音信息及声学特征，支持阿拉伯音乐调律和区域差异的计算研究。


<details>
  <summary>Details</summary>
Motivation: 通过开放数据集促进计算民族音乐学、音乐信息检索、文化研究和数字遗产保护等跨学科研究。

Method: 数据集包括手动标注的元数据、旋律和节奏模式标签，以及使用先进音高检测方法提取的声学特征。

Result: 案例研究展示了通过音高直方图分析区域微音差异的潜力。

Conclusion: ORD-CC32的开放共享为跨学科研究提供了重要资源。

Abstract: This paper introduces ORD-CC32 , an open research dataset derived from the 1932 Cairo Congress of Arab Music recordings, a historically significant collection representing diverse Arab musical traditions. The dataset includes structured metadata, melodic and rhythmic mode tags (maqam and iqa), manually labeled tonic information, and acoustic features extracted using state-of-the-art pitch detection methods. These resources support computational studies of tuning, temperament, and regional variations in Arab music. A case study using pitch histograms demonstrates the potential for data-driven analysis of microtonal differences across regions. By making this dataset openly available, we aim to enable interdisciplinary research in computational ethnomusicology, music information retrieval (MIR), cultural studies, and digital heritage preservation. ORD-CC32 is shared on Zenodo with tools for feature extraction and metadata retrieval.

</details>


### [25] [A Comparative Study on Proactive and Passive Detection of Deepfake Speech](https://arxiv.org/abs/2506.14398)
*Chia-Hua Wu,Wanying Ge,Xin Wang,Junichi Yamagishi,Yu Tsao,Hsin-Min Wang*

Main category: cs.SD

TL;DR: 提出一个框架，用于统一评估主动水印模型和被动深度伪造检测器在深度伪造语音检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案分为主动水印模型和被动检测器，但缺乏统一的评估协议，无法公平比较和选择最佳方案。

Method: 在共同数据集上训练和测试所有模型，使用共享指标评估性能，并分析其对不同对抗攻击的鲁棒性。

Result: 不同模型对不同语音属性失真表现出不同的脆弱性。

Conclusion: 提出的框架为联合评估和选择最佳深度伪造语音检测方案提供了基础。

Abstract: Solutions for defending against deepfake speech fall into two categories: proactive watermarking models and passive conventional deepfake detectors. While both address common threats, their differences in training, optimization, and evaluation prevent a unified protocol for joint evaluation and selecting the best solutions for different cases. This work proposes a framework to evaluate both model types in deepfake speech detection. To ensure fair comparison and minimize discrepancies, all models were trained and tested on common datasets, with performance evaluated using a shared metric. We also analyze their robustness against various adversarial attacks, showing that different models exhibit distinct vulnerabilities to different speech attribute distortions. Our training and evaluation code is available at Github.

</details>


### [26] [Evolving music theory for emerging musical languages](https://arxiv.org/abs/2506.14504)
*Emmanuel Deruty*

Main category: cs.SD

TL;DR: 本文重新探讨了当代流行音乐（CPM）中的音高概念，特别是在电子音乐中传统假设可能失效的背景下。通过现象学和归纳方法，提出音高并非本体论上的客观属性，而是由听众和条件塑造的感知构造。


<details>
  <summary>Details</summary>
Motivation: 传统音高理论在电子音乐等现代音乐形式中可能不适用，因此需要重新思考音高的本质。

Method: 采用现象学和归纳方法，分析准谐波音调，探讨音高的感知特性。

Result: 研究发现单个音调可以传达多个音高，导致音调分裂；音高感知可能是多稳态的，同一听众在不同时间可能有不同感知。音调的内部结构可能决定调音系统。

Conclusion: 音高应被视为基于感知可变性的模型，挑战了传统理论规范。

Abstract: This chapter reconsiders the concept of pitch in contemporary popular music (CPM), particularly in electronic contexts where traditional assumptions may fail. Drawing on phenomenological and inductive methods, it argues that pitch is not an ontologically objective property but a perceptual construct shaped by listeners and conditions. Analyses of quasi-harmonic tones reveal that a single tone can convey multiple pitches, giving rise to tonal fission. The perception of pitch may also be multistable, varying for the same listener over time. In this framework, the tuning system may emerge from a tone's internal structure. A parallel with the coastline paradox supports a model of pitch grounded in perceptual variability, challenging inherited theoretical norms.

</details>


### [27] [Refining music sample identification with a self-supervised graph neural network](https://arxiv.org/abs/2506.14684)
*Aditya Bhattacharjee,Ivan Meresman Higgs,Mark Sandler,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 提出了一种轻量级、可扩展的编码架构，结合图神经网络和对比学习框架，用于自动样本识别（ASID），在减少参数量的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决音频样本识别中因音乐制作变换（如时间拉伸、音高变换等）导致的识别困难问题。

Method: 采用两阶段方法：粗粒度相似性搜索筛选候选样本，再通过交叉注意力分类器优化排序。

Result: 模型参数仅为当前最优系统的9%，但性能相当，平均精度（mAP）达44.2%。

Conclusion: 该方法在轻量化和性能上取得平衡，并公开了新的数据集标注以支持短查询任务。

Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.

</details>


### [28] [Adaptive Accompaniment with ReaLchords](https://arxiv.org/abs/2506.14723)
*Yusong Wu,Tim Cooijmans,Kyle Kastner,Adam Roberts,Ian Simon,Alexander Scarlatos,Chris Donahue,Cassie Tarakajian,Shayegan Omidshafiei,Aaron Courville,Pablo Samuel Castro,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: ReaLchords是一个在线生成模型，用于即兴伴奏用户旋律，通过强化学习微调以实现实时协作。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型无法实时与其他音乐家协作，ReaLchords旨在填补这一空白，支持实时即兴创作。

Method: 结合最大似然预训练和强化学习微调，利用新颖的奖励模型和未来旋律蒸馏技术。

Result: 模型能适应陌生输入并生成合适的伴奏，通过定量实验和听测验证。

Conclusion: ReaLchords为实时协作创作开辟了新途径，适用于多种模态。

Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.

</details>


### [29] [Exploring Speaker Diarization with Mixture of Experts](https://arxiv.org/abs/2506.14750)
*Gaobin Yang,Maokui He,Shutong Niu,Ruoyu Wang,Hang Chen,Jun Du*

Main category: cs.SD

TL;DR: 提出了一种基于记忆感知多说话人嵌入和序列到序列架构的神经说话人日志系统（NSD-MS2S），并引入共享软专家混合模块（SS-MoE）扩展为NSD-MS2S-SSMoE，显著提升了鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决说话人日志任务中模型偏差和性能提升的挑战。

Method: 结合记忆感知多说话人嵌入模块与序列到序列架构，并引入SS-MoE模块。

Result: 在多个复杂数据集上取得最先进结果，表现出更强的鲁棒性和泛化能力。

Conclusion: 提出的方法在真实场景中表现优异，为说话人日志任务提供了有效解决方案。

Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.

</details>

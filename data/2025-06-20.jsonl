{"id": "2506.14864", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14864", "abs": "https://arxiv.org/abs/2506.14864", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "title": "pycnet-audio: A Python package to support bioacoustics data processing", "comment": null, "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that leverages recent improvements in purpose-made automated recording units (ARUs). The general approach is to deploy ARUs in the field to record on a programmed schedule for extended periods (weeks or months), after which the audio data are retrieved. These data must then be processed, typically either by measuring or analyzing characteristics of the audio itself (e.g. calculating acoustic indices), or by searching for some signal of interest within the recordings, e.g. vocalizations or other sounds produced by some target species, anthropogenic or environmental noise, etc. In the latter case, some method is required to locate the signal(s) of interest within the audio. While very small datasets can simply be searched manually, even modest projects can produce audio datasets on the order of 105 hours of recordings, making manual review impractical and necessitating some form of automated detection. pycnet-audio (Ruff 2024) is intended to provide a practical processing workflow for acoustic data, built around the PNW-Cnet model, which was initially developed by the U.S. Forest Service to support population monitoring of northern spotted owls (Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins 2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of ca. 80 forest wildlife species and numerous forms of anthropogenic and environmental noise (Ruff et al. 2021, 2023)."}
{"id": "2506.15000", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15000", "abs": "https://arxiv.org/abs/2506.15000", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "comment": null, "summary": "Speech enhancement, particularly denoising, is vital in improving the intelligibility and quality of speech signals for real-world applications, especially in noisy environments. While prior research has introduced various deep learning models for this purpose, many struggle to balance noise suppression, perceptual quality, and speaker-specific feature preservation, leaving a critical research gap in their comparative performance evaluation. This study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and U-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These models were chosen due to their relevance in the literature and code accessibility. The evaluation reveals that U-Net achieves high noise suppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and +364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality, attaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it well-suited for applications prioritizing natural and intelligible speech. Wave-U-Net balances these attributes with improvements in speaker-specific feature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and +27.38% on VPQAD. This research indicates how advanced methods can optimize trade-offs between noise suppression, perceptual quality, and speaker recognition. The findings may contribute to advancing voice biometrics, forensic audio analysis, telecommunication, and speaker verification in challenging acoustic conditions."}
{"id": "2506.15029", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15029", "abs": "https://arxiv.org/abs/2506.15029", "authors": ["Prateek Mehta", "Anasuya Patil"], "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "comment": "9 pages, 9 figures", "summary": "Knowledge extraction through sound is a distinctive property. Visually impaired individuals often rely solely on Braille books and audio recordings provided by NGOs. Due to limitations in these approaches, blind individuals often cannot access books of their choice. Speech is a more effective mode of communication than text for blind and visually impaired persons, as they can easily respond to sounds. This paper presents the development of an accurate, reliable, cost-effective, and user-friendly optical character recognition (OCR)-based speech synthesis system. The OCR-based system has been implemented using Laboratory Virtual Instrument Engineering Workbench (LabVIEW)."}
{"id": "2506.15154", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15154", "abs": "https://arxiv.org/abs/2506.15154", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "summary": "Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions."}
{"id": "2506.14906", "categories": ["eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.14906", "abs": "https://arxiv.org/abs/2506.14906", "authors": ["Robert Czupryniak", "Abhishek Chakraborty", "Andrew N. Jordan", "John C. Howell"], "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder", "comment": null, "summary": "We apply machine learning methods to demonstrate range superresolution in remote sensing radar detection. Specifically, we implement a denoising autoencoder to estimate the distance between two equal intensity scatterers in the subwavelength regime. The machine learning models are trained on waveforms subject to a bandlimit constraint such that ranges much smaller than the inverse bandlimit are optimized in their precision. The autoencoder achieves effective dimensionality reduction, with the bottleneck layer exhibiting a strong and consistent correlation with the true scatterer separation. We confirm reproducibility across different training sessions and network initializations by analyzing the scaled encoder outputs and their robustness to noise. We investigate the behavior of the bottleneck layer for the following types of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse, and a theoretically near-optimal pulse created from a spherical Bessel function basis. The Bessel signal performs best, followed by the triangle wave, with the sinc signal performing worst, highlighting the crucial role of signal design in the success of machine-learning-based range resolution."}
{"id": "2506.14973", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14973", "abs": "https://arxiv.org/abs/2506.14973", "authors": ["Jiamin Xie", "Ju Lin", "Yiteng Huang", "Tyler Vuong", "Zhaojiang Lin", "Zhaojun Yang", "Peng Su", "Prashant Rawat", "Sangeeta Srivastava", "Ming Sun", "Florian Metze"], "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition", "comment": "Accepted to Interspeech 2025", "summary": "Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech recognition capabilities. However, the ability of Speech LLMs to comprehend and process multi-channel audio with spatial cues remains a relatively uninvestigated area of research. In this work, we present directional-SpeechLlama, a novel approach that leverages the microphone array of smart glasses to achieve directional speech recognition, source localization, and bystander cross-talk suppression. To enhance the model's ability to understand directivity, we propose two key techniques: serialized directional output training (S-DOT) and contrastive direction data augmentation (CDDA). Experimental results show that our proposed directional-SpeechLlama effectively captures the relationship between textual cues and spatial audio, yielding strong performance in both speech recognition and source localization tasks."}
{"id": "2506.15514", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15514", "abs": "https://arxiv.org/abs/2506.15514", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "comment": "Accepted at 2025 ICME Workshop AI for Music", "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field of music information retrieval, despite great advances in automatic speech recognition (ASR) brought about by transformer-based architectures in recent years. One of the major challenges in ALT is the high amplitude of interfering audio signals relative to conventional ASR due to musical accompaniment. Recent advances in music source separation have enabled automatic extraction of high-quality separated vocals, which could potentially improve ALT performance. However, the effect of source separation has not been systematically investigated in order to establish best practices for its use. This work examines the impact of source separation on ALT using Whisper, a state-of-the-art open source ASR model. We evaluate Whisper's performance on original audio, separated vocals, and vocal stems across short-form and long-form transcription tasks. For short-form, we suggest a concatenation method that results in a consistent reduction in Word Error Rate (WER). For long-form, we propose an algorithm using source separation as a vocal activity detector to derive segment boundaries, which results in a consistent reduction in WER relative to Whisper's native long-form algorithm. Our approach achieves state-of-the-art results for an open source system on the Jam-ALT long-form ALT benchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the first dataset of long-form lyric transcripts following the Jam-ALT guidelines for which vocal stems are publicly available."}
{"id": "2506.14985", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14985", "abs": "https://arxiv.org/abs/2506.14985", "authors": ["Kuranage Roche Rayan Ranasinghe", "Hyeon Seok Rou", "Iván Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "George C. Alexandropoulos"], "title": "Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization", "comment": "Author's version of chapter from forthcoming book \"Reconfigurable Metasurfaces for Wireless Communications: Architectures, Modeling, and Optimization\"", "summary": "The doubly-dispersive (DD) channel structure has played a pivotal role in wireless communications, particularly in high-mobility scenarios and integrated sensing and communications (ISAC), due to its ability to capture the key fading effects experienced by a transmitted signal as it propagates through a dynamic medium. However, extending the DD framework to multiple-input multiple-output (MIMO) systems, especially in environments artificially enhanced by reconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces (SIM), remains a challenging open problem. In this chapter, a novel metasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary number of RISs, while also incorporating SIM at both the transmitter and receiver is introduced. Next, the application of this model to some key waveforms optimized for DD environments -- namely orthogonal frequency division multiplexing (OFDM), orthogonal time frequency space (OTFS), and affine frequency division multiplexing (AFDM) -- is discussed. Finally, the programmability of the proposed model is highlighted through an illustrative application, demonstrating its potential for enhancing waveform performance in SIM-assisted wireless systems."}
{"id": "2506.15456", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15456", "abs": "https://arxiv.org/abs/2506.15456", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "comment": "Accepted to Interspeech 2025", "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks."}
{"id": "2506.15530", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15530", "abs": "https://arxiv.org/abs/2506.15530", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "comment": null, "summary": "Breakthroughs in text-to-music generation models are transforming the creative landscape, equipping musicians with innovative tools for composition and experimentation like never before. However, controlling the generation process to achieve a specific desired outcome remains a significant challenge. Even a minor change in the text prompt, combined with the same random seed, can drastically alter the generated piece. In this paper, we explore the application of existing text-to-music diffusion models for instrument editing. Specifically, for an existing audio track, we aim to leverage a pretrained text-to-music diffusion model to edit the instrument while preserving the underlying content. Based on the insight that the model first focuses on the overall structure or content of the audio, then adds instrument information, and finally refines the quality, we show that selecting a well-chosen intermediate timestep, identified through an instrument classifier, yields a balance between preserving the original piece's content and achieving the desired timbre. Our method does not require additional training of the text-to-music diffusion model, nor does it compromise the generation process's speed."}
{"id": "2506.14992", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14992", "abs": "https://arxiv.org/abs/2506.14992", "authors": ["Zhihao Tao", "Athina P. Petropulu"], "title": "Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks", "comment": null, "summary": "We propose a novel directional modulation (DM) design for OFDM transmitters aided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS is configured to preserve the integrity of transmitted signals toward multiple legitimate users while scrambling the signal in all other directions. Existing TM-IRS design methods typically target a single user direction and follow predefined rule-based procedures, making them unsuitable for multi-user scenarios. Here, we propose a generative AI-based approach to design good sets of TM-IRS parameters out of a set of all possible quantized ranges of parameters. The design objective is to maximize the sum rate across the authorized directions. We model the TM-IRS parameter selection as a deterministic Markov decision process (MDP), where each terminal state corresponds to a specific configuration of TM-IRS parameters. GFlowNets are employed to learn a stochastic policy that samples TM-IRS parameter sets with probability proportional to their associated sum rate reward. Experimental results demonstrate that the proposed method effectively enhances the security of the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size of the TM-IRS configuration space, the GFlowNet is able to converge after training on fewer than 0.000001% of all possible configurations, demonstrating remarkable efficiency compared to exhaustive combinatorial search. Implementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to facilitate reproducibility."}
{"id": "2506.14864", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14864", "abs": "https://arxiv.org/abs/2506.14864", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "title": "pycnet-audio: A Python package to support bioacoustics data processing", "comment": null, "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that leverages recent improvements in purpose-made automated recording units (ARUs). The general approach is to deploy ARUs in the field to record on a programmed schedule for extended periods (weeks or months), after which the audio data are retrieved. These data must then be processed, typically either by measuring or analyzing characteristics of the audio itself (e.g. calculating acoustic indices), or by searching for some signal of interest within the recordings, e.g. vocalizations or other sounds produced by some target species, anthropogenic or environmental noise, etc. In the latter case, some method is required to locate the signal(s) of interest within the audio. While very small datasets can simply be searched manually, even modest projects can produce audio datasets on the order of 105 hours of recordings, making manual review impractical and necessitating some form of automated detection. pycnet-audio (Ruff 2024) is intended to provide a practical processing workflow for acoustic data, built around the PNW-Cnet model, which was initially developed by the U.S. Forest Service to support population monitoring of northern spotted owls (Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins 2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of ca. 80 forest wildlife species and numerous forms of anthropogenic and environmental noise (Ruff et al. 2021, 2023)."}
{"id": "2506.15548", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15548", "abs": "https://arxiv.org/abs/2506.15548", "authors": ["Junyan Jiang", "Daniel Chin", "Liwei Lin", "Xuanjie Liu", "Gus Xia"], "title": "Versatile Symbolic Music-for-Music Modeling via Function Alignment", "comment": null, "summary": "Many music AI models learn a map between music content and human-defined labels. However, many annotations, such as chords, can be naturally expressed within the music modality itself, e.g., as sequences of symbolic notes. This observation enables both understanding tasks (e.g., chord recognition) and conditional generation tasks (e.g., chord-conditioned melody generation) to be unified under a music-for-music sequence modeling paradigm. In this work, we propose parameter-efficient solutions for a variety of symbolic music-for-music tasks. The high-level idea is that (1) we utilize a pretrained Language Model (LM) for both the reference and the target sequence and (2) we link these two LMs via a lightweight adapter. Experiments show that our method achieves superior performance among different tasks such as chord recognition, melody generation, and drum track generation. All demos, code and model weights are publicly available."}
{"id": "2506.15125", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15125", "abs": "https://arxiv.org/abs/2506.15125", "authors": ["Linlin Wang", "Wei Wang", "Dezhao Wang", "Shanwen Wang"], "title": "Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks", "comment": "15 pages, 10 figures", "summary": "With the applicability of optical fiber-based distributed acoustic sensing (DAS) systems, effective signal processing and analysis approaches are needed to promote its popularization in the field of intelligent transportation systems (ITS). This paper presents a signal denoising algorithm using a hybrid deep-learning network (HDLNet). Without annotated data and time-consuming labeling, this self-supervised network runs in parallel, combining an autoencoder for denoising (DAE) and a long short-term memory (LSTM) for sequential processing. Additionally, a line-by-line matching algorithm for vehicle detection and tracking is introduced, thus realizing the complete processing of fiber signal denoising and feature extraction. Experiments were carried out on a self-established real highway tunnel dataset, showing that our proposed hybrid network yields more satisfactory denoising performance than Spatial-domain DAE."}
{"id": "2506.15000", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15000", "abs": "https://arxiv.org/abs/2506.15000", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "comment": null, "summary": "Speech enhancement, particularly denoising, is vital in improving the intelligibility and quality of speech signals for real-world applications, especially in noisy environments. While prior research has introduced various deep learning models for this purpose, many struggle to balance noise suppression, perceptual quality, and speaker-specific feature preservation, leaving a critical research gap in their comparative performance evaluation. This study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and U-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These models were chosen due to their relevance in the literature and code accessibility. The evaluation reveals that U-Net achieves high noise suppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and +364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality, attaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it well-suited for applications prioritizing natural and intelligible speech. Wave-U-Net balances these attributes with improvements in speaker-specific feature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and +27.38% on VPQAD. This research indicates how advanced methods can optimize trade-offs between noise suppression, perceptual quality, and speaker recognition. The findings may contribute to advancing voice biometrics, forensic audio analysis, telecommunication, and speaker verification in challenging acoustic conditions."}
{"id": "2506.15614", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15614", "abs": "https://arxiv.org/abs/2506.15614", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "title": "TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data", "comment": null, "summary": "This paper presents TTSOps, a fully automated closed-loop framework for constructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated web-scale speech data, often referred to as ``dark data,'' such as online videos. Conventional TTS training pipelines require well-curated corpora with high acoustic quality and accurate text-speech alignment, which severely limits scalability, speaker diversity, and real-world applicability. While recent studies have proposed acoustic-quality-based data selection techniques, they often overlook two critical aspects: (1) the inherent robustness of modern TTS models to noise, and (2) the potential contribution of perceptually low-quality yet informative samples. To address these issues, TTSOps introduces a data-centric training pipeline that integrates three core components: (1) automated data collection from dark data sources, (2) utterance-level dynamic selection of data cleansing methods based on training data quality, and (3) evaluation-in-the-loop data selection using automatically predicted mean opinion scores (MOS) to estimate each utterance's impact on model performance. Furthermore, TTSOps jointly optimizes the corpus and the TTS model in a closed-loop framework by dynamically adapting both data selection and data cleansing processes to the characteristics of the target TTS model. Extensive experiments on Japanese YouTube data demonstrate that TTSOps outperforms conventional acoustic-quality-based baselines in both the naturalness and speaker diversity of synthesized speech."}
{"id": "2506.15136", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15136", "abs": "https://arxiv.org/abs/2506.15136", "authors": ["Kehui Li", "Binggui Zhou", "Jiajia Guo", "Feifei Gao", "Guanghua Yang", "Shaodan Ma"], "title": "Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead", "comment": null, "summary": "Multi-user millimeter-wave communication relies on narrow beams and dense cell deployments to ensure reliable connectivity. However, tracking optimal beams for multiple mobile users across multiple base stations (BSs) results in significant signaling overhead. Recent works have explored the capability of out-of-band (OOB) modalities in obtaining spatial characteristics of wireless channels and reducing pilot overhead in single-BS single-user/multi-user systems. However, applying OOB modalities for multi-BS selection towards dense cell deployments leads to high coordination overhead, i.e, excessive computing overhead and high latency in data exchange. How to leverage OOB modalities to eliminate pilot overhead and achieve efficient multi-BS coordination in multi-BS systems remains largely unexplored. In this paper, we propose a novel OOB modality synergy (OMS) based mobility management scheme to realize multi-user beam prediction and proactive BS selection by synergizing two OOB modalities, i.e., vision and location. Specifically, mobile users are initially identified via spatial alignment of visual sensing and location feedback, and then tracked according to the temporal correlation in image sequence. Subsequently, a binary encoding map based gain and beam prediction network (BEM-GBPN) is designed to predict beamforming gains and optimal beams for mobile users at each BS, such that a central unit can control the BSs to perform user handoff and beam switching. Simulation results indicate that the proposed OMS-based mobility management scheme enhances beam prediction and BS selection accuracy and enables users to achieve 91% transmission rates of the optimal with zero pilot overhead and significantly improve multi-BS coordination efficiency compared to existing methods."}
{"id": "2506.15029", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15029", "abs": "https://arxiv.org/abs/2506.15029", "authors": ["Prateek Mehta", "Anasuya Patil"], "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "comment": "9 pages, 9 figures", "summary": "Knowledge extraction through sound is a distinctive property. Visually impaired individuals often rely solely on Braille books and audio recordings provided by NGOs. Due to limitations in these approaches, blind individuals often cannot access books of their choice. Speech is a more effective mode of communication than text for blind and visually impaired persons, as they can easily respond to sounds. This paper presents the development of an accurate, reliable, cost-effective, and user-friendly optical character recognition (OCR)-based speech synthesis system. The OCR-based system has been implemented using Laboratory Virtual Instrument Engineering Workbench (LabVIEW)."}
{"id": "2506.15456", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15456", "abs": "https://arxiv.org/abs/2506.15456", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "comment": "Accepted to Interspeech 2025", "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks."}
{"id": "2506.15148", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15148", "abs": "https://arxiv.org/abs/2506.15148", "authors": ["Yuxuan Xia", "Ángel F. García-Fernández", "Johan Karlsson", "Yu Ge", "Lennart Svensson", "Ting Yuan"], "title": "Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation", "comment": "7 pages, 4 figures", "summary": "This paper presents a generalization of the trajectory general optimal sub-pattern assignment (GOSPA) metric for evaluating multi-object tracking algorithms that provide trajectory estimates with track-level uncertainties. This metric builds on the recently introduced probabilistic GOSPA metric to account for both the existence and state estimation uncertainties of individual object states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a multidimensional assignment problem, and its linear programming relaxation--also a valid metric--is computable in polynomial time. Additionally, this metric retains the interpretability of TGOSPA, and we show that its decomposition yields intuitive costs terms associated to expected localization error and existence probability mismatch error for properly detected objects, expected missed and false detection error, and track switch error. The effectiveness of the proposed metric is demonstrated through a simulation study."}
{"id": "2506.15154", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15154", "abs": "https://arxiv.org/abs/2506.15154", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "summary": "Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions."}
{"id": "2506.15235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15235", "abs": "https://arxiv.org/abs/2506.15235", "authors": ["Taewon Kang", "Seunghyeon Park", "Pyo-Woong Son", "Jiwon Seo"], "title": "Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data", "comment": "Submitted to IEEE Access", "summary": "The vulnerabilities of global navigation satellite systems (GNSS) to signal interference have increased the demand for complementary positioning, navigation, and timing (PNT) systems. To address this, South Korea has decided to deploy an enhanced long-range navigation (eLoran) system as a complementary PNT solution. Similar to GNSS, eLoran provides highly accurate timing information, which is essential for applications such as telecommunications, financial systems, and power distribution. However, the primary sources of error for GNSS and eLoran differ. For eLoran, the main source of error is signal propagation delay over land, known as the additional secondary factor (ASF). This delay, influenced by ground conductivity and weather conditions along the signal path, is challenging to predict and mitigate. In this paper, we measure the time difference (TD) between GPS and eLoran using a time interval counter and analyze the correlations between eLoran/GPS TD and eleven meteorological factors. Accurate estimation of eLoran/GPS TD could enable eLoran to achieve timing accuracy comparable to that of GPS. We propose two estimation models for eLoran/GPS TD and compare their performance with existing TD estimation methods. The proposed WLR-AGRNN model captures the linear relationships between meteorological factors and eLoran/GPS TD using weighted linear regression (WLR) and models nonlinear relationships between outputs from expert networks through an anisotropic general regression neural network (AGRNN). The model incorporates terrain elevation to appropriately weight meteorological data, as elevation influences signal propagation delay. Experimental results based on four months of data demonstrate that the WLR-AGRNN model outperforms other models, highlighting its effectiveness in improving eLoran/GPS TD estimation accuracy."}
{"id": "2506.15514", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15514", "abs": "https://arxiv.org/abs/2506.15514", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "comment": "Accepted at 2025 ICME Workshop AI for Music", "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field of music information retrieval, despite great advances in automatic speech recognition (ASR) brought about by transformer-based architectures in recent years. One of the major challenges in ALT is the high amplitude of interfering audio signals relative to conventional ASR due to musical accompaniment. Recent advances in music source separation have enabled automatic extraction of high-quality separated vocals, which could potentially improve ALT performance. However, the effect of source separation has not been systematically investigated in order to establish best practices for its use. This work examines the impact of source separation on ALT using Whisper, a state-of-the-art open source ASR model. We evaluate Whisper's performance on original audio, separated vocals, and vocal stems across short-form and long-form transcription tasks. For short-form, we suggest a concatenation method that results in a consistent reduction in Word Error Rate (WER). For long-form, we propose an algorithm using source separation as a vocal activity detector to derive segment boundaries, which results in a consistent reduction in WER relative to Whisper's native long-form algorithm. Our approach achieves state-of-the-art results for an open source system on the Jam-ALT long-form ALT benchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the first dataset of long-form lyric transcripts following the Jam-ALT guidelines for which vocal stems are publicly available."}
{"id": "2506.15273", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15273", "abs": "https://arxiv.org/abs/2506.15273", "authors": ["Anup Mishra", "Čedomir Stefanović", "Xiuqiang Xu", "Petar Popovski", "Israel Leyva-Mayorga"], "title": "Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access", "comment": null, "summary": "Flexible and efficient wireless resource sharing across heterogeneous services is a key objective for future wireless networks. In this context, we investigate the performance of a system where latency-constrained internet-of-things (IoT) devices coexist with a broadband user. The base station adopts a grant-free access framework to manage resource allocation, either through orthogonal radio access network (RAN) slicing or by allowing shared access between services. For the IoT users, we propose a reinforcement learning (RL) approach based on double Q-Learning (QL) to optimise their repetition-based transmission strategy, allowing them to adapt to varying levels of interference and meet a predefined latency target. We evaluate the system's performance in terms of the cumulative distribution function of IoT users' latency, as well as the broadband user's throughput and energy efficiency (EE). Our results show that the proposed RL-based access policies significantly enhance the latency performance of IoT users in both RAN Slicing and RAN Sharing scenarios, while preserving desirable broadband throughput and EE. Furthermore, the proposed policies enable RAN Sharing to be energy-efficient at low IoT traffic levels, and RAN Slicing to be favourable under high IoT traffic."}
{"id": "2506.15530", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15530", "abs": "https://arxiv.org/abs/2506.15530", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "comment": null, "summary": "Breakthroughs in text-to-music generation models are transforming the creative landscape, equipping musicians with innovative tools for composition and experimentation like never before. However, controlling the generation process to achieve a specific desired outcome remains a significant challenge. Even a minor change in the text prompt, combined with the same random seed, can drastically alter the generated piece. In this paper, we explore the application of existing text-to-music diffusion models for instrument editing. Specifically, for an existing audio track, we aim to leverage a pretrained text-to-music diffusion model to edit the instrument while preserving the underlying content. Based on the insight that the model first focuses on the overall structure or content of the audio, then adds instrument information, and finally refines the quality, we show that selecting a well-chosen intermediate timestep, identified through an instrument classifier, yields a balance between preserving the original piece's content and achieving the desired timbre. Our method does not require additional training of the text-to-music diffusion model, nor does it compromise the generation process's speed."}
{"id": "2506.15338", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15338", "abs": "https://arxiv.org/abs/2506.15338", "authors": ["Islam M. Tanash", "Ayush Kumar Dwivedi", "Taneli Riihonen"], "title": "Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry", "comment": null, "summary": "This paper studies a high-altitude platform (HAP) network supported by reconfigurable intelligent surfaces (RISs). The practical irregular placement of HAPs and RISs is modeled using homogeneous Poisson point processes, while buildings that cause blockages in urban areas are modeled as a Boolean scheme of rectangles. We introduce a novel approach to characterize the statistical channel based on generalized Beta prime distribution. Analytical expressions for coverage probability and ergodic capacity in an interference-limited system are derived and validated through Monte Carlo simulations. The findings show notable performance improvements and reveal the impact of various system parameters, including blockages effect which contribute in mitigating interference from the other visible HAPs. This proposed system could enhance connectivity and enable effective data offloading in urban environments."}
{"id": "2506.15463", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15463", "abs": "https://arxiv.org/abs/2506.15463", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "title": "Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array", "comment": "5 Pages with 6 figures and 1 table", "summary": "In practical systems, recorded analog signals must be digitized for processing, introducing quantization as a critical aspect of data acquisition. While prior studies have examined quantization effects in various signal processing contexts, its impact on differential microphone arrays (DMAs), particularly in one-dimensional (1D) first-order configurations, remains unexplored. This paper investigates the influence of signal quantization on performance of first-order 1D DMAs across various beampatterns. An analytical expression for quantized beamformed output for a first-order 1D DMA has been formulated. The effect of signal quantization has been studied on array performance measures such as the Beampattern, Directivity Factor (DF), Front-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that beampattern shape remains structurally invariant across quantization bit depths, with quantization primarily affecting ND. DF and FBR remain constant with the varying number of quantization bits. Additionally, ND is shown to be frequency-independent; however, it increases with increasing quantization bit depths, enhancing interference suppression. The study also examines the effect of steering nulls across the azimuthal range, showing that ND degrades as the null moves closer to the source look direction, indicating reduced interference suppression."}
{"id": "2506.15470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15470", "abs": "https://arxiv.org/abs/2506.15470", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage", "comment": null, "summary": "With the deployment of large antenna arrays at high frequency bands, future wireless communication systems are likely to operate in the radiative near-field. Unlike far-field beam steering, near-field beams can be focused within a spatial region of finite depth, enabling spatial multiplexing in both the angular and range dimensions. This paper derives the beamdepth for a generalized uniform rectangular array (URA) and investigates how array geometry influences the near-field beamdepth and the limits where near-field beamfocusing is achievable. To characterize the near-field boundary in terms of beamfocusing and spatial multiplexing gains, we define the effective beamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis reveals that while a square URA achieves the narrowest beamdepth, the EBRD is maximized for a wide or tall URA. However, despite its narrow beamdepth, a square URA may experience a reduction in multiuser sum rate due to its severely constrained EBRD. Simulation results confirm that a wide or tall URA achieves a sum rate of 3.5 X more than that of a square URA, benefiting from the extended EBRD and improved spatial multiplexing capabilities."}
{"id": "2506.15670", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15670", "abs": "https://arxiv.org/abs/2506.15670", "authors": ["Özlem Tugfe Demir", "Mustafa Ozger", "Ferdi Kara", "Woong-Hee Lee", "Emil Björnson"], "title": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward", "comment": "7 pages, 5 figures", "summary": "This paper explores the integration of simultaneous wireless information and power transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO) technology operating in the upper mid-band frequency range (7-24 GHz). The near-field propagation achieved by gMIMO introduces unique opportunities for energy-efficient, high-capacity communication systems that cater to the demands of 6G wireless networks. Exploiting spherical wave propagation, near-field SWIPT with gMIMO enables precise energy and data delivery, enhancing spectral efficiency through beamfocusing and massive spatial multiplexing. This paper discusses theoretical principles, design challenges, and enabling solutions, including advanced channel estimation techniques, precoding strategies, and dynamic array configurations such as sparse and modular arrays. Through analytical insights and a case study, this paper demonstrates the feasibility of achieving optimized energy harvesting and data throughput in dense and dynamic environments. These findings contribute to advancing energy-autonomous Internet-of-Everything (IoE) deployments, smart factory networks, and other energy-autonomous applications aligned with the goals of next-generation wireless technologies."}
{"id": "2506.15530", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15530", "abs": "https://arxiv.org/abs/2506.15530", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "comment": null, "summary": "Breakthroughs in text-to-music generation models are transforming the creative landscape, equipping musicians with innovative tools for composition and experimentation like never before. However, controlling the generation process to achieve a specific desired outcome remains a significant challenge. Even a minor change in the text prompt, combined with the same random seed, can drastically alter the generated piece. In this paper, we explore the application of existing text-to-music diffusion models for instrument editing. Specifically, for an existing audio track, we aim to leverage a pretrained text-to-music diffusion model to edit the instrument while preserving the underlying content. Based on the insight that the model first focuses on the overall structure or content of the audio, then adds instrument information, and finally refines the quality, we show that selecting a well-chosen intermediate timestep, identified through an instrument classifier, yields a balance between preserving the original piece's content and achieving the desired timbre. Our method does not require additional training of the text-to-music diffusion model, nor does it compromise the generation process's speed."}

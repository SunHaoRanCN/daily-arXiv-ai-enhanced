{"id": "2506.14973", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14973", "abs": "https://arxiv.org/abs/2506.14973", "authors": ["Jiamin Xie", "Ju Lin", "Yiteng Huang", "Tyler Vuong", "Zhaojiang Lin", "Zhaojun Yang", "Peng Su", "Prashant Rawat", "Sangeeta Srivastava", "Ming Sun", "Florian Metze"], "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition", "comment": "Accepted to Interspeech 2025", "summary": "Recent studies have demonstrated that prompting large language models (LLM)\nwith audio encodings enables effective speech recognition capabilities.\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\naudio with spatial cues remains a relatively uninvestigated area of research.\nIn this work, we present directional-SpeechLlama, a novel approach that\nleverages the microphone array of smart glasses to achieve directional speech\nrecognition, source localization, and bystander cross-talk suppression. To\nenhance the model's ability to understand directivity, we propose two key\ntechniques: serialized directional output training (S-DOT) and contrastive\ndirection data augmentation (CDDA). Experimental results show that our proposed\ndirectional-SpeechLlama effectively captures the relationship between textual\ncues and spatial audio, yielding strong performance in both speech recognition\nand source localization tasks."}
{"id": "2506.15456", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15456", "abs": "https://arxiv.org/abs/2506.15456", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "comment": "Accepted to Interspeech 2025", "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks."}
{"id": "2506.14864", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14864", "abs": "https://arxiv.org/abs/2506.14864", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "title": "pycnet-audio: A Python package to support bioacoustics data processing", "comment": null, "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that\nleverages recent improvements in purpose-made automated recording units (ARUs).\nThe general approach is to deploy ARUs in the field to record on a programmed\nschedule for extended periods (weeks or months), after which the audio data are\nretrieved. These data must then be processed, typically either by measuring or\nanalyzing characteristics of the audio itself (e.g. calculating acoustic\nindices), or by searching for some signal of interest within the recordings,\ne.g. vocalizations or other sounds produced by some target species,\nanthropogenic or environmental noise, etc. In the latter case, some method is\nrequired to locate the signal(s) of interest within the audio. While very small\ndatasets can simply be searched manually, even modest projects can produce\naudio datasets on the order of 105 hours of recordings, making manual review\nimpractical and necessitating some form of automated detection. pycnet-audio\n(Ruff 2024) is intended to provide a practical processing workflow for acoustic\ndata, built around the PNW-Cnet model, which was initially developed by the\nU.S. Forest Service to support population monitoring of northern spotted owls\n(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins\n2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of\nca. 80 forest wildlife species and numerous forms of anthropogenic and\nenvironmental noise (Ruff et al. 2021, 2023)."}
{"id": "2506.15000", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15000", "abs": "https://arxiv.org/abs/2506.15000", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "comment": null, "summary": "Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions."}
{"id": "2506.14864", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.14864", "abs": "https://arxiv.org/abs/2506.14864", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "title": "pycnet-audio: A Python package to support bioacoustics data processing", "comment": null, "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that\nleverages recent improvements in purpose-made automated recording units (ARUs).\nThe general approach is to deploy ARUs in the field to record on a programmed\nschedule for extended periods (weeks or months), after which the audio data are\nretrieved. These data must then be processed, typically either by measuring or\nanalyzing characteristics of the audio itself (e.g. calculating acoustic\nindices), or by searching for some signal of interest within the recordings,\ne.g. vocalizations or other sounds produced by some target species,\nanthropogenic or environmental noise, etc. In the latter case, some method is\nrequired to locate the signal(s) of interest within the audio. While very small\ndatasets can simply be searched manually, even modest projects can produce\naudio datasets on the order of 105 hours of recordings, making manual review\nimpractical and necessitating some form of automated detection. pycnet-audio\n(Ruff 2024) is intended to provide a practical processing workflow for acoustic\ndata, built around the PNW-Cnet model, which was initially developed by the\nU.S. Forest Service to support population monitoring of northern spotted owls\n(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins\n2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of\nca. 80 forest wildlife species and numerous forms of anthropogenic and\nenvironmental noise (Ruff et al. 2021, 2023)."}
{"id": "2506.14906", "categories": ["eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.14906", "abs": "https://arxiv.org/abs/2506.14906", "authors": ["Robert Czupryniak", "Abhishek Chakraborty", "Andrew N. Jordan", "John C. Howell"], "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder", "comment": null, "summary": "We apply machine learning methods to demonstrate range superresolution in\nremote sensing radar detection. Specifically, we implement a denoising\nautoencoder to estimate the distance between two equal intensity scatterers in\nthe subwavelength regime. The machine learning models are trained on waveforms\nsubject to a bandlimit constraint such that ranges much smaller than the\ninverse bandlimit are optimized in their precision. The autoencoder achieves\neffective dimensionality reduction, with the bottleneck layer exhibiting a\nstrong and consistent correlation with the true scatterer separation. We\nconfirm reproducibility across different training sessions and network\ninitializations by analyzing the scaled encoder outputs and their robustness to\nnoise. We investigate the behavior of the bottleneck layer for the following\ntypes of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,\nand a theoretically near-optimal pulse created from a spherical Bessel function\nbasis. The Bessel signal performs best, followed by the triangle wave, with the\nsinc signal performing worst, highlighting the crucial role of signal design in\nthe success of machine-learning-based range resolution."}
{"id": "2506.15029", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS", "14J60", "I.2.7; I.4; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2506.15029", "abs": "https://arxiv.org/abs/2506.15029", "authors": ["Prateek Mehta", "Anasuya Patil"], "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "comment": "9 pages, 9 figures", "summary": "Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW)."}
{"id": "2506.15000", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15000", "abs": "https://arxiv.org/abs/2506.15000", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "comment": null, "summary": "Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions."}
{"id": "2506.14985", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14985", "abs": "https://arxiv.org/abs/2506.14985", "authors": ["Kuranage Roche Rayan Ranasinghe", "Hyeon Seok Rou", "Iván Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "George C. Alexandropoulos"], "title": "Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization", "comment": "Author's version of chapter from forthcoming book \"Reconfigurable\n  Metasurfaces for Wireless Communications: Architectures, Modeling, and\n  Optimization\"", "summary": "The doubly-dispersive (DD) channel structure has played a pivotal role in\nwireless communications, particularly in high-mobility scenarios and integrated\nsensing and communications (ISAC), due to its ability to capture the key fading\neffects experienced by a transmitted signal as it propagates through a dynamic\nmedium. However, extending the DD framework to multiple-input multiple-output\n(MIMO) systems, especially in environments artificially enhanced by\nreconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces\n(SIM), remains a challenging open problem. In this chapter, a novel\nmetasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary\nnumber of RISs, while also incorporating SIM at both the transmitter and\nreceiver is introduced. Next, the application of this model to some key\nwaveforms optimized for DD environments -- namely orthogonal frequency division\nmultiplexing (OFDM), orthogonal time frequency space (OTFS), and affine\nfrequency division multiplexing (AFDM) -- is discussed. Finally, the\nprogrammability of the proposed model is highlighted through an illustrative\napplication, demonstrating its potential for enhancing waveform performance in\nSIM-assisted wireless systems."}
{"id": "2506.15154", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS", "68T10 (Primary), 68T50 (Secondary)", "H.5.5; H.5.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.15154", "abs": "https://arxiv.org/abs/2506.15154", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions."}
{"id": "2506.15029", "categories": ["cs.SD", "cs.CL", "cs.CV", "eess.AS", "14J60", "I.2.7; I.4; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2506.15029", "abs": "https://arxiv.org/abs/2506.15029", "authors": ["Prateek Mehta", "Anasuya Patil"], "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "comment": "9 pages, 9 figures", "summary": "Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW)."}
{"id": "2506.14992", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.14992", "abs": "https://arxiv.org/abs/2506.14992", "authors": ["Zhihao Tao", "Athina P. Petropulu"], "title": "Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks", "comment": null, "summary": "We propose a novel directional modulation (DM) design for OFDM transmitters\naided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS\nis configured to preserve the integrity of transmitted signals toward multiple\nlegitimate users while scrambling the signal in all other directions. Existing\nTM-IRS design methods typically target a single user direction and follow\npredefined rule-based procedures, making them unsuitable for multi-user\nscenarios. Here, we propose a generative AI-based approach to design good sets\nof TM-IRS parameters out of a set of all possible quantized ranges of\nparameters. The design objective is to maximize the sum rate across the\nauthorized directions. We model the TM-IRS parameter selection as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a specific configuration of TM-IRS parameters. GFlowNets are\nemployed to learn a stochastic policy that samples TM-IRS parameter sets with\nprobability proportional to their associated sum rate reward. Experimental\nresults demonstrate that the proposed method effectively enhances the security\nof the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size\nof the TM-IRS configuration space, the GFlowNet is able to converge after\ntraining on fewer than 0.000001% of all possible configurations, demonstrating\nremarkable efficiency compared to exhaustive combinatorial search.\nImplementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to\nfacilitate reproducibility."}
{"id": "2506.15514", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15514", "abs": "https://arxiv.org/abs/2506.15514", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "comment": "Accepted at 2025 ICME Workshop AI for Music", "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field\nof music information retrieval, despite great advances in automatic speech\nrecognition (ASR) brought about by transformer-based architectures in recent\nyears. One of the major challenges in ALT is the high amplitude of interfering\naudio signals relative to conventional ASR due to musical accompaniment. Recent\nadvances in music source separation have enabled automatic extraction of\nhigh-quality separated vocals, which could potentially improve ALT performance.\nHowever, the effect of source separation has not been systematically\ninvestigated in order to establish best practices for its use. This work\nexamines the impact of source separation on ALT using Whisper, a\nstate-of-the-art open source ASR model. We evaluate Whisper's performance on\noriginal audio, separated vocals, and vocal stems across short-form and\nlong-form transcription tasks. For short-form, we suggest a concatenation\nmethod that results in a consistent reduction in Word Error Rate (WER). For\nlong-form, we propose an algorithm using source separation as a vocal activity\ndetector to derive segment boundaries, which results in a consistent reduction\nin WER relative to Whisper's native long-form algorithm. Our approach achieves\nstate-of-the-art results for an open source system on the Jam-ALT long-form ALT\nbenchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the\nfirst dataset of long-form lyric transcripts following the Jam-ALT guidelines\nfor which vocal stems are publicly available."}
{"id": "2506.15154", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS", "68T10 (Primary), 68T50 (Secondary)", "H.5.5; H.5.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.15154", "abs": "https://arxiv.org/abs/2506.15154", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions."}
{"id": "2506.15125", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15125", "abs": "https://arxiv.org/abs/2506.15125", "authors": ["Linlin Wang", "Wei Wang", "Dezhao Wang", "Shanwen Wang"], "title": "Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks", "comment": "15 pages, 10 figures", "summary": "With the applicability of optical fiber-based distributed acoustic sensing\n(DAS) systems, effective signal processing and analysis approaches are needed\nto promote its popularization in the field of intelligent transportation\nsystems (ITS). This paper presents a signal denoising algorithm using a hybrid\ndeep-learning network (HDLNet). Without annotated data and time-consuming\nlabeling, this self-supervised network runs in parallel, combining an\nautoencoder for denoising (DAE) and a long short-term memory (LSTM) for\nsequential processing. Additionally, a line-by-line matching algorithm for\nvehicle detection and tracking is introduced, thus realizing the complete\nprocessing of fiber signal denoising and feature extraction. Experiments were\ncarried out on a self-established real highway tunnel dataset, showing that our\nproposed hybrid network yields more satisfactory denoising performance than\nSpatial-domain DAE."}
{"id": "2506.15530", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15530", "abs": "https://arxiv.org/abs/2506.15530", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "comment": null, "summary": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed."}
{"id": "2506.15514", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.15514", "abs": "https://arxiv.org/abs/2506.15514", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "comment": "Accepted at 2025 ICME Workshop AI for Music", "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field\nof music information retrieval, despite great advances in automatic speech\nrecognition (ASR) brought about by transformer-based architectures in recent\nyears. One of the major challenges in ALT is the high amplitude of interfering\naudio signals relative to conventional ASR due to musical accompaniment. Recent\nadvances in music source separation have enabled automatic extraction of\nhigh-quality separated vocals, which could potentially improve ALT performance.\nHowever, the effect of source separation has not been systematically\ninvestigated in order to establish best practices for its use. This work\nexamines the impact of source separation on ALT using Whisper, a\nstate-of-the-art open source ASR model. We evaluate Whisper's performance on\noriginal audio, separated vocals, and vocal stems across short-form and\nlong-form transcription tasks. For short-form, we suggest a concatenation\nmethod that results in a consistent reduction in Word Error Rate (WER). For\nlong-form, we propose an algorithm using source separation as a vocal activity\ndetector to derive segment boundaries, which results in a consistent reduction\nin WER relative to Whisper's native long-form algorithm. Our approach achieves\nstate-of-the-art results for an open source system on the Jam-ALT long-form ALT\nbenchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the\nfirst dataset of long-form lyric transcripts following the Jam-ALT guidelines\nfor which vocal stems are publicly available."}
{"id": "2506.15136", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15136", "abs": "https://arxiv.org/abs/2506.15136", "authors": ["Kehui Li", "Binggui Zhou", "Jiajia Guo", "Feifei Gao", "Guanghua Yang", "Shaodan Ma"], "title": "Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead", "comment": null, "summary": "Multi-user millimeter-wave communication relies on narrow beams and dense\ncell deployments to ensure reliable connectivity. However, tracking optimal\nbeams for multiple mobile users across multiple base stations (BSs) results in\nsignificant signaling overhead. Recent works have explored the capability of\nout-of-band (OOB) modalities in obtaining spatial characteristics of wireless\nchannels and reducing pilot overhead in single-BS single-user/multi-user\nsystems. However, applying OOB modalities for multi-BS selection towards dense\ncell deployments leads to high coordination overhead, i.e, excessive computing\noverhead and high latency in data exchange. How to leverage OOB modalities to\neliminate pilot overhead and achieve efficient multi-BS coordination in\nmulti-BS systems remains largely unexplored. In this paper, we propose a novel\nOOB modality synergy (OMS) based mobility management scheme to realize\nmulti-user beam prediction and proactive BS selection by synergizing two OOB\nmodalities, i.e., vision and location. Specifically, mobile users are initially\nidentified via spatial alignment of visual sensing and location feedback, and\nthen tracked according to the temporal correlation in image sequence.\nSubsequently, a binary encoding map based gain and beam prediction network\n(BEM-GBPN) is designed to predict beamforming gains and optimal beams for\nmobile users at each BS, such that a central unit can control the BSs to\nperform user handoff and beam switching. Simulation results indicate that the\nproposed OMS-based mobility management scheme enhances beam prediction and BS\nselection accuracy and enables users to achieve 91% transmission rates of the\noptimal with zero pilot overhead and significantly improve multi-BS\ncoordination efficiency compared to existing methods."}
{"id": "2506.15530", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15530", "abs": "https://arxiv.org/abs/2506.15530", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "comment": null, "summary": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed."}
{"id": "2506.15148", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.15148", "abs": "https://arxiv.org/abs/2506.15148", "authors": ["Yuxuan Xia", "Ángel F. García-Fernández", "Johan Karlsson", "Yu Ge", "Lennart Svensson", "Ting Yuan"], "title": "Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation", "comment": "7 pages, 4 figures", "summary": "This paper presents a generalization of the trajectory general optimal\nsub-pattern assignment (GOSPA) metric for evaluating multi-object tracking\nalgorithms that provide trajectory estimates with track-level uncertainties.\nThis metric builds on the recently introduced probabilistic GOSPA metric to\naccount for both the existence and state estimation uncertainties of individual\nobject states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a\nmultidimensional assignment problem, and its linear programming\nrelaxation--also a valid metric--is computable in polynomial time.\nAdditionally, this metric retains the interpretability of TGOSPA, and we show\nthat its decomposition yields intuitive costs terms associated to expected\nlocalization error and existence probability mismatch error for properly\ndetected objects, expected missed and false detection error, and track switch\nerror. The effectiveness of the proposed metric is demonstrated through a\nsimulation study."}
{"id": "2506.15548", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15548", "abs": "https://arxiv.org/abs/2506.15548", "authors": ["Junyan Jiang", "Daniel Chin", "Liwei Lin", "Xuanjie Liu", "Gus Xia"], "title": "Versatile Symbolic Music-for-Music Modeling via Function Alignment", "comment": null, "summary": "Many music AI models learn a map between music content and human-defined\nlabels. However, many annotations, such as chords, can be naturally expressed\nwithin the music modality itself, e.g., as sequences of symbolic notes. This\nobservation enables both understanding tasks (e.g., chord recognition) and\nconditional generation tasks (e.g., chord-conditioned melody generation) to be\nunified under a music-for-music sequence modeling paradigm. In this work, we\npropose parameter-efficient solutions for a variety of symbolic music-for-music\ntasks. The high-level idea is that (1) we utilize a pretrained Language Model\n(LM) for both the reference and the target sequence and (2) we link these two\nLMs via a lightweight adapter. Experiments show that our method achieves\nsuperior performance among different tasks such as chord recognition, melody\ngeneration, and drum track generation. All demos, code and model weights are\npublicly available."}
{"id": "2506.15235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15235", "abs": "https://arxiv.org/abs/2506.15235", "authors": ["Taewon Kang", "Seunghyeon Park", "Pyo-Woong Son", "Jiwon Seo"], "title": "Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data", "comment": "Submitted to IEEE Access", "summary": "The vulnerabilities of global navigation satellite systems (GNSS) to signal\ninterference have increased the demand for complementary positioning,\nnavigation, and timing (PNT) systems. To address this, South Korea has decided\nto deploy an enhanced long-range navigation (eLoran) system as a complementary\nPNT solution. Similar to GNSS, eLoran provides highly accurate timing\ninformation, which is essential for applications such as telecommunications,\nfinancial systems, and power distribution. However, the primary sources of\nerror for GNSS and eLoran differ. For eLoran, the main source of error is\nsignal propagation delay over land, known as the additional secondary factor\n(ASF). This delay, influenced by ground conductivity and weather conditions\nalong the signal path, is challenging to predict and mitigate. In this paper,\nwe measure the time difference (TD) between GPS and eLoran using a time\ninterval counter and analyze the correlations between eLoran/GPS TD and eleven\nmeteorological factors. Accurate estimation of eLoran/GPS TD could enable\neLoran to achieve timing accuracy comparable to that of GPS. We propose two\nestimation models for eLoran/GPS TD and compare their performance with existing\nTD estimation methods. The proposed WLR-AGRNN model captures the linear\nrelationships between meteorological factors and eLoran/GPS TD using weighted\nlinear regression (WLR) and models nonlinear relationships between outputs from\nexpert networks through an anisotropic general regression neural network\n(AGRNN). The model incorporates terrain elevation to appropriately weight\nmeteorological data, as elevation influences signal propagation delay.\nExperimental results based on four months of data demonstrate that the\nWLR-AGRNN model outperforms other models, highlighting its effectiveness in\nimproving eLoran/GPS TD estimation accuracy."}
{"id": "2506.15614", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15614", "abs": "https://arxiv.org/abs/2506.15614", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "title": "TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data", "comment": null, "summary": "This paper presents TTSOps, a fully automated closed-loop framework for\nconstructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated\nweb-scale speech data, often referred to as ``dark data,'' such as online\nvideos. Conventional TTS training pipelines require well-curated corpora with\nhigh acoustic quality and accurate text-speech alignment, which severely limits\nscalability, speaker diversity, and real-world applicability. While recent\nstudies have proposed acoustic-quality-based data selection techniques, they\noften overlook two critical aspects: (1) the inherent robustness of modern TTS\nmodels to noise, and (2) the potential contribution of perceptually low-quality\nyet informative samples. To address these issues, TTSOps introduces a\ndata-centric training pipeline that integrates three core components: (1)\nautomated data collection from dark data sources, (2) utterance-level dynamic\nselection of data cleansing methods based on training data quality, and (3)\nevaluation-in-the-loop data selection using automatically predicted mean\nopinion scores (MOS) to estimate each utterance's impact on model performance.\nFurthermore, TTSOps jointly optimizes the corpus and the TTS model in a\nclosed-loop framework by dynamically adapting both data selection and data\ncleansing processes to the characteristics of the target TTS model. Extensive\nexperiments on Japanese YouTube data demonstrate that TTSOps outperforms\nconventional acoustic-quality-based baselines in both the naturalness and\nspeaker diversity of synthesized speech."}
{"id": "2506.15273", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15273", "abs": "https://arxiv.org/abs/2506.15273", "authors": ["Anup Mishra", "Čedomir Stefanović", "Xiuqiang Xu", "Petar Popovski", "Israel Leyva-Mayorga"], "title": "Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access", "comment": null, "summary": "Flexible and efficient wireless resource sharing across heterogeneous\nservices is a key objective for future wireless networks. In this context, we\ninvestigate the performance of a system where latency-constrained\ninternet-of-things (IoT) devices coexist with a broadband user. The base\nstation adopts a grant-free access framework to manage resource allocation,\neither through orthogonal radio access network (RAN) slicing or by allowing\nshared access between services. For the IoT users, we propose a reinforcement\nlearning (RL) approach based on double Q-Learning (QL) to optimise their\nrepetition-based transmission strategy, allowing them to adapt to varying\nlevels of interference and meet a predefined latency target. We evaluate the\nsystem's performance in terms of the cumulative distribution function of IoT\nusers' latency, as well as the broadband user's throughput and energy\nefficiency (EE). Our results show that the proposed RL-based access policies\nsignificantly enhance the latency performance of IoT users in both RAN Slicing\nand RAN Sharing scenarios, while preserving desirable broadband throughput and\nEE. Furthermore, the proposed policies enable RAN Sharing to be\nenergy-efficient at low IoT traffic levels, and RAN Slicing to be favourable\nunder high IoT traffic."}
{"id": "2506.15456", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.15456", "abs": "https://arxiv.org/abs/2506.15456", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "comment": "Accepted to Interspeech 2025", "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks."}
{"id": "2506.15338", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15338", "abs": "https://arxiv.org/abs/2506.15338", "authors": ["Islam M. Tanash", "Ayush Kumar Dwivedi", "Taneli Riihonen"], "title": "Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry", "comment": null, "summary": "This paper studies a high-altitude platform (HAP) network supported by\nreconfigurable intelligent surfaces (RISs). The practical irregular placement\nof HAPs and RISs is modeled using homogeneous Poisson point processes, while\nbuildings that cause blockages in urban areas are modeled as a Boolean scheme\nof rectangles. We introduce a novel approach to characterize the statistical\nchannel based on generalized Beta prime distribution. Analytical expressions\nfor coverage probability and ergodic capacity in an interference-limited system\nare derived and validated through Monte Carlo simulations. The findings show\nnotable performance improvements and reveal the impact of various system\nparameters, including blockages effect which contribute in mitigating\ninterference from the other visible HAPs. This proposed system could enhance\nconnectivity and enable effective data offloading in urban environments."}
{"id": "2506.15463", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15463", "abs": "https://arxiv.org/abs/2506.15463", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "title": "Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array", "comment": "5 Pages with 6 figures and 1 table", "summary": "In practical systems, recorded analog signals must be digitized for\nprocessing, introducing quantization as a critical aspect of data acquisition.\nWhile prior studies have examined quantization effects in various signal\nprocessing contexts, its impact on differential microphone arrays (DMAs),\nparticularly in one-dimensional (1D) first-order configurations, remains\nunexplored. This paper investigates the influence of signal quantization on\nperformance of first-order 1D DMAs across various beampatterns. An analytical\nexpression for quantized beamformed output for a first-order 1D DMA has been\nformulated. The effect of signal quantization has been studied on array\nperformance measures such as the Beampattern, Directivity Factor (DF),\nFront-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that\nbeampattern shape remains structurally invariant across quantization bit\ndepths, with quantization primarily affecting ND. DF and FBR remain constant\nwith the varying number of quantization bits. Additionally, ND is shown to be\nfrequency-independent; however, it increases with increasing quantization bit\ndepths, enhancing interference suppression. The study also examines the effect\nof steering nulls across the azimuthal range, showing that ND degrades as the\nnull moves closer to the source look direction, indicating reduced interference\nsuppression."}
{"id": "2506.15470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15470", "abs": "https://arxiv.org/abs/2506.15470", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage", "comment": null, "summary": "With the deployment of large antenna arrays at high frequency bands, future\nwireless communication systems are likely to operate in the radiative\nnear-field. Unlike far-field beam steering, near-field beams can be focused\nwithin a spatial region of finite depth, enabling spatial multiplexing in both\nthe angular and range dimensions. This paper derives the beamdepth for a\ngeneralized uniform rectangular array (URA) and investigates how array geometry\ninfluences the near-field beamdepth and the limits where near-field\nbeamfocusing is achievable. To characterize the near-field boundary in terms of\nbeamfocusing and spatial multiplexing gains, we define the effective\nbeamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis\nreveals that while a square URA achieves the narrowest beamdepth, the EBRD is\nmaximized for a wide or tall URA. However, despite its narrow beamdepth, a\nsquare URA may experience a reduction in multiuser sum rate due to its severely\nconstrained EBRD. Simulation results confirm that a wide or tall URA achieves a\nsum rate of 3.5 X more than that of a square URA, benefiting from the extended\nEBRD and improved spatial multiplexing capabilities."}
{"id": "2506.15670", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15670", "abs": "https://arxiv.org/abs/2506.15670", "authors": ["Özlem Tugfe Demir", "Mustafa Ozger", "Ferdi Kara", "Woong-Hee Lee", "Emil Björnson"], "title": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward", "comment": "7 pages, 5 figures", "summary": "This paper explores the integration of simultaneous wireless information and\npower transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)\ntechnology operating in the upper mid-band frequency range (7-24 GHz). The\nnear-field propagation achieved by gMIMO introduces unique opportunities for\nenergy-efficient, high-capacity communication systems that cater to the demands\nof 6G wireless networks. Exploiting spherical wave propagation, near-field\nSWIPT with gMIMO enables precise energy and data delivery, enhancing spectral\nefficiency through beamfocusing and massive spatial multiplexing. This paper\ndiscusses theoretical principles, design challenges, and enabling solutions,\nincluding advanced channel estimation techniques, precoding strategies, and\ndynamic array configurations such as sparse and modular arrays. Through\nanalytical insights and a case study, this paper demonstrates the feasibility\nof achieving optimized energy harvesting and data throughput in dense and\ndynamic environments. These findings contribute to advancing energy-autonomous\nInternet-of-Everything (IoE) deployments, smart factory networks, and other\nenergy-autonomous applications aligned with the goals of next-generation\nwireless technologies."}
{"id": "2506.15530", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.15530", "abs": "https://arxiv.org/abs/2506.15530", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "comment": null, "summary": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed."}

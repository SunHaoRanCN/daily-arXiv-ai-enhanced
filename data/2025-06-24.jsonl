{"id": "2506.17599", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17599", "abs": "https://arxiv.org/abs/2506.17599", "authors": ["Yutaka Jitsumatsu", "Liangchen Sun"], "title": "Two-Stage Prony-Based Estimation of Fractional Delay and Doppler Shifts in OTFS Modulation", "comment": "6 pages and 7 figures", "summary": "This paper addresses the estimation of fractional delay and Doppler shifts in\nmultipath channels that cause doubly selective fading-an essential task for\nintegrated sensing and communication (ISAC) systems in high-mobility\nenvironments. Orthogonal Time Frequency Space (OTFS) modulation enables simple\nand robust channel compensation under such conditions. However, fractional\ndelay and Doppler components introduce inter-path interference, degrading\nestimation accuracy. We propose a two-stage estimation method based on Prony's\ntechnique using OTFS pilot signals with M subchannels and N pilot repetitions.\nIn the first stage, Doppler frequencies are estimated by jointly solving M\ncoupled Prony equations, exploiting the periodicity of the pilot signal. In the\nsecond stage, delays are estimated by applying the discrete Fourier transform\n(DFT) and Prony's method to each Doppler component obtained in the first stage.\nThe proposed method can accurately estimate up to N-1 delay-Doppler parameters\nunder noiseless conditions. In noisy environments, conventional information\ncriteria such as AIC and BIC yield suboptimal performance; thus, a heuristic\nmodel order selection is adopted. Numerical simulations confirm that the\nproposed method achieves high estimation accuracy, highlighting its potential\nfor future ISAC frameworks."}
{"id": "2506.17740", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.17740", "abs": "https://arxiv.org/abs/2506.17740", "authors": ["Pengyu Han", "Zeyi Liu", "Shijin Chen", "Dongliang Zou", "Xiao He"], "title": "Rethinking the Role of Operating Conditions for Learning-based Multi-condition Fault Diagnosis", "comment": "6 pages, 6 figures, conference", "summary": "Multi-condition fault diagnosis is prevalent in industrial systems and\npresents substantial challenges for conventional diagnostic approaches. The\ndiscrepancy in data distributions across different operating conditions\ndegrades model performance when a model trained under one condition is applied\nto others. With the recent advancements in deep learning, transfer learning has\nbeen introduced to the fault diagnosis field as a paradigm for addressing\nmulti-condition fault diagnosis. Among these methods, domain generalization\napproaches can handle complex scenarios by extracting condition-invariant fault\nfeatures. Although many studies have considered fault diagnosis in specific\nmulti-condition scenarios, the extent to which operating conditions affect\nfault information has been scarcely studied, which is crucial. However, the\nextent to which operating conditions affect fault information has been scarcely\nstudied, which is crucial. When operating conditions have a significant impact\non fault features, directly applying domain generalization methods may lead the\nmodel to learn condition-specific information, thereby reducing its overall\ngeneralization ability. This paper investigates the performance of existing\nend-to-end domain generalization methods under varying conditions, specifically\nin variable-speed and variable-load scenarios, using multiple experiments on a\nreal-world gearbox. Additionally, a two-stage diagnostic framework is proposed,\naiming to improve fault diagnosis performance under scenarios with significant\noperating condition impacts. By incorporating a domain-generalized encoder with\na retraining strategy, the framework is able to extract condition-invariant\nfault features while simultaneously alleviating potential overfitting to the\nsource domain. Several experiments on a real-world gearbox dataset are\nconducted to validate the effectiveness of the proposed approach."}
{"id": "2506.17810", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17810", "abs": "https://arxiv.org/abs/2506.17810", "authors": ["Parisa Ramezani", "Seyed Jalaleddin Mousavirad", "Mattias O'Nils", "Emil Bj√∂rnson"], "title": "Machine Learning-Based Near-Field Localization in Mixed LoS/NLoS Scenarios", "comment": "Accepted at EUSIPCO 2025", "summary": "The conventional MUltiple SIgnal Classification (MUSIC) algorithm is\neffective for angle-of-arrival estimation in the far-field and can be extended\nfor full source localization in the near-field. However, it suffers from high\ncomputational complexity, which becomes especially prohibitive in near-field\nscenarios due to the need for exhaustive 3D grid searches. This paper presents\na machine learning-based approach for 3D localization of near-field sources in\nmixed line-of-sight (LoS)/non-LoS scenarios. A convolutional neural network\n(CNN) learns the mapping between the eigenvectors of the received signal's\ncovariance matrix at the anchor node and the sources' 3D locations. The\ndetailed description of the proposed CNN model is provided. The effectiveness\nand time efficiency of the proposed CNN-based localization approach is\ncorroborated via numerical simulations."}
{"id": "2506.17887", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17887", "abs": "https://arxiv.org/abs/2506.17887", "authors": ["Huixin Xu", "Jianhua Zhang", "Pan Tang", "Hongbo Xing", "Haiyang Miao", "Nan Zhang", "Jian Li", "Jianming Wu", "Wenfei Yang", "Zhening Zhang", "Wei Jiang", "Zijian He", "Afshin Haghighat", "Qixing Wang", "Guangyi Liu"], "title": "Near-Field Propagation and Spatial Non-Stationarity Channel Model for 6-24 GHz (FR3) Extremely Large-Scale MIMO: Adopted by 3GPP for 6G", "comment": null, "summary": "Next generation cellular deployments are expected to exploit the 6-24 GHz\nfrequency range 3 (FR3) and extremely large-scale multiple-input\nmultiple-output (XL-MIMO) to enable ultra-high data rates and reliability.\nHowever, the significantly enlarged antenna apertures and higher carrier\nfrequencies render the far-field and spatial stationarity assumptions in the\nexisting 3rd generation partnership project (3GPP) channel models invalid,\ngiving rise to new features such as near-field propagation and spatial\nnon-stationarity (SNS). Despite extensive prior research, incorporating these\nnew features within the standardized channel modeling framework remains an open\nissue. To address this, this paper presents a channel modeling framework for\nXL-MIMO systems that incorporates both near-field and SNS features, adopted by\n3GPP. For the near-field propagation feature, the framework models the\ndistances from the base station (BS) and user equipment to the spherical-wave\nsources associated with clusters. These distances are used to characterize\nelement-wise variations of path parameters, such as nonlinear changes in phase\nand angle. To capture the effect of SNS at the BS side, a stochastic-based\napproach is proposed to model SNS caused by incomplete scattering, by\nestablishing power attenuation factors from visibility probability and\nvisibility region to characterize antenna element-wise path power variation. In\naddition, a physical blocker-based approach is introduced to model SNS effects\ncaused by partial blockage. Finally, a simulation framework for near-field and\nSNS is developed within the structure of the existing 3GPP channel model.\nPerformance evaluations demonstrate that the near-field model captures higher\nchannel capacity potential compared to the far-field model. Coupling loss\nresults indicate that SNS leads to more pronounced propagation fading relative\nto the spatial stationary model."}
{"id": "2506.17686", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.17686", "abs": "https://arxiv.org/abs/2506.17686", "authors": ["Alican Gok", "Oguzhan Buyuksolak", "Osman Erman Okman", "Murat Saraclar"], "title": "Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models", "comment": "To be submitted to IEEE Signal Processing Letters, 5 pages, 3 figures", "summary": "Keyword Spotting plays a critical role in enabling hands-free interaction for\nbattery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the\nscalability and adaptability challenges of traditional systems by enabling\nrecognition of custom keywords with only a few examples. However, existing\nFS-KWS systems achieve subpar accuracy at desirable false acceptance rates,\nparticularly in resource-constrained edge environments. To address these\nissues, we propose a training scheme that leverages self-supervised learning\nmodels for robust feature extraction, dimensionality reduction, and knowledge\ndistillation. The teacher model, based on Wav2Vec 2.0 is trained using\nSub-center ArcFace loss, which enhances inter-class separability and\nintra-class compactness. To enable efficient deployment on edge devices, we\nintroduce attention-based dimensionality reduction and train a standard\nlightweight ResNet15 student model. We evaluate the proposed approach on the\nEnglish portion of the Multilingual Spoken Words Corpus (MSWC) and the Google\nSpeech Commands (GSC) datasets. Notably, the proposed training method improves\nthe 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%\nfalse alarm accuracy on the GSC dataset, thus making it significantly\nbetter-suited for a real use case scenario."}
{"id": "2506.17351", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17351", "abs": "https://arxiv.org/abs/2506.17351", "authors": ["Mostafa Shahin", "Beena Ahmed", "Julien Epps"], "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM", "comment": null, "summary": "Cognitive impairment (CI) is of growing public health concern, and early\ndetection is vital for effective intervention. Speech has gained attention as a\nnon-invasive and easily collectible biomarker for assessing cognitive decline.\nTraditional CI detection methods typically rely on supervised models trained on\nacoustic and linguistic features extracted from speech, which often require\nmanual annotation and may not generalise well across datasets and languages. In\nthis work, we propose the first zero-shot speech-based CI detection method\nusing the Qwen2- Audio AudioLLM, a model capable of processing both audio and\ntext inputs. By designing prompt-based instructions, we guide the model in\nclassifying speech samples as indicative of normal cognition or cognitive\nimpairment. We evaluate our approach on two datasets: one in English and\nanother multilingual, spanning different cognitive assessment tasks. Our\nresults show that the zero-shot AudioLLM approach achieves performance\ncomparable to supervised methods and exhibits promising generalizability and\nconsistency across languages, tasks, and datasets."}
{"id": "2506.18009", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.18009", "abs": "https://arxiv.org/abs/2506.18009", "authors": ["Kaitao Meng", "Kawon Han", "Christos Masouros", "Lajos Hanzo"], "title": "ISAC Network Planning: Sensing Coverage Analysis and 3-D BS Deployment Optimization", "comment": "13 pages, 11 figures, submitted to IEEE journal for possible\n  publication", "summary": "Integrated sensing and communication (ISAC) networks strive to deliver both\nhigh-precision target localization and high-throughput data services across the\nentire coverage area. In this work, we examine the fundamental trade-off\nbetween sensing and communication from the perspective of base station (BS)\ndeployment. Furthermore, we conceive a design that simultaneously maximizes the\ntarget localization coverage, while guaranteeing the desired communication\nperformance. In contrast to existing schemes optimized for a single target, an\neffective network-level approach has to ensure consistent localization accuracy\nthroughout the entire service area. While employing time-of-flight (ToF) based\nlocalization, we first analyze the deployment problem from a\nlocalization-performance coverage perspective, aiming for minimizing the area\nCramer-Rao Lower Bound (A-CRLB) to ensure uniformly high positioning accuracy\nacross the service area. We prove that for a fixed number of BSs, uniformly\nscaling the service area by a factor \\kappa increases the optimal A-CRLB in\nproportion to \\kappa^{2\\beta}, where \\beta is the BS-to-target pathloss\nexponent. Based on this, we derive an approximate scaling law that links the\nachievable A-CRLB across the area of interest to the dimensionality of the\nsensing area. We also show that cooperative BSs extends the coverage but yields\nmarginal A-CRLB improvement as the dimensionality of the sensing area grows."}
{"id": "2506.17690", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17690", "abs": "https://arxiv.org/abs/2506.17690", "authors": ["Julian Herreilers", "Christiaan Jacobs", "Thomas Niesler"], "title": "Low-resource keyword spotting using contrastively trained transformer acoustic word embeddings", "comment": "5 pages, 2 figures", "summary": "We introduce a new approach, the ContrastiveTransformer, that produces\nacoustic word embeddings (AWEs) for the purpose of very low-resource keyword\nspotting. The ContrastiveTransformer, an encoder-only model, directly optimises\nthe embedding space using normalised temperature-scaled cross entropy (NT-Xent)\nloss. We use this model to perform keyword spotting for radio broadcasts in\nLuganda and Bambara, the latter a severely under-resourced language. We compare\nour model to various existing AWE approaches, including those constructed from\nlarge pre-trained self-supervised models, a recurrent encoder which previously\nused the NT-Xent loss, and a DTW baseline. We demonstrate that the proposed\ncontrastive transformer approach offers performance improvements over all\nconsidered existing approaches to very low-resource keyword spotting in both\nlanguages."}
{"id": "2506.17409", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17409", "abs": "https://arxiv.org/abs/2506.17409", "authors": ["Quoc Thinh Vo", "Joe Woods", "Priontu Chowdhury", "David K. Han"], "title": "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation", "comment": "This paper has been accepted for the 33rd European Signal Processing\n  Conference (EUSIPCO) 2025 in Palermo, Italy", "summary": "Localizing acoustic sound sources in the ocean is a challenging task due to\nthe complex and dynamic nature of the environment. Factors such as high\nbackground noise, irregular underwater geometries, and varying acoustic\nproperties make accurate localization difficult. To address these obstacles, we\npropose a multi-branch network architecture designed to accurately predict the\ndistance between a moving acoustic source and a receiver, tested on real-world\nunderwater signal arrays. The network leverages Convolutional Neural Networks\n(CNNs) for robust spatial feature extraction and integrates Conformers with\nself-attention mechanism to effectively capture temporal dependencies. Log-mel\nspectrogram and generalized cross-correlation with phase transform (GCC-PHAT)\nfeatures are employed as input representations. To further enhance the model\nperformance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively\nadjusts the amplitude of input features, ensuring consistent energy levels\nacross varying ranges, signal strengths, and noise conditions. We assess the\nmodel's generalization capability by training it in one domain and testing it\nin a different domain, using only a limited amount of data from the test domain\nfor fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)\napproaches in similar settings, establishing new benchmarks for underwater\nsound localization."}
{"id": "2506.18067", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.18067", "abs": "https://arxiv.org/abs/2506.18067", "authors": ["Zhenkun Zhang", "Yining Xu", "Cunhua Pan", "Hong Ren", "Yiming Yu", "Jiangzhou Wang"], "title": "Cooperative Bistatic ISAC Systems for Low-Altitude Economy", "comment": null, "summary": "The burgeoning low-altitude economy (LAE) necessitates integrated sensing and\ncommunication (ISAC) systems capable of high-accuracy multi-target localization\nand velocity estimation under hardware and coverage constraints inherent in\nconventional ISAC architectures. This paper addresses these challenges by\nproposing a cooperative bistatic ISAC framework within MIMO-OFDM cellular\nnetworks, enabling robust sensing services for LAE applications through\nstandardized 5G New Radio (NR) infrastructure. We first develop a\nlow-complexity parameter extraction algorithm employing CANDECOMP/PARAFAC (CP)\ntensor decomposition, which exploits the inherent Vandermonde structure in\ndelay-related factor matrices to efficiently recover bistatic ranges, Doppler\nvelocities, and angles-of-arrival (AoA) from multi-dimensional received signal\ntensors. To resolve data association ambiguity across distributed\ntransmitter-receiver pairs and mitigate erroneous estimates, we further design\na robust fusion scheme based on the minimum spanning tree (MST) method,\nenabling joint 3D position and velocity reconstruction. Comprehensive\nsimulation results validate the framework's superiority in computational\nefficiency and sensing performance for low-altitude scenarios."}
{"id": "2506.18281", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.18281", "abs": "https://arxiv.org/abs/2506.18281", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Blind Source Separation in Biomedical Signals Using Variational Methods", "comment": "Presented at Southern Ontario Numerical Analysis Day (SONAD'25),\n  Contributed Talk 03", "summary": "This study introduces a novel unsupervised approach for separating\noverlapping heart and lung sounds using variational autoencoders (VAEs). In\nclinical settings, these sounds often interfere with each other, making manual\nseparation difficult and error-prone. The proposed model learns to encode mixed\nsignals into a structured latent space and reconstructs the individual\ncomponents using a probabilistic decoder, all without requiring labeled data or\nprior knowledge of source characteristics. We apply this method to real\nrecordings obtained from a clinical manikin using a digital stethoscope.\nResults demonstrate distinct latent clusters corresponding to heart and lung\nsources, as well as accurate reconstructions that preserve key spectral\nfeatures of the original signals. The approach offers a robust and\ninterpretable solution for blind source separation and has potential\napplications in portable diagnostic tools and intelligent stethoscope systems."}
{"id": "2506.17497", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17497", "abs": "https://arxiv.org/abs/2506.17497", "authors": ["Mingyang Yao", "Ke Chen"], "title": "From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training", "comment": "Proceedings of the 6th Conference on AI Music Creativity, AIMC 2025", "summary": "Despite progress in controllable symbolic music generation, data scarcity\nremains a challenge for certain control modalities. Composer-style music\ngeneration is a prime example, as only a few pieces per composer are available,\nlimiting the modeling of both styles and fundamental music elements (e.g.,\nmelody, chord, rhythm). In this paper, we investigate how general music\nknowledge learned from a broad corpus can enhance the mastery of specific\ncomposer styles, with a focus on piano piece generation. Our approach follows a\ntwo-stage training paradigm. First, we pre-train a REMI-based music generation\nmodel on a large corpus of pop, folk, and classical music. Then, we fine-tune\nit on a small, human-verified dataset from four renowned composers, namely\nBach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to\ncondition the model on style indicators. To evaluate the effectiveness of our\napproach, we conduct both objective and subjective evaluations on style\naccuracy and musicality. Experimental results demonstrate that our method\noutperforms ablations and baselines, achieving more precise composer-style\nmodeling and better musical aesthetics. Additionally, we provide observations\non how the model builds music concepts from the generality pre-training and\nrefines its stylistic understanding through the mastery fine-tuning."}
{"id": "2506.18177", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18177", "abs": "https://arxiv.org/abs/2506.18177", "authors": ["Mingchao Liang", "Florian Meyer"], "title": "Coherent Track-Before-Detect", "comment": null, "summary": "Accurately tracking an unknown and time-varying number of objects in complex\nenvironments is a significant challenge but a fundamental capability in a\nvariety of applications, including applied ocean sciences, surveillance,\nautonomous driving, and wireless communications. Conventional Bayesian\nmultiobject tracking (MOT) methods typically employ a detect-then-track (DTT)\napproach, where a frontend detector preprocesses raw sensor data to extract\nmeasurements for MOT. The irreversible nature of this preprocessing step can\ndiscard valuable object-related information, particularly impairing the ability\nto resolve weak or closely spaced objects. The track-before-detect (TBD)\nparadigm offers an alternative by operating directly on sensor data. However,\nexisting TBD approaches introduce simplifications to facilitate the development\nof inference methods, such as assuming known signal amplitudes or conditional\nindependence between sensor measurements given object states. These assumptions\ncan lead to suboptimal performance and limit the applicability of the resulting\nTBD methods in realistic scenarios.\n  This paper introduces coherent TBD based on a comprehensive signal model for\nsensor data. The new model accounts for sensor data correlations and amplitude\nfluctuations, enabling the accurate representation of the physics of the\ndata-generating process in TBD. Coherent TBD is suitable for a wide range of\nproblems in active and passive radar, active and passive sonar, as well as\nintegrated sensing and communication systems. Based on a factor graph\nrepresentation of the new measurement model, a scalable belief propagation (BP)\nmethod is developed to perform efficient Bayesian inference. Experimental\nresults, performed with both synthetic and real data, demonstrate that the\nproposed method outperforms state-of-the-art conventional MOT methods."}
{"id": "2506.18402", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18402", "abs": "https://arxiv.org/abs/2506.18402", "authors": ["Junyu Zhou", "Yanxiong Li", "Haolin Yu"], "title": "Infant Cry Emotion Recognition Using Improved ECAPA-TDNN with Multiscale Feature Fusion and Attention Enhancement", "comment": "Accepted for publication on Interspeech 2025. 5 pages, 2 tables and 7\n  figures", "summary": "Infant cry emotion recognition is crucial for parenting and medical\napplications. It faces many challenges, such as subtle emotional variations,\nnoise interference, and limited data. The existing methods lack the ability to\neffectively integrate multi-scale features and temporal-frequency\nrelationships. In this study, we propose a method for infant cry emotion\nrecognition using an improved Emphasized Channel Attention, Propagation and\nAggregation in Time Delay Neural Network (ECAPA-TDNN) with both multi-scale\nfeature fusion and attention enhancement. Experiments on a public dataset show\nthat the proposed method achieves accuracy of 82.20%, number of parameters of\n1.43 MB and FLOPs of 0.32 Giga. Moreover, our method has advantage over the\nbaseline methods in terms of accuracy. The code is at\nhttps://github.com/kkpretend/IETMA."}
{"id": "2506.17778", "categories": ["cs.SD", "eess.AS", "math.HO", "20-01 (Primary), 00A08 (secondary)"], "pdf": "https://arxiv.org/pdf/2506.17778", "abs": "https://arxiv.org/abs/2506.17778", "authors": ["Veronica Flynn", "Carmen Rovi"], "title": "Algebraic Structures in Microtonal Music", "comment": "17 pages, 12 figures. The content should be accessible for students\n  in a first course of Abstract Algebra. A musical background is not necessary.\n  Comments welcome!", "summary": "We will discuss how certain group theory structures are found in music\ntheory. Western music splits the octave into 12 equal tones called half-steps.\nWe can take this division further and split the octave into 24 equal tones by\nsplitting each half-step in two, called a quarter-step. By assigning each of\nthese 24 notes a number, we can discuss musical actions mathematically. In this\npaper, we analyze 24-tone microtonal music and explore how musical and harmonic\nstructures in this system can be interpreted in terms of group-theoretic\nstructures. This work extends the study by Crans, Fiore, and Satyendra."}
{"id": "2506.18218", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18218", "abs": "https://arxiv.org/abs/2506.18218", "authors": ["Yinghan Li", "Yiming Liu", "Wei Yu"], "title": "Multimodal Visual Image Based User Association and Beamforming Using Graph Neural Networks", "comment": "16 pages, 8 figures", "summary": "This paper proposes an approach that leverages multimodal data by integrating\nvisual images with radio frequency (RF) pilots to optimize user association and\nbeamforming in a downlink wireless cellular network under a max-min fairness\ncriterion. Traditional methods typically optimize wireless system parameters\nbased on channel state information (CSI). However, obtaining accurate CSI\nrequires extensive pilot transmissions, which lead to increased overhead and\nlatency. Moreover, the optimization of user association and beamforming is a\ndiscrete and non-convex optimization problem, which is challenging to solve\nanalytically. In this paper, we propose to incorporate visual camera data in\naddition to the RF pilots to perform the joint optimization of user association\nand beamforming. The visual image data helps enhance channel awareness, thereby\nreducing the dependency on extensive pilot transmissions for system\noptimization. We employ a learning-based approach based on using first a\ndetection neural network that estimates user locations from images, and\nsubsequently two graph neural networks (GNNs) that extract features for system\noptimization based on the location information and the received pilots,\nrespectively. Then, a multimodal GNN is constructed to integrate the features\nfor the joint optimization user association and beamforming. Simulation results\ndemonstrate that the proposed method achieves superior performance, while\nhaving low computational complexity and being interpretable and generalizable,\nmaking it an effective solution as compared to traditional methods based only\non RF pilots."}
{"id": "2506.18406", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18406", "abs": "https://arxiv.org/abs/2506.18406", "authors": ["Yongjie Si", "Yanxiong Li", "Jiaxin Tan", "Qianhua He", "Il-Youp Kwak"], "title": "Fully Few-shot Class-incremental Audio Classification Using Multi-level Embedding Extractor and Ridge Regression Classifier", "comment": "Accepted for publication on Interspeech 2025. 5 pages, 6 tables, 7\n  figures", "summary": "In the task of Few-shot Class-incremental Audio Classification (FCAC),\ntraining samples of each base class are required to be abundant to train model.\nHowever, it is not easy to collect abundant training samples for many base\nclasses due to data scarcity and high collection cost. We discuss a more\nrealistic issue, Fully FCAC (FFCAC), in which training samples of both base and\nincremental classes are only a few. Furthermore, we propose a FFCAC method\nusing a model which is decoupled into a multi-level embedding extractor and a\nridge regression classifier. The embedding extractor consists of an encoder of\naudio spectrogram Transformer and a fusion module, and is trained in the base\nsession but frozen in all incremental sessions. The classifier is updated\ncontinually in each incremental session. Results on three public datasets show\nthat our method exceeds current methods in accuracy, and has advantage over\nmost of them in complexity. The code is at https://github.com/YongjieSi/MAR."}
{"id": "2506.17815", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17815", "abs": "https://arxiv.org/abs/2506.17815", "authors": ["Julien Guinot", "Alain Riou", "Elio Quinton", "Gy√∂rgy Fazekas"], "title": "SLAP: Siamese Language-Audio Pretraining Without Negative Samples for Music Understanding", "comment": "Accepted to ISMIR 2025", "summary": "Joint embedding spaces have significantly advanced music understanding and\ngeneration by linking text and audio through multimodal contrastive learning.\nHowever, these approaches face large memory requirement limitations due to\nrelying on large batch sizes to effectively utilize negative samples. Further,\nmultimodal joint embedding spaces suffer from a modality gap wherein embeddings\nfrom different modalities lie in different manifolds of the embedding space. To\naddress these challenges, we propose Siamese Language-Audio Pretraining (SLAP),\na novel multimodal pretraining framework that allows learning powerful\nrepresentations without negative samples. SLAP adapts the Bootstrap Your Own\nLatent (BYOL) paradigm for multimodal audio-text training, promoting\nscalability in training multimodal embedding spaces.\n  We illustrate the ability of our model to learn meaningful relationships\nbetween music and text -- specifically, we show that SLAP outperforms CLAP on\ntasks such as text-music retrieval and zero-shot classification. We also\nobserve competitive downstream performance on several MIR tasks, including with\nlarger or supervised models (genre and instrument classification,\nauto-tagging). Additionally, our approach has attractive properties, such as a\nquantifiably reduced modality gap and improved robustness to batch size\nvariations on retrieval performance. Finally, its novel formulation unlocks\nlarge-scale training on a single GPU through gradient accumulation."}
{"id": "2506.18293", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18293", "abs": "https://arxiv.org/abs/2506.18293", "authors": ["Majumder Haider", "Imtiaz Ahmed", "Zoheb Hassan", "Kamrul Hasan", "H. Vincent Poor"], "title": "LLM-Integrated Digital Twins for Hierarchical Resource Allocation in 6G Networks", "comment": null, "summary": "Next-generation (NextG) wireless networks are expected to require\nintelligent, scalable, and context-aware radio resource management (RRM) to\nsupport ultra-dense deployments, diverse service requirements, and dynamic\nnetwork conditions. Digital twins (DTs) offer a powerful tool for network\nmanagement by creating high-fidelity virtual replicas that model real-time\nnetwork behavior, while large language models (LLMs) enhance decision-making\nthrough their advanced generalization and contextual reasoning capabilities.\nThis article proposes LLM-driven DTs for network optimization (LLM-DTNet), a\nhierarchical framework that integrates multi-layer DT architectures with\nLLM-based orchestration to enable adaptive, real-time RRM in heterogeneous\nNextG networks. We present the fundamentals and design considerations of\nLLM-DTNet while discussing its effectiveness in proactive and situation-aware\nnetwork management across terrestrial and non-terrestrial applications.\nFurthermore, we highlight key challenges, including scalable DT modeling,\nsecure LLM-DT integration, energy-efficient implementations, and multimodal\ndata processing, shaping future advancements in NextG intelligent wireless\nnetworks."}
{"id": "2506.18623", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18623", "abs": "https://arxiv.org/abs/2506.18623", "authors": ["Jiangyu Han", "Petr P√°lka", "Marc Delcroix", "Federico Landini", "Johan Rohdin", "Jan Cernock√Ω", "Luk√°≈° Burget"], "title": "Efficient and Generalizable Speaker Diarization via Structured Pruning of Self-Supervised Models", "comment": "11 pages, 6 figures", "summary": "Self-supervised learning (SSL) models such as WavLM have brought substantial\nimprovements to speaker diarization by providing rich contextual\nrepresentations. However, the high computational and memory costs of these\nmodels hinder their deployment in real-time and resource-constrained scenarios.\nIn this work, we present a comprehensive study on compressing SSL-based\ndiarization models through structured pruning guided by knowledge distillation.\nBuilding upon our previous work, we extend the analysis to include pruning\nobjectives based on multiply-accumulate operations (MACs), investigate\nmodule-wise and progressive pruning strategies, and examine the impact of\ntraining data quantity. Experimental results show that our method reduces model\nsize by up to 80% without degrading performance, achieving up to 4x faster\ninference on a single GPU. We further perform large-scale evaluations on a\ndiverse compound dataset comprising eight public diarization corpora, where our\nbest pruned model achieves state-of-the-art performance across most conditions.\nAdditionally, we show strong generalization to the CHiME-6 dataset, attaining\nperformance comparable to the third-place system in the CHiME-7 challenge\nwithout any domain adaptation. All models and code are publicly released to\nsupport reproducibility and future research."}
{"id": "2506.17818", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17818", "abs": "https://arxiv.org/abs/2506.17818", "authors": ["Angelos-Nikolaos Kanatas", "Charilaos Papaioannou", "Alexandros Potamianos"], "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning", "comment": "10 pages, 4 figures, accepted to the 26th International Society for\n  Music Information Retrieval conference (ISMIR 2025), to be held in Daejeon,\n  South Korea", "summary": "Recent advances in music foundation models have improved audio representation\nlearning, yet their effectiveness across diverse musical traditions remains\nlimited. We introduce CultureMERT-95M, a multi-culturally adapted foundation\nmodel developed to enhance cross-cultural music representation learning and\nunderstanding. To achieve this, we propose a two-stage continual pre-training\nstrategy that integrates learning rate re-warming and re-decaying, enabling\nstable adaptation even with limited computational resources. Training on a\n650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music\ntraditions, results in an average improvement of 4.9% in ROC-AUC and AP across\ndiverse non-Western music auto-tagging tasks, surpassing prior\nstate-of-the-art, with minimal forgetting on Western-centric benchmarks. We\nfurther investigate task arithmetic, an alternative approach to multi-cultural\nadaptation that merges single-culture adapted models in the weight space. Task\narithmetic performs on par with our multi-culturally trained model on\nnon-Western auto-tagging tasks and shows no regression on Western datasets.\nCross-cultural evaluation reveals that single-culture models transfer with\nvarying effectiveness across musical traditions, whereas the multi-culturally\nadapted model achieves the best overall performance. To support research on\nworld music representation learning, we publicly release CultureMERT-95M and\nCultureMERT-TA-95M, fostering the development of more culturally aware music\nfoundation models."}
{"id": "2506.18324", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18324", "abs": "https://arxiv.org/abs/2506.18324", "authors": ["Shiping Fu", "Yufan Chen", "Zhe Zhang", "Xiaolan Qiu", "Qixiang Ye"], "title": "ARSAR-Net: Adaptively Regularized SAR Imaging Network via Non-matrix-inversion ADMM", "comment": null, "summary": "Deep unfolding networks have constituted an emerging method for synthetic\naperture radar (SAR) imaging. However, baseline unfolding networks, when\nunfolded from the alternating direction method of multipliers (ADMM), lack\ngeneralization capability across scenes as the regularizers are empirically\ndesigned. In this study, we introduce a learnable regularizer to the unfolding\nnetwork and create an adaptively regularized SAR imaging network (ARSAR-Net),\nwhich pursues generalization capability across scenes, including varying\nsparsity levels and heterogeneous scenes with offshore ships, islands, urban\nareas, and mountainous terrain. In practice, the vanilla ARSAR-Net suffers from\ninherent structural limitations in 2D signal processing, primarily due to its\nreliance on matrix inversion. To conquer this challenge, we introduce the\nnon-matrix-inversion ADMM algorithm, which replaces inefficient matrix\ninversion with efficient linear approximations. Extensive validation through\nsimulated and real-data experiments (including GF-3 satellite echoes)\ndemonstrates ARSAR-Net's triple advantage: (1) 50\\% faster imaging speed\ncompared to existing unfolding networks, (2) up to 2.0 dB PSNR improvement in\nimaging quality, and (3) enhanced adaptability to complex heterogeneous scenes.\nThese advancements establish a new paradigm for computationally efficient and\ngeneralizable SAR imaging systems."}
{"id": "2506.17351", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17351", "abs": "https://arxiv.org/abs/2506.17351", "authors": ["Mostafa Shahin", "Beena Ahmed", "Julien Epps"], "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM", "comment": null, "summary": "Cognitive impairment (CI) is of growing public health concern, and early\ndetection is vital for effective intervention. Speech has gained attention as a\nnon-invasive and easily collectible biomarker for assessing cognitive decline.\nTraditional CI detection methods typically rely on supervised models trained on\nacoustic and linguistic features extracted from speech, which often require\nmanual annotation and may not generalise well across datasets and languages. In\nthis work, we propose the first zero-shot speech-based CI detection method\nusing the Qwen2- Audio AudioLLM, a model capable of processing both audio and\ntext inputs. By designing prompt-based instructions, we guide the model in\nclassifying speech samples as indicative of normal cognition or cognitive\nimpairment. We evaluate our approach on two datasets: one in English and\nanother multilingual, spanning different cognitive assessment tasks. Our\nresults show that the zero-shot AudioLLM approach achieves performance\ncomparable to supervised methods and exhibits promising generalizability and\nconsistency across languages, tasks, and datasets."}
{"id": "2506.17886", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17886", "abs": "https://arxiv.org/abs/2506.17886", "authors": ["Julien Guinot", "Elio Quinton", "Gy√∂rgy Fazekas"], "title": "GD-Retriever: Controllable Generative Text-Music Retrieval with Diffusion Models", "comment": "Accepted to ISMIR 2025", "summary": "Multimodal contrastive models have achieved strong performance in text-audio\nretrieval and zero-shot settings, but improving joint embedding spaces remains\nan active research area. Less attention has been given to making these systems\ncontrollable and interactive for users. In text-music retrieval, the ambiguity\nof freeform language creates a many-to-many mapping, often resulting in\ninflexible or unsatisfying results.\n  We introduce Generative Diffusion Retriever (GDR), a novel framework that\nleverages diffusion models to generate queries in a retrieval-optimized latent\nspace. This enables controllability through generative tools such as negative\nprompting and denoising diffusion implicit models (DDIM) inversion, opening a\nnew direction in retrieval control. GDR improves retrieval performance over\ncontrastive teacher models and supports retrieval in audio-only latent spaces\nusing non-jointly trained encoders. Finally, we demonstrate that GDR enables\neffective post-hoc manipulation of retrieval behavior, enhancing interactive\ncontrol for text-music retrieval tasks."}
{"id": "2506.18419", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18419", "abs": "https://arxiv.org/abs/2506.18419", "authors": ["Yuzhi Yang", "Omar Alhussein", "Atefeh Arani", "Zhaoyang Zhang", "M√©rouane Debbah"], "title": "Generative Diffusion Receivers: Achieving Pilot-Efficient MIMO-OFDM Communications", "comment": "Submitted to IEEE TNSE", "summary": "This paper focuses on wireless multiple-input multiple-output\n(MIMO)-orthogonal frequency division multiplex (OFDM) receivers. Traditional\nwireless receivers have relied on mathematical modeling and Bayesian inference,\nachieving remarkable success in most areas but falling short in their ability\nto characterize channel matrices. Neural networks (NNs) have demonstrated\nsignificant potential in this aspect. Nevertheless, integrating traditional\ninference methods with NNs presents challenges, particularly in tracking the\nerror progression. Given the inevitable presence of noise in wireless systems,\ngenerative models that are more resilient to noise are garnering increased\nattention. In this paper, we propose re-evaluating the MIMO-OFDM receiver using\ndiffusion models, which is a common generative approach. With diffusion models,\nwe can effectively leverage prior knowledge of channel matrices and incorporate\ntraditional signal estimation components. Specifically, we explore the\ndiffusion system and introduce an imagination-screening strategy to guide the\ndiffusion process. Furthermore, diffusion models enable adaptation to varying\nnoise levels and pilot schemes using the same NN, significantly reducing\ntraining and deployment costs. Simulated results reveal that, for pilot\ndensities ranging from 4-6 pilots per 64-subcarrier block and signal-to-noise\nratios (SNRs) from -4 dB to 0 dB, our proposed receiver reduces\nchannel-reconstruction error by up to two times compared to leading\ndeep-learning models, with the most pronounced improvements observed in\nlow-pilot conditions. Additionally, performance enhancements can be achieved\nwith a larger imagination size, despite increased computational complexity."}
{"id": "2506.17409", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17409", "abs": "https://arxiv.org/abs/2506.17409", "authors": ["Quoc Thinh Vo", "Joe Woods", "Priontu Chowdhury", "David K. Han"], "title": "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation", "comment": "This paper has been accepted for the 33rd European Signal Processing\n  Conference (EUSIPCO) 2025 in Palermo, Italy", "summary": "Localizing acoustic sound sources in the ocean is a challenging task due to\nthe complex and dynamic nature of the environment. Factors such as high\nbackground noise, irregular underwater geometries, and varying acoustic\nproperties make accurate localization difficult. To address these obstacles, we\npropose a multi-branch network architecture designed to accurately predict the\ndistance between a moving acoustic source and a receiver, tested on real-world\nunderwater signal arrays. The network leverages Convolutional Neural Networks\n(CNNs) for robust spatial feature extraction and integrates Conformers with\nself-attention mechanism to effectively capture temporal dependencies. Log-mel\nspectrogram and generalized cross-correlation with phase transform (GCC-PHAT)\nfeatures are employed as input representations. To further enhance the model\nperformance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively\nadjusts the amplitude of input features, ensuring consistent energy levels\nacross varying ranges, signal strengths, and noise conditions. We assess the\nmodel's generalization capability by training it in one domain and testing it\nin a different domain, using only a limited amount of data from the test domain\nfor fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)\napproaches in similar settings, establishing new benchmarks for underwater\nsound localization."}
{"id": "2506.18182", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18182", "abs": "https://arxiv.org/abs/2506.18182", "authors": ["Rita Singh", "Bhiksha Raj"], "title": "Human Voice is Unique", "comment": "15 pages, 1 figure, 2 tables", "summary": "Voice is increasingly being used as a biometric entity in many applications.\nThese range from speaker identification and verification systems to human\nprofiling technologies that attempt to estimate myriad aspects of the speaker's\npersona from their voice. However, for an entity to be a true biometric\nidentifier, it must be unique. This paper establishes a first framework for\ncalculating the uniqueness of human voice objectively. The approach in this\npaper is based on statistical considerations that take into account a set of\nmeasurable characteristics of the voice signal that bear a causal relationship\nto the vocal production process, but are not inter-dependent or derivable from\neach other. Depending on how we quantize these variables, we show that the\nchances of two people having the same voice in a world populated by 10 billion\npeople range from one in a few thousand, to one in a septillion or less. The\npaper also discusses the implications of these calculations on the choices made\nin voice processing applications."}
{"id": "2506.18432", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18432", "abs": "https://arxiv.org/abs/2506.18432", "authors": ["Wei Xu", "Zhaohui Yang", "Derrick Wing Kwan Ng", "Robert Schober", "H. Vincent Poor", "Zhaoyang Zhang", "Xiaohu You"], "title": "A New Pathway to Integrated Learning and Communication (ILAC): Large AI Model and Hyperdimensional Computing for Communication", "comment": "27 pages 14 figures", "summary": "The rapid evolution of forthcoming sixth-generation (6G) wireless networks\nnecessitates the seamless integration of artificial intelligence (AI) with\nwireless communications to support emerging intelligent applications that\ndemand both efficient communication and robust learning performance. This dual\nrequirement calls for a unified framework of integrated learning and\ncommunication (ILAC), where AI enhances communication through intelligent\nsignal processing and adaptive resource management, while wireless networks\nsupport AI model deployment by enabling efficient and reliable data exchanges.\nHowever, achieving this integration presents significant challenges in\npractice. Communication constraints, such as limited bandwidth and fluctuating\nchannels, hinder learning accuracy and convergence. Simultaneously, AI-driven\nlearning dynamics, including model updates and task-driven inference, introduce\nexcessive burdens on communication systems, necessitating flexible,\ncontext-aware transmission strategies. Finally, we present a case study on a\ncost-to-performance optimization problem, where task assignments, model size\nselection, bandwidth allocation, and transmission power control are jointly\noptimized, considering computational cost, communication efficiency, and\ninference accuracy. Leveraging the Dinkelbach and alternating optimization\nalgorithms, we offer a practical and effective solution to achieve an optimal\nbalance between learning performance and communication constraints."}
{"id": "2506.17497", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17497", "abs": "https://arxiv.org/abs/2506.17497", "authors": ["Mingyang Yao", "Ke Chen"], "title": "From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training", "comment": "Proceedings of the 6th Conference on AI Music Creativity, AIMC 2025", "summary": "Despite progress in controllable symbolic music generation, data scarcity\nremains a challenge for certain control modalities. Composer-style music\ngeneration is a prime example, as only a few pieces per composer are available,\nlimiting the modeling of both styles and fundamental music elements (e.g.,\nmelody, chord, rhythm). In this paper, we investigate how general music\nknowledge learned from a broad corpus can enhance the mastery of specific\ncomposer styles, with a focus on piano piece generation. Our approach follows a\ntwo-stage training paradigm. First, we pre-train a REMI-based music generation\nmodel on a large corpus of pop, folk, and classical music. Then, we fine-tune\nit on a small, human-verified dataset from four renowned composers, namely\nBach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to\ncondition the model on style indicators. To evaluate the effectiveness of our\napproach, we conduct both objective and subjective evaluations on style\naccuracy and musicality. Experimental results demonstrate that our method\noutperforms ablations and baselines, achieving more precise composer-style\nmodeling and better musical aesthetics. Additionally, we provide observations\non how the model builds music concepts from the generality pre-training and\nrefines its stylistic understanding through the mastery fine-tuning."}
{"id": "2506.18296", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18296", "abs": "https://arxiv.org/abs/2506.18296", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "title": "JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking Styles", "comment": "Accepted on Interspeech 2025", "summary": "We construct Japanese Idol Speech Corpus (JIS) to advance research in speech\ngeneration AI, including text-to-speech synthesis (TTS) and voice conversion\n(VC). JIS will facilitate more rigorous evaluations of speaker similarity in\nTTS and VC systems since all speakers in JIS belong to a highly specific\ncategory: \"young female live idols\" in Japan, and each speaker is identified by\na stage name, enabling researchers to recruit listeners familiar with these\nidols for listening experiments. With its unique speaker attributes, JIS will\nfoster compelling research, including generating voices tailored to listener\npreferences-an area not yet widely studied. JIS will be distributed free of\ncharge to promote research in speech generation AI, with usage restricted to\nnon-commercial, basic research. We describe the construction of JIS, provide an\noverview of Japanese live idol culture to support effective and ethical use of\nJIS, and offer a basic analysis to guide application of JIS."}
{"id": "2506.18465", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18465", "abs": "https://arxiv.org/abs/2506.18465", "authors": ["Marcin Wachowiak", "Andr√© Bourdoux", "Sofie Pollin"], "title": "Sizing Antenna Arrays for Near-field Communication and Sensing", "comment": "Submitted to IEEE Wireless Communications Letters", "summary": "This paper presents key performance metrics for near-field communication and\nsensing systems with a focus on their scaling behavior as a function of the\nantenna array aperture. Analytical expressions are derived for several standard\narray geometries to enable the design of the large antenna arrays for given\nsystem requirements. First, the near-field beam focusing is analyzed and the\nminimum beamdepth is observed to rapidly saturate to a low asymptotic limit as\nthe array aperture increases. In contrast, the near-field region span is shown\nto scale quadratically with the array aperture. Based on these two metrics, the\nmaximum number of resolvable beamspots at 3 dB separation is derived\nanalytically, exhibiting a linear dependence on the array aperture. Finally,\nthe number of significant singular values of a channel observed at the array's\nbroadside is estimated, showing a power-law dependence on the aperture. The\nresulting expressions provide practical design guidelines for evaluating\naperture requirements in near-field communication and sensing applications."}
{"id": "2506.17778", "categories": ["cs.SD", "eess.AS", "math.HO", "20-01 (Primary), 00A08 (secondary)"], "pdf": "https://arxiv.org/pdf/2506.17778", "abs": "https://arxiv.org/abs/2506.17778", "authors": ["Veronica Flynn", "Carmen Rovi"], "title": "Algebraic Structures in Microtonal Music", "comment": "17 pages, 12 figures. The content should be accessible for students\n  in a first course of Abstract Algebra. A musical background is not necessary.\n  Comments welcome!", "summary": "We will discuss how certain group theory structures are found in music\ntheory. Western music splits the octave into 12 equal tones called half-steps.\nWe can take this division further and split the octave into 24 equal tones by\nsplitting each half-step in two, called a quarter-step. By assigning each of\nthese 24 notes a number, we can discuss musical actions mathematically. In this\npaper, we analyze 24-tone microtonal music and explore how musical and harmonic\nstructures in this system can be interpreted in terms of group-theoretic\nstructures. This work extends the study by Crans, Fiore, and Satyendra."}
{"id": "2506.18307", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18307", "abs": "https://arxiv.org/abs/2506.18307", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "title": "Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation through Quantized Distribution Fitting", "comment": "Accepted on ICASSP 2025", "summary": "Speech quality assessment (SQA) aims to evaluate the quality of speech\nsamples without relying on time-consuming listener questionnaires. Recent\nefforts have focused on training neural-based SQA models to predict the mean\nopinion score (MOS) of speech samples produced by text-to-speech or voice\nconversion systems. This paper targets the enhancement of MOS prediction\nmodels' performance. We propose a novel score aggregation method to address the\nlimitations of conventional annotations for MOS, which typically involve\nratings on a scale from 1 to 5. Our method is based on the hypothesis that\nannotators internally consider continuous scores and then choose the nearest\ndiscrete rating. By modeling this process, we approximate the generative\ndistribution of ratings by quantizing the latent continuous distribution. We\nthen use the peak of this latent distribution, estimated through the loss\nbetween the quantized distribution and annotated ratings, as a new\nrepresentative value instead of MOS. Experimental results demonstrate that\nsubstituting MOSNet's predicted target with this proposed value improves\nprediction performance."}
{"id": "2506.18748", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18748", "abs": "https://arxiv.org/abs/2506.18748", "authors": ["Yigit Berkay Uslu", "Navid NaderiAlizadeh", "Mark Eisen", "Alejandro Ribeiro"], "title": "Fast State-Augmented Learning for Wireless Resource Allocation with Dual Variable Regression", "comment": "This work has been submitted to the IEEE TSP for possible publication", "summary": "We consider resource allocation problems in multi-user wireless networks,\nwhere the goal is to optimize a network-wide utility function subject to\nconstraints on the ergodic average performance of users. We demonstrate how a\nstate-augmented graph neural network (GNN) parametrization for the resource\nallocation policy circumvents the drawbacks of the ubiquitous dual subgradient\nmethods by representing the network configurations (or states) as graphs and\nviewing dual variables as dynamic inputs to the model, viewed as graph signals\nsupported over the graphs. Lagrangian maximizing state-augmented policies are\nlearned during the offline training phase, and the dual variables evolve\nthrough gradient updates while executing the learned state-augmented policies\nduring the inference phase. Our main contributions are to illustrate how\nnear-optimal initialization of dual multipliers for faster inference can be\naccomplished with dual variable regression, leveraging a secondary GNN\nparametrization, and how maximization of the Lagrangian over the multipliers\nsampled from the dual descent dynamics substantially improves the training of\nstate-augmented models. We demonstrate the superior performance of the proposed\nalgorithm with extensive numerical experiments in a case study of transmit\npower control. Finally, we prove a convergence result and an exponential\nprobability bound on the excursions of the dual function (iterate) optimality\ngaps."}
{"id": "2506.17815", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17815", "abs": "https://arxiv.org/abs/2506.17815", "authors": ["Julien Guinot", "Alain Riou", "Elio Quinton", "Gy√∂rgy Fazekas"], "title": "SLAP: Siamese Language-Audio Pretraining Without Negative Samples for Music Understanding", "comment": "Accepted to ISMIR 2025", "summary": "Joint embedding spaces have significantly advanced music understanding and\ngeneration by linking text and audio through multimodal contrastive learning.\nHowever, these approaches face large memory requirement limitations due to\nrelying on large batch sizes to effectively utilize negative samples. Further,\nmultimodal joint embedding spaces suffer from a modality gap wherein embeddings\nfrom different modalities lie in different manifolds of the embedding space. To\naddress these challenges, we propose Siamese Language-Audio Pretraining (SLAP),\na novel multimodal pretraining framework that allows learning powerful\nrepresentations without negative samples. SLAP adapts the Bootstrap Your Own\nLatent (BYOL) paradigm for multimodal audio-text training, promoting\nscalability in training multimodal embedding spaces.\n  We illustrate the ability of our model to learn meaningful relationships\nbetween music and text -- specifically, we show that SLAP outperforms CLAP on\ntasks such as text-music retrieval and zero-shot classification. We also\nobserve competitive downstream performance on several MIR tasks, including with\nlarger or supervised models (genre and instrument classification,\nauto-tagging). Additionally, our approach has attractive properties, such as a\nquantifiably reduced modality gap and improved robustness to batch size\nvariations on retrieval performance. Finally, its novel formulation unlocks\nlarge-scale training on a single GPU through gradient accumulation."}
{"id": "2506.18312", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18312", "abs": "https://arxiv.org/abs/2506.18312", "authors": ["Woosung Choi", "Junghyun Koo", "Kin Wai Cheuk", "Joan Serr√†", "Marco A. Mart√≠nez-Ram√≠rez", "Yukara Ikemiya", "Naoki Murata", "Yuhta Takida", "Wei-Hsiang Liao", "Yuki Mitsufuji"], "title": "Large-Scale Training Data Attribution for Music Generative Models via Unlearning", "comment": "accepted at ICML 2025 Workshop on Machine Learning for Audio", "summary": "This paper explores the use of unlearning methods for training data\nattribution (TDA) in music generative models trained on large-scale datasets.\nTDA aims to identify which specific training data points contributed to the\ngeneration of a particular output from a specific model. This is crucial in the\ncontext of AI-generated music, where proper recognition and credit for original\nartists are generally overlooked. By enabling white-box attribution, our work\nsupports a fairer system for acknowledging artistic contributions and addresses\npressing concerns related to AI ethics and copyright. We apply unlearning-based\nattribution to a text-to-music diffusion model trained on a large-scale dataset\nand investigate its feasibility and behavior in this setting. To validate the\nmethod, we perform a grid search over different hyperparameter configurations\nand quantitatively evaluate the consistency of the unlearning approach. We then\ncompare attribution patterns from unlearning with those from a similarity-based\napproach. Our findings suggest that unlearning-based approaches can be\neffectively adapted to music generative models, introducing large-scale TDA to\nthis domain and paving the way for more ethical and accountable AI systems for\nmusic creation."}
{"id": "2506.18863", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18863", "abs": "https://arxiv.org/abs/2506.18863", "authors": ["Sajjad Nassirpour", "Toan-Van Nguyen", "Hien Q. Ngo", "Le-Nam Tran", "Tharmalingam Ratnarajah", "Duy H. N. Nguyen"], "title": "Variational Bayesian Channel Estimation and Data Detection for Cell-Free Massive MIMO with Low-Resolution Quantized Fronthaul Links", "comment": "14 pages, 10 figures, accepted for journal publication", "summary": "We study the joint channel estimation and data detection (JED) problem in a\ncell-free massive multiple-input multiple-output (CF-mMIMO) network, where\naccess points (APs) communicate with a central processing unit (CPU) over\nfronthaul links. However, the bandwidth of these links is limited, and thus,\npresents challenges to the applicability of CF-mMIMO, especially with an\never-increasing number of users. To address this, we propose a method based on\nvariational Bayesian (VB) inference for performing the JED process, where the\nAPs forward low-resolution quantized versions of the signals to the CPU. We\nconsider two approaches: \\emph{quantization-and-estimation} (Q-E) and\n\\emph{estimation-and-quantization} (E-Q). In the Q-E approach, each AP uses a\nlow-bit quantizer to quantize the signal before forwarding it to the CPU, while\nin the E-Q approach, each AP first performs local channel estimation and then\nsends a low-bit quantized version of the estimated channel to the CPU. We\nevaluate the performance of our VB-based approach under perfect fronthaul link\n(PFL) with unquantized received signals, Q-E, and E-Q in terms of symbol error\nrate (SER), normalized mean square error (NMSE) of the channel estimation,\ncomputational complexity, and fronthaul signaling overhead. We also compare\nthese results with those of the linear minimum mean squared error (LMMSE)\nmethod under the PFL scenario. Our numerical results show that both the VB(Q-E)\nand VB(E-Q) approaches achieve superior performance compared to LMMSE(PFL),\nbenefiting from the nonlinear modeling inherent in VB. Furthermore, the VB(Q-E)\nmethod outperforms VB(E-Q) due to errors in the local channel estimation\nprocess at the APs within the VB(E-Q) approach."}
{"id": "2506.17818", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17818", "abs": "https://arxiv.org/abs/2506.17818", "authors": ["Angelos-Nikolaos Kanatas", "Charilaos Papaioannou", "Alexandros Potamianos"], "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning", "comment": "10 pages, 4 figures, accepted to the 26th International Society for\n  Music Information Retrieval conference (ISMIR 2025), to be held in Daejeon,\n  South Korea", "summary": "Recent advances in music foundation models have improved audio representation\nlearning, yet their effectiveness across diverse musical traditions remains\nlimited. We introduce CultureMERT-95M, a multi-culturally adapted foundation\nmodel developed to enhance cross-cultural music representation learning and\nunderstanding. To achieve this, we propose a two-stage continual pre-training\nstrategy that integrates learning rate re-warming and re-decaying, enabling\nstable adaptation even with limited computational resources. Training on a\n650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music\ntraditions, results in an average improvement of 4.9% in ROC-AUC and AP across\ndiverse non-Western music auto-tagging tasks, surpassing prior\nstate-of-the-art, with minimal forgetting on Western-centric benchmarks. We\nfurther investigate task arithmetic, an alternative approach to multi-cultural\nadaptation that merges single-culture adapted models in the weight space. Task\narithmetic performs on par with our multi-culturally trained model on\nnon-Western auto-tagging tasks and shows no regression on Western datasets.\nCross-cultural evaluation reveals that single-culture models transfer with\nvarying effectiveness across musical traditions, whereas the multi-culturally\nadapted model achieves the best overall performance. To support research on\nworld music representation learning, we publicly release CultureMERT-95M and\nCultureMERT-TA-95M, fostering the development of more culturally aware music\nfoundation models."}
{"id": "2506.18326", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18326", "abs": "https://arxiv.org/abs/2506.18326", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "title": "Selecting N-lowest scores for training MOS prediction models", "comment": "Accepted on ICASSP 2024", "summary": "The automatic speech quality assessment (SQA) has been extensively studied to\npredict the speech quality without time-consuming questionnaires. Recently,\nneural-based SQA models have been actively developed for speech samples\nproduced by text-to-speech or voice conversion, with a primary focus on\ntraining mean opinion score (MOS) prediction models. The quality of each speech\nsample may not be consistent across the entire duration, and it remains unclear\nwhich segments of the speech receive the primary focus from humans when\nassigning subjective evaluation for MOS calculation. We hypothesize that when\nhumans rate speech, they tend to assign more weight to low-quality speech\nsegments, and the variance in ratings for each sample is mainly due to\naccidental assignment of higher scores when overlooking the poor quality speech\nsegments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC\ndatasets. Based on the hypothesis, we propose the more reliable representative\nvalue N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments\nshow that LCC and SRCC improve compared to regular MOS when employing N_low-MOS\nto MOSNet training. This result suggests that N_low-MOS is a more intrinsic\nrepresentative value of subjective speech quality and makes MOSNet a better\ncomparator of VC models."}
{"id": "2506.18864", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.18864", "abs": "https://arxiv.org/abs/2506.18864", "authors": ["Hossein Kazemi", "Isaac N. O. Osahon", "Nikolay Ledentsov Jr.", "Ilya Titkov", "Nikolay Ledentsov", "Harald Haas"], "title": "Achieving 70 Gb/s Over A VCSEL-Based Optical Wireless Link Using A Multi-Mode Fiber-Coupled Receiver", "comment": "6 pages, 5 figures, 1 table", "summary": "In this paper, we demonstrate a laser-based optical wireless communication\n(OWC) system employing a 940 nm single-mode (SM) vertical cavity surface\nemitting laser (VCSEL) and a multi-mode (MM) fiber-coupled receiver, achieving\na record data rate beyond 70 Gb/s, while the optical transmit power is below 5\nmW. The use of a high speed fiber-optic photoreceiver avoids limiting the\ncommunication bandwidth by the receiver, enabling ultra-high capacity and\nenergy-efficient light fidelity (LiFi) links to unlock new applications. This\nwork experimentally validates the feasibility of ultra-high speed indoor OWC\nsystems using a single low-power and low-cost VCSEL for next-generation LiFi\nconnectivity."}
{"id": "2506.17886", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.17886", "abs": "https://arxiv.org/abs/2506.17886", "authors": ["Julien Guinot", "Elio Quinton", "Gy√∂rgy Fazekas"], "title": "GD-Retriever: Controllable Generative Text-Music Retrieval with Diffusion Models", "comment": "Accepted to ISMIR 2025", "summary": "Multimodal contrastive models have achieved strong performance in text-audio\nretrieval and zero-shot settings, but improving joint embedding spaces remains\nan active research area. Less attention has been given to making these systems\ncontrollable and interactive for users. In text-music retrieval, the ambiguity\nof freeform language creates a many-to-many mapping, often resulting in\ninflexible or unsatisfying results.\n  We introduce Generative Diffusion Retriever (GDR), a novel framework that\nleverages diffusion models to generate queries in a retrieval-optimized latent\nspace. This enables controllability through generative tools such as negative\nprompting and denoising diffusion implicit models (DDIM) inversion, opening a\nnew direction in retrieval control. GDR improves retrieval performance over\ncontrastive teacher models and supports retrieval in audio-only latent spaces\nusing non-jointly trained encoders. Finally, we demonstrate that GDR enables\neffective post-hoc manipulation of retrieval behavior, enhancing interactive\ncontrol for text-music retrieval tasks."}
{"id": "2506.18488", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18488", "abs": "https://arxiv.org/abs/2506.18488", "authors": ["Markus Frohmann", "Elena V. Epure", "Gabriel Meseguer-Brocal", "Markus Schedl", "Romain Hennequin"], "title": "AI-Generated Song Detection via Lyrics Transcripts", "comment": "Accepted to ISMIR 2025", "summary": "The recent rise in capabilities of AI-based music generation tools has\ncreated an upheaval in the music industry, necessitating the creation of\naccurate methods to detect such AI-generated content. This can be done using\naudio-based detectors; however, it has been shown that they struggle to\ngeneralize to unseen generators or when the audio is perturbed. Furthermore,\nrecent work used accurate and cleanly formatted lyrics sourced from a lyrics\nprovider database to detect AI-generated music. However, in practice, such\nperfect lyrics are not available (only the audio is); this leaves a substantial\ngap in applicability in real-life use cases. In this work, we instead propose\nsolving this gap by transcribing songs using general automatic speech\nrecognition (ASR) models. We do this using several detectors. The results on\ndiverse, multi-genre, and multi-lingual lyrics show generally strong detection\nperformance across languages and genres, particularly for our best-performing\nmodel using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that\nour method is more robust than state-of-the-art audio-based ones when the audio\nis perturbed in different ways and when evaluated on different music\ngenerators. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection."}
{"id": "2506.17409", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.17409", "abs": "https://arxiv.org/abs/2506.17409", "authors": ["Quoc Thinh Vo", "Joe Woods", "Priontu Chowdhury", "David K. Han"], "title": "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation", "comment": "This paper has been accepted for the 33rd European Signal Processing\n  Conference (EUSIPCO) 2025 in Palermo, Italy", "summary": "Localizing acoustic sound sources in the ocean is a challenging task due to\nthe complex and dynamic nature of the environment. Factors such as high\nbackground noise, irregular underwater geometries, and varying acoustic\nproperties make accurate localization difficult. To address these obstacles, we\npropose a multi-branch network architecture designed to accurately predict the\ndistance between a moving acoustic source and a receiver, tested on real-world\nunderwater signal arrays. The network leverages Convolutional Neural Networks\n(CNNs) for robust spatial feature extraction and integrates Conformers with\nself-attention mechanism to effectively capture temporal dependencies. Log-mel\nspectrogram and generalized cross-correlation with phase transform (GCC-PHAT)\nfeatures are employed as input representations. To further enhance the model\nperformance, we introduce an Adaptive Gain Control (AGC) layer, that adaptively\nadjusts the amplitude of input features, ensuring consistent energy levels\nacross varying ranges, signal strengths, and noise conditions. We assess the\nmodel's generalization capability by training it in one domain and testing it\nin a different domain, using only a limited amount of data from the test domain\nfor fine-tuning. Our proposed method outperforms state-of-the-art (SOTA)\napproaches in similar settings, establishing new benchmarks for underwater\nsound localization."}
{"id": "2506.18182", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18182", "abs": "https://arxiv.org/abs/2506.18182", "authors": ["Rita Singh", "Bhiksha Raj"], "title": "Human Voice is Unique", "comment": "15 pages, 1 figure, 2 tables", "summary": "Voice is increasingly being used as a biometric entity in many applications.\nThese range from speaker identification and verification systems to human\nprofiling technologies that attempt to estimate myriad aspects of the speaker's\npersona from their voice. However, for an entity to be a true biometric\nidentifier, it must be unique. This paper establishes a first framework for\ncalculating the uniqueness of human voice objectively. The approach in this\npaper is based on statistical considerations that take into account a set of\nmeasurable characteristics of the voice signal that bear a causal relationship\nto the vocal production process, but are not inter-dependent or derivable from\neach other. Depending on how we quantize these variables, we show that the\nchances of two people having the same voice in a world populated by 10 billion\npeople range from one in a few thousand, to one in a septillion or less. The\npaper also discusses the implications of these calculations on the choices made\nin voice processing applications."}
{"id": "2506.18510", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18510", "abs": "https://arxiv.org/abs/2506.18510", "authors": ["Duygu Altinok"], "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts", "comment": "Accepted to INTERSPEECH2025 workshop DISS2025", "summary": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints."}
{"id": "2506.18296", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18296", "abs": "https://arxiv.org/abs/2506.18296", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "title": "JIS: A Speech Corpus of Japanese Idol Speakers with Various Speaking Styles", "comment": "Accepted on Interspeech 2025", "summary": "We construct Japanese Idol Speech Corpus (JIS) to advance research in speech\ngeneration AI, including text-to-speech synthesis (TTS) and voice conversion\n(VC). JIS will facilitate more rigorous evaluations of speaker similarity in\nTTS and VC systems since all speakers in JIS belong to a highly specific\ncategory: \"young female live idols\" in Japan, and each speaker is identified by\na stage name, enabling researchers to recruit listeners familiar with these\nidols for listening experiments. With its unique speaker attributes, JIS will\nfoster compelling research, including generating voices tailored to listener\npreferences-an area not yet widely studied. JIS will be distributed free of\ncharge to promote research in speech generation AI, with usage restricted to\nnon-commercial, basic research. We describe the construction of JIS, provide an\noverview of Japanese live idol culture to support effective and ethical use of\nJIS, and offer a basic analysis to guide application of JIS."}
{"id": "2506.18671", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18671", "abs": "https://arxiv.org/abs/2506.18671", "authors": ["Yuqin Dai", "Wanlu Zhu", "Ronghui Li", "Xiu Li", "Zhenyu Zhang", "Jun Li", "Jian Yang"], "title": "TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography", "comment": null, "summary": "Music-driven dance generation has garnered significant attention due to its\nwide range of industrial applications, particularly in the creation of group\nchoreography. During the group dance generation process, however, most existing\nmethods still face three primary issues: multi-dancer collisions, single-dancer\nfoot sliding and abrupt swapping in the generation of long group dance. In this\npaper, we propose TCDiff++, a music-driven end-to-end framework designed to\ngenerate harmonious group dance. Specifically, to mitigate multi-dancer\ncollisions, we utilize a dancer positioning embedding to better maintain the\nrelative positioning among dancers. Additionally, we incorporate a\ndistance-consistency loss to ensure that inter-dancer distances remain within\nplausible ranges. To address the issue of single-dancer foot sliding, we\nintroduce a swap mode embedding to indicate dancer swapping patterns and design\na Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For\nlong group dance generation, we present a long group diffusion sampling\nstrategy that reduces abrupt position shifts by injecting positional\ninformation into the noisy input. Furthermore, we integrate a Sequence Decoder\nlayer to enhance the model's ability to selectively process long sequences.\nExtensive experiments demonstrate that our TCDiff++ achieves state-of-the-art\nperformance, particularly in long-duration scenarios, ensuring high-quality and\ncoherent group dance generation."}
{"id": "2506.18307", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18307", "abs": "https://arxiv.org/abs/2506.18307", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "title": "Rethinking Mean Opinion Scores in Speech Quality Assessment: Aggregation through Quantized Distribution Fitting", "comment": "Accepted on ICASSP 2025", "summary": "Speech quality assessment (SQA) aims to evaluate the quality of speech\nsamples without relying on time-consuming listener questionnaires. Recent\nefforts have focused on training neural-based SQA models to predict the mean\nopinion score (MOS) of speech samples produced by text-to-speech or voice\nconversion systems. This paper targets the enhancement of MOS prediction\nmodels' performance. We propose a novel score aggregation method to address the\nlimitations of conventional annotations for MOS, which typically involve\nratings on a scale from 1 to 5. Our method is based on the hypothesis that\nannotators internally consider continuous scores and then choose the nearest\ndiscrete rating. By modeling this process, we approximate the generative\ndistribution of ratings by quantizing the latent continuous distribution. We\nthen use the peak of this latent distribution, estimated through the loss\nbetween the quantized distribution and annotated ratings, as a new\nrepresentative value instead of MOS. Experimental results demonstrate that\nsubstituting MOSNet's predicted target with this proposed value improves\nprediction performance."}
{"id": "2506.18691", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18691", "abs": "https://arxiv.org/abs/2506.18691", "authors": ["Nasser-Eddine Monir", "Paul Magron", "Romain Serizel"], "title": "Evaluating Multichannel Speech Enhancement Algorithms at the Phoneme Scale Across Genders", "comment": null, "summary": "Multichannel speech enhancement algorithms are essential for improving the\nintelligibility of speech signals in noisy environments. These algorithms are\nusually evaluated at the utterance level, but this approach overlooks the\ndisparities in acoustic characteristics that are observed in different phoneme\ncategories and between male and female speakers. In this paper, we investigate\nthe impact of gender and phonetic content on speech enhancement algorithms. We\nmotivate this approach by outlining phoneme- and gender-specific spectral\nfeatures. Our experiments reveal that while utterance-level differences between\ngenders are minimal, significant variations emerge at the phoneme level.\nResults show that the tested algorithms better reduce interference with fewer\nartifacts on female speech, particularly in plosives, fricatives, and vowels.\nAdditionally, they demonstrate greater performance for female speech in terms\nof perceptual and speech recognition metrics."}
{"id": "2506.18312", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18312", "abs": "https://arxiv.org/abs/2506.18312", "authors": ["Woosung Choi", "Junghyun Koo", "Kin Wai Cheuk", "Joan Serr√†", "Marco A. Mart√≠nez-Ram√≠rez", "Yukara Ikemiya", "Naoki Murata", "Yuhta Takida", "Wei-Hsiang Liao", "Yuki Mitsufuji"], "title": "Large-Scale Training Data Attribution for Music Generative Models via Unlearning", "comment": "accepted at ICML 2025 Workshop on Machine Learning for Audio", "summary": "This paper explores the use of unlearning methods for training data\nattribution (TDA) in music generative models trained on large-scale datasets.\nTDA aims to identify which specific training data points contributed to the\ngeneration of a particular output from a specific model. This is crucial in the\ncontext of AI-generated music, where proper recognition and credit for original\nartists are generally overlooked. By enabling white-box attribution, our work\nsupports a fairer system for acknowledging artistic contributions and addresses\npressing concerns related to AI ethics and copyright. We apply unlearning-based\nattribution to a text-to-music diffusion model trained on a large-scale dataset\nand investigate its feasibility and behavior in this setting. To validate the\nmethod, we perform a grid search over different hyperparameter configurations\nand quantitatively evaluate the consistency of the unlearning approach. We then\ncompare attribution patterns from unlearning with those from a similarity-based\napproach. Our findings suggest that unlearning-based approaches can be\neffectively adapted to music generative models, introducing large-scale TDA to\nthis domain and paving the way for more ethical and accountable AI systems for\nmusic creation."}
{"id": "2506.18714", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18714", "abs": "https://arxiv.org/abs/2506.18714", "authors": ["Nasser-Eddine Monir", "Paul Magron", "Romain Serizel"], "title": "Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement", "comment": "This is the preprint of the paper submitted to the 26th IEEE\n  International Workshop on Multimedia Signal Processing (MMSP)", "summary": "Recent advances in deep learning have significantly improved multichannel\nspeech enhancement algorithms, yet conventional training loss functions such as\nthe scale-invariant signal-to-distortion ratio (SDR) may fail to preserve\nfine-grained spectral cues essential for phoneme intelligibility. In this work,\nwe propose perceptually-informed variants of the SDR loss, formulated in the\ntime-frequency domain and modulated by frequency-dependent weighting schemes.\nThese weights are designed to emphasize time-frequency regions where speech is\nprominent or where the interfering noise is particularly strong. We investigate\nboth fixed and adaptive strategies, including ANSI band-importance weights,\nspectral magnitude-based weighting, and dynamic weighting based on the relative\namount of speech and noise. We train the FaSNet multichannel speech enhancement\nmodel using these various losses. Experimental results show that while standard\nmetrics such as the SDR are only marginally improved, their perceptual\nfrequency-weighted counterparts exhibit a more substantial improvement.\nBesides, spectral and phoneme-level analysis indicates better consonant\nreconstruction, which points to a better preservation of certain acoustic cues."}
{"id": "2506.18326", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18326", "abs": "https://arxiv.org/abs/2506.18326", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko"], "title": "Selecting N-lowest scores for training MOS prediction models", "comment": "Accepted on ICASSP 2024", "summary": "The automatic speech quality assessment (SQA) has been extensively studied to\npredict the speech quality without time-consuming questionnaires. Recently,\nneural-based SQA models have been actively developed for speech samples\nproduced by text-to-speech or voice conversion, with a primary focus on\ntraining mean opinion score (MOS) prediction models. The quality of each speech\nsample may not be consistent across the entire duration, and it remains unclear\nwhich segments of the speech receive the primary focus from humans when\nassigning subjective evaluation for MOS calculation. We hypothesize that when\nhumans rate speech, they tend to assign more weight to low-quality speech\nsegments, and the variance in ratings for each sample is mainly due to\naccidental assignment of higher scores when overlooking the poor quality speech\nsegments. Motivated by the hypothesis, we analyze the VCC2018 and BVCC\ndatasets. Based on the hypothesis, we propose the more reliable representative\nvalue N_low-MOS, the mean of the $N$-lowest opinion scores. Our experiments\nshow that LCC and SRCC improve compared to regular MOS when employing N_low-MOS\nto MOSNet training. This result suggests that N_low-MOS is a more intrinsic\nrepresentative value of subjective speech quality and makes MOSNet a better\ncomparator of VC models."}
{"id": "2506.18729", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18729", "abs": "https://arxiv.org/abs/2506.18729", "authors": ["Fang-Duo Tsai", "Shih-Lun Wu", "Weijaw Lee", "Sheng-Ping Yang", "Bo-Rui Chen", "Hao-Chung Cheng", "Yi-Hsuan Yang"], "title": "MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners", "comment": "Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune\ntext-to-music generation models for precise conditioning using various\ntime-varying musical attributes and reference audio signals. The key finding is\nthat positional embeddings, which have been seldom used by text-to-music\ngeneration models in the conditioner for text conditions, are critical when the\ncondition of interest is a function of time. Using melody control as an\nexample, our experiments show that simply adding rotary positional embeddings\nto the decoupled cross-attention layers increases control accuracy from 56.6%\nto 61.1%, while requiring 6.75 times fewer trainable parameters than\nstate-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion\nTransformer model of Stable Audio Open. We evaluate various forms of musical\nattribute control, audio inpainting, and audio outpainting, demonstrating\nimproved controllability over MusicGen-Large and Stable Audio Open ControlNet\nat a significantly lower fine-tuning cost, with only 85M trainble parameters.\nSource code, model checkpoints, and demo examples are available at: https:\n//MuseControlLite.github.io/web/."}
{"id": "2506.18510", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18510", "abs": "https://arxiv.org/abs/2506.18510", "authors": ["Duygu Altinok"], "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts", "comment": "Accepted to INTERSPEECH2025 workshop DISS2025", "summary": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints."}
{"id": "2506.18843", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18843", "abs": "https://arxiv.org/abs/2506.18843", "authors": ["Heng-Jui Chang", "Saurabhchand Bhati", "James Glass", "Alexander H. Liu"], "title": "USAD: Universal Speech and Audio Representation via Distillation", "comment": "Preprint", "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks."}
{"id": "2506.18671", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18671", "abs": "https://arxiv.org/abs/2506.18671", "authors": ["Yuqin Dai", "Wanlu Zhu", "Ronghui Li", "Xiu Li", "Zhenyu Zhang", "Jun Li", "Jian Yang"], "title": "TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography", "comment": null, "summary": "Music-driven dance generation has garnered significant attention due to its\nwide range of industrial applications, particularly in the creation of group\nchoreography. During the group dance generation process, however, most existing\nmethods still face three primary issues: multi-dancer collisions, single-dancer\nfoot sliding and abrupt swapping in the generation of long group dance. In this\npaper, we propose TCDiff++, a music-driven end-to-end framework designed to\ngenerate harmonious group dance. Specifically, to mitigate multi-dancer\ncollisions, we utilize a dancer positioning embedding to better maintain the\nrelative positioning among dancers. Additionally, we incorporate a\ndistance-consistency loss to ensure that inter-dancer distances remain within\nplausible ranges. To address the issue of single-dancer foot sliding, we\nintroduce a swap mode embedding to indicate dancer swapping patterns and design\na Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For\nlong group dance generation, we present a long group diffusion sampling\nstrategy that reduces abrupt position shifts by injecting positional\ninformation into the noisy input. Furthermore, we integrate a Sequence Decoder\nlayer to enhance the model's ability to selectively process long sequences.\nExtensive experiments demonstrate that our TCDiff++ achieves state-of-the-art\nperformance, particularly in long-duration scenarios, ensuring high-quality and\ncoherent group dance generation."}
{"id": "2506.17686", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.17686", "abs": "https://arxiv.org/abs/2506.17686", "authors": ["Alican Gok", "Oguzhan Buyuksolak", "Osman Erman Okman", "Murat Saraclar"], "title": "Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models", "comment": "To be submitted to IEEE Signal Processing Letters, 5 pages, 3 figures", "summary": "Keyword Spotting plays a critical role in enabling hands-free interaction for\nbattery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the\nscalability and adaptability challenges of traditional systems by enabling\nrecognition of custom keywords with only a few examples. However, existing\nFS-KWS systems achieve subpar accuracy at desirable false acceptance rates,\nparticularly in resource-constrained edge environments. To address these\nissues, we propose a training scheme that leverages self-supervised learning\nmodels for robust feature extraction, dimensionality reduction, and knowledge\ndistillation. The teacher model, based on Wav2Vec 2.0 is trained using\nSub-center ArcFace loss, which enhances inter-class separability and\nintra-class compactness. To enable efficient deployment on edge devices, we\nintroduce attention-based dimensionality reduction and train a standard\nlightweight ResNet15 student model. We evaluate the proposed approach on the\nEnglish portion of the Multilingual Spoken Words Corpus (MSWC) and the Google\nSpeech Commands (GSC) datasets. Notably, the proposed training method improves\nthe 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%\nfalse alarm accuracy on the GSC dataset, thus making it significantly\nbetter-suited for a real use case scenario."}
{"id": "2506.18691", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18691", "abs": "https://arxiv.org/abs/2506.18691", "authors": ["Nasser-Eddine Monir", "Paul Magron", "Romain Serizel"], "title": "Evaluating Multichannel Speech Enhancement Algorithms at the Phoneme Scale Across Genders", "comment": null, "summary": "Multichannel speech enhancement algorithms are essential for improving the\nintelligibility of speech signals in noisy environments. These algorithms are\nusually evaluated at the utterance level, but this approach overlooks the\ndisparities in acoustic characteristics that are observed in different phoneme\ncategories and between male and female speakers. In this paper, we investigate\nthe impact of gender and phonetic content on speech enhancement algorithms. We\nmotivate this approach by outlining phoneme- and gender-specific spectral\nfeatures. Our experiments reveal that while utterance-level differences between\ngenders are minimal, significant variations emerge at the phoneme level.\nResults show that the tested algorithms better reduce interference with fewer\nartifacts on female speech, particularly in plosives, fricatives, and vowels.\nAdditionally, they demonstrate greater performance for female speech in terms\nof perceptual and speech recognition metrics."}
{"id": "2506.18281", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.18281", "abs": "https://arxiv.org/abs/2506.18281", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Blind Source Separation in Biomedical Signals Using Variational Methods", "comment": "Presented at Southern Ontario Numerical Analysis Day (SONAD'25),\n  Contributed Talk 03", "summary": "This study introduces a novel unsupervised approach for separating\noverlapping heart and lung sounds using variational autoencoders (VAEs). In\nclinical settings, these sounds often interfere with each other, making manual\nseparation difficult and error-prone. The proposed model learns to encode mixed\nsignals into a structured latent space and reconstructs the individual\ncomponents using a probabilistic decoder, all without requiring labeled data or\nprior knowledge of source characteristics. We apply this method to real\nrecordings obtained from a clinical manikin using a digital stethoscope.\nResults demonstrate distinct latent clusters corresponding to heart and lung\nsources, as well as accurate reconstructions that preserve key spectral\nfeatures of the original signals. The approach offers a robust and\ninterpretable solution for blind source separation and has potential\napplications in portable diagnostic tools and intelligent stethoscope systems."}
{"id": "2506.18714", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18714", "abs": "https://arxiv.org/abs/2506.18714", "authors": ["Nasser-Eddine Monir", "Paul Magron", "Romain Serizel"], "title": "Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement", "comment": "This is the preprint of the paper submitted to the 26th IEEE\n  International Workshop on Multimedia Signal Processing (MMSP)", "summary": "Recent advances in deep learning have significantly improved multichannel\nspeech enhancement algorithms, yet conventional training loss functions such as\nthe scale-invariant signal-to-distortion ratio (SDR) may fail to preserve\nfine-grained spectral cues essential for phoneme intelligibility. In this work,\nwe propose perceptually-informed variants of the SDR loss, formulated in the\ntime-frequency domain and modulated by frequency-dependent weighting schemes.\nThese weights are designed to emphasize time-frequency regions where speech is\nprominent or where the interfering noise is particularly strong. We investigate\nboth fixed and adaptive strategies, including ANSI band-importance weights,\nspectral magnitude-based weighting, and dynamic weighting based on the relative\namount of speech and noise. We train the FaSNet multichannel speech enhancement\nmodel using these various losses. Experimental results show that while standard\nmetrics such as the SDR are only marginally improved, their perceptual\nfrequency-weighted counterparts exhibit a more substantial improvement.\nBesides, spectral and phoneme-level analysis indicates better consonant\nreconstruction, which points to a better preservation of certain acoustic cues."}
{"id": "2506.18729", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18729", "abs": "https://arxiv.org/abs/2506.18729", "authors": ["Fang-Duo Tsai", "Shih-Lun Wu", "Weijaw Lee", "Sheng-Ping Yang", "Bo-Rui Chen", "Hao-Chung Cheng", "Yi-Hsuan Yang"], "title": "MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners", "comment": "Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune\ntext-to-music generation models for precise conditioning using various\ntime-varying musical attributes and reference audio signals. The key finding is\nthat positional embeddings, which have been seldom used by text-to-music\ngeneration models in the conditioner for text conditions, are critical when the\ncondition of interest is a function of time. Using melody control as an\nexample, our experiments show that simply adding rotary positional embeddings\nto the decoupled cross-attention layers increases control accuracy from 56.6%\nto 61.1%, while requiring 6.75 times fewer trainable parameters than\nstate-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion\nTransformer model of Stable Audio Open. We evaluate various forms of musical\nattribute control, audio inpainting, and audio outpainting, demonstrating\nimproved controllability over MusicGen-Large and Stable Audio Open ControlNet\nat a significantly lower fine-tuning cost, with only 85M trainble parameters.\nSource code, model checkpoints, and demo examples are available at: https:\n//MuseControlLite.github.io/web/."}
{"id": "2506.18843", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18843", "abs": "https://arxiv.org/abs/2506.18843", "authors": ["Heng-Jui Chang", "Saurabhchand Bhati", "James Glass", "Alexander H. Liu"], "title": "USAD: Universal Speech and Audio Representation via Distillation", "comment": "Preprint", "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks."}

{"id": "2506.18954", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18954", "abs": "https://arxiv.org/abs/2506.18954", "authors": ["Diego Di Carlo", "Mathieu Fontaine", "Aditya Arie Nugraha", "Yoshiaki Bando", "Kazuyoshi Yoshii"], "title": "SHAMaNS: Sound Localization with Hybrid Alpha-Stable Spatial Measure and Neural Steerer", "comment": "European Signal Processing Conference (EUSIPCO), Sep 2025, Palermo,\n  Italy", "summary": "This paper describes a sound source localization (SSL) technique that\ncombines an $\\alpha$-stable model for the observed signal with a neural\nnetwork-based approach for modeling steering vectors. Specifically, a\nphysics-informed neural network, referred to as Neural Steerer, is used to\ninterpolate measured steering vectors (SVs) on a fixed microphone array. This\nallows for a more robust estimation of the so-called $\\alpha$-stable spatial\nmeasure, which represents the most plausible direction of arrival (DOA) of a\ntarget signal. As an $\\alpha$-stable model for the non-Gaussian case ($\\alpha$\n$\\in$ (0, 2)) theoretically defines a unique spatial measure, we choose to\nleverage it to account for residual reconstruction error of the Neural Steerer\nin the downstream tasks. The objective scores indicate that our proposed\ntechnique outperforms state-of-the-art methods in the case of multiple sound\nsources."}
{"id": "2506.19014", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19014", "abs": "https://arxiv.org/abs/2506.19014", "authors": ["Abhay Kumar", "Kunal Verma", "Omkar More"], "title": "IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection", "comment": null, "summary": "Advancements in audio deepfake technology offers benefits like AI assistants,\nbetter accessibility for speech impairments, and enhanced entertainment.\nHowever, it also poses significant risks to security, privacy, and trust in\ndigital communications. Detecting and mitigating these threats requires\ncomprehensive datasets. Existing datasets lack diverse ethnic accents, making\nthem inadequate for many real-world scenarios. Consequently, models trained on\nthese datasets struggle to detect audio deepfakes in diverse linguistic and\ncultural contexts such as in South-Asian countries. Ironically, there is a\nstark lack of South-Asian speaker samples in the existing datasets despite\nconstituting a quarter of the worlds population. This work introduces the\nIndieFake Dataset (IFD), featuring 27.17 hours of bonafide and deepfake audio\nfrom 50 English speaking Indian speakers. IFD offers balanced data distribution\nand includes speaker-level characterization, absent in datasets like ASVspoof21\n(DF). We evaluated various baselines on IFD against existing ASVspoof21 (DF)\nand In-The-Wild (ITW) datasets. IFD outperforms ASVspoof21 (DF) and proves to\nbe more challenging compared to benchmark ITW dataset. The dataset will be\npublicly available upon acceptance."}
{"id": "2506.19108", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19108", "abs": "https://arxiv.org/abs/2506.19108", "authors": ["Darius Afchar", "Gabriel Meseguer-Brocal", "Kamil Akesbi", "Romain Hennequin"], "title": "A Fourier Explanation of AI-music Artifacts", "comment": "Accepted at ISMIR 2025", "summary": "The rapid rise of generative AI has transformed music creation, with millions\nof users engaging in AI-generated music. Despite its popularity, concerns\nregarding copyright infringement, job displacement, and ethical implications\nhave led to growing scrutiny and legal challenges. In parallel, AI-detection\nservices have emerged, yet these systems remain largely opaque and privately\ncontrolled, mirroring the very issues they aim to address. This paper explores\nthe fundamental properties of synthetic content and how it can be detected.\nSpecifically, we analyze deconvolution modules commonly used in generative\nmodels and mathematically prove that their outputs exhibit systematic frequency\nartifacts -- manifesting as small yet distinctive spectral peaks. This\nphenomenon, related to the well-known checkerboard artifact, is shown to be\ninherent to a chosen model architecture rather than a consequence of training\ndata or model weights. We validate our theoretical findings through extensive\nexperiments on open-source models, as well as commercial AI-music generators\nsuch as Suno and Udio. We use these insights to propose a simple and\ninterpretable detection criterion for AI-generated music. Despite its\nsimplicity, our method achieves detection accuracy on par with deep\nlearning-based approaches, surpassing 99% accuracy on several scenarios."}
{"id": "2506.19253", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19253", "abs": "https://arxiv.org/abs/2506.19253", "authors": ["Sajad Sadeghkhani", "Maryam Karimi Boroujeni", "Hilmi R. Dajani", "Saeid R. Seydnejad", "Christian Giguère"], "title": "A Robust Method for Pitch Tracking in the Frequency Following Response using Harmonic Amplitude Summation Filterbank", "comment": null, "summary": "The Frequency Following Response (FFR) reflects the brain's neural encoding\nof auditory stimuli including speech. Because the fundamental frequency (F0), a\nphysical correlate of pitch, is one of the essential features of speech, there\nhas been particular interest in characterizing the FFR at F0, especially when\nF0 varies over time. The standard method for extracting F0 in FFRs has been the\nAutocorrelation Function (ACF). This paper investigates\nharmonic-structure-based F0 estimation algorithms, originally developed for\nspeech and music, and resolves their poor performance when applied to FFRs in\ntwo steps. Firstly, given that unlike in speech or music, stimulus F0 of FFRs\nis already known, we introduce a stimulus-aware filterbank that selectively\naggregates amplitudes at F0 and its harmonics while suppressing noise at\nnon-harmonic frequencies. This method, called Harmonic Amplitude Summation\n(HAS), evaluates F0 candidates only within a range centered around the stimulus\nF0. Secondly, unlike other pitch tracking methods that select the highest peak,\nour method chooses the most prominent one, as it better reflects the underlying\nperiodicity of FFRs. To the best of our knowledge, this is the first study to\npropose an F0 estimation algorithm for FFRs that relies on harmonic structure.\nAnalyzing recorded FFRs from 16 normal hearing subjects to 4 natural speech\nstimuli with a wide F0 variation from 89 Hz to 452 Hz showed that this method\noutperformed ACF by reducing the average Root-Mean-Square-Error (RMSE) within\neach response and stimulus F0 contour pair by 8.8% to 47.4%, depending on the\nstimulus."}
{"id": "2506.19404", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19404", "abs": "https://arxiv.org/abs/2506.19404", "authors": ["Boaz Rafaely", "Stefan Weinzierl", "Or Berebi", "Fabian Brinkmann"], "title": "Loss functions incorporating auditory spatial perception in deep learning -- a review", "comment": "Submitted to I3DA 2025", "summary": "Binaural reproduction aims to deliver immersive spatial audio with high\nperceptual realism over headphones. Loss functions play a central role in\noptimizing and evaluating algorithms that generate binaural signals. However,\ntraditional signal-related difference measures often fail to capture the\nperceptual properties that are essential to spatial audio quality. This review\npaper surveys recent loss functions that incorporate spatial perception cues\nrelevant to binaural reproduction. It focuses on losses applied to binaural\nsignals, which are often derived from microphone recordings or Ambisonics\nsignals, while excluding those based on room impulse responses. Guided by the\nSpatial Audio Quality Inventory (SAQI), the review emphasizes perceptual\ndimensions related to source localization and room response, while excluding\ngeneral spectral-temporal attributes. The literature survey reveals a strong\nfocus on localization cues, such as interaural time and level differences\n(ITDs, ILDs), while reverberation and other room acoustic attributes remain\nless explored in loss function design. Recent works that estimate room acoustic\nparameters and develop embeddings that capture room characteristics indicate\ntheir potential for future integration into neural network training. The paper\nconcludes by highlighting future research directions toward more perceptually\ngrounded loss functions that better capture the listener's spatial experience."}
{"id": "2506.19090", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.19090", "abs": "https://arxiv.org/abs/2506.19090", "authors": ["Eunhyuk Park", "Seok-Hwan Park", "Osvaldo Simeone", "Marco Di Renzo", "Shlomo Shamai"], "title": "SIM-Enabled Hybrid Digital-Wave Beamforming for Fronthaul-Constrained Cell-Free Massive MIMO Systems", "comment": "Submitted to an IEEE journal", "summary": "As the dense deployment of access points (APs) in cell-free massive\nmultiple-input multiple-output (CF-mMIMO) systems presents significant\nchallenges, per-AP coverage can be expanded using large-scale antenna arrays\n(LAAs). However, this approach incurs high implementation costs and substantial\nfronthaul demands due to the need for dedicated RF chains for all antennas. To\naddress these challenges, we propose a hybrid beamforming framework that\nintegrates wave-domain beamforming via stacked intelligent metasurfaces (SIM)\nwith conventional digital processing. By dynamically manipulating\nelectromagnetic waves, SIM-equipped APs enhance beamforming gains while\nsignificantly reducing RF chain requirements. We formulate a joint optimization\nproblem for digital and wave-domain beamforming along with fronthaul\ncompression to maximize the weighted sum-rate for both uplink and downlink\ntransmission under finite-capacity fronthaul constraints. Given the high\ndimensionality and non-convexity of the problem, we develop alternating\noptimization-based algorithms that iteratively optimize digital and wave-domain\nvariables. Numerical results demonstrate that the proposed hybrid schemes\noutperform conventional hybrid schemes, that rely on randomly set wave-domain\nbeamformers or restrict digital beamforming to simple power control. Moreover,\nthe proposed scheme employing sufficiently deep SIMs achieves near\nfully-digital performance with fewer RF chains in most simulated cases, except\nin the downlink at low signal-to-noise ratios."}
{"id": "2506.19335", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19335", "abs": "https://arxiv.org/abs/2506.19335", "authors": ["Yuto Kondo", "Hirokazu Kameoka", "Kou Tanaka", "Takuhiro Kaneko", "Noboru Harada"], "title": "Learning to assess subjective impressions from speech", "comment": "Accepted on EUSIPCO 2024", "summary": "We tackle a new task of training neural network models that can assess\nsubjective impressions conveyed through speech and assign scores accordingly,\ninspired by the work on automatic speech quality assessment (SQA). Speech\nimpressions are often described using phrases like `cute voice.' We define such\nphrases as subjective voice descriptors (SVDs). Focusing on the difference in\nusage scenarios between the proposed task and automatic SQA, we design a\nframework capable of accommodating SVDs personalized to each individual, such\nas `my favorite voice.' In this work, we compiled a dataset containing speech\nlabels derived from both abosolute category ratings (ACR) and comparison\ncategory ratings (CCR).\n  As an evaluation metric for assessment performance, we introduce ppref, the\naccuracy of the predicted score ordering of two samples on CCR test samples.\nAlongside the conventional model and learning methods based on ACR data, we\nalso investigated RankNet learning using CCR data. We experimentally find that\nthe ppref is moderate even with very limited training data. We also discover\nthe CCR training is superior to the ACR training. These results support the\nidea that assessment models based on personalized SVDs, which typically must be\ntrained on limited data, can be effectively learned from CCR data."}
{"id": "2506.19774", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19774", "abs": "https://arxiv.org/abs/2506.19774", "authors": ["Jun Wang", "Xijuan Zeng", "Chunyu Qiang", "Ruilong Chen", "Shiyao Wang", "Le Wang", "Wangjing Zhou", "Pengfei Cai", "Jiahui Zhao", "Nan Li", "Zihan Li", "Yuzhe Liang", "Xiaopeng Wang", "Haorui Zheng", "Ming Wen", "Kang Yin", "Yiran Wang", "Nan Li", "Feng Deng", "Liang Dong", "Chen Zhang", "Di Zhang", "Kun Gai"], "title": "Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation", "comment": null, "summary": "We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation\nmodel that synthesizes high-quality audio synchronized with video content. In\nKling-Foley, we introduce multimodal diffusion transformers to model the\ninteractions between video, audio, and text modalities, and combine it with a\nvisual semantic representation module and an audio-visual synchronization\nmodule to enhance alignment capabilities. Specifically, these modules align\nvideo conditions with latent audio elements at the frame level, thereby\nimproving semantic alignment and audio-visual synchronization. Together with\ntext conditions, this integrated approach enables precise generation of\nvideo-matching sound effects. In addition, we propose a universal latent audio\ncodec that can achieve high-quality modeling in various scenarios such as sound\neffects, speech, singing, and music. We employ a stereo rendering method that\nimbues synthesized audio with a spatial presence. At the same time, in order to\nmake up for the incomplete types and annotations of the open-source benchmark,\nwe also open-source an industrial-level benchmark Kling-Audio-Eval. Our\nexperiments show that Kling-Foley trained with the flow matching objective\nachieves new audio-visual SOTA performance among public models in terms of\ndistribution matching, semantic alignment, temporal alignment and audio\nquality."}
{"id": "2506.19141", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19141", "abs": "https://arxiv.org/abs/2506.19141", "authors": ["Bruno Aristimunha", "Dung Truong", "Pierre Guetschel", "Seyed Yahya Shirazi", "Isabelle Guyon", "Alexandre R. Franco", "Michael P. Milham", "Aviv Dotan", "Scott Makeig", "Alexandre Gramfort", "Jean-Remi King", "Marie-Constance Corsi", "Pedro A. Valdés-Sosa", "Amit Majumdar", "Alan Evans", "Terrence J Sejnowski", "Oren Shriki", "Sylvain Chevallier", "Arnaud Delorme"], "title": "EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding", "comment": "Approved at Neurips Competition track. webpage:\n  https://eeg2025.github.io/", "summary": "Current electroencephalogram (EEG) decoding models are typically trained on\nsmall numbers of subjects performing a single task. Here, we introduce a\nlarge-scale, code-submission-based competition comprising two challenges.\nFirst, the Transfer Challenge asks participants to build and test a model that\ncan zero-shot decode new tasks and new subjects from their EEG data. Second,\nthe Psychopathology factor prediction Challenge asks participants to infer\nsubject measures of mental health from EEG data. For this, we use an\nunprecedented, multi-terabyte dataset of high-density EEG signals (128\nchannels) recorded from over 3,000 child to young adult subjects engaged in\nmultiple active and passive tasks. We provide several tunable neural network\nbaselines for each of these two challenges, including a simple network and\ndemographic-based regression models. Developing models that generalise across\ntasks and individuals will pave the way for ML network architectures capable of\nadapting to EEG data collected from diverse tasks and individuals. Similarly,\npredicting mental health-relevant personality trait values from EEG might\nidentify objective biomarkers useful for clinical diagnosis and design of\npersonalised treatment for psychological conditions. Ultimately, the advances\nspurred by this challenge could contribute to the development of computational\npsychiatry and useful neurotechnology, and contribute to breakthroughs in both\nfundamental neuroscience and applied clinical research."}
{"id": "2506.19398", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19398", "abs": "https://arxiv.org/abs/2506.19398", "authors": ["Shengkui Zhao", "Zexu Pan", "Bin Ma"], "title": "ClearerVoice-Studio: Bridging Advanced Speech Processing Research and Practical Deployment", "comment": "accepted by Interspeech 2025, 5 pages, 5 tables", "summary": "This paper introduces ClearerVoice-Studio, an open-source, AI-powered speech\nprocessing toolkit designed to bridge cutting-edge research and practical\napplication. Unlike broad platforms like SpeechBrain and ESPnet,\nClearerVoice-Studio focuses on interconnected speech tasks of speech\nenhancement, separation, super-resolution, and multimodal target speaker\nextraction. A key advantage is its state-of-the-art pretrained models,\nincluding FRCRN with 3 million uses and MossFormer with 2.5 million uses,\noptimized for real-world scenarios. It also offers model optimization tools,\nmulti-format audio support, the SpeechScore evaluation toolkit, and\nuser-friendly interfaces, catering to researchers, developers, and end-users.\nIts rapid adoption attracting 3000 GitHub stars and 239 forks highlights its\nacademic and industrial impact. This paper details ClearerVoice-Studio's\ncapabilities, architectures, training strategies, benchmarks, community impact,\nand future plan. Source code is available at\nhttps://github.com/modelscope/ClearerVoice-Studio."}
{"id": "2506.18954", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.18954", "abs": "https://arxiv.org/abs/2506.18954", "authors": ["Diego Di Carlo", "Mathieu Fontaine", "Aditya Arie Nugraha", "Yoshiaki Bando", "Kazuyoshi Yoshii"], "title": "SHAMaNS: Sound Localization with Hybrid Alpha-Stable Spatial Measure and Neural Steerer", "comment": "European Signal Processing Conference (EUSIPCO), Sep 2025, Palermo,\n  Italy", "summary": "This paper describes a sound source localization (SSL) technique that\ncombines an $\\alpha$-stable model for the observed signal with a neural\nnetwork-based approach for modeling steering vectors. Specifically, a\nphysics-informed neural network, referred to as Neural Steerer, is used to\ninterpolate measured steering vectors (SVs) on a fixed microphone array. This\nallows for a more robust estimation of the so-called $\\alpha$-stable spatial\nmeasure, which represents the most plausible direction of arrival (DOA) of a\ntarget signal. As an $\\alpha$-stable model for the non-Gaussian case ($\\alpha$\n$\\in$ (0, 2)) theoretically defines a unique spatial measure, we choose to\nleverage it to account for residual reconstruction error of the Neural Steerer\nin the downstream tasks. The objective scores indicate that our proposed\ntechnique outperforms state-of-the-art methods in the case of multiple sound\nsources."}
{"id": "2506.19358", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19358", "abs": "https://arxiv.org/abs/2506.19358", "authors": ["Yuanyuan Zhang", "Haocheng Zhao", "Sijie Xiong", "Rui Yang", "Eng Gee Lim", "Yutao Yue"], "title": "From High-SNR Radar Signal to ECG: A Transfer Learning Model with Cardio-Focusing Algorithm for Scenarios with Limited Data", "comment": null, "summary": "Electrocardiogram (ECG), as a crucial find-grained cardiac feature, has been\nsuccessfully recovered from radar signals in the literature, but the\nperformance heavily relies on the high-quality radar signal and numerous\nradar-ECG pairs for training, restricting the applications in new scenarios due\nto data scarcity. Therefore, this work will focus on radar-based ECG recovery\nin new scenarios with limited data and propose a cardio-focusing and -tracking\n(CFT) algorithm to precisely track the cardiac location to ensure an efficient\nacquisition of high-quality radar signals. Furthermore, a transfer learning\nmodel (RFcardi) is proposed to extract cardio-related information from the\nradar signal without ECG ground truth based on the intrinsic sparsity of\ncardiac features, and only a few synchronous radar-ECG pairs are required to\nfine-tune the pre-trained model for the ECG recovery. The experimental results\nreveal that the proposed CFT can dynamically identify the cardiac location, and\nthe RFcardi model can effectively generate faithful ECG recoveries after using\na small number of radar-ECG pairs for training. The code and dataset are\navailable after the publication."}
{"id": "2506.19441", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19441", "abs": "https://arxiv.org/abs/2506.19441", "authors": ["Christoph Minixhofer", "Ondrej Klejch", "Peter Bell"], "title": "TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems", "comment": null, "summary": "Evaluation of Text to Speech (TTS) systems is challenging and\nresource-intensive. Subjective metrics such as Mean Opinion Score (MOS) are not\neasily comparable between works. Objective metrics are frequently used, but\nrarely validated against subjective ones. Both kinds of metrics are challenged\nby recent TTS systems capable of producing synthetic speech indistinguishable\nfrom real speech. In this work, we introduce Text to Speech Distribution Score\n2 (TTSDS2), a more robust and improved version of TTSDS. Across a range of\ndomains and languages, it is the only one out of 16 compared metrics to\ncorrelate with a Spearman correlation above 0.50 for every domain and\nsubjective score evaluated. We also release a range of resources for evaluating\nsynthetic speech close to real speech: A dataset with over 11,000 subjective\nopinion score ratings; a pipeline for continually recreating a multilingual\ntest dataset to avoid data leakage; and a continually updated benchmark for TTS\nin 14 languages."}
{"id": "2506.19014", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19014", "abs": "https://arxiv.org/abs/2506.19014", "authors": ["Abhay Kumar", "Kunal Verma", "Omkar More"], "title": "IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection", "comment": null, "summary": "Advancements in audio deepfake technology offers benefits like AI assistants,\nbetter accessibility for speech impairments, and enhanced entertainment.\nHowever, it also poses significant risks to security, privacy, and trust in\ndigital communications. Detecting and mitigating these threats requires\ncomprehensive datasets. Existing datasets lack diverse ethnic accents, making\nthem inadequate for many real-world scenarios. Consequently, models trained on\nthese datasets struggle to detect audio deepfakes in diverse linguistic and\ncultural contexts such as in South-Asian countries. Ironically, there is a\nstark lack of South-Asian speaker samples in the existing datasets despite\nconstituting a quarter of the worlds population. This work introduces the\nIndieFake Dataset (IFD), featuring 27.17 hours of bonafide and deepfake audio\nfrom 50 English speaking Indian speakers. IFD offers balanced data distribution\nand includes speaker-level characterization, absent in datasets like ASVspoof21\n(DF). We evaluated various baselines on IFD against existing ASVspoof21 (DF)\nand In-The-Wild (ITW) datasets. IFD outperforms ASVspoof21 (DF) and proves to\nbe more challenging compared to benchmark ITW dataset. The dataset will be\npublicly available upon acceptance."}
{"id": "2506.19376", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19376", "abs": "https://arxiv.org/abs/2506.19376", "authors": ["Jinzhe Wang", "Qinghua Guo", "Xiaojun Yuan"], "title": "Holographic Communication via Recordable and Reconfigurable Metasurface", "comment": null, "summary": "Holographic surface based communication technologies are anticipated to play\na significant role in the next generation of wireless networks. The existing\nreconfigurable holographic surface (RHS)-based scheme only utilizes the\nreconstruction process of the holographic principle for beamforming, where the\nchannel sate information (CSI) is needed. However, channel estimation for CSI\nacquirement is a challenging task in metasurface based communications. In this\nstudy, inspired by both the recording and reconstruction processes of\nholography, we develop a novel holographic communication scheme by introducing\nrecordable and reconfigurable metasurfaces (RRMs), where channel estimation is\nnot needed thanks to the recording process. Then we analyze the input-output\nmutual information of the RRM-based communication system and compare it with\nthe existing RHS based system. Our results show that, without channel\nestimation, the proposed scheme achieves performance comparable to that of the\nRHS scheme with perfect CSI, suggesting a promising alternative for future\nwireless communication networks."}
{"id": "2506.19446", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19446", "abs": "https://arxiv.org/abs/2506.19446", "authors": ["Jaejun Lee", "Kyogu Lee"], "title": "Vo-Ve: An Explainable Voice-Vector for Speaker Identity Evaluation", "comment": "Interspeech 2025", "summary": "In this paper, we propose Vo-Ve, a novel voice-vector embedding that captures\nspeaker identity. Unlike conventional speaker embeddings, Vo-Ve is explainable,\nas it contains the probabilities of explicit voice attribute classes. Through\nextensive analysis, we demonstrate that Vo-Ve not only evaluates speaker\nsimilarity competitively with conventional techniques but also provides an\ninterpretable explanation in terms of voice attributes. We strongly believe\nthat Vo-Ve can enhance evaluation schemes across various speech tasks due to\nits high-level explainability."}
{"id": "2506.19253", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19253", "abs": "https://arxiv.org/abs/2506.19253", "authors": ["Sajad Sadeghkhani", "Maryam Karimi Boroujeni", "Hilmi R. Dajani", "Saeid R. Seydnejad", "Christian Giguère"], "title": "A Robust Method for Pitch Tracking in the Frequency Following Response using Harmonic Amplitude Summation Filterbank", "comment": null, "summary": "The Frequency Following Response (FFR) reflects the brain's neural encoding\nof auditory stimuli including speech. Because the fundamental frequency (F0), a\nphysical correlate of pitch, is one of the essential features of speech, there\nhas been particular interest in characterizing the FFR at F0, especially when\nF0 varies over time. The standard method for extracting F0 in FFRs has been the\nAutocorrelation Function (ACF). This paper investigates\nharmonic-structure-based F0 estimation algorithms, originally developed for\nspeech and music, and resolves their poor performance when applied to FFRs in\ntwo steps. Firstly, given that unlike in speech or music, stimulus F0 of FFRs\nis already known, we introduce a stimulus-aware filterbank that selectively\naggregates amplitudes at F0 and its harmonics while suppressing noise at\nnon-harmonic frequencies. This method, called Harmonic Amplitude Summation\n(HAS), evaluates F0 candidates only within a range centered around the stimulus\nF0. Secondly, unlike other pitch tracking methods that select the highest peak,\nour method chooses the most prominent one, as it better reflects the underlying\nperiodicity of FFRs. To the best of our knowledge, this is the first study to\npropose an F0 estimation algorithm for FFRs that relies on harmonic structure.\nAnalyzing recorded FFRs from 16 normal hearing subjects to 4 natural speech\nstimuli with a wide F0 variation from 89 Hz to 452 Hz showed that this method\noutperformed ACF by reducing the average Root-Mean-Square-Error (RMSE) within\neach response and stimulus F0 contour pair by 8.8% to 47.4%, depending on the\nstimulus."}
{"id": "2506.19451", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19451", "abs": "https://arxiv.org/abs/2506.19451", "authors": ["Seunghun Lee", "Jihong Park", "Jinho Choi", "Hyuncheol Park"], "title": "Low-Complexity Semantic Packet Aggregation for Token Communication via Lookahead Search", "comment": null, "summary": "Tokens are fundamental processing units of generative AI (GenAI) and large\nlanguage models (LLMs), and token communication (TC) is essential for enabling\nremote AI-generate content (AIGC) and wireless LLM applications. Unlike\ntraditional bits, each of which is independently treated, the semantics of each\ntoken depends on its surrounding context tokens. This inter-token dependency\nmakes TC vulnerable to outage channels, where the loss of a single token can\nsignificantly distort the original message semantics. Motivated by this, this\npaper focuses on optimizing token packetization to maximize the average token\nsimilarity (ATS) between the original and received token messages under outage\nchannels. Due to inter-token dependency, this token grouping problem is\ncombinatorial, with complexity growing exponentially with message length. To\naddress this, we propose a novel framework of semantic packet aggregation with\nlookahead search (SemPA-Look), built on two core ideas. First, it introduces\nthe residual semantic score (RSS) as a token-level surrogate for the\nmessage-level ATS, allowing robust semantic preservation even when a certain\ntoken packet is lost. Second, instead of full search, SemPA-Look applies a\nlookahead search-inspired algorithm that samples intra-packet token candidates\nwithout replacement (fixed depth), conditioned on inter-packet token candidates\nsampled with replacement (fixed width), thereby achieving linear complexity.\nExperiments on a remote AIGC task with the MS-COCO dataset (text captioned\nimages) demonstrate that SemPA-Look achieves high ATS and LPIPS scores\ncomparable to exhaustive search, while reducing computational complexity by up\nto 40$\\times$. Compared to other linear-complexity algorithms such as the\ngenetic algorithm (GA), SemPA-Look achieves 10$\\times$ lower complexity,\ndemonstrating its practicality for remote AIGC and other TC applications."}
{"id": "2506.19404", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19404", "abs": "https://arxiv.org/abs/2506.19404", "authors": ["Boaz Rafaely", "Stefan Weinzierl", "Or Berebi", "Fabian Brinkmann"], "title": "Loss functions incorporating auditory spatial perception in deep learning -- a review", "comment": "Submitted to I3DA 2025", "summary": "Binaural reproduction aims to deliver immersive spatial audio with high\nperceptual realism over headphones. Loss functions play a central role in\noptimizing and evaluating algorithms that generate binaural signals. However,\ntraditional signal-related difference measures often fail to capture the\nperceptual properties that are essential to spatial audio quality. This review\npaper surveys recent loss functions that incorporate spatial perception cues\nrelevant to binaural reproduction. It focuses on losses applied to binaural\nsignals, which are often derived from microphone recordings or Ambisonics\nsignals, while excluding those based on room impulse responses. Guided by the\nSpatial Audio Quality Inventory (SAQI), the review emphasizes perceptual\ndimensions related to source localization and room response, while excluding\ngeneral spectral-temporal attributes. The literature survey reveals a strong\nfocus on localization cues, such as interaural time and level differences\n(ITDs, ILDs), while reverberation and other room acoustic attributes remain\nless explored in loss function design. Recent works that estimate room acoustic\nparameters and develop embeddings that capture room characteristics indicate\ntheir potential for future integration into neural network training. The paper\nconcludes by highlighting future research directions toward more perceptually\ngrounded loss functions that better capture the listener's spatial experience."}
{"id": "2506.19398", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19398", "abs": "https://arxiv.org/abs/2506.19398", "authors": ["Shengkui Zhao", "Zexu Pan", "Bin Ma"], "title": "ClearerVoice-Studio: Bridging Advanced Speech Processing Research and Practical Deployment", "comment": "accepted by Interspeech 2025, 5 pages, 5 tables", "summary": "This paper introduces ClearerVoice-Studio, an open-source, AI-powered speech\nprocessing toolkit designed to bridge cutting-edge research and practical\napplication. Unlike broad platforms like SpeechBrain and ESPnet,\nClearerVoice-Studio focuses on interconnected speech tasks of speech\nenhancement, separation, super-resolution, and multimodal target speaker\nextraction. A key advantage is its state-of-the-art pretrained models,\nincluding FRCRN with 3 million uses and MossFormer with 2.5 million uses,\noptimized for real-world scenarios. It also offers model optimization tools,\nmulti-format audio support, the SpeechScore evaluation toolkit, and\nuser-friendly interfaces, catering to researchers, developers, and end-users.\nIts rapid adoption attracting 3000 GitHub stars and 239 forks highlights its\nacademic and industrial impact. This paper details ClearerVoice-Studio's\ncapabilities, architectures, training strategies, benchmarks, community impact,\nand future plan. Source code is available at\nhttps://github.com/modelscope/ClearerVoice-Studio."}
{"id": "2506.19470", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.19470", "abs": "https://arxiv.org/abs/2506.19470", "authors": ["Aniol Martí", "Luca Sanguinetti", "Jaume Riba", "Meritxell Lamarca"], "title": "Coherent and Noncoherent Detection in Dense Arrays: Can We Ignore Mutual Coupling?", "comment": "Accepted version of the article submitted to EUSIPCO 2025", "summary": "This paper investigates the impact of mutual coupling on MIMO systems with\ndensely deployed antennas. Leveraging multiport communication theory, we\nanalyze both coherent and noncoherent detection approaches in a single-user\nuplink scenario where the receiver ignores mutual coupling effects. Simulation\nresults indicate that while coherent detection is generally more accurate, it\nis highly sensitive to mismatches in the coupling model, leading to severe\nperformance degradation when antennas are closely spaced, to the point of\nbecoming unusable. Noncoherent detection, on the other hand, exhibits a higher\nerror probability but is more robust to coupling model mismatches."}
{"id": "2506.19774", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19774", "abs": "https://arxiv.org/abs/2506.19774", "authors": ["Jun Wang", "Xijuan Zeng", "Chunyu Qiang", "Ruilong Chen", "Shiyao Wang", "Le Wang", "Wangjing Zhou", "Pengfei Cai", "Jiahui Zhao", "Nan Li", "Zihan Li", "Yuzhe Liang", "Xiaopeng Wang", "Haorui Zheng", "Ming Wen", "Kang Yin", "Yiran Wang", "Nan Li", "Feng Deng", "Liang Dong", "Chen Zhang", "Di Zhang", "Kun Gai"], "title": "Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation", "comment": null, "summary": "We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation\nmodel that synthesizes high-quality audio synchronized with video content. In\nKling-Foley, we introduce multimodal diffusion transformers to model the\ninteractions between video, audio, and text modalities, and combine it with a\nvisual semantic representation module and an audio-visual synchronization\nmodule to enhance alignment capabilities. Specifically, these modules align\nvideo conditions with latent audio elements at the frame level, thereby\nimproving semantic alignment and audio-visual synchronization. Together with\ntext conditions, this integrated approach enables precise generation of\nvideo-matching sound effects. In addition, we propose a universal latent audio\ncodec that can achieve high-quality modeling in various scenarios such as sound\neffects, speech, singing, and music. We employ a stereo rendering method that\nimbues synthesized audio with a spatial presence. At the same time, in order to\nmake up for the incomplete types and annotations of the open-source benchmark,\nwe also open-source an industrial-level benchmark Kling-Audio-Eval. Our\nexperiments show that Kling-Foley trained with the flow matching objective\nachieves new audio-visual SOTA performance among public models in terms of\ndistribution matching, semantic alignment, temporal alignment and audio\nquality."}
{"id": "2506.19441", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19441", "abs": "https://arxiv.org/abs/2506.19441", "authors": ["Christoph Minixhofer", "Ondrej Klejch", "Peter Bell"], "title": "TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems", "comment": null, "summary": "Evaluation of Text to Speech (TTS) systems is challenging and\nresource-intensive. Subjective metrics such as Mean Opinion Score (MOS) are not\neasily comparable between works. Objective metrics are frequently used, but\nrarely validated against subjective ones. Both kinds of metrics are challenged\nby recent TTS systems capable of producing synthetic speech indistinguishable\nfrom real speech. In this work, we introduce Text to Speech Distribution Score\n2 (TTSDS2), a more robust and improved version of TTSDS. Across a range of\ndomains and languages, it is the only one out of 16 compared metrics to\ncorrelate with a Spearman correlation above 0.50 for every domain and\nsubjective score evaluated. We also release a range of resources for evaluating\nsynthetic speech close to real speech: A dataset with over 11,000 subjective\nopinion score ratings; a pipeline for continually recreating a multilingual\ntest dataset to avoid data leakage; and a continually updated benchmark for TTS\nin 14 languages."}
{"id": "2506.19476", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19476", "abs": "https://arxiv.org/abs/2506.19476", "authors": ["Kaidi Xu", "Shenglong Zhou", "Geoffrey Ye Li"], "title": "Neural Collapse based Deep Supervised Federated Learning for Signal Detection in OFDM Systems", "comment": null, "summary": "Future wireless networks are expected to be AI-empowered, making their\nperformance highly dependent on the quality of training datasets. However,\nphysical-layer entities often observe only partial wireless environments\ncharacterized by different power delay profiles. Federated learning is capable\nof addressing this limited observability, but often struggles with data\nheterogeneity. To tackle this challenge, we propose a neural collapse (NC)\ninspired deep supervised federated learning (NCDSFL) algorithm."}
{"id": "2506.19499", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.19499", "abs": "https://arxiv.org/abs/2506.19499", "authors": ["Noa Jie Vives Zaguirre", "Oscar Lasierra", "Filip Lemic", "Gerard Calvo Bartra", "Pablo José Galván Calderón", "Gines Garcia-Aviles", "Sergi Abadal", "Xavier Costa-Pérez"], "title": "Experimental Assessment of A Framework for In-body RF-backscattering Localization", "comment": "7 pages, 7 figures, 2 tables, accepted at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications 2025", "summary": "Localization of in-body devices is beneficial for Gastrointestinal (GI)\ndiagnosis and targeted treatment. Traditional methods such as imaging and\nendoscopy are invasive and limited in resolution, highlighting the need for\ninnovative alternatives. This study presents an experimental framework for\nRadio Frequency (RF)-backscatter-based in-body localization, inspired by the\nReMix approach, and evaluates its performance in real-world conditions. The\nexperimental setup includes an in-body backscatter device and various off-body\nantenna configurations to investigate harmonic generation and reception in air,\nchicken and pork tissues. The results indicate that optimal backscatter device\npositioning, antenna selection, and gain settings significantly impact\nperformance, with denser biological tissues leading to greater attenuation. The\nstudy also highlights challenges such as external interference and plastic\nenclosures affecting propagation. The findings emphasize the importance of\ninterference mitigation and refined propagation models to enhance performance."}
{"id": "2506.19526", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19526", "abs": "https://arxiv.org/abs/2506.19526", "authors": ["Prasetyo Putranto", "Anis Amazigh Hamza", "Sameh Mabrouki", "Nasrullah Armi", "Iyad Dayoub"], "title": "Reconfigurable Intelligent Surfaces for 6G and Beyond: A Comprehensive Survey from Theory to Deployment", "comment": "39 page, 21 figures, submitted to IEEE Communications Surveys &\n  Tutorials", "summary": "As the wireless research community moves toward shaping the vision of\nsixth-generation (6G) networks, reconfigurable intelligent surfaces (RIS) have\nemerged as a promising technology for controlling the propagation environment.\nAlthough RIS has not yet been standardized, its versatile applications and\nenabling capabilities have attracted growing attention in both academia and\nindustry. This survey presents a comprehensive review of RIS technology\nspanning theoretical foundations, design aspects, and practical deployment\nconsiderations. In contrast to existing surveys that focus on isolated aspects,\nthis work offers an integrated view covering use cases, control mechanisms,\nchannel sounding methodologies, and channel estimation strategies. Each of\nthese topics is reviewed through the lens of recent literature, synthesizing\nthe latest advancements to provide updated insights for both academic\nresearchers and industry practitioners. It further addresses emerging topics\nsuch as standardization activities and industrial perspectives, which are often\noverlooked in prior literature. By bridging theoretical insights with practical\nchallenges, this survey aims to provide a holistic understanding of RIS and\nsupport its evolution from a research concept toward real-world implementation."}
{"id": "2506.19612", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19612", "abs": "https://arxiv.org/abs/2506.19612", "authors": ["Dennis Laurijssen", "Rens Baeyens", "Walter Daems", "Jan Steckel"], "title": "A Wireless Self-Calibrating Ultrasound Microphone Array with Sub-Microsecond Synchronization", "comment": null, "summary": "We present a novel system architecture for a distributed wireless,\nself-calibrating ultrasound microphone network for synchronized in-air acoustic\nsensing. Once deployed the embedded nodes determine their position in the\nenvironment using the infrared optical tracking system found in the HTC Vive\nLighthouses. After self-calibration, the nodes start sampling the ultrasound\nmicrophone while embedding a synchronization signal in the data which is\nestablished using a wireless Sub-1GHz RF link. Data transmission is handled via\nthe Wi-Fi 6 radio that is embedded in the nodes' SoC, decoupling\nsynchronization from payload transport. A prototype system with a limited\namount of network nodes was used to verify the proposed distributed microphone\narray's wireless data acquisition and synchronization capabilities. This\narchitecture lays the groundwork for scalable, deployable ultrasound arrays for\nsound source localization applications in bio-acoustic research and industrial\nacoustic monitoring."}
{"id": "2506.19627", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19627", "abs": "https://arxiv.org/abs/2506.19627", "authors": ["Carmen Álvarez Roa", "Yunus Can Gültekin", "Kaiquan Wu", "Cornelis Willem Korevaar", "Alex Alvarado"], "title": "On Error Rate Approximations for FSO Systems with Weak Turbulence and Pointing Errors", "comment": null, "summary": "Atmospheric attenuation, atmospheric turbulence, geometric spread, and\npointing errors, degrade the performance of free-space optical transmission. In\nthe weak turbulence regime, the probability density function describing the\ndistribution of the channel fading coefficient that models these four effects\nis known in the literature. This function is an integral equation, which makes\nit difficult to find simple analytical expressions of important performance\nmetrics such as the bit error rate (BER) and symbol error rate (SER). In this\npaper, we present simple and accurate approximations of the average BER and SER\nfor pulse-amplitude modulation (PAM) in the weak turbulence regime for an\nintensity modulation and direct detection system. Our numerical results show\nthat the proposed expressions exhibit excellent accuracy when compared against\nMonte Carlo simulations. To demonstrate the usefulness of the developed\napproximations, we perform two asymptotic analyses. First, we investigate the\nadditional transmit power required to maintain the same SER when the spectral\nefficiency increases by 1 bit/symbol. Second, we study the asymptotic behavior\nof our SER approximation for dense PAM constellations and high transmit power."}
{"id": "2506.19684", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19684", "abs": "https://arxiv.org/abs/2506.19684", "authors": ["Felipe Villenas", "Kaiquan Wu", "Yunus Can Gültekin", "Jamal Riani", "Alex Alvarado"], "title": "Beyond 200 Gb/s/lane: An Analytical Approach to Optimal Detection in Shaped IM-DD Optical Links with Relative Intensity Noise", "comment": "preprint", "summary": "Next-generation intensity-modulation (IM) and direct-detection (DD) systems\nused in data centers are expected to operate at 400 Gb/s/lane and beyond. Such\nrates can be achieved by increasing the system bandwidth or the modulation\nformat, which in turn requires maintaining or increasing the signal-to-noise\nratio (SNR). Such SNR requirements can be achieved by increasing the\ntransmitted optical power. This increase in optical power causes the emergence\nof relative intensity noise (RIN), a signal-dependent impairment inherent to\nthe transmitter laser, which ultimately limits the performance of the system.\nIn this paper, we develop an analytical symbol error rate (SER) expression for\nthe optimal detector for the IM-DD optical link under study. The developed\nexpression takes into account the signal-dependent nature of RIN and does not\nmake any assumptions on the geometry or probability distribution of the\nconstellation. Our expression is therefore applicable to general\nprobabilistically and/or geometrically shaped systems. Unlike results available\nin the literature, our proposed expression provides a perfect match to\nnumerical simulations of probabilistic and geometrically shaped systems."}
{"id": "2506.19253", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19253", "abs": "https://arxiv.org/abs/2506.19253", "authors": ["Sajad Sadeghkhani", "Maryam Karimi Boroujeni", "Hilmi R. Dajani", "Saeid R. Seydnejad", "Christian Giguère"], "title": "A Robust Method for Pitch Tracking in the Frequency Following Response using Harmonic Amplitude Summation Filterbank", "comment": null, "summary": "The Frequency Following Response (FFR) reflects the brain's neural encoding\nof auditory stimuli including speech. Because the fundamental frequency (F0), a\nphysical correlate of pitch, is one of the essential features of speech, there\nhas been particular interest in characterizing the FFR at F0, especially when\nF0 varies over time. The standard method for extracting F0 in FFRs has been the\nAutocorrelation Function (ACF). This paper investigates\nharmonic-structure-based F0 estimation algorithms, originally developed for\nspeech and music, and resolves their poor performance when applied to FFRs in\ntwo steps. Firstly, given that unlike in speech or music, stimulus F0 of FFRs\nis already known, we introduce a stimulus-aware filterbank that selectively\naggregates amplitudes at F0 and its harmonics while suppressing noise at\nnon-harmonic frequencies. This method, called Harmonic Amplitude Summation\n(HAS), evaluates F0 candidates only within a range centered around the stimulus\nF0. Secondly, unlike other pitch tracking methods that select the highest peak,\nour method chooses the most prominent one, as it better reflects the underlying\nperiodicity of FFRs. To the best of our knowledge, this is the first study to\npropose an F0 estimation algorithm for FFRs that relies on harmonic structure.\nAnalyzing recorded FFRs from 16 normal hearing subjects to 4 natural speech\nstimuli with a wide F0 variation from 89 Hz to 452 Hz showed that this method\noutperformed ACF by reducing the average Root-Mean-Square-Error (RMSE) within\neach response and stimulus F0 contour pair by 8.8% to 47.4%, depending on the\nstimulus."}

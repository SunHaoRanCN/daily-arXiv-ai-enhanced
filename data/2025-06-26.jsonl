{"id": "2506.19875", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19875", "abs": "https://arxiv.org/abs/2506.19875", "authors": ["Taous Iatariene", "Can Cui", "Alexandre Guérin", "Romain Serizel"], "title": "Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers", "comment": "33rd European Signal Processing Conference (EUSIPCO 2025), Sep 2025,\n  Palerme (Italie), Italy", "summary": "Speaker tracking methods often rely on spatial observations to assign\ncoherent track identities over time. This raises limits in scenarios with\nintermittent and moving speakers, i.e., speakers that may change position when\nthey are inactive, thus leading to discontinuous spatial trajectories. This\npaper proposes to investigate the use of speaker embeddings, in a simple\nsolution to this issue. We propose to perform identity reassignment\npost-tracking, using speaker embeddings. We leverage trajectory-related\ninformation provided by an initial tracking step and multichannel audio signal.\nBeamforming is used to enhance the signal towards the speakers' positions in\norder to compute speaker embeddings. These are then used to assign new track\nidentities based on an enrollment pool. We evaluate the performance of the\nproposed speaker embedding-based identity reassignment method on a dataset\nwhere speakers change position during inactivity periods. Results show that it\nconsistently improves the identity assignment performance of neural and\nstandard tracking systems. In particular, we study the impact of beamforming\nand input duration for embedding extraction."}
{"id": "2506.19887", "categories": ["eess.AS", "cs.AI", "cs.SD", "68T10"], "pdf": "https://arxiv.org/pdf/2506.19887", "abs": "https://arxiv.org/abs/2506.19887", "authors": ["Hyo Jin Jon", "Longbin Jin", "Hyuntaek Jung", "Hyunseo Kim", "Donghun Min", "Eun Yi Kim"], "title": "MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition", "comment": "5 pages, 4 figures, 2 tables, 1 algorithm, Accepted to INTERSPEECH\n  2025", "summary": "This paper presents our contributions to the Speech Emotion Recognition in\nNaturalistic Conditions (SERNC) Challenge, where we address categorical emotion\nrecognition and emotional attribute prediction. To handle the complexities of\nnatural speech, including intra- and inter-subject variability, we propose\nMulti-level Acoustic-Textual Emotion Representation (MATER), a novel\nhierarchical framework that integrates acoustic and textual features at the\nword, utterance, and embedding levels. By fusing low-level lexical and acoustic\ncues with high-level contextualized representations, MATER effectively captures\nboth fine-grained prosodic variations and semantic nuances. Additionally, we\nintroduce an uncertainty-aware ensemble strategy to mitigate annotator\ninconsistencies, improving robustness in ambiguous emotional expressions. MATER\nranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of\n0.5928, securing second place in valence prediction with an impressive CCC of\n0.6941."}
{"id": "2506.20001", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20001", "abs": "https://arxiv.org/abs/2506.20001", "authors": ["Paul Didier", "Toon van Waterschoot", "Simon Doclo", "Jörg Bitzer", "Marc Moonen"], "title": "Improved Topology-Independent Distributed Adaptive Node-Specific Signal Estimation for Wireless Acoustic Sensor Networks", "comment": null, "summary": "This paper addresses the challenge of topology-independent (TI) distributed\nadaptive node-specific signal estimation (DANSE) in wireless acoustic sensor\nnetworks (WASNs) where sensor nodes exchange only fused versions of their local\nsignals. An algorithm named TI-DANSE has previously been presented to handle\nnon-fully connected WASNs. However, its slow iterative convergence towards the\noptimal solution limits its applicability. To address this, we propose in this\npaper the TI-DANSE+ algorithm. At each iteration in TI-DANSE+, the node set to\nupdate its local parameters is allowed to exploit each individual partial\nin-network sums transmitted by its neighbors in its local estimation problem,\nincreasing the available degrees of freedom and accelerating convergence with\nrespect to TI-DANSE. Additionally, a tree-pruning strategy is proposed to\nfurther increase convergence speed. TI-DANSE+ converges as fast as the DANSE\nalgorithm in fully connected WASNs while reducing transmit power usage. The\nconvergence properties of TI-DANSE+ are demonstrated in numerical simulations."}
{"id": "2506.20190", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.20190", "abs": "https://arxiv.org/abs/2506.20190", "authors": ["Marie Kunešová", "Zdeněk Hanzlíček", "Jindřich Matoušek"], "title": "An Exploration of ECAPA-TDNN and x-vector Speaker Representations in Zero-shot Multi-speaker TTS", "comment": "Accepted to TSD 2025", "summary": "Zero-shot multi-speaker text-to-speech (TTS) systems rely on speaker\nembeddings to synthesize speech in the voice of an unseen speaker, using only a\nshort reference utterance. While many speaker embeddings have been developed\nfor speaker recognition, their relative effectiveness in zero-shot TTS remains\nunderexplored. In this work, we employ a YourTTS-based TTS system to compare\nthree different speaker encoders - YourTTS's original H/ASP encoder, x-vector\nembeddings, and ECAPA-TDNN embeddings - within an otherwise fixed zero-shot TTS\nframework. All models were trained on the same dataset of Czech read speech and\nevaluated on 24 out-of-domain target speakers using both subjective and\nobjective methods. The subjective evaluation was conducted via a listening test\nfocused on speaker similarity, while the objective evaluation measured cosine\ndistances between speaker embeddings extracted from synthesized and real\nutterances. Across both evaluations, the original H/ASP encoder consistently\noutperformed the alternatives, with ECAPA-TDNN showing better results than\nx-vectors. These findings suggest that, despite the popularity of ECAPA-TDNN in\nspeaker recognition, it does not necessarily offer improvements for speaker\nsimilarity in zero-shot TTS in this configuration. Our study highlights the\nimportance of empirical evaluation when reusing speaker recognition embeddings\nin TTS and provides a framework for additional future comparisons."}
{"id": "2506.19860", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19860", "abs": "https://arxiv.org/abs/2506.19860", "authors": ["Oktay Karakuş", "Padraig Corcoran"], "title": "A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing", "comment": "11 pages, 4 figures, 2 tables", "summary": "Electric vehicle (EV) charging infrastructure is increasingly critical to\nsustainable transport systems, yet its resilience under environmental and\ninfrastructural stress remains underexplored. In this paper, we introduce\nRSERI-EV, a spatially explicit and multi-modal risk assessment framework that\ncombines remote sensing data, open infrastructure datasets, and spatial graph\nanalytics to evaluate the vulnerability of EV charging stations. RSERI-EV\nintegrates diverse data layers, including flood risk maps, land surface\ntemperature (LST) extremes, vegetation indices (NDVI), land use/land cover\n(LULC), proximity to electrical substations, and road accessibility to generate\na composite Resilience Score. We apply this framework to the country of Wales\nEV charger dataset to demonstrate its feasibility. A spatial $k$-nearest\nneighbours ($k$NN) graph is constructed over the charging network to enable\nneighbourhood-based comparisons and graph-aware diagnostics. Our prototype\nhighlights the value of multi-source data fusion and interpretable spatial\nreasoning in supporting climate-resilient, infrastructure-aware EV deployment."}
{"id": "2506.20609", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20609", "abs": "https://arxiv.org/abs/2506.20609", "authors": ["Ankit Shah", "Rita Singh", "Bhiksha Raj", "Alexander Hauptmann"], "title": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings", "comment": "4 pages + 1 References", "summary": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders."}
{"id": "2506.20288", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.20288", "abs": "https://arxiv.org/abs/2506.20288", "authors": ["Aleš Pražák", "Marie Kunešová", "Josef Psutka"], "title": "Lightweight Target-Speaker-Based Overlap Transcription for Practical Streaming ASR", "comment": null, "summary": "Overlapping speech remains a major challenge for automatic speech recognition\n(ASR) in real-world applications, particularly in broadcast media with dynamic,\nmulti-speaker interactions. We propose a light-weight, target-speaker-based\nextension to an existing streaming ASR system to enable practical transcription\nof overlapping speech with minimal computational overhead. Our approach\ncombines a speaker-independent (SI) model for standard operation with a\nspeaker-conditioned (SC) model selectively applied in overlapping scenarios.\nOverlap detection is achieved using a compact binary classifier trained on\nfrozen SI model output, offering accurate segmentation at negligible cost. The\nSC model employs Feature-wise Linear Modulation (FiLM) to incorporate speaker\nembeddings and is trained on synthetically mixed data to transcribe only the\ntarget speaker. Our method supports dynamic speaker tracking and reuses\nexisting modules with minimal modifications. Evaluated on a challenging set of\nCzech television debates with 16% overlap, the system reduced WER on\noverlapping segments from 68.0% (baseline) to 35.78% while increasing total\ncomputational load by only 44%. The proposed system offers an effective and\nscalable solution for overlap transcription in continuous ASR services."}
{"id": "2506.19956", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19956", "abs": "https://arxiv.org/abs/2506.19956", "authors": ["Srinivas Rahul Sapireddy", "Mostafizur Rahman"], "title": "Revisiting R: Statistical Envelope Analysis for Lightweight RF Modulation Classification", "comment": "Accepted at RFCoN 2025: First International Conference on RF\n  Communication and Networks", "summary": "Modulation classification plays a crucial role in wireless communication\nsystems, enabling applications such as cognitive radio, spectrum monitoring,\nand electronic warfare. Conventional techniques often involve deep learning or\ncomplex feature extraction, which, while effective, require substantial\ncomputational resources and memory. An early approach by Chan and Gadbois in\n1985 introduced a theoretical method for modulation classification using a\nmathematically derived parameter called R. The authors proved that the R value\n- the ratio of the variance to the square of the mean of the signal envelope -\ncan be a distinguishing feature for classification. In this work, we revisit\nthe R value and show that classification accuracy can be improved further\nthrough statistical methods. We extend R-value analysis to demonstrate its\neffectiveness even after signals are transformed using the Hilbert transform\nfollowed by the Short-Time Fourier Transform (STFT). Our analysis includes\ntesting on 300000 signals across AM, DSB, and SSB classes, with each class\nhaving 100000 random variations. On average, we achieve 98.60, 97.30, and 97.90\npercent classification accuracy for AM, DSB, and SSB signals after applying the\nHilbert transform. Similar or better accuracies are observed after applying the\nSTFT, reaching 98.80, 99.10, and 99.00 percent, respectively, for AM, DSB, and\nSSB types."}
{"id": "2506.19875", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.19875", "abs": "https://arxiv.org/abs/2506.19875", "authors": ["Taous Iatariene", "Can Cui", "Alexandre Guérin", "Romain Serizel"], "title": "Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers", "comment": "33rd European Signal Processing Conference (EUSIPCO 2025), Sep 2025,\n  Palerme (Italie), Italy", "summary": "Speaker tracking methods often rely on spatial observations to assign\ncoherent track identities over time. This raises limits in scenarios with\nintermittent and moving speakers, i.e., speakers that may change position when\nthey are inactive, thus leading to discontinuous spatial trajectories. This\npaper proposes to investigate the use of speaker embeddings, in a simple\nsolution to this issue. We propose to perform identity reassignment\npost-tracking, using speaker embeddings. We leverage trajectory-related\ninformation provided by an initial tracking step and multichannel audio signal.\nBeamforming is used to enhance the signal towards the speakers' positions in\norder to compute speaker embeddings. These are then used to assign new track\nidentities based on an enrollment pool. We evaluate the performance of the\nproposed speaker embedding-based identity reassignment method on a dataset\nwhere speakers change position during inactivity periods. Results show that it\nconsistently improves the identity assignment performance of neural and\nstandard tracking systems. In particular, we study the impact of beamforming\nand input duration for embedding extraction."}
{"id": "2506.20361", "categories": ["eess.AS", "cs.SD", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20361", "abs": "https://arxiv.org/abs/2506.20361", "authors": ["Yi Wang", "Oli Danyi Liu", "Peter Bell"], "title": "The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models", "comment": "Accepted by Interspeech 2025", "summary": "Human speech perception is multimodal. In natural speech, lip movements can\nprecede corresponding voicing by a non-negligible gap of 100-300 ms, especially\nfor specific consonants, affecting the time course of neural phonetic encoding\nin human listeners. However, it remains unexplored whether self-supervised\nlearning models, which have been used to simulate audio-visual integration in\nhumans, can capture this asynchronicity between audio and visual cues. We\ncompared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using\nlinear classifiers to track their phonetic decodability over time. We found\nthat phoneme information becomes available in AV-HuBERT embeddings only about\n20 ms before HuBERT, likely due to AV-HuBERT's lower temporal resolution and\nfeature concatenation process. It suggests AV-HuBERT does not adequately\ncapture the temporal dynamics of multimodal speech perception, limiting its\nsuitability for modeling the multimodal speech perception process."}
{"id": "2506.19957", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.19957", "abs": "https://arxiv.org/abs/2506.19957", "authors": ["Benjamin J. B. Deutschmann", "Xuhong Li", "Florian Meyer", "Erik Leitinger"], "title": "Posterior Cramér-Rao Bounds on Localization and Mapping Errors in Distributed MIMO SLAM", "comment": null, "summary": "Radio-frequency simultaneous localization and mapping (RF-SLAM) methods\njointly infer the position of mobile transmitters and receivers in wireless\nnetworks, together with a geometric map of the propagation environment. An\ninferred map of specular surfaces can be used to exploit non-line-of-sight\ncomponents of the multipath channel to increase robustness, bypass\nobstructions, and improve overall communication and positioning performance.\nWhile performance bounds for user location are well established, the literature\nlacks performance bounds for map information. This paper derives the mapping\nerror bound (MEB), i.e., the posterior Cram\\'er-Rao lower bound on the position\nand orientation of specular surfaces, for RF-SLAM. In particular, we consider a\nvery general scenario with single- and double-bounce reflections, as well as\ndistributed anchors. We demonstrate numerically that a state-of-the-art RF-SLAM\nalgorithm asymptotically converges to this MEB. The bounds assess not only the\nlocalization (position and orientation) but also the mapping performance of\nRF-SLAM algorithms in terms of global features."}
{"id": "2506.19887", "categories": ["eess.AS", "cs.AI", "cs.SD", "68T10"], "pdf": "https://arxiv.org/pdf/2506.19887", "abs": "https://arxiv.org/abs/2506.19887", "authors": ["Hyo Jin Jon", "Longbin Jin", "Hyuntaek Jung", "Hyunseo Kim", "Donghun Min", "Eun Yi Kim"], "title": "MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition", "comment": "5 pages, 4 figures, 2 tables, 1 algorithm, Accepted to INTERSPEECH\n  2025", "summary": "This paper presents our contributions to the Speech Emotion Recognition in\nNaturalistic Conditions (SERNC) Challenge, where we address categorical emotion\nrecognition and emotional attribute prediction. To handle the complexities of\nnatural speech, including intra- and inter-subject variability, we propose\nMulti-level Acoustic-Textual Emotion Representation (MATER), a novel\nhierarchical framework that integrates acoustic and textual features at the\nword, utterance, and embedding levels. By fusing low-level lexical and acoustic\ncues with high-level contextualized representations, MATER effectively captures\nboth fine-grained prosodic variations and semantic nuances. Additionally, we\nintroduce an uncertainty-aware ensemble strategy to mitigate annotator\ninconsistencies, improving robustness in ambiguous emotional expressions. MATER\nranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of\n0.5928, securing second place in valence prediction with an impressive CCC of\n0.6941."}
{"id": "2506.20050", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20050", "abs": "https://arxiv.org/abs/2506.20050", "authors": ["Muhammad Zeeshan Mumtaz", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Near-Field SWIPT Using XL-MIMO: Power Allocation and Subarray Activation", "comment": "Presented at IEEE International Conference on Communications (ICC)\n  2025", "summary": "This paper investigates the simultaneous wireless information and power\ntransfer (SWIPT) capability of a modular extremely large multiple-input\nmultiple-output (XL-MIMO) system, in the context of power consumption (PC)\nefficiency. The network users are divided into two functional categories:\ninformation decoding (ID) users and energy harvesting (EH) users.\nNon-stationary near-field channels are considered whilst the users are located\nin spatially distinct visibility regions (VRs). We formulate a two-tier joint\noptimization problem to minimize the PC, taking into account the power\nallocation (PA) for ID and EH users, along with the activation of constituent\nXL-MIMO subarrays. This complicated mixed-integer problem is transformed into\nmore tractable formulations and efficient algorithms are proposed for solving\nthem. The numerical results demonstrate that the overall PC of the XL-MIMO\nsystem for the proposed method is reduced by more than 60% in comparison to the\nbenchmark scheme of equal PA with full subarray activation (SA) and 30% against\nthe case of optimized PA with full SA, while satisfying the quality-of-service\n(QoS) constraints on both the downlink rate of the ID users and harvested\nenergy at the EH users."}
{"id": "2506.20190", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.20190", "abs": "https://arxiv.org/abs/2506.20190", "authors": ["Marie Kunešová", "Zdeněk Hanzlíček", "Jindřich Matoušek"], "title": "An Exploration of ECAPA-TDNN and x-vector Speaker Representations in Zero-shot Multi-speaker TTS", "comment": "Accepted to TSD 2025", "summary": "Zero-shot multi-speaker text-to-speech (TTS) systems rely on speaker\nembeddings to synthesize speech in the voice of an unseen speaker, using only a\nshort reference utterance. While many speaker embeddings have been developed\nfor speaker recognition, their relative effectiveness in zero-shot TTS remains\nunderexplored. In this work, we employ a YourTTS-based TTS system to compare\nthree different speaker encoders - YourTTS's original H/ASP encoder, x-vector\nembeddings, and ECAPA-TDNN embeddings - within an otherwise fixed zero-shot TTS\nframework. All models were trained on the same dataset of Czech read speech and\nevaluated on 24 out-of-domain target speakers using both subjective and\nobjective methods. The subjective evaluation was conducted via a listening test\nfocused on speaker similarity, while the objective evaluation measured cosine\ndistances between speaker embeddings extracted from synthesized and real\nutterances. Across both evaluations, the original H/ASP encoder consistently\noutperformed the alternatives, with ECAPA-TDNN showing better results than\nx-vectors. These findings suggest that, despite the popularity of ECAPA-TDNN in\nspeaker recognition, it does not necessarily offer improvements for speaker\nsimilarity in zero-shot TTS in this configuration. Our study highlights the\nimportance of empirical evaluation when reusing speaker recognition embeddings\nin TTS and provides a framework for additional future comparisons."}
{"id": "2506.20067", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20067", "abs": "https://arxiv.org/abs/2506.20067", "authors": ["Muhammad Zeeshan Mumtaz", "Mohammadali Mohammadi", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Near-Field Energy Harvesting Using XL-MIMO Over Non-Stationary Channels", "comment": "IEEE Wireless Communications Letters - Accepted for the next issue\n  publication", "summary": "This paper explores the maximization of the harvested power efficiency (HPE)\nin a modular extremely large multiple-input multiple-output (XL-MIMO) system,\nwhich supports energy harvesting (EH) for near-field users. These users are\nlocated in spatially distinct visibility regions (VRs) with non-stationary\nchannel characteristics. We propose to determine which sub-arrays are switched\non or off as well the power control coefficients at the sub-arrays to maximize\nthe HPE. The design can be processed via a multi-tier joint optimization\nframework based on fractional programming. The numerical results showcase that\nthe HPE performance of the proposed algorithm is nearly optimal, comparable to\nthat of exhaustive search. As a matter of fact, it achieves up to a 120% gain\nover the benchmark scheme which uses the entire XL-MIMO array with equal power\nallocation (PA) across sub-arrays, while significantly reducing the\ncomputational time."}
{"id": "2506.20288", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.20288", "abs": "https://arxiv.org/abs/2506.20288", "authors": ["Aleš Pražák", "Marie Kunešová", "Josef Psutka"], "title": "Lightweight Target-Speaker-Based Overlap Transcription for Practical Streaming ASR", "comment": null, "summary": "Overlapping speech remains a major challenge for automatic speech recognition\n(ASR) in real-world applications, particularly in broadcast media with dynamic,\nmulti-speaker interactions. We propose a light-weight, target-speaker-based\nextension to an existing streaming ASR system to enable practical transcription\nof overlapping speech with minimal computational overhead. Our approach\ncombines a speaker-independent (SI) model for standard operation with a\nspeaker-conditioned (SC) model selectively applied in overlapping scenarios.\nOverlap detection is achieved using a compact binary classifier trained on\nfrozen SI model output, offering accurate segmentation at negligible cost. The\nSC model employs Feature-wise Linear Modulation (FiLM) to incorporate speaker\nembeddings and is trained on synthetically mixed data to transcribe only the\ntarget speaker. Our method supports dynamic speaker tracking and reuses\nexisting modules with minimal modifications. Evaluated on a challenging set of\nCzech television debates with 16% overlap, the system reduced WER on\noverlapping segments from 68.0% (baseline) to 35.78% while increasing total\ncomputational load by only 44%. The proposed system offers an effective and\nscalable solution for overlap transcription in continuous ASR services."}
{"id": "2506.20079", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20079", "abs": "https://arxiv.org/abs/2506.20079", "authors": ["Reza Hadavian", "Dmitri Truhachev"], "title": "Low-Complexity Ordered Reliability Direct Error Pattern Testing (ORDEPT) Decoding with Likelihood Thresholding", "comment": null, "summary": "We propose a reduced complexity approach to pattern-based soft decoding of\nblock codes. We start from the ORDEPT decoding algorithm which tests a list of\npartial error patterns organized in the order of their likelihood and attempts\nto complete the patterns creating candidate codewords. We then propose an early\ntermination criterion. Once a candidate codeword is found, its log-likelihood\ndifference to the received sequence is compared to a preset threshold and the\ndecoding decision is instantly made in case the likelihood deviation is below\nthe threshold. We demonstrate that while keeping the same block error rate\n(BLER) performance, the proposed algorithm's latency and complexity is multiple\ntimes smaller than that of the state-of-the art competitors including the Chase\nII, ORBGRAND, GCD, and the very recent ORDEPT with Soft-Output GRAND\ntermination which necessitates several multiplications in each query\nprocessing."}
{"id": "2506.20361", "categories": ["eess.AS", "cs.SD", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20361", "abs": "https://arxiv.org/abs/2506.20361", "authors": ["Yi Wang", "Oli Danyi Liu", "Peter Bell"], "title": "The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models", "comment": "Accepted by Interspeech 2025", "summary": "Human speech perception is multimodal. In natural speech, lip movements can\nprecede corresponding voicing by a non-negligible gap of 100-300 ms, especially\nfor specific consonants, affecting the time course of neural phonetic encoding\nin human listeners. However, it remains unexplored whether self-supervised\nlearning models, which have been used to simulate audio-visual integration in\nhumans, can capture this asynchronicity between audio and visual cues. We\ncompared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using\nlinear classifiers to track their phonetic decodability over time. We found\nthat phoneme information becomes available in AV-HuBERT embeddings only about\n20 ms before HuBERT, likely due to AV-HuBERT's lower temporal resolution and\nfeature concatenation process. It suggests AV-HuBERT does not adequately\ncapture the temporal dynamics of multimodal speech perception, limiting its\nsuitability for modeling the multimodal speech perception process."}
{"id": "2506.20084", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20084", "abs": "https://arxiv.org/abs/2506.20084", "authors": ["Mohanad Obeed", "Ming Jian"], "title": "Joint Quantization and Pruning Neural Networks Approach: A Case Study on FSO Receivers", "comment": null, "summary": "Towards fast, hardware-efficient, and low-complexity receivers, we propose a\ncompression-aware learning approach and examine it on free-space optical (FSO)\nreceivers for turbulence mitigation. The learning approach jointly quantize,\nprune, and train a convolutional neural network (CNN). In addition, we propose\nto have the CNN weights of power of two values so we replace the multiplication\noperations bit-shifting operations in every layer that has significant lower\ncomputational cost. The compression idea in the proposed approach is that the\nloss function is updated and both the quantization levels and the pruning\nlimits are optimized in every epoch of training. The compressed CNN is examined\nfor two levels of compression (1-bit and 2-bits) over different FSO systems.\nThe numerical results show that the compression approach provides negligible\ndecrease in performance in case of 1-bit quantization and the same performance\nin case of 2-bits quantization, compared to the full-precision CNNs. In\ngeneral, the proposed IM/DD FSO receivers show better bit-error rate (BER)\nperformance (without the need for channel state information (CSI)) compared to\nthe maximum likelihood (ML) receivers that utilize imperfect CSI when the DL\nmodel is compressed whether with 1-bit or 2-bit quantization."}
{"id": "2506.20231", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20231", "abs": "https://arxiv.org/abs/2506.20231", "authors": ["Xinghe Li", "Kainan Cheng", "Huiyong Li", "Huiyong Li", "Ziyang Cheng"], "title": "Sensing-Aware Transmit Waveform/Receive Filter Design for OFDM-MBS Systems", "comment": null, "summary": "In this letter, we study the problem of cooperative sensing design for an\northogonal frequency division multiplexing (OFDM) multiple base stations (MBS)\nsystem. We consider a practical scenario where the base stations (BSs) exploit\ncertain subcarriers to realize a sensing function. Since the high sidelobe\nlevel (SLL) of OFDM waveforms degrades radar detection for weak targets, and\nthe cross-correlation generated by other BSs further exacerbates detection\nperformance, we devise a joint design scheme for OFDM sequence and receive\nfilter by minimizing the integrated sidelobe level (ISL) while satisfying\nmainlobe level, peak-to-average power ratio (PAPR) and spectrum allocation\nconstraints. To address this non-convex problem, we propose an alternating\noptimization (AO)-based algorithm. Numerical simulations validate the\neffectiveness of the proposed method, demonstrating the superiority of SSL\nreduction in the MBS system over the matched filtering method."}
{"id": "2506.20237", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20237", "abs": "https://arxiv.org/abs/2506.20237", "authors": ["Yusuf Yigit Pilavci", "Pierre Palud", "Julien Flamant", "Pierre-Antoine Thouvenin", "Jérémie Boulanger", "Pierre Chainais"], "title": "Time and covariance smoothing for restoration of bivariate signals", "comment": null, "summary": "In many applications and physical phenomena, bivariate signals are polarized,\ni.e. they trace an elliptical trajectory over time when viewed in the 2D planes\nof their two components. The smooth evolution of this elliptical trajectory,\ncalled polarization ellipse, is highly informative to solve ill-posed inverse\nproblems involving bivariate signals where the signal is collected through\nindirect, noisy or incomplete measurements. This work proposes a novel\nformulation and an efficient algorithm for reconstructing bivariate signals\nwith polarization regularization. The proposed formulation leverages the\ncompact representation of polarization through the instantaneous covariance\nmatrices. To address the resulting quartic optimization problem, we propose a\nwell-suited parameter splitting strategy which leads to an efficient iterative\nalgorithm (alternating direction method of multipliers (ADMM)) with convex\nsubproblems at each iteration. The performance of the proposed method is\nillustrated on numerical synthetic data experiments."}
{"id": "2506.20248", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20248", "abs": "https://arxiv.org/abs/2506.20248", "authors": ["Sajad Rezaie", "Mikko Honkala", "Dani Korpi", "Dick Carrillo Melgarejo", "Tomasz Izydorczyk", "Dimitri Gold", "Oana-Elena Barbu"], "title": "Superimposed DMRS for Spectrally Efficient 6G Uplink Multi-User OFDM: Classical vs AI/ML Receivers", "comment": "13 pages, this work has been submitted to IEEE for consideration for\n  publication", "summary": "Fifth-generation (5G) systems utilize orthogonal demodulation reference\nsignals (DMRS) to enable channel estimation at the receiver. These orthogonal\nDMRS-also referred to as pilots-are effective in avoiding pilot contamination\nand interference from both the user's own data and that of others. However,\nthis approach incurs a significant overhead, as a substantial portion of the\ntime-frequency resources must be reserved for pilot transmission. Moreover, the\noverhead increases with the number of users and transmission layers.\n  To address these limitations in the context of emerging sixth-generation (6G)\nsystems and to support data transmission across the entire time-frequency grid,\nthe superposition of data and DMRS symbols has been explored as an alternative\nDMRS transmission strategy. In this study, we propose an enhanced version of\nDeepRx, a deep convolutional neural network (CNN)-based receiver, capable of\nestimating the channel from received superimposed (SI) DMRS symbols and\nreliably detecting the transmitted data. We also design a conventional receiver\nfor comparison, which estimates the channel from SI DMRS using classical signal\nprocessing techniques. Extensive evaluations in both uplink single-user and\nmulti-user scenarios demonstrate that DeepRx consistently outperforms the\nconventional receivers in terms of performance."}
{"id": "2506.20287", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20287", "abs": "https://arxiv.org/abs/2506.20287", "authors": ["Xiaolu Yang", "Oscar Céspedes Vicente", "Christophe Caloz"], "title": "Analog OFDM based on Real-Time Fourier Transformation", "comment": "16 pages, 13 figures", "summary": "This paper proposes an analog orthogonal frequency division multiplexing\n(OFDM) architecture based on the real-time Fourier transform (RTFT). The core\nenabling component is a linear-chirp phaser with engineered group velocity\ndispersion (GVD), which realizes RTFT and performs frequency-to-time mapping in\nthe analog domain. In this architecture, conventional digital fast Fourier\ntransform (FFT) and inverse FFT (IFFT) processors are replaced by two\nlinear-chirp phasers with opposite group delay dispersions, respectively.\nTheoretical analysis demonstrates that, under specific phaser conditions, the\nOFDM signal generated by the RTFT-based analog system is mathematically\nequivalent to that of a conventional digital OFDM system. This equivalence is\nfurther supported by simulation results, which confirm accurate symbol\ntransmission and recovery, as well as robustness to multipath fading when a\nprefix is applied. Benefiting from the use of passive microwave components, the\nanalog OFDM system offers ultra-fast processing with reduced power consumption.\nOverall, this work establishes a foundation for fully analog or hybrid\nanalog-digital OFDM system, offering a promising solution for next-generation\nhigh-speed, wideband, and energy-efficient wireless communication platforms."}
{"id": "2506.20297", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20297", "abs": "https://arxiv.org/abs/2506.20297", "authors": ["Natalie Lang", "Maya Simhi", "Nir Shlezinger"], "title": "OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning", "comment": "Under review for publication in the IEEE", "summary": "Federated learning (FL) enables collaborative training across distributed\nclients without sharing raw data, often at the cost of substantial\ncommunication overhead induced by transmitting high-dimensional model updates.\nThis overhead can be alleviated by having the clients quantize their model\nupdates, with dithered lattice quantizers identified as an attractive scheme\ndue to its structural simplicity and convergence-preserving properties.\nHowever, existing lattice-based FL schemes typically rely on a fixed\nquantization rule, which is suboptimal in heterogeneous and dynamic\nenvironments where the model updates distribution varies across users and\ntraining rounds. In this work, we propose Online Learned Adaptive Lattices\n(OLALa), a heterogeneous FL framework where each client can adjust its\nquantizer online using lightweight local computations. We first derive\nconvergence guarantees for FL with non-fixed lattice quantizers and show that\nproper lattice adaptation can tighten the convergence bound. Then, we design an\nonline learning algorithm that enables clients to tune their quantizers\nthroughout the FL process while exchanging only a compact set of quantization\nparameters. Numerical experiments demonstrate that OLALa consistently improves\nlearning performance under various quantization rates, outperforming\nconventional fixed-codebook and non-adaptive schemes."}
{"id": "2506.20336", "categories": ["eess.SP", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.20336", "abs": "https://arxiv.org/abs/2506.20336", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saif Al-Kuwari", "Khalid Qaraqe"], "title": "A Unified Framework for UAV-Based Free-Space Quantum Links: Beam Shaping and Adaptive Field-of-View Control", "comment": null, "summary": "This paper develops a comprehensive analytical framework for modeling and\nperformance evaluation of unmanned aerial vehicles (UAVs)-to-ground quantum\ncommunication links, incorporating key physical impairments such as beam\ndivergence, pointing errors at both transmitter and receiver, atmospheric\nattenuation, turbulence-induced fading, narrow field-of-view (FoV) filtering,\nand background photon noise. To overcome the limitations of conventional\nwide-beam assumptions, we introduce a grid-based approximation for photon\ncapture probability that remains accurate under tightly focused beams.\nAnalytical expressions are derived for the quantum key generation rate and\nquantum bit error rate (QBER), enabling fast and reliable system-level\nevaluation. Our results reveal that secure quantum key distribution (QKD) over\nUAV-based free-space optical (FSO) links requires beam waists below 10 cm and\nsub-milliradian tracking precision to achieve Mbps-level key rates and QBER\nbelow $10^{-3}$. Additionally, we highlight the critical role of receiver FoV\nin balancing background noise rejection and misalignment tolerance, and propose\nadaptive FoV tuning strategies under varying illumination and alignment\nconditions. The proposed framework provides a tractable and accurate tool for\nthe design, optimization, and deployment of next-generation airborne quantum\ncommunication systems."}
{"id": "2506.20424", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20424", "abs": "https://arxiv.org/abs/2506.20424", "authors": ["Ziwei Liu", "Junyan He", "Shanshan Zhao", "Meng Hua", "Bin Lyu", "Xinjie Zhao", "Gengxin Zhang"], "title": "Active RIS Enabled NLoS LEO Satellite Communications: A Three-timescale Optimization Framework", "comment": "5 pages, 5 figures", "summary": "In this letter, we study an active reconfigurable intelligent surfaces (RIS)\nassisted Low Earth orbit (LEO) satellite communications under non-line-of-sight\n(NLoS) scenarios, where the active RIS is deployed to create visual\nline-of-sight links for reliable communication. To address the challenges of\nhigh energy consumption caused by frequent beamforming updates in active RIS,\nwe propose a three-timescale optimization framework that jointly designs the\ntransmit beamforming, RIS beamforming, and RIS direction vectors based on their\ncharacteristics. The goal is to maximize the system achievable rate while\nreducing energy consumption by controlling the RIS beamforming switching\nfrequency. Then, a two-layer solution framework is developed, incorporating\nfractional programming (FP), alternating optimization (AO), successive\napproximation (SCA), and penalty-based methods, to obtain the optimized\nsolution. Simulation results demonstrate that the proposed scheme can\neffectively improve system performance and reduce the energy consumption of the\nactive RIS."}
{"id": "2506.20534", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20534", "abs": "https://arxiv.org/abs/2506.20534", "authors": ["Dylan Sechet", "Matthieu Kowalski", "Samy Mokhtari", "Bruno Torrésani"], "title": "Revisiting CHAMPAGNE: Sparse Bayesian Learning as Reweighted Sparse Coding", "comment": "Sampling Theory and Applications (SampTA) 2025", "summary": "This paper revisits the CHAMPAGNE algorithm within the Sparse Bayesian\nLearning (SBL) framework and establishes its connection to reweighted sparse\ncoding. We demonstrate that the SBL objective can be reformulated as a\nreweighted $\\ell_{21}$-minimization problem, providing a more straightforward\ninterpretation of the sparsity mechanism and enabling the design of an\nefficient iterative algorithm. Additionally, we analyze the behavior of this\nreformulation in the low signal-to-noise ratio (SNR) regime, showing that it\nsimplifies to a weighted $\\ell_{21}$-regularized least squares problem.\nNumerical experiments validate the proposed approach, highlighting its improved\ncomputational efficiency and ability to produce exact sparse solutions,\nparticularly in simulated MEG source localization tasks."}
{"id": "2506.20589", "categories": ["eess.SP", "cs.ET", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2506.20589", "abs": "https://arxiv.org/abs/2506.20589", "authors": ["Jorge Torres Gómez", "Pit Hofmann", "Lisa Y. Debus", "Osman Tugay Başaran", "Sebastian Lotter", "Roya Khanzadeh", "Stefan Angerbauer", "Bige Deniz Unluturk", "Sergi Abadal", "Werner Haselmayr", "Frank H. P. Fitzek", "Robert Schober", "Falko Dressler"], "title": "Communicating Smartly in Molecular Communication Environments: Neural Networks in the Internet of Bio-Nano Things", "comment": "Paper submitted to IEEE Communications Surveys & Tutorials", "summary": "Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the\ngroundwork for innovative applications across the healthcare sector.\nNanodevices designed to operate within the body, managed remotely via the\ninternet, are envisioned to promptly detect and actuate on potential diseases.\nIn this vision, an inherent challenge arises due to the limited capabilities of\nindividual nanosensors; specifically, nanosensors must communicate with one\nanother to collaborate as a cluster. Aiming to research the boundaries of the\nclustering capabilities, this survey emphasizes data-driven communication\nstrategies in molecular communication (MC) channels as a means of linking\nnanosensors. Relying on the flexibility and robustness of machine learning (ML)\nmethods to tackle the dynamic nature of MC channels, the MC research community\nfrequently refers to neural network (NN) architectures. This interdisciplinary\nresearch field encompasses various aspects, including the use of NNs to\nfacilitate communication in MC environments, their implementation at the\nnanoscale, explainable approaches for NNs, and dataset generation for training.\nWithin this survey, we provide a comprehensive analysis of fundamental\nperspectives on recent trends in NN architectures for MC, the feasibility of\ntheir implementation at the nanoscale, applied explainable artificial\nintelligence (XAI) techniques, and the accessibility of datasets along with\nbest practices for their generation. Additionally, we offer open-source code\nrepositories that illustrate NN-based methods to support reproducible research\nfor key MC scenarios. Finally, we identify emerging research challenges, such\nas robust NN architectures, biologically integrated NN modules, and scalable\ntraining strategies."}
{"id": "2506.20597", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20597", "abs": "https://arxiv.org/abs/2506.20597", "authors": ["Soheyb Ribouh", "Osama Saleem", "Mohamed Ababsa"], "title": "Differential Transformer-driven 6G Physical Layer for Collaborative Perception Enhancement", "comment": null, "summary": "The emergence of 6G wireless networks promises to revolutionize vehicular\ncommunications by enabling ultra-reliable, low-latency, and high-capacity data\nexchange. In this context, collaborative perception techniques, where multiple\nvehicles or infrastructure nodes cooperate to jointly receive and decode\ntransmitted signals, aim to enhance reliability and spectral efficiency for\nConnected Autonomous Vehicle (CAV) applications. In this paper, we propose an\nend-to-end wireless neural receiver based on a Differential Transformer\narchitecture, tailored for 6G V2X communication with a specific focus on\nenabling collaborative perception among connected autonomous vehicles. Our\nmodel integrates key components of the 6G physical layer, designed to boost\nperformance in dynamic and challenging autonomous driving environments. We\nvalidate the proposed system across a range of scenarios, including\n3GPP-defined Urban Macro (UMa) channel. To assess the model's real-world\napplicability, we evaluate its robustness within a V2X framework. In a\ncollaborative perception scenario, our system processes heterogeneous LiDAR and\ncamera data from four connected vehicles in dynamic cooperative vehicular\nnetworks. The results show significant improvements over state-of-the-art\nmethods, achieving an average precision of 0.84, highlighting the potential of\nour proposed approach to enable robust, intelligent, and adaptive wireless\ncooperation for next-generation connected autonomous vehicles."}
{"id": "2506.20637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20637", "abs": "https://arxiv.org/abs/2506.20637", "authors": ["Fardad Vakilipoor", "Nora Hirschmann", "Julian Schladt", "Stefan Schwab", "Annette Reineke", "Robert Schober", "Kathrin Castiglione", "Maximilian Schaefer"], "title": "MC for Agriculture: A Framework for Nature-inspired Sustainable Pest Control", "comment": null, "summary": "In agriculture, molecular communication (MC) is envisioned as a framework to\naddress critical challenges such as smart pest control. While conventional\napproaches mostly rely on synthetic plant protection products, posing high\nrisks for the environment, harnessing plant signaling processes can lead to\ninnovative approaches for nature-inspired sustainable pest control. In this\npaper, we investigate an approach for sustainable pest control and reveal how\nthe MC paradigm can be employed for analysis and optimization. In particular,\nwe consider a system where herbivore-induced plant volatiles (HIPVs),\nspecifically methyl salicylate (MeSA), is encapsulated into microspheres\ndeployed on deployed on plant leaves. The controlled release of MeSA from the\nmicrospheres, acting as transmitters (TXs), supports pest deterrence and\nantagonist attraction, providing an eco-friendly alternative to synthetic plant\nprotection products. Based on experimental data, we investigate the MeSA\nrelease kinetics and obtain an analytical model. To describe the propagation of\nMeSA in farming environments, we employ a three dimensional (3D)\nadvection-diffusion model, incorporating realistic wind fields which are\npredominantly affecting particle propagation, and solve it by a finite\ndifference method (FDM). The proposed model is used to investigate the MeSA\ndistribution for different TX arrangements, representing different practical\nmicrosphere deployment strategies. Moreover, we introduce the coverage\neffectiveness index (CEI) as a novel metric to quantify the environmental\ncoverage of MeSA. This analysis offers valuable guidance for the practical\ndevelopment of microspheres and their deployment aimed at enhancing coverage\nand, consequently, the attraction of antagonistic insects."}

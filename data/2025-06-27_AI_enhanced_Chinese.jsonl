{"id": "2506.20783", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20783", "abs": "https://arxiv.org/abs/2506.20783", "authors": ["Zijun Wang", "Shawn Tsai", "Rama Kiran", "Rui Zhang"], "title": "Precise Near-Field Beam Training with DFT Codebook based on Amplitude-only Measurement", "comment": null, "summary": "Extremely large antenna arrays (ELAAs) operating in high-frequency bands have\nspurred the development of near-field communication, driving advancements in\nbeam training and signal processing design. In this work, we present a\nlow-complexity near-field beam training scheme that fully utilizes the\nconventional discrete Fourier transform (DFT) codebook designed for far-field\nusers. We begin by analyzing the received beam pattern in the near field and\nderive closed-form expressions for the beam width and central gain. These\nanalytical results enable the definition of an angle-dependent, modified\nRayleigh distance, which effectively distinguishes near-field and far-field\nuser regimes. Building on the analysis, we develop a direct and computationally\nefficient method to estimate user distance, with a complexity of O(1), and\nfurther improve its accuracy through a simple refinement. Simulation results\ndemonstrate significant gains in both single- and multi-user settings, with up\nto 2.38 dB SNR improvement over exhaustive search. To further enhance\nestimation accuracy, we additionally propose a maximum likelihood estimation\n(MLE) based refinement method, leveraging the Rician distribution of signal\namplitudes and achieving accuracy close to the Cramer--Rao bound (CRB).\nSimulation shows the single-user and multi-user achievable rates can both\napproach those obtained with ideal channel state information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u8fd1\u573a\u6ce2\u675f\u8bad\u7ec3\u65b9\u6848\uff0c\u5229\u7528\u8fdc\u573a\u7528\u6237\u7684DFT\u7801\u672c\uff0c\u901a\u8fc7\u5206\u6790\u8fd1\u573a\u6ce2\u675f\u6a21\u5f0f\u548c\u63a8\u5bfc\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7528\u6237\u8ddd\u79bb\u4f30\u8ba1\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6781\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\uff08ELAAs\uff09\u5728\u9ad8\u9891\u6bb5\u7684\u8fd1\u573a\u901a\u4fe1\u9700\u6c42\u63a8\u52a8\u4e86\u6ce2\u675f\u8bad\u7ec3\u548c\u4fe1\u53f7\u5904\u7406\u8bbe\u8ba1\u7684\u8fdb\u6b65\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u5ea6\u8f83\u9ad8\u3002", "method": "\u5206\u6790\u8fd1\u573a\u6ce2\u675f\u6a21\u5f0f\uff0c\u63a8\u5bfc\u6ce2\u675f\u5bbd\u5ea6\u548c\u4e2d\u5fc3\u589e\u76ca\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5b9a\u4e49\u89d2\u5ea6\u4f9d\u8d56\u7684\u4fee\u6b63\u745e\u5229\u8ddd\u79bb\uff0c\u63d0\u51faO(1)\u590d\u6742\u5ea6\u7684\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff08MLE\uff09\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5355\u7528\u6237\u548c\u591a\u7528\u6237\u573a\u666f\u4e0bSNR\u63d0\u5347\u8fbe2.38 dB\uff0c\u4e14\u53ef\u8fbe\u901f\u7387\u63a5\u8fd1\u7406\u60f3\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u65b9\u6848\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u573a\u901a\u4fe1\u6027\u80fd\uff0c\u4e3aELAAs\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20798", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20798", "abs": "https://arxiv.org/abs/2506.20798", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saif Al-Kuwari", "Khalid Qaraqe"], "title": "Physical Limits of Entanglement-Based Quantum Key Distribution over Long-Distance Satellite Links", "comment": null, "summary": "Entanglement-based quantum key distribution (QKD) protocols, such as E91 and\nBBM92, offer strong information-theoretic security and are naturally suited for\nsatellite-to-satellite QKD (SatQKD) links. However, implementing these\nprotocols over long-distance inter-satellite free-space optical (FSO) channels\nposes critical physical-layer challenges that are not addressed in the existing\nliterature. In particular, photon losses due to beam divergence, pointing\nerrors, and background noise can severely degrade the key generation rate and\nquantum bit error rate (QBER), especially under narrow receiver field-of-view\n(FoV) constraints. This paper presents a comprehensive performance analysis of\nentanglement-based inter-satellite QKD, focusing on photon-level modeling and\nthe impact of practical impairments. We develop analytical expressions for\nsignal detection probabilities, background photon influence, multi-pair\nemissions, and QBER, incorporating key parameters such as link distance,\ntransmitter tracking jitter, receiver misalignment, and photon pair generation\nrate. Simulation results reveal the nonlinear sensitivity of system performance\nto tracking error and FoV limitations, and highlight optimal parameter regimes\nthat jointly maximize secret key rate while maintaining QBER below acceptable\nthresholds. The proposed model provides actionable design insights for reliable\nand efficient deployment of entanglement-based SatQKD systems.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u7ea0\u7f20\u7684\u536b\u661f\u95f4\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u534f\u8bae\u5728\u957f\u8ddd\u79bb\u81ea\u7531\u7a7a\u95f4\u5149\uff08FSO\uff09\u4fe1\u9053\u4e2d\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5149\u5b50\u7ea7\u5efa\u6a21\u548c\u5b9e\u9645\u635f\u4f24\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u672a\u5145\u5206\u89e3\u51b3\u957f\u8ddd\u79bb\u536b\u661f\u95f4QKD\u94fe\u8def\u4e2d\u7684\u5149\u5b50\u635f\u8017\u3001\u6307\u5411\u8bef\u5dee\u548c\u80cc\u666f\u566a\u58f0\u7b49\u7269\u7406\u5c42\u6311\u6218\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f1a\u4e25\u91cd\u5f71\u54cd\u5bc6\u94a5\u751f\u6210\u7387\u548c\u91cf\u5b50\u6bd4\u7279\u8bef\u7801\u7387\uff08QBER\uff09\u3002", "method": "\u901a\u8fc7\u5149\u5b50\u7ea7\u5efa\u6a21\uff0c\u5f00\u53d1\u4e86\u4fe1\u53f7\u68c0\u6d4b\u6982\u7387\u3001\u80cc\u666f\u5149\u5b50\u5f71\u54cd\u3001\u591a\u5bf9\u53d1\u5c04\u548cQBER\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u7ed3\u5408\u94fe\u8def\u8ddd\u79bb\u3001\u53d1\u5c04\u5668\u8ddf\u8e2a\u6296\u52a8\u3001\u63a5\u6536\u5668\u5bf9\u51c6\u8bef\u5dee\u7b49\u5173\u952e\u53c2\u6570\u8fdb\u884c\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u7cfb\u7edf\u6027\u80fd\u5bf9\u8ddf\u8e2a\u8bef\u5dee\u548c\u63a5\u6536\u5668\u89c6\u573a\u9650\u5236\u5177\u6709\u975e\u7ebf\u6027\u654f\u611f\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u4fdd\u6301QBER\u4f4e\u4e8e\u53ef\u63a5\u53d7\u9608\u503c\u7684\u540c\u65f6\u6700\u5927\u5316\u5bc6\u94a5\u751f\u6210\u7387\u7684\u6700\u4f18\u53c2\u6570\u8303\u56f4\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u53ef\u9760\u9ad8\u6548\u5730\u90e8\u7f72\u57fa\u4e8e\u7ea0\u7f20\u7684\u536b\u661f\u95f4QKD\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2506.20823", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20823", "abs": "https://arxiv.org/abs/2506.20823", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna"], "title": "Compact Analytical Model for Real-Time Evaluation of OAM-Based Inter-Satellite Links", "comment": null, "summary": "This paper presents an efficient analytical framework for evaluating the\nperformance of inter-satellite communication systems utilizing orbital angular\nmomentum (OAM) beams under pointing errors. An accurate analytical model is\nfirst developed to characterize intermodal crosstalk caused by beam\nmisalignment in OAM-based inter-satellite links. Building upon this model, we\nderive efficient expressions to analyze and optimize system performance in\nterms of bit error rate (BER). Unlike traditional Monte Carlo-based methods\nthat are computationally intensive, the proposed approach offers accurate\nperformance predictions. This enables a substantial decrease in computation\ntime while maintaining high accuracy, thanks to the use of analytical\nexpressions for both crosstalk and BER. This fast and accurate evaluation\ncapability is particularly critical for dynamic low Earth orbit (LEO) satellite\nconstellations, where network topology and channel conditions change rapidly,\nrequiring real-time link adaptation. Furthermore, we systematically design and\nevaluate asymmetric OAM mode sets, which significantly outperform symmetric\nconfigurations in the presence of pointing errors. Our results also reveal key\ninsights into the interaction between beam divergence, tracking accuracy, and\nlink distance, demonstrating that the proposed framework enables real-time\noptimization of system parameters with high fidelity. The analytical findings\nare rigorously validated against extensive Monte Carlo simulations, confirming\ntheir practical applicability for high-mobility optical wireless systems such\nas LEO satellite networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8f68\u9053\u89d2\u52a8\u91cf\uff08OAM\uff09\u5149\u675f\u5728\u6307\u5411\u8bef\u5dee\u4e0b\u7684\u661f\u95f4\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\uff0c\u901a\u8fc7\u89e3\u6790\u6a21\u578b\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u65e0\u6cd5\u6ee1\u8db3\u52a8\u6001\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u661f\u5ea7\u5bf9\u5b9e\u65f6\u94fe\u8def\u9002\u914d\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u7cbe\u786e\u7684\u89e3\u6790\u6a21\u578b\uff0c\u5206\u6790OAM\u5149\u675f\u5bf9\u51c6\u8bef\u5dee\u5f15\u8d77\u7684\u6a21\u6001\u95f4\u4e32\u6270\uff0c\u5e76\u63a8\u5bfc\u51fa\u9ad8\u6548\u7684\u6bd4\u7279\u8bef\u7801\u7387\uff08BER\uff09\u4f18\u5316\u8868\u8fbe\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c\u975e\u5bf9\u79f0OAM\u6a21\u5f0f\u96c6\u5728\u6307\u5411\u8bef\u5dee\u4e0b\u8868\u73b0\u4f18\u4e8e\u5bf9\u79f0\u914d\u7f6e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u673a\u52a8\u6027\u5149\u65e0\u7ebf\u7cfb\u7edf\uff08\u5982LEO\u536b\u661f\u7f51\u7edc\uff09\u63d0\u4f9b\u4e86\u5b9e\u65f6\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2506.20858", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20858", "abs": "https://arxiv.org/abs/2506.20858", "authors": ["Jamil Farhat", "Gianni Pasolini", "Enrico Paolini", "Muhammad Asad Ullah", "Richard Demo Souza"], "title": "Doppler Estimation and Compensation Techniques in LoRa Direct-to-Satellite Communications", "comment": null, "summary": "Within the LPWAN framework, the LoRa modulation adopted by LoRaWAN technology\nhas garnered significant interest as a connectivity solution for IoT\napplications due to its ability to offer low-cost, low-power, and long-range\ncommunications. One emerging use case of LoRa is DtS connectivity, which\nextends coverage to remote areas for supporting IoT operations. The satellite\nIoT industry mainly prefers LEO because it has lower launch costs and less path\nloss compared to Geostationary orbit. However, a major drawback of LEO\nsatellites is the impact of the Doppler effect caused by their mobility.\nEarlier studies have confirmed that the Doppler effect significantly degrades\nthe LoRa DtS performance. In this paper, we propose four frameworks for Doppler\nestimation and compensation in LoRa DtS connectivity and numerically compare\nthe performance against the ideal scenario without the Doppler effect.\nFurthermore, we investigate the trade-offs among these frameworks by analyzing\nthe interplay between spreading factor, and other key parameters related to the\nDoppler effect. The results provide insights into how to achieve robust LoRa\nconfigurations for DtS connectivity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u56db\u79cd\u6846\u67b6\u7528\u4e8eLoRa DtS\u8fde\u63a5\u4e2d\u7684\u591a\u666e\u52d2\u6548\u5e94\u4f30\u8ba1\u4e0e\u8865\u507f\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6bd4\u8f83\u5206\u6790\u5176\u6027\u80fd\uff0c\u63a2\u8ba8\u4e86\u5173\u952e\u53c2\u6570\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "LEO\u536b\u661f\u5728LoRa DtS\u8fde\u63a5\u4e2d\u56e0\u591a\u666e\u52d2\u6548\u5e94\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u7814\u7a76\u8865\u507f\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u901a\u4fe1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u591a\u666e\u52d2\u4f30\u8ba1\u4e0e\u8865\u507f\u6846\u67b6\uff0c\u5e76\u4e0e\u7406\u60f3\u65e0\u591a\u666e\u52d2\u573a\u666f\u8fdb\u884c\u6570\u503c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u5206\u6790\u4e86\u6269\u9891\u56e0\u5b50\u7b49\u5173\u952e\u53c2\u6570\u4e0e\u591a\u666e\u52d2\u6548\u5e94\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u4f18\u5316LoRa DtS\u914d\u7f6e\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5b9e\u73b0\u7a33\u5065\u7684LoRa DtS\u8fde\u63a5\u914d\u7f6e\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2506.21074", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.21074", "abs": "https://arxiv.org/abs/2506.21074", "authors": ["Hankun Wang", "Yiwei Guo", "Chongtian Shao", "Bohan Li", "Xie Chen", "Kai Yu"], "title": "CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via Dynamic Frame Rate", "comment": "16 pages, 5 figures, 9 tables", "summary": "Neural speech codecs have been widely used in audio compression and various\ndownstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which\nallocate the same number of tokens to every equal-duration slice. However,\nspeech is inherently non-uniform in temporal information density. As a result,\nmany tokens are wasted on steady-state segments like long vowels and silences.\nTo address this mismatch, we present CodecSlime, a plugin-style method for\ncompressing temporal redundancy through supporting dynamic frame rate (DFR) on\nneural speech codecs for the first time. Our method is unsupervised and\narchitecture-agnostic, combining two key innovations, ScheDFR and\nMelt-and-Cool, for adapting inference and training, respectively. When\nintegrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR\n($\\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to\n46% relative to conventional FFR baselines with the same model architecture and\nsimilar bitrates, while other metrics are also competitive. CodecSlime also\nenables flexible trade-offs between reconstruction quality and bitrate: a\nsingle model supports inference at multiple frame rates and consistently\noutperforms FFR models at the corresponding frame rates. Audio samples are\navailable at https://acadarmeria.github.io/codecslime/.", "AI": {"tldr": "CodecSlime\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5e27\u7387\uff08DFR\uff09\u7684\u795e\u7ecf\u8bed\u97f3\u7f16\u89e3\u7801\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u5e27\u7387\uff08FFR\uff09\u5728\u8bed\u97f3\u538b\u7f29\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6bd4\u7279\u7387\u7075\u6d3b\u6027\u3002", "motivation": "\u8bed\u97f3\u7684\u65f6\u95f4\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5747\u5300\uff0c\u56fa\u5b9a\u5e27\u7387\u7f16\u89e3\u7801\u5668\u5728\u7a33\u6001\u6bb5\uff08\u5982\u957f\u5143\u97f3\u548c\u9759\u97f3\uff09\u6d6a\u8d39\u4e86\u5927\u91cf\u6807\u8bb0\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "CodecSlime\u7ed3\u5408\u4e86ScheDFR\uff08\u63a8\u7406\u9002\u5e94\uff09\u548cMelt-and-Cool\uff08\u8bad\u7ec3\u9002\u5e94\uff09\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u652f\u6301\u52a8\u6001\u5e27\u7387\uff0c\u65e0\u9700\u76d1\u7763\u4e14\u4e0e\u67b6\u6784\u65e0\u5173\u3002", "result": "\u572840 Hz DFR\uff08\u7ea6600 bps\uff09\u4e0b\uff0cCodecSlime\u7684\u91cd\u5efaWER\u6bd4\u4f20\u7edfFFR\u57fa\u7ebf\u964d\u4f4e\u4e8646%\uff0c\u5176\u4ed6\u6307\u6807\u4e5f\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u652f\u6301\u591a\u5e27\u7387\u63a8\u7406\u3002", "conclusion": "CodecSlime\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u7f16\u89e3\u7801\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u652f\u6301\u7075\u6d3b\u7684\u6bd4\u7279\u7387\u4e0e\u91cd\u5efa\u8d28\u91cf\u6743\u8861\uff0c\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u5e27\u7387\u65b9\u6cd5\u3002"}}
{"id": "2506.20945", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20945", "abs": "https://arxiv.org/abs/2506.20945", "authors": ["Rui Niu", "Weihao Wu", "Jie Chen", "Long Ma", "Zhiyong Wu"], "title": "A Multi-Stage Framework for Multimodal Controllable Speech Synthesis", "comment": "Accepted by ICME2025", "summary": "Controllable speech synthesis aims to control the style of generated speech\nusing reference input, which can be of various modalities. Existing face-based\nmethods struggle with robustness and generalization due to data quality\nconstraints, while text prompt methods offer limited diversity and fine-grained\ncontrol. Although multimodal approaches aim to integrate various modalities,\ntheir reliance on fully matched training data significantly constrains their\nperformance and applicability. This paper proposes a 3-stage multimodal\ncontrollable speech synthesis framework to address these challenges. For face\nencoder, we use supervised learning and knowledge distillation to tackle\ngeneralization issues. Furthermore, the text encoder is trained on both\ntext-face and text-speech data to enhance the diversity of the generated\nspeech. Experimental results demonstrate that this method outperforms\nsingle-modal baseline methods in both face based and text prompt based speech\nsynthesis, highlighting its effectiveness in generating high-quality speech.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u591a\u6a21\u6001\u53ef\u63a7\u8bed\u97f3\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u4eba\u8138\u7f16\u7801\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u6587\u672c\u7f16\u7801\u5668\u589e\u5f3a\u8bed\u97f3\u591a\u6837\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u4eba\u8138\u7684\u65b9\u6cd5\u56e0\u6570\u636e\u8d28\u91cf\u9650\u5236\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u6587\u672c\u63d0\u793a\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u4f18\u5316\u4eba\u8138\u7f16\u7801\u5668\uff0c\u540c\u65f6\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u4ee5\u6574\u5408\u6587\u672c-\u4eba\u8138\u548c\u6587\u672c-\u8bed\u97f3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e\u4eba\u8138\u548c\u6587\u672c\u63d0\u793a\u7684\u8bed\u97f3\u5408\u6210\u4e2d\u5747\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u8bed\u97f3\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u4f9d\u8d56\u548c\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2506.20863", "categories": ["eess.SP", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.20863", "abs": "https://arxiv.org/abs/2506.20863", "authors": ["Naoki Ishikawa", "Giuseppe Thadeu Freitas de Abreu", "Petar Popovski", "Robert W. Heath Jr"], "title": "Quantum-Accelerated Wireless Communications: Concepts, Connections, and Implications", "comment": "7 pages, 6 figures", "summary": "Quantum computing is poised to redefine the algorithmic foundations of\ncommunication systems. While quantum superposition and entanglement enable\nquadratic or exponential speedups for specific problems, identifying use cases\nwhere these advantages yield engineering benefits is, however, still\nnontrivial. This article presents the fundamentals of quantum computing in a\nstyle familiar to the communications society, outlining the current limits of\nfault-tolerant quantum computing and uncovering a mathematical harmony between\nquantum and wireless systems, which makes the topic more enticing to wireless\nresearchers. Based on a systematic review of pioneering and state-of-the-art\nstudies, we distill common design trends for the research and development of\nquantum-accelerated communication systems and highlight lessons learned. The\nkey insight is that classical heuristics can sharpen certain quantum\nparameters, underscoring the complementary strengths of classical and quantum\ncomputing. This article aims to catalyze interdisciplinary research at the\nfrontier of quantum information processing and future communication systems.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4ecd\u9700\u63a2\u7d22\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u91cf\u5b50\u8ba1\u7b97\u57fa\u7840\u53ca\u5176\u4e0e\u65e0\u7ebf\u7cfb\u7edf\u7684\u6570\u5b66\u8054\u7cfb\uff0c\u603b\u7ed3\u4e86\u91cf\u5b50\u52a0\u901f\u901a\u4fe1\u7cfb\u7edf\u7684\u8bbe\u8ba1\u8d8b\u52bf\uff0c\u5e76\u5f3a\u8c03\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bf9\u91cf\u5b50\u53c2\u6570\u7684\u4f18\u5316\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u4fc3\u8fdb\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e0e\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u524d\u6cbf\u7814\u7a76\uff0c\u603b\u7ed3\u91cf\u5b50\u52a0\u901f\u901a\u4fe1\u7cfb\u7edf\u7684\u8bbe\u8ba1\u8d8b\u52bf\uff0c\u5e76\u5206\u6790\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bf9\u91cf\u5b50\u53c2\u6570\u7684\u4f18\u5316\u4f5c\u7528\u3002", "result": "\u63ed\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u4e0e\u65e0\u7ebf\u7cfb\u7edf\u4e4b\u95f4\u7684\u6570\u5b66\u8054\u7cfb\uff0c\u5e76\u5c55\u793a\u4e86\u7ecf\u5178\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7684\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u53ef\u4f18\u5316\u91cf\u5b50\u53c2\u6570\uff0c\u63a8\u52a8\u8de8\u5b66\u79d1\u7814\u7a76\u3002"}}
{"id": "2506.21090", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21090", "abs": "https://arxiv.org/abs/2506.21090", "authors": ["Wanying Ge", "Xin Wang", "Xuechen Liu", "Junichi Yamagishi"], "title": "Post-training for Deepfake Speech Detection", "comment": null, "summary": "We introduce a post-training approach that adapts self-supervised learning\n(SSL) models for deepfake speech detection by bridging the gap between general\npre-training and domain-specific fine-tuning. We present AntiDeepfake models, a\nseries of post-trained models developed using a large-scale multilingual speech\ndataset containing over 56,000 hours of genuine speech and 18,000 hours of\nspeech with various artifacts in over one hundred languages. Experimental\nresults show that the post-trained models already exhibit strong robustness and\ngeneralization to unseen deepfake speech. When they are further fine-tuned on\nthe Deepfake-Eval-2024 dataset, these models consistently surpass existing\nstate-of-the-art detectors that do not leverage post-training. Model\ncheckpoints and source code are available online.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6865\u63a5\u901a\u7528\u9884\u8bad\u7ec3\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c06\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u68c0\u6d4b\u4e2d\u901a\u7528\u9884\u8bad\u7ec3\u4e0e\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff08\u5305\u542b\u8d85\u8fc756,000\u5c0f\u65f6\u7684\u771f\u5b9e\u8bed\u97f3\u548c18,000\u5c0f\u65f6\u7684\u5e26\u6709\u5404\u79cd\u4f2a\u5f71\u7684\u8bed\u97f3\uff09\u5f00\u53d1\u4e86AntiDeepfake\u6a21\u578b\u7cfb\u5217\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u540e\u8bad\u7ec3\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5728Deepfake-Eval-2024\u6570\u636e\u96c6\u4e0a\u8fdb\u4e00\u6b65\u5fae\u8c03\u540e\uff0c\u8fd9\u4e9b\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u4e0d\u5229\u7528\u540e\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\u3002", "conclusion": "\u540e\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.21086", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.3.1; H.3.3; H.3.4"], "pdf": "https://arxiv.org/pdf/2506.21086", "abs": "https://arxiv.org/abs/2506.21086", "authors": ["Guillem Cort\u00e8s-Sebasti\u00e0", "Benjamin Martin", "Emilio Molina", "Xavier Serra", "Romain Hennequin"], "title": "PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching", "comment": "Accepted at ISMIR 2025", "summary": "This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)\nsystem designed specifically around spectral peaks. This novel system is\ndesigned to leverage the sparse spectral coordinates typically computed by\ntraditional peak-based AFP methods. PeakNetFP performs hierarchical point\nfeature extraction techniques similar to the computer vision model PointNet++,\nand is trained using contrastive learning like in the state-of-the-art deep\nlearning AFP, NeuralFP. This combination allows PeakNetFP to outperform\nconventional AFP systems and achieves comparable performance to NeuralFP when\nhandling challenging time-stretched audio data. In extensive evaluation,\nPeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging\nfrom 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:\ncompared to NeuralFP, it has 100 times fewer parameters and uses 11 times\nsmaller input data. These features make PeakNetFP a lightweight and efficient\nsolution for AFP tasks where time stretching is involved. Overall, this system\nrepresents a promising direction for future AFP technologies, as it\nsuccessfully merges the lightweight nature of peak-based AFP with the\nadaptability and pattern recognition capabilities of neural network-based\napproaches, paving the way for more scalable and efficient solutions in the\nfield.", "AI": {"tldr": "PeakNetFP\u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u5cf0\u503c\u7684\u795e\u7ecf\u97f3\u9891\u6307\u7eb9\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u4f20\u7edf\u5cf0\u503c\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5728\u65f6\u95f4\u62c9\u4f38\u97f3\u9891\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53c2\u6570\u548c\u8f93\u5165\u6570\u636e\u91cf\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "\u4f20\u7edf\u5cf0\u503c\u97f3\u9891\u6307\u7eb9\u65b9\u6cd5\u5728\u65f6\u95f4\u62c9\u4f38\u97f3\u9891\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5982NeuralFP\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002PeakNetFP\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "PeakNetFP\u91c7\u7528\u7c7b\u4f3cPointNet++\u7684\u5206\u5c42\u70b9\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u5e76\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e86\u5cf0\u503c\u65b9\u6cd5\u7684\u7a00\u758f\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u3002", "result": "PeakNetFP\u572850%\u81f3200%\u7684\u65f6\u95f4\u62c9\u4f38\u8303\u56f4\u5185\u4fdd\u630190%\u4ee5\u4e0a\u7684Top-1\u547d\u4e2d\u7387\uff0c\u53c2\u6570\u548c\u8f93\u5165\u6570\u636e\u91cf\u5206\u522b\u6bd4NeuralFP\u5c11100\u500d\u548c11\u500d\u3002", "conclusion": "PeakNetFP\u6210\u529f\u878d\u5408\u4e86\u5cf0\u503c\u65b9\u6cd5\u7684\u8f7b\u91cf\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u9002\u5e94\u6027\uff0c\u4e3a\u97f3\u9891\u6307\u7eb9\u6280\u672f\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20970", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.20970", "abs": "https://arxiv.org/abs/2506.20970", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Fan Liu", "Yuanhao Cui"], "title": "Co-Design of Sensing, Communications, and Control for Low-Altitude Wireless Networks", "comment": null, "summary": "The rapid advancement of Internet of Things (IoT) services and the evolution\ntoward the sixth generation (6G) have positioned unmanned aerial vehicles\n(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This\nwork investigates the co-design of integrated sensing, communication, and\ncontrol ($\\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite\nblocklength (FBL) transmission. In particular, the UAVs continuously monitor\nthe state of the field robots and transmit their observations to the robot\ncontroller to ensure stable control while cooperating to localize an unknown\nsensing target (ST). To this end, a weighted optimization problem is first\nformulated by jointly considering the control and localization performance in\nterms of the linear quadratic regulator (LQR) cost and the determinant of the\nFisher information matrix (FIM), respectively. The resultant problem,\noptimizing resource allocations, the UAVs' deployment positions, and multi-user\nscheduling, is non-convex. To circumvent this challenge, we first derive a\nclosed-form expression of the LQR cost with respect to other variables.\nSubsequently, the non-convex optimization problem is decomposed into a series\nof sub-problems by leveraging the alternating optimization (AO) approach, in\nwhich the difference of convex functions (DC) programming and projected\ngradient descent (PGD) method are employed to obtain an efficient near-optimal\nsolution. Furthermore, the convergence and computational complexity of the\nproposed algorithm are thoroughly analyzed. Extensive simulation results are\npresented to validate the effectiveness of our proposed approach compared to\nthe benchmark schemes and reveal the trade-off between control and sensing\nperformance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u65e0\u4eba\u673a\u534f\u540c\u7cfb\u7edf\u4e2d\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u63a7\u5236\u7684\u8054\u5408\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u7684\u975e\u51f8\u95ee\u9898\u89e3\u51b3\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u670d\u52a1\u548c6G\u6280\u672f\u7684\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u5728\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\u3002\u7814\u7a76\u5982\u4f55\u4f18\u5316\u65e0\u4eba\u673a\u534f\u540c\u7cfb\u7edf\u4e2d\u7684\u611f\u77e5\u3001\u901a\u4fe1\u548c\u63a7\u5236\u6027\u80fd\uff0c\u4ee5\u652f\u6301\u7a33\u5b9a\u63a7\u5236\u548c\u76ee\u6807\u5b9a\u4f4d\u3002", "method": "\u901a\u8fc7\u8054\u5408\u8003\u8651\u63a7\u5236\u6027\u80fd\uff08LQR\u6210\u672c\uff09\u548c\u5b9a\u4f4d\u6027\u80fd\uff08Fisher\u4fe1\u606f\u77e9\u9635\u884c\u5217\u5f0f\uff09\uff0c\u63d0\u51fa\u52a0\u6743\u4f18\u5316\u95ee\u9898\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u5206\u89e3\u975e\u51f8\u95ee\u9898\uff0c\u7ed3\u5408DC\u7f16\u7a0b\u548cPGD\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u63a7\u5236\u4e0e\u611f\u77e5\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u65e0\u4eba\u673a\u534f\u540c\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u548c\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21174", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21174", "abs": "https://arxiv.org/abs/2506.21174", "authors": ["Jongyeon Park", "Joonhee Lee", "Do-Hyeon Lim", "Hong Kook Kim", "Hyeongcheol Geum", "Jeong Eun Lim"], "title": "Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4", "comment": "DCASE 2025 challenge Task4, 5 pages", "summary": "This technical report presents submission systems for Task 4 of the DCASE\n2025 Challenge. This model incorporates additional audio features (spectral\nroll-off and chroma features) into the embedding feature extracted from the\nmel-spectral feature to im-prove the classification capabilities of an\naudio-tagging model in the spatial semantic segmentation of sound scenes (S5)\nsystem. This approach is motivated by the fact that mixed audio often contains\nsubtle cues that are difficult to capture with mel-spectrograms alone. Thus,\nthese additional features offer alterna-tive perspectives for the model.\nSecond, an agent-based label correction system is applied to the outputs\nprocessed by the S5 system. This system reduces false positives, improving the\nfinal class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.\nFinally, we refine the training dataset to enhance the classi-fication accuracy\nof low-performing classes by removing irrele-vant samples and incorporating\nexternal data. That is, audio mix-tures are generated from a limited number of\ndata points; thus, even a small number of out-of-class data points could\ndegrade model performance. The experiments demonstrate that the submit-ted\nsystems employing these approaches relatively improve CA-SDRi by up to 14.7%\ncompared to the baseline of DCASE 2025 Challenge Task 4.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86DCASE 2025\u6311\u6218\u8d5b\u4efb\u52a14\u7684\u63d0\u4ea4\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u989d\u5916\u7684\u97f3\u9891\u7279\u5f81\u548c\u6539\u8fdb\u6807\u7b7e\u6821\u6b63\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u6df7\u5408\u97f3\u9891\u4e2d\u7684\u7ec6\u5fae\u7ebf\u7d22\u96be\u4ee5\u4ec5\u901a\u8fc7\u6885\u5c14\u9891\u8c31\u6355\u6349\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u989d\u5916\u7279\u5f81\u4ee5\u63d0\u4f9b\u66f4\u591a\u89c6\u89d2\u3002", "method": "\u7ed3\u5408\u4e86\u9891\u8c31\u6eda\u964d\u548c\u8272\u5ea6\u7279\u5f81\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6807\u7b7e\u6821\u6b63\u7cfb\u7edf\uff0c\u540c\u65f6\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5c06CA-SDRi\u6307\u6807\u76f8\u5bf9\u63d0\u5347\u4e8614.7%\u3002", "conclusion": "\u901a\u8fc7\u591a\u7279\u5f81\u878d\u5408\u548c\u6570\u636e\u96c6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u5206\u7c7b\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21167", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21167", "abs": "https://arxiv.org/abs/2506.21167", "authors": ["Dylan Sechet", "Francesca Bugiotti", "Matthieu Kowalski", "Edouard d'H\u00e9rouville", "Filip Langiewicz"], "title": "A Hierarchical Deep Learning Approach for Minority Instrument Detection", "comment": "International Conference on Digital Audio Effects (DAFx)", "summary": "Identifying instrument activities within audio excerpts is vital in music\ninformation retrieval, with significant implications for music cataloging and\ndiscovery. Prior deep learning endeavors in musical instrument recognition have\npredominantly emphasized instrument classes with ample data availability.\nRecent studies have demonstrated the applicability of hierarchical\nclassification in detecting instrument activities in orchestral music, even\nwith limited fine-grained annotations at the instrument level. Based on the\nHornbostel-Sachs classification, such a hierarchical classification system is\nevaluated using the MedleyDB dataset, renowned for its diversity and richness\nconcerning various instruments and music genres. This work presents various\nstrategies to integrate hierarchical structures into models and tests a new\nclass of models for hierarchical music prediction. This study showcases more\nreliable coarse-level instrument detection by bridging the gap between detailed\ninstrument identification and group-level recognition, paving the way for\nfurther advancements in this domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4e2d\u901a\u8fc7\u5c42\u6b21\u5206\u7c7b\u65b9\u6cd5\u8bc6\u522b\u4e50\u5668\u6d3b\u52a8\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528Hornbostel-Sachs\u5206\u7c7b\u7cfb\u7edf\u63d0\u5347\u4e50\u5668\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4e2d\u4e50\u5668\u8bc6\u522b\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c42\u6b21\u5206\u7c7b\u65b9\u6cd5\u63d0\u5347\u8bc6\u522b\u6548\u679c\u3002", "method": "\u57fa\u4e8eHornbostel-Sachs\u5206\u7c7b\u7cfb\u7edf\uff0c\u63d0\u51fa\u5c42\u6b21\u5206\u7c7b\u7b56\u7565\uff0c\u5e76\u5728MedleyDB\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5c55\u793a\u4e86\u5c42\u6b21\u5206\u7c7b\u65b9\u6cd5\u5728\u7c97\u7c92\u5ea6\u4e50\u5668\u68c0\u6d4b\u4e0a\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u5c42\u6b21\u5206\u7c7b\u65b9\u6cd5\u5728\u4e50\u5668\u8bc6\u522b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2506.21043", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21043", "abs": "https://arxiv.org/abs/2506.21043", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "title": "Analysis of Null Related Beampattern Measures and Signal Quantization Effects for Linear Differential Microphone Arrays", "comment": "10 pages, 15 Figures, 3 Tables", "summary": "A differential microphone array (DMA) offers enhanced capabilities to obtain\nsharp nulls at the cost of relatively broad peaks in the beam power pattern.\nThis can be used for applications that require nullification or attenuation of\ninterfering sources. To the best of our knowledge, the existing literature\nlacks measures that directly assess the efficacy of nulls, and null-related\nmeasures have not been investigated in the context of differential microphone\narrays (DMAs). This paper offers new insights about the utility of DMAs by\nproposing measures that characterize the nulls in their beam power patterns. We\ninvestigate the performance of differential beamformers by presenting and\nevaluating null-related measures namely null depth (ND) and Null Width (NW) as\na function of depth level relative to the beam power pattern maxima. A study of\nsignal quantization effects due to data acquisition for 1st, 2nd and 3rd order\nlinear DMAs and for different beampatterns i.e. dipole, cardioid, hypercardioid\nand supercardioid is presented. An analytical expression for the quantized\nbeamformed output for any general $ N^{th} $ order DMA is formulated.\nSimulation results of the variation of ND with number of quantization bits and\nthe variation of NW as a function of depth are also presented and inferences\nare drawn. Lab experiments are conducted in a fully anechoic room to support\nthe simulation results. The measured beampattern exhibits a pronounced null\ndepth, confirming the effectiveness of the experimental setup.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5dee\u5206\u9ea6\u514b\u98ce\u9635\u5217\uff08DMA\uff09\u6ce2\u675f\u529f\u7387\u6a21\u5f0f\u4e2d\u96f6\u70b9\u7684\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u91cf\u5316\u6548\u5e94\u5bf9\u4e0d\u540c\u9636\u6570DMA\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u76f4\u63a5\u8bc4\u4f30\u96f6\u70b9\u6548\u80fd\u7684\u6307\u6807\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aDMA\u7684\u5e94\u7528\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u4e86\u96f6\u70b9\u6df1\u5ea6\uff08ND\uff09\u548c\u96f6\u70b9\u5bbd\u5ea6\uff08NW\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u7814\u7a76\u4e861\u81f33\u9636\u7ebf\u6027DMA\u5728\u4e0d\u540c\u6ce2\u675f\u6a21\u5f0f\u4e0b\u7684\u91cf\u5316\u6548\u5e94\uff0c\u5e76\u63a8\u5bfc\u4e86N\u9636DMA\u7684\u91cf\u5316\u6ce2\u675f\u8f93\u51fa\u8868\u8fbe\u5f0f\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ND\u548cNW\u7684\u6709\u6548\u6027\uff0c\u91cf\u5316\u6bd4\u7279\u6570\u548c\u6df1\u5ea6\u5bf9ND\u548cNW\u7684\u5f71\u54cd\u5f97\u5230\u4e86\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30DMA\u7684\u96f6\u70b9\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2506.21386", "categories": ["eess.AS", "cs.CL", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21386", "abs": "https://arxiv.org/abs/2506.21386", "authors": ["Ghazal Al-Shwayyat", "Omer Nezih Gerek"], "title": "Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings", "comment": null, "summary": "Arabic dialect recognition presents a significant challenge in speech\ntechnology due to the linguistic diversity of Arabic and the scarcity of large\nannotated datasets, particularly for underrepresented dialects. This research\ninvestigates hybrid modeling strategies that integrate classical signal\nprocessing techniques with deep learning architectures to address this problem\nin low-resource scenarios. Two hybrid models were developed and evaluated: (1)\nMel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural\nNetwork (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with\na Recurrent Neural Network (RNN). The models were trained on a dialect-filtered\nsubset of the Common Voice Arabic dataset, with dialect labels assigned based\non speaker metadata. Experimental results demonstrate that the MFCC + CNN\narchitecture achieved superior performance, with an accuracy of 91.2% and\nstrong precision, recall, and F1-scores, significantly outperforming the\nWavelet + RNN configuration, which achieved an accuracy of 66.5%. These\nfindings highlight the effectiveness of leveraging spectral features with\nconvolutional models for Arabic dialect recognition, especially when working\nwith limited labeled data. The study also identifies limitations related to\ndataset size, potential regional overlaps in labeling, and model optimization,\nproviding a roadmap for future research. Recommendations for further\nimprovement include the adoption of larger annotated corpora, integration of\nself-supervised learning techniques, and exploration of advanced neural\narchitectures such as Transformers. Overall, this research establishes a strong\nbaseline for future developments in Arabic dialect recognition within\nresource-constrained environments.", "AI": {"tldr": "\u7814\u7a76\u7ed3\u5408\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u8d44\u6e90\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\uff0cMFCC+CNN\u8868\u73b0\u4f18\u4e8eDWT+RNN\u3002", "motivation": "\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u56e0\u8bed\u8a00\u591a\u6837\u6027\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u800c\u5177\u6311\u6218\u6027\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e24\u79cd\u6df7\u5408\u6a21\u578b\uff1aMFCC+CNN\u548cDWT+RNN\uff0c\u4f7f\u7528Common Voice\u963f\u62c9\u4f2f\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "MFCC+CNN\u51c6\u786e\u7387\u8fbe91.2%\uff0c\u663e\u8457\u4f18\u4e8eDWT+RNN\u768466.5%\uff0c\u9a8c\u8bc1\u4e86\u5377\u79ef\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u963f\u62c9\u4f2f\u65b9\u8a00\u8bc6\u522b\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5efa\u8bae\u672a\u6765\u91c7\u7528\u66f4\u5927\u6807\u6ce8\u6570\u636e\u96c6\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f\u3002"}}
{"id": "2506.21269", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21269", "abs": "https://arxiv.org/abs/2506.21269", "authors": ["Pengfei Fan", "Yuli Zhang", "Xinheng Wang", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Shangbo Wang"], "title": "Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou", "comment": null, "summary": "This study presents and publicly releases the Suzhou Urban Road Acoustic\nDataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive\ndata-acquisition protocols and annotation guidelines to ensure transparency and\nreproducibility of the experimental workflow. To model the coupling between\nvehicular noise and driving speed, we propose a bimodal-feature-fusion deep\nconvolutional neural network (BMCNN). During preprocessing, an adaptive\ndenoising and normalization strategy is applied to suppress environmental\nbackground interference; in the network architecture, parallel branches extract\nMel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,\nwhich are subsequently fused via a cross-modal attention mechanism in the\nintermediate feature space to fully exploit time-frequency information.\nExperimental results demonstrate that BMCNN achieves a classification accuracy\nof 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic\ndataset. Ablation studies and robustness tests on the Suzhou dataset further\nvalidate the contributions of each module to performance improvement and\noverfitting mitigation. The proposed acoustics-based speed classification\nmethod can be integrated into smart-city traffic management systems for\nreal-time noise monitoring and speed estimation, thereby optimizing traffic\nflow control, reducing roadside noise pollution, and supporting sustainable\nurban planning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u516c\u5f00\u4e86\u82cf\u5dde\u57ce\u5e02\u9053\u8def\u58f0\u5b66\u6570\u636e\u96c6\uff08SZUR-Acoustic Dataset\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u6001\u7279\u5f81\u878d\u5408\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08BMCNN\uff09\u6765\u5efa\u6a21\u8f66\u8f86\u566a\u58f0\u4e0e\u884c\u9a76\u901f\u5ea6\u7684\u5173\u7cfb\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBMCNN\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u8d21\u732e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u58f0\u5b66\u6570\u636e\u5efa\u6a21\u8f66\u8f86\u566a\u58f0\u4e0e\u901f\u5ea6\u7684\u5173\u7cfb\uff0c\u4ee5\u652f\u6301\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\uff0c\u4f18\u5316\u4ea4\u901a\u6d41\u63a7\u5236\u5e76\u51cf\u5c11\u566a\u58f0\u6c61\u67d3\u3002", "method": "\u63d0\u51faBMCNN\u7f51\u7edc\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u53bb\u566a\u548c\u5f52\u4e00\u5316\u9884\u5904\u7406\uff0c\u5e76\u884c\u63d0\u53d6MFCC\u548c\u5c0f\u6ce2\u5305\u80fd\u91cf\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u3002", "result": "BMCNN\u5728SZUR-Acoustic\u6570\u636e\u96c6\u4e0a\u5206\u7c7b\u51c6\u786e\u7387\u8fbe87.56%\uff0c\u5728IDMT-Traffic\u6570\u636e\u96c6\u4e0a\u8fbe96.28%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u96c6\u6210\u5230\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\u4e2d\uff0c\u7528\u4e8e\u5b9e\u65f6\u566a\u58f0\u76d1\u6d4b\u548c\u901f\u5ea6\u4f30\u8ba1\uff0c\u652f\u6301\u53ef\u6301\u7eed\u57ce\u5e02\u89c4\u5212\u3002"}}
{"id": "2506.21112", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21112", "abs": "https://arxiv.org/abs/2506.21112", "authors": ["Yancheng Wang", "Wei Guo", "Guanying Chen", "Ye Zhang", "Shuguang Cui"], "title": "Point Cloud Environment-Based Channel Knowledge Map Construction", "comment": null, "summary": "Channel knowledge map (CKM) provides certain levels of channel state\ninformation (CSI) for an area of interest, serving as a critical enabler for\nenvironment-aware communications by reducing the overhead of frequent CSI\nacquisition. However, existing CKM construction schemes adopt over-simplified\nenvironment information, which significantly compromises their accuracy. To\naddress this issue, this work proposes a joint model- and data-driven approach\nto construct CKM by leveraging point cloud environmental data along with a few\nsamples of location-tagged channel information. First, we propose a novel point\nselector to identify subsets of point cloud that contain environmental\ninformation relevant to multipath channel gains, by constructing a set of\nco-focal ellipsoids based on different time of arrival (ToAs). Then, we trained\na neural channel gain estimator to learn the mapping between each selected\nsubset and its corresponding channel gain, using a real-world dataset we\ncollected through field measurements, comprising environmental point clouds and\ncorresponding channel data. Finally, experimental results demonstrate that: For\nCKM construction of power delay profile (PDP), the proposed method achieves a\nroot mean squared error (RMSE) of 2.95 dB, significantly lower than the 7.32 dB\nachieved by the conventional ray-tracing method; for CKM construction of\nreceived power values, i.e., radio map, it achieves an RMSE of 1.04 dB,\nsurpassing the Kriging interpolation method with an RMSE of 1.68 dB.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u70b9\u4e91\u73af\u5883\u6570\u636e\u548c\u5c11\u91cf\u4f4d\u7f6e\u6807\u8bb0\u7684\u4fe1\u9053\u4fe1\u606f\u6784\u5efa\u4fe1\u9053\u77e5\u8bc6\u56fe\uff08CKM\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709CKM\u6784\u5efa\u65b9\u6848\u56e0\u4f7f\u7528\u8fc7\u4e8e\u7b80\u5316\u7684\u73af\u5883\u4fe1\u606f\u800c\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u5ef6\u7684\u5171\u7126\u692d\u7403\u4f53\u70b9\u9009\u62e9\u5668\u7b5b\u9009\u76f8\u5173\u70b9\u4e91\u5b50\u96c6\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u4fe1\u9053\u589e\u76ca\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728PDP\u548c\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u4e2d\u5206\u522b\u5b9e\u73b0\u4e862.95 dB\u548c1.04 dB\u7684RMSE\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u70b9\u4e91\u6570\u636e\u548c\u5c11\u91cf\u4fe1\u9053\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86CKM\u6784\u5efa\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.21448", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.21448", "abs": "https://arxiv.org/abs/2506.21448", "authors": ["Huadai Liu", "Jialei Wang", "Kaicheng Luo", "Wen Wang", "Qian Chen", "Zhou Zhao", "Wei Xue"], "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing", "comment": null, "summary": "While end-to-end video-to-audio generation has greatly improved, producing\nhigh-fidelity audio that authentically captures the nuances of visual content\nremains challenging. Like professionals in the creative industries, such\ngeneration requires sophisticated reasoning about items such as visual\ndynamics, acoustic environments, and temporal relationships. We present\n\\textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT)\nreasoning to enable stepwise, interactive audio generation and editing for\nvideos. Our approach decomposes the process into three complementary stages:\nfoundational foley generation that creates semantically coherent soundscapes,\ninteractive object-centric refinement through precise user interactions, and\ntargeted editing guided by natural language instructions. At each stage, a\nmultimodal large language model generates contextually aligned CoT reasoning\nthat guides a unified audio foundation model. Furthermore, we introduce\n\\textbf{AudioCoT}, a comprehensive dataset with structured reasoning\nannotations that establishes connections between visual content, textual\ndescriptions, and sound synthesis. Experiments demonstrate that ThinkSound\nachieves state-of-the-art performance in video-to-audio generation across both\naudio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio\nbenchmark. The demo page is available at https://ThinkSound-Demo.github.io.", "AI": {"tldr": "ThinkSound\u662f\u4e00\u4e2a\u57fa\u4e8eChain-of-Thought\u63a8\u7406\u7684\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u751f\u6210\u548c\u7f16\u8f91\u97f3\u9891\uff0c\u7ed3\u5408\u7528\u6237\u4ea4\u4e92\u548c\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u97f3\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u89c6\u89c9\u5185\u5bb9\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "ThinkSound\u5c06\u97f3\u9891\u751f\u6210\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u57fa\u7840\u97f3\u6548\u751f\u6210\u3001\u4ea4\u4e92\u5f0f\u5bf9\u8c61\u4e2d\u5fc3\u7ec6\u5316\u3001\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u7684\u7f16\u8f91\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u63a8\u7406\u3002", "result": "ThinkSound\u5728\u97f3\u9891\u6307\u6807\u548cCoT\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728Movie Gen Audio\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ThinkSound\u901a\u8fc7\u5206\u9636\u6bb5\u63a8\u7406\u548c\u7528\u6237\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.21298", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21298", "abs": "https://arxiv.org/abs/2506.21298", "authors": ["Atharva Mehta", "Shivam Chauhan", "Monojit Choudhury"], "title": "Exploring Adapter Design Tradeoffs for Low Resource Music Generation", "comment": "9 pages, 5 figures", "summary": "Fine-tuning large-scale music generation models, such as MusicGen and\nMustango, is a computationally expensive process, often requiring updates to\nbillions of parameters and, therefore, significant hardware resources.\nParameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based\nmethods, have emerged as a promising alternative, enabling adaptation with\nminimal trainable parameters while preserving model performance. However, the\ndesign choices for adapters, including their architecture, placement, and size,\nare numerous, and it is unclear which of these combinations would produce\noptimal adapters and why, for a given case of low-resource music genre. In this\npaper, we attempt to answer this question by studying various adapter\nconfigurations for two AI music models, MusicGen and Mustango, on two genres:\nHindustani Classical and Turkish Makam music.\n  Our findings reveal distinct trade-offs: convolution-based adapters excel in\ncapturing fine-grained local musical details such as ornamentations and short\nmelodic phrases, while transformer-based adapters better preserve long-range\ndependencies crucial for structured improvisation. Additionally, we analyze\ncomputational resource requirements across different adapter scales,\ndemonstrating how mid-sized adapters (40M parameters) achieve an optimal\nbalance between expressivity and quality. Furthermore, we find that Mustango, a\ndiffusion-based model, generates more diverse outputs with better adherence to\nthe description in the input prompt while lacking in providing stability in\nnotes, rhythm alignment, and aesthetics. Also, it is computationally intensive\nand requires significantly more time to train. In contrast, autoregressive\nmodels like MusicGen offer faster training and are more efficient, and can\nproduce better quality output in comparison, but have slightly higher\nredundancy in their generations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4f4e\u8d44\u6e90\u97f3\u4e50\u7c7b\u578b\uff08\u5982\u5370\u5ea6\u65af\u5766\u53e4\u5178\u548c\u571f\u8033\u5176Makam\u97f3\u4e50\uff09\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u4f18\u5316MusicGen\u548cMustango\u6a21\u578b\u7684\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u53d1\u73b0\u5377\u79ef\u548cTransformer\u9002\u914d\u5668\u5404\u6709\u4f18\u52bf\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u8d44\u6e90\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u9002\u914d\u5668\u8bbe\u8ba1\uff08\u67b6\u6784\u3001\u4f4d\u7f6e\u548c\u5927\u5c0f\uff09\u5bf9\u4f4e\u8d44\u6e90\u97f3\u4e50\u7c7b\u578b\u751f\u6210\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u97f3\u4e50\u751f\u6210\u6a21\u578b\u5fae\u8c03\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u9002\u914d\u5668\u914d\u7f6e\uff08\u5377\u79ef\u548cTransformer\uff09\u5728MusicGen\u548cMustango\u6a21\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5728\u5370\u5ea6\u65af\u5766\u53e4\u5178\u548c\u571f\u8033\u5176Makam\u97f3\u4e50\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5377\u79ef\u9002\u914d\u5668\u64c5\u957f\u6355\u6349\u5c40\u90e8\u97f3\u4e50\u7ec6\u8282\uff0cTransformer\u9002\u914d\u5668\u66f4\u9002\u5408\u957f\u7a0b\u4f9d\u8d56\uff1bMustango\u751f\u6210\u591a\u6837\u6027\u9ad8\u4f46\u7a33\u5b9a\u6027\u5dee\uff0cMusicGen\u8bad\u7ec3\u66f4\u5feb\u4e14\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u9002\u914d\u5668\u8bbe\u8ba1\u9700\u6839\u636e\u4efb\u52a1\u9700\u6c42\u9009\u62e9\uff0c\u4e2d\u89c4\u6a21\u9002\u914d\u5668\uff0840M\u53c2\u6570\uff09\u5728\u8868\u8fbe\u529b\u548c\u8d28\u91cf\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0cMustango\u548cMusicGen\u5404\u6709\u4f18\u52a3\u3002"}}
{"id": "2506.21123", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21123", "abs": "https://arxiv.org/abs/2506.21123", "authors": ["Hao Wu", "Chongwu Xie", "Xinyuan Yao", "Kang-Da Wu", "Shanchi Wu", "Rui Ni", "Guo-Yong Xiang", "Chen Gong"], "title": "Characterization of Rydberg-Atom Signal Reception of Dual-Frequency Signals Coupled with Two Energy Levels", "comment": null, "summary": "Rydberg atomic sensors have been adopted for novel radio frequency (RF)\nmeasurement technique and the sensing capability for signals in multiple\nfrequencies makes it attractive for multi-user communication. However, unlike\ntraditional antennas where the signals in multiple frequencies are orthogonal,\nthe received signals of atomic sensors corresponding to different energy levels\nwill be downconverted to the baseband simultaneously, resulting in multi-user\ninterference. Thus, in this paper, we analyze the mutual interference\ncharacteristics of two RF signals with different carrier frequencies coupling\ndifferent energy levels. We introduce the joint response coefficient based on\nthe receiver characteristics and analyze the interference of one user to\nanother. We analyze the bit-error rate (BER) and symbol-error rate (SER) for\ntwo signals coupling two different energy levels. We also conduct experiments\nto validate the BER and SER results.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u91cc\u5fb7\u5821\u539f\u5b50\u7684\u591a\u9891\u4fe1\u53f7\u4f20\u611f\u5668\u4e2d\u7684\u591a\u7528\u6237\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8054\u5408\u54cd\u5e94\u7cfb\u6570\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bef\u7801\u7387\u548c\u7b26\u53f7\u9519\u8bef\u7387\u3002", "motivation": "\u91cc\u5fb7\u5821\u539f\u5b50\u4f20\u611f\u5668\u5728\u591a\u9891\u4fe1\u53f7\u6d4b\u91cf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u591a\u7528\u6237\u5e72\u6270\u95ee\u9898\u9650\u5236\u4e86\u5176\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5206\u6790\u5e72\u6270\u7279\u6027\u3002", "method": "\u901a\u8fc7\u8054\u5408\u54cd\u5e94\u7cfb\u6570\u5206\u6790\u4e24\u4e2a\u4e0d\u540c\u8f7d\u9891\u4fe1\u53f7\u7684\u76f8\u4e92\u5e72\u6270\uff0c\u5e76\u8ba1\u7b97\u8bef\u7801\u7387\u548c\u7b26\u53f7\u9519\u8bef\u7387\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5e72\u6270\u5bf9\u8bef\u7801\u7387\u548c\u7b26\u53f7\u9519\u8bef\u7387\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u91cc\u5fb7\u5821\u539f\u5b50\u4f20\u611f\u5668\u5728\u591a\u7528\u6237\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5e72\u6270\u5206\u6790\u57fa\u7840\u3002"}}
{"id": "2506.21440", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21440", "abs": "https://arxiv.org/abs/2506.21440", "authors": ["Maxime Leiber", "Yosra Marnissi", "Axel Barrau", "Sylvain Meignen", "Laurent Massouli\u00e9"], "title": "Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform", "comment": "DSTFT, STFT, spectrogram, time-frequency, IEEE Transactions on Signal\n  Processing, 10 pages", "summary": "The short-time Fourier transform (STFT) is widely used for analyzing\nnon-stationary signals. However, its performance is highly sensitive to its\nparameters, and manual or heuristic tuning often yields suboptimal results. To\novercome this limitation, we propose a unified differentiable formulation of\nthe STFT that enables gradient-based optimization of its parameters. This\napproach addresses the limitations of traditional STFT parameter tuning\nmethods, which often rely on computationally intensive discrete searches. It\nenables fine-tuning of the time-frequency representation (TFR) based on any\ndesired criterion. Moreover, our approach integrates seamlessly with neural\nnetworks, allowing joint optimization of the STFT parameters and network\nweights. The efficacy of the proposed differentiable STFT in enhancing TFRs and\nimproving performance in downstream tasks is demonstrated through experiments\non both simulated and real-world data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u79bb\u6563\u641c\u7d22\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u65f6\u9891\u8868\u793a\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfSTFT\u53c2\u6570\u8c03\u6574\u4f9d\u8d56\u624b\u52a8\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u6548\u679c\u4e0d\u4f73\u4e14\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u53ef\u5fae\u5206STFT\u6846\u67b6\uff0c\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u7684\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u65f6\u9891\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u6539\u5584\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u53ef\u5fae\u5206STFT\u4e3a\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u4f18\u5316\u9014\u5f84\u3002"}}
{"id": "2506.21208", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21208", "abs": "https://arxiv.org/abs/2506.21208", "authors": ["Shengjie Liu", "Chenyang Yang"], "title": "Adversarial Training: Enhancing Out-of-Distribution Generalization for Learning Wireless Resource Allocation", "comment": null, "summary": "Deep neural networks (DNNs) have widespread applications for optimizing\nresource allocation. Yet, their performance is vulnerable to distribution\nshifts between training and test data, say channels. In this letter, we resort\nto adversarial training (AT) for enhancing out-of-distribution (OOD)\ngeneralizability of DNNs trained in unsupervised manner. We reformulate AT to\ncapture the OOD degradation, and propose a one-step gradient ascent method for\nAT. The proposed method is validated by optimizing hybrid precoding. Simulation\nresults showcase the enhanced OOD performance of multiple kinds of DNNs across\nvarious channel distributions, when only Rayleigh fading channels are used for\ntraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\uff08AT\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u65e0\u76d1\u7763\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u5206\u5e03\u504f\u79fb\uff08OOD\uff09\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6df7\u5408\u9884\u7f16\u7801\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u5206\u914d\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u504f\u79fb\uff08\u5982\u4fe1\u9053\u53d8\u5316\uff09\u7684\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u5176OOD\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u5bf9\u6297\u8bad\u7ec3\u4ee5\u6355\u6349OOD\u6027\u80fd\u9000\u5316\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4e00\u6b65\u68af\u5ea6\u4e0a\u5347\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u745e\u5229\u8870\u843d\u4fe1\u9053\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u591a\u79cdDNN\u5728\u4e0d\u540c\u4fe1\u9053\u5206\u5e03\u4e0b\u7684OOD\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u6539\u8fdb\u7684DNN\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u5206\u914d\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21478", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21478", "abs": "https://arxiv.org/abs/2506.21478", "authors": ["Kehan Sui", "Jinxu Xiang", "Fang Jin"], "title": "SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture", "comment": null, "summary": "Singing voice synthesis (SVS) aims to generate expressive and high-quality\nvocals from musical scores, requiring precise modeling of pitch, duration, and\narticulation. While diffusion-based models have achieved remarkable success in\nimage and video generation, their application to SVS remains challenging due to\nthe complex acoustic and musical characteristics of singing, often resulting in\nartifacts that degrade naturalness. In this work, we propose SmoothSinger, a\nconditional diffusion model designed to synthesize high quality and natural\nsinging voices. Unlike prior methods that depend on vocoders as a final stage\nand often introduce distortion, SmoothSinger refines low-quality synthesized\naudio directly in a unified framework, mitigating the degradation associated\nwith two-stage pipelines. The model adopts a reference-guided dual-branch\narchitecture, using low-quality audio from any baseline system as a reference\nto guide the denoising process, enabling more expressive and context-aware\nsynthesis. Furthermore, it enhances the conventional U-Net with a parallel\nlow-frequency upsampling path, allowing the model to better capture pitch\ncontours and long term spectral dependencies. To improve alignment during\ntraining, we replace reference audio with degraded ground truth audio,\naddressing temporal mismatch between reference and target signals. Experiments\non the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that\nSmoothSinger achieves state-of-the-art results in both objective and subjective\nevaluations. Extensive ablation studies confirm its effectiveness in reducing\nartifacts and improving the naturalness of synthesized voices.", "AI": {"tldr": "SmoothSinger\u662f\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u6b4c\u5531\u8bed\u97f3\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u76f4\u63a5\u4f18\u5316\u4f4e\u8d28\u91cf\u97f3\u9891\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u6d41\u7a0b\u7684\u5931\u771f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u548c\u8868\u73b0\u529b\u3002", "motivation": "\u6b4c\u5531\u8bed\u97f3\u5408\u6210\uff08SVS\uff09\u9700\u8981\u7cbe\u786e\u5efa\u6a21\u97f3\u9ad8\u3001\u65f6\u957f\u548c\u53d1\u97f3\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728SVS\u4e2d\u56e0\u590d\u6742\u58f0\u5b66\u548c\u97f3\u4e50\u7279\u6027\u5bfc\u81f4\u81ea\u7136\u5ea6\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u53c2\u8003\u5f15\u5bfc\u7684\u53cc\u5206\u652f\u67b6\u6784\uff0c\u901a\u8fc7\u4f4e\u8d28\u91cf\u97f3\u9891\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u4f4e\u9891\u4e0a\u91c7\u6837\u8def\u5f84\u4ee5\u6355\u6349\u97f3\u9ad8\u8f6e\u5ed3\u548c\u957f\u671f\u9891\u8c31\u4f9d\u8d56\u3002", "result": "\u5728Opencpop\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSmoothSinger\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4f2a\u5f71\u5e76\u63d0\u5347\u4e86\u81ea\u7136\u5ea6\u3002", "conclusion": "SmoothSinger\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u4f18\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b4c\u5531\u8bed\u97f3\u5408\u6210\u7684\u8d28\u91cf\u548c\u81ea\u7136\u5ea6\uff0c\u4e3aSVS\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21325", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21325", "abs": "https://arxiv.org/abs/2506.21325", "authors": ["Nima Mozaffarikhosravi", "Prathapasinghe Dharmawansa", "Italo Atzeni"], "title": "Localization-Based Beam Focusing in Near-Field Communications", "comment": null, "summary": "Shifting 6G-and-beyond wireless communication systems to higher frequency\nbands and the utilization of massive multiple-input multiple-output arrays will\nextend the near-field region, affecting beamforming and user localization\nschemes. In this paper, we propose a localization-based beam-focusing strategy\nthat leverages the dominant line-of-sight (LoS) propagation arising at mmWave\nand sub-THz frequencies. To support this approach, we analyze the 2D-MUSIC\nalgorithm for distance estimation by examining its spectrum in simplified,\ntractable setups with minimal numbers of antennas and users. Lastly, we compare\nthe proposed localization-based beam focusing, with locations estimated via\n2D-MUSIC, with zero forcing with pilot-based channel estimation in terms of\nuplink sum spectral efficiency. Our numerical results show that the proposed\nmethod becomes more effective under LoS-dominated propagation, short coherence\nblocks, and strong noise power arising at high carrier frequencies and with\nlarge bandwidths.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9a\u4f4d\u7684\u6ce2\u675f\u805a\u7126\u7b56\u7565\uff0c\u5229\u7528\u6beb\u7c73\u6ce2\u548c\u4e9a\u592a\u8d6b\u5179\u9891\u6bb5\u7684\u89c6\u8ddd\u4f20\u64ad\u4f18\u52bf\uff0c\u5e76\u901a\u8fc72D-MUSIC\u7b97\u6cd5\u8fdb\u884c\u8ddd\u79bb\u4f30\u8ba1\uff0c\u4e0e\u4f20\u7edf\u7684\u96f6\u5f3a\u8feb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u968f\u77406G\u53ca\u4ee5\u4e0a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5411\u66f4\u9ad8\u9891\u6bb5\u53d1\u5c55\u548c\u5927\u89c4\u6a21MIMO\u9635\u5217\u7684\u5e94\u7528\uff0c\u8fd1\u573a\u533a\u57df\u6269\u5c55\u4f1a\u5f71\u54cd\u6ce2\u675f\u6210\u5f62\u548c\u7528\u6237\u5b9a\u4f4d\u65b9\u6848\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5b9a\u4f4d\u7684\u6ce2\u675f\u805a\u7126\u7b56\u7565\uff0c\u5229\u7528\u89c6\u8ddd\u4f20\u64ad\u7279\u6027\uff0c\u5e76\u901a\u8fc72D-MUSIC\u7b97\u6cd5\u5206\u6790\u8ddd\u79bb\u4f30\u8ba1\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u89c6\u8ddd\u4f20\u64ad\u4e3b\u5bfc\u3001\u77ed\u76f8\u5e72\u5757\u548c\u9ad8\u566a\u58f0\u529f\u7387\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u6bd4\u4f20\u7edf\u96f6\u5f3a\u8feb\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "\u57fa\u4e8e\u5b9a\u4f4d\u7684\u6ce2\u675f\u805a\u7126\u7b56\u7565\u5728\u9ad8\u9891\u6bb5\u548c\u5927\u5e26\u5bbd\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u89c6\u8ddd\u4f20\u64ad\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2506.21375", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21375", "abs": "https://arxiv.org/abs/2506.21375", "authors": ["Ying Gao", "Qingqing Wu", "Weidong Mei", "Guangji Chen", "Wen Chen", "Ziyuan Zheng"], "title": "Integrating Movable Antennas and Intelligent Reflecting Surfaces for Coverage Enhancement", "comment": "13 pages, 8 figures, submitted to an IEEE journal for possible\n  publication on on May 8, 2025", "summary": "This paper investigates an intelligent reflecting surface (IRS)-aided movable\nantenna (MA) system, where multiple IRSs cooperate with a multi-MA base station\nto extend wireless coverage to multiple designated target areas. The objective\nis to maximize the worst-case signal-to-noise ratio (SNR) across all locations\nwithin these areas through joint optimization of MA positions, IRS reflection\ncoefficients, and transmit beamforming. To achieve this while balancing the\nperformance-cost trade-off, we propose three coverage-enhancement schemes: the\narea-adaptive MA-IRS scheme, the area-adaptive MA-staIRS scheme, and the shared\nMA-staIRS scheme, where staIRS denotes static IRSs with reflection coefficients\nconfigured only once during installation. These schemes lead to challenging\nnon-convex optimization problems with implicit objective functions, which are\ndifficult to solve optimally. To address these problems, we propose a general\nalgorithmic framework that can be applied to solve each problem efficiently\nalbeit suboptimally. Simulation results demonstrate that: 1) the proposed\nMA-based schemes consistently outperform their fixed-position antenna\n(FPA)-based counterparts under both area-adaptive and static IRS\nconfigurations, with the area-adaptive MA-IRS scheme achieving the best\nworst-case SNR performance; 2) as transmit antennas are typically far fewer\nthan IRS elements, the area-adaptive MA-staIRS scheme may underperform the\nbaseline FPA scheme with area-adaptive IRSs in terms of the worst-case SNR, but\na modest increase in antenna number can reverse this trend; 3) under a fixed\ntotal cost, the optimal MA-to-IRS-element ratio for the worst-case SNR\nmaximization is empirically found to be proportional to the reciprocal of their\nunit cost ratio.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86IRS\u8f85\u52a9\u7684MA\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316MA\u4f4d\u7f6e\u3001IRS\u53cd\u5c04\u7cfb\u6570\u548c\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\uff0c\u63d0\u5347\u591a\u76ee\u6807\u533a\u57df\u7684\u65e0\u7ebf\u8986\u76d6\u3002\u63d0\u51fa\u4e86\u4e09\u79cd\u8986\u76d6\u589e\u5f3a\u65b9\u6848\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7b97\u6cd5\u6846\u67b6\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cMA\u65b9\u6848\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\u65b9\u6848\uff0c\u4e14MA\u4e0eIRS\u5143\u7d20\u7684\u6700\u4f18\u6bd4\u4f8b\u4e0e\u6210\u672c\u6bd4\u6210\u53cd\u6bd4\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u8986\u76d6\u6269\u5c55\u95ee\u9898\uff0c\u901a\u8fc7IRS\u548cMA\u7684\u534f\u540c\u4f18\u5316\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\uff0c\u540c\u65f6\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u8986\u76d6\u589e\u5f3a\u65b9\u6848\uff08area-adaptive MA-IRS\u3001area-adaptive MA-staIRS\u3001shared MA-staIRS\uff09\uff0c\u5e76\u5f00\u53d1\u901a\u7528\u7b97\u6cd5\u6846\u67b6\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "MA\u65b9\u6848\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\u65b9\u6848\uff0carea-adaptive MA-IRS\u6027\u80fd\u6700\u4f73\uff1bMA\u4e0eIRS\u5143\u7d20\u6bd4\u4f8b\u4e0e\u6210\u672c\u6bd4\u6210\u53cd\u6bd4\u3002", "conclusion": "IRS\u8f85\u52a9\u7684MA\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u65e0\u7ebf\u8986\u76d6\uff0cMA\u65b9\u6848\u5728\u6027\u80fd\u4e0e\u6210\u672c\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002"}}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Precise Near-Field Beam Training with DFT Codebook based on Amplitude-only Measurement](https://arxiv.org/abs/2506.20783)
*Zijun Wang,Shawn Tsai,Rama Kiran,Rui Zhang*

Main category: eess.SP

TL;DR: 提出了一种低复杂度的近场波束训练方案，利用远场用户的DFT码本，通过分析近场波束模式，定义角度依赖的修正瑞利距离，并开发了O(1)复杂度的用户距离估计方法，仿真显示SNR提升达2.38 dB。


<details>
  <summary>Details</summary>
Motivation: 极大规模天线阵列（ELAAs）在高频段的应用推动了近场通信的发展，需要高效的波束训练和信号处理设计。

Method: 分析近场波束模式，推导波束宽度和中心增益的闭式解，定义修正瑞利距离，开发低复杂度用户距离估计方法，并提出基于MLE的细化方法。

Result: 仿真结果显示单用户和多用户场景下SNR提升达2.38 dB，且可达速率接近理想信道状态信息下的结果。

Conclusion: 提出的低复杂度方案在近场通信中表现优异，接近理想性能，适用于实际应用。

Abstract: Extremely large antenna arrays (ELAAs) operating in high-frequency bands have
spurred the development of near-field communication, driving advancements in
beam training and signal processing design. In this work, we present a
low-complexity near-field beam training scheme that fully utilizes the
conventional discrete Fourier transform (DFT) codebook designed for far-field
users. We begin by analyzing the received beam pattern in the near field and
derive closed-form expressions for the beam width and central gain. These
analytical results enable the definition of an angle-dependent, modified
Rayleigh distance, which effectively distinguishes near-field and far-field
user regimes. Building on the analysis, we develop a direct and computationally
efficient method to estimate user distance, with a complexity of O(1), and
further improve its accuracy through a simple refinement. Simulation results
demonstrate significant gains in both single- and multi-user settings, with up
to 2.38 dB SNR improvement over exhaustive search. To further enhance
estimation accuracy, we additionally propose a maximum likelihood estimation
(MLE) based refinement method, leveraging the Rician distribution of signal
amplitudes and achieving accuracy close to the Cramer--Rao bound (CRB).
Simulation shows the single-user and multi-user achievable rates can both
approach those obtained with ideal channel state information.

</details>


### [2] [Physical Limits of Entanglement-Based Quantum Key Distribution over Long-Distance Satellite Links](https://arxiv.org/abs/2506.20798)
*Mohammad Taghi Dabiri,Mazen Hasna,Saif Al-Kuwari,Khalid Qaraqe*

Main category: eess.SP

TL;DR: 该论文分析了基于纠缠态的卫星间量子密钥分发（QKD）协议在实际自由空间光（FSO）信道中的性能，重点研究了光子级建模和实际损伤的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的文献未充分解决长距离卫星间FSO信道中光子损耗、指向误差和背景噪声对QKD性能的严重影响。

Method: 论文开发了信号检测概率、背景光子影响、多对光子发射和QBER的解析表达式，结合了链路距离、发射机跟踪抖动、接收机对准误差等关键参数。

Result: 仿真结果表明系统性能对跟踪误差和视场限制具有非线性敏感性，并提出了在保持QBER可接受的同时最大化密钥生成率的最优参数范围。

Conclusion: 该模型为可靠高效部署基于纠缠态的卫星QKD系统提供了实用的设计指导。

Abstract: Entanglement-based quantum key distribution (QKD) protocols, such as E91 and
BBM92, offer strong information-theoretic security and are naturally suited for
satellite-to-satellite QKD (SatQKD) links. However, implementing these
protocols over long-distance inter-satellite free-space optical (FSO) channels
poses critical physical-layer challenges that are not addressed in the existing
literature. In particular, photon losses due to beam divergence, pointing
errors, and background noise can severely degrade the key generation rate and
quantum bit error rate (QBER), especially under narrow receiver field-of-view
(FoV) constraints. This paper presents a comprehensive performance analysis of
entanglement-based inter-satellite QKD, focusing on photon-level modeling and
the impact of practical impairments. We develop analytical expressions for
signal detection probabilities, background photon influence, multi-pair
emissions, and QBER, incorporating key parameters such as link distance,
transmitter tracking jitter, receiver misalignment, and photon pair generation
rate. Simulation results reveal the nonlinear sensitivity of system performance
to tracking error and FoV limitations, and highlight optimal parameter regimes
that jointly maximize secret key rate while maintaining QBER below acceptable
thresholds. The proposed model provides actionable design insights for reliable
and efficient deployment of entanglement-based SatQKD systems.

</details>


### [3] [Compact Analytical Model for Real-Time Evaluation of OAM-Based Inter-Satellite Links](https://arxiv.org/abs/2506.20823)
*Mohammad Taghi Dabiri,Mazen Hasna*

Main category: eess.SP

TL;DR: 提出了一种高效的解析框架，用于评估轨道角动量（OAM）光束在指向误差下的卫星间通信系统性能，显著减少了计算时间并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法计算量大，难以满足动态低地球轨道（LEO）卫星网络对实时链路适配的需求。

Method: 开发了准确的解析模型以描述OAM光束对准误差引起的模态间串扰，并推导了高效的表达式以分析和优化误码率（BER）。

Result: 提出的方法在计算时间上大幅减少，同时保持高精度，且非对称OAM模式集在指向误差下表现优于对称配置。

Conclusion: 该框架为高移动性光无线系统（如LEO卫星网络）提供了实时优化的高保真性能预测能力。

Abstract: This paper presents an efficient analytical framework for evaluating the
performance of inter-satellite communication systems utilizing orbital angular
momentum (OAM) beams under pointing errors. An accurate analytical model is
first developed to characterize intermodal crosstalk caused by beam
misalignment in OAM-based inter-satellite links. Building upon this model, we
derive efficient expressions to analyze and optimize system performance in
terms of bit error rate (BER). Unlike traditional Monte Carlo-based methods
that are computationally intensive, the proposed approach offers accurate
performance predictions. This enables a substantial decrease in computation
time while maintaining high accuracy, thanks to the use of analytical
expressions for both crosstalk and BER. This fast and accurate evaluation
capability is particularly critical for dynamic low Earth orbit (LEO) satellite
constellations, where network topology and channel conditions change rapidly,
requiring real-time link adaptation. Furthermore, we systematically design and
evaluate asymmetric OAM mode sets, which significantly outperform symmetric
configurations in the presence of pointing errors. Our results also reveal key
insights into the interaction between beam divergence, tracking accuracy, and
link distance, demonstrating that the proposed framework enables real-time
optimization of system parameters with high fidelity. The analytical findings
are rigorously validated against extensive Monte Carlo simulations, confirming
their practical applicability for high-mobility optical wireless systems such
as LEO satellite networks.

</details>


### [4] [Doppler Estimation and Compensation Techniques in LoRa Direct-to-Satellite Communications](https://arxiv.org/abs/2506.20858)
*Jamil Farhat,Gianni Pasolini,Enrico Paolini,Muhammad Asad Ullah,Richard Demo Souza*

Main category: eess.SP

TL;DR: 论文提出四种框架用于LoRa DtS连接中的多普勒效应估计与补偿，并与理想无多普勒场景进行性能对比，分析了关键参数间的权衡。


<details>
  <summary>Details</summary>
Motivation: LEO卫星的多普勒效应显著影响LoRa DtS性能，需探索补偿方法以提升连接稳定性。

Method: 提出四种多普勒估计与补偿框架，通过数值模拟比较性能，并分析扩频因子等参数的影响。

Result: 结果显示不同框架的性能差异，为优化LoRa DtS配置提供了依据。

Conclusion: 研究为LoRa DtS连接中的多普勒效应问题提供了解决方案，并指导了参数配置的优化。

Abstract: Within the LPWAN framework, the LoRa modulation adopted by LoRaWAN technology
has garnered significant interest as a connectivity solution for IoT
applications due to its ability to offer low-cost, low-power, and long-range
communications. One emerging use case of LoRa is DtS connectivity, which
extends coverage to remote areas for supporting IoT operations. The satellite
IoT industry mainly prefers LEO because it has lower launch costs and less path
loss compared to Geostationary orbit. However, a major drawback of LEO
satellites is the impact of the Doppler effect caused by their mobility.
Earlier studies have confirmed that the Doppler effect significantly degrades
the LoRa DtS performance. In this paper, we propose four frameworks for Doppler
estimation and compensation in LoRa DtS connectivity and numerically compare
the performance against the ideal scenario without the Doppler effect.
Furthermore, we investigate the trade-offs among these frameworks by analyzing
the interplay between spreading factor, and other key parameters related to the
Doppler effect. The results provide insights into how to achieve robust LoRa
configurations for DtS connectivity.

</details>


### [5] [Quantum-Accelerated Wireless Communications: Concepts, Connections, and Implications](https://arxiv.org/abs/2506.20863)
*Naoki Ishikawa,Giuseppe Thadeu Freitas de Abreu,Petar Popovski,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: 本文探讨了量子计算在通信系统中的潜力，强调了其在特定问题上的优势，并提出了量子与无线系统之间的数学联系。


<details>
  <summary>Details</summary>
Motivation: 激发无线研究人员对量子计算的兴趣，推动量子信息处理与未来通信系统的跨学科研究。

Method: 通过系统回顾前沿研究，总结量子加速通信系统的设计趋势，并分析经典启发式方法对量子参数的优化作用。

Result: 揭示了量子与无线系统之间的数学和谐，展示了经典与量子计算的互补优势。

Conclusion: 量子计算有望重塑通信系统的算法基础，但需进一步探索其工程应用潜力。

Abstract: Quantum computing is poised to redefine the algorithmic foundations of
communication systems. While quantum superposition and entanglement enable
quadratic or exponential speedups for specific problems, identifying use cases
where these advantages yield engineering benefits is, however, still
nontrivial. This article presents the fundamentals of quantum computing in a
style familiar to the communications society, outlining the current limits of
fault-tolerant quantum computing and uncovering a mathematical harmony between
quantum and wireless systems, which makes the topic more enticing to wireless
researchers. Based on a systematic review of pioneering and state-of-the-art
studies, we distill common design trends for the research and development of
quantum-accelerated communication systems and highlight lessons learned. The
key insight is that classical heuristics can sharpen certain quantum
parameters, underscoring the complementary strengths of classical and quantum
computing. This article aims to catalyze interdisciplinary research at the
frontier of quantum information processing and future communication systems.

</details>


### [6] [Co-Design of Sensing, Communications, and Control for Low-Altitude Wireless Networks](https://arxiv.org/abs/2506.20970)
*Haijia Jin,Jun Wu,Weijie Yuan,Fan Liu,Yuanhao Cui*

Main category: eess.SP

TL;DR: 该论文研究了多无人机协同系统中集成感知、通信与控制的联合设计，提出了一种基于交替优化的高效近优解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和6G的发展，无人机在低空无线网络中扮演关键角色，需要解决感知、通信与控制的联合优化问题。

Method: 通过交替优化方法分解非凸问题，结合DC规划和PGD方法求解，并分析算法收敛性和计算复杂度。

Result: 仿真结果表明，所提方法在控制和感知性能之间取得了有效权衡，优于基准方案。

Conclusion: 论文提出的联合优化方法为多无人机系统的感知与控制协同设计提供了高效解决方案。

Abstract: The rapid advancement of Internet of Things (IoT) services and the evolution
toward the sixth generation (6G) have positioned unmanned aerial vehicles
(UAVs) as critical enablers of low-altitude wireless networks (LAWNs). This
work investigates the co-design of integrated sensing, communication, and
control ($\mathbf{SC^{2}}$) for multi-UAV cooperative systems with finite
blocklength (FBL) transmission. In particular, the UAVs continuously monitor
the state of the field robots and transmit their observations to the robot
controller to ensure stable control while cooperating to localize an unknown
sensing target (ST). To this end, a weighted optimization problem is first
formulated by jointly considering the control and localization performance in
terms of the linear quadratic regulator (LQR) cost and the determinant of the
Fisher information matrix (FIM), respectively. The resultant problem,
optimizing resource allocations, the UAVs' deployment positions, and multi-user
scheduling, is non-convex. To circumvent this challenge, we first derive a
closed-form expression of the LQR cost with respect to other variables.
Subsequently, the non-convex optimization problem is decomposed into a series
of sub-problems by leveraging the alternating optimization (AO) approach, in
which the difference of convex functions (DC) programming and projected
gradient descent (PGD) method are employed to obtain an efficient near-optimal
solution. Furthermore, the convergence and computational complexity of the
proposed algorithm are thoroughly analyzed. Extensive simulation results are
presented to validate the effectiveness of our proposed approach compared to
the benchmark schemes and reveal the trade-off between control and sensing
performance.

</details>


### [7] [Analysis of Null Related Beampattern Measures and Signal Quantization Effects for Linear Differential Microphone Arrays](https://arxiv.org/abs/2506.21043)
*Shweta Pal,Arun Kumar,Monika Agrawal*

Main category: eess.SP

TL;DR: 本文提出了一种评估差分麦克风阵列（DMA）性能的新方法，重点关注波束功率模式中的零陷深度（ND）和零陷宽度（NW），并通过仿真和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏直接评估零陷效能的指标，本文旨在填补这一空白，为DMA的应用提供更精确的性能评估工具。

Method: 提出ND和NW作为评估指标，分析不同阶数和波束模式的DMA性能，并通过仿真和实验室实验验证。

Result: 仿真和实验结果均表明，所提出的指标能有效评估DMA的零陷性能，实验数据与仿真结果一致。

Conclusion: 本文提出的ND和NW指标为DMA的零陷性能提供了量化评估方法，实验验证了其有效性，为相关应用提供了理论支持。

Abstract: A differential microphone array (DMA) offers enhanced capabilities to obtain
sharp nulls at the cost of relatively broad peaks in the beam power pattern.
This can be used for applications that require nullification or attenuation of
interfering sources. To the best of our knowledge, the existing literature
lacks measures that directly assess the efficacy of nulls, and null-related
measures have not been investigated in the context of differential microphone
arrays (DMAs). This paper offers new insights about the utility of DMAs by
proposing measures that characterize the nulls in their beam power patterns. We
investigate the performance of differential beamformers by presenting and
evaluating null-related measures namely null depth (ND) and Null Width (NW) as
a function of depth level relative to the beam power pattern maxima. A study of
signal quantization effects due to data acquisition for 1st, 2nd and 3rd order
linear DMAs and for different beampatterns i.e. dipole, cardioid, hypercardioid
and supercardioid is presented. An analytical expression for the quantized
beamformed output for any general $ N^{th} $ order DMA is formulated.
Simulation results of the variation of ND with number of quantization bits and
the variation of NW as a function of depth are also presented and inferences
are drawn. Lab experiments are conducted in a fully anechoic room to support
the simulation results. The measured beampattern exhibits a pronounced null
depth, confirming the effectiveness of the experimental setup.

</details>


### [8] [Point Cloud Environment-Based Channel Knowledge Map Construction](https://arxiv.org/abs/2506.21112)
*Yancheng Wang,Wei Guo,Guanying Chen,Ye Zhang,Shuguang Cui*

Main category: eess.SP

TL;DR: 提出了一种联合模型和数据驱动的方法，利用点云环境数据和少量位置标记的信道信息构建信道知识地图（CKM），显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CKM构建方案过于简化环境信息，导致准确性不足，需要更精确的方法。

Method: 通过基于到达时间（ToA）的共焦椭球体选择相关点云子集，并训练神经网络估计信道增益。

Result: 在PDP和无线电地图构建中，RMSE分别为2.95 dB和1.04 dB，优于传统方法。

Conclusion: 该方法显著提升了CKM构建的准确性，为环境感知通信提供了更可靠的解决方案。

Abstract: Channel knowledge map (CKM) provides certain levels of channel state
information (CSI) for an area of interest, serving as a critical enabler for
environment-aware communications by reducing the overhead of frequent CSI
acquisition. However, existing CKM construction schemes adopt over-simplified
environment information, which significantly compromises their accuracy. To
address this issue, this work proposes a joint model- and data-driven approach
to construct CKM by leveraging point cloud environmental data along with a few
samples of location-tagged channel information. First, we propose a novel point
selector to identify subsets of point cloud that contain environmental
information relevant to multipath channel gains, by constructing a set of
co-focal ellipsoids based on different time of arrival (ToAs). Then, we trained
a neural channel gain estimator to learn the mapping between each selected
subset and its corresponding channel gain, using a real-world dataset we
collected through field measurements, comprising environmental point clouds and
corresponding channel data. Finally, experimental results demonstrate that: For
CKM construction of power delay profile (PDP), the proposed method achieves a
root mean squared error (RMSE) of 2.95 dB, significantly lower than the 7.32 dB
achieved by the conventional ray-tracing method; for CKM construction of
received power values, i.e., radio map, it achieves an RMSE of 1.04 dB,
surpassing the Kriging interpolation method with an RMSE of 1.68 dB.

</details>


### [9] [Characterization of Rydberg-Atom Signal Reception of Dual-Frequency Signals Coupled with Two Energy Levels](https://arxiv.org/abs/2506.21123)
*Hao Wu,Chongwu Xie,Xinyuan Yao,Kang-Da Wu,Shanchi Wu,Rui Ni,Guo-Yong Xiang,Chen Gong*

Main category: eess.SP

TL;DR: 论文分析了基于里德堡原子的多频信号传感器在多用户通信中的干扰问题，提出了联合响应系数，并通过实验验证了误码率和符号错误率。


<details>
  <summary>Details</summary>
Motivation: 传统天线中多频信号是正交的，而原子传感器中不同能级的信号会同时下变频到基带，导致多用户干扰。本文旨在分析这种干扰特性。

Method: 通过引入基于接收器特性的联合响应系数，分析了两信号在不同能级耦合时的干扰，并计算了误码率和符号错误率。

Result: 实验验证了误码率和符号错误率的理论分析结果。

Conclusion: 里德堡原子传感器在多频信号接收中存在干扰问题，需进一步优化以提升多用户通信性能。

Abstract: Rydberg atomic sensors have been adopted for novel radio frequency (RF)
measurement technique and the sensing capability for signals in multiple
frequencies makes it attractive for multi-user communication. However, unlike
traditional antennas where the signals in multiple frequencies are orthogonal,
the received signals of atomic sensors corresponding to different energy levels
will be downconverted to the baseband simultaneously, resulting in multi-user
interference. Thus, in this paper, we analyze the mutual interference
characteristics of two RF signals with different carrier frequencies coupling
different energy levels. We introduce the joint response coefficient based on
the receiver characteristics and analyze the interference of one user to
another. We analyze the bit-error rate (BER) and symbol-error rate (SER) for
two signals coupling two different energy levels. We also conduct experiments
to validate the BER and SER results.

</details>


### [10] [Adversarial Training: Enhancing Out-of-Distribution Generalization for Learning Wireless Resource Allocation](https://arxiv.org/abs/2506.21208)
*Shengjie Liu,Chenyang Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于对抗训练的方法，提升无监督训练的DNN在分布偏移下的泛化能力，并通过混合预编码优化验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源分配中广泛应用，但其性能易受训练与测试数据分布偏移的影响，如信道变化。

Method: 重新设计对抗训练以捕捉分布外退化，并提出一种一步梯度上升方法。

Result: 仿真结果显示，仅用瑞利衰落信道训练时，多种DNN在不同信道分布下均表现出增强的分布外性能。

Conclusion: 该方法有效提升了DNN在分布偏移下的泛化能力，适用于多种信道场景。

Abstract: Deep neural networks (DNNs) have widespread applications for optimizing
resource allocation. Yet, their performance is vulnerable to distribution
shifts between training and test data, say channels. In this letter, we resort
to adversarial training (AT) for enhancing out-of-distribution (OOD)
generalizability of DNNs trained in unsupervised manner. We reformulate AT to
capture the OOD degradation, and propose a one-step gradient ascent method for
AT. The proposed method is validated by optimizing hybrid precoding. Simulation
results showcase the enhanced OOD performance of multiple kinds of DNNs across
various channel distributions, when only Rayleigh fading channels are used for
training.

</details>


### [11] [Localization-Based Beam Focusing in Near-Field Communications](https://arxiv.org/abs/2506.21325)
*Nima Mozaffarikhosravi,Prathapasinghe Dharmawansa,Italo Atzeni*

Main category: eess.SP

TL;DR: 论文提出了一种基于定位的波束聚焦策略，利用毫米波和亚太赫兹频段的主导视距传播，并通过2D-MUSIC算法分析距离估计，最终在特定条件下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着6G及更高频段无线通信系统的应用，近场区域扩展影响了波束成形和用户定位方案，需要新的解决方案。

Method: 提出基于定位的波束聚焦策略，结合2D-MUSIC算法进行距离估计，并与传统零强迫方法进行比较。

Result: 数值结果表明，在视距传播主导、短相干块和强噪声功率条件下，新方法更有效。

Conclusion: 基于定位的波束聚焦策略在高频段和大带宽场景下具有优势。

Abstract: Shifting 6G-and-beyond wireless communication systems to higher frequency
bands and the utilization of massive multiple-input multiple-output arrays will
extend the near-field region, affecting beamforming and user localization
schemes. In this paper, we propose a localization-based beam-focusing strategy
that leverages the dominant line-of-sight (LoS) propagation arising at mmWave
and sub-THz frequencies. To support this approach, we analyze the 2D-MUSIC
algorithm for distance estimation by examining its spectrum in simplified,
tractable setups with minimal numbers of antennas and users. Lastly, we compare
the proposed localization-based beam focusing, with locations estimated via
2D-MUSIC, with zero forcing with pilot-based channel estimation in terms of
uplink sum spectral efficiency. Our numerical results show that the proposed
method becomes more effective under LoS-dominated propagation, short coherence
blocks, and strong noise power arising at high carrier frequencies and with
large bandwidths.

</details>


### [12] [Integrating Movable Antennas and Intelligent Reflecting Surfaces for Coverage Enhancement](https://arxiv.org/abs/2506.21375)
*Ying Gao,Qingqing Wu,Weidong Mei,Guangji Chen,Wen Chen,Ziyuan Zheng*

Main category: eess.SP

TL;DR: 论文研究了IRS辅助的MA系统，通过联合优化MA位置、IRS反射系数和发射波束成形，最大化目标区域的最差SNR，提出了三种覆盖增强方案和一种通用算法框架。


<details>
  <summary>Details</summary>
Motivation: 扩展无线覆盖范围至多个目标区域，同时平衡性能与成本。

Method: 提出了三种覆盖增强方案（area-adaptive MA-IRS、area-adaptive MA-staIRS、shared MA-staIRS）和一个通用算法框架。

Result: MA方案优于FPA方案，area-adaptive MA-IRS表现最佳；MA-staIRS在特定条件下可能不如FPA，但增加天线数可改善；MA与IRS元素的最优比例与成本比成反比。

Conclusion: MA系统在覆盖增强方面具有潜力，需根据成本和性能需求选择合适方案。

Abstract: This paper investigates an intelligent reflecting surface (IRS)-aided movable
antenna (MA) system, where multiple IRSs cooperate with a multi-MA base station
to extend wireless coverage to multiple designated target areas. The objective
is to maximize the worst-case signal-to-noise ratio (SNR) across all locations
within these areas through joint optimization of MA positions, IRS reflection
coefficients, and transmit beamforming. To achieve this while balancing the
performance-cost trade-off, we propose three coverage-enhancement schemes: the
area-adaptive MA-IRS scheme, the area-adaptive MA-staIRS scheme, and the shared
MA-staIRS scheme, where staIRS denotes static IRSs with reflection coefficients
configured only once during installation. These schemes lead to challenging
non-convex optimization problems with implicit objective functions, which are
difficult to solve optimally. To address these problems, we propose a general
algorithmic framework that can be applied to solve each problem efficiently
albeit suboptimally. Simulation results demonstrate that: 1) the proposed
MA-based schemes consistently outperform their fixed-position antenna
(FPA)-based counterparts under both area-adaptive and static IRS
configurations, with the area-adaptive MA-IRS scheme achieving the best
worst-case SNR performance; 2) as transmit antennas are typically far fewer
than IRS elements, the area-adaptive MA-staIRS scheme may underperform the
baseline FPA scheme with area-adaptive IRSs in terms of the worst-case SNR, but
a modest increase in antenna number can reverse this trend; 3) under a fixed
total cost, the optimal MA-to-IRS-element ratio for the worst-case SNR
maximization is empirically found to be proportional to the reciprocal of their
unit cost ratio.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [13] [CodecSlime: Temporal Redundancy Compression of Neural Speech Codec via Dynamic Frame Rate](https://arxiv.org/abs/2506.21074)
*Hankun Wang,Yiwei Guo,Chongtian Shao,Bohan Li,Xie Chen,Kai Yu*

Main category: eess.AS

TL;DR: CodecSlime提出了一种动态帧率（DFR）的神经语音编解码器插件方法，显著减少了冗余信息，提升了压缩效率。


<details>
  <summary>Details</summary>
Motivation: 语音信息在时间上分布不均匀，固定帧率（FFR）编解码器浪费了大量令牌在稳态段（如长元音和静音）。

Method: 结合ScheDFR和Melt-and-Cool两种创新方法，实现无监督、架构无关的动态帧率支持。

Result: 在40 Hz DFR下，重建WER相对FFR基线降低了46%，同时支持多帧率推理。

Conclusion: CodecSlime在压缩效率和重建质量上优于传统FFR方法，且具有灵活性。

Abstract: Neural speech codecs have been widely used in audio compression and various
downstream tasks. Current mainstream codecs are fixed-frame-rate (FFR), which
allocate the same number of tokens to every equal-duration slice. However,
speech is inherently non-uniform in temporal information density. As a result,
many tokens are wasted on steady-state segments like long vowels and silences.
To address this mismatch, we present CodecSlime, a plugin-style method for
compressing temporal redundancy through supporting dynamic frame rate (DFR) on
neural speech codecs for the first time. Our method is unsupervised and
architecture-agnostic, combining two key innovations, ScheDFR and
Melt-and-Cool, for adapting inference and training, respectively. When
integrated into a typical VQ-GAN codec backbone and operating at 40 Hz DFR
($\approx$ 600 bps), the reconstruction WER of CodecSlime is reduced by up to
46% relative to conventional FFR baselines with the same model architecture and
similar bitrates, while other metrics are also competitive. CodecSlime also
enables flexible trade-offs between reconstruction quality and bitrate: a
single model supports inference at multiple frame rates and consistently
outperforms FFR models at the corresponding frame rates. Audio samples are
available at https://acadarmeria.github.io/codecslime/.

</details>


### [14] [Post-training for Deepfake Speech Detection](https://arxiv.org/abs/2506.21090)
*Wanying Ge,Xin Wang,Xuechen Liu,Junichi Yamagishi*

Main category: eess.AS

TL;DR: 提出一种后训练方法，通过自监督学习（SSL）模型适应深度伪造语音检测，弥合通用预训练与领域特定微调之间的差距。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造语音检测中通用预训练与领域特定任务之间的不匹配问题。

Method: 使用大规模多语言语音数据集（56,000小时真实语音和18,000小时含人工痕迹语音）进行后训练，生成AntiDeepfake模型。

Result: 后训练模型在未见过的深度伪造语音上表现出强鲁棒性和泛化能力，进一步微调后超越现有最先进检测器。

Conclusion: 后训练方法显著提升深度伪造语音检测性能，模型和代码已开源。

Abstract: We introduce a post-training approach that adapts self-supervised learning
(SSL) models for deepfake speech detection by bridging the gap between general
pre-training and domain-specific fine-tuning. We present AntiDeepfake models, a
series of post-trained models developed using a large-scale multilingual speech
dataset containing over 56,000 hours of genuine speech and 18,000 hours of
speech with various artifacts in over one hundred languages. Experimental
results show that the post-trained models already exhibit strong robustness and
generalization to unseen deepfake speech. When they are further fine-tuned on
the Deepfake-Eval-2024 dataset, these models consistently surpass existing
state-of-the-art detectors that do not leverage post-training. Model
checkpoints and source code are available online.

</details>


### [15] [Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4](https://arxiv.org/abs/2506.21174)
*Jongyeon Park,Joonhee Lee,Do-Hyeon Lim,Hong Kook Kim,Hyeongcheol Geum,Jeong Eun Lim*

Main category: eess.AS

TL;DR: 该技术报告介绍了DCASE 2025挑战赛任务4的提交系统，通过加入额外音频特征和改进标签校正系统，提升了音频分类性能。


<details>
  <summary>Details</summary>
Motivation: 混合音频中的细微线索难以仅通过梅尔频谱捕捉，因此需要额外特征提供更多视角。

Method: 1. 在梅尔频谱特征中融入谱滚降和色度特征；2. 应用基于代理的标签校正系统减少误报；3. 优化训练数据集，去除无关样本并引入外部数据。

Result: 实验表明，这些方法使CA-SDRi相对基线提升了14.7%。

Conclusion: 通过多特征融合和数据集优化，显著提升了音频分类性能。

Abstract: This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.

</details>


### [16] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Main category: eess.AS

TL;DR: 研究探讨了结合传统信号处理与深度学习的混合模型，用于低资源阿拉伯方言识别，MFCC+CNN表现最佳（91.2%准确率），优于DWT+RNN（66.5%）。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言识别因语言多样性和标注数据稀缺而具挑战性，尤其在低资源场景下。

Method: 开发并评估了两种混合模型：MFCC+CNN和DWT+RNN，使用Common Voice阿拉伯数据集的方言子集进行训练。

Result: MFCC+CNN表现优异（91.2%准确率），显著优于DWT+RNN（66.5%）。

Conclusion: 研究为低资源阿拉伯方言识别提供了基线，建议未来采用更大标注数据集、自监督学习和先进架构（如Transformer）。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>


### [17] [ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing](https://arxiv.org/abs/2506.21448)
*Huadai Liu,Jialei Wang,Kaicheng Luo,Wen Wang,Qian Chen,Zhou Zhao,Wei Xue*

Main category: eess.AS

TL;DR: ThinkSound是一个基于Chain-of-Thought推理的框架，通过分阶段生成和编辑视频音频，结合多模态大语言模型和音频基础模型，实现了高保真音频生成。


<details>
  <summary>Details</summary>
Motivation: 当前端到端视频到音频生成技术难以捕捉视觉内容的细节，需要更复杂的推理能力。

Method: ThinkSound将音频生成分为三个阶段：基础音效生成、交互式对象中心细化、自然语言指导的编辑，并利用多模态大语言模型生成推理指导。

Result: ThinkSound在视频到音频生成任务中表现优异，在音频指标和CoT指标上均达到领先水平。

Conclusion: ThinkSound通过分阶段推理和多模态模型结合，显著提升了视频音频生成的质量和可控性。

Abstract: While end-to-end video-to-audio generation has greatly improved, producing
high-fidelity audio that authentically captures the nuances of visual content
remains challenging. Like professionals in the creative industries, such
generation requires sophisticated reasoning about items such as visual
dynamics, acoustic environments, and temporal relationships. We present
\textbf{ThinkSound}, a novel framework that leverages Chain-of-Thought (CoT)
reasoning to enable stepwise, interactive audio generation and editing for
videos. Our approach decomposes the process into three complementary stages:
foundational foley generation that creates semantically coherent soundscapes,
interactive object-centric refinement through precise user interactions, and
targeted editing guided by natural language instructions. At each stage, a
multimodal large language model generates contextually aligned CoT reasoning
that guides a unified audio foundation model. Furthermore, we introduce
\textbf{AudioCoT}, a comprehensive dataset with structured reasoning
annotations that establishes connections between visual content, textual
descriptions, and sound synthesis. Experiments demonstrate that ThinkSound
achieves state-of-the-art performance in video-to-audio generation across both
audio metrics and CoT metrics and excels in out-of-distribution Movie Gen Audio
benchmark. The demo page is available at https://ThinkSound-Demo.github.io.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [A Multi-Stage Framework for Multimodal Controllable Speech Synthesis](https://arxiv.org/abs/2506.20945)
*Rui Niu,Weihao Wu,Jie Chen,Long Ma,Zhiyong Wu*

Main category: cs.SD

TL;DR: 提出了一种3阶段多模态可控语音合成框架，通过监督学习和知识蒸馏解决泛化问题，结合文本-面部和文本-语音数据增强多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部的方法因数据质量限制缺乏鲁棒性和泛化能力，文本提示方法多样性不足且控制粒度有限，多模态方法因依赖完全匹配的训练数据而受限。

Method: 采用3阶段多模态框架：面部编码器通过监督学习和知识蒸馏提升泛化能力，文本编码器结合文本-面部和文本-语音数据增强多样性。

Result: 实验表明，该方法在基于面部和文本提示的语音合成中均优于单模态基线方法。

Conclusion: 该框架能有效生成高质量语音，解决了现有方法的局限性。

Abstract: Controllable speech synthesis aims to control the style of generated speech
using reference input, which can be of various modalities. Existing face-based
methods struggle with robustness and generalization due to data quality
constraints, while text prompt methods offer limited diversity and fine-grained
control. Although multimodal approaches aim to integrate various modalities,
their reliance on fully matched training data significantly constrains their
performance and applicability. This paper proposes a 3-stage multimodal
controllable speech synthesis framework to address these challenges. For face
encoder, we use supervised learning and knowledge distillation to tackle
generalization issues. Furthermore, the text encoder is trained on both
text-face and text-speech data to enhance the diversity of the generated
speech. Experimental results demonstrate that this method outperforms
single-modal baseline methods in both face based and text prompt based speech
synthesis, highlighting its effectiveness in generating high-quality speech.

</details>


### [19] [PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching](https://arxiv.org/abs/2506.21086)
*Guillem Cortès-Sebastià,Benjamin Martin,Emilio Molina,Xavier Serra,Romain Hennequin*

Main category: cs.SD

TL;DR: PeakNetFP是一种基于频谱峰的神经音频指纹系统，结合了传统峰值方法和深度学习，性能优于传统方法，与NeuralFP相当，但更轻量高效。


<details>
  <summary>Details</summary>
Motivation: 传统峰值音频指纹方法稀疏且高效，但缺乏深度学习的适应性和模式识别能力。PeakNetFP旨在结合两者的优势。

Method: 采用类似PointNet++的分层点特征提取技术，并通过对比学习训练，利用稀疏频谱坐标。

Result: 在时间拉伸音频数据上，Top-1命中率超过90%，参数比NeuralFP少100倍，输入数据小11倍。

Conclusion: PeakNetFP成功结合了峰值方法的轻量性和神经网络的适应性，为音频指纹技术提供了高效可扩展的解决方案。

Abstract: This work introduces PeakNetFP, the first neural audio fingerprinting (AFP)
system designed specifically around spectral peaks. This novel system is
designed to leverage the sparse spectral coordinates typically computed by
traditional peak-based AFP methods. PeakNetFP performs hierarchical point
feature extraction techniques similar to the computer vision model PointNet++,
and is trained using contrastive learning like in the state-of-the-art deep
learning AFP, NeuralFP. This combination allows PeakNetFP to outperform
conventional AFP systems and achieves comparable performance to NeuralFP when
handling challenging time-stretched audio data. In extensive evaluation,
PeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging
from 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages:
compared to NeuralFP, it has 100 times fewer parameters and uses 11 times
smaller input data. These features make PeakNetFP a lightweight and efficient
solution for AFP tasks where time stretching is involved. Overall, this system
represents a promising direction for future AFP technologies, as it
successfully merges the lightweight nature of peak-based AFP with the
adaptability and pattern recognition capabilities of neural network-based
approaches, paving the way for more scalable and efficient solutions in the
field.

</details>


### [20] [A Hierarchical Deep Learning Approach for Minority Instrument Detection](https://arxiv.org/abs/2506.21167)
*Dylan Sechet,Francesca Bugiotti,Matthieu Kowalski,Edouard d'Hérouville,Filip Langiewicz*

Main category: cs.SD

TL;DR: 论文探讨了在音乐信息检索中识别乐器活动的重要性，提出了一种基于Hornbostel-Sachs分类的层次分类系统，并在MedleyDB数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决乐器识别中因数据不足导致的细粒度标注问题，通过层次分类提升乐器活动的检测可靠性。

Method: 采用层次分类系统，结合Hornbostel-Sachs分类，利用MedleyDB数据集进行模型验证。

Result: 展示了在粗粒度乐器检测上的更高可靠性，为乐器识别领域提供了新思路。

Conclusion: 层次分类方法有效弥补了细粒度乐器识别与组别识别之间的差距，推动了该领域的进一步发展。

Abstract: Identifying instrument activities within audio excerpts is vital in music
information retrieval, with significant implications for music cataloging and
discovery. Prior deep learning endeavors in musical instrument recognition have
predominantly emphasized instrument classes with ample data availability.
Recent studies have demonstrated the applicability of hierarchical
classification in detecting instrument activities in orchestral music, even
with limited fine-grained annotations at the instrument level. Based on the
Hornbostel-Sachs classification, such a hierarchical classification system is
evaluated using the MedleyDB dataset, renowned for its diversity and richness
concerning various instruments and music genres. This work presents various
strategies to integrate hierarchical structures into models and tests a new
class of models for hierarchical music prediction. This study showcases more
reliable coarse-level instrument detection by bridging the gap between detailed
instrument identification and group-level recognition, paving the way for
further advancements in this domain.

</details>


### [21] [Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou](https://arxiv.org/abs/2506.21269)
*Pengfei Fan,Yuli Zhang,Xinheng Wang,Ruiyuan Jiang,Hankang Gu,Dongyao Jia,Shangbo Wang*

Main category: cs.SD

TL;DR: 该研究公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种双模态特征融合深度卷积神经网络（BMCNN）来建模车辆噪声与行驶速度的关系。实验表明BMCNN在两个数据集上表现优异，并验证了各模块对性能提升的贡献。


<details>
  <summary>Details</summary>
Motivation: 为了透明和可重复地研究车辆噪声与速度的关系，并支持智能城市交通管理系统的实时噪声监测和速度估计。

Method: 提出BMCNN网络，结合自适应去噪和归一化预处理，并行提取MFCC和小波包能量特征，并通过跨模态注意力机制融合。

Result: BMCNN在SZUR-Acoustic数据集上分类准确率为87.56%，在IDMT-Traffic数据集上为96.28%。消融实验验证了各模块的有效性。

Conclusion: 该方法可优化交通流控制、减少路边噪声污染，并支持可持续城市规划。

Abstract: This study presents and publicly releases the Suzhou Urban Road Acoustic
Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive
data-acquisition protocols and annotation guidelines to ensure transparency and
reproducibility of the experimental workflow. To model the coupling between
vehicular noise and driving speed, we propose a bimodal-feature-fusion deep
convolutional neural network (BMCNN). During preprocessing, an adaptive
denoising and normalization strategy is applied to suppress environmental
background interference; in the network architecture, parallel branches extract
Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,
which are subsequently fused via a cross-modal attention mechanism in the
intermediate feature space to fully exploit time-frequency information.
Experimental results demonstrate that BMCNN achieves a classification accuracy
of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic
dataset. Ablation studies and robustness tests on the Suzhou dataset further
validate the contributions of each module to performance improvement and
overfitting mitigation. The proposed acoustics-based speed classification
method can be integrated into smart-city traffic management systems for
real-time noise monitoring and speed estimation, thereby optimizing traffic
flow control, reducing roadside noise pollution, and supporting sustainable
urban planning.

</details>


### [22] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 研究了不同适配器配置对MusicGen和Mustango音乐生成模型的影响，发现卷积适配器擅长捕捉局部音乐细节，而Transformer适配器更适合长距离依赖。Mustango生成多样但稳定性差，MusicGen训练更快但输出冗余。


<details>
  <summary>Details</summary>
Motivation: 探索在低资源音乐类型中，如何通过参数高效微调（PEFT）技术优化适配器设计，以平衡性能和计算成本。

Method: 比较卷积和Transformer适配器在MusicGen和Mustango上的表现，分析不同规模和类型的适配器对Hindustani古典和土耳其Makam音乐的影响。

Result: 卷积适配器捕捉局部细节，Transformer适配器保持长距离依赖；Mustango生成多样但稳定性差，MusicGen训练更快但冗余较高。

Conclusion: 中规模适配器（40M参数）在表达性和质量间取得最佳平衡，为低资源音乐类型提供了高效微调方案。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [23] [Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform](https://arxiv.org/abs/2506.21440)
*Maxime Leiber,Yosra Marnissi,Axel Barrau,Sylvain Meignen,Laurent Massoulié*

Main category: cs.SD

TL;DR: 提出了一种可微分的短时傅里叶变换（STFT）方法，通过梯度优化参数，解决了传统方法依赖离散搜索的问题，并可与神经网络联合优化。


<details>
  <summary>Details</summary>
Motivation: 传统STFT参数调整依赖手动或启发式方法，效果不佳且计算量大。

Method: 提出统一的可微分STFT框架，支持基于梯度的参数优化，并与神经网络结合。

Result: 实验证明该方法能优化时频表示，提升下游任务性能。

Conclusion: 可微分STFT为参数优化提供了高效方案，适用于实际应用。

Abstract: The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.

</details>


### [24] [SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture](https://arxiv.org/abs/2506.21478)
*Kehan Sui,Jinxu Xiang,Fang Jin*

Main category: cs.SD

TL;DR: SmoothSinger是一种基于条件扩散模型的歌唱语音合成方法，通过统一框架直接优化低质量音频，避免了两阶段流程的失真问题，并在Opencpop数据集上取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 歌唱语音合成需要精确建模音高、时长和发音，而现有扩散模型在歌唱语音合成中存在自然度不足的问题。

Method: 提出SmoothSinger，采用参考引导的双分支架构和并行低频上采样路径，直接优化低质量音频，并改进训练中对齐问题。

Result: 在Opencpop数据集上，SmoothSinger在客观和主观评估中均达到最优效果，减少了伪影并提升了自然度。

Conclusion: SmoothSinger通过统一框架和结构改进，显著提升了歌唱语音合成的质量和自然度。

Abstract: Singing voice synthesis (SVS) aims to generate expressive and high-quality
vocals from musical scores, requiring precise modeling of pitch, duration, and
articulation. While diffusion-based models have achieved remarkable success in
image and video generation, their application to SVS remains challenging due to
the complex acoustic and musical characteristics of singing, often resulting in
artifacts that degrade naturalness. In this work, we propose SmoothSinger, a
conditional diffusion model designed to synthesize high quality and natural
singing voices. Unlike prior methods that depend on vocoders as a final stage
and often introduce distortion, SmoothSinger refines low-quality synthesized
audio directly in a unified framework, mitigating the degradation associated
with two-stage pipelines. The model adopts a reference-guided dual-branch
architecture, using low-quality audio from any baseline system as a reference
to guide the denoising process, enabling more expressive and context-aware
synthesis. Furthermore, it enhances the conventional U-Net with a parallel
low-frequency upsampling path, allowing the model to better capture pitch
contours and long term spectral dependencies. To improve alignment during
training, we replace reference audio with degraded ground truth audio,
addressing temporal mismatch between reference and target signals. Experiments
on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that
SmoothSinger achieves state-of-the-art results in both objective and subjective
evaluations. Extensive ablation studies confirm its effectiveness in reducing
artifacts and improving the naturalness of synthesized voices.

</details>

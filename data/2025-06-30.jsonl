{"id": "2506.21951", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21951", "abs": "https://arxiv.org/abs/2506.21951", "authors": ["Wenze Ren", "Yi-Cheng Lin", "Wen-Chin Huang", "Ryandhimas E. Zezario", "Szu-Wei Fu", "Sung-Feng Huang", "Erica Cooper", "Haibin Wu", "Hung-Yu Wei", "Hsin-Min Wang", "Hung-yi Lee", "Yu Tsao"], "title": "HighRateMOS: Sampling-Rate Aware Modeling for Speech Quality Assessment", "comment": "Under Review, 3 pages + 1 References", "summary": "Modern speech quality prediction models are trained on audio data resampled\nto a specific sampling rate. When faced with higher-rate audio at test time,\nthese models can produce biased scores. We introduce HighRateMOS, the first\nnon-intrusive mean opinion score (MOS) model that explicitly considers sampling\nrate. HighRateMOS ensembles three model variants that exploit the following\ninformation: (i) a learnable embedding of speech sampling rate, (ii) Wav2vec\n2.0 self-supervised embeddings, (iii) multi-scale CNN spectral features, and\n(iv) MFCC features. In AudioMOS 2025 Track3, HighRateMOS ranked first in five\nout of eight metrics. Our experiments confirm that modeling the sampling rate\ndirectly leads to more robust and sampling-rate-agnostic speech quality\npredictions."}
{"id": "2506.22001", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.22001", "abs": "https://arxiv.org/abs/2506.22001", "authors": ["Lu Han", "Junqi Zhao", "Renhua Peng"], "title": "WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation", "comment": "Accepted by Interspeech2025", "summary": "Current multi-channel speech enhancement systems mainly adopt single-output\narchitecture, which face significant challenges in preserving spatio-temporal\nsignal integrity during multiple-input multiple-output (MIMO) processing. To\naddress this limitation, we propose a novel neural network, termed WTFormer,\nfor MIMO speech enhancement that leverages the multi-resolution characteristics\nof wavelet transform and multi-dimensional collaborative attention to\neffectively capture globally distributed spatial features, while using\nConformer for time-frequency modeling. A multi task loss strategy accompanying\nMUSIC algorithm is further proposed for optimization training to protect\nspatial information to the greatest extent. Experimental results on the\nLibriSpeech dataset show that WTFormer can achieve comparable denoising\nperformance to advanced systems while preserving more spatial information with\nonly 0.98M parameters."}
{"id": "2506.22194", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22194", "abs": "https://arxiv.org/abs/2506.22194", "authors": ["Shunsuke Mitsumori", "Sara Kashiwagi", "Keitaro Tanaka", "Shigeo Morishima"], "title": "Cross-lingual Data Selection Using Clip-level Acoustic Similarity for Enhancing Low-resource Automatic Speech Recognition", "comment": "Accepted at INTERSPEECH 2025", "summary": "This paper presents a novel donor data selection method to enhance\nlow-resource automatic speech recognition (ASR). While ASR performs well in\nhigh-resource languages, its accuracy declines in low-resource settings due to\nlimited training data. A common solution is to leverage multilingual\nself-supervised learning (SSL) models with donor languages. However, existing\nmethods rely on language-level similarity, overlooking clip-level variations.\nTo address this limitation, we propose clip-wise acoustic token distribution\nsimilarity (CATDS), a fine-grained selection method that identifies\nacoustically relevant donor clips for better alignment with the target\nlanguage. Unlike existing clip-level selection methods, our method aligns with\nthe representation of SSL models and offers more challenging yet valuable\nsamples. Experimental results show that CATDS outperforms traditional selection\nmethods and can even utilize donor languages previously considered detrimental."}
{"id": "2506.22362", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22362", "abs": "https://arxiv.org/abs/2506.22362", "authors": ["Yang Yang", "Yunpeng Li", "George Sung", "Shao-Fu Shih", "Craig Dooley", "Alessio Centazzo", "Ramanan Rajeswaran"], "title": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding", "comment": null, "summary": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss."}
{"id": "2506.21664", "categories": ["eess.SP", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.21664", "abs": "https://arxiv.org/abs/2506.21664", "authors": ["Kevin Weinberger", "Aydin Sezgin"], "title": "When Every Symbol Counts: Resilient Wireless Systems Under Finite Blocklength Constraints", "comment": "6 pages, 3 figures, submitted to European Wireless 2025. arXiv admin\n  note: text overlap with arXiv:2504.11589", "summary": "As 6G evolves, wireless networks become essential for critical operations and\nenable innovative applications that demand seamless adaptation to dynamic\nenvironments and disruptions. Because these vital services require\nuninterrupted operation, their resilience to unforeseen disruptions is\nessential. However, implementing resilience necessitates rapid recovery\nprocedures, which operate in the finite blocklength (FBL) regime, where short\npackets and added error-correction overhead can severely degrade communication\nefficiency. Due to this performance loss, always attempting recovery can\nbackfire and result in worse outcomes than simply enduring the disruption under\nlonger blocklengths. In this work, we study these effects of FBL constraints\nwithin a resilience framework, incorporating reconfigurable intelligent\nsurfaces (RIS) to enhance adaptation capabilities. By actively shaping the\nwireless environment, RIS help counteract some of the performance losses caused\nby FBL, enabling more effective recovery from disruptions. Numerical results\nreveal two critical blocklength thresholds: the first enables full recovery\nfrom the FBL penalty, while the second, at a higher blocklength, allows the\nsystem to recover from both the FBL penalty and the initial disruption,\nyielding a significant improvement in resilience performance. Additionally, we\nshow that the number of RIS elements shifts these thresholds, enabling faster\nreconfiguration with shorter blocklengths and providing insights to the\ntrade-offs between rate, blocklength, and reconfiguration effort under FBL\nconditions."}
{"id": "2506.22023", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22023", "abs": "https://arxiv.org/abs/2506.22023", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "comment": "17 pages, 8 figures, 5 tables", "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems."}
{"id": "2506.22023", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22023", "abs": "https://arxiv.org/abs/2506.22023", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "comment": "17 pages, 8 figures, 5 tables", "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems."}
{"id": "2506.21690", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21690", "abs": "https://arxiv.org/abs/2506.21690", "authors": ["Hongqin Ke", "Jindan Xu", "Wei Xu", "Chau Yuen", "Zhaohua Lu"], "title": "Joint RIS-UE Association and Beamforming Design in RIS-Assisted Cell-Free MIMO Network", "comment": null, "summary": "Reconfigurable intelligent surface (RIS)-assisted cell-free (CF)\nmultiple-input multiple-output (MIMO) networks can significantly enhance system\nperformance. However, the extensive deployment of RIS elements imposes\nconsiderable channel acquisition overhead, with the high density of nodes and\nantennas in RIS-assisted CF networks amplifying this challenge. To tackle this\nissue, in this paper, we explore integrating RIS-user equipment (UE)\nassociation into downlink RIS-assisted CF transmitter design, which greatly\nreduces the channel acquisition costs. The key point is that once UEs are\nassociated with specific RISs, there is no need to frequently acquire channels\nfrom non-associated RISs. Then, we formulate the problem of joint RIS-UE\nassociation and beamforming at APs and RISs to maximize the weighted sum rate\n(WSR). In particular, we propose a two-stage framework to solve it. In the\nfirst stage, we apply a many-to-many matching algorithm to establish the RIS-UE\nassociation. In the second stage, we introduce a sequential optimization-based\nmethod that decomposes the joint optimization of RIS phase shifts and AP\nbeamforming into two distinct subproblems. To optimize the RIS phase shifts, we\nemploy the majorization-minimization (MM) algorithm to obtain a\nsemi-closed-form solution. For AP beamforming, we develop a joint block\ndiagonalization algorithm, which yields a closed-form solution. Simulation\nresults demonstrate the effectiveness of the proposed algorithm and show that,\nwhile RIS-UE association significantly reduces overhead, it incurs a minor\nperformance loss that remains within an acceptable range. Additionally, we\ninvestigate the impact of RIS deployment and conclude that RISs exhibit\nenhanced performance when positioned between APs and UEs."}
{"id": "2506.22237", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22237", "abs": "https://arxiv.org/abs/2506.22237", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "comment": "9 pages, 3 figures, 6 tables", "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment."}
{"id": "2506.22237", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22237", "abs": "https://arxiv.org/abs/2506.22237", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "comment": "9 pages, 3 figures, 6 tables", "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment."}
{"id": "2506.21772", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21772", "abs": "https://arxiv.org/abs/2506.21772", "authors": ["Noé Lallouet", "Tristan Cazenave", "Cyrille Enderli", "Stéphanie Gourdin"], "title": "Searching Efficient Deep Architectures for Radar Target Detection using Monte-Carlo Tree Search", "comment": null, "summary": "Recent research works establish deep neural networks as high performing tools\nfor radar target detection, especially on challenging environments (presence of\nclutter or interferences, multi-target scenarii...). However, the usually large\ncomputational complexity of these networks is one of the factors preventing\nthem from being widely implemented in embedded radar systems. We propose to\ninvestigate novel neural architecture search (NAS) methods, based on\nMonte-Carlo Tree Search (MCTS), for finding neural networks achieving the\nrequired detection performance and striving towards a lower computational\ncomplexity. We evaluate the searched architectures on endoclutter radar\nsignals, in order to compare their respective performance metrics and\ngeneralization properties. A novel network satisfying the required detection\nprobability while being significantly lighter than the expert-designed baseline\nis proposed."}
{"id": "2506.22311", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22311", "abs": "https://arxiv.org/abs/2506.22311", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Anomadarshi Barua"], "title": "Reconstructing Intelligible Speech from the Pressure Sensor Data in HVACs", "comment": null, "summary": "Pressure sensors are an integrated component of modern Heating, Ventilation,\nand Air Conditioning (HVAC) systems. As these pressure sensors operate within\nthe 0-10 Pa range, support high sampling frequencies of 0.5-2 kHz, and are\noften placed close to human proximity, they can be used to eavesdrop on\nconfidential conversation, since human speech has a similar audible range of\n0-10 Pa and a bandwidth of 4 kHz for intelligible quality. This paper presents\nWaLi, which reconstructs intelligible speech from the low-resolution and noisy\npressure sensor data by providing the following technical contributions: (i)\nWaLi reconstructs intelligible speech from a minimum of 0.5 kHz sampling\nfrequency of pressure sensors, whereas previous work can only detect hot\nwords/phrases. WaLi uses complex-valued conformer and Complex Global Attention\nBlock (CGAB) to capture inter-phoneme and intra-phoneme dependencies that exist\nin the low-resolution pressure sensor data. (ii) WaLi handles the transient\nnoise injected from HVAC fans and duct vibrations, by reconstructing both the\nclean magnitude and phase of the missing frequencies of the low-frequency\naliased components. Extensive measurement studies on real-world pressure\nsensors show an LSD of 1.24 and NISQA-MOS of 1.78 for 0.5 kHz to 8 kHz\nupsampling. We believe that such levels of accuracy pose a significant threat\nwhen viewed from a privacy perspective that has not been addressed before for\npressure sensors."}
{"id": "2506.22311", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22311", "abs": "https://arxiv.org/abs/2506.22311", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Anomadarshi Barua"], "title": "Reconstructing Intelligible Speech from the Pressure Sensor Data in HVACs", "comment": null, "summary": "Pressure sensors are an integrated component of modern Heating, Ventilation,\nand Air Conditioning (HVAC) systems. As these pressure sensors operate within\nthe 0-10 Pa range, support high sampling frequencies of 0.5-2 kHz, and are\noften placed close to human proximity, they can be used to eavesdrop on\nconfidential conversation, since human speech has a similar audible range of\n0-10 Pa and a bandwidth of 4 kHz for intelligible quality. This paper presents\nWaLi, which reconstructs intelligible speech from the low-resolution and noisy\npressure sensor data by providing the following technical contributions: (i)\nWaLi reconstructs intelligible speech from a minimum of 0.5 kHz sampling\nfrequency of pressure sensors, whereas previous work can only detect hot\nwords/phrases. WaLi uses complex-valued conformer and Complex Global Attention\nBlock (CGAB) to capture inter-phoneme and intra-phoneme dependencies that exist\nin the low-resolution pressure sensor data. (ii) WaLi handles the transient\nnoise injected from HVAC fans and duct vibrations, by reconstructing both the\nclean magnitude and phase of the missing frequencies of the low-frequency\naliased components. Extensive measurement studies on real-world pressure\nsensors show an LSD of 1.24 and NISQA-MOS of 1.78 for 0.5 kHz to 8 kHz\nupsampling. We believe that such levels of accuracy pose a significant threat\nwhen viewed from a privacy perspective that has not been addressed before for\npressure sensors."}
{"id": "2506.21796", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21796", "abs": "https://arxiv.org/abs/2506.21796", "authors": ["Dani Korpi", "Rachel Wang", "Jerry Wang", "Abdelrahman Ibrahim", "Carl Nuzman", "Runxin Wang", "Kursat Rasim Mestav", "Dustin Zhang", "Iraj Saniee", "Shawn Winston", "Gordana Pavlovic", "Wei Ding", "William J. Hillery", "Chenxi Hao", "Ram Thirunagari", "Jung Chang", "Jeehyun Kim", "Bartek Kozicki", "Dragan Samardzija", "Taesang Yoo", "Andreas Maeder", "Tingfang Ji", "Harish Viswanathan"], "title": "Demonstrating Interoperable Channel State Feedback Compression with Machine Learning", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Neural network-based compression and decompression of channel state feedback\nhas been one of the most widely studied applications of machine learning (ML)\nin wireless networks. Various simulation-based studies have shown that ML-based\nfeedback compression can result in reduced overhead and more accurate channel\ninformation. However, to the best of our knowledge, there are no real-life\nproofs of concepts demonstrating the benefits of ML-based channel feedback\ncompression in a practical setting, where the user equipment (UE) and base\nstation have no access to each others' ML models. In this paper, we present a\nnovel approach for training interoperable compression and decompression ML\nmodels in a confidential manner, and demonstrate the accuracy of the ensuing\nmodels using prototype UEs and base stations. The performance of the ML-based\nchannel feedback is measured both in terms of the accuracy of the reconstructed\nchannel information and achieved downlink throughput gains when using the\nchannel information for beamforming. The reported measurement results\ndemonstrate that it is possible to develop an accurate ML-based channel\nfeedback link without having to share ML models between device and network\nvendors. These results pave the way for a practical implementation of ML-based\nchannel feedback in commercial 6G networks."}
{"id": "2506.22321", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22321", "abs": "https://arxiv.org/abs/2506.22321", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "comment": null, "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB."}
{"id": "2506.22321", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22321", "abs": "https://arxiv.org/abs/2506.22321", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "comment": null, "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB."}
{"id": "2506.21798", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21798", "abs": "https://arxiv.org/abs/2506.21798", "authors": ["Xuhong Li", "Benjamin J. B. Deutschmann", "Erik Leitinger", "Florian Meyer"], "title": "Adaptive Multipath-Based SLAM for Distributed MIMO Systems", "comment": "30 pages. Submitted to IEEE Transactions on Wireless Communications", "summary": "Localizing users and mapping the environment using radio signals is a key\ntask in emerging applications such as reliable communications, location-aware\nsecurity, and safety critical navigation. Recently introduced multipath-based\nsimultaneous localization and mapping (MP-SLAM) can jointly localize a mobile\nagent and the reflective surfaces in radio frequency (RF) environments. Most\nexisting MP-SLAM methods assume that map features and their corresponding RF\npropagation paths are statistically independent, which neglects inherent\ndependencies arising when a single reflective surface contributes to different\npropagation paths or when an agent communicates with more than one base\nstation. Previous approaches that aim to fuse information across propagation\npaths are limited by their inability to perform ray tracing in environments\nwith nonconvex geometries. In this paper, we propose a Bayesian MP-SLAM method\nfor distributed MIMO systems that addresses this limitation. In particular, we\nuse amplitude statistics to establish adaptive time-varying detection\nprobabilities. Based on the resulting \"soft\" ray-tracing strategy, our method\ncan fuse information across propagation paths in RF environments with nonconvex\ngeometries. A Bayesian estimation method for the joint estimation of map\nfeatures and agent position is established by applying the message passing\nrules of the sum-product algorithm (SPA) to the factor graph that represents\nthe proposed statistical model. We also introduce an improved proposal PDF for\nparticle-based computation of SPA messages. This proposal PDF enables the early\ndetection of new surfaces that are solely supported by double-bounce paths. Our\nmethod is validated using synthetic RF measurements in a challenging scenario\nwith nonconvex geometries. The results demonstrate that it can provide accurate\nlocalization and mapping estimates as well as attain the posterior CRLB."}
{"id": "2506.22001", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.22001", "abs": "https://arxiv.org/abs/2506.22001", "authors": ["Lu Han", "Junqi Zhao", "Renhua Peng"], "title": "WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation", "comment": "Accepted by Interspeech2025", "summary": "Current multi-channel speech enhancement systems mainly adopt single-output\narchitecture, which face significant challenges in preserving spatio-temporal\nsignal integrity during multiple-input multiple-output (MIMO) processing. To\naddress this limitation, we propose a novel neural network, termed WTFormer,\nfor MIMO speech enhancement that leverages the multi-resolution characteristics\nof wavelet transform and multi-dimensional collaborative attention to\neffectively capture globally distributed spatial features, while using\nConformer for time-frequency modeling. A multi task loss strategy accompanying\nMUSIC algorithm is further proposed for optimization training to protect\nspatial information to the greatest extent. Experimental results on the\nLibriSpeech dataset show that WTFormer can achieve comparable denoising\nperformance to advanced systems while preserving more spatial information with\nonly 0.98M parameters."}
{"id": "2506.21803", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21803", "abs": "https://arxiv.org/abs/2506.21803", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "comment": "ICML 2025", "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP."}
{"id": "2506.21893", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21893", "abs": "https://arxiv.org/abs/2506.21893", "authors": ["Jingheng Zheng", "Hui Tian", "Wanli Ni", "Yang Tian", "Ping Zhang"], "title": "Improving Convergence for Semi-Federated Learning: An Energy-Efficient Approach by Manipulating Over-the-Air Distortion", "comment": null, "summary": "In this paper, we propose a hybrid learning framework that combines federated\nand split learning, termed semi-federated learning (SemiFL), in which\nover-the-air computation is utilized for gradient aggregation. A key idea is to\nstrategically adjust the learning rate by manipulating over-the-air distortion\nfor improving SemiFL's convergence. Specifically, we intentionally amplify\namplitude distortion to increase the learning rate in the non-stable region,\nthereby accelerating convergence and reducing communication energy consumption.\nIn the stable region, we suppress noise perturbation to maintain a small\nlearning rate for improving SemiFL's final convergence. Theoretical results\ndemonstrate the antagonistic effects of over-the-air distortion in different\nregions, under both independent and identically distributed (i.i.d.) and\nnon-i.i.d. data settings. Then, we formulate two energy consumption\nminimization problems, one for each region, which implements a two-region mean\nsquare error threshold configuration scheme. Accordingly, we propose two\nresource allocation algorithms with closed-form solutions. Simulation results\nshow that under different network and data distribution conditions,\nstrategically manipulating over-the-air distortion can efficiently adjust the\nlearning rate to improve SemiFL's convergence. Moreover, energy consumption can\nbe reduced by using the proposed algorithms."}
{"id": "2506.21966", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21966", "abs": "https://arxiv.org/abs/2506.21966", "authors": ["Osmel Martínez Rosabal", "Onel Alcaraz López", "Marco Di Renzo", "Richard Demo Souza", "Hirley Alves"], "title": "Movable Antennas-aided Wireless Energy Transfer for the Internet of Things", "comment": "7 pages, 5 figures, submitted to IEEE Transactions on Vehicular\n  Technology", "summary": "Recent advancements in movable antennas (MAs) technology create new\nopportunities for 6G and beyond wireless systems. MAs are promising for radio\nfrequency wireless energy transfer because they can dynamically adjust antenna\npositions, improving energy efficiency and scalability. This work aims to\nminimize the power consumed by an analog beamforming power beacon equipped with\nindependently-controlled MAs (IMAs) for charging multiple single-antenna\ndevices. To this end, we enforce a minimum separation among antennas and a\nminimum received power at the devices. The resulting optimization problem is\nnonlinear and nonconvex due to interdependencies among the variables. To tackle\nthis, we propose a semidefinite program guided particle swarm optimization\n(SgPSO) algorithm where each particle represents an antenna configuration, and\nthe fitness function optimizes the corresponding power allocation. SgPSO is\nutilized for configuring the MAs largely outperforming fixed array\nimplementations, particularly with more antennas or devices. We also present an\nalternative implementation using uniformly-spaced MAs, whose performance\nclosely approaches that of the IMAs, with the gap widening only as the number\nof devices grows. We also examine how increasing the number of antennas\npromotes near-field conditions, which decrease as devices become more widely\ndistributed."}
{"id": "2506.21983", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.21983", "abs": "https://arxiv.org/abs/2506.21983", "authors": ["Osama Saleem", "Mohammed Alfaqawi", "Pierre Merdrignac", "Abdelaziz Bensrhair", "Soheyb Ribouh"], "title": "Learning-Based Hybrid Neural Receiver for 6G-V2X Communications", "comment": null, "summary": "Neural receiver models are proposed to jointly optimize multiple\nfunctionalities of wireless receivers; however, a comprehensive receiver model\nthat replaces the entire physical layer blocks has not yet been presented in\nthe literature. In this work, we introduce a novel hybrid neural receiver\n(H-NR) built on Transformer encoder blocks and Graph Neural Network (GNN), as\npart of an end-to-end wireless communication framework. In our communication\nframework, we assume vehicle to network (V2N) uplink scenario where information\nis transmitted by vehicle and received at the base station (BS). Our proposed\nH-NR model replace OFDM resource grid demapping, channel estimation, signal\nequalization, demodulation, and channel decoding. To test the adaptability of\nour proposed model on unseen conditions, we evaluate its performance for\nvarious scenarios, including a vehicle speed of range [0-60] km/h, a carrier\nfrequency of 5.9GHz, and a cluster delay line (CDL) channel model. Furthermore,\nwe assess the performance of our proposed H-NR on multimodal data, such as\nimages, audio, GPS, radar, and LiDAR, to examine its adaptability in real-world\nuse cases. The simulation results clearly demonstrate that our proposed model\noutperforms the state-of-the-art neural receiver by approximately 0.5 dB in\nterms of reconstruction and error correction."}
{"id": "2506.22059", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22059", "abs": "https://arxiv.org/abs/2506.22059", "authors": ["Yupeng Zheng", "Yi Ma", "Rahim Tafazolli"], "title": "Hybrid Constellation Modulation for Symbol-Level Precoding in RIS-Enhanced MU-MISO Systems", "comment": "This work has been accepted by IEEE SPAWC 2025", "summary": "The application of symbol-level precoding (SLP) in reconfigurable intelligent\nsurfaces (RIS) enhanced multi-user multiple-input single-output (MU-MISO)\nsystems faces two main challenges. First, the state-of-the-art joint reflecting\nand SLP optimization approach requires exhaustive enumeration of all possible\ntransmit symbol combinations, resulting in scalability issues as the modulation\norder and number of users increase. Second, conventional quadrature amplitude\nmodulation (QAM) exhibits strict constructive interference (CI) regions,\nlimiting its effectiveness for CI exploitation in SLP. To address these\nchallenges, this paper proposes a novel modulation scheme, termed\nhybrid-constellation modulation (HCM), which has a structure of superposed QAM\nand ASK sub-constellations (SCs). HCM extends the CI regions compared to QAM.\nAdditionally, a two-stage reflecting and SLP optimization method is developed\nto support HCM. The proposed methods are designed for practical RIS with\ndiscrete phase shifts and has good scalability. Simulation results show that\nHCM achieves up to 1.5 dB and 1 dB SER gains over QAM with modulation order 16\nand 64, respectively."}
{"id": "2506.22082", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22082", "abs": "https://arxiv.org/abs/2506.22082", "authors": ["Dimitris Kompostiotis", "Dimitris Vordonis", "Vassilis Paliouras", "George C. Alexandropoulos"], "title": "Optimizing Indoor RIS-Aided Physical-Layer Security: A Codebook-Generation Methodology and Measurement-Based Analysis", "comment": "7 pages, 3 figures, 2 tables Accepted for publication in the 2025\n  IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC), Istanbul, Turkey, September 1-4, 2025; to appear in\n  IEEE PIMRC 2025 proceedings. copyright 2025 IEEE. Personal use of this\n  material is permitted", "summary": "Sixth-Generation (6G) wireless networks aim to support innovative\nInternet-of-Things (IoT) applications that demand faster and more secure data\ntransmission. While higher Open Systems Interconnection (OSI) layers employ\nmeasures like encryption and secure protocols to address data security,\nPhysical-Layer Security (PLS) focuses on preventing information leakage to\nEavesDroppers (EDs) and mitigating the effects of jammers and spoofing attacks.\nIn this context, the emerging technology of Reconfigurable Intelligent Surfaces\n(RISs) can play an instrumental role, enhancing PLS by intelligently reflecting\nelectromagnetic waves to benefit Legitimate Users (LUs) while obstructing EDs.\nThis paper presents practical indoor measurements to evaluate the capability of\nan RIS to enhance PLS, focusing on a varactor-based RIS technology designed for\nthe FR1 band at 3.55 GHz. A comparative analysis of state-of-the-art RIS-aided\nsecrecy optimization algorithms together with a novel approach designed in this\npaper, which relies on a newly generated RIS phase configuration codebook,\nhighlight the potential of RISs to improve both data rates for LUs as well as\nsecrecy against EDs in real-world indoor multipath environments. The results\nalso demonstrate the frequency selectivity of the RIS, proviging practical\ninsights on the optimization of the technology."}
{"id": "2506.22252", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22252", "abs": "https://arxiv.org/abs/2506.22252", "authors": ["Alphan Sahin"], "title": "On the Feasibility of Distributed Phase Synchronization for Coherent Signal Superposition", "comment": "Submitted to IEEE for publication", "summary": "In this study, we analyze the feasibility of distributed phase\nsynchronization for coherent signal superposition, a fundamental enabler for\nparadigms such as coherent over-the-air computation (OAC), distributed\nbeamforming, and interference alignment, under mobility and hardware\nimpairments. With the focus on coherent OAC, we introduce phase-coded pilots\n(PCPs), a strategy where the radios communicate with each other to eliminate\nthe round-trip phase change in the uplink (UL) and downlink (DL) to align the\nphase of the received symbol at a desired angle. In this study, considering a\ncarrier frequency offset (CFO)-resilient multi-user procedure, we derive the\nstatistics of the phase deviations to assess how fast the phase coherency\ndegrades. Our results show that residual CFO is a major factor determining the\nduration of phase coherency, in addition to the non-negligible effects of\nmobility and the number of nodes in the network. We also provide a\nproof-of-concept demonstration for coherent signal superposition by using\noff-the-shelf radios to demonstrate the feasibility of PCPs in practice."}
{"id": "2506.22277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22277", "abs": "https://arxiv.org/abs/2506.22277", "authors": ["Pengyang Song", "Jue Wang"], "title": "A Self-scaled Approximate $\\ell_0$ Regularization Robust Model for Outlier Detection", "comment": null, "summary": "Robust regression models in the presence of outliers have significant\npractical relevance in areas such as signal processing, financial econometrics,\nand energy management. Many existing robust regression methods, either grounded\nin statistical theory or sparse signal recovery, typically rely on the explicit\nor implicit assumption of outlier sparsity to filter anomalies and recover the\nunderlying signal or data. However, these methods often suffer from limited\nrobustness or high computational complexity, rendering them inefficient for\nlarge-scale problems. In this work, we propose a novel robust regression model\nbased on a Self-scaled Approximate l0 Regularization Model (SARM) scheme. By\nintroducing a self-scaling mechanism into the regularization term, the proposed\nmodel mitigates the negative impact of uneven or excessively large outlier\nmagnitudes on robustness. We also develop an alternating minimization algorithm\ngrounded in Proximal Operators and Block Coordinate Descent. We rigorously\nprove the algorithm convergence. Empirical comparisons with several\nstate-of-the-art robust regression methods demonstrate that SARM not only\nachieves superior robustness but also significantly improves computational\nefficiency. Motivated by both the theoretical error bound and empirical\nobservations, we further design a Two-Stage SARM (TSSARM) framework, which\nbetter utilizes sample information when the singular values of the design\nmatrix are widely spread, thereby enhancing robustness under certain\nconditions. Finally, we validate our approach on a real-world load forecasting\ntask. The experimental results show that our method substantially enhances the\nrobustness of load forecasting against adversarial data attacks, which is\nincreasingly critical in the era of heightened data security concerns."}
{"id": "2506.22411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22411", "abs": "https://arxiv.org/abs/2506.22411", "authors": ["Omar Barrera", "Sinwoo Cho", "Jack Kramer", "Vakhtang Chulukhadze", "Tzu-Hsuan Hsu", "Ruochen Lu"], "title": "19.3 GHz Acoustic Filter with High Close-in Rejection in Tri-layer Thin-Film Lithium Niobate", "comment": "4 Pages, 5 figures", "summary": "Acoustic filters are preferred front-end solutions at sub-6 GHz due to their\nsuperior frequency selectivity compared to electromagnetic (EM) counterparts.\nWith the ongoing development of 5G and the evolution toward 6G, there is a\ngrowing need to extend acoustic filter technologies into frequency range 3\n(FR3), which spans 7 to 24 GHz to accommodate emerging high-frequency bands.\nHowever, scaling acoustic filters beyond 10 GHz presents significant\nchallenges, as conventional platforms suffer from increased insertion loss (IL)\nand degraded out-of-band (OoB) rejection at higher frequencies. Recent\ninnovations have led to the emergence of periodically poled piezoelectric\nlithium niobate (P3F LN) laterally excited bulk acoustic resonators (XBARs),\noffering low-loss and high electromechanical coupling performance above 10 GHz.\nThis work presents the first tri-layer P3F LN filter operating at 19.3 GHz,\nachieving a low IL of 2.2 dB, a 3-dB fractional bandwidth (FBW) of 8.5%, and an\nimpressive 49 dB close in rejection. These results demonstrate strong potential\nfor integration into FR3 diplexers."}

{"id": "2506.22448", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.22448", "abs": "https://arxiv.org/abs/2506.22448", "authors": ["Yu Ma", "Xingyu Zhou", "Xiao Li", "Le Liang", "Shi Jin"], "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "summary": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless\nsystems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA\nsystem, addressing resource allocation challenges. A two-stage unsupervised\nlearning-based framework is proposed to jointly design RIS phase shifts, BS\nbeamforming, and resource block (RB) allocation. The framework includes\nBeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which\nallocates RBs using equivalent CSI derived from BeamNet outputs. Active\nbeamforming is implemented via maximum ratio transmission and water-filling. To\nhandle discrete constraints while ensuring differentiability, quantization and\nthe Gumbel-softmax trick are adopted. A customized loss and phased training\nenhance performance under QoS constraints. Simulations show the method achieves\n99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and\nit remains robust across varying channel and user conditions."}
{"id": "2506.22454", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22454", "abs": "https://arxiv.org/abs/2506.22454", "authors": ["Ana Luiza S. Tavares", "Artur Pedro M. Neto", "Francinaldo L. Gomes", "Paul Rodrigo dos Reis", "Arthur G. da Silva", "Antonio P. Junior", "Bruno D. Gomes"], "title": "Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach", "comment": "8 pages, 5 figures", "summary": "Accurate intraoperative localization of the subthalamic nucleus (STN) is\nessential for the efficacy of Deep Brain Stimulation (DBS) in patients with\nParkinson's disease. While microelectrode recordings (MERs) provide rich\nelectrophysiological information during DBS electrode implantation, current\nlocalization practices often rely on subjective interpretation of signal\nfeatures. In this study, we propose a quantitative framework that leverages\nnonlinear dynamics and entropy-based metrics to classify neural activity\nrecorded inside versus outside the STN. MER data from three patients were\npreprocessed using a robust artifact correction pipeline, segmented, and\nlabelled based on surgical annotations. A comprehensive set of recurrence\nquantification analysis, nonlinear, and entropy features were extracted from\neach segment. Multiple supervised classifiers were trained on every combination\nof feature domains using stratified 10-fold cross-validation, followed by\nstatistical comparison using paired Wilcoxon signed-rank tests with\nHolm-Bonferroni correction. The combination of entropy and nonlinear features\nyielded the highest discriminative power, and the Extra Trees classifier\nemerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and\nROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed\nrobust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the\npotential of nonlinear and entropy signal descriptors in supporting real-time,\ndata-driven decision-making during DBS surgeries"}
{"id": "2506.22455", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22455", "abs": "https://arxiv.org/abs/2506.22455", "authors": ["Dung Truong", "Arnaud Delorme"], "title": "Data Normalization Strategies for EEG Deep Learning", "comment": null, "summary": "Normalization is a critical yet often overlooked component in the\npreprocessing pipeline for EEG deep learning applications. The rise of\nlarge-scale pretraining paradigms such as self-supervised learning (SSL)\nintroduces a new set of tasks whose nature is substantially different from\nsupervised training common in EEG deep learning applications. This raises new\nquestions about optimal normalization strategies for the applicable task. In\nthis study, we systematically evaluate the impact of normalization granularity\n(recording vs. window level) and scope (cross-channel vs. within-channel) on\nboth supervised (age and gender prediction) and self-supervised (Contrastive\nPredictive Coding) tasks. Using high-density resting-state EEG from 2,836\nsubjects in the Healthy Brain Network dataset, we show that optimal\nnormalization strategies differ significantly between training paradigms.\nWindow-level within-channel normalization yields the best performance in\nsupervised tasks, while minimal or cross-channel normalization at the window\nlevel is more effective for SSL. These results underscore the necessity of\ntask-specific normalization choices and challenge the assumption that a\nuniversal normalization strategy can generalize across learning settings. Our\nfindings provide practical insights for developing robust EEG deep learning\npipelines as the field shifts toward large-scale, foundation model training."}
{"id": "2506.22456", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.22456", "abs": "https://arxiv.org/abs/2506.22456", "authors": ["Rahul Gulia", "Amlan Ganguly", "Andres Kwasinski", "Michael E. Kuhl", "Ehsan Rashedi", "Clark Hochgraf"], "title": "WISVA: Generative AI for 5G Network Optimization in Smart Warehouses", "comment": null, "summary": "The next decade will usher in a profound transformation of wireless\ncommunication, driven by the ever-increasing demand for data-intensive\napplications and the rapid adoption of emerging technologies. To fully unlock\nthe potential of 5G and beyond, substantial advancements are required in signal\nprocessing techniques, innovative network architectures, and efficient spectrum\nutilization strategies. These advancements facilitate seamless integration of\nemerging technologies, driving industrial digital transformation and\nconnectivity. This paper introduces a novel Variational Autoencoder (VAE)-based\nframework, Wireless Infrastructure for Smart Warehouses using VAE (WISVA),\ndesigned for accurate indoor radio propagation modeling in automated Industry\n4.0 environments such as warehouses and factory floors operating within 5G\nwireless bands. The research delves into the meticulous creation of training\ndata tensors, capturing complex electromagnetic (EM) wave behaviors influenced\nby diverse obstacles, and outlines the architecture and training methodology of\nthe proposed VAE model. The model's robustness and adaptability are showcased\nthrough its ability to predict signal-to-interference-plus-noise ratio (SINR)\nheatmaps across various scenarios, including denoising tasks, validation\ndatasets, extrapolation to unseen configurations, and previously unencountered\nwarehouse layouts. Compelling reconstruction error heatmaps are presented,\nhighlighting the superior accuracy of WISVA compared to traditional autoencoder\nmodels. The paper also analyzes the model's performance in handling complex\nsmart warehouse environments, demonstrating its potential as a key enabler for\noptimizing wireless infrastructure in Industry 4.0."}
{"id": "2506.22628", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22628", "abs": "https://arxiv.org/abs/2506.22628", "authors": ["Amir Salimi", "Abram Hindle", "Osmar R. Zaiane"], "title": "Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching", "comment": null, "summary": "Manual sound design with a synthesizer is inherently iterative: an artist\ncompares the synthesized output to a mental target, adjusts parameters, and\nrepeats until satisfied. Iterative sound-matching automates this workflow by\ncontinually programming a synthesizer under the guidance of a loss function (or\nsimilarity measure) toward a target sound. Prior comparisons of loss functions\nhave typically favored one metric over another, but only within narrow\nsettings: limited synthesis methods, few loss types, often without blind\nlistening tests. This leaves open the question of whether a universally optimal\nloss exists, or the choice of loss remains a creative decision conditioned on\nthe synthesis method and the sound designer's preference. We propose\ndifferentiable iterative sound-matching as the natural extension of the\navailable literature, since it combines the manual approach to sound design\nwith modern advances in machine learning. To analyze the variability of loss\nfunction performance across synthesizers, we implemented a mix of four novel\nand established differentiable loss functions, and paired them with\ndifferentiable subtractive, additive, and AM synthesizers. For each of the\nsixteen synthesizer--loss combinations, we ran 300 randomized sound-matching\ntrials. Performance was measured using parameter differences,\nspectrogram-distance metrics, and manually assigned listening scores. We\nobserved a moderate level of consistency among the three performance measures.\nOur post-hoc analysis shows that the loss function performance is highly\ndependent on the synthesizer. These findings underscore the value of expanding\nthe scope of sound-matching experiments and developing new similarity metrics\ntailored to specific synthesis techniques rather than pursuing\none-size-fits-all solutions."}
{"id": "2506.22646", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.22646", "abs": "https://arxiv.org/abs/2506.22646", "authors": ["Weiqing Wang", "Taejin Park", "Ivan Medennikov", "Jinhan Wang", "Kunal Dhawan", "He Huang", "Nithin Rao Koluguri", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR", "comment": "Accepted by INTERSPEECH 2025", "summary": "We propose a self-speaker adaptation method for streaming multi-talker\nautomatic speech recognition (ASR) that eliminates the need for explicit\nspeaker queries. Unlike conventional approaches requiring target speaker\nembeddings or enrollment audio, our technique dynamically adapts individual ASR\ninstances through speaker-wise speech activity prediction. The key innovation\ninvolves injecting speaker-specific kernels generated via speaker supervision\nactivations into selected ASR encoder layers. This enables instantaneous\nspeaker adaptation to target speakers while handling fully overlapped speech\neven in a streaming scenario. Experiments show state-of-the-art performance in\nboth offline and streaming scenarios, demonstrating that our self-adaptive\nmethod effectively addresses severe speech overlap through streamlined\nspeaker-focused recognition. The results validate the proposed self-speaker\nadaptation approach as a robust solution for multi-talker ASR under severe\noverlapping speech conditions."}
{"id": "2506.22457", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22457", "abs": "https://arxiv.org/abs/2506.22457", "authors": ["Iulia Orvas", "Andrei Radu", "Alessandra Galli", "Ana Neacsu", "Elisabetta Peri"], "title": "A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes", "comment": null, "summary": "Continuous, non-invasive pregnancy monitoring is crucial for minimising\npotential complications. The fetal electrocardiogram (fECG) represents a\npromising tool for assessing fetal health beyond clinical environments.\nHome-based monitoring necessitates the use of a minimal number of comfortable\nand durable electrodes, such as dry textile electrodes. However, this setup\npresents many challenges, including increased noise and motion artefacts, which\ncomplicate the accurate extraction of fECG signals. To overcome these\nchallenges, we introduce a pioneering method for extracting fECG from\nsingle-channel recordings obtained using dry textile electrodes using AI\ntechniques. We created a new dataset by simulating abdominal recordings,\nincluding noise closely resembling real-world characteristics of in-vivo\nrecordings through dry textile electrodes, alongside mECG and fECG. To ensure\nthe reliability of the extracted fECG, we propose an innovative pipeline based\non a complex-valued denoising network, Complex UNet. Unlike previous approaches\nthat focused solely on signal magnitude, our method processes both real and\nimaginary components of the spectrogram, addressing phase information and\npreventing incongruous predictions. We evaluated our novel pipeline against\ntraditional, well-established approaches, on both simulated and real data in\nterms of fECG extraction and R-peak detection. The results showcase that our\nsuggested method achieves new state-of-the-art results, enabling an accurate\nextraction of fECG morphology across all evaluated settings. This method is the\nfirst to effectively extract fECG signals from single-channel recordings using\ndry textile electrodes, making a significant advancement towards a fully\nnon-invasive and self-administered fECG extraction solution."}
{"id": "2506.22661", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22661", "abs": "https://arxiv.org/abs/2506.22661", "authors": ["R. Oguz Araz", "Guillem Cortès-Sebastià", "Emilio Molina", "Joan Serrà", "Xavier Serra", "Yuki Mitsufuji", "Dmitry Bogdanov"], "title": "Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for Music Identification", "comment": "Accepted to ISMIR2025", "summary": "Audio fingerprinting (AFP) allows the identification of unknown audio content\nby extracting compact representations, termed audio fingerprints, that are\ndesigned to remain robust against common audio degradations. Neural AFP methods\noften employ metric learning, where representation quality is influenced by the\nnature of the supervision and the utilized loss function. However, recent work\nunrealistically simulates real-life audio degradation during training,\nresulting in sub-optimal supervision. Additionally, although several modern\nmetric learning approaches have been proposed, current neural AFP methods\ncontinue to rely on the NT-Xent loss without exploring the recent advances or\nclassical alternatives. In this work, we propose a series of best practices to\nenhance the self-supervision by leveraging musical signal properties and\nrealistic room acoustics. We then present the first systematic evaluation of\nvarious metric learning approaches in the context of AFP, demonstrating that a\nself-supervised adaptation of the triplet loss yields superior performance. Our\nresults also reveal that training with multiple positive samples per anchor has\ncritically different effects across loss functions. Our approach is built upon\nthese insights and achieves state-of-the-art performance on both a large,\nsynthetically degraded dataset and a real-world dataset recorded using\nmicrophones in diverse music venues."}
{"id": "2506.22972", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22972", "abs": "https://arxiv.org/abs/2506.22972", "authors": ["Yu-Wen Chen", "Julia Hirschberg"], "title": "Adaptable Non-parametric Approach for Speech-based Symptom Assessment: Isolating Private Medical Data in a Retrieval Datastore", "comment": "IEEE MLSP 2025", "summary": "The automatic assessment of health-related acoustic cues has the potential to\nimprove healthcare accessibility and affordability. Although parametric models\nare promising, they face challenges in privacy and adaptability. To address\nthese, we propose a NoN-Parametric framework for Speech-based symptom\nAssessment (NoNPSA). By isolating medical data in a retrieval datastore, NoNPSA\navoids encoding private information in model parameters and enables efficient\ndata updates. A self-supervised learning (SSL) model pre-trained on\ngeneral-purpose datasets extracts features, which are used for similarity-based\nretrieval. Metadata-aware refinement filters the retrieved data, and associated\nlabels are used to compute an assessment score. Experimental results show that\nNoNPSA achieves competitive performance compared to fine-tuning SSL-based\nmethods, while enabling greater privacy, update efficiency, and\nadaptability--showcasing the potential of non-parametric approaches in\nhealthcare."}
{"id": "2506.22458", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22458", "abs": "https://arxiv.org/abs/2506.22458", "authors": ["S M Minhazur Rahman", "Md. Amrin Ibna Hasnath", "Rifatul Islam", "Ahmed Faizul Haque Dhrubo", "Mohammad Abdul Qayum"], "title": "A Portable and Cost-Effective System for Real-Time Air Quality Monitoring and Environmental Impact Assessment", "comment": "This is a 7-page paper with 5 figures, and it has not been submitted\n  to any conference", "summary": "Air pollution remains a major global issue that seriously impacts public\nhealth, environmental quality, and ultimately human health. To help monitor\nproblem, we have created and constructed a low-cost, real-time, portable air\nquality monitoring system using cheap sensors. The system measures critical\npollutants PM2.5, PM10, and carbon monoxide (CO), and environmental variables\nsuch as temperature and humidity. The system computes the Air Quality Index\n(AQI) and transmits the data via a Bluetooth connection. The data is relayed,\nin real time, to a mobile application. Because of its small size and low\nmanufacturing cost the system readily lends itself to indoor and outdoor use\nand in urban and rural environments. In this paper we give an account of the\nsystem design, development, and validation, while demonstrating its accuracy\nand low-cost capabilities. We also consider its wider environmental, social,\nand regulatory implications with regards to; improving public awareness, being\nused for sustainability purposes and providing valuable information for\ninformed decision making."}
{"id": "2506.22789", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22789", "abs": "https://arxiv.org/abs/2506.22789", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Kaan Kale", "Sandeep P. Chinchali", "Sriram Vishwanath"], "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "comment": "5 pages, 4 figures, Published at The Proceedings of Interspeech 2025,\n  code is available at http://www.github.com/UTAustin-SwarmLab/WavShape", "summary": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems."}
{"id": "2506.23371", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23371", "abs": "https://arxiv.org/abs/2506.23371", "authors": ["Frank Cwitkowitz", "Zhiyao Duan"], "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation", "comment": "Accepted to ISMIR 2025", "summary": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of\nMusic Information Retrieval (MIR) systems, and is critical for many\napplications and downstream tasks involving pitch, including music\ntranscription. However, existing methods are largely based on supervised\nlearning, and there are significant challenges in collecting annotated data for\nthe task. Recently, self-supervised techniques exploiting intrinsic properties\nof pitch and harmonic signals have shown promise for both monophonic and\npolyphonic pitch estimation, but these still remain inferior to supervised\nmethods. In this work, we extend the classic supervised MPE paradigm by\nincorporating several self-supervised objectives based on pitch-invariant and\npitch-equivariant properties. This joint training results in a substantial\nimprovement under closed training conditions, which naturally suggests that\napplying the same objectives to a broader collection of data will yield further\nimprovements. However, in doing so we uncover a phenomenon whereby our model\nsimultaneously overfits to the supervised data while degenerating on data used\nfor self-supervision only. We demonstrate and investigate this and offer our\ninsights on the underlying problem."}
{"id": "2506.22459", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22459", "abs": "https://arxiv.org/abs/2506.22459", "authors": ["Wending Heng", "Chaoyuan Liang", "Yihui Zhao", "Zhiqiang Zhang", "Glen Cooper", "Zhenhong Li"], "title": "Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Accurately decoding human motion intentions from surface electromyography\n(sEMG) is essential for myoelectric control and has wide applications in\nrehabilitation robotics and assistive technologies. However, existing\nsEMG-based motion estimation methods often rely on subject-specific\nmusculoskeletal (MSK) models that are difficult to calibrate, or purely\ndata-driven models that lack physiological consistency. This paper introduces a\nnovel Physics-Embedded Neural Network (PENN) that combines interpretable MSK\nforward-dynamics with data-driven residual learning, thereby preserving\nphysiological consistency while achieving accurate motion estimation. The PENN\nemploys a recursive temporal structure to propagate historical estimates and a\nlightweight convolutional neural network for residual correction, leading to\nrobust and temporally coherent estimations. A two-phase training strategy is\ndesigned for PENN. Experimental evaluations on six healthy subjects show that\nPENN outperforms state-of-the-art baseline methods in both root mean square\nerror (RMSE) and $R^2$ metrics."}
{"id": "2506.22810", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22810", "abs": "https://arxiv.org/abs/2506.22810", "authors": ["Shiyao Wang", "Jiaming Zhou", "Shiwan Zhao", "Yong Qin"], "title": "A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition", "comment": "accepted by Interspeech 2025", "summary": "Dysarthric speech recognition (DSR) enhances the accessibility of smart\ndevices for dysarthric speakers with limited mobility. Previously, DSR research\nwas constrained by the fact that existing datasets typically consisted of\nisolated words, command phrases, and a limited number of sentences spoken by a\nfew individuals. This constrained research to command-interaction systems and\nspeaker adaptation. The Speech Accessibility Project (SAP) changed this by\nreleasing a large and diverse English dysarthric dataset, leading to the SAP\nChallenge to build speaker- and text-independent DSR systems. We enhanced the\nWhisper model's performance on long dysarthric speech via a novel self-training\nmethod. This method increased training data and adapted the model to handle\npotentially incomplete speech segments encountered during inference. Our system\nachieved second place in both Word Error Rate and Semantic Score in the SAP\nChallenge."}
{"id": "2506.23553", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23553", "abs": "https://arxiv.org/abs/2506.23553", "authors": ["Taisei Takano", "Yuki Okamoto", "Yusuke Kanamori", "Yuki Saito", "Ryotaro Nagase", "Hiroshi Saruwatari"], "title": "Human-CLAP: Human-perception-based contrastive language-audio pretraining", "comment": null, "summary": "Contrastive language-audio pretraining (CLAP) is widely used for audio\ngeneration and recognition tasks. For example, CLAPScore, which utilizes the\nsimilarity of CLAP embeddings, has been a major metric for the evaluation of\nthe relevance between audio and text in text-to-audio. However, the\nrelationship between CLAPScore and human subjective evaluation scores is still\nunclarified. We show that CLAPScore has a low correlation with human subjective\nevaluation scores. Additionally, we propose a human-perception-based CLAP\ncalled Human-CLAP by training a contrastive language-audio model using the\nsubjective evaluation score. In our experiments, the results indicate that our\nHuman-CLAP improved the Spearman's rank correlation coefficient (SRCC) between\nthe CLAPScore and the subjective evaluation scores by more than 0.25 compared\nwith the conventional CLAP."}
{"id": "2506.22460", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22460", "abs": "https://arxiv.org/abs/2506.22460", "authors": ["Ibne Farabi Shihab"], "title": "Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods", "comment": null, "summary": "Using mobile phone video of the fingertip as a data source for estimating\nvital signs such as heart rate (HR) and respiratory rate (RR) during daily life\nhas long been suggested. While existing literature indicates that these\nestimates are accurate to within several beats or breaths per minute, the data\nused to draw these conclusions are typically collected in laboratory\nenvironments under careful experimental control, and yet the results are\nassumed to generalize to daily life. In an effort to test it, a team of\nresearchers collected a large dataset of mobile phone video recordings made\nduring daily life and annotated with ground truth HR and RR labels from N=111\nparticipants. They found that traditional algorithm performance on the\nfingerprint videos is worse than previously reported (7 times and 13 times\nworse for RR and HR, respectively). Fortunately, recent advancements in deep\nlearning, especially in convolutional neural networks (CNNs), offer a promising\nsolution to improve this performance. This study proposes a new method for\nestimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error\nin estimated HR by 68% and RR by 75%. These promising results suggest that\nregressor-based deep learning approaches should be used in estimating HR and\nRR."}
{"id": "2506.23094", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23094", "abs": "https://arxiv.org/abs/2506.23094", "authors": ["Qi He", "Gus Xia", "Ziyu Wang"], "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025", "summary": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines."}
{"id": "2506.23859", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23859", "abs": "https://arxiv.org/abs/2506.23859", "authors": ["Chenda Li", "Wangyou Zhang", "Wei Wang", "Robin Scheibler", "Kohei Saijo", "Samuele Cornell", "Yihui Fu", "Marvin Sach", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "Less is More: Data Curation Matters in Scaling Speech Enhancement", "comment": "Submitted to ASRU2025", "summary": "The vast majority of modern speech enhancement systems rely on data-driven\nneural network models. Conventionally, larger datasets are presumed to yield\nsuperior model performance, an observation empirically validated across\nnumerous tasks in other domains. However, recent studies reveal diminishing\nreturns when scaling speech enhancement data. We focus on a critical factor:\nprevalent quality issues in ``clean'' training labels within large-scale\ndatasets. This work re-examines this phenomenon and demonstrates that, within\nlarge-scale training sets, prioritizing high-quality training data is more\nimportant than merely expanding the data volume. Experimental findings suggest\nthat models trained on a carefully curated subset of 700 hours can outperform\nmodels trained on the 2,500-hour full dataset. This outcome highlights the\ncrucial role of data curation in scaling speech enhancement systems\neffectively."}
{"id": "2506.22461", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22461", "abs": "https://arxiv.org/abs/2506.22461", "authors": ["Chuan Li", "Ruoxuan Yang"], "title": "Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation", "comment": null, "summary": "Groundwater supports ecosystems, agriculture, and drinking water supplies\nworldwide, yet effective monitoring remains challenging due to sparse data,\ncomputational constraints, and delayed outputs from traditional approaches. We\ndevelop a machine learning pipeline that predicts groundwater level categories\nusing climate data, hydro-meteorological records, and physiographic attributes\nprocessed through AutoGluon's automated ensemble framework. Our approach\nintegrates geospatial preprocessing, domain-driven feature engineering, and\nautomated model selection to overcome conventional monitoring limitations.\nApplied to a large-scale French dataset (n $>$ 3,440,000 observations from\n1,500+ wells), the model achieves weighted F\\_1 scores of 0.927 on validation\ndata and 0.67 on temporally distinct test data. Scenario-based evaluations\ndemonstrate practical utility for early warning systems and water allocation\ndecisions under changing climate conditions. The open-source implementation\nprovides a scalable framework for integrating machine learning into national\ngroundwater monitoring networks, enabling more responsive and data-driven water\nmanagement strategies."}
{"id": "2506.23130", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23130", "abs": "https://arxiv.org/abs/2506.23130", "authors": ["Tao-Tao He", "Martin E. Malandro", "Douglas Shadle"], "title": "The Florence Price Art Song Dataset and Piano Accompaniment Generator", "comment": "8 pages, 4 figures. To appear in the proceedings of ISMIR 2025", "summary": "Florence B. Price was a composer in the early 20th century whose music\nreflects her upbringing in the American South, her African heritage, and her\nWestern classical training. She is noted as the first African-American woman to\nhave a symphony performed by a major orchestra. Her music has recently received\nrenewed attention from both the public and the research community, decades\nafter her death. In addition to other genres, Price was a prolific composer for\nsolo voice and piano. Music historians have documented the existence of 134 art\nsongs and piano/voice arrangements for spirituals and folk songs written by\nPrice. We release a digital catalog of 112 of these works in MuseScore,\nMusicXML, MIDI, and PDF format. We also use this dataset to fine-tune a\nsymbolic music generation model to generate accompaniments to melodies, and we\nconduct a blind listening experiment that shows that accompaniments generated\nby our model are perceived as being reflective of Florence Price's style more\nfrequently than accompaniments generated by a baseline model. We release our\nmodel as the Florence Price Piano Accompaniment Generator alongside our\ndataset."}
{"id": "2506.23874", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23874", "abs": "https://arxiv.org/abs/2506.23874", "authors": ["Jiahe Wang", "Chenda Li", "Wei Wang", "Wangyou Zhang", "Samuele Cornell", "Marvin Sach", "Robin Scheibler", "Kohei Saijo", "Yihui Fu", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition", "comment": "Submitted to ASRU2025", "summary": "The Mean Opinion Score (MOS) is fundamental to speech quality assessment.\nHowever, its acquisition requires significant human annotation. Although deep\nneural network approaches, such as DNSMOS and UTMOS, have been developed to\npredict MOS to avoid this issue, they often suffer from insufficient training\ndata. Recognizing that the comparison of speech enhancement (SE) systems\nprioritizes a reliable system comparison over absolute scores, we propose\nURGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK\ntakes homologous enhanced speech pairs as input to predict relative quality\nrankings. This pairwise paradigm efficiently utilizes limited training data, as\nall pairwise permutations of multiple systems constitute a training instance.\nExperiments across multiple open test sets demonstrate URGENT-PK's superior\nsystem-level ranking performance over state-of-the-art baselines, despite its\nsimple network architecture and limited training data."}
{"id": "2506.22462", "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.22462", "abs": "https://arxiv.org/abs/2506.22462", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living."}
{"id": "2506.23325", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23325", "abs": "https://arxiv.org/abs/2506.23325", "authors": ["Yitian Gong", "Luozhijie Jin", "Ruifan Deng", "Dong Zhang", "Xin Zhang", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "comment": null, "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer."}
{"id": "2506.22628", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22628", "abs": "https://arxiv.org/abs/2506.22628", "authors": ["Amir Salimi", "Abram Hindle", "Osmar R. Zaiane"], "title": "Evaluating Sound Similarity Metrics for Differentiable, Iterative Sound-Matching", "comment": null, "summary": "Manual sound design with a synthesizer is inherently iterative: an artist\ncompares the synthesized output to a mental target, adjusts parameters, and\nrepeats until satisfied. Iterative sound-matching automates this workflow by\ncontinually programming a synthesizer under the guidance of a loss function (or\nsimilarity measure) toward a target sound. Prior comparisons of loss functions\nhave typically favored one metric over another, but only within narrow\nsettings: limited synthesis methods, few loss types, often without blind\nlistening tests. This leaves open the question of whether a universally optimal\nloss exists, or the choice of loss remains a creative decision conditioned on\nthe synthesis method and the sound designer's preference. We propose\ndifferentiable iterative sound-matching as the natural extension of the\navailable literature, since it combines the manual approach to sound design\nwith modern advances in machine learning. To analyze the variability of loss\nfunction performance across synthesizers, we implemented a mix of four novel\nand established differentiable loss functions, and paired them with\ndifferentiable subtractive, additive, and AM synthesizers. For each of the\nsixteen synthesizer--loss combinations, we ran 300 randomized sound-matching\ntrials. Performance was measured using parameter differences,\nspectrogram-distance metrics, and manually assigned listening scores. We\nobserved a moderate level of consistency among the three performance measures.\nOur post-hoc analysis shows that the loss function performance is highly\ndependent on the synthesizer. These findings underscore the value of expanding\nthe scope of sound-matching experiments and developing new similarity metrics\ntailored to specific synthesis techniques rather than pursuing\none-size-fits-all solutions."}
{"id": "2506.22465", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.22465", "abs": "https://arxiv.org/abs/2506.22465", "authors": ["Jun Zhu", "Yin Xu", "Dazhi He", "Haoyang Li", "Yunfeng Guan", "Wenjun Zhang"], "title": "Preconditioned Conjugate Gradient for MIMO-AFDM System", "comment": "arXiv admin note: text overlap with arXiv:2503.10525", "summary": "Affine frequency division multiplexing (AFDM) is a promising chirp-assisted\nmulticarrier waveform for future high mobility communications. A significant\nchallenge in MIMO-AFDM systems is the multi-user interference (MUI), which can\nbe effectively addressed by employing precoding techniques. However, the\ncomplexity introduced by AFDM makes the precoding process computationally\nexpensive and challenging. To overcome this issue, We combine AFDM channel\nsparse property and using Preconditioned Conjugate Gradient (PCG) method to\niteratively process the precoding, thereby reducing the complexity of the\nprecoding design. Simulation results demonstrate that the proposed\nsparsification approach, coupled with the PCG method, achieving quite precoding\nperformance while significantly reducing computational complexity. This makes\nthe application of AFDM more feasible and efficient for high-mobility\ncommunication scenarios, paving the way for its broader implementation in\nnext-generation communication systems."}
{"id": "2506.23367", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23367", "abs": "https://arxiv.org/abs/2506.23367", "authors": ["Paige Tuttösí", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals."}
{"id": "2506.22661", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22661", "abs": "https://arxiv.org/abs/2506.22661", "authors": ["R. Oguz Araz", "Guillem Cortès-Sebastià", "Emilio Molina", "Joan Serrà", "Xavier Serra", "Yuki Mitsufuji", "Dmitry Bogdanov"], "title": "Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for Music Identification", "comment": "Accepted to ISMIR2025", "summary": "Audio fingerprinting (AFP) allows the identification of unknown audio content\nby extracting compact representations, termed audio fingerprints, that are\ndesigned to remain robust against common audio degradations. Neural AFP methods\noften employ metric learning, where representation quality is influenced by the\nnature of the supervision and the utilized loss function. However, recent work\nunrealistically simulates real-life audio degradation during training,\nresulting in sub-optimal supervision. Additionally, although several modern\nmetric learning approaches have been proposed, current neural AFP methods\ncontinue to rely on the NT-Xent loss without exploring the recent advances or\nclassical alternatives. In this work, we propose a series of best practices to\nenhance the self-supervision by leveraging musical signal properties and\nrealistic room acoustics. We then present the first systematic evaluation of\nvarious metric learning approaches in the context of AFP, demonstrating that a\nself-supervised adaptation of the triplet loss yields superior performance. Our\nresults also reveal that training with multiple positive samples per anchor has\ncritically different effects across loss functions. Our approach is built upon\nthese insights and achieves state-of-the-art performance on both a large,\nsynthetically degraded dataset and a real-world dataset recorded using\nmicrophones in diverse music venues."}
{"id": "2506.22467", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22467", "abs": "https://arxiv.org/abs/2506.22467", "authors": ["Roy Colglazier", "Jisoo Lee", "Haoyu Dong", "Hanxue Gu", "Yaqian Chen", "Joseph Cao", "Zafer Yildiz", "Zhonghao Liu", "Nicholas Konz", "Jichen Yang", "Jikai Zhang", "Yuwen Chen", "Lin Li", "Adrian Camarena", "Maciej A. Mazurowski"], "title": "SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI", "comment": "24 pages, 6 figures", "summary": "The quantity and quality of muscles are increasingly recognized as important\npredictors of health outcomes. While MRI offers a valuable modality for such\nassessments, obtaining precise quantitative measurements of musculature remains\nchallenging. This study aimed to develop a publicly available model for muscle\nsegmentation in MRIs and demonstrate its applicability across various\nanatomical locations and imaging sequences. A total of 362 MRIs from 160\npatients at a single tertiary center (Duke University Health System, 2016-2020)\nwere included, with 316 MRIs from 114 patients used for model development. The\nmodel was tested on two separate sets: one with 28 MRIs representing common\nsequence types, achieving an average Dice Similarity Coefficient (DSC) of\n88.45%, and another with 18 MRIs featuring less frequent sequences and\nabnormalities such as muscular atrophy, hardware, and significant noise,\nachieving 86.21% DSC. These results demonstrate the feasibility of a fully\nautomated deep learning algorithm for segmenting muscles on MRI across diverse\nsettings. The public release of this model enables consistent, reproducible\nresearch into the relationship between musculature and health."}
{"id": "2506.23437", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07", "E.1; H.1; I.2; I.5; J.2; K.4; C.4"], "pdf": "https://arxiv.org/pdf/2506.23437", "abs": "https://arxiv.org/abs/2506.23437", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi", "Fabio Graziosi"], "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection", "comment": "pre-print (submitted to the IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing)", "summary": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications."}
{"id": "2506.22789", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22789", "abs": "https://arxiv.org/abs/2506.22789", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Kaan Kale", "Sandeep P. Chinchali", "Sriram Vishwanath"], "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "comment": "5 pages, 4 figures, Published at The Proceedings of Interspeech 2025,\n  code is available at http://www.github.com/UTAustin-SwarmLab/WavShape", "summary": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems."}
{"id": "2506.22468", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22468", "abs": "https://arxiv.org/abs/2506.22468", "authors": ["Konstantinos Koutras", "Agorakis Bompotas", "Constantinos Halkiopoulos", "Athanasios Kalogeras", "Christos Alexakos"], "title": "Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting", "comment": "Version of submitted paper on 2023 IEEE International Smart Cities\n  Conference (ISC2), 1-6, 2023", "summary": "The Internet of Things (IoT) plays a major role today in smart building\ninfrastructures, from simple smart-home applications, to more sophisticated\nindustrial type installations. The vast amounts of data generated from relevant\nsystems can be processed in different ways revealing important information.\nThis is especially true in the era of edge computing, when advanced data\nanalysis and decision-making is gradually moving to the edge of the network\nwhere devices are generally characterised by low computing resources. In this\ncontext, one of the emerging main challenges is related to maintaining data\nanalysis accuracy even with less data that can be efficiently handled by low\nresource devices. The present work focuses on correlation analysis of data\nretrieved from a pilot IoT network installation monitoring a small smart office\nby means of environmental and energy consumption sensors. The research\nmotivation was to find statistical correlation between the monitoring variables\nthat will allow the use of machine learning (ML) prediction algorithms for\nenergy consumption reducing input parameters. For this to happen, a series of\nhypothesis tests for the correlation of three different environmental variables\nwith the energy consumption were carried out. A total of ninety tests were\nperformed, thirty for each pair of variables. In these tests, p-values showed\nthe existence of strong or semi-strong correlation with two environmental\nvariables, and of a weak correlation with a third one. Using the proposed\nmethodology, we manage without examining the entire data set to exclude weak\ncorrelated variables while keeping the same score of accuracy."}
{"id": "2506.23582", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23582", "abs": "https://arxiv.org/abs/2506.23582", "authors": ["Yusuke Kanamori", "Yuki Okamoto", "Taisei Takano", "Shinnosuke Takamichi", "Yuki Saito", "Hiroshi Saruwatari"], "title": "RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio", "comment": "Accepted to INTERSPEECH2025", "summary": "In text-to-audio (TTA) research, the relevance between input text and output\naudio is an important evaluation aspect. Traditionally, it has been evaluated\nfrom both subjective and objective perspectives. However, subjective evaluation\nis costly in terms of money and time, and objective evaluation is unclear\nregarding the correlation to subjective evaluation scores. In this study, we\nconstruct RELATE, an open-sourced dataset that subjectively evaluates the\nrelevance. Also, we benchmark a model for automatically predicting the\nsubjective evaluation score from synthesized audio. Our model outperforms a\nconventional CLAPScore model, and that trend extends to many sound categories."}
{"id": "2506.22810", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.22810", "abs": "https://arxiv.org/abs/2506.22810", "authors": ["Shiyao Wang", "Jiaming Zhou", "Shiwan Zhao", "Yong Qin"], "title": "A Self-Training Approach for Whisper to Enhance Long Dysarthric Speech Recognition", "comment": "accepted by Interspeech 2025", "summary": "Dysarthric speech recognition (DSR) enhances the accessibility of smart\ndevices for dysarthric speakers with limited mobility. Previously, DSR research\nwas constrained by the fact that existing datasets typically consisted of\nisolated words, command phrases, and a limited number of sentences spoken by a\nfew individuals. This constrained research to command-interaction systems and\nspeaker adaptation. The Speech Accessibility Project (SAP) changed this by\nreleasing a large and diverse English dysarthric dataset, leading to the SAP\nChallenge to build speaker- and text-independent DSR systems. We enhanced the\nWhisper model's performance on long dysarthric speech via a novel self-training\nmethod. This method increased training data and adapted the model to handle\npotentially incomplete speech segments encountered during inference. Our system\nachieved second place in both Word Error Rate and Semantic Score in the SAP\nChallenge."}
{"id": "2506.22469", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22469", "abs": "https://arxiv.org/abs/2506.22469", "authors": ["Chen Shang", "Dinh Thai Hoang", "Jiadong Yu"], "title": "Multi-Modal Beamforming with Model Compression and Modality Generation for V2X Networks", "comment": "13 pages, 6 figures", "summary": "Integrating sensing and communication (ISAC) has emerged as a cornerstone\ntechnology for predictive beamforming in 6G-enabled vehicle-to-everything (V2X)\nnetworks. However, existing ISAC paradigms rely solely on radio frequency (RF)\nsignal, limiting sensing resolution and robustness in V2X environments with\nhigh mobility and multipath interference. Fortunately, the widespread\ndeployment of diverse non-RF sensors such as cameras and LiDAR, along with the\nintegration of artificial intelligence (AI) and communication systems, offers\nnew opportunities to improve the synergy between sensing and communication.\nMotivated by this, this work develops a novel and robust communication\nframework that leverages multi-modal sensing data and advanced AI technologies\nto assist beamforming in dynamic and realistic vehicular scenarios.\nSpecifically, we propose a multi-modal learning framework for predictive\nbeamforming that integrates modality-specific branches and employs hierarchical\nTransformer to capture cross-modal features. By exploiting the intrinsic\ncorrelation between multi-modal sensing data and beamforming decisions, this\ndesign enhances the accuracy and robustness of beamforming in dynamic V2X\nscenarios. To enable practical deployment on resource-constrained edge device\n(i.e., the roadside unit), we then develop a module-aware compression strategy\nthat significantly reduces inference latency while preserving model\nperformance. Furthermore, to address potential modality missing in real-world\nscenarios, we introduce a generative model that is able to reconstruct missing\ninputs from available observations, allowing the framework to operate reliably\neven under incomplete sensing conditions. Extensive simulation results\nconducted on real-world datasets demonstrate that the proposed scheme\nconsistently outperforms existing baselines across various metrics."}
{"id": "2506.23670", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23670", "abs": "https://arxiv.org/abs/2506.23670", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "comment": null, "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration."}
{"id": "2506.23094", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23094", "abs": "https://arxiv.org/abs/2506.23094", "authors": ["Qi He", "Gus Xia", "Ziyu Wang"], "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025", "summary": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines."}
{"id": "2506.22471", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.22471", "abs": "https://arxiv.org/abs/2506.22471", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "Continual Learning for Wireless Channel Prediction", "comment": "Accepted at ICML Workshop on ML4Wireless", "summary": "Modern 5G/6G deployments routinely face cross-configuration handovers--users\ntraversing cells with different antenna layouts, carrier frequencies, and\nscattering statistics--which inflate channel-prediction NMSE by $37.5\\%$ on\naverage when models are naively fine-tuned. The proposed improvement frames\nthis mismatch as a continual-learning problem and benchmarks three adaptation\nfamilies: replay with loss-aware reservoirs, synaptic-importance\nregularization, and memory-free learning-without-forgetting. Across three\nrepresentative 3GPP urban micro scenarios, the best replay and regularization\nschemes cut the high-SNR error floor by up to 2~dB ($\\approx 35\\%$), while even\nthe lightweight distillation recovers up to $30\\%$ improvement over baseline\nhandover prediction schemes. These results show that targeted rehearsal and\nparameter anchoring are essential for handover-robust CSI prediction and\nsuggest a clear migration path for embedding continual-learning hooks into\ncurrent channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can\nbe found at\nhttps://github.com/ahmd-mohsin/continual-learning-channel-prediction.git."}
{"id": "2506.23869", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23869", "abs": "https://arxiv.org/abs/2506.23869", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "comment": "ISMIR (2025)", "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks."}
{"id": "2506.23130", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23130", "abs": "https://arxiv.org/abs/2506.23130", "authors": ["Tao-Tao He", "Martin E. Malandro", "Douglas Shadle"], "title": "The Florence Price Art Song Dataset and Piano Accompaniment Generator", "comment": "8 pages, 4 figures. To appear in the proceedings of ISMIR 2025", "summary": "Florence B. Price was a composer in the early 20th century whose music\nreflects her upbringing in the American South, her African heritage, and her\nWestern classical training. She is noted as the first African-American woman to\nhave a symphony performed by a major orchestra. Her music has recently received\nrenewed attention from both the public and the research community, decades\nafter her death. In addition to other genres, Price was a prolific composer for\nsolo voice and piano. Music historians have documented the existence of 134 art\nsongs and piano/voice arrangements for spirituals and folk songs written by\nPrice. We release a digital catalog of 112 of these works in MuseScore,\nMusicXML, MIDI, and PDF format. We also use this dataset to fine-tune a\nsymbolic music generation model to generate accompaniments to melodies, and we\nconduct a blind listening experiment that shows that accompaniments generated\nby our model are perceived as being reflective of Florence Price's style more\nfrequently than accompaniments generated by a baseline model. We release our\nmodel as the Florence Price Piano Accompaniment Generator alongside our\ndataset."}
{"id": "2506.22472", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22472", "abs": "https://arxiv.org/abs/2506.22472", "authors": ["Dylan Wilson", "Marco Pontin", "Peter Walters", "Perla Maiolino"], "title": "Optical Waveguide-based Spider Web Enables Resilient Impact Detection and Localization", "comment": null, "summary": "Spiders use their webs as multifunctional tools that enable capturing and\nlocalizing prey and more general environmental sensing through vibrations.\nInspired by their biological function, we present a spider web-inspired optical\nwaveguide system for resilient impulse detection and localization. The\nstructure consists of six clear thermoplastic polyurethane (TPU) waveguides\narranged radially and interconnected by a spiral TPU thread, mimicking orb\nspider webs. Light transmission losses, induced by vibrations, are measured via\ncoupled LEDs and photo-diodes, allowing real-time detection. We systematically\ncharacterize individual waveguides, analyzing key parameters such as tension,\nimpulse position, and break angle to optimize vibrational response. The\ncomplete system is validated through controlled experiments, revealing a 5 ms\npropagation delay in vibration transfer between adjacent radii, enhancing\nlocalization capabilities. We demonstrate a robust impulse detection and\nlocalization algorithm leveraging time delay analysis, achieving reliable event\nidentification even in cases of sensor failure. This study highlights the\npotential of bioinspired optical waveguide structures for adaptive sensing,\nwith applications in soft robotics, structural monitoring, and environmental\nsensing."}
{"id": "2506.23873", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23873", "abs": "https://arxiv.org/abs/2506.23873", "authors": ["Yuexuan Kong", "Gabriel Meseguer-Brocal", "Vincent Lostanlen", "Mathieu Lagrange", "Romain Hennequin"], "title": "Emergent musical properties of a transformer under contrastive self-supervised learning", "comment": "Accepted at ISMIR 2025", "summary": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR."}
{"id": "2506.23325", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23325", "abs": "https://arxiv.org/abs/2506.23325", "authors": ["Yitian Gong", "Luozhijie Jin", "Ruifan Deng", "Dong Zhang", "Xin Zhang", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "comment": null, "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer."}
{"id": "2506.22476", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.LG", "q-bio.NC", "I.2.6; J.3; H.1.2"], "pdf": "https://arxiv.org/pdf/2506.22476", "abs": "https://arxiv.org/abs/2506.22476", "authors": ["A. Subedi", "S. De", "L. Cavuoto", "S. Schwaitzberg", "M. Hackett", "J. Norfleet"], "title": "An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals", "comment": null, "summary": "Objective skill assessment in high-stakes procedural environments requires\nmodels that not only decode underlying cognitive and motor processes but also\ngeneralize across tasks, individuals, and experimental contexts. While prior\nwork has demonstrated the potential of functional near-infrared spectroscopy\n(fNIRS) for evaluating cognitive-motor performance, existing approaches are\noften task-specific, rely on extensive preprocessing, and lack robustness to\nnew procedures or conditions. Here, we introduce an interpretable\ntransformer-based foundation model trained on minimally processed fNIRS signals\nfor cross-procedural skill assessment. Pretrained using self-supervised\nlearning on data from laparoscopic surgical tasks and endotracheal intubation\n(ETI), the model achieves greater than 88% classification accuracy on all\ntasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It\ngeneralizes to a novel emergency airway procedure--cricothyrotomy--using fewer\nthan 30 labeled samples and a lightweight (less than 2k parameter) adapter\nmodule, attaining an AUC greater than 87%. Interpretability is achieved via a\nnovel channel attention mechanism--developed specifically for fNIRS--that\nidentifies functionally coherent prefrontal sub-networks validated through\nablation studies. Temporal attention patterns align with task-critical phases\nand capture stress-induced changes in neural variability, offering insight into\ndynamic cognitive states."}
{"id": "2506.23986", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23986", "abs": "https://arxiv.org/abs/2506.23986", "authors": ["Dake Guo", "Jixun Yao", "Linhan Ma", "Wang He", "Lei Xie"], "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding", "comment": null, "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}"}
{"id": "2506.23367", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23367", "abs": "https://arxiv.org/abs/2506.23367", "authors": ["Paige Tuttösí", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals."}
{"id": "2506.22488", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22488", "abs": "https://arxiv.org/abs/2506.22488", "authors": ["Xi Fu", "Weibang Jiang", "Rui Liu", "Gernot R. Müller-Putz", "Cuntai Guan"], "title": "Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning", "comment": null, "summary": "Accurate decoding of lower-limb motion from EEG signals is essential for\nadvancing brain-computer interface (BCI) applications in movement intent\nrecognition and control. However, challenges persist in achieving causal,\nphase-consistent predictions and in modeling both inter- and intra-subject\nvariability. To address these issues, we propose NeuroDyGait, a\ndomain-generalizable EEG-to-motion decoding framework that leverages structured\ncontrastive representation learning and relational domain modeling. The\nproposed method employs relative contrastive learning to achieve semantic\nalignment between EEG and motion embeddings. Furthermore, a multi-cycle gait\nreconstruction objective is introduced to enforce temporal coherence and\nmaintain biomechanical consistency. To promote inter-session generalization,\nduring fine-tuning, a domain dynamic decoding mechanism adaptively assigns\nsession-specific prediction heads and learns to mix their outputs based on\ninter-session relationships. NeuroDyGait enables zero-shot motion prediction\nfor unseen individuals without requiring adaptation and achieves superior\nperformance in cross-subject gait decoding on benchmark datasets. Additionally,\nit demonstrates strong phase-detection capabilities even without explicit phase\nsupervision during training. These findings highlight the potential of\nrelational domain learning in enabling scalable, target-free deployment of\nBCIs."}
{"id": "2506.22646", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.22646", "abs": "https://arxiv.org/abs/2506.22646", "authors": ["Weiqing Wang", "Taejin Park", "Ivan Medennikov", "Jinhan Wang", "Kunal Dhawan", "He Huang", "Nithin Rao Koluguri", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR", "comment": "Accepted by INTERSPEECH 2025", "summary": "We propose a self-speaker adaptation method for streaming multi-talker\nautomatic speech recognition (ASR) that eliminates the need for explicit\nspeaker queries. Unlike conventional approaches requiring target speaker\nembeddings or enrollment audio, our technique dynamically adapts individual ASR\ninstances through speaker-wise speech activity prediction. The key innovation\ninvolves injecting speaker-specific kernels generated via speaker supervision\nactivations into selected ASR encoder layers. This enables instantaneous\nspeaker adaptation to target speakers while handling fully overlapped speech\neven in a streaming scenario. Experiments show state-of-the-art performance in\nboth offline and streaming scenarios, demonstrating that our self-adaptive\nmethod effectively addresses severe speech overlap through streamlined\nspeaker-focused recognition. The results validate the proposed self-speaker\nadaptation approach as a robust solution for multi-talker ASR under severe\noverlapping speech conditions."}
{"id": "2506.23437", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07", "E.1; H.1; I.2; I.5; J.2; K.4; C.4"], "pdf": "https://arxiv.org/pdf/2506.23437", "abs": "https://arxiv.org/abs/2506.23437", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi", "Fabio Graziosi"], "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection", "comment": "pre-print (submitted to the IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing)", "summary": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications."}
{"id": "2506.22490", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22490", "abs": "https://arxiv.org/abs/2506.22490", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu"], "title": "MENGLAN: Multiscale Enhanced Nonparametric Gas Analyzer with Lightweight Architecture and Networks", "comment": null, "summary": "Accurate detection of ethylene concentrations in mixed gases is crucial in\nchemical production for safety and health purposes. Traditional methods are\nhindered by high cost and complexity, limiting their practical application.\nThis study proposes MENGLAN, a Multiscale Enhanced Nonparametric Gas Analyzer\nthat integrates a dual-stream structure, a Hybrid Multi-Head Attention\nmechanism, and a Feature Reactivation Module to enable real-time, lightweight,\nand high-precision ethylene concentration prediction. Results show that MENGLAN\nachieves superior performance, reduced computational demand, and enhanced\ndeployability compared to existing methods."}
{"id": "2506.23371", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23371", "abs": "https://arxiv.org/abs/2506.23371", "authors": ["Frank Cwitkowitz", "Zhiyao Duan"], "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation", "comment": "Accepted to ISMIR 2025", "summary": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of\nMusic Information Retrieval (MIR) systems, and is critical for many\napplications and downstream tasks involving pitch, including music\ntranscription. However, existing methods are largely based on supervised\nlearning, and there are significant challenges in collecting annotated data for\nthe task. Recently, self-supervised techniques exploiting intrinsic properties\nof pitch and harmonic signals have shown promise for both monophonic and\npolyphonic pitch estimation, but these still remain inferior to supervised\nmethods. In this work, we extend the classic supervised MPE paradigm by\nincorporating several self-supervised objectives based on pitch-invariant and\npitch-equivariant properties. This joint training results in a substantial\nimprovement under closed training conditions, which naturally suggests that\napplying the same objectives to a broader collection of data will yield further\nimprovements. However, in doing so we uncover a phenomenon whereby our model\nsimultaneously overfits to the supervised data while degenerating on data used\nfor self-supervision only. We demonstrate and investigate this and offer our\ninsights on the underlying problem."}
{"id": "2506.23582", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23582", "abs": "https://arxiv.org/abs/2506.23582", "authors": ["Yusuke Kanamori", "Yuki Okamoto", "Taisei Takano", "Shinnosuke Takamichi", "Yuki Saito", "Hiroshi Saruwatari"], "title": "RELATE: Subjective evaluation dataset for automatic evaluation of relevance between text and audio", "comment": "Accepted to INTERSPEECH2025", "summary": "In text-to-audio (TTA) research, the relevance between input text and output\naudio is an important evaluation aspect. Traditionally, it has been evaluated\nfrom both subjective and objective perspectives. However, subjective evaluation\nis costly in terms of money and time, and objective evaluation is unclear\nregarding the correlation to subjective evaluation scores. In this study, we\nconstruct RELATE, an open-sourced dataset that subjectively evaluates the\nrelevance. Also, we benchmark a model for automatically predicting the\nsubjective evaluation score from synthesized audio. Our model outperforms a\nconventional CLAPScore model, and that trend extends to many sound categories."}
{"id": "2506.22495", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22495", "abs": "https://arxiv.org/abs/2506.22495", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "comment": null, "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly."}
{"id": "2506.23553", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23553", "abs": "https://arxiv.org/abs/2506.23553", "authors": ["Taisei Takano", "Yuki Okamoto", "Yusuke Kanamori", "Yuki Saito", "Ryotaro Nagase", "Hiroshi Saruwatari"], "title": "Human-CLAP: Human-perception-based contrastive language-audio pretraining", "comment": null, "summary": "Contrastive language-audio pretraining (CLAP) is widely used for audio\ngeneration and recognition tasks. For example, CLAPScore, which utilizes the\nsimilarity of CLAP embeddings, has been a major metric for the evaluation of\nthe relevance between audio and text in text-to-audio. However, the\nrelationship between CLAPScore and human subjective evaluation scores is still\nunclarified. We show that CLAPScore has a low correlation with human subjective\nevaluation scores. Additionally, we propose a human-perception-based CLAP\ncalled Human-CLAP by training a contrastive language-audio model using the\nsubjective evaluation score. In our experiments, the results indicate that our\nHuman-CLAP improved the Spearman's rank correlation coefficient (SRCC) between\nthe CLAPScore and the subjective evaluation scores by more than 0.25 compared\nwith the conventional CLAP."}
{"id": "2506.23670", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23670", "abs": "https://arxiv.org/abs/2506.23670", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "comment": null, "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration."}
{"id": "2506.22549", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22549", "abs": "https://arxiv.org/abs/2506.22549", "authors": ["Omar Barrera", "Jack Kramer", "Lezli Matto", "Vakhtang Chulukhadze", "Sinwoo Cho", "Michael Liao", "Mark S. Goorsky", "Ruochen Lu"], "title": "50 GHz Piezoelectric Acoustic Filter", "comment": "8 pages, 10 Figures", "summary": "This paper presents significant frequency scaling of acoustic filter\ntechnology to 50 GHz. This achievement is enabled by the P3F LiNbO3 multilayer\nstack, in which piezoelectric thin-films of alternating orientations are\ntransferred in sequence, thereby allowing efficient exploitation of high-order\nmodes with high quality factor (Q) and coupling coefficient (k2) in a thicker\npiezoelectric stack. The demonstrated filter is comprised of twelfth-order\nsymmetric (S12) mode lateral-field-excited bulk acoustic wave resonators\n(XBARs), built on a 4-layer periodically poled piezoelectric (P3F) 128 Y-cut\nlithium niobate (LiNbO3) stack. The filter exhibits 3.3 dB insertion loss (IL)\nand a fractional bandwidth (FBW) of 2.9%. The miniature design, with a\nfootprint of 0.36 mm2, makes it promising for future wireless front-end\napplications. These results represent the highest frequency acoustic filters\nreported to date, setting a new benchmark in piezoelectric filter technology.\nUpon further development, the platform could enable filters further into the\nFR2 range, essential for next-generation communication systems."}
{"id": "2506.23859", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23859", "abs": "https://arxiv.org/abs/2506.23859", "authors": ["Chenda Li", "Wangyou Zhang", "Wei Wang", "Robin Scheibler", "Kohei Saijo", "Samuele Cornell", "Yihui Fu", "Marvin Sach", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "Less is More: Data Curation Matters in Scaling Speech Enhancement", "comment": "Submitted to ASRU2025", "summary": "The vast majority of modern speech enhancement systems rely on data-driven\nneural network models. Conventionally, larger datasets are presumed to yield\nsuperior model performance, an observation empirically validated across\nnumerous tasks in other domains. However, recent studies reveal diminishing\nreturns when scaling speech enhancement data. We focus on a critical factor:\nprevalent quality issues in ``clean'' training labels within large-scale\ndatasets. This work re-examines this phenomenon and demonstrates that, within\nlarge-scale training sets, prioritizing high-quality training data is more\nimportant than merely expanding the data volume. Experimental findings suggest\nthat models trained on a carefully curated subset of 700 hours can outperform\nmodels trained on the 2,500-hour full dataset. This outcome highlights the\ncrucial role of data curation in scaling speech enhancement systems\neffectively."}
{"id": "2506.23869", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23869", "abs": "https://arxiv.org/abs/2506.23869", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "comment": "ISMIR (2025)", "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks."}
{"id": "2506.22796", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22796", "abs": "https://arxiv.org/abs/2506.22796", "authors": ["Ruolin Du", "Zhiqiang Wei", "Zai Yang", "Lei Yang", "Yong Zeng", "Derrick Wing Kwan Ng", "Jinhong Yuan"], "title": "Channel Knowledge Map-assisted Dual-domain Tracking and Predictive Beamforming for High-Mobility Wireless Networks", "comment": null, "summary": "This paper introduces a novel channel knowledge map (CKM)-assisted\ndual-domain tracking and predictive beamforming scheme for high-mobility\nwireless networks. The central premise is that the CKM integrates both the\ncoordinate and beam domains, thereby enabling tracking in one domain via\ntreating the other domain's input as priors or measurements. In the coordinate\ndomain (C-Domain), an extended Kalman filter (EKF) is employed to predict and\ntrack the state (i.e., location and velocity) of a moving communication\nreceiver across time slots under both line-of-sight (LoS)-present and\nLoS-absent conditions, where the CKM provides a prior mapping from multipath\nchannel parameters to potential target locations. In the beam domain\n(B-Domain), the updated location of the receiver is fed back to CKM to offer a\npriori information of angle of arrival (AoA) variations, which are incorporated\nto establish beam transition models for effective beam tracking, depending on\nthe angular variation situation of each path. Then, we analyze the Cram\\'er-Rao\nBound (CRB) for AoA estimation for each path in the considered system and\npropose a jointly predictive beamforming and power allocation design to\nminimize AoA estimation errors, directly enhancing multipath beam tracking\naccuracy and indirectly improving target tracking performance. Simulation\nresults demonstrate that the proposed scheme achieves significant improvements\nin both target and beam tracking performance compared to the state-of-the-art\napproaches, particularly in AoA tracking of non-line-of-sight (NLoS) paths,\nhighlighting the potential gain of CKM in facilitating both target and beam\ntracking in high-mobility communications."}
{"id": "2506.23874", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.23874", "abs": "https://arxiv.org/abs/2506.23874", "authors": ["Jiahe Wang", "Chenda Li", "Wei Wang", "Wangyou Zhang", "Samuele Cornell", "Marvin Sach", "Robin Scheibler", "Kohei Saijo", "Yihui Fu", "Zhaoheng Ni", "Anurag Kumar", "Tim Fingscheidt", "Shinji Watanabe", "Yanmin Qian"], "title": "URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech Enhancement Competition", "comment": "Submitted to ASRU2025", "summary": "The Mean Opinion Score (MOS) is fundamental to speech quality assessment.\nHowever, its acquisition requires significant human annotation. Although deep\nneural network approaches, such as DNSMOS and UTMOS, have been developed to\npredict MOS to avoid this issue, they often suffer from insufficient training\ndata. Recognizing that the comparison of speech enhancement (SE) systems\nprioritizes a reliable system comparison over absolute scores, we propose\nURGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK\ntakes homologous enhanced speech pairs as input to predict relative quality\nrankings. This pairwise paradigm efficiently utilizes limited training data, as\nall pairwise permutations of multiple systems constitute a training instance.\nExperiments across multiple open test sets demonstrate URGENT-PK's superior\nsystem-level ranking performance over state-of-the-art baselines, despite its\nsimple network architecture and limited training data."}
{"id": "2506.23873", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23873", "abs": "https://arxiv.org/abs/2506.23873", "authors": ["Yuexuan Kong", "Gabriel Meseguer-Brocal", "Vincent Lostanlen", "Mathieu Lagrange", "Romain Hennequin"], "title": "Emergent musical properties of a transformer under contrastive self-supervised learning", "comment": "Accepted at ISMIR 2025", "summary": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR."}
{"id": "2506.22824", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22824", "abs": "https://arxiv.org/abs/2506.22824", "authors": ["Lingyun Xu", "Bowen Wang", "Huiyong Li", "Ziyang Cheng"], "title": "Sensing Security Oriented OFDM-ISAC Against Multi-Intercept Threats", "comment": null, "summary": "In recent years, security has emerged as a critical aspect of integrated\nsensing and communication (ISAC) systems. While significant research has\nfocused on secure communications, particularly in ensuring physical layer\nsecurity, the issue of sensing security has received comparatively less\nattention. This paper addresses the sensing security problem in ISAC,\nparticularly under the threat of multi-intercept adversaries. We consider a\nrealistic scenario in which the sensing target is an advanced electronic\nreconnaissance aircraft capable of employing multiple signal interception\ntechniques, such as power detection (PD) and cyclostationary analysis (CA). To\nevaluate sensing security under such sophisticated threats, we analyze two\ncritical features of the transmitted signal: (i) power distribution and (ii)\ncyclic spectrum. Further, we introduce a novel ergodic cyclic spectrum metric\nwhich leverages the intrinsic mathematical structure of cyclostationary signals\nto more comprehensively characterize their behavior. Building on this analysis,\nwe formulate a new ISAC design problem that explicitly considers sensing\nsecurity, and we develop a low-complexity, efficient optimization approach to\nsolve it. Simulation results demonstrate that the proposed metric is both\neffective and insightful, and that our ISAC design significantly enhances\nsensing security performance in the presence of multi-intercept threats."}
{"id": "2506.23986", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.23986", "abs": "https://arxiv.org/abs/2506.23986", "authors": ["Dake Guo", "Jixun Yao", "Linhan Ma", "Wang He", "Lei Xie"], "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention Mask for Speech Token Decoding", "comment": null, "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}"}
{"id": "2506.22844", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.22844", "abs": "https://arxiv.org/abs/2506.22844", "authors": ["Navid Keshtiarast", "Marina Petrova"], "title": "Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band", "comment": "Accepted for Publication in ICNS3 2025", "summary": "The ever-increasing demand for broadband and IoT wireless connectivity has\nrecently urged the regulators around the world to start opening the 6 GHz\nspectrum for unlicensed use. These bands will, for example, permit the use of\nadditional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access\ntechnologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To\nsupport QoS-sensitive applications with both technologies, fair and efficient\ncoexistence approaches between the two RATs, as well as with incumbents already\noperating in the 6 GHz band, are crucial. In this paper, we study through\nextensive simulations the achievable mean downlink throughput of both Wi-Fi 6E\nAPs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario\nunder high-interference conditions. We also explore how different parameter\nsettings e.g., MAC frame aggregation, energy detection threshold and maximum\nchannel occupancy time (MCOT) affect the coexistence. Our findings give\nimportant insights into how to tune the key parameters to design fair\ncoexistence policies."}
{"id": "2506.22903", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.22903", "abs": "https://arxiv.org/abs/2506.22903", "authors": ["Weicong Chen", "Jiajia Guo", "Yiming Cui", "Xiao Li", "Shi Jin"], "title": "Limited Feedback in RIS-Assisted Wireless Communications: Use Cases, Challenges, and Future Directions", "comment": "This work has been submitted for possible publication", "summary": "Channel state information (CSI) is essential to unlock the potential of\nreconfigurable intelligent surfaces (RISs) in wireless communication systems.\nSince massive RIS elements are typically implemented without baseband signal\nprocessing capabilities, limited CSI feedback is necessary when designing the\nreflection/refraction coefficients of the RIS. In this article, the unique\nRIS-assisted channel features, such as the RIS position-dependent channel\nfluctuation, the ultra-high dimensional sub-channel matrix, and the structured\nsparsity, are distilled from recent advances in limited feedback and used as\nguidelines for designing feedback schemes. We begin by illustrating the use\ncases and the corresponding challenges associated with RIS feedback. We then\ndiscuss how to leverage techniques such as channel customization,\nstructured-sparsity, autoencoders, and others to reduce feedback overhead and\ncomplexity when devising feedback schemes. Finally, we identify potential\nresearch directions by considering the unresolved challenges, the new RIS\narchitecture, and the integration with multi-modal information and artificial\nintelligence."}
{"id": "2506.22935", "categories": ["eess.SP", "cs.LG", "cs.NA", "math.NA", "94A12, 65T50, 68T05", "F.2.1; I.2.6; G.1.0"], "pdf": "https://arxiv.org/pdf/2506.22935", "abs": "https://arxiv.org/abs/2506.22935", "authors": ["Marc Bara Iniesta"], "title": "Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation", "comment": "16 pages, 4 figures, source code available at\n  https://github.com/marcbara/graf-psl-lpi (DOI: 10.5281/zenodo.15763301)", "summary": "The ambiguity function is fundamental to radar waveform design,\ncharacterizing range and Doppler resolution capabilities. However, its\ntraditional formulation involves non-differentiable operations, preventing\nintegration with gradient-based optimization methods and modern machine\nlearning frameworks. This paper presents the first complete mathematical\nframework and computational implementation for differentiable radar ambiguity\nfunctions. Our approach addresses the fundamental technical challenges that\nhave prevented the radar community from leveraging automatic differentiation:\nproper handling of complex-valued gradients using Wirtinger calculus, efficient\ncomputation through parallelized FFT operations, numerical stability throughout\ncascaded operations, and composability with arbitrary differentiable\noperations. We term this approach GRAF (Gradient-based Radar Ambiguity\nFunctions), which reformulates the ambiguity function computation to maintain\nmathematical equivalence while enabling gradient flow through the entire\npipeline. The resulting implementation provides a general-purpose\ndifferentiable ambiguity function compatible with modern automatic\ndifferentiation frameworks, enabling new research directions including neural\nnetwork-based waveform generation with ambiguity constraints, end-to-end\noptimization of radar systems, and integration of classical radar theory with\nmodern deep learning. We provide complete implementation details and\ndemonstrate computational efficiency suitable for practical applications. This\nwork establishes the mathematical and computational foundation for applying\nmodern machine learning techniques to radar waveform design, bridging classical\nradar signal processing with automatic differentiation frameworks."}
{"id": "2506.22943", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.22943", "abs": "https://arxiv.org/abs/2506.22943", "authors": ["Siyun Liang", "Chen Zhu", "Zhaohui Yang", "Changsheng You", "Dusit Niyato", "Kai-Kit Wong", "Zhaoyang Zhang"], "title": "Rate Maximization for Fluid Antenna System Assisted Semantic Communication", "comment": null, "summary": "In this paper, we investigate the problem of rate maximization in a fluid\nantenna system (FAS) assisted\n  semantic communication system. In the considered model, a base station (BS)\nwith multiple static antennas employs semantic extraction techniques to\ncompress the data ready to be sent to a user. The user equipped with a fluid\nantenna is located in the near field coverage region of the BS. Our aim is to\njointly optimize the transmit beamforming and the semantic compression rate at\nthe BS, as well as the selection of activated ports in FAS, to maximize the\nequivalent transmission ratio under a specific power budget. We design an\nalternating algorithm to solve the problem, where we obtain the optimal\nsemantic compression ratio is in closed form at each step. Simulation results\nvalidate the effectiveness of the proposed algorithm."}
{"id": "2506.23045", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.23045", "abs": "https://arxiv.org/abs/2506.23045", "authors": ["Saif Khan Mohammed", "Sandesh Rao Mattu", "Nishant Mehrotra", "Venkatesh Khammammetti", "Robert Calderbank"], "title": "Zak-OFDM: Low Complexity Joint Equalization of OFDM Carriers in Doubly-Spread Channels", "comment": null, "summary": "We communicate over wireless channels by first estimating and then equalizing\nthe effective channel. In Zak-OTFS (orthogonal time frequency space) modulation\nthe carrier waveform is a pulse in the delay-Doppler (DD) domain, formally a\nquasi-periodic localized function with specific periods along delay and\nDoppler. When the channel delay spread is less than the delay period, and the\nchannel Doppler spread is less than the Doppler period, the response to a\nsingle Zak-OTFS carrier provides an image of the scattering environment and can\nbe used to predict the effective channel at all other carriers. This makes\nchannel estimation straightforward, and there is no loss in spectral efficiency\nsince it is possible to design data and pilot signals that are mutually\nunbiased. However, the naive approach to equalization has complexity ${\\mathcal\nO}(M^3N^3)$ where $M$ and $N$ are respectively the number of delay and Doppler\nbins in an OTFS frame. We simplify equalization by transforming Zak-OTFS\ninformation symbols to CP-OFDM (cyclic prefix orthogonal frequency division\nmultiplexing) modulation.\n  Why not simply communicate with CP-OFDM? Inter-carrier interference (ICI) in\nCP-OFDM makes it is very challenging to acquire the complete frequency domain\n(FD) channel response between subcarriers in the presence of mobility and delay\nspread. We avoid this difficulty by estimating the effective channel in the DD\ndomain from which we are able to reconstruct the complete FD channel response.\nWe take advantage of CP-OFDM to design an ${\\mathcal O}(M^2N^2)$ low-complexity\nmethod of jointly equalizing all subcarriers, where $MN$ is the number of\nsubcarriers. Our approach removes the need for traditional pilots in CP-OFDM\nand reduces the need to vary carrier spacing with mobility."}
{"id": "2506.23118", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23118", "abs": "https://arxiv.org/abs/2506.23118", "authors": ["Liping Bai", "Yu Ge", "Henk Wymeersch"], "title": "Belief Propagation-based Target Handover in Distributed Integrated Sensing and Communication", "comment": null, "summary": "Distributed integrated sensing and communication (DISAC) systems are key\nenablers for 6G networks, offering the capability to jointly track multiple\ntargets using spatially distributed base stations (BSs). A fundamental\nchallenge in DISAC is the seamless and efficient handover of target tracks\nbetween BSs with partially overlapping fields of view, especially in dense and\ndynamic environments. In this paper, we propose a novel target handover\nframework based on belief propagation (BP) for multi-target tracking in DISAC\nsystems. By representing the probabilistic data association and tracking\nproblem through a factor graph, the proposed method enables efficient marginal\ninference with reduced computational complexity. Our framework introduces a\nprincipled handover criterion and message-passing strategy that minimizes\ninter-BS communication while maintaining tracking continuity and accuracy. We\ndemonstrate that the proposed handover procedure achieves performance\ncomparable to centralized processing, yet significantly reduces data exchange\nand processing overhead. Extensive simulations validate the robustness of the\napproach in urban tracking scenarios with closely spaced targets."}
{"id": "2506.23203", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23203", "abs": "https://arxiv.org/abs/2506.23203", "authors": ["Feng Shu", "Jiatong Bai", "Di Wu", "Wei Zhu", "Bin Deng", "Fuhui Zhou", "Jiangzhou Wang"], "title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver", "comment": null, "summary": "As a green MIMO structure, massive H$^2$AD is viewed as a potential\ntechnology for the future 6G wireless network. For such a structure, it is a\nchallenging task to design a low-complexity and high-performance fusion of\ntarget direction values sensed by different sub-array groups with fewer use of\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\nCRLB computation. This reduces complexity and prior knowledge dependence while\npreserving fusion performance. Moreover, a multi-branch deep neural network\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\nleveraging candidate angles from multiple subarrays. The subarray-specific\nbranch networks are integrated with a shared regression module to effectively\neliminate pseudo-solutions and fuse true angles. Simulation results show that\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\nto CRLB-based methods, while significantly reducing the reliance on prior\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\nestimation accuracy compared to CRLB-ratio-WF method."}
{"id": "2506.23368", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23368", "abs": "https://arxiv.org/abs/2506.23368", "authors": ["Istiaq Ahmed", "Md Asif Ul Hoq Khan", "MD Zahedul Islam", "Md Sakibul Hasan", "Tanaya Jakir", "Arat Hossain", "Joynal Abed", "Muhammad Hasanuzzaman", "Sadia Sharmeen Shatyi", "Kazi Nehal Hasnain"], "title": "Optimizing Solar Energy Production in the USA: Time-Series Analysis Using AI for Smart Energy Management", "comment": null, "summary": "As the US rapidly moves towards cleaner energy sources, solar energy is fast\nbecoming the pillar of its renewable energy mix. Even while solar energy is\nincreasingly being used, its variability is a key hindrance to grid stability,\nstorage efficiency, and system stability overall. Solar energy has emerged as\none of the fastest-growing renewable energy sources in the United States,\nadding noticeably to the country's energy mix. Retrospectively, the necessity\nof inserting the sun's energy into the grid without disrupting reliability and\ncost efficiencies highlights the necessity of good forecasting software and\nsmart control systems. The dataset utilized for this research project comprised\nboth hourly and daily solar energy production records collected from multiple\nutility-scale solar farms across diverse U.S. regions, including California,\nTexas, and Arizona. Training and evaluation of all models were performed with a\ntime-based cross-validation scheme, namely, sliding window validation. Both the\nRandom Forest and the XG-Boost models demonstrated noticeably greater and the\nsame performance across each of the measures considered, with relatively high\naccuracy. The almost perfect and equal performance by the Random Forest and\nXG-Boost models also shows both models to have learned the patterns in the data\nvery comprehensively, with high reliability in their predictions. By\nincorporating AI-powered time-series models like XG-Boost in grid management\nsoftware, utility companies can dynamically modify storage cycles in real-time\nas well as dispatch and peak load planning, based on their predictions.\nAI-powered solar forecasting also has profound implications for renewable\nenergy policy and planning, particularly as U.S. federal and state governments\naccelerate toward ambitious decarbonization goals."}
{"id": "2506.23410", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23410", "abs": "https://arxiv.org/abs/2506.23410", "authors": ["Byunghyun Lee", "Rang Liu", "David J. Love", "James V. Krogmeier", "A. Lee Swindlehurst"], "title": "Integrated Polarimetric Sensing and Communication with Polarization-Reconfigurable Arrays", "comment": null, "summary": "Polarization diversity offers a cost- and space-efficient solution to enhance\nthe performance of integrated sensing and communication systems. Polarimetric\nsensing exploits the signal's polarity to extract details about the target such\nas shape, pose, and material composition. From a communication perspective,\npolarization diversity can enhance the reliability and throughput of\ncommunication channels. This paper proposes an integrated polarimetric sensing\nand communication (IPSAC) system that jointly conducts polarimetric sensing and\ncommunications. We study the use of single-port polarization-reconfigurable\nantennas to adapt to channel depolarization effects, without the need for\nseparate RF chains for each polarization. We address the problem of optimizing\nwaveforms and polarizations based on two sensing metrics. We first consider\nminimizing the mean square error (MSE) of the target depolarization parameter\nestimate, which is a critical task for various polarimetric radar applications\nsuch as rainfall forecasting, vegetation identification, and target\nclassification. To address this nonconvex problem, we apply semi-definite\nrelaxation (SDR) and majorization-minimization (MM) optimization techniques.\nNext, we consider a design that maximizes the target\nsignal-to-interference-plus-noise ratio (SINR) leveraging prior knowledge of\nthe target and clutter depolarization statistics to enhance the target\ndetection performance. To tackle this problem, we modify the solution developed\nfor MSE minimization subject to the same quality-of-service (QoS) constraints.\nExtensive simulations show that the proposed polarization reconfiguration\nmethod substantially improves the depolarization parameter MSE. Furthermore,\nthe proposed method considerably boosts the target SINR due to polarization\ndiversity, particularly in cluttered environments."}
{"id": "2506.23432", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23432", "abs": "https://arxiv.org/abs/2506.23432", "authors": ["Mohammad Taghi Dabiri", "Mazen Hasna", "Saud Althunibat", "Khalid Qaraqe"], "title": "All-Optical Inter-Satellite Relays with Intelligent Beam Control: Harnessing Liquid Lenses and Optical Hard Limiters", "comment": null, "summary": "Low Earth orbit (LEO) satellite constellations are emerging as a key enabler\nof next-generation communications, offering global coverage and significantly\nlower latency compared to traditional terrestrial networks and geostationary\nsatellites. However, further latency reduction is essential for time-critical\napplications such as real-time sensing, autonomous systems, and interactive\nservices. One critical bottleneck is the optical-to-electrical (O/E) and\nelectrical-to-optical (E/O) conversions at intermediate nodes in multi-hop\nlinks, which introduce unwanted processing delays. To address this, we\ninvestigate an all-optical relay system based on Optical Hard Limiters (OHL),\nwhich operate purely in the optical domain to suppress noise and restore signal\nquality without requiring O/E conversions. First, we present a rigorous\nanalysis of inter-satellite multi-relay communication under the OHL relaying\narchitecture, comparing it against conventional Amplify-and-Forward (AF) and\nDecode-and-Forward (DF) schemes. Through this comparison, we highlight both the\nadvantages and limitations of OHL relays, including their particular\nsensitivity to parameter choices such as the threshold setting and divergence\nangle at the transmitter. Recognizing that a LEO constellation is inherently\ntime-varying - satellites move relative to one another, causing continuous\nchanges in link distances and tracking errors - we propose a joint optimization\nstrategy. This scheme adaptively tunes the OHL decision threshold and beam\ndivergence in real time to maintain optimal performance, ultimately lowering\nerror rates and latency. Extensive simulations in a large-scale LEO network\ndemonstrate the viability of our method and offer insights into practical\nimplementation for next-generation inter-satellite communication systems."}
{"id": "2506.23455", "categories": ["eess.SP", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.23455", "abs": "https://arxiv.org/abs/2506.23455", "authors": ["Jieao Zhu", "Linglong Dai"], "title": "General Signal Model and Capacity Limit for Rydberg Quantum Information System", "comment": "Submitted to TWC. In this paper, we compute the dynamic response of\n  Rydberg atomic receivers by solving the small-signal perturbation solution to\n  quantum master equation. Transfer functions of this quantum receiver is\n  derived, with the instantaneous bandwidths problem and the in-band blackbody\n  radiation noise computed theoretically for the first time", "summary": "Rydberg atomic receivers represent a transformative approach to achieving\nhigh-sensitivity, broadband, and miniaturized radio frequency (RF) reception.\nHowever, existing static signal models for Rydberg atomic receivers rely on the\nsteady-state assumption of atomic quantum states, which cannot fully describe\nthe signal reception process of dynamic signals. To fill in this gap, in this\npaper, we present a general model to compute the dynamic signal response of\nRydberg atomic receivers in closed form. Specifically, by applying small-signal\nperturbation techniques to the quantum master equation, we derive closed-form\nLaplace domain transfer functions that characterize the receiver's dynamic\nresponses to time-varying signal fields. To gain more insights into the\nquantum-based RF-photocurrent conversion process, we further introduce the\nconcept of quantum transconductance that describes the quantum system as an\nequivalent classical system. By applying quantum transconductance, we quantify\nthe influence of in-band blackbody radiation (BBR) noise on the atomic receiver\nsensitivity. Extensive simulations for Rydberg atomic receivers validate the\nproposed signal model, and demonstrate the possibility of quantum receivers to\noutperform classical electronic receivers through the improvement of quantum\ntransconductance."}
{"id": "2506.23472", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23472", "abs": "https://arxiv.org/abs/2506.23472", "authors": ["Ruixu Geng", "Yadong Li", "Dongheng Zhang", "Pengcheng Huang", "Binquan Wang", "Binbin Zhang", "Zhi Lu", "Yang Hu", "Yan Chen"], "title": "Automatic Phase Calibration for High-resolution mmWave Sensing via Ambient Radio Anchors", "comment": "13 pages, 21 figures", "summary": "Millimeter-wave (mmWave) radar systems with large array have pushed radar\nsensing into a new era, thanks to their high angular resolution. However, our\nlong-term experiments indicate that array elements exhibit phase drift over\ntime and require periodic phase calibration to maintain high-resolution,\ncreating an obstacle for practical high-resolution mmWave sensing.\nUnfortunately, existing calibration methods are inadequate for periodic\nrecalibration, either because they rely on artificial references or fail to\nprovide sufficient precision. To address this challenge, we introduce\nAutoCalib, the first framework designed to automatically and accurately\ncalibrate high-resolution mmWave radars by identifying Ambient Radio Anchors\n(ARAs)-naturally existing objects in ambient environments that offer stable\nphase references. AutoCalib achieves calibration by first generating spatial\nspectrum templates based on theoretical electromagnetic characteristics. It\nthen employs a pattern-matching and scoring mechanism to accurately detect\nthese anchors and select the optimal one for calibration. Extensive experiments\nacross 11 environments demonstrate that AutoCalib capable of identifying ARAs\nthat existing methods miss due to their focus on strong reflectors. AutoCalib's\ncalibration performance approaches corner reflectors (74% phase error\nreduction) while outperforming existing methods by 83%. Beyond radar\ncalibration, AutoCalib effectively supports other phase-dependent applications\nlike handheld imaging, delivering 96% of corner reflector calibration\nperformance without artificial references."}
{"id": "2506.23473", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23473", "abs": "https://arxiv.org/abs/2506.23473", "authors": ["Haotian Liu", "Zhiqing Wei", "Luyang Sun", "Ruizhong Xu", "Yixin Zhang", "Zhiyong Feng"], "title": "Cooperative Sensing in Cell-free Massive MIMO ISAC Systems: Performance Optimization and Signal Processing", "comment": "13 pages, 10 figures", "summary": "Integrated sensing and communication (ISAC), as a technology enabled seamless\nconnection between communication and sensing, is regarded a core enabling\ntechnology for these applications. However, the accuracy of single-node sensing\nin ISAC system is limited, prompting the emergence of multi-node cooperative\nsensing. In multi-node cooperative sensing, the synchronization error limits\nthe sensing accuracy, which can be mitigated by the architecture of cell-free\nmassive multi-input multi-output (CF-mMIMO), since the multiple nodes are\ninterconnected via optical fibers with high synchronization accuracy. However,\nthe multi-node cooperative sensing in CF-mMIMO ISAC systems faces the following\nchallenges: 1) The joint optimization of placement and resource allocation of\ndistributed access points (APs) to improve the sensing performance in\nmulti-target detection scenario is difficult; 2) The fusion of the sensing\ninformation from distributed APs with multi-view discrepancies is difficult. To\naddress these challenges, this paper proposes a joint placement and antenna\nresource optimization scheme for distributed APs to minimize the sensing\nCramr-Rao bound for targets' parameters within the area of interest. Then, a\nsymbol-level fusion-based multi-dynamic target sensing (SL-MDTS) scheme is\nprovided, effectively fusing sensing information from multiple APs. The\nsimulation results validate the effectiveness of the joint optimization scheme\nand the superiority of the SL-MDTS scheme. Compared to state-of-the-art\ngrid-based symbol-level sensing information fusion schemes, the proposed\nSL-MDTS scheme improves the accuracy of localization and velocity estimation by\n44 % and 41.4 %, respectively."}
{"id": "2506.23495", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23495", "abs": "https://arxiv.org/abs/2506.23495", "authors": ["Zihang Ding", "Jianhua Zhang", "Changsheng You", "Pan Tang", "Hongbo Xing", "Zhiqiang Yuan", "Jie Meng", "Guangyi Liu"], "title": "Far-Field vs. Near-Field Propagation Channels: Key Differences and Impact on 6G XL-MIMO Performance Evaluation", "comment": "13 pages, 8 figures, 2 tables, 52 references. Note: This article has\n  been submitted to China Communications and is currently under review", "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as\na promising technology for next-generation communication systems. However, this\nwill expand the near-field (NF) range, rendering more users more likely to be\nlocated in the NF region. In this paper, we aim to answer two questions: What\nare the new characteristics of the NF channel? Is it necessary to develop new\ntransciver techniques to maintain system performance within the NF region? To\nthis end, we first review current NF channel models and analyze the differences\nbetween the existing 3GPP TR 38.901 channel model and the NF channel model,\nincluding the spherical wavefront and spatially non-stationarity. Then, we\nprovide examples on how these differences affect the XL-MIMO system performance\nin terms of beamforming gain and achievable rate. Simulation results\ndemonstrate that, when using far-field (FF) technique under the NF channel, the\nmaximum normalized beam gain loss is less than 3 dB for most users in the NF\nregion defined by Rayleigh distance. Moreover, the achievable rate loss of beam\ntraining is less than 3% compared to that realized by NF technique. Finally, we\ndemonstrate the necessity of employing NF transceiver techniques based on\nsimulation results."}
{"id": "2506.23511", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.23511", "abs": "https://arxiv.org/abs/2506.23511", "authors": ["Ahmad Abdel-Qader", "Anas Chaaban", "Mohamed S. Shehata"], "title": "Mutli-Level Autoencoder: Deep Learning Based Channel Coding and Modulation", "comment": "Accepted at IWCMC 2025", "summary": "In this paper, we design a deep learning-based convolutional autoencoder for\nchannel coding and modulation. The objective is to develop an adaptive scheme\ncapable of operating at various signal-to-noise ratios (SNR)s without the need\nfor re-training. Additionally, the proposed framework allows validation by\ntesting all possible codes in the codebook, as opposed to previous AI-based\nencoder/decoder frameworks which relied on testing only a small subset of the\navailable codes. This limitation in earlier methods often led to unreliable\nconclusions when generalized to larger codebooks. In contrast to previous\nmethods, our multi-level encoding and decoding approach splits the message into\nblocks, where each encoder block processes a distinct group of $B$ bits. By\ndoing so, the proposed scheme can exhaustively test $2^{B}$ possible codewords\nfor each encoder/decoder level, constituting a layer of the overall scheme. The\nproposed model was compared to classical polar codes and TurboAE-MOD schemes,\nshowing improved reliability with achieving comparable, or even superior\nresults in some settings. Notably, the architecture can adapt to different SNRs\nby selectively removing one of the encoder/decoder layers without re-training,\nthus demonstrating flexibility and efficiency in practical wireless\ncommunication scenarios."}
{"id": "2506.23525", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23525", "abs": "https://arxiv.org/abs/2506.23525", "authors": ["Wentao Yu", "Khaled B. Letaief", "Lizhong Zheng"], "title": "Sensing for Free: Learn to Localize More Sources than Antennas without Pilots", "comment": "13 pages, 9 figures, 1 table", "summary": "Integrated sensing and communication (ISAC) represents a key paradigm for\nfuture wireless networks. However, existing approaches require waveform\nmodifications, dedicated pilots, or overhead that complicates standards\nintegration. We propose sensing for free - performing multi-source localization\nwithout pilots by reusing uplink data symbols, making sensing occur during\ntransmission and directly compatible with 3GPP 5G NR and 6G specifications.\nWith ever-increasing devices in dense 6G networks, this approach is\nparticularly compelling when combined with sparse arrays, which can localize\nmore sources than uniform arrays via an enlarged virtual array. Existing\npilot-free multi-source localization algorithms first reconstruct an extended\ncovariance matrix and apply subspace methods, incurring cubic complexity and\nlimited to second-order statistics. Performance degrades under non-Gaussian\ndata symbols and few snapshots, and higher-order statistics remain unexploited.\nWe address these challenges with an attention-only transformer that directly\nprocesses raw signal snapshots for grid-less end-to-end direction-of-arrival\n(DOA) estimation. The model efficiently captures higher-order statistics while\nbeing permutation-invariant and adaptive to varying snapshot counts. Our\nalgorithm greatly outperforms state-of-the-art AI-based benchmarks with over\n30x reduction in parameters and runtime, and enjoys excellent generalization\nunder practical mismatches. Applied to multi-user MIMO beam training, our\nalgorithm can localize uplink DOAs of multiple users during data transmission.\nThrough angular reciprocity, estimated uplink DOAs prune downlink beam sweeping\ncandidates and improve throughput via sensing-assisted beam management. This\nwork shows how reusing existing data transmission for sensing can enhance both\nmulti-source localization and beam management in 3GPP efforts towards 6G."}
{"id": "2506.23557", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23557", "abs": "https://arxiv.org/abs/2506.23557", "authors": ["Xuehan Wang", "Hengyu Zhang", "Jintao Wang", "Zhi Sun", "Bo Ai"], "title": "Data-Driven Modulation Optimization with LMMSE Equalization for Reliability Enhancement in Underwater Acoustic Communications", "comment": "6 pages, 3 figures. This paper has been accepted for presentation in\n  IEEE/CIC ICCC 2025", "summary": "Ultra-reliable underwater acoustic (UWA) communications serve as one of the\nkey enabling technologies for future space-air-ground-underwater integrated\nnetworks. However, the reliability of current UWA transmission is still\ninsufficient since severe performance degradation occurs for conventional\nmulticarrier systems in UWA channels with severe delay-scale spread. To solve\nthis problem, we exploit learning-inspired approaches to optimize the\nmodulation scheme under the assumption of linear minimum mean square error\n(LMMSE) equalization, where the discrete representation of waveforms is adopted\nby utilizing Nyquist filters. The optimization problem is first transferred\ninto maximizing the fairness of estimation mean square error (MSE) for each\ndata symbol since the total MSE is invariant considering the property of\northogonal modulation. The Siamese architecture is then adopted to obtain\nconsistent optimization results across various channel conditions, which avoids\nthe overhead of online feedback, cooperation, and deployment of neural networks\nand guarantees generalization. The overall scheme including the loss function,\nneural network structure, and training process is also investigated in depth in\nthis paper. The excellent performance and robustness of the proposed modulation\nscheme are verified by carrying out the bit error rate test over various UWA\nchannels with severe delay-scale spread."}
{"id": "2506.23568", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23568", "abs": "https://arxiv.org/abs/2506.23568", "authors": ["Lei Wang", "Xianxun Yao", "Tiancheng Song", "Guolin Sun"], "title": "A Fast and Accurate 3-D Reconstruction Algorithm for Near-Range Microwave Imaging with Handheld Synthetic Aperture Radar", "comment": null, "summary": "The design of image reconstruction algorithms for near-range handheld\nsynthetic aperture radar (SAR) systems has gained increasing popularity due to\nthe promising performance of portable millimeter-wave (MMW) imaging devices in\nvarious application fields. Time domain imaging algorithms including the\nbackprojection algorithm (BPA) and the Kirchhoff migration algorithm (KMA) are\nwidely adopted due to their direct applicability to arbitrary scan\ntrajectories. However, they suffer from time complexity issues that hinder\ntheir practical application. Wavenumber domain algorithms greatly improve the\ncomputational efficiency but most of them are restricted to specific array\ntopologies. Based on the factorization techniques as adopted in far-field\nsynthetic aperture radar imaging, the time domain fast factorized\nbackprojection algorithm for handheld synthetic aperture radar (HHFFBPA) is\nproposed. The local spectral properties of the radar images for handheld\nsystems are analyzed and analytical spectrum compression techniques are derived\nto realize efficient sampling of the subimages. Validated through numerical\nsimulations and experiments, HHFFBPA achieves fast and accurate 3-D imaging for\nhandheld synthetic aperture radar systems with arbitrary trajectories."}
{"id": "2506.23621", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23621", "abs": "https://arxiv.org/abs/2506.23621", "authors": ["Steffen Schieler", "Sebastian Semper", "Reiner Thomä"], "title": "Wireless Propagation Parameter Estimation with Convolutional Neural Networks", "comment": "This is the accepted version of the article published in the\n  International Journal of Microwave and Wireless Technologies with the DOI\n  10.1017/S1759078725000431", "summary": "Wireless channel propagation parameter estimation forms the foundation of\nchannel sounding, estimation, modeling, and sensing. This paper introduces a\nDeep Learning approach for joint delay- and Doppler estimation from frequency\nand time samples of a radio channel transfer function.\n  Our work estimates the two-dimensional path parameters from a channel impulse\nresponse containing an unknown number of paths. Compared to existing deep\nlearning-based methods, the parameters are not estimated via classification but\nin a quasi-grid-free manner. We employ a deterministic preprocessing scheme\nthat incorporates a multi-channel windowing to increase the estimator's\nrobustness and enables the use of a CNN architecture. The proposed architecture\nthen jointly estimates the number of paths along with the respective delay and\nDoppler-shift parameters of the paths. Hence, it jointly solves the model order\nselection and parameter estimation task. We also integrate the CNN into an\nexisting maximum-likelihood estimator framework for efficient initialization of\na gradient-based iteration, to provide more accurate estimates.\n  In the analysis, we compare our approach to other methods in terms of\nestimate accuracy and model order error on synthetic data. Finally, we\ndemonstrate its applicability to real-world measurement data from a anechoic\nbi-static RADAR emulation measurement."}
{"id": "2506.23750", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23750", "abs": "https://arxiv.org/abs/2506.23750", "authors": ["Ge Yan", "Lipeng Zhu", "He Sun", "Rui Zhang"], "title": "Wideband Coverage Enhancement for IRS-Aided Wireless Networks Based on Power Measurement", "comment": "5 pages, 6 figures", "summary": "By applying tunable phase shifts to incident waves via passive signal\nreflection, intelligent reflecting surface (IRS) can offer significant\nperformance improvement for wireless communication systems. To reap such\nperformance gain, channel knowledge for IRS-cascaded links is generally\nrequired, which is practically challenging to acquire due to their\nhigh-dimensional and time-varying characteristics. Conventional pilot-based\nchannel estimation incurs excessive overhead due to the large number of\nreflecting elements, thus undermining the IRS efficiency, especially for\nwideband systems with frequency-selective fading channels. To tackle this\nissue, we propose in this letter a power-measurement-based channel\nautocorrelation matrix estimation and coverage enhancement approach for\nIRS-aided orthogonal frequency division multiplexing (OFDM) systems.\nSpecifically, by estimating equivalent channel autocorrelation matrices of\nIRS-cascaded OFDM channels based on receive signal power and optimizing the IRS\nreflection vector based on them, the average coverage performance in the\nIRS-aided region is enhanced without the need for frequent reconfiguration of\nIRS reflection coefficients based on user instantaneous channels. Simulation\nresults validate the effectiveness of the proposed approach for improving the\naverage channel gain over the coverage region."}
{"id": "2506.23788", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2506.23788", "abs": "https://arxiv.org/abs/2506.23788", "authors": ["Naomi Stricker", "David Blaser", "Andres Gomez", "Lothar Thiele"], "title": "E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks", "comment": "This is the author's version of the work. Submitted to ACM TOSN on\n  June 2023. Major revision submitted on May 2024. Minor Revision submitted on\n  March 2025", "summary": "The ever-increasing number of distributed embedded systems in the context of\nthe Internet of Things (IoT), Wireless Sensor Networks (WSN), and\nCyber-Physical Systems (CPS) rely on wireless communication to collect and\nexchange data. Nodes can employ single-hop communication which, despite its\nease, may necessitate energy-intensive long-range communication to cover long\ndistances. Conversely, multi-hop communication allows for more energy-efficient\nshort-range communication since nodes can rely on other nodes to forward their\ndata. Yet, this approach requires relay nodes to be available and continuous\nmaintenance of a dynamically changing distributed state. At the same time,\nenergy harvesting has the potential to outperform traditional battery-based\nsystems by improving their lifetime, scalability with lower maintenance costs,\nand environmental impact. However, the limited and temporally and spatially\nvariable harvested energy poses significant challenges for networking in energy\nharvesting networks, particularly considering the energy demands and\ncharacteristics of both multi-hop and single-hop communication. We propose\nE-WAN, a protocol for energy harvesting wide-area low-power networks that\nbuilds on the concept of \\emph{virtual sub-networks} to enable\nresource-efficient multi-hop communication when possible and reliable however\nenergy-intensive point-to-point communication otherwise. Nodes autonomously and\ndynamically move between the two and adjust to changing network states and\nresources based only on easily obtainable network state information. We\nillustrate E-WAN's advantages both in terms of efficiency and adaptability in\nvarious communication and harvesting scenarios. Furthermore, we demonstrate\nE-WAN operating in a realistic setting by deploying an energy harvesting\nnetwork in a real-world indoor environment."}
{"id": "2506.23937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2506.23937", "abs": "https://arxiv.org/abs/2506.23937", "authors": ["Chu Li", "Marjan Boloori", "Eduard Jorswieck", "Aydin Sezgin"], "title": "Optimized Frequency-Diverse Movable Antenna Arrays for Directional Secrecy in Wireless Systems", "comment": null, "summary": "Movable-antenna (MA) arrays are envisioned as a promising technique for\nenhancing secrecy performance in wireless communications by leveraging\nadditional spatial degrees of freedom. However, when the eavesdropper is\nlocated in the same direction as the legitimate user, particularly in\nmmWave/THz bands where line-of-sight (LOS) propagation dominates, the secrecy\nperformance of MA arrays becomes significantly limited, thus directionally\ninsecure. To address this challenge, we employ a joint design that combines an\nMA array with a frequency-diverse array (FDA) at the transmitter to secure the\ntransmission across both range and direction. Specifically, we derive\nclosed-form expressions for the optimal antenna positions and frequency shifts,\nassuming small perturbations in both parameters from a linear frequency-diverse\nMA configuration. Furthermore, we compare the worst-case secrecy rate under\nthis minor perturbation assumption with that obtained under a general\nconstraint, where simulated annealing is employed to numerically determine the\noptimal parameters. Simulation results confirm that the proposed optimized\nfrequency diverse MA approach significantly enhances secrecy performance in the\npresence of an eavesdropper aligned with the direction of the legitimate\nreceiver."}
{"id": "2506.23966", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.23966", "abs": "https://arxiv.org/abs/2506.23966", "authors": ["Yanqing Xu", "Zhiguo Ding", "Robert Schober", "Tsung-Hui Chang"], "title": "Pinching-Antenna Systems with In-Waveguide Attenuation: Performance Analysis and Algorithm Design", "comment": "This paper aims to address a fundamental question in pinching-antenna\n  systems: Can in-waveguide attenuation be safely ignored without causing\n  significant performance degradation? Our analytical results provide a clear\n  answer -- YES, provided that certain mild and practically realizable\n  conditions on the system parameters are satisfied", "summary": "Pinching-antenna systems have emerged as a promising flexible-antenna\narchitecture for next-generation wireless networks, enabling enhanced\nadaptability and user-centric connectivity through antenna repositioning along\nwaveguides. However, existing studies often overlook in-waveguide signal\nattenuation and in the literature, there is no comprehensive analysis on\nwhether and under what conditions such an assumption is justified. This paper\naddresses this gap by explicitly incorporating in-waveguide attenuation into\nboth the system model and algorithm design, and studying its impact on the\ndownlink user data rates. We begin with a single-user scenario and derive a\nclosed-form expression for the globally optimal antenna placement, which\nreveals how the attenuation coefficient and the user-to-waveguide distance\njointly affect the optimal antenna position. Based on this analytical solution,\nwe further provide a theoretical analysis identifying the system conditions\nunder which the in-waveguide attenuation has an insignificant impact on the\nuser achievable rate. The study is then extended to the multi-user\nmultiple-input multiple-output setting, where two efficient algorithms are\ndeveloped, based on the weighted minimum mean square error method and the\nmaximum ratio combining method, to jointly optimize beamforming and antenna\nplacement. Simulation results validate the efficacy of the proposed algorithms\nand demonstrate that pinching-antenna systems substantially outperform\nconventional fixed-antenna baselines, underscoring their potential for future\nflexible wireless communications."}
{"id": "2506.24024", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.24024", "abs": "https://arxiv.org/abs/2506.24024", "authors": ["Nicolas Heintz", "Tom Francart", "Alexander Bertrand"], "title": "Post-processing of EEG-based Auditory Attention Decoding Decisions via Hidden Markov Models", "comment": null, "summary": "Auditory attention decoding (AAD) algorithms exploit brain signals, such as\nelectroencephalography (EEG), to identify which speaker a listener is focusing\non in a multi-speaker environment. While state-of-the-art AAD algorithms can\nidentify the attended speaker on short time windows, their predictions are\noften too inaccurate for practical use. In this work, we propose augmenting AAD\nwith a hidden Markov model (HMM) that models the temporal structure of\nattention. More specifically, the HMM relies on the fact that a subject is much\nless likely to switch attention than to keep attending the same speaker at any\nmoment in time. We show how a HMM can significantly improve existing AAD\nalgorithms in both causal (real-time) and non-causal (offline) settings. We\nfurther demonstrate that HMMs outperform existing postprocessing approaches in\nboth accuracy and responsiveness, and explore how various factors such as\nwindow length, switching frequency, and AAD accuracy influence overall\nperformance. The proposed method is computationally efficient, intuitive to use\nand applicable in both real-time and offline settings."}

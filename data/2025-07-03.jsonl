{"id": "2507.01021", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time."}
{"id": "2507.01022", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01022", "abs": "https://arxiv.org/abs/2507.01022", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "title": "Workflow-Based Evaluation of Music Generation Systems", "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field."}
{"id": "2507.01024", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01024", "abs": "https://arxiv.org/abs/2507.01024", "authors": ["George Igwegbe", "Martins Awojide", "Mboh Bless", "Nirel Kadzo"], "title": "Hello Afrika: Speech Commands in Kinyarwanda", "comment": "Data Science Africa, 2024", "summary": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a\nlanguage which are essential for non-contact control of and activation of\nlarger AI systems in devices used in everyday life especially for persons with\ndisabilities. Currently, there is a dearth of speech command models for African\nlanguages. The Hello Afrika project aims to address this issue and its first\niteration is focused on the Kinyarwanda language since the country has shown\ninterest in developing speech recognition technologies culminating in one of\nthe largest datasets on Mozilla Common Voice. The model was built off a custom\nspeech command corpus made up of general directives, numbers, and a wake word.\nThe final model was deployed on multiple devices (PC, Mobile Phone and Edge\nDevices) and the performance was assessed using suitable metrics."}
{"id": "2507.01172", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01172", "abs": "https://arxiv.org/abs/2507.01172", "authors": ["Marios Glytsos", "Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "title": "Classical Guitar Duet Separation using GuitarDuets -- a Dataset of Real and Synthesized Guitar Recordings", "comment": "In Proceedings of the 25th International Society for Music\n  Information Retrieval Conference (ISMIR 2024), San Francisco, USA, November\n  2024. The dataset is available at: https://zenodo.org/records/12802440", "summary": "Recent advancements in music source separation (MSS) have focused in the\nmulti-timbral case, with existing architectures tailored for the separation of\ndistinct instruments, overlooking thus the challenge of separating instruments\nwith similar timbral characteristics. Addressing this gap, our work focuses on\nmonotimbral MSS, specifically within the context of classical guitar duets. To\nthis end, we introduce the GuitarDuets dataset, featuring a combined total of\napproximately three hours of real and synthesized classical guitar duet\nrecordings, as well as note-level annotations of the synthesized duets. We\nperform an extensive cross-dataset evaluation by adapting Demucs, a\nstate-of-the-art MSS architecture, to monotimbral source separation.\nFurthermore, we develop a joint permutation-invariant transcription and\nseparation framework, to exploit note event predictions as auxiliary\ninformation. Our results indicate that utilizing both the real and synthesized\nsubsets of GuitarDuets leads to improved separation performance in an\nindependently recorded test set compared to utilizing solely one subset. We\nalso find that while the availability of ground-truth note labels greatly helps\nthe performance of the separation network, the predicted note estimates result\nonly in marginal improvement. Finally, we discuss the behavior of commonly\nutilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS."}
{"id": "2507.01339", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01339", "abs": "https://arxiv.org/abs/2507.01339", "authors": ["Yutong Wen", "Minje Kim", "Paris Smaragdis"], "title": "User-guided Generative Source Separation", "comment": null, "summary": "Music source separation (MSS) aims to extract individual instrument sources\nfrom their mixture. While most existing methods focus on the widely adopted\nfour-stem separation setup (vocals, bass, drums, and other instruments), this\napproach lacks the flexibility needed for real-world applications. To address\nthis, we propose GuideSep, a diffusion-based MSS model capable of\ninstrument-agnostic separation beyond the four-stem setup. GuideSep is\nconditioned on multiple inputs: a waveform mimicry condition, which can be\neasily provided by humming or playing the target melody, and mel-spectrogram\ndomain masks, which offer additional guidance for separation. Unlike prior\napproaches that relied on fixed class labels or sound queries, our conditioning\nscheme, coupled with the generative approach, provides greater flexibility and\napplicability. Additionally, we design a mask-prediction baseline using the\nsame model architecture to systematically compare predictive and generative\napproaches. Our objective and subjective evaluations demonstrate that GuideSep\nachieves high-quality separation while enabling more versatile instrument\nextraction, highlighting the potential of user participation in the\ndiffusion-based generative process for MSS. Our code and demo page are\navailable at https://yutongwen.github.io/GuideSep/"}
{"id": "2507.01227", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.01227", "abs": "https://arxiv.org/abs/2507.01227", "authors": ["Son T. Duong", "Tho Le-Ngoc"], "title": "Degrees of Freedom of Spatial Multiplexing in Distance Domain of Arbitrary Continuous-Aperture Array in Near-Field Region", "comment": "13 pages and 13 figures. Part of this work has been submitted to IEEE\n  Globecom 2025 (under review). This version has been submitted to IEEE\n  Transactions on Communications for possible publication", "summary": "Extremely large aperture array operating in the near-field regime unlocks\nadditional spatial resources that can be exploited to simultaneously serve\nmultiple users even when they share the same angular direction, a capability\nnot achievable in conventional far-field systems. A fundamental question,\nhowever, remains: What is the maximum spatial degree of freedom (DoF) of\nspatial multiplexing in the distance domain?\n  In this paper, we address this open problem by investigating the spatial DoF\nof a line-of-sight (LoS) channel between a large two-dimensional transmit\naperture and a linear receive array with collinearly-aligned elements (i.e., at\nthe same angular direction) but located at different distances from the\ntransmit aperture. We assume that both the aperture and linear array are\ncontinuous-aperture (CAP) arrays with an infinite number of elements and\ninfinitesimal spacing, which establishes an upper bound for the spatial degrees\nof freedom (DoF) in the case of finite elements. First, we assume an ideal case\nwhere the transmit array is a single piece and the linear array is on the broad\nside of the transmit array. By reformulating the channel as an integral\noperator with a Hermitian convolution kernel, we derive a closed-form\nexpression for the spatial DoF via the Fourier transform. Our analysis shows\nthat the spatial DoF in the distance domain is predominantly determined by the\nextreme boundaries of the array rather than its detailed interior structure. We\nfurther extend the framework to non-broadside configurations by employing a\nprojection method, which effectively converts the spatial DoF to an equivalent\nbroadside case. Finally, we extend our analytical framework to the modular\narray, which shows the spatial DoF gain over the single-piece array given the\nconstraint of the physical length of the array."}
{"id": "2507.01348", "categories": ["eess.AS", "cs.SD", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01348", "abs": "https://arxiv.org/abs/2507.01348", "authors": ["Cheng Zhuangfei", "Zhang Guangyan", "Tu Zehai", "Song Yangyang", "Mao Shuiyang", "Jiao Xiaoqi", "Li Jingyu", "Guo Yiwen", "Wu Jiasong"], "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech", "comment": "10 pages, includes references, 4 figures, 4 tables", "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."}
{"id": "2507.01563", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07 (Primary), 68T10 (Secondary)", "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.01563", "abs": "https://arxiv.org/abs/2507.01563", "authors": ["Marco Giordano", "Stefano Giacomelli", "Claudia Rinaldi", "Fabio Graziosi"], "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware", "comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437", "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices."}
{"id": "2507.01230", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.01230", "abs": "https://arxiv.org/abs/2507.01230", "authors": ["Yuri Abramovich", "Victor Abramovich", "Tanit Pongsiri"], "title": "Numerical Techniques for the Maximum Likelihood Toeplitz Covariance Matrix Estimation: Part I. Symmetric Toeplitz Matrices", "comment": null, "summary": "In several applications, one must estimate a real-valued (symmetric) Toeplitz\ncovariance matrix, typically shifted by the conjugated diagonal matrices of\nphase progression and phase \"calibration\" errors. Unlike the Hermitian Toeplitz\ncovariance matrices, these symmetric matrices have a unique potential\ncapability of being estimated regardless of these beam-steering phase\nprogression and/or phase \"calibration\" errors. This unique capability is the\nprimary motivation of this paper."}
{"id": "2507.01349", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01349", "abs": "https://arxiv.org/abs/2507.01349", "authors": ["Hitoshi Suda", "Junya Koguchi", "Shunsuke Yoshida", "Tomohiko Nakamura", "Satoru Fukayama", "Jun Ogata"], "title": "IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups", "comment": "Accepted at ISMIR 2025", "summary": "Japanese idol groups, comprising performers known as \"idols,\" are an\nindispensable part of Japanese pop culture. They frequently appear in live\nconcerts and television programs, entertaining audiences with their singing and\ndancing. Similar to other J-pop songs, idol group music covers a wide range of\nstyles, with various types of chord progressions and instrumental arrangements.\nThese tracks often feature numerous instruments and employ complex mastering\ntechniques, resulting in high signal loudness. Additionally, most songs include\na song division (utawari) structure, in which members alternate between singing\nsolos and performing together. Hence, these songs are well-suited for\nbenchmarking various music information processing techniques such as singer\ndiarization, music source separation, and automatic chord estimation under\nchallenging conditions. Focusing on these characteristics, we constructed a\nsong corpus titled IdolSongsJp by commissioning professional composers to\ncreate 15 tracks in the style of Japanese idol groups. This corpus includes not\nonly mastered audio tracks but also stems for music source separation, dry\nvocal tracks, and chord annotations. This paper provides a detailed description\nof the corpus, demonstrates its diversity through comparisons with real-world\nidol group songs, and presents its application in evaluating several music\ninformation processing techniques."}
{"id": "2507.01582", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01582", "abs": "https://arxiv.org/abs/2507.01582", "authors": ["Jing Luo", "Xinyu Yang", "Jie Wei"], "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder", "comment": "Accepted by IEEE SMC 2025", "summary": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain."}
{"id": "2507.01286", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01286", "abs": "https://arxiv.org/abs/2507.01286", "authors": ["Zai Yang", "Sikai Ge", "Wenlong Wang"], "title": "Pursuing the limit of chirp parameter identifiability: A computational approach", "comment": "10 pages", "summary": "In this paper, it is shown that a necessary condition for unique\nidentifiability of $K$ chirps from $N$ regularly spaced samples of their\nmixture is $N\\geq 2K$ when $K\\geq 2$. A necessary and sufficient condition is\nthat a rank-constrained matrix optimization problem has a unique solution; this\nis the first result of such kind. An algorithm is proposed to solve the\noptimization problem and to identify the parameters numerically. The lower\nbound of $N=2K$ is shown to be tight by providing diverse problem instances for\nwhich the proposed algorithm succeeds to identify the parameters. The\nadvantageous performance of the proposed algorithm is also demonstrated\ncompared with the state of the art."}
{"id": "2507.01356", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01356", "abs": "https://arxiv.org/abs/2507.01356", "authors": ["Hitoshi Suda", "Shinnosuke Takamichi", "Satoru Fukayama"], "title": "Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora", "comment": "Accepted at Interspeech 2025", "summary": "Perceived voice likability plays a crucial role in various social\ninteractions, such as partner selection and advertising. A system that provides\nreference likable voice samples tailored to target audiences would enable users\nto adjust their speaking style and voice quality, facilitating smoother\ncommunication. To this end, we propose a voice conversion method that controls\nthe likability of input speech while preserving both speaker identity and\nlinguistic content. To improve training data scalability, we train a likability\npredictor on an existing voice likability dataset and employ it to\nautomatically annotate a large speech synthesis corpus with likability ratings.\nExperimental evaluations reveal a significant correlation between the\npredictor's outputs and human-provided likability ratings. Subjective and\nobjective evaluations further demonstrate that the proposed approach\neffectively controls voice likability while preserving both speaker identity\nand linguistic content."}
{"id": "2507.01805", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01805", "abs": "https://arxiv.org/abs/2507.01805", "authors": ["Alejandro Sosa Welford", "Leonardo Pepino"], "title": "A Dataset for Automatic Assessment of TTS Quality in Spanish", "comment": "5 pages, 2 figures. Accepted at Interspeech 2025", "summary": "This work addresses the development of a database for the automatic\nassessment of text-to-speech (TTS) systems in Spanish, aiming to improve the\naccuracy of naturalness prediction models. The dataset consists of 4,326 audio\nsamples from 52 different TTS systems and human voices and is, up to our\nknowledge, the first of its kind in Spanish. To label the audios, a subjective\ntest was designed based on the ITU-T Rec. P.807 standard and completed by 92\nparticipants. Furthermore, the utility of the collected dataset was validated\nby training automatic naturalness prediction systems. We explored two\napproaches: fine-tuning an existing model originally trained for English, and\ntraining small downstream networks on top of frozen self-supervised speech\nmodels. Our models achieve a mean absolute error of 0.8 on a five-point MOS\nscale. Further analysis demonstrates the quality and diversity of the developed\ndataset, and its potential to advance TTS research in Spanish."}
{"id": "2507.01427", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01427", "abs": "https://arxiv.org/abs/2507.01427", "authors": ["Jun Wu", "Yuye Shi", "Weijie Yuan", "Qingqing Cheng", "Buyi Li", "Xinyuan Wei"], "title": "SDR-Empowered Environment Sensing Design and Experimental Validation Using OTFS-ISAC Signals", "comment": null, "summary": "This paper investigates the system design and experimental validation of\nintegrated sensing and communication (ISAC) for environmental sensing, which is\nexpected to be a critical enabler for next-generation wireless networks. We\nadvocate exploiting orthogonal time frequency space (OTFS) modulation for its\ninherent sparsity and stability in delay-Doppler (DD) domain channels,\nfacilitating a low-overhead environment sensing design. Moreover, a\ncomprehensive environmental sensing framework is developed, encompassing DD\ndomain channel estimation, target localization, and experimental validation. In\nparticular, we first explore the OTFS channel estimation in the presence of\nfractional delay and Doppler shifts. Given the estimated parameters, we propose\na three-ellipse positioning algorithm to localize the target's position,\nfollowed by determining the mobile transmitter's velocity. Additionally, to\nevaluate the performance of our proposed design, we conduct extensive\nsimulations and experiments using a software-defined radio (SDR)-based platform\nwith universal software radio peripheral (USRP). The experimental validations\ndemonstrate that our proposed approach outperforms the benchmarks in terms of\nlocalization accuracy and velocity estimation, confirming its effectiveness in\npractical environmental sensing applications."}
{"id": "2507.01611", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility."}
{"id": "2507.01021", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time."}
{"id": "2507.01445", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01445", "abs": "https://arxiv.org/abs/2507.01445", "authors": ["Yanfeng Zhang", "Xu Zhu", "Yujie Liu", "Yong Liang Guan", "David González G.", "Vincent K. N. Lau"], "title": "Basis Expansion Extrapolation based Long-Term Channel Prediction for Massive MIMO OTFS Systems", "comment": null, "summary": "Massive multi-input multi-output (MIMO) combined with orthogonal time\nfrequency space (OTFS) modulation has emerged as a promising technique for\nhigh-mobility scenarios. However, its performance could be severely degraded\ndue to channel aging caused by user mobility and high processing latency. In\nthis paper, an integrated scheme of uplink (UL) channel estimation and downlink\n(DL) channel prediction is proposed to alleviate channel aging in time division\nduplex (TDD) massive MIMO-OTFS systems. Specifically, first, an iterative basis\nexpansion model (BEM) based UL channel estimation scheme is proposed to\naccurately estimate UL channels with the aid of carefully designed OTFS frame\npattern. Then a set of Slepian sequences are used to model the estimated UL\nchannels, and the dynamic Slepian coefficients are fitted by a set of\northogonal polynomials. A channel predictor is derived to predict DL channels\nby iteratively extrapolating the Slepian coefficients. Simulation results\nverify that the proposed UL channel estimation and DL channel prediction\nschemes outperform the existing schemes in terms of normalized mean square\nerror of channel estimation/prediction and DL spectral efficiency, with less\npilot overhead."}
{"id": "2507.01750", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01750", "abs": "https://arxiv.org/abs/2507.01750", "authors": ["Jose A. Lopez", "Georg Stemmer", "Héctor Cordourier Maruri"], "title": "Generalizable Detection of Audio Deepfakes", "comment": "8 pages, 3 figures", "summary": "In this paper, we present our comprehensive study aimed at enhancing the\ngeneralization capabilities of audio deepfake detection models. We investigate\nthe performance of various pre-trained backbones, including Wav2Vec2, WavLM,\nand Whisper, across a diverse set of datasets, including those from the\nASVspoof challenges and additional sources. Our experiments focus on the\neffects of different data augmentation strategies and loss functions on model\nperformance. The results of our research demonstrate substantial enhancements\nin the generalization capabilities of audio deepfake detection models,\nsurpassing the performance of the top-ranked single system in the ASVspoof 5\nChallenge. This study contributes valuable insights into the optimization of\naudio models for more robust deepfake detection and facilitates future research\nin this critical area."}
{"id": "2507.01022", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01022", "abs": "https://arxiv.org/abs/2507.01022", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "title": "Workflow-Based Evaluation of Music Generation Systems", "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field."}
{"id": "2507.01575", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01575", "abs": "https://arxiv.org/abs/2507.01575", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang", "Alexander Artemenko"], "title": "Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability", "comment": "Accepted for publication in the IEEE VTC2025-Spring Conference, 7\n  pages", "summary": "Accurate indoor localization is crucial in industrial environments. Visible\nLight Communication (VLC) has emerged as a promising solution, offering high\naccuracy, energy efficiency, and minimal electromagnetic interference. However,\nVLC-based indoor localization faces challenges due to environmental\nvariability, such as lighting fluctuations and obstacles. To address these\nchallenges, we propose a Transfer Learning (TL)-based approach for VLC-based\nindoor localization. Using real-world data collected at a BOSCH factory, the TL\nframework integrates a deep neural network (DNN) to improve localization\naccuracy by 47\\%, reduce energy consumption by 32\\%, and decrease computational\ntime by 40\\% compared to the conventional models. The proposed solution is\nhighly adaptable under varying environmental conditions and achieves similar\naccuracy with only 30\\% of the dataset, making it a cost-efficient and scalable\noption for industrial applications in Industry 4.0."}
{"id": "2507.01765", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01765", "abs": "https://arxiv.org/abs/2507.01765", "authors": ["Sarina Meyer", "Ekaterina Kolos", "Ngoc Thang Vu"], "title": "First Steps Towards Voice Anonymization for Code-Switching Speech", "comment": "accepted at Interspeech 2025", "summary": "The goal of voice anonymization is to modify an audio such that the true\nidentity of its speaker is hidden. Research on this task is typically limited\nto the same English read speech datasets, thus the efficacy of current methods\nfor other types of speech data remains unknown. In this paper, we present the\nfirst investigation of voice anonymization for the multilingual phenomenon of\ncode-switching speech. We prepare two corpora for this task and propose\nadaptations to a multilingual anonymization model to make it applicable for\ncode-switching speech. By testing the anonymization performance of this and two\nlanguage-independent methods on the datasets, we find that only the\nmultilingual system performs well in terms of privacy and utility preservation.\nFurthermore, we observe challenges in performing utility evaluations on this\ndata because of its spontaneous character and the limited code-switching\nsupport by the multilingual speech recognition model."}
{"id": "2507.01024", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01024", "abs": "https://arxiv.org/abs/2507.01024", "authors": ["George Igwegbe", "Martins Awojide", "Mboh Bless", "Nirel Kadzo"], "title": "Hello Afrika: Speech Commands in Kinyarwanda", "comment": "Data Science Africa, 2024", "summary": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a\nlanguage which are essential for non-contact control of and activation of\nlarger AI systems in devices used in everyday life especially for persons with\ndisabilities. Currently, there is a dearth of speech command models for African\nlanguages. The Hello Afrika project aims to address this issue and its first\niteration is focused on the Kinyarwanda language since the country has shown\ninterest in developing speech recognition technologies culminating in one of\nthe largest datasets on Mozilla Common Voice. The model was built off a custom\nspeech command corpus made up of general directives, numbers, and a wake word.\nThe final model was deployed on multiple devices (PC, Mobile Phone and Edge\nDevices) and the performance was assessed using suitable metrics."}
{"id": "2507.01624", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01624", "abs": "https://arxiv.org/abs/2507.01624", "authors": ["Cong Zhou", "Changsheng You", "Shuo Shi", "Weidong Mei"], "title": "Frequency-switching Array Enhanced Physical-Layer Security in Terahertz Bands: A Movable Antenna Perspective", "comment": "In this paper, we propose to enhance physical-layer security by using\n  a new frequency-switching array, which is equivalent to movable antennas", "summary": "In this paper, we propose a new frequency-switching array (FSA) enhanced\nphysical-layer security (PLS) system in terahertz bands, where the carrier\nfrequency can be flexibly switched and small frequency offsets can be imposed\non each antenna at Alice, so as to eliminate information wiretapping by\nundesired eavesdroppers. First, we analytically show that by flexibly\ncontrolling the carrier frequency parameters, FSAs can effectively form\nuniform/non-uniform sparse arrays, hence resembling movable antennas (MAs) in\nthe control of inter-antenna spacing and providing additional degree-of-freedom\n(DoF) in the beam control. Although the proposed FSA experiences additional\npath-gain attenuation in the received signals, it can overcome several hardware\nand signal processing issues incurred by MAs, such as limited positioning\naccuracy, considerable response latency, and demanding hardware and energy\ncost. To shed useful insights, we first consider a secrecy-guaranteed problem\nwith a null-steering constraint for which maximum ratio transmission (MRT)\nbeamformer is considered at Alice and the frequency offsets are set as uniform\nfrequency increment. Interestingly, it is shown that the proposed FSA can\nflexibly realize null-steering over Eve in both the angular domain (by tuning\ncarrier frequency) and range domain (by controlling per-antenna frequency\noffset), thereby achieving improved PLS performance. Then, for the general\ncase, we propose an efficient algorithm to solve the formulated non-convex\nproblem by using the block coordinate descent (BCD) and projected gradient\nascent (PGA) techniques. Finally, numerical results demonstrate the convergence\nof the proposed optimization algorithm and its superiority over fixed-position\narrays (FPAs) in terms of secrecy-rate performance."}
{"id": "2507.01821", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanuël A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications."}
{"id": "2507.01348", "categories": ["eess.AS", "cs.SD", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01348", "abs": "https://arxiv.org/abs/2507.01348", "authors": ["Cheng Zhuangfei", "Zhang Guangyan", "Tu Zehai", "Song Yangyang", "Mao Shuiyang", "Jiao Xiaoqi", "Li Jingyu", "Guo Yiwen", "Wu Jiasong"], "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech", "comment": "10 pages, includes references, 4 figures, 4 tables", "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."}
{"id": "2507.01728", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01728", "abs": "https://arxiv.org/abs/2507.01728", "authors": ["Hao Wei", "Wanli Ni", "Wen Wang", "Wenjun Xu", "Dusit Niyato", "Ping Zhang"], "title": "Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach", "comment": null, "summary": "This letter proposes UniToCom, a unified token communication paradigm that\ntreats tokens as the fundamental units for both processing and wireless\ntransmission. Specifically, to enable efficient token representations, we\npropose a generative information bottleneck (GenIB) principle, which\nfacilitates the learning of tokens that preserve essential information while\nsupporting reliable generation across multiple modalities. By doing this,\nGenIB-based tokenization is conducive to improving the communication efficiency\nand reducing computational complexity. Additionally, we develop $\\sigma$-GenIB\nto address the challenges of variance collapse in autoregressive modeling,\nmaintaining representational diversity and stability. Moreover, we employ a\ncausal Transformer-based multimodal large language model (MLLM) at the receiver\nto unify the processing of both discrete and continuous tokens under the\nnext-token prediction paradigm. Simulation results validate the effectiveness\nand superiority of the proposed UniToCom compared to baselines under dynamic\nchannel conditions. By integrating token processing with MLLMs, UniToCom\nenables scalable and generalizable communication in favor of multimodal\nunderstanding and generation, providing a potential solution for\nnext-generation intelligent communications."}
{"id": "2507.01888", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01888", "abs": "https://arxiv.org/abs/2507.01888", "authors": ["Nina R. Benway", "Saba Tabatabaee", "Dongliang Wang", "Benjamin Munson", "Jonathan L. Preston", "Carol Espy-Wilson"], "title": "Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders", "comment": "This manuscript is in submission for publication. It has not yet been\n  peer reviewed", "summary": "Purpose: This study evaluated whether articulatory kinematics, inferred by\nArticulatory Phonology speech inversion neural networks, aligned with\nperceptual ratings of /r/ and /s/ in the speech of children with speech sound\ndisorders.\n  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961\nutterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual\nratings were standardized using the novel 5-point PERCEPT Rating Scale and\ntraining protocol. Two research questions examined if the articulatory patterns\nof inferred vocal tract variables aligned with the perceptual error category\nfor the phones investigated (e.g., tongue tip is more anterior in dentalized\n/s/ productions than in correct /s/). A third research question examined if\ngradient PERCEPT Rating Scale scores predicted articulatory proximity to\ncorrect productions.\n  Results: Estimated marginal means from linear mixed models supported 17 of 18\n/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,\nestimated marginal means from a second linear mixed model supported 7 of 15\nhypotheses, particularly those related to the tongue tip. A third linear mixed\nmodel revealed that PERCEPT Rating Scale scores significantly predicted\narticulatory proximity of errored phones to correct productions.\n  Conclusion: Inferred vocal tract variables differentiated category and\nmagnitude of articulatory errors for /r/, and to a lesser extent for /s/,\naligning with perceptual judgments. These findings support the clinical\ninterpretability of speech inversion vocal tract variables and the PERCEPT\nRating Scale in quantifying articulatory proximity to the target sound,\nparticularly for /r/."}
{"id": "2507.01349", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01349", "abs": "https://arxiv.org/abs/2507.01349", "authors": ["Hitoshi Suda", "Junya Koguchi", "Shunsuke Yoshida", "Tomohiko Nakamura", "Satoru Fukayama", "Jun Ogata"], "title": "IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups", "comment": "Accepted at ISMIR 2025", "summary": "Japanese idol groups, comprising performers known as \"idols,\" are an\nindispensable part of Japanese pop culture. They frequently appear in live\nconcerts and television programs, entertaining audiences with their singing and\ndancing. Similar to other J-pop songs, idol group music covers a wide range of\nstyles, with various types of chord progressions and instrumental arrangements.\nThese tracks often feature numerous instruments and employ complex mastering\ntechniques, resulting in high signal loudness. Additionally, most songs include\na song division (utawari) structure, in which members alternate between singing\nsolos and performing together. Hence, these songs are well-suited for\nbenchmarking various music information processing techniques such as singer\ndiarization, music source separation, and automatic chord estimation under\nchallenging conditions. Focusing on these characteristics, we constructed a\nsong corpus titled IdolSongsJp by commissioning professional composers to\ncreate 15 tracks in the style of Japanese idol groups. This corpus includes not\nonly mastered audio tracks but also stems for music source separation, dry\nvocal tracks, and chord annotations. This paper provides a detailed description\nof the corpus, demonstrates its diversity through comparisons with real-world\nidol group songs, and presents its application in evaluating several music\ninformation processing techniques."}
{"id": "2507.01743", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01743", "abs": "https://arxiv.org/abs/2507.01743", "authors": ["Lorenzo Pucci", "Luca Arcangeloni", "Andrea Giorgetti"], "title": "Position and Velocity Estimation Accuracy in MIMO-OFDM ISAC Networks: A Fisher Information Analysis", "comment": "19 pages, 6 figures, 3 tables", "summary": "Integrated sensing and communication (ISAC) is a core technology for future\nwireless networks, enabling high-resolution sensing and reliable data\ntransmission within a unified radio platform. This paper develops a theoretical\nframework to assess the estimation accuracy of target position and velocity in\nheterogeneous orthogonal frequency division multiplexing (OFDM)-based ISAC\nnetworks with multiple cooperative and distributed multiple-input\nmultiple-output (MIMO) base stations (BSs). Using Fisher information analysis,\nwe first derive closed-form Cram\\'er-Rao lower bounds (CRLBs) for target\nlocalization in single monostatic and bistatic configurations. We then analyze\nthe benefits of BS cooperation by deriving CRLBs for joint position and\nvelocity estimation in a general setting that encompasses multiple cooperating\nmonostatic systems and multistatic networks with multiple transmitters (Txs)\nand receivers (Rxs). The influence of key system parameters, including the\nnumber of BSs, bandwidth, antenna array configuration, and network geometry, is\nsystematically examined. Numerical results highlight the performance gains\nenabled by cooperative sensing and provide insights to guide the design of\nfuture ISAC systems."}
{"id": "2507.01339", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01339", "abs": "https://arxiv.org/abs/2507.01339", "authors": ["Yutong Wen", "Minje Kim", "Paris Smaragdis"], "title": "User-guided Generative Source Separation", "comment": null, "summary": "Music source separation (MSS) aims to extract individual instrument sources\nfrom their mixture. While most existing methods focus on the widely adopted\nfour-stem separation setup (vocals, bass, drums, and other instruments), this\napproach lacks the flexibility needed for real-world applications. To address\nthis, we propose GuideSep, a diffusion-based MSS model capable of\ninstrument-agnostic separation beyond the four-stem setup. GuideSep is\nconditioned on multiple inputs: a waveform mimicry condition, which can be\neasily provided by humming or playing the target melody, and mel-spectrogram\ndomain masks, which offer additional guidance for separation. Unlike prior\napproaches that relied on fixed class labels or sound queries, our conditioning\nscheme, coupled with the generative approach, provides greater flexibility and\napplicability. Additionally, we design a mask-prediction baseline using the\nsame model architecture to systematically compare predictive and generative\napproaches. Our objective and subjective evaluations demonstrate that GuideSep\nachieves high-quality separation while enabling more versatile instrument\nextraction, highlighting the potential of user participation in the\ndiffusion-based generative process for MSS. Our code and demo page are\navailable at https://yutongwen.github.io/GuideSep/"}
{"id": "2507.01356", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01356", "abs": "https://arxiv.org/abs/2507.01356", "authors": ["Hitoshi Suda", "Shinnosuke Takamichi", "Satoru Fukayama"], "title": "Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora", "comment": "Accepted at Interspeech 2025", "summary": "Perceived voice likability plays a crucial role in various social\ninteractions, such as partner selection and advertising. A system that provides\nreference likable voice samples tailored to target audiences would enable users\nto adjust their speaking style and voice quality, facilitating smoother\ncommunication. To this end, we propose a voice conversion method that controls\nthe likability of input speech while preserving both speaker identity and\nlinguistic content. To improve training data scalability, we train a likability\npredictor on an existing voice likability dataset and employ it to\nautomatically annotate a large speech synthesis corpus with likability ratings.\nExperimental evaluations reveal a significant correlation between the\npredictor's outputs and human-provided likability ratings. Subjective and\nobjective evaluations further demonstrate that the proposed approach\neffectively controls voice likability while preserving both speaker identity\nand linguistic content."}
{"id": "2507.01771", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.01771", "abs": "https://arxiv.org/abs/2507.01771", "authors": ["G. Andrew Siciliano", "Keith A. LeGrand", "Jackson Kulik"], "title": "Higher-Order Tensor-Based Deferral of Gaussian Splitting for Orbit Uncertainty Propagation", "comment": null, "summary": "Accurate propagation of orbital uncertainty is essential for a range of\napplications within space domain awareness. Adaptive Gaussian mixture-based\napproaches offer tractable nonlinear uncertainty propagation through splitting\nmixands to increase resolution in areas of stronger nonlinearities, as well as\nby reducing mixands to prevent unnecessary computational effort. Recent work\nintroduced principled heuristics that incorporate information from the system\ndynamics and initial uncertainty to determine optimal directions for splitting.\nThis paper develops adaptive uncertainty propagation methods based on these\nrobust splitting techniques. A deferred splitting algorithm tightly integrated\nwith higher-order splitting techniques is proposed and shown to offer\nsubstantial gains in computational efficiency without sacrificing accuracy.\nSecond-order propagation of mixand moments is also seen to improve accuracy\nwhile retaining significant computational savings from deferred splitting.\nDifferent immediate and deferred splitting methods are compared in three\nrepresentative test cases, including a geostationary orbit, a Molniya orbit,\nand a periodic three-body orbit."}
{"id": "2507.01563", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07 (Primary), 68T10 (Secondary)", "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.01563", "abs": "https://arxiv.org/abs/2507.01563", "authors": ["Marco Giordano", "Stefano Giacomelli", "Claudia Rinaldi", "Fabio Graziosi"], "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware", "comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437", "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices."}
{"id": "2507.01611", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility."}
{"id": "2507.01799", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01799", "abs": "https://arxiv.org/abs/2507.01799", "authors": ["Steffen Schieler", "Sebastian Semper", "Christian Schneider", "Reiner Thomä"], "title": "Measurement-based Evaluation of CNN-based Detection and Estimation for ISAC Systems", "comment": "2025 IEEE International Radar Conference (RADAR)", "summary": "In wireless sensing applications, such as ISAC, one of the first crucial\nsignal processing steps is the detection and estimation targets from a channel\nestimate. Effective algorithms in this context must be robust across a broad\nSNR range, capable of handling an unknown number of targets, and\ncomputationally efficient for real-time implementation. During the last decade,\ndifferent Machine Learning methods have emerged as promising solutions, either\nas standalone models or as complementing existing techniques. However, since\nmodels are often trained and evaluated on synthetic data from existing models,\napplying them to measurement is challenging. All the while, training directly\non measurement data is prohibitive in complex propagation scenarios as a\ngroundtruth is not available. Therefore, in this paper, we train a CNN approach\nfor target detection and estimation on synthetic data and evaluate it on\nmeasurement data from a suburban outdoor measurement. Using knowledge of the\nenvironment as well as available groundtruth positions, we study the detection\nprobability and accuracy of our approach. The results demonstrate that our\napproach works on measurement data and is suitable for joint detection and\nestimation of sensing targets in ISAC systems."}
{"id": "2507.01582", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01582", "abs": "https://arxiv.org/abs/2507.01582", "authors": ["Jing Luo", "Xinyu Yang", "Jie Wei"], "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder", "comment": "Accepted by IEEE SMC 2025", "summary": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain."}
{"id": "2507.01750", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01750", "abs": "https://arxiv.org/abs/2507.01750", "authors": ["Jose A. Lopez", "Georg Stemmer", "Héctor Cordourier Maruri"], "title": "Generalizable Detection of Audio Deepfakes", "comment": "8 pages, 3 figures", "summary": "In this paper, we present our comprehensive study aimed at enhancing the\ngeneralization capabilities of audio deepfake detection models. We investigate\nthe performance of various pre-trained backbones, including Wav2Vec2, WavLM,\nand Whisper, across a diverse set of datasets, including those from the\nASVspoof challenges and additional sources. Our experiments focus on the\neffects of different data augmentation strategies and loss functions on model\nperformance. The results of our research demonstrate substantial enhancements\nin the generalization capabilities of audio deepfake detection models,\nsurpassing the performance of the top-ranked single system in the ASVspoof 5\nChallenge. This study contributes valuable insights into the optimization of\naudio models for more robust deepfake detection and facilitates future research\nin this critical area."}
{"id": "2507.01611", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility."}
{"id": "2507.01805", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01805", "abs": "https://arxiv.org/abs/2507.01805", "authors": ["Alejandro Sosa Welford", "Leonardo Pepino"], "title": "A Dataset for Automatic Assessment of TTS Quality in Spanish", "comment": "5 pages, 2 figures. Accepted at Interspeech 2025", "summary": "This work addresses the development of a database for the automatic\nassessment of text-to-speech (TTS) systems in Spanish, aiming to improve the\naccuracy of naturalness prediction models. The dataset consists of 4,326 audio\nsamples from 52 different TTS systems and human voices and is, up to our\nknowledge, the first of its kind in Spanish. To label the audios, a subjective\ntest was designed based on the ITU-T Rec. P.807 standard and completed by 92\nparticipants. Furthermore, the utility of the collected dataset was validated\nby training automatic naturalness prediction systems. We explored two\napproaches: fine-tuning an existing model originally trained for English, and\ntraining small downstream networks on top of frozen self-supervised speech\nmodels. Our models achieve a mean absolute error of 0.8 on a five-point MOS\nscale. Further analysis demonstrates the quality and diversity of the developed\ndataset, and its potential to advance TTS research in Spanish."}
{"id": "2507.01821", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanuël A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications."}
{"id": "2507.01821", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanuël A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications."}
{"id": "2507.01339", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01339", "abs": "https://arxiv.org/abs/2507.01339", "authors": ["Yutong Wen", "Minje Kim", "Paris Smaragdis"], "title": "User-guided Generative Source Separation", "comment": null, "summary": "Music source separation (MSS) aims to extract individual instrument sources\nfrom their mixture. While most existing methods focus on the widely adopted\nfour-stem separation setup (vocals, bass, drums, and other instruments), this\napproach lacks the flexibility needed for real-world applications. To address\nthis, we propose GuideSep, a diffusion-based MSS model capable of\ninstrument-agnostic separation beyond the four-stem setup. GuideSep is\nconditioned on multiple inputs: a waveform mimicry condition, which can be\neasily provided by humming or playing the target melody, and mel-spectrogram\ndomain masks, which offer additional guidance for separation. Unlike prior\napproaches that relied on fixed class labels or sound queries, our conditioning\nscheme, coupled with the generative approach, provides greater flexibility and\napplicability. Additionally, we design a mask-prediction baseline using the\nsame model architecture to systematically compare predictive and generative\napproaches. Our objective and subjective evaluations demonstrate that GuideSep\nachieves high-quality separation while enabling more versatile instrument\nextraction, highlighting the potential of user participation in the\ndiffusion-based generative process for MSS. Our code and demo page are\navailable at https://yutongwen.github.io/GuideSep/"}
{"id": "2507.01563", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07 (Primary), 68T10 (Secondary)", "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.01563", "abs": "https://arxiv.org/abs/2507.01563", "authors": ["Marco Giordano", "Stefano Giacomelli", "Claudia Rinaldi", "Fabio Graziosi"], "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware", "comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437", "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices."}
{"id": "2507.01582", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01582", "abs": "https://arxiv.org/abs/2507.01582", "authors": ["Jing Luo", "Xinyu Yang", "Jie Wei"], "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder", "comment": "Accepted by IEEE SMC 2025", "summary": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain."}
{"id": "2507.01805", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01805", "abs": "https://arxiv.org/abs/2507.01805", "authors": ["Alejandro Sosa Welford", "Leonardo Pepino"], "title": "A Dataset for Automatic Assessment of TTS Quality in Spanish", "comment": "5 pages, 2 figures. Accepted at Interspeech 2025", "summary": "This work addresses the development of a database for the automatic\nassessment of text-to-speech (TTS) systems in Spanish, aiming to improve the\naccuracy of naturalness prediction models. The dataset consists of 4,326 audio\nsamples from 52 different TTS systems and human voices and is, up to our\nknowledge, the first of its kind in Spanish. To label the audios, a subjective\ntest was designed based on the ITU-T Rec. P.807 standard and completed by 92\nparticipants. Furthermore, the utility of the collected dataset was validated\nby training automatic naturalness prediction systems. We explored two\napproaches: fine-tuning an existing model originally trained for English, and\ntraining small downstream networks on top of frozen self-supervised speech\nmodels. Our models achieve a mean absolute error of 0.8 on a five-point MOS\nscale. Further analysis demonstrates the quality and diversity of the developed\ndataset, and its potential to advance TTS research in Spanish."}
{"id": "2507.01021", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time."}
{"id": "2507.01227", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.01227", "abs": "https://arxiv.org/abs/2507.01227", "authors": ["Son T. Duong", "Tho Le-Ngoc"], "title": "Degrees of Freedom of Spatial Multiplexing in Distance Domain of Arbitrary Continuous-Aperture Array in Near-Field Region", "comment": "13 pages and 13 figures. Part of this work has been submitted to IEEE\n  Globecom 2025 (under review). This version has been submitted to IEEE\n  Transactions on Communications for possible publication", "summary": "Extremely large aperture array operating in the near-field regime unlocks\nadditional spatial resources that can be exploited to simultaneously serve\nmultiple users even when they share the same angular direction, a capability\nnot achievable in conventional far-field systems. A fundamental question,\nhowever, remains: What is the maximum spatial degree of freedom (DoF) of\nspatial multiplexing in the distance domain?\n  In this paper, we address this open problem by investigating the spatial DoF\nof a line-of-sight (LoS) channel between a large two-dimensional transmit\naperture and a linear receive array with collinearly-aligned elements (i.e., at\nthe same angular direction) but located at different distances from the\ntransmit aperture. We assume that both the aperture and linear array are\ncontinuous-aperture (CAP) arrays with an infinite number of elements and\ninfinitesimal spacing, which establishes an upper bound for the spatial degrees\nof freedom (DoF) in the case of finite elements. First, we assume an ideal case\nwhere the transmit array is a single piece and the linear array is on the broad\nside of the transmit array. By reformulating the channel as an integral\noperator with a Hermitian convolution kernel, we derive a closed-form\nexpression for the spatial DoF via the Fourier transform. Our analysis shows\nthat the spatial DoF in the distance domain is predominantly determined by the\nextreme boundaries of the array rather than its detailed interior structure. We\nfurther extend the framework to non-broadside configurations by employing a\nprojection method, which effectively converts the spatial DoF to an equivalent\nbroadside case. Finally, we extend our analytical framework to the modular\narray, which shows the spatial DoF gain over the single-piece array given the\nconstraint of the physical length of the array."}
{"id": "2507.01021", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time."}
{"id": "2507.01022", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01022", "abs": "https://arxiv.org/abs/2507.01022", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "title": "Workflow-Based Evaluation of Music Generation Systems", "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field."}
{"id": "2507.01230", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.01230", "abs": "https://arxiv.org/abs/2507.01230", "authors": ["Yuri Abramovich", "Victor Abramovich", "Tanit Pongsiri"], "title": "Numerical Techniques for the Maximum Likelihood Toeplitz Covariance Matrix Estimation: Part I. Symmetric Toeplitz Matrices", "comment": null, "summary": "In several applications, one must estimate a real-valued (symmetric) Toeplitz\ncovariance matrix, typically shifted by the conjugated diagonal matrices of\nphase progression and phase \"calibration\" errors. Unlike the Hermitian Toeplitz\ncovariance matrices, these symmetric matrices have a unique potential\ncapability of being estimated regardless of these beam-steering phase\nprogression and/or phase \"calibration\" errors. This unique capability is the\nprimary motivation of this paper."}
{"id": "2507.01022", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01022", "abs": "https://arxiv.org/abs/2507.01022", "authors": ["Shayan Dadman", "Bernt Arild Bremdal", "Andreas Bergsland"], "title": "Workflow-Based Evaluation of Music Generation Systems", "comment": "54 pages, 3 figures, 6 tables, 5 appendices", "summary": "This study presents an exploratory evaluation of Music Generation Systems\n(MGS) within contemporary music production workflows by examining eight\nopen-source systems. The evaluation framework combines technical insights with\npractical experimentation through criteria specifically designed to investigate\nthe practical and creative affordances of the systems within the iterative,\nnon-linear nature of music production. Employing a single-evaluator methodology\nas a preliminary phase, this research adopts a mixed approach utilizing\nqualitative methods to form hypotheses subsequently assessed through\nquantitative metrics. The selected systems represent architectural diversity\nacross both symbolic and audio-based music generation approaches, spanning\ncomposition, arrangement, and sound design tasks. The investigation addresses\nlimitations of current MGS in music production, challenges and opportunities\nfor workflow integration, and development potential as collaborative tools\nwhile maintaining artistic authenticity. Findings reveal these systems function\nprimarily as complementary tools enhancing rather than replacing human\nexpertise. They exhibit limitations in maintaining thematic and structural\ncoherence that emphasize the indispensable role of human creativity in tasks\ndemanding emotional depth and complex decision-making. This study contributes a\nstructured evaluation framework that considers the iterative nature of music\ncreation. It identifies methodological refinements necessary for subsequent\ncomprehensive evaluations and determines viable areas for AI integration as\ncollaborative tools in creative workflows. The research provides\nempirically-grounded insights to guide future development in the field."}
{"id": "2507.01024", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01024", "abs": "https://arxiv.org/abs/2507.01024", "authors": ["George Igwegbe", "Martins Awojide", "Mboh Bless", "Nirel Kadzo"], "title": "Hello Afrika: Speech Commands in Kinyarwanda", "comment": "Data Science Africa, 2024", "summary": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a\nlanguage which are essential for non-contact control of and activation of\nlarger AI systems in devices used in everyday life especially for persons with\ndisabilities. Currently, there is a dearth of speech command models for African\nlanguages. The Hello Afrika project aims to address this issue and its first\niteration is focused on the Kinyarwanda language since the country has shown\ninterest in developing speech recognition technologies culminating in one of\nthe largest datasets on Mozilla Common Voice. The model was built off a custom\nspeech command corpus made up of general directives, numbers, and a wake word.\nThe final model was deployed on multiple devices (PC, Mobile Phone and Edge\nDevices) and the performance was assessed using suitable metrics."}
{"id": "2507.01286", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01286", "abs": "https://arxiv.org/abs/2507.01286", "authors": ["Zai Yang", "Sikai Ge", "Wenlong Wang"], "title": "Pursuing the limit of chirp parameter identifiability: A computational approach", "comment": "10 pages", "summary": "In this paper, it is shown that a necessary condition for unique\nidentifiability of $K$ chirps from $N$ regularly spaced samples of their\nmixture is $N\\geq 2K$ when $K\\geq 2$. A necessary and sufficient condition is\nthat a rank-constrained matrix optimization problem has a unique solution; this\nis the first result of such kind. An algorithm is proposed to solve the\noptimization problem and to identify the parameters numerically. The lower\nbound of $N=2K$ is shown to be tight by providing diverse problem instances for\nwhich the proposed algorithm succeeds to identify the parameters. The\nadvantageous performance of the proposed algorithm is also demonstrated\ncompared with the state of the art."}
{"id": "2507.01024", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01024", "abs": "https://arxiv.org/abs/2507.01024", "authors": ["George Igwegbe", "Martins Awojide", "Mboh Bless", "Nirel Kadzo"], "title": "Hello Afrika: Speech Commands in Kinyarwanda", "comment": "Data Science Africa, 2024", "summary": "Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a\nlanguage which are essential for non-contact control of and activation of\nlarger AI systems in devices used in everyday life especially for persons with\ndisabilities. Currently, there is a dearth of speech command models for African\nlanguages. The Hello Afrika project aims to address this issue and its first\niteration is focused on the Kinyarwanda language since the country has shown\ninterest in developing speech recognition technologies culminating in one of\nthe largest datasets on Mozilla Common Voice. The model was built off a custom\nspeech command corpus made up of general directives, numbers, and a wake word.\nThe final model was deployed on multiple devices (PC, Mobile Phone and Edge\nDevices) and the performance was assessed using suitable metrics."}
{"id": "2507.01172", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01172", "abs": "https://arxiv.org/abs/2507.01172", "authors": ["Marios Glytsos", "Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "title": "Classical Guitar Duet Separation using GuitarDuets -- a Dataset of Real and Synthesized Guitar Recordings", "comment": "In Proceedings of the 25th International Society for Music\n  Information Retrieval Conference (ISMIR 2024), San Francisco, USA, November\n  2024. The dataset is available at: https://zenodo.org/records/12802440", "summary": "Recent advancements in music source separation (MSS) have focused in the\nmulti-timbral case, with existing architectures tailored for the separation of\ndistinct instruments, overlooking thus the challenge of separating instruments\nwith similar timbral characteristics. Addressing this gap, our work focuses on\nmonotimbral MSS, specifically within the context of classical guitar duets. To\nthis end, we introduce the GuitarDuets dataset, featuring a combined total of\napproximately three hours of real and synthesized classical guitar duet\nrecordings, as well as note-level annotations of the synthesized duets. We\nperform an extensive cross-dataset evaluation by adapting Demucs, a\nstate-of-the-art MSS architecture, to monotimbral source separation.\nFurthermore, we develop a joint permutation-invariant transcription and\nseparation framework, to exploit note event predictions as auxiliary\ninformation. Our results indicate that utilizing both the real and synthesized\nsubsets of GuitarDuets leads to improved separation performance in an\nindependently recorded test set compared to utilizing solely one subset. We\nalso find that while the availability of ground-truth note labels greatly helps\nthe performance of the separation network, the predicted note estimates result\nonly in marginal improvement. Finally, we discuss the behavior of commonly\nutilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS."}
{"id": "2507.01427", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01427", "abs": "https://arxiv.org/abs/2507.01427", "authors": ["Jun Wu", "Yuye Shi", "Weijie Yuan", "Qingqing Cheng", "Buyi Li", "Xinyuan Wei"], "title": "SDR-Empowered Environment Sensing Design and Experimental Validation Using OTFS-ISAC Signals", "comment": null, "summary": "This paper investigates the system design and experimental validation of\nintegrated sensing and communication (ISAC) for environmental sensing, which is\nexpected to be a critical enabler for next-generation wireless networks. We\nadvocate exploiting orthogonal time frequency space (OTFS) modulation for its\ninherent sparsity and stability in delay-Doppler (DD) domain channels,\nfacilitating a low-overhead environment sensing design. Moreover, a\ncomprehensive environmental sensing framework is developed, encompassing DD\ndomain channel estimation, target localization, and experimental validation. In\nparticular, we first explore the OTFS channel estimation in the presence of\nfractional delay and Doppler shifts. Given the estimated parameters, we propose\na three-ellipse positioning algorithm to localize the target's position,\nfollowed by determining the mobile transmitter's velocity. Additionally, to\nevaluate the performance of our proposed design, we conduct extensive\nsimulations and experiments using a software-defined radio (SDR)-based platform\nwith universal software radio peripheral (USRP). The experimental validations\ndemonstrate that our proposed approach outperforms the benchmarks in terms of\nlocalization accuracy and velocity estimation, confirming its effectiveness in\npractical environmental sensing applications."}
{"id": "2507.01348", "categories": ["eess.AS", "cs.SD", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01348", "abs": "https://arxiv.org/abs/2507.01348", "authors": ["Cheng Zhuangfei", "Zhang Guangyan", "Tu Zehai", "Song Yangyang", "Mao Shuiyang", "Jiao Xiaoqi", "Li Jingyu", "Guo Yiwen", "Wu Jiasong"], "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech", "comment": "10 pages, includes references, 4 figures, 4 tables", "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."}
{"id": "2507.01348", "categories": ["eess.AS", "cs.SD", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01348", "abs": "https://arxiv.org/abs/2507.01348", "authors": ["Cheng Zhuangfei", "Zhang Guangyan", "Tu Zehai", "Song Yangyang", "Mao Shuiyang", "Jiao Xiaoqi", "Li Jingyu", "Guo Yiwen", "Wu Jiasong"], "title": "SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech", "comment": "10 pages, includes references, 4 figures, 4 tables", "summary": "Foreign accent conversion (FAC) in speech processing remains a challenging\ntask. Building on the remarkable success of large language models (LLMs) in\nText-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based\ntechniques for FAC, which we term SpeechAccentLLM. At the core of this\nframework, we introduce SpeechCodeVAE, the first model to integrate\nconnectionist temporal classification (CTC) directly into codebook\ndiscretization for speech content tokenization. This novel architecture\ngenerates tokens with a unique \"locality\" property, as validated by experiments\ndemonstrating optimal trade-offs among content faithfulness, temporal\ncoherence, and structural recoverability. Then, to address data scarcity for\nthe FAC module, we adopted a multitask learning strategy that jointly trains\nthe FAC and TTS modules. Beyond mitigating data limitations, this approach\nyielded accelerated convergence and superior speech quality compared to\nstandalone FAC training. Moreover, leveraging the salient properties of our\ndiscrete speech representations, we introduce SpeechRestorer, a postprocessing\narchitecture designed to refine LLM-generated outputs. This module effectively\nmitigates stochastic errors prevalent in LLM inference pipelines while\nenhancing prosodic continuity, as validated by ablation experiments."}
{"id": "2507.01445", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01445", "abs": "https://arxiv.org/abs/2507.01445", "authors": ["Yanfeng Zhang", "Xu Zhu", "Yujie Liu", "Yong Liang Guan", "David González G.", "Vincent K. N. Lau"], "title": "Basis Expansion Extrapolation based Long-Term Channel Prediction for Massive MIMO OTFS Systems", "comment": null, "summary": "Massive multi-input multi-output (MIMO) combined with orthogonal time\nfrequency space (OTFS) modulation has emerged as a promising technique for\nhigh-mobility scenarios. However, its performance could be severely degraded\ndue to channel aging caused by user mobility and high processing latency. In\nthis paper, an integrated scheme of uplink (UL) channel estimation and downlink\n(DL) channel prediction is proposed to alleviate channel aging in time division\nduplex (TDD) massive MIMO-OTFS systems. Specifically, first, an iterative basis\nexpansion model (BEM) based UL channel estimation scheme is proposed to\naccurately estimate UL channels with the aid of carefully designed OTFS frame\npattern. Then a set of Slepian sequences are used to model the estimated UL\nchannels, and the dynamic Slepian coefficients are fitted by a set of\northogonal polynomials. A channel predictor is derived to predict DL channels\nby iteratively extrapolating the Slepian coefficients. Simulation results\nverify that the proposed UL channel estimation and DL channel prediction\nschemes outperform the existing schemes in terms of normalized mean square\nerror of channel estimation/prediction and DL spectral efficiency, with less\npilot overhead."}
{"id": "2507.01349", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01349", "abs": "https://arxiv.org/abs/2507.01349", "authors": ["Hitoshi Suda", "Junya Koguchi", "Shunsuke Yoshida", "Tomohiko Nakamura", "Satoru Fukayama", "Jun Ogata"], "title": "IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups", "comment": "Accepted at ISMIR 2025", "summary": "Japanese idol groups, comprising performers known as \"idols,\" are an\nindispensable part of Japanese pop culture. They frequently appear in live\nconcerts and television programs, entertaining audiences with their singing and\ndancing. Similar to other J-pop songs, idol group music covers a wide range of\nstyles, with various types of chord progressions and instrumental arrangements.\nThese tracks often feature numerous instruments and employ complex mastering\ntechniques, resulting in high signal loudness. Additionally, most songs include\na song division (utawari) structure, in which members alternate between singing\nsolos and performing together. Hence, these songs are well-suited for\nbenchmarking various music information processing techniques such as singer\ndiarization, music source separation, and automatic chord estimation under\nchallenging conditions. Focusing on these characteristics, we constructed a\nsong corpus titled IdolSongsJp by commissioning professional composers to\ncreate 15 tracks in the style of Japanese idol groups. This corpus includes not\nonly mastered audio tracks but also stems for music source separation, dry\nvocal tracks, and chord annotations. This paper provides a detailed description\nof the corpus, demonstrates its diversity through comparisons with real-world\nidol group songs, and presents its application in evaluating several music\ninformation processing techniques."}
{"id": "2507.01349", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01349", "abs": "https://arxiv.org/abs/2507.01349", "authors": ["Hitoshi Suda", "Junya Koguchi", "Shunsuke Yoshida", "Tomohiko Nakamura", "Satoru Fukayama", "Jun Ogata"], "title": "IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups", "comment": "Accepted at ISMIR 2025", "summary": "Japanese idol groups, comprising performers known as \"idols,\" are an\nindispensable part of Japanese pop culture. They frequently appear in live\nconcerts and television programs, entertaining audiences with their singing and\ndancing. Similar to other J-pop songs, idol group music covers a wide range of\nstyles, with various types of chord progressions and instrumental arrangements.\nThese tracks often feature numerous instruments and employ complex mastering\ntechniques, resulting in high signal loudness. Additionally, most songs include\na song division (utawari) structure, in which members alternate between singing\nsolos and performing together. Hence, these songs are well-suited for\nbenchmarking various music information processing techniques such as singer\ndiarization, music source separation, and automatic chord estimation under\nchallenging conditions. Focusing on these characteristics, we constructed a\nsong corpus titled IdolSongsJp by commissioning professional composers to\ncreate 15 tracks in the style of Japanese idol groups. This corpus includes not\nonly mastered audio tracks but also stems for music source separation, dry\nvocal tracks, and chord annotations. This paper provides a detailed description\nof the corpus, demonstrates its diversity through comparisons with real-world\nidol group songs, and presents its application in evaluating several music\ninformation processing techniques."}
{"id": "2507.01575", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01575", "abs": "https://arxiv.org/abs/2507.01575", "authors": ["Masood Jan", "Wafa Njima", "Xun Zhang", "Alexander Artemenko"], "title": "Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability", "comment": "Accepted for publication in the IEEE VTC2025-Spring Conference, 7\n  pages", "summary": "Accurate indoor localization is crucial in industrial environments. Visible\nLight Communication (VLC) has emerged as a promising solution, offering high\naccuracy, energy efficiency, and minimal electromagnetic interference. However,\nVLC-based indoor localization faces challenges due to environmental\nvariability, such as lighting fluctuations and obstacles. To address these\nchallenges, we propose a Transfer Learning (TL)-based approach for VLC-based\nindoor localization. Using real-world data collected at a BOSCH factory, the TL\nframework integrates a deep neural network (DNN) to improve localization\naccuracy by 47\\%, reduce energy consumption by 32\\%, and decrease computational\ntime by 40\\% compared to the conventional models. The proposed solution is\nhighly adaptable under varying environmental conditions and achieves similar\naccuracy with only 30\\% of the dataset, making it a cost-efficient and scalable\noption for industrial applications in Industry 4.0."}
{"id": "2507.01356", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01356", "abs": "https://arxiv.org/abs/2507.01356", "authors": ["Hitoshi Suda", "Shinnosuke Takamichi", "Satoru Fukayama"], "title": "Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora", "comment": "Accepted at Interspeech 2025", "summary": "Perceived voice likability plays a crucial role in various social\ninteractions, such as partner selection and advertising. A system that provides\nreference likable voice samples tailored to target audiences would enable users\nto adjust their speaking style and voice quality, facilitating smoother\ncommunication. To this end, we propose a voice conversion method that controls\nthe likability of input speech while preserving both speaker identity and\nlinguistic content. To improve training data scalability, we train a likability\npredictor on an existing voice likability dataset and employ it to\nautomatically annotate a large speech synthesis corpus with likability ratings.\nExperimental evaluations reveal a significant correlation between the\npredictor's outputs and human-provided likability ratings. Subjective and\nobjective evaluations further demonstrate that the proposed approach\neffectively controls voice likability while preserving both speaker identity\nand linguistic content."}
{"id": "2507.01356", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01356", "abs": "https://arxiv.org/abs/2507.01356", "authors": ["Hitoshi Suda", "Shinnosuke Takamichi", "Satoru Fukayama"], "title": "Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora", "comment": "Accepted at Interspeech 2025", "summary": "Perceived voice likability plays a crucial role in various social\ninteractions, such as partner selection and advertising. A system that provides\nreference likable voice samples tailored to target audiences would enable users\nto adjust their speaking style and voice quality, facilitating smoother\ncommunication. To this end, we propose a voice conversion method that controls\nthe likability of input speech while preserving both speaker identity and\nlinguistic content. To improve training data scalability, we train a likability\npredictor on an existing voice likability dataset and employ it to\nautomatically annotate a large speech synthesis corpus with likability ratings.\nExperimental evaluations reveal a significant correlation between the\npredictor's outputs and human-provided likability ratings. Subjective and\nobjective evaluations further demonstrate that the proposed approach\neffectively controls voice likability while preserving both speaker identity\nand linguistic content."}
{"id": "2507.01624", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01624", "abs": "https://arxiv.org/abs/2507.01624", "authors": ["Cong Zhou", "Changsheng You", "Shuo Shi", "Weidong Mei"], "title": "Frequency-switching Array Enhanced Physical-Layer Security in Terahertz Bands: A Movable Antenna Perspective", "comment": "In this paper, we propose to enhance physical-layer security by using\n  a new frequency-switching array, which is equivalent to movable antennas", "summary": "In this paper, we propose a new frequency-switching array (FSA) enhanced\nphysical-layer security (PLS) system in terahertz bands, where the carrier\nfrequency can be flexibly switched and small frequency offsets can be imposed\non each antenna at Alice, so as to eliminate information wiretapping by\nundesired eavesdroppers. First, we analytically show that by flexibly\ncontrolling the carrier frequency parameters, FSAs can effectively form\nuniform/non-uniform sparse arrays, hence resembling movable antennas (MAs) in\nthe control of inter-antenna spacing and providing additional degree-of-freedom\n(DoF) in the beam control. Although the proposed FSA experiences additional\npath-gain attenuation in the received signals, it can overcome several hardware\nand signal processing issues incurred by MAs, such as limited positioning\naccuracy, considerable response latency, and demanding hardware and energy\ncost. To shed useful insights, we first consider a secrecy-guaranteed problem\nwith a null-steering constraint for which maximum ratio transmission (MRT)\nbeamformer is considered at Alice and the frequency offsets are set as uniform\nfrequency increment. Interestingly, it is shown that the proposed FSA can\nflexibly realize null-steering over Eve in both the angular domain (by tuning\ncarrier frequency) and range domain (by controlling per-antenna frequency\noffset), thereby achieving improved PLS performance. Then, for the general\ncase, we propose an efficient algorithm to solve the formulated non-convex\nproblem by using the block coordinate descent (BCD) and projected gradient\nascent (PGA) techniques. Finally, numerical results demonstrate the convergence\nof the proposed optimization algorithm and its superiority over fixed-position\narrays (FPAs) in terms of secrecy-rate performance."}
{"id": "2507.01611", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility."}
{"id": "2507.01611", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility."}
{"id": "2507.01728", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01728", "abs": "https://arxiv.org/abs/2507.01728", "authors": ["Hao Wei", "Wanli Ni", "Wen Wang", "Wenjun Xu", "Dusit Niyato", "Ping Zhang"], "title": "Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach", "comment": null, "summary": "This letter proposes UniToCom, a unified token communication paradigm that\ntreats tokens as the fundamental units for both processing and wireless\ntransmission. Specifically, to enable efficient token representations, we\npropose a generative information bottleneck (GenIB) principle, which\nfacilitates the learning of tokens that preserve essential information while\nsupporting reliable generation across multiple modalities. By doing this,\nGenIB-based tokenization is conducive to improving the communication efficiency\nand reducing computational complexity. Additionally, we develop $\\sigma$-GenIB\nto address the challenges of variance collapse in autoregressive modeling,\nmaintaining representational diversity and stability. Moreover, we employ a\ncausal Transformer-based multimodal large language model (MLLM) at the receiver\nto unify the processing of both discrete and continuous tokens under the\nnext-token prediction paradigm. Simulation results validate the effectiveness\nand superiority of the proposed UniToCom compared to baselines under dynamic\nchannel conditions. By integrating token processing with MLLMs, UniToCom\nenables scalable and generalizable communication in favor of multimodal\nunderstanding and generation, providing a potential solution for\nnext-generation intelligent communications."}
{"id": "2507.01750", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01750", "abs": "https://arxiv.org/abs/2507.01750", "authors": ["Jose A. Lopez", "Georg Stemmer", "Héctor Cordourier Maruri"], "title": "Generalizable Detection of Audio Deepfakes", "comment": "8 pages, 3 figures", "summary": "In this paper, we present our comprehensive study aimed at enhancing the\ngeneralization capabilities of audio deepfake detection models. We investigate\nthe performance of various pre-trained backbones, including Wav2Vec2, WavLM,\nand Whisper, across a diverse set of datasets, including those from the\nASVspoof challenges and additional sources. Our experiments focus on the\neffects of different data augmentation strategies and loss functions on model\nperformance. The results of our research demonstrate substantial enhancements\nin the generalization capabilities of audio deepfake detection models,\nsurpassing the performance of the top-ranked single system in the ASVspoof 5\nChallenge. This study contributes valuable insights into the optimization of\naudio models for more robust deepfake detection and facilitates future research\nin this critical area."}
{"id": "2507.01750", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.01750", "abs": "https://arxiv.org/abs/2507.01750", "authors": ["Jose A. Lopez", "Georg Stemmer", "Héctor Cordourier Maruri"], "title": "Generalizable Detection of Audio Deepfakes", "comment": "8 pages, 3 figures", "summary": "In this paper, we present our comprehensive study aimed at enhancing the\ngeneralization capabilities of audio deepfake detection models. We investigate\nthe performance of various pre-trained backbones, including Wav2Vec2, WavLM,\nand Whisper, across a diverse set of datasets, including those from the\nASVspoof challenges and additional sources. Our experiments focus on the\neffects of different data augmentation strategies and loss functions on model\nperformance. The results of our research demonstrate substantial enhancements\nin the generalization capabilities of audio deepfake detection models,\nsurpassing the performance of the top-ranked single system in the ASVspoof 5\nChallenge. This study contributes valuable insights into the optimization of\naudio models for more robust deepfake detection and facilitates future research\nin this critical area."}
{"id": "2507.01743", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01743", "abs": "https://arxiv.org/abs/2507.01743", "authors": ["Lorenzo Pucci", "Luca Arcangeloni", "Andrea Giorgetti"], "title": "Position and Velocity Estimation Accuracy in MIMO-OFDM ISAC Networks: A Fisher Information Analysis", "comment": "19 pages, 6 figures, 3 tables", "summary": "Integrated sensing and communication (ISAC) is a core technology for future\nwireless networks, enabling high-resolution sensing and reliable data\ntransmission within a unified radio platform. This paper develops a theoretical\nframework to assess the estimation accuracy of target position and velocity in\nheterogeneous orthogonal frequency division multiplexing (OFDM)-based ISAC\nnetworks with multiple cooperative and distributed multiple-input\nmultiple-output (MIMO) base stations (BSs). Using Fisher information analysis,\nwe first derive closed-form Cram\\'er-Rao lower bounds (CRLBs) for target\nlocalization in single monostatic and bistatic configurations. We then analyze\nthe benefits of BS cooperation by deriving CRLBs for joint position and\nvelocity estimation in a general setting that encompasses multiple cooperating\nmonostatic systems and multistatic networks with multiple transmitters (Txs)\nand receivers (Rxs). The influence of key system parameters, including the\nnumber of BSs, bandwidth, antenna array configuration, and network geometry, is\nsystematically examined. Numerical results highlight the performance gains\nenabled by cooperative sensing and provide insights to guide the design of\nfuture ISAC systems."}
{"id": "2507.01821", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanuël A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications."}
{"id": "2507.01765", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01765", "abs": "https://arxiv.org/abs/2507.01765", "authors": ["Sarina Meyer", "Ekaterina Kolos", "Ngoc Thang Vu"], "title": "First Steps Towards Voice Anonymization for Code-Switching Speech", "comment": "accepted at Interspeech 2025", "summary": "The goal of voice anonymization is to modify an audio such that the true\nidentity of its speaker is hidden. Research on this task is typically limited\nto the same English read speech datasets, thus the efficacy of current methods\nfor other types of speech data remains unknown. In this paper, we present the\nfirst investigation of voice anonymization for the multilingual phenomenon of\ncode-switching speech. We prepare two corpora for this task and propose\nadaptations to a multilingual anonymization model to make it applicable for\ncode-switching speech. By testing the anonymization performance of this and two\nlanguage-independent methods on the datasets, we find that only the\nmultilingual system performs well in terms of privacy and utility preservation.\nFurthermore, we observe challenges in performing utility evaluations on this\ndata because of its spontaneous character and the limited code-switching\nsupport by the multilingual speech recognition model."}
{"id": "2507.01771", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.01771", "abs": "https://arxiv.org/abs/2507.01771", "authors": ["G. Andrew Siciliano", "Keith A. LeGrand", "Jackson Kulik"], "title": "Higher-Order Tensor-Based Deferral of Gaussian Splitting for Orbit Uncertainty Propagation", "comment": null, "summary": "Accurate propagation of orbital uncertainty is essential for a range of\napplications within space domain awareness. Adaptive Gaussian mixture-based\napproaches offer tractable nonlinear uncertainty propagation through splitting\nmixands to increase resolution in areas of stronger nonlinearities, as well as\nby reducing mixands to prevent unnecessary computational effort. Recent work\nintroduced principled heuristics that incorporate information from the system\ndynamics and initial uncertainty to determine optimal directions for splitting.\nThis paper develops adaptive uncertainty propagation methods based on these\nrobust splitting techniques. A deferred splitting algorithm tightly integrated\nwith higher-order splitting techniques is proposed and shown to offer\nsubstantial gains in computational efficiency without sacrificing accuracy.\nSecond-order propagation of mixand moments is also seen to improve accuracy\nwhile retaining significant computational savings from deferred splitting.\nDifferent immediate and deferred splitting methods are compared in three\nrepresentative test cases, including a geostationary orbit, a Molniya orbit,\nand a periodic three-body orbit."}
{"id": "2507.01821", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanuël A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications."}
{"id": "2507.01799", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01799", "abs": "https://arxiv.org/abs/2507.01799", "authors": ["Steffen Schieler", "Sebastian Semper", "Christian Schneider", "Reiner Thomä"], "title": "Measurement-based Evaluation of CNN-based Detection and Estimation for ISAC Systems", "comment": "2025 IEEE International Radar Conference (RADAR)", "summary": "In wireless sensing applications, such as ISAC, one of the first crucial\nsignal processing steps is the detection and estimation targets from a channel\nestimate. Effective algorithms in this context must be robust across a broad\nSNR range, capable of handling an unknown number of targets, and\ncomputationally efficient for real-time implementation. During the last decade,\ndifferent Machine Learning methods have emerged as promising solutions, either\nas standalone models or as complementing existing techniques. However, since\nmodels are often trained and evaluated on synthetic data from existing models,\napplying them to measurement is challenging. All the while, training directly\non measurement data is prohibitive in complex propagation scenarios as a\ngroundtruth is not available. Therefore, in this paper, we train a CNN approach\nfor target detection and estimation on synthetic data and evaluate it on\nmeasurement data from a suburban outdoor measurement. Using knowledge of the\nenvironment as well as available groundtruth positions, we study the detection\nprobability and accuracy of our approach. The results demonstrate that our\napproach works on measurement data and is suitable for joint detection and\nestimation of sensing targets in ISAC systems."}
{"id": "2507.01888", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01888", "abs": "https://arxiv.org/abs/2507.01888", "authors": ["Nina R. Benway", "Saba Tabatabaee", "Dongliang Wang", "Benjamin Munson", "Jonathan L. Preston", "Carol Espy-Wilson"], "title": "Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders", "comment": "This manuscript is in submission for publication. It has not yet been\n  peer reviewed", "summary": "Purpose: This study evaluated whether articulatory kinematics, inferred by\nArticulatory Phonology speech inversion neural networks, aligned with\nperceptual ratings of /r/ and /s/ in the speech of children with speech sound\ndisorders.\n  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961\nutterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual\nratings were standardized using the novel 5-point PERCEPT Rating Scale and\ntraining protocol. Two research questions examined if the articulatory patterns\nof inferred vocal tract variables aligned with the perceptual error category\nfor the phones investigated (e.g., tongue tip is more anterior in dentalized\n/s/ productions than in correct /s/). A third research question examined if\ngradient PERCEPT Rating Scale scores predicted articulatory proximity to\ncorrect productions.\n  Results: Estimated marginal means from linear mixed models supported 17 of 18\n/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,\nestimated marginal means from a second linear mixed model supported 7 of 15\nhypotheses, particularly those related to the tongue tip. A third linear mixed\nmodel revealed that PERCEPT Rating Scale scores significantly predicted\narticulatory proximity of errored phones to correct productions.\n  Conclusion: Inferred vocal tract variables differentiated category and\nmagnitude of articulatory errors for /r/, and to a lesser extent for /s/,\naligning with perceptual judgments. These findings support the clinical\ninterpretability of speech inversion vocal tract variables and the PERCEPT\nRating Scale in quantifying articulatory proximity to the target sound,\nparticularly for /r/."}
{"id": "2507.01611", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01611", "abs": "https://arxiv.org/abs/2507.01611", "authors": ["Shaowen Chen", "Tomoki Toda"], "title": "QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model", "comment": "This manuscript is currently under review for publication in the IEEE\n  Transactions on Audio, Speech, and Language Processing. This work has been\n  submitted to the IEEE for possible publication", "summary": "Vocoders, encoding speech signals into acoustic features and allowing for\nspeech signal reconstruction from them, have been studied for decades.\nRecently, the rise of deep learning has particularly driven the development of\nneural vocoders to generate high-quality speech signals. On the other hand, the\nexisting end-to-end neural vocoders suffer from a black-box nature that blinds\nthe speech production mechanism and the intrinsic structure of speech,\nresulting in the ambiguity of separately modeling source excitation and\nresonance characteristics and the loss of flexibly synthesizing or modifying\nspeech with high quality. Moreover, their sequence-wise waveform generation\nusually requires complicated networks, leading to substantial time consumption.\nIn this work, inspired by the quasi-harmonic model (QHM) that represents speech\nas sparse components, we combine the neural network and QHM synthesis process\nto propose a novel framework for the neural vocoder. Accordingly, speech\nsignals can be encoded into autoregressive moving average (ARMA) functions to\nmodel the resonance characteristics, yielding accurate estimates of the\namplitudes and phases of quasi-harmonics at any frequency. Subsequently, the\nspeech can be resynthesized and arbitrarily modified in terms of pitch shifting\nand time stretching with high quality, whereas the time consumption and network\nsize decrease. The experiments indicate that the proposed method leverages the\nstrengths of QHM, the ARMA model, and neural networks, leading to the\noutperformance of our methods over other methods in terms of generation speed,\nsynthesis quality, and modification flexibility."}
{"id": "2507.01339", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01339", "abs": "https://arxiv.org/abs/2507.01339", "authors": ["Yutong Wen", "Minje Kim", "Paris Smaragdis"], "title": "User-guided Generative Source Separation", "comment": null, "summary": "Music source separation (MSS) aims to extract individual instrument sources\nfrom their mixture. While most existing methods focus on the widely adopted\nfour-stem separation setup (vocals, bass, drums, and other instruments), this\napproach lacks the flexibility needed for real-world applications. To address\nthis, we propose GuideSep, a diffusion-based MSS model capable of\ninstrument-agnostic separation beyond the four-stem setup. GuideSep is\nconditioned on multiple inputs: a waveform mimicry condition, which can be\neasily provided by humming or playing the target melody, and mel-spectrogram\ndomain masks, which offer additional guidance for separation. Unlike prior\napproaches that relied on fixed class labels or sound queries, our conditioning\nscheme, coupled with the generative approach, provides greater flexibility and\napplicability. Additionally, we design a mask-prediction baseline using the\nsame model architecture to systematically compare predictive and generative\napproaches. Our objective and subjective evaluations demonstrate that GuideSep\nachieves high-quality separation while enabling more versatile instrument\nextraction, highlighting the potential of user participation in the\ndiffusion-based generative process for MSS. Our code and demo page are\navailable at https://yutongwen.github.io/GuideSep/"}
{"id": "2507.01821", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.01821", "abs": "https://arxiv.org/abs/2507.01821", "authors": ["Hesam Eftekhari", "Srikanth Raj Chetupalli", "Shrishti Saha Shetu", "Emanuël A. P. Habets", "Oliver Thiergart"], "title": "Low-Complexity Neural Wind Noise Reduction for Audio Recordings", "comment": null, "summary": "Wind noise significantly degrades the quality of outdoor audio recordings,\nyet remains difficult to suppress in real-time on resource-constrained devices.\nIn this work, we propose a low-complexity single-channel deep neural network\nthat leverages the spectral characteristics of wind noise. Experimental results\nshow that our method achieves performance comparable to the state-of-the-art\nlow-complexity ULCNet model. The proposed model, with only 249K parameters and\nroughly 73 MHz of computational power, is suitable for embedded and mobile\naudio applications."}
{"id": "2507.01563", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07 (Primary), 68T10 (Secondary)", "B.1.5; B.4.5; C.3; C.4; I.2; K.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.01563", "abs": "https://arxiv.org/abs/2507.01563", "authors": ["Marco Giordano", "Stefano Giacomelli", "Claudia Rinaldi", "Fabio Graziosi"], "title": "Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware", "comment": "10 pages, 10 figures, submitted to\n  https://internetofsounds2025.ieee-is2.org/. arXiv admin note: text overlap\n  with arXiv:2506.23437", "summary": "We present a full-stack emergency vehicle (EV) siren detection system\ndesigned for real-time deployment on embedded hardware. The proposed approach\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\nEPANNs, and optimized for binary sound event detection under urban acoustic\nconditions. A key contribution is the creation of curated and semantically\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\ndeveloped using a custom AudioSet-Tools framework to overcome the low\nreliability of standard AudioSet annotations. The system is deployed on a\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\na multithreaded inference engine with adaptive frame sizing, probability\nsmoothing, and a decision-state machine to control false positive activations.\nA remote WebSocket interface provides real-time monitoring and facilitates live\ndemonstration capabilities. Performance is evaluated using both framewise and\nevent-based metrics across multiple configurations. Results show the system\nachieves low-latency detection with improved robustness under realistic audio\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\nSED solutions that can form distributed acoustic monitoring networks, enabling\ncollaborative emergency vehicle tracking across smart city infrastructures\nthrough WebSocket connectivity on low-cost edge devices."}
{"id": "2507.01582", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01582", "abs": "https://arxiv.org/abs/2507.01582", "authors": ["Jing Luo", "Xinyu Yang", "Jie Wei"], "title": "Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder", "comment": "Accepted by IEEE SMC 2025", "summary": "The creativity of classical music arises not only from composers who craft\nthe musical sheets but also from performers who interpret the static notations\nwith expressive nuances. This paper addresses the challenge of generating\nclassical piano performances from scratch, aiming to emulate the dual roles of\ncomposer and pianist in the creative process. We introduce the Expressive\nCompound Word (ECP) representation, which effectively captures both the\nmetrical structure and expressive nuances of classical performances. Building\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\n(VQ-VAE) branch that generates score-related content, representing the\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\ninformation and an orthogonal Transformer decoder for efficient compound tokens\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\ngenerates classical performances with superior musical quality compared to\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\nmusical score datasets contribute to a significant performance gain."}
{"id": "2507.01805", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01805", "abs": "https://arxiv.org/abs/2507.01805", "authors": ["Alejandro Sosa Welford", "Leonardo Pepino"], "title": "A Dataset for Automatic Assessment of TTS Quality in Spanish", "comment": "5 pages, 2 figures. Accepted at Interspeech 2025", "summary": "This work addresses the development of a database for the automatic\nassessment of text-to-speech (TTS) systems in Spanish, aiming to improve the\naccuracy of naturalness prediction models. The dataset consists of 4,326 audio\nsamples from 52 different TTS systems and human voices and is, up to our\nknowledge, the first of its kind in Spanish. To label the audios, a subjective\ntest was designed based on the ITU-T Rec. P.807 standard and completed by 92\nparticipants. Furthermore, the utility of the collected dataset was validated\nby training automatic naturalness prediction systems. We explored two\napproaches: fine-tuning an existing model originally trained for English, and\ntraining small downstream networks on top of frozen self-supervised speech\nmodels. Our models achieve a mean absolute error of 0.8 on a five-point MOS\nscale. Further analysis demonstrates the quality and diversity of the developed\ndataset, and its potential to advance TTS research in Spanish."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [eess.AS](#eess.AS) [Total: 12]
- [cs.SD](#cs.SD) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Degrees of Freedom of Spatial Multiplexing in Distance Domain of Arbitrary Continuous-Aperture Array in Near-Field Region](https://arxiv.org/abs/2507.01227)
*Son T. Duong,Tho Le-Ngoc*

Main category: eess.SP

TL;DR: 论文研究了在近场区域运行的极大孔径阵列的空间自由度（DoF），探讨了在距离域中空间复用的最大自由度，并给出了闭式解。


<details>
  <summary>Details</summary>
Motivation: 传统远场系统无法在相同角度方向上同时服务多个用户，而近场区域的空间资源可以解决这一问题。研究旨在确定距离域中空间复用的最大自由度。

Method: 通过分析二维发射孔径与线性接收阵列之间的视距（LoS）信道，假设为连续孔径阵列，推导了空间自由度的闭式表达式，并扩展到非正侧配置和模块化阵列。

Result: 研究表明，距离域的空间自由度主要由阵列的极端边界决定，而非其内部结构。模块化阵列在物理长度限制下比单块阵列具有更高的自由度增益。

Conclusion: 论文为近场区域的空间复用提供了理论框架，证明了阵列边界对自由度的主导作用，并展示了模块化阵列的优势。

Abstract: Extremely large aperture array operating in the near-field regime unlocks
additional spatial resources that can be exploited to simultaneously serve
multiple users even when they share the same angular direction, a capability
not achievable in conventional far-field systems. A fundamental question,
however, remains: What is the maximum spatial degree of freedom (DoF) of
spatial multiplexing in the distance domain?
  In this paper, we address this open problem by investigating the spatial DoF
of a line-of-sight (LoS) channel between a large two-dimensional transmit
aperture and a linear receive array with collinearly-aligned elements (i.e., at
the same angular direction) but located at different distances from the
transmit aperture. We assume that both the aperture and linear array are
continuous-aperture (CAP) arrays with an infinite number of elements and
infinitesimal spacing, which establishes an upper bound for the spatial degrees
of freedom (DoF) in the case of finite elements. First, we assume an ideal case
where the transmit array is a single piece and the linear array is on the broad
side of the transmit array. By reformulating the channel as an integral
operator with a Hermitian convolution kernel, we derive a closed-form
expression for the spatial DoF via the Fourier transform. Our analysis shows
that the spatial DoF in the distance domain is predominantly determined by the
extreme boundaries of the array rather than its detailed interior structure. We
further extend the framework to non-broadside configurations by employing a
projection method, which effectively converts the spatial DoF to an equivalent
broadside case. Finally, we extend our analytical framework to the modular
array, which shows the spatial DoF gain over the single-piece array given the
constraint of the physical length of the array.

</details>


### [2] [Numerical Techniques for the Maximum Likelihood Toeplitz Covariance Matrix Estimation: Part I. Symmetric Toeplitz Matrices](https://arxiv.org/abs/2507.01230)
*Yuri Abramovich,Victor Abramovich,Tanit Pongsiri*

Main category: eess.SP

TL;DR: 论文探讨了对称Toeplitz协方差矩阵在存在相位误差时的独特估计能力。


<details>
  <summary>Details</summary>
Motivation: 对称Toeplitz协方差矩阵能够在存在波束导向相位误差或校准误差的情况下被估计，这是其独特能力，也是本文的主要动机。

Method: 未明确提及具体方法，但围绕对称Toeplitz协方差矩阵的估计展开。

Result: 未明确提及具体结果，但强调了对称Toeplitz矩阵在相位误差下的估计潜力。

Conclusion: 对称Toeplitz协方差矩阵在相位误差下仍具有估计能力，这一特性具有重要应用价值。

Abstract: In several applications, one must estimate a real-valued (symmetric) Toeplitz
covariance matrix, typically shifted by the conjugated diagonal matrices of
phase progression and phase "calibration" errors. Unlike the Hermitian Toeplitz
covariance matrices, these symmetric matrices have a unique potential
capability of being estimated regardless of these beam-steering phase
progression and/or phase "calibration" errors. This unique capability is the
primary motivation of this paper.

</details>


### [3] [Pursuing the limit of chirp parameter identifiability: A computational approach](https://arxiv.org/abs/2507.01286)
*Zai Yang,Sikai Ge,Wenlong Wang*

Main category: eess.SP

TL;DR: 论文证明了从N个规则采样点中唯一识别K个线性调频信号的混合的必要条件是N≥2K（K≥2），并提出了一个算法解决优化问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何从采样数据中唯一识别多个线性调频信号的混合参数。

Method: 提出一个算法解决秩约束矩阵优化问题，并通过数值实验验证其有效性。

Result: 证明了N=2K是紧的下界，算法在多种实例中成功识别参数，且性能优于现有方法。

Conclusion: 论文首次给出了唯一可识别的充要条件，并提出了高效的算法，为信号处理提供了新工具。

Abstract: In this paper, it is shown that a necessary condition for unique
identifiability of $K$ chirps from $N$ regularly spaced samples of their
mixture is $N\geq 2K$ when $K\geq 2$. A necessary and sufficient condition is
that a rank-constrained matrix optimization problem has a unique solution; this
is the first result of such kind. An algorithm is proposed to solve the
optimization problem and to identify the parameters numerically. The lower
bound of $N=2K$ is shown to be tight by providing diverse problem instances for
which the proposed algorithm succeeds to identify the parameters. The
advantageous performance of the proposed algorithm is also demonstrated
compared with the state of the art.

</details>


### [4] [SDR-Empowered Environment Sensing Design and Experimental Validation Using OTFS-ISAC Signals](https://arxiv.org/abs/2507.01427)
*Jun Wu,Yuye Shi,Weijie Yuan,Qingqing Cheng,Buyi Li,Xinyuan Wei*

Main category: eess.SP

TL;DR: 本文研究了集成传感与通信（ISAC）的系统设计和实验验证，提出利用OTFS调制的稀疏性和稳定性，开发了环境感知框架，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 探索下一代无线网络中环境感知的关键技术，利用OTFS调制的优势实现高效低开销的感知设计。

Method: 提出基于OTFS调制的DD域信道估计、目标定位和三椭圆定位算法，并通过SDR平台进行实验验证。

Result: 实验结果表明，所提方法在定位精度和速度估计上优于基准方法。

Conclusion: 该方法在环境感知应用中具有实际有效性，为下一代无线网络提供了可行的解决方案。

Abstract: This paper investigates the system design and experimental validation of
integrated sensing and communication (ISAC) for environmental sensing, which is
expected to be a critical enabler for next-generation wireless networks. We
advocate exploiting orthogonal time frequency space (OTFS) modulation for its
inherent sparsity and stability in delay-Doppler (DD) domain channels,
facilitating a low-overhead environment sensing design. Moreover, a
comprehensive environmental sensing framework is developed, encompassing DD
domain channel estimation, target localization, and experimental validation. In
particular, we first explore the OTFS channel estimation in the presence of
fractional delay and Doppler shifts. Given the estimated parameters, we propose
a three-ellipse positioning algorithm to localize the target's position,
followed by determining the mobile transmitter's velocity. Additionally, to
evaluate the performance of our proposed design, we conduct extensive
simulations and experiments using a software-defined radio (SDR)-based platform
with universal software radio peripheral (USRP). The experimental validations
demonstrate that our proposed approach outperforms the benchmarks in terms of
localization accuracy and velocity estimation, confirming its effectiveness in
practical environmental sensing applications.

</details>


### [5] [Basis Expansion Extrapolation based Long-Term Channel Prediction for Massive MIMO OTFS Systems](https://arxiv.org/abs/2507.01445)
*Yanfeng Zhang,Xu Zhu,Yujie Liu,Yong Liang Guan,David González G.,Vincent K. N. Lau*

Main category: eess.SP

TL;DR: 提出了一种结合上行链路（UL）信道估计和下行链路（DL）信道预测的方案，以缓解TDD大规模MIMO-OTFS系统中的信道老化问题。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO与OTFS调制在高移动性场景中表现优异，但用户移动性和高处理延迟会导致信道老化，严重影响性能。

Method: 采用迭代基扩展模型（BEM）进行UL信道估计，利用Slepian序列建模UL信道，并通过正交多项式拟合动态Slepian系数，进而预测DL信道。

Result: 仿真结果表明，所提方案在信道估计/预测的归一化均方误差和DL频谱效率方面优于现有方案，且导频开销更小。

Conclusion: 该方案有效缓解了信道老化问题，提升了系统性能。

Abstract: Massive multi-input multi-output (MIMO) combined with orthogonal time
frequency space (OTFS) modulation has emerged as a promising technique for
high-mobility scenarios. However, its performance could be severely degraded
due to channel aging caused by user mobility and high processing latency. In
this paper, an integrated scheme of uplink (UL) channel estimation and downlink
(DL) channel prediction is proposed to alleviate channel aging in time division
duplex (TDD) massive MIMO-OTFS systems. Specifically, first, an iterative basis
expansion model (BEM) based UL channel estimation scheme is proposed to
accurately estimate UL channels with the aid of carefully designed OTFS frame
pattern. Then a set of Slepian sequences are used to model the estimated UL
channels, and the dynamic Slepian coefficients are fitted by a set of
orthogonal polynomials. A channel predictor is derived to predict DL channels
by iteratively extrapolating the Slepian coefficients. Simulation results
verify that the proposed UL channel estimation and DL channel prediction
schemes outperform the existing schemes in terms of normalized mean square
error of channel estimation/prediction and DL spectral efficiency, with less
pilot overhead.

</details>


### [6] [Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability](https://arxiv.org/abs/2507.01575)
*Masood Jan,Wafa Njima,Xun Zhang,Alexander Artemenko*

Main category: eess.SP

TL;DR: 提出了一种基于迁移学习的VLC室内定位方法，显著提升了精度、降低了能耗和计算时间。


<details>
  <summary>Details</summary>
Motivation: 工业环境中精准室内定位需求迫切，VLC技术虽有潜力但受环境变化影响。

Method: 采用迁移学习框架结合深度神经网络，利用真实工厂数据优化模型。

Result: 定位精度提升47%，能耗降低32%，计算时间减少40%，且仅需30%数据集即可达到类似精度。

Conclusion: 该方法高效、适应性强，适合工业4.0应用。

Abstract: Accurate indoor localization is crucial in industrial environments. Visible
Light Communication (VLC) has emerged as a promising solution, offering high
accuracy, energy efficiency, and minimal electromagnetic interference. However,
VLC-based indoor localization faces challenges due to environmental
variability, such as lighting fluctuations and obstacles. To address these
challenges, we propose a Transfer Learning (TL)-based approach for VLC-based
indoor localization. Using real-world data collected at a BOSCH factory, the TL
framework integrates a deep neural network (DNN) to improve localization
accuracy by 47\%, reduce energy consumption by 32\%, and decrease computational
time by 40\% compared to the conventional models. The proposed solution is
highly adaptable under varying environmental conditions and achieves similar
accuracy with only 30\% of the dataset, making it a cost-efficient and scalable
option for industrial applications in Industry 4.0.

</details>


### [7] [Frequency-switching Array Enhanced Physical-Layer Security in Terahertz Bands: A Movable Antenna Perspective](https://arxiv.org/abs/2507.01624)
*Cong Zhou,Changsheng You,Shuo Shi,Weidong Mei*

Main category: eess.SP

TL;DR: 提出了一种基于频率切换阵列（FSA）的太赫兹频段物理层安全系统，通过灵活切换载波频率和小频率偏移消除窃听。


<details>
  <summary>Details</summary>
Motivation: 解决传统移动天线（MAs）在硬件和信号处理上的限制，如定位精度低、延迟高和成本高，同时提升物理层安全性能。

Method: 通过控制载波频率参数形成稀疏阵列，结合最大比率传输（MRT）波束成形和块坐标下降（BCD）与投影梯度上升（PGA）算法优化。

Result: FSA能在角度和距离域灵活实现零陷，显著提升保密率性能，优于固定位置阵列（FPAs）。

Conclusion: FSA系统在太赫兹频段具有硬件友好性和高安全性，为物理层安全提供了新思路。

Abstract: In this paper, we propose a new frequency-switching array (FSA) enhanced
physical-layer security (PLS) system in terahertz bands, where the carrier
frequency can be flexibly switched and small frequency offsets can be imposed
on each antenna at Alice, so as to eliminate information wiretapping by
undesired eavesdroppers. First, we analytically show that by flexibly
controlling the carrier frequency parameters, FSAs can effectively form
uniform/non-uniform sparse arrays, hence resembling movable antennas (MAs) in
the control of inter-antenna spacing and providing additional degree-of-freedom
(DoF) in the beam control. Although the proposed FSA experiences additional
path-gain attenuation in the received signals, it can overcome several hardware
and signal processing issues incurred by MAs, such as limited positioning
accuracy, considerable response latency, and demanding hardware and energy
cost. To shed useful insights, we first consider a secrecy-guaranteed problem
with a null-steering constraint for which maximum ratio transmission (MRT)
beamformer is considered at Alice and the frequency offsets are set as uniform
frequency increment. Interestingly, it is shown that the proposed FSA can
flexibly realize null-steering over Eve in both the angular domain (by tuning
carrier frequency) and range domain (by controlling per-antenna frequency
offset), thereby achieving improved PLS performance. Then, for the general
case, we propose an efficient algorithm to solve the formulated non-convex
problem by using the block coordinate descent (BCD) and projected gradient
ascent (PGA) techniques. Finally, numerical results demonstrate the convergence
of the proposed optimization algorithm and its superiority over fixed-position
arrays (FPAs) in terms of secrecy-rate performance.

</details>


### [8] [Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach](https://arxiv.org/abs/2507.01728)
*Hao Wei,Wanli Ni,Wen Wang,Wenjun Xu,Dusit Niyato,Ping Zhang*

Main category: eess.SP

TL;DR: UniToCom提出了一种统一的令牌通信范式，将令牌作为处理和无线传输的基本单位，通过生成信息瓶颈（GenIB）原则优化令牌表示，提升通信效率并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态通信中令牌表示和生成的效率与可靠性问题，同时应对动态信道条件下的挑战。

Method: 提出GenIB原则优化令牌表示，开发σ-GenIB防止方差崩溃，并使用基于因果Transformer的多模态大语言模型（MLLM）统一处理离散和连续令牌。

Result: 仿真结果表明，UniToCom在动态信道条件下优于基线方法，提升了通信效率和可扩展性。

Conclusion: UniToCom为下一代智能通信提供了可扩展且通用的解决方案，支持多模态理解和生成。

Abstract: This letter proposes UniToCom, a unified token communication paradigm that
treats tokens as the fundamental units for both processing and wireless
transmission. Specifically, to enable efficient token representations, we
propose a generative information bottleneck (GenIB) principle, which
facilitates the learning of tokens that preserve essential information while
supporting reliable generation across multiple modalities. By doing this,
GenIB-based tokenization is conducive to improving the communication efficiency
and reducing computational complexity. Additionally, we develop $\sigma$-GenIB
to address the challenges of variance collapse in autoregressive modeling,
maintaining representational diversity and stability. Moreover, we employ a
causal Transformer-based multimodal large language model (MLLM) at the receiver
to unify the processing of both discrete and continuous tokens under the
next-token prediction paradigm. Simulation results validate the effectiveness
and superiority of the proposed UniToCom compared to baselines under dynamic
channel conditions. By integrating token processing with MLLMs, UniToCom
enables scalable and generalizable communication in favor of multimodal
understanding and generation, providing a potential solution for
next-generation intelligent communications.

</details>


### [9] [Position and Velocity Estimation Accuracy in MIMO-OFDM ISAC Networks: A Fisher Information Analysis](https://arxiv.org/abs/2507.01743)
*Lorenzo Pucci,Luca Arcangeloni,Andrea Giorgetti*

Main category: eess.SP

TL;DR: 本文提出了一种理论框架，用于评估异构OFDM-based ISAC网络中目标位置和速度的估计精度，通过Fisher信息分析推导CRLBs，并探讨了基站协作的优势。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要高分辨率感知和可靠数据传输的统一平台，ISAC技术是实现这一目标的核心。

Method: 使用Fisher信息分析，推导单静态和双静态配置下的CRLBs，并扩展到多基站协作和多静态网络。

Result: 数值结果展示了协作感知的性能增益，并分析了关键系统参数的影响。

Conclusion: 研究为未来ISAC系统的设计提供了理论指导和实践参考。

Abstract: Integrated sensing and communication (ISAC) is a core technology for future
wireless networks, enabling high-resolution sensing and reliable data
transmission within a unified radio platform. This paper develops a theoretical
framework to assess the estimation accuracy of target position and velocity in
heterogeneous orthogonal frequency division multiplexing (OFDM)-based ISAC
networks with multiple cooperative and distributed multiple-input
multiple-output (MIMO) base stations (BSs). Using Fisher information analysis,
we first derive closed-form Cram\'er-Rao lower bounds (CRLBs) for target
localization in single monostatic and bistatic configurations. We then analyze
the benefits of BS cooperation by deriving CRLBs for joint position and
velocity estimation in a general setting that encompasses multiple cooperating
monostatic systems and multistatic networks with multiple transmitters (Txs)
and receivers (Rxs). The influence of key system parameters, including the
number of BSs, bandwidth, antenna array configuration, and network geometry, is
systematically examined. Numerical results highlight the performance gains
enabled by cooperative sensing and provide insights to guide the design of
future ISAC systems.

</details>


### [10] [Higher-Order Tensor-Based Deferral of Gaussian Splitting for Orbit Uncertainty Propagation](https://arxiv.org/abs/2507.01771)
*G. Andrew Siciliano,Keith A. LeGrand,Jackson Kulik*

Main category: eess.SP

TL;DR: 本文提出了一种基于自适应高斯混合的轨道不确定性传播方法，通过延迟分裂算法和高阶分裂技术提高计算效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 轨道不确定性的精确传播对空间领域感知至关重要，需要解决非线性传播中的计算效率和精度问题。

Method: 采用自适应高斯混合方法，结合延迟分裂算法和高阶分裂技术，优化分裂方向以减少计算负担。

Result: 延迟分裂算法显著提高了计算效率，二阶传播进一步提升了精度，测试案例验证了方法的有效性。

Conclusion: 该方法在计算效率和精度上取得了平衡，适用于多种轨道场景。

Abstract: Accurate propagation of orbital uncertainty is essential for a range of
applications within space domain awareness. Adaptive Gaussian mixture-based
approaches offer tractable nonlinear uncertainty propagation through splitting
mixands to increase resolution in areas of stronger nonlinearities, as well as
by reducing mixands to prevent unnecessary computational effort. Recent work
introduced principled heuristics that incorporate information from the system
dynamics and initial uncertainty to determine optimal directions for splitting.
This paper develops adaptive uncertainty propagation methods based on these
robust splitting techniques. A deferred splitting algorithm tightly integrated
with higher-order splitting techniques is proposed and shown to offer
substantial gains in computational efficiency without sacrificing accuracy.
Second-order propagation of mixand moments is also seen to improve accuracy
while retaining significant computational savings from deferred splitting.
Different immediate and deferred splitting methods are compared in three
representative test cases, including a geostationary orbit, a Molniya orbit,
and a periodic three-body orbit.

</details>


### [11] [Measurement-based Evaluation of CNN-based Detection and Estimation for ISAC Systems](https://arxiv.org/abs/2507.01799)
*Steffen Schieler,Sebastian Semper,Christian Schneider,Reiner Thomä*

Main category: eess.SP

TL;DR: 论文提出了一种基于CNN的目标检测与估计方法，用于无线传感应用（如ISAC），并在实测数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决无线传感中目标检测与估计的挑战，包括广泛的SNR范围、未知目标数量和实时计算需求。

Method: 使用合成数据训练CNN模型，并在郊区户外实测数据上进行评估。

Result: 方法在实测数据上表现良好，适用于ISAC系统中的联合检测与估计。

Conclusion: CNN方法在无线传感中具有实际应用潜力，尤其在复杂传播场景下。

Abstract: In wireless sensing applications, such as ISAC, one of the first crucial
signal processing steps is the detection and estimation targets from a channel
estimate. Effective algorithms in this context must be robust across a broad
SNR range, capable of handling an unknown number of targets, and
computationally efficient for real-time implementation. During the last decade,
different Machine Learning methods have emerged as promising solutions, either
as standalone models or as complementing existing techniques. However, since
models are often trained and evaluated on synthetic data from existing models,
applying them to measurement is challenging. All the while, training directly
on measurement data is prohibitive in complex propagation scenarios as a
groundtruth is not available. Therefore, in this paper, we train a CNN approach
for target detection and estimation on synthetic data and evaluate it on
measurement data from a suburban outdoor measurement. Using knowledge of the
environment as well as available groundtruth positions, we study the detection
probability and accuracy of our approach. The results demonstrate that our
approach works on measurement data and is suitable for joint detection and
estimation of sensing targets in ISAC systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [12] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: 提出了一种开源框架，用于命令式听写，填补了资源密集型在线系统与高延迟批处理之间的空白。


<details>
  <summary>Details</summary>
Motivation: 解决现有听写系统在资源利用和延迟方面的不足，提供更高效的解决方案。

Method: 使用语音活动检测（VAD）分割音频，并利用Whisper模型并行转录，支持多音频复用。

Result: 在印度约15%的法庭中部署，实时数据显示随着用户并发增加，延迟显著降低。

Conclusion: 该框架高效且兼容性强，适用于多种ASR架构，开源实现可供实时交互。

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


### [13] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: 本研究通过评估八种开源音乐生成系统（MGS），探讨其在当代音乐制作工作流中的实用性，提出了一种结合技术与实践的评估框架，并发现MGS主要作为辅助工具而非替代人类创造力。


<details>
  <summary>Details</summary>
Motivation: 探索音乐生成系统在音乐制作中的实际应用潜力，填补现有系统在主题和结构连贯性上的不足。

Method: 采用单评估者方法，结合定性和定量分析，评估八种具有架构多样性的开源MGS。

Result: MGS主要作为辅助工具，增强而非替代人类创造力，但在情感深度和复杂决策任务中仍需人类主导。

Conclusion: 研究提出了一个结构化评估框架，为未来MGS开发和AI在创意工作流中的整合提供了实证指导。

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


### [14] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Main category: eess.AS

TL;DR: Hello Afrika项目旨在解决非洲语言语音命令模型的缺乏问题，首阶段专注于卢旺达语，构建了一个基于通用指令、数字和唤醒词的语音命令模型，并在多种设备上部署和评估。


<details>
  <summary>Details</summary>
Motivation: 非洲语言缺乏语音命令模型，限制了非接触式控制和AI系统的应用，尤其是对残障人士。Hello Afrika项目旨在填补这一空白。

Method: 项目构建了一个自定义语音命令语料库，包含通用指令、数字和唤醒词，并基于此开发模型，部署于PC、手机和边缘设备。

Result: 模型在多种设备上部署，并通过合适的指标评估性能。

Conclusion: Hello Afrika项目成功为卢旺达语开发了语音命令模型，为非洲语言的语音技术发展奠定了基础。

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [15] [Classical Guitar Duet Separation using GuitarDuets -- a Dataset of Real and Synthesized Guitar Recordings](https://arxiv.org/abs/2507.01172)
*Marios Glytsos,Christos Garoufis,Athanasia Zlatintsi,Petros Maragos*

Main category: eess.AS

TL;DR: 论文提出了一种针对单调音色音乐源分离（MSS）的方法，专注于古典吉他二重奏，并引入了GuitarDuets数据集。通过改进Demucs架构和结合音符预测，提升了分离效果。


<details>
  <summary>Details</summary>
Motivation: 现有MSS方法多针对多音色分离，忽视了相似音色乐器的分离问题。本文旨在填补这一空白，专注于单调音色MSS，尤其是古典吉他二重奏。

Method: 引入GuitarDuets数据集，改进Demucs架构用于单调音色分离，并开发了联合排列不变转录和分离框架，利用音符预测作为辅助信息。

Result: 结合真实和合成数据集的训练提升了分离效果，但音符预测的辅助作用有限。同时讨论了SDR和SI-SDR等指标在单调音色MSS中的表现。

Conclusion: 论文展示了单调音色MSS的可行性，并强调了数据集多样性对性能的重要性，同时指出了音符预测的潜力与局限。

Abstract: Recent advancements in music source separation (MSS) have focused in the
multi-timbral case, with existing architectures tailored for the separation of
distinct instruments, overlooking thus the challenge of separating instruments
with similar timbral characteristics. Addressing this gap, our work focuses on
monotimbral MSS, specifically within the context of classical guitar duets. To
this end, we introduce the GuitarDuets dataset, featuring a combined total of
approximately three hours of real and synthesized classical guitar duet
recordings, as well as note-level annotations of the synthesized duets. We
perform an extensive cross-dataset evaluation by adapting Demucs, a
state-of-the-art MSS architecture, to monotimbral source separation.
Furthermore, we develop a joint permutation-invariant transcription and
separation framework, to exploit note event predictions as auxiliary
information. Our results indicate that utilizing both the real and synthesized
subsets of GuitarDuets leads to improved separation performance in an
independently recorded test set compared to utilizing solely one subset. We
also find that while the availability of ground-truth note labels greatly helps
the performance of the separation network, the predicted note estimates result
only in marginal improvement. Finally, we discuss the behavior of commonly
utilized metrics, such as SDR and SI-SDR, in the context of monotimbral MSS.

</details>


### [16] [SpeechAccentLLM: A Unified Framework for Foreign Accent Conversion and Text to Speech](https://arxiv.org/abs/2507.01348)
*Cheng Zhuangfei,Zhang Guangyan,Tu Zehai,Song Yangyang,Mao Shuiyang,Jiao Xiaoqi,Li Jingyu,Guo Yiwen,Wu Jiasong*

Main category: eess.AS

TL;DR: 该研究提出了一种基于大语言模型（LLM）的外语口音转换（FAC）框架SpeechAccentLLM，通过SpeechCodeVAE模型实现语音内容标记化，并结合多任务学习和后处理模块SpeechRestorer提升性能。


<details>
  <summary>Details</summary>
Motivation: 外语口音转换在语音处理中仍具挑战性，研究旨在利用LLM在TTS任务中的成功经验，解决FAC任务中的数据稀缺和性能问题。

Method: 提出SpeechCodeVAE模型，将CTC直接集成到码书离散化中；采用多任务学习联合训练FAC和TTS模块；引入后处理模块SpeechRestorer优化输出。

Result: 实验验证了标记的“局部性”特性，多任务学习加速收敛并提升语音质量，SpeechRestorer有效减少随机错误并增强韵律连续性。

Conclusion: SpeechAccentLLM框架在内容忠实性、时序一致性和结构可恢复性方面取得平衡，为FAC任务提供了高效解决方案。

Abstract: Foreign accent conversion (FAC) in speech processing remains a challenging
task. Building on the remarkable success of large language models (LLMs) in
Text-to-Speech (TTS) tasks, this study investigates the adaptation of LLM-based
techniques for FAC, which we term SpeechAccentLLM. At the core of this
framework, we introduce SpeechCodeVAE, the first model to integrate
connectionist temporal classification (CTC) directly into codebook
discretization for speech content tokenization. This novel architecture
generates tokens with a unique "locality" property, as validated by experiments
demonstrating optimal trade-offs among content faithfulness, temporal
coherence, and structural recoverability. Then, to address data scarcity for
the FAC module, we adopted a multitask learning strategy that jointly trains
the FAC and TTS modules. Beyond mitigating data limitations, this approach
yielded accelerated convergence and superior speech quality compared to
standalone FAC training. Moreover, leveraging the salient properties of our
discrete speech representations, we introduce SpeechRestorer, a postprocessing
architecture designed to refine LLM-generated outputs. This module effectively
mitigates stochastic errors prevalent in LLM inference pipelines while
enhancing prosodic continuity, as validated by ablation experiments.

</details>


### [17] [IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups](https://arxiv.org/abs/2507.01349)
*Hitoshi Suda,Junya Koguchi,Shunsuke Yoshida,Tomohiko Nakamura,Satoru Fukayama,Jun Ogata*

Main category: eess.AS

TL;DR: 论文构建了一个名为IdolSongsJp的日本偶像团体歌曲语料库，用于测试音乐信息处理技术。


<details>
  <summary>Details</summary>
Motivation: 日本偶像团体歌曲具有复杂的音乐结构和制作技术，适合作为音乐信息处理技术的测试基准。

Method: 通过专业作曲家创作15首偶像风格歌曲，构建包含多种音频和标注数据的语料库。

Result: 语料库展示了多样性，并成功应用于多项音乐信息处理技术的评估。

Conclusion: IdolSongsJp语料库为音乐信息处理研究提供了有价值的资源。

Abstract: Japanese idol groups, comprising performers known as "idols," are an
indispensable part of Japanese pop culture. They frequently appear in live
concerts and television programs, entertaining audiences with their singing and
dancing. Similar to other J-pop songs, idol group music covers a wide range of
styles, with various types of chord progressions and instrumental arrangements.
These tracks often feature numerous instruments and employ complex mastering
techniques, resulting in high signal loudness. Additionally, most songs include
a song division (utawari) structure, in which members alternate between singing
solos and performing together. Hence, these songs are well-suited for
benchmarking various music information processing techniques such as singer
diarization, music source separation, and automatic chord estimation under
challenging conditions. Focusing on these characteristics, we constructed a
song corpus titled IdolSongsJp by commissioning professional composers to
create 15 tracks in the style of Japanese idol groups. This corpus includes not
only mastered audio tracks but also stems for music source separation, dry
vocal tracks, and chord annotations. This paper provides a detailed description
of the corpus, demonstrates its diversity through comparisons with real-world
idol group songs, and presents its application in evaluating several music
information processing techniques.

</details>


### [18] [Voice Conversion for Likability Control via Automated Rating of Speech Synthesis Corpora](https://arxiv.org/abs/2507.01356)
*Hitoshi Suda,Shinnosuke Takamichi,Satoru Fukayama*

Main category: eess.AS

TL;DR: 提出一种语音转换方法，控制语音的受欢迎度，同时保留说话者身份和语言内容。


<details>
  <summary>Details</summary>
Motivation: 语音受欢迎度在社交互动中至关重要，如伴侣选择和广告。提供参考样本可帮助用户调整说话风格。

Method: 训练受欢迎度预测器，自动标注大规模语音合成语料库，控制语音转换。

Result: 预测器输出与人类评分显著相关，方法有效控制受欢迎度且保留身份和内容。

Conclusion: 该方法在控制语音受欢迎度方面效果显著，同时保持说话者身份和语言内容。

Abstract: Perceived voice likability plays a crucial role in various social
interactions, such as partner selection and advertising. A system that provides
reference likable voice samples tailored to target audiences would enable users
to adjust their speaking style and voice quality, facilitating smoother
communication. To this end, we propose a voice conversion method that controls
the likability of input speech while preserving both speaker identity and
linguistic content. To improve training data scalability, we train a likability
predictor on an existing voice likability dataset and employ it to
automatically annotate a large speech synthesis corpus with likability ratings.
Experimental evaluations reveal a significant correlation between the
predictor's outputs and human-provided likability ratings. Subjective and
objective evaluations further demonstrate that the proposed approach
effectively controls voice likability while preserving both speaker identity
and linguistic content.

</details>


### [19] [QHARMA-GAN: Quasi-Harmonic Neural Vocoder based on Autoregressive Moving Average Model](https://arxiv.org/abs/2507.01611)
*Shaowen Chen,Tomoki Toda*

Main category: eess.AS

TL;DR: 该论文提出了一种结合神经网络和准谐波模型（QHM）的新型神经声码器框架，解决了现有端到端神经声码器的黑盒问题，提高了语音合成的质量和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端神经声码器存在黑盒性质，无法清晰建模语音的激励和共振特性，且合成速度慢。

Method: 结合QHM和自回归移动平均（ARMA）模型，将语音信号编码为ARMA函数，以准确估计准谐波的幅度和相位。

Result: 实验表明，该方法在生成速度、合成质量和修改灵活性上优于其他方法。

Conclusion: 该方法有效结合了QHM、ARMA和神经网络的优势，显著提升了语音合成的性能。

Abstract: Vocoders, encoding speech signals into acoustic features and allowing for
speech signal reconstruction from them, have been studied for decades.
Recently, the rise of deep learning has particularly driven the development of
neural vocoders to generate high-quality speech signals. On the other hand, the
existing end-to-end neural vocoders suffer from a black-box nature that blinds
the speech production mechanism and the intrinsic structure of speech,
resulting in the ambiguity of separately modeling source excitation and
resonance characteristics and the loss of flexibly synthesizing or modifying
speech with high quality. Moreover, their sequence-wise waveform generation
usually requires complicated networks, leading to substantial time consumption.
In this work, inspired by the quasi-harmonic model (QHM) that represents speech
as sparse components, we combine the neural network and QHM synthesis process
to propose a novel framework for the neural vocoder. Accordingly, speech
signals can be encoded into autoregressive moving average (ARMA) functions to
model the resonance characteristics, yielding accurate estimates of the
amplitudes and phases of quasi-harmonics at any frequency. Subsequently, the
speech can be resynthesized and arbitrarily modified in terms of pitch shifting
and time stretching with high quality, whereas the time consumption and network
size decrease. The experiments indicate that the proposed method leverages the
strengths of QHM, the ARMA model, and neural networks, leading to the
outperformance of our methods over other methods in terms of generation speed,
synthesis quality, and modification flexibility.

</details>


### [20] [Generalizable Detection of Audio Deepfakes](https://arxiv.org/abs/2507.01750)
*Jose A. Lopez,Georg Stemmer,Héctor Cordourier Maruri*

Main category: eess.AS

TL;DR: 研究通过预训练骨干网络（如Wav2Vec2、WavLM和Whisper）及数据增强策略，显著提升了音频深度伪造检测模型的泛化能力，性能超过ASVspoof 5挑战赛的顶级单系统。


<details>
  <summary>Details</summary>
Motivation: 提升音频深度伪造检测模型的泛化能力，以应对多样化的数据集和实际应用场景。

Method: 研究了多种预训练骨干网络（Wav2Vec2、WavLM、Whisper）及不同数据增强策略和损失函数对模型性能的影响。

Result: 模型泛化能力显著提升，性能超过ASVspoof 5挑战赛的顶级单系统。

Conclusion: 研究为音频模型的优化提供了宝贵见解，推动了更鲁棒的深度伪造检测技术的发展。

Abstract: In this paper, we present our comprehensive study aimed at enhancing the
generalization capabilities of audio deepfake detection models. We investigate
the performance of various pre-trained backbones, including Wav2Vec2, WavLM,
and Whisper, across a diverse set of datasets, including those from the
ASVspoof challenges and additional sources. Our experiments focus on the
effects of different data augmentation strategies and loss functions on model
performance. The results of our research demonstrate substantial enhancements
in the generalization capabilities of audio deepfake detection models,
surpassing the performance of the top-ranked single system in the ASVspoof 5
Challenge. This study contributes valuable insights into the optimization of
audio models for more robust deepfake detection and facilitates future research
in this critical area.

</details>


### [21] [First Steps Towards Voice Anonymization for Code-Switching Speech](https://arxiv.org/abs/2507.01765)
*Sarina Meyer,Ekaterina Kolos,Ngoc Thang Vu*

Main category: eess.AS

TL;DR: 本文研究了多语言代码转换语音的匿名化问题，提出了适应多语言匿名化模型的改进方法，并验证了其在隐私和实用性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语音匿名化研究主要基于英语朗读语音数据集，对其他类型语音数据的匿名化效果未知，尤其是多语言代码转换语音。

Method: 准备两个语料库，改进多语言匿名化模型以适用于代码转换语音，并测试其与两种语言无关方法的性能。

Result: 仅多语言系统在隐私和实用性方面表现良好，但实用性评估因语音自发性和多语言语音识别模型支持有限而存在挑战。

Conclusion: 多语言匿名化模型适用于代码转换语音，但需进一步改进实用性评估方法。

Abstract: The goal of voice anonymization is to modify an audio such that the true
identity of its speaker is hidden. Research on this task is typically limited
to the same English read speech datasets, thus the efficacy of current methods
for other types of speech data remains unknown. In this paper, we present the
first investigation of voice anonymization for the multilingual phenomenon of
code-switching speech. We prepare two corpora for this task and propose
adaptations to a multilingual anonymization model to make it applicable for
code-switching speech. By testing the anonymization performance of this and two
language-independent methods on the datasets, we find that only the
multilingual system performs well in terms of privacy and utility preservation.
Furthermore, we observe challenges in performing utility evaluations on this
data because of its spontaneous character and the limited code-switching
support by the multilingual speech recognition model.

</details>


### [22] [Low-Complexity Neural Wind Noise Reduction for Audio Recordings](https://arxiv.org/abs/2507.01821)
*Hesam Eftekhari,Srikanth Raj Chetupalli,Shrishti Saha Shetu,Emanuël A. P. Habets,Oliver Thiergart*

Main category: eess.AS

TL;DR: 提出了一种低复杂度的单通道深度神经网络，用于实时抑制风噪，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 风噪显著降低户外音频录制质量，且难以在资源受限设备上实时抑制。

Method: 利用风噪的频谱特性，设计了一种低复杂度的单通道深度神经网络。

Result: 实验表明，该方法性能与最先进的低复杂度ULCNet模型相当，仅需249K参数和约73 MHz计算能力。

Conclusion: 该模型适用于嵌入式及移动音频应用，能有效抑制风噪。

Abstract: Wind noise significantly degrades the quality of outdoor audio recordings,
yet remains difficult to suppress in real-time on resource-constrained devices.
In this work, we propose a low-complexity single-channel deep neural network
that leverages the spectral characteristics of wind noise. Experimental results
show that our method achieves performance comparable to the state-of-the-art
low-complexity ULCNet model. The proposed model, with only 249K parameters and
roughly 73 MHz of computational power, is suitable for embedded and mobile
audio applications.

</details>


### [23] [Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders](https://arxiv.org/abs/2507.01888)
*Nina R. Benway,Saba Tabatabaee,Dongliang Wang,Benjamin Munson,Jonathan L. Preston,Carol Espy-Wilson*

Main category: eess.AS

TL;DR: 该研究通过发音运动学方法评估了儿童语音障碍中/r/和/s/的发音与感知评分的关联，发现发音变量能有效区分/r/的错误类别和程度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证发音运动学是否能与感知评分一致，从而为语音障碍的临床评估提供量化工具。

Method: 使用发音音系学神经网络推断发音变量，结合5点PERCEPT评分量表，分析118名儿童和3名成人的5,961个发音样本。

Result: 线性混合模型支持了大部分/r/的假设（17/18），但对/s/的支持较少（7/15）。PERCEPT评分显著预测了发音接近目标音的程度。

Conclusion: 发音变量能有效区分/r/的错误类别和程度，支持其在临床评估中的应用，尤其是/r/的量化分析。

Abstract: Purpose: This study evaluated whether articulatory kinematics, inferred by
Articulatory Phonology speech inversion neural networks, aligned with
perceptual ratings of /r/ and /s/ in the speech of children with speech sound
disorders.
  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961
utterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual
ratings were standardized using the novel 5-point PERCEPT Rating Scale and
training protocol. Two research questions examined if the articulatory patterns
of inferred vocal tract variables aligned with the perceptual error category
for the phones investigated (e.g., tongue tip is more anterior in dentalized
/s/ productions than in correct /s/). A third research question examined if
gradient PERCEPT Rating Scale scores predicted articulatory proximity to
correct productions.
  Results: Estimated marginal means from linear mixed models supported 17 of 18
/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,
estimated marginal means from a second linear mixed model supported 7 of 15
hypotheses, particularly those related to the tongue tip. A third linear mixed
model revealed that PERCEPT Rating Scale scores significantly predicted
articulatory proximity of errored phones to correct productions.
  Conclusion: Inferred vocal tract variables differentiated category and
magnitude of articulatory errors for /r/, and to a lesser extent for /s/,
aligning with perceptual judgments. These findings support the clinical
interpretability of speech inversion vocal tract variables and the PERCEPT
Rating Scale in quantifying articulatory proximity to the target sound,
particularly for /r/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [24] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: GuideSep是一个基于扩散模型的音乐源分离方法，支持乐器无关的分离，通过波形模仿和频谱掩码提供灵活指导。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于四音轨分离，缺乏灵活性。

Method: 结合波形模仿条件和频谱掩码，采用扩散模型进行乐器无关分离。

Result: GuideSep实现高质量分离，支持更灵活的乐器提取。

Conclusion: 用户参与的扩散生成方法在音乐源分离中具有潜力。

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [25] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/abs/2507.01563)
*Marco Giordano,Stefano Giacomelli,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: 提出了一种基于E2PANNs的实时嵌入式紧急车辆警笛检测系统，通过优化数据集和部署策略，实现了低延迟和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决城市环境中紧急车辆警笛检测的低可靠性问题，并探索低成本边缘设备的分布式声学监测网络。

Method: 使用E2PANNs卷积神经网络，结合定制数据集（AudioSet-EV等）和多线程推理引擎，部署在Raspberry Pi 5上。

Result: 系统在现实音频条件下表现出低延迟检测和更高的鲁棒性。

Conclusion: 证明了在低成本边缘设备上部署分布式声学监测网络的可行性，支持智能城市基础设施中的协作紧急车辆跟踪。

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


### [26] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Main category: cs.SD

TL;DR: 该论文提出了一种名为XMVAE的模型，用于从零生成古典钢琴表演，结合作曲家和演奏家的双重角色，通过ECP表示和双分支结构实现高质量的表演生成。


<details>
  <summary>Details</summary>
Motivation: 古典音乐的创意不仅来自作曲家，还来自演奏家对静态乐谱的诠释。本文旨在模拟这一双重角色，解决从零生成古典钢琴表演的挑战。

Method: 引入ECP表示捕捉表演的韵律结构和表现细节，提出XMVAE模型，包含VQ-VAE分支（作曲家）和VAE分支（演奏家），使用多尺度编码器和正交Transformer解码器。

Result: XMVAE生成的古典表演在客观和主观评估中均优于现有模型，且作曲家分支的预训练显著提升了性能。

Conclusion: XMVAE成功模拟了作曲家和演奏家的双重角色，生成高质量的古典钢琴表演，为音乐生成领域提供了新思路。

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


### [27] [A Dataset for Automatic Assessment of TTS Quality in Spanish](https://arxiv.org/abs/2507.01805)
*Alejandro Sosa Welford,Leonardo Pepino*

Main category: cs.SD

TL;DR: 开发了一个西班牙语TTS系统自动评估数据库，用于改进自然度预测模型的准确性，包含4,326个音频样本，并通过主观测试标注。验证了数据集的实用性，训练了两种自动预测模型，平均绝对误差为0.8。


<details>
  <summary>Details</summary>
Motivation: 改进西班牙语TTS系统的自然度预测模型，填补该领域数据集的空白。

Method: 构建包含4,326个音频样本的数据库，基于ITU-T Rec. P.807标准进行主观测试标注，并训练两种自动预测模型（微调现有模型和训练小型下游网络）。

Result: 模型在五级MOS量表上的平均绝对误差为0.8，数据集质量和多样性得到验证。

Conclusion: 该数据集为西班牙语TTS研究提供了重要资源，展示了其在改进自然度预测方面的潜力。

Abstract: This work addresses the development of a database for the automatic
assessment of text-to-speech (TTS) systems in Spanish, aiming to improve the
accuracy of naturalness prediction models. The dataset consists of 4,326 audio
samples from 52 different TTS systems and human voices and is, up to our
knowledge, the first of its kind in Spanish. To label the audios, a subjective
test was designed based on the ITU-T Rec. P.807 standard and completed by 92
participants. Furthermore, the utility of the collected dataset was validated
by training automatic naturalness prediction systems. We explored two
approaches: fine-tuning an existing model originally trained for English, and
training small downstream networks on top of frozen self-supervised speech
models. Our models achieve a mean absolute error of 0.8 on a five-point MOS
scale. Further analysis demonstrates the quality and diversity of the developed
dataset, and its potential to advance TTS research in Spanish.

</details>

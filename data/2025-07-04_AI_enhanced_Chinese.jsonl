{"id": "2507.02115", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02115", "abs": "https://arxiv.org/abs/2507.02115", "authors": ["Zirui Li", "Lauri Juvela", "Mikko Kurimo"], "title": "Pronunciation Editing for Finnish Speech using Phonetic Posteriorgrams", "comment": "5 pages; 1 figure; Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Synthesizing second-language (L2) speech is potentially highly valued for L2\nlanguage learning experience and feedback. However, due to the lack of L2\nspeech synthesis datasets, it is difficult to synthesize L2 speech for\nlow-resourced languages. In this paper, we provide a practical solution for\nediting native speech to approximate L2 speech and present PPG2Speech, a\ndiffusion-based multispeaker Phonetic-Posteriorgrams-to-Speech model that is\ncapable of editing a single phoneme without text alignment. We use Matcha-TTS's\nflow-matching decoder as the backbone, transforming Phonetic Posteriorgrams\n(PPGs) to mel-spectrograms conditioned on external speaker embeddings and\npitch. PPG2Speech strengthens the Matcha-TTS's flow-matching decoder with\nClassifier-free Guidance (CFG) and Sway Sampling. We also propose a new\ntask-specific objective evaluation metric, the Phonetic Aligned Consistency\n(PAC), between the edited PPGs and the PPGs extracted from the synthetic speech\nfor editing effects. We validate the effectiveness of our method on Finnish, a\nlow-resourced, nearly phonetic language, using approximately 60 hours of data.\nWe conduct objective and subjective evaluations of our approach to compare its\nnaturalness, speaker similarity, and editing effectiveness with TTS-based\nediting. Our source code is published at\nhttps://github.com/aalto-speech/PPG2Speech.", "AI": {"tldr": "\u63d0\u51faPPG2Speech\u6a21\u578b\uff0c\u901a\u8fc7\u7f16\u8f91\u539f\u751f\u8bed\u97f3\u5408\u6210\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u8bed\u97f3\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "motivation": "\u89e3\u51b3\u7f3a\u4e4fL2\u8bed\u97f3\u5408\u6210\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684L2\u8bed\u97f3\u5408\u6210\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548cPhonetic-Posteriorgrams\uff08PPGs\uff09\uff0c\u7ed3\u5408Matcha-TTS\u7684\u89e3\u7801\u5668\uff0c\u4f7f\u7528Classifier-free Guidance\u548cSway Sampling\u589e\u5f3a\u3002", "result": "\u5728\u82ac\u5170\u8bed\u4e0a\u9a8c\u8bc1\uff0c60\u5c0f\u65f6\u6570\u636e\uff0c\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u663e\u793a\u81ea\u7136\u5ea6\u548c\u7f16\u8f91\u6548\u679c\u4f18\u4e8eTTS\u65b9\u6cd5\u3002", "conclusion": "PPG2Speech\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00L2\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.02192", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02192", "abs": "https://arxiv.org/abs/2507.02192", "authors": ["Chun-Wei Ho", "Pin-Jui Ku", "Hao Yen", "Sabato Marco Siniscalchi", "Yu Tsao", "Chin-Hui Lee"], "title": "An Investigation on Combining Geometry and Consistency Constraints into Phase Estimation for Speech Enhancement", "comment": "5 pages", "summary": "We propose a novel iterative phase estimation framework, termed multi-source\nGriffin-Lim algorithm (MSGLA), for speech enhancement (SE) under additive noise\nconditions. The core idea is to leverage the ad-hoc consistency constraint of\ncomplex-valued short-time Fourier transform (STFT) spectrograms to address the\nsign ambiguity challenge commonly encountered in geometry-based phase\nestimation. Furthermore, we introduce a variant of the geometric constraint\nframework based on the law of sines and cosines, formulating a new phase\nreconstruction algorithm using noise phase estimates. We first validate the\nproposed technique through a series of oracle experiments, demonstrating its\neffectiveness under ideal conditions. We then evaluate its performance on the\nVB-DMD and WSJ0-CHiME3 data sets, and show that the proposed MSGLA variants\nmatch well or slightly outperform existing algorithms, including direct phase\nestimation and DNN-based sign prediction, especially in terms of background\nnoise suppression.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fed\u4ee3\u76f8\u4f4d\u4f30\u8ba1\u6846\u67b6MSGLA\uff0c\u7528\u4e8e\u8bed\u97f3\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u76f8\u4f4d\u4f30\u8ba1\u4e2d\u7684\u7b26\u53f7\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728\u566a\u58f0\u6291\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u51e0\u4f55\u76f8\u4f4d\u4f30\u8ba1\u4e2d\u5e38\u89c1\u7684\u7b26\u53f7\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u97f3\u589e\u5f3a\u6548\u679c\u3002", "method": "\u5229\u7528\u590d\u6570STFT\u8c31\u56fe\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u7ed3\u5408\u6b63\u5f26\u548c\u4f59\u5f26\u5b9a\u5f8b\uff0c\u63d0\u51fa\u65b0\u7684\u76f8\u4f4d\u91cd\u5efa\u7b97\u6cd5\u3002", "result": "\u5728VB-DMD\u548cWSJ0-CHiME3\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMSGLA\u5728\u566a\u58f0\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "conclusion": "MSGLA\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u566a\u58f0\u6291\u5236\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.02530", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02530", "abs": "https://arxiv.org/abs/2507.02530", "authors": ["Mateo C\u00e1mara", "Juan Guti\u00e9rrez", "Mar\u00eda Pilar Daza", "Jos\u00e9 Luis Blanco"], "title": "Open-Source System for Multilingual Translation and Cloned Speech Synthesis", "comment": "Presented at Forum Acusticum Euronoise 2025", "summary": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios.", "AI": {"tldr": "\u5f00\u6e90\u7cfb\u7edf\u7ed3\u5408Whisper\u548cVAD\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0c\u5229\u7528LLM\u5b9e\u73b0\u591a\u8bed\u8a00\u7ffb\u8bd1\u548c\u8bed\u97f3\u518d\u751f\uff0c\u652f\u6301\u672c\u5730\u6216API\u90e8\u7f72\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7ffb\u8bd1\u548c\u8bed\u97f3\u64ad\u653e\u7b49\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6c9f\u901a\u548c\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6574\u5408Whisper\u548cVAD\u8fdb\u884c\u8bed\u97f3\u8bc6\u522b\uff0c\u901a\u8fc7LLM\u5206\u6bb5\u548c\u7ffb\u8bd1\uff0c\u7ed3\u5408TTS\u6a21\u5757\u5b9e\u73b0\u8bed\u97f3\u514b\u9686\u3002", "result": "\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u663e\u793a\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u591a\u8bed\u8a00\u573a\u666f\u3002", "conclusion": "\u5f00\u6e90\u7cfb\u7edf\u4e3a\u591a\u8bed\u8a00\u6c9f\u901a\u63d0\u4f9b\u4e86\u521b\u65b0\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u65e0\u969c\u788d\u4ea4\u6d41\u3002"}}
{"id": "2507.02562", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.02562", "abs": "https://arxiv.org/abs/2507.02562", "authors": ["Yuzhu Wang", "Archontis Politis", "Konstantinos Drossos", "Tuomas Virtanen"], "title": "Multi-Utterance Speech Separation and Association Trained on Short Segments", "comment": "5 pages, accepted by WASPAA 2025", "summary": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387-\u65f6\u95f4\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08FTRNN\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u5206\u79bb\u4e2d\u56e0\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u957f\u5ea6\u4e0d\u5339\u914d\u800c\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u5206\u79bb\u4e2d\u56e0\u8ba1\u7b97\u9650\u5236\u53ea\u80fd\u8bad\u7ec3\u77ed\u7247\u6bb5\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u5904\u7406\u66f4\u957f\u7684\u5f55\u97f3\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "FTRNN\u91c7\u7528\u5168\u9891\u5e26\u6a21\u5757\u5efa\u6a21\u9891\u7387\u4f9d\u8d56\u6027\u548c\u5b50\u9891\u5e26\u6a21\u5757\u5efa\u6a21\u65f6\u95f4\u6a21\u5f0f\uff0c\u65e0\u9700\u5206\u5272\u957f\u97f3\u9891\u5373\u53ef\u5904\u7406\u3002", "result": "FTRNN\u5728\u663e\u8457\u957f\u4e8e\u8bad\u7ec3\u7247\u6bb5\uff0821-121\u79d2\uff09\u7684\u97f3\u9891\u4e0a\u8868\u73b0\u9c81\u68d2\uff0c\u4e14\u4fdd\u6301\u8bf4\u8bdd\u4eba\u5173\u8054\u6027\u3002", "conclusion": "FTRNN\u8f7b\u91cf\u9ad8\u6548\uff080.9M\u53c2\u6570\uff09\uff0c\u65e0\u9700\u5206\u6bb5\u5904\u7406\uff0c\u907f\u514d\u4e86\u8fb9\u754c\u5931\u771f\uff0c\u7b80\u5316\u4e86\u90e8\u7f72\u3002"}}
{"id": "2507.02243", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02243", "abs": "https://arxiv.org/abs/2507.02243", "authors": ["Peilan Wang", "Jun Fang", "Xianlong Zeng", "Bin Wang", "Zhi Chen", "Yonina C. Eldar"], "title": "Derivative-Free Optimization-Empowered Wireless Channel Reconfiguration for 6G", "comment": "7 pages", "summary": "Reconfigurable antennas, including reconfigurable intelligent surface (RIS),\nmovable antenna (MA), fluid antenna (FA), and other advanced antenna\ntechniques, have been studied extensively in the context of reshaping wireless\npropagation environments for 6G and beyond wireless communications.\nNevertheless, how to reconfigure/optimize the real-time controllable\ncoefficients to achieve a favorable end-to-end wireless channel remains a\nsubstantial challenge, as it usually requires accurate modeling of the complex\ninteraction between the reconfigurable devices and the electromagnetic waves,\nas well as knowledge of implicit channel propagation parameters. In this paper,\nwe introduce a derivative-free optimization (a.k.a., zeroth-order (ZO)\noptimization) technique to directly optimize reconfigurable coefficients to\nshape the wireless end-to-end channel, without the need of channel modeling and\nestimation of the implicit environmental propagation parameters. We present the\nfundamental principles of ZO optimization and discuss its potential advantages\nin wireless channel reconfiguration. Two case studies for RIS and movable\nantenna-enabled single-input single-output (SISO) systems are provided to show\nthe superiority of ZO-based methods as compared to state-of-the-art techniques.\nFinally, we outline promising future research directions and offer concluding\ninsights on derivative-free optimization for reconfigurable antenna\ntechnologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4fe1\u9053\u5efa\u6a21\u548c\u9690\u5f0f\u73af\u5883\u4f20\u64ad\u53c2\u6570\u4f30\u8ba1\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u4f18\u5316\u53ef\u91cd\u6784\u5929\u7ebf\u7684\u7cfb\u6570\uff0c\u4ee5\u6539\u5584\u65e0\u7ebf\u7aef\u5230\u7aef\u4fe1\u9053\u3002", "motivation": "\u57286G\u53ca\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u4e2d\uff0c\u5982\u4f55\u5b9e\u65f6\u4f18\u5316\u53ef\u91cd\u6784\u5929\u7ebf\u7684\u7cfb\u6570\u4ee5\u5b9e\u73b0\u7406\u60f3\u7684\u65e0\u7ebf\u4fe1\u9053\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u4fe1\u9053\u5efa\u6a21\u548c\u9690\u5f0f\u53c2\u6570\u4f30\u8ba1\u3002", "method": "\u91c7\u7528\u96f6\u9636\u4f18\u5316\u6280\u672f\uff0c\u76f4\u63a5\u4f18\u5316\u53ef\u91cd\u6784\u7cfb\u6570\uff0c\u65e0\u9700\u4f9d\u8d56\u4fe1\u9053\u5efa\u6a21\u6216\u9690\u5f0f\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u901a\u8fc7RIS\u548c\u53ef\u79fb\u52a8\u5929\u7ebf\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u6280\u672f\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u96f6\u9636\u4f18\u5316\u4e3a\u53ef\u91cd\u6784\u5929\u7ebf\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.01974", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01974", "abs": "https://arxiv.org/abs/2507.01974", "authors": ["J\u00e9r\u00e9my Rouch", "M Ducrettet", "S Haupert", "R Emonet", "F S\u00e8be"], "title": "Acoustic evaluation of a neural network dedicated to the detection of animal vocalisations", "comment": null, "summary": "The accessibility of long-duration recorders, adapted to sometimes demanding\nfield conditions, has enabled the deployment of extensive animal population\nmonitoring campaigns through ecoacoustics. The effectiveness of automatic\nsignal detection methods, increasingly based on neural approaches, is\nfrequently evaluated solely through machine learning metrics, while acoustic\nanalysis of performance remains rare. As part of the acoustic monitoring of\nRock Ptarmigan populations, we propose here a simple method for acoustic\nanalysis of the detection system's performance. The proposed measure is based\non relating the signal-to-noise ratio of synthetic signals to their probability\nof detection. We show how this measure provides information about the system\nand allows optimisation of its training. We also show how it enables modelling\nof the detection distance, thus offering the possibility of evaluating its\ndynamics according to the sound environment and accessing an estimation of the\nspatial density of calls.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u566a\u6bd4\u4e0e\u68c0\u6d4b\u6982\u7387\u5173\u7cfb\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6001\u58f0\u5b66\u76d1\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u5316\u5176\u8bad\u7ec3\u548c\u5efa\u6a21\u68c0\u6d4b\u8ddd\u79bb\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u8bc4\u4f30\u591a\u4f9d\u8d56\u673a\u5668\u5b66\u4e60\u6307\u6807\uff0c\u7f3a\u4e4f\u58f0\u5b66\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u5408\u6210\u4fe1\u53f7\u7684\u4fe1\u566a\u6bd4\u4e0e\u5176\u68c0\u6d4b\u6982\u7387\u5173\u8054\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u58f0\u5b66\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u7cfb\u7edf\u6027\u80fd\u4fe1\u606f\uff0c\u4f18\u5316\u8bad\u7ec3\uff0c\u5e76\u5efa\u6a21\u68c0\u6d4b\u8ddd\u79bb\uff0c\u4ece\u800c\u8bc4\u4f30\u52a8\u6001\u73af\u5883\u548c\u4f30\u7b97\u53eb\u58f0\u7a7a\u95f4\u5bc6\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6001\u58f0\u5b66\u76d1\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.02755", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02755", "abs": "https://arxiv.org/abs/2507.02755", "authors": ["Caleb Rascon", "Luis Gato-Diaz", "Eduardo Garc\u00eda-Alarc\u00f3n"], "title": "Multi-agent Auditory Scene Analysis", "comment": "Submitted to Applied Intelligence", "summary": "Auditory scene analysis (ASA) aims to retrieve information from the acoustic\nenvironment, by carrying out three main tasks: sound source location,\nseparation, and classification. These tasks are traditionally executed with a\nlinear data flow, where the sound sources are first located; then, using their\nlocation, each source is separated into its own audio stream; from each of\nwhich, information is extracted that is relevant to the application scenario\n(audio event detection, speaker identification, emotion classification, etc.).\nHowever, running these tasks linearly increases the overall response time,\nwhile making the last tasks (separation and classification) highly sensitive to\nerrors of the first task (location). A considerable amount of effort and\ncomputational complexity has been employed in the state-of-the-art to develop\ntechniques that are the least error-prone possible. However, doing so gives\nrise to an ASA system that is non-viable in many applications that require a\nsmall computational footprint and a low response time, such as bioacoustics,\nhearing-aid design, search and rescue, human-robot interaction, etc. To this\neffect, in this work, a multi-agent approach is proposed to carry out ASA where\nthe tasks are run in parallel, with feedback loops between them to compensate\nfor local errors, such as: using the quality of the separation output to\ncorrect the location error; and using the classification result to reduce the\nlocalization's sensitivity towards interferences. The result is a multi-agent\nauditory scene analysis (MASA) system that is robust against local errors,\nwithout a considerable increase in complexity, and with a low response time.\nThe complete proposed MASA system is provided as a framework that uses\nopen-source tools for sound acquisition and reproduction (JACK) and inter-agent\ncommunication (ROS2), allowing users to add their own agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u5e76\u884c\u5904\u7406\u7684\u542c\u89c9\u573a\u666f\u5206\u6790\uff08MASA\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u4efb\u52a1\u95f4\u7684\u53cd\u9988\u5faa\u73af\u51cf\u5c11\u9519\u8bef\uff0c\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u7ebf\u6027\u6d41\u7a0b\u7684\u542c\u89c9\u573a\u666f\u5206\u6790\uff08ASA\uff09\u7cfb\u7edf\u54cd\u5e94\u65f6\u95f4\u957f\u4e14\u6613\u53d7\u521d\u59cb\u4efb\u52a1\u9519\u8bef\u5f71\u54cd\uff0c\u96be\u4ee5\u6ee1\u8db3\u4f4e\u8ba1\u7b97\u91cf\u548c\u5feb\u901f\u54cd\u5e94\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u5e76\u884c\u5904\u7406\uff0c\u4efb\u52a1\u95f4\u5f15\u5165\u53cd\u9988\u5faa\u73af\uff08\u5982\u5229\u7528\u5206\u79bb\u8d28\u91cf\u4fee\u6b63\u5b9a\u4f4d\u9519\u8bef\uff09\uff0c\u7ed3\u5408\u5f00\u6e90\u5de5\u5177\uff08JACK\u548cROS2\uff09\u6784\u5efa\u6846\u67b6\u3002", "result": "MASA\u7cfb\u7edf\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u5c40\u90e8\u9519\u8bef\u5e76\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\u3002", "conclusion": "MASA\u7cfb\u7edf\u4e3a\u9700\u8981\u5feb\u901f\u54cd\u5e94\u548c\u4f4e\u8ba1\u7b97\u91cf\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02262", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02262", "abs": "https://arxiv.org/abs/2507.02262", "authors": ["Eric Mason", "Sippanon Kitimoon", "Hrushikesh Mhaskar"], "title": "Localized kernel method for separation of linear chirps", "comment": null, "summary": "The task of separating a superposition of signals into its individual\ncomponents is a common challenge encountered in various signal processing\napplications, especially in domains such as audio and radar signals. A previous\npaper by Chui and Mhaskar proposes a method called Signal Separation Operator\n(SSO) to find the instantaneous frequencies and amplitudes of such\nsuperpositions where both of these change continuously and slowly over time. In\nthis paper, we amplify and modify this method in order to separate chirp\nsignals in the presence of crossovers, a very low SNR, and discontinuities. We\ngive a theoretical analysis of the behavior of SSO in the presence of noise to\nexamine the relationship between the minimal separation, minimal amplitude,\nSNR, and sampling frequency. Our method is illustrated with a few examples, and\nnumerical results are reported on a simulated dataset comprising 7 simulated\nsignals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u4ea4\u53c9\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u4e0d\u8fde\u7eed\u6027\u7684\u7ebf\u6027\u8c03\u9891\u4fe1\u53f7\uff0c\u5e76\u5206\u6790\u4e86\u566a\u58f0\u5bf9\u5206\u79bb\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u4fe1\u53f7\u5904\u7406\u4e2d\u53e0\u52a0\u4fe1\u53f7\u5206\u79bb\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4ea4\u53c9\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u4e0d\u8fde\u7eed\u6027\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u6539\u8fdb\u548c\u6269\u5c55\u4e86Chui\u548cMhaskar\u63d0\u51fa\u7684\u4fe1\u53f7\u5206\u79bb\u7b97\u5b50\uff08SSO\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u79bb\u7ebf\u6027\u8c03\u9891\u4fe1\u53f7\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u6570\u636e\u96c6\uff087\u4e2a\u4fe1\u53f7\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u6700\u5c0f\u5206\u79bb\u8ddd\u79bb\u3001\u6700\u5c0f\u632f\u5e45\u3001\u4fe1\u566a\u6bd4\u548c\u91c7\u6837\u9891\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u6539\u8fdb\u7684SSO\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u5206\u79bb\u4fe1\u53f7\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.02176", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02176", "abs": "https://arxiv.org/abs/2507.02176", "authors": ["Marc-Andr\u00e9 Carbonneau", "Benjamin van Niekerk", "Hugo Seut\u00e9", "Jean-Philippe Letendre", "Herman Kamper", "Julian Za\u00efdi"], "title": "Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis", "comment": "Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop", "summary": "Modeling voice identity is challenging due to its multifaceted nature. In\ngenerative speech systems, identity is often assessed using automatic speaker\nverification (ASV) embeddings, designed for discrimination rather than\ncharacterizing identity. This paper investigates which aspects of a voice are\ncaptured in such representations. We find that widely used ASV embeddings focus\nmainly on static features like timbre and pitch range, while neglecting dynamic\nelements such as rhythm. We also identify confounding factors that compromise\nspeaker similarity measurements and suggest mitigation strategies. To address\nthese gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm\npatterns. This work contributes to the ongoing challenge of assessing speaker\nidentity consistency in the context of ever-better voice cloning systems. We\npublicly release our code.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86ASV\u5d4c\u5165\u5728\u6355\u6349\u8bed\u97f3\u8eab\u4efd\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86U3D\u6307\u6807\u4ee5\u8bc4\u4f30\u52a8\u6001\u8282\u594f\u6a21\u5f0f\u3002", "motivation": "\u7531\u4e8e\u8bed\u97f3\u8eab\u4efd\u7684\u591a\u7ef4\u6027\uff0c\u751f\u6210\u8bed\u97f3\u7cfb\u7edf\u4e2d\u5e38\u4f7f\u7528ASV\u5d4c\u5165\u8bc4\u4f30\u8eab\u4efd\uff0c\u4f46\u5176\u8bbe\u8ba1\u521d\u8877\u662f\u533a\u5206\u800c\u975e\u8868\u5f81\u8eab\u4efd\u3002", "method": "\u7814\u7a76\u53d1\u73b0ASV\u5d4c\u5165\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u7279\u5f81\uff08\u5982\u97f3\u8272\u548c\u97f3\u9ad8\u8303\u56f4\uff09\uff0c\u800c\u5ffd\u7565\u52a8\u6001\u5143\u7d20\uff08\u5982\u8282\u594f\uff09\uff0c\u5e76\u63d0\u51faU3D\u6307\u6807\u3002", "result": "ASV\u5d4c\u5165\u5b58\u5728\u5c40\u9650\u6027\uff0cU3D\u80fd\u6709\u6548\u8bc4\u4f30\u52a8\u6001\u8282\u594f\u6a21\u5f0f\u3002", "conclusion": "\u8bba\u6587\u4e3a\u8bed\u97f3\u514b\u9686\u7cfb\u7edf\u4e2d\u8bc4\u4f30\u8eab\u4efd\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u3002"}}
{"id": "2507.02768", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.02768", "abs": "https://arxiv.org/abs/2507.02768", "authors": ["Ke-Han Lu", "Zhehuai Chen", "Szu-Wei Fu", "Chao-Han Huck Yang", "Sung-Feng Huang", "Chih-Kai Yang", "Chee-En Yu", "Chun-Wei Chen", "Wei-Chih Chen", "Chien-yu Huang", "Yi-Cheng Lin", "Yu-Xiang Lin", "Chi-An Fu", "Chun-Yi Kuan", "Wenze Ren", "Xuanjun Chen", "Wei-Ping Huang", "En-Pei Hu", "Tzu-Quan Lin", "Yuan-Kuei Wu", "Kuan-Po Huang", "Hsiao-Ying Huang", "Huang-Cheng Chou", "Kai-Wei Chang", "Cheng-Han Chiang", "Boris Ginsburg", "Yu-Chiang Frank Wang", "Hung-yi Lee"], "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment", "comment": "Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio", "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.", "AI": {"tldr": "DeSTA2.5-Audio\u662f\u4e00\u79cd\u901a\u7528\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u8de8\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8bed\u8a00\u80fd\u529b\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u97f3\u9891\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALM\uff09\u5728\u589e\u5f3a\u542c\u89c9\u80fd\u529b\u65f6\u4f1a\u5bfc\u81f4\u8bed\u8a00\u80fd\u529b\u7684\u9057\u5fd8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u53c8\u80fd\u5b9e\u73b0\u97f3\u9891-\u6587\u672c\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDeSTA\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u8bad\u7ec3\u76ee\u6807\u6784\u5efa\u5927\u89c4\u6a21\u4efb\u52a1\u65e0\u5173\u6570\u636e\u96c6DeSTA-AQA5M\uff0c\u907f\u514d\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u3002", "result": "DeSTA2.5-Audio\u5728\u591a\u4e2a\u97f3\u9891\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u81ea\u751f\u6210\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u6784\u5efa\u8bbe\u8ba1\u5728LALM\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u4e14\u9c81\u68d2\u7684LALM\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2507.02346", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02346", "abs": "https://arxiv.org/abs/2507.02346", "authors": ["Hedieh Taremizadeh", "Emanuele Grossi", "Luca Venturino"], "title": "STAR-RIS Transceivers: Integrated Sensing and Communication with Pulsed Signals", "comment": "Accepted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), Isola delle Femmine, Palermo, Italy", "summary": "This study examines an integrated sensing and communication (ISAC)\ntransceiver featuring a simultaneous transmitting and reflecting reconfigurable\nintelligent surface (STAR-RIS) and a receiver equipped with a passive\nelectronically scanned array (PESA) and a single digital channel. By utilizing\na periodic pulsed signal emitted by a feeder, we introduce at the STAR-RIS a\nspace modulation to illuminate two angular directions observed by the radar\nreceiver, one in each half-space, and a time modulation to distinguish the\ncorresponding echoes from prospective moving targets and embed communication\nmessages. The proposed time modulation employs orthogonal binary codebooks with\ndifferent trade-offs in transmission and error rates, while having minimal\nimpact on the radar performance, evaluated by probability of detection and root\nmean square error in the radial velocity estimation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u6536\u53d1\u5668\uff0c\u7ed3\u5408\u4e86\u540c\u65f6\u53d1\u5c04\u548c\u53cd\u5c04\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08STAR-RIS\uff09\u548c\u914d\u5907\u88ab\u52a8\u7535\u5b50\u626b\u63cf\u9635\u5217\uff08PESA\uff09\u7684\u63a5\u6536\u5668\u3002\u901a\u8fc7\u5468\u671f\u6027\u8109\u51b2\u4fe1\u53f7\u548c\u7a7a\u95f4/\u65f6\u95f4\u8c03\u5236\uff0c\u5b9e\u73b0\u4e86\u53cc\u89d2\u5ea6\u7167\u5c04\u548c\u901a\u4fe1\u6d88\u606f\u5d4c\u5165\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u96f7\u8fbe\u548c\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u6548\u96c6\u6210\uff0c\u540c\u65f6\u4f18\u5316\u6027\u80fd\u3002", "method": "\u4f7f\u7528STAR-RIS\u548cPESA\u63a5\u6536\u5668\uff0c\u901a\u8fc7\u5468\u671f\u6027\u8109\u51b2\u4fe1\u53f7\u5b9e\u73b0\u7a7a\u95f4\u548c\u65f6\u95f4\u8c03\u5236\uff0c\u7ed3\u5408\u6b63\u4ea4\u4e8c\u8fdb\u5236\u7801\u672c\u8fdb\u884c\u901a\u4fe1\u6d88\u606f\u5d4c\u5165\u3002", "result": "\u65f6\u95f4\u8c03\u5236\u5bf9\u96f7\u8fbe\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u901a\u4fe1\u6d88\u606f\u7684\u5d4c\u5165\u548c\u76ee\u6807\u7684\u68c0\u6d4b\u4e0e\u901f\u5ea6\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u901a\u4fe1\u548c\u96f7\u8fbe\u6027\u80fd\u3002"}}
{"id": "2507.02273", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02273", "abs": "https://arxiv.org/abs/2507.02273", "authors": ["Yen-Tung Yeh", "Junghyun Koo", "Marco A. Mart\u00ednez-Ram\u00edrez", "Wei-Hsiang Liao", "Yi-Hsuan Yang", "Yuki Mitsufuji"], "title": "Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures", "comment": "ISMIR 2025", "summary": "General-purpose audio representations have proven effective across diverse\nmusic information retrieval applications, yet their utility in intelligent\nmusic production remains limited by insufficient understanding of audio effects\n(Fx). Although previous approaches have emphasized audio effects analysis at\nthe mixture level, this focus falls short for tasks demanding instrument-wise\naudio effects understanding, such as automatic mixing. In this work, we present\nFx-Encoder++, a novel model designed to extract instrument-wise audio effects\nrepresentations from music mixtures. Our approach leverages a contrastive\nlearning framework and introduces an \"extractor\" mechanism that, when provided\nwith instrument queries (audio or text), transforms mixture-level audio effects\nembeddings into instrument-wise audio effects embeddings. We evaluated our\nmodel across retrieval and audio effects parameter matching tasks, testing its\nperformance across a diverse range of instruments. The results demonstrate that\nFx-Encoder++ outperforms previous approaches at mixture level and show a novel\nability to extract effects representation instrument-wise, addressing a\ncritical capability gap in intelligent music production systems.", "AI": {"tldr": "Fx-Encoder++\u662f\u4e00\u79cd\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u548c\u63d0\u53d6\u5668\u673a\u5236\uff0c\u4ece\u97f3\u4e50\u6df7\u5408\u4e2d\u63d0\u53d6\u4e50\u5668\u7ea7\u522b\u7684\u97f3\u9891\u6548\u679c\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u667a\u80fd\u97f3\u4e50\u751f\u4ea7\u4e2d\u4e50\u5668\u7ea7\u522b\u97f3\u9891\u6548\u679c\u7406\u89e3\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u97f3\u9891\u8868\u793a\u5728\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u667a\u80fd\u97f3\u4e50\u751f\u4ea7\u4e2d\u56e0\u7f3a\u4e4f\u5bf9\u97f3\u9891\u6548\u679c\uff08Fx\uff09\u7684\u4e50\u5668\u7ea7\u522b\u7406\u89e3\u800c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u548c\u63d0\u53d6\u5668\u673a\u5236\uff0c\u5c06\u6df7\u5408\u7ea7\u522b\u7684\u97f3\u9891\u6548\u679c\u5d4c\u5165\u8f6c\u6362\u4e3a\u4e50\u5668\u7ea7\u522b\u7684\u5d4c\u5165\u3002", "result": "Fx-Encoder++\u5728\u68c0\u7d22\u548c\u97f3\u9891\u6548\u679c\u53c2\u6570\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e50\u5668\u7ea7\u522b\u6548\u679c\u8868\u793a\u63d0\u53d6\u3002", "conclusion": "Fx-Encoder++\u586b\u8865\u4e86\u667a\u80fd\u97f3\u4e50\u751f\u4ea7\u7cfb\u7edf\u4e2d\u4e50\u5668\u7ea7\u522b\u97f3\u9891\u6548\u679c\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.02791", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.02791", "abs": "https://arxiv.org/abs/2507.02791", "authors": ["Jakob Kienegger", "Alina Mannanova", "Huajian Fang", "Timo Gerkmann"], "title": "Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient Extraction of Moving Speakers under Weak Guidance", "comment": "Accepted at IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "Recent works on deep non-linear spatially selective filters demonstrate\nexceptional enhancement performance with computationally lightweight\narchitectures for stationary speakers of known directions. However, to maintain\nthis performance in dynamic scenarios, resource-intensive data-driven tracking\nalgorithms become necessary to provide precise spatial guidance conditioned on\nthe initial direction of a target speaker. As this additional computational\noverhead hinders application in resource-constrained scenarios such as\nreal-time speech enhancement, we present a novel strategy utilizing a\nlow-complexity tracking algorithm in the form of a particle filter instead.\nAssuming a causal, sequential processing style, we introduce temporal feedback\nto leverage the enhanced speech signal of the spatially selective filter to\ncompensate for the limited modeling capabilities of the particle filter.\nEvaluation on a synthetic dataset illustrates how the autoregressive interplay\nbetween both algorithms drastically improves tracking accuracy and leads to\nstrong enhancement performance. A listening test with real-world recordings\ncomplements these findings by indicating a clear trend towards our proposed\nself-steering pipeline as preferred choice over comparable methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u7c92\u5b50\u6ee4\u6ce2\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u7ed3\u5408\u65f6\u7a7a\u53cd\u9988\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u4e2d\u7684\u8bed\u97f3\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u975e\u7ebf\u6027\u7a7a\u95f4\u9009\u62e9\u6027\u6ee4\u6ce2\u5668\u5728\u9759\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u52a8\u6001\u573a\u666f\u4e2d\u9700\u8981\u9ad8\u8ba1\u7b97\u8d44\u6e90\u7684\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u65f6\u8bed\u97f3\u589e\u5f3a\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4f4e\u590d\u6742\u5ea6\u7684\u7c92\u5b50\u6ee4\u6ce2\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u65f6\u7a7a\u53cd\u9988\uff0c\u5229\u7528\u589e\u5f3a\u7684\u8bed\u97f3\u4fe1\u53f7\u8865\u507f\u7c92\u5b50\u6ee4\u6ce2\u7684\u5efa\u6a21\u5c40\u9650\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u589e\u5f3a\u6027\u80fd\uff1b\u771f\u5b9e\u5f55\u97f3\u7684\u542c\u529b\u6d4b\u8bd5\u8868\u660e\u5176\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u5bfc\u5411\u7ba1\u9053\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u662f\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2507.02348", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.02348", "abs": "https://arxiv.org/abs/2507.02348", "authors": ["Yiming Xu", "Dongfang Xu", "Xianghao Yu", "Shenghui Song", "Zhiguo Ding", "Robert Schober"], "title": "Joint Radiation Power, Antenna Position, and Beamforming Optimization for Pinching-Antenna Systems with Motion Power Consumption", "comment": "13 pages", "summary": "Pinching-antenna systems (PASS) have been recently proposed to improve the\nperformance of wireless networks by reconfiguring both the large-scale and\nsmall-scale channel conditions. However, existing studies ignore the physical\nconstraints of antenna placement and assume fixed antenna radiation power. To\nfill this research gap, this paper investigates the design of PASS taking into\naccount the motion power consumption of pinching-antennas (PAs) and the impact\nof adjustable antenna radiation power. To that end, we minimize the average\npower consumption for a given quality-of-service (QoS) requirement, by jointly\noptimizing the antenna positions, antenna radiation power ratios, and transmit\nbeamforming. To the best of the authors' knowledge, this is the first work to\nconsider radiation power optimization in PASS, which provides an additional\ndegree of freedom (DoF) for system design. The cases with both continuous and\ndiscrete antenna placement are considered, where the main challenge lies in the\nfact that the antenna positions affect both the magnitude and phase of the\nchannel coefficients of PASS, making system optimization very challenging. To\ntackle the resulting unique obstacles, an alternating direction method of\nmultipliers (ADMM)-based framework is proposed to solve the problem for\ncontinuous antenna movement, while its discrete counterpart is formulated as a\nmixed integer nonlinear programming (MINLP) problem and solved by the block\ncoordinate descent (BCD) method. Simulation results validate the performance\nenhancement achieved by incorporating PA movement power assumption and\nadjustable radiation power into PASS design, while also demonstrating the\nefficiency of the proposed optimization framework. The benefits of PASS over\nconventional multiple-input multiple-output (MIMO) systems in mitigating the\nlarge-scale path loss and inter-user interference is also revealed.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8003\u8651\u5929\u7ebf\u8fd0\u52a8\u529f\u8017\u548c\u53ef\u8c03\u8f90\u5c04\u529f\u7387\u7684Pinching-antenna\u7cfb\u7edf\uff08PASS\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u3001\u8f90\u5c04\u529f\u7387\u6bd4\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u6700\u5c0f\u5316\u529f\u8017\u5e76\u6ee1\u8db3QoS\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u5929\u7ebf\u653e\u7f6e\u7684\u7269\u7406\u7ea6\u675f\u548c\u56fa\u5b9a\u8f90\u5c04\u529f\u7387\uff0c\u672c\u6587\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9996\u6b21\u8003\u8651\u8f90\u5c04\u529f\u7387\u4f18\u5316\u3002", "method": "\u91c7\u7528ADMM\u6846\u67b6\u89e3\u51b3\u8fde\u7eed\u5929\u7ebf\u79fb\u52a8\u95ee\u9898\uff0c\u79bb\u6563\u95ee\u9898\u5219\u901a\u8fc7BCD\u65b9\u6cd5\u6c42\u89e3MINLP\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86PASS\u8bbe\u8ba1\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5176\u5728\u51cf\u5c11\u8def\u5f84\u635f\u8017\u548c\u7528\u6237\u95f4\u5e72\u6270\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "PASS\u901a\u8fc7\u4f18\u5316\u5929\u7ebf\u8fd0\u52a8\u548c\u8f90\u5c04\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMIMO\u7cfb\u7edf\u3002"}}
{"id": "2507.02380", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02380", "abs": "https://arxiv.org/abs/2507.02380", "authors": ["Fangru Zhou", "Jun Zhao", "Guoxin Wang"], "title": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning", "comment": null, "summary": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git.", "AI": {"tldr": "JoyTTS\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u6280\u672f\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u804a\u5929\u673a\u5668\u4eba\uff0c\u652f\u6301\u8bed\u97f3\u514b\u9686\u529f\u80fd\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408LLM\u548cTTS\u6280\u672f\uff0c\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u4e14\u529f\u80fd\u4e30\u5bcc\u7684\u8bed\u97f3\u804a\u5929\u673a\u5668\u4eba\uff0c\u4fc3\u8fdb\u793e\u533a\u8fdb\u4e00\u6b65\u5f00\u53d1\u548c\u4f18\u5316\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90\u6a21\u578bMiniCPM-o\u548cCosyVoice2\uff0c\u4f7f\u75282000\u5c0f\u65f6\u7684\u5bf9\u8bdd\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6d4b\u8bd5\u673a\u5668seed-tts-zh\u4e0a\uff0cSS\u5f97\u5206\u4e3a0.73\uff0cWER\u4e3a5.09\u3002", "conclusion": "JoyTTS\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u8bed\u97f3\u514b\u9686\u548c\u5bf9\u8bdd\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u793e\u533a\u4f7f\u7528\u548c\u6539\u8fdb\u3002"}}
{"id": "2507.02815", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.02815", "abs": "https://arxiv.org/abs/2507.02815", "authors": ["You Zhang", "Andrew Francl", "Ruohan Gao", "Paul Calamia", "Zhiyao Duan", "Ishwarya Ananthabhotla"], "title": "Towards Perception-Informed Latent HRTF Representations", "comment": "Accepted by IEEE WASPAA 2025, camera-ready version", "summary": "Personalized head-related transfer functions (HRTFs) are essential for\nensuring a realistic auditory experience over headphones, because they take\ninto account individual anatomical differences that affect listening. Most\nmachine learning approaches to HRTF personalization rely on a learned\nlow-dimensional latent space to generate or select custom HRTFs for a listener.\nHowever, these latent representations are typically learned in a manner that\noptimizes for spectral reconstruction but not for perceptual compatibility,\nmeaning they may not necessarily align with perceptual distance. In this work,\nwe first study whether traditionally learned HRTF representations are well\ncorrelated with perceptual relations using auditory-based objective perceptual\nmetrics; we then propose a method for explicitly embedding HRTFs into a\nperception-informed latent space, leveraging a metric-based loss function and\nsupervision via Metric Multidimensional Scaling (MMDS). Finally, we demonstrate\nthe applicability of these learned representations to the task of HRTF\npersonalization. We suggest that our method has the potential to render\npersonalized spatial audio, leading to an improved listening experience.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u7684HRTF\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u7684\u611f\u77e5\u4e00\u81f4\u6027\u6765\u6539\u8fdb\u542c\u89c9\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edfHRTF\u4e2a\u6027\u5316\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u5b66\u4e60\u4e2d\u6ce8\u91cd\u9891\u8c31\u91cd\u5efa\u800c\u975e\u611f\u77e5\u517c\u5bb9\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0e\u611f\u77e5\u8ddd\u79bb\u4e0d\u4e00\u81f4\u3002", "method": "\u7814\u7a76\u4f20\u7edfHRTF\u8868\u793a\u4e0e\u611f\u77e5\u5173\u7cfb\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5ea6\u91cf\u635f\u5931\u51fd\u6570\u548cMMDS\u76d1\u7763\u7684\u611f\u77e5\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728HRTF\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u8868\u660e\u5176\u80fd\u6539\u8fdb\u7a7a\u95f4\u97f3\u9891\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u63d0\u5347\u4e2a\u6027\u5316\u7a7a\u95f4\u97f3\u9891\u7684\u542c\u89c9\u4f53\u9a8c\u3002"}}
{"id": "2507.02352", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02352", "abs": "https://arxiv.org/abs/2507.02352", "authors": ["Georgios Mylonopoulos", "Luca Venturino", "Emanuele Grossi", "Stefano Buzzi", "Ciro D'Elia"], "title": "Track-before-detect in RIS-aided Integrated Sensing and Communication", "comment": "Accepted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), Isola delle Femmine, Palermo, Italy", "summary": "This study considers a base station equipped with sensing and communication\ncapabilities, which serves a ground user and scans a portion of the sky via a\npassive reconfigurable intelligent surface. To achieve more favorable system\ntradeoffs, we utilize a multi-frame radar detector, comprising a detector, a\nplot-extractor, and a track-before-detect processor. The main idea proposed\nhere is that user spectral efficiency can be enhanced by increasing the number\nof scans jointly processed by the multi-frame radar detector while maintaining\nthe same sensing performance. A numerical analysis is conducted to verify the\neffectiveness of the proposed solution and to evaluate the achievable system\ntradeoffs.", "AI": {"tldr": "\u901a\u8fc7\u591a\u5e27\u96f7\u8fbe\u68c0\u6d4b\u5668\u8054\u5408\u5904\u7406\u66f4\u591a\u626b\u63cf\uff0c\u63d0\u5347\u7528\u6237\u9891\u8c31\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u57fa\u7ad9\u540c\u65f6\u652f\u6301\u611f\u77e5\u548c\u901a\u4fe1\u529f\u80fd\u7684\u7cfb\u7edf\u4f18\u5316\uff0c\u4ee5\u6539\u5584\u7cfb\u7edf\u6027\u80fd\u6743\u8861\u3002", "method": "\u91c7\u7528\u591a\u5e27\u96f7\u8fbe\u68c0\u6d4b\u5668\uff08\u5305\u62ec\u68c0\u6d4b\u5668\u3001\u7ed8\u56fe\u63d0\u53d6\u5668\u548c\u68c0\u6d4b\u524d\u8ddf\u8e2a\u5904\u7406\u5668\uff09\uff0c\u589e\u52a0\u8054\u5408\u5904\u7406\u7684\u626b\u63cf\u6b21\u6570\u3002", "result": "\u6570\u503c\u5206\u6790\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u53ef\u5b9e\u73b0\u7684\u7cfb\u7edf\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u9891\u8c31\u6548\u7387\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2507.02391", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02391", "abs": "https://arxiv.org/abs/2507.02391", "authors": ["Mostafa Sadeghi", "Jean-Eudes Ayilo", "Romain Serizel", "Xavier Alameda-Pineda"], "title": "Posterior Transition Modeling for Unsupervised Diffusion-Based Speech Enhancement", "comment": null, "summary": "We explore unsupervised speech enhancement using diffusion models as\nexpressive generative priors for clean speech. Existing approaches guide the\nreverse diffusion process using noisy speech through an approximate,\nnoise-perturbed likelihood score, combined with the unconditional score via a\ntrade-off hyperparameter. In this work, we propose two alternative algorithms\nthat directly model the conditional reverse transition distribution of\ndiffusion states. The first method integrates the diffusion prior with the\nobservation model in a principled way, removing the need for hyperparameter\ntuning. The second defines a diffusion process over the noisy speech itself,\nyielding a fully tractable and exact likelihood score. Experiments on the\nWSJ0-QUT and VoiceBank-DEMAND datasets demonstrate improved enhancement metrics\nand greater robustness to domain shifts compared to both supervised and\nunsupervised baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u975e\u76d1\u7763\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5e72\u51c0\u8bed\u97f3\u7684\u751f\u6210\u5148\u9a8c\uff0c\u6539\u8fdb\u73b0\u6709\u975e\u76d1\u7763\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e00\u79cd\u5c06\u6269\u6563\u5148\u9a8c\u4e0e\u89c2\u6d4b\u6a21\u578b\u7ed3\u5408\uff0c\u53e6\u4e00\u79cd\u5728\u566a\u58f0\u8bed\u97f3\u4e0a\u5b9a\u4e49\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5728WSJ0-QUT\u548cVoiceBank-DEMAND\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u548c\u975e\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u8bed\u97f3\u589e\u5f3a\u548c\u9886\u57df\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.02374", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02374", "abs": "https://arxiv.org/abs/2507.02374", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Ruizhi Ruan", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim", "Abbas Jamalipour"], "title": "Predictive Control over LAWN: Joint Trajectory Design and Resource Allocation", "comment": null, "summary": "Low-altitude wireless networks (LAWNs) have been envisioned as flexible and\ntransformative platforms for enabling delay-sensitive control applications in\nInternet of Things (IoT) systems. In this work, we investigate the real-time\nwireless control over a LAWN system, where an aerial drone is employed to serve\nmultiple mobile automated guided vehicles (AGVs) via finite blocklength (FBL)\ntransmission. Toward this end, we adopt the model predictive control (MPC) to\nensure accurate trajectory tracking, while we analyze the communication\nreliability using the outage probability. Subsequently, we formulate an\noptimization problem to jointly determine control policy, transmit power\nallocation, and drone trajectory by accounting for the maximum travel distance\nand control input constraints. To address the resultant non-convex optimization\nproblem, we first derive the closed-form expression of the outage probability\nunder FBL transmission. Based on this, we reformulate the original problem as a\nquadratic programming (QP) problem, followed by developing an alternating\noptimization (AO) framework. Specifically, we employ the projected gradient\ndescent (PGD) method and the successive convex approximation (SCA) technique to\nachieve computationally efficient sub-optimal solutions. Furthermore, we\nthoroughly analyze the convergence and computational complexity of the proposed\nalgorithm. Extensive simulations and AirSim-based experiments are conducted to\nvalidate the superiority of our proposed approach compared to the baseline\nschemes in terms of control performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\u4e2d\u901a\u8fc7\u6709\u9650\u5757\u957f\u5ea6\uff08FBL\uff09\u4f20\u8f93\u5b9e\u73b0\u65e0\u4eba\u673a\u5bf9\u591a\u79fb\u52a8\u81ea\u52a8\u5bfc\u5f15\u8f66\uff08AGV\uff09\u7684\u5b9e\u65f6\u65e0\u7ebf\u63a7\u5236\uff0c\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u4f18\u5316\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u5728\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u652f\u6301\u5b9e\u65f6\u63a7\u5236\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u4e0e\u79fb\u52a8AGV\u4e4b\u95f4\u7684\u901a\u4fe1\u548c\u63a7\u5236\u95ee\u9898\u3002", "method": "\u7ed3\u5408MPC\u548cFBL\u4f20\u8f93\u5206\u6790\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u8054\u5408\u4f18\u5316\u63a7\u5236\u7b56\u7565\u3001\u529f\u7387\u5206\u914d\u548c\u65e0\u4eba\u673a\u8f68\u8ff9\u7684\u975e\u51f8\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u6846\u67b6\u6c42\u89e3\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u63a7\u5236\u6027\u80fd\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u4f18\u5316\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5b9e\u65f6\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2507.02606", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02606", "abs": "https://arxiv.org/abs/2507.02606", "authors": ["Wei Fan", "Kejiang Chen", "Chang Liu", "Weiming Zhang", "Nenghai Yu"], "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks", "comment": "Accepted by ICML 2025", "summary": "The rapid advancement of speech generation models has heightened privacy and\nsecurity concerns related to voice cloning (VC). Recent studies have\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\nperturbations. However, determined attackers can mitigate these protective\nperturbations and successfully execute VC. In this study, we conduct the first\nsystematic evaluation of these protective perturbations against VC under\nrealistic threat models that include perturbation purification. Our findings\nreveal that while existing purification methods can neutralize a considerable\nportion of the protective perturbations, they still lead to distortions in the\nfeature space of VC models, which degrades the performance of VC. From this\nperspective, we propose a novel two-stage purification method: (1) Purify the\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\nclean speech distribution. Experimental results demonstrate that our method\noutperforms state-of-the-art purification methods in disrupting VC defenses.\nOur study reveals the limitations of adversarial perturbation-based VC defenses\nand underscores the urgent need for more robust solutions to mitigate the\nsecurity and privacy risks posed by VC. The code and audio samples are\navailable at https://de-antifake.github.io.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5bf9\u6297\u6027\u6270\u52a8\u5728\u9632\u6b62\u8bed\u97f3\u514b\u9686\uff08VC\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u51c0\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8bed\u97f3\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u672a\u7ecf\u6388\u6743\u7684\u8bed\u97f3\u514b\u9686\uff08VC\uff09\u3002\u73b0\u6709\u5bf9\u6297\u6027\u6270\u52a8\u9632\u5fa1\u65b9\u6cd5\u6613\u88ab\u653b\u51fb\u8005\u7ed5\u8fc7\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u51c0\u5316\u65b9\u6cd5\uff1a\u5148\u51c0\u5316\u6270\u52a8\u8bed\u97f3\uff0c\u518d\u901a\u8fc7\u97f3\u7d20\u5f15\u5bfc\u5c06\u5176\u5bf9\u9f50\u5230\u5e72\u51c0\u8bed\u97f3\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u7834\u574fVC\u9632\u5fa1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u51c0\u5316\u65b9\u6cd5\uff0c\u4f46\u4ecd\u63ed\u793a\u4e86\u5bf9\u6297\u6027\u6270\u52a8\u9632\u5fa1\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u5e94\u5bf9VC\u5e26\u6765\u7684\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2507.02385", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02385", "abs": "https://arxiv.org/abs/2507.02385", "authors": ["Tong Ding", "Luca Venturino", "Emanuele Grossi"], "title": "Parameter estimation of range-migrating targets using OTFS signals from LEO satellites", "comment": "submitted to IEEE journal for possible publication", "summary": "This study investigates a communication-centric integrated sensing and\ncommunication (ISAC) system that utilizes orthogonal time frequency space\n(OTFS) modulated signals emitted by low Earth orbit (LEO) satellites to\nestimate the parameters of space targets experiencing range migration,\nhenceforth referred to as high-speed targets. Leveraging the specific signal\nprocessing performed by OTFS transceivers, we derive a novel input-output model\nfor the echo generated by a high-speed target in scenarios where ideal and\nrectangular shaping filters are employed. Our findings reveal that the target\nresponse exhibits a sparse structure in the delay-Doppler domain, dependent\nsolely upon the initial range and range-rate; notably, range migration causes a\nspread in the target response, marking a significant departure from previous\nstudies. Utilizing this signal structure, we propose an approximate\nimplementation of the maximum likelihood estimator for the target's initial\nrange, range-rate, and amplitude. The estimation process involves obtaining\ncoarse information on the target response using a block orthogonal matching\npursuit algorithm, followed by a refinement step using a bank of matched\nfilters focused on a smaller range and range-rate region. Finally, numerical\nexamples are provided to evaluate the estimation performance.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOTFS\u8c03\u5236\u7684LEO\u536b\u661f\u901a\u4fe1-\u611f\u77e5\u4e00\u4f53\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f30\u8ba1\u9ad8\u901f\u8fd0\u52a8\u76ee\u6807\u7684\u53c2\u6570\uff0c\u63ed\u793a\u4e86\u76ee\u6807\u54cd\u5e94\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u7684\u7a00\u758f\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u7684\u8fd1\u4f3c\u5b9e\u73b0\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u9ad8\u901f\u8fd0\u52a8\u76ee\u6807\u5728\u901a\u4fe1-\u611f\u77e5\u4e00\u4f53\u5316\u7cfb\u7edf\u4e2d\u7684\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5b58\u5728\u8ddd\u79bb\u8fc1\u79fb\u7684\u76ee\u6807\u3002", "method": "\u5229\u7528OTFS\u8c03\u5236\u4fe1\u53f7\u5904\u7406\uff0c\u63a8\u5bfc\u4e86\u76ee\u6807\u56de\u6ce2\u7684\u8f93\u5165\u8f93\u51fa\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u548c\u5339\u914d\u6ee4\u6ce2\u5668\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u8fd1\u4f3c\u5b9e\u73b0\u65b9\u6cd5\u3002", "result": "\u76ee\u6807\u54cd\u5e94\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u5448\u73b0\u7a00\u758f\u7ed3\u6784\uff0c\u4e14\u8ddd\u79bb\u8fc1\u79fb\u4f1a\u5bfc\u81f4\u76ee\u6807\u54cd\u5e94\u6269\u6563\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u901f\u8fd0\u52a8\u76ee\u6807\u7684\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86OTFS\u8c03\u5236\u5728\u901a\u4fe1-\u611f\u77e5\u4e00\u4f53\u5316\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02666", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02666", "abs": "https://arxiv.org/abs/2507.02666", "authors": ["Junyu Wang", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning", "comment": "Accepted at Interspeech2025", "summary": "In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\uff08ASDA\uff09\uff0c\u901a\u8fc7\u53cc\u8f6f\u6700\u5927\u64cd\u4f5c\u548c\u5dee\u5206\u7cfb\u6570\u4f18\u5316\u6ce8\u610f\u529b\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u6807\u51c6Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u5206\u914d\u6743\u91cd\u7ed9\u65e0\u5173\u4fe1\u606f\uff0c\u5f71\u54cd\u6a21\u578b\u5224\u522b\u80fd\u529b\u3002", "method": "\u5f15\u5165\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u53cc\u8f6f\u6700\u5927\u64cd\u4f5c\u548c\u5dee\u5206\u7cfb\u6570\u4f18\u5316\u6ce8\u610f\u529b\u5206\u914d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5305\u62ec\u97f3\u9891\u5206\u7c7b\u3001\u5173\u952e\u8bcd\u8bc6\u522b\u548c\u73af\u5883\u58f0\u97f3\u5206\u7c7b\u3002", "conclusion": "ASDA\u5728\u97f3\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.02427", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02427", "abs": "https://arxiv.org/abs/2507.02427", "authors": ["Jia Guo", "Chenyang Yang"], "title": "When Attention is Beneficial for Learning Wireless Resource Allocation Efficiently?", "comment": "13 pages, 6 figures", "summary": "Owing to the use of attention mechanism to leverage the dependency across\ntokens, Transformers are efficient for natural language processing. By\nharnessing permutation properties broadly exist in resource allocation\npolicies, each mapping measurable environmental parameters (e.g., channel\nmatrix) to optimized variables (e.g., precoding matrix), graph neural networks\n(GNNs) are promising for learning these policies efficiently in terms of\nscalability and generalizability. To reap the benefits of both architectures,\nthere is a recent trend of incorporating attention mechanism with GNNs for\nlearning wireless policies. Nevertheless, is the attention mechanism really\nneeded for resource allocation? In this paper, we strive to answer this\nquestion by analyzing the structures of functions defined on sets and numerical\nalgorithms, given that the permutation properties of wireless policies are\ninduced by the involved sets (say user set). In particular, we prove that the\npermutation equivariant functions on a single set can be recursively expressed\nby two types of functions: one involves attention, and the other does not. We\nproceed to re-express the numerical algorithms for optimizing several\nrepresentative resource allocation problems in recursive forms. We find that\nwhen interference (say multi-user or inter-data stream interference) is not\nreflected in the measurable parameters of a policy, attention needs to be used\nto model the interference. With the insight, we establish a framework of\ndesigning GNNs by aligning with the structures. By taking reconfigurable\nintelligent surface-aided hybrid precoding as an example, the learning\nefficiency of the proposed GNN is validated via simulations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u901a\u8fc7\u5206\u6790\u96c6\u5408\u4e0a\u7684\u51fd\u6570\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u610f\u529b\u4e0e\u975e\u6ce8\u610f\u529b\u673a\u5236\u7684GNN\u8bbe\u8ba1\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u6ce8\u610f\u529b\u673a\u5236\u5728\u65e0\u7ebf\u8d44\u6e90\u5206\u914d\u7b56\u7565\u4e2d\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u4ee5\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u96c6\u5408\u4e0a\u7684\u7f6e\u6362\u7b49\u53d8\u51fd\u6570\u7ed3\u6784\uff0c\u63d0\u51fa\u9012\u5f52\u8868\u8fbe\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6570\u503c\u7b97\u6cd5\u9a8c\u8bc1\u6ce8\u610f\u529b\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "result": "\u5f53\u5e72\u6270\u672a\u53cd\u6620\u5728\u7b56\u7565\u7684\u53ef\u6d4b\u53c2\u6570\u4e2d\u65f6\uff0c\u9700\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\uff1b\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684GNN\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0GNN\u7684\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u5206\u914d\u7b56\u7565\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.02556", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02556", "abs": "https://arxiv.org/abs/2507.02556", "authors": ["C. S. Ramalingam"], "title": "Corrections to Published Values of Frequency Sampling Filter Transition Coefficients", "comment": "11 pages, 5 figures", "summary": "Tables of optimal transition coefficients and peak sidelobe level (PSL, in\ndB) associated with frequency sampling filter (FSF) design were published by\nRabiner et al. (Jun 1970), and reproduced, for example, in the book Digital\nSignal Processing by Proakis and Manolakis (4/e, 2007). A set of values are\nalso given in Appendix H of Understanding Digital Signal Processing} by Lyons\n(3/e, 2011), but there are significant differences between these two sets. For\nexample, for $N=16$ and BW$=4$, two different transition coefficient values\nhave been reported, viz., $0.38916626$ (Rabiner, et al.) and $0.34918551$\n(Lyons). Neither is optimal, for we find the optimum value to be $0.40474097$.\nThe published values of the corresponding PSLs were also found to be incorrect.\nIn this paper we give the optimal values of the transition coefficients and PSL\nvalues as estimated by our program for the lowpass and bandpass filters listed\nin Rabiner et al. and Lyons.", "AI": {"tldr": "\u672c\u6587\u7ea0\u6b63\u4e86Rabiner\u7b49\u4eba\u548cLyons\u5173\u4e8e\u9891\u7387\u91c7\u6837\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e2d\u8fc7\u6e21\u7cfb\u6570\u548c\u5cf0\u503c\u65c1\u74e3\u7535\u5e73\uff08PSL\uff09\u7684\u9519\u8bef\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u540e\u7684\u6b63\u786e\u503c\u3002", "motivation": "\u5148\u524d\u6587\u732e\u4e2d\u5173\u4e8e\u9891\u7387\u91c7\u6837\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u7684\u8fc7\u6e21\u7cfb\u6570\u548cPSL\u503c\u5b58\u5728\u4e0d\u4e00\u81f4\u548c\u9519\u8bef\uff0c\u9700\u8981\u63d0\u4f9b\u51c6\u786e\u7684\u6700\u4f18\u503c\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u8ba1\u7b97\u5e76\u9a8c\u8bc1\u4e86\u4f4e\u901a\u548c\u5e26\u901a\u6ee4\u6ce2\u5668\u7684\u6700\u4f18\u8fc7\u6e21\u7cfb\u6570\u548cPSL\u503c\u3002", "result": "\u53d1\u73b0\u4e86\u6587\u732e\u4e2d\u62a5\u544a\u7684\u503c\u4e0d\u6b63\u786e\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u503c\uff0c\u4f8b\u5982\u5bf9\u4e8eN=16\u548cBW=4\uff0c\u6700\u4f18\u8fc7\u6e21\u7cfb\u6570\u4e3a0.40474097\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u9891\u7387\u91c7\u6837\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e2d\u6700\u4f18\u8fc7\u6e21\u7cfb\u6570\u548cPSL\u7684\u51c6\u786e\u6570\u636e\uff0c\u7ea0\u6b63\u4e86\u5148\u524d\u6587\u732e\u4e2d\u7684\u9519\u8bef\u3002"}}
{"id": "2507.02641", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02641", "abs": "https://arxiv.org/abs/2507.02641", "authors": ["Shuaixin Yang", "Yijia Li", "Yue Xiao", "Yong Liang Guan", "Xianfu Lei", "Zhiguo Ding"], "title": "Pinching-Antenna-Assisted Index Modulation: Channel Modeling, Transceiver Design, and Performance Analysis", "comment": null, "summary": "In this paper, a novel pinching-antenna assisted index modulation (PA-IM)\nscheme is proposed for improving the spectral efficiency without increasing the\nhardware complexity, where the information bits are conveyed not only by the\nconventional M-ary quadrature amplitude modulation (QAM) symbols but also by\nthe indices of pinching antenna (PA) position patterns. To realize the full\npotential of this scheme, this paper focuses on the comprehensive transceiver\ndesign, addressing key challenges in signal detection at the receiver and\nperformance optimization at thetransmitter. First, a comprehensive channel\nmodel is formulated for this architecture, which sophisticatedly integrates the\ndeterministic in-waveguide propagation effects with the stochastic nature of\nwireless channels, including both largescale path loss and small-scale fading.\nNext, to overcome the prohibitive complexity of optimal maximum likelihood (ML)\ndetection, a low-complexity box-optimized sphere decoding (BOSD) algorithm is\ndesigned, which adaptively prunes the search space whilst preserving optimal ML\nperformance. Furthermore, an analytical upper bound on the bit error rate (BER)\nis derived and validated by the simulations. Moreover, a new transmit precoding\nmethod is designed using manifold optimization, which minimizes the BER by\njointly optimizing the complex-valued precoding coefficients across the\nwaveguides for the sake of maximizing the minimum Euclidean distance of all\nreceived signal points. Finally, the simulation results demonstrate that the\nproposed PA-IM scheme attains a significant performance gain over its\nconventional counterparts and that the overall BER of the pinching-antenna\nsystem is substantially improved by the proposed precoding design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684PA-IM\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408QAM\u7b26\u53f7\u548cPA\u4f4d\u7f6e\u6a21\u5f0f\u7d22\u5f15\u63d0\u9ad8\u9891\u8c31\u6548\u7387\uff0c\u8bbe\u8ba1\u4e86\u4f4e\u590d\u6742\u5ea6\u7684BOSD\u68c0\u6d4b\u7b97\u6cd5\u548c\u57fa\u4e8e\u6d41\u5f62\u4f18\u5316\u7684\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u786c\u4ef6\u590d\u6742\u5ea6\u4e0d\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u9891\u8c31\u6548\u7387\uff0c\u540c\u65f6\u89e3\u51b3\u4fe1\u53f7\u68c0\u6d4b\u548c\u6027\u80fd\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\u3002", "method": "1. \u5efa\u7acb\u7efc\u5408\u4fe1\u9053\u6a21\u578b\uff1b2. \u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u7684BOSD\u68c0\u6d4b\u7b97\u6cd5\uff1b3. \u63a8\u5bfcBER\u4e0a\u754c\uff1b4. \u4f7f\u7528\u6d41\u5f62\u4f18\u5316\u8bbe\u8ba1\u9884\u7f16\u7801\u65b9\u6cd5\u3002", "result": "PA-IM\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9884\u7f16\u7801\u8bbe\u8ba1\u5927\u5e45\u964d\u4f4e\u4e86\u7cfb\u7edf\u7684BER\u3002", "conclusion": "PA-IM\u65b9\u6848\u901a\u8fc7\u521b\u65b0\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9891\u8c31\u6548\u7387\u548c\u6027\u80fd\u7684\u53cc\u91cd\u63d0\u5347\u3002"}}
{"id": "2507.02802", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02802", "abs": "https://arxiv.org/abs/2507.02802", "authors": ["Jiazhe Li", "Nicol\u00f2 Decarli", "Francesco Guidi", "Heng Dong", "Anna Guerra", "Alessandro Bazzi", "Zhuoming Li"], "title": "AREE-Based Decoupled Design of Hybrid Beamformers in mmWave XL-MIMO Systems", "comment": null, "summary": "Hybrid beamforming has been widely employed in mmWave communications such as\nvehicular-to-everything (V2X) scenarios, as a compromise between hardware\ncomplexity and spectral efficiency. However, the inherent coupling between\nanalog and digital precoders in hybrid array architecture significantly limits\nthe computational and spectral efficiency of existing algorithms. To address\nthis issue, we propose an alternating residual error elimination (AREE)\nalgorithm, which decomposes the hybrid beamforming problem into two\nlow-dimensional subproblems, each exhibiting a favorable matrix structure that\nenables effective decoupling of analog and digital precoders from the matrix\nproduct formulation. These subproblems iteratively eliminate each other's\nresidual errors, driving the original problem toward the optimal hybrid\nbeamforming performance. The proposed initialization ensures rapid convergence,\nwhile a low-complexity geometric channel SVD algorithm is developed by\ntransforming the high-dimensional sparse channel into a low-dimensional\nequivalent, thereby simplifying the derivation of subproblems. Simulation\nresults demonstrate that the AREE algorithm effectively decouples analog and\ndigital precoders with low complexity, achieves fast convergence, and offers\nhigher spectral efficiency than existing beamforming methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u6b8b\u5dee\u6d88\u9664\uff08AREE\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u6df7\u5408\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002", "motivation": "\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u5728\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u6a21\u62df\u548c\u6570\u5b57\u9884\u7f16\u7801\u5668\u7684\u8026\u5408\u9650\u5236\u4e86\u8ba1\u7b97\u548c\u9891\u8c31\u6548\u7387\u3002", "method": "\u5c06\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u4f4e\u7ef4\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u66ff\u6d88\u9664\u6b8b\u5dee\u8bef\u5dee\u5b9e\u73b0\u89e3\u8026\uff0c\u5e76\u91c7\u7528\u51e0\u4f55\u4fe1\u9053SVD\u7b97\u6cd5\u7b80\u5316\u8ba1\u7b97\u3002", "result": "AREE\u7b97\u6cd5\u80fd\u4ee5\u4f4e\u590d\u6742\u5ea6\u89e3\u8026\u6a21\u62df\u548c\u6570\u5b57\u9884\u7f16\u7801\u5668\uff0c\u5feb\u901f\u6536\u655b\uff0c\u4e14\u9891\u8c31\u6548\u7387\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AREE\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u4e2d\u7684\u8026\u5408\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.02824", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02824", "abs": "https://arxiv.org/abs/2507.02824", "authors": ["Po-Heng Chou", "Ching-Wen Chen", "Wan-Jen Huang", "Walid Saad", "Yu Tsao", "Ronald Y. Chang"], "title": "DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift", "comment": "5 pages, 4 figures, 2 tables, accepted by IEEE Globecom 2024\n  Workshops", "summary": "In this paper, the precoding design is investigated for maximizing the\nthroughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)\nsystems with obstructed direct communication paths. In particular, a\nreconfigurable intelligent surface (RIS) is employed to enhance MIMO\ntransmissions, considering mmWave characteristics related to line-of-sight\n(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal\ncodewords in the continuous phase shift is computationally intensive and\ntime-consuming. To reduce computational complexity, permuted discrete Fourier\ntransform (DFT) vectors are used for finding codebook design, incorporating\namplitude responses for practical or ideal RIS systems. However, even if the\ndiscrete phase shift is adopted in the ES, it results in significant\ncomputation and is time-consuming. Instead, the trained deep neural network\n(DNN) is developed to facilitate faster codeword selection. Simulation results\nshow that the DNN maintains sub-optimal spectral efficiency even as the\ndistance between the end-user and the RIS has variations in the testing phase.\nThese results highlight the potential of DNN in advancing RIS-aided systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u4e2d\u901a\u8fc7\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u4f18\u5316\u9884\u7f16\u7801\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u4f4e\u590d\u6742\u5ea6\u7801\u5b57\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7a77\u4e3e\u641c\u7d22\uff08ES\uff09\u5728\u8fde\u7eed\u76f8\u4f4d\u504f\u79fb\u4e2d\u5bfb\u627e\u6700\u4f18\u7801\u5b57\u8ba1\u7b97\u590d\u6742\u4e14\u8017\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u5411\u91cf\u8bbe\u8ba1\u7801\u672c\uff0c\u5e76\u7ed3\u5408DNN\u52a0\u901f\u7801\u5b57\u9009\u62e9\u3002", "result": "\u4eff\u771f\u8868\u660eDNN\u5728\u6d4b\u8bd5\u9636\u6bb5\u80fd\u4fdd\u6301\u6b21\u4f18\u9891\u8c31\u6548\u7387\uff0c\u9002\u5e94\u8ddd\u79bb\u53d8\u5316\u3002", "conclusion": "DNN\u5728RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}

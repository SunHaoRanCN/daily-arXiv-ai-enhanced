{"id": "2507.02915", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02915", "abs": "https://arxiv.org/abs/2507.02915", "authors": ["Ludovic Tuncay", "Etienne Labbé", "Emmanouil Benetos", "Thomas Pellegrini"], "title": "Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning", "comment": null, "summary": "Building on the Joint-Embedding Predictive Architecture (JEPA) paradigm, a\nrecent self-supervised learning framework that predicts latent representations\nof masked regions in high-level feature spaces, we propose Audio-JEPA (Audio\nJoint-Embedding Predictive Architecture), tailored specifically for audio data.\nAudio-JEPA uses a simple Vision Transformer backbone to predict latent\nrepresentations of masked spectrogram patches rather than reconstructing raw\naudio. We pre-train on unlabeled AudioSet clips (10s, 32kHz) with random patch\nmasking on mel-spectrograms. We evaluate on the X-ARES suite covering speech,\nmusic, and environmental sound tasks. Although our implementation is a\nstraightforward translation of the original model to audio, the results still\nshow comparable performance to wav2vec 2.0 and data2vec while using less than\none-fifth of their training data and with no hyper-parameter tuning. All code\nand pretrained checkpoints will be released on GitHub."}
{"id": "2507.03251", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03251", "abs": "https://arxiv.org/abs/2507.03251", "authors": ["HyeYoung Lee", "Muhammad Nadeem"], "title": "Toward Efficient Speech Emotion Recognition via Spectral Learning and Attention", "comment": null, "summary": "Speech Emotion Recognition (SER) traditionally relies on auditory data\nanalysis for emotion classification. Several studies have adopted different\nmethods for SER. However, existing SER methods often struggle to capture subtle\nemotional variations and generalize across diverse datasets. In this article,\nwe use Mel-Frequency Cepstral Coefficients (MFCCs) as spectral features to\nbridge the gap between computational emotion processing and human auditory\nperception. To further improve robustness and feature diversity, we propose a\nnovel 1D-CNN-based SER framework that integrates data augmentation techniques.\nMFCC features extracted from the augmented data are processed using a 1D\nConvolutional Neural Network (CNN) architecture enhanced with channel and\nspatial attention mechanisms. These attention modules allow the model to\nhighlight key emotional patterns, enhancing its ability to capture subtle\nvariations in speech signals. The proposed method delivers cutting-edge\nperformance, achieving the accuracy of 97.49% for SAVEE, 99.23% for RAVDESS,\n89.31% for CREMA-D, 99.82% for TESS, 99.53% for EMO-DB, and 96.39% for EMOVO.\nExperimental results show new benchmarks in SER, demonstrating the\neffectiveness of our approach in recognizing emotional expressions with high\nprecision. Our evaluation demonstrates that the integration of advanced Deep\nLearning (DL) methods substantially enhances generalization across diverse\ndatasets, underscoring their potential to advance SER for real-world deployment\nin assistive technologies and human-computer interaction."}
{"id": "2507.03377", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03377", "abs": "https://arxiv.org/abs/2507.03377", "authors": ["Masato Murata", "Koichi Miyazaki", "Tomoki Koriyama", "Tomoki Toda"], "title": "Eigenvoice Synthesis based on Model Editing for Speaker Generation", "comment": "Accepted by INTERSPEECH 2025", "summary": "Speaker generation task aims to create unseen speaker voice without reference\nspeech. The key to the task is defining a speaker space that represents diverse\nspeakers to determine the generated speaker trait. However, the effective way\nto define this speaker space remains unclear. Eigenvoice synthesis is one of\nthe promising approaches in the traditional parametric synthesis framework,\nsuch as HMM-based methods, which define a low-dimensional speaker space using\npre-stored speaker features. This study proposes a novel DNN-based eigenvoice\nsynthesis method via model editing. Unlike prior methods, our method defines a\nspeaker space in the DNN model parameter space. By directly sampling new DNN\nmodel parameters in this space, we can create diverse speaker voices.\nExperimental results showed the capability of our method to generate diverse\nspeakers' speech. Moreover, we discovered a gender-dominant axis in the created\nspeaker space, indicating the potential to control speaker attributes."}
{"id": "2507.03382", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03382", "abs": "https://arxiv.org/abs/2507.03382", "authors": ["Masato Murata", "Koichi Miyazaki", "Tomoki Koriyama"], "title": "Speaker-agnostic Emotion Vector for Cross-speaker Emotion Intensity Control", "comment": "Accepted by INTERSPEECH 2025", "summary": "Cross-speaker emotion intensity control aims to generate emotional speech of\na target speaker with desired emotion intensities using only their neutral\nspeech. A recently proposed method, emotion arithmetic, achieves emotion\nintensity control using a single-speaker emotion vector. Although this prior\nmethod has shown promising results in the same-speaker setting, it lost speaker\nconsistency in the cross-speaker setting due to mismatches between the emotion\nvector of the source and target speakers. To overcome this limitation, we\npropose a speaker-agnostic emotion vector designed to capture shared emotional\nexpressions across multiple speakers. This speaker-agnostic emotion vector is\napplicable to arbitrary speakers. Experimental results demonstrate that the\nproposed method succeeds in cross-speaker emotion intensity control while\nmaintaining speaker consistency, speech quality, and controllability, even in\nthe unseen speaker case."}
{"id": "2507.02879", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02879", "abs": "https://arxiv.org/abs/2507.02879", "authors": ["Naimahmed Nesaragi", "Hemin Ali Qadir", "Per Steiner Halvorsen", "Ilangko Balasingham"], "title": "Biaxialformer: Leveraging Channel Independence and Inter-Channel Correlations in EEG Signal Decoding for Predicting Neurological Outcomes", "comment": "12 pages, 3 figures, Article", "summary": "Accurate decoding of EEG signals requires comprehensive modeling of both\ntemporal dynamics within individual channels and spatial dependencies across\nchannels. While Transformer-based models utilizing channel-independence (CI)\nstrategies have demonstrated strong performance in various time series tasks,\nthey often overlook the inter-channel correlations that are critical in\nmultivariate EEG signals. This omission can lead to information degradation and\nreduced prediction accuracy, particularly in complex tasks such as neurological\noutcome prediction. To address these challenges, we propose Biaxialformer,\ncharacterized by a meticulously engineered two-stage attention-based framework.\nThis model independently captures both sequence-specific (temporal) and\nchannel-specific (spatial) EEG information, promoting synergy and mutual\nreinforcement across channels without sacrificing CI. By employing joint\nlearning of positional encodings, Biaxialformer preserves both temporal and\nspatial relationships in EEG data, mitigating the interchannel correlation\nforgetting problem common in traditional CI models. Additionally, a\ntokenization module with variable receptive fields balance the extraction of\nfine-grained, localized features and broader temporal dependencies. To enhance\nspatial feature extraction, we leverage bipolar EEG signals, which capture\ninter-hemispheric brain interactions, a critical but often overlooked aspect in\nEEG analysis. Our study broadens the use of Transformer-based models by\naddressing the challenge of predicting neurological outcomes in comatose\npatients. Using the multicenter I-CARE data from five hospitals, we validate\nthe robustness and generalizability of Biaxialformer with an average AUC\n0.7688, AUPRC 0.8643, and F1 0.6518 in a cross-hospital scenario."}
{"id": "2507.03149", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.03149", "abs": "https://arxiv.org/abs/2507.03149", "authors": ["Kevin Huang", "Sean Foley", "Jihwan Lee", "Yoonjeong Lee", "Dani Byrd", "Shrikanth Narayanan"], "title": "On the Relationship between Accent Strength and Articulatory Features", "comment": "Accepted for Interspeech2025", "summary": "This paper explores the relationship between accent strength and articulatory\nfeatures inferred from acoustic speech. To quantify accent strength, we compare\nphonetic transcriptions with transcriptions based on dictionary-based\nreferences, computing phoneme-level difference as a measure of accent strength.\nThe proposed framework leverages recent self-supervised learning articulatory\ninversion techniques to estimate articulatory features. Analyzing a corpus of\nread speech from American and British English speakers, this study examines\ncorrelations between derived articulatory parameters and accent strength\nproxies, associating systematic articulatory differences with indexed accent\nstrength. Results indicate that tongue positioning patterns distinguish the two\ndialects, with notable differences inter-dialects in rhotic and low back\nvowels. These findings contribute to automated accent analysis and articulatory\nmodeling for speech processing applications."}
{"id": "2507.03395", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03395", "abs": "https://arxiv.org/abs/2507.03395", "authors": ["Luca A. Lanzendörfer", "Florian Grötschla", "Karim Galal", "Roger Wattenhofer"], "title": "MaskBeat: Loopable Drum Beat Generation", "comment": "Extended Abstract ISMIR 2025", "summary": "We present MaskBeat, a transformer-based approach for loopable drum pattern\ngeneration. Rather than predicting drum hits sequentially, our method uses\nbidirectional attention with iterative refinement, allowing instruments to be\ngenerated in parallel while maintaining musical coherence. Additionally, we\nintroduce custom loss functions that capture drum-specific musical\nrelationships. Our experiments show that MaskBeat generates higher quality and\nmore musically coherent drum patterns than baseline approaches."}
{"id": "2507.03109", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03109", "abs": "https://arxiv.org/abs/2507.03109", "authors": ["Gerald Enzner", "Niklas Knaepper", "Aleksej Chinaev"], "title": "Cross-Comparison of Neural Architectures and Data Sets for Digital Self-Interference Modeling", "comment": null, "summary": "Inband full-duplex communication requires accurate modeling and cancellation\nof self-interference, specifically in the digital domain. Neural networks are\npresently candidate models for capturing nonlinearity of the self-interference\npath. This work utilizes synthetic and real data from different sources to\nevaluate and cross-compare performances of previously proposed neural\nself-interference models from different sources. The relevance of the analysis\nconsists in the mutual assessment of methods on data they were not specifically\ndesigned for. We find that our previously proposed Hammerstein model represents\nthe range of data sets well, while being significantly smaller in terms of the\nnumber of parameters. A new Wiener-Hammerstein model further enhances the\ngeneralization performance."}
{"id": "2507.03887", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03887", "abs": "https://arxiv.org/abs/2507.03887", "authors": ["Yuxiang Zhao", "Yunchong Xiao", "Yushen Chen", "Zhikang Niu", "Shuai Wang", "Kai Yu", "Xie Chen"], "title": "Traceable TTS: Toward Watermark-Free TTS with Strong Traceability", "comment": null, "summary": "Recent advances in Text-To-Speech (TTS) technology have enabled synthetic\nspeech to mimic human voices with remarkable realism, raising significant\nsecurity concerns. This underscores the need for traceable TTS models-systems\ncapable of tracing their synthesized speech without compromising quality or\nsecurity. However, existing methods predominantly rely on explicit watermarking\non speech or on vocoder, which degrades speech quality and is vulnerable to\nspoofing. To address these limitations, we propose a novel framework for model\nattribution. Instead of embedding watermarks, we train the TTS model and\ndiscriminator using a joint training method that significantly improves\ntraceability generalization while preserving-and even slightly improving-audio\nquality. This is the first work toward watermark-free TTS with strong\ntraceability. To promote progress in related fields, we will release the code\nupon acceptance of the paper."}
{"id": "2507.03466", "categories": ["cs.SD", "cs.SY", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.03466", "abs": "https://arxiv.org/abs/2507.03466", "authors": ["Mahdi Ali Pour", "Utku Gunay Acer"], "title": "Direction Estimation of Sound Sources Using Microphone Arrays and Signal Strength", "comment": "5 pages", "summary": "Sound-tracking refers to the process of determining the direction from which\na sound originates, making it a fundamental component of sound source\nlocalization. This capability is essential in a variety of applications,\nincluding security systems, acoustic monitoring, and speaker tracking, where\naccurately identifying the direction of a sound source enables real-time\nresponses, efficient resource allocation, and improved situational awareness.\nWhile sound-tracking is closely related to localization, it specifically\nfocuses on identifying the direction of the sound source rather than estimating\nits exact position in space. Despite its utility, sound-tracking systems face\nseveral challenges, such as maintaining directional accuracy and precision,\nalong with the need for sophisticated hardware configurations and complex\nsignal processing algorithms. This paper presents a sound-tracking method using\nthree electret microphones. We estimate the direction of a sound source using a\nlightweight method that analyzes signals from three strategically placed\nmicrophones. By comparing the average power of the received signals, the system\ninfers the most probable direction of the sound. The results indicate that the\npower level from each microphone effectively determines the sound source\ndirection. Our system employs a straightforward and cost-effective hardware\ndesign, ensuring simplicity and affordability in implementation. It achieves a\nlocalization error of less than 6 degrees and a precision of 98%. Additionally,\nits effortless integration with various systems makes it versatile and\nadaptable. Consequently, this technique presents a robust and reliable solution\nfor sound-tracking and localization, with potential applications spanning\ndiverse domains such as security systems, smart homes, and acoustic monitoring."}
{"id": "2507.03246", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03246", "abs": "https://arxiv.org/abs/2507.03246", "authors": ["Muhammad Khalil", "Ke Wang", "Jinho Choi"], "title": "Enhancing Satellite Quantum Key Distribution with Dual Band Reconfigurable Intelligent Surfaces", "comment": "11", "summary": "This paper presents a novel system architecture for hybrid satellite\ncommunications, integrating quantum key distribution (QKD) and classical radio\nfrequency (RF) data transmission using a dual-band reconfigurable intelligent\nsurface (RIS). The motivation is to address the growing need for global,\nsecure, and reliable communications by leveraging the security of quantum\noptical links and the robustness of classical RF channels within a unified\nframework. By employing a frequency-selective RIS, the system independently\noptimizes both quantum (850 nm) and classical (S-band) channels in real time,\ndynamically adapting to environmental fluctuations such as atmospheric\nturbulence and rain attenuation. The joint optimization of the quantum bit\nerror rate (QBER) and the classical signal-to noise ratio (SNR) is formulated\nas a quadratic unconstrained binary optimization (QUBO) problem, enabling\nefficient adaptive phase control utilizing both quantum and classical\ncomputational methods. Comprehensive theoretical modeling and simulations,\nbenchmarked against experimental data from the Micius satellite, demonstrate\nsubstantial performance gains. Notably, the RIS assisted system reduces QBER\nfrom approximately 2.5% to 0.7%, increases the secure key rate (SKR) to over\n30,000 bits per second, and enhances classical RF SNR by about 3 dB at high\nelevation angles. These results illustrate the practical potential of hybrid\nRIS-assisted satellite links to deliver robust, efficient, and secure global\ncommunications."}
{"id": "2507.03912", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.03912", "abs": "https://arxiv.org/abs/2507.03912", "authors": ["Tomoki Koriyama"], "title": "Prosody Labeling with Phoneme-BERT and Speech Foundation Models", "comment": "Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "This paper proposes a model for automatic prosodic label annotation, where\nthe predicted labels can be used for training a prosody-controllable\ntext-to-speech model. The proposed model utilizes not only rich acoustic\nfeatures extracted by a self-supervised-learning (SSL)-based model or a Whisper\nencoder, but also linguistic features obtained from phoneme-input pretrained\nlinguistic foundation models such as PnG BERT and PL-BERT. The concatenation of\nacoustic and linguistic features is used to predict phoneme-level prosodic\nlabels. In the experimental evaluation on Japanese prosodic labels, including\npitch accents and phrase break indices, it was observed that the combination of\nboth speech and linguistic foundation models enhanced the prediction accuracy\ncompared to using either a speech or linguistic input alone. Specifically, we\nachieved 89.8% prediction accuracy in accent labels, 93.2% in high-low pitch\naccents, and 94.3% in break indices."}
{"id": "2507.03468", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03468", "abs": "https://arxiv.org/abs/2507.03468", "authors": ["Hieu-Thi Luong", "Inbal Rimons", "Haim Permuter", "Kong Aik Lee", "Eng Siong Chng"], "title": "Robust Localization of Partially Fake Speech: Metrics, Models, and Out-of-Domain Evaluation", "comment": "Submitted to APSIPA 2025", "summary": "Partial audio deepfake localization pose unique challenges and remain\nunderexplored compared to full-utterance spoofing detection. While recent\nmethods report strong in-domain performance, their real-world utility remains\nunclear. In this analysis, we critically examine the limitations of current\nevaluation practices, particularly the widespread use of Equal Error Rate\n(EER), which often obscures generalization and deployment readiness. We propose\nreframing the localization task as a sequential anomaly detection problem and\nadvocate for the use of threshold-dependent metrics such as accuracy,\nprecision, recall, and F1-score, which better reflect real-world behavior.\nSpecifically, we analyze the performance of the open-source Coarse-to-Fine\nProposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on\nthe in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the\nLlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our\nreproduced version of the same model performs worse on in-domain data (9.84%)\nbut better on the out-of-domain sets (41.72% and 14.98%, respectively). This\nhighlights the risks of over-optimizing for in-domain EER, which can lead to\nmodels that perform poorly in real-world scenarios. It also suggests that while\ndeep learning models can be effective on in-domain data, they generalize poorly\nto out-of-domain scenarios, failing to detect novel synthetic samples and\nmisclassifying unfamiliar bona fide audio. Finally, we observe that adding more\nbona fide or fully synthetic utterances to the training data often degrades\nperformance, whereas adding partially fake utterances improves it."}
{"id": "2507.03351", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.03351", "abs": "https://arxiv.org/abs/2507.03351", "authors": ["Yuqi Ye", "Li You", "Hao Xu", "Ahmed Elzanaty", "Kai-Kit Wong", "Xiqi Gao"], "title": "Specific Absorption Rate-Aware Multiuser MIMO Assisted by Fluid Antenna System", "comment": "12 pages, 9 figures, to appear in IEEE Transactions on Wireless\n  Communications", "summary": "With the development of the upcoming sixth-generation (6G) wireless networks,\nthere is a pressing need for innovative technologies capable of satisfying\nheightened performance indicators. Fluid antenna system (FAS) is proposed\nrecently as a promising technique to achieve higher data rates and more\ndiversity gains by dynamically changing the positions of the antennas to form a\nmore desirable channel. However, worries regarding the possibly harmful effects\nof electromagnetic (EM) radiation emitted by devices have arisen as a result of\nthe rapid evolution of advanced techniques in wireless communication systems.\nSpecific absorption rate (SAR) is a widely adopted metric to quantify EM\nradiation worldwide. In this paper, we investigate the SAR-aware multiuser\nmultiple-input multiple-output (MIMO) communications assisted by FAS. In\nparticular, a two-layer iterative algorithm is proposed to minimize the SAR\nvalue under signal-to-interference-plus-noise ratio (SINR) and FAS constraints.\nMoreover, the minimum weighted SINR maximization problem under SAR and FAS\nconstraints is studied by finding its relationship with the SAR minimization\nproblem. Simulation results verify that the proposed SAR-aware FAS design\noutperforms the adaptive backoff and fixed-position antenna designs."}
{"id": "2507.04094", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04094", "abs": "https://arxiv.org/abs/2507.04094", "authors": ["Yi-Cheng Lin", "Jia-Hung Chen", "Hung-yi Lee"], "title": "MMMOS: Multi-domain Multi-axis Audio Quality Assessment", "comment": "4 pages including 1 page of reference. ASRU Audio MOS 2025 Challenge\n  paper", "summary": "Accurate audio quality estimation is essential for developing and evaluating\naudio generation, retrieval, and enhancement systems. Existing non-intrusive\nassessment models predict a single Mean Opinion Score (MOS) for speech, merging\ndiverse perceptual factors and failing to generalize beyond speech. We propose\nMMMOS, a no-reference, multi-domain audio quality assessment system that\nestimates four orthogonal axes: Production Quality, Production Complexity,\nContent Enjoyment, and Content Usefulness across speech, music, and\nenvironmental sounds. MMMOS fuses frame-level embeddings from three pretrained\nencoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with\nfour loss functions. By ensembling the top eight models, MMMOS shows a 20-30%\nreduction in mean squared error and a 4-5% increase in Kendall's {\\tau} versus\nbaseline, gains first place in six of eight Production Complexity metrics, and\nranks among the top three on 17 of 32 challenge metrics."}
{"id": "2507.03482", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03482", "abs": "https://arxiv.org/abs/2507.03482", "authors": ["Pablo Alonso-Jiménez", "Pedro Ramoneda", "R. Oguz Araz", "Andrea Poltronieri", "Dmitry Bogdanov"], "title": "OMAR-RQ: Open Music Audio Representation Model Trained with Multi-Feature Masked Token Prediction", "comment": null, "summary": "Developing open-source foundation models is essential for advancing research\nin music audio understanding and ensuring access to powerful, multipurpose\nrepresentations for music information retrieval. We present OMAR-RQ, a model\ntrained with self-supervision via masked token classification methodologies\nusing a large-scale dataset with over 330,000 hours of music audio. We\nexperiment with different input features and quantization options, and achieve\nstate-of-the-art performance in music tagging, pitch estimation, chord\nrecognition, beat tracking, segmentation, and difficulty estimation among open\nself-supervised models. We open-source our training and evaluation pipelines\nand model weights, available at https://github.com/mtg/omar-rq."}
{"id": "2507.03523", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03523", "abs": "https://arxiv.org/abs/2507.03523", "authors": ["Dieter Coppens", "Adnan Shahid", "Eli De Poorter"], "title": "UWB TDoA Error Correction using Transformers: Patching and Positional Encoding Strategies", "comment": "13 pages, 10 figures, 7 tables", "summary": "Despite their high accuracy, UWB-based localization systems suffer\ninaccuracies when deployed in industrial locations with many obstacles due to\nmultipath effects and non-line-of-sight (NLOS) conditions. In such\nenvironments, current error mitigation approaches for time difference of\narrival (TDoA) localization typically exclude NLOS links. However, this\nexclusion approach leads to geometric dilution of precision problems and this\napproach is infeasible when the majority of links are NLOS. To address these\nlimitations, we propose a transformer-based TDoA position correction method\nthat uses raw channel impulse responses (CIRs) from all available anchor nodes\nto compute position corrections. We introduce different CIR ordering, patching\nand positional encoding strategies for the transformer, and analyze each\nproposed technique's scalability and performance gains. Based on experiments on\nreal-world UWB measurements, our approach can provide accuracies of up to 0.39\nm in a complex environment consisting of (almost) only NLOS signals, which is\nan improvement of 73.6 % compared to the TDoA baseline."}
{"id": "2507.04108", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04108", "abs": "https://arxiv.org/abs/2507.04108", "authors": ["Yhonatan Gayer", "Vladimir Tourbabin", "Zamir Ben-Hur", "David Alon", "Boaz Rafaely"], "title": "Ambisonics Encoder for Wearable Array with Improved Binaural Reproduction", "comment": "Published in Forum Acousticum 2025, 6 pages, 2 figures", "summary": "Ambisonics Signal Matching (ASM) is a recently proposed signal-independent\napproach to encoding Ambisonic signal from wearable microphone arrays, enabling\nefficient and standardized spatial sound reproduction. However, reproduction\naccuracy is currently limited due to the non-ideal layout of the microphones.\nThis research introduces an enhanced ASM encoder that reformulates the loss\nfunction by integrating a Binaural Signal Matching (BSM) term into the\noptimization framework. The aim of this reformulation is to improve the\naccuracy of binaural reproduction when integrating the Ambisonic signal with\nHead-Related Transfer Functions (HRTFs), making the encoded Ambisonic signal\nbetter suited for binaural reproduction. This paper first presents the\nmathematical formulation developed to align the ASM and BSM objectives in a\nsingle loss function, followed by a simulation study with a simulated\nmicrophone array mounted on a rigid sphere representing a head-mounted wearable\narray. The analysis shows that improved binaural reproduction with the encoded\nAmbisonic signal can be achieved using this joint ASM-BSM optimization, thereby\nenabling higher-quality binaural playback for virtual and augmented reality\napplications based on Ambisonics."}
{"id": "2507.03594", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03594", "abs": "https://arxiv.org/abs/2507.03594", "authors": ["Terry Yi Zhong", "Cristian Tejedor-Garcia", "Martha Larson", "Bastiaan R. Bloem"], "title": "RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification", "comment": "Accepted for TSD 2025", "summary": "Parkinson's Disease (PD) affects over 10 million people globally, with speech\nimpairments often preceding motor symptoms by years, making speech a valuable\nmodality for early, non-invasive detection. While recent deep-learning models\nachieve high accuracy, they typically lack the explainability required for\nclinical use. To address this, we propose RECA-PD, a novel, robust, and\nexplainable cross-attention architecture that combines interpretable speech\nfeatures with self-supervised representations. RECA-PD matches state-of-the-art\nperformance in Speech-based PD detection while providing explanations that are\nmore consistent and more clinically meaningful. Additionally, we demonstrate\nthat performance degradation in certain speech tasks (e.g., monologue) can be\nmitigated by segmenting long recordings. Our findings indicate that performance\nand explainability are not necessarily mutually exclusive. Future work will\nenhance the usability of explanations for non-experts and explore severity\nestimation to increase the real-world clinical relevance."}
{"id": "2507.03609", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03609", "abs": "https://arxiv.org/abs/2507.03609", "authors": ["Shiyong Chen", "Jia Guo", "Shengqian Han"], "title": "Implicit Neural Representation of Beamforming for Continuous Aperture Array (CAPA) System", "comment": "5 pages, 3 figures", "summary": "In this paper, a learning-based approach for optimizing downlink beamforming\nin continuous aperture array (CAPA) systems is proposed, where a MIMO scenario\nthat both the base station (BS) and the user are equipped with CAPA is\nconsidered. As the beamforming in the CAPA system is a function that maps a\ncoordinate on the aperture to the beamforming weight at the coordinate, a DNN\ncalled BeaINR is proposed to parameterize this function, which is called\nimplicit neural representation (INR). We further find that the optimal\nbeamforming function lies in the subspace of channel function, i.e., it can be\nexpressed as a weighted integral of channel function. Based on this finding, we\npropose another DNN called CoefINR to learn the weighting coefficient with INR,\nwhich has lower complexity than learning the beamforming function with BeaINR.\nSimulation results show that the proposed INR-based methods outperform\nnumerical baselines in both spectral efficiency (SE) and inference time, with\nCoefINR offering additional training efficiency."}
{"id": "2507.04264", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04264", "abs": "https://arxiv.org/abs/2507.04264", "authors": ["Kyeomeun Jang", "Jiaying Li", "Yinuo Wang"], "title": "The Overview of Segmental Durations Modification Algorithms on Speech Signal Characteristics", "comment": null, "summary": "This paper deeply evaluates and analyzes several mainstream algorithms that\ncan arbitrarily modify the duration of any portion of a given speech signal\nwithout changing the essential properties (e.g., pitch contour, power spectrum,\netc.) of the original signal. Arbitrary modification in this context means that\nthe duration of any region of the signal can be changed by specifying the\nstarting and ending time for modification or the target duration of the\nspecified interval, which can be either a fixed value of duration in the time\ndomain or a scaling factor of the original duration. In addition, arbitrary\nmodification also indicates any number of intervals can be modified at the same\ntime."}
{"id": "2507.03599", "categories": ["cs.SD", "cs.AI", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03599", "abs": "https://arxiv.org/abs/2507.03599", "authors": ["Roser Batlle-Roca", "Laura Ibáñez-Martínez", "Xavier Serra", "Emilia Gómez", "Martín Rocamora"], "title": "MusGO: A Community-Driven Framework For Assessing Openness in Music-Generative AI", "comment": "Accepted at ISMIR 2025", "summary": "Since 2023, generative AI has rapidly advanced in the music domain. Despite\nsignificant technological advancements, music-generative models raise critical\nethical challenges, including a lack of transparency and accountability, along\nwith risks such as the replication of artists' works, which highlights the\nimportance of fostering openness. With upcoming regulations such as the EU AI\nAct encouraging open models, many generative models are being released labelled\nas 'open'. However, the definition of an open model remains widely debated. In\nthis article, we adapt a recently proposed evidence-based framework for\nassessing openness in LLMs to the music domain. Using feedback from a survey of\n110 participants from the Music Information Retrieval (MIR) community, we\nrefine the framework into MusGO (Music-Generative Open AI), which comprises 13\nopenness categories: 8 essential and 5 desirable. We evaluate 16\nstate-of-the-art generative models and provide an openness leaderboard that is\nfully open to public scrutiny and community contributions. Through this work,\nwe aim to clarify the concept of openness in music-generative AI and promote\nits transparent and responsible development."}
{"id": "2507.03639", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03639", "abs": "https://arxiv.org/abs/2507.03639", "authors": ["Daniel D. Stancil"], "title": "Multipath-Enhanced Measurement of Antenna Patterns: Theory", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Traditional antenna pattern measurements involve minimizing the impact of\nmultipath propagation in the measurement environment. In contrast, this work\nintroduces a measurement approach that uses rather than mitigates multipath\npropagation. This is referred to as the Multipath-Enhanced Antenna Pattern\n(MEAP) Measurement technique. In this respect the approach has some kinship\nwith Multiple-Input Multiple-Output (MIMO) systems. The advantage in the case\nof MIMO systems is increased capacity; in the MEAP approach the advantage is\nelimination of the need for creating an anechoic environment. The approach uses\nmeasurements with reference antennas to calibrate the multipath channel matrix,\nand vector spherical harmonics for efficient pattern representation. After\npresenting the mathematical details of the method, numerical calculations\nillustrating the approach are presented. Experimental results are described in\na companion paper."}
{"id": "2507.04368", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04368", "abs": "https://arxiv.org/abs/2507.04368", "authors": ["Qiquan Zhang", "Moran Chen", "Zeyang Song", "Hexin Liu", "Xiangyu Zhang", "Haizhou Li"], "title": "Long-Context Modeling Networks for Monaural Speech Enhancement: A Comparative Study", "comment": "Accepted by WASPAA 2025, 5 pages", "summary": "Advanced long-context modeling backbone networks, such as Transformer,\nConformer, and Mamba, have demonstrated state-of-the-art performance in speech\nenhancement. However, a systematic and comprehensive comparative study of these\nbackbones within a unified speech enhancement framework remains lacking. In\naddition, xLSTM, a more recent and efficient variant of LSTM, has shown\npromising results in language modeling and as a general-purpose vision\nbackbone. In this paper, we investigate the capability of xLSTM in speech\nenhancement, and conduct a comprehensive comparison and analysis of the\nTransformer, Conformer, Mamba, and xLSTM backbones within a unified framework,\nconsidering both causal and noncausal configurations. Overall, xLSTM and Mamba\nachieve better performance than Transformer and Conformer. Mamba demonstrates\nsignificantly superior training and inference efficiency, particularly for long\nspeech inputs, whereas xLSTM suffers from the slowest processing speed."}
{"id": "2507.04048", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04048", "abs": "https://arxiv.org/abs/2507.04048", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Ye Gao"], "title": "CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning", "comment": "Accepted to Interspeech2025", "summary": "Speech Emotion Recognition (SER) is fundamental to affective computing and\nhuman-computer interaction, yet existing models struggle to generalize across\ndiverse acoustic conditions. While Contrastive Language-Audio Pretraining\n(CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for\ncapturing emotional cues, making it suboptimal for SER. To address this, we\npropose CLEP-DG, a framework that enhances CLAP's robustness in emotion\nrecognition. First, we fine-tune CLAP to obtain CLEP, adapting it on\nlarge-scale emotional speech datasets to better encode emotion-relevant\nfeatures. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a\ntext-driven augmentation strategy that optimizes learnable prompt vectors to\nmodel diverse acoustic environments without additional labeled audio. Finally,\nleveraging cross-modal transferability, we train a classifier on text-derived\nembeddings and apply it to the audio encoder during inference, mitigating\ndomain shifts between textual supervision and audio-based emotion recognition.\nExperiments across five benchmark datasets show that CLEP-DG outperforms prior\nCLAP-based approaches, achieving state-of-the-art performance in both\nsupervised and domain generalization settings."}
{"id": "2507.03647", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03647", "abs": "https://arxiv.org/abs/2507.03647", "authors": ["Daniel D. Stancil", "Alexander R. Allen"], "title": "Multipath-Enhanced Measurement of Antenna Patterns: Experiment", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In a companion paper we presented the theory for an antenna pattern measuring\ntechnique that uses (rather than mitigates) the properties of a multipath\nenvironment. Here we use measurements in a typical home garage to\nexperimentally demonstrate the feasibility of the technique. A half-wavelength\nelectric dipole with different orientations was used as both the calibration\nand test antennas. For simplicity, we limited the modeling of the antenna\npattern to using only the three $l=1$ vector spherical harmonics. Three methods\nwere used to analyze the measurements: a matrix inversion method using only 3\nsense antennas, a least-square-error technique, and a least-square-error\ntechnique with a constant power constraint imposed. The two least-square-error\ntechniques used the measurements from 10 sense antennas. The constrained\nleast-square-error technique was found to give the best results."}
{"id": "2507.04845", "categories": ["eess.AS", "cs.LG", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04845", "abs": "https://arxiv.org/abs/2507.04845", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "Spatial and Semantic Embedding Integration for Stereo Sound Event Localization and Detection in Regular Videos", "comment": null, "summary": "This report presents our systems submitted to the audio-only and audio-visual\ntracks of the DCASE2025 Task 3 Challenge: Stereo Sound Event Localization and\nDetection (SELD) in Regular Video Content. SELD is a complex task that combines\ntemporal event classification with spatial localization, requiring reasoning\nacross spatial, temporal, and semantic dimensions. The last is arguably the\nmost challenging to model. Traditional SELD architectures rely on multichannel\ninput, which limits their ability to leverage large-scale pre-training due to\ndata constraints. To address this, we enhance standard SELD architectures with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. Additionally, we incorporate\nautocorrelation-based acoustic features to improve distance estimation. We\npre-train our models on curated synthetic audio and audio-visual datasets and\napply a left-right channel swapping augmentation to further increase the\ntraining data. Both our audio-only and audio-visual systems substantially\noutperform the challenge baselines on the development set, demonstrating the\neffectiveness of our strategy. Performance is further improved through model\nensembling and a visual post-processing step based on human keypoints. Future\nwork will investigate the contribution of each modality and explore\narchitectural variants to further enhance results."}
{"id": "2507.04230", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04230", "abs": "https://arxiv.org/abs/2507.04230", "authors": ["Kun Fang", "Hanwen Zhang", "Ziyu Wang", "Ichiro Fujinaga"], "title": "High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics", "comment": null, "summary": "Piano sustain pedal detection has previously been approached as a binary\non/off classification task, limiting its application in real-world piano\nperformance scenarios where pedal depth significantly influences musical\nexpression. This paper presents a novel approach for high-resolution estimation\nthat predicts continuous pedal depth values. We introduce a Transformer-based\narchitecture that not only matches state-of-the-art performance on the\ntraditional binary classification task but also achieves high accuracy in\ncontinuous pedal depth estimation. Furthermore, by estimating continuous\nvalues, our model provides musically meaningful predictions for sustain pedal\nusage, whereas baseline models struggle to capture such nuanced expressions\nwith their binary detection approach. Additionally, this paper investigates the\ninfluence of room acoustics on sustain pedal estimation using a synthetic\ndataset that includes varied acoustic conditions. We train our model with\ndifferent combinations of room settings and test it in an unseen new\nenvironment using a \"leave-one-out\" approach. Our findings show that the two\nbaseline models and ours are not robust to unseen room conditions. Statistical\nanalysis further confirms that reverberation influences model predictions and\nintroduces an overestimation bias."}
{"id": "2507.03702", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03702", "abs": "https://arxiv.org/abs/2507.03702", "authors": ["Almutasem Bellah Enad", "Hadi Sarieddeen", "Jihad Fahs", "Hakim Jemaa", "Tareq Y. Al-Naffouri"], "title": "Performance Analysis of Data Detection in the THz-Band under Channel-Correlated Noise", "comment": null, "summary": "We present a comprehensive symbol error rate (SER) analysis framework for\nlink-level terahertz (THz)-band communication systems under linear zero-forcing\n(ZF) data detection. First, we derive the mismatched SER for indoor THz systems\nunder independent channel and noise assumptions, calculating the probability\ndensity function of the ratio of Gaussian noise to $\\alpha$-$\\mu$ channels\nresulting from ZF filtering. Next, we derive the precise SER under correlated\nchannel and noise conditions, modeling dependencies using the copula method.\nFinally, we evaluate the SER for THz channels with correlated distortion noise\nfrom hardware impairments. Simulations demonstrate that the proposed framework\ncorrects for multi-dB SERs resulting from the channel-noise independence\nassumption."}
{"id": "2507.04879", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.04879", "abs": "https://arxiv.org/abs/2507.04879", "authors": ["Riccardo Miccini", "Minje Kim", "Clément Laroche", "Luca Pezzarossa", "Paris Smaragdis"], "title": "Adaptive Slimming for Scalable and Efficient Speech Enhancement", "comment": "Accepted for publication at the 2025 IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA 2025)", "summary": "Speech enhancement (SE) enables robust speech recognition, real-time\ncommunication, hearing aids, and other applications where speech quality is\ncrucial. However, deploying such systems on resource-constrained devices\ninvolves choosing a static trade-off between performance and computational\nefficiency. In this paper, we introduce dynamic slimming to DEMUCS, a popular\nSE architecture, making it scalable and input-adaptive. Slimming lets the model\noperate at different utilization factors (UF), each corresponding to a\ndifferent performance/efficiency trade-off, effectively mimicking multiple\nmodel sizes without the extra storage costs. In addition, a router subnet,\ntrained end-to-end with the backbone, determines the optimal UF for the current\ninput. Thus, the system saves resources by adaptively selecting smaller UFs\nwhen additional complexity is unnecessary. We show that our solution is\nPareto-optimal against individual UFs, confirming the benefits of dynamic\nrouting. When training the proposed dynamically-slimmable model to use 10% of\nits capacity on average, we obtain the same or better speech quality as the\nequivalent static 25% utilization while reducing MACs by 29%."}
{"id": "2507.04349", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04349", "abs": "https://arxiv.org/abs/2507.04349", "authors": ["Jaeseok Jeong", "Yuna Lee", "Mingi Kwon", "Youngjung Uh"], "title": "TTS-CtrlNet: Time varying emotion aligned text-to-speech generation with ControlNet", "comment": null, "summary": "Recent advances in text-to-speech (TTS) have enabled natural speech\nsynthesis, but fine-grained, time-varying emotion control remains challenging.\nExisting methods often allow only utterance-level control and require full\nmodel fine-tuning with a large emotion speech dataset, which can degrade\nperformance. Inspired by adding conditional control to the existing model in\nControlNet (Zhang et al, 2023), we propose the first ControlNet-based approach\nfor controllable flow-matching TTS (TTS-CtrlNet), which freezes the original\nmodel and introduces a trainable copy of it to process additional conditions.\nWe show that TTS-CtrlNet can boost the pretrained large TTS model by adding\nintuitive, scalable, and time-varying emotion control while inheriting the\nability of the original model (e.g., zero-shot voice cloning & naturalness).\nFurthermore, we provide practical recipes for adding emotion control: 1)\noptimal architecture design choice with block analysis, 2) emotion-specific\nflow step, and 3) flexible control scale.\n  Experiments show that ours can effectively add an emotion controller to\nexisting TTS, and achieves state-of-the-art performance with emotion similarity\nscores: Emo-SIM and Aro-Val SIM. The project page is available at:\nhttps://curryjung.github.io/ttsctrlnet_project_page"}
{"id": "2507.03729", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03729", "abs": "https://arxiv.org/abs/2507.03729", "authors": ["Leila Marandi", "Khaled Humadi", "Gunes Karabulut Kurt", "Wessam Ajib", "Wei-Ping Zhu"], "title": "Improving SAGIN Resilience to Jamming with Reconfigurable Intelligent Surfaces", "comment": "Accepted at IEEE VTC-Fall 2025. 7 pages, 4 figures", "summary": "This study investigates the anti-jamming space-air-ground integrated network\n(SAGIN) scenario wherein a reconfigurable intelligent surface (RIS) is deployed\non a fixed Unmanned Aerial Vehicle (UAV) to counteract malevolent jamming\nattacks. In contrast to existing research, in this paper, we consider that a\nLow Earth Orbit (LEO) satellite is sending the signal to the user on the ground\nin the presence of jamming from a Geostationary Equatorial Orbit (GEO)\nsatellite side. We aim to maximize the signal-to-jamming plus noise ratio\n(SJNR) by optimizing the RIS beamforming and transmit power of the LEO\nsatellite. Assuming the availability of global channel state information (CSI)\nat the RIS, we propose alternating optimization (AO) and semidefinite\nrelaxation (SDR) techniques to address the complexity. Simulation results show\nthat the optimization schemes lead to considerable performance improvements.\nThe results also indicate that, given the high jamming power and the relatively\nsmall number of RIS elements, deploying the RIS on UAVs near the user is more\neffective in mitigating the impact of jamming interferers."}
{"id": "2507.05053", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05053", "abs": "https://arxiv.org/abs/2507.05053", "authors": ["Katarina C. Poole", "Julie Meyer", "Vincent Martin", "Rapolas Daugintis", "Nils Marggraf-Turley", "Jack Webb", "Ludovic Pirard", "Nicola La Magna", "Oliver Turvey", "Lorenzo Picinali"], "title": "The Extended SONICOM HRTF Dataset and Spatial Audio Metrics Toolbox", "comment": "For dataset:\n  https://www.axdesign.co.uk/tools-and-devices/sonicom-hrtf-dataset. For\n  toolbox: https://github.com/Katarina-Poole/Spatial-Audio-Metrics. Conference:\n  Forum Acusticum 2025", "summary": "Headphone-based spatial audio uses head-related transfer functions (HRTFs) to\nsimulate real-world acoustic environments. HRTFs are unique to everyone, due to\npersonal morphology, shaping how sound waves interact with the body before\nreaching the eardrums. Here we present the extended SONICOM HRTF dataset which\nexpands on the previous version released in 2023. The total number of measured\nsubjects has now been increased to 300, with demographic information for a\nsubset of the participants, providing context for the dataset's population and\nrelevance. The dataset incorporates synthesised HRTFs for 200 of the 300\nsubjects, generated using Mesh2HRTF, alongside pre-processed 3D scans of the\nhead and ears, optimised for HRTF synthesis. This rich dataset facilitates\nrapid and iterative optimisation of HRTF synthesis algorithms, allowing the\nautomatic generation of large data. The optimised scans enable seamless\nmorphological modifications, providing insights into how anatomical changes\nimpact HRTFs, and the larger sample size enhances the effectiveness of machine\nlearning approaches. To support analysis, we also introduce the Spatial Audio\nMetrics (SAM) Toolbox, a Python package designed for efficient analysis and\nvisualisation of HRTF data, offering customisable tools for advanced research.\nTogether, the extended dataset and toolbox offer a comprehensive resource for\nadvancing personalised spatial audio research and development."}
{"id": "2507.04419", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04419", "abs": "https://arxiv.org/abs/2507.04419", "authors": ["Ryan A. McCarthy", "You Zhang", "Samuel A. Verburg", "William F. Jenkins", "Peter Gerstoft"], "title": "Machine Learning in Acoustics: A Review and Open-Source Repository", "comment": "Accepted by npj Acoustics, 22 pages, 12 figures", "summary": "Acoustic data provide scientific and engineering insights in fields ranging\nfrom bioacoustics and communications to ocean and earth sciences. In this\nreview, we survey recent advances and the transformative potential of machine\nlearning (ML) in acoustics, including deep learning (DL). Using the Python\nhigh-level programming language, we demonstrate a broad collection of ML\ntechniques to detect and find patterns for classification, regression, and\ngeneration in acoustics data automatically. We have ML examples including\nacoustic data classification, generative modeling for spatial audio, and\nphysics-informed neural networks. This work includes AcousticsML, a set of\npractical Jupyter notebook examples on GitHub demonstrating ML benefits and\nencouraging researchers and practitioners to apply reproducible data-driven\napproaches to acoustic challenges."}
{"id": "2507.03814", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03814", "abs": "https://arxiv.org/abs/2507.03814", "authors": ["Rayan Salmi", "Guorui Lu", "Qinyu Chen"], "title": "SHAP-AAD: DeepSHAP-Guided Channel Reduction for EEG Auditory Attention Detection", "comment": "5 pages, conference", "summary": "Electroencephalography (EEG)-based auditory attention detection (AAD) offers\na non-invasive way to enhance hearing aids, but conventional methods rely on\ntoo many electrodes, limiting wearability and comfort. This paper presents\nSHAP-AAD, a two-stage framework that combines DeepSHAP-based channel selection\nwith a lightweight temporal convolutional network (TCN) for efficient AAD using\nfewer channels.DeepSHAP, an explainable AI technique, is applied to a\nConvolutional Neural Network (CNN) trained on topographic alpha-power maps to\nrank channel importance, and the top-k EEG channels are used to train a compact\nTCN. Experiments on the DTU dataset show that using 32 channels yields\ncomparable accuracy to the full 64-channel setup (79.21% vs. 81.06%) on\naverage. In some cases, even 8 channels can deliver satisfactory accuracy.\nThese results demonstrate the effectiveness of SHAP-AAD in reducing complexity\nwhile preserving high detection performance."}
{"id": "2507.02915", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02915", "abs": "https://arxiv.org/abs/2507.02915", "authors": ["Ludovic Tuncay", "Etienne Labbé", "Emmanouil Benetos", "Thomas Pellegrini"], "title": "Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning", "comment": null, "summary": "Building on the Joint-Embedding Predictive Architecture (JEPA) paradigm, a\nrecent self-supervised learning framework that predicts latent representations\nof masked regions in high-level feature spaces, we propose Audio-JEPA (Audio\nJoint-Embedding Predictive Architecture), tailored specifically for audio data.\nAudio-JEPA uses a simple Vision Transformer backbone to predict latent\nrepresentations of masked spectrogram patches rather than reconstructing raw\naudio. We pre-train on unlabeled AudioSet clips (10s, 32kHz) with random patch\nmasking on mel-spectrograms. We evaluate on the X-ARES suite covering speech,\nmusic, and environmental sound tasks. Although our implementation is a\nstraightforward translation of the original model to audio, the results still\nshow comparable performance to wav2vec 2.0 and data2vec while using less than\none-fifth of their training data and with no hyper-parameter tuning. All code\nand pretrained checkpoints will be released on GitHub."}
{"id": "2507.04554", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04554", "abs": "https://arxiv.org/abs/2507.04554", "authors": ["Nik Vaessen", "David A. van Leeuwen", "Roeland Ordelman"], "title": "Self-supervised learning of speech representations with Dutch archival data", "comment": "accepted at interspeech 2025", "summary": "This paper explores the use of Dutch archival television broadcast data for\nself-supervised learning of speech foundation models, specifically wav2vec 2.0.\nWe first study data quality assumptions for pre-training, and show how music,\nnoise and speaker overlap affect SSL convergence and downstream fine-tuning\nperformance. Secondly, we explore effectively pre-processing strategies to\nconvert the noisy broadcast dataset into a qualitative dataset for\npre-training, by using Whisper and WhisperX., Thirdly, we compare mono-lingual\nand multi-lingual pre-training with equivalent amounts of data, and show that\nmono-lingual pre-training is more robust to out-of-domain data. Lastly, we\nachieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a\ncontinuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k\nhour archival dataset."}
{"id": "2507.03856", "categories": ["eess.SP", "cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.03856", "abs": "https://arxiv.org/abs/2507.03856", "authors": ["Abiy Tasissa", "Waltenegus Dargie"], "title": "Robust Node Localization for Rough and Extreme Deployment Environments", "comment": "25 pages, 7 figures", "summary": "Many applications have been identified which require the deployment of\nlarge-scale low-power wireless sensor networks. Some of the deployment\nenvironments, however, impose harsh operation conditions due to intense\ncross-technology interference, extreme weather conditions (heavy rainfall,\nexcessive heat, etc.), or rough motion, thereby affecting the quality and\npredictability of the wireless links the nodes establish. In localization\ntasks, these conditions often lead to significant errors in estimating the\nposition of target nodes. Motivated by the practical deployments of sensors on\nthe surface of different water bodies, we address the problem of identifying\nsusceptible nodes and robustly estimating their positions. We formulate these\ntasks as a compressive sensing problem and propose algorithms for both node\nidentification and robust estimation. Additionally, we design an optimal anchor\nconfiguration to maximize the robustness of the position estimation task. Our\nnumerical results and comparisons with competitive methods demonstrate that the\nproposed algorithms achieve both objectives with a modest number of anchors.\nSince our method relies only on target-to-anchor distances, it is broadly\napplicable and yields resilient, robust localization."}
{"id": "2507.03251", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03251", "abs": "https://arxiv.org/abs/2507.03251", "authors": ["HyeYoung Lee", "Muhammad Nadeem"], "title": "Toward Efficient Speech Emotion Recognition via Spectral Learning and Attention", "comment": null, "summary": "Speech Emotion Recognition (SER) traditionally relies on auditory data\nanalysis for emotion classification. Several studies have adopted different\nmethods for SER. However, existing SER methods often struggle to capture subtle\nemotional variations and generalize across diverse datasets. In this article,\nwe use Mel-Frequency Cepstral Coefficients (MFCCs) as spectral features to\nbridge the gap between computational emotion processing and human auditory\nperception. To further improve robustness and feature diversity, we propose a\nnovel 1D-CNN-based SER framework that integrates data augmentation techniques.\nMFCC features extracted from the augmented data are processed using a 1D\nConvolutional Neural Network (CNN) architecture enhanced with channel and\nspatial attention mechanisms. These attention modules allow the model to\nhighlight key emotional patterns, enhancing its ability to capture subtle\nvariations in speech signals. The proposed method delivers cutting-edge\nperformance, achieving the accuracy of 97.49% for SAVEE, 99.23% for RAVDESS,\n89.31% for CREMA-D, 99.82% for TESS, 99.53% for EMO-DB, and 96.39% for EMOVO.\nExperimental results show new benchmarks in SER, demonstrating the\neffectiveness of our approach in recognizing emotional expressions with high\nprecision. Our evaluation demonstrates that the integration of advanced Deep\nLearning (DL) methods substantially enhances generalization across diverse\ndatasets, underscoring their potential to advance SER for real-world deployment\nin assistive technologies and human-computer interaction."}
{"id": "2507.04598", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04598", "abs": "https://arxiv.org/abs/2507.04598", "authors": ["Sho Inoue", "Kun Zhou", "Shuai Wang", "Haizhou Li"], "title": "Multi-Step Prediction and Control of Hierarchical Emotion Distribution in Text-to-Speech Synthesis", "comment": "Accepted to APSIPA Transactions on Signal and Information Processing", "summary": "We investigate hierarchical emotion distribution (ED) for achieving\nmulti-level quantitative control of emotion rendering in text-to-speech\nsynthesis (TTS). We introduce a novel multi-step hierarchical ED prediction\nmodule that quantifies emotion variance at the utterance, word, and phoneme\nlevels. By predicting emotion variance in a multi-step manner, we leverage\nglobal emotional context to refine local emotional variations, thereby\ncapturing the intrinsic hierarchical structure of speech emotion. Our approach\nis validated through its integration into a variance adaptor and an external\nmodule design compatible with various TTS systems. Both objective and\nsubjective evaluations demonstrate that the proposed framework significantly\nenhances emotional expressiveness and enables precise control of emotion\nrendering across multiple speech granularities."}
{"id": "2507.03858", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03858", "abs": "https://arxiv.org/abs/2507.03858", "authors": ["Can Zheng", "Chung G. Kang"], "title": "A Variational Bayesian Detector for Affine Frequency Division Multiplexing", "comment": "5 pages, 3 figures", "summary": "This paper proposes a variational Bayesian (VB) detector for affine frequency\ndivision multiplexing (AFDM) systems. The proposed method estimates the symbol\nprobability distribution by minimizing the Kullback-Leibler (KL) divergence\nbetween the true posterior and an approximate distribution, thereby enabling\nlow-complexity soft-decision detection. Compared to conventional approaches\nsuch as zero-forcing (ZF), Linear minimum mean square rrror (LMMSE), and the\nmessage passing algorithm (MPA), the proposed detector demonstrates lower bit\nerror rates (BER), faster convergence, and improved robustness under complex\nmultipath channels. Simulation results confirm its dual advantages in\ncomputational efficiency and detection performance."}
{"id": "2507.03377", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03377", "abs": "https://arxiv.org/abs/2507.03377", "authors": ["Masato Murata", "Koichi Miyazaki", "Tomoki Koriyama", "Tomoki Toda"], "title": "Eigenvoice Synthesis based on Model Editing for Speaker Generation", "comment": "Accepted by INTERSPEECH 2025", "summary": "Speaker generation task aims to create unseen speaker voice without reference\nspeech. The key to the task is defining a speaker space that represents diverse\nspeakers to determine the generated speaker trait. However, the effective way\nto define this speaker space remains unclear. Eigenvoice synthesis is one of\nthe promising approaches in the traditional parametric synthesis framework,\nsuch as HMM-based methods, which define a low-dimensional speaker space using\npre-stored speaker features. This study proposes a novel DNN-based eigenvoice\nsynthesis method via model editing. Unlike prior methods, our method defines a\nspeaker space in the DNN model parameter space. By directly sampling new DNN\nmodel parameters in this space, we can create diverse speaker voices.\nExperimental results showed the capability of our method to generate diverse\nspeakers' speech. Moreover, we discovered a gender-dominant axis in the created\nspeaker space, indicating the potential to control speaker attributes."}
{"id": "2507.04776", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04776", "abs": "https://arxiv.org/abs/2507.04776", "authors": ["Jun-You Wang", "Li Su"], "title": "Improving BERT for Symbolic Music Understanding Using Token Denoising and Pianoroll Prediction", "comment": "Accepted at ISMIR 2025", "summary": "We propose a pre-trained BERT-like model for symbolic music understanding\nthat achieves competitive performance across a wide range of downstream tasks.\nTo achieve this target, we design two novel pre-training objectives, namely\ntoken correction and pianoroll prediction. First, we sample a portion of note\ntokens and corrupt them with a limited amount of noise, and then train the\nmodel to denoise the corrupted tokens; second, we also train the model to\npredict bar-level and local pianoroll-derived representations from the\ncorrupted note tokens. We argue that these objectives guide the model to better\nlearn specific musical knowledge such as pitch intervals. For evaluation, we\npropose a benchmark that incorporates 12 downstream tasks ranging from chord\nestimation to symbolic genre classification. Results confirm the effectiveness\nof the proposed pre-training objectives on downstream tasks."}
{"id": "2507.03951", "categories": ["eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.03951", "abs": "https://arxiv.org/abs/2507.03951", "authors": ["Amnon Balanov", "Alon Zabatani", "Tamir Bendory"], "title": "Structure from Noise: Confirmation Bias in Particle Picking in Structural Biology", "comment": null, "summary": "Confirmation bias is a fundamental challenge in cryo-electron microscopy\n(cryo-EM) and cryo-electron tomography (cryo-ET), where prior expectations can\nlead to systematic errors in data interpretation. This bias may emerge at\nmultiple stages of the reconstruction pipeline, and in particular in the\ncritical particle picking stage, where 2D particles (in cryo-EM) or 3D\nsubtomograms (in cryo-ET) are extracted from highly noisy micrographs or\ntomograms. Focusing on two widely used methodologies, template matching and\ndeep neural networks, we combine theoretical analysis with controlled\nexperiments to demonstrate that both methods, when applied to pure noise, can\nproduce persistent molecular structures, a phenomenon we term structure from\nnoise. This artifact highlights a critical vulnerability in current workflows:\nthe potential for particle-picking algorithms to inject strong prior-driven\nbias into downstream analyses. We then propose practical mitigation strategies\nto reduce the impact of such biases. Together, our findings deepen the\ntheoretical understanding of confirmation bias in cryo-EM and cryo-ET and call\nfor cautious interpretation of reconstructions, primarily when relying on\ntemplate-driven particle picking."}
{"id": "2507.03382", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03382", "abs": "https://arxiv.org/abs/2507.03382", "authors": ["Masato Murata", "Koichi Miyazaki", "Tomoki Koriyama"], "title": "Speaker-agnostic Emotion Vector for Cross-speaker Emotion Intensity Control", "comment": "Accepted by INTERSPEECH 2025", "summary": "Cross-speaker emotion intensity control aims to generate emotional speech of\na target speaker with desired emotion intensities using only their neutral\nspeech. A recently proposed method, emotion arithmetic, achieves emotion\nintensity control using a single-speaker emotion vector. Although this prior\nmethod has shown promising results in the same-speaker setting, it lost speaker\nconsistency in the cross-speaker setting due to mismatches between the emotion\nvector of the source and target speakers. To overcome this limitation, we\npropose a speaker-agnostic emotion vector designed to capture shared emotional\nexpressions across multiple speakers. This speaker-agnostic emotion vector is\napplicable to arbitrary speakers. Experimental results demonstrate that the\nproposed method succeeds in cross-speaker emotion intensity control while\nmaintaining speaker consistency, speech quality, and controllability, even in\nthe unseen speaker case."}
{"id": "2507.04817", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04817", "abs": "https://arxiv.org/abs/2507.04817", "authors": ["Mathilde Abrassart", "Nicolas Obin", "Axel Roebel"], "title": "Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and Duration Parameters", "comment": "8 pages, 4 figures", "summary": "Precise control over speech characteristics, such as pitch, duration, and\nspeech rate, remains a significant challenge in the field of voice conversion.\nThe ability to manipulate parameters like pitch and syllable rate is an\nimportant element for effective identity conversion, but can also be used\nindependently for voice transformation, achieving goals that were historically\naddressed by vocoder-based methods.\n  In this work, we explore a convolutional neural network-based approach that\naims to provide means for modifying fundamental frequency (F0), phoneme\nsequences, intensity, and speaker identity. Rather than relying on\ndisentanglement techniques, our model is explicitly conditioned on these\nfactors to generate mel spectrograms, which are then converted into waveforms\nusing a universal neural vocoder. Accordingly, during inference, F0 contours,\nphoneme sequences, and speaker embeddings can be freely adjusted, allowing for\nintuitively controlled voice transformations.\n  We evaluate our approach on speaker conversion and expressive speech tasks\nusing both perceptual and objective metrics. The results suggest that the\nproposed method offers substantial flexibility, while maintaining high\nintelligibility and speaker similarity."}
{"id": "2507.03959", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03959", "abs": "https://arxiv.org/abs/2507.03959", "authors": ["Tim Brühl", "Jenny Glönkler", "Robin Schwager", "Tin Stribor Sohn", "Tim Dieter Eberhardt", "Sören Hohmann"], "title": "SAFERad: A Framework to Enable Radar Data for Safety-Relevant Perception Tasks", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Radar sensors play a crucial role for perception systems in automated driving\nbut suffer from a high level of noise. In the past, this could be solved by\nstrict filters, which remove most false positives at the expense of undetected\nobjects. Future highly automated functions are much more demanding with respect\nto error rate. Hence, if the radar sensor serves as a component of perception\nsystems for such functions, a simple filter strategy cannot be applied. In this\npaper, we present a modified filtering approach which is characterized by the\nidea to vary the filtering depending on the potential of harmful collision with\nthe object which is potentially represented by the radar point. We propose an\nalgorithm which determines a criticality score for each point based on the\nplanned or presumable trajectory of the automated vehicle. Points identified as\nvery critical can trigger manifold actions to confirm or deny object presence.\nOur pipeline introduces criticality regions. The filter threshold in these\ncriticality regions is omitted. Commonly known radar data sets do not or barely\nfeature critical scenes. Thus, we present an approach to evaluate our framework\nby adapting the planned trajectory towards vulnerable road users, which serve\nas ground truth critical points. Evaluation of the criticality metric prove\nhigh recall rates. Besides, our post-processing algorithm lowers the rate of\nnon-clustered critical points by 74.8 % in an exemplary setup compared to a\nmoderate, generic filter."}
{"id": "2507.03395", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03395", "abs": "https://arxiv.org/abs/2507.03395", "authors": ["Luca A. Lanzendörfer", "Florian Grötschla", "Karim Galal", "Roger Wattenhofer"], "title": "MaskBeat: Loopable Drum Beat Generation", "comment": "Extended Abstract ISMIR 2025", "summary": "We present MaskBeat, a transformer-based approach for loopable drum pattern\ngeneration. Rather than predicting drum hits sequentially, our method uses\nbidirectional attention with iterative refinement, allowing instruments to be\ngenerated in parallel while maintaining musical coherence. Additionally, we\nintroduce custom loss functions that capture drum-specific musical\nrelationships. Our experiments show that MaskBeat generates higher quality and\nmore musically coherent drum patterns than baseline approaches."}
{"id": "2507.04858", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04858", "abs": "https://arxiv.org/abs/2507.04858", "authors": ["António Sá Pinto"], "title": "Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for Maracatu", "comment": "Accepted at ISMIR 2025", "summary": "We explore transfer learning strategies for musical onset detection in the\nAfro-Brazilian Maracatu tradition, which features complex rhythmic patterns\nthat challenge conventional models. We adapt two Temporal Convolutional Network\narchitectures: one pre-trained for onset detection (intra-task) and another for\nbeat tracking (inter-task). Using only 5-second annotated snippets per\ninstrument, we fine-tune these models through layer-wise retraining strategies\nfor five traditional percussion instruments. Our results demonstrate\nsignificant improvements over baseline performance, with F1 scores reaching up\nto 0.998 in the intra-task setting and improvements of over 50 percentage\npoints in best-case scenarios. The cross-task adaptation proves particularly\neffective for time-keeping instruments, where onsets naturally align with beat\npositions. The optimal fine-tuning configuration varies by instrument,\nhighlighting the importance of instrument-specific adaptation strategies. This\napproach addresses the challenges of underrepresented musical traditions,\noffering an efficient human-in-the-loop methodology that minimizes annotation\neffort while maximizing performance. Our findings contribute to more inclusive\nmusic information retrieval tools applicable beyond Western musical contexts."}
{"id": "2507.03977", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03977", "abs": "https://arxiv.org/abs/2507.03977", "authors": ["Hanqi Wang", "Yang Liu", "Peng Ye", "Liang Song"], "title": "MMOC: Self-Supervised EEG Emotion Recognition Framework with Multi-Model Online Collaboration", "comment": null, "summary": "Electroencephalography (EEG) emotion recognition plays a crucial role in\nhuman-computer interaction, particularly in healthcare and neuroscience. While\nsupervised learning has been widely used, its reliance on manual annotations\nintroduces high costs and potential bias. Self-supervised learning (SSL) offers\na promising alternative by generating labels through pretext tasks. However,\nhigh inter-subject variability in EEG signals leads to significant data drift,\nlimiting self-supervised models' generalization across unseen subjects.\nTraditional domain adaptation (DA) methods require access to target-domain data\nduring training. Although domain generalization (DG) avoids this constraint, it\noften falls short in handling complex data drift due to limited coverage of\npossible target distributions. To tackle these challenges, we propose MMOC, a\nself-supervised framework with multi-model online collaboration (MMOC), to\nachieve online adaptation to unseen data. MMOC trains multiple base models\nusing diverse strategies rooted in reconstruction and contrastive learning,\nenabling each model to develop distinct generalization capabilities. During\ninference, MMOC dynamically activates the most suitable model for each test\nsample via a loss-based routing mechanism that evaluates both contrastive and\nreconstruction losses. This dual consideration allows for a comprehensive\nmeasurement of data drift at both structural and semantic levels. Experimental\nresults on the SEED and Dreamer datasets show that MMOC achieves\nstate-of-the-art performance: 85.39% on SEED, and 68.77% and 69.37% on Dreamer\narousal and valence dimensions, respectively. MMOC effectively mitigates\ninter-subject data drift, offering a practical solution for real-world EEG\nemotion recognition."}
{"id": "2507.03466", "categories": ["cs.SD", "cs.SY", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.03466", "abs": "https://arxiv.org/abs/2507.03466", "authors": ["Mahdi Ali Pour", "Utku Gunay Acer"], "title": "Direction Estimation of Sound Sources Using Microphone Arrays and Signal Strength", "comment": "5 pages", "summary": "Sound-tracking refers to the process of determining the direction from which\na sound originates, making it a fundamental component of sound source\nlocalization. This capability is essential in a variety of applications,\nincluding security systems, acoustic monitoring, and speaker tracking, where\naccurately identifying the direction of a sound source enables real-time\nresponses, efficient resource allocation, and improved situational awareness.\nWhile sound-tracking is closely related to localization, it specifically\nfocuses on identifying the direction of the sound source rather than estimating\nits exact position in space. Despite its utility, sound-tracking systems face\nseveral challenges, such as maintaining directional accuracy and precision,\nalong with the need for sophisticated hardware configurations and complex\nsignal processing algorithms. This paper presents a sound-tracking method using\nthree electret microphones. We estimate the direction of a sound source using a\nlightweight method that analyzes signals from three strategically placed\nmicrophones. By comparing the average power of the received signals, the system\ninfers the most probable direction of the sound. The results indicate that the\npower level from each microphone effectively determines the sound source\ndirection. Our system employs a straightforward and cost-effective hardware\ndesign, ensuring simplicity and affordability in implementation. It achieves a\nlocalization error of less than 6 degrees and a precision of 98%. Additionally,\nits effortless integration with various systems makes it versatile and\nadaptable. Consequently, this technique presents a robust and reliable solution\nfor sound-tracking and localization, with potential applications spanning\ndiverse domains such as security systems, smart homes, and acoustic monitoring."}
{"id": "2507.04864", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04864", "abs": "https://arxiv.org/abs/2507.04864", "authors": ["Alexander Fichtinger", "Jan Schlüter", "Gerhard Widmer"], "title": "Music Boomerang: Reusing Diffusion Models for Data Augmentation and Audio Manipulation", "comment": "Accepted at SMC 2025. Code at https://malex1106.github.io/boomify/", "summary": "Generative models of music audio are typically used to generate output based\nsolely on a text prompt or melody. Boomerang sampling, recently proposed for\nthe image domain, allows generating output close to an existing example, using\nany pretrained diffusion model. In this work, we explore its application in the\naudio domain as a tool for data augmentation or content manipulation.\nSpecifically, implementing Boomerang sampling for Stable Audio Open, we augment\ntraining data for a state-of-the-art beat tracker, and attempt to replace\nmusical instruments in recordings. Our results show that the rhythmic structure\nof existing examples is mostly preserved, that it improves performance of the\nbeat tracker, but only in scenarios of limited training data, and that it can\naccomplish text-based instrument replacement on monophonic inputs. We publish\nour implementation to invite experiments on data augmentation in other tasks\nand explore further applications."}
{"id": "2507.03987", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03987", "abs": "https://arxiv.org/abs/2507.03987", "authors": ["Penggao Yan", "Baoshan Song", "Xiao Xia", "Weisong Wen", "Li-Ta Hsu"], "title": "An Efficient Detector for Faulty GNSS Measurements Detection With Non-Gaussian Noises", "comment": "Submitted to NAVIGATION, Journal of the Institute of Navigation", "summary": "Fault detection is crucial to ensure the reliability of navigation systems.\nHowever, mainstream fault detection methods are developed based on Gaussian\nassumptions on nominal errors, while current attempts at non-Gaussian fault\ndetection are either heuristic or lack rigorous statistical properties. The\nperformance and reliability of these methods are challenged in real-world\napplications. This paper proposes the jackknife detector, a fault detection\nmethod tailored for linearized pseudorange-based positioning systems under\nnon-Gaussian nominal errors. Specifically, by leveraging the jackknife\ntechnique, a test statistic is derived as a linear combination of measurement\nerrors, eliminating the need for restrictive distributional assumptions while\nmaintaining computational efficiency. A hypothesis test with the Bonferroni\ncorrection is then constructed to detect potential faults in measurements.\nTheoretical analysis proves the equivalence between the jackknife detector and\nthe solution separation (SS) detector, while revealing the former's superior\ncomputational efficiency. Through a worldwide simulation and a real-world\nsatellite clock anomaly detection experiment--both involving non-Gaussian\nnominal errors--the proposed jackknife detector demonstrates equivalent\ndetection performance to the SS detector but achieves a fourfold improvement in\ncomputational efficiency. These results highlight the jackknife detector's\nsubstantial potential for real-time applications requiring robust and efficient\nfault detection in non-Gaussian noise environments."}
{"id": "2507.03468", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03468", "abs": "https://arxiv.org/abs/2507.03468", "authors": ["Hieu-Thi Luong", "Inbal Rimons", "Haim Permuter", "Kong Aik Lee", "Eng Siong Chng"], "title": "Robust Localization of Partially Fake Speech: Metrics, Models, and Out-of-Domain Evaluation", "comment": "Submitted to APSIPA 2025", "summary": "Partial audio deepfake localization pose unique challenges and remain\nunderexplored compared to full-utterance spoofing detection. While recent\nmethods report strong in-domain performance, their real-world utility remains\nunclear. In this analysis, we critically examine the limitations of current\nevaluation practices, particularly the widespread use of Equal Error Rate\n(EER), which often obscures generalization and deployment readiness. We propose\nreframing the localization task as a sequential anomaly detection problem and\nadvocate for the use of threshold-dependent metrics such as accuracy,\nprecision, recall, and F1-score, which better reflect real-world behavior.\nSpecifically, we analyze the performance of the open-source Coarse-to-Fine\nProposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on\nthe in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the\nLlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our\nreproduced version of the same model performs worse on in-domain data (9.84%)\nbut better on the out-of-domain sets (41.72% and 14.98%, respectively). This\nhighlights the risks of over-optimizing for in-domain EER, which can lead to\nmodels that perform poorly in real-world scenarios. It also suggests that while\ndeep learning models can be effective on in-domain data, they generalize poorly\nto out-of-domain scenarios, failing to detect novel synthetic samples and\nmisclassifying unfamiliar bona fide audio. Finally, we observe that adding more\nbona fide or fully synthetic utterances to the training data often degrades\nperformance, whereas adding partially fake utterances improves it."}
{"id": "2507.04955", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04955", "abs": "https://arxiv.org/abs/2507.04955", "authors": ["Fathinah Izzati", "Xinyue Li", "Gus Xia"], "title": "EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation", "comment": null, "summary": "We propose Expotion (Facial Expression and Motion Control for Multimodal\nMusic Generation), a generative model leveraging multimodal visual controls -\nspecifically, human facial expressions and upper-body motion - as well as text\nprompts to produce expressive and temporally accurate music. We adopt\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\ngeneration model, enabling fine-grained adaptation to the multimodal controls\nusing a small dataset. To ensure precise synchronization between video and\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\nExperiments demonstrate that integrating visual features alongside textual\ndescriptions enhances the overall quality of generated music in terms of\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\nvideo, and text adherence, surpassing both proposed baselines and existing\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\nexpressive facial and upper-body gestures aligned with corresponding music,\nproviding significant potential for future research in multimodal and\ninteractive music generation."}
{"id": "2507.04021", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04021", "abs": "https://arxiv.org/abs/2507.04021", "authors": ["Niklas Vaara", "Pekka Sangi", "Miguel Bordallo López", "Janne Heikkilä"], "title": "Differentiable High-Performance Ray Tracing-Based Simulation of Radio Propagation with Point Clouds", "comment": null, "summary": "Ray tracing is a widely used deterministic method for radio propagation\nsimulations, capable of producing physically accurate multipath components. The\naccuracy depends on the quality of the environment model and its\nelectromagnetic properties. Recent advances in computer vision and machine\nlearning have made it possible to reconstruct detailed environment models\naugmented with semantic segmentation labels.\n  In this letter, we propose a differentiable ray tracing-based radio\npropagation simulator that operates directly on point clouds. We showcase the\nefficiency of our method by simulating multi-bounce propagation paths with up\nto five interactions with specular reflections and diffuse scattering in two\nindoor scenarios, each completing in less than 90 ms. Lastly, we demonstrate\nhow the differentiability of electromagnetic computations can be combined with\nsegmentation labels to learn the electromagnetic properties of the environment."}
{"id": "2507.03482", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03482", "abs": "https://arxiv.org/abs/2507.03482", "authors": ["Pablo Alonso-Jiménez", "Pedro Ramoneda", "R. Oguz Araz", "Andrea Poltronieri", "Dmitry Bogdanov"], "title": "OMAR-RQ: Open Music Audio Representation Model Trained with Multi-Feature Masked Token Prediction", "comment": null, "summary": "Developing open-source foundation models is essential for advancing research\nin music audio understanding and ensuring access to powerful, multipurpose\nrepresentations for music information retrieval. We present OMAR-RQ, a model\ntrained with self-supervision via masked token classification methodologies\nusing a large-scale dataset with over 330,000 hours of music audio. We\nexperiment with different input features and quantization options, and achieve\nstate-of-the-art performance in music tagging, pitch estimation, chord\nrecognition, beat tracking, segmentation, and difficulty estimation among open\nself-supervised models. We open-source our training and evaluation pipelines\nand model weights, available at https://github.com/mtg/omar-rq."}
{"id": "2507.04963", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04963", "abs": "https://arxiv.org/abs/2507.04963", "authors": ["Šimon Libřický", "Jan Hajič jr"], "title": "Modeling the Difficulty of Saxophone Music", "comment": null, "summary": "In learning music, difficulty is an important factor in choice of repertoire,\nchoice of tempo, and structure of practice. These choices are typically done\nwith the guidance of a teacher; however, not all learners have access to one.\nWhile piano and strings have had some attention devoted to automated difficulty\nestimation, wind instruments have so far been under-served. In this paper, we\npropose a method for estimating the difficulty of pieces for winds and\nimplement it for the tenor saxophone. We take the cost-of-traversal approach,\nmodelling the part as a sequence of transitions -- note pairs. We estimate\ntransition costs from newly collected recordings of trill speeds, comparing\nrepresentations of saxophone fingerings at various levels of expert input. We\nthen compute and visualise the cost of the optimal path through the part, at a\ngiven tempo. While we present this model for the tenor saxophone, the same\npipeline can be applied to other woodwinds, and our experiments show that with\nappropriate feature design, only a small proportion of possible trills is\nneeded to estimate the costs well. Thus, we present a practical way of\ndiversifying the capabilities of MIR in music education to the wind family of\ninstruments."}
{"id": "2507.04040", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04040", "abs": "https://arxiv.org/abs/2507.04040", "authors": ["Zihang Song", "Qihao Peng", "Pei Xiao", "Bipin Rajendran", "Osvaldo Simeone"], "title": "CSI-Free Symbol Detection for Atomic MIMO Receivers via In-Context Learning", "comment": null, "summary": "Atomic receivers based on Rydberg vapor cells as sensors of electromagnetic\nfields offer a promising alternative to conventional radio frequency\nfront-ends. In multi-antenna configurations, the magnitude-only,\nphase-insensitive measurements produced by atomic receivers pose challenges for\ntraditional detection methods. Existing solutions rely on two-step iterative\noptimization processes, which suffer from cascaded channel estimation errors\nand high computational complexity. We propose a channel state information\n(CSI)-free symbol detection method based on in-context learning (ICL), which\ndirectly maps pilot-response pairs to data symbol predictions without explicit\nchannel estimation. Simulation results show that ICL achieves competitive\naccuracy with {higher computational efficiency} compared to existing solutions."}
{"id": "2507.03594", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03594", "abs": "https://arxiv.org/abs/2507.03594", "authors": ["Terry Yi Zhong", "Cristian Tejedor-Garcia", "Martha Larson", "Bastiaan R. Bloem"], "title": "RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification", "comment": "Accepted for TSD 2025", "summary": "Parkinson's Disease (PD) affects over 10 million people globally, with speech\nimpairments often preceding motor symptoms by years, making speech a valuable\nmodality for early, non-invasive detection. While recent deep-learning models\nachieve high accuracy, they typically lack the explainability required for\nclinical use. To address this, we propose RECA-PD, a novel, robust, and\nexplainable cross-attention architecture that combines interpretable speech\nfeatures with self-supervised representations. RECA-PD matches state-of-the-art\nperformance in Speech-based PD detection while providing explanations that are\nmore consistent and more clinically meaningful. Additionally, we demonstrate\nthat performance degradation in certain speech tasks (e.g., monologue) can be\nmitigated by segmenting long recordings. Our findings indicate that performance\nand explainability are not necessarily mutually exclusive. Future work will\nenhance the usability of explanations for non-experts and explore severity\nestimation to increase the real-world clinical relevance."}
{"id": "2507.04966", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04966", "abs": "https://arxiv.org/abs/2507.04966", "authors": ["Sandipan Dhar", "Mayank Gupta", "Preeti Rao"], "title": "LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning", "comment": "10 pages, 5 figures, 3 Tables", "summary": "The field of Singing Voice Synthesis (SVS) has seen significant advancements\nin recent years due to the rapid progress of diffusion-based approaches.\nHowever, capturing vocal style, genre-specific pitch inflections, and\nlanguage-dependent characteristics remains challenging, particularly in\nlow-resource scenarios. To address this, we propose LAPS-Diff, a diffusion\nmodel integrated with language-aware embeddings and a vocal-style guided\nlearning mechanism, specifically designed for Bollywood Hindi singing style. We\ncurate a Hindi SVS dataset and leverage pre-trained language models to extract\nword and phone-level embeddings for an enriched lyrics representation.\nAdditionally, we incorporated a style encoder and a pitch extraction model to\ncompute style and pitch losses, capturing features essential to the naturalness\nand expressiveness of the synthesized singing, particularly in terms of vocal\nstyle and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec\nmodels to extract musical and contextual embeddings, serving as conditional\npriors to refine the acoustic feature generation process further. Based on\nobjective and subjective evaluations, we demonstrate that LAPS-Diff\nsignificantly improves the quality of the generated samples compared to the\nconsidered state-of-the-art (SOTA) model for our constrained dataset that is\ntypical of the low resource scenario."}
{"id": "2507.04178", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04178", "abs": "https://arxiv.org/abs/2507.04178", "authors": ["Idban Alamzadeh", "Michael Inman", "Mohammadreza F. Imani"], "title": "Experimental Demonstration of Computational AoA Detection Using Conformal Frequency Diverse Metasurface Antennas", "comment": "5 pages, 6 figures, IEEE AWPL", "summary": "Devices that detect angle-of-arrival (AoA) over a wide field of view are\ncrucial for various applications such as wireless communication and navigation.\nThey are often installed on platforms with challenging mechanical and stealth\nconstraints like vehicles, drones, and helmets, where traditional methods --\nmechanically rotating antennas or conformal arrays -- tend to be bulky, heavy,\nand costly. A recent work has proposed a conformal frequency diverse antenna\nthat is designed to produce angularly diverse patterns that encode angular\ninformation into frequency sweeps. This capability allows AoA to be determined\nacross the entire horizon using only two receiving units. This paper\nexperimentally validates this concept, detailing the prototyping process and\npractical design considerations. The AoA detection capabilities of the proposed\ndevice are confirmed through experimental demonstrations. The proposed\nconformal metasurfaces offer an alternative hardware solution for sensing over\nlarge fields of view, with potential applications in radar sensing, situational\nawareness, and navigation."}
{"id": "2507.03599", "categories": ["cs.SD", "cs.AI", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.03599", "abs": "https://arxiv.org/abs/2507.03599", "authors": ["Roser Batlle-Roca", "Laura Ibáñez-Martínez", "Xavier Serra", "Emilia Gómez", "Martín Rocamora"], "title": "MusGO: A Community-Driven Framework For Assessing Openness in Music-Generative AI", "comment": "Accepted at ISMIR 2025", "summary": "Since 2023, generative AI has rapidly advanced in the music domain. Despite\nsignificant technological advancements, music-generative models raise critical\nethical challenges, including a lack of transparency and accountability, along\nwith risks such as the replication of artists' works, which highlights the\nimportance of fostering openness. With upcoming regulations such as the EU AI\nAct encouraging open models, many generative models are being released labelled\nas 'open'. However, the definition of an open model remains widely debated. In\nthis article, we adapt a recently proposed evidence-based framework for\nassessing openness in LLMs to the music domain. Using feedback from a survey of\n110 participants from the Music Information Retrieval (MIR) community, we\nrefine the framework into MusGO (Music-Generative Open AI), which comprises 13\nopenness categories: 8 essential and 5 desirable. We evaluate 16\nstate-of-the-art generative models and provide an openness leaderboard that is\nfully open to public scrutiny and community contributions. Through this work,\nwe aim to clarify the concept of openness in music-generative AI and promote\nits transparent and responsible development."}
{"id": "2507.03912", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.03912", "abs": "https://arxiv.org/abs/2507.03912", "authors": ["Tomoki Koriyama"], "title": "Prosody Labeling with Phoneme-BERT and Speech Foundation Models", "comment": "Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "This paper proposes a model for automatic prosodic label annotation, where\nthe predicted labels can be used for training a prosody-controllable\ntext-to-speech model. The proposed model utilizes not only rich acoustic\nfeatures extracted by a self-supervised-learning (SSL)-based model or a Whisper\nencoder, but also linguistic features obtained from phoneme-input pretrained\nlinguistic foundation models such as PnG BERT and PL-BERT. The concatenation of\nacoustic and linguistic features is used to predict phoneme-level prosodic\nlabels. In the experimental evaluation on Japanese prosodic labels, including\npitch accents and phrase break indices, it was observed that the combination of\nboth speech and linguistic foundation models enhanced the prediction accuracy\ncompared to using either a speech or linguistic input alone. Specifically, we\nachieved 89.8% prediction accuracy in accent labels, 93.2% in high-low pitch\naccents, and 94.3% in break indices."}
{"id": "2507.04195", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04195", "abs": "https://arxiv.org/abs/2507.04195", "authors": ["Ziyang Lu", "M. Cenk Gursoy", "Chilukuri K. Mohan", "Pramod K. Varshney"], "title": "Adaptive Resource Management in Cognitive Radar via Deep Deterministic Policy Gradient", "comment": null, "summary": "In this paper, scanning for target detection, and multi-target tracking in a\ncognitive radar system are considered, and adaptive radar resource management\nis investigated. In particular, time management for radar scanning and tracking\nof multiple maneuvering targets subject to budget constraints is studied with\nthe goal to jointly maximize the tracking and scanning performances of a\ncognitive radar. We tackle the constrained optimization problem of allocating\nthe dwell time to track individual targets by employing a deep deterministic\npolicy gradient (DDPG) based reinforcement learning approach. We propose a\nconstrained deep reinforcement learning (CDRL) algorithm that updates the DDPG\nneural networks and dual variables simultaneously. Numerical results show that\nthe radar can autonomously allocate time appropriately so as to maximize the\nreward function without exceeding the time constraint."}
{"id": "2507.04048", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04048", "abs": "https://arxiv.org/abs/2507.04048", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Ye Gao"], "title": "CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning", "comment": "Accepted to Interspeech2025", "summary": "Speech Emotion Recognition (SER) is fundamental to affective computing and\nhuman-computer interaction, yet existing models struggle to generalize across\ndiverse acoustic conditions. While Contrastive Language-Audio Pretraining\n(CLAP) provides strong multimodal alignment, it lacks dedicated mechanisms for\ncapturing emotional cues, making it suboptimal for SER. To address this, we\npropose CLEP-DG, a framework that enhances CLAP's robustness in emotion\nrecognition. First, we fine-tune CLAP to obtain CLEP, adapting it on\nlarge-scale emotional speech datasets to better encode emotion-relevant\nfeatures. Then, we introduce Acoustic Context Prompt Tuning (ACPT), a\ntext-driven augmentation strategy that optimizes learnable prompt vectors to\nmodel diverse acoustic environments without additional labeled audio. Finally,\nleveraging cross-modal transferability, we train a classifier on text-derived\nembeddings and apply it to the audio encoder during inference, mitigating\ndomain shifts between textual supervision and audio-based emotion recognition.\nExperiments across five benchmark datasets show that CLEP-DG outperforms prior\nCLAP-based approaches, achieving state-of-the-art performance in both\nsupervised and domain generalization settings."}
{"id": "2507.04879", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.04879", "abs": "https://arxiv.org/abs/2507.04879", "authors": ["Riccardo Miccini", "Minje Kim", "Clément Laroche", "Luca Pezzarossa", "Paris Smaragdis"], "title": "Adaptive Slimming for Scalable and Efficient Speech Enhancement", "comment": "Accepted for publication at the 2025 IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA 2025)", "summary": "Speech enhancement (SE) enables robust speech recognition, real-time\ncommunication, hearing aids, and other applications where speech quality is\ncrucial. However, deploying such systems on resource-constrained devices\ninvolves choosing a static trade-off between performance and computational\nefficiency. In this paper, we introduce dynamic slimming to DEMUCS, a popular\nSE architecture, making it scalable and input-adaptive. Slimming lets the model\noperate at different utilization factors (UF), each corresponding to a\ndifferent performance/efficiency trade-off, effectively mimicking multiple\nmodel sizes without the extra storage costs. In addition, a router subnet,\ntrained end-to-end with the backbone, determines the optimal UF for the current\ninput. Thus, the system saves resources by adaptively selecting smaller UFs\nwhen additional complexity is unnecessary. We show that our solution is\nPareto-optimal against individual UFs, confirming the benefits of dynamic\nrouting. When training the proposed dynamically-slimmable model to use 10% of\nits capacity on average, we obtain the same or better speech quality as the\nequivalent static 25% utilization while reducing MACs by 29%."}
{"id": "2507.04284", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.04284", "abs": "https://arxiv.org/abs/2507.04284", "authors": ["Penggao Yan", "Ronghe Jin", "Junyi Zhang", "Cheng-Wei Wang", "Li-Ta Hsu"], "title": "High-Availability Integrity Monitoring for Multi-Constellation GNSS Navigation with Non-Gaussian Errors", "comment": "Submitted to IEEE Transactions on Instrumentation and Measurement", "summary": "Global navigation satellite systems (GNSS) are essential for aviation,\nrequiring strict integrity monitoring to alert users to hazardously misleading\ninformation. Conventional receiver autonomous integrity monitoring (RAIM) and\nadvanced RAIM (ARAIM) rely heavily on Gaussian models in bounding nominal\nerrors, which can be overly conservative with real-world non-Gaussian errors\nwith heavy tails, such as the satellite clock and orbit errors. This paper\nproposes an extended jackknife detector capable of detecting multiple\nsimultaneous faults with non-Gaussian nominal errors. Furthermore, an integrity\nmonitoring algorithm, jackknife ARAIM, is developed by systematically\nexploiting the properties of the jackknife detector in the range domain. A\ntight bound of the integrity risk is derived by quantifying the impacts of\nhypothetical fault vectors on the position solution. The proposed method is\nexamined in worldwide simulations, with the nominal measurement error simulated\nbased on authentic experimental data, which reveals different findings in\nexisting research. In a setting of a single Global Positioning System (GPS)\nconstellation, the proposed method reduces the 99.5 percentile vertical\nprotection level (VPL) 45m, where the VPL of the baseline ARAIM is larger than\n50m in most user locations. For dual-constellation (GPS-Galileo) settings,\nbaseline ARAIM suffers VPL inflation over 60m due to the over-conservatism\ninduced by the heavy-tailed Galileo signal-in-space range errors, whereas the\nproposed jackknife ARAIM retains VPL below 40m, achieving over 92% normal\noperations for a 35m Vertical Alert Limit. These improvements have promising\npotential to support localizer performance with vertical guidance (LPV) with a\ndecision height of 200 ft, enhancing integrity and availability for\nmulti-constellation GNSS applications."}
{"id": "2507.04230", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04230", "abs": "https://arxiv.org/abs/2507.04230", "authors": ["Kun Fang", "Hanwen Zhang", "Ziyu Wang", "Ichiro Fujinaga"], "title": "High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics", "comment": null, "summary": "Piano sustain pedal detection has previously been approached as a binary\non/off classification task, limiting its application in real-world piano\nperformance scenarios where pedal depth significantly influences musical\nexpression. This paper presents a novel approach for high-resolution estimation\nthat predicts continuous pedal depth values. We introduce a Transformer-based\narchitecture that not only matches state-of-the-art performance on the\ntraditional binary classification task but also achieves high accuracy in\ncontinuous pedal depth estimation. Furthermore, by estimating continuous\nvalues, our model provides musically meaningful predictions for sustain pedal\nusage, whereas baseline models struggle to capture such nuanced expressions\nwith their binary detection approach. Additionally, this paper investigates the\ninfluence of room acoustics on sustain pedal estimation using a synthetic\ndataset that includes varied acoustic conditions. We train our model with\ndifferent combinations of room settings and test it in an unseen new\nenvironment using a \"leave-one-out\" approach. Our findings show that the two\nbaseline models and ours are not robust to unseen room conditions. Statistical\nanalysis further confirms that reverberation influences model predictions and\nintroduces an overestimation bias."}
{"id": "2507.04292", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04292", "abs": "https://arxiv.org/abs/2507.04292", "authors": ["Fan Zhang", "Tianqi Mao", "Mingkun Li", "Meng Hua", "Jinshu Chen", "Christos Masouros", "Zhaocheng Wang"], "title": "Near-Field ISAC for THz Wireless Systems", "comment": null, "summary": "Sixth-generation (6G) wireless networks are expected not only to provide\nhigh-speed connectivity but also to support reliable sensing capabilities,\ngiving rise to the integrated sensing and communication (ISAC) paradigm. To\nenable higher data rates and more accurate sensing, terahertz (THz) systems\nempowered by extremely large multiple-input-multiple-output (XL-MIMO)\ntechnology are envisioned as key enablers for future ISAC systems. Owing to the\nsubstantial increase in both effective array aperture and carrier frequency, a\nconsiderable portion of future ISAC applications is anticipated to fall within\nthe near-field coverage region, instead of the conventional far-field. However,\nmost existing ISAC techniques are designed under the far-field planar wave\nassumption, struggling to accommodate the unique characteristics of THz\nnear-field propagation. To motivate future research into near-field ISAC\nresearch, we systematically investigate the characteristics of THz near-field\npropagation and explore its potential to facilitate ISAC systems. Specifically,\nwe analyze three fundamental characteristics of THz near-field propagation and\nreview state-of-the-art techniques that exploit these features to boost both\ncommunication and sensing performance. To further harness the angular-range\ncoupling effect, we zoom into a particularly interesting approach to near-field\nsensing based on wavenumber domain. Besides, to exploit the beam squint effect,\nan ISAC resource allocation framework is introduced to support integrated\nmulti-angle sensing and multi-user communication. Finally, we outline promising\ndirections for future research in this emerging area."}
{"id": "2507.04419", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04419", "abs": "https://arxiv.org/abs/2507.04419", "authors": ["Ryan A. McCarthy", "You Zhang", "Samuel A. Verburg", "William F. Jenkins", "Peter Gerstoft"], "title": "Machine Learning in Acoustics: A Review and Open-Source Repository", "comment": "Accepted by npj Acoustics, 22 pages, 12 figures", "summary": "Acoustic data provide scientific and engineering insights in fields ranging\nfrom bioacoustics and communications to ocean and earth sciences. In this\nreview, we survey recent advances and the transformative potential of machine\nlearning (ML) in acoustics, including deep learning (DL). Using the Python\nhigh-level programming language, we demonstrate a broad collection of ML\ntechniques to detect and find patterns for classification, regression, and\ngeneration in acoustics data automatically. We have ML examples including\nacoustic data classification, generative modeling for spatial audio, and\nphysics-informed neural networks. This work includes AcousticsML, a set of\npractical Jupyter notebook examples on GitHub demonstrating ML benefits and\nencouraging researchers and practitioners to apply reproducible data-driven\napproaches to acoustic challenges."}
{"id": "2507.04435", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04435", "abs": "https://arxiv.org/abs/2507.04435", "authors": ["Yanliang Jin", "Runze Yu", "Yuan Gao", "Shengli Liu", "Xiaoli Chu", "Kai-Kit Wong", "Chan-Byoung Chae"], "title": "Context-Aware Deep Learning for Robust Channel Extrapolation in Fluid Antenna Systems", "comment": null, "summary": "Fluid antenna systems (FAS) offer remarkable spatial flexibility but face\nsignificant challenges in acquiring high-resolution channel state information\n(CSI), leading to considerable overhead. To address this issue, we propose\nCANet, a robust deep learning model for channel extrapolation in FAS. CANet\ncombines context-adaptive modeling with a cross-scale attention mechanism and\nis built on a ConvNeXt v2 backbone to improve extrapolation accuracy for\nunobserved antenna ports. To further enhance robustness, we introduce a novel\nspatial amplitude perturbation strategy, inspired by frequency-domain\naugmentation techniques in image processing. This motivates the incorporation\nof a Fourier-domain loss function, capturing frequency-domain consistency,\nalongside a spectral structure consistency loss that reinforces learning\nstability under perturbations. Our simulation results demonstrate that CANet\noutperforms benchmark models across a wide range of signal-to-noise ratio (SNR)\nlevels."}
{"id": "2507.04554", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04554", "abs": "https://arxiv.org/abs/2507.04554", "authors": ["Nik Vaessen", "David A. van Leeuwen", "Roeland Ordelman"], "title": "Self-supervised learning of speech representations with Dutch archival data", "comment": "accepted at interspeech 2025", "summary": "This paper explores the use of Dutch archival television broadcast data for\nself-supervised learning of speech foundation models, specifically wav2vec 2.0.\nWe first study data quality assumptions for pre-training, and show how music,\nnoise and speaker overlap affect SSL convergence and downstream fine-tuning\nperformance. Secondly, we explore effectively pre-processing strategies to\nconvert the noisy broadcast dataset into a qualitative dataset for\npre-training, by using Whisper and WhisperX., Thirdly, we compare mono-lingual\nand multi-lingual pre-training with equivalent amounts of data, and show that\nmono-lingual pre-training is more robust to out-of-domain data. Lastly, we\nachieve a state-of-the-art LARGE wav2vec 2.0 model for the Dutch language, by a\ncontinuation of pre-training a wav2vec 2.0 XLS-R model checkpoint with our 55k\nhour archival dataset."}
{"id": "2507.04657", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04657", "abs": "https://arxiv.org/abs/2507.04657", "authors": ["Liangxin Qian", "Jun Zhao"], "title": "Enhancing Data Processing Efficiency in Blockchain Enabled Metaverse over Wireless Communications", "comment": "This paper is accepted by IEEE Transactions on Mobile Computing.\n  arXiv admin note: substantial text overlap with arXiv:2411.16083", "summary": "In the rapidly evolving landscape of the Metaverse, enhanced by blockchain\ntechnology, the efficient processing of data has emerged as a critical\nchallenge, especially in wireless communication systems. Addressing this\nchallenge, our paper introduces the innovative concept of data processing\nefficiency (DPE), aiming to maximize processed bits per unit of resource\nconsumption in blockchain-empowered Metaverse environments. To achieve this, we\npropose the DPE-Aware User Association and Resource Allocation (DAUR)\nalgorithm, a tailored optimization framework for blockchain-enabled Metaverse\nwireless communication systems characterized by joint computing and\ncommunication resource constraints. The DAUR algorithm transforms the nonconvex\nproblem of maximizing the sum of DPE ratios into a solvable convex optimization\nproblem. It alternates the optimization of key variables, including user\nassociation, work offloading ratios, task-specific computing resource\ndistribution, bandwidth allocation, user power usage ratios, and server\ncomputing resource allocation ratios. Our extensive numerical results\ndemonstrate the DAUR algorithm's effectiveness in DPE."}
{"id": "2507.04598", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04598", "abs": "https://arxiv.org/abs/2507.04598", "authors": ["Sho Inoue", "Kun Zhou", "Shuai Wang", "Haizhou Li"], "title": "Multi-Step Prediction and Control of Hierarchical Emotion Distribution in Text-to-Speech Synthesis", "comment": "Accepted to APSIPA Transactions on Signal and Information Processing", "summary": "We investigate hierarchical emotion distribution (ED) for achieving\nmulti-level quantitative control of emotion rendering in text-to-speech\nsynthesis (TTS). We introduce a novel multi-step hierarchical ED prediction\nmodule that quantifies emotion variance at the utterance, word, and phoneme\nlevels. By predicting emotion variance in a multi-step manner, we leverage\nglobal emotional context to refine local emotional variations, thereby\ncapturing the intrinsic hierarchical structure of speech emotion. Our approach\nis validated through its integration into a variance adaptor and an external\nmodule design compatible with various TTS systems. Both objective and\nsubjective evaluations demonstrate that the proposed framework significantly\nenhances emotional expressiveness and enables precise control of emotion\nrendering across multiple speech granularities."}
{"id": "2507.04662", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04662", "abs": "https://arxiv.org/abs/2507.04662", "authors": ["Tao Du", "Jie Yang", "Fan Liu", "Jiaxiang Guo", "Shuqiang Xia", "Chao-Kai Wen", "Shi Jin"], "title": "Simultaneous Localization and Mapping Using Active mmWave Sensing in 5G NR", "comment": "7 pages, 7 figures. Accepted for publication at the 2025 IEEE\n  International Conference on Communications (ICC). \\c{opyright} 2025 IEEE.\n  Personal use is permitted, but permission from IEEE must be obtained for all\n  other uses", "summary": "Millimeter-wave (mmWave) 5G New Radio (NR) communication systems, with their\nhigh-resolution antenna arrays and extensive bandwidth, offer a transformative\nopportunity for high-throughput data transmission and advanced environmental\nsensing. Although passive sensing-based SLAM techniques can estimate user\nlocations and environmental reflections simultaneously, their effectiveness is\noften constrained by assumptions of specular reflections and oversimplified map\nrepresentations. To overcome these limitations, this work employs a mmWave 5G\nNR system for active sensing, enabling it to function similarly to a laser\nscanner for point cloud generation. Specifically, point clouds are extracted\nfrom the power delay profile estimated from each beam direction using a binary\nsearch approach. To ensure accuracy, hardware delays are calibrated with\nmultiple predefined target points. Pose variations of the terminal are then\nestimated from point cloud data gathered along continuous trajectory viewpoints\nusing point cloud registration algorithms. Loop closure detection and pose\ngraph optimization are subsequently applied to refine the sensing results,\nachieving precise terminal localization and detailed radio map reconstruction.\nThe system is implemented and validated through both simulations and\nexperiments, confirming the effectiveness of the proposed approach."}
{"id": "2507.04776", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04776", "abs": "https://arxiv.org/abs/2507.04776", "authors": ["Jun-You Wang", "Li Su"], "title": "Improving BERT for Symbolic Music Understanding Using Token Denoising and Pianoroll Prediction", "comment": "Accepted at ISMIR 2025", "summary": "We propose a pre-trained BERT-like model for symbolic music understanding\nthat achieves competitive performance across a wide range of downstream tasks.\nTo achieve this target, we design two novel pre-training objectives, namely\ntoken correction and pianoroll prediction. First, we sample a portion of note\ntokens and corrupt them with a limited amount of noise, and then train the\nmodel to denoise the corrupted tokens; second, we also train the model to\npredict bar-level and local pianoroll-derived representations from the\ncorrupted note tokens. We argue that these objectives guide the model to better\nlearn specific musical knowledge such as pitch intervals. For evaluation, we\npropose a benchmark that incorporates 12 downstream tasks ranging from chord\nestimation to symbolic genre classification. Results confirm the effectiveness\nof the proposed pre-training objectives on downstream tasks."}
{"id": "2507.04807", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04807", "abs": "https://arxiv.org/abs/2507.04807", "authors": ["Xunqiang Lan", "Xiao Tang", "Ruonan Zhang", "Bin Li", "Yichen Wang", "Dusit Niyato", "Zhu Han"], "title": "UAV-Assisted Integrated Communication and Over-the-Air Computation with Interference Awareness", "comment": "Accepted @ IEEE TCOM", "summary": "Over the air computation (AirComp) is a promising technique that addresses\nbig data collection and fast wireless data aggregation. However, in a network\nwhere wireless communication and AirComp coexist, mutual interference becomes a\ncritical challenge. In this paper, we propose to employ an unmanned aerial\nvehicle (UAV) to enable integrated communication and AirComp, where we\ncapitalize on UAV mobility with alleviated interference for performance\nenhancement. Particularly, we aim to maximize the sum of user transmission rate\nwith the guaranteed AirComp accuracy requirement, where we jointly optimize the\ntransmission strategy, signal normalizing factor, scheduling strategy, and UAV\ntrajectory. We decouple the formulated problem into two layers where the outer\nlayer is for UAV trajectory and scheduling, and the inner layer is for\ntransmission and computation. Then, we solve the inner layer problem through\nalternating optimization, and the outer layer is solved through soft actor\ncritic based deep reinforcement learning. Simulation results show the\nconvergence of the proposed learning process and also demonstrate the\nperformance superiority of our proposal as compared with the baselines in\nvarious situations."}
{"id": "2507.04817", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04817", "abs": "https://arxiv.org/abs/2507.04817", "authors": ["Mathilde Abrassart", "Nicolas Obin", "Axel Roebel"], "title": "Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and Duration Parameters", "comment": "8 pages, 4 figures", "summary": "Precise control over speech characteristics, such as pitch, duration, and\nspeech rate, remains a significant challenge in the field of voice conversion.\nThe ability to manipulate parameters like pitch and syllable rate is an\nimportant element for effective identity conversion, but can also be used\nindependently for voice transformation, achieving goals that were historically\naddressed by vocoder-based methods.\n  In this work, we explore a convolutional neural network-based approach that\naims to provide means for modifying fundamental frequency (F0), phoneme\nsequences, intensity, and speaker identity. Rather than relying on\ndisentanglement techniques, our model is explicitly conditioned on these\nfactors to generate mel spectrograms, which are then converted into waveforms\nusing a universal neural vocoder. Accordingly, during inference, F0 contours,\nphoneme sequences, and speaker embeddings can be freely adjusted, allowing for\nintuitively controlled voice transformations.\n  We evaluate our approach on speaker conversion and expressive speech tasks\nusing both perceptual and objective metrics. The results suggest that the\nproposed method offers substantial flexibility, while maintaining high\nintelligibility and speaker similarity."}
{"id": "2507.04997", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04997", "abs": "https://arxiv.org/abs/2507.04997", "authors": ["Mostafa Rahmani", "Junbo Zhao", "Vida Ranjbar", "Ahmed Al-Tahmeesschi", "Hamed Ahmadi", "Sofie Pollin", "Alister G. Burr"], "title": "Exploring O-RAN Compression Techniques in Decentralized Distributed MIMO Systems: Reducing Fronthaul Load", "comment": "Accepted in IEEE PIMRC 2025", "summary": "This paper explores the application of uplink fronthaul compression\ntechniques within Open RAN (O-RAN) to mitigate fronthaul load in decentralized\ndistributed MIMO (DD-MIMO) systems. With the ever-increasing demand for high\ndata rates and system scalability, the fronthaul load becomes a critical\nbottleneck. Our method uses O-RAN compression techniques to efficiently\ncompress the fronthaul signals. The goal is to greatly lower the fronthaul load\nwhile having little effect on the overall system performance, as shown by Block\nError Rate (BLER) curves. Through rigorous link-level simulations, we compare\nour quantization strategies against a benchmark scenario with no quantization,\nproviding insights into the trade-offs between fronthaul data rate reduction\nand link performance integrity. The results demonstrate that our proposed\nquantization techniques not only lower the fronthaul load but also maintain a\ncompetitive link quality, making them a viable solution for enhancing the\nefficiency of next-generation wireless networks. This study underscores the\npotential of quantization in O-RAN contexts to achieve optimal balance between\nsystem capacity and performance, paving the way for more scalable and robust\nDD-MIMO deployments."}
{"id": "2507.04858", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04858", "abs": "https://arxiv.org/abs/2507.04858", "authors": ["António Sá Pinto"], "title": "Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for Maracatu", "comment": "Accepted at ISMIR 2025", "summary": "We explore transfer learning strategies for musical onset detection in the\nAfro-Brazilian Maracatu tradition, which features complex rhythmic patterns\nthat challenge conventional models. We adapt two Temporal Convolutional Network\narchitectures: one pre-trained for onset detection (intra-task) and another for\nbeat tracking (inter-task). Using only 5-second annotated snippets per\ninstrument, we fine-tune these models through layer-wise retraining strategies\nfor five traditional percussion instruments. Our results demonstrate\nsignificant improvements over baseline performance, with F1 scores reaching up\nto 0.998 in the intra-task setting and improvements of over 50 percentage\npoints in best-case scenarios. The cross-task adaptation proves particularly\neffective for time-keeping instruments, where onsets naturally align with beat\npositions. The optimal fine-tuning configuration varies by instrument,\nhighlighting the importance of instrument-specific adaptation strategies. This\napproach addresses the challenges of underrepresented musical traditions,\noffering an efficient human-in-the-loop methodology that minimizes annotation\neffort while maximizing performance. Our findings contribute to more inclusive\nmusic information retrieval tools applicable beyond Western musical contexts."}
{"id": "2507.05071", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.05071", "abs": "https://arxiv.org/abs/2507.05071", "authors": ["Burak Ahmet Ozden", "Fatih Cogen", "Erdogan Aydin"], "title": "Deep Learning Based Antenna Selection Technique for RIS-Empowered RQSM System", "comment": "6 pages, 5 figures, 2 tables", "summary": "Reconfigurable intelligent surface (RIS) technology has attracted\nconsiderable interest due to its ability to control wireless propagation with\nminimal power usage. Receive quadrature spatial modulation (RQSM) scheme\ntransmits data bits in both in-phase ($I$) and quadrature ($Q$) channels,\ndoubling the number of active receive antenna indices and improving spectral\nefficiency compared to the traditional receive spatial modulation (RSM)\ntechnique. Also, capacity-optimized antenna selection (COAS) improves error\nperformance by selecting antennas with the best channel conditions. This paper\nproposes a new deep neural network (DNN)-based antenna selection method,\nsupported by the COAS technique, to improve the error performance of the\nRIS-RQSM system. Monte Carlo simulations of the proposed DNN-COAS-RIS-RQSM\nsystem using the quadrature amplitude modulation (QAM) technique for Rayleigh\nfading channels are performed and compared with the COAS-RIS-RQSM system. Also,\na comparative analysis of the computational complexities of the DNN and COAS\ntechniques is conducted to evaluate the trade-offs between error performance\nand complexity."}
{"id": "2507.04864", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04864", "abs": "https://arxiv.org/abs/2507.04864", "authors": ["Alexander Fichtinger", "Jan Schlüter", "Gerhard Widmer"], "title": "Music Boomerang: Reusing Diffusion Models for Data Augmentation and Audio Manipulation", "comment": "Accepted at SMC 2025. Code at https://malex1106.github.io/boomify/", "summary": "Generative models of music audio are typically used to generate output based\nsolely on a text prompt or melody. Boomerang sampling, recently proposed for\nthe image domain, allows generating output close to an existing example, using\nany pretrained diffusion model. In this work, we explore its application in the\naudio domain as a tool for data augmentation or content manipulation.\nSpecifically, implementing Boomerang sampling for Stable Audio Open, we augment\ntraining data for a state-of-the-art beat tracker, and attempt to replace\nmusical instruments in recordings. Our results show that the rhythmic structure\nof existing examples is mostly preserved, that it improves performance of the\nbeat tracker, but only in scenarios of limited training data, and that it can\naccomplish text-based instrument replacement on monophonic inputs. We publish\nour implementation to invite experiments on data augmentation in other tasks\nand explore further applications."}
{"id": "2507.05099", "categories": ["eess.SP", "hep-ex"], "pdf": "https://arxiv.org/pdf/2507.05099", "abs": "https://arxiv.org/abs/2507.05099", "authors": ["Marc Neu", "Isabel Haide", "Timo Justinger", "Till Rädler", "Valdrin Dajaku", "Torben Ferber", "Jürgen Becker"], "title": "Real-Time Graph-based Point Cloud Networks on FPGAs via Stall-Free Deep Pipelining", "comment": "Accepted to IEEE SBCCI 2025", "summary": "Graph-based Point Cloud Networks (PCNs) are powerful tools for processing\nsparse sensor data with irregular geometries, as found in high-energy physics\ndetectors. However, deploying models in such environments remains challenging\ndue to stringent real-time requirements for both latency, and throughput. In\nthis work, we present a deeply pipelined dataflow architecture for executing\ngraph-based PCNs on FPGAs. Our method supports efficient processing of dynamic,\nsparse point clouds while meeting hard real-time constraints. We introduce\nspecialized processing elements for core graph operations, such as GraVNet\nconvolution and condensation point clustering, and demonstrate our design on\nthe AMD Versal VCK190. Compared to a GPU baseline, our FPGA implementation\nachieves up to 5.25x speedup in throughput while maintaining latencies below 10\n{\\mu}s, satisfying the demands of real-time trigger systems in particle physics\nexperiments. An open-source reference implementation is provided."}
{"id": "2507.04955", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04955", "abs": "https://arxiv.org/abs/2507.04955", "authors": ["Fathinah Izzati", "Xinyue Li", "Gus Xia"], "title": "EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation", "comment": null, "summary": "We propose Expotion (Facial Expression and Motion Control for Multimodal\nMusic Generation), a generative model leveraging multimodal visual controls -\nspecifically, human facial expressions and upper-body motion - as well as text\nprompts to produce expressive and temporally accurate music. We adopt\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\ngeneration model, enabling fine-grained adaptation to the multimodal controls\nusing a small dataset. To ensure precise synchronization between video and\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\nExperiments demonstrate that integrating visual features alongside textual\ndescriptions enhances the overall quality of generated music in terms of\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\nvideo, and text adherence, surpassing both proposed baselines and existing\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\nexpressive facial and upper-body gestures aligned with corresponding music,\nproviding significant potential for future research in multimodal and\ninteractive music generation."}
{"id": "2507.05111", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.05111", "abs": "https://arxiv.org/abs/2507.05111", "authors": ["Hao Zhang", "Fuhui Zhou", "Wei Wang", "Qihui Wu", "Chau Yuen"], "title": "A Federated Learning-based Lightweight Network with Zero Trust for UAV Authentication", "comment": "accepted by IEEE Transactions on Information Forensics and Security", "summary": "Unmanned aerial vehicles (UAVs) are increasingly being integrated into\nnext-generation networks to enhance communication coverage and network\ncapacity. However, the dynamic and mobile nature of UAVs poses significant\nsecurity challenges, including jamming, eavesdropping, and cyber-attacks. To\naddress these security challenges, this paper proposes a federated\nlearning-based lightweight network with zero trust for enhancing the security\nof UAV networks. A novel lightweight spectrogram network is proposed for UAV\nauthentication and rejection, which can effectively authenticate and reject\nUAVs based on spectrograms. Experiments highlight LSNet's superior performance\nin identifying both known and unknown UAV classes, demonstrating significant\nimprovements over existing benchmarks in terms of accuracy, model compactness,\nand storage requirements. Notably, LSNet achieves an accuracy of over $80\\%$\nfor known UAV types and an Area Under the Receiver Operating Characteristic\n(AUROC) of $0.7$ for unknown types when trained with all five clients. Further\nanalyses explore the impact of varying the number of clients and the presence\nof unknown UAVs, reinforcing the practical applicability and effectiveness of\nour proposed framework in real-world FL scenarios."}
{"id": "2507.04963", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04963", "abs": "https://arxiv.org/abs/2507.04963", "authors": ["Šimon Libřický", "Jan Hajič jr"], "title": "Modeling the Difficulty of Saxophone Music", "comment": null, "summary": "In learning music, difficulty is an important factor in choice of repertoire,\nchoice of tempo, and structure of practice. These choices are typically done\nwith the guidance of a teacher; however, not all learners have access to one.\nWhile piano and strings have had some attention devoted to automated difficulty\nestimation, wind instruments have so far been under-served. In this paper, we\npropose a method for estimating the difficulty of pieces for winds and\nimplement it for the tenor saxophone. We take the cost-of-traversal approach,\nmodelling the part as a sequence of transitions -- note pairs. We estimate\ntransition costs from newly collected recordings of trill speeds, comparing\nrepresentations of saxophone fingerings at various levels of expert input. We\nthen compute and visualise the cost of the optimal path through the part, at a\ngiven tempo. While we present this model for the tenor saxophone, the same\npipeline can be applied to other woodwinds, and our experiments show that with\nappropriate feature design, only a small proportion of possible trills is\nneeded to estimate the costs well. Thus, we present a practical way of\ndiversifying the capabilities of MIR in music education to the wind family of\ninstruments."}
{"id": "2507.02915", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.02915", "abs": "https://arxiv.org/abs/2507.02915", "authors": ["Ludovic Tuncay", "Etienne Labbé", "Emmanouil Benetos", "Thomas Pellegrini"], "title": "Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning", "comment": null, "summary": "Building on the Joint-Embedding Predictive Architecture (JEPA) paradigm, a\nrecent self-supervised learning framework that predicts latent representations\nof masked regions in high-level feature spaces, we propose Audio-JEPA (Audio\nJoint-Embedding Predictive Architecture), tailored specifically for audio data.\nAudio-JEPA uses a simple Vision Transformer backbone to predict latent\nrepresentations of masked spectrogram patches rather than reconstructing raw\naudio. We pre-train on unlabeled AudioSet clips (10s, 32kHz) with random patch\nmasking on mel-spectrograms. We evaluate on the X-ARES suite covering speech,\nmusic, and environmental sound tasks. Although our implementation is a\nstraightforward translation of the original model to audio, the results still\nshow comparable performance to wav2vec 2.0 and data2vec while using less than\none-fifth of their training data and with no hyper-parameter tuning. All code\nand pretrained checkpoints will be released on GitHub."}
{"id": "2507.04966", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.04966", "abs": "https://arxiv.org/abs/2507.04966", "authors": ["Sandipan Dhar", "Mayank Gupta", "Preeti Rao"], "title": "LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning", "comment": "10 pages, 5 figures, 3 Tables", "summary": "The field of Singing Voice Synthesis (SVS) has seen significant advancements\nin recent years due to the rapid progress of diffusion-based approaches.\nHowever, capturing vocal style, genre-specific pitch inflections, and\nlanguage-dependent characteristics remains challenging, particularly in\nlow-resource scenarios. To address this, we propose LAPS-Diff, a diffusion\nmodel integrated with language-aware embeddings and a vocal-style guided\nlearning mechanism, specifically designed for Bollywood Hindi singing style. We\ncurate a Hindi SVS dataset and leverage pre-trained language models to extract\nword and phone-level embeddings for an enriched lyrics representation.\nAdditionally, we incorporated a style encoder and a pitch extraction model to\ncompute style and pitch losses, capturing features essential to the naturalness\nand expressiveness of the synthesized singing, particularly in terms of vocal\nstyle and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec\nmodels to extract musical and contextual embeddings, serving as conditional\npriors to refine the acoustic feature generation process further. Based on\nobjective and subjective evaluations, we demonstrate that LAPS-Diff\nsignificantly improves the quality of the generated samples compared to the\nconsidered state-of-the-art (SOTA) model for our constrained dataset that is\ntypical of the low resource scenario."}
{"id": "2507.04108", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04108", "abs": "https://arxiv.org/abs/2507.04108", "authors": ["Yhonatan Gayer", "Vladimir Tourbabin", "Zamir Ben-Hur", "David Alon", "Boaz Rafaely"], "title": "Ambisonics Encoder for Wearable Array with Improved Binaural Reproduction", "comment": "Published in Forum Acousticum 2025, 6 pages, 2 figures", "summary": "Ambisonics Signal Matching (ASM) is a recently proposed signal-independent\napproach to encoding Ambisonic signal from wearable microphone arrays, enabling\nefficient and standardized spatial sound reproduction. However, reproduction\naccuracy is currently limited due to the non-ideal layout of the microphones.\nThis research introduces an enhanced ASM encoder that reformulates the loss\nfunction by integrating a Binaural Signal Matching (BSM) term into the\noptimization framework. The aim of this reformulation is to improve the\naccuracy of binaural reproduction when integrating the Ambisonic signal with\nHead-Related Transfer Functions (HRTFs), making the encoded Ambisonic signal\nbetter suited for binaural reproduction. This paper first presents the\nmathematical formulation developed to align the ASM and BSM objectives in a\nsingle loss function, followed by a simulation study with a simulated\nmicrophone array mounted on a rigid sphere representing a head-mounted wearable\narray. The analysis shows that improved binaural reproduction with the encoded\nAmbisonic signal can be achieved using this joint ASM-BSM optimization, thereby\nenabling higher-quality binaural playback for virtual and augmented reality\napplications based on Ambisonics."}
{"id": "2507.04264", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04264", "abs": "https://arxiv.org/abs/2507.04264", "authors": ["Kyeomeun Jang", "Jiaying Li", "Yinuo Wang"], "title": "The Overview of Segmental Durations Modification Algorithms on Speech Signal Characteristics", "comment": null, "summary": "This paper deeply evaluates and analyzes several mainstream algorithms that\ncan arbitrarily modify the duration of any portion of a given speech signal\nwithout changing the essential properties (e.g., pitch contour, power spectrum,\netc.) of the original signal. Arbitrary modification in this context means that\nthe duration of any region of the signal can be changed by specifying the\nstarting and ending time for modification or the target duration of the\nspecified interval, which can be either a fixed value of duration in the time\ndomain or a scaling factor of the original duration. In addition, arbitrary\nmodification also indicates any number of intervals can be modified at the same\ntime."}
{"id": "2507.04419", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04419", "abs": "https://arxiv.org/abs/2507.04419", "authors": ["Ryan A. McCarthy", "You Zhang", "Samuel A. Verburg", "William F. Jenkins", "Peter Gerstoft"], "title": "Machine Learning in Acoustics: A Review and Open-Source Repository", "comment": "Accepted by npj Acoustics, 22 pages, 12 figures", "summary": "Acoustic data provide scientific and engineering insights in fields ranging\nfrom bioacoustics and communications to ocean and earth sciences. In this\nreview, we survey recent advances and the transformative potential of machine\nlearning (ML) in acoustics, including deep learning (DL). Using the Python\nhigh-level programming language, we demonstrate a broad collection of ML\ntechniques to detect and find patterns for classification, regression, and\ngeneration in acoustics data automatically. We have ML examples including\nacoustic data classification, generative modeling for spatial audio, and\nphysics-informed neural networks. This work includes AcousticsML, a set of\npractical Jupyter notebook examples on GitHub demonstrating ML benefits and\nencouraging researchers and practitioners to apply reproducible data-driven\napproaches to acoustic challenges."}
{"id": "2507.04845", "categories": ["eess.AS", "cs.LG", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.04845", "abs": "https://arxiv.org/abs/2507.04845", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "Spatial and Semantic Embedding Integration for Stereo Sound Event Localization and Detection in Regular Videos", "comment": null, "summary": "This report presents our systems submitted to the audio-only and audio-visual\ntracks of the DCASE2025 Task 3 Challenge: Stereo Sound Event Localization and\nDetection (SELD) in Regular Video Content. SELD is a complex task that combines\ntemporal event classification with spatial localization, requiring reasoning\nacross spatial, temporal, and semantic dimensions. The last is arguably the\nmost challenging to model. Traditional SELD architectures rely on multichannel\ninput, which limits their ability to leverage large-scale pre-training due to\ndata constraints. To address this, we enhance standard SELD architectures with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. Additionally, we incorporate\nautocorrelation-based acoustic features to improve distance estimation. We\npre-train our models on curated synthetic audio and audio-visual datasets and\napply a left-right channel swapping augmentation to further increase the\ntraining data. Both our audio-only and audio-visual systems substantially\noutperform the challenge baselines on the development set, demonstrating the\neffectiveness of our strategy. Performance is further improved through model\nensembling and a visual post-processing step based on human keypoints. Future\nwork will investigate the contribution of each modality and explore\narchitectural variants to further enhance results."}

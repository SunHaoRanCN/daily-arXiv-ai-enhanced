{"id": "2507.05396", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05396", "abs": "https://arxiv.org/abs/2507.05396", "authors": ["Juliette Florin"], "title": "Comparative Analysis of Finite Difference and Finite Element Method for Audio Waveform Simulation", "comment": "46 pages, 38 figures, Link to the source code:\n  https://gitlab.eurecom.fr/florin/models", "summary": "In many industries, including aerospace and defense, waveform analysis is\ncommonly conducted to compute the resonance of physical objects, with the\nFinite Element Method (FEM) being the standard approach. The Finite Difference\nMethod (FDM) is seldom used, and this preference is often stated without formal\njustification in the literature. In this work, the accuracy, feasibility, and\ntime of simulation of FEM and FDM are compared by simulating the vibration of a\nguitar string. Python simulations for both methods are implemented, and their\nresults are compared against analytical solutions and experimental data.\nAdditionally, FDM is applied to analyze the sound of a cycling bell to assess\nits reliability compared to a real cycling bell. Final results show that both\nFEM and FDM yield similar error margins and accurately predict the system's\nbehavior. Moreover, the errors from FEM and FDM follow the same periodicity\nwith a phase shift when varying the assumed analytical tension and without a\nphase shift when changing the time interval. However, FEM converges faster with\nincreasing mesh complexity, whereas FDM demonstrates quicker computational\nperformance and achieves stable solutions even with bigger time intervals.\nDespite this FDM is limited to simpler configurations and often demands\nextensive mathematical formulation, which can become cumbersome for intricate\nshapes. For example, modeling a hemispherical object using FDM results in\nsignificant simulation times and big calculations. In conclusion, while FDM may\noffer faster convergence and computation time in certain cases, FEM remains the\npreferred method in industrial contexts due to its flexibility, scalability,\nand ease of implementation for complex geometries."}
{"id": "2507.05399", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05399", "abs": "https://arxiv.org/abs/2507.05399", "authors": ["Srikanth Korse", "Oliver Thiergart", "Emanuel A. P. Habets"], "title": "Sample Rate Offset Compensated Acoustic Echo Cancellation For Multi-Device Scenarios", "comment": "Published in IWAENC 2024", "summary": "Acoustic echo cancellation (AEC) in multi-device scenarios is a challenging\nproblem due to sample rate offset (SRO) between devices. The SRO hinders the\nconvergence of the AEC filter, diminishing its performance. To address this ,\nwe approach the multi-device AEC scenario as a multi-channel AEC problem\ninvolving a multi-channel Kalman filter, SRO estimation, and resampling of\nfar-end signals. Experiments in a two-device scenario show that our system\nmitigates the divergence of the multi-channel Kalman filter in the presence of\nSRO for both correlated and uncorrelated playback signals during echo-only and\ndouble-talk. Additionally, for devices with correlated playback signals, an\nindependent single-channel AEC filter is crucial to ensure fast convergence of\nSRO estimation."}
{"id": "2507.05402", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05402", "abs": "https://arxiv.org/abs/2507.05402", "authors": ["Srikanth Korse", "Andreas Walther", "Emanuel A. P. Habets"], "title": "Stereo Reproduction in the Presence of Sample Rate Offsets", "comment": "Accepted to WASPAA 2025", "summary": "One of the main challenges in synchronizing wirelessly connected loudspeakers\nfor spatial audio reproduction is clock skew. Clock skew arises from sample\nrate offsets ( SROs) between the loudspeakers, caused by the use of independent\ndevice clocks. While network-based protocols like Precision Time Protocol (PTP)\nand Network Time Protocol (NTP) are explored, the impact of SROs on spatial\naudio reproduction and its perceptual consequences remains underexplored. We\npropose an audio-domain SRO compensation method using spatial filtering to\nisolate loudspeaker contributions. These filtered signals, along with the\noriginal playback signal, are used to estimate the SROs, and their influence is\ncompensated for prior to spatial audio reproduction. We evaluate the effect of\nthe compensation method in a subjective listening test. The results of these\ntests as well as objective metrics demonstrate that the proposed method\nmitigates the perceptual degradation introduced by SROs by preserving the\nspatial cues."}
{"id": "2507.05409", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05409", "abs": "https://arxiv.org/abs/2507.05409", "authors": ["Andrea Eichenseer", "Srikanth Korse", "Guillaume Fuchs", "Markus Multrus"], "title": "Parametric Object Coding in IVAS: Efficient Coding of Multiple Audio Objects at Low Bit Rates", "comment": "Published in ICASSP 2025", "summary": "The recently standardized 3GPP codec for Immersive Voice and Audio Services\n(IVAS) includes a parametric mode for efficiently coding multiple audio objects\nat low bit rates. In this mode, parametric side information is obtained from\nboth the object metadata and the input audio objects. The side information\ncomprises directional information, indices of two dominant objects, and the\npower ratio between these two dominant objects. It is transmitted to the\ndecoder along with a stereo downmix. In IVAS, parametric object coding allows\nfor transmitting three or four arbitrarily placed objects at bit rates of 24.4\nor 32 kbit/s and faithfully reconstructing the spatial image of the original\naudio scene. Subjective listening tests confirm that IVAS provides a comparable\nimmersive experience at lower bit rate and complexity compared to coding the\naudio objects independently using Enhanced Voice Services (EVS)."}
{"id": "2507.05657", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05657", "abs": "https://arxiv.org/abs/2507.05657", "authors": ["Manan Mittal", "Ryan M. Corey", "Andrew C. Singer"], "title": "Adaptive Linearly Constrained Minimum Variance Volumetric Active Noise Control", "comment": "5 pages, 6 figures", "summary": "Traditional volumetric noise control typically relies on multipoint error\nminimization to suppress sound energy across a region, but offers limited\nflexibility in shaping spatial responses. This paper introduces a time-domain\nformulation for linearly constrained minimum variance active noise control\n(LCMV ANC) for spatial control filter design. We demonstrate how the LCMV ANC\noptimization framework allows system designers to prioritize noise reduction at\nspecific spatial locations through strategically defined linear constraints,\nproviding a more flexible alternative to uniformly weighted multipoint error\nminimization. An adaptive algorithm based on filtered-X least mean squares\n(FxLMS) is derived for online adaptation of filter coefficients. Simulation and\nexperimental results validate the proposed method's noise reduction and\nconstraint adherence, demonstrating effective, spatially selective, and\nbroadband noise control compared to multipoint volumetric noise control."}
{"id": "2507.05625", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.05625", "abs": "https://arxiv.org/abs/2507.05625", "authors": ["Zhen Chen", "Jianqing Li", "Xiu Yin Zhang", "Kai-Kit Wong", "Chan-Byoung Chae", "Yangyang Zhang"], "title": "Iterative Sparse Asymptotic Minimum Variance Based Channel Estimation in Fluid Antenna System", "comment": "9 pages, 7 figures", "summary": "With fluid antenna system (FAS) gradually establishing itself as a possible\nenabling technology for next generation wireless communications, channel\nestimation for FAS has become a pressing issue. Existing methodologies however\nface limitations in noise suppression. To overcome this, in this paper, we\npropose a maximum likelihood (ML)-based channel estimation approach tailored\nfor FAS systems, designed to mitigate noise interference and enhance estimation\naccuracy. By capitalizing on the inherent sparsity of wireless channels, we\nintegrate an ML-based iterative tomographic algorithm to systematically reduce\nnoise perturbations during the channel estimation process. Furthermore, the\nproposed approach leverages spatial correlation within the FAS channel to\noptimize estimation accuracy and spectral efficiency. Simulation results\nconfirm the efficacy of the proposed method, demonstrating superior channel\nestimation accuracy and robustness compared to existing benchmark techniques."}
{"id": "2507.05609", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05609", "abs": "https://arxiv.org/abs/2507.05609", "authors": ["Yang Liu", "Li Wan", "Yiteng Huang", "Yong Xu", "yangyang shi", "Saurabh Adya", "ming sun", "Florian Metze"], "title": "MMW: Side Talk Rejection Multi-Microphone Whisper on Smart Glasses", "comment": null, "summary": "Smart glasses are increasingly positioned as the next-generation interface\nfor ubiquitous access to large language models (LLMs). Nevertheless, achieving\nreliable interaction in real-world noisy environments remains a major\nchallenge, particularly due to interference from side speech. In this work, we\nintroduce a novel side-talk rejection multi-microphone Whisper (MMW) framework\nfor smart glasses, incorporating three key innovations. First, we propose a Mix\nBlock based on a Tri-Mamba architecture to effectively fuse multi-channel audio\nat the raw waveform level, while maintaining compatibility with streaming\nprocessing. Second, we design a Frame Diarization Mamba Layer to enhance\nframe-level side-talk suppression, facilitating more efficient fine-tuning of\nWhisper models. Third, we employ a Multi-Scale Group Relative Policy\nOptimization (GRPO) strategy to jointly optimize frame-level and\nutterance-level side speech suppression. Experimental evaluations demonstrate\nthat the proposed MMW system can reduce the word error rate (WER) by 4.95\\% in\nnoisy conditions."}
{"id": "2507.05662", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05662", "abs": "https://arxiv.org/abs/2507.05662", "authors": ["Manan Mittal", "Ryan M. Corey", "Andrew C. Singer"], "title": "Beamforming with Random Projections: Upper and Lower Bounds", "comment": "5 pages, 3 figures", "summary": "Beamformers often trade off white noise gain against the ability to suppress\ninterferers. With distributed microphone arrays, this trade-off becomes crucial\nas different arrays capture vastly different magnitude and phase differences\nfor each source. We propose the use of multiple random projections as a\nfirst-stage preprocessing scheme in a data-driven approach to dimensionality\nreduction and beamforming. We show that a mixture beamformer derived from the\nuse of multiple such random projections can effectively outperform the minimum\nvariance distortionless response (MVDR) beamformer in terms of signal-to-noise\nratio (SNR) and signal-to-interferer-and-noise ratio (SINR) gain. Moreover, our\nmethod introduces computational complexity as a trade-off in the design of\nadaptive beamformers, alongside noise gain and interferer suppression. This\nadded degree of freedom allows the algorithm to better exploit the inherent\nstructure of the received signal and achieve better real-time performance while\nrequiring fewer computations. Finally, we derive upper and lower bounds for the\noutput power of the compressed beamformer when compared to the full complexity\nMVDR beamformer."}
{"id": "2507.05897", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.05897", "abs": "https://arxiv.org/abs/2507.05897", "authors": ["Almutasem Bellah Enad", "Jihad Fahs", "Hadi Sarieddeen", "Hakim Jemaa", "Tareq Y. Al-Naffouri"], "title": "Performance Analysis of Linear Detection under Noise-Dependent Fast-Fading Channels", "comment": null, "summary": "This paper presents a performance analysis framework for linear detection in\nfast-fading channels with possibly correlated channel and noise. The framework\nis both accurate and adaptable, making it well-suited for analyzing a wide\nrange of channel and noise models. As such, it serves as a valuable tool for\nthe design and evaluation of detection algorithms in next-generation wireless\ncommunication systems. By characterizing the distribution of the effective\nnoise after zero-forcing filtering, we derive a semi-analytical and asymptotic\nexpression for the symbol error rate under Rayleigh fading and\nchannel-dependent additive circular complex Gaussian noise. The proposed\napproach demonstrates excellent agreement with integration-based benchmarks as\nconfirmed by numerical simulations thus validating its accuracy. The framework\nis flexible and can be extended to various channel and noise models, offering a\nvaluable tool for the design and analysis of detection algorithms in\nnext-generation communication systems."}
{"id": "2507.05635", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05635", "abs": "https://arxiv.org/abs/2507.05635", "authors": ["Md. Mahbub Hasan", "Md Rakibul Hasan", "Md Zakir Hossain", "Tom Gedeon"], "title": "Frequency-Specific Neural Response and Cross-Correlation Analysis of Envelope Following Responses to Native Speech and Music Using Multichannel EEG Signals: A Case Study", "comment": null, "summary": "Although native speech and music envelope following responses (EFRs) play a\ncrucial role in auditory processing and cognition, their frequency profile,\nsuch as the dominating frequency and spectral coherence, is largely unknown. We\nhave assumed that the auditory pathway - which transmits envelope components of\nspeech and music to the scalp through time-varying neurophysiological processes\n- is a linear time-varying system, with the envelope and the multi-channel EEG\nresponses as excitation and response, respectively. This paper investigates the\ntransfer function of this system through two analytical techniques -\ntime-averaged spectral responses and cross-spectral density - in the frequency\ndomain at four different positions of the human scalp. Our findings suggest\nthat alpha (8-11 Hz), lower gamma (53-56 Hz), and higher gamma (78-81 Hz) bands\nare the peak responses of the system. These frequently appearing dominant\nfrequency responses may be the key components of familiar speech perception,\nmaintaining attention, binding acoustic features, and memory processing. The\ncross-spectral density, which reflects the spatial neural coherence of the\nhuman brain, shows that 10-13 Hz, 27-29 Hz, and 62-64 Hz are common for all\nchannel pairs. As neural coherences are frequently observed in these\nfrequencies among native participants, we suggest that these distributed neural\nprocesses are also dominant in native speech and music perception."}
{"id": "2507.05729", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05729", "abs": "https://arxiv.org/abs/2507.05729", "authors": ["Katsuhiko Yamamoto", "Koichi Miyazaki"], "title": "Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners", "comment": "Accepted by INTERSPEECH 2025", "summary": "Speech intelligibility prediction (SIP) models have been used as objective\nmetrics to assess intelligibility for hearing-impaired (HI) listeners. In the\nClarity Prediction Challenge 2 (CPC2), non-intrusive binaural SIP models based\non transformers showed high prediction accuracy. However, the self-attention\nmechanism theoretically incurs high computational and memory costs, making it a\nbottleneck for low-latency, power-efficient devices. This may also degrade the\ntemporal processing of binaural SIPs. Therefore, we propose Mamba-based SIP\nmodels instead of transformers for the temporal processing blocks. Experimental\nresults show that our proposed SIP model achieves competitive performance\ncompared to the baseline while maintaining a relatively small number of\nparameters. Our analysis suggests that the SIP model based on bidirectional\nMamba effectively captures contextual and spatial speech information from\nbinaural signals."}
{"id": "2507.06020", "categories": ["eess.SP", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.06020", "abs": "https://arxiv.org/abs/2507.06020", "authors": ["Bo Zhou", "Kaijie Xu", "Yinghui Quan", "Mengdao Xing"], "title": "A Differential Evolution Algorithm with Neighbor-hood Mutation for DOA Estimation", "comment": null, "summary": "Two-dimensional (2D) Multiple Signal Classification algorithm is a powerful\ntechnique for high-resolution direction-of-arrival (DOA) estimation in array\nsignal processing. However, the exhaustive search over the 2D an-gular domain\nleads to high computa-tional cost, limiting its applicability in real-time\nscenarios. In this work, we reformulate the peak-finding process as a\nmultimodal optimization prob-lem, and propose a Differential Evolu-tion\nalgorithm with Neighborhood Mutation (DE-NM) to efficiently lo-cate multiple\nspectral peaks without requiring dense grid sampling. Simu-lation results\ndemonstrate that the proposed method achieves comparable estimation accuracy to\nthe traditional grid search, while significantly reduc-ing computation time.\nThis strategy presents a promising solution for real-time, high-resolution DOA\nestimation in practical applications. The imple-mentation code is available at\nhttps://github.com/zzb-nice/DOA_multimodel_optimize."}
{"id": "2507.05688", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05688", "abs": "https://arxiv.org/abs/2507.05688", "authors": ["Liang Xu", "Longfei Felix Yan", "W. Bastiaan Kleijn"], "title": "Robust One-step Speech Enhancement via Consistency Distillation", "comment": "Accepted to IEEE WASPAA 2025. 6 pages, 1 figures", "summary": "Diffusion models have shown strong performance in speech enhancement, but\ntheir real-time applicability has been limited by multi-step iterative\nsampling. Consistency distillation has recently emerged as a promising\nalternative by distilling a one-step consistency model from a multi-step\ndiffusion-based teacher model. However, distilled consistency models are\ninherently biased towards the sampling trajectory of the teacher model, making\nthem less robust to noise and prone to inheriting inaccuracies from the teacher\nmodel. To address this limitation, we propose ROSE-CD: Robust One-step Speech\nEnhancement via Consistency Distillation, a novel approach for distilling a\none-step consistency model. Specifically, we introduce a randomized learning\ntrajectory to improve the model's robustness to noise. Furthermore, we jointly\noptimize the one-step model with two time-domain auxiliary losses, enabling it\nto recover from teacher-induced errors and surpass the teacher model in overall\nperformance. This is the first pure one-step consistency distillation model for\ndiffusion-based speech enhancement, achieving 54 times faster inference speed\nand superior performance compared to its 30-step teacher model. Experiments on\nthe VoiceBank-DEMAND dataset demonstrate that the proposed model achieves\nstate-of-the-art performance in terms of speech quality. Moreover, its\ngeneralization ability is validated on both an out-of-domain dataset and\nreal-world noisy recordings."}
{"id": "2507.05900", "categories": ["cs.SD", "cs.LG", "eess.AS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.05900", "abs": "https://arxiv.org/abs/2507.05900", "authors": ["Zengjing Chen", "Lu Wang", "Chengzhi Xing"], "title": "Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning", "comment": null, "summary": "This study addresses the problem of stable acoustic relay assignment in an\nunderwater acoustic network. Unlike the objectives of most existing literature,\ntwo distinct objectives, namely classical stable arrangement and ambiguous\nstable arrangement, are considered. To achieve these stable arrangements, a\nlaser chaos-based multi-processing learning (LC-ML) method is introduced to\nefficiently obtain high throughput and rapidly attain stability. In order to\nsufficiently explore the relay's decision-making, this method uses random\nnumbers generated by laser chaos to learn the assignment of relays to multiple\nsource nodes. This study finds that the laser chaos-based random number and\nmulti-processing in the exchange process have a positive effect on higher\nthroughput and strong adaptability with environmental changing over time.\nMeanwhile, ambiguous cognitions result in the stable configuration with less\nvolatility compared to accurate ones. This provides a practical and useful\nmethod and can be the basis for relay selection in complex underwater\nenvironments."}
{"id": "2507.06028", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.06028", "abs": "https://arxiv.org/abs/2507.06028", "authors": ["Emanuele Grossi", "Marco Lops", "Luca Venturino"], "title": "RIS-Enabled Transmitter Design for Joint Radar and Communication", "comment": "Accepted to the 2025 IEEE Radar Conference, Krakow, Poland", "summary": "Achieving efficient and cost-effective transmit beampattern control for\nintegrated sensing and communication (ISAC) systems is a significant challenge.\nThis paper addresses this by proposing a dual-function radar communication\n(DFRC) transmitter based on a reconfigurable intelligent surface (RIS)\nilluminated by a limited number of active sources. We formulate and solve the\njoint design of source waveforms and RIS phase shifts to match a desired\nspace-frequency radiation pattern, and we evaluate the resulting ISAC system's\nperformance in terms of radar detection probability and data transmission rate.\nNumerical results demonstrate the promising capabilities of this RIS-enabled\ntransmitter for ISAC applications."}
{"id": "2507.05727", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05727", "abs": "https://arxiv.org/abs/2507.05727", "authors": ["He Wang", "Linhan Ma", "Dake Guo", "Xiong Wang", "Lei Xie", "Jin Xu", "Junyang Lin"], "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark", "comment": "18 pages, 4 figures", "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench."}
{"id": "2507.05911", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05911", "abs": "https://arxiv.org/abs/2507.05911", "authors": ["Changfeng Gao", "Zhihao Du", "Shiliang Zhang"], "title": "Differentiable Reward Optimization for LLM based TTS system", "comment": null, "summary": "This paper proposes a novel Differentiable Reward Optimization (DiffRO)\nmethod aimed at enhancing the performance of neural codec language models based\ntext-to-speech (TTS) systems. In contrast to conventional reinforcement\nlearning from human feedback (RLHF) approaches applied to TTS, DiffRO directly\ncompute the rewards based on neural codec tokens, rather than relying on\nsynthesized audio. Furthermore, we employ the Gumbel-Softmax technique to\nrender the reward function differentiable, thereby streamlining the RLHF\ntraining process. Additionally, we introduce a multi-task reward (MTR) model\nwhich can provide feedback from different perspectives and find that it can\naugment the system's capability to follow instructions effectively.Experimental\nresults indicate that DiffRO significantly improves the pronunciation accuracy\nof the TTS system, achieving state-of-the-art (SOTA) WER results on the\nseed-tts-eval benchmark. Moreover, with the integration of the MTR model, we\ndemonstrate the ability to control emotional and quality attributes in a\nzero-shot manner."}
{"id": "2507.06048", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06048", "abs": "https://arxiv.org/abs/2507.06048", "authors": ["Aseel Qsibat", "Habiba Akhleifa", "Abdelhamid Salem", "Khaled Rabie", "Xingwang Li", "Thokozani Shongwe", "Mohamad A. Alawad", "Yazeed Alkhrijah"], "title": "Secure Communication of UAV-mounted STAR-RIS under Phase Shift Errors", "comment": null, "summary": "This paper investigates the secure communication capabilities of a\nnon-orthogonal multiple access (NOMA) network supported by a STAR-RIS\n(simultaneously transmitting and reflecting reconfigurable intelligent surface)\ndeployed on an unmanned aerial vehicle (UAV), in the presence of passive\neavesdroppers. The STAR-RIS facilitates concurrent signal reflection and\ntransmission, allowing multiple legitimate users-grouped via NOMA-to be served\nefficiently, thereby improving spectral utilization. Each user contends with an\nassociated eavesdropper, creating a stringent security scenario. Under Nakagami\nfading conditions and accounting for phase shift inaccuracies in the STAR-RIS,\nclosed-form expressions for the ergodic secrecy rates of users in both\ntransmission and reflection paths are derived. An optimization framework is\nthen developed to jointly adjust the UAV's positioning and the STAR-RIS power\nsplitting coefficient, aiming to maximize the system's secrecy rate. The\nproposed approach enhances secure transmission in STAR-RIS-NOMA configurations\nunder realistic hardware constraints and offers valuable guidance for the\ndesign of future 6G wireless networks."}
{"id": "2507.06179", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06179", "abs": "https://arxiv.org/abs/2507.06179", "authors": ["Mohamed Elminshawi", "Srikanth Raj Chetupalli", "Emanuël A. P. Habets"], "title": "Dynamic Slimmable Networks for Efficient Speech Separation", "comment": "This manuscript has been submitted to IEEE Transactions on Audio,\n  Speech and Language Processing", "summary": "Recent progress in speech separation has been largely driven by advances in\ndeep neural networks, yet their high computational and memory requirements\nhinder deployment on resource-constrained devices. A significant inefficiency\nin conventional systems arises from using static network architectures that\nmaintain constant computational complexity across all input segments,\nregardless of their characteristics. This approach is sub-optimal for simpler\nsegments that do not require intensive processing, such as silence or\nnon-overlapping speech. To address this limitation, we propose a dynamic\nslimmable network (DSN) for speech separation that adaptively adjusts its\ncomputational complexity based on the input signal. The DSN combines a\nslimmable network, which can operate at different network widths, with a\nlightweight gating module that dynamically determines the required width by\nanalyzing the local input characteristics. To balance performance and\nefficiency, we introduce a signal-dependent complexity loss that penalizes\nunnecessary computation based on segmental reconstruction error. Experiments on\nclean and noisy two-speaker mixtures from the WSJ0-2mix and WHAM! datasets show\nthat the DSN achieves a better performance-efficiency trade-off than\nindividually trained static networks of different sizes."}
{"id": "2507.06070", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06070", "abs": "https://arxiv.org/abs/2507.06070", "authors": ["Christos Nikou", "Theodoros Giannakopoulos"], "title": "Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol", "comment": "International Journal of Music Science, Technology and Art, 15 pages,\n  7 figures", "summary": "Recent advances in song identification leverage deep neural networks to learn\ncompact audio fingerprints directly from raw waveforms. While these methods\nperform well under controlled conditions, their accuracy drops significantly in\nreal-world scenarios where the audio is captured via mobile devices in noisy\nenvironments. In this paper, we introduce a novel evaluation protocol designed\nto better reflect such real-world conditions. We generate three recordings of\nthe same audio, each with increasing levels of noise, captured using a mobile\ndevice's microphone. Our results reveal a substantial performance drop for two\nstate-of-the-art CNN-based models under this protocol, compared to previously\nreported benchmarks. Additionally, we highlight the critical role of the\naugmentation pipeline during training with contrastive loss. By introduction\nlow pass and high pass filters in the augmentation pipeline we significantly\nincrease the performance of both systems in our proposed evaluation.\nFurthermore, we develop a transformer-based model with a tailored projection\nmodule and demonstrate that transferring knowledge from a semantically relevant\ndomain yields a more robust solution. The transformer architecture outperforms\nCNN-based models across all noise levels, and query durations. In low noise\nconditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in\nfinding the correct song, surpassing by 14%, and by 18.5% the second-best\nperforming model, respectively, Under heavy noise levels, we achieve a\ndetection rate 56.5% for 15-second query duration. All experiments are\nconducted on public large-scale dataset of over 100K songs, with queries\nmatched against a database of 56 million vectors."}
{"id": "2507.06066", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06066", "abs": "https://arxiv.org/abs/2507.06066", "authors": ["Yuelong Qiu", "Di Wu", "Yong Zeng", "Yanqun Tang", "Nan Cheng", "Chenhao Qi"], "title": "AI-based Environment-Aware XL-MIMO Channel Estimation with Location-Specific Prior Knowledge Enabled by CKM", "comment": "13 pages, 11 figures, 1 table, Under review at IEEE Transactions on\n  Communications", "summary": "Accurate and efficient acquisition of wireless channel state information\n(CSI) is crucial to enhance the communication performance of wireless systems.\nHowever, with the continuous densification of wireless links, increased channel\ndimensions, and the use of higher-frequency bands, channel estimation in the\nsixth generation (6G) and beyond wireless networks faces new challenges, such\nas insufficient orthogonal pilot sequences, inadequate signal-to-noise ratio\n(SNR) for channel training, and more sophisticated channel statistical\ndistributions in complex environment. These challenges pose significant\ndifficulties for classical channel estimation algorithms like least squares\n(LS) and maximum a posteriori (MAP). To address this problem, we propose a\nnovel environment-aware channel estimation framework with location-specific\nprior channel distribution enabled by the new concept of channel knowledge map\n(CKM). To this end, we propose a new type of CKM called channel score function\nmap (CSFM), which learns the channel probability density function (PDF) using\nartificial intelligence (AI) techniques. To fully exploit the prior information\nin CSFM, we propose a plug-and-play (PnP) based algorithm to decouple the\nregularized MAP channel estimation problem, thereby reducing the complexity of\nthe optimization process. Besides, we employ Tweedie's formula to establish a\nconnection between the channel score function, defined as the logarithmic\ngradient of the channel PDF, and the channel denoiser. This allows the use of\nthe high-precision, environment-aware channel denoiser from the CSFM to\napproximate the channel score function, thus enabling efficient processing of\nthe decoupled channel statistical components. Simulation results show that the\nproposed CSFM-PnP based channel estimation technique significantly outperforms\nthe conventional techniques in the aforementioned challenging scenarios."}
{"id": "2507.05657", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05657", "abs": "https://arxiv.org/abs/2507.05657", "authors": ["Manan Mittal", "Ryan M. Corey", "Andrew C. Singer"], "title": "Adaptive Linearly Constrained Minimum Variance Volumetric Active Noise Control", "comment": "5 pages, 6 figures", "summary": "Traditional volumetric noise control typically relies on multipoint error\nminimization to suppress sound energy across a region, but offers limited\nflexibility in shaping spatial responses. This paper introduces a time-domain\nformulation for linearly constrained minimum variance active noise control\n(LCMV ANC) for spatial control filter design. We demonstrate how the LCMV ANC\noptimization framework allows system designers to prioritize noise reduction at\nspecific spatial locations through strategically defined linear constraints,\nproviding a more flexible alternative to uniformly weighted multipoint error\nminimization. An adaptive algorithm based on filtered-X least mean squares\n(FxLMS) is derived for online adaptation of filter coefficients. Simulation and\nexperimental results validate the proposed method's noise reduction and\nconstraint adherence, demonstrating effective, spatially selective, and\nbroadband noise control compared to multipoint volumetric noise control."}
{"id": "2507.06116", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06116", "abs": "https://arxiv.org/abs/2507.06116", "authors": ["Xintong Hu", "Yixuan Chen", "Rui Yang", "Wenxiang Guo", "Changhao Pan"], "title": "Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis", "comment": null, "summary": "Automatic speech quality assessment plays a crucial role in the development\nof speech synthesis systems, but existing models exhibit significant\nperformance variations across different granularity levels of prediction tasks.\nThis paper proposes an enhanced MOS prediction system based on self-supervised\nlearning speech models, incorporating a Mixture of Experts (MoE) classification\nhead and utilizing synthetic data from multiple commercial generation models\nfor data augmentation. Our method builds upon existing self-supervised models\nsuch as wav2vec2, designing a specialized MoE architecture to address different\ntypes of speech quality assessment tasks. We also collected a large-scale\nsynthetic speech dataset encompassing the latest text-to-speech, speech\nconversion, and speech enhancement systems. However, despite the adoption of\nthe MoE architecture and expanded dataset, the model's performance improvements\nin sentence-level prediction tasks remain limited. Our work reveals the\nlimitations of current methods in handling sentence-level quality assessment,\nprovides new technical pathways for the field of automatic speech quality\nassessment, and also delves into the fundamental causes of performance\ndifferences across different assessment granularities."}
{"id": "2507.05635", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05635", "abs": "https://arxiv.org/abs/2507.05635", "authors": ["Md. Mahbub Hasan", "Md Rakibul Hasan", "Md Zakir Hossain", "Tom Gedeon"], "title": "Frequency-Specific Neural Response and Cross-Correlation Analysis of Envelope Following Responses to Native Speech and Music Using Multichannel EEG Signals: A Case Study", "comment": null, "summary": "Although native speech and music envelope following responses (EFRs) play a\ncrucial role in auditory processing and cognition, their frequency profile,\nsuch as the dominating frequency and spectral coherence, is largely unknown. We\nhave assumed that the auditory pathway - which transmits envelope components of\nspeech and music to the scalp through time-varying neurophysiological processes\n- is a linear time-varying system, with the envelope and the multi-channel EEG\nresponses as excitation and response, respectively. This paper investigates the\ntransfer function of this system through two analytical techniques -\ntime-averaged spectral responses and cross-spectral density - in the frequency\ndomain at four different positions of the human scalp. Our findings suggest\nthat alpha (8-11 Hz), lower gamma (53-56 Hz), and higher gamma (78-81 Hz) bands\nare the peak responses of the system. These frequently appearing dominant\nfrequency responses may be the key components of familiar speech perception,\nmaintaining attention, binding acoustic features, and memory processing. The\ncross-spectral density, which reflects the spatial neural coherence of the\nhuman brain, shows that 10-13 Hz, 27-29 Hz, and 62-64 Hz are common for all\nchannel pairs. As neural coherences are frequently observed in these\nfrequencies among native participants, we suggest that these distributed neural\nprocesses are also dominant in native speech and music perception."}
{"id": "2507.05662", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05662", "abs": "https://arxiv.org/abs/2507.05662", "authors": ["Manan Mittal", "Ryan M. Corey", "Andrew C. Singer"], "title": "Beamforming with Random Projections: Upper and Lower Bounds", "comment": "5 pages, 3 figures", "summary": "Beamformers often trade off white noise gain against the ability to suppress\ninterferers. With distributed microphone arrays, this trade-off becomes crucial\nas different arrays capture vastly different magnitude and phase differences\nfor each source. We propose the use of multiple random projections as a\nfirst-stage preprocessing scheme in a data-driven approach to dimensionality\nreduction and beamforming. We show that a mixture beamformer derived from the\nuse of multiple such random projections can effectively outperform the minimum\nvariance distortionless response (MVDR) beamformer in terms of signal-to-noise\nratio (SNR) and signal-to-interferer-and-noise ratio (SINR) gain. Moreover, our\nmethod introduces computational complexity as a trade-off in the design of\nadaptive beamformers, alongside noise gain and interferer suppression. This\nadded degree of freedom allows the algorithm to better exploit the inherent\nstructure of the received signal and achieve better real-time performance while\nrequiring fewer computations. Finally, we derive upper and lower bounds for the\noutput power of the compressed beamformer when compared to the full complexity\nMVDR beamformer."}
{"id": "2507.05396", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05396", "abs": "https://arxiv.org/abs/2507.05396", "authors": ["Juliette Florin"], "title": "Comparative Analysis of Finite Difference and Finite Element Method for Audio Waveform Simulation", "comment": "46 pages, 38 figures, Link to the source code:\n  https://gitlab.eurecom.fr/florin/models", "summary": "In many industries, including aerospace and defense, waveform analysis is\ncommonly conducted to compute the resonance of physical objects, with the\nFinite Element Method (FEM) being the standard approach. The Finite Difference\nMethod (FDM) is seldom used, and this preference is often stated without formal\njustification in the literature. In this work, the accuracy, feasibility, and\ntime of simulation of FEM and FDM are compared by simulating the vibration of a\nguitar string. Python simulations for both methods are implemented, and their\nresults are compared against analytical solutions and experimental data.\nAdditionally, FDM is applied to analyze the sound of a cycling bell to assess\nits reliability compared to a real cycling bell. Final results show that both\nFEM and FDM yield similar error margins and accurately predict the system's\nbehavior. Moreover, the errors from FEM and FDM follow the same periodicity\nwith a phase shift when varying the assumed analytical tension and without a\nphase shift when changing the time interval. However, FEM converges faster with\nincreasing mesh complexity, whereas FDM demonstrates quicker computational\nperformance and achieves stable solutions even with bigger time intervals.\nDespite this FDM is limited to simpler configurations and often demands\nextensive mathematical formulation, which can become cumbersome for intricate\nshapes. For example, modeling a hemispherical object using FDM results in\nsignificant simulation times and big calculations. In conclusion, while FDM may\noffer faster convergence and computation time in certain cases, FEM remains the\npreferred method in industrial contexts due to its flexibility, scalability,\nand ease of implementation for complex geometries."}
{"id": "2507.05729", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05729", "abs": "https://arxiv.org/abs/2507.05729", "authors": ["Katsuhiko Yamamoto", "Koichi Miyazaki"], "title": "Non-Intrusive Binaural Speech Intelligibility Prediction Using Mamba for Hearing-Impaired Listeners", "comment": "Accepted by INTERSPEECH 2025", "summary": "Speech intelligibility prediction (SIP) models have been used as objective\nmetrics to assess intelligibility for hearing-impaired (HI) listeners. In the\nClarity Prediction Challenge 2 (CPC2), non-intrusive binaural SIP models based\non transformers showed high prediction accuracy. However, the self-attention\nmechanism theoretically incurs high computational and memory costs, making it a\nbottleneck for low-latency, power-efficient devices. This may also degrade the\ntemporal processing of binaural SIPs. Therefore, we propose Mamba-based SIP\nmodels instead of transformers for the temporal processing blocks. Experimental\nresults show that our proposed SIP model achieves competitive performance\ncompared to the baseline while maintaining a relatively small number of\nparameters. Our analysis suggests that the SIP model based on bidirectional\nMamba effectively captures contextual and spatial speech information from\nbinaural signals."}
{"id": "2507.05402", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05402", "abs": "https://arxiv.org/abs/2507.05402", "authors": ["Srikanth Korse", "Andreas Walther", "Emanuel A. P. Habets"], "title": "Stereo Reproduction in the Presence of Sample Rate Offsets", "comment": "Accepted to WASPAA 2025", "summary": "One of the main challenges in synchronizing wirelessly connected loudspeakers\nfor spatial audio reproduction is clock skew. Clock skew arises from sample\nrate offsets ( SROs) between the loudspeakers, caused by the use of independent\ndevice clocks. While network-based protocols like Precision Time Protocol (PTP)\nand Network Time Protocol (NTP) are explored, the impact of SROs on spatial\naudio reproduction and its perceptual consequences remains underexplored. We\npropose an audio-domain SRO compensation method using spatial filtering to\nisolate loudspeaker contributions. These filtered signals, along with the\noriginal playback signal, are used to estimate the SROs, and their influence is\ncompensated for prior to spatial audio reproduction. We evaluate the effect of\nthe compensation method in a subjective listening test. The results of these\ntests as well as objective metrics demonstrate that the proposed method\nmitigates the perceptual degradation introduced by SROs by preserving the\nspatial cues."}
{"id": "2507.05900", "categories": ["cs.SD", "cs.LG", "eess.AS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.05900", "abs": "https://arxiv.org/abs/2507.05900", "authors": ["Zengjing Chen", "Lu Wang", "Chengzhi Xing"], "title": "Stable Acoustic Relay Assignment with High Throughput via Lase Chaos-based Reinforcement Learning", "comment": null, "summary": "This study addresses the problem of stable acoustic relay assignment in an\nunderwater acoustic network. Unlike the objectives of most existing literature,\ntwo distinct objectives, namely classical stable arrangement and ambiguous\nstable arrangement, are considered. To achieve these stable arrangements, a\nlaser chaos-based multi-processing learning (LC-ML) method is introduced to\nefficiently obtain high throughput and rapidly attain stability. In order to\nsufficiently explore the relay's decision-making, this method uses random\nnumbers generated by laser chaos to learn the assignment of relays to multiple\nsource nodes. This study finds that the laser chaos-based random number and\nmulti-processing in the exchange process have a positive effect on higher\nthroughput and strong adaptability with environmental changing over time.\nMeanwhile, ambiguous cognitions result in the stable configuration with less\nvolatility compared to accurate ones. This provides a practical and useful\nmethod and can be the basis for relay selection in complex underwater\nenvironments."}
{"id": "2507.05635", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05635", "abs": "https://arxiv.org/abs/2507.05635", "authors": ["Md. Mahbub Hasan", "Md Rakibul Hasan", "Md Zakir Hossain", "Tom Gedeon"], "title": "Frequency-Specific Neural Response and Cross-Correlation Analysis of Envelope Following Responses to Native Speech and Music Using Multichannel EEG Signals: A Case Study", "comment": null, "summary": "Although native speech and music envelope following responses (EFRs) play a\ncrucial role in auditory processing and cognition, their frequency profile,\nsuch as the dominating frequency and spectral coherence, is largely unknown. We\nhave assumed that the auditory pathway - which transmits envelope components of\nspeech and music to the scalp through time-varying neurophysiological processes\n- is a linear time-varying system, with the envelope and the multi-channel EEG\nresponses as excitation and response, respectively. This paper investigates the\ntransfer function of this system through two analytical techniques -\ntime-averaged spectral responses and cross-spectral density - in the frequency\ndomain at four different positions of the human scalp. Our findings suggest\nthat alpha (8-11 Hz), lower gamma (53-56 Hz), and higher gamma (78-81 Hz) bands\nare the peak responses of the system. These frequently appearing dominant\nfrequency responses may be the key components of familiar speech perception,\nmaintaining attention, binding acoustic features, and memory processing. The\ncross-spectral density, which reflects the spatial neural coherence of the\nhuman brain, shows that 10-13 Hz, 27-29 Hz, and 62-64 Hz are common for all\nchannel pairs. As neural coherences are frequently observed in these\nfrequencies among native participants, we suggest that these distributed neural\nprocesses are also dominant in native speech and music perception."}
{"id": "2507.05911", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.05911", "abs": "https://arxiv.org/abs/2507.05911", "authors": ["Changfeng Gao", "Zhihao Du", "Shiliang Zhang"], "title": "Differentiable Reward Optimization for LLM based TTS system", "comment": null, "summary": "This paper proposes a novel Differentiable Reward Optimization (DiffRO)\nmethod aimed at enhancing the performance of neural codec language models based\ntext-to-speech (TTS) systems. In contrast to conventional reinforcement\nlearning from human feedback (RLHF) approaches applied to TTS, DiffRO directly\ncompute the rewards based on neural codec tokens, rather than relying on\nsynthesized audio. Furthermore, we employ the Gumbel-Softmax technique to\nrender the reward function differentiable, thereby streamlining the RLHF\ntraining process. Additionally, we introduce a multi-task reward (MTR) model\nwhich can provide feedback from different perspectives and find that it can\naugment the system's capability to follow instructions effectively.Experimental\nresults indicate that DiffRO significantly improves the pronunciation accuracy\nof the TTS system, achieving state-of-the-art (SOTA) WER results on the\nseed-tts-eval benchmark. Moreover, with the integration of the MTR model, we\ndemonstrate the ability to control emotional and quality attributes in a\nzero-shot manner."}
{"id": "2507.05688", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05688", "abs": "https://arxiv.org/abs/2507.05688", "authors": ["Liang Xu", "Longfei Felix Yan", "W. Bastiaan Kleijn"], "title": "Robust One-step Speech Enhancement via Consistency Distillation", "comment": "Accepted to IEEE WASPAA 2025. 6 pages, 1 figures", "summary": "Diffusion models have shown strong performance in speech enhancement, but\ntheir real-time applicability has been limited by multi-step iterative\nsampling. Consistency distillation has recently emerged as a promising\nalternative by distilling a one-step consistency model from a multi-step\ndiffusion-based teacher model. However, distilled consistency models are\ninherently biased towards the sampling trajectory of the teacher model, making\nthem less robust to noise and prone to inheriting inaccuracies from the teacher\nmodel. To address this limitation, we propose ROSE-CD: Robust One-step Speech\nEnhancement via Consistency Distillation, a novel approach for distilling a\none-step consistency model. Specifically, we introduce a randomized learning\ntrajectory to improve the model's robustness to noise. Furthermore, we jointly\noptimize the one-step model with two time-domain auxiliary losses, enabling it\nto recover from teacher-induced errors and surpass the teacher model in overall\nperformance. This is the first pure one-step consistency distillation model for\ndiffusion-based speech enhancement, achieving 54 times faster inference speed\nand superior performance compared to its 30-step teacher model. Experiments on\nthe VoiceBank-DEMAND dataset demonstrate that the proposed model achieves\nstate-of-the-art performance in terms of speech quality. Moreover, its\ngeneralization ability is validated on both an out-of-domain dataset and\nreal-world noisy recordings."}
{"id": "2507.06070", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06070", "abs": "https://arxiv.org/abs/2507.06070", "authors": ["Christos Nikou", "Theodoros Giannakopoulos"], "title": "Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol", "comment": "International Journal of Music Science, Technology and Art, 15 pages,\n  7 figures", "summary": "Recent advances in song identification leverage deep neural networks to learn\ncompact audio fingerprints directly from raw waveforms. While these methods\nperform well under controlled conditions, their accuracy drops significantly in\nreal-world scenarios where the audio is captured via mobile devices in noisy\nenvironments. In this paper, we introduce a novel evaluation protocol designed\nto better reflect such real-world conditions. We generate three recordings of\nthe same audio, each with increasing levels of noise, captured using a mobile\ndevice's microphone. Our results reveal a substantial performance drop for two\nstate-of-the-art CNN-based models under this protocol, compared to previously\nreported benchmarks. Additionally, we highlight the critical role of the\naugmentation pipeline during training with contrastive loss. By introduction\nlow pass and high pass filters in the augmentation pipeline we significantly\nincrease the performance of both systems in our proposed evaluation.\nFurthermore, we develop a transformer-based model with a tailored projection\nmodule and demonstrate that transferring knowledge from a semantically relevant\ndomain yields a more robust solution. The transformer architecture outperforms\nCNN-based models across all noise levels, and query durations. In low noise\nconditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in\nfinding the correct song, surpassing by 14%, and by 18.5% the second-best\nperforming model, respectively, Under heavy noise levels, we achieve a\ndetection rate 56.5% for 15-second query duration. All experiments are\nconducted on public large-scale dataset of over 100K songs, with queries\nmatched against a database of 56 million vectors."}
{"id": "2507.05727", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.05727", "abs": "https://arxiv.org/abs/2507.05727", "authors": ["He Wang", "Linhan Ma", "Dake Guo", "Xiong Wang", "Lei Xie", "Jin Xu", "Junyang Lin"], "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark", "comment": "18 pages, 4 figures", "summary": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench."}
{"id": "2507.06116", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06116", "abs": "https://arxiv.org/abs/2507.06116", "authors": ["Xintong Hu", "Yixuan Chen", "Rui Yang", "Wenxiang Guo", "Changhao Pan"], "title": "Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis", "comment": null, "summary": "Automatic speech quality assessment plays a crucial role in the development\nof speech synthesis systems, but existing models exhibit significant\nperformance variations across different granularity levels of prediction tasks.\nThis paper proposes an enhanced MOS prediction system based on self-supervised\nlearning speech models, incorporating a Mixture of Experts (MoE) classification\nhead and utilizing synthetic data from multiple commercial generation models\nfor data augmentation. Our method builds upon existing self-supervised models\nsuch as wav2vec2, designing a specialized MoE architecture to address different\ntypes of speech quality assessment tasks. We also collected a large-scale\nsynthetic speech dataset encompassing the latest text-to-speech, speech\nconversion, and speech enhancement systems. However, despite the adoption of\nthe MoE architecture and expanded dataset, the model's performance improvements\nin sentence-level prediction tasks remain limited. Our work reveals the\nlimitations of current methods in handling sentence-level quality assessment,\nprovides new technical pathways for the field of automatic speech quality\nassessment, and also delves into the fundamental causes of performance\ndifferences across different assessment granularities."}

{"id": "2507.06249", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06249", "abs": "https://arxiv.org/abs/2507.06249", "authors": ["Saierdaer Yusuyin", "Te Ma", "Hao Huang", "Zhijian Ou"], "title": "Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation", "comment": "submitted to IEEE TASLP", "summary": "Recently, pre-trained models with phonetic supervision have demonstrated\ntheir advantages for crosslingual speech recognition in data efficiency and\ninformation sharing across languages. However, a limitation is that a\npronunciation lexicon is needed for such phoneme-based crosslingual speech\nrecognition. In this study, we aim to eliminate the need for pronunciation\nlexicons and propose a latent variable model based method, with phonemes being\ntreated as discrete latent variables. The new method consists of a\nspeech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a\ngrapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model.\nTo jointly train the three models, we utilize the joint stochastic\napproximation (JSA) algorithm, which is a stochastic extension of the EM\n(expectation-maximization) algorithm and has demonstrated superior performance\nparticularly in estimating discrete latent variable models. Based on the\nWhistle multilingual pre-trained S2P model, crosslingual experiments are\nconducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of\nphoneme supervision, the new method, JSA-SPG, achieves 5\\% error rate\nreductions compared to the best crosslingual fine-tuning approach using subword\nor full phoneme supervision. Furthermore, it is found that in language domain\nadaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms\nthe standard practice of language model fusion via the auxiliary support of the\nG2P model by 9% error rate reductions. To facilitate reproducibility and\nencourage further exploration in this field, we open-source the JSA-SPG\ntraining code and complete pipeline."}
{"id": "2507.06470", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.06470", "abs": "https://arxiv.org/abs/2507.06470", "authors": ["Nicholas Klein", "Hemlata Tak", "Elie Khoury"], "title": "Open-Set Source Tracing of Audio Deepfake Systems", "comment": "Accepted by INTERSPEECH 2025 as part of the special session \"Source\n  Tracing: The Origins of Synthetic or Manipulated Speech\"", "summary": "Existing research on source tracing of audio deepfake systems has focused\nprimarily on the closed-set scenario, while studies that evaluate open-set\nperformance are limited to a small number of unseen systems. Due to the large\nnumber of emerging audio deepfake systems, robust open-set source tracing is\ncritical. We leverage the protocol of the Interspeech 2025 special session on\nsource tracing to evaluate methods for improving open-set source tracing\nperformance. We introduce a novel adaptation to the energy score for\nout-of-distribution (OOD) detection, softmax energy (SME). We find that\nreplacing the typical temperature-scaled energy score with SME provides a\nrelative average improvement of 31% in the standard FPR95 (false positive rate\nat true positive rate of 95%) measure. We further explore SME-guided training\nas well as copy synthesis, codec, and reverberation augmentations, yielding an\nFPR95 of 8.3%."}
{"id": "2507.06566", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06566", "abs": "https://arxiv.org/abs/2507.06566", "authors": ["Srikanth Korse", "Mohamed Elminshawi", "Emanuel A. P. Habets", "Srikanth Raj Chetupalli"], "title": "Training Strategies for Modality Dropout Resilient Multi-Modal Target Speaker Extraction", "comment": "Published in ICASSPW 2024 (HSCMA)", "summary": "The primary goal of multi-modal TSE (MTSE) is to extract a target speaker\nfrom a speech mixture using complementary information from different\nmodalities, such as audio enrolment and visual feeds corresponding to the\ntarget speaker. MTSE systems are expected to perform well even when one of the\nmodalities is unavailable. In practice, the systems often suffer from modality\ndominance, where one of the modalities outweighs the others, thereby limiting\nrobustness. Our study investigates training strategies and the effect of\narchitectural choices, particularly the normalization layers, in yielding a\nrobust MTSE system in both non-causal and causal configurations. In particular,\nwe propose the use of modality dropout training (MDT) as a superior strategy to\nstandard and multi-task training (MTT) strategies. Experiments conducted on\ntwo-speaker mixtures from the LRS3 dataset show the MDT strategy to be\neffective irrespective of the employed normalization layer. In contrast, the\nmodels trained with the standard and MTT strategies are susceptible to modality\ndominance, and their performance depends on the chosen normalization layer.\nAdditionally, we demonstrate that the system trained with MDT strategy is\nrobust to using extracted speech as the enrollment signal, highlighting its\npotential applicability in scenarios where the target speaker is not enrolled."}
{"id": "2507.06917", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06917", "abs": "https://arxiv.org/abs/2507.06917", "authors": ["Noah Jaffe", "John Ashley Burgoyne"], "title": "Musical Source Separation Bake-Off: Comparing Objective Metrics with Human Perception", "comment": null, "summary": "Music source separation aims to extract individual sound sources (e.g.,\nvocals, drums, guitar) from a mixed music recording. However, evaluating the\nquality of separated audio remains challenging, as commonly used metrics like\nthe source-to-distortion ratio (SDR) do not always align with human perception.\nIn this study, we conducted a large-scale listener evaluation on the MUSDB18\ntest set, collecting approximately 30 ratings per track from seven distinct\nlistener groups. We compared several objective energy-ratio metrics, including\nlegacy measures (BSSEval v4, SI-SDR variants), and embedding-based alternatives\n(Frechet Audio Distance using CLAP-LAION-music, EnCodec, VGGish, Wave2Vec2, and\nHuBERT). While SDR remains the best-performing metric for vocal estimates, our\nresults show that the scale-invariant signal-to-artifacts ratio (SI-SAR) better\npredicts listener ratings for drums and bass stems. Frechet Audio Distance\n(FAD) computed with the CLAP-LAION-music embedding also performs\ncompetitively--achieving Kendall's tau values of 0.25 for drums and 0.19 for\nbass--matching or surpassing energy-based metrics for those stems. However,\nnone of the embedding-based metrics, including CLAP, correlate positively with\nhuman perception for vocal estimates. These findings highlight the need for\nstem-specific evaluation strategies and suggest that no single metric reliably\nreflects perceptual quality across all source types. We release our raw\nlistener ratings to support reproducibility and further research."}
{"id": "2507.06329", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06329", "abs": "https://arxiv.org/abs/2507.06329", "authors": ["Michael Clemens", "Ana Marasović"], "title": "MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing", "comment": "Published at COLM 2025. Code and dataset are available here\n  http://mclemcrew.github.io/mixassist-website", "summary": "While AI presents significant potential for enhancing music mixing and\nmastering workflows, current research predominantly emphasizes end-to-end\nautomation or generation, often overlooking the collaborative and instructional\ndimensions vital for co-creative processes. This gap leaves artists,\nparticularly amateurs seeking to develop expertise, underserved. To bridge\nthis, we introduce MixAssist, a novel audio-language dataset capturing the\nsituated, multi-turn dialogue between expert and amateur music producers during\ncollaborative mixing sessions. Comprising 431 audio-grounded conversational\nturns derived from 7 in-depth sessions involving 12 producers, MixAssist\nprovides a unique resource for training and evaluating audio-language models\nthat can comprehend and respond to the complexities of real-world music\nproduction dialogues. Our evaluations, including automated LLM-as-a-judge\nassessments and human expert comparisons, demonstrate that fine-tuning models\nsuch as Qwen-Audio on MixAssist can yield promising results, with Qwen\nsignificantly outperforming other tested models in generating helpful,\ncontextually relevant mixing advice. By focusing on co-creative instruction\ngrounded in audio context, MixAssist enables the development of intelligent AI\nassistants designed to support and augment the creative process in music\nmixing."}
{"id": "2507.06588", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06588", "abs": "https://arxiv.org/abs/2507.06588", "authors": ["Zhengyu Zhang", "Neeraj Varshney", "Jelena Senic", "Raied Caromi", "Samuel Berweger", "Camillo Gentile", "Enrico M. Vitucci", "Ruisi He", "Vittorio Degli-Esposti"], "title": "Deep Learning-based Human Gesture Channel Modeling for Integrated Sensing and Communication Scenarios", "comment": null, "summary": "With the development of Integrated Sensing and Communication (ISAC) for\nSixth-Generation (6G) wireless systems, contactless human recognition has\nemerged as one of the key application scenarios. Since human gesture motion\ninduces subtle and random variations in wireless multipath propagation, how to\naccurately model human gesture channels has become a crucial issue for the\ndesign and validation of ISAC systems. To this end, this paper proposes a deep\nlearning-based human gesture channel modeling framework for ISAC scenarios, in\nwhich the human body is decomposed into multiple body parts, and the mapping\nbetween human gestures and their corresponding multipath characteristics is\nlearned from real-world measurements. Specifically, a Poisson neural network is\nemployed to predict the number of Multi-Path Components (MPCs) for each human\nbody part, while Conditional Variational Auto-Encoders (C-VAEs) are reused to\ngenerate the scattering points, which are further used to reconstruct\ncontinuous channel impulse responses and micro-Doppler signatures. Simulation\nresults demonstrate that the proposed method achieves high accuracy and\ngeneralization across different gestures and subjects, providing an\ninterpretable approach for data augmentation and the evaluation of\ngesture-based ISAC systems."}
{"id": "2507.07068", "categories": ["eess.AS", "cs.SD", "68T05", "I.2.7; I.5.1; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.07068", "abs": "https://arxiv.org/abs/2507.07068", "authors": ["Dipayan Bhadra", "Mehrab Hosain", "Fatema Alam"], "title": "Deep Feed-Forward Neural Network for Bangla Isolated Speech Recognition", "comment": "12 pages, 3 figures, 4 tables. published in Jatiya Kabi Kazi Nazrul\n  Islam University, Vol. 10 No. 1-2, 2025 https://jkkniu.edu.bd/13817-2/", "summary": "As the most important human-machine interfacing tool, an insignificant amount\nof work has been carried out on Bangla Speech Recognition compared to the\nEnglish language. Motivated by this, in this work, the performance of\nspeaker-independent isolated speech recognition systems has been implemented\nand analyzed using a dataset that is created containing both isolated Bangla\nand English spoken words. An approach using the Mel Frequency Cepstral\nCoefficient (MFCC) and Deep Feed-Forward Fully Connected Neural Network (DFFNN)\nof 7 layers as a classifier is proposed in this work to recognize isolated\nspoken words. This work shows 93.42% recognition accuracy which is better\ncompared to most of the works done previously on Bangla speech recognition\nconsidering the number of classes and dataset size."}
{"id": "2507.06481", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06481", "abs": "https://arxiv.org/abs/2507.06481", "authors": ["Changheon Han", "Yuseop Sim", "Hoin Jung", "Jiho Lee", "Hojun Lee", "Yun Seok Kang", "Sucheol Woo", "Garam Kim", "Hyung Wook Park", "Martin Byung-Guk Jun"], "title": "IMPACT: Industrial Machine Perception via Acoustic Cognitive Transformer", "comment": null, "summary": "Acoustic signals from industrial machines offer valuable insights for anomaly\ndetection, predictive maintenance, and operational efficiency enhancement.\nHowever, existing task-specific, supervised learning methods often scale poorly\nand fail to generalize across diverse industrial scenarios, whose acoustic\ncharacteristics are distinct from general audio. Furthermore, the scarcity of\naccessible, large-scale datasets and pretrained models tailored for industrial\naudio impedes community-driven research and benchmarking. To address these\nchallenges, we introduce DINOS (Diverse INdustrial Operation Sounds), a\nlarge-scale open-access dataset. DINOS comprises over 74,149 audio samples\n(exceeding 1,093 hours) collected from various industrial acoustic scenarios.\nWe also present IMPACT (Industrial Machine Perception via Acoustic Cognitive\nTransformer), a novel foundation model for industrial machine sound analysis.\nIMPACT is pretrained on DINOS in a self-supervised manner. By jointly\noptimizing utterance and frame-level losses, it captures both global semantics\nand fine-grained temporal structures. This makes its representations suitable\nfor efficient fine-tuning on various industrial downstream tasks with minimal\nlabeled data. Comprehensive benchmarking across 30 distinct downstream tasks\n(spanning four machine types) demonstrates that IMPACT outperforms existing\nmodels on 24 tasks, establishing its superior effectiveness and robustness,\nwhile providing a new performance benchmark for future research."}
{"id": "2507.06612", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06612", "abs": "https://arxiv.org/abs/2507.06612", "authors": ["Peng Jiang", "Ming Li", "Rang Liu", "Qian Liu"], "title": "Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization to Estimation", "comment": "Under review", "summary": "Cell-free integrated sensing and communication (ISAC) systems have emerged as\na promising paradigm for sixth-generation (6G) networks, enabling simultaneous\nhigh-rate data transmission and high-precision radar sensing through\ncooperative distributed access points (APs). Fully exploiting these\ncapabilities requires a unified design that bridges system-level optimization\nwith multi-target parameter estimation. This paper proposes an end-to-end graph\nlearning approach to close this gap, modeling the entire cell-free ISAC network\nas a heterogeneous graph to jointly design the AP mode selection, user\nassociation, precoding, and echo signal processing for multi-target position\nand velocity estimation. In particular, we propose two novel heterogeneous\ngraph learning frameworks: a dynamic graph learning framework and a lightweight\nmirror-based graph attention network (mirror-GAT) framework. The dynamic graph\nlearning framework employs structural and temporal attention mechanisms\nintegrated with a three-dimensional convolutional neural network (3D-CNN),\nenabling superior performance and robustness in cell-free ISAC environments.\nConversely, the mirror-GAT framework significantly reduces computational\ncomplexity and signaling overhead through a bi-level iterative structure with\nshare adjacency. Simulation results validate that both proposed\ngraph-learning-based frameworks achieve significant improvements in\nmulti-target position and velocity estimation accuracy compared to conventional\nheuristic and optimization-based designs. Particularly, the mirror-GAT\nframework demonstrates substantial reductions in computational time and\nsignaling overhead, underscoring its suitability for practical deployments."}
{"id": "2507.07087", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07087", "abs": "https://arxiv.org/abs/2507.07087", "authors": ["Klaus Brümann", "Kouei Yamaoka", "Nobutaka Ono", "Simon Doclo"], "title": "Incremental Averaging Method to Improve Graph-Based Time-Difference-of-Arrival Estimation", "comment": null, "summary": "Estimating the position of a speech source based on\ntime-differences-of-arrival (TDOAs) is often adversely affected by background\nnoise and reverberation. A popular method to estimate the TDOA between a\nmicrophone pair involves maximizing a generalized cross-correlation with phase\ntransform (GCC-PHAT) function. Since the TDOAs across different microphone\npairs satisfy consistency relations, generally only a small subset of\nmicrophone pairs are used for source position estimation. Although the set of\nmicrophone pairs is often determined based on a reference microphone, recently\na more robust method has been proposed to determine the set of microphone pairs\nby computing the minimum spanning tree (MST) of a signal graph of GCC-PHAT\nfunction reliabilities. To reduce the influence of noise and reverberation on\nthe TDOA estimation accuracy, in this paper we propose to compute the GCC-PHAT\nfunctions of the MST based on an average of multiple cross-power spectral\ndensities (CPSDs) using an incremental method. In each step of the method, we\nincrease the number of CPSDs over which we average by considering CPSDs\ncomputed indirectly via other microphones from previous steps. Using signals\nrecorded in a noisy and reverberant laboratory with an array of spatially\ndistributed microphones, the performance of the proposed method is evaluated in\nterms of TDOA estimation error and 2D source position estimation error.\nExperimental results for different source and microphone configurations and\nthree reverberation conditions show that the proposed method considering\nmultiple CPSDs improves the TDOA estimation and source position estimation\naccuracy compared to the reference microphone- and MST-based methods that rely\non a single CPSD as well as steered-response power-based source position\nestimation."}
{"id": "2507.06670", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06670", "abs": "https://arxiv.org/abs/2507.06670", "authors": ["Wenxiang Guo", "Yu Zhang", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Zhetao Chen", "Wenhao Xu", "Fei Wu", "Zhou Zhao"], "title": "STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation", "comment": "9 pages, 2 figures", "summary": "Recent breakthroughs in singing voice synthesis (SVS) have heightened the\ndemand for high-quality annotated datasets, yet manual annotation remains\nprohibitively labor-intensive and resource-intensive. Existing automatic\nsinging annotation (ASA) methods, however, primarily tackle isolated aspects of\nthe annotation pipeline. To address this fundamental challenge, we present\nSTARS, which is, to our knowledge, the first unified framework that\nsimultaneously addresses singing transcription, alignment, and refined style\nannotation. Our framework delivers comprehensive multi-level annotations\nencompassing: (1) precise phoneme-audio alignment, (2) robust note\ntranscription and temporal localization, (3) expressive vocal technique\nidentification, and (4) global stylistic characterization including emotion and\npace. The proposed architecture employs hierarchical acoustic feature\nprocessing across frame, word, phoneme, note, and sentence levels. The novel\nnon-autoregressive local acoustic encoders enable structured hierarchical\nrepresentation learning. Experimental validation confirms the framework's\nsuperior performance across multiple evaluation dimensions compared to existing\nannotation approaches. Furthermore, applications in SVS training demonstrate\nthat models utilizing STARS-annotated data achieve significantly enhanced\nperceptual naturalness and precise style control. This work not only overcomes\ncritical scalability challenges in the creation of singing datasets but also\npioneers new methodologies for controllable singing voice synthesis. Audio\nsamples are available at https://gwx314.github.io/stars-demo/."}
{"id": "2507.06805", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06805", "abs": "https://arxiv.org/abs/2507.06805", "authors": ["Osmel Martínez Rosabal", "Onel Alcaraz López", "Victoria Dala Pegorara Souto", "Richard Demo Souza", "Samuel Montejo-Sánchez", "Robert Schober", "Hirley Alves"], "title": "Wireless Energy Transfer Beamforming Optimization for Intelligent Transmitting Surface", "comment": "13 pages, 9 figures, 2 tables, submitted to IEEE Transactions on\n  Wireless Communications", "summary": "Radio frequency (RF) wireless energy transfer (WET) is a promising technology\nfor powering the growing ecosystem of Internet of Things (IoT) devices using\npower beacons (PBs). Recent research focuses on designing efficient PB\narchitectures that can support numerous antennas. In this context, PBs equipped\nwith intelligent surfaces present a promising approach, enabling physically\nlarge, reconfigurable arrays. Motivated by these advantages, this work aims to\nminimize the power consumption of a PB equipped with a passive intelligent\ntransmitting surface (ITS) and a collocated digital beamforming-based feeder to\ncharge multiple single-antenna devices. To model the PB's power consumption\naccurately, we consider power amplifiers nonlinearities, ITS control power, and\nfeeder-to-ITS air interface losses. The resulting optimization problem is\nhighly nonlinear and nonconvex due to the high-power amplifier (HPA), the\nreceived power constraints at the devices, and the unit-modulus constraint\nimposed by the phase shifter configuration of the ITS. To tackle this issue, we\napply successive convex approximation (SCA) to iteratively solve convex\nsubproblems that jointly optimize the digital precoder and phase configuration.\nGiven SCA's sensitivity to initialization, we propose an algorithm that ensures\ninitialization feasibility while balancing convergence speed and solution\nquality. We compare the proposed ITS-equipped PB's power consumption against\nbenchmark architectures featuring digital and hybrid analog-digital\nbeamforming. Results demonstrate that the proposed architecture efficiently\nscales with the number of RF chains and ITS elements. We also show that\nnonuniform ITS power distribution influences beamforming and can shift a device\nbetween near- and far-field regions, even with a constant aperture."}
{"id": "2507.06329", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06329", "abs": "https://arxiv.org/abs/2507.06329", "authors": ["Michael Clemens", "Ana Marasović"], "title": "MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing", "comment": "Published at COLM 2025. Code and dataset are available here\n  http://mclemcrew.github.io/mixassist-website", "summary": "While AI presents significant potential for enhancing music mixing and\nmastering workflows, current research predominantly emphasizes end-to-end\nautomation or generation, often overlooking the collaborative and instructional\ndimensions vital for co-creative processes. This gap leaves artists,\nparticularly amateurs seeking to develop expertise, underserved. To bridge\nthis, we introduce MixAssist, a novel audio-language dataset capturing the\nsituated, multi-turn dialogue between expert and amateur music producers during\ncollaborative mixing sessions. Comprising 431 audio-grounded conversational\nturns derived from 7 in-depth sessions involving 12 producers, MixAssist\nprovides a unique resource for training and evaluating audio-language models\nthat can comprehend and respond to the complexities of real-world music\nproduction dialogues. Our evaluations, including automated LLM-as-a-judge\nassessments and human expert comparisons, demonstrate that fine-tuning models\nsuch as Qwen-Audio on MixAssist can yield promising results, with Qwen\nsignificantly outperforming other tested models in generating helpful,\ncontextually relevant mixing advice. By focusing on co-creative instruction\ngrounded in audio context, MixAssist enables the development of intelligent AI\nassistants designed to support and augment the creative process in music\nmixing."}
{"id": "2507.06674", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06674", "abs": "https://arxiv.org/abs/2507.06674", "authors": ["Wei-Jaw Lee", "Fang-Chih Hsieh", "Xuanjun Chen", "Fang-Duo Tsai", "Yi-Hsuan Yang"], "title": "Exploring State-Space-Model based Language Model in Music Generation", "comment": "Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "summary": "The recent surge in State Space Models (SSMs), particularly the emergence of\nMamba, has established them as strong alternatives or complementary modules to\nTransformers across diverse domains. In this work, we aim to explore the\npotential of Mamba-based architectures for text-to-music generation. We adopt\ndiscrete tokens of Residual Vector Quantization (RVQ) as the modeling\nrepresentation and empirically find that a single-layer codebook can capture\nsemantic information in music. Motivated by this observation, we focus on\nmodeling a single-codebook representation and adapt SiMBA, originally designed\nas a Mamba-based encoder, to function as a decoder for sequence modeling. We\ncompare its performance against a standard Transformer-based decoder. Our\nresults suggest that, under limited-resource settings, SiMBA achieves much\nfaster convergence and generates outputs closer to the ground truth. This\ndemonstrates the promise of SSMs for efficient and expressive text-to-music\ngeneration. We put audio examples on Github."}
{"id": "2507.06833", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06833", "abs": "https://arxiv.org/abs/2507.06833", "authors": ["Haoyu Wang", "Shuangfeng Han", "Xiaoyun Wang", "Zhi Sun"], "title": "Enhancing Environment Generalizability for Deep Learning-Based CSI Feedback", "comment": null, "summary": "Accurate and low-overhead channel state information (CSI) feedback is\nessential to boost the capacity of frequency division duplex (FDD) massive\nmultiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback\nsignificantly outperforms conventional approaches. Nevertheless, current deep\nlearning-based CSI feedback algorithms exhibit limited generalizability to\nunseen environments, which obviously increases the deployment cost. In this\npaper, we first model the distribution shift of CSI across different\nenvironments, which is composed of the distribution shift of multipath\nstructure and a single-path. Then, EG-CsiNet is proposed as a novel CSI\nfeedback learning framework to enhance environment-generalizability.\nExplicitly, EG-CsiNet comprises the modules of multipath decoupling and\nfine-grained alignment, which can address the distribution shift of multipath\nstructure and a single path. Based on extensive simulations, the proposed\nEG-CsiNet can robustly enhance the generalizability in unseen environments\ncompared to the state-of-the-art, especially in challenging conditions with a\nsingle source environment."}
{"id": "2507.06481", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06481", "abs": "https://arxiv.org/abs/2507.06481", "authors": ["Changheon Han", "Yuseop Sim", "Hoin Jung", "Jiho Lee", "Hojun Lee", "Yun Seok Kang", "Sucheol Woo", "Garam Kim", "Hyung Wook Park", "Martin Byung-Guk Jun"], "title": "IMPACT: Industrial Machine Perception via Acoustic Cognitive Transformer", "comment": null, "summary": "Acoustic signals from industrial machines offer valuable insights for anomaly\ndetection, predictive maintenance, and operational efficiency enhancement.\nHowever, existing task-specific, supervised learning methods often scale poorly\nand fail to generalize across diverse industrial scenarios, whose acoustic\ncharacteristics are distinct from general audio. Furthermore, the scarcity of\naccessible, large-scale datasets and pretrained models tailored for industrial\naudio impedes community-driven research and benchmarking. To address these\nchallenges, we introduce DINOS (Diverse INdustrial Operation Sounds), a\nlarge-scale open-access dataset. DINOS comprises over 74,149 audio samples\n(exceeding 1,093 hours) collected from various industrial acoustic scenarios.\nWe also present IMPACT (Industrial Machine Perception via Acoustic Cognitive\nTransformer), a novel foundation model for industrial machine sound analysis.\nIMPACT is pretrained on DINOS in a self-supervised manner. By jointly\noptimizing utterance and frame-level losses, it captures both global semantics\nand fine-grained temporal structures. This makes its representations suitable\nfor efficient fine-tuning on various industrial downstream tasks with minimal\nlabeled data. Comprehensive benchmarking across 30 distinct downstream tasks\n(spanning four machine types) demonstrates that IMPACT outperforms existing\nmodels on 24 tasks, establishing its superior effectiveness and robustness,\nwhile providing a new performance benchmark for future research."}
{"id": "2507.06769", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.06769", "abs": "https://arxiv.org/abs/2507.06769", "authors": ["Yuancheng Luo", "Dmitriy Yamkovoy", "Guillermo Garcia"], "title": "Constraint Optimized Multichannel Mixer-limiter Design", "comment": "For submission to ICASSP 2026", "summary": "Multichannel audio mixer and limiter designs are conventionally decoupled for\ncontent reproduction over loudspeaker arrays due to high computational\ncomplexity and run-time costs. We propose a coupled mixer-limiter-envelope\ndesign formulated as an efficient linear-constrained quadratic program that\nminimizes a distortion objective over multichannel gain variables subject to\nsample mixture constraints. Novel methods for asymmetric constant overlap-add\nwindow optimization, objective function approximation, variable and constraint\nreduction are presented. Experiments demonstrate distortion reduction of the\ncoupled design, and computational trade-offs required for efficient real-time\nprocessing."}
{"id": "2507.06849", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06849", "abs": "https://arxiv.org/abs/2507.06849", "authors": ["Yizhuo Wu", "Ang Li", "Chang Gao"], "title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion", "comment": "Under Review", "summary": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD."}
{"id": "2507.06670", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06670", "abs": "https://arxiv.org/abs/2507.06670", "authors": ["Wenxiang Guo", "Yu Zhang", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Zhetao Chen", "Wenhao Xu", "Fei Wu", "Zhou Zhao"], "title": "STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation", "comment": "9 pages, 2 figures", "summary": "Recent breakthroughs in singing voice synthesis (SVS) have heightened the\ndemand for high-quality annotated datasets, yet manual annotation remains\nprohibitively labor-intensive and resource-intensive. Existing automatic\nsinging annotation (ASA) methods, however, primarily tackle isolated aspects of\nthe annotation pipeline. To address this fundamental challenge, we present\nSTARS, which is, to our knowledge, the first unified framework that\nsimultaneously addresses singing transcription, alignment, and refined style\nannotation. Our framework delivers comprehensive multi-level annotations\nencompassing: (1) precise phoneme-audio alignment, (2) robust note\ntranscription and temporal localization, (3) expressive vocal technique\nidentification, and (4) global stylistic characterization including emotion and\npace. The proposed architecture employs hierarchical acoustic feature\nprocessing across frame, word, phoneme, note, and sentence levels. The novel\nnon-autoregressive local acoustic encoders enable structured hierarchical\nrepresentation learning. Experimental validation confirms the framework's\nsuperior performance across multiple evaluation dimensions compared to existing\nannotation approaches. Furthermore, applications in SVS training demonstrate\nthat models utilizing STARS-annotated data achieve significantly enhanced\nperceptual naturalness and precise style control. This work not only overcomes\ncritical scalability challenges in the creation of singing datasets but also\npioneers new methodologies for controllable singing voice synthesis. Audio\nsamples are available at https://gwx314.github.io/stars-demo/."}
{"id": "2507.06794", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06794", "abs": "https://arxiv.org/abs/2507.06794", "authors": ["Anastasia Ananeva", "Anton Tomilov", "Marina Volkova"], "title": "Revealing the Hidden Temporal Structure of HubertSoft Embeddings based on the Russian Phonetic Corpus", "comment": "11 pages, 5 figures, Specom 2025 conference", "summary": "Self-supervised learning (SSL) models such as Wav2Vec 2.0 and HuBERT have\nshown remarkable success in extracting phonetic information from raw audio\nwithout labelled data. While prior work has demonstrated that SSL embeddings\nencode phonetic features at the frame level, it remains unclear whether these\nmodels preserve temporal structure, specifically, whether embeddings at phoneme\nboundaries reflect the identity and order of adjacent phonemes. This study\ninvestigates the extent to which boundary-sensitive embeddings from HubertSoft,\na soft-clustering variant of HuBERT, encode phoneme transitions. Using the\nCORPRES Russian speech corpus, we labelled 20 ms embedding windows with\ntriplets of phonemes corresponding to their start, centre, and end segments. A\nneural network was trained to predict these positions separately, and multiple\nevaluation metrics, such as ordered, unordered accuracy and a flexible centre\naccuracy, were used to assess temporal sensitivity. Results show that\nembeddings extracted at phoneme boundaries capture both phoneme identity and\ntemporal order, with especially high accuracy at segment boundaries. Confusion\npatterns further suggest that the model encodes articulatory detail and\ncoarticulatory effects. These findings contribute to our understanding of the\ninternal structure of SSL speech representations and their potential for\nphonological analysis and fine-grained transcription tasks."}
{"id": "2507.06904", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06904", "abs": "https://arxiv.org/abs/2507.06904", "authors": ["Yu Liu", "Qu Luo", "Gaojie Chen", "Pei Xiao", "Ahmed Elzanaty", "Mohsen Khalily", "Rahim Tafazolli"], "title": "Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA Assisted Wireless Communication Systems", "comment": null, "summary": "To address the limitations of traditional reconfigurable intelligent surfaces\n(RIS) in spatial control capability, this paper introduces the concept of the\nfluid antenna system (FAS) and proposes a fluid simultaneously transmitting and\nreflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA)\nmulti-user communication system. In this system, each FSTAR-RIS element is\ncapable of flexible mobility and can dynamically adjust its position in\nresponse to environmental variations, thereby enabling simultaneous service to\nusers in both the transmission and reflection zones. This significantly\nenhances the system's spatial degrees of freedom (DoF) and service\nadaptability. To maximize the system's weighted sum-rate, we formulate a\nnon-convex optimization problem that jointly optimizes the base station\nbeamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the\nelement positions. An alternating optimization (AO) algorithm is developed,\nincorporating successive convex approximation (SCA), semi-definite relaxation\n(SDR), and majorization-minimization (MM) techniques. In particular, to address\nthe complex channel coupling introduced by the coexistence of direct and\nFSTAR-RIS paths, the MM framework is employed in the element position\noptimization subproblem, enabling an efficient iterative solution strategy.\nSimulation results validate that the proposed system achieves up to a 27%\nincrease in total sum rate compared to traditional STAR-RIS systems and\nrequires approximately 50% fewer RIS elements to attain the same performance,\nhighlighting its effectiveness for cost-efficient large-scale deployment."}
{"id": "2507.06674", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06674", "abs": "https://arxiv.org/abs/2507.06674", "authors": ["Wei-Jaw Lee", "Fang-Chih Hsieh", "Xuanjun Chen", "Fang-Duo Tsai", "Yi-Hsuan Yang"], "title": "Exploring State-Space-Model based Language Model in Music Generation", "comment": "Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "summary": "The recent surge in State Space Models (SSMs), particularly the emergence of\nMamba, has established them as strong alternatives or complementary modules to\nTransformers across diverse domains. In this work, we aim to explore the\npotential of Mamba-based architectures for text-to-music generation. We adopt\ndiscrete tokens of Residual Vector Quantization (RVQ) as the modeling\nrepresentation and empirically find that a single-layer codebook can capture\nsemantic information in music. Motivated by this observation, we focus on\nmodeling a single-codebook representation and adapt SiMBA, originally designed\nas a Mamba-based encoder, to function as a decoder for sequence modeling. We\ncompare its performance against a standard Transformer-based decoder. Our\nresults suggest that, under limited-resource settings, SiMBA achieves much\nfaster convergence and generates outputs closer to the ground truth. This\ndemonstrates the promise of SSMs for efficient and expressive text-to-music\ngeneration. We put audio examples on Github."}
{"id": "2507.06815", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06815", "abs": "https://arxiv.org/abs/2507.06815", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michele Esposito", "Michel Dumontier"], "title": "Data-Balanced Curriculum Learning for Audio Question Answering", "comment": null, "summary": "Audio question answering (AQA) requires models to understand acoustic content\nand perform complex reasoning. Current models struggle with dataset imbalances\nand unstable training dynamics. This work combines curriculum learning with\nstatistical data balancing to address these challenges. The method labels\nquestion difficulty using language models, then trains progressively from easy\nto hard examples. Statistical filtering removes overrepresented audio\ncategories, and guided decoding constrains outputs to valid multiple-choice\nformats. Experiments on the DCASE 2025 training set and five additional public\ndatasets show that data curation improves accuracy by 11.7% over baseline\nmodels, achieving 64.2% on the DCASE 2025 benchmark."}
{"id": "2507.06932", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06932", "abs": "https://arxiv.org/abs/2507.06932", "authors": ["Lunhao Duan", "Xingyu Lu", "Yushuang Liu", "Jianchao Yang", "Hong Gu"], "title": "Precise Representation Model of SAR Saturated Interference: Mechanism and Verification", "comment": null, "summary": "Synthetic Aperture Radar (SAR) is highly susceptible to Radio Frequency\nInterference (RFI). Due to the performance limitations of components such as\ngain controllers and analog-to-digital converters in SAR receivers, high-power\ninterference can easily cause saturation of the SAR receiver, resulting in\nnonlinear distortion of the interfered echoes, which are distorted in both the\ntime domain and frequency domain. Some scholars have analyzed the impact of SAR\nreceiver saturation on target echoes through simulations. However, the\nsaturation function has non-smooth characteristics, making it difficult to\nconduct accurate analysis using traditional analytical methods. Current related\nstudies have approximated and analyzed the saturation function based on the\nhyperbolic tangent function, but there are approximation errors. Therefore,\nthis paper proposes a saturation interference analysis model based on Bessel\nfunctions, and verifies the accuracy of the proposed saturation interference\nanalysis model by simulating and comparing it with the traditional saturation\nmodel based on smooth function approximation. This model can provide certain\nguidance for further work such as saturation interference suppression."}
{"id": "2507.06769", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.06769", "abs": "https://arxiv.org/abs/2507.06769", "authors": ["Yuancheng Luo", "Dmitriy Yamkovoy", "Guillermo Garcia"], "title": "Constraint Optimized Multichannel Mixer-limiter Design", "comment": "For submission to ICASSP 2026", "summary": "Multichannel audio mixer and limiter designs are conventionally decoupled for\ncontent reproduction over loudspeaker arrays due to high computational\ncomplexity and run-time costs. We propose a coupled mixer-limiter-envelope\ndesign formulated as an efficient linear-constrained quadratic program that\nminimizes a distortion objective over multichannel gain variables subject to\nsample mixture constraints. Novel methods for asymmetric constant overlap-add\nwindow optimization, objective function approximation, variable and constraint\nreduction are presented. Experiments demonstrate distortion reduction of the\ncoupled design, and computational trade-offs required for efficient real-time\nprocessing."}
{"id": "2507.06826", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06826", "abs": "https://arxiv.org/abs/2507.06826", "authors": ["Yoshiki Masuyama", "François G. Germain", "Gordon Wichern", "Christopher Ick", "Jonathan Le Roux"], "title": "Physics-Informed Direction-Aware Neural Acoustic Fields", "comment": "Accepted to WASPAA 2025", "summary": "This paper presents a physics-informed neural network (PINN) for modeling\nfirst-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have\ndemonstrated promising performance in sound field interpolation by combining\nthe powerful modeling capability of neural networks and the physical principles\nof sound propagation. In room acoustics, PINNs have typically been trained to\nrepresent the sound pressure measured by omnidirectional microphones where the\nwave equation or its frequency-domain counterpart, i.e., the Helmholtz\nequation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial\ncharacteristics and are useful for immersive audio generation with a wide range\nof applications. In this paper, we extend the PINN framework to model FOA RIRs.\nWe derive two physics-informed priors for FOA RIRs based on the correspondence\nbetween the particle velocity and the (X, Y, Z)-channels of FOA. These priors\nassociate the predicted W-channel and other channels through their partial\nderivatives and impose the physically feasible relationship on the four\nchannels. Our experiments confirm the effectiveness of the proposed method\ncompared with a neural network without the physics-informed prior."}
{"id": "2507.06997", "categories": ["eess.SP", "cs.ET", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.06997", "abs": "https://arxiv.org/abs/2507.06997", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks", "comment": null, "summary": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity."}
{"id": "2507.06794", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06794", "abs": "https://arxiv.org/abs/2507.06794", "authors": ["Anastasia Ananeva", "Anton Tomilov", "Marina Volkova"], "title": "Revealing the Hidden Temporal Structure of HubertSoft Embeddings based on the Russian Phonetic Corpus", "comment": "11 pages, 5 figures, Specom 2025 conference", "summary": "Self-supervised learning (SSL) models such as Wav2Vec 2.0 and HuBERT have\nshown remarkable success in extracting phonetic information from raw audio\nwithout labelled data. While prior work has demonstrated that SSL embeddings\nencode phonetic features at the frame level, it remains unclear whether these\nmodels preserve temporal structure, specifically, whether embeddings at phoneme\nboundaries reflect the identity and order of adjacent phonemes. This study\ninvestigates the extent to which boundary-sensitive embeddings from HubertSoft,\na soft-clustering variant of HuBERT, encode phoneme transitions. Using the\nCORPRES Russian speech corpus, we labelled 20 ms embedding windows with\ntriplets of phonemes corresponding to their start, centre, and end segments. A\nneural network was trained to predict these positions separately, and multiple\nevaluation metrics, such as ordered, unordered accuracy and a flexible centre\naccuracy, were used to assess temporal sensitivity. Results show that\nembeddings extracted at phoneme boundaries capture both phoneme identity and\ntemporal order, with especially high accuracy at segment boundaries. Confusion\npatterns further suggest that the model encodes articulatory detail and\ncoarticulatory effects. These findings contribute to our understanding of the\ninternal structure of SSL speech representations and their potential for\nphonological analysis and fine-grained transcription tasks."}
{"id": "2507.07043", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07043", "abs": "https://arxiv.org/abs/2507.07043", "authors": ["Haris Khan", "Shumaila Asif", "Hassan Nasir"], "title": "Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation", "comment": "22 pages, 4 figures, submitted as a systematic literature review in\n  AI-based hearing assistance. (June 2025)", "summary": "The integration of artificial intelligence into hearing assistance marks a\nparadigm shift from traditional amplification-based systems to intelligent,\ncontext-aware audio processing. This systematic literature review evaluates\nadvances in AI-driven selective noise cancellation (SNC) for hearing aids,\nhighlighting technological evolution, implementation challenges, and future\nresearch directions. We synthesize findings across deep learning architectures,\nhardware deployment strategies, clinical validation studies, and user-centric\ndesign. The review traces progress from early machine learning models to\nstate-of-the-art deep networks, including Convolutional Recurrent Networks for\nreal-time inference and Transformer-based architectures for high-accuracy\nseparation. Key findings include significant gains over traditional methods,\nwith recent models achieving up to 18.3 dB SI-SDR improvement on\nnoisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and\npromising clinical outcomes. Yet, challenges remain in bridging lab-grade\nmodels with real-world deployment - particularly around power constraints,\nenvironmental variability, and personalization. Identified research gaps\ninclude hardware-software co-design, standardized evaluation protocols, and\nregulatory considerations for AI-enhanced hearing devices. Future work must\nprioritize lightweight models, continual learning, contextual-based\nclassification and clinical translation to realize transformative hearing\nsolutions for millions globally."}
{"id": "2507.07067", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07067", "abs": "https://arxiv.org/abs/2507.07067", "authors": ["Clement Ruah", "Houssem Sifaou", "Osvaldo Simeone", "Bashir M. Al-Hashimi"], "title": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference."}
{"id": "2507.06815", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.06815", "abs": "https://arxiv.org/abs/2507.06815", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michele Esposito", "Michel Dumontier"], "title": "Data-Balanced Curriculum Learning for Audio Question Answering", "comment": null, "summary": "Audio question answering (AQA) requires models to understand acoustic content\nand perform complex reasoning. Current models struggle with dataset imbalances\nand unstable training dynamics. This work combines curriculum learning with\nstatistical data balancing to address these challenges. The method labels\nquestion difficulty using language models, then trains progressively from easy\nto hard examples. Statistical filtering removes overrepresented audio\ncategories, and guided decoding constrains outputs to valid multiple-choice\nformats. Experiments on the DCASE 2025 training set and five additional public\ndatasets show that data curation improves accuracy by 11.7% over baseline\nmodels, achieving 64.2% on the DCASE 2025 benchmark."}
{"id": "2507.07046", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07046", "abs": "https://arxiv.org/abs/2507.07046", "authors": ["Shahana Yasmin Chowdhury", "Bithi Banik", "Md Tamjidul Hoque", "Shreya Banerjee"], "title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering", "comment": "17 pages, 11 figures", "summary": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of\nhuman-computer interaction (HCI) and the evolution of artificial intelligence\n(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:\nneutral, happy, sad, angry, fear, disgust, and surprise, which are trained on\nfive datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).\nThe model achieves high accuracy on individual datasets, including 97.83% on\nRAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS\nand EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,\noutperforming previously reported results. To our knowledge, no existing study\nhas evaluated a single SER model across all five benchmark datasets (i.e.,\nR+T+S+C+E) simultaneously. In our work, we introduce this comprehensive\ncombination and achieve a remarkable overall accuracy of 93.76%. These results\nconfirm the robustness and generalizability of our DCRF-BiLSTM framework across\ndiverse datasets."}
{"id": "2507.07081", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07081", "abs": "https://arxiv.org/abs/2507.07081", "authors": ["Lorenzo Pucci", "Andrea Giorgetti"], "title": "Joint Target Acquisition and Refined Position Estimation in OFDM-based ISAC Networks", "comment": "5 pages, 6 figures; This paper was presented at the 2025 IEEE SPAWC\n  conference", "summary": "This paper addresses joint target acquisition and position estimation in an\nOFDM-based integrated sensing and communication (ISAC) network with base\nstation (BS) cooperation via a fusion center. A two-stage framework is\nproposed: in the first stage, each BS computes range-angle maps to detect\ntargets and estimate coarse positions, exploiting spatial diversity. In the\nsecond stage, refined localization is performed using a cooperative maximum\nlikelihood (ML) estimator over predefined regions of interest (RoIs) within a\nshared global reference frame. Numerical results demonstrate that the proposed\napproach not only improves detection performance through BS cooperation but\nalso achieves centimeter-level localization accuracy, highlighting the\neffectiveness of the refined estimation technique."}
{"id": "2507.06826", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06826", "abs": "https://arxiv.org/abs/2507.06826", "authors": ["Yoshiki Masuyama", "François G. Germain", "Gordon Wichern", "Christopher Ick", "Jonathan Le Roux"], "title": "Physics-Informed Direction-Aware Neural Acoustic Fields", "comment": "Accepted to WASPAA 2025", "summary": "This paper presents a physics-informed neural network (PINN) for modeling\nfirst-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have\ndemonstrated promising performance in sound field interpolation by combining\nthe powerful modeling capability of neural networks and the physical principles\nof sound propagation. In room acoustics, PINNs have typically been trained to\nrepresent the sound pressure measured by omnidirectional microphones where the\nwave equation or its frequency-domain counterpart, i.e., the Helmholtz\nequation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial\ncharacteristics and are useful for immersive audio generation with a wide range\nof applications. In this paper, we extend the PINN framework to model FOA RIRs.\nWe derive two physics-informed priors for FOA RIRs based on the correspondence\nbetween the particle velocity and the (X, Y, Z)-channels of FOA. These priors\nassociate the predicted W-channel and other channels through their partial\nderivatives and impose the physically feasible relationship on the four\nchannels. Our experiments confirm the effectiveness of the proposed method\ncompared with a neural network without the physics-informed prior."}
{"id": "2507.07058", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07058", "abs": "https://arxiv.org/abs/2507.07058", "authors": ["Martin Sondermann", "Pinar Bisgin", "Niklas Tschorn", "Anja Burmann", "Christoph M. Friedrich"], "title": "Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification", "comment": "Preprint Version. Accepted at EMBC 2025", "summary": "The automated classification of phonocardiogram (PCG) recordings represents a\nsubstantial advancement in cardiovascular diagnostics. This paper presents a\nsystematic comparison of four distinct models for heart murmur detection: two\nspecialized convolutional neural networks (CNNs) and two zero-shot universal\naudio transformers (BEATs), evaluated using fixed-length and heart cycle\nnormalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart\ncycle normalization method tailored to individual cardiac rhythms is\nintroduced. The findings indicate the following AUROC values: the CNN model\nwith fixed-length windowing achieves 79.5%, the CNN model with heart cycle\nnormalization scores 75.4%, the BEATs transformer with fixed-length windowing\nachieves 65.7%, and the BEATs transformer with heart cycle normalization\nresults in 70.1%.\n  The findings indicate that physiological signal constraints, especially those\nintroduced by different normalization strategies, have a substantial impact on\nmodel performance. The research provides evidence-based guidelines for\narchitecture selection in clinical settings, emphasizing the need for a balance\nbetween accuracy and computational efficiency. Although specialized CNNs\ndemonstrate superior performance overall, the zero-shot transformer models may\noffer promising efficiency advantages during development, such as faster\ntraining and evaluation cycles, despite their lower classification accuracy.\nThese findings highlight the potential of automated classification systems to\nenhance cardiac diagnostics and improve patient care."}
{"id": "2507.06769", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.06769", "abs": "https://arxiv.org/abs/2507.06769", "authors": ["Yuancheng Luo", "Dmitriy Yamkovoy", "Guillermo Garcia"], "title": "Constraint Optimized Multichannel Mixer-limiter Design", "comment": "For submission to ICASSP 2026", "summary": "Multichannel audio mixer and limiter designs are conventionally decoupled for\ncontent reproduction over loudspeaker arrays due to high computational\ncomplexity and run-time costs. We propose a coupled mixer-limiter-envelope\ndesign formulated as an efficient linear-constrained quadratic program that\nminimizes a distortion objective over multichannel gain variables subject to\nsample mixture constraints. Novel methods for asymmetric constant overlap-add\nwindow optimization, objective function approximation, variable and constraint\nreduction are presented. Experiments demonstrate distortion reduction of the\ncoupled design, and computational trade-offs required for efficient real-time\nprocessing."}
{"id": "2507.07043", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07043", "abs": "https://arxiv.org/abs/2507.07043", "authors": ["Haris Khan", "Shumaila Asif", "Hassan Nasir"], "title": "Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation", "comment": "22 pages, 4 figures, submitted as a systematic literature review in\n  AI-based hearing assistance. (June 2025)", "summary": "The integration of artificial intelligence into hearing assistance marks a\nparadigm shift from traditional amplification-based systems to intelligent,\ncontext-aware audio processing. This systematic literature review evaluates\nadvances in AI-driven selective noise cancellation (SNC) for hearing aids,\nhighlighting technological evolution, implementation challenges, and future\nresearch directions. We synthesize findings across deep learning architectures,\nhardware deployment strategies, clinical validation studies, and user-centric\ndesign. The review traces progress from early machine learning models to\nstate-of-the-art deep networks, including Convolutional Recurrent Networks for\nreal-time inference and Transformer-based architectures for high-accuracy\nseparation. Key findings include significant gains over traditional methods,\nwith recent models achieving up to 18.3 dB SI-SDR improvement on\nnoisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and\npromising clinical outcomes. Yet, challenges remain in bridging lab-grade\nmodels with real-world deployment - particularly around power constraints,\nenvironmental variability, and personalization. Identified research gaps\ninclude hardware-software co-design, standardized evaluation protocols, and\nregulatory considerations for AI-enhanced hearing devices. Future work must\nprioritize lightweight models, continual learning, contextual-based\nclassification and clinical translation to realize transformative hearing\nsolutions for millions globally."}
{"id": "2507.07066", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07066", "abs": "https://arxiv.org/abs/2507.07066", "authors": ["Adrian S. Roman", "Iran R. Roman", "Juan P. Bello"], "title": "Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach", "comment": null, "summary": "Acoustic mapping techniques have long been used in spatial audio processing\nfor direction of arrival estimation (DoAE). Traditional beamforming methods for\nacoustic mapping, while interpretable, often rely on iterative solvers that can\nbe computationally intensive and sensitive to acoustic variability. On the\nother hand, recent supervised deep learning approaches offer feedforward speed\nand robustness but require large labeled datasets and lack interpretability.\nDespite their strengths, both methods struggle to consistently generalize\nacross diverse acoustic setups and array configurations, limiting their broader\napplicability. We introduce the Latent Acoustic Mapping (LAM) model, a\nself-supervised framework that bridges the interpretability of traditional\nmethods with the adaptability and efficiency of deep learning methods. LAM\ngenerates high-resolution acoustic maps, adapts to varying acoustic conditions,\nand operates efficiently across different microphone arrays. We assess its\nrobustness on DoAE using the LOCATA and STARSS benchmarks. LAM achieves\ncomparable or superior localization performance to existing supervised methods.\nAdditionally, we show that LAM's acoustic maps can serve as effective features\nfor supervised models, further enhancing DoAE accuracy and underscoring its\npotential to advance adaptive, high-performance sound localization systems."}
{"id": "2507.06826", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06826", "abs": "https://arxiv.org/abs/2507.06826", "authors": ["Yoshiki Masuyama", "François G. Germain", "Gordon Wichern", "Christopher Ick", "Jonathan Le Roux"], "title": "Physics-Informed Direction-Aware Neural Acoustic Fields", "comment": "Accepted to WASPAA 2025", "summary": "This paper presents a physics-informed neural network (PINN) for modeling\nfirst-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have\ndemonstrated promising performance in sound field interpolation by combining\nthe powerful modeling capability of neural networks and the physical principles\nof sound propagation. In room acoustics, PINNs have typically been trained to\nrepresent the sound pressure measured by omnidirectional microphones where the\nwave equation or its frequency-domain counterpart, i.e., the Helmholtz\nequation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial\ncharacteristics and are useful for immersive audio generation with a wide range\nof applications. In this paper, we extend the PINN framework to model FOA RIRs.\nWe derive two physics-informed priors for FOA RIRs based on the correspondence\nbetween the particle velocity and the (X, Y, Z)-channels of FOA. These priors\nassociate the predicted W-channel and other channels through their partial\nderivatives and impose the physically feasible relationship on the four\nchannels. Our experiments confirm the effectiveness of the proposed method\ncompared with a neural network without the physics-informed prior."}
{"id": "2507.07046", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07046", "abs": "https://arxiv.org/abs/2507.07046", "authors": ["Shahana Yasmin Chowdhury", "Bithi Banik", "Md Tamjidul Hoque", "Shreya Banerjee"], "title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering", "comment": "17 pages, 11 figures", "summary": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of\nhuman-computer interaction (HCI) and the evolution of artificial intelligence\n(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:\nneutral, happy, sad, angry, fear, disgust, and surprise, which are trained on\nfive datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).\nThe model achieves high accuracy on individual datasets, including 97.83% on\nRAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS\nand EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,\noutperforming previously reported results. To our knowledge, no existing study\nhas evaluated a single SER model across all five benchmark datasets (i.e.,\nR+T+S+C+E) simultaneously. In our work, we introduce this comprehensive\ncombination and achieve a remarkable overall accuracy of 93.76%. These results\nconfirm the robustness and generalizability of our DCRF-BiLSTM framework across\ndiverse datasets."}
{"id": "2507.06470", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.06470", "abs": "https://arxiv.org/abs/2507.06470", "authors": ["Nicholas Klein", "Hemlata Tak", "Elie Khoury"], "title": "Open-Set Source Tracing of Audio Deepfake Systems", "comment": "Accepted by INTERSPEECH 2025 as part of the special session \"Source\n  Tracing: The Origins of Synthetic or Manipulated Speech\"", "summary": "Existing research on source tracing of audio deepfake systems has focused\nprimarily on the closed-set scenario, while studies that evaluate open-set\nperformance are limited to a small number of unseen systems. Due to the large\nnumber of emerging audio deepfake systems, robust open-set source tracing is\ncritical. We leverage the protocol of the Interspeech 2025 special session on\nsource tracing to evaluate methods for improving open-set source tracing\nperformance. We introduce a novel adaptation to the energy score for\nout-of-distribution (OOD) detection, softmax energy (SME). We find that\nreplacing the typical temperature-scaled energy score with SME provides a\nrelative average improvement of 31% in the standard FPR95 (false positive rate\nat true positive rate of 95%) measure. We further explore SME-guided training\nas well as copy synthesis, codec, and reverberation augmentations, yielding an\nFPR95 of 8.3%."}
{"id": "2507.07043", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07043", "abs": "https://arxiv.org/abs/2507.07043", "authors": ["Haris Khan", "Shumaila Asif", "Hassan Nasir"], "title": "Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation", "comment": "22 pages, 4 figures, submitted as a systematic literature review in\n  AI-based hearing assistance. (June 2025)", "summary": "The integration of artificial intelligence into hearing assistance marks a\nparadigm shift from traditional amplification-based systems to intelligent,\ncontext-aware audio processing. This systematic literature review evaluates\nadvances in AI-driven selective noise cancellation (SNC) for hearing aids,\nhighlighting technological evolution, implementation challenges, and future\nresearch directions. We synthesize findings across deep learning architectures,\nhardware deployment strategies, clinical validation studies, and user-centric\ndesign. The review traces progress from early machine learning models to\nstate-of-the-art deep networks, including Convolutional Recurrent Networks for\nreal-time inference and Transformer-based architectures for high-accuracy\nseparation. Key findings include significant gains over traditional methods,\nwith recent models achieving up to 18.3 dB SI-SDR improvement on\nnoisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and\npromising clinical outcomes. Yet, challenges remain in bridging lab-grade\nmodels with real-world deployment - particularly around power constraints,\nenvironmental variability, and personalization. Identified research gaps\ninclude hardware-software co-design, standardized evaluation protocols, and\nregulatory considerations for AI-enhanced hearing devices. Future work must\nprioritize lightweight models, continual learning, contextual-based\nclassification and clinical translation to realize transformative hearing\nsolutions for millions globally."}
{"id": "2507.07058", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07058", "abs": "https://arxiv.org/abs/2507.07058", "authors": ["Martin Sondermann", "Pinar Bisgin", "Niklas Tschorn", "Anja Burmann", "Christoph M. Friedrich"], "title": "Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification", "comment": "Preprint Version. Accepted at EMBC 2025", "summary": "The automated classification of phonocardiogram (PCG) recordings represents a\nsubstantial advancement in cardiovascular diagnostics. This paper presents a\nsystematic comparison of four distinct models for heart murmur detection: two\nspecialized convolutional neural networks (CNNs) and two zero-shot universal\naudio transformers (BEATs), evaluated using fixed-length and heart cycle\nnormalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart\ncycle normalization method tailored to individual cardiac rhythms is\nintroduced. The findings indicate the following AUROC values: the CNN model\nwith fixed-length windowing achieves 79.5%, the CNN model with heart cycle\nnormalization scores 75.4%, the BEATs transformer with fixed-length windowing\nachieves 65.7%, and the BEATs transformer with heart cycle normalization\nresults in 70.1%.\n  The findings indicate that physiological signal constraints, especially those\nintroduced by different normalization strategies, have a substantial impact on\nmodel performance. The research provides evidence-based guidelines for\narchitecture selection in clinical settings, emphasizing the need for a balance\nbetween accuracy and computational efficiency. Although specialized CNNs\ndemonstrate superior performance overall, the zero-shot transformer models may\noffer promising efficiency advantages during development, such as faster\ntraining and evaluation cycles, despite their lower classification accuracy.\nThese findings highlight the potential of automated classification systems to\nenhance cardiac diagnostics and improve patient care."}
{"id": "2507.07068", "categories": ["eess.AS", "cs.SD", "68T05", "I.2.7; I.5.1; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.07068", "abs": "https://arxiv.org/abs/2507.07068", "authors": ["Dipayan Bhadra", "Mehrab Hosain", "Fatema Alam"], "title": "Deep Feed-Forward Neural Network for Bangla Isolated Speech Recognition", "comment": "12 pages, 3 figures, 4 tables. published in Jatiya Kabi Kazi Nazrul\n  Islam University, Vol. 10 No. 1-2, 2025 https://jkkniu.edu.bd/13817-2/", "summary": "As the most important human-machine interfacing tool, an insignificant amount\nof work has been carried out on Bangla Speech Recognition compared to the\nEnglish language. Motivated by this, in this work, the performance of\nspeaker-independent isolated speech recognition systems has been implemented\nand analyzed using a dataset that is created containing both isolated Bangla\nand English spoken words. An approach using the Mel Frequency Cepstral\nCoefficient (MFCC) and Deep Feed-Forward Fully Connected Neural Network (DFFNN)\nof 7 layers as a classifier is proposed in this work to recognize isolated\nspoken words. This work shows 93.42% recognition accuracy which is better\ncompared to most of the works done previously on Bangla speech recognition\nconsidering the number of classes and dataset size."}
{"id": "2507.07087", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07087", "abs": "https://arxiv.org/abs/2507.07087", "authors": ["Klaus Brümann", "Kouei Yamaoka", "Nobutaka Ono", "Simon Doclo"], "title": "Incremental Averaging Method to Improve Graph-Based Time-Difference-of-Arrival Estimation", "comment": null, "summary": "Estimating the position of a speech source based on\ntime-differences-of-arrival (TDOAs) is often adversely affected by background\nnoise and reverberation. A popular method to estimate the TDOA between a\nmicrophone pair involves maximizing a generalized cross-correlation with phase\ntransform (GCC-PHAT) function. Since the TDOAs across different microphone\npairs satisfy consistency relations, generally only a small subset of\nmicrophone pairs are used for source position estimation. Although the set of\nmicrophone pairs is often determined based on a reference microphone, recently\na more robust method has been proposed to determine the set of microphone pairs\nby computing the minimum spanning tree (MST) of a signal graph of GCC-PHAT\nfunction reliabilities. To reduce the influence of noise and reverberation on\nthe TDOA estimation accuracy, in this paper we propose to compute the GCC-PHAT\nfunctions of the MST based on an average of multiple cross-power spectral\ndensities (CPSDs) using an incremental method. In each step of the method, we\nincrease the number of CPSDs over which we average by considering CPSDs\ncomputed indirectly via other microphones from previous steps. Using signals\nrecorded in a noisy and reverberant laboratory with an array of spatially\ndistributed microphones, the performance of the proposed method is evaluated in\nterms of TDOA estimation error and 2D source position estimation error.\nExperimental results for different source and microphone configurations and\nthree reverberation conditions show that the proposed method considering\nmultiple CPSDs improves the TDOA estimation and source position estimation\naccuracy compared to the reference microphone- and MST-based methods that rely\non a single CPSD as well as steered-response power-based source position\nestimation."}
{"id": "2507.07066", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07066", "abs": "https://arxiv.org/abs/2507.07066", "authors": ["Adrian S. Roman", "Iran R. Roman", "Juan P. Bello"], "title": "Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach", "comment": null, "summary": "Acoustic mapping techniques have long been used in spatial audio processing\nfor direction of arrival estimation (DoAE). Traditional beamforming methods for\nacoustic mapping, while interpretable, often rely on iterative solvers that can\nbe computationally intensive and sensitive to acoustic variability. On the\nother hand, recent supervised deep learning approaches offer feedforward speed\nand robustness but require large labeled datasets and lack interpretability.\nDespite their strengths, both methods struggle to consistently generalize\nacross diverse acoustic setups and array configurations, limiting their broader\napplicability. We introduce the Latent Acoustic Mapping (LAM) model, a\nself-supervised framework that bridges the interpretability of traditional\nmethods with the adaptability and efficiency of deep learning methods. LAM\ngenerates high-resolution acoustic maps, adapts to varying acoustic conditions,\nand operates efficiently across different microphone arrays. We assess its\nrobustness on DoAE using the LOCATA and STARSS benchmarks. LAM achieves\ncomparable or superior localization performance to existing supervised methods.\nAdditionally, we show that LAM's acoustic maps can serve as effective features\nfor supervised models, further enhancing DoAE accuracy and underscoring its\npotential to advance adaptive, high-performance sound localization systems."}

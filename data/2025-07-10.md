<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.SD](#cs.SD) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Deep Learning-based Human Gesture Channel Modeling for Integrated Sensing and Communication Scenarios](https://arxiv.org/abs/2507.06588)
*Zhengyu Zhang,Neeraj Varshney,Jelena Senic,Raied Caromi,Samuel Berweger,Camillo Gentile,Enrico M. Vitucci,Ruisi He,Vittorio Degli-Esposti*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的人体手势信道建模框架，用于6G无线系统中的集成感知与通信（ISAC）场景，通过分解人体部位并学习手势与多径特性的映射关系，实现了高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线系统中ISAC的发展，非接触式人体识别成为关键应用场景之一，而人体手势运动引起的无线多径传播变化需要准确建模。

Method: 采用泊松神经网络预测各人体部位的多径分量数量，并利用条件变分自编码器生成散射点，进而重构连续信道脉冲响应和微多普勒特征。

Result: 仿真结果表明，该方法在不同手势和受试者中均表现出高精度和泛化能力。

Conclusion: 该方法为手势ISAC系统的数据增强和评估提供了一种可解释的解决方案。

Abstract: With the development of Integrated Sensing and Communication (ISAC) for
Sixth-Generation (6G) wireless systems, contactless human recognition has
emerged as one of the key application scenarios. Since human gesture motion
induces subtle and random variations in wireless multipath propagation, how to
accurately model human gesture channels has become a crucial issue for the
design and validation of ISAC systems. To this end, this paper proposes a deep
learning-based human gesture channel modeling framework for ISAC scenarios, in
which the human body is decomposed into multiple body parts, and the mapping
between human gestures and their corresponding multipath characteristics is
learned from real-world measurements. Specifically, a Poisson neural network is
employed to predict the number of Multi-Path Components (MPCs) for each human
body part, while Conditional Variational Auto-Encoders (C-VAEs) are reused to
generate the scattering points, which are further used to reconstruct
continuous channel impulse responses and micro-Doppler signatures. Simulation
results demonstrate that the proposed method achieves high accuracy and
generalization across different gestures and subjects, providing an
interpretable approach for data augmentation and the evaluation of
gesture-based ISAC systems.

</details>


### [2] [Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization to Estimation](https://arxiv.org/abs/2507.06612)
*Peng Jiang,Ming Li,Rang Liu,Qian Liu*

Main category: eess.SP

TL;DR: 本文提出了一种端到端的图学习方法，用于优化无蜂窝集成感知与通信（ISAC）系统，通过动态图学习和轻量级镜像图注意力网络（mirror-GAT）框架，显著提升了多目标位置和速度估计的精度。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝ISAC系统在6G网络中具有潜力，但需要统一的系统级优化与多目标参数估计方法。

Method: 提出动态图学习框架（结合3D-CNN）和轻量级mirror-GAT框架，联合设计AP模式选择、用户关联、预编码和回波信号处理。

Result: 仿真结果表明，两种框架在多目标估计精度上优于传统方法，mirror-GAT框架还显著降低了计算复杂性和信令开销。

Conclusion: 所提框架为无蜂窝ISAC系统的实际部署提供了高效且性能优越的解决方案。

Abstract: Cell-free integrated sensing and communication (ISAC) systems have emerged as
a promising paradigm for sixth-generation (6G) networks, enabling simultaneous
high-rate data transmission and high-precision radar sensing through
cooperative distributed access points (APs). Fully exploiting these
capabilities requires a unified design that bridges system-level optimization
with multi-target parameter estimation. This paper proposes an end-to-end graph
learning approach to close this gap, modeling the entire cell-free ISAC network
as a heterogeneous graph to jointly design the AP mode selection, user
association, precoding, and echo signal processing for multi-target position
and velocity estimation. In particular, we propose two novel heterogeneous
graph learning frameworks: a dynamic graph learning framework and a lightweight
mirror-based graph attention network (mirror-GAT) framework. The dynamic graph
learning framework employs structural and temporal attention mechanisms
integrated with a three-dimensional convolutional neural network (3D-CNN),
enabling superior performance and robustness in cell-free ISAC environments.
Conversely, the mirror-GAT framework significantly reduces computational
complexity and signaling overhead through a bi-level iterative structure with
share adjacency. Simulation results validate that both proposed
graph-learning-based frameworks achieve significant improvements in
multi-target position and velocity estimation accuracy compared to conventional
heuristic and optimization-based designs. Particularly, the mirror-GAT
framework demonstrates substantial reductions in computational time and
signaling overhead, underscoring its suitability for practical deployments.

</details>


### [3] [Wireless Energy Transfer Beamforming Optimization for Intelligent Transmitting Surface](https://arxiv.org/abs/2507.06805)
*Osmel Martínez Rosabal,Onel Alcaraz López,Victoria Dala Pegorara Souto,Richard Demo Souza,Samuel Montejo-Sánchez,Robert Schober,Hirley Alves*

Main category: eess.SP

TL;DR: 该论文研究了利用智能传输表面（ITS）的无线能量传输技术，旨在最小化功率信标（PB）的功耗，并通过优化数字预编码和相位配置实现高效能量传输。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）设备的增长，无线能量传输（WET）技术成为关键。智能传输表面（ITS）为功率信标（PB）提供了高效、可重构的阵列设计，但需解决其功耗优化问题。

Method: 采用连续凸近似（SCA）方法，通过迭代求解凸子问题，联合优化数字预编码和相位配置，同时提出了一种确保初始化可行性的算法。

Result: 结果表明，配备ITS的PB架构在功耗上优于数字和混合模拟-数字波束成形基准架构，并能高效扩展射频链和ITS元素数量。

Conclusion: 非均匀ITS功率分布影响波束成形，并能在恒定孔径下将设备从近场区域转移到远场区域，展示了ITS在无线能量传输中的潜力。

Abstract: Radio frequency (RF) wireless energy transfer (WET) is a promising technology
for powering the growing ecosystem of Internet of Things (IoT) devices using
power beacons (PBs). Recent research focuses on designing efficient PB
architectures that can support numerous antennas. In this context, PBs equipped
with intelligent surfaces present a promising approach, enabling physically
large, reconfigurable arrays. Motivated by these advantages, this work aims to
minimize the power consumption of a PB equipped with a passive intelligent
transmitting surface (ITS) and a collocated digital beamforming-based feeder to
charge multiple single-antenna devices. To model the PB's power consumption
accurately, we consider power amplifiers nonlinearities, ITS control power, and
feeder-to-ITS air interface losses. The resulting optimization problem is
highly nonlinear and nonconvex due to the high-power amplifier (HPA), the
received power constraints at the devices, and the unit-modulus constraint
imposed by the phase shifter configuration of the ITS. To tackle this issue, we
apply successive convex approximation (SCA) to iteratively solve convex
subproblems that jointly optimize the digital precoder and phase configuration.
Given SCA's sensitivity to initialization, we propose an algorithm that ensures
initialization feasibility while balancing convergence speed and solution
quality. We compare the proposed ITS-equipped PB's power consumption against
benchmark architectures featuring digital and hybrid analog-digital
beamforming. Results demonstrate that the proposed architecture efficiently
scales with the number of RF chains and ITS elements. We also show that
nonuniform ITS power distribution influences beamforming and can shift a device
between near- and far-field regions, even with a constant aperture.

</details>


### [4] [Enhancing Environment Generalizability for Deep Learning-Based CSI Feedback](https://arxiv.org/abs/2507.06833)
*Haoyu Wang,Shuangfeng Han,Xiaoyun Wang,Zhi Sun*

Main category: eess.SP

TL;DR: EG-CsiNet提出了一种新型CSI反馈学习框架，通过多路径解耦和细粒度对齐模块，解决了不同环境中CSI分布偏移问题，显著提升了在未见环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在FDD大规模MIMO系统中，准确且低开销的CSI反馈对提升容量至关重要。现有深度学习算法在未见环境中的泛化能力有限，增加了部署成本。

Method: 提出EG-CsiNet框架，包含多路径解耦和细粒度对齐模块，分别处理多路径结构和单一路径的分布偏移。

Result: 通过大量仿真验证，EG-CsiNet在未见环境中表现出更强的泛化能力，尤其在单源环境的挑战性条件下优于现有方法。

Conclusion: EG-CsiNet通过解决分布偏移问题，显著提升了CSI反馈的环境泛化能力，为FDD大规模MIMO系统的部署提供了更优解决方案。

Abstract: Accurate and low-overhead channel state information (CSI) feedback is
essential to boost the capacity of frequency division duplex (FDD) massive
multiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback
significantly outperforms conventional approaches. Nevertheless, current deep
learning-based CSI feedback algorithms exhibit limited generalizability to
unseen environments, which obviously increases the deployment cost. In this
paper, we first model the distribution shift of CSI across different
environments, which is composed of the distribution shift of multipath
structure and a single-path. Then, EG-CsiNet is proposed as a novel CSI
feedback learning framework to enhance environment-generalizability.
Explicitly, EG-CsiNet comprises the modules of multipath decoupling and
fine-grained alignment, which can address the distribution shift of multipath
structure and a single path. Based on extensive simulations, the proposed
EG-CsiNet can robustly enhance the generalizability in unseen environments
compared to the state-of-the-art, especially in challenging conditions with a
single source environment.

</details>


### [5] [OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion](https://arxiv.org/abs/2507.06849)
*Yizhuo Wu,Ang Li,Chang Gao*

Main category: eess.SP

TL;DR: OpenDPDv2框架通过TRes-DeltaGRU算法和节能方法，在保持高性能的同时降低DPD模型的能耗。


<details>
  <summary>Details</summary>
Motivation: 传统NN DPD依赖大量参数，能耗高，需优化以减少功耗。

Method: 提出OpenDPDv2框架，结合TRes-DeltaGRU算法和固定点量化、动态时间稀疏性技术。

Result: FP32模型ACPR达-59.4 dBc，EVM为-42.1 dBc；优化后能耗降低4.5倍，性能仍保持较高水平。

Conclusion: OpenDPDv2在降低能耗的同时保持高性能，适用于宽带RF PA系统。

Abstract: Neural network (NN)-based Digital Predistortion (DPD) stands out in improving
signal quality in wideband radio frequency (RF) power amplifiers (PAs)
employing complex modulation. However, NN DPDs usually rely on a large number
of parameters for effective linearization and can significantly contribute to
the energy consumption of the digital back-end in RF systems. This paper
presents OpenDPDv2, a unified framework for PA modeling, DPD learning, and
model optimization to reduce power consumption while maintaining high
linearization performance. The optimization techniques feature a novel DPD
algorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The
top-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an
Adjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude
(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal
sparsity of input signals and hidden neurons, the inference energy of our model
can be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM
with 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth
256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,
datasets, and documentation are publicly accessible at:
https://github.com/lab-emi/OpenDPD.

</details>


### [6] [Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA Assisted Wireless Communication Systems](https://arxiv.org/abs/2507.06904)
*Yu Liu,Qu Luo,Gaojie Chen,Pei Xiao,Ahmed Elzanaty,Mohsen Khalily,Rahim Tafazolli*

Main category: eess.SP

TL;DR: 本文提出了一种新型的流体天线系统（FAS）辅助的非正交多址（NOMA）多用户通信系统，通过动态调整RIS元素位置，显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统可重构智能表面（RIS）在空间控制能力上存在局限性，需要更灵活的解决方案。

Method: 提出流体同时传输和反射RIS（FSTAR-RIS）系统，结合交替优化（AO）算法，优化基站波束成形、RIS系数及元素位置。

Result: 仿真结果显示，系统总速率提升27%，且所需RIS元素减少50%。

Conclusion: FSTAR-RIS系统在性能和成本效率上优于传统方案，适合大规模部署。

Abstract: To address the limitations of traditional reconfigurable intelligent surfaces
(RIS) in spatial control capability, this paper introduces the concept of the
fluid antenna system (FAS) and proposes a fluid simultaneously transmitting and
reflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA)
multi-user communication system. In this system, each FSTAR-RIS element is
capable of flexible mobility and can dynamically adjust its position in
response to environmental variations, thereby enabling simultaneous service to
users in both the transmission and reflection zones. This significantly
enhances the system's spatial degrees of freedom (DoF) and service
adaptability. To maximize the system's weighted sum-rate, we formulate a
non-convex optimization problem that jointly optimizes the base station
beamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the
element positions. An alternating optimization (AO) algorithm is developed,
incorporating successive convex approximation (SCA), semi-definite relaxation
(SDR), and majorization-minimization (MM) techniques. In particular, to address
the complex channel coupling introduced by the coexistence of direct and
FSTAR-RIS paths, the MM framework is employed in the element position
optimization subproblem, enabling an efficient iterative solution strategy.
Simulation results validate that the proposed system achieves up to a 27%
increase in total sum rate compared to traditional STAR-RIS systems and
requires approximately 50% fewer RIS elements to attain the same performance,
highlighting its effectiveness for cost-efficient large-scale deployment.

</details>


### [7] [Precise Representation Model of SAR Saturated Interference: Mechanism and Verification](https://arxiv.org/abs/2507.06932)
*Lunhao Duan,Xingyu Lu,Yushuang Liu,Jianchao Yang,Hong Gu*

Main category: eess.SP

TL;DR: 本文提出了一种基于贝塞尔函数的SAR接收机饱和干扰分析模型，验证了其准确性，并对比了传统平滑函数近似模型的误差。


<details>
  <summary>Details</summary>
Motivation: SAR接收机易受高功率射频干扰导致饱和，传统分析方法因饱和函数非平滑特性难以准确分析，现有研究存在近似误差。

Method: 提出基于贝塞尔函数的饱和干扰分析模型，通过仿真与传统平滑函数近似模型对比验证。

Result: 新模型准确性更高，为饱和干扰抑制等进一步工作提供指导。

Conclusion: 贝塞尔函数模型能更准确地分析SAR接收机饱和干扰，优于传统方法。

Abstract: Synthetic Aperture Radar (SAR) is highly susceptible to Radio Frequency
Interference (RFI). Due to the performance limitations of components such as
gain controllers and analog-to-digital converters in SAR receivers, high-power
interference can easily cause saturation of the SAR receiver, resulting in
nonlinear distortion of the interfered echoes, which are distorted in both the
time domain and frequency domain. Some scholars have analyzed the impact of SAR
receiver saturation on target echoes through simulations. However, the
saturation function has non-smooth characteristics, making it difficult to
conduct accurate analysis using traditional analytical methods. Current related
studies have approximated and analyzed the saturation function based on the
hyperbolic tangent function, but there are approximation errors. Therefore,
this paper proposes a saturation interference analysis model based on Bessel
functions, and verifies the accuracy of the proposed saturation interference
analysis model by simulating and comparing it with the traditional saturation
model based on smooth function approximation. This model can provide certain
guidance for further work such as saturation interference suppression.

</details>


### [8] [Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks](https://arxiv.org/abs/2507.06997)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 论文提出了一种基于联邦学习的多智能体强化学习（MARL）策略，用于增强超5G网络中多蜂窝网络的物理层安全性（PLS）。通过比较DQN和RDPG两种方法，发现RDPG收敛更快且性能优于分布式DRL方法。


<details>
  <summary>Details</summary>
Motivation: 在超5G网络中，多蜂窝网络面临窃听者威胁，需要一种高效且隐私保护的方法来提升物理层安全性。

Method: 采用联邦学习的MARL策略，每个基站作为DRL智能体，使用DQN和RDPG两种方法优化保密率，仅共享网络参数而非用户数据。

Result: RDPG比DQN收敛更快，且性能优于分布式DRL方法，同时揭示了安全性与复杂性的权衡。

Conclusion: 联邦学习的MARL策略在提升物理层安全性方面具有潜力，RDPG是更优选择，但需权衡安全性与复杂性。

Abstract: This paper explores the application of a federated learning-based multi-agent
reinforcement learning (MARL) strategy to enhance physical-layer security (PLS)
in a multi-cellular network within the context of beyond 5G networks. At each
cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent
that interacts with the surrounding environment to maximize the secrecy rate of
legitimate users in the presence of an eavesdropper. This eavesdropper attempts
to intercept the confidential information shared between the BS and its
authorized users. The DRL agents are deemed to be federated since they only
share their network parameters with a central server and not the private data
of their legitimate users. Two DRL approaches, deep Q-network (DQN) and
Reinforce deep policy gradient (RDPG), are explored and compared. The results
demonstrate that RDPG converges more rapidly than DQN. In addition, we
demonstrate that the proposed method outperforms the distributed DRL approach.
Furthermore, the outcomes illustrate the trade-off between security and
complexity.

</details>


### [9] [How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks](https://arxiv.org/abs/2507.07067)
*Clement Ruah,Houssem Sifaou,Osvaldo Simeone,Bashir M. Al-Hashimi*

Main category: eess.SP

TL;DR: 论文探讨了利用数字孪生技术解决电信领域AI模型训练数据不足的问题，并提出了两种缩小仿真与现实差距的策略。


<details>
  <summary>Details</summary>
Motivation: 电信领域AI模型训练面临数据稀缺问题，现有数据集难以覆盖网络环境的独特条件和变异性。数字孪生技术可通过仿真生成特定场景数据，但需解决仿真与现实的差距问题。

Method: 提出两种策略：1）通过真实测量校准数字孪生；2）采用仿真与现实差距感知的训练方法，包括基于贝叶斯学习的环境建模和基于预测驱动推断的损失函数优化。

Result: 评估了两种方法，分别从环境和训练损失层面建模仿真与现实差距，为缩小差距提供了可行方案。

Conclusion: 数字孪生技术结合仿真与现实差距处理策略，有望提升电信领域AI模型的训练效果。

Abstract: Training effective artificial intelligence models for telecommunications is
challenging due to the scarcity of deployment-specific data. Real data
collection is expensive, and available datasets often fail to capture the
unique operational conditions and contextual variability of the network
environment. Digital twinning provides a potential solution to this problem, as
simulators tailored to the current network deployment can generate
site-specific data to augment the available training datasets. However, there
is a need to develop solutions to bridge the inherent simulation-to-reality
(sim-to-real) gap between synthetic and real-world data. This paper reviews
recent advances on two complementary strategies: 1) the calibration of digital
twins (DTs) through real-world measurements, and 2) the use of sim-to-real
gap-aware training strategies to robustly handle residual discrepancies between
digital twin-generated and real data. For the latter, we evaluate two
conceptually distinct methods that model the sim-to-real gap either at the
level of the environment via Bayesian learning or at the level of the training
loss via prediction-powered inference.

</details>


### [10] [Joint Target Acquisition and Refined Position Estimation in OFDM-based ISAC Networks](https://arxiv.org/abs/2507.07081)
*Lorenzo Pucci,Andrea Giorgetti*

Main category: eess.SP

TL;DR: 提出了一种基于OFDM的ISAC网络中联合目标获取和位置估计的两阶段框架，通过基站合作实现厘米级定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决OFDM网络中目标检测与位置估计的联合问题，利用基站协作提升性能。

Method: 第一阶段：各基站计算距离-角度图检测目标并估计粗略位置；第二阶段：在共享全局参考系中，通过协作最大似然估计器进行精细定位。

Result: 数值结果表明，该方法通过基站协作提升了检测性能，并实现了厘米级定位精度。

Conclusion: 提出的两阶段框架在目标检测和定位精度上表现出色，验证了其有效性。

Abstract: This paper addresses joint target acquisition and position estimation in an
OFDM-based integrated sensing and communication (ISAC) network with base
station (BS) cooperation via a fusion center. A two-stage framework is
proposed: in the first stage, each BS computes range-angle maps to detect
targets and estimate coarse positions, exploiting spatial diversity. In the
second stage, refined localization is performed using a cooperative maximum
likelihood (ML) estimator over predefined regions of interest (RoIs) within a
shared global reference frame. Numerical results demonstrate that the proposed
approach not only improves detection performance through BS cooperation but
also achieves centimeter-level localization accuracy, highlighting the
effectiveness of the refined estimation technique.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation](https://arxiv.org/abs/2507.06249)
*Saierdaer Yusuyin,Te Ma,Hao Huang,Zhijian Ou*

Main category: eess.AS

TL;DR: 提出了一种基于潜在变量模型的方法（JSA-SPG），无需发音词典即可实现跨语言语音识别，通过联合训练S2P、P2G和G2P模型，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 现有基于音素的跨语言语音识别需要发音词典，限制了其应用。本研究旨在消除这一需求。

Method: 采用潜在变量模型，将音素视为离散潜在变量，联合训练S2P、P2G和G2P模型，使用JSA算法优化。

Result: 在波兰语和印尼语实验中，仅需10分钟音素监督，错误率降低5%；在语言领域适应中，错误率降低9%。

Conclusion: JSA-SPG方法高效且无需发音词典，显著提升跨语言语音识别性能，开源代码促进进一步研究。

Abstract: Recently, pre-trained models with phonetic supervision have demonstrated
their advantages for crosslingual speech recognition in data efficiency and
information sharing across languages. However, a limitation is that a
pronunciation lexicon is needed for such phoneme-based crosslingual speech
recognition. In this study, we aim to eliminate the need for pronunciation
lexicons and propose a latent variable model based method, with phonemes being
treated as discrete latent variables. The new method consists of a
speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a
grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model.
To jointly train the three models, we utilize the joint stochastic
approximation (JSA) algorithm, which is a stochastic extension of the EM
(expectation-maximization) algorithm and has demonstrated superior performance
particularly in estimating discrete latent variable models. Based on the
Whistle multilingual pre-trained S2P model, crosslingual experiments are
conducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of
phoneme supervision, the new method, JSA-SPG, achieves 5\% error rate
reductions compared to the best crosslingual fine-tuning approach using subword
or full phoneme supervision. Furthermore, it is found that in language domain
adaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms
the standard practice of language model fusion via the auxiliary support of the
G2P model by 9% error rate reductions. To facilitate reproducibility and
encourage further exploration in this field, we open-source the JSA-SPG
training code and complete pipeline.

</details>


### [12] [Open-Set Source Tracing of Audio Deepfake Systems](https://arxiv.org/abs/2507.06470)
*Nicholas Klein,Hemlata Tak,Elie Khoury*

Main category: eess.AS

TL;DR: 论文提出了一种改进的开放集音频深度伪造源追踪方法，通过引入软最大能量（SME）和多种增强技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注闭集场景，而开放集性能评估有限。由于音频深度伪造系统数量庞大，开放集源追踪的鲁棒性至关重要。

Method: 提出软最大能量（SME）用于OOD检测，并探索SME引导训练及多种增强技术（如合成、编解码和混响增强）。

Result: SME替换传统能量分数后，FPR95指标相对提升31%，最终FPR95达到8.3%。

Conclusion: SME和增强技术的结合显著提升了开放集音频深度伪造源追踪的性能。

Abstract: Existing research on source tracing of audio deepfake systems has focused
primarily on the closed-set scenario, while studies that evaluate open-set
performance are limited to a small number of unseen systems. Due to the large
number of emerging audio deepfake systems, robust open-set source tracing is
critical. We leverage the protocol of the Interspeech 2025 special session on
source tracing to evaluate methods for improving open-set source tracing
performance. We introduce a novel adaptation to the energy score for
out-of-distribution (OOD) detection, softmax energy (SME). We find that
replacing the typical temperature-scaled energy score with SME provides a
relative average improvement of 31% in the standard FPR95 (false positive rate
at true positive rate of 95%) measure. We further explore SME-guided training
as well as copy synthesis, codec, and reverberation augmentations, yielding an
FPR95 of 8.3%.

</details>


### [13] [Training Strategies for Modality Dropout Resilient Multi-Modal Target Speaker Extraction](https://arxiv.org/abs/2507.06566)
*Srikanth Korse,Mohamed Elminshawi,Emanuel A. P. Habets,Srikanth Raj Chetupalli*

Main category: eess.AS

TL;DR: 本文研究了多模态目标说话人提取（MTSE）系统，提出了一种名为模态丢弃训练（MDT）的策略，以解决模态主导问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态TSE系统在实际应用中常因模态主导问题而性能受限，本文旨在通过训练策略和架构选择提升系统的鲁棒性。

Method: 提出模态丢弃训练（MDT）策略，并与标准训练和多任务训练（MTT）策略进行比较，同时探讨归一化层的影响。

Result: 实验表明，MDT策略在LRS3数据集上表现优异，不受归一化层选择影响，且对提取语音作为注册信号具有鲁棒性。

Conclusion: MDT策略能有效解决模态主导问题，提升MTSE系统的鲁棒性和适用性。

Abstract: The primary goal of multi-modal TSE (MTSE) is to extract a target speaker
from a speech mixture using complementary information from different
modalities, such as audio enrolment and visual feeds corresponding to the
target speaker. MTSE systems are expected to perform well even when one of the
modalities is unavailable. In practice, the systems often suffer from modality
dominance, where one of the modalities outweighs the others, thereby limiting
robustness. Our study investigates training strategies and the effect of
architectural choices, particularly the normalization layers, in yielding a
robust MTSE system in both non-causal and causal configurations. In particular,
we propose the use of modality dropout training (MDT) as a superior strategy to
standard and multi-task training (MTT) strategies. Experiments conducted on
two-speaker mixtures from the LRS3 dataset show the MDT strategy to be
effective irrespective of the employed normalization layer. In contrast, the
models trained with the standard and MTT strategies are susceptible to modality
dominance, and their performance depends on the chosen normalization layer.
Additionally, we demonstrate that the system trained with MDT strategy is
robust to using extracted speech as the enrollment signal, highlighting its
potential applicability in scenarios where the target speaker is not enrolled.

</details>


### [14] [Musical Source Separation Bake-Off: Comparing Objective Metrics with Human Perception](https://arxiv.org/abs/2507.06917)
*Noah Jaffe,John Ashley Burgoyne*

Main category: eess.AS

TL;DR: 研究发现，音乐源分离的质量评估中，SDR对歌声分离效果最佳，而SI-SAR对鼓和贝斯分离更符合人类感知。CLAP-LAION-music嵌入的FAD表现也较好，但无嵌入方法适用于所有声源。


<details>
  <summary>Details</summary>
Motivation: 当前音乐源分离评估指标（如SDR）与人类感知不一致，需更准确的评估方法。

Method: 通过大规模听众评估（MUSDB18测试集），比较多种客观指标（SDR、SI-SAR、FAD等）与人类评分的相关性。

Result: SDR对歌声分离效果最好，SI-SAR对鼓和贝斯更优，CLAP-LAION-music嵌入的FAD表现接近传统指标。

Conclusion: 需针对不同声源采用特定评估策略，无单一指标适用于所有声源。公开听众评分数据以支持后续研究。

Abstract: Music source separation aims to extract individual sound sources (e.g.,
vocals, drums, guitar) from a mixed music recording. However, evaluating the
quality of separated audio remains challenging, as commonly used metrics like
the source-to-distortion ratio (SDR) do not always align with human perception.
In this study, we conducted a large-scale listener evaluation on the MUSDB18
test set, collecting approximately 30 ratings per track from seven distinct
listener groups. We compared several objective energy-ratio metrics, including
legacy measures (BSSEval v4, SI-SDR variants), and embedding-based alternatives
(Frechet Audio Distance using CLAP-LAION-music, EnCodec, VGGish, Wave2Vec2, and
HuBERT). While SDR remains the best-performing metric for vocal estimates, our
results show that the scale-invariant signal-to-artifacts ratio (SI-SAR) better
predicts listener ratings for drums and bass stems. Frechet Audio Distance
(FAD) computed with the CLAP-LAION-music embedding also performs
competitively--achieving Kendall's tau values of 0.25 for drums and 0.19 for
bass--matching or surpassing energy-based metrics for those stems. However,
none of the embedding-based metrics, including CLAP, correlate positively with
human perception for vocal estimates. These findings highlight the need for
stem-specific evaluation strategies and suggest that no single metric reliably
reflects perceptual quality across all source types. We release our raw
listener ratings to support reproducibility and further research.

</details>


### [15] [Deep Feed-Forward Neural Network for Bangla Isolated Speech Recognition](https://arxiv.org/abs/2507.07068)
*Dipayan Bhadra,Mehrab Hosain,Fatema Alam*

Main category: eess.AS

TL;DR: 本文提出了一种基于MFCC和7层深度前馈全连接神经网络的孤立语音识别方法，用于孟加拉语和英语单词识别，准确率达93.42%。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语语音识别研究较少，本文旨在填补这一空白并提升识别性能。

Method: 使用MFCC提取特征，采用7层深度前馈全连接神经网络作为分类器。

Result: 识别准确率达到93.42%，优于以往大多数孟加拉语语音识别研究。

Conclusion: 该方法在孤立单词识别中表现出色，为孟加拉语语音识别提供了有效解决方案。

Abstract: As the most important human-machine interfacing tool, an insignificant amount
of work has been carried out on Bangla Speech Recognition compared to the
English language. Motivated by this, in this work, the performance of
speaker-independent isolated speech recognition systems has been implemented
and analyzed using a dataset that is created containing both isolated Bangla
and English spoken words. An approach using the Mel Frequency Cepstral
Coefficient (MFCC) and Deep Feed-Forward Fully Connected Neural Network (DFFNN)
of 7 layers as a classifier is proposed in this work to recognize isolated
spoken words. This work shows 93.42% recognition accuracy which is better
compared to most of the works done previously on Bangla speech recognition
considering the number of classes and dataset size.

</details>


### [16] [Incremental Averaging Method to Improve Graph-Based Time-Difference-of-Arrival Estimation](https://arxiv.org/abs/2507.07087)
*Klaus Brümann,Kouei Yamaoka,Nobutaka Ono,Simon Doclo*

Main category: eess.AS

TL;DR: 提出了一种基于多CPSD平均的GCC-PHAT方法，通过增量计算提升TDOA和声源定位精度，优于传统单CPSD方法。


<details>
  <summary>Details</summary>
Motivation: 背景噪声和混响会降低基于TDOA的声源定位精度，现有方法（如GCC-PHAT和MST）依赖单CPSD，性能受限。

Method: 通过增量方法平均多个CPSD计算GCC-PHAT函数，逐步增加间接计算的CPSD数量。

Result: 实验表明，多CPSD方法在TDOA和2D声源定位误差上优于单CPSD和基于参考麦克风的方法。

Conclusion: 多CPSD平均方法显著提升了噪声和混响环境下的声源定位精度。

Abstract: Estimating the position of a speech source based on
time-differences-of-arrival (TDOAs) is often adversely affected by background
noise and reverberation. A popular method to estimate the TDOA between a
microphone pair involves maximizing a generalized cross-correlation with phase
transform (GCC-PHAT) function. Since the TDOAs across different microphone
pairs satisfy consistency relations, generally only a small subset of
microphone pairs are used for source position estimation. Although the set of
microphone pairs is often determined based on a reference microphone, recently
a more robust method has been proposed to determine the set of microphone pairs
by computing the minimum spanning tree (MST) of a signal graph of GCC-PHAT
function reliabilities. To reduce the influence of noise and reverberation on
the TDOA estimation accuracy, in this paper we propose to compute the GCC-PHAT
functions of the MST based on an average of multiple cross-power spectral
densities (CPSDs) using an incremental method. In each step of the method, we
increase the number of CPSDs over which we average by considering CPSDs
computed indirectly via other microphones from previous steps. Using signals
recorded in a noisy and reverberant laboratory with an array of spatially
distributed microphones, the performance of the proposed method is evaluated in
terms of TDOA estimation error and 2D source position estimation error.
Experimental results for different source and microphone configurations and
three reverberation conditions show that the proposed method considering
multiple CPSDs improves the TDOA estimation and source position estimation
accuracy compared to the reference microphone- and MST-based methods that rely
on a single CPSD as well as steered-response power-based source position
estimation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [17] [MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing](https://arxiv.org/abs/2507.06329)
*Michael Clemens,Ana Marasović*

Main category: cs.SD

TL;DR: MixAssist是一个新的音频-语言数据集，旨在捕捉专家与业余音乐制作人在协作混音会话中的对话，填补了当前AI研究中忽视协作与教学维度的空白。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究过于关注端到端自动化或生成，忽视了音乐制作中的协作与教学需求，尤其是业余者的学习需求。

Method: 通过收集7个深度会话中的431个音频对话，构建MixAssist数据集，并利用其微调音频-语言模型（如Qwen-Audio）。

Result: Qwen-Audio在生成有帮助且上下文相关的混音建议方面显著优于其他模型。

Conclusion: MixAssist为开发支持音乐混音创意过程的智能AI助手提供了基础。

Abstract: While AI presents significant potential for enhancing music mixing and
mastering workflows, current research predominantly emphasizes end-to-end
automation or generation, often overlooking the collaborative and instructional
dimensions vital for co-creative processes. This gap leaves artists,
particularly amateurs seeking to develop expertise, underserved. To bridge
this, we introduce MixAssist, a novel audio-language dataset capturing the
situated, multi-turn dialogue between expert and amateur music producers during
collaborative mixing sessions. Comprising 431 audio-grounded conversational
turns derived from 7 in-depth sessions involving 12 producers, MixAssist
provides a unique resource for training and evaluating audio-language models
that can comprehend and respond to the complexities of real-world music
production dialogues. Our evaluations, including automated LLM-as-a-judge
assessments and human expert comparisons, demonstrate that fine-tuning models
such as Qwen-Audio on MixAssist can yield promising results, with Qwen
significantly outperforming other tested models in generating helpful,
contextually relevant mixing advice. By focusing on co-creative instruction
grounded in audio context, MixAssist enables the development of intelligent AI
assistants designed to support and augment the creative process in music
mixing.

</details>


### [18] [IMPACT: Industrial Machine Perception via Acoustic Cognitive Transformer](https://arxiv.org/abs/2507.06481)
*Changheon Han,Yuseop Sim,Hoin Jung,Jiho Lee,Hojun Lee,Yun Seok Kang,Sucheol Woo,Garam Kim,Hyung Wook Park,Martin Byung-Guk Jun*

Main category: cs.SD

TL;DR: 论文提出了DINOS数据集和IMPACT模型，用于工业机器声音分析，解决了现有方法泛化性差和数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法在工业音频场景中泛化性差，且缺乏大规模数据集和预训练模型。

Method: 提出DINOS数据集和IMPACT模型，通过自监督学习联合优化全局和局部特征。

Result: IMPACT在30个下游任务中优于现有模型，24个任务表现最佳。

Conclusion: DINOS和IMPACT为工业音频分析提供了新基准，推动了社区研究。

Abstract: Acoustic signals from industrial machines offer valuable insights for anomaly
detection, predictive maintenance, and operational efficiency enhancement.
However, existing task-specific, supervised learning methods often scale poorly
and fail to generalize across diverse industrial scenarios, whose acoustic
characteristics are distinct from general audio. Furthermore, the scarcity of
accessible, large-scale datasets and pretrained models tailored for industrial
audio impedes community-driven research and benchmarking. To address these
challenges, we introduce DINOS (Diverse INdustrial Operation Sounds), a
large-scale open-access dataset. DINOS comprises over 74,149 audio samples
(exceeding 1,093 hours) collected from various industrial acoustic scenarios.
We also present IMPACT (Industrial Machine Perception via Acoustic Cognitive
Transformer), a novel foundation model for industrial machine sound analysis.
IMPACT is pretrained on DINOS in a self-supervised manner. By jointly
optimizing utterance and frame-level losses, it captures both global semantics
and fine-grained temporal structures. This makes its representations suitable
for efficient fine-tuning on various industrial downstream tasks with minimal
labeled data. Comprehensive benchmarking across 30 distinct downstream tasks
(spanning four machine types) demonstrates that IMPACT outperforms existing
models on 24 tasks, establishing its superior effectiveness and robustness,
while providing a new performance benchmark for future research.

</details>


### [19] [STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation](https://arxiv.org/abs/2507.06670)
*Wenxiang Guo,Yu Zhang,Changhao Pan,Zhiyuan Zhu,Ruiqi Li,Zhetao Chen,Wenhao Xu,Fei Wu,Zhou Zhao*

Main category: cs.SD

TL;DR: STARS是一个统一的框架，首次同时解决歌唱转录、对齐和风格标注问题，提供多级注释，显著提升歌唱数据集的可扩展性和可控性。


<details>
  <summary>Details</summary>
Motivation: 手动标注歌唱数据集成本高昂，现有自动标注方法仅解决部分问题，因此需要一种统一框架来全面解决歌唱标注的挑战。

Method: STARS采用分层声学特征处理和非自回归局部声学编码器，实现结构化分层表示学习，涵盖音素对齐、音符转录、声乐技术识别和风格标注。

Result: 实验验证STARS在多个评估维度上优于现有方法，且其标注数据显著提升歌唱合成的自然性和风格控制精度。

Conclusion: STARS不仅解决了歌唱数据集的可扩展性问题，还为可控歌唱合成开辟了新方法。

Abstract: Recent breakthroughs in singing voice synthesis (SVS) have heightened the
demand for high-quality annotated datasets, yet manual annotation remains
prohibitively labor-intensive and resource-intensive. Existing automatic
singing annotation (ASA) methods, however, primarily tackle isolated aspects of
the annotation pipeline. To address this fundamental challenge, we present
STARS, which is, to our knowledge, the first unified framework that
simultaneously addresses singing transcription, alignment, and refined style
annotation. Our framework delivers comprehensive multi-level annotations
encompassing: (1) precise phoneme-audio alignment, (2) robust note
transcription and temporal localization, (3) expressive vocal technique
identification, and (4) global stylistic characterization including emotion and
pace. The proposed architecture employs hierarchical acoustic feature
processing across frame, word, phoneme, note, and sentence levels. The novel
non-autoregressive local acoustic encoders enable structured hierarchical
representation learning. Experimental validation confirms the framework's
superior performance across multiple evaluation dimensions compared to existing
annotation approaches. Furthermore, applications in SVS training demonstrate
that models utilizing STARS-annotated data achieve significantly enhanced
perceptual naturalness and precise style control. This work not only overcomes
critical scalability challenges in the creation of singing datasets but also
pioneers new methodologies for controllable singing voice synthesis. Audio
samples are available at https://gwx314.github.io/stars-demo/.

</details>


### [20] [Exploring State-Space-Model based Language Model in Music Generation](https://arxiv.org/abs/2507.06674)
*Wei-Jaw Lee,Fang-Chih Hsieh,Xuanjun Chen,Fang-Duo Tsai,Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: 探索基于Mamba的架构在文本到音乐生成中的潜力，发现单层码本可捕捉音乐语义信息，并证明SiMBA在有限资源下比Transformer更快收敛且生成更接近真实结果。


<details>
  <summary>Details</summary>
Motivation: 研究SSMs（特别是Mamba）作为Transformer替代或补充模块的潜力，尤其是在文本到音乐生成领域。

Method: 采用RVQ离散令牌作为建模表示，发现单层码本足以捕捉音乐语义信息，并将SiMBA（原为Mamba编码器）调整为解码器进行序列建模。

Result: 在有限资源下，SiMBA比标准Transformer解码器收敛更快，生成结果更接近真实音乐。

Conclusion: SSMs（如SiMBA）在高效且富有表现力的文本到音乐生成中具有潜力。

Abstract: The recent surge in State Space Models (SSMs), particularly the emergence of
Mamba, has established them as strong alternatives or complementary modules to
Transformers across diverse domains. In this work, we aim to explore the
potential of Mamba-based architectures for text-to-music generation. We adopt
discrete tokens of Residual Vector Quantization (RVQ) as the modeling
representation and empirically find that a single-layer codebook can capture
semantic information in music. Motivated by this observation, we focus on
modeling a single-codebook representation and adapt SiMBA, originally designed
as a Mamba-based encoder, to function as a decoder for sequence modeling. We
compare its performance against a standard Transformer-based decoder. Our
results suggest that, under limited-resource settings, SiMBA achieves much
faster convergence and generates outputs closer to the ground truth. This
demonstrates the promise of SSMs for efficient and expressive text-to-music
generation. We put audio examples on Github.

</details>


### [21] [Constraint Optimized Multichannel Mixer-limiter Design](https://arxiv.org/abs/2507.06769)
*Yuancheng Luo,Dmitriy Yamkovoy,Guillermo Garcia*

Main category: cs.SD

TL;DR: 提出了一种耦合混音器-限制器-包络设计，通过线性约束二次规划优化多通道增益变量，降低失真并提高实时处理效率。


<details>
  <summary>Details</summary>
Motivation: 传统多通道音频混音器和限制器设计因计算复杂度和运行时成本高而分离，本研究旨在通过耦合设计解决这一问题。

Method: 采用线性约束二次规划，结合非对称常数重叠加窗优化、目标函数近似及变量和约束减少方法。

Result: 实验表明耦合设计能有效降低失真，并展示了高效实时处理的计算权衡。

Conclusion: 耦合设计在多通道音频处理中具有显著优势，为实时应用提供了可行的解决方案。

Abstract: Multichannel audio mixer and limiter designs are conventionally decoupled for
content reproduction over loudspeaker arrays due to high computational
complexity and run-time costs. We propose a coupled mixer-limiter-envelope
design formulated as an efficient linear-constrained quadratic program that
minimizes a distortion objective over multichannel gain variables subject to
sample mixture constraints. Novel methods for asymmetric constant overlap-add
window optimization, objective function approximation, variable and constraint
reduction are presented. Experiments demonstrate distortion reduction of the
coupled design, and computational trade-offs required for efficient real-time
processing.

</details>


### [22] [Revealing the Hidden Temporal Structure of HubertSoft Embeddings based on the Russian Phonetic Corpus](https://arxiv.org/abs/2507.06794)
*Anastasia Ananeva,Anton Tomilov,Marina Volkova*

Main category: cs.SD

TL;DR: 研究探讨了HuBERTSoft模型在音素边界处的嵌入是否捕捉音素身份和时序结构，发现其能有效编码音素过渡和发音细节。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习模型（如HuBERTSoft）是否在音素边界处保留时序结构，以验证其对音素过渡的编码能力。

Method: 使用CORPRES俄语语料库，标记20毫秒嵌入窗口的音素三元组，训练神经网络预测音素位置，并通过多种指标评估时序敏感性。

Result: 边界处的嵌入能捕捉音素身份和时序顺序，边界准确性高，且模型编码了发音细节和协同发音效应。

Conclusion: 研究揭示了自监督学习语音表征的内部结构，表明其在音系分析和细粒度转录任务中的潜力。

Abstract: Self-supervised learning (SSL) models such as Wav2Vec 2.0 and HuBERT have
shown remarkable success in extracting phonetic information from raw audio
without labelled data. While prior work has demonstrated that SSL embeddings
encode phonetic features at the frame level, it remains unclear whether these
models preserve temporal structure, specifically, whether embeddings at phoneme
boundaries reflect the identity and order of adjacent phonemes. This study
investigates the extent to which boundary-sensitive embeddings from HubertSoft,
a soft-clustering variant of HuBERT, encode phoneme transitions. Using the
CORPRES Russian speech corpus, we labelled 20 ms embedding windows with
triplets of phonemes corresponding to their start, centre, and end segments. A
neural network was trained to predict these positions separately, and multiple
evaluation metrics, such as ordered, unordered accuracy and a flexible centre
accuracy, were used to assess temporal sensitivity. Results show that
embeddings extracted at phoneme boundaries capture both phoneme identity and
temporal order, with especially high accuracy at segment boundaries. Confusion
patterns further suggest that the model encodes articulatory detail and
coarticulatory effects. These findings contribute to our understanding of the
internal structure of SSL speech representations and their potential for
phonological analysis and fine-grained transcription tasks.

</details>


### [23] [Data-Balanced Curriculum Learning for Audio Question Answering](https://arxiv.org/abs/2507.06815)
*Gijs Wijngaard,Elia Formisano,Michele Esposito,Michel Dumontier*

Main category: cs.SD

TL;DR: 论文提出了一种结合课程学习和统计数据平衡的方法，以解决音频问答（AQA）中的数据集不平衡和训练不稳定性问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前音频问答模型在处理数据集不平衡和训练动态不稳定时表现不佳，需要一种更有效的方法来优化模型训练。

Method: 通过语言模型标注问题难度，采用从易到难的渐进训练策略，并结合统计过滤去除过表示的音频类别，同时使用引导解码约束输出格式。

Result: 在DCASE 2025训练集和五个公开数据集上的实验表明，该方法比基线模型准确率提高了11.7%，在DCASE 2025基准上达到64.2%。

Conclusion: 结合课程学习和数据平衡的方法有效提升了音频问答模型的性能，为类似任务提供了新的解决方案。

Abstract: Audio question answering (AQA) requires models to understand acoustic content
and perform complex reasoning. Current models struggle with dataset imbalances
and unstable training dynamics. This work combines curriculum learning with
statistical data balancing to address these challenges. The method labels
question difficulty using language models, then trains progressively from easy
to hard examples. Statistical filtering removes overrepresented audio
categories, and guided decoding constrains outputs to valid multiple-choice
formats. Experiments on the DCASE 2025 training set and five additional public
datasets show that data curation improves accuracy by 11.7% over baseline
models, achieving 64.2% on the DCASE 2025 benchmark.

</details>


### [24] [Physics-Informed Direction-Aware Neural Acoustic Fields](https://arxiv.org/abs/2507.06826)
*Yoshiki Masuyama,François G. Germain,Gordon Wichern,Christopher Ick,Jonathan Le Roux*

Main category: cs.SD

TL;DR: 本文提出了一种基于物理信息的神经网络（PINN）用于建模一阶Ambisonic（FOA）房间脉冲响应（RIRs）。通过结合神经网络的强大建模能力和声波传播的物理原理，PINN在声场插值中表现出色。


<details>
  <summary>Details</summary>
Motivation: FOA RIRs不仅提供空间特性，还可用于沉浸式音频生成，具有广泛应用。然而，现有PINN方法主要针对全向麦克风测量的声压，未充分利用FOA RIRs的物理特性。

Method: 扩展PINN框架以建模FOA RIRs，并基于粒子速度与FOA（X, Y, Z）通道的对应关系，推导了两个物理信息先验。这些先验通过偏导数关联预测的W通道与其他通道，并在四个通道上施加物理可行关系。

Result: 实验证实，与无物理信息先验的神经网络相比，所提方法更有效。

Conclusion: 通过引入物理信息先验，PINN能够更准确地建模FOA RIRs，为沉浸式音频生成提供更可靠的工具。

Abstract: This paper presents a physics-informed neural network (PINN) for modeling
first-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have
demonstrated promising performance in sound field interpolation by combining
the powerful modeling capability of neural networks and the physical principles
of sound propagation. In room acoustics, PINNs have typically been trained to
represent the sound pressure measured by omnidirectional microphones where the
wave equation or its frequency-domain counterpart, i.e., the Helmholtz
equation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial
characteristics and are useful for immersive audio generation with a wide range
of applications. In this paper, we extend the PINN framework to model FOA RIRs.
We derive two physics-informed priors for FOA RIRs based on the correspondence
between the particle velocity and the (X, Y, Z)-channels of FOA. These priors
associate the predicted W-channel and other channels through their partial
derivatives and impose the physically feasible relationship on the four
channels. Our experiments confirm the effectiveness of the proposed method
compared with a neural network without the physics-informed prior.

</details>


### [25] [Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation](https://arxiv.org/abs/2507.07043)
*Haris Khan,Shumaila Asif,Hassan Nasir*

Main category: cs.SD

TL;DR: 本文综述了人工智能在助听器中的应用，重点分析了选择性噪声消除技术的进展、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统助听器基于放大技术，而AI驱动的智能音频处理有望提供更优的解决方案。本文旨在评估AI在选择性噪声消除中的技术演进和实际应用挑战。

Method: 通过系统文献综述，分析了深度学习架构（如卷积循环网络和Transformer）、硬件部署策略、临床验证及用户中心设计。

Result: 最新模型在噪声-混响基准上实现了18.3 dB的SI-SDR提升，实时处理延迟低于10毫秒，临床效果显著。但仍存在功耗、环境适应性和个性化等挑战。

Conclusion: 未来研究需关注轻量模型、持续学习、上下文分类及临床转化，以推动AI助听器的全球应用。

Abstract: The integration of artificial intelligence into hearing assistance marks a
paradigm shift from traditional amplification-based systems to intelligent,
context-aware audio processing. This systematic literature review evaluates
advances in AI-driven selective noise cancellation (SNC) for hearing aids,
highlighting technological evolution, implementation challenges, and future
research directions. We synthesize findings across deep learning architectures,
hardware deployment strategies, clinical validation studies, and user-centric
design. The review traces progress from early machine learning models to
state-of-the-art deep networks, including Convolutional Recurrent Networks for
real-time inference and Transformer-based architectures for high-accuracy
separation. Key findings include significant gains over traditional methods,
with recent models achieving up to 18.3 dB SI-SDR improvement on
noisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and
promising clinical outcomes. Yet, challenges remain in bridging lab-grade
models with real-world deployment - particularly around power constraints,
environmental variability, and personalization. Identified research gaps
include hardware-software co-design, standardized evaluation protocols, and
regulatory considerations for AI-enhanced hearing devices. Future work must
prioritize lightweight models, continual learning, contextual-based
classification and clinical translation to realize transformative hearing
solutions for millions globally.

</details>


### [26] [A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering](https://arxiv.org/abs/2507.07046)
*Shahana Yasmin Chowdhury,Bithi Banik,Md Tamjidul Hoque,Shreya Banerjee*

Main category: cs.SD

TL;DR: 提出DCRF-BiLSTM模型用于语音情感识别，在多个数据集上表现优异，综合准确率达93.76%。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别在人机交互和人工智能发展中至关重要，但现有研究未在多个基准数据集上全面评估单一模型。

Method: 使用DCRF-BiLSTM模型识别七种情感，并在五个数据集（RAVDESS、TESS、SAVEE、EmoDB、Crema-D）上训练和测试。

Result: 模型在单个数据集上准确率高达97.83%至100%，综合数据集上达98.82%，首次在五个数据集上综合评估，整体准确率为93.76%。

Conclusion: DCRF-BiLSTM模型具有鲁棒性和泛化能力，适用于多样化的语音情感识别任务。

Abstract: Nowadays, speech emotion recognition (SER) plays a vital role in the field of
human-computer interaction (HCI) and the evolution of artificial intelligence
(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:
neutral, happy, sad, angry, fear, disgust, and surprise, which are trained on
five datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).
The model achieves high accuracy on individual datasets, including 97.83% on
RAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS
and EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,
outperforming previously reported results. To our knowledge, no existing study
has evaluated a single SER model across all five benchmark datasets (i.e.,
R+T+S+C+E) simultaneously. In our work, we introduce this comprehensive
combination and achieve a remarkable overall accuracy of 93.76%. These results
confirm the robustness and generalizability of our DCRF-BiLSTM framework across
diverse datasets.

</details>


### [27] [Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification](https://arxiv.org/abs/2507.07058)
*Martin Sondermann,Pinar Bisgin,Niklas Tschorn,Anja Burmann,Christoph M. Friedrich*

Main category: cs.SD

TL;DR: 论文比较了四种模型（两种CNN和两种BEATs变换器）用于心音图分类，发现CNN性能更优，但BEATs在开发效率上有优势。


<details>
  <summary>Details</summary>
Motivation: 探索不同模型和归一化方法对心音图自动分类性能的影响，为临床诊断提供指导。

Method: 使用PhysioNet2022数据集，比较两种CNN和两种BEATs变换器，采用固定长度和心周期归一化方法。

Result: CNN固定长度窗口AUROC为79.5%，心周期归一化为75.4%；BEATs固定长度窗口为65.7%，心周期归一化为70.1%。

Conclusion: CNN性能更优，但BEATs在开发效率上有潜力；需平衡准确性和计算效率，以提升心脏诊断自动化。

Abstract: The automated classification of phonocardiogram (PCG) recordings represents a
substantial advancement in cardiovascular diagnostics. This paper presents a
systematic comparison of four distinct models for heart murmur detection: two
specialized convolutional neural networks (CNNs) and two zero-shot universal
audio transformers (BEATs), evaluated using fixed-length and heart cycle
normalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart
cycle normalization method tailored to individual cardiac rhythms is
introduced. The findings indicate the following AUROC values: the CNN model
with fixed-length windowing achieves 79.5%, the CNN model with heart cycle
normalization scores 75.4%, the BEATs transformer with fixed-length windowing
achieves 65.7%, and the BEATs transformer with heart cycle normalization
results in 70.1%.
  The findings indicate that physiological signal constraints, especially those
introduced by different normalization strategies, have a substantial impact on
model performance. The research provides evidence-based guidelines for
architecture selection in clinical settings, emphasizing the need for a balance
between accuracy and computational efficiency. Although specialized CNNs
demonstrate superior performance overall, the zero-shot transformer models may
offer promising efficiency advantages during development, such as faster
training and evaluation cycles, despite their lower classification accuracy.
These findings highlight the potential of automated classification systems to
enhance cardiac diagnostics and improve patient care.

</details>


### [28] [Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach](https://arxiv.org/abs/2507.07066)
*Adrian S. Roman,Iran R. Roman,Juan P. Bello*

Main category: cs.SD

TL;DR: 提出了一种自监督的潜在声学映射（LAM）模型，结合传统方法的可解释性和深度学习的效率，用于高分辨率声学映射和方向估计。


<details>
  <summary>Details</summary>
Motivation: 传统波束形成方法计算量大且对声学变化敏感，而深度学习需要大量标注数据且缺乏可解释性，两者均难以适应多样化的声学设置和阵列配置。

Method: 引入LAM模型，一种自监督框架，生成高分辨率声学映射，适应不同声学条件和麦克风阵列。

Result: 在LOCATA和STARSS基准测试中，LAM的定位性能与现有监督方法相当或更优，其声学映射还可作为监督模型的有效特征。

Conclusion: LAM模型在自适应高性能声音定位系统中具有潜力，结合了可解释性和效率。

Abstract: Acoustic mapping techniques have long been used in spatial audio processing
for direction of arrival estimation (DoAE). Traditional beamforming methods for
acoustic mapping, while interpretable, often rely on iterative solvers that can
be computationally intensive and sensitive to acoustic variability. On the
other hand, recent supervised deep learning approaches offer feedforward speed
and robustness but require large labeled datasets and lack interpretability.
Despite their strengths, both methods struggle to consistently generalize
across diverse acoustic setups and array configurations, limiting their broader
applicability. We introduce the Latent Acoustic Mapping (LAM) model, a
self-supervised framework that bridges the interpretability of traditional
methods with the adaptability and efficiency of deep learning methods. LAM
generates high-resolution acoustic maps, adapts to varying acoustic conditions,
and operates efficiently across different microphone arrays. We assess its
robustness on DoAE using the LOCATA and STARSS benchmarks. LAM achieves
comparable or superior localization performance to existing supervised methods.
Additionally, we show that LAM's acoustic maps can serve as effective features
for supervised models, further enhancing DoAE accuracy and underscoring its
potential to advance adaptive, high-performance sound localization systems.

</details>

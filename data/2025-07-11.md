<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Three-Dimensional Millimeter-Wave Imaging Using Active Incoherent Fourier Processing and Pulse Compression](https://arxiv.org/abs/2507.07239)
*Jorge R. Colon-Berrios,Jason M. Merlo,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 提出了一种结合二维空间傅里叶域成像技术和传统雷达脉冲压缩的新型三维成像方法，通过噪声信号和线性调频信号的联合处理实现三维场景重建。


<details>
  <summary>Details</summary>
Motivation: 传统成像技术难以同时实现高分辨率的三维成像，因此需要一种新的方法结合空间和时间信息。

Method: 使用四个发射器，其中三个发射空间和时间不相关的噪声信号，第四个发射已知的线性调频脉冲信号，通过干涉处理和匹配滤波实现三维成像。

Result: 通过仿真和实验数据（38 GHz毫米波成像系统）验证了该方法能够重建目标的三维图像。

Conclusion: 该方法成功实现了三维场景的高分辨率成像，为复杂环境下的目标检测提供了新思路。

Abstract: We present a novel three-dimensional (3D) imaging approach that combines
two-dimensional spatial Fourier-domain imaging techniques with traditional
radar pulse compression to recover both cross-range and down-range scene
information. The imaging system employs four transmitters, three of which emit
spatially and temporally incoherent noise signals, while the fourth transmits a
known linear frequency modulated (LFM) pulsed signal. The spatial incoherence
of the noise signals enables sampling of the 2D spatial Fourier spectrum of the
scene from which two-dimensional cross-range (azimuth and elevation) images can
be formed via interferometric processing. Simultaneously, the LFM signal
enables high-resolution downrange imaging through matched filtering. The
received signals consist of a superposition of the noise sources and the known
pulse allowing for joint recovery of all three dimensions. We describe the
system architecture and waveform design, and demonstrate the imaging technique
using both simulations with a linear array and experimental data from a 38 GHz
active incoherent millimeter-wave imaging system with 23-element randomized
array. Results show the reconstruction of targets in three dimensions.

</details>


### [2] [A RIS-Enabled Computational Radar Coincidence Imaging](https://arxiv.org/abs/2507.07285)
*Kavian Zirak,Mohammadreza F. Imani*

Main category: eess.SP

TL;DR: 论文提出了一种结合雷达重合成像（RCI）和计算成像技术的创新成像方法，利用可重构智能表面（RISs）生成空间多样的散斑图案，实现高信噪比和低杂波的成像效果。


<details>
  <summary>Details</summary>
Motivation: 传统计算成像依赖随机图案，而扫描成像需要大量测量。本文旨在结合两者的优势，通过RISs生成定向光束的散斑图案，提高成像质量和效率。

Method: 利用RISs将光束重定向至感兴趣区域（ROI），形成空间多样的散斑图案。通过计算成像框架，仅需少量测量即可重构高质量图像。

Result: 数值模拟表明，该方法在信噪比和杂波抑制方面优于传统技术，适用于安全筛查、无线用户跟踪和活动识别。

Conclusion: 该方法通过RISs和计算成像的结合，实现了高效、高质量的成像，具有广泛的应用潜力。

Abstract: This paper introduces an innovative imaging method using reconfigurable
intelligent surfaces (RISs) by combining radar coincidence imaging (RCI) and
computational imaging techniques. In the proposed framework, RISs
simultaneously redirect beams toward a desired region of interest (ROI). The
interference of these beams forms spatially diverse speckle patterns that carry
information about the entire ROI. As a result, this method can take advantage
of the benefits of both random patterns and spotlight imaging. Since the
speckle pattern is formed by directive beams (instead of random patterns
typically used in computational imaging), this approach results in a higher
signal-to-noise ratio (SNR) and reduced clutter. In contrast to raster
scanning, which requires the number of measurements to be at least equal to the
number of unknowns, our proposed approach follows a computational imaging
framework and can obtain high-quality images even when only a few measurements
are taken. Using numerical simulation, we demonstrate this method's
capabilities and contrast it against other conventional techniques. The
proposed imaging approach can be applied to security screening, wireless user
tracking, and activity recognition.

</details>


### [3] [mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar](https://arxiv.org/abs/2507.07331)
*Anurag Pallaprolu,Winston Hurst,Yasamin Mostofi*

Main category: eess.SP

TL;DR: 提出了一种基于毫米波雷达提取人群运动模式并推断语义的新框架，结合信号处理和几何图分析，实现了高保真的人群流动表征和语义推断。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在复杂人群流动模式中难以提取高保真运动信息和语义的问题。

Method: 结合光学流估计与噪声过滤生成毫米波流场，转换为有向几何图，并通过雅可比矩阵分析提取语义。

Result: 在21次实验中，框架成功重建复杂人群流动结构，并准确推断出流动转向、边界变化等语义。

Conclusion: 验证了框架的有效性，展示了其在人群分析应用中的潜力。

Abstract: In this paper, we present a novel framework for extracting underlying crowd
motion patterns and inferring crowd semantics using mmWave radar. First, our
proposed signal processing pipeline combines optical flow estimation concepts
from vision with novel statistical and morphological noise filtering to
generate high-fidelity mmWave flow fields - compact 2D vector representations
of crowd motion. We then introduce a novel approach that transforms these
fields into directed geometric graphs, where edges capture dominant flow
currents, vertices mark crowd splitting or merging, and flow distribution is
quantified across edges. Finally, we show that by analyzing the local Jacobian
and computing the corresponding curl and divergence, we can extract key crowd
semantics for both structured and diffused crowds. We conduct 21 experiments on
crowds of up to (and including) 20 people across 3 areas, using commodity
mmWave radar. Our framework achieves high-fidelity graph reconstruction of the
underlying flow structure, even for complex crowd patterns, demonstrating
strong spatial alignment and precise quantitative characterization of flow
split ratios. Finally, our curl and divergence analysis accurately infers key
crowd semantics, e.g., abrupt turns, boundaries where flow directions shift,
dispersions, and gatherings. Overall, these findings validate our framework,
underscoring its potential for various crowd analytics applications.

</details>


### [4] [Featureless Wireless Communications using Enhanced Autoencoder](https://arxiv.org/abs/2507.07474)
*Ruhui Zhang,Wei Lin,Binbin Chen*

Main category: eess.SP

TL;DR: 论文提出了一种基于自动编码器（AE）的无线通信方法，通过改进损失函数和输入编码方式，生成低检测/拦截概率（LPD/LPI）的特征缺失信号，并显著降低块错误率（BLER）。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用AE生成特征缺失信号，以提高无线通信的安全性和可靠性。

Method: 1. 引入包含KL散度项的损失函数，增强信号噪声特性；2. 用预编码的二进制输入替代独热编码，支持长源消息块。

Result: 实验表明，该方法提升了信号的特征缺失性，并显著降低了BLER。

Conclusion: 基于AE的方法在安全和可靠的无线通信系统中具有潜力。

Abstract: Artificial intelligence (AI) techniques, particularly autoencoders (AEs),
have gained significant attention in wireless communication systems. This paper
investigates using an AE to generate featureless signals with a low probability
of detection and interception (LPD/LPI). Firstly, we introduce a novel loss
function that adds a KL divergence term to the categorical cross entropy,
enhancing the noise like characteristics of AE-generated signals while
preserving block error rate (BLER). Secondly, to support long source message
blocks for the AE's inputs, we replace one-hot inputs of source blocks with
binary inputs pre-encoded by conventional error correction coding schemes. The
AE's outputs are then decoded back to the source blocks using the same scheme.
This design enables the AE to learn the coding structure, yielding superior
BLER performance on coded blocks and the BLER of the source blocks is further
decreased by the error correction decoder. Moreover, we also validate the AE
based communication system in the over-the-air communication. Experimental
results demonstrate that our proposed methods improve the featureless
properties of AE signals and significantly reduce the BLER of message blocks,
underscoring the promise of our AE-based approach for secure and reliable
wireless communication systems.

</details>


### [5] [Leveraging Power Amplifier Distortion for Physical Layer Security](https://arxiv.org/abs/2507.07567)
*Reza Ghasemi Alavicheh,Thomas Feys,MD Arifur Rahman,François Rottenberg*

Main category: eess.SP

TL;DR: 提出了一种利用功率放大器非线性失真的物理层安全新方法，通过Z3RO预编码器将失真导向非用户位置以增强安全性。


<details>
  <summary>Details</summary>
Motivation: 传统物理层安全技术通常注入正交于合法信道的人工噪声，而本文发现功率放大器的非线性失真可被利用来提升安全性。

Method: 采用Z3RO预编码器，通过多天线负极性抵消用户位置的失真，将失真导向非用户位置以干扰潜在窃听者。

Result: 数值模拟显示，Z3RO预编码器在特定条件下比传统最大比传输预编码提升2.5倍的保密速率。

Conclusion: 功率放大器的非线性失真可被有效利用以增强物理层安全性能。

Abstract: This paper introduces a new approach to physical layer security (PLS) by
leveraging power amplifier (PA) nonlinear distortion through distortion-aware
precoding. While some conventional PLS techniques inject artificial noise
orthogonal to legitimate channels, we demonstrate that inherent PA
nonlinearities typically considered undesirable can be exploited to enhance
security. The zero 3rd order (Z3RO) precoder applies a negative polarity to
several antennas to cancel the PA distortion at the user location, resulting in
distortion being transmitted in non-user locations. Redirecting the distortion
to non-user locations creates interference for potential eavesdroppers,
lowering their signal-to-noise-and-distortion ratio (SNDR). Numerical
simulations reveal that the Z3RO precoder achieves up to a $2.5\times$
improvement in secrecy rate compared to conventional maximum ratio transmission
(MRT) precoding under a $10\%$ outage probability, SNR of $32$ dB and $-5$ dB
input back-off (IBO) where the PAs enter the saturation regime.

</details>


### [6] [RIS-assisted ISAC Systems for Industrial Revolution 6.0: Exploring the Near-field and Far-field Coexistence](https://arxiv.org/abs/2507.07643)
*Seonghoon Yoo,Jaemin Jung,Seongah Jeong,Jinkyu Kang,Markku Juntti,Joonhyuk Kang*

Main category: eess.SP

TL;DR: 本文研究了在近场和远场共存的工业物联网（IIoT）中，可重构智能表面（RIS）辅助的集成感知与通信（ISAC）系统，通过联合优化RIS相位、带宽分配和接收波束成形，显著提升了感知精度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 工业物联网（IIoT）需要无缝集成多种设备，而集成感知与通信（ISAC）是实现实时控制和自动化的关键技术。本文旨在解决近场和远场共存场景下的ISAC性能优化问题。

Method: 系统由全双工接入点（AP）、RIS和多个IIoT设备组成，结合传统感知频段和ISAC频段，采用上行非正交多址（NOMA）技术。通过SCA-AO算法和SDR技术联合优化RIS相位、带宽分配和接收波束成形。

Result: 数值结果表明，所提方法在RIS和设备配置下显著优于仅依赖ISAC或传统感知频段的方法，实现了更高的感知精度和通信效率。

Conclusion: RIS辅助的ISAC系统在近场和远场共存场景下表现出色，为IIoT的实时控制和自动化提供了高效解决方案。

Abstract: The Industrial Internet of Things (IIoT) has emerged as a key technology for
realizing the vision of Industry 6.0, requiring the seamless integration of
diverse connected devices. In particular, integrated sensing and communication
(ISAC) plays a critical role in supporting real-time control and automation
within IIoT systems. In this paper, we explore reconfigurable intelligent
surface (RIS)-assisted ISAC systems for IIoT in the coexistence of near-field
and far-field regions. The system consists of a full-duplex access point (AP),
a RIS and multiple IIoT devices, where the near-field devices simultaneously
perform sensing and communication, while the far-field devices rely on a
RIS-assisted communication. To enhance spectral efficiency for both sensing and
communication functionalities, we consider the use of both traditional
sensing-only (SO) and ISAC frequency bands. Moreover, uplink non-orthogonal
multiple access (NOMA) is employed to facilitate the sequential decoding of
superimposed communication and sensing signals from IIoT devices. To maximize
sensing accuracy in terms of Cram${\Grave{\textrm{e}}}$r-Rao bound (CRB), we
formulate a joint optimization of RIS phase shift, bandwidth splitting ratio
and receive beamforming vector subject to the minimum data rate requirements of
IIoT devices and resource budget constraints. The algorithmic solution is
developed via the successive convex approximation (SCA)-based alternating
optimization (AO) method with the semi-definite relaxation (SDR) technique.
Numerical results demonstrate that the proposed method significantly
outperforms conventional methods relying solely on either ISAC or SO band by
achieving superior performance across RIS and device configurations, while
ensuring robust ISAC performance under the near-field and far-field coexistence
scenarios.

</details>


### [7] [Consistent and Asymptotically Efficient Localization from Bearing-only Measurements](https://arxiv.org/abs/2507.07647)
*Shenghua Hu,Guangyang Zeng,Wenchao Xue,Haitao Fang,Biqiang Mu*

Main category: eess.SP

TL;DR: 论文提出了一种两步估计器，用于解决基于方位测量的信号源定位问题，具有低计算复杂度和与最大似然估计器相同的渐近性能。


<details>
  <summary>Details</summary>
Motivation: 解决最大似然估计器在非凸优化问题中的计算挑战，同时保持其渐近性能。

Method: 通过代数操作构建线性最小二乘问题，获得初步一致估计器，再通过高斯-牛顿迭代实现高效估计。

Result: 仿真结果表明，所提出的两步估计器在大样本量下表现优异。

Conclusion: 该方法在计算效率和渐近性能之间取得了平衡，适用于实际应用。

Abstract: We study the problem of signal source localization using bearing-only
measurements. Initially, we present easily verifiable geometric conditions for
sensor deployment to ensure the asymptotic identifiability of the model and
demonstrate the consistency and asymptotic efficiency of the maximum likelihood
(ML) estimator. However, obtaining the ML estimator is challenging due to its
association with a non-convex optimization problem. To address this, we propose
a two-step estimator that shares the same asymptotic properties as the ML
estimator while offering low computational complexity, linear in the number of
measurements. The primary challenge lies in obtaining a preliminary consistent
estimator in the first step. To achieve this, we construct a linear
least-squares problem through algebraic operations on the measurement nonlinear
model to first obtain a biased closed-form solution. We then eliminate the bias
using the data to yield an asymptotically unbiased and consistent estimator.
The key to this process is obtaining a consistent estimator of the variance of
the sine of the noise by taking the reciprocal of the maximum eigenvalue of a
specially constructed matrix from the data. In the second step, we perform a
single Gauss-Newton iteration using the preliminary consistent estimator as the
initial value, achieving the same asymptotic properties as the ML estimator.
Finally, simulation results demonstrate the superior performance of the
proposed two-step estimator for large sample sizes.

</details>


### [8] [Signal Prediction for Loss Mitigation in Tactile Internet: A Leader-Follower Game-Theoretic Approach](https://arxiv.org/abs/2507.07692)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种基于Stackelberg博弈的Leader-Follower方法，用于预测触觉互联网中的信号丢失，显著降低了延迟要求。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网需要超低延迟和高可靠性，但在丢包和延迟情况下，信号预测成为恢复丢失信号的可行方案。

Method: 采用合作Stackelberg博弈的Leader-Follower方法，使用户和机器人能学习和预测动作。

Result: 预测准确率在人类端为80.62%-95.03%，机器人端为70.44%-89.77%，并通过泰勒展开建立了信号丢失的上限。

Conclusion: 该方法显著提升了触觉互联网的鲁棒性，并放宽了延迟要求。

Abstract: Tactile Internet (TI) requires achieving ultra-low latency and highly
reliable packet delivery for haptic signals. In the presence of packet loss and
delay, the signal prediction method provides a viable solution for recovering
the missing signals. To this end, we introduce the Leader-Follower (LeFo)
approach based on a cooperative Stackelberg game, which enables both users and
robots to learn and predict actions. With accurate prediction, the
teleoperation system can safely relax its strict delay requirements. Our method
achieves high prediction accuracy, ranging from 80.62% to 95.03% for remote
robot signals at the Human ($H$) side and from 70.44% to 89.77% for human
operation signals at the remote Robot ($R$) side. We also establish an upper
bound for maximum signal loss using Taylor Expansion, ensuring robustness.

</details>


### [9] [Flying Base Stations for Offshore Wind Farm Monitoring and Control: Holistic Performance Evaluation and Optimization](https://arxiv.org/abs/2507.07832)
*Xinyi Lin,Peizheng Li,Adnan Aijaz*

Main category: eess.SP

TL;DR: 该论文提出了一种基于飞行基站（FBS）的方法，用于解决海上风电场中可靠低延迟通信的挑战，通过多目标优化框架最小化延迟。


<details>
  <summary>Details</summary>
Motivation: 海上风电场环境恶劣且缺乏基础设施，传统通信方式难以满足实时监控和控制的需求。

Method: 开发了一个端到端延迟模型，结合轨迹规划、波束成形和资源分配的多目标优化框架。

Result: 仿真结果表明，该方法在不同功率水平下均能有效最小化延迟，并优于基线设计。

Conclusion: FBS方法为海上风电场提供了一种灵活、高效的实时通信解决方案。

Abstract: Ensuring reliable and low-latency communication in offshore wind farms is
critical for efficient monitoring and control, yet remains challenging due to
the harsh environment and lack of infrastructure. This paper investigates a
flying base station (FBS) approach for wide-area monitoring and control in the
UK Hornsea offshore wind farm project. By leveraging mobile, flexible FBS
platforms in the remote and harsh offshore environment, the proposed system
offers real-time connectivity for turbines without the need for deploying
permanent infrastructure at the sea. We develop a detailed and practical
end-to-end latency model accounting for five key factors: flight duration,
connection establishment, turbine state information upload, computational
delay, and control transmission, to provide a holistic perspective often
missing in prior studies. Furthermore, we combine trajectory planning,
beamforming, and resource allocation into a multi-objective optimization
framework for the overall latency minimization, specifically designed for
large-scale offshore wind farm deployments. Simulation results verify the
effectiveness of our proposed method in minimizing latency and enhancing
efficiency in FBS-assisted offshore monitoring across various power levels,
while consistently outperforming baseline designs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Generic Speech Enhancement with Self-Supervised Representation Space Loss](https://arxiv.org/abs/2507.07631)
*Hiroshi Sato,Tsubasa Ochiai,Marc Delcroix,Takafumi Moriya,Takanori Ashihara,Ryo Masumura*

Main category: eess.AS

TL;DR: 提出一种通用的语音增强前端，通过自监督学习特征表示优化增强信号与干净信号的距离，提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强需针对不同任务调整，难以泛化到未知任务，本研究旨在构建通用前端以支持多任务。

Method: 提出新训练准则，最小化增强信号与干净信号在自监督学习特征表示域的距离。

Result: 实验证明，该方法在保持增强信号感知质量的同时，提升了多任务性能。

Conclusion: 该通用语音增强前端有效支持多任务，且性能优于传统方法。

Abstract: Single-channel speech enhancement is utilized in various tasks to mitigate
the effect of interfering signals. Conventionally, to ensure the speech
enhancement performs optimally, the speech enhancement has needed to be tuned
for each task. Thus, generalizing speech enhancement models to unknown
downstream tasks has been challenging. This study aims to construct a generic
speech enhancement front-end that can improve the performance of back-ends to
solve multiple downstream tasks. To this end, we propose a novel training
criterion that minimizes the distance between the enhanced and the ground truth
clean signal in the feature representation domain of self-supervised learning
models. Since self-supervised learning feature representations effectively
express high-level speech information useful for solving various downstream
tasks, the proposal is expected to make speech enhancement models preserve such
information. Experimental validation demonstrates that the proposal improves
the performance of multiple speech tasks while maintaining the perceptual
quality of the enhanced signal.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [11] [Audio-Visual Speech Separation via Bottleneck Iterative Network](https://arxiv.org/abs/2507.07270)
*Sidong Zhang,Shiv Shankar,Trang Nguyen,Andrea Fanelli,Madalina Fiterau*

Main category: cs.SD

TL;DR: 提出了一种名为Bottleneck Iterative Network (BIN)的迭代表示细化方法，通过轻量级融合块和融合令牌提升模型性能，同时避免模型规模大幅增加。


<details>
  <summary>Details</summary>
Motivation: 整合非听觉信息可以提高语音分离模型的性能，但现有模型通常过于昂贵或轻量但能力不足。

Method: 使用BIN方法，通过迭代轻量级融合块和融合令牌优化表示，平衡性能与训练成本。

Result: 在NTCD-TIMIT和LRS3+WHAM!数据集上，BIN在SI-SDRi指标上优于现有基准模型，同时训练和GPU推理时间减少50%以上。

Conclusion: BIN方法在提升语音分离性能的同时，显著降低了计算成本。

Abstract: Integration of information from non-auditory cues can significantly improve
the performance of speech-separation models. Often such models use deep
modality-specific networks to obtain unimodal features, and risk being too
costly or lightweight but lacking capacity. In this work, we present an
iterative representation refinement approach called Bottleneck Iterative
Network (BIN), a technique that repeatedly progresses through a lightweight
fusion block, while bottlenecking fusion representations by fusion tokens. This
helps improve the capacity of the model, while avoiding major increase in model
size and balancing between the model performance and training cost. We test BIN
on challenging noisy audio-visual speech separation tasks, and show that our
approach consistently outperforms state-of-the-art benchmark models with
respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously
achieving a reduction of more than 50% in training and GPU inference time
across nearly all settings.

</details>


### [12] [SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion Models](https://arxiv.org/abs/2507.07318)
*Christian Templin,Yanda Zhu,Hao Wang*

Main category: cs.SD

TL;DR: 论文提出了一种名为SonicMotion的端到端模型，用于生成具有动态声源的3D场景空间音频，并提供了两种变体。同时，还发布了一个新的模拟空间音频-字幕对数据集。


<details>
  <summary>Details</summary>
Motivation: 空间音频在沉浸式娱乐（如VR/AR）中至关重要，但现有的一阶Ambisonics（FOA）生成模型需要扩展以支持动态声源。

Method: 提出了SonicMotion模型，包含两种变体，分别针对用户输入和声源定位精度。此外，还创建了一个模拟空间音频-字幕对数据集。

Result: 模型在语义对齐和音频质量上达到了先进水平，并能捕捉所需的空间属性。

Conclusion: SonicMotion模型成功扩展了FOA生成能力，为动态声源3D场景提供了有效解决方案。

Abstract: Spatial audio is an integral part of immersive entertainment, such as VR/AR,
and has seen increasing popularity in cinema and music as well. The most common
format of spatial audio is described as first-order Ambisonics (FOA). We seek
to extend recent advancements in FOA generative AI models to enable the
generation of 3D scenes with dynamic sound sources. Our proposed end-to-end
model, SonicMotion, comes in two variations which vary in their user input and
level of precision in sound source localization. In addition to our model, we
also present a new dataset of simulated spatial audio-caption pairs. Evaluation
of our models demonstrate that they are capable of matching the semantic
alignment and audio quality of state of the art models while capturing the
desired spatial attributes.

</details>


### [13] [VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching](https://arxiv.org/abs/2507.07384)
*Yu Chen,Xinyuan Qian,Hongxu Zhu,Jiadong Wang,Kainan Chen,Haizhou Li*

Main category: cs.SD

TL;DR: 论文提出了一种跨实例音频-视觉定位（CI-AVL）任务，通过利用同一声音事件类别的不同实例图像来定位目标声源，减少对配对数据的依赖并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉声源定位方法在多源场景中难以选择性隔离目标声源，视觉语义特征与声学空间特征存在不对齐，且过度依赖配对数据。

Method: 提出VP-SelDoA方法，通过语义级模态融合和频率-时间ConMamba架构生成目标选择性掩码，并开发语义-空间匹配机制对齐异构特征。

Result: 在VGG-SSL数据集上实验，方法优于现有技术，MAE为12.04，ACC为78.23%。

Conclusion: CI-AVL任务及VP-SelDoA方法有效解决了现有挑战，提升了定位性能。

Abstract: Audio-visual sound source localization (AV-SSL) identifies the position of a
sound source by exploiting the complementary strengths of auditory and visual
signals. However, existing AV-SSL methods encounter three major challenges: 1)
inability to selectively isolate the target sound source in multi-source
scenarios, 2) misalignment between semantic visual features and spatial
acoustic features, and 3) overreliance on paired audio-visual data. To overcome
these limitations, we introduce Cross-Instance Audio-Visual Localization
(CI-AVL), a novel task that leverages images from different instances of the
same sound event category to localize target sound sources, thereby reducing
dependence on paired data while enhancing generalization capabilities. Our
proposed VP-SelDoA tackles this challenging task through a semantic-level
modality fusion and employs a Frequency-Temporal ConMamba architecture to
generate target-selective masks for sound isolation. We further develop a
Semantic-Spatial Matching mechanism that aligns the heterogeneous semantic and
spatial features via integrated cross- and self-attention mechanisms. To
facilitate the CI-AVL research, we construct a large-scale dataset named
VGG-SSL, comprising 13,981 spatial audio clips across 296 sound event
categories. Extensive experiments show that our proposed method outperforms
state-of-the-art audio-visual localization methods, achieving a mean absolute
error (MAE) of 12.04 and an accuracy (ACC) of 78.23%.

</details>


### [14] [DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction](https://arxiv.org/abs/2507.07526)
*Cunhang Fan,Sheng Zhang,Jingjing Zhang,Enrui Liu,Xinhui Li,Minggang Zhao,Zhao Lv*

Main category: cs.SD

TL;DR: 论文提出DMF2Mel网络，通过动态多尺度融合技术解决脑信号解码中长序列建模的挑战，显著提升连续想象语音的mel频谱重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有技术在单词或字母级别的听觉刺激mel频谱重建上取得进展，但在分钟级连续想象语音的精确重建上仍面临挑战，传统模型难以平衡时间依赖建模效率与长序列解码中的信息保留。

Method: 提出DMF2Mel网络，包含四个核心组件：DC-FAM（动态对比特征聚合模块）、HAMS-Net（分层注意力引导多尺度网络）、SplineMap注意力机制和convMamba（双向状态空间模块）。

Result: 在SparrKULee数据集上，DMF2Mel在已知受试者mel频谱重建中Pearson相关系数达0.074（比基线提升48%），未知受试者达0.048（提升35%）。

Conclusion: DMF2Mel通过动态多尺度融合技术有效解决了长序列建模问题，显著提升了连续想象语音的mel频谱重建性能。

Abstract: Decoding speech from brain signals is a challenging research problem.
Although existing technologies have made progress in reconstructing the mel
spectrograms of auditory stimuli at the word or letter level, there remain core
challenges in the precise reconstruction of minute-level continuous imagined
speech: traditional models struggle to balance the efficiency of temporal
dependency modeling and information retention in long-sequence decoding. To
address this issue, this paper proposes the Dynamic Multiscale Fusion Network
(DMF2Mel), which consists of four core components: the Dynamic Contrastive
Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided
Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the
bidirectional state space module (convMamba). Specifically, the DC-FAM
separates speech-related "foreground features" from noisy "background features"
through local convolution and global attention mechanisms, effectively
suppressing interference and enhancing the representation of transient signals.
HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of
high-level semantics and low-level details. The SplineMap attention mechanism
integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine
global context modeling with spline-based local fitting. The convMamba captures
long-range temporal dependencies with linear complexity and enhances nonlinear
dynamic modeling capabilities. Results on the SparrKULee dataset show that
DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram
reconstruction for known subjects (a 48% improvement over the baseline) and
0.048 for unknown subjects (a 35% improvement over the baseline).Code is
available at: https://github.com/fchest/DMF2Mel.

</details>


### [15] [Assessing the Alignment of Audio Representations with Timbre Similarity Ratings](https://arxiv.org/abs/2507.07764)
*Haokun Tian,Stefan Lattner,Charalampos Saitis*

Main category: cs.SD

TL;DR: 论文提出了一种评估音频表示与人类音色相似性判断对齐的方法，发现基于CLAP模型和声音匹配模型的风格嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统音色空间方法存在扩展性和泛化性问题，而深度学习在音频质量评估和图像相似性中表现优异，但缺乏足够的人类评分数据训练模型。

Method: 引入指标评估音频表示与人类音色相似性判断的对齐性，比较嵌入距离的绝对值和排名。

Result: CLAP模型和声音匹配模型的风格嵌入显著优于其他表示方法。

Conclusion: 风格嵌入在建模音色相似性方面具有潜力。

Abstract: Psychoacoustical so-called "timbre spaces" map perceptual similarity ratings
of instrument sounds onto low-dimensional embeddings via multidimensional
scaling, but suffer from scalability issues and are incapable of
generalization. Recent results from audio (music and speech) quality assessment
as well as image similarity have shown that deep learning is able to produce
embeddings that align well with human perception while being largely free from
these constraints. Although the existing human-rated timbre similarity data is
not large enough to train deep neural networks (2,614 pairwise ratings on 334
audio samples), it can serve as test-only data for audio models. In this paper,
we introduce metrics to assess the alignment of diverse audio representations
with human judgments of timbre similarity by comparing both the absolute values
and the rankings of embedding distances to human similarity ratings. Our
evaluation involves three signal-processing-based representations, twelve
representations extracted from pre-trained models, and three representations
extracted from a novel sound matching model. Among them, the style embeddings
inspired by image style transfer, extracted from the CLAP model and the sound
matching model, remarkably outperform the others, showing their potential in
modeling timbre similarity.

</details>


### [16] [SecureSpeech: Prompt-based Speaker and Content Protection](https://arxiv.org/abs/2507.07799)
*Belinda Soh Hui Hui,Xiaoxiao Miao,Xin Wang*

Main category: cs.SD

TL;DR: 本文提出了一种基于提示的语音生成管道，通过双重匿名化（说话者身份和内容）保护隐私，同时保持内容保留和音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决语音领域中因身份盗用和说话者重新识别引发的隐私问题。

Method: 1) 生成与源说话者身份无关的说话者身份；2) 使用命名实体识别模型和大语言模型替换敏感内容；3) 通过文本到语音合成模型生成隐私友好的语音。

Result: 实验结果显示在显著保护隐私的同时，保持了良好的内容保留和音频质量。

Conclusion: 该管道有效平衡了隐私保护和语音质量，并探讨了说话者描述对生成语音的潜在偏见。

Abstract: Given the increasing privacy concerns from identity theft and the
re-identification of speakers through content in the speech field, this paper
proposes a prompt-based speech generation pipeline that ensures dual
anonymization of both speaker identity and spoken content. This is addressed
through 1) generating a speaker identity unlinkable to the source speaker,
controlled by descriptors, and 2) replacing sensitive content within the
original text using a name entity recognition model and a large language model.
The pipeline utilizes the anonymized speaker identity and text to generate
high-fidelity, privacy-friendly speech via a text-to-speech synthesis model.
Experimental results demonstrate an achievement of significant privacy
protection while maintaining a decent level of content retention and audio
quality. This paper also investigates the impact of varying speaker
descriptions on the utility and privacy of generated speech to determine
potential biases.

</details>


### [17] [End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced by Semi-supervised Learning](https://arxiv.org/abs/2507.07806)
*Zhao Ren,Rathi Adarshi Rammohan,Kevin Scheck,Sheng Li,Tanja Schultz*

Main category: cs.SD

TL;DR: 论文提出了一种半监督学习方法，结合大规模未标注数据和小规模标注数据，用于语音情感和意图识别，实验表明该方法显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注语音数据成本高昂，限制了机器学习模型的训练，因此需要利用未标注数据提升识别效果。

Method: 采用端到端的声学和语言模型，结合多任务学习，比较了两种半监督学习方法（fix-match和full-match）。

Result: 半监督学习方法显著提升了识别性能，最佳模型的后期融合在平衡指标上分别优于声学和文本基线12.3%和10.4%。

Conclusion: 半监督学习能有效利用未标注数据，提升语音情感和意图识别的性能。

Abstract: Emotion and intent recognition from speech is essential and has been widely
investigated in human-computer interaction. The rapid development of social
media platforms, chatbots, and other technologies has led to a large volume of
speech data streaming from users. Nevertheless, annotating such data manually
is expensive, making it challenging to train machine learning models for
recognition purposes. To this end, we propose applying semi-supervised learning
to incorporate a large scale of unlabelled data alongside a relatively smaller
set of labelled data. We train end-to-end acoustic and linguistic models, each
employing multi-task learning for emotion and intent recognition. Two
semi-supervised learning approaches, including fix-match learning and
full-match learning, are compared. The experimental results demonstrate that
the semi-supervised learning approaches improve model performance in speech
emotion and intent recognition from both acoustic and text data. The late
fusion of the best models outperforms the acoustic and text baselines by joint
recognition balance metrics of 12.3% and 10.4%, respectively.

</details>


### [18] [Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders](https://arxiv.org/abs/2507.07867)
*Dimitrios Bralios,Jonah Casebeer,Paris Smaragdis*

Main category: cs.SD

TL;DR: 提出了一种名为“Re-Bottleneck”的后处理方法，通过调整预训练自编码器的瓶颈结构，优化潜在空间以适应多样化下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器和自编码器通常仅关注重建保真度，忽略了潜在空间结构对多样化应用的适应性。

Method: 在预训练自编码器的瓶颈部分引入“Re-Bottleneck”，通过潜在空间损失训练，赋予用户定义的结构。

Result: 实验表明，该方法能在不牺牲重建质量的情况下优化潜在通道排序、对齐语义嵌入并引入等变性。

Conclusion: Re-Bottleneck框架灵活高效，可定制神经音频模型的表示，满足多样化应用需求。

Abstract: Neural audio codecs and autoencoders have emerged as versatile models for
audio compression, transmission, feature-extraction, and latent-space
generation. However, a key limitation is that most are trained to maximize
reconstruction fidelity, often neglecting the specific latent structure
necessary for optimal performance in diverse downstream applications. We
propose a simple, post-hoc framework to address this by modifying the
bottleneck of a pre-trained autoencoder. Our method introduces a
"Re-Bottleneck", an inner bottleneck trained exclusively through latent space
losses to instill user-defined structure. We demonstrate the framework's
effectiveness in three experiments. First, we enforce an ordering on latent
channels without sacrificing reconstruction quality. Second, we align latents
with semantic embeddings, analyzing the impact on downstream diffusion
modeling. Third, we introduce equivariance, ensuring that a filtering operation
on the input waveform directly corresponds to a specific transformation in the
latent space. Ultimately, our Re-Bottleneck framework offers a flexible and
efficient way to tailor representations of neural audio models, enabling them
to seamlessly meet the varied demands of different applications with minimal
additional training.

</details>


### [19] [Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models](https://arxiv.org/abs/2507.07877)
*Chen Feng,Yicheng Lin,Shaojie Zhuo,Chenzheng Su,Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Xiaopeng Zhang*

Main category: cs.SD

TL;DR: 论文探讨了在资源受限的边缘设备上部署自动语音识别（ASR）模型的挑战，并评估了八种先进的训练后量化（PTQ）方法对两种ASR模型的影响。


<details>
  <summary>Details</summary>
Motivation: 由于边缘设备在内存、计算和功耗上的严格限制，需要量化技术来优化ASR模型的部署。

Method: 研究采用了八种SOTA PTQ方法，对Whisper和Moonshine两种ASR模型进行了系统评估，分析了不同量化配置对权重和激活的影响。

Result: 结果表明，即使使用3位量化，高性能模型仍能保持准确性，尤其是在采用先进PTQ技术时。

Conclusion: 研究为在低功耗边缘设备上优化ASR模型提供了有价值的见解。

Abstract: Recent advances in Automatic Speech Recognition (ASR) have demonstrated
remarkable accuracy and robustness in diverse audio applications, such as live
transcription and voice command processing. However, deploying these models on
resource constrained edge devices (e.g., IoT device, wearables) still presents
substantial challenges due to strict limits on memory, compute and power.
Quantization, particularly Post-Training Quantization (PTQ), offers an
effective way to reduce model size and inference cost without retraining.
Despite its importance, the performance implications of various advanced
quantization methods and bit-width configurations on ASR models remain unclear.
In this work, we present a comprehensive benchmark of eight state-of-the-art
(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and
Moonshine. We systematically evaluate model performances (i.e., accuracy,
memory I/O and bit operations) across seven diverse datasets from the open ASR
leaderboard, analyzing the impact of quantization and various configurations on
both weights and activations. Built on an extension of the LLM compression
toolkit, our framework integrates edge-ASR models, diverse advanced
quantization algorithms, a unified calibration and evaluation data pipeline,
and detailed analysis tools. Our results characterize the trade-offs between
efficiency and accuracy, demonstrating that even 3-bit quantization can succeed
on high capacity models when using advanced PTQ techniques. These findings
provide valuable insights for optimizing ASR models on low-power, always-on
edge devices.

</details>


### [20] [LISTEN: Lightweight Industrial Sound-representable Transformer for Edge Notification](https://arxiv.org/abs/2507.07879)
*Changheon Han,Yun Seok Kang,Yuseop Sim,Martin Byung-Guk Jun,Hyung Wook Park*

Main category: cs.SD

TL;DR: 提出了一种轻量级工业声音基础模型LISTEN，适用于边缘设备，解决了传统深度学习模型依赖大数据和高计算资源的问题。


<details>
  <summary>Details</summary>
Motivation: 工业声学分析需要大量标注数据和高计算资源，限制了其在生产现场的广泛应用。

Method: 采用知识蒸馏技术，开发了轻量级模型LISTEN，能在低成本边缘设备上实时运行。

Result: LISTEN在基准任务中表现接近其大型父模型，且只需少量数据和训练资源。

Conclusion: LISTEN成功集成到边缘设备中，验证了其在实际生产环境中的性能和泛化能力。

Abstract: Deep learning-based machine listening is broadening the scope of industrial
acoustic analysis for applications like anomaly detection and predictive
maintenance, thereby improving manufacturing efficiency and reliability.
Nevertheless, its reliance on large, task-specific annotated datasets for every
new task limits widespread implementation on shop floors. While emerging sound
foundation models aim to alleviate data dependency, they are too large and
computationally expensive, requiring cloud infrastructure or high-end hardware
that is impractical for on-site, real-time deployment. We address this gap with
LISTEN (Lightweight Industrial Sound-representable Transformer for Edge
Notification), a kilobyte-sized industrial sound foundation model. Using
knowledge distillation, LISTEN runs in real-time on low-cost edge devices. On
benchmark downstream tasks, it performs nearly identically to its much larger
parent model, even when fine-tuned with minimal datasets and training resource.
Beyond the model itself, we demonstrate its real-world utility by integrating
LISTEN into a complete machine monitoring framework on an edge device with an
Industrial Internet of Things (IIoT) sensor and system, validating its
performance and generalization capabilities on a live manufacturing shop floor.

</details>


### [21] [Input Conditioned Layer Dropping in Speech Foundation Models](https://arxiv.org/abs/2507.07954)
*Abdul Hannan,Daniele Falavigna,Alessio Brutti*

Main category: cs.SD

TL;DR: 提出了一种基于输入驱动的层丢弃方法（LD），用于动态调整语音模型的推理计算负载，优于随机丢弃，性能与早期退出相当或更好。


<details>
  <summary>Details</summary>
Motivation: 在边缘和物联网环境中，计算资源随时间变化，需要动态架构来适应。现有层丢弃方法在层选择或架构修改方面存在局限。

Method: 提出输入驱动的LD，利用输入特征和轻量级层选择网络确定最佳处理层组合。

Result: 在4个语音和音频基准测试中，使用两种预训练基础模型，验证了方法的有效性。

Conclusion: 输入驱动的LD显著优于随机丢弃，性能与早期退出相当或更好，适用于动态计算资源环境。

Abstract: Curating foundation speech models for edge and IoT settings, where
computational resources vary over time, requires dynamic architectures
featuring adaptable reduction strategies. One emerging approach is layer
dropping ($\mathcal{LD}$) which skips fraction of the layers of a backbone
network during inference to reduce the computational load. This allows
transforming static models into dynamic ones. However, existing approaches
exhibit limitations either in the mode of selecting layers or by significantly
modifying the neural architecture. To this end, we propose input-driven
$\mathcal{LD}$ that employs the network's input features and a lightweight
layer selecting network to determine the optimum combination of processing
layers. Extensive experimentation on 4 speech and audio public benchmarks,
using two different pre-trained foundation models, demonstrates the
effectiveness of our approach, thoroughly outperforming random dropping and
producing on-par (or better) results to early exit.

</details>

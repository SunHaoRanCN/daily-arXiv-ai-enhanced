{"id": "2507.07239", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07239", "abs": "https://arxiv.org/abs/2507.07239", "authors": ["Jorge R. Colon-Berrios", "Jason M. Merlo", "Jeffrey A. Nanzer"], "title": "Three-Dimensional Millimeter-Wave Imaging Using Active Incoherent Fourier Processing and Pulse Compression", "comment": null, "summary": "We present a novel three-dimensional (3D) imaging approach that combines\ntwo-dimensional spatial Fourier-domain imaging techniques with traditional\nradar pulse compression to recover both cross-range and down-range scene\ninformation. The imaging system employs four transmitters, three of which emit\nspatially and temporally incoherent noise signals, while the fourth transmits a\nknown linear frequency modulated (LFM) pulsed signal. The spatial incoherence\nof the noise signals enables sampling of the 2D spatial Fourier spectrum of the\nscene from which two-dimensional cross-range (azimuth and elevation) images can\nbe formed via interferometric processing. Simultaneously, the LFM signal\nenables high-resolution downrange imaging through matched filtering. The\nreceived signals consist of a superposition of the noise sources and the known\npulse allowing for joint recovery of all three dimensions. We describe the\nsystem architecture and waveform design, and demonstrate the imaging technique\nusing both simulations with a linear array and experimental data from a 38 GHz\nactive incoherent millimeter-wave imaging system with 23-element randomized\narray. Results show the reconstruction of targets in three dimensions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e8c\u7ef4\u7a7a\u95f4\u5085\u91cc\u53f6\u57df\u6210\u50cf\u6280\u672f\u548c\u4f20\u7edf\u96f7\u8fbe\u8109\u51b2\u538b\u7f29\u7684\u65b0\u578b\u4e09\u7ef4\u6210\u50cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u4fe1\u53f7\u548c\u7ebf\u6027\u8c03\u9891\u4fe1\u53f7\u7684\u8054\u5408\u5904\u7406\u5b9e\u73b0\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u6210\u50cf\u6280\u672f\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u4e09\u7ef4\u6210\u50cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u7ed3\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u53d1\u5c04\u5668\uff0c\u5176\u4e2d\u4e09\u4e2a\u53d1\u5c04\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0d\u76f8\u5173\u7684\u566a\u58f0\u4fe1\u53f7\uff0c\u7b2c\u56db\u4e2a\u53d1\u5c04\u5df2\u77e5\u7684\u7ebf\u6027\u8c03\u9891\u8109\u51b2\u4fe1\u53f7\uff0c\u901a\u8fc7\u5e72\u6d89\u5904\u7406\u548c\u5339\u914d\u6ee4\u6ce2\u5b9e\u73b0\u4e09\u7ef4\u6210\u50cf\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u6570\u636e\uff0838 GHz\u6beb\u7c73\u6ce2\u6210\u50cf\u7cfb\u7edf\uff09\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u91cd\u5efa\u76ee\u6807\u7684\u4e09\u7ef4\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4e09\u7ef4\u573a\u666f\u7684\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07285", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07285", "abs": "https://arxiv.org/abs/2507.07285", "authors": ["Kavian Zirak", "Mohammadreza F. Imani"], "title": "A RIS-Enabled Computational Radar Coincidence Imaging", "comment": null, "summary": "This paper introduces an innovative imaging method using reconfigurable\nintelligent surfaces (RISs) by combining radar coincidence imaging (RCI) and\ncomputational imaging techniques. In the proposed framework, RISs\nsimultaneously redirect beams toward a desired region of interest (ROI). The\ninterference of these beams forms spatially diverse speckle patterns that carry\ninformation about the entire ROI. As a result, this method can take advantage\nof the benefits of both random patterns and spotlight imaging. Since the\nspeckle pattern is formed by directive beams (instead of random patterns\ntypically used in computational imaging), this approach results in a higher\nsignal-to-noise ratio (SNR) and reduced clutter. In contrast to raster\nscanning, which requires the number of measurements to be at least equal to the\nnumber of unknowns, our proposed approach follows a computational imaging\nframework and can obtain high-quality images even when only a few measurements\nare taken. Using numerical simulation, we demonstrate this method's\ncapabilities and contrast it against other conventional techniques. The\nproposed imaging approach can be applied to security screening, wireless user\ntracking, and activity recognition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u96f7\u8fbe\u91cd\u5408\u6210\u50cf\uff08RCI\uff09\u548c\u8ba1\u7b97\u6210\u50cf\u6280\u672f\u7684\u521b\u65b0\u6210\u50cf\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RISs\uff09\u751f\u6210\u7a7a\u95f4\u591a\u6837\u7684\u6563\u6591\u56fe\u6848\uff0c\u5b9e\u73b0\u9ad8\u4fe1\u566a\u6bd4\u548c\u4f4e\u6742\u6ce2\u7684\u6210\u50cf\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u6210\u50cf\u4f9d\u8d56\u968f\u673a\u56fe\u6848\uff0c\u800c\u626b\u63cf\u6210\u50cf\u9700\u8981\u5927\u91cf\u6d4b\u91cf\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u901a\u8fc7RISs\u751f\u6210\u5b9a\u5411\u5149\u675f\u7684\u6563\u6591\u56fe\u6848\uff0c\u63d0\u9ad8\u6210\u50cf\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u5229\u7528RISs\u5c06\u5149\u675f\u91cd\u5b9a\u5411\u81f3\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\uff0c\u5f62\u6210\u7a7a\u95f4\u591a\u6837\u7684\u6563\u6591\u56fe\u6848\u3002\u901a\u8fc7\u8ba1\u7b97\u6210\u50cf\u6846\u67b6\uff0c\u4ec5\u9700\u5c11\u91cf\u6d4b\u91cf\u5373\u53ef\u91cd\u6784\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "result": "\u6570\u503c\u6a21\u62df\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fe1\u566a\u6bd4\u548c\u6742\u6ce2\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u7b5b\u67e5\u3001\u65e0\u7ebf\u7528\u6237\u8ddf\u8e2a\u548c\u6d3b\u52a8\u8bc6\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7RISs\u548c\u8ba1\u7b97\u6210\u50cf\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u6210\u50cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07331", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07331", "abs": "https://arxiv.org/abs/2507.07331", "authors": ["Anurag Pallaprolu", "Winston Hurst", "Yasamin Mostofi"], "title": "mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar", "comment": null, "summary": "In this paper, we present a novel framework for extracting underlying crowd\nmotion patterns and inferring crowd semantics using mmWave radar. First, our\nproposed signal processing pipeline combines optical flow estimation concepts\nfrom vision with novel statistical and morphological noise filtering to\ngenerate high-fidelity mmWave flow fields - compact 2D vector representations\nof crowd motion. We then introduce a novel approach that transforms these\nfields into directed geometric graphs, where edges capture dominant flow\ncurrents, vertices mark crowd splitting or merging, and flow distribution is\nquantified across edges. Finally, we show that by analyzing the local Jacobian\nand computing the corresponding curl and divergence, we can extract key crowd\nsemantics for both structured and diffused crowds. We conduct 21 experiments on\ncrowds of up to (and including) 20 people across 3 areas, using commodity\nmmWave radar. Our framework achieves high-fidelity graph reconstruction of the\nunderlying flow structure, even for complex crowd patterns, demonstrating\nstrong spatial alignment and precise quantitative characterization of flow\nsplit ratios. Finally, our curl and divergence analysis accurately infers key\ncrowd semantics, e.g., abrupt turns, boundaries where flow directions shift,\ndispersions, and gatherings. Overall, these findings validate our framework,\nunderscoring its potential for various crowd analytics applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u63d0\u53d6\u4eba\u7fa4\u8fd0\u52a8\u6a21\u5f0f\u5e76\u63a8\u65ad\u8bed\u4e49\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u548c\u51e0\u4f55\u56fe\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u4eba\u7fa4\u6d41\u52a8\u8868\u5f81\u548c\u8bed\u4e49\u63a8\u65ad\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u4eba\u7fa4\u6d41\u52a8\u6a21\u5f0f\u4e2d\u96be\u4ee5\u63d0\u53d6\u9ad8\u4fdd\u771f\u8fd0\u52a8\u4fe1\u606f\u548c\u8bed\u4e49\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5149\u5b66\u6d41\u4f30\u8ba1\u4e0e\u566a\u58f0\u8fc7\u6ee4\u751f\u6210\u6beb\u7c73\u6ce2\u6d41\u573a\uff0c\u8f6c\u6362\u4e3a\u6709\u5411\u51e0\u4f55\u56fe\uff0c\u5e76\u901a\u8fc7\u96c5\u53ef\u6bd4\u77e9\u9635\u5206\u6790\u63d0\u53d6\u8bed\u4e49\u3002", "result": "\u572821\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u6210\u529f\u91cd\u5efa\u590d\u6742\u4eba\u7fa4\u6d41\u52a8\u7ed3\u6784\uff0c\u5e76\u51c6\u786e\u63a8\u65ad\u51fa\u6d41\u52a8\u8f6c\u5411\u3001\u8fb9\u754c\u53d8\u5316\u7b49\u8bed\u4e49\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4eba\u7fa4\u5206\u6790\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.07474", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07474", "abs": "https://arxiv.org/abs/2507.07474", "authors": ["Ruhui Zhang", "Wei Lin", "Binbin Chen"], "title": "Featureless Wireless Communications using Enhanced Autoencoder", "comment": null, "summary": "Artificial intelligence (AI) techniques, particularly autoencoders (AEs),\nhave gained significant attention in wireless communication systems. This paper\ninvestigates using an AE to generate featureless signals with a low probability\nof detection and interception (LPD/LPI). Firstly, we introduce a novel loss\nfunction that adds a KL divergence term to the categorical cross entropy,\nenhancing the noise like characteristics of AE-generated signals while\npreserving block error rate (BLER). Secondly, to support long source message\nblocks for the AE's inputs, we replace one-hot inputs of source blocks with\nbinary inputs pre-encoded by conventional error correction coding schemes. The\nAE's outputs are then decoded back to the source blocks using the same scheme.\nThis design enables the AE to learn the coding structure, yielding superior\nBLER performance on coded blocks and the BLER of the source blocks is further\ndecreased by the error correction decoder. Moreover, we also validate the AE\nbased communication system in the over-the-air communication. Experimental\nresults demonstrate that our proposed methods improve the featureless\nproperties of AE signals and significantly reduce the BLER of message blocks,\nunderscoring the promise of our AE-based approach for secure and reliable\nwireless communication systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\uff08AE\uff09\u7684\u65e0\u7ebf\u901a\u4fe1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u8f93\u5165\u7f16\u7801\u65b9\u5f0f\uff0c\u751f\u6210\u4f4e\u68c0\u6d4b/\u62e6\u622a\u6982\u7387\uff08LPD/LPI\uff09\u7684\u7279\u5f81\u7f3a\u5931\u4fe1\u53f7\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u5757\u9519\u8bef\u7387\uff08BLER\uff09\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528AE\u751f\u6210\u7279\u5f81\u7f3a\u5931\u4fe1\u53f7\uff0c\u4ee5\u63d0\u9ad8\u65e0\u7ebf\u901a\u4fe1\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "1. \u5f15\u5165\u5305\u542bKL\u6563\u5ea6\u9879\u7684\u635f\u5931\u51fd\u6570\uff0c\u589e\u5f3a\u4fe1\u53f7\u566a\u58f0\u7279\u6027\uff1b2. \u7528\u9884\u7f16\u7801\u7684\u4e8c\u8fdb\u5236\u8f93\u5165\u66ff\u4ee3\u72ec\u70ed\u7f16\u7801\uff0c\u652f\u6301\u957f\u6e90\u6d88\u606f\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u4fe1\u53f7\u7684\u7279\u5f81\u7f3a\u5931\u6027\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86BLER\u3002", "conclusion": "\u57fa\u4e8eAE\u7684\u65b9\u6cd5\u5728\u5b89\u5168\u548c\u53ef\u9760\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.07270", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07270", "abs": "https://arxiv.org/abs/2507.07270", "authors": ["Sidong Zhang", "Shiv Shankar", "Trang Nguyen", "Andrea Fanelli", "Madalina Fiterau"], "title": "Audio-Visual Speech Separation via Bottleneck Iterative Network", "comment": "Accepted to the 42nd International Conference on Machine Learning\n  Workshop on Machine Learning for Audio", "summary": "Integration of information from non-auditory cues can significantly improve\nthe performance of speech-separation models. Often such models use deep\nmodality-specific networks to obtain unimodal features, and risk being too\ncostly or lightweight but lacking capacity. In this work, we present an\niterative representation refinement approach called Bottleneck Iterative\nNetwork (BIN), a technique that repeatedly progresses through a lightweight\nfusion block, while bottlenecking fusion representations by fusion tokens. This\nhelps improve the capacity of the model, while avoiding major increase in model\nsize and balancing between the model performance and training cost. We test BIN\non challenging noisy audio-visual speech separation tasks, and show that our\napproach consistently outperforms state-of-the-art benchmark models with\nrespect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously\nachieving a reduction of more than 50% in training and GPU inference time\nacross nearly all settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBottleneck Iterative Network (BIN)\u7684\u8fed\u4ee3\u8868\u793a\u7ec6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u878d\u5408\u5757\u548c\u878d\u5408\u4ee4\u724c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u6a21\u578b\u89c4\u6a21\u5927\u5e45\u589e\u52a0\u3002", "motivation": "\u6574\u5408\u975e\u542c\u89c9\u4fe1\u606f\u53ef\u4ee5\u63d0\u9ad8\u8bed\u97f3\u5206\u79bb\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u901a\u5e38\u8fc7\u4e8e\u6602\u8d35\u6216\u8f7b\u91cf\u4f46\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528BIN\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u8f7b\u91cf\u7ea7\u878d\u5408\u5757\u548c\u878d\u5408\u4ee4\u724c\u4f18\u5316\u8868\u793a\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u8bad\u7ec3\u6210\u672c\u3002", "result": "\u5728NTCD-TIMIT\u548cLRS3+WHAM!\u6570\u636e\u96c6\u4e0a\uff0cBIN\u5728SI-SDRi\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u540c\u65f6\u8bad\u7ec3\u548cGPU\u63a8\u7406\u65f6\u95f4\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "BIN\u65b9\u6cd5\u5728\u63d0\u5347\u8bed\u97f3\u5206\u79bb\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.07631", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07631", "abs": "https://arxiv.org/abs/2507.07631", "authors": ["Hiroshi Sato", "Tsubasa Ochiai", "Marc Delcroix", "Takafumi Moriya", "Takanori Ashihara", "Ryo Masumura"], "title": "Generic Speech Enhancement with Self-Supervised Representation Space Loss", "comment": "22 pages, 3 figures. Accepted for Frontiers in signal processing", "summary": "Single-channel speech enhancement is utilized in various tasks to mitigate\nthe effect of interfering signals. Conventionally, to ensure the speech\nenhancement performs optimally, the speech enhancement has needed to be tuned\nfor each task. Thus, generalizing speech enhancement models to unknown\ndownstream tasks has been challenging. This study aims to construct a generic\nspeech enhancement front-end that can improve the performance of back-ends to\nsolve multiple downstream tasks. To this end, we propose a novel training\ncriterion that minimizes the distance between the enhanced and the ground truth\nclean signal in the feature representation domain of self-supervised learning\nmodels. Since self-supervised learning feature representations effectively\nexpress high-level speech information useful for solving various downstream\ntasks, the proposal is expected to make speech enhancement models preserve such\ninformation. Experimental validation demonstrates that the proposal improves\nthe performance of multiple speech tasks while maintaining the perceptual\nquality of the enhanced signal.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u8bed\u97f3\u589e\u5f3a\u524d\u7aef\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u8868\u793a\u4f18\u5316\u589e\u5f3a\u4fe1\u53f7\u4e0e\u5e72\u51c0\u4fe1\u53f7\u7684\u8ddd\u79bb\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u589e\u5f3a\u9700\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8c03\u6574\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u77e5\u4efb\u52a1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6784\u5efa\u901a\u7528\u524d\u7aef\u4ee5\u652f\u6301\u591a\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u65b0\u8bad\u7ec3\u51c6\u5219\uff0c\u6700\u5c0f\u5316\u589e\u5f3a\u4fe1\u53f7\u4e0e\u5e72\u51c0\u4fe1\u53f7\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u8868\u793a\u57df\u7684\u8ddd\u79bb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u589e\u5f3a\u4fe1\u53f7\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u901a\u7528\u8bed\u97f3\u589e\u5f3a\u524d\u7aef\u6709\u6548\u652f\u6301\u591a\u4efb\u52a1\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.07567", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07567", "abs": "https://arxiv.org/abs/2507.07567", "authors": ["Reza Ghasemi Alavicheh", "Thomas Feys", "MD Arifur Rahman", "Fran\u00e7ois Rottenberg"], "title": "Leveraging Power Amplifier Distortion for Physical Layer Security", "comment": null, "summary": "This paper introduces a new approach to physical layer security (PLS) by\nleveraging power amplifier (PA) nonlinear distortion through distortion-aware\nprecoding. While some conventional PLS techniques inject artificial noise\northogonal to legitimate channels, we demonstrate that inherent PA\nnonlinearities typically considered undesirable can be exploited to enhance\nsecurity. The zero 3rd order (Z3RO) precoder applies a negative polarity to\nseveral antennas to cancel the PA distortion at the user location, resulting in\ndistortion being transmitted in non-user locations. Redirecting the distortion\nto non-user locations creates interference for potential eavesdroppers,\nlowering their signal-to-noise-and-distortion ratio (SNDR). Numerical\nsimulations reveal that the Z3RO precoder achieves up to a $2.5\\times$\nimprovement in secrecy rate compared to conventional maximum ratio transmission\n(MRT) precoding under a $10\\%$ outage probability, SNR of $32$ dB and $-5$ dB\ninput back-off (IBO) where the PAs enter the saturation regime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u5931\u771f\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Z3RO\u9884\u7f16\u7801\u5668\u5c06\u5931\u771f\u5bfc\u5411\u975e\u7528\u6237\u4f4d\u7f6e\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u5c42\u5b89\u5168\u6280\u672f\u901a\u5e38\u6ce8\u5165\u6b63\u4ea4\u4e8e\u5408\u6cd5\u4fe1\u9053\u7684\u4eba\u5de5\u566a\u58f0\uff0c\u800c\u672c\u6587\u53d1\u73b0\u529f\u7387\u653e\u5927\u5668\u7684\u975e\u7ebf\u6027\u5931\u771f\u53ef\u88ab\u5229\u7528\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528Z3RO\u9884\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u591a\u5929\u7ebf\u8d1f\u6781\u6027\u62b5\u6d88\u7528\u6237\u4f4d\u7f6e\u7684\u5931\u771f\uff0c\u5c06\u5931\u771f\u5bfc\u5411\u975e\u7528\u6237\u4f4d\u7f6e\u4ee5\u5e72\u6270\u6f5c\u5728\u7a83\u542c\u8005\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0cZ3RO\u9884\u7f16\u7801\u5668\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6bd4\u4f20\u7edf\u6700\u5927\u6bd4\u4f20\u8f93\u9884\u7f16\u7801\u63d0\u53472.5\u500d\u7684\u4fdd\u5bc6\u901f\u7387\u3002", "conclusion": "\u529f\u7387\u653e\u5927\u5668\u7684\u975e\u7ebf\u6027\u5931\u771f\u53ef\u88ab\u6709\u6548\u5229\u7528\u4ee5\u589e\u5f3a\u7269\u7406\u5c42\u5b89\u5168\u6027\u80fd\u3002"}}
{"id": "2507.07318", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07318", "abs": "https://arxiv.org/abs/2507.07318", "authors": ["Christian Templin", "Yanda Zhu", "Hao Wang"], "title": "SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion Models", "comment": null, "summary": "Spatial audio is an integral part of immersive entertainment, such as VR/AR,\nand has seen increasing popularity in cinema and music as well. The most common\nformat of spatial audio is described as first-order Ambisonics (FOA). We seek\nto extend recent advancements in FOA generative AI models to enable the\ngeneration of 3D scenes with dynamic sound sources. Our proposed end-to-end\nmodel, SonicMotion, comes in two variations which vary in their user input and\nlevel of precision in sound source localization. In addition to our model, we\nalso present a new dataset of simulated spatial audio-caption pairs. Evaluation\nof our models demonstrate that they are capable of matching the semantic\nalignment and audio quality of state of the art models while capturing the\ndesired spatial attributes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSonicMotion\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u52a8\u6001\u58f0\u6e90\u76843D\u573a\u666f\u7a7a\u95f4\u97f3\u9891\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u79cd\u53d8\u4f53\u3002\u540c\u65f6\uff0c\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6a21\u62df\u7a7a\u95f4\u97f3\u9891-\u5b57\u5e55\u5bf9\u6570\u636e\u96c6\u3002", "motivation": "\u7a7a\u95f4\u97f3\u9891\u5728\u6c89\u6d78\u5f0f\u5a31\u4e50\uff08\u5982VR/AR\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u4e00\u9636Ambisonics\uff08FOA\uff09\u751f\u6210\u6a21\u578b\u9700\u8981\u6269\u5c55\u4ee5\u652f\u6301\u52a8\u6001\u58f0\u6e90\u3002", "method": "\u63d0\u51fa\u4e86SonicMotion\u6a21\u578b\uff0c\u5305\u542b\u4e24\u79cd\u53d8\u4f53\uff0c\u5206\u522b\u9488\u5bf9\u7528\u6237\u8f93\u5165\u548c\u58f0\u6e90\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u6a21\u62df\u7a7a\u95f4\u97f3\u9891-\u5b57\u5e55\u5bf9\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u97f3\u9891\u8d28\u91cf\u4e0a\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u80fd\u6355\u6349\u6240\u9700\u7684\u7a7a\u95f4\u5c5e\u6027\u3002", "conclusion": "SonicMotion\u6a21\u578b\u6210\u529f\u6269\u5c55\u4e86FOA\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u52a8\u6001\u58f0\u6e903D\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07643", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07643", "abs": "https://arxiv.org/abs/2507.07643", "authors": ["Seonghoon Yoo", "Jaemin Jung", "Seongah Jeong", "Jinkyu Kang", "Markku Juntti", "Joonhyuk Kang"], "title": "RIS-assisted ISAC Systems for Industrial Revolution 6.0: Exploring the Near-field and Far-field Coexistence", "comment": null, "summary": "The Industrial Internet of Things (IIoT) has emerged as a key technology for\nrealizing the vision of Industry 6.0, requiring the seamless integration of\ndiverse connected devices. In particular, integrated sensing and communication\n(ISAC) plays a critical role in supporting real-time control and automation\nwithin IIoT systems. In this paper, we explore reconfigurable intelligent\nsurface (RIS)-assisted ISAC systems for IIoT in the coexistence of near-field\nand far-field regions. The system consists of a full-duplex access point (AP),\na RIS and multiple IIoT devices, where the near-field devices simultaneously\nperform sensing and communication, while the far-field devices rely on a\nRIS-assisted communication. To enhance spectral efficiency for both sensing and\ncommunication functionalities, we consider the use of both traditional\nsensing-only (SO) and ISAC frequency bands. Moreover, uplink non-orthogonal\nmultiple access (NOMA) is employed to facilitate the sequential decoding of\nsuperimposed communication and sensing signals from IIoT devices. To maximize\nsensing accuracy in terms of Cram${\\Grave{\\textrm{e}}}$r-Rao bound (CRB), we\nformulate a joint optimization of RIS phase shift, bandwidth splitting ratio\nand receive beamforming vector subject to the minimum data rate requirements of\nIIoT devices and resource budget constraints. The algorithmic solution is\ndeveloped via the successive convex approximation (SCA)-based alternating\noptimization (AO) method with the semi-definite relaxation (SDR) technique.\nNumerical results demonstrate that the proposed method significantly\noutperforms conventional methods relying solely on either ISAC or SO band by\nachieving superior performance across RIS and device configurations, while\nensuring robust ISAC performance under the near-field and far-field coexistence\nscenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8fd1\u573a\u548c\u8fdc\u573a\u5171\u5b58\u7684\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u4e2d\uff0c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316RIS\u76f8\u4f4d\u3001\u5e26\u5bbd\u5206\u914d\u548c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u9700\u8981\u65e0\u7f1d\u96c6\u6210\u591a\u79cd\u8bbe\u5907\uff0c\u800c\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u662f\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u548c\u81ea\u52a8\u5316\u7684\u5173\u952e\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd1\u573a\u548c\u8fdc\u573a\u5171\u5b58\u573a\u666f\u4e0b\u7684ISAC\u6027\u80fd\u4f18\u5316\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u7531\u5168\u53cc\u5de5\u63a5\u5165\u70b9\uff08AP\uff09\u3001RIS\u548c\u591a\u4e2aIIoT\u8bbe\u5907\u7ec4\u6210\uff0c\u7ed3\u5408\u4f20\u7edf\u611f\u77e5\u9891\u6bb5\u548cISAC\u9891\u6bb5\uff0c\u91c7\u7528\u4e0a\u884c\u975e\u6b63\u4ea4\u591a\u5740\uff08NOMA\uff09\u6280\u672f\u3002\u901a\u8fc7SCA-AO\u7b97\u6cd5\u548cSDR\u6280\u672f\u8054\u5408\u4f18\u5316RIS\u76f8\u4f4d\u3001\u5e26\u5bbd\u5206\u914d\u548c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728RIS\u548c\u8bbe\u5907\u914d\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56ISAC\u6216\u4f20\u7edf\u611f\u77e5\u9891\u6bb5\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u611f\u77e5\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "RIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\u5728\u8fd1\u573a\u548c\u8fdc\u573a\u5171\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3aIIoT\u7684\u5b9e\u65f6\u63a7\u5236\u548c\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07384", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07384", "abs": "https://arxiv.org/abs/2507.07384", "authors": ["Yu Chen", "Xinyuan Qian", "Hongxu Zhu", "Jiadong Wang", "Kainan Chen", "Haizhou Li"], "title": "VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching", "comment": "Under Review", "summary": "Audio-visual sound source localization (AV-SSL) identifies the position of a\nsound source by exploiting the complementary strengths of auditory and visual\nsignals. However, existing AV-SSL methods encounter three major challenges: 1)\ninability to selectively isolate the target sound source in multi-source\nscenarios, 2) misalignment between semantic visual features and spatial\nacoustic features, and 3) overreliance on paired audio-visual data. To overcome\nthese limitations, we introduce Cross-Instance Audio-Visual Localization\n(CI-AVL), a novel task that leverages images from different instances of the\nsame sound event category to localize target sound sources, thereby reducing\ndependence on paired data while enhancing generalization capabilities. Our\nproposed VP-SelDoA tackles this challenging task through a semantic-level\nmodality fusion and employs a Frequency-Temporal ConMamba architecture to\ngenerate target-selective masks for sound isolation. We further develop a\nSemantic-Spatial Matching mechanism that aligns the heterogeneous semantic and\nspatial features via integrated cross- and self-attention mechanisms. To\nfacilitate the CI-AVL research, we construct a large-scale dataset named\nVGG-SSL, comprising 13,981 spatial audio clips across 296 sound event\ncategories. Extensive experiments show that our proposed method outperforms\nstate-of-the-art audio-visual localization methods, achieving a mean absolute\nerror (MAE) of 12.04 and an accuracy (ACC) of 78.23%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5b9e\u4f8b\u97f3\u9891-\u89c6\u89c9\u5b9a\u4f4d\uff08CI-AVL\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u5229\u7528\u540c\u4e00\u58f0\u97f3\u4e8b\u4ef6\u7c7b\u522b\u7684\u4e0d\u540c\u5b9e\u4f8b\u56fe\u50cf\u6765\u5b9a\u4f4d\u76ee\u6807\u58f0\u6e90\uff0c\u51cf\u5c11\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u5728\u591a\u6e90\u573a\u666f\u4e2d\u96be\u4ee5\u9009\u62e9\u6027\u9694\u79bb\u76ee\u6807\u58f0\u6e90\uff0c\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\u4e0e\u58f0\u5b66\u7a7a\u95f4\u7279\u5f81\u5b58\u5728\u4e0d\u5bf9\u9f50\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u3002", "method": "\u63d0\u51faVP-SelDoA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u7ea7\u6a21\u6001\u878d\u5408\u548c\u9891\u7387-\u65f6\u95f4ConMamba\u67b6\u6784\u751f\u6210\u76ee\u6807\u9009\u62e9\u6027\u63a9\u7801\uff0c\u5e76\u5f00\u53d1\u8bed\u4e49-\u7a7a\u95f4\u5339\u914d\u673a\u5236\u5bf9\u9f50\u5f02\u6784\u7279\u5f81\u3002", "result": "\u5728VGG-SSL\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cMAE\u4e3a12.04\uff0cACC\u4e3a78.23%\u3002", "conclusion": "CI-AVL\u4efb\u52a1\u53caVP-SelDoA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\uff0c\u63d0\u5347\u4e86\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2507.07647", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.07647", "abs": "https://arxiv.org/abs/2507.07647", "authors": ["Shenghua Hu", "Guangyang Zeng", "Wenchao Xue", "Haitao Fang", "Biqiang Mu"], "title": "Consistent and Asymptotically Efficient Localization from Bearing-only Measurements", "comment": null, "summary": "We study the problem of signal source localization using bearing-only\nmeasurements. Initially, we present easily verifiable geometric conditions for\nsensor deployment to ensure the asymptotic identifiability of the model and\ndemonstrate the consistency and asymptotic efficiency of the maximum likelihood\n(ML) estimator. However, obtaining the ML estimator is challenging due to its\nassociation with a non-convex optimization problem. To address this, we propose\na two-step estimator that shares the same asymptotic properties as the ML\nestimator while offering low computational complexity, linear in the number of\nmeasurements. The primary challenge lies in obtaining a preliminary consistent\nestimator in the first step. To achieve this, we construct a linear\nleast-squares problem through algebraic operations on the measurement nonlinear\nmodel to first obtain a biased closed-form solution. We then eliminate the bias\nusing the data to yield an asymptotically unbiased and consistent estimator.\nThe key to this process is obtaining a consistent estimator of the variance of\nthe sine of the noise by taking the reciprocal of the maximum eigenvalue of a\nspecially constructed matrix from the data. In the second step, we perform a\nsingle Gauss-Newton iteration using the preliminary consistent estimator as the\ninitial value, achieving the same asymptotic properties as the ML estimator.\nFinally, simulation results demonstrate the superior performance of the\nproposed two-step estimator for large sample sizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u65b9\u4f4d\u6d4b\u91cf\u7684\u4fe1\u53f7\u6e90\u5b9a\u4f4d\u95ee\u9898\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u76f8\u540c\u7684\u6e10\u8fd1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u5728\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6e10\u8fd1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4ee3\u6570\u64cd\u4f5c\u6784\u5efa\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff0c\u83b7\u5f97\u521d\u6b65\u4e00\u81f4\u4f30\u8ba1\u5668\uff0c\u518d\u901a\u8fc7\u9ad8\u65af-\u725b\u987f\u8fed\u4ee3\u5b9e\u73b0\u9ad8\u6548\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u6b65\u4f30\u8ba1\u5668\u5728\u5927\u6837\u672c\u91cf\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6e10\u8fd1\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.07526", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07526", "abs": "https://arxiv.org/abs/2507.07526", "authors": ["Cunhang Fan", "Sheng Zhang", "Jingjing Zhang", "Enrui Liu", "Xinhui Li", "Minggang Zhao", "Zhao Lv"], "title": "DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction", "comment": "Accepted by ACM MM 2025", "summary": "Decoding speech from brain signals is a challenging research problem.\nAlthough existing technologies have made progress in reconstructing the mel\nspectrograms of auditory stimuli at the word or letter level, there remain core\nchallenges in the precise reconstruction of minute-level continuous imagined\nspeech: traditional models struggle to balance the efficiency of temporal\ndependency modeling and information retention in long-sequence decoding. To\naddress this issue, this paper proposes the Dynamic Multiscale Fusion Network\n(DMF2Mel), which consists of four core components: the Dynamic Contrastive\nFeature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided\nMulti-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the\nbidirectional state space module (convMamba). Specifically, the DC-FAM\nseparates speech-related \"foreground features\" from noisy \"background features\"\nthrough local convolution and global attention mechanisms, effectively\nsuppressing interference and enhancing the representation of transient signals.\nHAMS-Net, based on the U-Net framework,achieves cross-scale fusion of\nhigh-level semantics and low-level details. The SplineMap attention mechanism\nintegrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine\nglobal context modeling with spline-based local fitting. The convMamba captures\nlong-range temporal dependencies with linear complexity and enhances nonlinear\ndynamic modeling capabilities. Results on the SparrKULee dataset show that\nDMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram\nreconstruction for known subjects (a 48% improvement over the baseline) and\n0.048 for unknown subjects (a 35% improvement over the baseline).Code is\navailable at: https://github.com/fchest/DMF2Mel.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDMF2Mel\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u5c3a\u5ea6\u878d\u5408\u6280\u672f\u89e3\u51b3\u8111\u4fe1\u53f7\u89e3\u7801\u4e2d\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u8fde\u7eed\u60f3\u8c61\u8bed\u97f3\u7684mel\u9891\u8c31\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u5355\u8bcd\u6216\u5b57\u6bcd\u7ea7\u522b\u7684\u542c\u89c9\u523a\u6fc0mel\u9891\u8c31\u91cd\u5efa\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5206\u949f\u7ea7\u8fde\u7eed\u60f3\u8c61\u8bed\u97f3\u7684\u7cbe\u786e\u91cd\u5efa\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5e73\u8861\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u6548\u7387\u4e0e\u957f\u5e8f\u5217\u89e3\u7801\u4e2d\u7684\u4fe1\u606f\u4fdd\u7559\u3002", "method": "\u63d0\u51faDMF2Mel\u7f51\u7edc\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aDC-FAM\uff08\u52a8\u6001\u5bf9\u6bd4\u7279\u5f81\u805a\u5408\u6a21\u5757\uff09\u3001HAMS-Net\uff08\u5206\u5c42\u6ce8\u610f\u529b\u5f15\u5bfc\u591a\u5c3a\u5ea6\u7f51\u7edc\uff09\u3001SplineMap\u6ce8\u610f\u529b\u673a\u5236\u548cconvMamba\uff08\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u5757\uff09\u3002", "result": "\u5728SparrKULee\u6570\u636e\u96c6\u4e0a\uff0cDMF2Mel\u5728\u5df2\u77e5\u53d7\u8bd5\u8005mel\u9891\u8c31\u91cd\u5efa\u4e2dPearson\u76f8\u5173\u7cfb\u6570\u8fbe0.074\uff08\u6bd4\u57fa\u7ebf\u63d0\u534748%\uff09\uff0c\u672a\u77e5\u53d7\u8bd5\u8005\u8fbe0.048\uff08\u63d0\u534735%\uff09\u3002", "conclusion": "DMF2Mel\u901a\u8fc7\u52a8\u6001\u591a\u5c3a\u5ea6\u878d\u5408\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u60f3\u8c61\u8bed\u97f3\u7684mel\u9891\u8c31\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2507.07692", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07692", "abs": "https://arxiv.org/abs/2507.07692", "authors": ["Mohammad Ali Vahedifar", "Qi Zhang"], "title": "Signal Prediction for Loss Mitigation in Tactile Internet: A Leader-Follower Game-Theoretic Approach", "comment": "This work has been accepted for publication in the IEEE Machine\n  Learning and Signal Processing Conference (MLSP 2025)", "summary": "Tactile Internet (TI) requires achieving ultra-low latency and highly\nreliable packet delivery for haptic signals. In the presence of packet loss and\ndelay, the signal prediction method provides a viable solution for recovering\nthe missing signals. To this end, we introduce the Leader-Follower (LeFo)\napproach based on a cooperative Stackelberg game, which enables both users and\nrobots to learn and predict actions. With accurate prediction, the\nteleoperation system can safely relax its strict delay requirements. Our method\nachieves high prediction accuracy, ranging from 80.62% to 95.03% for remote\nrobot signals at the Human ($H$) side and from 70.44% to 89.77% for human\noperation signals at the remote Robot ($R$) side. We also establish an upper\nbound for maximum signal loss using Taylor Expansion, ensuring robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStackelberg\u535a\u5f08\u7684Leader-Follower\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u89e6\u89c9\u4e92\u8054\u7f51\u4e2d\u7684\u4fe1\u53f7\u4e22\u5931\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u8981\u6c42\u3002", "motivation": "\u89e6\u89c9\u4e92\u8054\u7f51\u9700\u8981\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u5728\u4e22\u5305\u548c\u5ef6\u8fdf\u60c5\u51b5\u4e0b\uff0c\u4fe1\u53f7\u9884\u6d4b\u6210\u4e3a\u6062\u590d\u4e22\u5931\u4fe1\u53f7\u7684\u53ef\u884c\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5408\u4f5cStackelberg\u535a\u5f08\u7684Leader-Follower\u65b9\u6cd5\uff0c\u4f7f\u7528\u6237\u548c\u673a\u5668\u4eba\u80fd\u5b66\u4e60\u548c\u9884\u6d4b\u52a8\u4f5c\u3002", "result": "\u9884\u6d4b\u51c6\u786e\u7387\u5728\u4eba\u7c7b\u7aef\u4e3a80.62%-95.03%\uff0c\u673a\u5668\u4eba\u7aef\u4e3a70.44%-89.77%\uff0c\u5e76\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00\u5efa\u7acb\u4e86\u4fe1\u53f7\u4e22\u5931\u7684\u4e0a\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89e6\u89c9\u4e92\u8054\u7f51\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u653e\u5bbd\u4e86\u5ef6\u8fdf\u8981\u6c42\u3002"}}
{"id": "2507.07764", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07764", "abs": "https://arxiv.org/abs/2507.07764", "authors": ["Haokun Tian", "Stefan Lattner", "Charalampos Saitis"], "title": "Assessing the Alignment of Audio Representations with Timbre Similarity Ratings", "comment": "Accepted to ISMIR 2025", "summary": "Psychoacoustical so-called \"timbre spaces\" map perceptual similarity ratings\nof instrument sounds onto low-dimensional embeddings via multidimensional\nscaling, but suffer from scalability issues and are incapable of\ngeneralization. Recent results from audio (music and speech) quality assessment\nas well as image similarity have shown that deep learning is able to produce\nembeddings that align well with human perception while being largely free from\nthese constraints. Although the existing human-rated timbre similarity data is\nnot large enough to train deep neural networks (2,614 pairwise ratings on 334\naudio samples), it can serve as test-only data for audio models. In this paper,\nwe introduce metrics to assess the alignment of diverse audio representations\nwith human judgments of timbre similarity by comparing both the absolute values\nand the rankings of embedding distances to human similarity ratings. Our\nevaluation involves three signal-processing-based representations, twelve\nrepresentations extracted from pre-trained models, and three representations\nextracted from a novel sound matching model. Among them, the style embeddings\ninspired by image style transfer, extracted from the CLAP model and the sound\nmatching model, remarkably outperform the others, showing their potential in\nmodeling timbre similarity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u97f3\u9891\u8868\u793a\u4e0e\u4eba\u7c7b\u97f3\u8272\u76f8\u4f3c\u6027\u5224\u65ad\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8eCLAP\u6a21\u578b\u548c\u58f0\u97f3\u5339\u914d\u6a21\u578b\u7684\u98ce\u683c\u5d4c\u5165\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u97f3\u8272\u7a7a\u95f4\u65b9\u6cd5\u5b58\u5728\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u5728\u97f3\u9891\u8d28\u91cf\u8bc4\u4f30\u548c\u56fe\u50cf\u76f8\u4f3c\u6027\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u8db3\u591f\u7684\u4eba\u7c7b\u8bc4\u5206\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u5f15\u5165\u6307\u6807\u8bc4\u4f30\u97f3\u9891\u8868\u793a\u4e0e\u4eba\u7c7b\u97f3\u8272\u76f8\u4f3c\u6027\u5224\u65ad\u7684\u5bf9\u9f50\u6027\uff0c\u6bd4\u8f83\u5d4c\u5165\u8ddd\u79bb\u7684\u7edd\u5bf9\u503c\u548c\u6392\u540d\u3002", "result": "CLAP\u6a21\u578b\u548c\u58f0\u97f3\u5339\u914d\u6a21\u578b\u7684\u98ce\u683c\u5d4c\u5165\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u8868\u793a\u65b9\u6cd5\u3002", "conclusion": "\u98ce\u683c\u5d4c\u5165\u5728\u5efa\u6a21\u97f3\u8272\u76f8\u4f3c\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.07832", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07832", "abs": "https://arxiv.org/abs/2507.07832", "authors": ["Xinyi Lin", "Peizheng Li", "Adnan Aijaz"], "title": "Flying Base Stations for Offshore Wind Farm Monitoring and Control: Holistic Performance Evaluation and Optimization", "comment": "Accepted by PIMRC 2025", "summary": "Ensuring reliable and low-latency communication in offshore wind farms is\ncritical for efficient monitoring and control, yet remains challenging due to\nthe harsh environment and lack of infrastructure. This paper investigates a\nflying base station (FBS) approach for wide-area monitoring and control in the\nUK Hornsea offshore wind farm project. By leveraging mobile, flexible FBS\nplatforms in the remote and harsh offshore environment, the proposed system\noffers real-time connectivity for turbines without the need for deploying\npermanent infrastructure at the sea. We develop a detailed and practical\nend-to-end latency model accounting for five key factors: flight duration,\nconnection establishment, turbine state information upload, computational\ndelay, and control transmission, to provide a holistic perspective often\nmissing in prior studies. Furthermore, we combine trajectory planning,\nbeamforming, and resource allocation into a multi-objective optimization\nframework for the overall latency minimization, specifically designed for\nlarge-scale offshore wind farm deployments. Simulation results verify the\neffectiveness of our proposed method in minimizing latency and enhancing\nefficiency in FBS-assisted offshore monitoring across various power levels,\nwhile consistently outperforming baseline designs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98de\u884c\u57fa\u7ad9\uff08FBS\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6d77\u4e0a\u98ce\u7535\u573a\u4e2d\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u7684\u6311\u6218\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "motivation": "\u6d77\u4e0a\u98ce\u7535\u573a\u73af\u5883\u6076\u52a3\u4e14\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd\uff0c\u4f20\u7edf\u901a\u4fe1\u65b9\u5f0f\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u76d1\u63a7\u548c\u63a7\u5236\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u5ef6\u8fdf\u6a21\u578b\uff0c\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u3001\u6ce2\u675f\u6210\u5f62\u548c\u8d44\u6e90\u5206\u914d\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u529f\u7387\u6c34\u5e73\u4e0b\u5747\u80fd\u6709\u6548\u6700\u5c0f\u5316\u5ef6\u8fdf\uff0c\u5e76\u4f18\u4e8e\u57fa\u7ebf\u8bbe\u8ba1\u3002", "conclusion": "FBS\u65b9\u6cd5\u4e3a\u6d77\u4e0a\u98ce\u7535\u573a\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u5b9e\u65f6\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07799", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07799", "abs": "https://arxiv.org/abs/2507.07799", "authors": ["Belinda Soh Hui Hui", "Xiaoxiao Miao", "Xin Wang"], "title": "SecureSpeech: Prompt-based Speaker and Content Protection", "comment": "Accepted by IEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Given the increasing privacy concerns from identity theft and the\nre-identification of speakers through content in the speech field, this paper\nproposes a prompt-based speech generation pipeline that ensures dual\nanonymization of both speaker identity and spoken content. This is addressed\nthrough 1) generating a speaker identity unlinkable to the source speaker,\ncontrolled by descriptors, and 2) replacing sensitive content within the\noriginal text using a name entity recognition model and a large language model.\nThe pipeline utilizes the anonymized speaker identity and text to generate\nhigh-fidelity, privacy-friendly speech via a text-to-speech synthesis model.\nExperimental results demonstrate an achievement of significant privacy\nprotection while maintaining a decent level of content retention and audio\nquality. This paper also investigates the impact of varying speaker\ndescriptions on the utility and privacy of generated speech to determine\npotential biases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u97f3\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u53cc\u91cd\u533f\u540d\u5316\uff08\u8bf4\u8bdd\u8005\u8eab\u4efd\u548c\u5185\u5bb9\uff09\u4fdd\u62a4\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u4fdd\u7559\u548c\u97f3\u9891\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u9886\u57df\u4e2d\u56e0\u8eab\u4efd\u76d7\u7528\u548c\u8bf4\u8bdd\u8005\u91cd\u65b0\u8bc6\u522b\u5f15\u53d1\u7684\u9690\u79c1\u95ee\u9898\u3002", "method": "1) \u751f\u6210\u4e0e\u6e90\u8bf4\u8bdd\u8005\u8eab\u4efd\u65e0\u5173\u7684\u8bf4\u8bdd\u8005\u8eab\u4efd\uff1b2) \u4f7f\u7528\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u66ff\u6362\u654f\u611f\u5185\u5bb9\uff1b3) \u901a\u8fc7\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u6a21\u578b\u751f\u6210\u9690\u79c1\u53cb\u597d\u7684\u8bed\u97f3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u663e\u8457\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5185\u5bb9\u4fdd\u7559\u548c\u97f3\u9891\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7ba1\u9053\u6709\u6548\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u8bed\u97f3\u8d28\u91cf\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bf4\u8bdd\u8005\u63cf\u8ff0\u5bf9\u751f\u6210\u8bed\u97f3\u7684\u6f5c\u5728\u504f\u89c1\u3002"}}
{"id": "2507.07806", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07806", "abs": "https://arxiv.org/abs/2507.07806", "authors": ["Zhao Ren", "Rathi Adarshi Rammohan", "Kevin Scheck", "Sheng Li", "Tanja Schultz"], "title": "End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced by Semi-supervised Learning", "comment": "Accepted by EMBC 2025", "summary": "Emotion and intent recognition from speech is essential and has been widely\ninvestigated in human-computer interaction. The rapid development of social\nmedia platforms, chatbots, and other technologies has led to a large volume of\nspeech data streaming from users. Nevertheless, annotating such data manually\nis expensive, making it challenging to train machine learning models for\nrecognition purposes. To this end, we propose applying semi-supervised learning\nto incorporate a large scale of unlabelled data alongside a relatively smaller\nset of labelled data. We train end-to-end acoustic and linguistic models, each\nemploying multi-task learning for emotion and intent recognition. Two\nsemi-supervised learning approaches, including fix-match learning and\nfull-match learning, are compared. The experimental results demonstrate that\nthe semi-supervised learning approaches improve model performance in speech\nemotion and intent recognition from both acoustic and text data. The late\nfusion of the best models outperforms the acoustic and text baselines by joint\nrecognition balance metrics of 12.3% and 10.4%, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u548c\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8\u8bed\u97f3\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u56e0\u6b64\u9700\u8981\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u8bc6\u522b\u6548\u679c\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u7684\u58f0\u5b66\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08fix-match\u548cfull-match\uff09\u3002", "result": "\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd\uff0c\u6700\u4f73\u6a21\u578b\u7684\u540e\u671f\u878d\u5408\u5728\u5e73\u8861\u6307\u6807\u4e0a\u5206\u522b\u4f18\u4e8e\u58f0\u5b66\u548c\u6587\u672c\u57fa\u7ebf12.3%\u548c10.4%\u3002", "conclusion": "\u534a\u76d1\u7763\u5b66\u4e60\u80fd\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u63d0\u5347\u8bed\u97f3\u60c5\u611f\u548c\u610f\u56fe\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.07867", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07867", "abs": "https://arxiv.org/abs/2507.07867", "authors": ["Dimitrios Bralios", "Jonah Casebeer", "Paris Smaragdis"], "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders", "comment": "Accepted at IEEE MLSP 2025", "summary": "Neural audio codecs and autoencoders have emerged as versatile models for\naudio compression, transmission, feature-extraction, and latent-space\ngeneration. However, a key limitation is that most are trained to maximize\nreconstruction fidelity, often neglecting the specific latent structure\nnecessary for optimal performance in diverse downstream applications. We\npropose a simple, post-hoc framework to address this by modifying the\nbottleneck of a pre-trained autoencoder. Our method introduces a\n\"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space\nlosses to instill user-defined structure. We demonstrate the framework's\neffectiveness in three experiments. First, we enforce an ordering on latent\nchannels without sacrificing reconstruction quality. Second, we align latents\nwith semantic embeddings, analyzing the impact on downstream diffusion\nmodeling. Third, we introduce equivariance, ensuring that a filtering operation\non the input waveform directly corresponds to a specific transformation in the\nlatent space. Ultimately, our Re-Bottleneck framework offers a flexible and\nefficient way to tailor representations of neural audio models, enabling them\nto seamlessly meet the varied demands of different applications with minimal\nadditional training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cRe-Bottleneck\u201d\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u7684\u74f6\u9888\u7ed3\u6784\uff0c\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u4ee5\u9002\u5e94\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u548c\u81ea\u7f16\u7801\u5668\u901a\u5e38\u4ec5\u5173\u6ce8\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u5bf9\u591a\u6837\u5316\u5e94\u7528\u7684\u9002\u5e94\u6027\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u7684\u74f6\u9888\u90e8\u5206\u5f15\u5165\u201cRe-Bottleneck\u201d\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u635f\u5931\u8bad\u7ec3\uff0c\u8d4b\u4e88\u7528\u6237\u5b9a\u4e49\u7684\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4e0d\u727a\u7272\u91cd\u5efa\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u6f5c\u5728\u901a\u9053\u6392\u5e8f\u3001\u5bf9\u9f50\u8bed\u4e49\u5d4c\u5165\u5e76\u5f15\u5165\u7b49\u53d8\u6027\u3002", "conclusion": "Re-Bottleneck\u6846\u67b6\u7075\u6d3b\u9ad8\u6548\uff0c\u53ef\u5b9a\u5236\u795e\u7ecf\u97f3\u9891\u6a21\u578b\u7684\u8868\u793a\uff0c\u6ee1\u8db3\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2507.07877", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07877", "abs": "https://arxiv.org/abs/2507.07877", "authors": ["Chen Feng", "Yicheng Lin", "Shaojie Zhuo", "Chenzheng Su", "Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Xiaopeng Zhang"], "title": "Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models", "comment": null, "summary": "Recent advances in Automatic Speech Recognition (ASR) have demonstrated\nremarkable accuracy and robustness in diverse audio applications, such as live\ntranscription and voice command processing. However, deploying these models on\nresource constrained edge devices (e.g., IoT device, wearables) still presents\nsubstantial challenges due to strict limits on memory, compute and power.\nQuantization, particularly Post-Training Quantization (PTQ), offers an\neffective way to reduce model size and inference cost without retraining.\nDespite its importance, the performance implications of various advanced\nquantization methods and bit-width configurations on ASR models remain unclear.\nIn this work, we present a comprehensive benchmark of eight state-of-the-art\n(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and\nMoonshine. We systematically evaluate model performances (i.e., accuracy,\nmemory I/O and bit operations) across seven diverse datasets from the open ASR\nleaderboard, analyzing the impact of quantization and various configurations on\nboth weights and activations. Built on an extension of the LLM compression\ntoolkit, our framework integrates edge-ASR models, diverse advanced\nquantization algorithms, a unified calibration and evaluation data pipeline,\nand detailed analysis tools. Our results characterize the trade-offs between\nefficiency and accuracy, demonstrating that even 3-bit quantization can succeed\non high capacity models when using advanced PTQ techniques. These findings\nprovide valuable insights for optimizing ASR models on low-power, always-on\nedge devices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u7684\u6311\u6218\uff0c\u5e76\u8bc4\u4f30\u4e86\u516b\u79cd\u5148\u8fdb\u7684\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u65b9\u6cd5\u5bf9\u4e24\u79cdASR\u6a21\u578b\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u5728\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u529f\u8017\u4e0a\u7684\u4e25\u683c\u9650\u5236\uff0c\u9700\u8981\u91cf\u5316\u6280\u672f\u6765\u4f18\u5316ASR\u6a21\u578b\u7684\u90e8\u7f72\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u516b\u79cdSOTA PTQ\u65b9\u6cd5\uff0c\u5bf9Whisper\u548cMoonshine\u4e24\u79cdASR\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u91cf\u5316\u914d\u7f6e\u5bf9\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u4f7f\u75283\u4f4d\u91cf\u5316\uff0c\u9ad8\u6027\u80fd\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u91c7\u7528\u5148\u8fdbPTQ\u6280\u672f\u65f6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f18\u5316ASR\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.07879", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07879", "abs": "https://arxiv.org/abs/2507.07879", "authors": ["Changheon Han", "Yun Seok Kang", "Yuseop Sim", "Martin Byung-Guk Jun", "Hyung Wook Park"], "title": "LISTEN: Lightweight Industrial Sound-representable Transformer for Edge Notification", "comment": null, "summary": "Deep learning-based machine listening is broadening the scope of industrial\nacoustic analysis for applications like anomaly detection and predictive\nmaintenance, thereby improving manufacturing efficiency and reliability.\nNevertheless, its reliance on large, task-specific annotated datasets for every\nnew task limits widespread implementation on shop floors. While emerging sound\nfoundation models aim to alleviate data dependency, they are too large and\ncomputationally expensive, requiring cloud infrastructure or high-end hardware\nthat is impractical for on-site, real-time deployment. We address this gap with\nLISTEN (Lightweight Industrial Sound-representable Transformer for Edge\nNotification), a kilobyte-sized industrial sound foundation model. Using\nknowledge distillation, LISTEN runs in real-time on low-cost edge devices. On\nbenchmark downstream tasks, it performs nearly identically to its much larger\nparent model, even when fine-tuned with minimal datasets and training resource.\nBeyond the model itself, we demonstrate its real-world utility by integrating\nLISTEN into a complete machine monitoring framework on an edge device with an\nIndustrial Internet of Things (IIoT) sensor and system, validating its\nperformance and generalization capabilities on a live manufacturing shop floor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5de5\u4e1a\u58f0\u97f3\u57fa\u7840\u6a21\u578bLISTEN\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u5927\u6570\u636e\u548c\u9ad8\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u58f0\u5b66\u5206\u6790\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u9ad8\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5176\u5728\u751f\u4ea7\u73b0\u573a\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578bLISTEN\uff0c\u80fd\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "result": "LISTEN\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u5176\u5927\u578b\u7236\u6a21\u578b\uff0c\u4e14\u53ea\u9700\u5c11\u91cf\u6570\u636e\u548c\u8bad\u7ec3\u8d44\u6e90\u3002", "conclusion": "LISTEN\u6210\u529f\u96c6\u6210\u5230\u8fb9\u7f18\u8bbe\u5907\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.07954", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07954", "abs": "https://arxiv.org/abs/2507.07954", "authors": ["Abdul Hannan", "Daniele Falavigna", "Alessio Brutti"], "title": "Input Conditioned Layer Dropping in Speech Foundation Models", "comment": "Accepted at IEEE MLSP 2025", "summary": "Curating foundation speech models for edge and IoT settings, where\ncomputational resources vary over time, requires dynamic architectures\nfeaturing adaptable reduction strategies. One emerging approach is layer\ndropping ($\\mathcal{LD}$) which skips fraction of the layers of a backbone\nnetwork during inference to reduce the computational load. This allows\ntransforming static models into dynamic ones. However, existing approaches\nexhibit limitations either in the mode of selecting layers or by significantly\nmodifying the neural architecture. To this end, we propose input-driven\n$\\mathcal{LD}$ that employs the network's input features and a lightweight\nlayer selecting network to determine the optimum combination of processing\nlayers. Extensive experimentation on 4 speech and audio public benchmarks,\nusing two different pre-trained foundation models, demonstrates the\neffectiveness of our approach, thoroughly outperforming random dropping and\nproducing on-par (or better) results to early exit.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u9a71\u52a8\u7684\u5c42\u4e22\u5f03\u65b9\u6cd5\uff08LD\uff09\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u8bed\u97f3\u6a21\u578b\u7684\u63a8\u7406\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u4f18\u4e8e\u968f\u673a\u4e22\u5f03\uff0c\u6027\u80fd\u4e0e\u65e9\u671f\u9000\u51fa\u76f8\u5f53\u6216\u66f4\u597d\u3002", "motivation": "\u5728\u8fb9\u7f18\u548c\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u8ba1\u7b97\u8d44\u6e90\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u9700\u8981\u52a8\u6001\u67b6\u6784\u6765\u9002\u5e94\u3002\u73b0\u6709\u5c42\u4e22\u5f03\u65b9\u6cd5\u5728\u5c42\u9009\u62e9\u6216\u67b6\u6784\u4fee\u6539\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u8f93\u5165\u9a71\u52a8\u7684LD\uff0c\u5229\u7528\u8f93\u5165\u7279\u5f81\u548c\u8f7b\u91cf\u7ea7\u5c42\u9009\u62e9\u7f51\u7edc\u786e\u5b9a\u6700\u4f73\u5904\u7406\u5c42\u7ec4\u5408\u3002", "result": "\u57284\u4e2a\u8bed\u97f3\u548c\u97f3\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4e24\u79cd\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8f93\u5165\u9a71\u52a8\u7684LD\u663e\u8457\u4f18\u4e8e\u968f\u673a\u4e22\u5f03\uff0c\u6027\u80fd\u4e0e\u65e9\u671f\u9000\u51fa\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u8ba1\u7b97\u8d44\u6e90\u73af\u5883\u3002"}}

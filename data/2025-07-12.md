<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Three-Dimensional Millimeter-Wave Imaging Using Active Incoherent Fourier Processing and Pulse Compression](https://arxiv.org/abs/2507.07239)
*Jorge R. Colon-Berrios,Jason M. Merlo,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 提出了一种结合二维空间傅里叶域成像技术与传统雷达脉冲压缩的三维成像方法，利用噪声信号和线性调频脉冲信号实现场景的三维重建。


<details>
  <summary>Details</summary>
Motivation: 传统成像方法在同时获取高分辨率跨范围和下范围信息方面存在挑战，需要一种新的技术来高效恢复三维场景信息。

Method: 系统采用四个发射器，其中三个发射空间和时间不相干的噪声信号，第四个发射已知的线性调频脉冲信号。通过干涉处理和匹配滤波分别恢复跨范围和下范围信息。

Result: 通过仿真和实验数据验证，成功重建了目标的三维图像。

Conclusion: 该方法有效结合了噪声信号和脉冲信号的优点，实现了高分辨率的三维成像。

Abstract: We present a novel three-dimensional (3D) imaging approach that combines
two-dimensional spatial Fourier-domain imaging techniques with traditional
radar pulse compression to recover both cross-range and down-range scene
information. The imaging system employs four transmitters, three of which emit
spatially and temporally incoherent noise signals, while the fourth transmits a
known linear frequency modulated (LFM) pulsed signal. The spatial incoherence
of the noise signals enables sampling of the 2D spatial Fourier spectrum of the
scene from which two-dimensional cross-range (azimuth and elevation) images can
be formed via interferometric processing. Simultaneously, the LFM signal
enables high-resolution downrange imaging through matched filtering. The
received signals consist of a superposition of the noise sources and the known
pulse allowing for joint recovery of all three dimensions. We describe the
system architecture and waveform design, and demonstrate the imaging technique
using both simulations with a linear array and experimental data from a 38 GHz
active incoherent millimeter-wave imaging system with 23-element randomized
array. Results show the reconstruction of targets in three dimensions.

</details>


### [2] [A RIS-Enabled Computational Radar Coincidence Imaging](https://arxiv.org/abs/2507.07285)
*Kavian Zirak,Mohammadreza F. Imani*

Main category: eess.SP

TL;DR: 本文提出了一种结合雷达重合成像（RCI）和计算成像技术的创新成像方法，利用可重构智能表面（RISs）生成高信噪比和低杂波的图像。


<details>
  <summary>Details</summary>
Motivation: 传统成像方法如光栅扫描需要大量测量，而计算成像通常依赖随机模式，导致信噪比低。本文旨在通过RISs生成定向光束，结合RCI和计算成像，提升成像质量。

Method: 利用RISs将光束重定向至感兴趣区域（ROI），通过光束干涉形成空间多样的散斑图案，结合RCI和计算成像技术，实现高效成像。

Result: 数值模拟表明，该方法在少量测量下即可生成高质量图像，信噪比高且杂波少，优于传统技术。

Conclusion: 该方法适用于安全筛查、无线用户跟踪和活动识别，展示了RISs在成像领域的潜力。

Abstract: This paper introduces an innovative imaging method using reconfigurable
intelligent surfaces (RISs) by combining radar coincidence imaging (RCI) and
computational imaging techniques. In the proposed framework, RISs
simultaneously redirect beams toward a desired region of interest (ROI). The
interference of these beams forms spatially diverse speckle patterns that carry
information about the entire ROI. As a result, this method can take advantage
of the benefits of both random patterns and spotlight imaging. Since the
speckle pattern is formed by directive beams (instead of random patterns
typically used in computational imaging), this approach results in a higher
signal-to-noise ratio (SNR) and reduced clutter. In contrast to raster
scanning, which requires the number of measurements to be at least equal to the
number of unknowns, our proposed approach follows a computational imaging
framework and can obtain high-quality images even when only a few measurements
are taken. Using numerical simulation, we demonstrate this method's
capabilities and contrast it against other conventional techniques. The
proposed imaging approach can be applied to security screening, wireless user
tracking, and activity recognition.

</details>


### [3] [mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar](https://arxiv.org/abs/2507.07331)
*Anurag Pallaprolu,Winston Hurst,Yasamin Mostofi*

Main category: eess.SP

TL;DR: 提出了一种基于毫米波雷达的新框架，用于提取人群运动模式并推断人群语义，结合信号处理和几何图分析，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从毫米波雷达数据中高效提取人群运动模式和语义的问题，为人群分析提供新方法。

Method: 结合光学流估计与噪声过滤生成高保真流场，将其转化为有向几何图，并通过雅可比矩阵分析提取语义。

Result: 在21个实验中，框架成功重建复杂人群的流结构，并准确推断出关键语义（如转向、边界、分散等）。

Conclusion: 框架验证有效，具有广泛的人群分析应用潜力。

Abstract: In this paper, we present a novel framework for extracting underlying crowd
motion patterns and inferring crowd semantics using mmWave radar. First, our
proposed signal processing pipeline combines optical flow estimation concepts
from vision with novel statistical and morphological noise filtering to
generate high-fidelity mmWave flow fields - compact 2D vector representations
of crowd motion. We then introduce a novel approach that transforms these
fields into directed geometric graphs, where edges capture dominant flow
currents, vertices mark crowd splitting or merging, and flow distribution is
quantified across edges. Finally, we show that by analyzing the local Jacobian
and computing the corresponding curl and divergence, we can extract key crowd
semantics for both structured and diffused crowds. We conduct 21 experiments on
crowds of up to (and including) 20 people across 3 areas, using commodity
mmWave radar. Our framework achieves high-fidelity graph reconstruction of the
underlying flow structure, even for complex crowd patterns, demonstrating
strong spatial alignment and precise quantitative characterization of flow
split ratios. Finally, our curl and divergence analysis accurately infers key
crowd semantics, e.g., abrupt turns, boundaries where flow directions shift,
dispersions, and gatherings. Overall, these findings validate our framework,
underscoring its potential for various crowd analytics applications.

</details>


### [4] [Featureless Wireless Communications using Enhanced Autoencoder](https://arxiv.org/abs/2507.07474)
*Ruhui Zhang,Wei Lin,Binbin Chen*

Main category: eess.SP

TL;DR: 论文提出了一种基于自动编码器（AE）的无线通信系统，通过改进损失函数和输入编码方式，生成低检测/拦截概率（LPD/LPI）的无特征信号，同时降低误块率（BLER）。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统中，信号的安全性和可靠性至关重要。传统方法难以同时实现低检测概率和高可靠性，因此需要探索新的AI技术解决方案。

Method: 1. 提出新的损失函数，结合KL散度和分类交叉熵，增强信号的噪声特性；2. 使用二进制输入替代独热编码，支持更长的源消息块；3. 结合传统纠错编码方案，优化BLER性能。

Result: 实验表明，该方法显著提升了信号的无特征性，并大幅降低了BLER，验证了其在无线通信中的有效性。

Conclusion: 基于AE的方法为安全可靠的无线通信系统提供了有前景的解决方案。

Abstract: Artificial intelligence (AI) techniques, particularly autoencoders (AEs),
have gained significant attention in wireless communication systems. This paper
investigates using an AE to generate featureless signals with a low probability
of detection and interception (LPD/LPI). Firstly, we introduce a novel loss
function that adds a KL divergence term to the categorical cross entropy,
enhancing the noise like characteristics of AE-generated signals while
preserving block error rate (BLER). Secondly, to support long source message
blocks for the AE's inputs, we replace one-hot inputs of source blocks with
binary inputs pre-encoded by conventional error correction coding schemes. The
AE's outputs are then decoded back to the source blocks using the same scheme.
This design enables the AE to learn the coding structure, yielding superior
BLER performance on coded blocks and the BLER of the source blocks is further
decreased by the error correction decoder. Moreover, we also validate the AE
based communication system in the over-the-air communication. Experimental
results demonstrate that our proposed methods improve the featureless
properties of AE signals and significantly reduce the BLER of message blocks,
underscoring the promise of our AE-based approach for secure and reliable
wireless communication systems.

</details>


### [5] [Leveraging Power Amplifier Distortion for Physical Layer Security](https://arxiv.org/abs/2507.07567)
*Reza Ghasemi Alavicheh,Thomas Feys,MD Arifur Rahman,François Rottenberg*

Main category: eess.SP

TL;DR: 本文提出了一种利用功率放大器非线性失真的物理层安全新方法，通过Z3RO预编码器将失真重定向到非用户位置，从而提升安全性。


<details>
  <summary>Details</summary>
Motivation: 传统物理层安全技术通常注入与合法信道正交的人工噪声，而本文探索了利用功率放大器非线性失真（通常被视为不利因素）来增强安全性。

Method: 采用Z3RO预编码器，通过对多个天线施加负极性，消除用户位置的失真，并将失真传输到非用户位置，干扰潜在窃听者。

Result: 数值模拟显示，Z3RO预编码器在特定条件下（如10%中断概率、32 dB SNR和-5 dB IBO）比传统MRT预编码器保密率提升2.5倍。

Conclusion: 利用功率放大器非线性失真的Z3RO预编码器是一种有效的物理层安全增强方法。

Abstract: This paper introduces a new approach to physical layer security (PLS) by
leveraging power amplifier (PA) nonlinear distortion through distortion-aware
precoding. While some conventional PLS techniques inject artificial noise
orthogonal to legitimate channels, we demonstrate that inherent PA
nonlinearities typically considered undesirable can be exploited to enhance
security. The zero 3rd order (Z3RO) precoder applies a negative polarity to
several antennas to cancel the PA distortion at the user location, resulting in
distortion being transmitted in non-user locations. Redirecting the distortion
to non-user locations creates interference for potential eavesdroppers,
lowering their signal-to-noise-and-distortion ratio (SNDR). Numerical
simulations reveal that the Z3RO precoder achieves up to a $2.5\times$
improvement in secrecy rate compared to conventional maximum ratio transmission
(MRT) precoding under a $10\%$ outage probability, SNR of $32$ dB and $-5$ dB
input back-off (IBO) where the PAs enter the saturation regime.

</details>


### [6] [RIS-assisted ISAC Systems for Industrial Revolution 6.0: Exploring the Near-field and Far-field Coexistence](https://arxiv.org/abs/2507.07643)
*Seonghoon Yoo,Jaemin Jung,Seongah Jeong,Jinkyu Kang,Markku Juntti,Joonhyuk Kang*

Main category: eess.SP

TL;DR: 本文研究了RIS辅助的ISAC系统在IIoT中的应用，优化了频谱效率和感知精度，并通过SCA-AO方法实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 工业物联网（IIoT）需要实时控制和自动化，而ISAC技术在此背景下至关重要。本文旨在通过RIS辅助的ISAC系统，解决近场和远场区域共存的问题。

Method: 提出了一种RIS辅助的ISAC系统，结合SO和ISAC频段，采用NOMA技术，并通过SCA-AO和SDR方法优化RIS相位、带宽分配和波束成形。

Result: 数值结果表明，该方法在频谱效率和感知精度上显著优于传统方法，适用于多种RIS和设备配置。

Conclusion: RIS辅助的ISAC系统在近场和远场共存场景下表现出色，为IIoT提供了高效的解决方案。

Abstract: The Industrial Internet of Things (IIoT) has emerged as a key technology for
realizing the vision of Industry 6.0, requiring the seamless integration of
diverse connected devices. In particular, integrated sensing and communication
(ISAC) plays a critical role in supporting real-time control and automation
within IIoT systems. In this paper, we explore reconfigurable intelligent
surface (RIS)-assisted ISAC systems for IIoT in the coexistence of near-field
and far-field regions. The system consists of a full-duplex access point (AP),
a RIS and multiple IIoT devices, where the near-field devices simultaneously
perform sensing and communication, while the far-field devices rely on a
RIS-assisted communication. To enhance spectral efficiency for both sensing and
communication functionalities, we consider the use of both traditional
sensing-only (SO) and ISAC frequency bands. Moreover, uplink non-orthogonal
multiple access (NOMA) is employed to facilitate the sequential decoding of
superimposed communication and sensing signals from IIoT devices. To maximize
sensing accuracy in terms of Cram${\Grave{\textrm{e}}}$r-Rao bound (CRB), we
formulate a joint optimization of RIS phase shift, bandwidth splitting ratio
and receive beamforming vector subject to the minimum data rate requirements of
IIoT devices and resource budget constraints. The algorithmic solution is
developed via the successive convex approximation (SCA)-based alternating
optimization (AO) method with the semi-definite relaxation (SDR) technique.
Numerical results demonstrate that the proposed method significantly
outperforms conventional methods relying solely on either ISAC or SO band by
achieving superior performance across RIS and device configurations, while
ensuring robust ISAC performance under the near-field and far-field coexistence
scenarios.

</details>


### [7] [Consistent and Asymptotically Efficient Localization from Bearing-only Measurements](https://arxiv.org/abs/2507.07647)
*Shenghua Hu,Guangyang Zeng,Wenchao Xue,Haitao Fang,Biqiang Mu*

Main category: eess.SP

TL;DR: 论文研究了基于纯方位测量的信号源定位问题，提出了一种两阶段估计器，具有与最大似然估计器相同的渐近特性，但计算复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 解决最大似然估计器因非凸优化问题难以计算的问题，同时保持其渐近特性。

Method: 通过代数操作构建线性最小二乘问题，获得有偏闭式解，再消除偏差得到一致估计器；第二步使用高斯-牛顿迭代。

Result: 仿真结果表明，所提出的两阶段估计器在大样本量下性能优越。

Conclusion: 两阶段估计器在计算效率和渐近性能上均优于传统方法。

Abstract: We study the problem of signal source localization using bearing-only
measurements. Initially, we present easily verifiable geometric conditions for
sensor deployment to ensure the asymptotic identifiability of the model and
demonstrate the consistency and asymptotic efficiency of the maximum likelihood
(ML) estimator. However, obtaining the ML estimator is challenging due to its
association with a non-convex optimization problem. To address this, we propose
a two-step estimator that shares the same asymptotic properties as the ML
estimator while offering low computational complexity, linear in the number of
measurements. The primary challenge lies in obtaining a preliminary consistent
estimator in the first step. To achieve this, we construct a linear
least-squares problem through algebraic operations on the measurement nonlinear
model to first obtain a biased closed-form solution. We then eliminate the bias
using the data to yield an asymptotically unbiased and consistent estimator.
The key to this process is obtaining a consistent estimator of the variance of
the sine of the noise by taking the reciprocal of the maximum eigenvalue of a
specially constructed matrix from the data. In the second step, we perform a
single Gauss-Newton iteration using the preliminary consistent estimator as the
initial value, achieving the same asymptotic properties as the ML estimator.
Finally, simulation results demonstrate the superior performance of the
proposed two-step estimator for large sample sizes.

</details>


### [8] [Signal Prediction for Loss Mitigation in Tactile Internet: A Leader-Follower Game-Theoretic Approach](https://arxiv.org/abs/2507.07692)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种基于合作Stackelberg游戏的Leader-Follower方法，用于预测触觉信号，以解决延迟和丢包问题，提高远程操作的可靠性。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网（TI）需要实现超低延迟和高可靠性的信号传输，但在丢包和延迟情况下，信号预测成为关键解决方案。

Method: 采用Leader-Follower（LeFo）方法，基于合作Stackelberg游戏，使用户和机器人能够学习和预测动作。

Result: 预测准确率在人类端（H）为80.62%至95.03%，机器人端（R）为70.44%至89.77%，并通过泰勒展开建立了信号丢失的上限。

Conclusion: 该方法有效降低了延迟要求，提高了系统的鲁棒性和可靠性。

Abstract: Tactile Internet (TI) requires achieving ultra-low latency and highly
reliable packet delivery for haptic signals. In the presence of packet loss and
delay, the signal prediction method provides a viable solution for recovering
the missing signals. To this end, we introduce the Leader-Follower (LeFo)
approach based on a cooperative Stackelberg game, which enables both users and
robots to learn and predict actions. With accurate prediction, the
teleoperation system can safely relax its strict delay requirements. Our method
achieves high prediction accuracy, ranging from 80.62% to 95.03% for remote
robot signals at the Human ($H$) side and from 70.44% to 89.77% for human
operation signals at the remote Robot ($R$) side. We also establish an upper
bound for maximum signal loss using Taylor Expansion, ensuring robustness.

</details>


### [9] [Flying Base Stations for Offshore Wind Farm Monitoring and Control: Holistic Performance Evaluation and Optimization](https://arxiv.org/abs/2507.07832)
*Xinyi Lin,Peizheng Li,Adnan Aijaz*

Main category: eess.SP

TL;DR: 本文提出了一种基于飞行基站（FBS）的方法，用于英国Hornsea海上风电场的广域监测与控制，通过优化延迟模型和多目标优化框架，显著降低了通信延迟并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 海上风电场因环境恶劣且缺乏基础设施，难以实现可靠的低延迟通信，亟需创新的解决方案。

Method: 开发了一个端到端延迟模型，结合飞行基站平台，通过轨迹规划、波束成形和资源分配的多目标优化框架，最小化整体延迟。

Result: 仿真结果表明，该方法在不同功率水平下均能有效降低延迟，并优于基准设计。

Conclusion: FBS方法为海上风电场的实时监测与控制提供了一种高效且灵活的解决方案。

Abstract: Ensuring reliable and low-latency communication in offshore wind farms is
critical for efficient monitoring and control, yet remains challenging due to
the harsh environment and lack of infrastructure. This paper investigates a
flying base station (FBS) approach for wide-area monitoring and control in the
UK Hornsea offshore wind farm project. By leveraging mobile, flexible FBS
platforms in the remote and harsh offshore environment, the proposed system
offers real-time connectivity for turbines without the need for deploying
permanent infrastructure at the sea. We develop a detailed and practical
end-to-end latency model accounting for five key factors: flight duration,
connection establishment, turbine state information upload, computational
delay, and control transmission, to provide a holistic perspective often
missing in prior studies. Furthermore, we combine trajectory planning,
beamforming, and resource allocation into a multi-objective optimization
framework for the overall latency minimization, specifically designed for
large-scale offshore wind farm deployments. Simulation results verify the
effectiveness of our proposed method in minimizing latency and enhancing
efficiency in FBS-assisted offshore monitoring across various power levels,
while consistently outperforming baseline designs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Generic Speech Enhancement with Self-Supervised Representation Space Loss](https://arxiv.org/abs/2507.07631)
*Hiroshi Sato,Tsubasa Ochiai,Marc Delcroix,Takafumi Moriya,Takanori Ashihara,Ryo Masumura*

Main category: eess.AS

TL;DR: 提出一种通用的语音增强前端，通过自监督学习特征表示优化训练准则，提升多种下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强模型需针对每项任务调整，难以泛化到未知任务，因此研究旨在构建通用前端。

Method: 提出新训练准则，最小化增强信号与干净信号在自监督学习特征表示域的距离。

Result: 实验证明，该方法提升多种语音任务性能，同时保持增强信号的感知质量。

Conclusion: 该方法有效构建通用语音增强前端，适用于多种下游任务。

Abstract: Single-channel speech enhancement is utilized in various tasks to mitigate
the effect of interfering signals. Conventionally, to ensure the speech
enhancement performs optimally, the speech enhancement has needed to be tuned
for each task. Thus, generalizing speech enhancement models to unknown
downstream tasks has been challenging. This study aims to construct a generic
speech enhancement front-end that can improve the performance of back-ends to
solve multiple downstream tasks. To this end, we propose a novel training
criterion that minimizes the distance between the enhanced and the ground truth
clean signal in the feature representation domain of self-supervised learning
models. Since self-supervised learning feature representations effectively
express high-level speech information useful for solving various downstream
tasks, the proposal is expected to make speech enhancement models preserve such
information. Experimental validation demonstrates that the proposal improves
the performance of multiple speech tasks while maintaining the perceptual
quality of the enhanced signal.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [11] [Audio-Visual Speech Separation via Bottleneck Iterative Network](https://arxiv.org/abs/2507.07270)
*Sidong Zhang,Shiv Shankar,Trang Nguyen,Andrea Fanelli,Madalina Fiterau*

Main category: cs.SD

TL;DR: 提出了一种名为Bottleneck Iterative Network (BIN)的轻量级融合方法，通过迭代表示细化提升语音分离模型的性能，同时减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 非听觉信息的整合可以显著提升语音分离模型的性能，但现有方法通常过于昂贵或轻量但能力不足。

Method: 采用轻量级融合块和融合令牌的迭代表示细化技术，平衡模型性能和训练成本。

Result: 在NTCD-TIMIT和LRS3+WHAM!数据集上，BIN在SI-SDRi指标上优于现有基准模型，同时减少50%以上的训练和推理时间。

Conclusion: BIN是一种高效且性能优越的语音分离方法，适用于复杂任务。

Abstract: Integration of information from non-auditory cues can significantly improve
the performance of speech-separation models. Often such models use deep
modality-specific networks to obtain unimodal features, and risk being too
costly or lightweight but lacking capacity. In this work, we present an
iterative representation refinement approach called Bottleneck Iterative
Network (BIN), a technique that repeatedly progresses through a lightweight
fusion block, while bottlenecking fusion representations by fusion tokens. This
helps improve the capacity of the model, while avoiding major increase in model
size and balancing between the model performance and training cost. We test BIN
on challenging noisy audio-visual speech separation tasks, and show that our
approach consistently outperforms state-of-the-art benchmark models with
respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously
achieving a reduction of more than 50% in training and GPU inference time
across nearly all settings.

</details>


### [12] [SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion Models](https://arxiv.org/abs/2507.07318)
*Christian Templin,Yanda Zhu,Hao Wang*

Main category: cs.SD

TL;DR: 论文提出了一种名为SonicMotion的端到端模型，用于生成具有动态声源的3D场景空间音频，并发布了一个新的模拟空间音频-字幕对数据集。


<details>
  <summary>Details</summary>
Motivation: 空间音频在沉浸式娱乐（如VR/AR）中至关重要，但现有的一阶Ambisonics（FOA）生成模型需要扩展以支持动态声源的3D场景生成。

Method: 提出了SonicMotion模型的两种变体，根据用户输入和声源定位精度进行区分，并构建了一个新的模拟空间音频-字幕对数据集。

Result: 模型在语义对齐和音频质量上与现有先进模型相当，同时能捕捉所需的空间属性。

Conclusion: SonicMotion模型为动态声源的3D场景空间音频生成提供了有效解决方案。

Abstract: Spatial audio is an integral part of immersive entertainment, such as VR/AR,
and has seen increasing popularity in cinema and music as well. The most common
format of spatial audio is described as first-order Ambisonics (FOA). We seek
to extend recent advancements in FOA generative AI models to enable the
generation of 3D scenes with dynamic sound sources. Our proposed end-to-end
model, SonicMotion, comes in two variations which vary in their user input and
level of precision in sound source localization. In addition to our model, we
also present a new dataset of simulated spatial audio-caption pairs. Evaluation
of our models demonstrate that they are capable of matching the semantic
alignment and audio quality of state of the art models while capturing the
desired spatial attributes.

</details>


### [13] [VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching](https://arxiv.org/abs/2507.07384)
*Yu Chen,Xinyuan Qian,Hongxu Zhu,Jiadong Wang,Kainan Chen,Haizhou Li*

Main category: cs.SD

TL;DR: 论文提出CI-AVL任务和VP-SelDoA方法，通过跨实例数据减少对配对音频-视觉数据的依赖，并利用语义-空间匹配机制提升定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有AV-SSL方法在多源场景中无法选择性隔离目标声源，且视觉语义特征与声学空间特征存在不对齐问题，同时过度依赖配对数据。

Method: 提出VP-SelDoA方法，结合语义级模态融合和Frequency-Temporal ConMamba架构生成目标选择性掩码，并通过Semantic-Spatial Matching机制对齐特征。

Result: 在VGG-SSL数据集上，MAE为12.04，ACC为78.23%，优于现有方法。

Conclusion: CI-AVL任务和VP-SelDoA方法有效解决了现有AV-SSL的局限性，提升了定位性能和泛化能力。

Abstract: Audio-visual sound source localization (AV-SSL) identifies the position of a
sound source by exploiting the complementary strengths of auditory and visual
signals. However, existing AV-SSL methods encounter three major challenges: 1)
inability to selectively isolate the target sound source in multi-source
scenarios, 2) misalignment between semantic visual features and spatial
acoustic features, and 3) overreliance on paired audio-visual data. To overcome
these limitations, we introduce Cross-Instance Audio-Visual Localization
(CI-AVL), a novel task that leverages images from different instances of the
same sound event category to localize target sound sources, thereby reducing
dependence on paired data while enhancing generalization capabilities. Our
proposed VP-SelDoA tackles this challenging task through a semantic-level
modality fusion and employs a Frequency-Temporal ConMamba architecture to
generate target-selective masks for sound isolation. We further develop a
Semantic-Spatial Matching mechanism that aligns the heterogeneous semantic and
spatial features via integrated cross- and self-attention mechanisms. To
facilitate the CI-AVL research, we construct a large-scale dataset named
VGG-SSL, comprising 13,981 spatial audio clips across 296 sound event
categories. Extensive experiments show that our proposed method outperforms
state-of-the-art audio-visual localization methods, achieving a mean absolute
error (MAE) of 12.04 and an accuracy (ACC) of 78.23%.

</details>


### [14] [DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction](https://arxiv.org/abs/2507.07526)
*Cunhang Fan,Sheng Zhang,Jingjing Zhang,Enrui Liu,Xinhui Li,Minggang Zhao,Zhao Lv*

Main category: cs.SD

TL;DR: 论文提出DMF2Mel网络，通过动态多尺度融合技术解决脑信号解码中长序列建模的挑战，显著提升梅尔频谱重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有技术在解码脑信号时难以平衡长序列建模的效率和信息保留，尤其在连续想象语音的分钟级重建中存在挑战。

Method: 提出DMF2Mel网络，包含DC-FAM、HAMS-Net、SplineMap注意力和convMamba模块，分别用于特征分离、多尺度融合、全局-局部建模和长时序依赖捕捉。

Result: 在SparrKULee数据集上，DMF2Mel的梅尔频谱重建Pearson系数为0.074（已知被试）和0.048（未知被试），分别比基线提升48%和35%。

Conclusion: DMF2Mel通过动态多尺度融合技术有效解决了脑信号解码中的长序列建模问题，显著提升了重建性能。

Abstract: Decoding speech from brain signals is a challenging research problem.
Although existing technologies have made progress in reconstructing the mel
spectrograms of auditory stimuli at the word or letter level, there remain core
challenges in the precise reconstruction of minute-level continuous imagined
speech: traditional models struggle to balance the efficiency of temporal
dependency modeling and information retention in long-sequence decoding. To
address this issue, this paper proposes the Dynamic Multiscale Fusion Network
(DMF2Mel), which consists of four core components: the Dynamic Contrastive
Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided
Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the
bidirectional state space module (convMamba). Specifically, the DC-FAM
separates speech-related "foreground features" from noisy "background features"
through local convolution and global attention mechanisms, effectively
suppressing interference and enhancing the representation of transient signals.
HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of
high-level semantics and low-level details. The SplineMap attention mechanism
integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine
global context modeling with spline-based local fitting. The convMamba captures
long-range temporal dependencies with linear complexity and enhances nonlinear
dynamic modeling capabilities. Results on the SparrKULee dataset show that
DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram
reconstruction for known subjects (a 48% improvement over the baseline) and
0.048 for unknown subjects (a 35% improvement over the baseline).Code is
available at: https://github.com/fchest/DMF2Mel.

</details>


### [15] [Assessing the Alignment of Audio Representations with Timbre Similarity Ratings](https://arxiv.org/abs/2507.07764)
*Haokun Tian,Stefan Lattner,Charalampos Saitis*

Main category: cs.SD

TL;DR: 论文提出了一种评估音频表示与人类音色相似性判断对齐的指标，发现基于CLAP模型和声音匹配模型的风格嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决传统音色空间的可扩展性和泛化能力不足问题，探索深度学习在音色相似性建模中的潜力。

Method: 引入指标比较音频嵌入距离与人类相似性评分的绝对值和排名，评估了多种音频表示。

Result: CLAP模型和声音匹配模型的风格嵌入显著优于其他表示。

Conclusion: 风格嵌入在音色相似性建模中表现出潜力，为未来研究提供了方向。

Abstract: Psychoacoustical so-called "timbre spaces" map perceptual similarity ratings
of instrument sounds onto low-dimensional embeddings via multidimensional
scaling, but suffer from scalability issues and are incapable of
generalization. Recent results from audio (music and speech) quality assessment
as well as image similarity have shown that deep learning is able to produce
embeddings that align well with human perception while being largely free from
these constraints. Although the existing human-rated timbre similarity data is
not large enough to train deep neural networks (2,614 pairwise ratings on 334
audio samples), it can serve as test-only data for audio models. In this paper,
we introduce metrics to assess the alignment of diverse audio representations
with human judgments of timbre similarity by comparing both the absolute values
and the rankings of embedding distances to human similarity ratings. Our
evaluation involves three signal-processing-based representations, twelve
representations extracted from pre-trained models, and three representations
extracted from a novel sound matching model. Among them, the style embeddings
inspired by image style transfer, extracted from the CLAP model and the sound
matching model, remarkably outperform the others, showing their potential in
modeling timbre similarity.

</details>


### [16] [SecureSpeech: Prompt-based Speaker and Content Protection](https://arxiv.org/abs/2507.07799)
*Belinda Soh Hui Hui,Xiaoxiao Miao,Xin Wang*

Main category: cs.SD

TL;DR: 本文提出了一种基于提示的语音生成流程，通过双重匿名化（说话者身份和内容）保护隐私，同时保持内容保留和音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决语音领域中因身份盗用和说话者重新识别引发的隐私问题。

Method: 1) 生成与源说话者不可链接的身份；2) 使用命名实体识别和大语言模型替换敏感内容；3) 通过文本转语音模型生成高质量匿名语音。

Result: 实验结果显示显著隐私保护效果，同时内容保留和音频质量良好。

Conclusion: 该流程有效平衡隐私与实用性，并探讨了说话者描述对生成语音的潜在偏见影响。

Abstract: Given the increasing privacy concerns from identity theft and the
re-identification of speakers through content in the speech field, this paper
proposes a prompt-based speech generation pipeline that ensures dual
anonymization of both speaker identity and spoken content. This is addressed
through 1) generating a speaker identity unlinkable to the source speaker,
controlled by descriptors, and 2) replacing sensitive content within the
original text using a name entity recognition model and a large language model.
The pipeline utilizes the anonymized speaker identity and text to generate
high-fidelity, privacy-friendly speech via a text-to-speech synthesis model.
Experimental results demonstrate an achievement of significant privacy
protection while maintaining a decent level of content retention and audio
quality. This paper also investigates the impact of varying speaker
descriptions on the utility and privacy of generated speech to determine
potential biases.

</details>


### [17] [End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced by Semi-supervised Learning](https://arxiv.org/abs/2507.07806)
*Zhao Ren,Rathi Adarshi Rammohan,Kevin Scheck,Sheng Li,Tanja Schultz*

Main category: cs.SD

TL;DR: 论文提出了一种半监督学习方法，结合未标注和少量标注数据，用于语音情感和意图识别，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于手动标注语音数据成本高昂，难以训练机器学习模型，因此研究如何利用大量未标注数据提升识别性能。

Method: 采用端到端的声学和语言模型，结合多任务学习，比较了两种半监督学习方法（fix-match和full-match）。

Result: 实验表明，半监督学习方法显著提升了识别性能，最佳模型的后期融合在联合识别平衡指标上分别优于声学和文本基线12.3%和10.4%。

Conclusion: 半监督学习在语音情感和意图识别中具有显著优势，能够有效利用未标注数据提升模型性能。

Abstract: Emotion and intent recognition from speech is essential and has been widely
investigated in human-computer interaction. The rapid development of social
media platforms, chatbots, and other technologies has led to a large volume of
speech data streaming from users. Nevertheless, annotating such data manually
is expensive, making it challenging to train machine learning models for
recognition purposes. To this end, we propose applying semi-supervised learning
to incorporate a large scale of unlabelled data alongside a relatively smaller
set of labelled data. We train end-to-end acoustic and linguistic models, each
employing multi-task learning for emotion and intent recognition. Two
semi-supervised learning approaches, including fix-match learning and
full-match learning, are compared. The experimental results demonstrate that
the semi-supervised learning approaches improve model performance in speech
emotion and intent recognition from both acoustic and text data. The late
fusion of the best models outperforms the acoustic and text baselines by joint
recognition balance metrics of 12.3% and 10.4%, respectively.

</details>


### [18] [Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders](https://arxiv.org/abs/2507.07867)
*Dimitrios Bralios,Jonah Casebeer,Paris Smaragdis*

Main category: cs.SD

TL;DR: 论文提出了一种名为“Re-Bottleneck”的后处理框架，通过修改预训练自编码器的瓶颈层，为其潜在空间注入用户定义的结构，从而优化下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器和自编码器通常专注于重建保真度，而忽略了潜在空间结构对多样化下游任务的重要性。

Method: 通过引入“Re-Bottleneck”框架，在预训练自编码器的瓶颈层中仅通过潜在空间损失训练，以强制用户定义的结构。

Result: 实验表明，该方法能在不牺牲重建质量的情况下实现潜在通道排序、语义嵌入对齐和等变性。

Conclusion: Re-Bottleneck框架提供了一种灵活高效的方式，使神经音频模型的表示能够适应多样化应用需求。

Abstract: Neural audio codecs and autoencoders have emerged as versatile models for
audio compression, transmission, feature-extraction, and latent-space
generation. However, a key limitation is that most are trained to maximize
reconstruction fidelity, often neglecting the specific latent structure
necessary for optimal performance in diverse downstream applications. We
propose a simple, post-hoc framework to address this by modifying the
bottleneck of a pre-trained autoencoder. Our method introduces a
"Re-Bottleneck", an inner bottleneck trained exclusively through latent space
losses to instill user-defined structure. We demonstrate the framework's
effectiveness in three experiments. First, we enforce an ordering on latent
channels without sacrificing reconstruction quality. Second, we align latents
with semantic embeddings, analyzing the impact on downstream diffusion
modeling. Third, we introduce equivariance, ensuring that a filtering operation
on the input waveform directly corresponds to a specific transformation in the
latent space. Ultimately, our Re-Bottleneck framework offers a flexible and
efficient way to tailor representations of neural audio models, enabling them
to seamlessly meet the varied demands of different applications with minimal
additional training.

</details>


### [19] [Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models](https://arxiv.org/abs/2507.07877)
*Chen Feng,Yicheng Lin,Shaojie Zhuo,Chenzheng Su,Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Xiaopeng Zhang*

Main category: cs.SD

TL;DR: 论文全面评估了八种先进的PTQ方法在Whisper和Moonshine模型上的表现，发现3位量化在高容量模型中仍能保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 研究量化方法在资源受限的边缘设备上优化ASR模型的性能。

Method: 使用LLM压缩工具包扩展的框架，系统评估八种PTQ方法在七种数据集上的表现。

Result: 3位量化在高容量模型中表现良好，提供了效率与准确性的权衡。

Conclusion: 研究结果为低功耗边缘设备上的ASR模型优化提供了重要参考。

Abstract: Recent advances in Automatic Speech Recognition (ASR) have demonstrated
remarkable accuracy and robustness in diverse audio applications, such as live
transcription and voice command processing. However, deploying these models on
resource constrained edge devices (e.g., IoT device, wearables) still presents
substantial challenges due to strict limits on memory, compute and power.
Quantization, particularly Post-Training Quantization (PTQ), offers an
effective way to reduce model size and inference cost without retraining.
Despite its importance, the performance implications of various advanced
quantization methods and bit-width configurations on ASR models remain unclear.
In this work, we present a comprehensive benchmark of eight state-of-the-art
(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and
Moonshine. We systematically evaluate model performances (i.e., accuracy,
memory I/O and bit operations) across seven diverse datasets from the open ASR
leaderboard, analyzing the impact of quantization and various configurations on
both weights and activations. Built on an extension of the LLM compression
toolkit, our framework integrates edge-ASR models, diverse advanced
quantization algorithms, a unified calibration and evaluation data pipeline,
and detailed analysis tools. Our results characterize the trade-offs between
efficiency and accuracy, demonstrating that even 3-bit quantization can succeed
on high capacity models when using advanced PTQ techniques. These findings
provide valuable insights for optimizing ASR models on low-power, always-on
edge devices.

</details>


### [20] [LISTEN: Lightweight Industrial Sound-representable Transformer for Edge Notification](https://arxiv.org/abs/2507.07879)
*Changheon Han,Yun Seok Kang,Yuseop Sim,Martin Byung-Guk Jun,Hyung Wook Park*

Main category: cs.SD

TL;DR: LISTEN是一个轻量级工业声音基础模型，通过知识蒸馏实现低成本边缘设备的实时运行，性能接近大型模型。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在工业声音分析中依赖大数据集和高计算资源的问题，实现现场实时部署。

Method: 使用知识蒸馏技术开发轻量级模型LISTEN，适用于低成本边缘设备。

Result: LISTEN在基准任务中表现接近大型模型，且在小数据集和低资源下仍有效。

Conclusion: LISTEN成功集成到工业物联网系统中，验证了其在实际制造环境中的性能和泛化能力。

Abstract: Deep learning-based machine listening is broadening the scope of industrial
acoustic analysis for applications like anomaly detection and predictive
maintenance, thereby improving manufacturing efficiency and reliability.
Nevertheless, its reliance on large, task-specific annotated datasets for every
new task limits widespread implementation on shop floors. While emerging sound
foundation models aim to alleviate data dependency, they are too large and
computationally expensive, requiring cloud infrastructure or high-end hardware
that is impractical for on-site, real-time deployment. We address this gap with
LISTEN (Lightweight Industrial Sound-representable Transformer for Edge
Notification), a kilobyte-sized industrial sound foundation model. Using
knowledge distillation, LISTEN runs in real-time on low-cost edge devices. On
benchmark downstream tasks, it performs nearly identically to its much larger
parent model, even when fine-tuned with minimal datasets and training resource.
Beyond the model itself, we demonstrate its real-world utility by integrating
LISTEN into a complete machine monitoring framework on an edge device with an
Industrial Internet of Things (IIoT) sensor and system, validating its
performance and generalization capabilities on a live manufacturing shop floor.

</details>


### [21] [Input Conditioned Layer Dropping in Speech Foundation Models](https://arxiv.org/abs/2507.07954)
*Abdul Hannan,Daniele Falavigna,Alessio Brutti*

Main category: cs.SD

TL;DR: 论文提出了一种基于输入驱动的层丢弃方法（LD），用于动态调整语音模型的架构，以适应边缘和物联网设备的计算资源变化。


<details>
  <summary>Details</summary>
Motivation: 在边缘和物联网环境中，计算资源随时间变化，需要动态架构来优化模型性能。现有层丢弃方法在层选择模式或架构修改上存在局限性。

Method: 提出输入驱动的LD方法，利用输入特征和轻量级层选择网络确定最佳处理层组合。

Result: 在4个语音和音频基准测试中，该方法显著优于随机丢弃，性能与早期退出方法相当或更好。

Conclusion: 输入驱动的LD方法有效提升了动态语音模型的性能，适用于资源受限环境。

Abstract: Curating foundation speech models for edge and IoT settings, where
computational resources vary over time, requires dynamic architectures
featuring adaptable reduction strategies. One emerging approach is layer
dropping ($\mathcal{LD}$) which skips fraction of the layers of a backbone
network during inference to reduce the computational load. This allows
transforming static models into dynamic ones. However, existing approaches
exhibit limitations either in the mode of selecting layers or by significantly
modifying the neural architecture. To this end, we propose input-driven
$\mathcal{LD}$ that employs the network's input features and a lightweight
layer selecting network to determine the optimum combination of processing
layers. Extensive experimentation on 4 speech and audio public benchmarks,
using two different pre-trained foundation models, demonstrates the
effectiveness of our approach, thoroughly outperforming random dropping and
producing on-par (or better) results to early exit.

</details>

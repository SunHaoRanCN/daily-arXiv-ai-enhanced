<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Three-Dimensional Millimeter-Wave Imaging Using Active Incoherent Fourier Processing and Pulse Compression](https://arxiv.org/abs/2507.07239)
*Jorge R. Colon-Berrios,Jason M. Merlo,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 提出了一种结合二维空间傅里叶域成像技术与传统雷达脉冲压缩的三维成像方法，通过噪声信号和线性调频信号的联合处理，实现目标的三维重建。


<details>
  <summary>Details</summary>
Motivation: 传统成像方法在同时获取高分辨率跨范围和下范围信息方面存在挑战，需要一种新的技术来解决这一问题。

Method: 系统采用四个发射器，其中三个发射空间和时间不相干的噪声信号，第四个发射已知的线性调频脉冲信号，通过干涉处理和匹配滤波实现三维成像。

Result: 通过仿真和实验数据验证，成功重建了目标的三维图像。

Conclusion: 该方法有效结合了噪声信号和线性调频信号的优势，实现了高分辨率的三维成像。

Abstract: We present a novel three-dimensional (3D) imaging approach that combines
two-dimensional spatial Fourier-domain imaging techniques with traditional
radar pulse compression to recover both cross-range and down-range scene
information. The imaging system employs four transmitters, three of which emit
spatially and temporally incoherent noise signals, while the fourth transmits a
known linear frequency modulated (LFM) pulsed signal. The spatial incoherence
of the noise signals enables sampling of the 2D spatial Fourier spectrum of the
scene from which two-dimensional cross-range (azimuth and elevation) images can
be formed via interferometric processing. Simultaneously, the LFM signal
enables high-resolution downrange imaging through matched filtering. The
received signals consist of a superposition of the noise sources and the known
pulse allowing for joint recovery of all three dimensions. We describe the
system architecture and waveform design, and demonstrate the imaging technique
using both simulations with a linear array and experimental data from a 38 GHz
active incoherent millimeter-wave imaging system with 23-element randomized
array. Results show the reconstruction of targets in three dimensions.

</details>


### [2] [A RIS-Enabled Computational Radar Coincidence Imaging](https://arxiv.org/abs/2507.07285)
*Kavian Zirak,Mohammadreza F. Imani*

Main category: eess.SP

TL;DR: 本文提出了一种结合雷达重合成像（RCI）和计算成像技术的创新成像方法，利用可重构智能表面（RISs）生成高信噪比和低杂波的图像。


<details>
  <summary>Details</summary>
Motivation: 传统成像方法如光栅扫描需要大量测量，而计算成像虽减少测量次数但通常依赖随机模式，导致信噪比低。本文旨在结合两者的优势。

Method: 通过RISs将波束重定向至感兴趣区域（ROI），利用波束干涉形成空间多样的散斑图案，结合RCI和计算成像技术。

Result: 数值模拟显示，该方法在少量测量下即可获得高质量图像，信噪比高且杂波少。

Conclusion: 该方法适用于安全筛查、无线用户跟踪和活动识别，具有实际应用潜力。

Abstract: This paper introduces an innovative imaging method using reconfigurable
intelligent surfaces (RISs) by combining radar coincidence imaging (RCI) and
computational imaging techniques. In the proposed framework, RISs
simultaneously redirect beams toward a desired region of interest (ROI). The
interference of these beams forms spatially diverse speckle patterns that carry
information about the entire ROI. As a result, this method can take advantage
of the benefits of both random patterns and spotlight imaging. Since the
speckle pattern is formed by directive beams (instead of random patterns
typically used in computational imaging), this approach results in a higher
signal-to-noise ratio (SNR) and reduced clutter. In contrast to raster
scanning, which requires the number of measurements to be at least equal to the
number of unknowns, our proposed approach follows a computational imaging
framework and can obtain high-quality images even when only a few measurements
are taken. Using numerical simulation, we demonstrate this method's
capabilities and contrast it against other conventional techniques. The
proposed imaging approach can be applied to security screening, wireless user
tracking, and activity recognition.

</details>


### [3] [mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar](https://arxiv.org/abs/2507.07331)
*Anurag Pallaprolu,Winston Hurst,Yasamin Mostofi*

Main category: eess.SP

TL;DR: 提出了一种基于毫米波雷达的新框架，用于提取人群运动模式并推断语义，通过信号处理和几何图转换实现高保真重建和语义分析。


<details>
  <summary>Details</summary>
Motivation: 解决从毫米波雷达数据中提取人群运动模式和语义的挑战，为人群分析提供新方法。

Method: 结合光流估计与噪声过滤生成毫米波流场，转换为有向几何图，并通过雅可比矩阵分析提取语义。

Result: 在21次实验中成功重建复杂人群流动结构，并准确推断出人群语义（如转向、边界等）。

Conclusion: 验证了框架的有效性，展示了其在人群分析应用中的潜力。

Abstract: In this paper, we present a novel framework for extracting underlying crowd
motion patterns and inferring crowd semantics using mmWave radar. First, our
proposed signal processing pipeline combines optical flow estimation concepts
from vision with novel statistical and morphological noise filtering to
generate high-fidelity mmWave flow fields - compact 2D vector representations
of crowd motion. We then introduce a novel approach that transforms these
fields into directed geometric graphs, where edges capture dominant flow
currents, vertices mark crowd splitting or merging, and flow distribution is
quantified across edges. Finally, we show that by analyzing the local Jacobian
and computing the corresponding curl and divergence, we can extract key crowd
semantics for both structured and diffused crowds. We conduct 21 experiments on
crowds of up to (and including) 20 people across 3 areas, using commodity
mmWave radar. Our framework achieves high-fidelity graph reconstruction of the
underlying flow structure, even for complex crowd patterns, demonstrating
strong spatial alignment and precise quantitative characterization of flow
split ratios. Finally, our curl and divergence analysis accurately infers key
crowd semantics, e.g., abrupt turns, boundaries where flow directions shift,
dispersions, and gatherings. Overall, these findings validate our framework,
underscoring its potential for various crowd analytics applications.

</details>


### [4] [Featureless Wireless Communications using Enhanced Autoencoder](https://arxiv.org/abs/2507.07474)
*Ruhui Zhang,Wei Lin,Binbin Chen*

Main category: eess.SP

TL;DR: 论文提出了一种基于自动编码器（AE）的无线通信系统，通过改进损失函数和输入编码方式，生成低检测/拦截概率（LPD/LPI）的无特征信号，同时降低块错误率（BLER）。


<details>
  <summary>Details</summary>
Motivation: 研究利用AE生成无特征信号以增强无线通信的安全性和可靠性。

Method: 1. 引入新的损失函数，结合KL散度和分类交叉熵；2. 用二进制输入替代独热编码，结合传统纠错编码方案。

Result: 实验表明，该方法提升了信号的无特征性，并显著降低了BLER。

Conclusion: 基于AE的方法在安全可靠的无线通信系统中具有潜力。

Abstract: Artificial intelligence (AI) techniques, particularly autoencoders (AEs),
have gained significant attention in wireless communication systems. This paper
investigates using an AE to generate featureless signals with a low probability
of detection and interception (LPD/LPI). Firstly, we introduce a novel loss
function that adds a KL divergence term to the categorical cross entropy,
enhancing the noise like characteristics of AE-generated signals while
preserving block error rate (BLER). Secondly, to support long source message
blocks for the AE's inputs, we replace one-hot inputs of source blocks with
binary inputs pre-encoded by conventional error correction coding schemes. The
AE's outputs are then decoded back to the source blocks using the same scheme.
This design enables the AE to learn the coding structure, yielding superior
BLER performance on coded blocks and the BLER of the source blocks is further
decreased by the error correction decoder. Moreover, we also validate the AE
based communication system in the over-the-air communication. Experimental
results demonstrate that our proposed methods improve the featureless
properties of AE signals and significantly reduce the BLER of message blocks,
underscoring the promise of our AE-based approach for secure and reliable
wireless communication systems.

</details>


### [5] [Leveraging Power Amplifier Distortion for Physical Layer Security](https://arxiv.org/abs/2507.07567)
*Reza Ghasemi Alavicheh,Thomas Feys,MD Arifur Rahman,François Rottenberg*

Main category: eess.SP

TL;DR: 本文提出了一种利用功率放大器非线性失真的物理层安全新方法，通过Z3RO预编码器将失真导向非用户位置，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 传统物理层安全技术通常注入与合法信道正交的人工噪声，而本文发现功率放大器的非线性失真可被利用以增强安全性。

Method: 采用Z3RO预编码器，通过多天线负极性抵消用户位置的失真，将失真导向非用户位置以干扰潜在窃听者。

Result: 数值模拟显示，Z3RO预编码器在特定条件下比传统MRT预编码的保密率提升2.5倍。

Conclusion: 功率放大器的非线性失真可被有效利用以提升物理层安全性能。

Abstract: This paper introduces a new approach to physical layer security (PLS) by
leveraging power amplifier (PA) nonlinear distortion through distortion-aware
precoding. While some conventional PLS techniques inject artificial noise
orthogonal to legitimate channels, we demonstrate that inherent PA
nonlinearities typically considered undesirable can be exploited to enhance
security. The zero 3rd order (Z3RO) precoder applies a negative polarity to
several antennas to cancel the PA distortion at the user location, resulting in
distortion being transmitted in non-user locations. Redirecting the distortion
to non-user locations creates interference for potential eavesdroppers,
lowering their signal-to-noise-and-distortion ratio (SNDR). Numerical
simulations reveal that the Z3RO precoder achieves up to a $2.5\times$
improvement in secrecy rate compared to conventional maximum ratio transmission
(MRT) precoding under a $10\%$ outage probability, SNR of $32$ dB and $-5$ dB
input back-off (IBO) where the PAs enter the saturation regime.

</details>


### [6] [RIS-assisted ISAC Systems for Industrial Revolution 6.0: Exploring the Near-field and Far-field Coexistence](https://arxiv.org/abs/2507.07643)
*Seonghoon Yoo,Jaemin Jung,Seongah Jeong,Jinkyu Kang,Markku Juntti,Joonhyuk Kang*

Main category: eess.SP

TL;DR: 论文研究了RIS辅助的ISAC系统在IIoT中的应用，结合近场和远场区域，优化了频谱效率和感知精度。


<details>
  <summary>Details</summary>
Motivation: 实现工业6.0愿景需要无缝集成多样化设备，ISAC在IIoT系统中对实时控制和自动化至关重要。

Method: 提出RIS辅助的ISAC系统，结合SO和ISAC频段，采用NOMA技术，通过SCA-AO和SDR方法优化资源分配。

Result: 数值结果表明，该方法在频谱效率和感知精度上显著优于传统方法，适应近场和远场共存场景。

Conclusion: RIS辅助的ISAC系统在IIoT中具有优越性能，为工业6.0提供了可行的技术方案。

Abstract: The Industrial Internet of Things (IIoT) has emerged as a key technology for
realizing the vision of Industry 6.0, requiring the seamless integration of
diverse connected devices. In particular, integrated sensing and communication
(ISAC) plays a critical role in supporting real-time control and automation
within IIoT systems. In this paper, we explore reconfigurable intelligent
surface (RIS)-assisted ISAC systems for IIoT in the coexistence of near-field
and far-field regions. The system consists of a full-duplex access point (AP),
a RIS and multiple IIoT devices, where the near-field devices simultaneously
perform sensing and communication, while the far-field devices rely on a
RIS-assisted communication. To enhance spectral efficiency for both sensing and
communication functionalities, we consider the use of both traditional
sensing-only (SO) and ISAC frequency bands. Moreover, uplink non-orthogonal
multiple access (NOMA) is employed to facilitate the sequential decoding of
superimposed communication and sensing signals from IIoT devices. To maximize
sensing accuracy in terms of Cram${\Grave{\textrm{e}}}$r-Rao bound (CRB), we
formulate a joint optimization of RIS phase shift, bandwidth splitting ratio
and receive beamforming vector subject to the minimum data rate requirements of
IIoT devices and resource budget constraints. The algorithmic solution is
developed via the successive convex approximation (SCA)-based alternating
optimization (AO) method with the semi-definite relaxation (SDR) technique.
Numerical results demonstrate that the proposed method significantly
outperforms conventional methods relying solely on either ISAC or SO band by
achieving superior performance across RIS and device configurations, while
ensuring robust ISAC performance under the near-field and far-field coexistence
scenarios.

</details>


### [7] [Consistent and Asymptotically Efficient Localization from Bearing-only Measurements](https://arxiv.org/abs/2507.07647)
*Shenghua Hu,Guangyang Zeng,Wenchao Xue,Haitao Fang,Biqiang Mu*

Main category: eess.SP

TL;DR: 提出了一种基于两步估计器的信号源定位方法，解决了最大似然估计器的非凸优化问题，具有低计算复杂度和渐近高效性。


<details>
  <summary>Details</summary>
Motivation: 解决仅基于方位测量的信号源定位问题，尤其是最大似然估计器的非凸优化挑战。

Method: 1. 通过代数操作构建线性最小二乘问题，获得有偏闭式解；2. 消除偏差，得到渐近无偏且一致的估计器；3. 使用高斯-牛顿迭代进一步优化。

Result: 仿真结果表明，所提出的两步估计器在大样本量下性能优越。

Conclusion: 两步估计器在计算效率和渐近性能上与最大似然估计器相当，适用于实际应用。

Abstract: We study the problem of signal source localization using bearing-only
measurements. Initially, we present easily verifiable geometric conditions for
sensor deployment to ensure the asymptotic identifiability of the model and
demonstrate the consistency and asymptotic efficiency of the maximum likelihood
(ML) estimator. However, obtaining the ML estimator is challenging due to its
association with a non-convex optimization problem. To address this, we propose
a two-step estimator that shares the same asymptotic properties as the ML
estimator while offering low computational complexity, linear in the number of
measurements. The primary challenge lies in obtaining a preliminary consistent
estimator in the first step. To achieve this, we construct a linear
least-squares problem through algebraic operations on the measurement nonlinear
model to first obtain a biased closed-form solution. We then eliminate the bias
using the data to yield an asymptotically unbiased and consistent estimator.
The key to this process is obtaining a consistent estimator of the variance of
the sine of the noise by taking the reciprocal of the maximum eigenvalue of a
specially constructed matrix from the data. In the second step, we perform a
single Gauss-Newton iteration using the preliminary consistent estimator as the
initial value, achieving the same asymptotic properties as the ML estimator.
Finally, simulation results demonstrate the superior performance of the
proposed two-step estimator for large sample sizes.

</details>


### [8] [Signal Prediction for Loss Mitigation in Tactile Internet: A Leader-Follower Game-Theoretic Approach](https://arxiv.org/abs/2507.07692)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种基于Stackelberg博弈的Leader-Follower方法，用于预测触觉信号，以解决延迟和丢包问题，提升远程操作的可靠性。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网（TI）需要超低延迟和高可靠性的信号传输。在丢包和延迟的情况下，信号预测方法成为恢复丢失信号的有效解决方案。

Method: 采用基于Stackelberg博弈的Leader-Follower方法，使用户和机器人能够学习和预测动作，从而放松对延迟的严格要求。

Result: 预测准确率在人类端（H）为80.62%至95.03%，在机器人端（R）为70.44%至89.77%。同时通过泰勒展开建立了信号丢失的上限，确保鲁棒性。

Conclusion: 该方法显著提升了触觉信号的预测准确性和系统鲁棒性，为触觉互联网的实际应用提供了可行方案。

Abstract: Tactile Internet (TI) requires achieving ultra-low latency and highly
reliable packet delivery for haptic signals. In the presence of packet loss and
delay, the signal prediction method provides a viable solution for recovering
the missing signals. To this end, we introduce the Leader-Follower (LeFo)
approach based on a cooperative Stackelberg game, which enables both users and
robots to learn and predict actions. With accurate prediction, the
teleoperation system can safely relax its strict delay requirements. Our method
achieves high prediction accuracy, ranging from 80.62% to 95.03% for remote
robot signals at the Human ($H$) side and from 70.44% to 89.77% for human
operation signals at the remote Robot ($R$) side. We also establish an upper
bound for maximum signal loss using Taylor Expansion, ensuring robustness.

</details>


### [9] [Flying Base Stations for Offshore Wind Farm Monitoring and Control: Holistic Performance Evaluation and Optimization](https://arxiv.org/abs/2507.07832)
*Xinyi Lin,Peizheng Li,Adnan Aijaz*

Main category: eess.SP

TL;DR: 论文提出了一种基于飞行基站（FBS）的方法，用于解决英国Hornsea海上风电场中可靠低延迟通信的挑战，通过多目标优化框架最小化延迟。


<details>
  <summary>Details</summary>
Motivation: 海上风电场环境恶劣且缺乏基础设施，导致可靠低延迟通信难以实现，影响监测与控制效率。

Method: 开发了端到端延迟模型，结合轨迹规划、波束成形和资源分配的多目标优化框架，以最小化延迟。

Result: 仿真结果表明，该方法在不同功率水平下均能有效最小化延迟并提升效率，优于基线设计。

Conclusion: FBS方法为海上风电场提供了一种无需永久基础设施的实时通信解决方案，显著提升了监测与控制效率。

Abstract: Ensuring reliable and low-latency communication in offshore wind farms is
critical for efficient monitoring and control, yet remains challenging due to
the harsh environment and lack of infrastructure. This paper investigates a
flying base station (FBS) approach for wide-area monitoring and control in the
UK Hornsea offshore wind farm project. By leveraging mobile, flexible FBS
platforms in the remote and harsh offshore environment, the proposed system
offers real-time connectivity for turbines without the need for deploying
permanent infrastructure at the sea. We develop a detailed and practical
end-to-end latency model accounting for five key factors: flight duration,
connection establishment, turbine state information upload, computational
delay, and control transmission, to provide a holistic perspective often
missing in prior studies. Furthermore, we combine trajectory planning,
beamforming, and resource allocation into a multi-objective optimization
framework for the overall latency minimization, specifically designed for
large-scale offshore wind farm deployments. Simulation results verify the
effectiveness of our proposed method in minimizing latency and enhancing
efficiency in FBS-assisted offshore monitoring across various power levels,
while consistently outperforming baseline designs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Generic Speech Enhancement with Self-Supervised Representation Space Loss](https://arxiv.org/abs/2507.07631)
*Hiroshi Sato,Tsubasa Ochiai,Marc Delcroix,Takafumi Moriya,Takanori Ashihara,Ryo Masumura*

Main category: eess.AS

TL;DR: 提出一种基于自监督学习特征表示的新型训练准则，构建通用的语音增强前端，提升多种下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强模型需针对每个任务单独调整，难以泛化到未知任务，因此需要一种通用的前端解决方案。

Method: 通过最小化增强信号与干净信号在自监督学习特征表示域的距离，保留对下游任务有用的高层语音信息。

Result: 实验证明该方法在提升多种语音任务性能的同时，保持了增强信号的感知质量。

Conclusion: 该方法成功构建了通用的语音增强前端，适用于多种下游任务。

Abstract: Single-channel speech enhancement is utilized in various tasks to mitigate
the effect of interfering signals. Conventionally, to ensure the speech
enhancement performs optimally, the speech enhancement has needed to be tuned
for each task. Thus, generalizing speech enhancement models to unknown
downstream tasks has been challenging. This study aims to construct a generic
speech enhancement front-end that can improve the performance of back-ends to
solve multiple downstream tasks. To this end, we propose a novel training
criterion that minimizes the distance between the enhanced and the ground truth
clean signal in the feature representation domain of self-supervised learning
models. Since self-supervised learning feature representations effectively
express high-level speech information useful for solving various downstream
tasks, the proposal is expected to make speech enhancement models preserve such
information. Experimental validation demonstrates that the proposal improves
the performance of multiple speech tasks while maintaining the perceptual
quality of the enhanced signal.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [11] [Audio-Visual Speech Separation via Bottleneck Iterative Network](https://arxiv.org/abs/2507.07270)
*Sidong Zhang,Shiv Shankar,Trang Nguyen,Andrea Fanelli,Madalina Fiterau*

Main category: cs.SD

TL;DR: 论文提出了一种名为Bottleneck Iterative Network（BIN）的方法，通过轻量级融合块和融合令牌优化语音分离模型的性能，同时减少训练和推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有语音分离模型通常使用深度模态特定网络获取单模态特征，但存在成本高或性能不足的问题。

Method: 提出BIN方法，通过迭代表示精炼和轻量级融合块，利用融合令牌限制融合表示，提升模型性能而不显著增加模型规模。

Result: 在NTCD-TIMIT和LRS3+WHAM!数据集上，BIN在SI-SDRi指标上优于现有基准模型，同时训练和GPU推理时间减少50%以上。

Conclusion: BIN在性能和效率之间取得了平衡，为语音分离任务提供了一种高效解决方案。

Abstract: Integration of information from non-auditory cues can significantly improve
the performance of speech-separation models. Often such models use deep
modality-specific networks to obtain unimodal features, and risk being too
costly or lightweight but lacking capacity. In this work, we present an
iterative representation refinement approach called Bottleneck Iterative
Network (BIN), a technique that repeatedly progresses through a lightweight
fusion block, while bottlenecking fusion representations by fusion tokens. This
helps improve the capacity of the model, while avoiding major increase in model
size and balancing between the model performance and training cost. We test BIN
on challenging noisy audio-visual speech separation tasks, and show that our
approach consistently outperforms state-of-the-art benchmark models with
respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously
achieving a reduction of more than 50% in training and GPU inference time
across nearly all settings.

</details>


### [12] [SonicMotion: Dynamic Spatial Audio Soundscapes with Latent Diffusion Models](https://arxiv.org/abs/2507.07318)
*Christian Templin,Yanda Zhu,Hao Wang*

Main category: cs.SD

TL;DR: 论文提出了一种名为SonicMotion的端到端模型，用于生成具有动态声源的3D场景空间音频，并提供了两种变体。同时，还发布了一个新的模拟空间音频-字幕对数据集。


<details>
  <summary>Details</summary>
Motivation: 空间音频在沉浸式娱乐（如VR/AR）中至关重要，但现有技术需要进一步扩展以支持动态声源的生成。

Method: 提出了SonicMotion模型，包含两种变体，分别针对不同用户输入和声源定位精度需求。同时构建了一个新的模拟数据集。

Result: 模型在语义对齐和音频质量上与现有技术相当，同时能捕捉所需的空间属性。

Conclusion: SonicMotion模型和数据集为动态声源的空间音频生成提供了有效解决方案。

Abstract: Spatial audio is an integral part of immersive entertainment, such as VR/AR,
and has seen increasing popularity in cinema and music as well. The most common
format of spatial audio is described as first-order Ambisonics (FOA). We seek
to extend recent advancements in FOA generative AI models to enable the
generation of 3D scenes with dynamic sound sources. Our proposed end-to-end
model, SonicMotion, comes in two variations which vary in their user input and
level of precision in sound source localization. In addition to our model, we
also present a new dataset of simulated spatial audio-caption pairs. Evaluation
of our models demonstrate that they are capable of matching the semantic
alignment and audio quality of state of the art models while capturing the
desired spatial attributes.

</details>


### [13] [VP-SelDoA: Visual-prompted Selective DoA Estimation of Target Sound via Semantic-Spatial Matching](https://arxiv.org/abs/2507.07384)
*Yu Chen,Xinyuan Qian,Hongxu Zhu,Jiadong Wang,Kainan Chen,Haizhou Li*

Main category: cs.SD

TL;DR: 论文提出了一种跨实例音频-视觉定位（CI-AVL）任务，通过利用同一声音事件类别的不同实例图像来定位目标声源，减少对配对数据的依赖并提升泛化能力。方法VP-SelDoA结合语义级模态融合和频率-时间ConMamba架构，生成目标选择性掩码，并通过语义-空间匹配机制对齐异质特征。实验表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉声源定位方法在多源场景中无法选择性隔离目标声源，视觉语义特征与声学空间特征存在不对齐，且过度依赖配对数据。

Method: 提出CI-AVL任务，采用VP-SelDoA方法，结合语义级模态融合和频率-时间ConMamba架构，生成目标选择性掩码，并通过语义-空间匹配机制对齐特征。

Result: 在构建的VGG-SSL数据集上，MAE为12.04，ACC为78.23%，优于现有方法。

Conclusion: CI-AVL任务及VP-SelDoA方法有效解决了现有挑战，提升了定位性能并减少了对配对数据的依赖。

Abstract: Audio-visual sound source localization (AV-SSL) identifies the position of a
sound source by exploiting the complementary strengths of auditory and visual
signals. However, existing AV-SSL methods encounter three major challenges: 1)
inability to selectively isolate the target sound source in multi-source
scenarios, 2) misalignment between semantic visual features and spatial
acoustic features, and 3) overreliance on paired audio-visual data. To overcome
these limitations, we introduce Cross-Instance Audio-Visual Localization
(CI-AVL), a novel task that leverages images from different instances of the
same sound event category to localize target sound sources, thereby reducing
dependence on paired data while enhancing generalization capabilities. Our
proposed VP-SelDoA tackles this challenging task through a semantic-level
modality fusion and employs a Frequency-Temporal ConMamba architecture to
generate target-selective masks for sound isolation. We further develop a
Semantic-Spatial Matching mechanism that aligns the heterogeneous semantic and
spatial features via integrated cross- and self-attention mechanisms. To
facilitate the CI-AVL research, we construct a large-scale dataset named
VGG-SSL, comprising 13,981 spatial audio clips across 296 sound event
categories. Extensive experiments show that our proposed method outperforms
state-of-the-art audio-visual localization methods, achieving a mean absolute
error (MAE) of 12.04 and an accuracy (ACC) of 78.23%.

</details>


### [14] [DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel Spectrogram Reconstruction](https://arxiv.org/abs/2507.07526)
*Cunhang Fan,Sheng Zhang,Jingjing Zhang,Enrui Liu,Xinhui Li,Minggang Zhao,Zhao Lv*

Main category: cs.SD

TL;DR: 论文提出DMF2Mel网络，通过动态多尺度融合解决脑信号解码中的长序列建模问题，显著提升了梅尔频谱重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术在解码脑信号时难以平衡时间依赖建模效率与长序列信息保留，特别是在连续想象语音的精确重建上存在挑战。

Method: 提出DMF2Mel网络，包含DC-FAM、HAMS-Net、SplineMap注意力机制和convMamba模块，分别用于特征分离、多尺度融合、全局与局部建模及长序列依赖捕获。

Result: 在SparrKULee数据集上，DMF2Mel对已知和未知受试者的梅尔频谱重建分别提升48%和35%。

Conclusion: DMF2Mel通过动态多尺度融合有效解决了脑信号解码中的长序列建模问题，显著提升了性能。

Abstract: Decoding speech from brain signals is a challenging research problem.
Although existing technologies have made progress in reconstructing the mel
spectrograms of auditory stimuli at the word or letter level, there remain core
challenges in the precise reconstruction of minute-level continuous imagined
speech: traditional models struggle to balance the efficiency of temporal
dependency modeling and information retention in long-sequence decoding. To
address this issue, this paper proposes the Dynamic Multiscale Fusion Network
(DMF2Mel), which consists of four core components: the Dynamic Contrastive
Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided
Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the
bidirectional state space module (convMamba). Specifically, the DC-FAM
separates speech-related "foreground features" from noisy "background features"
through local convolution and global attention mechanisms, effectively
suppressing interference and enhancing the representation of transient signals.
HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of
high-level semantics and low-level details. The SplineMap attention mechanism
integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine
global context modeling with spline-based local fitting. The convMamba captures
long-range temporal dependencies with linear complexity and enhances nonlinear
dynamic modeling capabilities. Results on the SparrKULee dataset show that
DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram
reconstruction for known subjects (a 48% improvement over the baseline) and
0.048 for unknown subjects (a 35% improvement over the baseline).Code is
available at: https://github.com/fchest/DMF2Mel.

</details>


### [15] [Assessing the Alignment of Audio Representations with Timbre Similarity Ratings](https://arxiv.org/abs/2507.07764)
*Haokun Tian,Stefan Lattner,Charalampos Saitis*

Main category: cs.SD

TL;DR: 论文提出了一种评估音频表示与人类音色相似性判断对齐的指标，发现基于CLAP模型和声音匹配模型的风格嵌入表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统音色空间方法存在可扩展性和泛化能力问题，而深度学习在音频质量评估和图像相似性中表现优异，但缺乏足够的人类评分数据训练深度模型。

Method: 引入指标比较音频嵌入距离与人类相似性评分的绝对值和排名，评估了三种信号处理表示、十二种预训练模型表示和三种新声音匹配模型表示。

Result: CLAP模型和声音匹配模型的风格嵌入表现最佳，显示出在音色相似性建模中的潜力。

Conclusion: 深度学习风格嵌入在音色相似性任务中优于传统方法，为未来研究提供了方向。

Abstract: Psychoacoustical so-called "timbre spaces" map perceptual similarity ratings
of instrument sounds onto low-dimensional embeddings via multidimensional
scaling, but suffer from scalability issues and are incapable of
generalization. Recent results from audio (music and speech) quality assessment
as well as image similarity have shown that deep learning is able to produce
embeddings that align well with human perception while being largely free from
these constraints. Although the existing human-rated timbre similarity data is
not large enough to train deep neural networks (2,614 pairwise ratings on 334
audio samples), it can serve as test-only data for audio models. In this paper,
we introduce metrics to assess the alignment of diverse audio representations
with human judgments of timbre similarity by comparing both the absolute values
and the rankings of embedding distances to human similarity ratings. Our
evaluation involves three signal-processing-based representations, twelve
representations extracted from pre-trained models, and three representations
extracted from a novel sound matching model. Among them, the style embeddings
inspired by image style transfer, extracted from the CLAP model and the sound
matching model, remarkably outperform the others, showing their potential in
modeling timbre similarity.

</details>


### [16] [SecureSpeech: Prompt-based Speaker and Content Protection](https://arxiv.org/abs/2507.07799)
*Belinda Soh Hui Hui,Xiaoxiao Miao,Xin Wang*

Main category: cs.SD

TL;DR: 提出了一种基于提示的语音生成管道，实现说话者身份和内容的双重匿名化，通过生成不可链接的说话者身份和替换敏感内容，同时保持音频质量和内容保留。


<details>
  <summary>Details</summary>
Motivation: 解决语音领域中身份盗窃和说话者重新识别的隐私问题。

Method: 1) 生成与源说话者不可链接的身份；2) 使用命名实体识别和大语言模型替换敏感内容；3) 通过文本到语音合成生成隐私友好的语音。

Result: 实验结果显示显著的隐私保护效果，同时保持了良好的内容保留和音频质量。

Conclusion: 该管道有效平衡了隐私保护和语音质量，并探讨了说话者描述对生成语音的潜在偏见影响。

Abstract: Given the increasing privacy concerns from identity theft and the
re-identification of speakers through content in the speech field, this paper
proposes a prompt-based speech generation pipeline that ensures dual
anonymization of both speaker identity and spoken content. This is addressed
through 1) generating a speaker identity unlinkable to the source speaker,
controlled by descriptors, and 2) replacing sensitive content within the
original text using a name entity recognition model and a large language model.
The pipeline utilizes the anonymized speaker identity and text to generate
high-fidelity, privacy-friendly speech via a text-to-speech synthesis model.
Experimental results demonstrate an achievement of significant privacy
protection while maintaining a decent level of content retention and audio
quality. This paper also investigates the impact of varying speaker
descriptions on the utility and privacy of generated speech to determine
potential biases.

</details>


### [17] [End-to-end Acoustic-linguistic Emotion and Intent Recognition Enhanced by Semi-supervised Learning](https://arxiv.org/abs/2507.07806)
*Zhao Ren,Rathi Adarshi Rammohan,Kevin Scheck,Sheng Li,Tanja Schultz*

Main category: cs.SD

TL;DR: 论文提出了一种半监督学习方法，结合少量标注数据和大量未标注数据，用于语音情感和意图识别，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于手动标注语音数据成本高昂，难以训练机器学习模型，因此需要利用半监督学习解决这一问题。

Method: 采用端到端的声学和语言模型，结合多任务学习，比较了两种半监督学习方法（fix-match和full-match）。

Result: 实验结果表明，半监督学习方法显著提升了识别性能，最佳模型的后期融合在联合识别平衡指标上分别优于声学和文本基线12.3%和10.4%。

Conclusion: 半监督学习在语音情感和意图识别中具有显著优势，能够有效利用未标注数据提升模型性能。

Abstract: Emotion and intent recognition from speech is essential and has been widely
investigated in human-computer interaction. The rapid development of social
media platforms, chatbots, and other technologies has led to a large volume of
speech data streaming from users. Nevertheless, annotating such data manually
is expensive, making it challenging to train machine learning models for
recognition purposes. To this end, we propose applying semi-supervised learning
to incorporate a large scale of unlabelled data alongside a relatively smaller
set of labelled data. We train end-to-end acoustic and linguistic models, each
employing multi-task learning for emotion and intent recognition. Two
semi-supervised learning approaches, including fix-match learning and
full-match learning, are compared. The experimental results demonstrate that
the semi-supervised learning approaches improve model performance in speech
emotion and intent recognition from both acoustic and text data. The late
fusion of the best models outperforms the acoustic and text baselines by joint
recognition balance metrics of 12.3% and 10.4%, respectively.

</details>


### [18] [Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders](https://arxiv.org/abs/2507.07867)
*Dimitrios Bralios,Jonah Casebeer,Paris Smaragdis*

Main category: cs.SD

TL;DR: 提出了一种名为“Re-Bottleneck”的后处理框架，通过修改预训练自编码器的瓶颈层，为潜在空间注入用户定义的结构，以优化下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器和自编码器通常仅关注重建保真度，忽略了潜在空间结构对多样化下游任务的重要性。

Method: 通过引入“Re-Bottleneck”，在预训练模型的瓶颈层上训练潜在空间损失，以注入特定结构。

Result: 实验表明，该方法能在不牺牲重建质量的情况下优化潜在通道排序、对齐语义嵌入并引入等变性。

Conclusion: Re-Bottleneck框架提供了一种灵活高效的方式，使神经音频模型能适应多样化应用需求。

Abstract: Neural audio codecs and autoencoders have emerged as versatile models for
audio compression, transmission, feature-extraction, and latent-space
generation. However, a key limitation is that most are trained to maximize
reconstruction fidelity, often neglecting the specific latent structure
necessary for optimal performance in diverse downstream applications. We
propose a simple, post-hoc framework to address this by modifying the
bottleneck of a pre-trained autoencoder. Our method introduces a
"Re-Bottleneck", an inner bottleneck trained exclusively through latent space
losses to instill user-defined structure. We demonstrate the framework's
effectiveness in three experiments. First, we enforce an ordering on latent
channels without sacrificing reconstruction quality. Second, we align latents
with semantic embeddings, analyzing the impact on downstream diffusion
modeling. Third, we introduce equivariance, ensuring that a filtering operation
on the input waveform directly corresponds to a specific transformation in the
latent space. Ultimately, our Re-Bottleneck framework offers a flexible and
efficient way to tailor representations of neural audio models, enabling them
to seamlessly meet the varied demands of different applications with minimal
additional training.

</details>


### [19] [Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition Models](https://arxiv.org/abs/2507.07877)
*Chen Feng,Yicheng Lin,Shaojie Zhuo,Chenzheng Su,Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Xiaopeng Zhang*

Main category: cs.SD

TL;DR: 论文研究了在资源受限的边缘设备上部署自动语音识别（ASR）模型的挑战，通过后训练量化（PTQ）方法减少模型大小和推理成本，并全面评估了八种先进PTQ方法在Whisper和Moonshine模型上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR模型在多样音频应用中表现出色，但在边缘设备上部署仍面临内存、计算和功耗限制的挑战，量化是解决这些问题的有效方法，但其性能影响尚不明确。

Method: 研究扩展了LLM压缩工具包，集成了边缘ASR模型、多种先进量化算法、统一校准和评估数据流程，以及详细分析工具，对八种PTQ方法和两种ASR模型家族进行了系统评估。

Result: 研究结果表明，即使3位量化在高容量模型上也能成功，通过先进PTQ技术实现了效率与准确性的平衡。

Conclusion: 这些发现为在低功耗、常开边缘设备上优化ASR模型提供了有价值的见解。

Abstract: Recent advances in Automatic Speech Recognition (ASR) have demonstrated
remarkable accuracy and robustness in diverse audio applications, such as live
transcription and voice command processing. However, deploying these models on
resource constrained edge devices (e.g., IoT device, wearables) still presents
substantial challenges due to strict limits on memory, compute and power.
Quantization, particularly Post-Training Quantization (PTQ), offers an
effective way to reduce model size and inference cost without retraining.
Despite its importance, the performance implications of various advanced
quantization methods and bit-width configurations on ASR models remain unclear.
In this work, we present a comprehensive benchmark of eight state-of-the-art
(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and
Moonshine. We systematically evaluate model performances (i.e., accuracy,
memory I/O and bit operations) across seven diverse datasets from the open ASR
leaderboard, analyzing the impact of quantization and various configurations on
both weights and activations. Built on an extension of the LLM compression
toolkit, our framework integrates edge-ASR models, diverse advanced
quantization algorithms, a unified calibration and evaluation data pipeline,
and detailed analysis tools. Our results characterize the trade-offs between
efficiency and accuracy, demonstrating that even 3-bit quantization can succeed
on high capacity models when using advanced PTQ techniques. These findings
provide valuable insights for optimizing ASR models on low-power, always-on
edge devices.

</details>


### [20] [LISTEN: Lightweight Industrial Sound-representable Transformer for Edge Notification](https://arxiv.org/abs/2507.07879)
*Changheon Han,Yun Seok Kang,Yuseop Sim,Martin Byung-Guk Jun,Hyung Wook Park*

Main category: cs.SD

TL;DR: LISTEN是一个轻量级工业声音基础模型，通过知识蒸馏实现低成本边缘设备上的实时运行，性能接近大型模型，适用于工业场景。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在工业声音分析中依赖大数据集和高计算资源的问题，推动边缘设备上的实时应用。

Method: 采用知识蒸馏技术，开发轻量级模型LISTEN，并在边缘设备上集成IIoT系统验证。

Result: LISTEN在基准任务中表现接近大型模型，且能在小数据集和低资源下微调。

Conclusion: LISTEN为工业声音分析提供了一种高效、实用的边缘解决方案。

Abstract: Deep learning-based machine listening is broadening the scope of industrial
acoustic analysis for applications like anomaly detection and predictive
maintenance, thereby improving manufacturing efficiency and reliability.
Nevertheless, its reliance on large, task-specific annotated datasets for every
new task limits widespread implementation on shop floors. While emerging sound
foundation models aim to alleviate data dependency, they are too large and
computationally expensive, requiring cloud infrastructure or high-end hardware
that is impractical for on-site, real-time deployment. We address this gap with
LISTEN (Lightweight Industrial Sound-representable Transformer for Edge
Notification), a kilobyte-sized industrial sound foundation model. Using
knowledge distillation, LISTEN runs in real-time on low-cost edge devices. On
benchmark downstream tasks, it performs nearly identically to its much larger
parent model, even when fine-tuned with minimal datasets and training resource.
Beyond the model itself, we demonstrate its real-world utility by integrating
LISTEN into a complete machine monitoring framework on an edge device with an
Industrial Internet of Things (IIoT) sensor and system, validating its
performance and generalization capabilities on a live manufacturing shop floor.

</details>


### [21] [Input Conditioned Layer Dropping in Speech Foundation Models](https://arxiv.org/abs/2507.07954)
*Abdul Hannan,Daniele Falavigna,Alessio Brutti*

Main category: cs.SD

TL;DR: 论文提出了一种输入驱动的层丢弃方法，用于动态调整语音模型的推理计算负载，优于随机丢弃和早期退出方法。


<details>
  <summary>Details</summary>
Motivation: 在边缘和物联网环境中，计算资源随时间变化，需要动态架构来适应。现有层丢弃方法在层选择或架构修改上存在局限性。

Method: 提出输入驱动的层丢弃（LD），利用输入特征和轻量级层选择网络确定最佳处理层组合。

Result: 在4个语音和音频基准测试中，使用两种预训练基础模型，表现优于随机丢弃，与早期退出方法相当或更好。

Conclusion: 输入驱动的LD方法有效，适用于动态调整语音模型的计算负载。

Abstract: Curating foundation speech models for edge and IoT settings, where
computational resources vary over time, requires dynamic architectures
featuring adaptable reduction strategies. One emerging approach is layer
dropping ($\mathcal{LD}$) which skips fraction of the layers of a backbone
network during inference to reduce the computational load. This allows
transforming static models into dynamic ones. However, existing approaches
exhibit limitations either in the mode of selecting layers or by significantly
modifying the neural architecture. To this end, we propose input-driven
$\mathcal{LD}$ that employs the network's input features and a lightweight
layer selecting network to determine the optimum combination of processing
layers. Extensive experimentation on 4 speech and audio public benchmarks,
using two different pre-trained foundation models, demonstrates the
effectiveness of our approach, thoroughly outperforming random dropping and
producing on-par (or better) results to early exit.

</details>

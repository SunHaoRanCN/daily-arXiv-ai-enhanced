{"id": "2507.08145", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08145", "abs": "https://arxiv.org/abs/2507.08145", "authors": ["A. A. Nutfaji", "Moustafa Hassan Elmallah"], "title": "AI-Augmented Visible Light Communication: A Framework for Noise Mitigation and Secure Data Transmission", "comment": "currently 4 pages. However, we're planning to work more on the topic", "summary": "This paper presents a proposed AI Deep Learning model that addresses common\nchallenges encountered in Visible Light Communication (VLC) systems. In this\nwork, we run a Python simulation that models a basic VLC system primarily\naffected by Additive White Gaussian Noise (AWGN). A Deep Neural Network (DNN)\nis then trained to equalize the noisy signal received and improve signal\nintegrity. The system evaluates and compares the Bit Error Rate (BER) before\nand after equalization to demonstrate the effectiveness of the proposed model.\nThis paper starts by introducing the concept of visible light communication,\nthen it dives deep into some details about the process of VLC and the\nchallenges it faces, shortly after we propose our project which helps overcome\nthese challenges. We finally conclude with a lead for future work, highlighting\nthe areas that are most suitable for future improvements."}
{"id": "2507.08293", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08293", "abs": "https://arxiv.org/abs/2507.08293", "authors": ["Haoran Yin", "Yanqun Tang", "Yuanhan Ni", "Zulin Wang", "Gaojie Chen", "Jun Xiong", "Kai Yang", "Marios Kountouris", "Yong Liang Guan", "Yong Zeng"], "title": "Ambiguity Function Analysis of AFDM Signals for Integrated Sensing and Communications", "comment": "14 pages, 14 figures. Under revision in an IEEE Journal", "summary": "Affine frequency division multiplexing (AFDM) is a promising chirp-based\nwaveform with high flexibility and resilience, making it well-suited for\nnext-generation wireless networks, particularly in high-mobility scenarios. In\nthis paper, we investigate the ambiguity functions (AFs) of AFDM signals, which\nfundamentally characterize their range and velocity estimation capabilities in\nboth monostatic and bistatic settings. Specifically, we first derive the\nauto-ambiguity function (AAF) of an AFDM chirp subcarrier, revealing its\n\"spike-like\" local property and \"periodic-like\" global property along the\nrotated delay and Doppler dimensions. This structure naturally forms a\nparallelogram for each localized pulse of the AAF of the AFDM chirp subcarrier,\nenabling unambiguous target sensing. Then, we study the cross-ambiguity\nfunction (CAF) between two different AFDM chirp subcarriers, which exhibits the\nsame local and global properties as the AAF but with an additional shift along\nthe Doppler dimension. We then extend our analysis to the AF of various typical\nAFDM frames, considering both deterministic pilot and random data symbols. In\nparticular, we demonstrate that inserting guard symbols in AFDM facilitates\ninterference-free sensing. Simulation results validate our theoretical\nfindings, highlighting AFDM's strong potential for ISAC applications."}
{"id": "2507.08399", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08399", "abs": "https://arxiv.org/abs/2507.08399", "authors": ["Karen Adam", "Clémentine Aguet", "Patrick Theurillat", "Florent Baty", "Maximilian Boesch", "Damien Ferrario", "Mathieu Lemay", "Martin Brutsche", "Fabian Braun"], "title": "Unobtrusive Reflectance Photoplethysmography for Detecting and Severity Grading of Sleep Apnea via Oxygen Desaturation Index", "comment": "Accepted to BMT2025", "summary": "Sleep apnea is a common chronic sleep-related disorder which is known to be a\ncomorbidity for cerebro- and cardio-vascular disease. Diagnosis of sleep apnea\nusually requires an overnight polysomnography at the sleep laboratory. In this\npaper, we used a wearable device which measures reflectance\nphotoplethysmography (PPG) at the wrist and upper arm to estimate continuous\nSpO2 levels during sleep and subsequently derive an oxygen desaturation index\n(ODI) for each patient. On a cohort of 170 patients undergoing sleep apnea\nscreening, we evaluated whether this ODI value could represent a surrogate\nmarker for the apnea-hypopnea index (AHI) for the diagnosis and severity\nassessment of sleep apnea. As the ODI was simultaneously obtained at the\nfingertip, upper arm and wrist, we compared ODI diagnostic performance\ndepending on the measurement location. We then further evaluated the accuracy\nof ODI as a direct predictor for moderate and severe sleep apnea as defined by\nestablished AHI thresholds. We found that ODI values obtained at the upper arm\nwere good predictors for moderate or severe sleep apnea, with 86% accuracy, 96%\nsensitivity and 70% specificity, whereas ODI values obtained at the wrist were\nless reliable as a diagnostic tool."}
{"id": "2507.08423", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08423", "abs": "https://arxiv.org/abs/2507.08423", "authors": ["Massimo Rosamilia", "Augusto Aubry", "Alessio Balleri", "Antonio De Maio", "Marco Martorella"], "title": "Exploiting Cognition in ISAR Processing for Spectral Compatibility Applications", "comment": "submitted to IEEE Transactions on Radar Systems", "summary": "This paper introduces and analyzes the concept of a cognitive inverse\nsynthetic aperture radar (ISAR) ensuring spectral compatibility in crowded\nelectromagnetic environments. In such a context, the proposed approach\nalternates between environmental perception, recognizing possible emitters in\nits frequency range, and an action stage, synthesizing and transmitting a\ntailored radar waveform to achieve the desired imaging task while guaranteeing\nspectral coexistence with overlaid emitters. The perception is carried out by a\nspectrum sensing module providing the true relevant spectral parameters of the\nsources in the environment. The action stage employs a tailored signal design\nprocess, synthesizing a radar waveform with bespoke spectral notches, enabling\nISAR imaging over a wide spectral bandwidth without interfering with the other\nradio frequency (RF) sources. A key enabling requirement for the proposed\napplication is the capability to successfully recover possible missing data in\nthe frequency domain (induced by spectral notches) and in the slow-time\ndimension (enabling concurrent RF activities still in a cognitive fashion).\nThis process is carried out by resorting to advanced methods based on either\nthe compressed-sensing framework or a rank-minimization recovery strategy. The\ncapabilities of the proposed system are assessed exploiting a dataset of drone\nmeasurements in the frequency band between 13 GHz and 15 GHz. Results highlight\nthe effectiveness of the devised architecture to enable spectral compatibility\nwhile delivering high-quality ISAR images as well as additional RF activities."}
{"id": "2507.08051", "categories": ["cs.SD", "eess.AS", "eess.SP", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2507.08051", "abs": "https://arxiv.org/abs/2507.08051", "authors": ["Louis Lalay", "Mathieu Fontaine", "Roland Badeau"], "title": "Modèle physique variationnel pour l'estimation de réponses impulsionnelles de salles", "comment": "in French language. GRETSI, Aug 2025, Strasbourg (67000), France", "summary": "Room impulse response estimation is essential for tasks like speech\ndereverberation, which improves automatic speech recognition. Most existing\nmethods rely on either statistical signal processing or deep neural networks\ndesigned to replicate signal processing principles. However, combining\nstatistical and physical modeling for RIR estimation remains largely\nunexplored. This paper proposes a novel approach integrating both aspects\nthrough a theoretically grounded model. The RIR is decomposed into\ninterpretable parameters: white Gaussian noise filtered by a\nfrequency-dependent exponential decay (e.g. modeling wall absorption) and an\nautoregressive filter (e.g. modeling microphone response). A variational\nfree-energy cost function enables practical parameter estimation. As a proof of\nconcept, we show that given dry and reverberant speech signals, the proposed\nmethod outperforms classical deconvolution in noisy environments, as validated\nby objective metrics."}
{"id": "2507.08135", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.08135", "abs": "https://arxiv.org/abs/2507.08135", "authors": ["Chunxi Wang", "Maoshen Jia", "Wenyu Jin"], "title": "DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse Response Estimation", "comment": "14 pages, 9 figures, submitted to IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing", "summary": "Room Impulse Responses (RIRs) accurately characterize acoustic properties of\nindoor environments and play a crucial role in applications such as speech\nenhancement, speech recognition, and audio rendering in augmented reality (AR)\nand virtual reality (VR). Existing blind estimation methods struggle to achieve\npractical accuracy. To overcome this challenge, we propose the dynamic\naudio-room acoustic synthesis (DARAS) model, a novel deep learning framework\nthat is explicitly designed for blind RIR estimation from monaural reverberant\nspeech signals. First, a dedicated deep audio encoder effectively extracts\nrelevant nonlinear latent space features. Second, the Mamba-based\nself-supervised blind room parameter estimation (MASS-BRPE) module, utilizing\nthe efficient Mamba state space model (SSM), accurately estimates key room\nacoustic parameters and features. Third, the system incorporates a hybrid-path\ncross-attention feature fusion module, enhancing deep integration between audio\nand room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT)\ndecoder adaptively segments early reflections and late reverberation to improve\nthe realism of synthesized RIRs. Experimental results, including a MUSHRA-based\nsubjective listening study, demonstrate that DARAS substantially outperforms\nexisting baseline models, providing a robust and effective solution for\npractical blind RIR estimation in real-world acoustic environments."}
{"id": "2507.08470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08470", "abs": "https://arxiv.org/abs/2507.08470", "authors": ["Benedikt Geiger", "Fred Buchali", "Vahid Aref", "Laurent Schmalen"], "title": "A Temporal Gaussian Noise Model for Equalization-enhanced Phase Noise", "comment": "Accepted at 51st European Conference on Optical Communication (ECOC),\n  Copenhagen, Denmark", "summary": "Equalization-enhanced Phase Noise causes burst-like distortions in high\nsymbol-rate transmission systems. We propose a temporal Gaussian noise model\nthat captures these distortions by introducing a time-varying distortion power.\nValidated through simulations and experiments, it enables accurate and simple\nperformance prediction for high symbol-rate transmission systems."}
{"id": "2507.08128", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08128", "abs": "https://arxiv.org/abs/2507.08128", "authors": ["Arushi Goel", "Sreyan Ghosh", "Jaehyeon Kim", "Sonal Kumar", "Zhifeng Kong", "Sang-gil Lee", "Chao-Han Huck Yang", "Ramani Duraiswami", "Dinesh Manocha", "Rafael Valle", "Bryan Catanzaro"], "title": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models", "comment": "Code, Datasets and Models: https://research.nvidia.com/labs/adlr/AF3/", "summary": "We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large\naudio-language model that advances reasoning and understanding across speech,\nsound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder\ntrained using a novel strategy for joint representation learning across all 3\nmodalities of speech, sound, and music; (ii) flexible, on-demand thinking,\nallowing the model to do chain-of-thought-type reasoning before answering;\n(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning\n(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To\nenable these capabilities, we propose several large-scale training datasets\ncurated using novel strategies, including AudioSkills-XL, LongAudio-XL,\nAF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based\ntraining strategy. Trained on only open-source audio data, AF3 achieves new\nSOTA results on over 20+ (long) audio understanding and reasoning benchmarks,\nsurpassing both open-weight and closed-source models trained on much larger\ndatasets."}
{"id": "2507.08227", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.08227", "abs": "https://arxiv.org/abs/2507.08227", "authors": ["Yang Xiao", "Ting Dang", "Rohan Kumar Das"], "title": "RawTFNet: A Lightweight CNN Architecture for Speech Anti-spoofing", "comment": "Submitted to APSIPA ASC 2025", "summary": "Automatic speaker verification (ASV) systems are often affected by spoofing\nattacks. Recent transformer-based models have improved anti-spoofing\nperformance by learning strong feature representations. However, these models\nusually need high computing power. To address this, we introduce RawTFNet, a\nlightweight CNN model designed for audio signals. The RawTFNet separates\nfeature processing along time and frequency dimensions, which helps to capture\nthe fine-grained details of synthetic speech. We tested RawTFNet on the\nASVspoof 2021 LA and DF evaluation datasets. The results show that RawTFNet\nreaches comparable performance to that of the state-of-the-art models, while\nalso using fewer computing resources. The code and models will be made publicly\navailable."}
{"id": "2507.08653", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08653", "abs": "https://arxiv.org/abs/2507.08653", "authors": ["Berire Gunes Reyhan", "Sinem Coleri"], "title": "Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees", "comment": "15 Pages, to be published in IEEE Transactions on Communications", "summary": "In Wireless Networked Control Systems (WNCSs), control and communication\nsystems must be co-designed due to their strong interdependence. This paper\npresents a novel optimization theory-based safe deep reinforcement learning\n(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction\nwhile optimizing performance, for the first time in the literature. The\napproach minimizes power consumption under key constraints, including Peak Age\nof Information (PAoI) violation probability, transmit power, and schedulability\nin the finite blocklength regime. PAoI violation probability is uniquely\nderived by combining stochastic maximum allowable transfer interval (MATI) and\nmaximum allowable packet delay (MAD) constraints in a multi-sensor network. The\nframework consists of two stages: optimization theory and safe DRL. The first\nstage derives optimality conditions to establish mathematical relationships\namong variables, simplifying and decomposing the problem. The second stage\nemploys a safe DRL model where a teacher-student framework guides the DRL agent\n(student). The control mechanism (teacher) evaluates compliance with system\nconstraints and suggests the nearest feasible action when needed. Extensive\nsimulations show that the proposed framework outperforms rule-based and other\noptimization theory based DRL benchmarks, achieving faster convergence, higher\nrewards, and greater stability."}
{"id": "2507.08236", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08236", "abs": "https://arxiv.org/abs/2507.08236", "authors": ["Anthony Miyaguchi", "Murilo Gustineli", "Adrian Cheung"], "title": "Distilling Spectrograms into Tokens: Fast and Lightweight Bioacoustic Classification for BirdCLEF+ 2025", "comment": "Working note submitted to CLEF 2025 under the LifeCLEF lab", "summary": "The BirdCLEF+ 2025 challenge requires classifying 206 species, including\nbirds, mammals, insects, and amphibians, from soundscape recordings under a\nstrict 90-minute CPU-only inference deadline, making many state-of-the-art deep\nlearning approaches impractical. To address this constraint, the DS@GT BirdCLEF\nteam explored two strategies. First, we establish competitive baselines by\noptimizing pre-trained models from the Bioacoustics Model Zoo for CPU\ninference. Using TFLite, we achieved a nearly 10x inference speedup for the\nPerch model, enabling it to run in approximately 16 minutes and achieve a final\nROC-AUC score of 0.729 on the public leaderboard post-competition and 0.711 on\nthe private leaderboard. The best model from the zoo was BirdSetEfficientNetB1,\nwith a public score of 0.810 and a private score of 0.778. Second, we introduce\na novel, lightweight pipeline named Spectrogram Token Skip-Gram (STSG) that\ntreats bioacoustics as a sequence modeling task. This method converts audio\ninto discrete \"spectrogram tokens\" by clustering Mel-spectrograms using Faiss\nK-means and then learns high-quality contextual embeddings for these tokens in\nan unsupervised manner with a Word2Vec skip-gram model. For classification,\nembeddings within a 5-second window are averaged and passed to a linear model.\nWith a projected inference time of 6 minutes for a 700-minute test set, the\nSTSG approach achieved a final ROC-AUC public score of 0.559 and a private\nscore of 0.520, demonstrating the viability of fast tokenization approaches\nwith static embeddings for bioacoustic classification. Supporting code for this\npaper can be found at https://github.com/dsgt-arc/birdclef-2025."}
{"id": "2507.08051", "categories": ["cs.SD", "eess.AS", "eess.SP", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2507.08051", "abs": "https://arxiv.org/abs/2507.08051", "authors": ["Louis Lalay", "Mathieu Fontaine", "Roland Badeau"], "title": "Modèle physique variationnel pour l'estimation de réponses impulsionnelles de salles", "comment": "in French language. GRETSI, Aug 2025, Strasbourg (67000), France", "summary": "Room impulse response estimation is essential for tasks like speech\ndereverberation, which improves automatic speech recognition. Most existing\nmethods rely on either statistical signal processing or deep neural networks\ndesigned to replicate signal processing principles. However, combining\nstatistical and physical modeling for RIR estimation remains largely\nunexplored. This paper proposes a novel approach integrating both aspects\nthrough a theoretically grounded model. The RIR is decomposed into\ninterpretable parameters: white Gaussian noise filtered by a\nfrequency-dependent exponential decay (e.g. modeling wall absorption) and an\nautoregressive filter (e.g. modeling microphone response). A variational\nfree-energy cost function enables practical parameter estimation. As a proof of\nconcept, we show that given dry and reverberant speech signals, the proposed\nmethod outperforms classical deconvolution in noisy environments, as validated\nby objective metrics."}
{"id": "2507.08670", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.08670", "abs": "https://arxiv.org/abs/2507.08670", "authors": ["Xiaojing Yan", "Saeed Razavikia", "Carlo Fischione"], "title": "Multi-Symbol Digital AirComp via Modulation Design and Power Adaptation", "comment": null, "summary": "In this paper, we consider digital over-the-air computation (AirComp) and\nintroduce a new multi-symbol modulation framework called sequential modulation\nfor AirComp (SeMAC). Building upon ChannelComp, a general framework for\ndesigning modulation schemes to support arbitrary function computation over a\nmultiple access channel (MAC), SeMAC maps each input value to a sequence of\nmodulated symbols using distinct constellation diagrams across multiple time\nslots. This extension generalizes ChannelComp by enabling flexible modulation\ndesign across multiple transmissions, thereby enhancing reliability against\nchannel noise. We formulate the modulation design as a non-convex optimization\nproblem, apply matrix lifting to relax it into a semidefinite programming\n(SDP), and recover a feasible modulation solution by solving a low rank\napproximation. For scenarios where the modulation formats cannot be changed, we\nfurther develop a power adaptation scheme that adjusts amplitude and phase of\nthe modulated symbols while preserving the modulation structure. Numerical\nresults show that SeMAC can achieve a reliable computation by reducing the\ncomputation error up to 18 dB compared to other existing methods, particularly\nfor the product function."}
{"id": "2507.08319", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08319", "abs": "https://arxiv.org/abs/2507.08319", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "title": "Active Learning for Text-to-Speech Synthesis with Informative Sample Collection", "comment": null, "summary": "The construction of high-quality datasets is a cornerstone of modern\ntext-to-speech (TTS) systems. However, the increasing scale of available data\nposes significant challenges, including storage constraints. To address these\nissues, we propose a TTS corpus construction method based on active learning.\nUnlike traditional feed-forward and model-agnostic corpus construction\napproaches, our method iteratively alternates between data collection and model\ntraining, thereby focusing on acquiring data that is more informative for model\nimprovement. This approach enables the construction of a data-efficient corpus.\nExperimental results demonstrate that the corpus constructed using our method\nenables higher-quality speech synthesis than corpora of the same size."}
{"id": "2507.08128", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08128", "abs": "https://arxiv.org/abs/2507.08128", "authors": ["Arushi Goel", "Sreyan Ghosh", "Jaehyeon Kim", "Sonal Kumar", "Zhifeng Kong", "Sang-gil Lee", "Chao-Han Huck Yang", "Ramani Duraiswami", "Dinesh Manocha", "Rafael Valle", "Bryan Catanzaro"], "title": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models", "comment": "Code, Datasets and Models: https://research.nvidia.com/labs/adlr/AF3/", "summary": "We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large\naudio-language model that advances reasoning and understanding across speech,\nsound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder\ntrained using a novel strategy for joint representation learning across all 3\nmodalities of speech, sound, and music; (ii) flexible, on-demand thinking,\nallowing the model to do chain-of-thought-type reasoning before answering;\n(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning\n(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To\nenable these capabilities, we propose several large-scale training datasets\ncurated using novel strategies, including AudioSkills-XL, LongAudio-XL,\nAF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based\ntraining strategy. Trained on only open-source audio data, AF3 achieves new\nSOTA results on over 20+ (long) audio understanding and reasoning benchmarks,\nsurpassing both open-weight and closed-source models trained on much larger\ndatasets."}
{"id": "2507.08051", "categories": ["cs.SD", "eess.AS", "eess.SP", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2507.08051", "abs": "https://arxiv.org/abs/2507.08051", "authors": ["Louis Lalay", "Mathieu Fontaine", "Roland Badeau"], "title": "Modèle physique variationnel pour l'estimation de réponses impulsionnelles de salles", "comment": "in French language. GRETSI, Aug 2025, Strasbourg (67000), France", "summary": "Room impulse response estimation is essential for tasks like speech\ndereverberation, which improves automatic speech recognition. Most existing\nmethods rely on either statistical signal processing or deep neural networks\ndesigned to replicate signal processing principles. However, combining\nstatistical and physical modeling for RIR estimation remains largely\nunexplored. This paper proposes a novel approach integrating both aspects\nthrough a theoretically grounded model. The RIR is decomposed into\ninterpretable parameters: white Gaussian noise filtered by a\nfrequency-dependent exponential decay (e.g. modeling wall absorption) and an\nautoregressive filter (e.g. modeling microphone response). A variational\nfree-energy cost function enables practical parameter estimation. As a proof of\nconcept, we show that given dry and reverberant speech signals, the proposed\nmethod outperforms classical deconvolution in noisy environments, as validated\nby objective metrics."}
{"id": "2507.08333", "categories": ["cs.SD", "cs.AI", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08333", "abs": "https://arxiv.org/abs/2507.08333", "authors": ["Tali Dror", "Iftach Shoham", "Moshe Buchris", "Oren Gal", "Haim Permuter", "Gilad Katz", "Eliya Nachmani"], "title": "Audio Inpanting using Discrete Diffusion Model", "comment": null, "summary": "Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/"}
{"id": "2507.08236", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08236", "abs": "https://arxiv.org/abs/2507.08236", "authors": ["Anthony Miyaguchi", "Murilo Gustineli", "Adrian Cheung"], "title": "Distilling Spectrograms into Tokens: Fast and Lightweight Bioacoustic Classification for BirdCLEF+ 2025", "comment": "Working note submitted to CLEF 2025 under the LifeCLEF lab", "summary": "The BirdCLEF+ 2025 challenge requires classifying 206 species, including\nbirds, mammals, insects, and amphibians, from soundscape recordings under a\nstrict 90-minute CPU-only inference deadline, making many state-of-the-art deep\nlearning approaches impractical. To address this constraint, the DS@GT BirdCLEF\nteam explored two strategies. First, we establish competitive baselines by\noptimizing pre-trained models from the Bioacoustics Model Zoo for CPU\ninference. Using TFLite, we achieved a nearly 10x inference speedup for the\nPerch model, enabling it to run in approximately 16 minutes and achieve a final\nROC-AUC score of 0.729 on the public leaderboard post-competition and 0.711 on\nthe private leaderboard. The best model from the zoo was BirdSetEfficientNetB1,\nwith a public score of 0.810 and a private score of 0.778. Second, we introduce\na novel, lightweight pipeline named Spectrogram Token Skip-Gram (STSG) that\ntreats bioacoustics as a sequence modeling task. This method converts audio\ninto discrete \"spectrogram tokens\" by clustering Mel-spectrograms using Faiss\nK-means and then learns high-quality contextual embeddings for these tokens in\nan unsupervised manner with a Word2Vec skip-gram model. For classification,\nembeddings within a 5-second window are averaged and passed to a linear model.\nWith a projected inference time of 6 minutes for a 700-minute test set, the\nSTSG approach achieved a final ROC-AUC public score of 0.559 and a private\nscore of 0.520, demonstrating the viability of fast tokenization approaches\nwith static embeddings for bioacoustic classification. Supporting code for this\npaper can be found at https://github.com/dsgt-arc/birdclef-2025."}
{"id": "2507.08412", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08412", "abs": "https://arxiv.org/abs/2507.08412", "authors": ["Modan Tailleur", "Mathieu Lagrange", "Pierre Aumond", "Vincent Tourre"], "title": "Enforcing Speech Content Privacy in Environmental Sound Recordings using Segment-wise Waveform Reversal", "comment": null, "summary": "Environmental sound recordings often contain intelligible speech, raising\nprivacy concerns that limit analysis, sharing and reuse of data. In this paper,\nwe introduce a method that renders speech unintelligible while preserving both\nthe integrity of the acoustic scene, and the overall audio quality. Our\napproach involves reversing waveform segments to distort speech content. This\nprocess is enhanced through a voice activity detection and speech separation\npipeline, which allows for more precise targeting of speech.\n  In order to demonstrate the effectivness of the proposed approach, we\nconsider a three-part evaluation protocol that assesses: 1) speech\nintelligibility using Word Error Rate (WER), 2) sound sources detectability\nusing Sound source Classification Accuracy-Drop (SCAD) from a widely used\npre-trained model, and 3) audio quality using the Fr\\'echet Audio Distance\n(FAD), computed with our reference dataset that contains unaltered speech.\nExperiments on this simulated evaluation dataset, which consists of linear\nmixtures of speech and environmental sound scenes, show that our method\nachieves satisfactory speech intelligibility reduction (97.9% WER), minimal\ndegradation of the sound sources detectability (2.7% SCAD), and high perceptual\nquality (FAD of 1.40). An ablation study further highlights the contribution of\neach component of the pipeline. We also show that incorporating random splicing\nto our speech content privacy enforcement method can enhance the algorithm's\nrobustness to attempt to recover the clean speech, at a slight cost of audio\nquality."}
{"id": "2507.08319", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08319", "abs": "https://arxiv.org/abs/2507.08319", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "title": "Active Learning for Text-to-Speech Synthesis with Informative Sample Collection", "comment": null, "summary": "The construction of high-quality datasets is a cornerstone of modern\ntext-to-speech (TTS) systems. However, the increasing scale of available data\nposes significant challenges, including storage constraints. To address these\nissues, we propose a TTS corpus construction method based on active learning.\nUnlike traditional feed-forward and model-agnostic corpus construction\napproaches, our method iteratively alternates between data collection and model\ntraining, thereby focusing on acquiring data that is more informative for model\nimprovement. This approach enables the construction of a data-efficient corpus.\nExperimental results demonstrate that the corpus constructed using our method\nenables higher-quality speech synthesis than corpora of the same size."}
{"id": "2507.08530", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08530", "abs": "https://arxiv.org/abs/2507.08530", "authors": ["Jingjing Tang", "Xin Wang", "Zhe Zhang", "Junichi Yamagishi", "Geraint Wiggins", "George Fazekas"], "title": "MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling", "comment": "Accepted by ISMIR 2025", "summary": "Generating expressive audio performances from music scores requires models to\ncapture both instrument acoustics and human interpretation. Traditional music\nperformance synthesis pipelines follow a two-stage approach, first generating\nexpressive performance MIDI from a score, then synthesising the MIDI into\naudio. However, the synthesis models often struggle to generalise across\ndiverse MIDI sources, musical styles, and recording environments. To address\nthese challenges, we propose MIDI-VALLE, a neural codec language model adapted\nfrom the VALLE framework, which was originally designed for zero-shot\npersonalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio\nsynthesis, we improve the architecture to condition on a reference audio\nperformance and its corresponding MIDI. Unlike previous TTS-based systems that\nrely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens,\nfacilitating a more consistent and robust modelling of piano performances.\nFurthermore, the model's generalisation ability is enhanced by training on an\nextensive and diverse piano performance dataset. Evaluation results show that\nMIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving\nover 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the\nlistening test, MIDI-VALLE received 202 votes compared to 58 for the baseline,\ndemonstrating improved synthesis quality and generalisation across diverse\nperformance MIDI inputs."}
{"id": "2507.08333", "categories": ["cs.SD", "cs.AI", "cs.IT", "cs.LG", "eess.AS", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08333", "abs": "https://arxiv.org/abs/2507.08333", "authors": ["Tali Dror", "Iftach Shoham", "Moshe Buchris", "Oren Gal", "Haim Permuter", "Gilad Katz", "Eliya Nachmani"], "title": "Audio Inpanting using Discrete Diffusion Model", "comment": null, "summary": "Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/"}
{"id": "2507.08557", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08557", "abs": "https://arxiv.org/abs/2507.08557", "authors": ["Yuxuan Jiang", "Zehua Chen", "Zeqian Ju", "Chang Li", "Weibei Dou", "Jun Zhu"], "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation", "comment": "Accepted at ACM MM 2025", "summary": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/"}
{"id": "2507.08412", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08412", "abs": "https://arxiv.org/abs/2507.08412", "authors": ["Modan Tailleur", "Mathieu Lagrange", "Pierre Aumond", "Vincent Tourre"], "title": "Enforcing Speech Content Privacy in Environmental Sound Recordings using Segment-wise Waveform Reversal", "comment": null, "summary": "Environmental sound recordings often contain intelligible speech, raising\nprivacy concerns that limit analysis, sharing and reuse of data. In this paper,\nwe introduce a method that renders speech unintelligible while preserving both\nthe integrity of the acoustic scene, and the overall audio quality. Our\napproach involves reversing waveform segments to distort speech content. This\nprocess is enhanced through a voice activity detection and speech separation\npipeline, which allows for more precise targeting of speech.\n  In order to demonstrate the effectivness of the proposed approach, we\nconsider a three-part evaluation protocol that assesses: 1) speech\nintelligibility using Word Error Rate (WER), 2) sound sources detectability\nusing Sound source Classification Accuracy-Drop (SCAD) from a widely used\npre-trained model, and 3) audio quality using the Fr\\'echet Audio Distance\n(FAD), computed with our reference dataset that contains unaltered speech.\nExperiments on this simulated evaluation dataset, which consists of linear\nmixtures of speech and environmental sound scenes, show that our method\nachieves satisfactory speech intelligibility reduction (97.9% WER), minimal\ndegradation of the sound sources detectability (2.7% SCAD), and high perceptual\nquality (FAD of 1.40). An ablation study further highlights the contribution of\neach component of the pipeline. We also show that incorporating random splicing\nto our speech content privacy enforcement method can enhance the algorithm's\nrobustness to attempt to recover the clean speech, at a slight cost of audio\nquality."}
{"id": "2507.08626", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08626", "abs": "https://arxiv.org/abs/2507.08626", "authors": ["Davide Salvi", "Viola Negroni", "Sara Mandelli", "Paolo Bestagini", "Stefano Tubaro"], "title": "Phoneme-Level Analysis for Person-of-Interest Speech Deepfake Detection", "comment": "Accepted at ICCV Workshop - Authenticity & Provenance in the age of\n  Generative AI", "summary": "Recent advances in generative AI have made the creation of speech deepfakes\nwidely accessible, posing serious challenges to digital trust. To counter this,\nvarious speech deepfake detection strategies have been proposed, including\nPerson-of-Interest (POI) approaches, which focus on identifying impersonations\nof specific individuals by modeling and analyzing their unique vocal traits.\nDespite their excellent performance, the existing methods offer limited\ngranularity and lack interpretability. In this work, we propose a POI-based\nspeech deepfake detection method that operates at the phoneme level. Our\napproach decomposes reference audio into phonemes to construct a detailed\nspeaker profile. In inference, phonemes from a test sample are individually\ncompared against this profile, enabling fine-grained detection of synthetic\nartifacts. The proposed method achieves comparable accuracy to traditional\napproaches while offering superior robustness and interpretability, key aspects\nin multimedia forensics. By focusing on phoneme analysis, this work explores a\nnovel direction for explainable, speaker-centric deepfake detection."}
{"id": "2507.08530", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08530", "abs": "https://arxiv.org/abs/2507.08530", "authors": ["Jingjing Tang", "Xin Wang", "Zhe Zhang", "Junichi Yamagishi", "Geraint Wiggins", "George Fazekas"], "title": "MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling", "comment": "Accepted by ISMIR 2025", "summary": "Generating expressive audio performances from music scores requires models to\ncapture both instrument acoustics and human interpretation. Traditional music\nperformance synthesis pipelines follow a two-stage approach, first generating\nexpressive performance MIDI from a score, then synthesising the MIDI into\naudio. However, the synthesis models often struggle to generalise across\ndiverse MIDI sources, musical styles, and recording environments. To address\nthese challenges, we propose MIDI-VALLE, a neural codec language model adapted\nfrom the VALLE framework, which was originally designed for zero-shot\npersonalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio\nsynthesis, we improve the architecture to condition on a reference audio\nperformance and its corresponding MIDI. Unlike previous TTS-based systems that\nrely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens,\nfacilitating a more consistent and robust modelling of piano performances.\nFurthermore, the model's generalisation ability is enhanced by training on an\nextensive and diverse piano performance dataset. Evaluation results show that\nMIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving\nover 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the\nlistening test, MIDI-VALLE received 202 votes compared to 58 for the baseline,\ndemonstrating improved synthesis quality and generalisation across diverse\nperformance MIDI inputs."}
{"id": "2507.08768", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08768", "abs": "https://arxiv.org/abs/2507.08768", "authors": ["Peter Sullivan", "Muhammad Abdul-Mageed"], "title": "On Barriers to Archival Audio Processing", "comment": "Update with Acknowledgements of ICNSLP 2025 paper", "summary": "In this study, we leverage a unique UNESCO collection of mid-20th century\nradio recordings to probe the robustness of modern off-the-shelf language\nidentification (LID) and speaker recognition (SR) methods, especially with\nrespect to the impact of multilingual speakers and cross-age recordings. Our\nfindings suggest that LID systems, such as Whisper, are increasingly adept at\nhandling second-language and accented speech. However, speaker embeddings\nremain a fragile component of speech processing pipelines that is prone to\nbiases related to the channel, age, and language. Issues which will need to be\novercome should archives aim to employ SR methods for speaker indexing."}
{"id": "2507.08557", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08557", "abs": "https://arxiv.org/abs/2507.08557", "authors": ["Yuxuan Jiang", "Zehua Chen", "Zeqian Ju", "Chang Li", "Weibei Dou", "Jun Zhu"], "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation", "comment": "Accepted at ACM MM 2025", "summary": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/"}
{"id": "2507.08135", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.08135", "abs": "https://arxiv.org/abs/2507.08135", "authors": ["Chunxi Wang", "Maoshen Jia", "Wenyu Jin"], "title": "DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse Response Estimation", "comment": "14 pages, 9 figures, submitted to IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing", "summary": "Room Impulse Responses (RIRs) accurately characterize acoustic properties of\nindoor environments and play a crucial role in applications such as speech\nenhancement, speech recognition, and audio rendering in augmented reality (AR)\nand virtual reality (VR). Existing blind estimation methods struggle to achieve\npractical accuracy. To overcome this challenge, we propose the dynamic\naudio-room acoustic synthesis (DARAS) model, a novel deep learning framework\nthat is explicitly designed for blind RIR estimation from monaural reverberant\nspeech signals. First, a dedicated deep audio encoder effectively extracts\nrelevant nonlinear latent space features. Second, the Mamba-based\nself-supervised blind room parameter estimation (MASS-BRPE) module, utilizing\nthe efficient Mamba state space model (SSM), accurately estimates key room\nacoustic parameters and features. Third, the system incorporates a hybrid-path\ncross-attention feature fusion module, enhancing deep integration between audio\nand room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT)\ndecoder adaptively segments early reflections and late reverberation to improve\nthe realism of synthesized RIRs. Experimental results, including a MUSHRA-based\nsubjective listening study, demonstrate that DARAS substantially outperforms\nexisting baseline models, providing a robust and effective solution for\npractical blind RIR estimation in real-world acoustic environments."}
{"id": "2507.08626", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08626", "abs": "https://arxiv.org/abs/2507.08626", "authors": ["Davide Salvi", "Viola Negroni", "Sara Mandelli", "Paolo Bestagini", "Stefano Tubaro"], "title": "Phoneme-Level Analysis for Person-of-Interest Speech Deepfake Detection", "comment": "Accepted at ICCV Workshop - Authenticity & Provenance in the age of\n  Generative AI", "summary": "Recent advances in generative AI have made the creation of speech deepfakes\nwidely accessible, posing serious challenges to digital trust. To counter this,\nvarious speech deepfake detection strategies have been proposed, including\nPerson-of-Interest (POI) approaches, which focus on identifying impersonations\nof specific individuals by modeling and analyzing their unique vocal traits.\nDespite their excellent performance, the existing methods offer limited\ngranularity and lack interpretability. In this work, we propose a POI-based\nspeech deepfake detection method that operates at the phoneme level. Our\napproach decomposes reference audio into phonemes to construct a detailed\nspeaker profile. In inference, phonemes from a test sample are individually\ncompared against this profile, enabling fine-grained detection of synthetic\nartifacts. The proposed method achieves comparable accuracy to traditional\napproaches while offering superior robustness and interpretability, key aspects\nin multimedia forensics. By focusing on phoneme analysis, this work explores a\nnovel direction for explainable, speaker-centric deepfake detection."}
{"id": "2507.08227", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.08227", "abs": "https://arxiv.org/abs/2507.08227", "authors": ["Yang Xiao", "Ting Dang", "Rohan Kumar Das"], "title": "RawTFNet: A Lightweight CNN Architecture for Speech Anti-spoofing", "comment": "Submitted to APSIPA ASC 2025", "summary": "Automatic speaker verification (ASV) systems are often affected by spoofing\nattacks. Recent transformer-based models have improved anti-spoofing\nperformance by learning strong feature representations. However, these models\nusually need high computing power. To address this, we introduce RawTFNet, a\nlightweight CNN model designed for audio signals. The RawTFNet separates\nfeature processing along time and frequency dimensions, which helps to capture\nthe fine-grained details of synthetic speech. We tested RawTFNet on the\nASVspoof 2021 LA and DF evaluation datasets. The results show that RawTFNet\nreaches comparable performance to that of the state-of-the-art models, while\nalso using fewer computing resources. The code and models will be made publicly\navailable."}
{"id": "2507.08768", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08768", "abs": "https://arxiv.org/abs/2507.08768", "authors": ["Peter Sullivan", "Muhammad Abdul-Mageed"], "title": "On Barriers to Archival Audio Processing", "comment": "Update with Acknowledgements of ICNSLP 2025 paper", "summary": "In this study, we leverage a unique UNESCO collection of mid-20th century\nradio recordings to probe the robustness of modern off-the-shelf language\nidentification (LID) and speaker recognition (SR) methods, especially with\nrespect to the impact of multilingual speakers and cross-age recordings. Our\nfindings suggest that LID systems, such as Whisper, are increasingly adept at\nhandling second-language and accented speech. However, speaker embeddings\nremain a fragile component of speech processing pipelines that is prone to\nbiases related to the channel, age, and language. Issues which will need to be\novercome should archives aim to employ SR methods for speaker indexing."}

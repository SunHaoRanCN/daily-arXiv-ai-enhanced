<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 7]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.SD](#cs.SD) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [AI-Augmented Visible Light Communication: A Framework for Noise Mitigation and Secure Data Transmission](https://arxiv.org/abs/2507.08145)
*A. A. Nutfaji,Moustafa Hassan Elmallah*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的AI模型，用于解决可见光通信（VLC）系统中的常见问题，通过训练深度神经网络（DNN）来均衡噪声信号，提高信号完整性。


<details>
  <summary>Details</summary>
Motivation: 可见光通信系统常受到加性高斯白噪声（AWGN）的影响，导致信号质量下降，因此需要一种有效的方法来改善信号完整性。

Method: 使用Python模拟基础VLC系统，并训练DNN对噪声信号进行均衡，通过比较均衡前后的误码率（BER）评估模型效果。

Result: 实验结果表明，提出的DNN模型能有效降低误码率，提高信号完整性。

Conclusion: 该模型为VLC系统中的噪声问题提供了解决方案，并指出了未来改进的方向。

Abstract: This paper presents a proposed AI Deep Learning model that addresses common
challenges encountered in Visible Light Communication (VLC) systems. In this
work, we run a Python simulation that models a basic VLC system primarily
affected by Additive White Gaussian Noise (AWGN). A Deep Neural Network (DNN)
is then trained to equalize the noisy signal received and improve signal
integrity. The system evaluates and compares the Bit Error Rate (BER) before
and after equalization to demonstrate the effectiveness of the proposed model.
This paper starts by introducing the concept of visible light communication,
then it dives deep into some details about the process of VLC and the
challenges it faces, shortly after we propose our project which helps overcome
these challenges. We finally conclude with a lead for future work, highlighting
the areas that are most suitable for future improvements.

</details>


### [2] [Ambiguity Function Analysis of AFDM Signals for Integrated Sensing and Communications](https://arxiv.org/abs/2507.08293)
*Haoran Yin,Yanqun Tang,Yuanhan Ni,Zulin Wang,Gaojie Chen,Jun Xiong,Kai Yang,Marios Kountouris,Yong Liang Guan,Yong Zeng*

Main category: eess.SP

TL;DR: 本文研究了AFDM信号的模糊函数，揭示了其在单静态和双静态设置中的范围和速度估计能力，展示了AFDM在ISAC应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: AFDM是一种具有高灵活性和鲁棒性的波形，适合下一代无线网络，尤其是在高移动性场景中。研究其模糊函数有助于理解其感知能力。

Method: 通过推导AFDM啁啾子载波的自动模糊函数（AAF）和交叉模糊函数（CAF），分析其局部和全局特性，并扩展到典型AFDM帧的模糊函数分析。

Result: AAF具有“尖峰状”局部特性和“周期性”全局特性，CAF具有类似特性但多普勒维度有额外偏移。插入保护符号可实现无干扰感知。

Conclusion: AFDM在ISAC应用中表现出强大的潜力，仿真结果验证了理论分析。

Abstract: Affine frequency division multiplexing (AFDM) is a promising chirp-based
waveform with high flexibility and resilience, making it well-suited for
next-generation wireless networks, particularly in high-mobility scenarios. In
this paper, we investigate the ambiguity functions (AFs) of AFDM signals, which
fundamentally characterize their range and velocity estimation capabilities in
both monostatic and bistatic settings. Specifically, we first derive the
auto-ambiguity function (AAF) of an AFDM chirp subcarrier, revealing its
"spike-like" local property and "periodic-like" global property along the
rotated delay and Doppler dimensions. This structure naturally forms a
parallelogram for each localized pulse of the AAF of the AFDM chirp subcarrier,
enabling unambiguous target sensing. Then, we study the cross-ambiguity
function (CAF) between two different AFDM chirp subcarriers, which exhibits the
same local and global properties as the AAF but with an additional shift along
the Doppler dimension. We then extend our analysis to the AF of various typical
AFDM frames, considering both deterministic pilot and random data symbols. In
particular, we demonstrate that inserting guard symbols in AFDM facilitates
interference-free sensing. Simulation results validate our theoretical
findings, highlighting AFDM's strong potential for ISAC applications.

</details>


### [3] [Unobtrusive Reflectance Photoplethysmography for Detecting and Severity Grading of Sleep Apnea via Oxygen Desaturation Index](https://arxiv.org/abs/2507.08399)
*Karen Adam,Clémentine Aguet,Patrick Theurillat,Florent Baty,Maximilian Boesch,Damien Ferrario,Mathieu Lemay,Martin Brutsche,Fabian Braun*

Main category: eess.SP

TL;DR: 通过可穿戴设备测量PPG信号，评估不同部位（手腕、上臂）的ODI值对睡眠呼吸暂停的诊断性能，发现上臂ODI值更可靠。


<details>
  <summary>Details</summary>
Motivation: 睡眠呼吸暂停是常见慢性疾病，传统诊断需实验室多导睡眠监测，希望通过可穿戴设备简化诊断流程。

Method: 使用可穿戴设备测量手腕和上臂的PPG信号，计算ODI值，并与传统AHI指标对比。

Result: 上臂ODI值对中重度睡眠呼吸暂停的诊断准确率达86%，敏感性和特异性分别为96%和70%，手腕ODI值可靠性较低。

Conclusion: 上臂ODI值可作为睡眠呼吸暂停诊断的替代指标，手腕测量可靠性不足。

Abstract: Sleep apnea is a common chronic sleep-related disorder which is known to be a
comorbidity for cerebro- and cardio-vascular disease. Diagnosis of sleep apnea
usually requires an overnight polysomnography at the sleep laboratory. In this
paper, we used a wearable device which measures reflectance
photoplethysmography (PPG) at the wrist and upper arm to estimate continuous
SpO2 levels during sleep and subsequently derive an oxygen desaturation index
(ODI) for each patient. On a cohort of 170 patients undergoing sleep apnea
screening, we evaluated whether this ODI value could represent a surrogate
marker for the apnea-hypopnea index (AHI) for the diagnosis and severity
assessment of sleep apnea. As the ODI was simultaneously obtained at the
fingertip, upper arm and wrist, we compared ODI diagnostic performance
depending on the measurement location. We then further evaluated the accuracy
of ODI as a direct predictor for moderate and severe sleep apnea as defined by
established AHI thresholds. We found that ODI values obtained at the upper arm
were good predictors for moderate or severe sleep apnea, with 86% accuracy, 96%
sensitivity and 70% specificity, whereas ODI values obtained at the wrist were
less reliable as a diagnostic tool.

</details>


### [4] [Exploiting Cognition in ISAR Processing for Spectral Compatibility Applications](https://arxiv.org/abs/2507.08423)
*Massimo Rosamilia,Augusto Aubry,Alessio Balleri,Antonio De Maio,Marco Martorella*

Main category: eess.SP

TL;DR: 论文提出了一种认知逆合成孔径雷达（ISAR）方法，通过感知环境和动态设计雷达波形，实现在拥挤电磁环境中的频谱兼容性。


<details>
  <summary>Details</summary>
Motivation: 在拥挤的电磁环境中，传统雷达可能与其他射频源产生干扰。研究旨在通过认知方法实现频谱共存，同时完成高质量的ISAR成像任务。

Method: 方法包括环境感知（频谱感知模块）和动态波形设计（生成带频谱陷波的雷达波形），并利用压缩感知或秩最小化策略恢复缺失数据。

Result: 实验结果表明，该方法在13-15 GHz频段内有效实现了频谱兼容性，并生成了高质量的ISAR图像。

Conclusion: 该认知ISAR系统能够在复杂电磁环境中实现频谱共存，同时保持高质量的成像性能。

Abstract: This paper introduces and analyzes the concept of a cognitive inverse
synthetic aperture radar (ISAR) ensuring spectral compatibility in crowded
electromagnetic environments. In such a context, the proposed approach
alternates between environmental perception, recognizing possible emitters in
its frequency range, and an action stage, synthesizing and transmitting a
tailored radar waveform to achieve the desired imaging task while guaranteeing
spectral coexistence with overlaid emitters. The perception is carried out by a
spectrum sensing module providing the true relevant spectral parameters of the
sources in the environment. The action stage employs a tailored signal design
process, synthesizing a radar waveform with bespoke spectral notches, enabling
ISAR imaging over a wide spectral bandwidth without interfering with the other
radio frequency (RF) sources. A key enabling requirement for the proposed
application is the capability to successfully recover possible missing data in
the frequency domain (induced by spectral notches) and in the slow-time
dimension (enabling concurrent RF activities still in a cognitive fashion).
This process is carried out by resorting to advanced methods based on either
the compressed-sensing framework or a rank-minimization recovery strategy. The
capabilities of the proposed system are assessed exploiting a dataset of drone
measurements in the frequency band between 13 GHz and 15 GHz. Results highlight
the effectiveness of the devised architecture to enable spectral compatibility
while delivering high-quality ISAR images as well as additional RF activities.

</details>


### [5] [A Temporal Gaussian Noise Model for Equalization-enhanced Phase Noise](https://arxiv.org/abs/2507.08470)
*Benedikt Geiger,Fred Buchali,Vahid Aref,Laurent Schmalen*

Main category: eess.SP

TL;DR: 提出了一种时间高斯噪声模型，用于捕捉高符号率传输系统中的突发失真，并通过时变失真功率实现简单准确的性能预测。


<details>
  <summary>Details</summary>
Motivation: 高符号率传输系统中，均衡增强相位噪声会导致突发失真，需要一种模型来准确捕捉这些失真。

Method: 提出了一种时间高斯噪声模型，引入时变失真功率来描述这些失真。

Result: 通过仿真和实验验证，该模型能够准确且简单地预测高符号率传输系统的性能。

Conclusion: 该模型为高符号率传输系统的性能预测提供了一种有效且实用的方法。

Abstract: Equalization-enhanced Phase Noise causes burst-like distortions in high
symbol-rate transmission systems. We propose a temporal Gaussian noise model
that captures these distortions by introducing a time-varying distortion power.
Validated through simulations and experiments, it enables accurate and simple
performance prediction for high symbol-rate transmission systems.

</details>


### [6] [Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees](https://arxiv.org/abs/2507.08653)
*Berire Gunes Reyhan,Sinem Coleri*

Main category: eess.SP

TL;DR: 提出了一种基于优化理论的安全深度强化学习框架，用于超可靠无线网络控制系统，首次在文献中同时满足约束条件和性能优化。


<details>
  <summary>Details</summary>
Motivation: 无线网络控制系统中，控制和通信系统需协同设计，现有方法难以同时满足约束条件和性能优化。

Method: 采用两阶段方法：优化理论阶段推导最优条件，安全深度强化学习阶段通过师生框架指导代理。

Result: 仿真表明，该框架优于基于规则和其他优化理论的深度强化学习方法，收敛更快、奖励更高、稳定性更强。

Conclusion: 该框架为超可靠无线网络控制系统提供了一种有效的协同设计方法，首次实现了约束与性能的双重优化。

Abstract: In Wireless Networked Control Systems (WNCSs), control and communication
systems must be co-designed due to their strong interdependence. This paper
presents a novel optimization theory-based safe deep reinforcement learning
(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction
while optimizing performance, for the first time in the literature. The
approach minimizes power consumption under key constraints, including Peak Age
of Information (PAoI) violation probability, transmit power, and schedulability
in the finite blocklength regime. PAoI violation probability is uniquely
derived by combining stochastic maximum allowable transfer interval (MATI) and
maximum allowable packet delay (MAD) constraints in a multi-sensor network. The
framework consists of two stages: optimization theory and safe DRL. The first
stage derives optimality conditions to establish mathematical relationships
among variables, simplifying and decomposing the problem. The second stage
employs a safe DRL model where a teacher-student framework guides the DRL agent
(student). The control mechanism (teacher) evaluates compliance with system
constraints and suggests the nearest feasible action when needed. Extensive
simulations show that the proposed framework outperforms rule-based and other
optimization theory based DRL benchmarks, achieving faster convergence, higher
rewards, and greater stability.

</details>


### [7] [Multi-Symbol Digital AirComp via Modulation Design and Power Adaptation](https://arxiv.org/abs/2507.08670)
*Xiaojing Yan,Saeed Razavikia,Carlo Fischione*

Main category: eess.SP

TL;DR: 提出了一种名为SeMAC的多符号调制框架，用于数字空中计算（AirComp），通过多时隙的灵活调制设计提高抗噪声能力，并通过优化和功率调整方案显著降低计算误差。


<details>
  <summary>Details</summary>
Motivation: 现有ChannelComp框架在多接入信道（MAC）上支持任意函数计算，但缺乏跨多时隙的灵活调制设计，限制了抗噪声性能。

Method: 提出SeMAC框架，将输入值映射到多时隙的调制符号序列，通过非凸优化和半定规划（SDP）设计调制方案，并开发功率调整方案以适应固定调制格式。

Result: 数值结果表明，SeMAC可将计算误差降低高达18 dB，尤其在乘积函数计算中表现优异。

Conclusion: SeMAC通过多时隙调制设计和功率调整，显著提升了数字空中计算的可靠性和性能。

Abstract: In this paper, we consider digital over-the-air computation (AirComp) and
introduce a new multi-symbol modulation framework called sequential modulation
for AirComp (SeMAC). Building upon ChannelComp, a general framework for
designing modulation schemes to support arbitrary function computation over a
multiple access channel (MAC), SeMAC maps each input value to a sequence of
modulated symbols using distinct constellation diagrams across multiple time
slots. This extension generalizes ChannelComp by enabling flexible modulation
design across multiple transmissions, thereby enhancing reliability against
channel noise. We formulate the modulation design as a non-convex optimization
problem, apply matrix lifting to relax it into a semidefinite programming
(SDP), and recover a feasible modulation solution by solving a low rank
approximation. For scenarios where the modulation formats cannot be changed, we
further develop a power adaptation scheme that adjusts amplitude and phase of
the modulated symbols while preserving the modulation structure. Numerical
results show that SeMAC can achieve a reliable computation by reducing the
computation error up to 18 dB compared to other existing methods, particularly
for the product function.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [8] [DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse Response Estimation](https://arxiv.org/abs/2507.08135)
*Chunxi Wang,Maoshen Jia,Wenyu Jin*

Main category: eess.AS

TL;DR: 论文提出了一种名为DARAS的深度学习框架，用于从单声道混响语音信号中盲估计房间脉冲响应（RIR），显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有盲估计方法在实际应用中难以达到足够精度，因此需要一种更有效的解决方案。

Method: DARAS框架包括深度音频编码器、Mamba自监督盲房间参数估计模块、混合路径交叉注意力特征融合模块以及动态声学调谐解码器。

Result: 实验结果表明，DARAS在主观听感测试中显著优于现有基线模型。

Conclusion: DARAS为实际应用中的盲RIR估计提供了鲁棒且有效的解决方案。

Abstract: Room Impulse Responses (RIRs) accurately characterize acoustic properties of
indoor environments and play a crucial role in applications such as speech
enhancement, speech recognition, and audio rendering in augmented reality (AR)
and virtual reality (VR). Existing blind estimation methods struggle to achieve
practical accuracy. To overcome this challenge, we propose the dynamic
audio-room acoustic synthesis (DARAS) model, a novel deep learning framework
that is explicitly designed for blind RIR estimation from monaural reverberant
speech signals. First, a dedicated deep audio encoder effectively extracts
relevant nonlinear latent space features. Second, the Mamba-based
self-supervised blind room parameter estimation (MASS-BRPE) module, utilizing
the efficient Mamba state space model (SSM), accurately estimates key room
acoustic parameters and features. Third, the system incorporates a hybrid-path
cross-attention feature fusion module, enhancing deep integration between audio
and room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT)
decoder adaptively segments early reflections and late reverberation to improve
the realism of synthesized RIRs. Experimental results, including a MUSHRA-based
subjective listening study, demonstrate that DARAS substantially outperforms
existing baseline models, providing a robust and effective solution for
practical blind RIR estimation in real-world acoustic environments.

</details>


### [9] [RawTFNet: A Lightweight CNN Architecture for Speech Anti-spoofing](https://arxiv.org/abs/2507.08227)
*Yang Xiao,Ting Dang,Rohan Kumar Das*

Main category: eess.AS

TL;DR: 论文提出了一种轻量级CNN模型RawTFNet，用于提高自动说话人验证系统的抗欺骗攻击能力，同时减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的模型虽然性能优越，但计算成本高，因此需要一种更高效的解决方案。

Method: 设计了RawTFNet，一种轻量级CNN模型，通过在时间和频率维度上分离特征处理，捕捉合成语音的细节。

Result: 在ASVspoof 2021 LA和DF数据集上测试，RawTFNet性能与最先进模型相当，但计算资源更少。

Conclusion: RawTFNet是一种高效且轻量级的解决方案，适用于抗欺骗攻击任务。

Abstract: Automatic speaker verification (ASV) systems are often affected by spoofing
attacks. Recent transformer-based models have improved anti-spoofing
performance by learning strong feature representations. However, these models
usually need high computing power. To address this, we introduce RawTFNet, a
lightweight CNN model designed for audio signals. The RawTFNet separates
feature processing along time and frequency dimensions, which helps to capture
the fine-grained details of synthetic speech. We tested RawTFNet on the
ASVspoof 2021 LA and DF evaluation datasets. The results show that RawTFNet
reaches comparable performance to that of the state-of-the-art models, while
also using fewer computing resources. The code and models will be made publicly
available.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [10] [Modèle physique variationnel pour l'estimation de réponses impulsionnelles de salles](https://arxiv.org/abs/2507.08051)
*Louis Lalay,Mathieu Fontaine,Roland Badeau*

Main category: cs.SD

TL;DR: 提出了一种结合统计和物理模型的新方法，用于房间脉冲响应（RIR）估计，通过可解释的参数分解和变分自由能成本函数实现参数估计，在噪声环境中优于传统解卷积方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖统计信号处理或深度神经网络，而结合统计和物理模型的RIR估计方法尚未充分探索。

Method: 将RIR分解为可解释参数（高斯白噪声通过频率相关指数衰减和自回归滤波），并使用变分自由能成本函数进行参数估计。

Result: 在噪声环境中，该方法在干声和混响声信号上的表现优于传统解卷积方法，并通过客观指标验证。

Conclusion: 提出的方法为RIR估计提供了一种理论和实践上可行的新途径，尤其在噪声环境中表现优异。

Abstract: Room impulse response estimation is essential for tasks like speech
dereverberation, which improves automatic speech recognition. Most existing
methods rely on either statistical signal processing or deep neural networks
designed to replicate signal processing principles. However, combining
statistical and physical modeling for RIR estimation remains largely
unexplored. This paper proposes a novel approach integrating both aspects
through a theoretically grounded model. The RIR is decomposed into
interpretable parameters: white Gaussian noise filtered by a
frequency-dependent exponential decay (e.g. modeling wall absorption) and an
autoregressive filter (e.g. modeling microphone response). A variational
free-energy cost function enables practical parameter estimation. As a proof of
concept, we show that given dry and reverberant speech signals, the proposed
method outperforms classical deconvolution in noisy environments, as validated
by objective metrics.

</details>


### [11] [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
*Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: AF3是一个开源的先进音频-语言模型，支持语音、声音和音乐的多模态推理与理解，具备链式推理、多轮对话、长音频处理等功能，并在多个基准测试中取得最优成绩。


<details>
  <summary>Details</summary>
Motivation: 提升音频-语言模型在多模态（语音、声音、音乐）中的推理和理解能力，填补现有模型在长音频处理和交互功能上的不足。

Method: 引入AF-Whisper统一音频编码器，采用五阶段课程训练策略，结合多个大规模数据集（如AudioSkills-XL、LongAudio-XL等）。

Result: 在20多个长音频理解和推理基准测试中达到最优性能，超越开源和闭源模型。

Conclusion: AF3通过创新的训练策略和数据集设计，实现了多模态音频理解和推理的突破，为开源社区提供了强大的工具。

Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large
audio-language model that advances reasoning and understanding across speech,
sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder
trained using a novel strategy for joint representation learning across all 3
modalities of speech, sound, and music; (ii) flexible, on-demand thinking,
allowing the model to do chain-of-thought-type reasoning before answering;
(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning
(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To
enable these capabilities, we propose several large-scale training datasets
curated using novel strategies, including AudioSkills-XL, LongAudio-XL,
AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based
training strategy. Trained on only open-source audio data, AF3 achieves new
SOTA results on over 20+ (long) audio understanding and reasoning benchmarks,
surpassing both open-weight and closed-source models trained on much larger
datasets.

</details>


### [12] [Distilling Spectrograms into Tokens: Fast and Lightweight Bioacoustic Classification for BirdCLEF+ 2025](https://arxiv.org/abs/2507.08236)
*Anthony Miyaguchi,Murilo Gustineli,Adrian Cheung*

Main category: cs.SD

TL;DR: BirdCLEF+ 2025挑战中，团队通过优化预训练模型和提出轻量级STSG方法，在CPU限制下实现了高效的生物声学分类。


<details>
  <summary>Details</summary>
Motivation: 在严格的90分钟CPU推理限制下，传统深度学习方法不适用，需探索高效解决方案。

Method: 1. 优化Bioacoustics Model Zoo的预训练模型；2. 提出STSG方法，将音频转换为离散标记并学习上下文嵌入。

Result: 优化模型在公开和私有排行榜上分别得分为0.810和0.778；STSG方法得分为0.559和0.520。

Conclusion: 优化模型和STSG方法展示了在计算限制下高效生物声学分类的可行性。

Abstract: The BirdCLEF+ 2025 challenge requires classifying 206 species, including
birds, mammals, insects, and amphibians, from soundscape recordings under a
strict 90-minute CPU-only inference deadline, making many state-of-the-art deep
learning approaches impractical. To address this constraint, the DS@GT BirdCLEF
team explored two strategies. First, we establish competitive baselines by
optimizing pre-trained models from the Bioacoustics Model Zoo for CPU
inference. Using TFLite, we achieved a nearly 10x inference speedup for the
Perch model, enabling it to run in approximately 16 minutes and achieve a final
ROC-AUC score of 0.729 on the public leaderboard post-competition and 0.711 on
the private leaderboard. The best model from the zoo was BirdSetEfficientNetB1,
with a public score of 0.810 and a private score of 0.778. Second, we introduce
a novel, lightweight pipeline named Spectrogram Token Skip-Gram (STSG) that
treats bioacoustics as a sequence modeling task. This method converts audio
into discrete "spectrogram tokens" by clustering Mel-spectrograms using Faiss
K-means and then learns high-quality contextual embeddings for these tokens in
an unsupervised manner with a Word2Vec skip-gram model. For classification,
embeddings within a 5-second window are averaged and passed to a linear model.
With a projected inference time of 6 minutes for a 700-minute test set, the
STSG approach achieved a final ROC-AUC public score of 0.559 and a private
score of 0.520, demonstrating the viability of fast tokenization approaches
with static embeddings for bioacoustic classification. Supporting code for this
paper can be found at https://github.com/dsgt-arc/birdclef-2025.

</details>


### [13] [Active Learning for Text-to-Speech Synthesis with Informative Sample Collection](https://arxiv.org/abs/2507.08319)
*Kentaro Seki,Shinnosuke Takamichi,Takaaki Saeki,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: 提出了一种基于主动学习的TTS语料库构建方法，通过迭代数据收集与模型训练，构建高效语料库，提升语音合成质量。


<details>
  <summary>Details</summary>
Motivation: 现代TTS系统依赖高质量数据集，但数据规模增长带来存储挑战，需更高效的数据构建方法。

Method: 采用主动学习方法，交替进行数据收集与模型训练，优先获取对模型改进更有效的数据。

Result: 实验表明，该方法构建的语料库在相同规模下能实现更高质量的语音合成。

Conclusion: 基于主动学习的TTS语料库构建方法高效且有效，优于传统方法。

Abstract: The construction of high-quality datasets is a cornerstone of modern
text-to-speech (TTS) systems. However, the increasing scale of available data
poses significant challenges, including storage constraints. To address these
issues, we propose a TTS corpus construction method based on active learning.
Unlike traditional feed-forward and model-agnostic corpus construction
approaches, our method iteratively alternates between data collection and model
training, thereby focusing on acquiring data that is more informative for model
improvement. This approach enables the construction of a data-efficient corpus.
Experimental results demonstrate that the corpus constructed using our method
enables higher-quality speech synthesis than corpora of the same size.

</details>


### [14] [Audio Inpanting using Discrete Diffusion Model](https://arxiv.org/abs/2507.08333)
*Tali Dror,Iftach Shoham,Moshe Buchris,Oren Gal,Haim Permuter,Gilad Katz,Eliya Nachmani*

Main category: cs.SD

TL;DR: 提出了一种基于离散扩散模型的音频修复方法，适用于长缺失段的音频重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在修复超过100毫秒的音频缺失段时质量下降，需要更稳定的解决方案。

Method: 使用预训练的音频标记器生成离散潜在空间表示，并在该空间中进行扩散建模。

Result: 在MusicNet和MTG数据集上，该方法在长达300毫秒和500毫秒的缺失段中表现优于基线。

Conclusion: 该方法为长缺失段的音频修复提供了鲁棒的解决方案。

Abstract: Audio inpainting refers to the task of reconstructing missing segments in
corrupted audio recordings. While prior approaches-including waveform and
spectrogram-based diffusion models-have shown promising results for short gaps,
they often degrade in quality when gaps exceed 100 milliseconds (ms). In this
work, we introduce a novel inpainting method based on discrete diffusion
modeling, which operates over tokenized audio representations produced by a
pre-trained audio tokenizer. Our approach models the generative process
directly in the discrete latent space, enabling stable and semantically
coherent reconstruction of missing audio. We evaluate the method on the
MusicNet dataset using both objective and perceptual metrics across gap
durations up to 300 ms. We further evaluated our approach on the MTG dataset,
extending the gap duration to 500 ms. Experimental results demonstrate that our
method achieves competitive or superior performance compared to existing
baselines, particularly for longer gaps, offering a robust solution for
restoring degraded musical recordings. Audio examples of our proposed method
can be found at https://iftach21.github.io/

</details>


### [15] [Enforcing Speech Content Privacy in Environmental Sound Recordings using Segment-wise Waveform Reversal](https://arxiv.org/abs/2507.08412)
*Modan Tailleur,Mathieu Lagrange,Pierre Aumond,Vincent Tourre*

Main category: cs.SD

TL;DR: 提出一种方法，通过反转波形段和语音分离技术，在保护环境声音完整性的同时降低语音可懂度。


<details>
  <summary>Details</summary>
Motivation: 解决环境声音记录中语音隐私问题，限制数据分析和共享。

Method: 使用语音活动检测和语音分离技术，反转波形段以扭曲语音内容。

Result: 在模拟数据集上，方法显著降低语音可懂度（97.9% WER），同时保持声音检测能力（2.7% SCAD）和高音频质量（FAD 1.40）。

Conclusion: 该方法有效保护语音隐私，同时保留环境声音完整性，随机拼接可增强鲁棒性。

Abstract: Environmental sound recordings often contain intelligible speech, raising
privacy concerns that limit analysis, sharing and reuse of data. In this paper,
we introduce a method that renders speech unintelligible while preserving both
the integrity of the acoustic scene, and the overall audio quality. Our
approach involves reversing waveform segments to distort speech content. This
process is enhanced through a voice activity detection and speech separation
pipeline, which allows for more precise targeting of speech.
  In order to demonstrate the effectivness of the proposed approach, we
consider a three-part evaluation protocol that assesses: 1) speech
intelligibility using Word Error Rate (WER), 2) sound sources detectability
using Sound source Classification Accuracy-Drop (SCAD) from a widely used
pre-trained model, and 3) audio quality using the Fr\'echet Audio Distance
(FAD), computed with our reference dataset that contains unaltered speech.
Experiments on this simulated evaluation dataset, which consists of linear
mixtures of speech and environmental sound scenes, show that our method
achieves satisfactory speech intelligibility reduction (97.9% WER), minimal
degradation of the sound sources detectability (2.7% SCAD), and high perceptual
quality (FAD of 1.40). An ablation study further highlights the contribution of
each component of the pipeline. We also show that incorporating random splicing
to our speech content privacy enforcement method can enhance the algorithm's
robustness to attempt to recover the clean speech, at a slight cost of audio
quality.

</details>


### [16] [MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling](https://arxiv.org/abs/2507.08530)
*Jingjing Tang,Xin Wang,Zhe Zhang,Junichi Yamagishi,Geraint Wiggins,George Fazekas*

Main category: cs.SD

TL;DR: MIDI-VALLE是一种基于VALLE框架的神经编解码语言模型，用于从音乐乐谱生成富有表现力的音频表演，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统音乐表演合成模型在多样化的MIDI来源、音乐风格和录音环境中泛化能力不足。

Method: 改进VALLE框架，通过参考音频表演及其对应MIDI进行条件建模，将MIDI和音频编码为离散标记。

Result: 在ATEPP和Maestro数据集上，Frechet Audio Distance降低75%以上，听众测试中获202票（基线58票）。

Conclusion: MIDI-VALLE在合成质量和泛化能力上显著提升，适用于多样化的表演MIDI输入。

Abstract: Generating expressive audio performances from music scores requires models to
capture both instrument acoustics and human interpretation. Traditional music
performance synthesis pipelines follow a two-stage approach, first generating
expressive performance MIDI from a score, then synthesising the MIDI into
audio. However, the synthesis models often struggle to generalise across
diverse MIDI sources, musical styles, and recording environments. To address
these challenges, we propose MIDI-VALLE, a neural codec language model adapted
from the VALLE framework, which was originally designed for zero-shot
personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio
synthesis, we improve the architecture to condition on a reference audio
performance and its corresponding MIDI. Unlike previous TTS-based systems that
rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens,
facilitating a more consistent and robust modelling of piano performances.
Furthermore, the model's generalisation ability is enhanced by training on an
extensive and diverse piano performance dataset. Evaluation results show that
MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving
over 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the
listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline,
demonstrating improved synthesis quality and generalisation across diverse
performance MIDI inputs.

</details>


### [17] [FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation](https://arxiv.org/abs/2507.08557)
*Yuxuan Jiang,Zehua Chen,Zeqian Ju,Chang Li,Weibei Dou,Jun Zhu*

Main category: cs.SD

TL;DR: FreeAudio是一种无需训练的时序控制文本到音频（T2A）生成框架，首次实现长时序控制T2A生成，如“猫头鹰在2.4s-5.2s鸣叫”。


<details>
  <summary>Details</summary>
Motivation: 现有T2A方法因音频-文本对齐数据有限，难以处理复杂时序提示。FreeAudio旨在解决这一问题，实现高质量长时序控制生成。

Method: 1) 使用LLM规划非重叠时间窗口并重写描述；2) 引入解耦聚合注意力控制（时序控制）和上下文潜在组合（局部平滑）与参考引导（全局一致性）。

Result: FreeAudio在无需训练方法中达到SOTA时序控制T2A质量，与训练方法相当，且长时序生成质量媲美Stable Audio。

Conclusion: FreeAudio为时序控制长时序T2A合成开辟了新途径，展示了无需训练方法的潜力。

Abstract: Text-to-audio (T2A) generation has achieved promising results with the recent
advances in generative models. However, because of the limited quality and
quantity of temporally-aligned audio-text pairs, existing T2A methods struggle
to handle the complex text prompts that contain precise timing control, e.g.,
"owl hooted at 2.4s-5.2s". Recent works have explored data augmentation
techniques or introduced timing conditions as model inputs to enable
timing-conditioned 10-second T2A generation, while their synthesis quality is
still limited. In this work, we propose a novel training-free timing-controlled
T2A framework, FreeAudio, making the first attempt to enable timing-controlled
long-form T2A generation, e.g., "owl hooted at 2.4s-5.2s and crickets chirping
at 0s-24s". Specifically, we first employ an LLM to plan non-overlapping time
windows and recaption each with a refined natural language description, based
on the input text and timing prompts. Then we introduce: 1) Decoupling and
Aggregating Attention Control for precise timing control; 2) Contextual Latent
Composition for local smoothness and Reference Guidance for global consistency.
Extensive experiments show that: 1) FreeAudio achieves state-of-the-art
timing-conditioned T2A synthesis quality among training-free methods and is
comparable to leading training-based methods; 2) FreeAudio demonstrates
comparable long-form generation quality with training-based Stable Audio and
paves the way for timing-controlled long-form T2A synthesis. Demo samples are
available at: https://freeaudio.github.io/FreeAudio/

</details>


### [18] [Phoneme-Level Analysis for Person-of-Interest Speech Deepfake Detection](https://arxiv.org/abs/2507.08626)
*Davide Salvi,Viola Negroni,Sara Mandelli,Paolo Bestagini,Stefano Tubaro*

Main category: cs.SD

TL;DR: 提出了一种基于音素的语音深度伪造检测方法，通过分解参考音频为音素构建详细说话者档案，实现细粒度检测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步使得语音深度伪造广泛传播，威胁数字信任。现有方法虽性能优秀，但粒度有限且缺乏可解释性。

Method: 将参考音频分解为音素构建说话者档案，测试样本的音素与档案逐一比对，检测合成痕迹。

Result: 方法在准确性上与传统方法相当，同时具备更强的鲁棒性和可解释性。

Conclusion: 通过音素分析，为可解释的说话者中心深度伪造检测开辟了新方向。

Abstract: Recent advances in generative AI have made the creation of speech deepfakes
widely accessible, posing serious challenges to digital trust. To counter this,
various speech deepfake detection strategies have been proposed, including
Person-of-Interest (POI) approaches, which focus on identifying impersonations
of specific individuals by modeling and analyzing their unique vocal traits.
Despite their excellent performance, the existing methods offer limited
granularity and lack interpretability. In this work, we propose a POI-based
speech deepfake detection method that operates at the phoneme level. Our
approach decomposes reference audio into phonemes to construct a detailed
speaker profile. In inference, phonemes from a test sample are individually
compared against this profile, enabling fine-grained detection of synthetic
artifacts. The proposed method achieves comparable accuracy to traditional
approaches while offering superior robustness and interpretability, key aspects
in multimedia forensics. By focusing on phoneme analysis, this work explores a
novel direction for explainable, speaker-centric deepfake detection.

</details>


### [19] [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
*Peter Sullivan,Muhammad Abdul-Mageed*

Main category: cs.SD

TL;DR: 研究利用UNESCO的20世纪中期广播录音，测试现代语言识别（LID）和说话人识别（SR）方法的鲁棒性，发现LID系统（如Whisper）对第二语言和口音语音处理能力提升，但SR系统仍存在偏见问题。


<details>
  <summary>Details</summary>
Motivation: 探讨现代LID和SR方法在多语言说话人和跨年龄录音中的表现，以评估其在实际档案应用中的可行性。

Method: 利用UNESCO的20世纪中期广播录音，测试Whisper等LID系统和SR系统的性能。

Result: LID系统对第二语言和口音语音处理能力较强，但SR系统在通道、年龄和语言方面仍存在偏见。

Conclusion: SR系统需克服偏见问题，才能用于档案中的说话人索引。

Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century
radio recordings to probe the robustness of modern off-the-shelf language
identification (LID) and speaker recognition (SR) methods, especially with
respect to the impact of multilingual speakers and cross-age recordings. Our
findings suggest that LID systems, such as Whisper, are increasingly adept at
handling second-language and accented speech. However, speaker embeddings
remain a fragile component of speech processing pipelines that is prone to
biases related to the channel, age, and language. Issues which will need to be
overcome should archives aim to employ SR methods for speaker indexing.

</details>

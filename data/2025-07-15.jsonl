{"id": "2507.08882", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.5"], "pdf": "https://arxiv.org/pdf/2507.08882", "abs": "https://arxiv.org/abs/2507.08882", "authors": ["Janaki Viswanathan", "Alexander Blatt", "Konrad Hagemann", "Dietrich Klakow"], "title": "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers", "comment": "8 pages, 2 figures, 4 tables, publication identification number\n  (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online\n  publication- https://d-nb.info/127614606X/34 & Katalogeintrag:\n  https://d-nb.info/127614606X/", "summary": "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models."}
{"id": "2507.09116", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09116", "abs": "https://arxiv.org/abs/2507.09116", "authors": ["Bingshen Mu", "Kun Wei", "Pengcheng Guo", "Lei Xie"], "title": "Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition", "comment": "IEEE Transactions on Audio, Speech and Language Processing", "summary": "Despite substantial improvements in ASR, performance tends to degrade when\nfaced with adverse conditions such as speaker accents. Generative error\ncorrection (GER) leverages the rich linguistic knowledge and exceptional\nreasoning ability of LLMs, significantly outperforming typical LM methods.\nHowever, it lacks specificity in accented speech scenarios. In this study, we\nleverage GER to improve the accuracy of transcription predictions by addressing\nthe two primary features of accented speech recognition. To fully leverage\npronunciation information, we propose the multi-modal GER, which integrates\npronunciation information from the speech modality, and the multi-granularity\nGER, which incorporates fine-grained phoneme-level information related to\npronunciation. These two methods enable the LLM to utilize the pronunciation\ninformation of accented speech and the semantic information from word-level\nhypotheses for accurate transcription predictions through LoRA fine-tuning. On\nthe one hand, we employ a three-stage training strategy to train separate\nmulti-modal GER models for each accent to obtain mono-accent LoRA experts. By\nadopting our proposed HDMoLE method, which incorporates hierarchical routing\nand dynamic thresholds within the mixture of LoRA experts, we effectively merge\nmultiple mono-accent LoRA experts within a single multi-modal GER to overcome\nthe challenges posed by accent diversity. On the other hand, multi-granularity\nGER leverages the N-best word-level and phoneme-level hypotheses generated by\nthe HDMoLE model to predict the final accented speech transcriptions.\nExperimental results on the multi-accent English dataset demonstrate the\nefficacy of our proposed methods. Our methods achieve a remarkable relative WER\nreduction of 67.35% compared to the Whisper-large-v3 baseline."}
{"id": "2507.09195", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09195", "abs": "https://arxiv.org/abs/2507.09195", "authors": ["Parthasaarathy Sudarsanam", "Archontis Politis"], "title": "Towards Spatial Audio Understanding via Question Answering", "comment": null, "summary": "In this paper, we introduce a novel framework for spatial audio understanding\nof first-order ambisonic (FOA) signals through a question answering (QA)\nparadigm, aiming to extend the scope of sound event localization and detection\n(SELD) towards spatial scene understanding and reasoning. First, we curate and\nrelease fine-grained spatio-temporal textual descriptions for the STARSS23\ndataset using a rule-based approach, and further enhance linguistic diversity\nusing large language model (LLM)-based rephrasing. We also introduce a QA\ndataset aligned with the STARSS23 scenes, covering various aspects such as\nevent presence, localization, spatial, and temporal relationships. To increase\nlanguage variety, we again leverage LLMs to generate multiple rephrasings per\nquestion. Finally, we develop a baseline spatial audio QA model that takes FOA\nsignals and natural language questions as input and provides answers regarding\nvarious occurrences, temporal, and spatial relationships of sound events in the\nscene formulated as a classification task. Despite being trained solely with\nscene-level question answering supervision, our model achieves performance that\nis comparable to a fully supervised sound event localization and detection\nmodel trained with frame-level spatiotemporal annotations. The results\nhighlight the potential of language-guided approaches for spatial audio\nunderstanding and open new directions for integrating linguistic supervision\ninto spatial scene analysis."}
{"id": "2507.09310", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09310", "abs": "https://arxiv.org/abs/2507.09310", "authors": ["Dominika Woszczyk", "Manuel Sam Ribeiro", "Thomas Merritt", "Daniel Korzekwa"], "title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "comment": "Presented at Clarity Challenge 2023", "summary": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity."}
{"id": "2507.09070", "categories": ["eess.AS", "cs.SD", "68T07", "I.2.7; I.2.6; G.3; H.5.5"], "pdf": "https://arxiv.org/pdf/2507.09070", "abs": "https://arxiv.org/abs/2507.09070", "authors": ["Shivam Mehta", "Yingru Liu", "Zhenyu Tang", "Kainan Peng", "Vimal Manohar", "Shun Zhang", "Mike Seltzer", "Qing He", "Mingbo Ma"], "title": "SemAlignVC: Enhancing zero-shot timbre conversion using semantic alignment", "comment": "6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop\n  (SSW) 2025", "summary": "Zero-shot voice conversion (VC) synthesizes speech in a target speaker's\nvoice while preserving linguistic and paralinguistic content. However, timbre\nleakage-where source speaker traits persist-remains a challenge, especially in\nneural codec and LLM-based VC, where quantized representations entangle speaker\nidentity with content. We introduce SemAlignVC, an architecture designed to\nprevent timbre leakage using SemAlign, a novel method that aligns text and\naudio representations to ensure speaker-independent semantic encoding. This\ndisentangled representation conditions an autoregressive transformer for\nhigh-fidelity conversion without explicit speaker embeddings. Experiments show\nSemAlignVC significantly reduces timbre leakage, outperforming baselines in\nspeaker timbre similarity, intelligibility, and naturalness, making it a\nrobust, privacy-preserving, and generalizable VC solution. Audio samples can be\naccessed at https://shivammehta25.github.io/SemAlignVC/"}
{"id": "2507.08821", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08821", "abs": "https://arxiv.org/abs/2507.08821", "authors": ["Pedro D. Alvim", "Hugerles S. Silva", "Ugo S. Dias", "Osamah S. Badarneh", "Felipe A. P. Figueiredo", "Rausley A. A. de Souza"], "title": "LNN-powered Fluid Antenna Multiple Access", "comment": null, "summary": "Fluid antenna systems represent an innovative approach in wireless\ncommunication, recently applied in multiple access to optimize the\nsignal-to-interference-plus-noise ratio through port selection. This letter\nframes the port selection problem as a multi-label classification task for the\nfirst time, improving best-port selection with limited port observations. We\naddress this challenge by leveraging liquid neural networks (LNNs) to predict\nthe optimal port under emerging fluid antenna multiple access scenarios\nalongside a more general $\\alpha$-$\\mu$ fading model. We also apply\nhyperparameter optimization to refine LNN architectures for different\nobservation scenarios. Our approach yields lower outage probability values than\nexisting methods."}
{"id": "2507.09342", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09342", "abs": "https://arxiv.org/abs/2507.09342", "authors": ["Emmanuel Adetiba", "Abdultaofeek Abayomi", "Raymond J. Kala", "Ayodele H. Ifijeh", "Oluwatobi E. Dare", "Olabode Idowu-Bismark", "Gabriel O. Sobola", "Joy N. Adetiba", "Monsurat Adepeju Lateef", "Heather Cole-Lewis"], "title": "BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus", "comment": null, "summary": "There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for\nhigh resource-to-low resource language pairs such as English-to-Yoruba. Thus,\nin this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech\nTranslation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a\nhybrid architecture we developed for large-scale direct S2ST corpus creation at\nreduced cost. To achieve this, we leveraged non speech-to-speech Standard\nYoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as\nthe corresponding Standard English (SE) transcripts. YORULECT Corpus is small\nscale(1,504) samples, and it does not have paired English audios. Therefore, we\ngenerated the SE audios using pre-trained AI models (i.e. Facebook MMS). We\nalso developed an audio augmentation algorithm named AcoustAug based on three\nlatent acoustic features to generate augmented audios from the raw audios of\nthe two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,\nwhich gives a total of 24,064 sample size. The total audio duration for the two\nlanguages is 41.20 hours. This size is quite significant. Beyond building S2ST\nmodels, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve\nexisting ones. The created corpus and Coqui framework were used to build a\npretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The\nYoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates\nmoderate fundamental pitch similarity with the reference real-time audio.\nUltimately, the corpus architecture in this study can be leveraged by\nresearchers and developers to curate datasets for multilingual\nhigh-resource-to-low-resource African languages. This will bridge the huge\ndigital divides in translations among high and low-resource language pairs.\nBENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at\n(https://bit.ly/40bGMwi)."}
{"id": "2507.09161", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09161", "abs": "https://arxiv.org/abs/2507.09161", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition", "comment": "Presented at Queen's University Biological Station Seminars of\n  Graduate Research in Ontario, Lake Shift Dissertation Camp (QUBS '25)", "summary": "Large language models have shown a remarkable ability to extract meaning from\nunstructured data, offering new ways to interpret biomedical signals beyond\ntraditional numerical methods. In this study, we present a matrix factorization\nframework for bioacoustic signal analysis which is enhanced by large language\nmodels. The focus is on separating bioacoustic signals that commonly overlap in\nclinical recordings, using matrix factorization to decompose the mixture into\ninterpretable components. A large language model is then applied to the\nseparated signals to associate distinct acoustic patterns with potential\nmedical conditions such as cardiac rhythm disturbances or respiratory\nabnormalities. Recordings were obtained from a digital stethoscope applied to a\nclinical manikin to ensure a controlled and high-fidelity acquisition\nenvironment. This hybrid approach does not require labeled data or prior\nknowledge of source types, and it provides a more interpretable and accessible\nframework for clinical decision support. The method demonstrates promise for\nintegration into future intelligent diagnostic tools."}
{"id": "2507.08950", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08950", "abs": "https://arxiv.org/abs/2507.08950", "authors": ["Xue Zhang", "Abla Kammoun", "Mohamed-Slim Alouini"], "title": "Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems", "comment": null, "summary": "This paper investigates the asymptotic behavior of the deterministic and\nstochastic Cram\\'er-Rao Bounds (CRB) for semi-blind channel estimation in\nmassive multiple-input multiple-output (MIMO) systems. We derive and analyze\nmathematically tractable expressions for both metrics under various asymptotic\nregimes, which govern the growth rates of the number of antennas, the number of\nusers, the training sequence length, and the transmission block length. Unlike\nthe existing work, our results show that the CRB can be made arbitrarily small\nas the transmission block length increases, but only when the training sequence\nlength grows at the same rate and the number of users remains fixed. However,\nif the number of training sequences remains proportional to the number of\nusers, the channel estimation error is always lower-bounded by a non-vanishing\nconstant. Numerical results are presented to support our findings and\ndemonstrate the advantages of semi-blind channel estimation in reducing the\nrequired number of training sequences."}
{"id": "2507.09376", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS", "H.5.5"], "pdf": "https://arxiv.org/pdf/2507.09376", "abs": "https://arxiv.org/abs/2507.09376", "authors": ["Bilkent Samsurya"], "title": "Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering", "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "summary": "Accurate sound propagation simulation is essential for delivering immersive\nexperiences in virtual applications, yet industry methods for acoustic modeling\noften do not account for the full breadth of acoustic wave phenomena. This\npaper proposes a novel two-dimensional (2D) finite-difference time-domain\n(FDTD) framework that simulates sound propagation as a wave-based model in\nUnreal Engine, with an emphasis on capturing lower frequency wave phenomena,\nembedding occlusion, diffraction, reflection and interference in generated\nimpulse responses. The process begins by discretizing the scene geometry into a\n2D grid via a top-down projection from which obstacle masks and boundary\nconditions are derived. A Python-based FDTD solver injects a sine sweep at a\nsource position, and virtual quadraphonic microphone arrays record pressure\nfield responses at pre-defined listener positions. De-convolution of the\npressure responses yields multi-channel impulse responses that retain spatial\ndirectionality which are then integrated into Unreal Engine's audio pipeline\nfor dynamic playback. Benchmark tests confirm agreement with analytical\nexpectations, and the paper outlines hybrid extensions aimed at commercial\nviability."}
{"id": "2507.09226", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09226", "abs": "https://arxiv.org/abs/2507.09226", "authors": ["Shota Horiguchi", "Naohiro Tawara", "Takanori Ashihara", "Atsushi Ando", "Marc Delcroix"], "title": "Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?", "comment": null, "summary": "Neural speaker diarization is widely used for overlap-aware speaker\ndiarization, but it requires large multi-speaker datasets for training. To meet\nthis data requirement, large datasets are often constructed by combining\nmultiple corpora, including those originally designed for multi-speaker\nautomatic speech recognition (ASR). However, ASR datasets often feature loosely\ndefined segment boundaries that do not align with the stricter conventions of\ndiarization benchmarks. In this work, we show that such boundary looseness\nsignificantly impacts the diarization error rate, reducing evaluation\nreliability. We also reveal that models trained on data with varying boundary\nprecision tend to learn dataset-specific looseness, leading to poor\ngeneralization across out-of-domain datasets. Training with standardized tight\nboundaries via forced alignment improves not only diarization performance,\nespecially in streaming scenarios, but also ASR performance when combined with\nsimple post-processing."}
{"id": "2507.08974", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.08974", "abs": "https://arxiv.org/abs/2507.08974", "authors": ["Thien Hieu Hoang", "Tri Nhu Do", "Georges Kaddoum"], "title": "Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM", "comment": null, "summary": "Accurate channel estimation is crucial for the improvement of signal\nprocessing performance in wireless communications. However, traditional\nmodel-based methods frequently experience difficulties in dynamic environments.\nSimilarly, alternative machine-learning approaches typically lack\ngeneralization across different datasets due to variations in channel\ncharacteristics. To address this issue, in this study, we propose a novel\ndomain adaptation approach to bridge the gap between the quasi-static channel\nmodel (QSCM) and the map-based channel model (MBCM). Specifically, we first\nproposed a channel estimation pipeline that takes into account realistic\nchannel simulation to train our foundation model. Then, we proposed domain\nadaptation methods to address the estimation problem. Using simulation-based\ntraining to reduce data requirements for effective application in practical\nwireless environments, we find that the proposed strategy enables robust model\nperformance, even with limited true channel information."}
{"id": "2507.09510", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09510", "abs": "https://arxiv.org/abs/2507.09510", "authors": ["Shu Wu", "Anbin Qi", "Yanzhang Xie", "Xiang Xie"], "title": "SC-TSE: Speaker Consistency-Aware Target Speaker Extraction", "comment": "Accept to Interspeech2025", "summary": "Target Speaker Extraction (TSE) uses a reference cue to extract the target\nspeech from a mixture. In TSE systems relying on audio cues, the speaker\nembedding from the enrolled speech is crucial to performance. However, these\nembeddings may suffer from speaker identity confusion. Unlike previous studies\nthat focus on improving speaker embedding extraction, we improve TSE\nperformance from the perspective of speaker consistency. In this paper, we\npropose a speaker consistency-aware target speaker extraction method that\nincorporates a centroid-based speaker consistency loss. This approach enhances\nTSE performance by ensuring speaker consistency between the enrolled and\nextracted speech. In addition, we integrate conditional loss suppression into\nthe training process. The experimental results validate the effectiveness of\nour proposed methods in advancing the TSE performance. A speech demo is\navailable online.\\footnote{https://sc-tse.netlify.app/"}
{"id": "2507.09318", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09318", "abs": "https://arxiv.org/abs/2507.09318", "authors": ["Han Zhu", "Wei Kang", "Liyong Guo", "Zengwei Yao", "Fangjun Kuang", "Weiji Zhuang", "Zhaoqing Li", "Zhifeng Han", "Dong Zhang", "Xin Zhang", "Xingchen Song", "Long Lin", "Daniel Povey"], "title": "ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching", "comment": null, "summary": "Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice."}
{"id": "2507.08999", "categories": ["eess.SP", "I.5.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08999", "abs": "https://arxiv.org/abs/2507.08999", "authors": ["Duc Vu", "Selin Aviyente"], "title": "Hypergraph Overlapping Community Detection for Brain Networks", "comment": "6 Pages, Accepted for IEEE MLSP 2025", "summary": "Functional magnetic resonance imaging (fMRI) has been commonly used to\nconstruct functional connectivity networks (FCNs) of the human brain. TFCNs are\nprimarily limited to quantifying pairwise relationships between ROIs ignoring\nhigher order dependencies between multiple brain regions. Recently, hypergraph\nconstruction methods from fMRI time series data have been proposed to\ncharacterize the high-order relations among multiple ROIs. While there have\nbeen multiple methods for constructing hypergraphs from fMRI time series, the\nquestion of how to characterize the topology of these hypergraphs remains open.\nIn this paper, we make two key contributions to the field of community\ndetection in brain hypernetworks. First, we construct a hypergraph for each\nsubject capturing high order dependencies between regions. Second, we introduce\na spectral clustering based approach on hypergraphs to detect overlapping\ncommunity structure. Finally, the proposed method is implemented to detect the\nconsensus community structure across multiple subjects. The proposed method is\napplied to resting state fMRI data from Human Connectome Project to summarize\nthe overlapping community structure across a group of healthy young adults."}
{"id": "2507.09606", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09606", "abs": "https://arxiv.org/abs/2507.09606", "authors": ["Yuanjian Chen", "Han Yin"], "title": "Ensemble Confidence Calibration for Sound Event Detection in Open-environment", "comment": null, "summary": "Sound event detection (SED) has made strong progress in controlled\nenvironments with clear event categories. However, real-world applications\noften take place in open environments. In such cases, current methods often\nproduce predictions with too much confidence and lack proper ways to measure\nuncertainty. This limits their ability to adapt and perform well in new\nsituations. To solve this problem, we are the first to use ensemble methods in\nSED to improve robustness against out-of-domain (OOD) inputs. We propose a\nconfidence calibration method called Energy-based Open-World Softmax\n(EOW-Softmax), which helps the system better handle uncertainty in unknown\nscenes. We further apply EOW-Softmax to sound occurrence and overlap detection\n(SOD) by adjusting the prediction. In this way, the model becomes more\nadaptable while keeping its ability to detect overlapping events. Experiments\nshow that our method improves performance in open environments. It reduces\noverconfidence and increases the ability to handle OOD situations."}
{"id": "2507.09350", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09350", "abs": "https://arxiv.org/abs/2507.09350", "authors": ["Wiebke Middelberg", "Jung-Suk Lee", "Saeed Bagheri Sereshki", "Ali Aroudi", "Vladimir Tourbabin", "Daniel D. E. Wong"], "title": "Microphone Occlusion Mitigation for Own-Voice Enhancement in Head-Worn Microphone Arrays Using Switching-Adaptive Beamforming", "comment": "Accepted for publication at WASPAA 2025", "summary": "Enhancing the user's own-voice for head-worn microphone arrays is an\nimportant task in noisy environments to allow for easier speech communication\nand user-device interaction. However, a rarely addressed challenge is the\nchange of the microphones' transfer functions when one or more of the\nmicrophones gets occluded by skin, clothes or hair. The underlying problem for\nbeamforming-based speech enhancement is the (potentially rapidly) changing\ntransfer functions of both the own-voice and the noise component that have to\nbe accounted for to achieve optimal performance. In this paper, we address the\nproblem of an occluded microphone in a head-worn microphone array. We\ninvestigate three alternative mitigation approaches by means of (i)\nconventional adaptive beamforming, (ii) switching between a-priori estimates of\nthe beamformer coefficients for the occluded and unoccluded state, and (iii) a\nhybrid approach using a switching-adaptive beamformer. In an evaluation with\nreal-world recordings and simulated occlusion, we demonstrate the advantages of\nthe different approaches in terms of noise reduction, own-voice distortion and\nrobustness against voice activity detection errors."}
{"id": "2507.09215", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09215", "abs": "https://arxiv.org/abs/2507.09215", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Kun Yang"], "title": "Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems", "comment": null, "summary": "The bistatic Integrated Sensing and Communication (ISAC) is poised to become\na key application for next generation communication networks (e.g., B5G/6G),\nproviding simultaneous sensing and communication services with minimal changes\nto existing network infrastructure and hardware. However, a significant\nchallenge in bistatic cooperative sensing is clock asynchronism, arising from\nthe use of different clocks at far separated transmitters and receivers. This\nasynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),\npotentially causing sensing ambiguity. Traditional synchronization methods\ntypically rely on static reference links or GNSS-based timing sources, both of\nwhich are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.\nTo overcome these limitations, we propose a Time-Varying Offset Estimation\n(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which\nleverages the geometrically predictable characteristics of the Line-of-Sight\n(LoS) path to enable robust, infrastructure-free\n  synchronization. The framework treats the LoS delay and the Doppler shift as\ndynamic observations and models their evolution as a hidden stochastic process.\nA state-space formulation is developed to jointly estimate TO and CFO via an\nExtended Kalman Filter (EKF), enabling real-time tracking of clock offsets\nacross successive frames. Furthermore, the estimated offsets are subsequently\napplied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)\ncomponents, thereby enhancing the high-resolution target sensing performance.\nExtensive simulation results demonstrate that the proposed TVOE method improves\nthe estimation accuracy by 60%."}
{"id": "2507.09618", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09618", "abs": "https://arxiv.org/abs/2507.09618", "authors": ["Jilamika Wongpithayadisai", "Chompakorn Chaksangchaichot", "Soravitt Sangnark", "Patawee Prakrankamanant", "Krit Gangwanpongpun", "Siwa Boonpunmongkol", "Premmarin Milindasuta", "Dangkamon Na-Pombejra", "Sarana Nutanong", "Ekapol Chuangsuwanich"], "title": "THAI Speech Emotion Recognition (THAI-SER) corpus", "comment": null, "summary": "We present the first sizeable corpus of Thai speech emotion recognition,\nTHAI-SER, containing 41 hours and 36 minutes (27,854 utterances) from 100\nrecordings made in different recording environments: Zoom and two studio\nsetups. The recordings contain both scripted and improvised sessions, acted by\n200 professional actors (112 females and 88 males, aged 18 to 55) and were\ndirected by professional directors. There are five primary emotions: neutral,\nangry, happy, sad, and frustrated, assigned to the actors when recording\nutterances. The utterances are annotated with an emotional category using\ncrowdsourcing. To control the annotation process's quality, we also design an\nextensive filtering and quality control scheme to ensure that the majority\nagreement score remains above 0.71. We evaluate our annotated corpus using two\nmetrics: inter-annotator reliability and human recognition accuracy.\nInter-annotator reliability score was calculated using Krippendorff's alpha,\nwhere our corpus, after filtering, achieved an alpha score of 0.692, higher\nthan a recommendation of 0.667. For human recognition accuracy, our corpus\nscored up to 0.772 post-filtering. We also provide the results of the model\ntrained on the corpus evaluated on both in-corpus and cross-corpus setups. The\ncorpus is publicly available under a Creative Commons BY-SA 4.0, as well as our\ncodes for the experiments."}
{"id": "2507.09372", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09372", "abs": "https://arxiv.org/abs/2507.09372", "authors": ["Philippe Gonzalez", "Torsten Dau", "Tobias May"], "title": "Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model", "comment": "Accepted to Clarity 2025 Workshop", "summary": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference."}
{"id": "2507.09218", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09218", "abs": "https://arxiv.org/abs/2507.09218", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Chaochao Wang", "Xianchao Zhang", "Kun Yang"], "title": "Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC", "comment": null, "summary": "Bistatic Integrated Sensing and Communication (ISAC) is poised to become a\ncornerstone technology in next-generation communication networks, such as\nBeyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and\ncommunication functions without requiring significant modifications to existing\ninfrastructure. Despite its promising potential, a major challenge in bistatic\ncooperative sensing lies in the degradation of sensing accuracy, primarily\ncaused by the inherently weak received signals resulting from high reflection\nlosses in complex environments. Traditional methods have predominantly relied\non adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by\ndynamically adjusting the filter coefficients. However, these methods often\nstruggle to adapt effectively to the increasingly complex and diverse network\ntopologies. To address these challenges, we propose a novel Image\nSuper-Resolution-based Signal Enhancement (ISR-SE) framework that significantly\nimproves the recognition and recovery capabilities of ISAC signals.\nSpecifically, we first perform a time-frequency analysis by applying the\nShort-Time Fourier Transform (STFT) to the received signals, generating\nspectrograms that capture the frequency, magnitude, and phase components. These\ncomponents are then mapped into RGB images, where each channel represents one\nof the extracted features, enabling a more intuitive and informative\nvisualization of the signal structure. To enhance these RGB images, we design\nan improved denoising network that combines the strengths of the UNet\narchitecture and diffusion models. This hybrid architecture leverages UNet's\nmulti-scale feature extraction and the generative capacity of diffusion models\nto perform effective image denoising, thereby improving the quality and clarity\nof signal representations under low-SNR conditions."}
{"id": "2507.09750", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09750", "abs": "https://arxiv.org/abs/2507.09750", "authors": ["Enric Gusó", "Joanna Luberadzka", "Umut Sayin", "Xavier Serra"], "title": "MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients", "comment": "Accepted to WASPAA25", "summary": "We investigate the effects of four strategies for improving the ecological\nvalidity of synthetic room impulse response (RIR) datasets for monoaural Speech\nEnhancement (SE). We implement three features on top of the traditional image\nsource method-based (ISM) shoebox RIRs: multiband absorption coefficients,\nsource directivity and receiver directivity. We additionally consider\nmesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3\nmodel for each RIR dataset and evaluate the performance on a test set of real\nRIRs both objectively and subjectively. We find that RIRs which use\nfrequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain\n+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs\ndataset is publicly available for free download."}
{"id": "2507.09499", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09499", "abs": "https://arxiv.org/abs/2507.09499", "authors": ["Yuke Lin", "Ming Cheng", "Ze Li", "Ming Li"], "title": "The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM Challenge", "comment": "Technical Report for MLC-SLM Challenge in Interspeech2025", "summary": "We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to\nperform multi-speaker automatic speech recognition directly from raw audio\nwithout Oracle speaker labels or time boundaries. Our approach builds upon a\ndiarization-aware framework integrating speaker embeddings and temporal\nutterance boundaries into a Qwen2.5-based large language model (LLM). Then, we\nenhance the system's multilingual performance by fine-tuning language-specific\nadapters and LoRA modules within the LLM decoder. Finally, our system achieves\nthe tcpWER of 23.56\\% and 18.08\\% on the development and test sets of the\nMLC-SLM dataset, substantially outperforming the official baseline."}
{"id": "2507.09244", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09244", "abs": "https://arxiv.org/abs/2507.09244", "authors": ["Nishant Gupta", "Muris Sarajlic", "Erik G. Larsson"], "title": "Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming", "comment": "Accepted for Publication in IEEE VTC-Spring 2025, held at Oslo,\n  Norway", "summary": "The dense and distributed deployment of sub-THz radio units (RUs) alongside\nsub-10 GHz access point (AP) is a promising approach to provide high data rate\nand reliable coverage for future 6G applications. However, beam search or RU\nselection for the sub-THz RUs incurs significant overhead and high power\nconsumption. To address this, we introduce a method that leverages deep\nlearning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs\nusing the sub-10 GHz channel characteristics. A novel aspect of this work is\nthe consideration of inter-band beam configuration (IBBC), defined as the\nbroadside angle between the low-band and high-band antenna patterns of the user\nequipment (UE). Since IBBC indicates the beamforming information or UE's\norientation, it is typically not shared with the network as a part of\nsignalling. Therefore, we propose a solution strategy to infer a suitable\nsub-THz RU even when UEs do not share their IBBC information. Simulation\nresults illustrate the performance of the inferred sub-THz RU and highlights\nthe detrimental impact of neglecting UE orientation on the systems performance."}
{"id": "2507.09904", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09904", "abs": "https://arxiv.org/abs/2507.09904", "authors": ["Fabian Ritter-Gutierrez", "Yi-Cheng Lin", "Jui-Chiang Wei", "Jeremy H. M. Wong", "Nancy F. Chen", "Hung-yi Lee"], "title": "ASTAR-NTU solution to AudioMOS Challenge 2025 Track1", "comment": "Under Review - Submitted to AudioMOS Challenge 2025 - ASRU 2025", "summary": "Evaluation of text-to-music systems is constrained by the cost and\navailability of collecting experts for assessment. AudioMOS 2025 Challenge\ntrack 1 is created to automatically predict music impression (MI) as well as\ntext alignment (TA) between the prompt and the generated musical piece. This\npaper reports our winning system, which uses a dual-branch architecture with\npre-trained MuQ and RoBERTa models as audio and text encoders. A\ncross-attention mechanism fuses the audio and text representations. For\ntraining, we reframe the MI and TA prediction as a classification task. To\nincorporate the ordinal nature of MOS scores, one-hot labels are converted to a\nsoft distribution using a Gaussian kernel. On the official test set, a single\nmodel trained with this method achieves a system-level Spearman's Rank\nCorrelation Coefficient (SRCC) of 0.991 for MI and 0.952 for TA, corresponding\nto a relative improvement of 21.21\\% in MI SRCC and 31.47\\% in TA SRCC over the\nchallenge baseline."}
{"id": "2507.09570", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09570", "abs": "https://arxiv.org/abs/2507.09570", "authors": ["Wenmiao Gao", "Han Yin"], "title": "Enhancing Stereo Sound Event Detection with BiMamba and Pretrained PSELDnet", "comment": null, "summary": "Pre-training methods have greatly improved the performance of sound event\nlocalization and detection (SELD). However, existing Transformer-based models\nstill face high computational cost. To solve this problem, we present a stereo\nSELD system using a pre-trained PSELDnet and a bidirectional Mamba sequence\nmodel. Specifically, we replace the Conformer module with a BiMamba module. We\nalso use asymmetric convolutions to better capture the time and frequency\nrelationships in the audio signal. Test results on the DCASE2025 Task 3\ndevelopment dataset show that our method performs better than both the baseline\nand the original PSELDnet with a Conformer decoder. In addition, the proposed\nmodel costs fewer computing resources than the baselines. These results show\nthat the BiMamba architecture is effective for solving key challenges in SELD\ntasks. The source code is publicly accessible at https://github.com/\nalexandergwm/DCASE2025 TASK3 Stereo PSELD Mamba."}
{"id": "2507.09268", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09268", "abs": "https://arxiv.org/abs/2507.09268", "authors": ["Xiangjun Li", "Zilong Liu", "Zhengchun Zhou", "Pingzhi Fan"], "title": "Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has recently emerged as an\nexcellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is\nproposed whereby the delay-Doppler (DD) coupling phase is considered.\nSpecifically, we study matched filtering (MF) assisted channel estimation (CE)\nfor AFDM systems in complex doubly selective channels. By deriving the complete\ninput-output relationship, the inter-chirp-carrier interference,\nsignal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of\nAFDM, are investigated in discrete affine Fourier transform (DAFT) domain.\nFurther, we look into the path ambiguity problem and show that it may lead to\nsevere performance deterioration in fractional-delay fractional-Doppler\nchannels. To address such a problem, we introduce an MF assisted CE scheme\nbuilding upon a novel pilot arrangement across two consecutive AFDM\ntransmissions. This allows us to sequentially estimate the parameters of each\npath by exploiting the separability and approximate orthogonality of different\npaths in the DAFT domain, thus leading to significantly reduced complexity.\nFurthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is\nproposed to avoid significantly redundant computation, which can be extended to\ntypical wide-band systems. Extensive simulation results indicate that the\nproposed schemes offer superior advantages in terms of their improved\ncommunication performance and lower complexity."}
{"id": "2507.10313", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10313", "abs": "https://arxiv.org/abs/2507.10313", "authors": ["Yiru Yang"], "title": "DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation", "comment": null, "summary": "We present a demo of DQLoRA, an Adapter-Guided Distillation framework for\nrobust speech recognition under low-resource and noisy conditions. Our method\nemploys a frozen Whisper model as the teacher to provide semantic supervision,\nand a lightweight Wav2Vec2 student equipped with QLoRA-based Adapters. Training\nis conducted on the FLEURS dataset augmented with DNS-style noise. The student\nis optimized by jointly minimizing CTC loss and KL-based distillation loss,\nenabling efficient adaptation while preserving recognition accuracy."}
{"id": "2507.09806", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09806", "abs": "https://arxiv.org/abs/2507.09806", "authors": ["Mirco Pezzoli", "Federico Miotello", "Shoichi Koyama", "Fabio Antonacci"], "title": "Low-Rank Adaptation of Deep Prior Neural Networks For Room Impulse Response Reconstruction", "comment": "to appear in IEEE WASPAA", "summary": "The Deep Prior framework has emerged as a powerful generative tool which can\nbe used for reconstructing sound fields in an environment from few sparse\npressure measurements. It employs a neural network that is trained solely on a\nlimited set of available data and acts as an implicit prior which guides the\nsolution of the underlying optimization problem. However, a significant\nlimitation of the Deep Prior approach is its inability to generalize to new\nacoustic configurations, such as changes in the position of a sound source. As\na consequence, the network must be retrained from scratch for every new setup,\nwhich is both computationally intensive and time-consuming. To address this, we\ninvestigate transfer learning in Deep Prior via Low-Rank Adaptation (LoRA),\nwhich enables efficient fine-tuning of a pre-trained neural network by\nintroducing a low-rank decomposition of trainable parameters, thus allowing the\nnetwork to adapt to new measurement sets with minimal computational overhead.\nWe embed LoRA into a MultiResUNet-based Deep Prior model and compare its\nadaptation performance against full fine-tuning of all parameters as well as\nclassical retraining, particularly in scenarios where only a limited number of\nmicrophones are used. The results indicate that fine-tuning, whether done\ncompletely or via LoRA, is especially advantageous when the source location is\nthe sole changing parameter, preserving high physical fidelity, and\nhighlighting the value of transfer learning for acoustics applications."}
{"id": "2507.09386", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09386", "abs": "https://arxiv.org/abs/2507.09386", "authors": ["Ruangrawee Kitichotkul", "Shashwath Bharadwaj", "Joshua Rapp", "Yanting Ma", "Alexander Mehta", "Vivek K Goyal"], "title": "Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging", "comment": "20 pages, 15 figures, to be presented at the International Conference\n  on Computer Vision (ICCV) 2025", "summary": "Conventional wisdom suggests that single-photon lidar (SPL) should operate in\nlow-light conditions to minimize dead-time effects. Many methods have been\ndeveloped to mitigate these effects in synchronous SPL systems. However,\nsolutions for free-running SPL remain limited despite the advantage of reduced\nhistogram distortion from dead times. To improve the accuracy of free-running\nSPL, we propose a computationally efficient joint maximum likelihood estimator\nof the signal flux, the background flux, and the depth using only histograms,\nalong with a complementary regularization framework that incorporates a learned\npoint cloud score model as a prior. Simulations and experiments demonstrate\nthat free-running SPL yields lower estimation errors than its synchronous\ncounterpart under identical conditions, with our regularization further\nimproving accuracy."}
{"id": "2507.10447", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10447", "abs": "https://arxiv.org/abs/2507.10447", "authors": ["Tomasz Sroka", "Tomasz Wężowicz", "Dominik Sidorczuk", "Mateusz Modrzejewski"], "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations", "comment": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure", "summary": "With the rapid advancement of generative audio models, distinguishing between\nhuman-composed and generated music is becoming increasingly challenging. As a\nresponse, models for detecting fake music have been proposed. In this work, we\nexplore the robustness of such systems under audio augmentations. To evaluate\nmodel generalization, we constructed a dataset consisting of both real and\nsynthetic music generated using several systems. We then apply a range of audio\ntransformations and analyze how they affect classification accuracy. We test\nthe performance of a recent state-of-the-art musical deepfake detection model\nin the presence of audio augmentations. The performance of the model decreases\nsignificantly even with the introduction of light augmentations."}
{"id": "2507.09834", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09834", "abs": "https://arxiv.org/abs/2507.09834", "authors": ["Shu-wen Yang", "Byeonggeun Kim", "Kuan-Po Huang", "Qingming Tang", "Huy Phan", "Bo-Ru Lu", "Harsha Sundar", "Shalini Ghosh", "Hung-yi Lee", "Chieh-Chi Kao", "Chao Wang"], "title": "Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction", "comment": "Accepted by ICML 2025. Project website: https://audiomntp.github.io/", "summary": "Autoregressive next-token prediction with the Transformer decoder has become\na de facto standard in large language models (LLMs), achieving remarkable\nsuccess in Natural Language Processing (NLP) at scale. Extending this paradigm\nto audio poses unique challenges due to its inherently continuous nature. We\nresearch audio generation with a causal language model (LM) without discrete\ntokens. We leverage token-wise diffusion to model the continuous distribution\nof the next continuous-valued token. Our approach delivers significant\nimprovements over previous discrete solution, AudioGen, achieving 20% and 40%\nrelative gains on AudioCaps in Frechet Audio Distance (FAD) and\nKullback-Leibler (KL) divergence, respectively. Additionally, we propose a\nnovel masked next-token prediction task that incorporates masked prediction\ninto the causal LM framework. On AudioCaps, the innovation yields 41% and 33%\nrelative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)\nmodels, respectively, and is on par with the state-of-the-art (SOTA) diffusion\nmodels. Furthermore, we achieve these results with significantly fewer\nparameters -- 193M for our Base and 462M for our Large models."}
{"id": "2507.09408", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09408", "abs": "https://arxiv.org/abs/2507.09408", "authors": ["Sajedeh Norouzi", "Mostafa Rahmani", "Yi Chu", "Torsten Braun", "Kaushik Chowdhury", "Alister Burr"], "title": "Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation", "comment": "Accepted in IEEE PIMRC 2025", "summary": "Effective channel estimation CE is critical for optimizing the performance of\n5G New Radio NR systems particularly in dynamic environments where traditional\nmethods struggle with complexity and adaptability This paper introduces\nGraphNet a novel lightweight Graph Neural Network GNNbased estimator designed\nto enhance CE in 5G NR Our proposed method utilizes a GNN architecture that\nminimizes computational overhead while capturing essential features necessary\nfor accurate CE We evaluate GraphNet across various channel conditions from\nslowvarying to highly dynamic environments and compare its performance to\nChannelNet a wellknown deep learningbased CE method GraphNet not only matches\nChannelNets performance in stable conditions but significantly outperforms it\nin highvariation scenarios particularly in terms of Block Error Rate It also\nincludes builtin noise estimation that enhances robustness in challenging\nchannel conditions Furthermore its significantly lighter computational\nfootprint makes GraphNet highly suitable for realtime deployment especially on\nedge devices with limited computational resources By underscoring the potential\nof GNNs to transform CE processes GraphNet offers a scalable and robust\nsolution that aligns with the evolving demands of 5G technologies highlighting\nits efficiency and performance as a nextgeneration solution for wireless\ncommunication systems"}
{"id": "2507.10456", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10456", "abs": "https://arxiv.org/abs/2507.10456", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "title": "Radif corpus: a symbolic dataset for non-metric iranian classical music", "comment": null, "summary": "Non-metric music forms the core of the repertoire in Iranian classical music.\nDastgahi music serves as the underlying theoretical system for both Iranian art\nmusic and certain folk traditions. At the heart of Iranian classical music lies\nthe radif, a foundational repertoire that organizes melodic material central to\nperformance and pedagogy.\n  In this study, we introduce the first digital corpus representing the\ncomplete non-metrical radif repertoire, covering all 13 existing components of\nthis repertoire. We provide MIDI files (about 281 minutes in total) and data\nspreadsheets describing notes, note durations, intervals, and hierarchical\nstructures for 228 pieces of music. We faithfully represent the tonality\nincluding quarter-tones, and the non-metric aspect. Furthermore, we provide\nsupporting basic statistics, and measures of complexity and similarity over the\ncorpus.\n  Our corpus provides a platform for computational studies of Iranian classical\nmusic. Researchers might employ it in studying melodic patterns, investigating\nimprovisational styles, or for other tasks in music information retrieval,\nmusic theory, and computational (ethno)musicology."}
{"id": "2507.09929", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09929", "abs": "https://arxiv.org/abs/2507.09929", "authors": ["Haoyang Li", "Nana Hou", "Yuchen Hu", "Jixun Yao", "Sabato Marco Siniscalchi", "Eng Siong Chng"], "title": "Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization", "comment": null, "summary": "This work investigates speech enhancement (SE) from the perspective of\nlanguage models (LMs). We propose a novel method that leverages Direct\nPreference Optimization (DPO) to improve the perceptual quality of enhanced\nspeech. Using UTMOS, a neural MOS prediction model, as a proxy for human\nratings, our approach guides optimization toward perceptually preferred\noutputs. This differs from existing LM-based SE methods that focus on\nmaximizing the likelihood of clean speech tokens, which may misalign with human\nperception and degrade quality despite low prediction error. Experiments on the\n2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO\nto a pretrained LM-based SE model yields consistent improvements across various\nspeech quality metrics, with relative gains of up to 56%. To our knowledge,\nthis is the first application of DPO to SE and the first to incorporate proxy\nperceptual feedback into LM-based SE training, pointing to a promising\ndirection for perceptually aligned SE."}
{"id": "2507.09458", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09458", "abs": "https://arxiv.org/abs/2507.09458", "authors": ["Wang Ning", "Zhang Chenyu", "Sun Yanshi", "Min Minghui", "Liu Yuanwei", "Li Shiyin"], "title": "An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation", "comment": "13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is\n  Paper-TW-Jul-25-1790", "summary": "Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which\neffectively utilizes both NOMA and orthogonal multiple access (OMA)\ntechnologies through flexible resource allocation in a single transmission, has\ndemonstrated immense potential for enhancing the performance of wireless\ncommunication systems. To further release the potential of HNOMA, this paper\nproposes a novel design of H-NOMA which jointly incorporates hybrid successive\ninterference cancellation (HSIC) and power adaptation (PA) in the NOMA\ntransmission phase. To reveal the potential of the proposed HSIC-PA aided\nH-NOMA scheme, closed-form expression for the probability of the event that\nH-NOMA can achieve a higher data rate than pure OMA by consuming less energy is\nrigorously derived. Furthermore, the asymptotic analysis demonstrates that the\nprobability of the proposed H-NOMA scheme approaches 1 in the high\nsignal-to-noise ratio (SNR) regime without any constraints on either users'\ntarget rates or transmit power ratios. This represents a significant\nimprovement over conventional H-NOMA schemes, which require specific\nrestrictive conditions to achieve probability 1 at high SNRs as shown in\nexisting work. The above observation indicates that with less energy\nconsumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate\nthan pure OMA with probability 1 at high SNRs, and hence a higher energy\nefficiency. Finally, numerical results are provided to verify the accuracy of\nthe analysis and also demonstrate the superior performance of the proposed\nH-NOMA scheme."}
{"id": "2507.10464", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10464", "abs": "https://arxiv.org/abs/2507.10464", "authors": ["Sarthak Yadav", "Sergios Theodoridis", "Zheng-Hua Tan"], "title": "AudioMAE++: learning better masked audio representations with SwiGLU FFNs", "comment": "TO APPEAR AT IEEE MLSP 2025", "summary": "Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged\nas a prominent approach for learning self-supervised audio representations.\nWhile several recent papers have evaluated key aspects of training MAEs on\naudio data, the majority of these approaches still leverage vanilla transformer\nbuilding blocks, whereas the transformer community has seen steady integration\nof newer architectural advancements. In this work, we propose AudioMAE++, a\nrevamped audio masked autoencoder with two such enhancements, namely\nmacaron-style transformer blocks with gated linear units. When pretrained on\nthe AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE\nbased approaches on 10 diverse downstream tasks, demonstrating excellent\nperformance on audio classification and speech-based benchmarks. The proposed\nAudioMAE++ models also demonstrate excellent scaling characteristics,\noutperforming directly comparable standard MAE baselines with up to 4x more\nparameters."}
{"id": "2507.10159", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10159", "abs": "https://arxiv.org/abs/2507.10159", "authors": ["Giovanni Bologni", "Richard Heusdens", "Richard C. Hendriks"], "title": "Cyclic Multichannel Wiener Filter for Acoustic Beamforming", "comment": "Comments: Accepted for publication at the 2025 IEEE Workshop on\n  Applications of Signal Processing to Audio and Acoustics (WASPAA 2025). IEEE\n  retains copyright", "summary": "Acoustic beamforming models typically assume wide-sense stationarity of\nspeech signals within short time frames. However, voiced speech is better\nmodeled as a cyclostationary (CS) process, a random process whose mean and\nautocorrelation are $T_1$-periodic, where $\\alpha_1=1/T_1$ corresponds to the\nfundamental frequency of vowels. Higher harmonic frequencies are found at\ninteger multiples of the fundamental. This work introduces a cyclic\nmultichannel Wiener filter (cMWF) for speech enhancement derived from a\ncyclostationary model. This beamformer exploits spectral correlation across the\nharmonic frequencies of the signal to further reduce the mean-squared error\n(MSE) between the target and the processed input. The proposed cMWF is optimal\nin the MSE sense and reduces to the MWF when the target is wide-sense\nstationary. Experiments on simulated data demonstrate considerable improvements\nin scale-invariant signal-to-distortion ratio (SI-SDR) on synthetic data but\nalso indicate high sensitivity to the accuracy of the estimated fundamental\nfrequency $\\alpha_1$, which limits effectiveness on real data."}
{"id": "2507.09535", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09535", "abs": "https://arxiv.org/abs/2507.09535", "authors": ["Chaoran Li", "Xingguo Xu", "Siyuan Mu"], "title": "Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs", "comment": null, "summary": "In the context of Synthetic Aperture Radar (SAR) image recognition,\ntraditional methods often struggle with the intrinsic limitations of SAR data,\nsuch as weak texture, high noise, and ambiguous object boundaries. This work\nexplores a novel perspective by reformulating SAR target recognition as a\nmultimodal reasoning task. We leverage multimodal large language models\n(MLLMs), specifically GPT-4o, to perform target classification based on SAR\nimagery, guided by candidate categories and enhanced with Chain-of-Thought\n(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,\ncomprising raw SAR images, structured target annotations, candidate label sets,\nand GPT-generated CoT reasoning chains. Experimental results show that the\nMLLMs are capable of generating logically coherent and interpretable inferences\nin most scenarios. Our analysis highlights both the strengths and current\nlimitations of MLLMs in interpreting SAR imagery, and we provide detailed\ninsights into model behavior through failure case analysis. This work\ndemonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines\nand establishes a foundation for future research in SAR-oriented visual\nreasoning."}
{"id": "2507.10534", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10534", "abs": "https://arxiv.org/abs/2507.10534", "authors": ["Qihui Yang", "Taylor Berg-Kirkpatrick", "Julian McAuley", "Zachary Novack"], "title": "WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling", "comment": null, "summary": "Despite rapid progress in end-to-end AI music generation, AI-driven modeling\nof professional Digital Signal Processing (DSP) workflows remains challenging.\nIn particular, while there is growing interest in neural black-box modeling of\naudio effect graphs (e.g. reverb, compression, equalization), AI-based\napproaches struggle to replicate the nuanced signal flow and parameter\ninteractions used in professional workflows. Existing differentiable plugin\napproaches often diverge from real-world tools, exhibiting inferior performance\nrelative to simplified neural controllers under equivalent computational\nconstraints. We introduce WildFX, a pipeline containerized with Docker for\ngenerating multi-track audio mixing datasets with rich effect graphs, powered\nby a professional Digital Audio Workstation (DAW) backend. WildFX supports\nseamless integration of cross-platform commercial plugins or any plugins in the\nwild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,\nsidechains, crossovers) and achieving efficient parallelized processing. A\nminimalist metadata interface simplifies project/plugin configuration.\nExperiments demonstrate the pipeline's validity through blind estimation of\nmixing graphs, plugin/gain parameters, and its ability to bridge AI research\nwith practical DSP demands. The code is available on:\nhttps://github.com/IsaacYQH/WildFX."}
{"id": "2507.10176", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10176", "abs": "https://arxiv.org/abs/2507.10176", "authors": ["Giovanni Bologni", "Richard Heusdens", "Richard C. Hendriks"], "title": "Harmonics to the Rescue: Why Voiced Speech is Not a Wss Process", "comment": "Comments: Accepted at the 2024 International Workshop on Acoustic\n  Signal Enhancement (IWAENC 2024)", "summary": "Speech processing algorithms often rely on statistical knowledge of the\nunderlying process. Despite many years of research, however, the debate on the\nmost appropriate statistical model for speech still continues. Speech is\ncommonly modeled as a wide-sense stationary (WSS) process. However, the use of\nthe WSS model for spectrally correlated processes is fundamentally wrong, as\nWSS implies spectral uncorrelation. In this paper, we demonstrate that voiced\nspeech can be more accurately represented as a cyclostationary (CS) process. By\nemploying the CS rather than the WSS model for processes that are inherently\ncorrelated across frequency, it is possible to improve the estimation of\ncross-power spectral densities (PSDs), source separation, and beamforming. We\nillustrate how the correlation between harmonic frequencies of CS processes can\nenhance system identification, and validate our findings using both simulated\nand real speech data."}
{"id": "2507.09561", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09561", "abs": "https://arxiv.org/abs/2507.09561", "authors": ["Can Wang", "Wei Liu", "Hanzhi Ma", "Xiaonan Jiang", "Erping Li", "Steven Gao"], "title": "Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This article presents a physics-aware convolutional long short-term memory\n(PC-LSTM) network for efficient and accurate extraction of mutual impedance\nmatrices in dipole antenna arrays. By reinterpreting the Green's function\nthrough a physics-aware neural network and embedding it into an adaptive loss\nfunction, the proposed machine learning-based approach achieves enhanced\nphysical interpretability in mutual coupling modeling. Also, an attention\nmechanism is carefully designed to calibrate complex-valued features by fusing\nthe real and imaginary parts of the Green's function matrix. These fused\nrepresentations are then processed by a convolutional long short-term memory\nnetwork, and the impedance matrix of the linear antenna array can be finally\nderived. Validation against five benchmarks underscores the efficacy of the\nproposed approach, demonstrating accurate impedance extraction with up to a 7x\nspeedup compared to CST Microwave Studio, making it a fast alternative to\nfull-wave simulations for mutual coupling characterization."}
{"id": "2507.09070", "categories": ["eess.AS", "cs.SD", "68T07", "I.2.7; I.2.6; G.3; H.5.5"], "pdf": "https://arxiv.org/pdf/2507.09070", "abs": "https://arxiv.org/abs/2507.09070", "authors": ["Shivam Mehta", "Yingru Liu", "Zhenyu Tang", "Kainan Peng", "Vimal Manohar", "Shun Zhang", "Mike Seltzer", "Qing He", "Mingbo Ma"], "title": "SemAlignVC: Enhancing zero-shot timbre conversion using semantic alignment", "comment": "6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop\n  (SSW) 2025", "summary": "Zero-shot voice conversion (VC) synthesizes speech in a target speaker's\nvoice while preserving linguistic and paralinguistic content. However, timbre\nleakage-where source speaker traits persist-remains a challenge, especially in\nneural codec and LLM-based VC, where quantized representations entangle speaker\nidentity with content. We introduce SemAlignVC, an architecture designed to\nprevent timbre leakage using SemAlign, a novel method that aligns text and\naudio representations to ensure speaker-independent semantic encoding. This\ndisentangled representation conditions an autoregressive transformer for\nhigh-fidelity conversion without explicit speaker embeddings. Experiments show\nSemAlignVC significantly reduces timbre leakage, outperforming baselines in\nspeaker timbre similarity, intelligibility, and naturalness, making it a\nrobust, privacy-preserving, and generalizable VC solution. Audio samples can be\naccessed at https://shivammehta25.github.io/SemAlignVC/"}
{"id": "2507.10200", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10200", "abs": "https://arxiv.org/abs/2507.10200", "authors": ["Stefano Bannò", "Rao Ma", "Mengjie Qian", "Siyuan Tang", "Kate Knill", "Mark Gales"], "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs", "comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)", "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors."}
{"id": "2507.09713", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09713", "abs": "https://arxiv.org/abs/2507.09713", "authors": ["Burak Ahmet Ozden", "Erdogan Aydin", "Ahmet Elbir", "Filiz Gurkan"], "title": "A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks", "comment": "17 pages, 14 figures", "summary": "With the development of wireless network technologies, the wireless image\ntransmission area has become prominent. The need for high resolution, data\ntraffic density, widespread use of multimedia applications, and the importance\nof high rate and reliable image transmission in medical and military fields\nnecessitate the design of novel and high-performance wireless image\ntransmission systems. This paper proposes a code index modulation (CIM)-based\nimage transmission (CIM-IT) system that utilizes spreading code index and\nquadrature amplitude modulation (QAM) symbol for image transmission over a\nwireless channel. The proposed CIM-IT system maps bits to each pixel value of\nthe image to be transmitted and transmits these bits over a wireless channel\nusing a single-input and multiple-output system comprising code index\nmodulation and QAM techniques. At the receiver, the active spreading code index\nand the selected QAM symbol are estimated using a despreading-based maximum\nlikelihood detector, and the corresponding bits are obtained. The image\nconveyed from the transmitter is then reconstructed at the receiver side using\nthe pixel values corresponding to the bits. The obtained noisy image is\nenhanced using important enhancement filters. In addition, an advanced filter\nis proposed to improve the transmitted degraded image with optimum results.\nFurthermore, error performance, spectral efficiency, energy efficiency, and\nthroughputof the CIM-IT system are performed and the results are compared with\ntraditional wireless communication techniques."}
{"id": "2507.09161", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09161", "abs": "https://arxiv.org/abs/2507.09161", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition", "comment": "Presented at Queen's University Biological Station Seminars of\n  Graduate Research in Ontario, Lake Shift Dissertation Camp (QUBS '25)", "summary": "Large language models have shown a remarkable ability to extract meaning from\nunstructured data, offering new ways to interpret biomedical signals beyond\ntraditional numerical methods. In this study, we present a matrix factorization\nframework for bioacoustic signal analysis which is enhanced by large language\nmodels. The focus is on separating bioacoustic signals that commonly overlap in\nclinical recordings, using matrix factorization to decompose the mixture into\ninterpretable components. A large language model is then applied to the\nseparated signals to associate distinct acoustic patterns with potential\nmedical conditions such as cardiac rhythm disturbances or respiratory\nabnormalities. Recordings were obtained from a digital stethoscope applied to a\nclinical manikin to ensure a controlled and high-fidelity acquisition\nenvironment. This hybrid approach does not require labeled data or prior\nknowledge of source types, and it provides a more interpretable and accessible\nframework for clinical decision support. The method demonstrates promise for\nintegration into future intelligent diagnostic tools."}
{"id": "2507.10264", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10264", "abs": "https://arxiv.org/abs/2507.10264", "authors": ["Takuya Fujimura", "Kevin Wilkinghoff", "Keisuke Imoto", "Tomoki Toda"], "title": "ASDKit: A Toolkit for Comprehensive Evaluation of Anomalous Sound Detection Methods", "comment": null, "summary": "In this paper, we introduce ASDKit, a toolkit for anomalous sound detection\n(ASD) task. Our aim is to facilitate ASD research by providing an open-source\nframework that collects and carefully evaluates various ASD methods. First,\nASDKit provides training and evaluation scripts for a wide range of ASD\nmethods, all handled within a unified framework. For instance, it includes the\nautoencoder-based official DCASE baseline, representative discriminative\nmethods, and self-supervised learning-based methods. Second, it supports\ncomprehensive evaluation on the DCASE 2020--2024 datasets, enabling careful\nassessment of ASD performance, which is highly sensitive to factors such as\ndatasets and random seeds. In our experiments, we re-evaluate various ASD\nmethods using ASDKit and identify consistently effective techniques across\nmultiple datasets and trials. We also demonstrate that ASDKit reproduces the\nstate-of-the-art-level performance on the considered datasets."}
{"id": "2507.09776", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09776", "abs": "https://arxiv.org/abs/2507.09776", "authors": ["Mihir Kavishwar", "Naresh Shanbhag"], "title": "Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing", "comment": "Code available at: https://github.com/mihirvk2/CSNR-optimal-ADC", "summary": "Analog in-memory computing (AIMC) is an energy-efficient alternative to\ndigital architectures for accelerating machine learning and signal processing\nworkloads. However, its energy efficiency is limited by the high energy cost of\nthe column analog-to-digital converters (ADCs). Reducing the ADC precision is\nan effective approach to lowering its energy cost. However, doing so also\nreduces the AIMC's computational accuracy thereby making it critical to\nidentify the minimum precision required to meet a target accuracy. Prior works\noverestimate the ADC precision requirements by modeling quantization error as\ninput-independent noise, maximizing the signal-to-quantization-noise ratio\n(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address\nthese limitations by developing analytical expressions for estimating the\ncompute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and\npropose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a\ncircuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we\nshow that for a 256-dimensional binary dot product, CACTUS reduces the ADC\nprecision requirements by 3b while achieving 6dB higher CSNR over prior\nmethods. We also delineate operating conditions under which our proposed\nCSNR-optimal ADCs outperform conventional SQNR-optimal ADCs."}
{"id": "2507.09226", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09226", "abs": "https://arxiv.org/abs/2507.09226", "authors": ["Shota Horiguchi", "Naohiro Tawara", "Takanori Ashihara", "Atsushi Ando", "Marc Delcroix"], "title": "Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?", "comment": null, "summary": "Neural speaker diarization is widely used for overlap-aware speaker\ndiarization, but it requires large multi-speaker datasets for training. To meet\nthis data requirement, large datasets are often constructed by combining\nmultiple corpora, including those originally designed for multi-speaker\nautomatic speech recognition (ASR). However, ASR datasets often feature loosely\ndefined segment boundaries that do not align with the stricter conventions of\ndiarization benchmarks. In this work, we show that such boundary looseness\nsignificantly impacts the diarization error rate, reducing evaluation\nreliability. We also reveal that models trained on data with varying boundary\nprecision tend to learn dataset-specific looseness, leading to poor\ngeneralization across out-of-domain datasets. Training with standardized tight\nboundaries via forced alignment improves not only diarization performance,\nespecially in streaming scenarios, but also ASR performance when combined with\nsimple post-processing."}
{"id": "2507.08882", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.5"], "pdf": "https://arxiv.org/pdf/2507.08882", "abs": "https://arxiv.org/abs/2507.08882", "authors": ["Janaki Viswanathan", "Alexander Blatt", "Konrad Hagemann", "Dietrich Klakow"], "title": "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers", "comment": "8 pages, 2 figures, 4 tables, publication identification number\n  (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online\n  publication- https://d-nb.info/127614606X/34 & Katalogeintrag:\n  https://d-nb.info/127614606X/", "summary": "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models."}
{"id": "2507.09894", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.09894", "abs": "https://arxiv.org/abs/2507.09894", "authors": ["Saif Khan Mohammed", "Amit Kumar Pathak", "Muhammad Ubadah", "Ronny Hadani", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Precoded Zak-OTFS for Per-Carrier Equalization", "comment": null, "summary": "In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform\nis a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic\nlocalized function with specific periods along delay and Doppler. When the\nchannel delay spread is less than the delay period, and the channel Doppler\nspread is less than the Doppler period, the response to a single Zak-OTFS\ncarrier provides an image of the scattering environment and can be used to\npredict the effective channel at all other carriers. The image of the\nscattering environment changes slowly, making it possible to employ precoding\nat the transmitter. Precoding techniques were developed more than thirty years\nago for wireline modem channels (V.34 standard) defined by linear convolution\nwhere a pulse in the time domain (TD) is used to probe the one-dimensional\npartial response channel. The action of a doubly spread channel on Zak-OTFS\nmodulation determines a two-dimensional partial response channel defined by\ntwisted convolution, and we develop a novel precoding technique for this\nchannel. The proposed precoder leads to separate equalization of each DD\ncarrier which has significantly lower complexity than joint equalization of all\ncarriers. Further, the effective precoded channel results in non-interfering DD\ncarriers which significantly reduces the overhead of guard carriers separating\ndata and pilot carriers, which improves the spectral efficiency significantly."}
{"id": "2507.09372", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09372", "abs": "https://arxiv.org/abs/2507.09372", "authors": ["Philippe Gonzalez", "Torsten Dau", "Tobias May"], "title": "Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model", "comment": "Accepted to Clarity 2025 Workshop", "summary": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference."}
{"id": "2507.09116", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09116", "abs": "https://arxiv.org/abs/2507.09116", "authors": ["Bingshen Mu", "Kun Wei", "Pengcheng Guo", "Lei Xie"], "title": "Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition", "comment": "IEEE Transactions on Audio, Speech and Language Processing", "summary": "Despite substantial improvements in ASR, performance tends to degrade when\nfaced with adverse conditions such as speaker accents. Generative error\ncorrection (GER) leverages the rich linguistic knowledge and exceptional\nreasoning ability of LLMs, significantly outperforming typical LM methods.\nHowever, it lacks specificity in accented speech scenarios. In this study, we\nleverage GER to improve the accuracy of transcription predictions by addressing\nthe two primary features of accented speech recognition. To fully leverage\npronunciation information, we propose the multi-modal GER, which integrates\npronunciation information from the speech modality, and the multi-granularity\nGER, which incorporates fine-grained phoneme-level information related to\npronunciation. These two methods enable the LLM to utilize the pronunciation\ninformation of accented speech and the semantic information from word-level\nhypotheses for accurate transcription predictions through LoRA fine-tuning. On\nthe one hand, we employ a three-stage training strategy to train separate\nmulti-modal GER models for each accent to obtain mono-accent LoRA experts. By\nadopting our proposed HDMoLE method, which incorporates hierarchical routing\nand dynamic thresholds within the mixture of LoRA experts, we effectively merge\nmultiple mono-accent LoRA experts within a single multi-modal GER to overcome\nthe challenges posed by accent diversity. On the other hand, multi-granularity\nGER leverages the N-best word-level and phoneme-level hypotheses generated by\nthe HDMoLE model to predict the final accented speech transcriptions.\nExperimental results on the multi-accent English dataset demonstrate the\nefficacy of our proposed methods. Our methods achieve a remarkable relative WER\nreduction of 67.35% compared to the Whisper-large-v3 baseline."}
{"id": "2507.09895", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09895", "abs": "https://arxiv.org/abs/2507.09895", "authors": ["Hyung-Joo Moon", "Chan-Byoung Chae", "Kai-Kit Wong", "Robert W. Heath Jr"], "title": "AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission", "comment": "7 pages, 6 figures, IEEE Communications Magazine (under revision)", "summary": "Massive Aerial Processing for X MAP-X is an innovative framework for\nreconstructing spatially correlated ground data, such as environmental or\nindustrial measurements distributed across a wide area, into data maps using a\nsingle high altitude pseudo-satellite (HAPS) and a large number of distributed\nsensors. With subframe-level data reconstruction, MAP-X provides a\ntransformative solution for latency-sensitive IoT applications. This article\nexplores two distinct approaches for AI integration in the post-processing\nstage of MAP-X. The DNN-based pointwise estimation approach enables real-time,\nadaptive reconstruction through online training, while the CNN-based image\nreconstruction approach improves reconstruction accuracy through offline\ntraining with non-real-time data. Simulation results show that both approaches\nsignificantly outperform the conventional inverse discrete Fourier transform\n(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced\nMAP-X, we propose a ground-HAPS cooperation framework, where terrestrial\nstations collect, process, and relay training data to the HAPS. With its\nenhanced capability in reconstructing field data, AI-enhanced MAP-X is\napplicable to various real-world use cases, including disaster response and\nnetwork management."}
{"id": "2507.09499", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09499", "abs": "https://arxiv.org/abs/2507.09499", "authors": ["Yuke Lin", "Ming Cheng", "Ze Li", "Ming Li"], "title": "The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM Challenge", "comment": "Technical Report for MLC-SLM Challenge in Interspeech2025", "summary": "We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to\nperform multi-speaker automatic speech recognition directly from raw audio\nwithout Oracle speaker labels or time boundaries. Our approach builds upon a\ndiarization-aware framework integrating speaker embeddings and temporal\nutterance boundaries into a Qwen2.5-based large language model (LLM). Then, we\nenhance the system's multilingual performance by fine-tuning language-specific\nadapters and LoRA modules within the LLM decoder. Finally, our system achieves\nthe tcpWER of 23.56\\% and 18.08\\% on the development and test sets of the\nMLC-SLM dataset, substantially outperforming the official baseline."}
{"id": "2507.09195", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09195", "abs": "https://arxiv.org/abs/2507.09195", "authors": ["Parthasaarathy Sudarsanam", "Archontis Politis"], "title": "Towards Spatial Audio Understanding via Question Answering", "comment": null, "summary": "In this paper, we introduce a novel framework for spatial audio understanding\nof first-order ambisonic (FOA) signals through a question answering (QA)\nparadigm, aiming to extend the scope of sound event localization and detection\n(SELD) towards spatial scene understanding and reasoning. First, we curate and\nrelease fine-grained spatio-temporal textual descriptions for the STARSS23\ndataset using a rule-based approach, and further enhance linguistic diversity\nusing large language model (LLM)-based rephrasing. We also introduce a QA\ndataset aligned with the STARSS23 scenes, covering various aspects such as\nevent presence, localization, spatial, and temporal relationships. To increase\nlanguage variety, we again leverage LLMs to generate multiple rephrasings per\nquestion. Finally, we develop a baseline spatial audio QA model that takes FOA\nsignals and natural language questions as input and provides answers regarding\nvarious occurrences, temporal, and spatial relationships of sound events in the\nscene formulated as a classification task. Despite being trained solely with\nscene-level question answering supervision, our model achieves performance that\nis comparable to a fully supervised sound event localization and detection\nmodel trained with frame-level spatiotemporal annotations. The results\nhighlight the potential of language-guided approaches for spatial audio\nunderstanding and open new directions for integrating linguistic supervision\ninto spatial scene analysis."}
{"id": "2507.09987", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09987", "abs": "https://arxiv.org/abs/2507.09987", "authors": ["Zihang Zeng", "Shu Sun", "Meixia Tao", "Yin Xu", "Xianghao Yu"], "title": "VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling", "comment": null, "summary": "Wireless channel modeling in complex environments is crucial for wireless\ncommunication system design and deployment. Traditional channel modeling\napproaches face challenges in balancing accuracy, efficiency, and scalability,\nwhile recent neural approaches such as neural radiance field (NeRF) suffer from\nlong training and slow inference. To tackle these challenges, we propose\nvoxelized radiance field (VoxelRF), a novel neural representation for wireless\nchannel modeling that enables fast and accurate synthesis of spatial spectra.\nVoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based\nmethods with trilinear interpolation of voxel grid-based representation, and\ntwo shallow MLPs to model both propagation and transmitter-dependent effects.\nTo further accelerate training and improve generalization, we introduce\nprogressive learning, empty space skipping, and an additional background\nentropy loss function. Experimental results demonstrate that VoxelRF achieves\ncompetitive accuracy with significantly reduced computation and limited\ntraining data, making it more practical for real-time and resource-constrained\nwireless applications."}
{"id": "2507.09570", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09570", "abs": "https://arxiv.org/abs/2507.09570", "authors": ["Wenmiao Gao", "Han Yin"], "title": "Enhancing Stereo Sound Event Detection with BiMamba and Pretrained PSELDnet", "comment": null, "summary": "Pre-training methods have greatly improved the performance of sound event\nlocalization and detection (SELD). However, existing Transformer-based models\nstill face high computational cost. To solve this problem, we present a stereo\nSELD system using a pre-trained PSELDnet and a bidirectional Mamba sequence\nmodel. Specifically, we replace the Conformer module with a BiMamba module. We\nalso use asymmetric convolutions to better capture the time and frequency\nrelationships in the audio signal. Test results on the DCASE2025 Task 3\ndevelopment dataset show that our method performs better than both the baseline\nand the original PSELDnet with a Conformer decoder. In addition, the proposed\nmodel costs fewer computing resources than the baselines. These results show\nthat the BiMamba architecture is effective for solving key challenges in SELD\ntasks. The source code is publicly accessible at https://github.com/\nalexandergwm/DCASE2025 TASK3 Stereo PSELD Mamba."}
{"id": "2507.09310", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09310", "abs": "https://arxiv.org/abs/2507.09310", "authors": ["Dominika Woszczyk", "Manuel Sam Ribeiro", "Thomas Merritt", "Daniel Korzekwa"], "title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "comment": "Presented at Clarity Challenge 2023", "summary": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity."}
{"id": "2507.09999", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09999", "abs": "https://arxiv.org/abs/2507.09999", "authors": ["Lital Dabush", "Nir Shlezinger", "Tirza Routtenberg"], "title": "Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A broad range of applications involve signals with irregular structures that\ncan be represented as a graph. As the underlying structures can change over\ntime, the tracking dynamic graph topologies from observed signals is a\nfundamental challenge in graph signal processing (GSP), with applications in\nvarious domains, such as power systems, the brain-machine interface, and\ncommunication systems. In this paper, we propose a method for tracking dynamic\nchanges in graph topologies. Our approach builds on a representation of the\ndynamics as a graph-based nonlinear state-space model (SSM), where the\nobservations are graph signals generated through graph filtering, and the\nunderlying evolving topology serves as the latent states. In our formulation,\nthe graph Laplacian matrix is parameterized using the incidence matrix and edge\nweights, enabling a structured representation of the state. In order to track\nthe evolving topology in the resulting SSM, we develop a sparsity-aware\nextended Kalman filter (EKF) that integrates $\\ell_1$-regularized updates\nwithin the filtering process. Furthermore, a dynamic programming scheme to\nefficiently compute the Jacobian of the graph filter is introduced. Our\nnumerical study demonstrates the ability of the proposed method to accurately\ntrack sparse and time-varying graphs under realistic conditions, with highly\nnonlinear measurements, various noise levels, and different change rates, while\nmaintaining low computational complexity."}
{"id": "2507.09806", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09806", "abs": "https://arxiv.org/abs/2507.09806", "authors": ["Mirco Pezzoli", "Federico Miotello", "Shoichi Koyama", "Fabio Antonacci"], "title": "Low-Rank Adaptation of Deep Prior Neural Networks For Room Impulse Response Reconstruction", "comment": "to appear in IEEE WASPAA", "summary": "The Deep Prior framework has emerged as a powerful generative tool which can\nbe used for reconstructing sound fields in an environment from few sparse\npressure measurements. It employs a neural network that is trained solely on a\nlimited set of available data and acts as an implicit prior which guides the\nsolution of the underlying optimization problem. However, a significant\nlimitation of the Deep Prior approach is its inability to generalize to new\nacoustic configurations, such as changes in the position of a sound source. As\na consequence, the network must be retrained from scratch for every new setup,\nwhich is both computationally intensive and time-consuming. To address this, we\ninvestigate transfer learning in Deep Prior via Low-Rank Adaptation (LoRA),\nwhich enables efficient fine-tuning of a pre-trained neural network by\nintroducing a low-rank decomposition of trainable parameters, thus allowing the\nnetwork to adapt to new measurement sets with minimal computational overhead.\nWe embed LoRA into a MultiResUNet-based Deep Prior model and compare its\nadaptation performance against full fine-tuning of all parameters as well as\nclassical retraining, particularly in scenarios where only a limited number of\nmicrophones are used. The results indicate that fine-tuning, whether done\ncompletely or via LoRA, is especially advantageous when the source location is\nthe sole changing parameter, preserving high physical fidelity, and\nhighlighting the value of transfer learning for acoustics applications."}
{"id": "2507.09342", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09342", "abs": "https://arxiv.org/abs/2507.09342", "authors": ["Emmanuel Adetiba", "Abdultaofeek Abayomi", "Raymond J. Kala", "Ayodele H. Ifijeh", "Oluwatobi E. Dare", "Olabode Idowu-Bismark", "Gabriel O. Sobola", "Joy N. Adetiba", "Monsurat Adepeju Lateef", "Heather Cole-Lewis"], "title": "BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus", "comment": null, "summary": "There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for\nhigh resource-to-low resource language pairs such as English-to-Yoruba. Thus,\nin this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech\nTranslation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a\nhybrid architecture we developed for large-scale direct S2ST corpus creation at\nreduced cost. To achieve this, we leveraged non speech-to-speech Standard\nYoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as\nthe corresponding Standard English (SE) transcripts. YORULECT Corpus is small\nscale(1,504) samples, and it does not have paired English audios. Therefore, we\ngenerated the SE audios using pre-trained AI models (i.e. Facebook MMS). We\nalso developed an audio augmentation algorithm named AcoustAug based on three\nlatent acoustic features to generate augmented audios from the raw audios of\nthe two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,\nwhich gives a total of 24,064 sample size. The total audio duration for the two\nlanguages is 41.20 hours. This size is quite significant. Beyond building S2ST\nmodels, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve\nexisting ones. The created corpus and Coqui framework were used to build a\npretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The\nYoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates\nmoderate fundamental pitch similarity with the reference real-time audio.\nUltimately, the corpus architecture in this study can be leveraged by\nresearchers and developers to curate datasets for multilingual\nhigh-resource-to-low-resource African languages. This will bridge the huge\ndigital divides in translations among high and low-resource language pairs.\nBENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at\n(https://bit.ly/40bGMwi)."}
{"id": "2507.10063", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10063", "abs": "https://arxiv.org/abs/2507.10063", "authors": ["Hongpu Zhang", "Shu Sun", "Hangsong Yan", "Jianhua Mo"], "title": "Deep Learning-Based Beamforming Design Using Target Beam Patterns", "comment": "6 pages, 5 figures", "summary": "This paper proposes a deep learning-based beamforming design framework that\ndirectly maps a target beam pattern to optimal beamforming vectors across\nmultiple antenna array architectures, including digital, analog, and hybrid\nbeamforming. The proposed method employs a lightweight encoder-decoder network\nwhere the encoder compresses the complex beam pattern into a low-dimensional\nfeature vector and the decoder reconstructs the beamforming vector while\nsatisfying hardware constraints. To address training challenges under diverse\nand limited channel station information (CSI) conditions, a two-stage training\nprocess is introduced, which consists of an offline pre-training for robust\nfeature extraction using an auxiliary module, followed by online training of\nthe decoder with a composite loss function that ensures alignment between the\nsynthesized and target beam patterns in terms of the main lobe shape and side\nlobe suppression. Simulation results based on NYUSIM-generated channels show\nthat the proposed method can achieve spectral efficiency close to that of fully\ndigital beamforming under limited CSI and outperforms representative existing\nmethods."}
{"id": "2507.09834", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.09834", "abs": "https://arxiv.org/abs/2507.09834", "authors": ["Shu-wen Yang", "Byeonggeun Kim", "Kuan-Po Huang", "Qingming Tang", "Huy Phan", "Bo-Ru Lu", "Harsha Sundar", "Shalini Ghosh", "Hung-yi Lee", "Chieh-Chi Kao", "Chao Wang"], "title": "Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction", "comment": "Accepted by ICML 2025. Project website: https://audiomntp.github.io/", "summary": "Autoregressive next-token prediction with the Transformer decoder has become\na de facto standard in large language models (LLMs), achieving remarkable\nsuccess in Natural Language Processing (NLP) at scale. Extending this paradigm\nto audio poses unique challenges due to its inherently continuous nature. We\nresearch audio generation with a causal language model (LM) without discrete\ntokens. We leverage token-wise diffusion to model the continuous distribution\nof the next continuous-valued token. Our approach delivers significant\nimprovements over previous discrete solution, AudioGen, achieving 20% and 40%\nrelative gains on AudioCaps in Frechet Audio Distance (FAD) and\nKullback-Leibler (KL) divergence, respectively. Additionally, we propose a\nnovel masked next-token prediction task that incorporates masked prediction\ninto the causal LM framework. On AudioCaps, the innovation yields 41% and 33%\nrelative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)\nmodels, respectively, and is on par with the state-of-the-art (SOTA) diffusion\nmodels. Furthermore, we achieve these results with significantly fewer\nparameters -- 193M for our Base and 462M for our Large models."}
{"id": "2507.09376", "categories": ["cs.SD", "cs.HC", "cs.MM", "eess.AS", "H.5.5"], "pdf": "https://arxiv.org/pdf/2507.09376", "abs": "https://arxiv.org/abs/2507.09376", "authors": ["Bilkent Samsurya"], "title": "Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering", "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "summary": "Accurate sound propagation simulation is essential for delivering immersive\nexperiences in virtual applications, yet industry methods for acoustic modeling\noften do not account for the full breadth of acoustic wave phenomena. This\npaper proposes a novel two-dimensional (2D) finite-difference time-domain\n(FDTD) framework that simulates sound propagation as a wave-based model in\nUnreal Engine, with an emphasis on capturing lower frequency wave phenomena,\nembedding occlusion, diffraction, reflection and interference in generated\nimpulse responses. The process begins by discretizing the scene geometry into a\n2D grid via a top-down projection from which obstacle masks and boundary\nconditions are derived. A Python-based FDTD solver injects a sine sweep at a\nsource position, and virtual quadraphonic microphone arrays record pressure\nfield responses at pre-defined listener positions. De-convolution of the\npressure responses yields multi-channel impulse responses that retain spatial\ndirectionality which are then integrated into Unreal Engine's audio pipeline\nfor dynamic playback. Benchmark tests confirm agreement with analytical\nexpectations, and the paper outlines hybrid extensions aimed at commercial\nviability."}
{"id": "2507.10145", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.10145", "abs": "https://arxiv.org/abs/2507.10145", "authors": ["Ryohei Fukuma", "Yoshinobu Kawahara", "Okito Yamashita", "Kei Majima", "Haruhiko Kishima", "Takufumi Yanagisawa"], "title": "Intrinsic frequency distribution characterises neural dynamics", "comment": null, "summary": "Decomposing multivariate time series with certain basic dynamics is crucial\nfor understanding, predicting and controlling nonlinear spatiotemporally\ndynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method\nfor decomposing nonlinear spatiotemporal dynamics into several basic dynamics\n(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,\nunlike Fourier transform-based methods, which are used to decompose a\nsingle-channel signal into the amplitudes of sinusoidal waves with discrete\nfrequencies at a regular interval, DMD can derive the intrinsic frequencies of\na multichannel signal on the basis of the available data; furthermore, it can\ncapture nonstationary components such as alternations between states with\ndifferent intrinsic frequencies. Here, we propose the use of the distribution\nof intrinsic frequencies derived from DMDs (DM frequencies) to characterise\nneural activities. The distributions of DM frequencies in the\nelectroencephalograms of healthy subjects and patients with dementia or\nParkinson's disease in a resting state were evaluated. By using the\ndistributions, these patients were distinguished from healthy subjects with\nsignificantly greater accuracy than when using amplitude spectra derived by\ndiscrete Fourier transform. This finding suggests that the distribution of DM\nfrequencies exhibits distinct behaviour from amplitude spectra, and therefore,\nthe distribution may serve as a new biomarker by characterising the nonlinear\nspatiotemporal dynamics of electrophysiological signals."}
{"id": "2507.09510", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09510", "abs": "https://arxiv.org/abs/2507.09510", "authors": ["Shu Wu", "Anbin Qi", "Yanzhang Xie", "Xiang Xie"], "title": "SC-TSE: Speaker Consistency-Aware Target Speaker Extraction", "comment": "Accept to Interspeech2025", "summary": "Target Speaker Extraction (TSE) uses a reference cue to extract the target\nspeech from a mixture. In TSE systems relying on audio cues, the speaker\nembedding from the enrolled speech is crucial to performance. However, these\nembeddings may suffer from speaker identity confusion. Unlike previous studies\nthat focus on improving speaker embedding extraction, we improve TSE\nperformance from the perspective of speaker consistency. In this paper, we\npropose a speaker consistency-aware target speaker extraction method that\nincorporates a centroid-based speaker consistency loss. This approach enhances\nTSE performance by ensuring speaker consistency between the enrolled and\nextracted speech. In addition, we integrate conditional loss suppression into\nthe training process. The experimental results validate the effectiveness of\nour proposed methods in advancing the TSE performance. A speech demo is\navailable online.\\footnote{https://sc-tse.netlify.app/"}
{"id": "2507.10167", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10167", "abs": "https://arxiv.org/abs/2507.10167", "authors": ["Kaidi Wang", "Zhiguo Ding", "Naofal Al-Dhahir"], "title": "Pinching-Antenna Systems for Physical Layer Security", "comment": null, "summary": "This letter investigates the potential of pinching-antenna systems for\nenhancing physical layer security. By pre-installing multiple pinching antennas\nat discrete positions along a waveguide, the capability of the considered\nsystem to perform amplitude and phase adjustment is validated through the\nformulation of a secrecy rate maximization problem. Specifically, amplitude\ncontrol is applied to enhance the signal quality at the legitimate user, while\nphase alignment is designed to degrade the received signal quality at the\neavesdropper. This cooperation among pinching antennas is modeled as a\ncoalitional game, and a corresponding antenna activation algorithm is proposed.\nThe individual impact of each antenna is quantified based on the Shapley value\nand marginal contribution, providing a fair and efficient method for\nperformance evaluation. Simulation results show that the considered\npinching-antenna system achieves significant improvements in secrecy rate, and\nthat the Shapley value based algorithm outperforms conventional coalition value\nbased solutions."}
{"id": "2507.09606", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09606", "abs": "https://arxiv.org/abs/2507.09606", "authors": ["Yuanjian Chen", "Han Yin"], "title": "Ensemble Confidence Calibration for Sound Event Detection in Open-environment", "comment": null, "summary": "Sound event detection (SED) has made strong progress in controlled\nenvironments with clear event categories. However, real-world applications\noften take place in open environments. In such cases, current methods often\nproduce predictions with too much confidence and lack proper ways to measure\nuncertainty. This limits their ability to adapt and perform well in new\nsituations. To solve this problem, we are the first to use ensemble methods in\nSED to improve robustness against out-of-domain (OOD) inputs. We propose a\nconfidence calibration method called Energy-based Open-World Softmax\n(EOW-Softmax), which helps the system better handle uncertainty in unknown\nscenes. We further apply EOW-Softmax to sound occurrence and overlap detection\n(SOD) by adjusting the prediction. In this way, the model becomes more\nadaptable while keeping its ability to detect overlapping events. Experiments\nshow that our method improves performance in open environments. It reduces\noverconfidence and increases the ability to handle OOD situations."}
{"id": "2507.10173", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10173", "abs": "https://arxiv.org/abs/2507.10173", "authors": ["Kaidi Wang", "Chongjun Ouyang", "Yuanwei Liu", "Zhiguo Ding"], "title": "Pinching-Antenna Systems with LoS Blockages", "comment": null, "summary": "The aim of this letter is to explore the capability of pinching-antenna\nsystems to construct line-of-sight (LoS) links in the presence of LoS\nblockages. Specifically, pinching antennas are pre-installed at preconfigured\npositions along waveguides and can be selectively activated to create LoS links\nfor enhancing desired signals and non-line-of-sight (NLoS) links for\neliminating inter-user interference. On this basis, a sum-rate maximization\nproblem is formulated by jointly optimizing waveguide assignment and antenna\nactivation. To solve this problem, a matching based algorithm is proposed using\ntwo distinct preference designs. Simulation results demonstrate that the\nconsidered pinching-antenna system and proposed solutions can dynamically\nestablish LoS links and effectively exploit LoS blockages to mitigate\ninterference, thereby significantly improving system throughput."}
{"id": "2507.09618", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09618", "abs": "https://arxiv.org/abs/2507.09618", "authors": ["Jilamika Wongpithayadisai", "Chompakorn Chaksangchaichot", "Soravitt Sangnark", "Patawee Prakrankamanant", "Krit Gangwanpongpun", "Siwa Boonpunmongkol", "Premmarin Milindasuta", "Dangkamon Na-Pombejra", "Sarana Nutanong", "Ekapol Chuangsuwanich"], "title": "THAI Speech Emotion Recognition (THAI-SER) corpus", "comment": null, "summary": "We present the first sizeable corpus of Thai speech emotion recognition,\nTHAI-SER, containing 41 hours and 36 minutes (27,854 utterances) from 100\nrecordings made in different recording environments: Zoom and two studio\nsetups. The recordings contain both scripted and improvised sessions, acted by\n200 professional actors (112 females and 88 males, aged 18 to 55) and were\ndirected by professional directors. There are five primary emotions: neutral,\nangry, happy, sad, and frustrated, assigned to the actors when recording\nutterances. The utterances are annotated with an emotional category using\ncrowdsourcing. To control the annotation process's quality, we also design an\nextensive filtering and quality control scheme to ensure that the majority\nagreement score remains above 0.71. We evaluate our annotated corpus using two\nmetrics: inter-annotator reliability and human recognition accuracy.\nInter-annotator reliability score was calculated using Krippendorff's alpha,\nwhere our corpus, after filtering, achieved an alpha score of 0.692, higher\nthan a recommendation of 0.667. For human recognition accuracy, our corpus\nscored up to 0.772 post-filtering. We also provide the results of the model\ntrained on the corpus evaluated on both in-corpus and cross-corpus setups. The\ncorpus is publicly available under a Creative Commons BY-SA 4.0, as well as our\ncodes for the experiments."}
{"id": "2507.10308", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10308", "abs": "https://arxiv.org/abs/2507.10308", "authors": ["Hung Nguyen-Kha", "Vu Nguyen Ha", "Eva Lagunas", "Symeon Chatzinotas", "Joel Grotz"], "title": "Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks", "comment": "ACCEPTED FOR PUBLICATION IN IEEE TRANSACTIONS ON COMMUNICATIONS", "summary": "This paper investigates downlink transmission in 5G Integrated\nSatellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in\nurban environments, where base stations (BSs) and Low Earth Orbit (LEO)\nsatellites (LSats) cooperate to serve moving UEs over shared C-band frequency\ncarriers. Urban settings, characterized by dense obstructions, together with UE\nmobility, and the dynamic movement and coverage of LSats pose significant\nchallenges to user association and resource allocation. To address these\nchallenges, we formulate a multi-objective optimization problem designed to\nimprove both throughput and seamless handover (HO). Particularly, the\nformulated problem balances sum-rate (SR) maximization and connection change\n(CC) minimization through a weighted trade-off by jointly optimizing power\nallocation and BS-UE/LSat-UE associations over a given time window. This is a\nmixed-integer and non-convex problem which is inherently difficult to solve. To\nsolve this problem efficiently, we propose an iterative algorithm based on the\nSuccessive Convex Approximation (SCA) technique. Furthermore, we introduce a\npractical prediction-based algorithm capable of providing efficient solutions\nin real-world implementations. Especially, the simulations use a realistic 3D\nmap of London and UE routes obtained from the Google Navigator application to\nensure practical examination. Thanks to these realistic data, the simulation\nresults can show valuable insights into the link budget assessment in urban\nareas due to the impact of buildings on transmission links under the blockage,\nreflection, and diffraction effects. Furthermore, the numerical results\ndemonstrate the effectiveness of our proposed algorithms in terms of SR and the\nCC-number compared to the greedy and benchmark algorithms."}
{"id": "2507.09750", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09750", "abs": "https://arxiv.org/abs/2507.09750", "authors": ["Enric Gusó", "Joanna Luberadzka", "Umut Sayin", "Xavier Serra"], "title": "MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients", "comment": "Accepted to WASPAA25", "summary": "We investigate the effects of four strategies for improving the ecological\nvalidity of synthetic room impulse response (RIR) datasets for monoaural Speech\nEnhancement (SE). We implement three features on top of the traditional image\nsource method-based (ISM) shoebox RIRs: multiband absorption coefficients,\nsource directivity and receiver directivity. We additionally consider\nmesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3\nmodel for each RIR dataset and evaluate the performance on a test set of real\nRIRs both objectively and subjectively. We find that RIRs which use\nfrequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain\n+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs\ndataset is publicly available for free download."}
{"id": "2507.10159", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10159", "abs": "https://arxiv.org/abs/2507.10159", "authors": ["Giovanni Bologni", "Richard Heusdens", "Richard C. Hendriks"], "title": "Cyclic Multichannel Wiener Filter for Acoustic Beamforming", "comment": "Comments: Accepted for publication at the 2025 IEEE Workshop on\n  Applications of Signal Processing to Audio and Acoustics (WASPAA 2025). IEEE\n  retains copyright", "summary": "Acoustic beamforming models typically assume wide-sense stationarity of\nspeech signals within short time frames. However, voiced speech is better\nmodeled as a cyclostationary (CS) process, a random process whose mean and\nautocorrelation are $T_1$-periodic, where $\\alpha_1=1/T_1$ corresponds to the\nfundamental frequency of vowels. Higher harmonic frequencies are found at\ninteger multiples of the fundamental. This work introduces a cyclic\nmultichannel Wiener filter (cMWF) for speech enhancement derived from a\ncyclostationary model. This beamformer exploits spectral correlation across the\nharmonic frequencies of the signal to further reduce the mean-squared error\n(MSE) between the target and the processed input. The proposed cMWF is optimal\nin the MSE sense and reduces to the MWF when the target is wide-sense\nstationary. Experiments on simulated data demonstrate considerable improvements\nin scale-invariant signal-to-distortion ratio (SI-SDR) on synthetic data but\nalso indicate high sensitivity to the accuracy of the estimated fundamental\nfrequency $\\alpha_1$, which limits effectiveness on real data."}
{"id": "2507.09904", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09904", "abs": "https://arxiv.org/abs/2507.09904", "authors": ["Fabian Ritter-Gutierrez", "Yi-Cheng Lin", "Jui-Chiang Wei", "Jeremy H. M. Wong", "Nancy F. Chen", "Hung-yi Lee"], "title": "ASTAR-NTU solution to AudioMOS Challenge 2025 Track1", "comment": "Under Review - Submitted to AudioMOS Challenge 2025 - ASRU 2025", "summary": "Evaluation of text-to-music systems is constrained by the cost and\navailability of collecting experts for assessment. AudioMOS 2025 Challenge\ntrack 1 is created to automatically predict music impression (MI) as well as\ntext alignment (TA) between the prompt and the generated musical piece. This\npaper reports our winning system, which uses a dual-branch architecture with\npre-trained MuQ and RoBERTa models as audio and text encoders. A\ncross-attention mechanism fuses the audio and text representations. For\ntraining, we reframe the MI and TA prediction as a classification task. To\nincorporate the ordinal nature of MOS scores, one-hot labels are converted to a\nsoft distribution using a Gaussian kernel. On the official test set, a single\nmodel trained with this method achieves a system-level Spearman's Rank\nCorrelation Coefficient (SRCC) of 0.991 for MI and 0.952 for TA, corresponding\nto a relative improvement of 21.21\\% in MI SRCC and 31.47\\% in TA SRCC over the\nchallenge baseline."}
{"id": "2507.10176", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10176", "abs": "https://arxiv.org/abs/2507.10176", "authors": ["Giovanni Bologni", "Richard Heusdens", "Richard C. Hendriks"], "title": "Harmonics to the Rescue: Why Voiced Speech is Not a Wss Process", "comment": "Comments: Accepted at the 2024 International Workshop on Acoustic\n  Signal Enhancement (IWAENC 2024)", "summary": "Speech processing algorithms often rely on statistical knowledge of the\nunderlying process. Despite many years of research, however, the debate on the\nmost appropriate statistical model for speech still continues. Speech is\ncommonly modeled as a wide-sense stationary (WSS) process. However, the use of\nthe WSS model for spectrally correlated processes is fundamentally wrong, as\nWSS implies spectral uncorrelation. In this paper, we demonstrate that voiced\nspeech can be more accurately represented as a cyclostationary (CS) process. By\nemploying the CS rather than the WSS model for processes that are inherently\ncorrelated across frequency, it is possible to improve the estimation of\ncross-power spectral densities (PSDs), source separation, and beamforming. We\nillustrate how the correlation between harmonic frequencies of CS processes can\nenhance system identification, and validate our findings using both simulated\nand real speech data."}
{"id": "2507.10313", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10313", "abs": "https://arxiv.org/abs/2507.10313", "authors": ["Yiru Yang"], "title": "DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation", "comment": null, "summary": "We present a demo of DQLoRA, an Adapter-Guided Distillation framework for\nrobust speech recognition under low-resource and noisy conditions. Our method\nemploys a frozen Whisper model as the teacher to provide semantic supervision,\nand a lightweight Wav2Vec2 student equipped with QLoRA-based Adapters. Training\nis conducted on the FLEURS dataset augmented with DNS-style noise. The student\nis optimized by jointly minimizing CTC loss and KL-based distillation loss,\nenabling efficient adaptation while preserving recognition accuracy."}
{"id": "2507.10447", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10447", "abs": "https://arxiv.org/abs/2507.10447", "authors": ["Tomasz Sroka", "Tomasz Wężowicz", "Dominik Sidorczuk", "Mateusz Modrzejewski"], "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations", "comment": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure", "summary": "With the rapid advancement of generative audio models, distinguishing between\nhuman-composed and generated music is becoming increasingly challenging. As a\nresponse, models for detecting fake music have been proposed. In this work, we\nexplore the robustness of such systems under audio augmentations. To evaluate\nmodel generalization, we constructed a dataset consisting of both real and\nsynthetic music generated using several systems. We then apply a range of audio\ntransformations and analyze how they affect classification accuracy. We test\nthe performance of a recent state-of-the-art musical deepfake detection model\nin the presence of audio augmentations. The performance of the model decreases\nsignificantly even with the introduction of light augmentations."}
{"id": "2507.10456", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10456", "abs": "https://arxiv.org/abs/2507.10456", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "title": "Radif corpus: a symbolic dataset for non-metric iranian classical music", "comment": null, "summary": "Non-metric music forms the core of the repertoire in Iranian classical music.\nDastgahi music serves as the underlying theoretical system for both Iranian art\nmusic and certain folk traditions. At the heart of Iranian classical music lies\nthe radif, a foundational repertoire that organizes melodic material central to\nperformance and pedagogy.\n  In this study, we introduce the first digital corpus representing the\ncomplete non-metrical radif repertoire, covering all 13 existing components of\nthis repertoire. We provide MIDI files (about 281 minutes in total) and data\nspreadsheets describing notes, note durations, intervals, and hierarchical\nstructures for 228 pieces of music. We faithfully represent the tonality\nincluding quarter-tones, and the non-metric aspect. Furthermore, we provide\nsupporting basic statistics, and measures of complexity and similarity over the\ncorpus.\n  Our corpus provides a platform for computational studies of Iranian classical\nmusic. Researchers might employ it in studying melodic patterns, investigating\nimprovisational styles, or for other tasks in music information retrieval,\nmusic theory, and computational (ethno)musicology."}
{"id": "2507.10464", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10464", "abs": "https://arxiv.org/abs/2507.10464", "authors": ["Sarthak Yadav", "Sergios Theodoridis", "Zheng-Hua Tan"], "title": "AudioMAE++: learning better masked audio representations with SwiGLU FFNs", "comment": "TO APPEAR AT IEEE MLSP 2025", "summary": "Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged\nas a prominent approach for learning self-supervised audio representations.\nWhile several recent papers have evaluated key aspects of training MAEs on\naudio data, the majority of these approaches still leverage vanilla transformer\nbuilding blocks, whereas the transformer community has seen steady integration\nof newer architectural advancements. In this work, we propose AudioMAE++, a\nrevamped audio masked autoencoder with two such enhancements, namely\nmacaron-style transformer blocks with gated linear units. When pretrained on\nthe AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE\nbased approaches on 10 diverse downstream tasks, demonstrating excellent\nperformance on audio classification and speech-based benchmarks. The proposed\nAudioMAE++ models also demonstrate excellent scaling characteristics,\noutperforming directly comparable standard MAE baselines with up to 4x more\nparameters."}

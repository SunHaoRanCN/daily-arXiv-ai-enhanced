<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 24]
- [eess.AS](#eess.AS) [Total: 15]
- [cs.SD](#cs.SD) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 将流体天线系统的端口选择问题建模为多标签分类任务，利用液态神经网络（LNNs）优化最佳端口选择，并通过超参数优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在有限端口观测条件下优化信号干扰噪声比的问题。

Method: 将端口选择问题建模为多标签分类任务，使用液态神经网络（LNNs）进行预测，并应用超参数优化调整网络结构。

Result: 提出的方法在中断概率上优于现有方法。

Conclusion: 通过液态神经网络和超参数优化，流体天线系统的端口选择性能得到显著提升。

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


### [2] [Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems](https://arxiv.org/abs/2507.08950)
*Xue Zhang,Abla Kammoun,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 研究了大规模MIMO系统中半盲信道估计的确定性及随机Cramér-Rao界（CRB）的渐近行为，发现CRB可随传输块长度增加而任意减小，但需训练序列长度同步增长且用户数固定。


<details>
  <summary>Details</summary>
Motivation: 探索半盲信道估计在大规模MIMO系统中的性能极限，以优化训练序列需求。

Method: 推导并分析不同渐近条件下CRB的数学表达式，涵盖天线数、用户数、训练序列长度及传输块长度的增长关系。

Result: CRB可随传输块长度增加而无限减小，但需训练序列同步增长且用户数固定；否则，信道估计误差存在非零下限。

Conclusion: 半盲信道估计能显著减少所需训练序列数量，但需满足特定条件以实现最优性能。

Abstract: This paper investigates the asymptotic behavior of the deterministic and
stochastic Cram\'er-Rao Bounds (CRB) for semi-blind channel estimation in
massive multiple-input multiple-output (MIMO) systems. We derive and analyze
mathematically tractable expressions for both metrics under various asymptotic
regimes, which govern the growth rates of the number of antennas, the number of
users, the training sequence length, and the transmission block length. Unlike
the existing work, our results show that the CRB can be made arbitrarily small
as the transmission block length increases, but only when the training sequence
length grows at the same rate and the number of users remains fixed. However,
if the number of training sequences remains proportional to the number of
users, the channel estimation error is always lower-bounded by a non-vanishing
constant. Numerical results are presented to support our findings and
demonstrate the advantages of semi-blind channel estimation in reducing the
required number of training sequences.

</details>


### [3] [Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM](https://arxiv.org/abs/2507.08974)
*Thien Hieu Hoang,Tri Nhu Do,Georges Kaddoum*

Main category: eess.SP

TL;DR: 提出一种新颖的域适应方法，用于解决无线通信中动态环境下的信道估计问题，结合准静态信道模型和地图信道模型。


<details>
  <summary>Details</summary>
Motivation: 传统模型在动态环境中表现不佳，机器学习方法缺乏跨数据集的泛化能力。

Method: 提出信道估计流程和域适应方法，利用仿真数据训练基础模型。

Result: 策略在真实信道信息有限的情况下仍能保持稳健性能。

Conclusion: 该方法有效减少了实际应用中的数据需求，提升了信道估计的鲁棒性。

Abstract: Accurate channel estimation is crucial for the improvement of signal
processing performance in wireless communications. However, traditional
model-based methods frequently experience difficulties in dynamic environments.
Similarly, alternative machine-learning approaches typically lack
generalization across different datasets due to variations in channel
characteristics. To address this issue, in this study, we propose a novel
domain adaptation approach to bridge the gap between the quasi-static channel
model (QSCM) and the map-based channel model (MBCM). Specifically, we first
proposed a channel estimation pipeline that takes into account realistic
channel simulation to train our foundation model. Then, we proposed domain
adaptation methods to address the estimation problem. Using simulation-based
training to reduce data requirements for effective application in practical
wireless environments, we find that the proposed strategy enables robust model
performance, even with limited true channel information.

</details>


### [4] [Hypergraph Overlapping Community Detection for Brain Networks](https://arxiv.org/abs/2507.08999)
*Duc Vu,Selin Aviyente*

Main category: eess.SP

TL;DR: 该论文提出了一种基于超图的方法，用于从fMRI数据中捕捉大脑区域的高阶依赖关系，并通过谱聚类检测重叠社区结构。


<details>
  <summary>Details</summary>
Motivation: 传统的功能连接网络（FCNs）仅量化大脑区域间的成对关系，忽略了高阶依赖关系。超图方法可以更好地表征多区域间的关系，但其拓扑结构特征尚不明确。

Method: 1. 为每个受试者构建超图以捕捉高阶依赖关系；2. 提出基于谱聚类的超图方法检测重叠社区结构；3. 在多受试者中检测共识社区结构。

Result: 方法应用于Human Connectome Project的静息态fMRI数据，总结了健康年轻成年人的重叠社区结构。

Conclusion: 提出的方法能够有效表征大脑超网络的高阶依赖关系，并检测重叠社区结构，为脑网络分析提供了新工具。

Abstract: Functional magnetic resonance imaging (fMRI) has been commonly used to
construct functional connectivity networks (FCNs) of the human brain. TFCNs are
primarily limited to quantifying pairwise relationships between ROIs ignoring
higher order dependencies between multiple brain regions. Recently, hypergraph
construction methods from fMRI time series data have been proposed to
characterize the high-order relations among multiple ROIs. While there have
been multiple methods for constructing hypergraphs from fMRI time series, the
question of how to characterize the topology of these hypergraphs remains open.
In this paper, we make two key contributions to the field of community
detection in brain hypernetworks. First, we construct a hypergraph for each
subject capturing high order dependencies between regions. Second, we introduce
a spectral clustering based approach on hypergraphs to detect overlapping
community structure. Finally, the proposed method is implemented to detect the
consensus community structure across multiple subjects. The proposed method is
applied to resting state fMRI data from Human Connectome Project to summarize
the overlapping community structure across a group of healthy young adults.

</details>


### [5] [Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems](https://arxiv.org/abs/2507.09215)
*Yi Wang,Keke Zu,Luping Xiang,Martin Haardt,Kun Yang*

Main category: eess.SP

TL;DR: 论文提出了一种针对时钟异步双基地ISAC系统的时变偏移估计（TVOE）框架，通过利用视距路径的几何特性实现稳健同步，显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 双基地ISAC在下一代通信网络（如B5G/6G）中具有重要应用潜力，但时钟异步问题导致定时偏移和载波频率偏移，传统同步方法在无人机场景中不可靠。

Method: 提出TVOE框架，将视距延迟和多普勒频移建模为动态观测，通过扩展卡尔曼滤波器实时估计偏移，并校正非视距分量。

Result: 仿真结果表明，TVOE方法将估计精度提高了60%。

Conclusion: TVOE框架为时钟异步双基地ISAC系统提供了一种高效且无需额外基础设施的同步解决方案。

Abstract: The bistatic Integrated Sensing and Communication (ISAC) is poised to become
a key application for next generation communication networks (e.g., B5G/6G),
providing simultaneous sensing and communication services with minimal changes
to existing network infrastructure and hardware. However, a significant
challenge in bistatic cooperative sensing is clock asynchronism, arising from
the use of different clocks at far separated transmitters and receivers. This
asynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),
potentially causing sensing ambiguity. Traditional synchronization methods
typically rely on static reference links or GNSS-based timing sources, both of
which are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.
To overcome these limitations, we propose a Time-Varying Offset Estimation
(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which
leverages the geometrically predictable characteristics of the Line-of-Sight
(LoS) path to enable robust, infrastructure-free
  synchronization. The framework treats the LoS delay and the Doppler shift as
dynamic observations and models their evolution as a hidden stochastic process.
A state-space formulation is developed to jointly estimate TO and CFO via an
Extended Kalman Filter (EKF), enabling real-time tracking of clock offsets
across successive frames. Furthermore, the estimated offsets are subsequently
applied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)
components, thereby enhancing the high-resolution target sensing performance.
Extensive simulation results demonstrate that the proposed TVOE method improves
the estimation accuracy by 60%.

</details>


### [6] [Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC](https://arxiv.org/abs/2507.09218)
*Yi Wang,Keke Zu,Luping Xiang,Martin Haardt,Chaochao Wang,Xianchao Zhang,Kun Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于图像超分辨率的信号增强框架（ISR-SE），用于提升双基地集成感知与通信（ISAC）中的信号质量，解决了复杂环境下信号弱的问题。


<details>
  <summary>Details</summary>
Motivation: 双基地ISAC在下一代通信网络中潜力巨大，但复杂环境中的高反射损耗导致信号弱、感知精度下降，传统自适应滤波方法难以应对复杂网络拓扑。

Method: 采用短时傅里叶变换（STFT）生成频谱图，映射为RGB图像，结合UNet和扩散模型的混合架构进行图像去噪和增强。

Result: 提出的ISR-SE框架显著提升了低信噪比条件下信号的识别和恢复能力。

Conclusion: ISR-SE框架为双基地ISAC提供了一种有效的信号增强方法，适用于复杂网络环境。

Abstract: Bistatic Integrated Sensing and Communication (ISAC) is poised to become a
cornerstone technology in next-generation communication networks, such as
Beyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and
communication functions without requiring significant modifications to existing
infrastructure. Despite its promising potential, a major challenge in bistatic
cooperative sensing lies in the degradation of sensing accuracy, primarily
caused by the inherently weak received signals resulting from high reflection
losses in complex environments. Traditional methods have predominantly relied
on adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by
dynamically adjusting the filter coefficients. However, these methods often
struggle to adapt effectively to the increasingly complex and diverse network
topologies. To address these challenges, we propose a novel Image
Super-Resolution-based Signal Enhancement (ISR-SE) framework that significantly
improves the recognition and recovery capabilities of ISAC signals.
Specifically, we first perform a time-frequency analysis by applying the
Short-Time Fourier Transform (STFT) to the received signals, generating
spectrograms that capture the frequency, magnitude, and phase components. These
components are then mapped into RGB images, where each channel represents one
of the extracted features, enabling a more intuitive and informative
visualization of the signal structure. To enhance these RGB images, we design
an improved denoising network that combines the strengths of the UNet
architecture and diffusion models. This hybrid architecture leverages UNet's
multi-scale feature extraction and the generative capacity of diffusion models
to perform effective image denoising, thereby improving the quality and clarity
of signal representations under low-SNR conditions.

</details>


### [7] [Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming](https://arxiv.org/abs/2507.09244)
*Nishant Gupta,Muris Sarajlic,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出了一种利用深度学习从sub-10 GHz信道特性推断适合的sub-THz RU的方法，解决了beam搜索或RU选择的高开销问题，并考虑了UE的IBBC信息。


<details>
  <summary>Details</summary>
Motivation: 未来6G应用中，sub-THz RU的密集部署需要解决beam搜索或RU选择的高开销和高功耗问题。

Method: 利用深度学习从sub-10 GHz信道特性推断适合的sub-THz RU，并考虑了UE的IBBC信息。

Result: 仿真结果表明该方法能有效推断适合的sub-THz RU，并揭示了忽略UE方向对系统性能的负面影响。

Conclusion: 该方法为6G网络中sub-THz RU的高效选择提供了可行方案，同时强调了UE方向信息的重要性。

Abstract: The dense and distributed deployment of sub-THz radio units (RUs) alongside
sub-10 GHz access point (AP) is a promising approach to provide high data rate
and reliable coverage for future 6G applications. However, beam search or RU
selection for the sub-THz RUs incurs significant overhead and high power
consumption. To address this, we introduce a method that leverages deep
learning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs
using the sub-10 GHz channel characteristics. A novel aspect of this work is
the consideration of inter-band beam configuration (IBBC), defined as the
broadside angle between the low-band and high-band antenna patterns of the user
equipment (UE). Since IBBC indicates the beamforming information or UE's
orientation, it is typically not shared with the network as a part of
signalling. Therefore, we propose a solution strategy to infer a suitable
sub-THz RU even when UEs do not share their IBBC information. Simulation
results illustrate the performance of the inferred sub-THz RU and highlights
the detrimental impact of neglecting UE orientation on the systems performance.

</details>


### [8] [Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels](https://arxiv.org/abs/2507.09268)
*Xiangjun Li,Zilong Liu,Zhengchun Zhou,Pingzhi Fan*

Main category: eess.SP

TL;DR: 提出了一种增强型AFDM波形，通过考虑延迟-多普勒耦合相位，研究了匹配滤波辅助的信道估计方法，解决了路径模糊问题，并提出了低复杂度的MF-GFS方案。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂双选择性信道中AFDM系统的信道估计问题，尤其是路径模糊导致的性能下降。

Method: 通过推导完整的输入输出关系，分析干扰和SINR，提出基于连续AFDM传输的MF辅助CE方案，并引入GFS降低复杂度。

Result: 仿真结果表明，所提方案在通信性能和复杂度方面具有显著优势。

Conclusion: 增强型AFDM和MF-GFS方案能有效提升6G波形的性能，同时降低实现复杂度。

Abstract: Affine frequency division multiplexing (AFDM) has recently emerged as an
excellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is
proposed whereby the delay-Doppler (DD) coupling phase is considered.
Specifically, we study matched filtering (MF) assisted channel estimation (CE)
for AFDM systems in complex doubly selective channels. By deriving the complete
input-output relationship, the inter-chirp-carrier interference,
signal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of
AFDM, are investigated in discrete affine Fourier transform (DAFT) domain.
Further, we look into the path ambiguity problem and show that it may lead to
severe performance deterioration in fractional-delay fractional-Doppler
channels. To address such a problem, we introduce an MF assisted CE scheme
building upon a novel pilot arrangement across two consecutive AFDM
transmissions. This allows us to sequentially estimate the parameters of each
path by exploiting the separability and approximate orthogonality of different
paths in the DAFT domain, thus leading to significantly reduced complexity.
Furthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is
proposed to avoid significantly redundant computation, which can be extended to
typical wide-band systems. Extensive simulation results indicate that the
proposed schemes offer superior advantages in terms of their improved
communication performance and lower complexity.

</details>


### [9] [Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging](https://arxiv.org/abs/2507.09386)
*Ruangrawee Kitichotkul,Shashwath Bharadwaj,Joshua Rapp,Yanting Ma,Alexander Mehta,Vivek K Goyal*

Main category: eess.SP

TL;DR: 论文提出了一种高效的最大似然估计方法，结合正则化框架，提升了自由运行单光子激光雷达（SPL）的精度，实验证明其优于同步SPL。


<details>
  <summary>Details</summary>
Motivation: 自由运行SPL在减少死区时间引起的直方图失真方面具有优势，但现有解决方案有限，需提升其精度。

Method: 提出了一种基于直方图的联合最大似然估计方法，结合学习点云分数模型的正则化框架。

Result: 自由运行SPL在相同条件下比同步SPL误差更低，正则化进一步提高了精度。

Conclusion: 该方法显著提升了自由运行SPL的性能，为实际应用提供了更优解决方案。

Abstract: Conventional wisdom suggests that single-photon lidar (SPL) should operate in
low-light conditions to minimize dead-time effects. Many methods have been
developed to mitigate these effects in synchronous SPL systems. However,
solutions for free-running SPL remain limited despite the advantage of reduced
histogram distortion from dead times. To improve the accuracy of free-running
SPL, we propose a computationally efficient joint maximum likelihood estimator
of the signal flux, the background flux, and the depth using only histograms,
along with a complementary regularization framework that incorporates a learned
point cloud score model as a prior. Simulations and experiments demonstrate
that free-running SPL yields lower estimation errors than its synchronous
counterpart under identical conditions, with our regularization further
improving accuracy.

</details>


### [10] [Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation](https://arxiv.org/abs/2507.09408)
*Sajedeh Norouzi,Mostafa Rahmani,Yi Chu,Torsten Braun,Kaushik Chowdhury,Alister Burr*

Main category: eess.SP

TL;DR: GraphNet是一种基于图神经网络（GNN）的轻量级信道估计方法，用于优化5G NR系统性能，尤其在动态环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法在复杂和动态环境中难以适应，亟需一种高效且轻量的解决方案。

Method: 采用GNN架构，减少计算开销并捕捉关键特征，同时内置噪声估计以增强鲁棒性。

Result: 在稳定条件下与ChannelNet性能相当，在高动态环境中表现更优，且计算资源占用更低。

Conclusion: GraphNet为5G信道估计提供了高效、可扩展的解决方案，适合边缘设备实时部署。

Abstract: Effective channel estimation CE is critical for optimizing the performance of
5G New Radio NR systems particularly in dynamic environments where traditional
methods struggle with complexity and adaptability This paper introduces
GraphNet a novel lightweight Graph Neural Network GNNbased estimator designed
to enhance CE in 5G NR Our proposed method utilizes a GNN architecture that
minimizes computational overhead while capturing essential features necessary
for accurate CE We evaluate GraphNet across various channel conditions from
slowvarying to highly dynamic environments and compare its performance to
ChannelNet a wellknown deep learningbased CE method GraphNet not only matches
ChannelNets performance in stable conditions but significantly outperforms it
in highvariation scenarios particularly in terms of Block Error Rate It also
includes builtin noise estimation that enhances robustness in challenging
channel conditions Furthermore its significantly lighter computational
footprint makes GraphNet highly suitable for realtime deployment especially on
edge devices with limited computational resources By underscoring the potential
of GNNs to transform CE processes GraphNet offers a scalable and robust
solution that aligns with the evolving demands of 5G technologies highlighting
its efficiency and performance as a nextgeneration solution for wireless
communication systems

</details>


### [11] [An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation](https://arxiv.org/abs/2507.09458)
*Wang Ning,Zhang Chenyu,Sun Yanshi,Min Minghui,Liu Yuanwei,Li Shiyin*

Main category: eess.SP

TL;DR: 本文提出了一种结合混合连续干扰消除（HSIC）和功率适配（PA）的新型H-NOMA设计，显著提升了无线通信系统的性能。


<details>
  <summary>Details</summary>
Motivation: 为了进一步释放H-NOMA的潜力，结合HSIC和PA技术以提升数据速率和能效。

Method: 在NOMA传输阶段联合使用HSIC和PA技术，并通过闭式表达式和渐近分析验证其性能。

Result: 在高信噪比下，提出的H-NOMA方案能以更少能耗实现比纯OMA更高的数据速率，且概率趋近于1。

Conclusion: HSIC-PA辅助的H-NOMA在能效和数据速率上优于传统方案，数值结果验证了其优越性。

Abstract: Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which
effectively utilizes both NOMA and orthogonal multiple access (OMA)
technologies through flexible resource allocation in a single transmission, has
demonstrated immense potential for enhancing the performance of wireless
communication systems. To further release the potential of HNOMA, this paper
proposes a novel design of H-NOMA which jointly incorporates hybrid successive
interference cancellation (HSIC) and power adaptation (PA) in the NOMA
transmission phase. To reveal the potential of the proposed HSIC-PA aided
H-NOMA scheme, closed-form expression for the probability of the event that
H-NOMA can achieve a higher data rate than pure OMA by consuming less energy is
rigorously derived. Furthermore, the asymptotic analysis demonstrates that the
probability of the proposed H-NOMA scheme approaches 1 in the high
signal-to-noise ratio (SNR) regime without any constraints on either users'
target rates or transmit power ratios. This represents a significant
improvement over conventional H-NOMA schemes, which require specific
restrictive conditions to achieve probability 1 at high SNRs as shown in
existing work. The above observation indicates that with less energy
consumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate
than pure OMA with probability 1 at high SNRs, and hence a higher energy
efficiency. Finally, numerical results are provided to verify the accuracy of
the analysis and also demonstrate the superior performance of the proposed
H-NOMA scheme.

</details>


### [12] [Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs](https://arxiv.org/abs/2507.09535)
*Chaoran Li,Xingguo Xu,Siyuan Mu*

Main category: eess.SP

TL;DR: 该论文提出了一种基于多模态大语言模型（MLLMs）的SAR目标识别方法，通过结合候选类别和Chain-of-Thought推理，显著提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统SAR图像识别方法受限于数据特性（如弱纹理、高噪声和模糊边界），因此探索MLLMs在SAR分析中的潜力。

Method: 利用GPT-4o进行多模态推理，结合SAR图像、候选标签和CoT推理链，构建新数据集FAIR-CSAR。

Result: 实验表明MLLMs能生成逻辑连贯且可解释的推理，但也存在局限性。

Conclusion: 证明了MLLMs在SAR分析中的可行性，为未来研究奠定了基础。

Abstract: In the context of Synthetic Aperture Radar (SAR) image recognition,
traditional methods often struggle with the intrinsic limitations of SAR data,
such as weak texture, high noise, and ambiguous object boundaries. This work
explores a novel perspective by reformulating SAR target recognition as a
multimodal reasoning task. We leverage multimodal large language models
(MLLMs), specifically GPT-4o, to perform target classification based on SAR
imagery, guided by candidate categories and enhanced with Chain-of-Thought
(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,
comprising raw SAR images, structured target annotations, candidate label sets,
and GPT-generated CoT reasoning chains. Experimental results show that the
MLLMs are capable of generating logically coherent and interpretable inferences
in most scenarios. Our analysis highlights both the strengths and current
limitations of MLLMs in interpreting SAR imagery, and we provide detailed
insights into model behavior through failure case analysis. This work
demonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines
and establishes a foundation for future research in SAR-oriented visual
reasoning.

</details>


### [13] [Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling](https://arxiv.org/abs/2507.09561)
*Can Wang,Wei Liu,Hanzhi Ma,Xiaonan Jiang,Erping Li,Steven Gao*

Main category: eess.SP

TL;DR: 提出了一种物理感知的卷积长短期记忆网络（PC-LSTM），用于高效准确地提取偶极天线阵列的互阻抗矩阵，通过结合物理知识和神经网络提升建模的物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如全波仿真）计算复杂且耗时，需要一种更高效且物理可解释的方法来建模天线阵列的互耦合特性。

Method: 通过重新解释格林函数并嵌入自适应损失函数，结合注意力机制校准复值特征，最后用卷积LSTM网络处理并提取阻抗矩阵。

Result: 验证表明，该方法能准确提取阻抗矩阵，速度比CST Microwave Studio快7倍。

Conclusion: PC-LSTM是一种快速且物理可解释的替代方案，适用于互耦合特性建模。

Abstract: This article presents a physics-aware convolutional long short-term memory
(PC-LSTM) network for efficient and accurate extraction of mutual impedance
matrices in dipole antenna arrays. By reinterpreting the Green's function
through a physics-aware neural network and embedding it into an adaptive loss
function, the proposed machine learning-based approach achieves enhanced
physical interpretability in mutual coupling modeling. Also, an attention
mechanism is carefully designed to calibrate complex-valued features by fusing
the real and imaginary parts of the Green's function matrix. These fused
representations are then processed by a convolutional long short-term memory
network, and the impedance matrix of the linear antenna array can be finally
derived. Validation against five benchmarks underscores the efficacy of the
proposed approach, demonstrating accurate impedance extraction with up to a 7x
speedup compared to CST Microwave Studio, making it a fast alternative to
full-wave simulations for mutual coupling characterization.

</details>


### [14] [A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks](https://arxiv.org/abs/2507.09713)
*Burak Ahmet Ozden,Erdogan Aydin,Ahmet Elbir,Filiz Gurkan*

Main category: eess.SP

TL;DR: 提出了一种基于码索引调制（CIM）的无线图像传输系统（CIM-IT），结合扩频码索引和QAM技术，通过最大似然检测和图像增强滤波器优化传输效果。


<details>
  <summary>Details</summary>
Motivation: 随着无线网络技术的发展，高分辨率、高可靠性的图像传输需求日益增长，尤其在医疗和军事领域。

Method: 利用CIM和QAM技术映射图像像素值，通过单输入多输出系统传输，接收端使用解扩最大似然检测器和增强滤波器恢复图像。

Result: 与传统无线通信技术相比，CIM-IT系统在误码性能、频谱效率、能量效率和吞吐量方面表现更优。

Conclusion: CIM-IT系统为高可靠性和高效能的无线图像传输提供了一种有效的解决方案。

Abstract: With the development of wireless network technologies, the wireless image
transmission area has become prominent. The need for high resolution, data
traffic density, widespread use of multimedia applications, and the importance
of high rate and reliable image transmission in medical and military fields
necessitate the design of novel and high-performance wireless image
transmission systems. This paper proposes a code index modulation (CIM)-based
image transmission (CIM-IT) system that utilizes spreading code index and
quadrature amplitude modulation (QAM) symbol for image transmission over a
wireless channel. The proposed CIM-IT system maps bits to each pixel value of
the image to be transmitted and transmits these bits over a wireless channel
using a single-input and multiple-output system comprising code index
modulation and QAM techniques. At the receiver, the active spreading code index
and the selected QAM symbol are estimated using a despreading-based maximum
likelihood detector, and the corresponding bits are obtained. The image
conveyed from the transmitter is then reconstructed at the receiver side using
the pixel values corresponding to the bits. The obtained noisy image is
enhanced using important enhancement filters. In addition, an advanced filter
is proposed to improve the transmitted degraded image with optimum results.
Furthermore, error performance, spectral efficiency, energy efficiency, and
throughputof the CIM-IT system are performed and the results are compared with
traditional wireless communication techniques.

</details>


### [15] [Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing](https://arxiv.org/abs/2507.09776)
*Mihir Kavishwar,Naresh Shanbhag*

Main category: eess.SP

TL;DR: 论文提出了一种名为CACTUS的算法，用于优化模拟内存计算（AIMC）中的ADC精度需求，通过计算信噪比（CSNR）来提升能效和准确性。


<details>
  <summary>Details</summary>
Motivation: AIMC的高能效受限于高能耗的ADC，降低ADC精度虽能减少能耗但会影响计算准确性，因此需要找到满足目标精度的最小ADC精度。

Method: 开发了计算信噪比（CSNR）的解析表达式，并提出CACTUS算法优化ADC参数。

Result: 在28nm CMOS工艺的SRAM AIMC模型中，CACTUS将ADC精度需求降低了3位，同时CSNR提高了6dB。

Conclusion: CACTUS算法在特定条件下优于传统SQNR优化的ADC，显著提升了AIMC的能效和准确性。

Abstract: Analog in-memory computing (AIMC) is an energy-efficient alternative to
digital architectures for accelerating machine learning and signal processing
workloads. However, its energy efficiency is limited by the high energy cost of
the column analog-to-digital converters (ADCs). Reducing the ADC precision is
an effective approach to lowering its energy cost. However, doing so also
reduces the AIMC's computational accuracy thereby making it critical to
identify the minimum precision required to meet a target accuracy. Prior works
overestimate the ADC precision requirements by modeling quantization error as
input-independent noise, maximizing the signal-to-quantization-noise ratio
(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address
these limitations by developing analytical expressions for estimating the
compute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and
propose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a
circuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we
show that for a 256-dimensional binary dot product, CACTUS reduces the ADC
precision requirements by 3b while achieving 6dB higher CSNR over prior
methods. We also delineate operating conditions under which our proposed
CSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.

</details>


### [16] [Precoded Zak-OTFS for Per-Carrier Equalization](https://arxiv.org/abs/2507.09894)
*Saif Khan Mohammed,Amit Kumar Pathak,Muhammad Ubadah,Ronny Hadani,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: Zak-OTFS调制通过延迟-多普勒（DD）域的脉冲波形探测散射环境，提出了一种新型预编码技术，显著降低了复杂度并提高了频谱效率。


<details>
  <summary>Details</summary>
Motivation: 研究Zak-OTFS调制在双扩展信道中的预编码技术，以降低复杂度并提升频谱效率。

Method: 利用Zak-OTFS调制的DD域脉冲波形探测散射环境，开发了一种基于扭曲卷积的二维部分响应信道的预编码技术。

Result: 预编码技术实现了DD载波的独立均衡，复杂度显著低于联合均衡，同时减少了保护载波的开销，提高了频谱效率。

Conclusion: 提出的预编码技术为Zak-OTFS调制在双扩展信道中的应用提供了高效且低复杂度的解决方案。

Abstract: In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform
is a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic
localized function with specific periods along delay and Doppler. When the
channel delay spread is less than the delay period, and the channel Doppler
spread is less than the Doppler period, the response to a single Zak-OTFS
carrier provides an image of the scattering environment and can be used to
predict the effective channel at all other carriers. The image of the
scattering environment changes slowly, making it possible to employ precoding
at the transmitter. Precoding techniques were developed more than thirty years
ago for wireline modem channels (V.34 standard) defined by linear convolution
where a pulse in the time domain (TD) is used to probe the one-dimensional
partial response channel. The action of a doubly spread channel on Zak-OTFS
modulation determines a two-dimensional partial response channel defined by
twisted convolution, and we develop a novel precoding technique for this
channel. The proposed precoder leads to separate equalization of each DD
carrier which has significantly lower complexity than joint equalization of all
carriers. Further, the effective precoded channel results in non-interfering DD
carriers which significantly reduces the overhead of guard carriers separating
data and pilot carriers, which improves the spectral efficiency significantly.

</details>


### [17] [AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission](https://arxiv.org/abs/2507.09895)
*Hyung-Joo Moon,Chan-Byoung Chae,Kai-Kit Wong,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: MAP-X框架通过HAPS和分布式传感器重建空间相关地面数据，支持低延迟IoT应用。两种AI方法（DNN和CNN）显著优于传统IDFT方法，并提出地面-HAPS协作框架。


<details>
  <summary>Details</summary>
Motivation: 解决大规模地面数据重建的延迟和精度问题，满足实时IoT应用需求。

Method: 采用DNN点估计（实时）和CNN图像重建（离线）两种AI方法，结合地面-HAPS协作框架。

Result: 两种AI方法均显著优于传统IDFT方法，提升重建精度和实时性。

Conclusion: AI增强的MAP-X适用于灾难响应和网络管理等实际场景，具有广泛潜力。

Abstract: Massive Aerial Processing for X MAP-X is an innovative framework for
reconstructing spatially correlated ground data, such as environmental or
industrial measurements distributed across a wide area, into data maps using a
single high altitude pseudo-satellite (HAPS) and a large number of distributed
sensors. With subframe-level data reconstruction, MAP-X provides a
transformative solution for latency-sensitive IoT applications. This article
explores two distinct approaches for AI integration in the post-processing
stage of MAP-X. The DNN-based pointwise estimation approach enables real-time,
adaptive reconstruction through online training, while the CNN-based image
reconstruction approach improves reconstruction accuracy through offline
training with non-real-time data. Simulation results show that both approaches
significantly outperform the conventional inverse discrete Fourier transform
(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced
MAP-X, we propose a ground-HAPS cooperation framework, where terrestrial
stations collect, process, and relay training data to the HAPS. With its
enhanced capability in reconstructing field data, AI-enhanced MAP-X is
applicable to various real-world use cases, including disaster response and
network management.

</details>


### [18] [VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling](https://arxiv.org/abs/2507.09987)
*Zihang Zeng,Shu Sun,Meixia Tao,Yin Xu,Xianghao Yu*

Main category: eess.SP

TL;DR: 提出了一种名为VoxelRF的新型神经表示方法，用于无线信道建模，通过体素化表示和浅层MLP实现快速准确的频谱合成。


<details>
  <summary>Details</summary>
Motivation: 传统无线信道建模方法在准确性、效率和可扩展性方面存在挑战，而现有神经方法（如NeRF）训练和推理速度慢。

Method: VoxelRF采用体素网格表示和三线性插值替代NeRF中的多层感知机，结合浅层MLP建模传播和发射器相关效应，并引入渐进学习、空域跳过和背景熵损失函数加速训练。

Result: 实验表明，VoxelRF在计算量减少和训练数据有限的情况下仍保持竞争力，适用于实时和资源受限的无线应用。

Conclusion: VoxelRF为复杂环境中的无线信道建模提供了一种高效且实用的解决方案。

Abstract: Wireless channel modeling in complex environments is crucial for wireless
communication system design and deployment. Traditional channel modeling
approaches face challenges in balancing accuracy, efficiency, and scalability,
while recent neural approaches such as neural radiance field (NeRF) suffer from
long training and slow inference. To tackle these challenges, we propose
voxelized radiance field (VoxelRF), a novel neural representation for wireless
channel modeling that enables fast and accurate synthesis of spatial spectra.
VoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based
methods with trilinear interpolation of voxel grid-based representation, and
two shallow MLPs to model both propagation and transmitter-dependent effects.
To further accelerate training and improve generalization, we introduce
progressive learning, empty space skipping, and an additional background
entropy loss function. Experimental results demonstrate that VoxelRF achieves
competitive accuracy with significantly reduced computation and limited
training data, making it more practical for real-time and resource-constrained
wireless applications.

</details>


### [19] [Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs](https://arxiv.org/abs/2507.09999)
*Lital Dabush,Nir Shlezinger,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 提出了一种基于图信号处理的动态图拓扑跟踪方法，通过非线性状态空间模型和稀疏感知扩展卡尔曼滤波器实现高效跟踪。


<details>
  <summary>Details</summary>
Motivation: 动态图拓扑跟踪在图信号处理中具有广泛应用，如电力系统、脑机接口和通信系统，但现有方法难以处理非线性测量和高噪声环境。

Method: 采用基于图的状态空间模型，参数化图拉普拉斯矩阵，并开发了稀疏感知扩展卡尔曼滤波器和动态规划计算图滤波器的雅可比矩阵。

Result: 数值研究表明，该方法能在高噪声和非线性测量条件下准确跟踪稀疏时变图，同时保持低计算复杂度。

Conclusion: 该方法为动态图拓扑跟踪提供了一种高效且鲁棒的解决方案，适用于多种实际应用场景。

Abstract: A broad range of applications involve signals with irregular structures that
can be represented as a graph. As the underlying structures can change over
time, the tracking dynamic graph topologies from observed signals is a
fundamental challenge in graph signal processing (GSP), with applications in
various domains, such as power systems, the brain-machine interface, and
communication systems. In this paper, we propose a method for tracking dynamic
changes in graph topologies. Our approach builds on a representation of the
dynamics as a graph-based nonlinear state-space model (SSM), where the
observations are graph signals generated through graph filtering, and the
underlying evolving topology serves as the latent states. In our formulation,
the graph Laplacian matrix is parameterized using the incidence matrix and edge
weights, enabling a structured representation of the state. In order to track
the evolving topology in the resulting SSM, we develop a sparsity-aware
extended Kalman filter (EKF) that integrates $\ell_1$-regularized updates
within the filtering process. Furthermore, a dynamic programming scheme to
efficiently compute the Jacobian of the graph filter is introduced. Our
numerical study demonstrates the ability of the proposed method to accurately
track sparse and time-varying graphs under realistic conditions, with highly
nonlinear measurements, various noise levels, and different change rates, while
maintaining low computational complexity.

</details>


### [20] [Deep Learning-Based Beamforming Design Using Target Beam Patterns](https://arxiv.org/abs/2507.10063)
*Hongpu Zhang,Shu Sun,Hangsong Yan,Jianhua Mo*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的波束成形设计框架，通过轻量级编码器-解码器网络直接映射目标波束模式到最优波束成形向量，适用于多种天线架构。


<details>
  <summary>Details</summary>
Motivation: 解决在多天线架构（数字、模拟和混合波束成形）下，如何高效设计满足硬件约束的波束成形向量的问题。

Method: 采用两阶段训练过程：离线预训练提取鲁棒特征，在线训练解码器并使用复合损失函数确保波束模式匹配。

Result: 仿真结果表明，该方法在有限信道状态信息下接近全数字波束成形的频谱效率，并优于现有方法。

Conclusion: 该方法为波束成形设计提供了一种高效且通用的解决方案，尤其在有限信道信息条件下表现优异。

Abstract: This paper proposes a deep learning-based beamforming design framework that
directly maps a target beam pattern to optimal beamforming vectors across
multiple antenna array architectures, including digital, analog, and hybrid
beamforming. The proposed method employs a lightweight encoder-decoder network
where the encoder compresses the complex beam pattern into a low-dimensional
feature vector and the decoder reconstructs the beamforming vector while
satisfying hardware constraints. To address training challenges under diverse
and limited channel station information (CSI) conditions, a two-stage training
process is introduced, which consists of an offline pre-training for robust
feature extraction using an auxiliary module, followed by online training of
the decoder with a composite loss function that ensures alignment between the
synthesized and target beam patterns in terms of the main lobe shape and side
lobe suppression. Simulation results based on NYUSIM-generated channels show
that the proposed method can achieve spectral efficiency close to that of fully
digital beamforming under limited CSI and outperforms representative existing
methods.

</details>


### [21] [Intrinsic frequency distribution characterises neural dynamics](https://arxiv.org/abs/2507.10145)
*Ryohei Fukuma,Yoshinobu Kawahara,Okito Yamashita,Kei Majima,Haruhiko Kishima,Takufumi Yanagisawa*

Main category: eess.SP

TL;DR: 该论文提出使用动态模式分解（DMD）的固有频率分布来表征神经活动，并证明其在区分健康受试者与痴呆或帕金森病患者方面比传统傅里叶变换更准确。


<details>
  <summary>Details</summary>
Motivation: 理解、预测和控制非线性时空动态系统（如大脑）需要分解多变量时间序列的基本动态。传统傅里叶变换方法无法捕捉非平稳成分，而DMD可以提取多通道信号的固有频率。

Method: 使用DMD分解脑电图信号，提取动态模式的固有频率分布，并将其应用于健康受试者和患者的数据分析。

Result: DM频率分布在区分患者与健康受试者时，比离散傅里叶变换的振幅谱更准确。

Conclusion: DM频率分布可作为表征非线性时空动态的新生物标志物，具有潜在临床应用价值。

Abstract: Decomposing multivariate time series with certain basic dynamics is crucial
for understanding, predicting and controlling nonlinear spatiotemporally
dynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method
for decomposing nonlinear spatiotemporal dynamics into several basic dynamics
(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,
unlike Fourier transform-based methods, which are used to decompose a
single-channel signal into the amplitudes of sinusoidal waves with discrete
frequencies at a regular interval, DMD can derive the intrinsic frequencies of
a multichannel signal on the basis of the available data; furthermore, it can
capture nonstationary components such as alternations between states with
different intrinsic frequencies. Here, we propose the use of the distribution
of intrinsic frequencies derived from DMDs (DM frequencies) to characterise
neural activities. The distributions of DM frequencies in the
electroencephalograms of healthy subjects and patients with dementia or
Parkinson's disease in a resting state were evaluated. By using the
distributions, these patients were distinguished from healthy subjects with
significantly greater accuracy than when using amplitude spectra derived by
discrete Fourier transform. This finding suggests that the distribution of DM
frequencies exhibits distinct behaviour from amplitude spectra, and therefore,
the distribution may serve as a new biomarker by characterising the nonlinear
spatiotemporal dynamics of electrophysiological signals.

</details>


### [22] [Pinching-Antenna Systems for Physical Layer Security](https://arxiv.org/abs/2507.10167)
*Kaidi Wang,Zhiguo Ding,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: 研究了捏合天线系统在增强物理层安全性方面的潜力，通过多天线协作和博弈论方法提升保密率。


<details>
  <summary>Details</summary>
Motivation: 探索捏合天线系统在物理层安全中的应用潜力，解决传统方法在信号调整和安全性上的不足。

Method: 通过预装多个捏合天线，设计振幅和相位调整策略，建模为联盟博弈，并基于Shapley值提出天线激活算法。

Result: 仿真结果表明，该系统显著提升了保密率，且Shapley值算法优于传统方法。

Conclusion: 捏合天线系统结合博弈论方法能有效增强物理层安全性，为未来研究提供了新思路。

Abstract: This letter investigates the potential of pinching-antenna systems for
enhancing physical layer security. By pre-installing multiple pinching antennas
at discrete positions along a waveguide, the capability of the considered
system to perform amplitude and phase adjustment is validated through the
formulation of a secrecy rate maximization problem. Specifically, amplitude
control is applied to enhance the signal quality at the legitimate user, while
phase alignment is designed to degrade the received signal quality at the
eavesdropper. This cooperation among pinching antennas is modeled as a
coalitional game, and a corresponding antenna activation algorithm is proposed.
The individual impact of each antenna is quantified based on the Shapley value
and marginal contribution, providing a fair and efficient method for
performance evaluation. Simulation results show that the considered
pinching-antenna system achieves significant improvements in secrecy rate, and
that the Shapley value based algorithm outperforms conventional coalition value
based solutions.

</details>


### [23] [Pinching-Antenna Systems with LoS Blockages](https://arxiv.org/abs/2507.10173)
*Kaidi Wang,Chongjun Ouyang,Yuanwei Liu,Zhiguo Ding*

Main category: eess.SP

TL;DR: 研究了捏合天线系统在视线（LoS）受阻时构建LoS链路的能力，通过优化波导分配和天线激活，显著提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 探索在LoS受阻情况下，如何利用捏合天线系统动态建立LoS链路并消除干扰。

Method: 提出基于匹配的算法，联合优化波导分配和天线激活，采用两种偏好设计。

Result: 仿真结果表明，该系统能有效利用LoS受阻情况，显著提升系统吞吐量。

Conclusion: 捏合天线系统及其解决方案能动态建立LoS链路并有效消除干扰，提升性能。

Abstract: The aim of this letter is to explore the capability of pinching-antenna
systems to construct line-of-sight (LoS) links in the presence of LoS
blockages. Specifically, pinching antennas are pre-installed at preconfigured
positions along waveguides and can be selectively activated to create LoS links
for enhancing desired signals and non-line-of-sight (NLoS) links for
eliminating inter-user interference. On this basis, a sum-rate maximization
problem is formulated by jointly optimizing waveguide assignment and antenna
activation. To solve this problem, a matching based algorithm is proposed using
two distinct preference designs. Simulation results demonstrate that the
considered pinching-antenna system and proposed solutions can dynamically
establish LoS links and effectively exploit LoS blockages to mitigate
interference, thereby significantly improving system throughput.

</details>


### [24] [Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks](https://arxiv.org/abs/2507.10308)
*Hung Nguyen-Kha,Vu Nguyen Ha,Eva Lagunas,Symeon Chatzinotas,Joel Grotz*

Main category: eess.SP

TL;DR: 论文研究了5G卫星-地面网络（ISTN）中下行链路传输，针对城市环境中移动用户（UEs）的挑战，提出了一种多目标优化算法，以提升吞吐量和无缝切换性能。


<details>
  <summary>Details</summary>
Motivation: 城市环境中密集的障碍物、UE移动性及LEO卫星的动态覆盖对用户关联和资源分配提出了挑战。

Method: 提出了一种基于SCA技术的迭代算法和实用的预测算法，联合优化功率分配和用户关联。

Result: 仿真结果表明，所提算法在吞吐量和连接切换次数上优于贪婪算法和基准算法。

Conclusion: 算法在复杂城市环境中有效提升了系统性能。

Abstract: This paper investigates downlink transmission in 5G Integrated
Satellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in
urban environments, where base stations (BSs) and Low Earth Orbit (LEO)
satellites (LSats) cooperate to serve moving UEs over shared C-band frequency
carriers. Urban settings, characterized by dense obstructions, together with UE
mobility, and the dynamic movement and coverage of LSats pose significant
challenges to user association and resource allocation. To address these
challenges, we formulate a multi-objective optimization problem designed to
improve both throughput and seamless handover (HO). Particularly, the
formulated problem balances sum-rate (SR) maximization and connection change
(CC) minimization through a weighted trade-off by jointly optimizing power
allocation and BS-UE/LSat-UE associations over a given time window. This is a
mixed-integer and non-convex problem which is inherently difficult to solve. To
solve this problem efficiently, we propose an iterative algorithm based on the
Successive Convex Approximation (SCA) technique. Furthermore, we introduce a
practical prediction-based algorithm capable of providing efficient solutions
in real-world implementations. Especially, the simulations use a realistic 3D
map of London and UE routes obtained from the Google Navigator application to
ensure practical examination. Thanks to these realistic data, the simulation
results can show valuable insights into the link budget assessment in urban
areas due to the impact of buildings on transmission links under the blockage,
reflection, and diffraction effects. Furthermore, the numerical results
demonstrate the effectiveness of our proposed algorithms in terms of SR and the
CC-number compared to the greedy and benchmark algorithms.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [25] [SemAlignVC: Enhancing zero-shot timbre conversion using semantic alignment](https://arxiv.org/abs/2507.09070)
*Shivam Mehta,Yingru Liu,Zhenyu Tang,Kainan Peng,Vimal Manohar,Shun Zhang,Mike Seltzer,Qing He,Mingbo Ma*

Main category: eess.AS

TL;DR: SemAlignVC是一种新型零样本语音转换架构，通过SemAlign方法对齐文本和音频表示，减少音色泄漏，提升转换质量。


<details>
  <summary>Details</summary>
Motivation: 解决神经编解码器和基于LLM的语音转换中音色泄漏问题，确保说话人无关的语义编码。

Method: 使用SemAlign对齐文本和音频表示，结合自回归变换器进行高保真转换，无需显式说话人嵌入。

Result: 实验表明SemAlignVC显著减少音色泄漏，在音色相似性、清晰度和自然度上优于基线方法。

Conclusion: SemAlignVC是一种鲁棒、隐私保护且可泛化的语音转换解决方案。

Abstract: Zero-shot voice conversion (VC) synthesizes speech in a target speaker's
voice while preserving linguistic and paralinguistic content. However, timbre
leakage-where source speaker traits persist-remains a challenge, especially in
neural codec and LLM-based VC, where quantized representations entangle speaker
identity with content. We introduce SemAlignVC, an architecture designed to
prevent timbre leakage using SemAlign, a novel method that aligns text and
audio representations to ensure speaker-independent semantic encoding. This
disentangled representation conditions an autoregressive transformer for
high-fidelity conversion without explicit speaker embeddings. Experiments show
SemAlignVC significantly reduces timbre leakage, outperforming baselines in
speaker timbre similarity, intelligibility, and naturalness, making it a
robust, privacy-preserving, and generalizable VC solution. Audio samples can be
accessed at https://shivammehta25.github.io/SemAlignVC/

</details>


### [26] [Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition](https://arxiv.org/abs/2507.09161)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: eess.AS

TL;DR: 论文提出了一种结合矩阵分解和大语言模型的生物声学信号分析方法，用于分离临床记录中重叠的信号，并将其与潜在医学条件关联。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在生物医学信号分析中存在局限性，大语言模型为提取非结构化数据的意义提供了新途径。

Method: 使用矩阵分解框架分离重叠的生物声学信号，并应用大语言模型将分离后的信号与医学条件关联。

Result: 方法在受控环境中验证，无需标记数据或先验知识，为临床决策支持提供了可解释的框架。

Conclusion: 该方法有望整合到未来的智能诊断工具中。

Abstract: Large language models have shown a remarkable ability to extract meaning from
unstructured data, offering new ways to interpret biomedical signals beyond
traditional numerical methods. In this study, we present a matrix factorization
framework for bioacoustic signal analysis which is enhanced by large language
models. The focus is on separating bioacoustic signals that commonly overlap in
clinical recordings, using matrix factorization to decompose the mixture into
interpretable components. A large language model is then applied to the
separated signals to associate distinct acoustic patterns with potential
medical conditions such as cardiac rhythm disturbances or respiratory
abnormalities. Recordings were obtained from a digital stethoscope applied to a
clinical manikin to ensure a controlled and high-fidelity acquisition
environment. This hybrid approach does not require labeled data or prior
knowledge of source types, and it provides a more interpretable and accessible
framework for clinical decision support. The method demonstrates promise for
integration into future intelligent diagnostic tools.

</details>


### [27] [Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?](https://arxiv.org/abs/2507.09226)
*Shota Horiguchi,Naohiro Tawara,Takanori Ashihara,Atsushi Ando,Marc Delcroix*

Main category: eess.AS

TL;DR: 论文探讨了神经说话人日志中边界定义不严格对性能的影响，提出通过强制对齐标准化边界以提升性能。


<details>
  <summary>Details</summary>
Motivation: 多说话人数据集常由多个语料库组合而成，其中ASR数据集的边界定义较松散，影响说话人日志的评估可靠性和模型泛化能力。

Method: 通过强制对齐标准化边界，并结合简单后处理。

Result: 标准化边界不仅提升了说话人日志性能（尤其在流式场景），还改善了ASR性能。

Conclusion: 标准化边界是提升说话人日志和ASR性能的有效方法。

Abstract: Neural speaker diarization is widely used for overlap-aware speaker
diarization, but it requires large multi-speaker datasets for training. To meet
this data requirement, large datasets are often constructed by combining
multiple corpora, including those originally designed for multi-speaker
automatic speech recognition (ASR). However, ASR datasets often feature loosely
defined segment boundaries that do not align with the stricter conventions of
diarization benchmarks. In this work, we show that such boundary looseness
significantly impacts the diarization error rate, reducing evaluation
reliability. We also reveal that models trained on data with varying boundary
precision tend to learn dataset-specific looseness, leading to poor
generalization across out-of-domain datasets. Training with standardized tight
boundaries via forced alignment improves not only diarization performance,
especially in streaming scenarios, but also ASR performance when combined with
simple post-processing.

</details>


### [28] [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
*Han Zhu,Wei Kang,Liyong Guo,Zengwei Yao,Fangjun Kuang,Weiji Zhuang,Zhaoqing Li,Zhifeng Han,Dong Zhang,Xin Zhang,Xingchen Song,Long Lin,Daniel Povey*

Main category: eess.AS

TL;DR: ZipVoice-Dialog是一种基于流匹配的非自回归零样本语音对话生成模型，解决了现有自回归模型推理慢和不稳定的问题，并引入了OpenDialog数据集和评估基准。


<details>
  <summary>Details</summary>
Motivation: 生成语音对话比单语音文本转语音更具挑战性，需要真实的轮流发言和不同的说话者音色，现有自回归模型存在推理慢和不稳定的问题。

Method: 1) 使用说话者轮流嵌入实现精确轮流发言；2) 采用课程学习策略稳定语音-文本对齐；3) 实现立体声对话生成。此外，构建了6.8k小时的OpenDialog数据集。

Result: ZipVoice-Dialog在可懂度、轮流发言准确性、说话者相似性和推理速度上表现优异。

Conclusion: ZipVoice-Dialog通过创新设计和OpenDialog数据集，显著提升了语音对话生成的性能，相关资源已开源。

Abstract: Generating spoken dialogue is more challenging than monologue text-to-speech
(TTS) due to the need for realistic turn-taking and distinct speaker timbres.
Existing spoken dialogue generation models, being auto-regressive, suffer from
slow and unstable inference. To overcome these limitations, we introduce
ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation
model built upon flow matching. Key designs include: 1) speaker-turn embeddings
for precise speaker turn-taking; 2) a curriculum learning strategy for stable
speech-text alignment; 3) specialized strategies to enable stereo dialogue
generation. Additionally, recognizing the lack of open-source large-scale
spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue
dataset from in-the-wild speech data. Furthermore, we established a benchmark
to comprehensively evaluate various models. Experimental results demonstrate
that ZipVoice-Dialog achieves superior performance in intelligibility, speaker
turn-taking accuracy, speaker similarity, and inference speed. Our codes, model
checkpoints, demo samples, and the OpenDialog dataset are all publicly
available at https://github.com/k2-fsa/ZipVoice.

</details>


### [29] [Microphone Occlusion Mitigation for Own-Voice Enhancement in Head-Worn Microphone Arrays Using Switching-Adaptive Beamforming](https://arxiv.org/abs/2507.09350)
*Wiebke Middelberg,Jung-Suk Lee,Saeed Bagheri Sereshki,Ali Aroudi,Vladimir Tourbabin,Daniel D. E. Wong*

Main category: eess.AS

TL;DR: 论文研究了头戴式麦克风阵列中麦克风被遮挡时的语音增强问题，提出了三种解决方法并评估了其效果。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂环境中，头戴式麦克风阵列的语音增强对用户语音通信和设备交互至关重要，但麦克风被遮挡时传输函数的变化问题鲜少被研究。

Method: 研究了三种方法：(i) 传统自适应波束成形，(ii) 在遮挡和非遮挡状态间切换预估计的波束成形系数，(iii) 混合方法（切换-自适应波束成形）。

Result: 通过真实录音和模拟遮挡实验，评估了不同方法在降噪、语音失真和语音活动检测错误鲁棒性方面的优势。

Conclusion: 论文为解决麦克风遮挡问题提供了多种有效方法，为实际应用提供了参考。

Abstract: Enhancing the user's own-voice for head-worn microphone arrays is an
important task in noisy environments to allow for easier speech communication
and user-device interaction. However, a rarely addressed challenge is the
change of the microphones' transfer functions when one or more of the
microphones gets occluded by skin, clothes or hair. The underlying problem for
beamforming-based speech enhancement is the (potentially rapidly) changing
transfer functions of both the own-voice and the noise component that have to
be accounted for to achieve optimal performance. In this paper, we address the
problem of an occluded microphone in a head-worn microphone array. We
investigate three alternative mitigation approaches by means of (i)
conventional adaptive beamforming, (ii) switching between a-priori estimates of
the beamformer coefficients for the occluded and unoccluded state, and (iii) a
hybrid approach using a switching-adaptive beamformer. In an evaluation with
real-world recordings and simulated occlusion, we demonstrate the advantages of
the different approaches in terms of noise reduction, own-voice distortion and
robustness against voice activity detection errors.

</details>


### [30] [Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model](https://arxiv.org/abs/2507.09372)
*Philippe Gonzalez,Torsten Dau,Tobias May*

Main category: eess.AS

TL;DR: 本文提出了一种基于多任务学习的深度学习方法，同时处理噪声抑制（NR）和听力损失补偿（HLC），通过可微分听觉模型优化任务平衡。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中缺乏灵活性和任务平衡的问题，特别是在噪声抑制和听力损失补偿的联合优化中。

Method: 采用多任务学习框架，利用可微分听觉模型同时预测去噪和补偿信号，输入为含噪语音和听力图。

Result: 系统在客观指标上表现与单独任务训练的系统相当，且能在推理时调整NR和HLC的平衡。

Conclusion: 多任务学习方法有效解决了NR和HLC的联合优化问题，提供了灵活的任务平衡能力。

Abstract: Deep learning-based hearing loss compensation (HLC) seeks to enhance speech
intelligibility and quality for hearing impaired listeners using neural
networks. One major challenge of HLC is the lack of a ground-truth target.
Recent works have used neural networks to emulate non-differentiable auditory
peripheral models in closed-loop frameworks, but this approach lacks
flexibility. Alternatively, differentiable auditory models allow direct
optimization, yet previous studies focused on individual listener profiles, or
joint noise reduction (NR) and HLC without balancing each task. This work
formulates NR and HLC as a multi-task learning problem, training a system to
simultaneously predict denoised and compensated signals from noisy speech and
audiograms using a differentiable auditory model. Results show the system
achieves similar objective metric performance to systems trained for each task
separately, while being able to adjust the balance between NR and HLC during
inference.

</details>


### [31] [The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM Challenge](https://arxiv.org/abs/2507.09499)
*Yuke Lin,Ming Cheng,Ze Li,Ming Li*

Main category: eess.AS

TL;DR: DKU系统在MLC-SLM挑战赛任务2中，通过集成说话人嵌入和时间边界到Qwen2.5 LLM，实现了无需说话人标签的多说话人语音识别，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决多说话人语音识别中无需Oracle说话人标签和时间边界的挑战。

Method: 采用基于Qwen2.5的LLM，集成说话人嵌入和时间边界，并通过语言特定适配器和LoRA模块增强多语言性能。

Result: 在开发和测试集上分别达到23.56%和18.08%的tcpWER，显著优于基线。

Conclusion: DKU系统在多说话人语音识别任务中表现出色，验证了方法的有效性。

Abstract: We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to
perform multi-speaker automatic speech recognition directly from raw audio
without Oracle speaker labels or time boundaries. Our approach builds upon a
diarization-aware framework integrating speaker embeddings and temporal
utterance boundaries into a Qwen2.5-based large language model (LLM). Then, we
enhance the system's multilingual performance by fine-tuning language-specific
adapters and LoRA modules within the LLM decoder. Finally, our system achieves
the tcpWER of 23.56\% and 18.08\% on the development and test sets of the
MLC-SLM dataset, substantially outperforming the official baseline.

</details>


### [32] [Enhancing Stereo Sound Event Detection with BiMamba and Pretrained PSELDnet](https://arxiv.org/abs/2507.09570)
*Wenmiao Gao,Han Yin*

Main category: eess.AS

TL;DR: 提出了一种基于预训练PSELDnet和双向Mamba序列模型的立体声SELD系统，以降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的SELD模型计算成本高，需改进。

Method: 用BiMamba模块替换Conformer模块，并采用非对称卷积捕捉音频信号的时频关系。

Result: 在DCASE2025 Task 3开发数据集上表现优于基线及原始PSELDnet，且计算资源更少。

Conclusion: BiMamba架构能有效解决SELD任务中的关键挑战。

Abstract: Pre-training methods have greatly improved the performance of sound event
localization and detection (SELD). However, existing Transformer-based models
still face high computational cost. To solve this problem, we present a stereo
SELD system using a pre-trained PSELDnet and a bidirectional Mamba sequence
model. Specifically, we replace the Conformer module with a BiMamba module. We
also use asymmetric convolutions to better capture the time and frequency
relationships in the audio signal. Test results on the DCASE2025 Task 3
development dataset show that our method performs better than both the baseline
and the original PSELDnet with a Conformer decoder. In addition, the proposed
model costs fewer computing resources than the baselines. These results show
that the BiMamba architecture is effective for solving key challenges in SELD
tasks. The source code is publicly accessible at https://github.com/
alexandergwm/DCASE2025 TASK3 Stereo PSELD Mamba.

</details>


### [33] [Low-Rank Adaptation of Deep Prior Neural Networks For Room Impulse Response Reconstruction](https://arxiv.org/abs/2507.09806)
*Mirco Pezzoli,Federico Miotello,Shoichi Koyama,Fabio Antonacci*

Main category: eess.AS

TL;DR: Deep Prior框架通过LoRA实现高效迁移学习，解决其在新声学配置下泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: Deep Prior框架在稀疏压力测量下重建声场时表现优异，但无法泛化到新的声学配置（如声源位置变化），需重新训练，计算成本高。

Method: 采用Low-Rank Adaptation (LoRA)进行迁移学习，通过低秩分解可训练参数，实现预训练网络的高效微调，嵌入MultiResUNet-based Deep Prior模型。

Result: 实验表明，LoRA微调在声源位置变化时表现优异，物理保真度高，计算开销低。

Conclusion: 迁移学习（尤其是LoRA）在声学应用中具有显著价值，能高效适应新配置。

Abstract: The Deep Prior framework has emerged as a powerful generative tool which can
be used for reconstructing sound fields in an environment from few sparse
pressure measurements. It employs a neural network that is trained solely on a
limited set of available data and acts as an implicit prior which guides the
solution of the underlying optimization problem. However, a significant
limitation of the Deep Prior approach is its inability to generalize to new
acoustic configurations, such as changes in the position of a sound source. As
a consequence, the network must be retrained from scratch for every new setup,
which is both computationally intensive and time-consuming. To address this, we
investigate transfer learning in Deep Prior via Low-Rank Adaptation (LoRA),
which enables efficient fine-tuning of a pre-trained neural network by
introducing a low-rank decomposition of trainable parameters, thus allowing the
network to adapt to new measurement sets with minimal computational overhead.
We embed LoRA into a MultiResUNet-based Deep Prior model and compare its
adaptation performance against full fine-tuning of all parameters as well as
classical retraining, particularly in scenarios where only a limited number of
microphones are used. The results indicate that fine-tuning, whether done
completely or via LoRA, is especially advantageous when the source location is
the sole changing parameter, preserving high physical fidelity, and
highlighting the value of transfer learning for acoustics applications.

</details>


### [34] [Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction](https://arxiv.org/abs/2507.09834)
*Shu-wen Yang,Byeonggeun Kim,Kuan-Po Huang,Qingming Tang,Huy Phan,Bo-Ru Lu,Harsha Sundar,Shalini Ghosh,Hung-yi Lee,Chieh-Chi Kao,Chao Wang*

Main category: eess.AS

TL;DR: 论文提出了一种基于连续值令牌的因果语言模型（LM）用于音频生成，通过令牌级扩散建模连续分布，显著优于离散方法，并在参数更少的情况下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩展自回归Transformer解码器到音频生成领域，解决音频连续性的挑战。

Method: 采用令牌级扩散建模连续值令牌分布，并提出掩码下一令牌预测任务。

Result: 在AudioCaps上，FAD和KL散度分别提升20%和40%，且参数更少（Base 193M，Large 462M）。

Conclusion: 该方法在音频生成中高效且性能优越，为连续数据建模提供了新思路。

Abstract: Autoregressive next-token prediction with the Transformer decoder has become
a de facto standard in large language models (LLMs), achieving remarkable
success in Natural Language Processing (NLP) at scale. Extending this paradigm
to audio poses unique challenges due to its inherently continuous nature. We
research audio generation with a causal language model (LM) without discrete
tokens. We leverage token-wise diffusion to model the continuous distribution
of the next continuous-valued token. Our approach delivers significant
improvements over previous discrete solution, AudioGen, achieving 20% and 40%
relative gains on AudioCaps in Frechet Audio Distance (FAD) and
Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a
novel masked next-token prediction task that incorporates masked prediction
into the causal LM framework. On AudioCaps, the innovation yields 41% and 33%
relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)
models, respectively, and is on par with the state-of-the-art (SOTA) diffusion
models. Furthermore, we achieve these results with significantly fewer
parameters -- 193M for our Base and 462M for our Large models.

</details>


### [35] [Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization](https://arxiv.org/abs/2507.09929)
*Haoyang Li,Nana Hou,Yuchen Hu,Jixun Yao,Sabato Marco Siniscalchi,Eng Siong Chng*

Main category: eess.AS

TL;DR: 本文提出了一种基于语言模型的语音增强方法，利用直接偏好优化（DPO）提升感知质量，实验显示在多个指标上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的语音增强方法可能与人耳感知不一致，本文旨在通过DPO优化感知质量。

Method: 使用UTMOS作为人类评分的代理，通过DPO优化预训练的语言模型，以生成更符合感知的语音。

Result: 在2020年深度噪声抑制挑战测试集上，DPO使语音质量指标相对提升高达56%。

Conclusion: 这是首次将DPO应用于语音增强，展示了感知对齐的语音增强的潜力。

Abstract: This work investigates speech enhancement (SE) from the perspective of
language models (LMs). We propose a novel method that leverages Direct
Preference Optimization (DPO) to improve the perceptual quality of enhanced
speech. Using UTMOS, a neural MOS prediction model, as a proxy for human
ratings, our approach guides optimization toward perceptually preferred
outputs. This differs from existing LM-based SE methods that focus on
maximizing the likelihood of clean speech tokens, which may misalign with human
perception and degrade quality despite low prediction error. Experiments on the
2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO
to a pretrained LM-based SE model yields consistent improvements across various
speech quality metrics, with relative gains of up to 56%. To our knowledge,
this is the first application of DPO to SE and the first to incorporate proxy
perceptual feedback into LM-based SE training, pointing to a promising
direction for perceptually aligned SE.

</details>


### [36] [Cyclic Multichannel Wiener Filter for Acoustic Beamforming](https://arxiv.org/abs/2507.10159)
*Giovanni Bologni,Richard Heusdens,Richard C. Hendriks*

Main category: eess.AS

TL;DR: 论文提出了一种基于循环平稳模型的循环多通道维纳滤波器（cMWF），用于语音增强，通过利用谐波频率的谱相关性进一步减少目标与处理输入之间的均方误差。


<details>
  <summary>Details</summary>
Motivation: 传统声学波束形成模型假设语音信号在短时间帧内是广义平稳的，但实际语音更适合建模为循环平稳过程，其均值和自相关具有周期性。

Method: 提出了一种循环多通道维纳滤波器（cMWF），基于循环平稳模型，利用谐波频率的谱相关性优化语音增强。

Result: 在模拟数据上，cMWF在尺度不变信噪比（SI-SDR）上有显著提升，但对基频估计精度高度敏感，限制了其在真实数据上的效果。

Conclusion: cMWF在理论上是最优的，但在实际应用中需进一步改进基频估计的准确性。

Abstract: Acoustic beamforming models typically assume wide-sense stationarity of
speech signals within short time frames. However, voiced speech is better
modeled as a cyclostationary (CS) process, a random process whose mean and
autocorrelation are $T_1$-periodic, where $\alpha_1=1/T_1$ corresponds to the
fundamental frequency of vowels. Higher harmonic frequencies are found at
integer multiples of the fundamental. This work introduces a cyclic
multichannel Wiener filter (cMWF) for speech enhancement derived from a
cyclostationary model. This beamformer exploits spectral correlation across the
harmonic frequencies of the signal to further reduce the mean-squared error
(MSE) between the target and the processed input. The proposed cMWF is optimal
in the MSE sense and reduces to the MWF when the target is wide-sense
stationary. Experiments on simulated data demonstrate considerable improvements
in scale-invariant signal-to-distortion ratio (SI-SDR) on synthetic data but
also indicate high sensitivity to the accuracy of the estimated fundamental
frequency $\alpha_1$, which limits effectiveness on real data.

</details>


### [37] [Harmonics to the Rescue: Why Voiced Speech is Not a Wss Process](https://arxiv.org/abs/2507.10176)
*Giovanni Bologni,Richard Heusdens,Richard C. Hendriks*

Main category: eess.AS

TL;DR: 论文提出语音更适合建模为循环平稳（CS）过程，而非传统的广义平稳（WSS）过程，从而提升频谱估计和信号处理性能。


<details>
  <summary>Details</summary>
Motivation: 传统WSS模型无法准确描述语音的频谱相关性，导致信号处理效果受限。

Method: 采用循环平稳（CS）模型分析语音，利用其谐波频率相关性改进系统识别和信号处理。

Result: 实验验证CS模型能更准确地估计交叉功率谱密度，提升源分离和波束成形性能。

Conclusion: 循环平稳模型更适合语音处理，为相关应用提供了更优的理论基础。

Abstract: Speech processing algorithms often rely on statistical knowledge of the
underlying process. Despite many years of research, however, the debate on the
most appropriate statistical model for speech still continues. Speech is
commonly modeled as a wide-sense stationary (WSS) process. However, the use of
the WSS model for spectrally correlated processes is fundamentally wrong, as
WSS implies spectral uncorrelation. In this paper, we demonstrate that voiced
speech can be more accurately represented as a cyclostationary (CS) process. By
employing the CS rather than the WSS model for processes that are inherently
correlated across frequency, it is possible to improve the estimation of
cross-power spectral densities (PSDs), source separation, and beamforming. We
illustrate how the correlation between harmonic frequencies of CS processes can
enhance system identification, and validate our findings using both simulated
and real speech data.

</details>


### [38] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bannò,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Main category: eess.AS

TL;DR: NLA方法利用自然语言描述评估第二语言，通过开源LLM Qwen 2.5 72B在零样本设置下评估S&I Corpus，表现接近SOTA语音LLM，优于BERT模型。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能像人类评估者一样解释和应用自然语言描述，用于第二语言评估。

Method: 使用开源LLM Qwen 2.5 72B，在零样本设置下评估S&I Corpus的响应。

Result: NLA表现接近SOTA语音LLM，优于BERT模型，在任务不匹配时表现突出，且可推广到其他数据类型和语言。

Conclusion: NLA基于可解释的语言描述，具有广泛适用性和高解释性，是一种有效的评估方法。

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


### [39] [ASDKit: A Toolkit for Comprehensive Evaluation of Anomalous Sound Detection Methods](https://arxiv.org/abs/2507.10264)
*Takuya Fujimura,Kevin Wilkinghoff,Keisuke Imoto,Tomoki Toda*

Main category: eess.AS

TL;DR: ASDKit是一个用于异常声音检测（ASD）的开源工具包，旨在通过统一框架支持多种ASD方法的训练和评估，并在多个数据集上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 促进ASD研究，提供开源工具包以统一评估多种ASD方法，解决性能评估中数据集和随机种子敏感性问题。

Method: ASDKit整合了多种ASD方法（如自编码器、判别方法和自监督学习方法），并提供训练和评估脚本，支持在DCASE 2020--2024数据集上的全面评估。

Result: 实验表明，ASDKit能复现最新技术水平，并在多个数据集和试验中识别出一致有效的技术。

Conclusion: ASDKit为ASD研究提供了高效且可靠的评估框架，有助于推动该领域的发展。

Abstract: In this paper, we introduce ASDKit, a toolkit for anomalous sound detection
(ASD) task. Our aim is to facilitate ASD research by providing an open-source
framework that collects and carefully evaluates various ASD methods. First,
ASDKit provides training and evaluation scripts for a wide range of ASD
methods, all handled within a unified framework. For instance, it includes the
autoencoder-based official DCASE baseline, representative discriminative
methods, and self-supervised learning-based methods. Second, it supports
comprehensive evaluation on the DCASE 2020--2024 datasets, enabling careful
assessment of ASD performance, which is highly sensitive to factors such as
datasets and random seeds. In our experiments, we re-evaluate various ASD
methods using ASDKit and identify consistently effective techniques across
multiple datasets and trials. We also demonstrate that ASDKit reproduces the
state-of-the-art-level performance on the considered datasets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [40] [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
*Janaki Viswanathan,Alexander Blatt,Konrad Hagemann,Dietrich Klakow*

Main category: cs.SD

TL;DR: 论文研究了匿名化空中交通管制语音数据的压力检测方法，展示了隐私保护与高性能深度学习模型的兼容性。


<details>
  <summary>Details</summary>
Motivation: 空中交通管制（ATC）工作压力大且错误后果严重，需检测压力以维持安全标准，但语音数据处理涉及隐私限制（如GDPR）。

Method: 评估了多种架构用于匿名化ATCO语音的压力检测，测试了匿名化版本的SUSAS数据集和ATC模拟数据集。

Result: 最佳网络在匿名化SUSAS数据集上达到93.6%的准确率，在匿名化ATC模拟数据集上达到80.1%的准确率。

Conclusion: 隐私保护不一定是构建高性能深度学习模型的障碍。

Abstract: Air traffic control (ATC) demands multi-tasking under time pressure with high
consequences of an error. This can induce stress. Detecting stress is a key
point in maintaining the high safety standards of ATC. However, processing ATC
voice data entails privacy restrictions, e.g. the General Data Protection
Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with
these restrictions. In this paper, different architectures for stress detection
for anonymized ATCO speech are evaluated. Our best networks reach a stress
detection accuracy of 93.6% on an anonymized version of the Speech Under
Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our
anonymized ATC simulation dataset. This shows that privacy does not have to be
an impediment in building well-performing deep-learning-based models.

</details>


### [41] [Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition](https://arxiv.org/abs/2507.09116)
*Bingshen Mu,Kun Wei,Pengcheng Guo,Lei Xie*

Main category: cs.SD

TL;DR: 论文提出多模态和多粒度的生成式错误校正（GER）方法，结合发音信息提升带口音语音识别的准确性，显著降低词错误率（WER）。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR系统已有显著改进，但在面对口音等不利条件时性能下降。现有GER方法缺乏对口音语音的针对性。

Method: 提出多模态GER（整合语音模态发音信息）和多粒度GER（结合细粒度音素信息），通过LoRA微调利用发音和语义信息。采用三阶段训练策略和HDMoLE方法合并多口音专家。

Result: 在多口音英语数据集上，相比Whisper-large-v3基线，WER相对降低67.35%。

Conclusion: 多模态和多粒度GER方法有效提升带口音语音识别性能，HDMoLE方法成功应对口音多样性挑战。

Abstract: Despite substantial improvements in ASR, performance tends to degrade when
faced with adverse conditions such as speaker accents. Generative error
correction (GER) leverages the rich linguistic knowledge and exceptional
reasoning ability of LLMs, significantly outperforming typical LM methods.
However, it lacks specificity in accented speech scenarios. In this study, we
leverage GER to improve the accuracy of transcription predictions by addressing
the two primary features of accented speech recognition. To fully leverage
pronunciation information, we propose the multi-modal GER, which integrates
pronunciation information from the speech modality, and the multi-granularity
GER, which incorporates fine-grained phoneme-level information related to
pronunciation. These two methods enable the LLM to utilize the pronunciation
information of accented speech and the semantic information from word-level
hypotheses for accurate transcription predictions through LoRA fine-tuning. On
the one hand, we employ a three-stage training strategy to train separate
multi-modal GER models for each accent to obtain mono-accent LoRA experts. By
adopting our proposed HDMoLE method, which incorporates hierarchical routing
and dynamic thresholds within the mixture of LoRA experts, we effectively merge
multiple mono-accent LoRA experts within a single multi-modal GER to overcome
the challenges posed by accent diversity. On the other hand, multi-granularity
GER leverages the N-best word-level and phoneme-level hypotheses generated by
the HDMoLE model to predict the final accented speech transcriptions.
Experimental results on the multi-accent English dataset demonstrate the
efficacy of our proposed methods. Our methods achieve a remarkable relative WER
reduction of 67.35% compared to the Whisper-large-v3 baseline.

</details>


### [42] [Towards Spatial Audio Understanding via Question Answering](https://arxiv.org/abs/2507.09195)
*Parthasaarathy Sudarsanam,Archontis Politis*

Main category: cs.SD

TL;DR: 论文提出了一种基于问答范式的新型空间音频理解框架，扩展了声音事件定位与检测（SELD）的范围，以实现空间场景的理解与推理。


<details>
  <summary>Details</summary>
Motivation: 旨在通过语言引导的方法提升空间音频的理解能力，并将语言监督融入空间场景分析中。

Method: 通过规则和大型语言模型（LLM）增强STARSS23数据集的文本描述多样性，并构建问答数据集；开发了一个基于分类任务的基线空间音频问答模型。

Result: 模型仅通过场景级问答监督训练，性能接近完全监督的帧级时空标注模型。

Conclusion: 语言引导方法在空间音频理解中具有潜力，为语言监督在空间场景分析中的应用开辟了新方向。

Abstract: In this paper, we introduce a novel framework for spatial audio understanding
of first-order ambisonic (FOA) signals through a question answering (QA)
paradigm, aiming to extend the scope of sound event localization and detection
(SELD) towards spatial scene understanding and reasoning. First, we curate and
release fine-grained spatio-temporal textual descriptions for the STARSS23
dataset using a rule-based approach, and further enhance linguistic diversity
using large language model (LLM)-based rephrasing. We also introduce a QA
dataset aligned with the STARSS23 scenes, covering various aspects such as
event presence, localization, spatial, and temporal relationships. To increase
language variety, we again leverage LLMs to generate multiple rephrasings per
question. Finally, we develop a baseline spatial audio QA model that takes FOA
signals and natural language questions as input and provides answers regarding
various occurrences, temporal, and spatial relationships of sound events in the
scene formulated as a classification task. Despite being trained solely with
scene-level question answering supervision, our model achieves performance that
is comparable to a fully supervised sound event localization and detection
model trained with frame-level spatiotemporal annotations. The results
highlight the potential of language-guided approaches for spatial audio
understanding and open new directions for integrating linguistic supervision
into spatial scene analysis.

</details>


### [43] [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
*Dominika Woszczyk,Manuel Sam Ribeiro,Thomas Merritt,Daniel Korzekwa*

Main category: cs.SD

TL;DR: 本文研究了Lombard语音风格的语音转换（VC）技术，用于提升语音清晰度，尤其是在听力损失和嘈杂环境中。通过比较隐式和显式声学特征条件模型，发现隐式条件策略在保持说话人相似性的同时，实现了与显式条件模型相当的清晰度提升。


<details>
  <summary>Details</summary>
Motivation: Lombard语音风格能提升语音清晰度，但数据收集困难且成本高。语音转换技术可作为数据不足时的替代方案。

Method: 比较了隐式和显式声学特征条件的语音转换模型，用于Lombard风格转换。

Result: 隐式条件策略在保持说话人相似性的同时，实现了与显式条件模型相当的清晰度提升。

Conclusion: 隐式条件策略是Lombard语音风格转换的有效方法，兼具清晰度和说话人相似性。

Abstract: Text-to-Speech (TTS) systems in Lombard speaking style can improve the
overall intelligibility of speech, useful for hearing loss and noisy
conditions. However, training those models requires a large amount of data and
the Lombard effect is challenging to record due to speaker and noise
variability and tiring recording conditions. Voice conversion (VC) has been
shown to be a useful augmentation technique to train TTS systems in the absence
of recorded data from the target speaker in the target speaking style. In this
paper, we are concerned with Lombard speaking style transfer. Our goal is to
convert speaker identity while preserving the acoustic attributes that define
the Lombard speaking style. We compare voice conversion models with implicit
and explicit acoustic feature conditioning. We observe that our proposed
implicit conditioning strategy achieves an intelligibility gain comparable to
the model conditioned on explicit acoustic features, while also preserving
speaker similarity.

</details>


### [44] [BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus](https://arxiv.org/abs/2507.09342)
*Emmanuel Adetiba,Abdultaofeek Abayomi,Raymond J. Kala,Ayodele H. Ifijeh,Oluwatobi E. Dare,Olabode Idowu-Bismark,Gabriel O. Sobola,Joy N. Adetiba,Monsurat Adepeju Lateef,Heather Cole-Lewis*

Main category: cs.SD

TL;DR: 研究构建了一个英语-约鲁巴语双语语音翻译语料库（BENYO-S2ST-Corpus-1），通过混合架构和音频增强算法显著降低了成本，并展示了其在语音合成模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决高资源到低资源语言对（如英语-约鲁巴语）语音翻译数据集短缺的问题。

Method: 利用现有约鲁巴语音频和转录数据，通过预训练AI模型生成英语音频，并开发音频增强算法（AcoustAug）扩展数据集。

Result: 构建了包含24,064个样本（41.20小时）的语料库，并成功训练了一个约鲁巴语语音合成模型（YoruTTS-0.5）。

Conclusion: 该语料库架构可用于其他非洲语言对的语音翻译数据集构建，有助于缩小高资源与低资源语言之间的数字鸿沟。

Abstract: There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for
high resource-to-low resource language pairs such as English-to-Yoruba. Thus,
in this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech
Translation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a
hybrid architecture we developed for large-scale direct S2ST corpus creation at
reduced cost. To achieve this, we leveraged non speech-to-speech Standard
Yoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as
the corresponding Standard English (SE) transcripts. YORULECT Corpus is small
scale(1,504) samples, and it does not have paired English audios. Therefore, we
generated the SE audios using pre-trained AI models (i.e. Facebook MMS). We
also developed an audio augmentation algorithm named AcoustAug based on three
latent acoustic features to generate augmented audios from the raw audios of
the two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,
which gives a total of 24,064 sample size. The total audio duration for the two
languages is 41.20 hours. This size is quite significant. Beyond building S2ST
models, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve
existing ones. The created corpus and Coqui framework were used to build a
pretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The
YoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates
moderate fundamental pitch similarity with the reference real-time audio.
Ultimately, the corpus architecture in this study can be leveraged by
researchers and developers to curate datasets for multilingual
high-resource-to-low-resource African languages. This will bridge the huge
digital divides in translations among high and low-resource language pairs.
BENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at
(https://bit.ly/40bGMwi).

</details>


### [45] [Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering](https://arxiv.org/abs/2507.09376)
*Bilkent Samsurya*

Main category: cs.SD

TL;DR: 提出了一种基于2D FDTD的声波传播模拟框架，用于虚拟应用中的沉浸式音频体验，重点捕捉低频波现象。


<details>
  <summary>Details</summary>
Motivation: 现有工业声学建模方法未能全面考虑声波现象，影响了虚拟应用的沉浸感。

Method: 通过2D网格离散场景几何，利用Python FDTD求解器模拟声波传播，生成多通道脉冲响应并集成到Unreal Engine音频管线。

Result: 测试结果与理论预期一致，验证了框架的有效性。

Conclusion: 该框架为商业应用提供了可行的声学模拟解决方案，并提出了混合扩展方向。

Abstract: Accurate sound propagation simulation is essential for delivering immersive
experiences in virtual applications, yet industry methods for acoustic modeling
often do not account for the full breadth of acoustic wave phenomena. This
paper proposes a novel two-dimensional (2D) finite-difference time-domain
(FDTD) framework that simulates sound propagation as a wave-based model in
Unreal Engine, with an emphasis on capturing lower frequency wave phenomena,
embedding occlusion, diffraction, reflection and interference in generated
impulse responses. The process begins by discretizing the scene geometry into a
2D grid via a top-down projection from which obstacle masks and boundary
conditions are derived. A Python-based FDTD solver injects a sine sweep at a
source position, and virtual quadraphonic microphone arrays record pressure
field responses at pre-defined listener positions. De-convolution of the
pressure responses yields multi-channel impulse responses that retain spatial
directionality which are then integrated into Unreal Engine's audio pipeline
for dynamic playback. Benchmark tests confirm agreement with analytical
expectations, and the paper outlines hybrid extensions aimed at commercial
viability.

</details>


### [46] [SC-TSE: Speaker Consistency-Aware Target Speaker Extraction](https://arxiv.org/abs/2507.09510)
*Shu Wu,Anbin Qi,Yanzhang Xie,Xiang Xie*

Main category: cs.SD

TL;DR: 提出了一种基于说话者一致性的目标说话者提取方法，通过引入质心一致性损失和条件损失抑制，提升了TSE性能。


<details>
  <summary>Details</summary>
Motivation: 现有的TSE系统依赖说话者嵌入，但存在身份混淆问题，因此从说话者一致性角度改进性能。

Method: 提出质心一致性损失和条件损失抑制，确保注册语音与提取语音的说话者一致性。

Result: 实验验证了所提方法的有效性，提升了TSE性能。

Conclusion: 通过说话者一致性优化，显著改善了目标说话者提取的效果。

Abstract: Target Speaker Extraction (TSE) uses a reference cue to extract the target
speech from a mixture. In TSE systems relying on audio cues, the speaker
embedding from the enrolled speech is crucial to performance. However, these
embeddings may suffer from speaker identity confusion. Unlike previous studies
that focus on improving speaker embedding extraction, we improve TSE
performance from the perspective of speaker consistency. In this paper, we
propose a speaker consistency-aware target speaker extraction method that
incorporates a centroid-based speaker consistency loss. This approach enhances
TSE performance by ensuring speaker consistency between the enrolled and
extracted speech. In addition, we integrate conditional loss suppression into
the training process. The experimental results validate the effectiveness of
our proposed methods in advancing the TSE performance. A speech demo is
available online.\footnote{https://sc-tse.netlify.app/

</details>


### [47] [Ensemble Confidence Calibration for Sound Event Detection in Open-environment](https://arxiv.org/abs/2507.09606)
*Yuanjian Chen,Han Yin*

Main category: cs.SD

TL;DR: 论文提出了一种基于能量的开放世界Softmax（EOW-Softmax）方法，用于声音事件检测（SED），以提升模型在开放环境中的鲁棒性和不确定性处理能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的声音事件检测常面临开放环境，现有方法对未知场景的预测过于自信且缺乏不确定性度量，限制了其适应性。

Method: 采用集成方法，提出EOW-Softmax进行置信度校准，并应用于声音发生与重叠检测（SOD），调整预测以提升适应性。

Result: 实验表明，该方法在开放环境中表现更优，减少了过度自信并提升了处理未知场景的能力。

Conclusion: EOW-Softmax有效提升了SED在开放环境中的鲁棒性和适应性，同时保持了重叠事件检测的能力。

Abstract: Sound event detection (SED) has made strong progress in controlled
environments with clear event categories. However, real-world applications
often take place in open environments. In such cases, current methods often
produce predictions with too much confidence and lack proper ways to measure
uncertainty. This limits their ability to adapt and perform well in new
situations. To solve this problem, we are the first to use ensemble methods in
SED to improve robustness against out-of-domain (OOD) inputs. We propose a
confidence calibration method called Energy-based Open-World Softmax
(EOW-Softmax), which helps the system better handle uncertainty in unknown
scenes. We further apply EOW-Softmax to sound occurrence and overlap detection
(SOD) by adjusting the prediction. In this way, the model becomes more
adaptable while keeping its ability to detect overlapping events. Experiments
show that our method improves performance in open environments. It reduces
overconfidence and increases the ability to handle OOD situations.

</details>


### [48] [THAI Speech Emotion Recognition (THAI-SER) corpus](https://arxiv.org/abs/2507.09618)
*Jilamika Wongpithayadisai,Chompakorn Chaksangchaichot,Soravitt Sangnark,Patawee Prakrankamanant,Krit Gangwanpongpun,Siwa Boonpunmongkol,Premmarin Milindasuta,Dangkamon Na-Pombejra,Sarana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.SD

TL;DR: 介绍了首个大规模的泰语语音情感识别语料库THAI-SER，包含41小时36分钟的语音数据，标注了五种情感，并通过众包和质量控制确保标注质量。


<details>
  <summary>Details</summary>
Motivation: 填补泰语语音情感识别领域的数据空白，提供高质量标注的语料库。

Method: 通过专业演员录制脚本和即兴会话，使用众包标注情感，并设计质量控制方案。

Result: 语料库的标注质量高（Krippendorff's alpha为0.692），人类识别准确率达0.772。

Conclusion: THAI-SER为泰语情感识别研究提供了可靠资源，并公开了数据和实验代码。

Abstract: We present the first sizeable corpus of Thai speech emotion recognition,
THAI-SER, containing 41 hours and 36 minutes (27,854 utterances) from 100
recordings made in different recording environments: Zoom and two studio
setups. The recordings contain both scripted and improvised sessions, acted by
200 professional actors (112 females and 88 males, aged 18 to 55) and were
directed by professional directors. There are five primary emotions: neutral,
angry, happy, sad, and frustrated, assigned to the actors when recording
utterances. The utterances are annotated with an emotional category using
crowdsourcing. To control the annotation process's quality, we also design an
extensive filtering and quality control scheme to ensure that the majority
agreement score remains above 0.71. We evaluate our annotated corpus using two
metrics: inter-annotator reliability and human recognition accuracy.
Inter-annotator reliability score was calculated using Krippendorff's alpha,
where our corpus, after filtering, achieved an alpha score of 0.692, higher
than a recommendation of 0.667. For human recognition accuracy, our corpus
scored up to 0.772 post-filtering. We also provide the results of the model
trained on the corpus evaluated on both in-corpus and cross-corpus setups. The
corpus is publicly available under a Creative Commons BY-SA 4.0, as well as our
codes for the experiments.

</details>


### [49] [MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients](https://arxiv.org/abs/2507.09750)
*Enric Gusó,Joanna Luberadzka,Umut Sayin,Xavier Serra*

Main category: cs.SD

TL;DR: 研究了四种策略对合成房间脉冲响应（RIR）数据集生态效度的提升效果，发现多频带吸收系数（MB-RIRs）在真实RIR测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 提升合成RIR数据集的生态效度，以改进单声道语音增强（SE）的性能。

Method: 在传统ISM shoebox RIRs基础上增加了多频带吸收系数、声源指向性和接收器指向性，并对比了SoundSpaces数据集的基于网格的RIRs。使用DeepFilternet3模型训练和评估。

Result: MB-RIRs在真实RIR测试中表现最佳，SDR提升0.51dB，MUSHRA评分提高8.9分。

Conclusion: MB-RIRs显著提升了语音增强性能，数据集已公开免费下载。

Abstract: We investigate the effects of four strategies for improving the ecological
validity of synthetic room impulse response (RIR) datasets for monoaural Speech
Enhancement (SE). We implement three features on top of the traditional image
source method-based (ISM) shoebox RIRs: multiband absorption coefficients,
source directivity and receiver directivity. We additionally consider
mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3
model for each RIR dataset and evaluate the performance on a test set of real
RIRs both objectively and subjectively. We find that RIRs which use
frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain
+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs
dataset is publicly available for free download.

</details>


### [50] [ASTAR-NTU solution to AudioMOS Challenge 2025 Track1](https://arxiv.org/abs/2507.09904)
*Fabian Ritter-Gutierrez,Yi-Cheng Lin,Jui-Chiang Wei,Jeremy H. M. Wong,Nancy F. Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 论文提出了一种双分支架构系统，用于自动预测音乐印象（MI）和文本对齐（TA），在AudioMOS 2025挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于专家评估的成本和可用性限制，需要自动化方法来评估文本到音乐系统的性能。

Method: 使用预训练的MuQ和RoBERTa模型作为音频和文本编码器，通过交叉注意力机制融合表示，并将MI和TA预测重构为分类任务。

Result: 在官方测试集上，系统在MI和TA的SRCC分别达到0.991和0.952，相对基线提升21.21%和31.47%。

Conclusion: 该方法显著提升了自动评估文本到音乐系统的性能，为未来研究提供了有效工具。

Abstract: Evaluation of text-to-music systems is constrained by the cost and
availability of collecting experts for assessment. AudioMOS 2025 Challenge
track 1 is created to automatically predict music impression (MI) as well as
text alignment (TA) between the prompt and the generated musical piece. This
paper reports our winning system, which uses a dual-branch architecture with
pre-trained MuQ and RoBERTa models as audio and text encoders. A
cross-attention mechanism fuses the audio and text representations. For
training, we reframe the MI and TA prediction as a classification task. To
incorporate the ordinal nature of MOS scores, one-hot labels are converted to a
soft distribution using a Gaussian kernel. On the official test set, a single
model trained with this method achieves a system-level Spearman's Rank
Correlation Coefficient (SRCC) of 0.991 for MI and 0.952 for TA, corresponding
to a relative improvement of 21.21\% in MI SRCC and 31.47\% in TA SRCC over the
challenge baseline.

</details>


### [51] [DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation](https://arxiv.org/abs/2507.10313)
*Yiru Yang*

Main category: cs.SD

TL;DR: DQLoRA是一个基于适配器引导的蒸馏框架，用于低资源和噪声条件下的鲁棒语音识别。


<details>
  <summary>Details</summary>
Motivation: 解决低资源和噪声条件下语音识别的鲁棒性问题。

Method: 使用冻结的Whisper模型作为教师提供语义监督，轻量级Wav2Vec2学生模型结合QLoRA适配器，训练采用FLEURS数据集加噪声增强，联合最小化CTC损失和KL蒸馏损失。

Result: 实现了高效的适配同时保持识别准确性。

Conclusion: DQLoRA框架在低资源和噪声条件下表现出色。

Abstract: We present a demo of DQLoRA, an Adapter-Guided Distillation framework for
robust speech recognition under low-resource and noisy conditions. Our method
employs a frozen Whisper model as the teacher to provide semantic supervision,
and a lightweight Wav2Vec2 student equipped with QLoRA-based Adapters. Training
is conducted on the FLEURS dataset augmented with DNS-style noise. The student
is optimized by jointly minimizing CTC loss and KL-based distillation loss,
enabling efficient adaptation while preserving recognition accuracy.

</details>


### [52] [Evaluating Fake Music Detection Performance Under Audio Augmentations](https://arxiv.org/abs/2507.10447)
*Tomasz Sroka,Tomasz Wężowicz,Dominik Sidorczuk,Mateusz Modrzejewski*

Main category: cs.SD

TL;DR: 研究探讨了生成音频模型检测系统在音频增强下的鲁棒性，发现即使轻微增强也会显著降低检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成音频模型的快速发展，区分人类创作与生成音乐变得困难，需评估检测模型的鲁棒性。

Method: 构建包含真实与合成音乐的数据集，应用多种音频变换，测试最新音乐深度伪造检测模型的性能。

Result: 模型性能在音频增强下显著下降，即使是轻微增强。

Conclusion: 当前音乐深度伪造检测模型对音频增强的鲁棒性不足，需进一步改进。

Abstract: With the rapid advancement of generative audio models, distinguishing between
human-composed and generated music is becoming increasingly challenging. As a
response, models for detecting fake music have been proposed. In this work, we
explore the robustness of such systems under audio augmentations. To evaluate
model generalization, we constructed a dataset consisting of both real and
synthetic music generated using several systems. We then apply a range of audio
transformations and analyze how they affect classification accuracy. We test
the performance of a recent state-of-the-art musical deepfake detection model
in the presence of audio augmentations. The performance of the model decreases
significantly even with the introduction of light augmentations.

</details>


### [53] [Radif corpus: a symbolic dataset for non-metric iranian classical music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 该研究首次构建了伊朗古典音乐中非节拍性radif曲目的完整数字语料库，包含13个组成部分的MIDI文件和详细数据表格，为计算音乐学研究提供了平台。


<details>
  <summary>Details</summary>
Motivation: 伊朗古典音乐的核心是非节拍性音乐形式，尤其是radif曲目，但缺乏数字化资源。研究旨在填补这一空白，支持计算音乐学的研究。

Method: 研究构建了包含228首曲目的数字语料库，提供MIDI文件和数据表格，详细记录音符、音程、层次结构等信息，并支持统计分析。

Result: 语料库包含281分钟的MIDI文件，涵盖13个radif组成部分，提供了音调、非节拍性等细节，并支持复杂性和相似性分析。

Conclusion: 该数字语料库为伊朗古典音乐的计算研究提供了基础，可用于旋律模式、即兴风格等研究领域。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music.
Dastgahi music serves as the underlying theoretical system for both Iranian art
music and certain folk traditions. At the heart of Iranian classical music lies
the radif, a foundational repertoire that organizes melodic material central to
performance and pedagogy.
  In this study, we introduce the first digital corpus representing the
complete non-metrical radif repertoire, covering all 13 existing components of
this repertoire. We provide MIDI files (about 281 minutes in total) and data
spreadsheets describing notes, note durations, intervals, and hierarchical
structures for 228 pieces of music. We faithfully represent the tonality
including quarter-tones, and the non-metric aspect. Furthermore, we provide
supporting basic statistics, and measures of complexity and similarity over the
corpus.
  Our corpus provides a platform for computational studies of Iranian classical
music. Researchers might employ it in studying melodic patterns, investigating
improvisational styles, or for other tasks in music information retrieval,
music theory, and computational (ethno)musicology.

</details>


### [54] [AudioMAE++: learning better masked audio representations with SwiGLU FFNs](https://arxiv.org/abs/2507.10464)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 论文提出AudioMAE++，改进音频掩码自编码器，采用macaron风格变换块和门控线性单元，在AudioSet数据集上预训练后，在10项下游任务中表现优于现有MAE方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频MAE方法仍使用传统变换块，而变换器领域已有新架构进展，因此提出改进。

Method: 采用macaron风格变换块和门控线性单元增强音频MAE。

Result: 在AudioSet预训练后，AudioMAE++在10项下游任务中表现优于现有MAE方法，且参数规模扩展性优异。

Conclusion: AudioMAE++通过新架构改进显著提升性能，适用于音频分类和语音任务。

Abstract: Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged
as a prominent approach for learning self-supervised audio representations.
While several recent papers have evaluated key aspects of training MAEs on
audio data, the majority of these approaches still leverage vanilla transformer
building blocks, whereas the transformer community has seen steady integration
of newer architectural advancements. In this work, we propose AudioMAE++, a
revamped audio masked autoencoder with two such enhancements, namely
macaron-style transformer blocks with gated linear units. When pretrained on
the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE
based approaches on 10 diverse downstream tasks, demonstrating excellent
performance on audio classification and speech-based benchmarks. The proposed
AudioMAE++ models also demonstrate excellent scaling characteristics,
outperforming directly comparable standard MAE baselines with up to 4x more
parameters.

</details>


### [55] [WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling](https://arxiv.org/abs/2507.10534)
*Qihui Yang,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.SD

TL;DR: WildFX是一个基于Docker的管道，用于生成多轨音频混合数据集，支持专业DAW后端和跨平台插件集成，以解决AI建模专业DSP工作流的挑战。


<details>
  <summary>Details</summary>
Motivation: AI在专业音频效果图（如混响、压缩、均衡）的建模中存在性能不足，难以复现实际工作流中的信号流和参数交互。

Method: WildFX通过Docker容器化，集成专业DAW后端和跨平台插件（VST/VST3/LV2/CLAP），支持复杂结构（如侧链、分频器）和并行处理。

Result: 实验验证了WildFX在盲估计混合图和插件/增益参数方面的有效性，能够连接AI研究与实际DSP需求。

Conclusion: WildFX为AI音乐生成提供了实用的工具，填补了现有方法与专业工作流之间的差距。

Abstract: Despite rapid progress in end-to-end AI music generation, AI-driven modeling
of professional Digital Signal Processing (DSP) workflows remains challenging.
In particular, while there is growing interest in neural black-box modeling of
audio effect graphs (e.g. reverb, compression, equalization), AI-based
approaches struggle to replicate the nuanced signal flow and parameter
interactions used in professional workflows. Existing differentiable plugin
approaches often diverge from real-world tools, exhibiting inferior performance
relative to simplified neural controllers under equivalent computational
constraints. We introduce WildFX, a pipeline containerized with Docker for
generating multi-track audio mixing datasets with rich effect graphs, powered
by a professional Digital Audio Workstation (DAW) backend. WildFX supports
seamless integration of cross-platform commercial plugins or any plugins in the
wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,
sidechains, crossovers) and achieving efficient parallelized processing. A
minimalist metadata interface simplifies project/plugin configuration.
Experiments demonstrate the pipeline's validity through blind estimation of
mixing graphs, plugin/gain parameters, and its ability to bridge AI research
with practical DSP demands. The code is available on:
https://github.com/IsaacYQH/WildFX.

</details>

{"id": "2507.10827", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10827", "abs": "https://arxiv.org/abs/2507.10827", "authors": ["Mengzhe Geng", "Patrick Littell", "Aidan Pine", "PENÁĆ", "Marc Tessier", "Roland Kuhn"], "title": "Supporting SENĆOTEN Language Documentation Efforts with Automatic Speech Recognition", "comment": "Accepted by ComputEL-8", "summary": "The SEN\\'{C}OTEN language, spoken on the Saanich peninsula of southern\nVancouver Island, is in the midst of vigorous language revitalization efforts\nto turn the tide of language loss as a result of colonial language policies. To\nsupport these on-the-ground efforts, the community is turning to digital\ntechnology. Automatic Speech Recognition (ASR) technology holds great promise\nfor accelerating language documentation and the creation of educational\nresources. However, developing ASR systems for SEN\\'{C}OTEN is challenging due\nto limited data and significant vocabulary variation from its polysynthetic\nstructure and stress-driven metathesis. To address these challenges, we propose\nan ASR-driven documentation pipeline that leverages augmented speech data from\na text-to-speech (TTS) system and cross-lingual transfer learning with Speech\nFoundation Models (SFMs). An n-gram language model is also incorporated via\nshallow fusion or n-best restoring to maximize the use of available data.\nExperiments on the SEN\\'{C}OTEN dataset show a word error rate (WER) of 19.34%\nand a character error rate (CER) of 5.09% on the test set with a 57.02%\nout-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER\nimproves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the\npotential of our ASR-driven pipeline to support SEN\\'{C}OTEN language\ndocumentation."}
{"id": "2507.10706", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10706", "abs": "https://arxiv.org/abs/2507.10706", "authors": ["Pradyumna Kunchala", "Ashish Patwari"], "title": "A Leap-on-Success Exhaustive Search Method to Find Optimal Robust Minimum Redundancy Arrays (RMRAs): New Array Configurations for Sensor Counts 11 to 20", "comment": "21 pages, 8 Tables, 4 Figures", "summary": "Two-fold redundant sparse arrays (TFRAs) are designed to maintain accurate\ndirection estimation even in the event of a single sensor failure, leveraging\nthe deliberate coarray redundancy infused into their design. Robust Minimum\nRedundancy Arrays (RMRAs), a specialized class of TFRAs, optimize this\nredundancy to achieve the maximum possible aperture for a given number of\nsensors. However, finding optimal RMRA configurations is an NP-hard problem,\nwith prior research reporting optimal solutions only for arrays of up to ten\nsensors. This paper presents newly discovered optimal RMRA configurations for\narray sizes 11 to 15, identified using a novel Leap-on-Success exhaustive\nsearch algorithm that efficiently reduces computational effort by terminating\nthe search upon locating optimal solutions. The robustness of these arrays was\nvalidated under all single-element failure scenarios using MATLAB simulations,\nconfirming their superior resilience compared to some existing TFRAs vulnerable\nto failures at specific sensor positions. Furthermore, near-optimal\nconfigurations for array sizes 16 to 20 are also reported, highlighting the\npotential applicability of the proposed method for larger array designs given\nsufficient computational resources. This work not only advances the\nstate-of-the-art in RMRA design but also introduces an effective search\nmethodology that can be leveraged for future explorations in array\nconfiguration optimization."}
{"id": "2507.10985", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10985", "abs": "https://arxiv.org/abs/2507.10985", "authors": ["Andrew Valdivia", "Yueming Zhang", "Hailu Xu", "Amir Ghasemkhani", "Xin Qin"], "title": "Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison", "comment": null, "summary": "This paper presents a novel approach for detecting mispronunciations by\nanalyzing deviations between a user's original speech and their voice-cloned\ncounterpart with corrected pronunciation. We hypothesize that regions with\nmaximal acoustic deviation between the original and cloned utterances indicate\npotential mispronunciations. Our method leverages recent advances in voice\ncloning to generate a synthetic version of the user's voice with proper\npronunciation, then performs frame-by-frame comparisons to identify problematic\nsegments. Experimental results demonstrate the effectiveness of this approach\nin pinpointing specific pronunciation errors without requiring predefined\nphonetic rules or extensive training data for each target language."}
{"id": "2507.10838", "categories": ["eess.SP", "cs.IT", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10838", "abs": "https://arxiv.org/abs/2507.10838", "authors": ["Gokberk Yaylali", "Ahmad Ali Khan", "Dionysios S. Kalogerias"], "title": "Waterfilling at the Edge: Optimal Percentile Resource Allocation via Risk-Averse Reduction", "comment": "5 pages, 5 figures", "summary": "We address deterministic resource allocation in point-to-point multi-terminal\nAWGN channels without inter-terminal interference, with particular focus on\noptimizing quantile transmission rates for cell-edge terminal service.\nClassical utility-based approaches -- such as minimum rate, sumrate, and\nproportional fairness -- are either overconservative, or inappropriate, or do\nnot provide a rigorous and/or interpretable foundation for fair rate\noptimization at the edge. To overcome these challenges, we employ Conditional\nValue-at-Risk (CVaR), a popular coherent risk measure, and establish its\nequivalence with the sum-least-$\\alpha$th-quantile (SL$\\alpha$Q) utility. This\nconnection enables an exact convex reformulation of the SL$\\alpha$Q\nmaximization problem, facilitating analytical tractability and precise and\ninterpretable control over cell-edge terminal performance. Utilizing Lagrangian\nduality, we provide (for the first time) parameterized closed-form solutions\nfor the optimal resource policy -- which is of waterfilling-type -- as well as\nthe associated (auxiliary) Value-at-Risk variable. We further develop a novel\ninexact dual subgradient descent algorithm of minimal complexity to determine\nglobally optimal resource policies, and we rigorously establish its\nconvergence. The resulting edge waterfilling algorithm iteratively and\nefficiently allocates resources while explicitly ensuring transmission rate\nfairness across (cell-edge) terminals. Several (even large-scale) numerical\nexperiments validate the effectiveness of the proposed method for enabling\nrobust quantile rate optimization at the edge."}
{"id": "2507.11096", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.11096", "abs": "https://arxiv.org/abs/2507.11096", "authors": ["Vassilis Sioros", "Alexandros Potamianos", "Giorgos Paraskevopoulos"], "title": "EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing", "comment": null, "summary": "In this study, we investigate leveraging cross-attention control for\nefficient audio editing within auto-regressive models. Inspired by image\nediting methodologies, we develop a Prompt-to-Prompt-like approach that guides\nedits through cross and self-attention mechanisms. Integrating a\ndiffusion-based strategy, influenced by Auffusion, we extend the model's\nfunctionality to support refinement edits, establishing a baseline for\nprompt-guided audio editing. Additionally, we introduce an alternative approach\nby incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and\npropose three editing mechanisms, based on Replacement, Reweighting, and\nRefinement of the attention scores. We employ commonly-used music-specific\nevaluation metrics and a human study, to gauge time-varying controllability,\nadherence to global text cues, and overall audio realism. The automatic and\nhuman evaluations indicate that the proposed combination of prompt-to-prompt\nguidance with autoregressive generation models significantly outperforms the\ndiffusion-based baseline in terms of melody, dynamics, and tempo of the\ngenerated audio. Our code is available at https://github.com/billsioros/EditGen"}
{"id": "2507.11036", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11036", "abs": "https://arxiv.org/abs/2507.11036", "authors": ["Salman Liaquat", "Ijaz Haider Naqvi", "Nor Muzlifah Mahyuddin"], "title": "Dual RIS-Assisted Monostatic L-Band Radar Target Detection in NLoS Scenarios", "comment": "Accepted for presentation at the 9th International Conference on\n  Communications and Future Internet", "summary": "The use of a single Reconfigurable Intelligent Surface (RIS) to boost the\nsignal-to-noise ratio (SNR) at the radar offers significant improvement in\ndetecting targets, especially in non-line-of-sight (NLoS) scenarios. However,\nthere are scenarios where no path exists between the radar and the target, even\nwith a single RIS-assisted radar, due to other present obstacles. This paper\nderives an expression for SNR in target detection scenarios where dual RISs\nassist a monostatic radar in NLoS situations. We calculate the power received\nat the radar through a dual RIS configuration. We show that the SNR performance\nof RIS-assisted radars can improve with known locations of the radar and RISs.\nOur results demonstrate that the required accuracy in target localization can\nbe achieved by controlling the number of RISs, the number of unit cells in each\nRIS, and properly selecting the locations of RISs to cover the desired region.\nThe performance of dual RIS-assisted radar systems can surpass that of single\nRIS-assisted radar systems under favourable alignment and sufficiently large\nRIS sizes."}
{"id": "2507.11233", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.11233", "abs": "https://arxiv.org/abs/2507.11233", "authors": ["David Marttila", "Joshua D. Reiss"], "title": "Improving Neural Pitch Estimation with SWIPE Kernels", "comment": "Accepted at ISMIR 2025", "summary": "Neural networks have become the dominant technique for accurate pitch and\nperiodicity estimation. Although a lot of research has gone into improving\nnetwork architectures and training paradigms, most approaches operate directly\non the raw audio waveform or on general-purpose time-frequency representations.\nWe investigate the use of Sawtooth-Inspired Pitch Estimation (SWIPE) kernels as\nan audio frontend and find that these hand-crafted, task-specific features can\nmake neural pitch estimators more accurate, robust to noise, and more\nparameter-efficient. We evaluate supervised and self-supervised\nstate-of-the-art architectures on common datasets and show that the SWIPE audio\nfrontend allows for reducing the network size by an order of magnitude without\nperformance degradation. Additionally, we show that the SWIPE algorithm on its\nown is much more accurate than commonly reported, outperforming\nstate-of-the-art self-supervised neural pitch estimators."}
{"id": "2507.11093", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.11093", "abs": "https://arxiv.org/abs/2507.11093", "authors": ["Wenxuan Sun", "Mingjie Shao", "Luteng Zhu", "Yao Ge", "Tong Zhang", "Zhi Liu"], "title": "Optimizing Fluid Antenna Configurations for Constructive Interference Precoding", "comment": null, "summary": "The fluid antenna system (FAS) has emerged as a new physical-layer concept to\nprovide enhanced propagation conditions for multiuser multiple-input\nmultiple-output (MIMO) communications over conventional fixed arrays. This work\nfocuses on minimizing the maximum symbol error probability (SEP) under $M$-ary\nphase shift keying (MPSK) signaling in a multiuser downlink equipped with FAS,\nwhere each antenna moves within nonoverlapping intervals. This specific problem\nof joint SEP minimization with FAS and constructive interference (CI) precoding\nhas not been previously addressed. The resulting problem turns out to be a\nnonconvex and nonsmooth optimization challenge. We transform the SEP\nminimization problem into a safety margin maximization problem in constructive\ninterference precoding. Then, we customize a smoothing technique and a block\ncoordinate descent (BCD) algorithm, with emphasis on low computational\ncomplexity. Simulation results show that our approach can reduce bit error rate\n(BER) compared to both the fixed arrays and FAS designed by existing particle\nswarm optimization (PSO). Also, our approach shows attractively low\ncomputational complexity compared to PSO benchmarks."}
{"id": "2507.11435", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11435", "abs": "https://arxiv.org/abs/2507.11435", "authors": ["Francesco Paissan", "Gordon Wichern", "Yoshiki Masuyama", "Ryo Aihara", "François G. Germain", "Kohei Saijo", "Jonathan Le Roux"], "title": "FasTUSS: Faster Task-Aware Unified Source Separation", "comment": "Accepted to WASPAA 2025", "summary": "Time-Frequency (TF) dual-path models are currently among the best performing\naudio source separation network architectures, achieving state-of-the-art\nperformance in speech enhancement, music source separation, and cinematic audio\nsource separation. While they are characterized by a relatively low parameter\ncount, they still require a considerable number of operations, implying a\nhigher execution time. This problem is exacerbated by the trend towards bigger\nmodels trained on large amounts of data to solve more general tasks, such as\nthe recently introduced task-aware unified source separation (TUSS) model.\nTUSS, which aims to solve audio source separation tasks using a single,\nconditional model, is built upon TF-Locoformer, a TF dual-path model combining\nconvolution and attention layers. The task definition comes in the form of a\nsequence of prompts that specify the number and type of sources to be\nextracted. In this paper, we analyze the design choices of TUSS with the goal\nof optimizing its performance-complexity trade-off. We derive two more\nefficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original\nmodel's operations by 81\\% and 73\\% with minor performance drops of 1.2~dB and\n0.4~dB averaged over all benchmarks, respectively. Additionally, we investigate\nthe impact of prompt conditioning to derive a causal TUSS model."}
{"id": "2507.11224", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11224", "abs": "https://arxiv.org/abs/2507.11224", "authors": ["Ali Khandan Boroujeni", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Stefan Köpsell", "Ghazal Bagheri", "Rafael F. Schaefer"], "title": "Fairness-Aware Secure Integrated Sensing and Communications with Fractional Programming", "comment": "Submitted to an IEEE journal", "summary": "We propose a novel secure integrated sensing and communications (ISAC) system\ndesigned to serve multiple communication users (CUs) and targets. To that end,\nwe formulate an optimization problem that maximizes the secrecy rate under\nconstraints balancing both communication and sensing requirements. To enhance\nfairness among users, an entropy-regularized fairness metric is introduced\nwithin the problem framework. We then propose a solution employing an\naccelerated quadratic transform (QT) with a non-homogeneous bound to\niteratively solve two subproblems, thereby effectively optimizing the overall\nobjective. This approach ensures robust security and fairness in resource\nallocation for ISAC systems. Finally, simulation results verify the performance\ngains in terms of average secrecy rate, average data rate, and beam gain."}
{"id": "2507.11070", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.11070", "abs": "https://arxiv.org/abs/2507.11070", "authors": ["Xinmeng Luan", "Mirco Pezzoli", "Fabio Antonacci", "Augusto Sarti"], "title": "Physics-Informed Transfer Learning for Data-Driven Sound Source Reconstruction in Near-Field Acoustic Holography", "comment": "to appear in IEEE WASPAA 2025", "summary": "We propose a transfer learning framework for sound source reconstruction in\nNear-field Acoustic Holography (NAH), which adapts a well-trained data-driven\nmodel from one type of sound source to another using a physics-informed\nprocedure. The framework comprises two stages: (1) supervised pre-training of a\ncomplex-valued convolutional neural network (CV-CNN) on a large dataset, and\n(2) purely physics-informed fine-tuning on a single data sample based on the\nKirchhoff-Helmholtz integral. This method follows the principles of transfer\nlearning by enabling generalization across different datasets through\nphysics-informed adaptation. The effectiveness of the approach is validated by\ntransferring a pre-trained model from a rectangular plate dataset to a violin\ntop plate dataset, where it shows improved reconstruction accuracy compared to\nthe pre-trained model and delivers performance comparable to that of\nCompressive-Equivalent Source Method (C-ESM). Furthermore, for successful\nmodes, the fine-tuned model outperforms both the pre-trained model and C-ESM in\naccuracy."}
{"id": "2507.11249", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.11249", "abs": "https://arxiv.org/abs/2507.11249", "authors": ["Ruohai Guo", "Jiang Zhu", "Xing Jiang", "Fengzhong Qu"], "title": "Fast and Efficient Implementation of the Maximum Likelihood Estimation for the Linear Regression with Gaussian Model Uncertainty", "comment": null, "summary": "The linear regression model with a random variable (RV) measurement matrix,\nwhere the mean of the random measurement matrix has full column rank, has been\nextensively studied. In particular, the quasiconvexity of the maximum\nlikelihood estimation (MLE) problem was established, and the corresponding\nCramer-Rao bound (CRB) was derived, leading to the development of an efficient\nbisection-based algorithm known as RV-ML. In contrast, this work extends the\nanalysis to both overdetermined and underdetermined cases, allowing the mean of\nthe random measurement matrix to be rank-deficient. A remarkable contribution\nis the proof that the equivalent MLE problem is convex and satisfies strong\nduality, strengthening previous quasiconvexity results. Moreover, it is shown\nthat in underdetermined scenarios, the randomness in the measurement matrix can\nbe beneficial for estimation under certain conditions. In addition, a fast and\nunified implementation of the MLE solution, referred to as generalized RV-ML\n(GRV-ML), is proposed, which handles a more general case including both\nunderdetermined and overdetermined systems. Extensive numerical simulations are\nprovided to validate the theoretical findings."}
{"id": "2507.11284", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11284", "abs": "https://arxiv.org/abs/2507.11284", "authors": ["Mohamed-Amine Lahmeri", "Pouya Fakharizadeh", "Víctor Mustieles-Pérez", "Martin Vossiek", "Gerhard Krieger", "Robert Schober"], "title": "Sensing Accuracy Optimization for Multi-UAV SAR Interferometry with Data Offloading", "comment": null, "summary": "The integration of unmanned aerial vehicles (UAVs) with radar imaging sensors\nhas revolutionized the monitoring of dynamic and local Earth surface processes\nby enabling high-resolution and cost-effective remote sensing. This paper\ninvestigates the optimization of the sensing accuracy of a UAV swarm deployed\nto perform multi-baseline interferometric synthetic aperture radar (InSAR)\nsensing. In conventional single-baseline InSAR systems, only one synthetic\naperture radar (SAR) antenna pair acquires two SAR images from two distinct\nangles to generate a digital elevation model (DEM) of the target area. However,\nmulti-baseline InSAR extends this concept by aggregating multiple acquisitions\nfrom different angles, thus, significantly enhancing the vertical accuracy of\nthe DEM. The heavy computations required for this process are performed on the\nground and, therefore, the radar data is transmitted in real time to a ground\nstation (GS) via a frequency-division multiple access (FDMA) air-to-ground\nbackhaul link. This work focuses on improving the sensing precision by\nminimizing the height error of the averaged DEM while simultaneously ensuring\nsensing and communication quality-of-service (QoS). To this end, the UAV\nformation, velocity, and communication power allocation are jointly optimized\nusing evolutionary algorithms (EAs). Our approach is benchmarked against\nestablished optimization methods, including genetic algorithms (GAs), simulated\nannealing (SA), and deep reinforcement learning (DRL) techniques. Numerical\nresults show that the proposed solution outperforms these baseline schemes and\nachieves sub-decimeter vertical accuracy in several scenarios. These findings\nunderline the potential of coordinated UAV swarms for delivering high-precision\nand real-time Earth observations through radar interferometry."}
{"id": "2507.11383", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11383", "abs": "https://arxiv.org/abs/2507.11383", "authors": ["V S V Sandeep", "Sai Dinesh Kancharana", "Arun Pachai Kannu"], "title": "Sparse Regression Codes exploit Multi-User Diversity without CSI", "comment": null, "summary": "We study sparse regression codes (SPARC) for multiple access channels with\nmultiple receive antennas, in non-coherent flat fading channels. We propose a\nnovel practical decoder, referred to as maximum likelihood matching pursuit\n(MLMP), which greedily finds the support of the codewords of users with partial\nmaximum likelihood metrics. As opposed to the conventional\nsuccessive-cancellation based greedy algorithms, MLMP works as a\nsuccessive-combining energy detector. We also propose MLMP modifications to\nimprove the performance at high code rates. Our studies in short block lengths\nshow that, even without any channel state information, SPARC with MLMP decoder\nachieves multi-user diversity in some scenarios, giving better error\nperformance with multiple users than that of the corresponding single-user\ncase. We also show that SPARC with MLMP performs better than conventional\nsparse recovery algorithms and pilot-aided transmissions with polar codes."}
{"id": "2507.11413", "categories": ["eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11413", "abs": "https://arxiv.org/abs/2507.11413", "authors": ["Christos N. Efrem", "Ioannis Krikidis"], "title": "Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty", "comment": "5 pages, 3 figures", "summary": "We study the joint power allocation and reflecting element (RE) activation to\nmaximize the energy efficiency (EE) in communication systems assisted by an\nintelligent reflecting surface (IRS), taking into account imperfections in\nchannel state information (CSI). The robust optimization problem is mixed\ninteger, i.e., the optimization variables are continuous (transmit power) and\ndiscrete (binary states of REs). In order to solve this challenging problem we\ndevelop two algorithms. The first one is an alternating optimization (AO)\nmethod that attains a suboptimal solution with low complexity, based on the\nLambert W function and a dynamic programming (DP) algorithm. The second one is\na branch-and-bound (B&B) method that uses AO as its subroutine and is formally\nguaranteed to achieve a globally optimal solution. Both algorithms do not\nrequire any external optimization solver for their implementation. Furthermore,\nnumerical results show that the proposed algorithms outperform the baseline\nschemes, AO achieves near-optimal performance in most cases, and B&B has low\ncomputational complexity on average."}
{"id": "2507.11091", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11091", "abs": "https://arxiv.org/abs/2507.11091", "authors": ["Yhonatan Gayer", "Vladimir Tourbabin", "Zamir Ben Hur", "David Lou Alon", "Boaz Rafaely"], "title": "Array-Aware Ambisonics and HRTF Encoding for Binaural Reproduction With Wearable Arrays", "comment": null, "summary": "This work introduces a novel method for binaural reproduction from arbitrary\nmicrophone arrays, based on array-aware optimization of Ambisonics encoding\nthrough Head-Related Transfer Function (HRTF) pre-processing. The proposed\napproach integrates array-specific information into the HRTF processing\npipeline, leading to improved spatial accuracy in binaural rendering. Objective\nevaluations demonstrate superior performance under simulated wearable-array and\nhead rotations compared to conventional Ambisonics encoding method. A listening\nexperiment further confirms that the method achieves significantly higher\nperceptual ratings in both timbre and spatial quality. Fully compatible with\nstandard Ambisonics, the proposed method offers a practical solution for\nspatial audio rendering in applications such as virtual reality, augmented\nreality, and wearable audio capture."}
{"id": "2507.11435", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11435", "abs": "https://arxiv.org/abs/2507.11435", "authors": ["Francesco Paissan", "Gordon Wichern", "Yoshiki Masuyama", "Ryo Aihara", "François G. Germain", "Kohei Saijo", "Jonathan Le Roux"], "title": "FasTUSS: Faster Task-Aware Unified Source Separation", "comment": "Accepted to WASPAA 2025", "summary": "Time-Frequency (TF) dual-path models are currently among the best performing\naudio source separation network architectures, achieving state-of-the-art\nperformance in speech enhancement, music source separation, and cinematic audio\nsource separation. While they are characterized by a relatively low parameter\ncount, they still require a considerable number of operations, implying a\nhigher execution time. This problem is exacerbated by the trend towards bigger\nmodels trained on large amounts of data to solve more general tasks, such as\nthe recently introduced task-aware unified source separation (TUSS) model.\nTUSS, which aims to solve audio source separation tasks using a single,\nconditional model, is built upon TF-Locoformer, a TF dual-path model combining\nconvolution and attention layers. The task definition comes in the form of a\nsequence of prompts that specify the number and type of sources to be\nextracted. In this paper, we analyze the design choices of TUSS with the goal\nof optimizing its performance-complexity trade-off. We derive two more\nefficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original\nmodel's operations by 81\\% and 73\\% with minor performance drops of 1.2~dB and\n0.4~dB averaged over all benchmarks, respectively. Additionally, we investigate\nthe impact of prompt conditioning to derive a causal TUSS model."}
{"id": "2507.10783", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.10783", "abs": "https://arxiv.org/abs/2507.10783", "authors": ["Kristóf Müller", "Janka Hatvani", "Márton Áron Goda", "Miklós Koller"], "title": "Standardized Evaluation of Fetal Phonocardiography Processing Methods", "comment": "17 pages, 7 figures, 7 tables", "summary": "Motivation. Phonocardiography can give access to the fetal heart rate as well\nas direct heart sound data, and is entirely passive, using no radiation of any\nkind. Approach. We discuss the currently available methods for fetal heart\nsound detection and heart rate estimation and compare them using a common\nbenchmarking platform and a pre-selected testing dataset. Compared to previous\nreviews, we evaluated the discussed methods in a standardized manner for a fair\ncomparison. Our tests included tolerance-based detection accuracy, error rates\nfor label insertions, deletions, and substitutions, and statistical measures\nfor heart rate mean square error. Results. Based on our results, there is no\ndefinite best method that can achieve the highest scores in all of the tests,\nand simpler methods could perform comparably to more complex ones. The best\nmodel for first heart sound detection achieved 97.6% F1-score, 97.4% positive\npredictive value, and 12.2+-8.0 ms mean absolute error. In terms of second\nheart sound detection the best model had 91.4% F1-score, 91.3% positive\npredictive value, and 17.3+-12.2 ms mean absolute error. For fetal heart rate a\n0.644 mean square error was achieved by the best method. Significance. Our main\nconclusion is that further standardization is required in fetal heart rate and\nheart sound detection method evaluation. The tests and algorithm\nimplementations are openly available at:\nhttps://github.com/mulkr/standard-fpcg-evaluation."}
{"id": "2507.11070", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.11070", "abs": "https://arxiv.org/abs/2507.11070", "authors": ["Xinmeng Luan", "Mirco Pezzoli", "Fabio Antonacci", "Augusto Sarti"], "title": "Physics-Informed Transfer Learning for Data-Driven Sound Source Reconstruction in Near-Field Acoustic Holography", "comment": "to appear in IEEE WASPAA 2025", "summary": "We propose a transfer learning framework for sound source reconstruction in\nNear-field Acoustic Holography (NAH), which adapts a well-trained data-driven\nmodel from one type of sound source to another using a physics-informed\nprocedure. The framework comprises two stages: (1) supervised pre-training of a\ncomplex-valued convolutional neural network (CV-CNN) on a large dataset, and\n(2) purely physics-informed fine-tuning on a single data sample based on the\nKirchhoff-Helmholtz integral. This method follows the principles of transfer\nlearning by enabling generalization across different datasets through\nphysics-informed adaptation. The effectiveness of the approach is validated by\ntransferring a pre-trained model from a rectangular plate dataset to a violin\ntop plate dataset, where it shows improved reconstruction accuracy compared to\nthe pre-trained model and delivers performance comparable to that of\nCompressive-Equivalent Source Method (C-ESM). Furthermore, for successful\nmodes, the fine-tuned model outperforms both the pre-trained model and C-ESM in\naccuracy."}
{"id": "2507.11091", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11091", "abs": "https://arxiv.org/abs/2507.11091", "authors": ["Yhonatan Gayer", "Vladimir Tourbabin", "Zamir Ben Hur", "David Lou Alon", "Boaz Rafaely"], "title": "Array-Aware Ambisonics and HRTF Encoding for Binaural Reproduction With Wearable Arrays", "comment": null, "summary": "This work introduces a novel method for binaural reproduction from arbitrary\nmicrophone arrays, based on array-aware optimization of Ambisonics encoding\nthrough Head-Related Transfer Function (HRTF) pre-processing. The proposed\napproach integrates array-specific information into the HRTF processing\npipeline, leading to improved spatial accuracy in binaural rendering. Objective\nevaluations demonstrate superior performance under simulated wearable-array and\nhead rotations compared to conventional Ambisonics encoding method. A listening\nexperiment further confirms that the method achieves significantly higher\nperceptual ratings in both timbre and spatial quality. Fully compatible with\nstandard Ambisonics, the proposed method offers a practical solution for\nspatial audio rendering in applications such as virtual reality, augmented\nreality, and wearable audio capture."}
{"id": "2507.11306", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.11306", "abs": "https://arxiv.org/abs/2507.11306", "authors": ["Marvin Sach", "Yihui Fu", "Kohei Saijo", "Wangyou Zhang", "Samuele Cornell", "Robin Scheibler", "Chenda Li", "Anurag Kumar", "Wei Wang", "Yanmin Qian", "Shinji Watanabe", "Tim Fingscheidt"], "title": "P.808 Multilingual Speech Enhancement Testing: Approach and Results of URGENT 2025 Challenge", "comment": "5 pages, 2 figures", "summary": "In speech quality estimation for speech enhancement (SE) systems, subjective\nlistening tests so far are considered as the gold standard. This should be even\nmore true considering the large influx of new generative or hybrid methods into\nthe field, revealing issues of some objective metrics. Efforts such as the\nInterspeech 2025 URGENT Speech Enhancement Challenge also involving non-English\ndatasets add the aspect of multilinguality to the testing procedure. In this\npaper, we provide a brief recap of the ITU-T P.808 crowdsourced subjective\nlistening test method. A first novel contribution is our proposed process of\nlocalizing both text and audio components of Naderi and Cutler's implementation\nof crowdsourced subjective absolute category rating (ACR) listening tests\ninvolving text-to-speech (TTS). Further, we provide surprising analyses of and\ninsights into URGENT Challenge results, tackling the reliability of (P.808) ACR\nsubjective testing as gold standard in the age of generative AI. Particularly,\nit seems that for generative SE methods, subjective (ACR MOS) and objective\n(DNSMOS, NISQA) reference-free metrics should be accompanied by objective phone\nfidelity metrics to reliably detect hallucinations. Finally, in the accepted\nversion, we will release our localization scripts and methods for easy\ndeployment for new multilingual speech enhancement subjective evaluations\naccording to ITU-T P.808."}
{"id": "2507.11427", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.11427", "abs": "https://arxiv.org/abs/2507.11427", "authors": ["Paul A. Bereuter", "Benjamin Stahl", "Mark D. Plumbley", "Alois Sontacchi"], "title": "Towards Reliable Objective Evaluation Metrics for Generative Singing Voice Separation Models", "comment": "Accepted for presentation at the IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA 2025), 5 pages", "summary": "Traditional Blind Source Separation Evaluation (BSS-Eval) metrics were\noriginally designed to evaluate linear audio source separation models based on\nmethods such as time-frequency masking. However, recent generative models may\nintroduce nonlinear relationships between the separated and reference signals,\nlimiting the reliability of these metrics for objective evaluation. To address\nthis issue, we conduct a Degradation Category Rating listening test and analyze\ncorrelations between the obtained degradation mean opinion scores (DMOS) and a\nset of objective audio quality metrics for the task of singing voice\nseparation. We evaluate three state-of-the-art discriminative models and two\nnew competitive generative models. For both discriminative and generative\nmodels, intrusive embedding-based metrics show higher correlations with DMOS\nthan conventional intrusive metrics such as BSS-Eval. For discriminative\nmodels, the highest correlation is achieved by the MSE computed on Music2Latent\nembeddings. When it comes to the evaluation of generative models, the strongest\ncorrelations are evident for the multi-resolution STFT loss and the MSE\ncalculated on MERT-L12 embeddings, with the latter also providing the most\nbalanced correlation across both model types. Our results highlight the\nlimitations of BSS-Eval metrics for evaluating generative singing voice\nseparation models and emphasize the need for careful selection and validation\nof alternative evaluation metrics for the task of singing voice separation."}
{"id": "2507.10985", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10985", "abs": "https://arxiv.org/abs/2507.10985", "authors": ["Andrew Valdivia", "Yueming Zhang", "Hailu Xu", "Amir Ghasemkhani", "Xin Qin"], "title": "Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison", "comment": null, "summary": "This paper presents a novel approach for detecting mispronunciations by\nanalyzing deviations between a user's original speech and their voice-cloned\ncounterpart with corrected pronunciation. We hypothesize that regions with\nmaximal acoustic deviation between the original and cloned utterances indicate\npotential mispronunciations. Our method leverages recent advances in voice\ncloning to generate a synthetic version of the user's voice with proper\npronunciation, then performs frame-by-frame comparisons to identify problematic\nsegments. Experimental results demonstrate the effectiveness of this approach\nin pinpointing specific pronunciation errors without requiring predefined\nphonetic rules or extensive training data for each target language."}
{"id": "2507.11435", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11435", "abs": "https://arxiv.org/abs/2507.11435", "authors": ["Francesco Paissan", "Gordon Wichern", "Yoshiki Masuyama", "Ryo Aihara", "François G. Germain", "Kohei Saijo", "Jonathan Le Roux"], "title": "FasTUSS: Faster Task-Aware Unified Source Separation", "comment": "Accepted to WASPAA 2025", "summary": "Time-Frequency (TF) dual-path models are currently among the best performing\naudio source separation network architectures, achieving state-of-the-art\nperformance in speech enhancement, music source separation, and cinematic audio\nsource separation. While they are characterized by a relatively low parameter\ncount, they still require a considerable number of operations, implying a\nhigher execution time. This problem is exacerbated by the trend towards bigger\nmodels trained on large amounts of data to solve more general tasks, such as\nthe recently introduced task-aware unified source separation (TUSS) model.\nTUSS, which aims to solve audio source separation tasks using a single,\nconditional model, is built upon TF-Locoformer, a TF dual-path model combining\nconvolution and attention layers. The task definition comes in the form of a\nsequence of prompts that specify the number and type of sources to be\nextracted. In this paper, we analyze the design choices of TUSS with the goal\nof optimizing its performance-complexity trade-off. We derive two more\nefficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original\nmodel's operations by 81\\% and 73\\% with minor performance drops of 1.2~dB and\n0.4~dB averaged over all benchmarks, respectively. Additionally, we investigate\nthe impact of prompt conditioning to derive a causal TUSS model."}

{"id": "2507.11636", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11636", "abs": "https://arxiv.org/abs/2507.11636", "authors": ["Junyi Fan", "Donald Williamson"], "title": "JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs", "comment": "Accepted to WASPAA 2025", "summary": "Speech quality assessment (SQA) is often used to learn a mapping from a\nhigh-dimensional input space to a scalar that represents the mean opinion score\n(MOS) of the perceptual speech quality. Learning such a mapping is challenging\nfor many reasons, but largely because MOS exhibits high levels of inherent\nvariance due to perceptual and experimental-design differences. Many solutions\nhave been proposed, but many approaches do not properly incorporate perceptual\nfactors into their learning algorithms (beyond the MOS label), which could lead\nto unsatisfactory results. To this end, we propose JSQA, a two-stage framework\nthat pretrains an audio encoder using perceptually-guided contrastive learning\non just noticeable difference (JND) pairs, followed by fine-tuning for MOS\nprediction. We first generate pairs of audio data within JND levels, which are\nthen used to pretrain an encoder to leverage perceptual quality similarity\ninformation and map it into an embedding space. The JND pairs come from clean\nLibriSpeech utterances that are mixed with background noise from CHiME-3, at\ndifferent signal-to-noise ratios (SNRs). The encoder is later fine-tuned with\naudio samples from the NISQA dataset for MOS prediction. Experimental results\nsuggest that perceptually-inspired contrastive pretraining significantly\nimproves the model performance evaluated by various metrics when compared\nagainst the same network trained from scratch without pretraining. These\nfindings suggest that incorporating perceptual factors into pretraining greatly\ncontributes to the improvement in performance for SQA.", "AI": {"tldr": "JSQA\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u97f3\u9891\u7f16\u7801\u5668\uff0c\u518d\u5fae\u8c03\u7528\u4e8eMOS\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\uff08SQA\uff09\u4e2d\uff0cMOS\u6807\u7b7e\u7684\u9ad8\u65b9\u5dee\u548c\u7f3a\u4e4f\u611f\u77e5\u56e0\u7d20\u7684\u878d\u5165\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "JSQA\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eJND\u5bf9\u7684\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u97f3\u9891\u7f16\u7801\u5668\uff1b2\uff09\u5fae\u8c03\u7528\u4e8eMOS\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u611f\u77e5\u5bf9\u6bd4\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5c06\u611f\u77e5\u56e0\u7d20\u878d\u5165\u9884\u8bad\u7ec3\u5bf9SQA\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.12045", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12045", "abs": "https://arxiv.org/abs/2507.12045", "authors": ["Junwei Ji", "Dongyuan Shi", "Zhengding Luo", "Boxiang Wang", "Woon-Seng Gan"], "title": "Self-Boosted Weight-Constrained FxLMS: A Robustness Distributed Active Noise Control Algorithm Without Internode Communication", "comment": null, "summary": "Compared to the conventional centralized multichannel active noise control\n(MCANC) algorithm, which requires substantial computational resources,\ndecentralized approaches exhibit higher computational efficiency but typically\nresult in inferior noise reduction performance. To enhance performance,\ndistributed ANC methods have been introduced, enabling information exchange\namong ANC nodes; however, the resulting communication latency often compromises\nsystem stability. To overcome these limitations, we propose a self-boosted\nweight-constrained filtered-reference least mean square (SB-WCFxLMS) algorithm\nfor the distributed MCANC system without internode communication. The WCFxLMS\nalgorithm is specifically designed to mitigate divergence issues caused by the\ninternode cross-talk effect. The self-boosted strategy lets each ANC node\nindependently adapt its constraint parameters based on its local noise\nreduction performance, thus ensuring effective noise cancellation without the\nneed for inter-node communication. With the assistance of this mechanism, this\napproach significantly reduces both computational complexity and communication\noverhead. Numerical simulations employing real acoustic paths and compressor\nnoise validate the effectiveness and robustness of the proposed system. The\nresults demonstrate that our proposed method achieves satisfactory noise\ncancellation performance with minimal resource requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u589e\u5f3a\u6743\u91cd\u7ea6\u675f\u6ee4\u6ce2\u53c2\u8003\u6700\u5c0f\u5747\u65b9\uff08SB-WCFxLMS\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u591a\u901a\u9053\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u7cfb\u7edf\uff0c\u65e0\u9700\u8282\u70b9\u95f4\u901a\u4fe1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u591a\u901a\u9053\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\uff08MCANC\uff09\u7b97\u6cd5\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800c\u5206\u6563\u5f0f\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\u4f46\u566a\u58f0\u6291\u5236\u6027\u80fd\u8f83\u5dee\u3002\u5206\u5e03\u5f0fANC\u65b9\u6cd5\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901a\u4fe1\u5ef6\u8fdf\u5f71\u54cd\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86WCFxLMS\u7b97\u6cd5\u4ee5\u89e3\u51b3\u8282\u70b9\u95f4\u4e32\u6270\u6548\u5e94\u5f15\u8d77\u7684\u53d1\u6563\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u81ea\u589e\u5f3a\u7b56\u7565\u4f7f\u6bcf\u4e2aANC\u8282\u70b9\u72ec\u7acb\u8c03\u6574\u7ea6\u675f\u53c2\u6570\uff0c\u65e0\u9700\u8282\u70b9\u95f4\u901a\u4fe1\u3002", "result": "\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u9700\u6c42\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6ee1\u610f\u7684\u566a\u58f0\u6291\u5236\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684SB-WCFxLMS\u7b97\u6cd5\u5728\u5206\u5e03\u5f0fMCANC\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u987e\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2507.12081", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12081", "abs": "https://arxiv.org/abs/2507.12081", "authors": ["Ahmad Aloradi", "\u00dcnal Ege Gaznepoglu", "Emanu\u00ebl A. P. Habets", "Daniel Tenbrinck"], "title": "VoxATtack: A Multimodal Attack on Voice Anonymization Systems", "comment": "5 pages, 3 figures, 3 tables, accepted at WASPAA 2025", "summary": "Voice anonymization systems aim to protect speaker privacy by obscuring vocal\ntraits while preserving the linguistic content relevant for downstream\napplications. However, because these linguistic cues remain intact, they can be\nexploited to identify semantic speech patterns associated with specific\nspeakers. In this work, we present VoxATtack, a novel multimodal\nde-anonymization model that incorporates both acoustic and textual information\nto attack anonymization systems. While previous research has focused on\nrefining speaker representations extracted from speech, we show that\nincorporating textual information with a standard ECAPA-TDNN improves the\nattacker's performance. Our proposed VoxATtack model employs a dual-branch\narchitecture, with an ECAPA-TDNN processing anonymized speech and a pretrained\nBERT encoding the transcriptions. Both outputs are projected into embeddings of\nequal dimensionality and then fused based on confidence weights computed on a\nper-utterance basis. When evaluating our approach on the VoicePrivacy Attacker\nChallenge (VPAC) dataset, it outperforms the top-ranking attackers on five out\nof seven benchmarks, namely B3, B4, B5, T8-5, and T12-5. To further boost\nperformance, we leverage anonymized speech and SpecAugment as augmentation\ntechniques. This enhancement enables VoxATtack to achieve state-of-the-art on\nall VPAC benchmarks, after scoring 20.6% and 27.2% average equal error rate on\nT10-2 and T25-1, respectively. Our results demonstrate that incorporating\ntextual information and selective data augmentation reveals critical\nvulnerabilities in current voice anonymization methods and exposes potential\nweaknesses in the datasets used to evaluate them.", "AI": {"tldr": "VoxATtack\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u53bb\u533f\u540d\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u58f0\u5b66\u548c\u6587\u672c\u4fe1\u606f\u653b\u51fb\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u4fdd\u7559\u8bed\u8a00\u5185\u5bb9\u4f46\u53ef\u80fd\u66b4\u9732\u8bf4\u8bdd\u8005\u8bed\u4e49\u6a21\u5f0f\uff0c\u9700\u7814\u7a76\u5176\u6f0f\u6d1e\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0cECAPA-TDNN\u5904\u7406\u533f\u540d\u8bed\u97f3\uff0cBERT\u7f16\u7801\u6587\u672c\uff0c\u878d\u5408\u540e\u57fa\u4e8e\u7f6e\u4fe1\u6743\u91cd\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728VPAC\u6570\u636e\u96c6\u4e0a\uff0cVoxATtack\u57285/7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6570\u636e\u589e\u5f3a\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u548c\u6570\u636e\u589e\u5f3a\u63ed\u793a\u4e86\u5f53\u524d\u8bed\u97f3\u533f\u540d\u5316\u65b9\u6cd5\u7684\u6f0f\u6d1e\u548c\u6570\u636e\u96c6\u6f5c\u5728\u5f31\u70b9\u3002"}}
{"id": "2507.12122", "categories": ["eess.AS", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12122", "abs": "https://arxiv.org/abs/2507.12122", "authors": ["Tong Xiao", "Reinhild Roden", "Matthias Blau", "Simon Doclo"], "title": "Soft-Constrained Spatially Selective Active Noise Control for Open-fitting Hearables", "comment": "Accepted at IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "Recent advances in spatially selective active noise control (SSANC) using\nmultiple microphones have enabled hearables to suppress undesired noise while\npreserving desired speech from a specific direction. Aiming to achieve minimal\nspeech distortion, a hard constraint has been used in previous work in the\noptimization problem to compute the control filter. In this work, we propose a\nsoft-constrained SSANC system that uses a frequency-independent parameter to\ntrade off between speech distortion and noise reduction. We derive both time-\nand frequency-domain formulations, and show that conventional active noise\ncontrol and hard-constrained SSANC represent two limiting cases of the proposed\ndesign. We evaluate the system through simulations using a pair of open-fitting\nhearables in an anechoic environment with one speech source and two noise\nsources. The simulation results validate the theoretical derivations and\ndemonstrate that for a broad range of the trade-off parameter, the\nsignal-to-noise ratio and the speech quality and intelligibility in terms of\nPESQ and ESTOI can be substantially improved compared to the hard-constrained\ndesign.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u7ea6\u675f\u7684\u7a7a\u95f4\u9009\u62e9\u6027\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\uff08SSANC\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u9891\u7387\u65e0\u5173\u53c2\u6570\u5728\u8bed\u97f3\u5931\u771f\u548c\u566a\u58f0\u6291\u5236\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u566a\u6bd4\u548c\u8bed\u97f3\u8d28\u91cf\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u8bed\u97f3\u5931\u771f\uff0c\u540c\u65f6\u5b9e\u73b0\u566a\u58f0\u6291\u5236\uff0c\u6539\u8fdb\u4f20\u7edf\u786c\u7ea6\u675fSSANC\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u8f6f\u7ea6\u675fSSANC\u7cfb\u7edf\uff0c\u63a8\u5bfc\u65f6\u57df\u548c\u9891\u57df\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8f6f\u7ea6\u675f\u8bbe\u8ba1\u5728\u5e7f\u6cdb\u53c2\u6570\u8303\u56f4\u5185\u663e\u8457\u63d0\u5347\u4fe1\u566a\u6bd4\u3001\u8bed\u97f3\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\uff08PESQ\u548cESTOI\uff09\u3002", "conclusion": "\u8f6f\u7ea6\u675fSSANC\u7cfb\u7edf\u4f18\u4e8e\u786c\u7ea6\u675f\u8bbe\u8ba1\uff0c\u4e3a\u566a\u58f0\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11777", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.11777", "abs": "https://arxiv.org/abs/2507.11777", "authors": ["Ivan Viakhirev", "Daniil Sirota", "Aleksandr Smirnov", "Kirill Borodin"], "title": "Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake Detection", "comment": null, "summary": "Advances in voice conversion and text-to-speech synthesis have made automatic\nspeaker verification (ASV) systems more susceptible to spoofing attacks. This\nwork explores modest refinements to the AASIST anti-spoofing architecture. It\nincorporates a frozen Wav2Vec 2.0 encoder to retain self-supervised speech\nrepresentations in limited-data settings, substitutes the original graph\nattention block with a standardized multi-head attention module using\nheterogeneous query projections, and replaces heuristic frame-segment fusion\nwith a trainable, context-aware integration layer. When evaluated on the\nASVspoof 5 corpus, the proposed system reaches a 7.6\\% equal error rate (EER),\nimproving on a re-implemented AASIST baseline under the same training\nconditions. Ablation experiments suggest that each architectural change\ncontributes to the overall performance, indicating that targeted adjustments to\nestablished models may help strengthen speech deepfake detection in practical\nscenarios. The code is publicly available at\nhttps://github.com/KORALLLL/AASIST_SCALING.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6539\u8fdbAASIST\u67b6\u6784\uff0c\u7ed3\u5408Wav2Vec 2.0\u7f16\u7801\u5668\u548c\u591a\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u8bed\u97f3\u9632\u4f2a\u68c0\u6d4b\u6027\u80fd\uff0c\u5728ASVspoof 5\u6570\u636e\u96c6\u4e0a\u8fbe\u52307.6%\u7684EER\u3002", "motivation": "\u8bed\u97f3\u8f6c\u6362\u548c\u6587\u672c\u5230\u8bed\u97f3\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u66f4\u5bb9\u6613\u53d7\u5230\u6b3a\u9a97\u653b\u51fb\uff0c\u9700\u8981\u6539\u8fdb\u9632\u4f2a\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u51bb\u7ed3\u7684Wav2Vec 2.0\u7f16\u7801\u5668\uff0c\u66ff\u6362\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u4e3a\u591a\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408\u5c42\u3002", "result": "\u5728ASVspoof 5\u6570\u636e\u96c6\u4e0a\uff0c\u6539\u8fdb\u540e\u7684\u7cfb\u7edfEER\u4e3a7.6%\uff0c\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u9488\u5bf9\u5df2\u6709\u6a21\u578b\u7684\u9488\u5bf9\u6027\u8c03\u6574\u53ef\u4ee5\u63d0\u5347\u8bed\u97f3\u9632\u4f2a\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.11783", "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC", "A.1; I.2; I.5; J.3"], "pdf": "https://arxiv.org/pdf/2507.11783", "abs": "https://arxiv.org/abs/2507.11783", "authors": ["Gayal Kuruppu", "Neeraj Wagh", "Yogatheesan Varatharajah"], "title": "Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions", "comment": "20 pages, 5 figures, 2 tables", "summary": "Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e8610\u79cd\u65e9\u671fEEG\u57fa\u7840\u6a21\u578b\uff08EEG-FMs\uff09\uff0c\u5206\u6790\u4e86\u5176\u65b9\u6cd5\u3001\u5b9e\u8bc1\u7ed3\u679c\u53ca\u7814\u7a76\u7a7a\u767d\uff0c\u6307\u51fa\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u76d1\u7763\u5f0fEEG\u7f16\u7801\u5668\u5728\u7a33\u5065\u6027\u548c\u6807\u6ce8\u4f9d\u8d56\u4e0a\u7684\u4e0d\u8db3\u4fc3\u4f7f\u8f6c\u5411\u81ea\u76d1\u7763\u7684EEG\u57fa\u7840\u6a21\u578b\uff08EEG-FMs\uff09\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u6548\u679c\u548c\u957f\u671f\u7814\u7a76\u65b9\u5411\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff010\u79cd\u65e9\u671fEEG-FMs\uff0c\u5206\u6790\u5176\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u5e8f\u5217\u7684\u5efa\u6a21\u548cTransformer\u67b6\u6784\uff09\u548c\u81ea\u76d1\u7763\u7b56\u7565\uff08\u5982\u63a9\u7801\u5e8f\u5217\u91cd\u5efa\uff09\u3002", "result": "\u53d1\u73b0\u5f53\u524dEEG-FMs\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u4e00\u81f4\u4e14\u6709\u9650\uff0c\u96be\u4ee5\u8bc4\u4f30\u5176\u5b9e\u7528\u6027\uff1b\u672a\u6765\u9700\u6807\u51c6\u5316\u8bc4\u4f30\u3001\u6269\u5c55\u89c4\u6a21\u5e76\u4f18\u5316\u5b66\u4e60\u6d41\u7a0b\u3002", "conclusion": "\u672a\u6765\u5e94\u901a\u8fc7\u5f00\u53d1\u57fa\u51c6\u3001\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u7ed3\u5408\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u63d0\u5347EEG-FMs\u7684\u5b9e\u7528\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11812", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11812", "abs": "https://arxiv.org/abs/2507.11812", "authors": ["Wei Huang", "Yuqiang Huang", "Yanan Wu", "Tianhe Xu", "Junting Wang", "Hao Zhang"], "title": "A Multimodal Data Fusion Generative Adversarial Network for Real Time Underwater Sound Speed Field Construction", "comment": null, "summary": "Sound speed profiles (SSPs) are essential parameters underwater that affects\nthe propagation mode of underwater signals and has a critical impact on the\nenergy efficiency of underwater acoustic communication and accuracy of\nunderwater acoustic positioning. Traditionally, SSPs can be obtained by\nmatching field processing (MFP), compressive sensing (CS), and deep learning\n(DL) methods. However, existing methods mainly rely on on-site underwater sonar\nobservation data, which put forward strict requirements on the deployment of\nsonar observation systems. To achieve high-precision estimation of sound\nvelocity distribution in a given sea area without on-site underwater data\nmeasurement, we propose a multi-modal data-fusion generative adversarial\nnetwork model with residual attention block (MDF-RAGAN) for SSP construction.\nTo improve the model's ability for capturing global spatial feature\ncorrelations, we embedded the attention mechanisms, and use residual modules\nfor deeply capturing small disturbances in the deep ocean sound velocity\ndistribution caused by changes of SST. Experimental results on real open\ndataset show that the proposed model outperforms other state-of-the-art\nmethods, which achieves an accuracy with an error of less than 0.3m/s.\nSpecifically, MDF-RAGAN not only outperforms convolutional neural network (CNN)\nand spatial interpolation (SITP) by nearly a factor of two, but also achieves\nabout 65.8\\% root mean square error (RMSE) reduction compared to mean profile,\nwhich fully reflects the enhancement of overall profile matching by\nmulti-source fusion and cross-modal attention.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6a21\u578b\uff08MDF-RAGAN\uff09\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u58f0\u901f\u5206\u5e03\uff0c\u65e0\u9700\u73b0\u573a\u6c34\u4e0b\u6570\u636e\u6d4b\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u73b0\u573a\u58f0\u7eb3\u89c2\u6d4b\u6570\u636e\uff0c\u90e8\u7f72\u8981\u6c42\u4e25\u683c\uff0c\u9650\u5236\u4e86\u58f0\u901f\u5206\u5e03\u7684\u4f30\u8ba1\u7cbe\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6a21\u578b\uff0c\u5d4c\u5165\u6ce8\u610f\u529b\u673a\u5236\u548c\u6b8b\u5dee\u6a21\u5757\uff0c\u4ee5\u6355\u6349\u5168\u5c40\u7a7a\u95f4\u7279\u5f81\u548c\u5c0f\u6270\u52a8\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u8bef\u5dee\u5c0f\u4e8e0.3m/s\uff0c\u4f18\u4e8eCNN\u548cSITP\u65b9\u6cd5\uff0cRMSE\u964d\u4f4e65.8%\u3002", "conclusion": "MDF-RAGAN\u901a\u8fc7\u591a\u6e90\u878d\u5408\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u663e\u8457\u63d0\u5347\u4e86\u58f0\u901f\u5206\u5e03\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u5339\u914d\u6027\u3002"}}
{"id": "2507.11846", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11846", "abs": "https://arxiv.org/abs/2507.11846", "authors": ["Yulu Guo", "Tongjia Zhang", "Xiangwen Gu", "Shu Sun", "Meixia Tao", "Ruifeng Gao"], "title": "Directional Measurements and Analysis for FR3 Low-Altitude Channels in a Campus Environment", "comment": null, "summary": "In this paper, we present detailed low-altitude channel measurements at the\nFR3 band in an outdoor campus environment. Using a time-domain channel sounder\nsystem, we conduct two types of measurements: path loss measurements by moving\nthe transmitter (Tx) at one-meter intervals along a 26-point rooftop path, and\ndirectional power angular spectrum measurements through antenna scanning at\nhalf-power beam width intervals. The path loss analysis across different Rx\nshows that the close-in model outperforms conventional 3GPP models and\nheight-corrected variants, with path loss exponents close to free space values\nindicating line-of-sight dominance. The power angular spectrum measurements\nshow that propagation behavior varies significantly with environmental\nconditions. Closer Rx exhibit stronger sensitivity to ground reflections during\ndownward Tx tilting, while obstructed links display uniform angular\ncharacteristics due to dominant scattering effects, and corridor environments\nproduce asymmetric power distributions. These results indicate that\nlow-altitude propagation is characterized by complex interactions between Tx\nheight and ground scattering mechanisms, providing fundamental insights for\nchannel modeling in emerging mid-band communication systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4f4e\u7a7aFR3\u9891\u6bb5\u4fe1\u9053\u6d4b\u91cf\uff0c\u5206\u6790\u4e86\u8def\u5f84\u635f\u8017\u548c\u529f\u7387\u89d2\u8c31\uff0c\u53d1\u73b0\u8fd1\u8ddd\u79bb\u6a21\u578b\u4f18\u4e8e3GPP\u6a21\u578b\uff0c\u4e14\u4f20\u64ad\u884c\u4e3a\u53d7\u73af\u5883\u6761\u4ef6\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4f4e\u7a7aFR3\u9891\u6bb5\u4fe1\u9053\u7279\u6027\uff0c\u4e3a\u65b0\u5174\u4e2d\u9891\u901a\u4fe1\u7cfb\u7edf\u7684\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u57fa\u7840\u89c1\u89e3\u3002", "method": "\u4f7f\u7528\u65f6\u57df\u4fe1\u9053\u63a2\u6d4b\u7cfb\u7edf\u8fdb\u884c\u8def\u5f84\u635f\u8017\u6d4b\u91cf\u548c\u5b9a\u5411\u529f\u7387\u89d2\u8c31\u6d4b\u91cf\u3002", "result": "\u8fd1\u8ddd\u79bb\u6a21\u578b\u4f18\u4e8e3GPP\u6a21\u578b\uff0c\u4f20\u64ad\u884c\u4e3a\u53d7\u5730\u9762\u53cd\u5c04\u548c\u6563\u5c04\u6548\u5e94\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u4f4e\u7a7a\u4f20\u64ad\u7279\u6027\u590d\u6742\uff0c\u9700\u8003\u8651\u53d1\u5c04\u9ad8\u5ea6\u548c\u5730\u9762\u6563\u5c04\u673a\u5236\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2507.11925", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.11925", "abs": "https://arxiv.org/abs/2507.11925", "authors": ["Shuichiro Nishigori", "Koichi Saito", "Naoki Murata", "Masato Hirano", "Shusuke Takahashi", "Yuki Mitsufuji"], "title": "Schr\u00f6dinger Bridge Consistency Trajectory Models for Speech Enhancement", "comment": null, "summary": "Speech enhancement (SE) utilizing diffusion models is a promising technology\nthat improves speech quality in noisy speech data. Furthermore, the\nSchr\\\"odinger bridge (SB) has recently been used in diffusion-based SE to\nimprove speech quality by resolving a mismatch between the endpoint of the\nforward process and the starting point of the reverse process. However, the SB\nstill exhibits slow inference owing to the necessity of a large number of\nfunction evaluations (NFE) for inference to obtain high-quality results. While\nConsistency Models (CMs) address this issue by employing consistency training\nthat uses distillation from pretrained models in the field of image generation,\nit does not improve generation quality when the number of steps increases. As a\nsolution to this problem, Consistency Trajectory Models (CTMs) not only\naccelerate inference speed but also maintain a favorable trade-off between\nquality and speed. Furthermore, SoundCTM demonstrates the applicability of CTM\ntechniques to the field of sound generation. In this paper, we present\nSchr\\\"odinger bridge Consistency Trajectory Models (SBCTM) by applying the\nCTM's technique to the Schr\\\"odinger bridge for SE. Additionally, we introduce\na novel auxiliary loss, including a perceptual loss, into the original CTM's\ntraining framework. As a result, SBCTM achieves an approximately 16x\nimprovement in the real-time factor (RTF) compared to the conventional\nSchr\\\"odinger bridge for SE. Furthermore, the favorable trade-off between\nquality and speed in SBCTM allows for time-efficient inference by limiting\nmulti-step refinement to cases where 1-step inference is insufficient. Our\ncode, pretrained models, and audio samples are available at\nhttps://github.com/sony/sbctm/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchr\u00f6dinger\u6865\u7684\u4e00\u81f4\u6027\u8f68\u8ff9\u6a21\u578b\uff08SBCTM\uff09\uff0c\u7528\u4e8e\u8bed\u97f3\u589e\u5f3a\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u6846\u67b6\u548c\u5f15\u5165\u8f85\u52a9\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3Schr\u00f6dinger\u6865\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u56e0\u9700\u8981\u5927\u91cf\u51fd\u6570\u8bc4\u4f30\u800c\u5bfc\u81f4\u7684\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "\u5c06\u4e00\u81f4\u6027\u8f68\u8ff9\u6a21\u578b\uff08CTM\uff09\u6280\u672f\u5e94\u7528\u4e8eSchr\u00f6dinger\u6865\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8f85\u52a9\u635f\u5931\uff08\u5305\u62ec\u611f\u77e5\u635f\u5931\uff09\u6539\u8fdb\u8bad\u7ec3\u6846\u67b6\u3002", "result": "SBCTM\u5b9e\u73b0\u4e86\u7ea616\u500d\u7684\u5b9e\u65f6\u56e0\u5b50\uff08RTF\uff09\u63d0\u5347\uff0c\u5e76\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "SBCTM\u5728\u8bed\u97f3\u589e\u5f3a\u9886\u57df\u5c55\u793a\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u573a\u666f\u3002"}}
{"id": "2507.11912", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11912", "abs": "https://arxiv.org/abs/2507.11912", "authors": ["Tzu-Hsuan Chou", "Nicolo Michelusi", "David J. Love", "James V. Krogmeier"], "title": "Joint UAV Placement and Transceiver Design in Multi-User Wireless Relay Networks", "comment": "This paper is accepted for publication in IEEE Transactions on\n  Communications. 17 pages", "summary": "In this paper, a novel approach is proposed to improve the minimum\nsignal-to-interference-plus-noise-ratio (SINR) among users in non-orthogonal\nmulti-user wireless relay networks, by optimizing the placement of unmanned\naerial vehicle (UAV) relays, relay beamforming, and receive combining. The\ndesign is separated into two problems: beamforming-aware UAV placement\noptimization and transceiver design for minimum SINR maximization. A\nsignificant challenge in beamforming-aware UAV placement optimization is the\nlack of instantaneous channel state information (CSI) prior to deploying UAV\nrelays, making it difficult to derive the beamforming SINR in non-orthogonal\nmulti-user transmission. To address this issue, an approximation of the\nexpected beamforming SINR is derived using the narrow beam property of a\nmassive MIMO base station. Based on this, a UAV placement algorithm is proposed\nto provide UAV positions that improve the minimum expected beamforming SINR\namong users, using a difference-of-convex framework. Subsequently, after\ndeploying the UAV relays to the optimized positions, and with estimated CSI\navailable, a joint relay beamforming and receive combining (JRBC) algorithm is\nproposed to optimize the transceiver to improve the minimum beamforming SINR\namong users, using a block-coordinate descent approach. Numerical results show\nthat the UAV placement algorithm combined with the JRBC algorithm provides a\n4.6 dB SINR improvement over state-of-the-art schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65e0\u4eba\u673a\u4e2d\u7ee7\u653e\u7f6e\u3001\u6ce2\u675f\u6210\u5f62\u548c\u63a5\u6536\u7ec4\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u975e\u6b63\u4ea4\u591a\u7528\u6237\u65e0\u7ebf\u4e2d\u7ee7\u7f51\u7edc\u4e2d\u7684\u6700\u5c0f\u4fe1\u5e72\u566a\u6bd4\uff08SINR\uff09\u3002", "motivation": "\u5728\u975e\u6b63\u4ea4\u591a\u7528\u6237\u4f20\u8f93\u4e2d\uff0c\u7f3a\u4e4f\u77ac\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u5bfc\u81f4\u65e0\u4eba\u673a\u4e2d\u7ee7\u653e\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u8bbe\u8ba1\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u57fa\u4e8e\u6ce2\u675f\u6210\u5f62\u7684\u65e0\u4eba\u673a\u653e\u7f6e\u4f18\u5316\u548c\u6700\u5c0fSINR\u6700\u5927\u5316\u7684\u6536\u53d1\u5668\u8bbe\u8ba1\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u7a84\u6ce2\u675f\u7279\u6027\u7684\u9884\u671fSINR\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u51f8\u5dee\u6846\u67b6\u4f18\u5316\u65e0\u4eba\u673a\u4f4d\u7f6e\u3002\u968f\u540e\uff0c\u63d0\u51fa\u8054\u5408\u4e2d\u7ee7\u6ce2\u675f\u6210\u5f62\u548c\u63a5\u6536\u7ec4\u5408\uff08JRBC\uff09\u7b97\u6cd5\u4f18\u5316\u6536\u53d1\u5668\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u65b9\u6848\u63d0\u9ad8\u4e864.6 dB\u7684SINR\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u65e0\u4eba\u673a\u653e\u7f6e\u548c\u6536\u53d1\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u975e\u6b63\u4ea4\u591a\u7528\u6237\u7f51\u7edc\u4e2d\u7684\u6700\u5c0fSINR\u3002"}}
{"id": "2507.12015", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12015", "abs": "https://arxiv.org/abs/2507.12015", "authors": ["Haoxun Li", "Leyuan Qu", "Jiaxi Hu", "Taihao Li"], "title": "EME-TTS: Unlocking the Emphasis and Emotion Link in Speech Synthesis", "comment": "Accepted by INTERSPEECH 2025", "summary": "In recent years, emotional Text-to-Speech (TTS) synthesis and\nemphasis-controllable speech synthesis have advanced significantly. However,\ntheir interaction remains underexplored. We propose Emphasis Meets Emotion TTS\n(EME-TTS), a novel framework designed to address two key research questions:\n(1) how to effectively utilize emphasis to enhance the expressiveness of\nemotional speech, and (2) how to maintain the perceptual clarity and stability\nof target emphasis across different emotions. EME-TTS employs weakly supervised\nlearning with emphasis pseudo-labels and variance-based emphasis features.\nAdditionally, the proposed Emphasis Perception Enhancement (EPE) block enhances\nthe interaction between emotional signals and emphasis positions. Experimental\nresults show that EME-TTS, when combined with large language models for\nemphasis position prediction, enables more natural emotional speech synthesis\nwhile preserving stable and distinguishable target emphasis across emotions.\nSynthesized samples are available on-line.", "AI": {"tldr": "EME-TTS\u662f\u4e00\u4e2a\u7ed3\u5408\u60c5\u611f\u548c\u5f3a\u8c03\u63a7\u5236\u7684TTS\u6846\u67b6\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u548cEPE\u5757\u589e\u5f3a\u60c5\u611f\u8868\u8fbe\u548c\u5f3a\u8c03\u7a33\u5b9a\u6027\u3002", "motivation": "\u63a2\u7d22\u60c5\u611fTTS\u4e0e\u5f3a\u8c03\u63a7\u5236\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u60c5\u611f\u8bed\u97f3\u7684\u8868\u8fbe\u529b\u548c\u5f3a\u8c03\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u5f31\u76d1\u7763\u5b66\u4e60\uff08\u5f3a\u8c03\u4f2a\u6807\u7b7e\u548c\u65b9\u5dee\u7279\u5f81\uff09\u548cEPE\u5757\u589e\u5f3a\u60c5\u611f\u4e0e\u5f3a\u8c03\u7684\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEME-TTS\u80fd\u751f\u6210\u66f4\u81ea\u7136\u7684\u60c5\u611f\u8bed\u97f3\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u8c03\u7684\u7a33\u5b9a\u6027\u548c\u533a\u5206\u6027\u3002", "conclusion": "EME-TTS\u4e3a\u60c5\u611f\u548c\u5f3a\u8c03\u63a7\u5236\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11913", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11913", "abs": "https://arxiv.org/abs/2507.11913", "authors": ["Chen Zhu", "Siyun Liang", "Zhouxiang Zhao", "Jianrong Bao", "Zhaohui Yang", "Zhaoyang Zhang", "Dusit Niyato"], "title": "Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission", "comment": null, "summary": "Semantic communication emphasizes the transmission of meaning rather than raw\nsymbols. It offers a promising solution to alleviate network congestion and\nimprove transmission efficiency. In this paper, we propose a wireless image\ncommunication framework that employs probability graphs as shared semantic\nknowledge base among distributed users. High-level image semantics are\nrepresented via scene graphs, and a two-stage compression algorithm is devised\nto remove predictable components based on learned conditional and co-occurrence\nprobabilities. At the transmitter, the algorithm filters redundant relations\nand entity pairs, while at the receiver, semantic recovery leverages the same\nprobability graphs to reconstruct omitted information. For further research, we\nalso put forward a multi-round semantic compression algorithm with its\ntheoretical performance analysis. Simulation results demonstrate that our\nsemantic-aware scheme achieves superior transmission throughput and satiable\nsemantic alignment, validating the efficacy of leveraging high-level semantics\nfor image communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u56fe\u7684\u65e0\u7ebf\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u538b\u7f29\u7b97\u6cd5\u53bb\u9664\u5197\u4f59\u4fe1\u606f\uff0c\u63d0\u5347\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7f51\u7edc\u62e5\u585e\u548c\u4f20\u8f93\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u901a\u4fe1\u4f20\u8f93\u610f\u4e49\u800c\u975e\u539f\u59cb\u7b26\u53f7\u3002", "method": "\u4f7f\u7528\u573a\u666f\u56fe\u8868\u793a\u9ad8\u7ea7\u8bed\u4e49\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u538b\u7f29\u7b97\u6cd5\u53bb\u9664\u53ef\u9884\u6d4b\u6210\u5206\uff0c\u53d1\u9001\u7aef\u8fc7\u6ee4\u5197\u4f59\u5173\u7cfb\uff0c\u63a5\u6536\u7aef\u5229\u7528\u6982\u7387\u56fe\u6062\u590d\u8bed\u4e49\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5728\u4f20\u8f93\u541e\u5410\u91cf\u548c\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5229\u7528\u9ad8\u7ea7\u8bed\u4e49\u8fdb\u884c\u56fe\u50cf\u901a\u4fe1\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8f6e\u8bed\u4e49\u538b\u7f29\u7b97\u6cd5\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.12042", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12042", "abs": "https://arxiv.org/abs/2507.12042", "authors": ["Kazuki Shimada", "Archontis Politis", "Iran R. Roman", "Parthasaarathy Sudarsanam", "David Diaz-Guerra", "Ruchi Pandey", "Kengo Uchida", "Yuichiro Koyama", "Naoya Takahashi", "Takashi Shibuya", "Shusuke Takahashi", "Tuomas Virtanen", "Yuki Mitsufuji"], "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification", "comment": "5 pages, 2 figures", "summary": "This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.", "AI": {"tldr": "DCASE2025\u6311\u6218\u8d5b\u4efb\u52a13\u805a\u7126\u4e8e\u7acb\u4f53\u58f0\u97f3\u9891\u6570\u636e\u7684\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\uff08SELD\uff09\uff0c\u5f15\u5165\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5e76\u8c03\u6574\u4e86\u8bc4\u4f30\u6307\u6807\u4ee5\u9002\u5e94\u6709\u9650\u89c6\u91ce\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u7acb\u4f53\u58f0\u97f3\u9891\u6570\u636e\u5728\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u66f4\u5e38\u89c1\u7684\u6709\u9650\u89c6\u91ce\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u58f0\u97f3\u9891\u548c\u89c6\u9891\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u57fa\u7ebf\u7cfb\u7edf\u6574\u5408\u4e86\u4e8b\u4ef6\u5206\u7c7b\u3001\u5b9a\u4f4d\u53ca\u5c4f\u5e55\u5185\u5916\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u57fa\u7ebf\u7cfb\u7edf\u5728\u7acb\u4f53\u58f0\u97f3\u9891\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7acb\u4f53\u58f0\u97f3\u9891\u6570\u636e\u5728\u6709\u9650\u89c6\u91ce\u573a\u666f\u4e0b\u7684SELD\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11919", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11919", "abs": "https://arxiv.org/abs/2507.11919", "authors": ["Wei Zhou", "Wei-Jian Li", "Wei-Xin Ren"], "title": "STFT-based Time-Frequency Mode Decomposition: A Fast and Robust Method for Multicomponent Signal Analysis", "comment": null, "summary": "The decomposition of complex, multicomponent, and non-stationary signals into\ntheir constituent modes is a fundamental yet significant challenge in science\nand engineering. Existing methods often struggle with a trade-off among\naccuracy, computational cost, and the need for prior information such as the\nnumber of modes. This paper introduces time-frequency mode decomposition\n(TFMD), a novel framework for the fast, robust, and adaptive decomposition of\nsuch signals. TFMD operates on the principle that modes form contiguous\nhigh-energy regions in the time-frequency domain. Its non-iterative pipeline\nreframes signal decomposition as an image segmentation task: a signal is\ntransformed into a spectrogram, which is then smoothed to enhance the\ncontinuity of these high-energy regions. A sequence of adaptive thresholding\nand connected-component labeling with size-based filtering is then employed to\nautomatically segment the spectrogram and generate a mask for each mode. The\nmodes are finally reconstructed via the inverse short-time Fourier transform.\nValidation on diverse synthetic signals demonstrates that TFMD accurately\ndetermines the number of modes and reconstructs them with high fidelity. Its\nperformance is particularly strong in high-noise conditions. A comparative\nanalysis confirms that TFMD provides robust, competitive performance across a\nwider variety of signal types, while a theoretical complexity analysis reveals\nits superior computational efficiency stemming from its non-iterative design.\nThe method's practical utility is further demonstrated by successfully\nextracting modal responses from a real-world footbridge vibration signal. TFMD\nprovides a computationally efficient and powerful paradigm for multicomponent\nsignal analysis, offering a compelling balance of accuracy, versatility, and\nefficiency for large-scale or time-sensitive applications.", "AI": {"tldr": "TFMD\u662f\u4e00\u79cd\u65b0\u578b\u7684\u975e\u8fed\u4ee3\u4fe1\u53f7\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3a\u65f6\u9891\u57df\u56fe\u50cf\u5e76\u5206\u5272\u9ad8\u80fd\u91cf\u533a\u57df\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u591a\u7ec4\u5206\u4fe1\u53f7\u5206\u89e3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5148\u9a8c\u4fe1\u606f\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u5c06\u4fe1\u53f7\u5206\u89e3\u4efb\u52a1\u8f6c\u5316\u4e3a\u56fe\u50cf\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u6ed1\u3001\u81ea\u9002\u5e94\u9608\u503c\u548c\u8fde\u901a\u533a\u57df\u6807\u8bb0\u7b49\u6280\u672f\u63d0\u53d6\u6a21\u5f0f\u3002", "result": "\u5728\u5408\u6210\u4fe1\u53f7\u548c\u5b9e\u9645\u632f\u52a8\u4fe1\u53f7\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TFMD\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u529f\u80fd\u5f3a\u5927\u7684\u591a\u7ec4\u5206\u4fe1\u53f7\u5206\u6790\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6216\u65f6\u95f4\u654f\u611f\u7684\u5e94\u7528\u3002"}}
{"id": "2507.12090", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12090", "abs": "https://arxiv.org/abs/2507.12090", "authors": ["Panos Kakoulidis", "Iakovi Alexiou", "Junkwang Oh", "Gunu Jho", "Inchul Hwang", "Pirros Tsiakoulis", "Aimilios Chalamandaris"], "title": "MambaRate: Speech Quality Assessment Across Different Sampling Rates", "comment": "Submitted to ASRU 2025 (AudioMOS Challenge 2025 Track 3)", "summary": "We propose MambaRate, which predicts Mean Opinion Scores (MOS) with limited\nbias regarding the sampling rate of the waveform under evaluation. It is\ndesigned for Track 3 of the AudioMOS Challenge 2025, which focuses on\npredicting MOS for speech in high sampling frequencies. Our model leverages\nself-supervised embeddings and selective state space modeling. The target\nratings are encoded in a continuous representation via Gaussian radial basis\nfunctions (RBF). The results of the challenge were based on the system-level\nSpearman's Rank Correllation Coefficient (SRCC) metric. An initial MambaRate\nversion (T16 system) outperformed the pre-trained baseline (B03) by ~14% in a\nfew-shot setting without pre-training. T16 ranked fourth out of five in the\nchallenge, differing by ~6% from the winning system. We present additional\nresults on the BVCC dataset as well as ablations with different representations\nas input, which outperform the initial T16 version.", "AI": {"tldr": "MambaRate\u662f\u4e00\u79cd\u9884\u6d4b\u97f3\u9891MOS\u7684\u6a21\u578b\uff0c\u9488\u5bf9\u9ad8\u91c7\u6837\u7387\u8bed\u97f3\u8bbe\u8ba1\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5d4c\u5165\u548c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff0c\u5728AudioMOS Challenge 2025\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u4f46\u672a\u593a\u51a0\u3002", "motivation": "\u89e3\u51b3\u9ad8\u91c7\u6837\u7387\u8bed\u97f3MOS\u9884\u6d4b\u4e2d\u7684\u91c7\u6837\u7387\u504f\u5dee\u95ee\u9898\uff0c\u53c2\u4e0eAudioMOS Challenge 2025 Track 3\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u5d4c\u5165\u548c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\uff0c\u4f7f\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\u7f16\u7801\u76ee\u6807\u8bc4\u5206\u3002", "result": "\u521d\u59cb\u7248\u672cT16\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u57fa\u7ebf14%\uff0c\u6392\u540d\u7b2c\u56db\uff1b\u5728BVCC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MambaRate\u5728MOS\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2507.12000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12000", "abs": "https://arxiv.org/abs/2507.12000", "authors": ["Jiahong Ning", "Ce Zheng", "Tingting Yang"], "title": "DSSD: Efficient Edge-Device Deployment and Collaborative Inference via Distributed Split Speculative Decoding", "comment": "ICML 2025", "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5206\u5272\u63a8\u6d4b\u89e3\u7801\uff08DSSD\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u5907\u4e0e\u8fb9\u7f18\u534f\u4f5c\u51cf\u5c11\u901a\u4fe1\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bbe\u5907-\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u8d44\u6e90\u9650\u5236\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u5206\u5272\u63a8\u6d4b\u89e3\u7801\uff08DSSD\uff09\uff0c\u5c06\u9a8c\u8bc1\u9636\u6bb5\u5206\u5272\u5728\u8bbe\u5907\u548c\u8fb9\u7f18\u4e4b\u95f4\uff0c\u51cf\u5c11\u4e0a\u884c\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDSSD\u5728\u51cf\u5c11\u901a\u4fe1\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DSSD\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bbe\u5907-\u8fb9\u7f18\u534f\u4f5c\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2507.12136", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12136", "abs": "https://arxiv.org/abs/2507.12136", "authors": ["Silvia Arellano", "Chunghsin Yeh", "Gautam Bhattacharya", "Daniel Arteaga"], "title": "Room Impulse Response Generation Conditioned on Acoustic Parameters", "comment": "4+1 pages, 2 figures; accepted in IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA 2025)", "summary": "The generation of room impulse responses (RIRs) using deep neural networks\nhas attracted growing research interest due to its applications in virtual and\naugmented reality, audio postproduction, and related fields. Most existing\napproaches condition generative models on physical descriptions of a room, such\nas its size, shape, and surface materials. However, this reliance on geometric\ninformation limits their usability in scenarios where the room layout is\nunknown or when perceptual realism (how a space sounds to a listener) is more\nimportant than strict physical accuracy. In this study, we propose an\nalternative strategy: conditioning RIR generation directly on a set of RIR\nacoustic parameters. These parameters include various measures of reverberation\ntime and direct sound to reverberation ratio, both broadband and bandwise. By\nspecifying how the space should sound instead of how it should look, our method\nenables more flexible and perceptually driven RIR generation. We explore both\nautoregressive and non-autoregressive generative models operating in the\nDescript Audio Codec domain, using either discrete token sequences or\ncontinuous embeddings. Specifically, we have selected four models to evaluate:\nan autoregressive transformer, the MaskGIT model, a flow matching model, and a\nclassifier-based approach. Objective and subjective evaluations are performed\nto compare these methods with state-of-the-art alternatives. Results show that\nthe proposed models match or outperform state-of-the-art alternatives, with the\nMaskGIT model achieving the best performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u58f0\u5b66\u53c2\u6570\u800c\u975e\u51e0\u4f55\u4fe1\u606f\u7684\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff08RIR\uff09\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u611f\u77e5\u9a71\u52a8\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684RIR\u751f\u6210\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u751f\u6210\u6a21\u578b\uff0c\u5176\u4e2dMaskGIT\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u623f\u95f4\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u672a\u77e5\u5e03\u5c40\u6216\u66f4\u6ce8\u91cd\u611f\u77e5\u771f\u5b9e\u6027\u7684\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u76f4\u63a5\u57fa\u4e8eRIR\u58f0\u5b66\u53c2\u6570\uff08\u5982\u6df7\u54cd\u65f6\u95f4\u548c\u76f4\u8fbe\u58f0\u4e0e\u6df7\u54cd\u6bd4\uff09\u751f\u6210RIR\uff0c\u63a2\u7d22\u4e86\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u5728Descript\u97f3\u9891\u7f16\u89e3\u7801\u57df\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cMaskGIT\u6a21\u578b\u6027\u80fd\u6700\u4f73\u3002", "conclusion": "\u57fa\u4e8e\u58f0\u5b66\u53c2\u6570\u7684\u65b9\u6cd5\u5728RIR\u751f\u6210\u4e2d\u66f4\u5177\u7075\u6d3b\u6027\u548c\u611f\u77e5\u9a71\u52a8\u6027\uff0cMaskGIT\u6a21\u578b\u4e3a\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2507.12010", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12010", "abs": "https://arxiv.org/abs/2507.12010", "authors": ["Julia Beuster", "Carsten Andrich", "Sebastian Giehl", "Marc Miranda", "Lorenz Mohr", "Dieter Novotny", "Tom Kaufmann", "Christian Schneider", "Reiner Thom\u00e4"], "title": "Enhancing Situational Awareness in ISAC Networks via Drone Swarms: A Real-World Channel Sounding Data Set", "comment": null, "summary": "With the upcoming capabilities of integrated sensing and communication (ISAC)\nand the incorporation of user equipment (UE) like unmanned aerial vehicles\n(UAVs) in 6G mobile networks, there is a significant opportunity to enhance\nsituational awareness through multi-static radar sensing in meshed ISAC\nnetworks. This paper presents a real-world channel sounding data set acquired\nusing a testbed with synchronized, distributed ground-based sensor nodes and\nflying sensor nodes within a swarm of up to four drones. The conducted\nmeasurement campaign is designed to sense the bi-static reflectivity of objects\nsuch as parking cars, vertical take-off and landing (VTOL) aircraft, and small\ndrones in multi-path environments. We detail the rationale behind the selection\nof the included scenarios and the configuration of the participating nodesand\npresent exemplary results to demonstrate the potential of using collaborating\ndrone swarms for multi-static radar tracking and localization in air-to-air\n(A2A) and air-to-ground (A2G) scenarios. The data sets are publicly available\nto support the development and validation of future ISAC algorithms in\nreal-world environments rather than relying solely on simulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e6G\u7f51\u7edc\u4e2d\u65e0\u4eba\u673a\u7fa4\u7684\u591a\u9759\u6001\u96f7\u8fbe\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u4fe1\u9053\u6d4b\u91cf\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002", "motivation": "\u5229\u75286G\u7f51\u7edc\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u548c\u65e0\u4eba\u673a\u8bbe\u5907\uff08UAVs\uff09\u63d0\u5347\u591a\u9759\u6001\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\uff0c\u589e\u5f3a\u73af\u5883\u611f\u77e5\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u5730\u9762\u548c\u98de\u884c\u4f20\u611f\u5668\u8282\u70b9\u7ec4\u6210\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91c7\u96c6\u591a\u8def\u5f84\u73af\u5883\u4e0b\u7684\u53cc\u9759\u6001\u53cd\u5c04\u7387\u6570\u636e\u3002", "result": "\u5c55\u793a\u4e86\u65e0\u4eba\u673a\u7fa4\u534f\u4f5c\u5728\u591a\u9759\u6001\u96f7\u8fbe\u8ddf\u8e2a\u548c\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765ISAC\u7b97\u6cd5\u5f00\u53d1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765ISAC\u7b97\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9a8c\u8bc1\u57fa\u7840\uff0c\u51cf\u5c11\u5bf9\u6a21\u62df\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.12175", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12175", "abs": "https://arxiv.org/abs/2507.12175", "authors": ["Sungkyun Chang", "Simon Dixon", "Emmanouil Benetos"], "title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection", "comment": "Accepted to WASPAA 2025", "summary": "This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.", "AI": {"tldr": "RUMAA\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u97f3\u4e50\u8868\u6f14\u5206\u6790\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u4e50\u8c31\u5230\u8868\u6f14\u7684\u5bf9\u9f50\u3001\u4e50\u8c31\u611f\u77e5\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5355\u72ec\u5904\u7406\u4e50\u8c31\u5bf9\u9f50\u3001\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u4e50\u8c31\u548c\u97f3\u9891\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u4e09\u6d41\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u4ee3\u7406\u4efb\u52a1\u6355\u6349\u4efb\u52a1\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5f00\u94a2\u7434\u97f3\u4e50\u6570\u636e\u96c6\u4e0a\uff0cRUMAA\u5728\u975e\u91cd\u590d\u4e50\u8c31\u4e0a\u8fbe\u5230\u6700\u4f18\u5bf9\u9f50\u6027\u80fd\uff0c\u5728\u91cd\u590d\u4e50\u8c31\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u4f9b\u826f\u597d\u7684\u8f6c\u5f55\u548c\u9519\u8bef\u68c0\u6d4b\u7ed3\u679c\u3002", "conclusion": "RUMAA\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u97f3\u4e50\u8868\u6f14\u5206\u6790\uff0c\u5c24\u5176\u5728\u5904\u7406\u91cd\u590d\u4e50\u8c31\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.12132", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12132", "abs": "https://arxiv.org/abs/2507.12132", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "comment": null, "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWi-Fi CSI\u7684\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\u63d0\u5347\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Wi-Fi CSI\u5728\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u4ecd\u4e0d\u8db3\u4ee5\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u53d7\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u542f\u53d1\uff0c\u4ece\u4e00\u7ef4\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\uff0c\u5e76\u6784\u5efa\u7edf\u4e00\u7684DoRF\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Wi-Fi HAR\u7684\u6cdb\u5316\u51c6\u786e\u6027\u3002", "conclusion": "DoRF\u5728\u5b9e\u7528\u4f20\u611f\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.12197", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12197", "abs": "https://arxiv.org/abs/2507.12197", "authors": ["Yichen Han", "Xiaoyang Hao", "Keming Chen", "Weibo Xiong", "Jun He", "Ruonan Zhang", "Junjie Cao", "Yue Liu", "Bowen Li", "Dongrui Zhang", "Hui Xia", "Huilei Fu", "Kai Jia", "Kaixuan Guo", "Mingli Jin", "Qingyun Meng", "Ruidong Ma", "Ruiqian Fang", "Shaotong Guo", "Xuhui Li", "Yang Xiang", "Ying Zhang", "Yulong Liu", "Yunfeng Li", "Yuyi Zhang", "Yuze Zhou", "Zhen Wang", "Zhaowen Chen"], "title": "Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations", "comment": null, "summary": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation.", "AI": {"tldr": "QTTS\u662f\u4e00\u79cd\u57fa\u4e8e\u65b0\u578b\u97f3\u9891\u7f16\u89e3\u7801\u5668QDAC\u7684TTS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7801\u672c\u5efa\u6a21\u548c\u5e76\u884c\u9884\u6d4b\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5408\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52TTS\u65b9\u6cd5\u56e0\u5355\u7801\u672c\u8868\u793a\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u96be\u4ee5\u6062\u590d\u7ec6\u8282\uff08\u5982\u97f5\u5f8b\u3001\u97f3\u8272\uff09\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u6b4c\u5531\u6216\u97f3\u4e50\u5408\u6210\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "QTTS\u91c7\u7528QDAC\u7f16\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u5206\u5c42\u5e76\u884c\u67b6\u6784\u548c\u5ef6\u8fdf\u591a\u5934\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5408\u6210\u548c\u5feb\u901f\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQTTS\u5728\u5408\u6210\u8d28\u91cf\u548c\u8868\u8fbe\u5185\u5bb9\u4fdd\u7559\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u7801\u672c\u5efa\u6a21\u662f\u63d0\u5347\u8bed\u97f3\u548c\u97f3\u9891\u751f\u6210\u4fdd\u771f\u5ea6\u7684\u6709\u6548\u65b9\u5411\u3002"}}
{"id": "2507.12146", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12146", "abs": "https://arxiv.org/abs/2507.12146", "authors": ["Carl Collmann", "Bitan Banerjee", "Ahmad Nimr", "Gerhard Fettweis"], "title": "A Practical Analysis: Understanding Phase Noise Modelling in Time and Frequency Domain for Phase-Locked Loops", "comment": "14 Pages", "summary": "In MIMO systems, the presence of phase noise is a significant factor that can\ndegrade performance. For MIMO testbeds build from SDR devices, phase noise\ncannot be ignored, particular in applications that require phase\nsynchronization. This is especially relevant in MIMO systems that employ\ndigital beamforming, where precise phase alignment is crucial. Accordingly,\naccurate phase noise modelling of SDR devices is essential. However, the\ninformation provided in data sheets for different SDR models varies widely and\nis often insufficient for comprehensive characterization of their phase noise\nperformance. While numerical simulations of PLL phase noise behavior are\ndocumented in the literature, there is a lack of extensive measurements\nsupported by appropriate system modelling. In this work, we present a practical\nphase noise modeling methodology applied to an SDR from the USRP X310 series.\nBased on measurement data, we derive estimates of key PLL performance\nindicators such as cycle-to-cycle jitter, oscillator constants, and PLL\nbandwidth. Furthermore, we propose a parametric model for the phase noise PSD\nof the PLL circuit and provide corresponding parameter estimates. This model\ncan be used for further investigation into the impact of phase noise on MIMO\nsystem performance implemented by similar SDR devices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9USRP X310\u7cfb\u5217SDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7f3a\u4e4f\u5b9e\u6d4b\u6570\u636e\u652f\u6301\u7684\u7a7a\u767d\u3002", "motivation": "MIMO\u7cfb\u7edf\u4e2d\u76f8\u4f4d\u566a\u58f0\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u5c24\u5176\u662f\u9700\u8981\u76f8\u4f4d\u540c\u6b65\u7684\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u5e94\u7528\u3002\u7136\u800c\uff0cSDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u6027\u80fd\u5728\u6570\u636e\u624b\u518c\u4e2d\u4fe1\u606f\u4e0d\u8db3\uff0c\u73b0\u6709\u6587\u732e\u4e5f\u7f3a\u4e4f\u5b9e\u6d4b\u652f\u6301\u7684\u7cfb\u7edf\u5efa\u6a21\u3002", "method": "\u57fa\u4e8e\u6d4b\u91cf\u6570\u636e\uff0c\u8bba\u6587\u63d0\u51fa\u4e86USRP X310\u7cfb\u5217SDR\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f30\u8ba1\u4e86PLL\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\uff08\u5982\u5468\u671f\u6296\u52a8\u3001\u632f\u8361\u5668\u5e38\u6570\u548cPLL\u5e26\u5bbd\uff09\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u4f4d\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\u7684\u53c2\u6570\u5316\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u6d4b\u91cf\u6570\u636e\uff0c\u8bba\u6587\u5f97\u51fa\u4e86PLL\u6027\u80fd\u6307\u6807\u7684\u4f30\u8ba1\u503c\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u4f4d\u566a\u58f0PSD\u7684\u53c2\u6570\u5316\u6a21\u578b\u53ca\u5176\u53c2\u6570\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u7528\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u76f8\u4f4d\u566a\u58f0\u5bf9\u7c7b\u4f3cSDR\u8bbe\u5907\u5b9e\u73b0\u7684MIMO\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.12210", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12210", "abs": "https://arxiv.org/abs/2507.12210", "authors": ["Jialiang Zhu", "Sanoopkumar P. S.", "Arman Farhang"], "title": "PAPR of DFT-s-OTFS with Pulse Shaping", "comment": null, "summary": "Orthogonal Time Frequency Space (OTFS) suffers from high peak-to-average\npower ratio (PAPR) when the number of Doppler bins is large. To address this\nissue, a discrete Fourier transform spread OTFS (DFT-s-OTFS) scheme is employed\nby applying DFT spreading across the Doppler dimension. This paper presents a\nthorough PAPR analysis of DFT-s-OTFS in the uplink scenario using different\npulse shaping filters and resource allocation strategies. Specifically, we\nderive a PAPR upper bound of DFT-s-OTFS with interleaved and block Doppler\nresource allocation schemes. Our analysis reveals that DFT-s-OTFS with\ninterleaved allocation yields a lower PAPR than that of block allocation.\nFurthermore, we show that interleaved allocation produces a periodic\ntime-domain signal composed of repeated quadrature amplitude modulated (QAM)\nsymbols which simplifies the transmitter design. Based on our analytical\nresults, the root raised cosine (RRC) pulse generally results in a higher\nmaximum PAPR compared to the rectangular pulse. Simulation results confirm the\nvalidity of the derived PAPR upper bounds. Furthermore, we also demonstrate\nthrough BER simulation analysis that the DFT-s-OTFS gives the same performance\nas OTFS without DFT spreading.", "AI": {"tldr": "DFT-s-OTFS\u65b9\u6848\u901a\u8fc7\u5728\u591a\u666e\u52d2\u7ef4\u5ea6\u5e94\u7528DFT\u6269\u5c55\uff0c\u964d\u4f4e\u4e86OTFS\u7684\u9ad8PAPR\u95ee\u9898\u3002\u5206\u6790\u8868\u660e\uff0c\u4ea4\u9519\u5206\u914d\u7b56\u7565\u6bd4\u5757\u5206\u914d\u7b56\u7565PAPR\u66f4\u4f4e\uff0c\u4e14\u7b80\u5316\u4e86\u53d1\u5c04\u673a\u8bbe\u8ba1\u3002RRC\u8109\u51b2\u7684PAPR\u9ad8\u4e8e\u77e9\u5f62\u8109\u51b2\u3002BER\u6027\u80fd\u4e0e\u672a\u6269\u5c55\u7684OTFS\u76f8\u540c\u3002", "motivation": "\u89e3\u51b3OTFS\u5728\u591a\u666e\u52d2bin\u6570\u91cf\u5927\u65f6\u7684\u9ad8PAPR\u95ee\u9898\u3002", "method": "\u91c7\u7528DFT\u6269\u5c55OTFS\uff08DFT-s-OTFS\uff09\uff0c\u5206\u6790\u4e0d\u540c\u8109\u51b2\u6574\u5f62\u6ee4\u6ce2\u5668\u548c\u8d44\u6e90\u5206\u914d\u7b56\u7565\u4e0b\u7684PAPR\u3002", "result": "\u4ea4\u9519\u5206\u914d\u7b56\u7565PAPR\u66f4\u4f4e\uff0cRRC\u8109\u51b2PAPR\u8f83\u9ad8\uff0cBER\u6027\u80fd\u4e0eOTFS\u76f8\u540c\u3002", "conclusion": "DFT-s-OTFS\u53ef\u6709\u6548\u964d\u4f4ePAPR\uff0c\u4e14\u4e0d\u5f71\u54cdBER\u6027\u80fd\uff0c\u9002\u5408\u4e0a\u884c\u94fe\u8def\u573a\u666f\u3002"}}
{"id": "2507.12211", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12211", "abs": "https://arxiv.org/abs/2507.12211", "authors": ["Sa\u00fal Fenollosa"], "title": "Cell Sensing: Traffic detection", "comment": "39 pages, 16 figures. Student project report at the\n  Telecommunications Circuits Laboratory (TCL), EPFL. Supervised by Prof.\n  Andreas Burg and Sitian Li", "summary": "This work presents a passive sensing system for traffic monitoring using\nambient Long Term Evolution (LTE) signals as a non-intrusive and scalable\nalternative to traditional surveillance methods. The approach employs a\ndual-receiver architecture analyzing Channel State Information (CSI) to isolate\ndifferential Doppler shifts induced by moving targets, effectively mitigating\nhardware-induced phase impairments. Implemented with a Software Defined Radio\n(SDR) platform and srsRAN software, the system demonstrated over 90% detection\naccuracy for speeds above 6000 mm/min in controlled indoor tests, and provided\nreliable speed estimations for pedestrians and vehicles in outdoor evaluations.\nDespite challenges at low speeds, directional ambiguity, and multipath fading\nin urban settings, the results validate LTE-based passive sensing as a feasible\ntraffic monitoring method, identifying critical areas for future research such\nas angle-of-arrival (AoA) integration, machine learning, and real-time embedded\nsystem development.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LTE\u4fe1\u53f7\u8fdb\u884c\u4ea4\u901a\u76d1\u6d4b\u7684\u88ab\u52a8\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u63a5\u6536\u5668\u67b6\u6784\u5206\u6790CSI\uff0c\u6709\u6548\u9694\u79bb\u79fb\u52a8\u76ee\u6807\u5f15\u8d77\u7684\u591a\u666e\u52d2\u9891\u79fb\uff0c\u5e76\u5728\u5ba4\u5185\u5916\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u76d1\u6d4b\u65b9\u6cd5\u5b58\u5728\u4fb5\u5165\u6027\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5229\u7528LTE\u4fe1\u53f7\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u63a5\u6536\u5668\u67b6\u6784\u5206\u6790CSI\uff0c\u7ed3\u5408SDR\u5e73\u53f0\u548csrsRAN\u8f6f\u4ef6\uff0c\u9694\u79bb\u591a\u666e\u52d2\u9891\u79fb\u5e76\u51cf\u5c11\u786c\u4ef6\u5f15\u8d77\u7684\u76f8\u4f4d\u5931\u771f\u3002", "result": "\u5ba4\u5185\u6d4b\u8bd5\u4e2d\u901f\u5ea6\u9ad8\u4e8e6000 mm/min\u7684\u76ee\u6807\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u5ba4\u5916\u6d4b\u8bd5\u4e2d\u80fd\u53ef\u9760\u4f30\u8ba1\u884c\u4eba\u548c\u8f66\u8f86\u901f\u5ea6\u3002", "conclusion": "LTE\u88ab\u52a8\u611f\u77e5\u5728\u4ea4\u901a\u76d1\u6d4b\u4e2d\u53ef\u884c\uff0c\u4f46\u9700\u89e3\u51b3\u4f4e\u901f\u3001\u65b9\u5411\u6a21\u7cca\u548c\u591a\u5f84\u8870\u843d\u7b49\u95ee\u9898\uff0c\u672a\u6765\u53ef\u7ed3\u5408AoA\u3001\u673a\u5668\u5b66\u4e60\u548c\u5b9e\u65f6\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\u3002"}}
{"id": "2507.12221", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12221", "abs": "https://arxiv.org/abs/2507.12221", "authors": ["Alejandro Castilla", "Sa\u00fal Fenollosa", "Monika Drozdowska", "Alejandro Lopez-Escudero", "Sergio Mic\u00f2-Rosa", "Narcis Cardona"], "title": "Novel Approach to Dual-Channel Estimation in Integrated Sensing and Communications for 6G", "comment": "6 pages, 13 figures. Accepted for publication at the 2024 IEEE 35th\n  International Symposium on Personal, Indoor and Mobile Radio Communications\n  (PIMRC)", "summary": "Integrated Sensing and Communication (ISAC) design is crucial for 6G and\nharmonizes environmental data sensing with communication, emphasizing the need\nto understand and model these elements. This paper delves into dual-channel\nmodels for ISAC, employing channel extraction techniques to validate and\nenhance accuracy. Focusing on millimeter wave (mmWave) radars, it explores the\nextraction of the bistatic sensing channel from monostatic measurements and\nsubsequent communication channel estimation. The proposed methods involve\ninterference extraction, module and phase correlation analyses, chirp\nclustering, and auto-clutter reduction. A comprehensive set-up in an anechoic\nchamber with controlled scenarios evaluates the proposed techniques,\ndemonstrating successful channel extraction and validation through Root Mean\nSquare Delay Spread (RMS DS), Power Delay Profile (PDP), and Angle of Arrival\n(AoA) analysis. Comparison with Ray-Tracing (RT) simulations confirms the\neffectiveness of the proposed approach, presenting an innovative stride towards\nfully integrated sensing and communication in future networks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e866G\u4e2d\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7684\u53cc\u901a\u9053\u6a21\u578b\uff0c\u901a\u8fc7\u6beb\u7c73\u6ce2\u96f7\u8fbe\u63d0\u53d6\u53cc\u57fa\u5730\u4f20\u611f\u901a\u9053\uff0c\u5e76\u9a8c\u8bc1\u4e86\u901a\u4fe1\u901a\u9053\u4f30\u8ba1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u548c\u5efa\u6a21\u53cc\u901a\u9053\u6a21\u578b\uff0c\u4ee5\u63d0\u5347ISAC\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5e72\u6270\u63d0\u53d6\u3001\u6a21\u5757\u548c\u76f8\u4f4d\u76f8\u5173\u5206\u6790\u3001\u7ebf\u6027\u8c03\u9891\u805a\u7c7b\u548c\u81ea\u52a8\u6742\u6ce2\u51cf\u5c11\u7b49\u6280\u672f\uff0c\u4ece\u5355\u57fa\u5730\u6d4b\u91cf\u4e2d\u63d0\u53d6\u53cc\u57fa\u5730\u4f20\u611f\u901a\u9053\u3002", "result": "\u5728\u6d88\u58f0\u5ba4\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7RMS DS\u3001PDP\u548cAoA\u5206\u6790\u5c55\u793a\u4e86\u6210\u529f\u7684\u901a\u9053\u63d0\u53d6\uff0c\u5e76\u4e0e\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u7ed3\u679c\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u672a\u6765\u7f51\u7edc\u4e2d\u5b8c\u5168\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u8fdb\u5c55\u3002"}}
{"id": "2507.12235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12235", "abs": "https://arxiv.org/abs/2507.12235", "authors": ["Sa\u00fal Fenollosa", "Monika Drozdowska", "Wenfei Yang", "Sergio Mic\u00f3-Rosa", "Alejandro Castilla", "Alejandro Lopez-Escudero", "Jian Li", "Narcis Cardona"], "title": "Frequency-responsive RCS characteristics and scaling implications for ISAC development", "comment": "6 pages, 12 figures, 3 tables. Accepted for publication at the 2024\n  IEEE Global Communications Conference (GLOBECOM), WS-02: Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "summary": "This paper presents an investigation on the Radar Cross-Section (RCS) of\nvarious targets, with the objective of analysing how RCS properties vary with\nfrequency. Targets such as an Automated Guided Vehicle (AGV), a pedestrian, and\na full-scale car were measured in the frequency bands referred to in industry\nstandards as FR2 and FR3. Measurements were taken in diverse environments,\nindoors and outdoors, to ensure comprehensive scenario coverage. The\nmethodology employed in RCS extraction performs background subtraction,\nfollowed by time-domain gating to isolate the influence of the target. This\nanalysis compares the RCS values and how the points of greatest contribution\nare distributed across different bands based on the range response of the RCS.\nAnalysis of the results demonstrated how RCS values change with frequency and\ntarget shape, providing insights into the electromagnetic behaviour of these\ntargets. Key findings highlight how much scaling RCS values based on frequency\nand geometry is complex and varies among different types of materials and\nshapes. These insights are instrumental for advancing sensing systems and\nenhancing 3GPP channel models, particularly for Integrated Sensing and\nCommunications (ISAC) techniques proposed for 6G standards.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u76ee\u6807\u7684\u96f7\u8fbe\u6563\u5c04\u622a\u9762\uff08RCS\uff09\u968f\u9891\u7387\u53d8\u5316\u7684\u7279\u6027\uff0c\u5305\u62ecAGV\u3001\u884c\u4eba\u548c\u5168\u5c3a\u5bf8\u6c7d\u8f66\uff0c\u6d4b\u91cf\u73af\u5883\u591a\u6837\uff0c\u65b9\u6cd5\u5305\u62ec\u80cc\u666f\u6263\u9664\u548c\u65f6\u57df\u95e8\u63a7\u3002", "motivation": "\u5206\u6790RCS\u968f\u9891\u7387\u548c\u76ee\u6807\u5f62\u72b6\u7684\u53d8\u5316\uff0c\u4e3a6G\u6807\u51c6\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6280\u672f\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528\u80cc\u666f\u6263\u9664\u548c\u65f6\u57df\u95e8\u63a7\u65b9\u6cd5\u63d0\u53d6RCS\uff0c\u6d4b\u91cf\u5728FR2\u548cFR3\u9891\u6bb5\u8fdb\u884c\u3002", "result": "RCS\u503c\u968f\u9891\u7387\u548c\u76ee\u6807\u5f62\u72b6\u53d8\u5316\u663e\u8457\uff0c\u4e0d\u540c\u6750\u6599\u548c\u5f62\u72b6\u7684RCS\u7f29\u653e\u590d\u6742\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u63d0\u5347\u4f20\u611f\u7cfb\u7edf\u548c\u4f18\u53163GPP\u4fe1\u9053\u6a21\u578b\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5c24\u5176\u9002\u7528\u4e8e6G ISAC\u6280\u672f\u3002"}}
{"id": "2507.12301", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.12301", "abs": "https://arxiv.org/abs/2507.12301", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli", "Zhi Ding"], "title": "Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning", "comment": null, "summary": "Deep learning-based implicit channel state information (CSI) feedback has\nbeen introduced to enhance spectral efficiency in massive MIMO systems.\nExisting methods often show performance degradation in ultra-low-rate scenarios\nand inadaptability across diverse environments. In this paper, we propose\nDual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework\nincorporating two novel plug-in preprocessing modules to achieve ultra-low\nfeedback rates while maintaining high environmental robustness. First, a novel\nbi-directional correlation enhancement module is proposed to strengthen the\ncorrelation between uplink and downlink CSI eigenvector matrices. This module\nprojects highly correlated uplink and downlink channel matrices into their\nrespective eigenspaces, effectively reducing redundancy for ultra-low-rate\nfeedback. Second, an innovative input format alignment module is designed to\nmaintain consistent data distributions at both encoder and decoder sides\nwithout extra transmission overhead, thereby enhancing robustness against\nenvironmental variations. Finally, we develop an efficient transformer-based\nimplicit CSI feedback network to exploit angular-delay domain sparsity and\nbi-directional correlation for ultra-low-rate CSI compression. Simulation\nresults demonstrate successful reduction of the feedback overhead by 85%\ncompared with the state-of-the-art method and robustness against unseen\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDual-ImRUNet\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u9884\u5904\u7406\u6a21\u5757\u5b9e\u73b0\u8d85\u4f4e\u53cd\u9988\u7387\u548c\u9ad8\u73af\u5883\u9c81\u68d2\u6027\uff0c\u53cd\u9988\u5f00\u9500\u51cf\u5c1185%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u4f4e\u901f\u7387\u573a\u666f\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u76f8\u5173\u6027\u589e\u5f3a\u6a21\u5757\u548c\u8f93\u5165\u683c\u5f0f\u5bf9\u9f50\u6a21\u5757\uff0c\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u9690\u5f0fCSI\u53cd\u9988\u7f51\u7edc\u3002", "result": "\u53cd\u9988\u5f00\u9500\u51cf\u5c1185%\uff0c\u5e76\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "Dual-ImRUNet\u5728\u8d85\u4f4e\u901f\u7387\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.12317", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12317", "abs": "https://arxiv.org/abs/2507.12317", "authors": ["Martin Agebj\u00e4r", "Gustav Zetterqvist", "Fredrik Gustafsson", "Johan Wahlstr\u00f6m", "Gustaf Hendeby"], "title": "Road Roughness Estimation via Fusion of Standard Onboard Automotive Sensors", "comment": "Accepted for publication in FUSION 2025 - 28th International\n  Conference on Information Fusion (FUSION), IEEE (2025)", "summary": "Road roughness significantly affects vehicle vibrations and ride quality. We\nintroduce a Kalman filter (KF)-based method for estimating road roughness in\nterms of the international roughness index (IRI) by fusing inertial and speed\nmeasurements, offering a cost-effective solution for pavement monitoring. The\nmethod involves system identification on a physical vehicle to estimate\nrealistic model parameters, followed by KF-based reconstruction of the\nlongitudinal road profile to compute IRI values. It explores IRI estimation\nusing vertical and lateral vibrations, the latter more common in modern\nvehicles. Validation on 230 km of real-world data shows promising results, with\nIRI estimation errors ranging from 1% to 10% of the reference values. However,\naccuracy deteriorates significantly when using only lateral vibrations,\nhighlighting their limitations. These findings demonstrate the potential of\nKF-based estimation for efficient road roughness monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u60ef\u6027\u548c\u901f\u5ea6\u6d4b\u91cf\u6765\u4f30\u8ba1\u9053\u8def\u7c97\u7cd9\u5ea6\uff08IRI\uff09\uff0c\u4e3a\u8def\u9762\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u9053\u8def\u7c97\u7cd9\u5ea6\u5bf9\u8f66\u8f86\u632f\u52a8\u548c\u4e58\u5750\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\u6765\u76d1\u6d4b\u8def\u9762\u72b6\u51b5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc6\u522b\u7269\u7406\u8f66\u8f86\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\uff0c\u7136\u540e\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u91cd\u5efa\u7eb5\u5411\u9053\u8def\u8f6e\u5ed3\u4ee5\u8ba1\u7b97IRI\u503c\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u5782\u76f4\u548c\u6a2a\u5411\u632f\u52a8\u7684IRI\u4f30\u8ba1\u3002", "result": "\u5728230\u516c\u91cc\u7684\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0cIRI\u4f30\u8ba1\u8bef\u5dee\u4e3a\u53c2\u8003\u503c\u76841%\u523010%\uff0c\u4f46\u4ec5\u4f7f\u7528\u6a2a\u5411\u632f\u52a8\u65f6\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5361\u5c14\u66fc\u6ee4\u6ce2\u65b9\u6cd5\u5728\u9053\u8def\u7c97\u7cd9\u5ea6\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6a2a\u5411\u632f\u52a8\u7684\u4f7f\u7528\u5b58\u5728\u5c40\u9650\u6027\u3002"}}

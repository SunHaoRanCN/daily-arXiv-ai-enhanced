<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS](https://arxiv.org/abs/2507.12593)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种基于Zak-OTFS的差分通信方案，无需周期性发送导频，通过利用检测到的数据作为导频进行信道估计，提高能效和频谱效率。


<details>
  <summary>Details</summary>
Motivation: Zak-OTFS在延迟-多普勒（DD）域的信道可预测，但仍需周期性导频。本文旨在消除这一需求，提高系统效率。

Method: 利用DD域信道的预测能力，将检测到的数据符号作为导频，用于后续数据检测，形成差分通信。

Result: 方案在能效和频谱效率上优于传统导频方案，且复杂度更低，误码率表现更好。

Conclusion: 提出的差分通信方案有效减少了导频需求，提升了Zak-OTFS系统的性能。

Abstract: Zak-transform based orthogonal time frequency space (Zak-OTFS) is a
delay-Doppler (DD) domain modulation scheme in which the signal processing is
carried out in the DD domain. The channel when viewed in the DD domain is
predictable. However, even with Zak-OTFS, pilots need to be sent periodically,
albeit at a lower rate. In this paper, we propose a differential communication
scheme for Zak-OTFS systems that alleviates the need for periodic pilot
transmission. Towards this, we analytically show that the detected data can be
used as a pilot and that the channel estimate obtained from the detected data
can enable further detection enabling the "differential" aspect of the
communication. Specifically, we leverage the prediction capability of the DD
channel in Zak-OTFS to use the channel estimate (obtained from detected data
symbols treated as pilots) in the previous instant to detect data in the next
instant and propagate this forward. The advantages are two fold. First, it
allows the data symbols to enjoy higher energy since the energy that would
otherwise be required for pilot symbols can also be allocated to data symbols.
Second, it allows for full spectral efficiency compared to point or embedded
pilots. Comparison with the full spectral efficiency achieving spread pilot
scheme shows that the proposed method achieves better bit-error rate at lower
complexity.

</details>


### [2] [Achieving Robust Channel Estimation Neural Networks by Designed Training Data](https://arxiv.org/abs/2507.12630)
*Dianxin Luan,John Thompson*

Main category: eess.SP

TL;DR: 论文提出了一种离线训练的神经网络设计方法，通过生成合成训练数据集，确保网络在未见过的信道上也能达到一定的均方误差（MSE）性能，无需实际信道信息或在线参数更新。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在新数据上性能下降，且物理信道通常是时变的，而在线训练因低延迟和有限计算资源难以实现。因此，需要设计离线训练的神经网络，使其在未知信道上也能鲁棒工作。

Method: 提出了生成合成训练数据集的设计准则，确保训练后的网络在新信道上达到特定MSE性能。进一步提出基准设计，支持不同信道配置的智能操作。

Result: 仿真表明，神经网络在固定和可变延迟扩展的信道上均实现了鲁棒的泛化性能，且泛化能力与网络架构无关。

Conclusion: 提出的方法为无线信道中的神经网络应用提供了无需在线调整的鲁棒解决方案，适用于不同信道配置。

Abstract: Channel estimation is crucial in cognitive communications, as it enables
intelligent spectrum sensing and adaptive transmission by providing accurate
information about the current channel state. However, in many papers neural
networks are frequently tested by training and testing on one example channel
or similar channels. This is because data-driven methods often degrade on new
data which they are not trained on, as they cannot extrapolate their training
knowledge. This is despite the fact physical channels are often assumed to be
time-variant. However, due to the low latency requirements and limited
computing resources, neural networks may not have enough time and computing
resources to execute online training to fine-tune the parameters. This
motivates us to design offline-trained neural networks that can perform
robustly over wireless channels, but without any actual channel information
being known at design time. In this paper, we propose design criteria to
generate synthetic training datasets for neural networks, which guarantee that
after training the resulting networks achieve a certain mean squared error
(MSE) on new and previously unseen channels. Therefore, neural network
solutions require no prior channel information or parameters update for
real-world implementations. Based on the proposed design criteria, we further
propose a benchmark design which ensures intelligent operation for different
channel profiles. To demonstrate general applicability, we use neural networks
with different levels of complexity to show that the generalization achieved
appears to be independent of neural network architecture. From simulations,
neural networks achieve robust generalization to wireless channels with both
fixed channel profiles and variable delay spreads.

</details>


### [3] [A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis](https://arxiv.org/abs/2507.12645)
*Mohammed Guhdar,Ramadhan J. Mstafa,Abdulhakeem O. Mohammed*

Main category: eess.SP

TL;DR: 提出了一种统一的深度学习框架，用于处理多种生物信号（如ECG和EEG），通过ResNet-CNN结合注意力机制和新型数据增强策略，解决了信号差异和类别不平衡问题，并在多个基准数据集上实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号（如ECG和EEG）的多样性和类别不平衡问题限制了传统方法的性能，亟需一种统一的架构来高效处理这些信号。

Method: 结合ResNet-CNN和注意力机制，采用时间域信号拼接的数据增强策略，并使用Focal Loss函数处理类别不平衡。预处理包括小波去噪、基线去除和标准化。

Result: 在UCI Seizure EEG、MIT-BIH Arrhythmia和PTB Diagnostic ECG数据集上分别达到99.96%、99.78%和100%的准确率，且内存占用低，适合部署在低端设备上。

Conclusion: 该框架在多种生物信号处理中表现出色，解决了信号差异和类别不平衡问题，具有实际部署潜力。

Abstract: The increasing need for accurate and unified analysis of diverse biological
signals, such as ECG and EEG, is paramount for comprehensive patient
assessment, especially in synchronous monitoring. Despite advances in
multi-sensor fusion, a critical gap remains in developing unified architectures
that effectively process and extract features from fundamentally different
physiological signals. Another challenge is the inherent class imbalance in
many biomedical datasets, often causing biased performance in traditional
methods. This study addresses these issues by proposing a novel and unified
deep learning framework that achieves state-of-the-art performance across
different signal types. Our method integrates a ResNet-based CNN with an
attention mechanism, enhanced by a novel data augmentation strategy:
time-domain concatenation of multiple augmented variants of each signal to
generate richer representations. Unlike prior work, we scientifically increase
signal complexity to achieve future-reaching capabilities, which resulted in
the best predictions compared to the state of the art. Preprocessing steps
included wavelet denoising, baseline removal, and standardization. Class
imbalance was effectively managed through the combined use of this advanced
data augmentation and the Focal Loss function. Regularization techniques were
applied during training to ensure generalization. We rigorously evaluated the
proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH
Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,
and 100%, respectively, demonstrating robustness across diverse signal types
and clinical contexts. Finally, the architecture requires ~130 MB of memory and
processes each sample in ~10 ms, suggesting suitability for deployment on
low-end or wearable devices.

</details>


### [4] [Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers](https://arxiv.org/abs/2507.12706)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 论文提出了一种基于多机器学习分类器一致投票的保守卫星选择策略，以增强ZSM定位方法，显著提高了城市GNSS环境中的定位成功率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GNSS信号常因建筑物遮挡和多径效应导致定位误差，需改进现有方法以提高定位精度。

Method: 使用RF、GBDT和SVM三种分类器对GPS信号进行LOS/NLOS分类，仅当所有分类器一致且置信度超过阈值时才选择卫星用于定位。

Result: 实验表明，该方法显著提高了定位成功率和接收器包含率，尽管卫星数量减少导致定位范围略有增加。

Conclusion: 该方法在城市GNSS环境中有效提升了定位可靠性，证明了其实际应用价值。

Abstract: In urban environments, global navigation satellite system (GNSS) positioning
is often compromised by signal blockages and multipath effects caused by
buildings, leading to significant positioning errors. To address this issue,
this study proposes a robust enhancement of zonotope shadow matching
(ZSM)-based positioning by employing a conservative satellite selection
strategy using unanimous voting across multiple machine learning classifiers.
Three distinct models - random forest (RF), gradient boosting decision tree
(GBDT), and support vector machine (SVM) - were trained to perform
line-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global
positioning system (GPS) signal features. A satellite is selected for
positioning only when all classifiers unanimously agree on its classification
and their associated confidence scores exceed a threshold. Experiments with
real-world GPS data collected in dense urban areas demonstrate that the
proposed method significantly improves the positioning success rate and the
receiver containment rate, even with imperfect LOS/NLOS classification.
Although a slight increase in the position bound was observed due to the
reduced number of satellites used, overall positioning reliability was
substantially enhanced, indicating the effectiveness of the proposed approach
in urban GNSS environments.

</details>


### [5] [Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO](https://arxiv.org/abs/2507.12917)
*Xi Ding,Luca Kunz,E. Jorswieck*

Main category: eess.SP

TL;DR: 本文提出了一种基于半定松弛（SDR）的联合波束成形（BF）优化框架，用于小规模无小区MIMO（CF-MIMO）系统中的联合感知与通信（JSAC），确保全局最优解且无需后处理。


<details>
  <summary>Details</summary>
Motivation: 现有JSAC优化方法缺乏全局最优性或需要额外降秩步骤，本文旨在解决这些问题。

Method: 采用SDR方法设计BF策略，并引入独立的BF策略作为性能基准。

Result: 提出的框架提供了全局最优且计算高效的BF设计。

Conclusion: 该框架为下一代无线网络的发展提供了有价值的见解。

Abstract: This paper studies optimal joint beamforming (BF) for joint sensing and
communication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While
prior works have explored JSAC optimization using methods such as successive
convex approximation (SCA) and semidefinite relaxation (SDR), many of these
approaches either lack global optimality or require additional rank-reduction
steps. In contrast, we propose an SDR-based optimization framework that
guarantees globally optimal solutions without post-processing. To benchmark its
performance, we introduce a standalone BF strategy that dedicates each access
point (AP) exclusively to either communication or sensing. The proposed
formulation builds upon a general multi-user system model, enabling future
extensions beyond the single-user setting. Overall, our framework offers a
globally optimal and computationally efficient BF design, providing valuable
insights for the development of next-generation wireless networks.

</details>


### [6] [Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation](https://arxiv.org/abs/2507.13037)
*Guangyao Liu,Tianqi Mao,Yanqun Tang,Jingjing Zhao,Zhenyu Xiao*

Main category: eess.SP

TL;DR: 本文提出了一种多模式索引调制方案MM-AFDM-IM，用于提升AFDM的频谱和能量效率，通过动态选择星座模式和激活chirp传递额外信息，并分析了其误码率性能。


<details>
  <summary>Details</summary>
Motivation: 针对高移动性通信场景，AFDM是一种有前景的多载波技术，但其频谱和能量效率仍有提升空间。

Method: 提出MM-AFDM-IM方案，通过动态选择星座模式和激活chirp传递额外信息，无需额外能耗。

Result: 仿真结果表明，MM-AFDM-IM在误码率性能上优于传统方案。

Conclusion: MM-AFDM-IM是一种高效的多载波调制方案，适用于高移动性通信场景。

Abstract: Affine frequency division multiplexing (AFDM), a promising multicarrier
technique utilizing chirp signals, has been envisioned as an effective solution
for high-mobility communication scenarios. In this paper, we develop a
multiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,
which aims to further improve the spectral and energy efficiencies of AFDM.
Specifically, multiple constellation alphabets are selected for different
chirp-based subcarriers (chirps). Aside from classical amplitude/phase
modulation, additional information bits can be conveyed by the dynamic patterns
of both constellation mode selection and chirp activation, without extra energy
consumption. Furthermore, we discuss the mode selection strategy and derive an
asymptotically tight upper bound on the bit error rate (BER) of the proposed
scheme under maximum-likelihood detection. Simulation results are provided to
demonstrate the superior performance of MM-AFDM-IM compared to conventional
benchmark schemes.

</details>


### [7] [Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects](https://arxiv.org/abs/2507.13080)
*Morteza Alijani,Wout Joseph,David Plets*

Main category: eess.SP

TL;DR: 本文介绍了无调制可见光定位（uVLP）技术，作为传统VLP的低成本替代方案，利用现有照明设施实现高精度室内定位。


<details>
  <summary>Details</summary>
Motivation: 传统VLP因调制LED导致成本高和照明效率低，限制了其广泛应用，uVLP通过利用未调制光源解决了这些问题。

Method: uVLP利用未调制光源（如传统LED）的光信号，分为基于强度和基于成像的接收技术，并提出分类框架。

Result: uVLP提供了一种低成本、高效的室内定位方案，并提出了技术分类和未来研究方向。

Conclusion: uVLP具有潜力成为广泛部署的室内定位技术，但仍需解决现有挑战以实现其规模化应用。

Abstract: Visible Light Positioning (VLP) has emerged as a promising technology for
next-generation indoor positioning systems (IPS), particularly within the scope
of sixth-generation (6G) wireless networks. Its attractiveness stems from
leveraging existing lighting infrastructures equipped with light-emitting
diodes (LEDs), enabling cost-efficient deployments and achieving high-precision
positioning accuracy in the centimeter-todecimeter range. However, widespread
adoption of traditional VLP solutions faces significant barriers due to the
increased costs and operational complexity associated with modulating LEDs,
which consequently reduces illumination efficiency by lowering their radiant
flux. To address these limitations, recent research has introduced the concept
of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals
of Opportunity (LSOOP) emitted by unmodulated illumination sources such as
conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure
alternative for indoor positioning by eliminating the need for modulation
hardware and maintaining lighting efficiency. This paper delineates the
fundamental principles of uVLP, provides a comparative analysis of uVLP versus
conventional VLP methods, and classifies existing uVLP techniques according to
receiver technologies into intensity-based methods (e.g., photodiodes, solar
cells, etc.) and imaging-based methods. Additionally, we propose a
comprehensive taxonomy categorizing techniques into demultiplexed and
undemultiplexed approaches. Within this structured framework, we critically
review current advancements in uVLP, discuss prevailing challenges, and outline
promising research directions essential for developing robust, scalable, and
widely deployable uVLP solutions.

</details>


### [8] [Angle Estimation of a Single Source with Massive Uniform Circular Arrays](https://arxiv.org/abs/2507.13086)
*Mingyan Gong*

Main category: eess.SP

TL;DR: 提出了一种基于均匀圆形阵列（UCA）的简单二维DOA估计方法，适用于实时信号处理，并能处理非均匀噪声。


<details>
  <summary>Details</summary>
Motivation: 均匀线性阵列只能估计方位角，而UCA能提供360度方位角和仰角信息，因此需要一种简单高效的DOA估计方法。

Method: 通过量化方位角并计算协方差，再通过显式公式估计仰角，计算简单。

Result: 数值结果表明，该方法能有效估计方位角和仰角，并可作为高精度多维搜索的初始点。

Conclusion: 该方法计算简单，适用于实时处理，且在非均匀噪声下仍有效。

Abstract: Estimating the directions of arrival (DOAs) of incoming plane waves is an
essential topic in array signal processing. Widely adopted uniform linear
arrays can only provide estimates of source azimuth. Thus, uniform circular
arrays (UCAs) are attractive in that they can provide $360^{\circ}$ azimuthal
coverage and additional elevation angle information. Considering that with a
massive UCA, its polar angles of array sensors can approximately represent
azimuth angles over $360^{\circ}$ using angle quantization, a simple
two-dimensional DOA estimation method for a single source is proposed. In this
method, the quantized azimuth angle estimate is obtained by only calculating
and comparing a number of covariances, based on which the elevation angle
estimate is then obtained by an explicit formula. Thus, the proposed method is
computationally simple and suitable for real-time signal processing. Numerical
results verify that the proposed method can obtain azimuth as well as elevation
angle estimates and the estimates can be used as starting points of
multidimensional searches for methods with higher accuracy. Additionally, the
proposed method can still work in the presence of nonuniform noise.

</details>


### [9] [Multifrequency system model for multiport time-modulated scatterers](https://arxiv.org/abs/2507.13130)
*Aleksandr D. Kuznetsov,Jari Holopainen,Ville Viikari*

Main category: eess.SP

TL;DR: 提出了一种基于多端口S参数的多频散射模型，适用于非周期性结构，支持时空调制和动态负载控制。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测多频和非周期性散射结构的性能，特别是在时空调制和动态负载控制下。

Method: 扩展多端口S参数模型，纳入结构散射、互耦合和非数字调制，适用于多频和非周期性配置。

Result: 模型验证实验表明其准确性和实用性，适用于广泛的通信和传感系统。

Conclusion: 该模型为多频和非周期性散射结构提供了精确的分析和优化工具。

Abstract: Utilizing scatterers in communication engineering, such as reconfigurable
intelligent surfaces (RISs) and backscatter systems, requires physically
consistent models for accurate performance prediction. A multiport model, which
also accounts for structural scattering, has been developed for non-periodic
scatterers. However, many emerging systems operate at multiple frequencies or
generate intermodulation harmonics, particularly when incorporating space-time
modulation (STM) or dynamic load control. These functionalities demand advanced
modeling approaches capable of capturing scattering behavior across several
frequencies and directions simultaneously. This article extends a multiport
S-parameters-based model for predicting the scattering properties of
multifrequency operating structures. The model extends the applicability of
convenient S-matrix models to time-modulated multiport structures. Unlike known
approaches, this model incorporates structural scattering, mutual coupling, the
possibility of non-digital modulation, and non-periodic configurations,
enabling precise analysis and optimization for a broad range of communication
and sensing systems. Validation against experimental results for a space-time
modulated scattering structure demonstrates the accuracy and practical
applicability of the proposed model.

</details>


### [10] [Disentangling coincident cell events using deep transfer learning and compressive sensing](https://arxiv.org/abs/2507.13176)
*Moritz Leuthner,Rafael Vorländer,Oliver Hayden*

Main category: eess.SP

TL;DR: 提出了一种结合全卷积神经网络（FCN）和压缩感知（CS）的混合框架，用于解决单细胞分析中信号重叠问题，显著提高了事件恢复和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 单细胞分析中的信号重叠问题严重影响了信号保真度，限制了诊断和治疗的准确性。

Method: 使用FCN估计重叠事件数量，结合CS模块重建单个信号成分，实现了高保真度的单细胞特征恢复。

Result: 相比传统算法，恢复事件数量提升21%，分类准确率超过97%。

Conclusion: 该框架为非光学单细胞传感平台提供了自动化、通用化的解决方案，扩展了细胞计数在医学和诊断中的应用。

Abstract: Accurate single-cell analysis is critical for diagnostics, immunomonitoring,
and cell therapy, but coincident events - where multiple cells overlap in a
sensing zone - can severely compromise signal fidelity. We present a hybrid
framework combining a fully convolutional neural network (FCN) with compressive
sensing (CS) to disentangle such overlapping events in one-dimensional sensor
data. The FCN, trained on bead-derived datasets, accurately estimates
coincident event counts and generalizes to immunomagnetically labeled CD4+ and
CD14+ cells in whole blood without retraining. Using this count, the CS module
reconstructs individual signal components with high fidelity, enabling precise
recovery of single-cell features, including velocity, amplitude, and
hydrodynamic diameter. Benchmarking against conventional state-machine
algorithms shows superior performance - recovering up to 21% more events and
improving classification accuracy beyond 97%. Explinability via class
activation maps and parameterized Gaussian template fitting ensures
transparency and clinical interpretability. Demonstrated with magnetic flow
cytometry (MFC), the framework is compatible with other waveform-generating
modalities, including impedance cytometry, nanopore, and resistive pulse
sensing. This work lays the foundation for next-generation non-optical
single-cell sensing platforms that are automated, generalizable, and capable of
resolving overlapping events, broadening the utility of cytometry in
translational medicine and precision diagnostics, e.g. cell-interaction
studies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models](https://arxiv.org/abs/2507.12595)
*Orchid Chetia Phukan,Mohd Mujtaba Akhtar,Girish,Arun Balaji Buduru*

Main category: eess.AS

TL;DR: 该论文提出了一种基于多语言语音基础模型（SFMs）的情感伪造检测（EFD）方法，并通过THAMA模型融合技术显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 情感伪造检测（EFD）需要捕捉语音中的细微变化（如音调、音强），而多语言SFMs因其跨语言预训练能力，可能更适合此任务。

Method: 比较了多种SOTA SFMs，并提出THAMA模型融合技术，结合Tucker分解和Hadamard乘积以实现高效融合。

Result: 多语言SFMs在相同语言和跨语言评估中表现优异，THAMA进一步提升了性能，超越了现有方法。

Conclusion: 多语言SFMs与THAMA的结合在EFD任务中表现出色，为未来研究提供了新方向。

Abstract: In this work, we address EmoFake Detection (EFD). We hypothesize that
multilingual speech foundation models (SFMs) will be particularly effective for
EFD due to their pre-training across diverse languages, enabling a nuanced
understanding of variations in pitch, tone, and intensity. To validate this, we
conduct a comprehensive comparative analysis of state-of-the-art (SOTA) SFMs.
Our results shows the superiority of multilingual SFMs for same language
(in-domain) as well as cross-lingual (out-domain) evaluation. To our end, we
also propose, THAMA for fusion of foundation models (FMs) motivated by related
research where combining FMs have shown improved performance. THAMA leverages
the complementary conjunction of tucker decomposition and hadamard product for
effective fusion. With THAMA, synergized with cooperative multilingual SFMs
achieves topmost performance across in-domain and out-domain settings,
outperforming individual FMs, baseline fusion techniques, and prior SOTA
methods.

</details>


### [12] [DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization](https://arxiv.org/abs/2507.12890)
*Huakang Chen,Yuepeng Jiang,Guobin Ma,Chunbo Hao,Shuai Wang,Jixun Yao,Ziqian Ning,Meng Meng,Jian Luan,Lei Xie*

Main category: eess.AS

TL;DR: DiffRhythm+ 是一种改进的扩散模型，用于可控且灵活的全长歌曲生成，解决了数据不平衡和风格控制不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前全长歌曲生成系统存在数据不平衡、可控性不足和音乐质量不一致的问题，DiffRhythm+ 旨在解决这些限制。

Method: DiffRhythm+ 使用扩展且平衡的训练数据集，并引入多模态风格调节策略，结合文本和参考音频进行精确风格控制。

Result: 实验表明，DiffRhythm+ 在自然度、编曲复杂性和听众满意度方面显著优于之前的系统。

Conclusion: DiffRhythm+ 通过改进数据集和增强控制能力，实现了更高质量和更灵活的全长歌曲生成。

Abstract: Songs, as a central form of musical art, exemplify the richness of human
intelligence and creativity. While recent advances in generative modeling have
enabled notable progress in long-form song generation, current systems for
full-length song synthesis still face major challenges, including data
imbalance, insufficient controllability, and inconsistent musical quality.
DiffRhythm, a pioneering diffusion-based model, advanced the field by
generating full-length songs with expressive vocals and accompaniment. However,
its performance was constrained by an unbalanced model training dataset and
limited controllability over musical style, resulting in noticeable quality
disparities and restricted creative flexibility. To address these limitations,
we propose DiffRhythm+, an enhanced diffusion-based framework for controllable
and flexible full-length song generation. DiffRhythm+ leverages a substantially
expanded and balanced training dataset to mitigate issues such as repetition
and omission of lyrics, while also fostering the emergence of richer musical
skills and expressiveness. The framework introduces a multi-modal style
conditioning strategy, enabling users to precisely specify musical styles
through both descriptive text and reference audio, thereby significantly
enhancing creative control and diversity. We further introduce direct
performance optimization aligned with user preferences, guiding the model
toward consistently preferred outputs across evaluation metrics. Extensive
experiments demonstrate that DiffRhythm+ achieves significant improvements in
naturalness, arrangement complexity, and listener satisfaction over previous
systems.

</details>


### [13] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: UniSLU是一个统一框架，通过单一架构联合建模多个SLU任务，提升任务交互并利用异构数据集，实验证明其优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有SLU方法依赖独立模型架构，增加了系统复杂性，限制了任务间交互，未能充分利用异构数据集。

Method: 提出统一表示和生成方法，联合建模ASR、spoken NER和SA任务，并与大语言模型集成。

Result: 在公开SLU数据集上表现优异，超越基准方法。

Conclusion: UniSLU适用于现实语音多媒体场景，代码和模型将开源。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


### [14] [AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers with Multi-Scale and Multi-Task Learning](https://arxiv.org/abs/2507.12972)
*Daning Zhang,Ying Wei*

Main category: eess.AS

TL;DR: AVFSNet是一个结合音频和视觉信息的语音分离模型，用于处理未知说话人数量的混合信号，通过并行架构和多尺度编码实现高效分离。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要已知说话人数量，而未知数量场景的研究较少且泛化能力有限，因此提出AVFSNet以解决这一问题。

Method: AVFSNet采用多尺度编码和并行架构，结合视觉信息增强噪声适应性，同时优化说话人计数和多说话人分离任务。

Result: 实验表明，AVFSNet在多个评估指标上达到最先进水平，并在不同数据集上表现优异。

Conclusion: AVFSNet通过音频-视觉融合和多任务优化，显著提升了未知说话人数量场景下的语音分离性能。

Abstract: Separating target speech from mixed signals containing flexible speaker
quantities presents a challenging task. While existing methods demonstrate
strong separation performance and noise robustness, they predominantly assume
prior knowledge of speaker counts in mixtures. The limited research addressing
unknown speaker quantity scenarios exhibits significantly constrained
generalization capabilities in real acoustic environments. To overcome these
challenges, this paper proposes AVFSNet -- an audio-visual speech separation
model integrating multi-scale encoding and parallel architecture -- jointly
optimized for speaker counting and multi-speaker separation tasks. The model
independently separates each speaker in parallel while enhancing environmental
noise adaptability through visual information integration. Comprehensive
experimental evaluations demonstrate that AVFSNet achieves state-of-the-art
results across multiple evaluation metrics and delivers outstanding performance
on diverse datasets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [15] [Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates](https://arxiv.org/abs/2507.12563)
*Carlos De La Vega Martin,Rodrigo Diaz Fernandez,Mark Sandler*

Main category: cs.SD

TL;DR: 本文比较分析了基于神经网络的方法来解决非线性弹性板振动问题，用于物理建模音频合成，评估了多个最先进模型在自回归长序列预测中的表现，并讨论了实时音频合成的应用前景


<details>
  <summary>Details</summary>
Motivation: 传统的有限差分和有限元数值方法虽然精度高但计算量大，限制了在实时音频应用中的使用，因此需要探索神经网络方法来解决薄弹性板振动建模问题，以实现高效的物理建模音频合成

Method: 对多个最先进的神经网络模型进行比较分析，这些模型在短序列上训练，然后以自回归方式预测长序列。重点评估模型在非线性弹性板振动预测中的性能表现

Result: 研究揭示了现有神经网络模型的一些局限性，特别指出仅仅关注时域预测误差是不够的。模型在长序列自回归预测中存在一定的性能限制

Conclusion: 讨论了这些发现对实时音频合成的意义，并提出了改进神经网络方法来建模非线性振动的未来研究方向，为物理建模音频合成领域提供了重要的技术洞察

Abstract: Physical modelling synthesis aims to generate audio from physical simulations
of vibrating structures. Thin elastic plates are a common model for drum
membranes. Traditional numerical methods like finite differences and finite
elements offer high accuracy but are computationally demanding, limiting their
use in real-time audio applications. This paper presents a comparative analysis
of neural network-based approaches for solving the vibration of nonlinear
elastic plates. We evaluate several state-of-the-art models, trained on short
sequences, for prediction of long sequences in an autoregressive fashion. We
show some of the limitations of these models, and why is not enough to look at
the prediction error in the time domain. We discuss the implications for
real-time audio synthesis and propose future directions for improving neural
approaches to model nonlinear vibration.

</details>


### [16] [Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine](https://arxiv.org/abs/2507.12701)
*Anastasia Kuznetsova,Inseon Jang,Wootaek Lim,Minje Kim*

Main category: cs.SD

TL;DR: 提出了一种高效的音频编码方法（ACoM），专注于机器任务而非人类感知，通过任务特定损失和残差向量量化实现超低比特率。


<details>
  <summary>Details</summary>
Motivation: 传统神经音频编解码器注重高保真重建，而ACoM更关注机器任务的效率和性能。

Method: 采用任务特定损失和残差向量量化（RVQ）损失，压缩和量化已训练模型的中间特征表示。

Result: 在自动语音识别和音频分类任务中表现优异，支持超低比特率（<200 bps）且性能损失极小。

Conclusion: 该方法灵活适应不同比特率和模型规模，具有广泛的任务和架构适用性。

Abstract: Neural audio codecs, leveraging quantization algorithms, have significantly
impacted various speech/audio tasks. While high-fidelity reconstruction is
paramount for human perception, audio coding for machines (ACoM) prioritizes
efficient compression and downstream task performance, disregarding perceptual
nuances. This work introduces an efficient ACoM method that can compress and
quantize any chosen intermediate feature representation of an already trained
speech/audio downstream model. Our approach employs task-specific loss guidance
alongside residual vector quantization (RVQ) losses, providing ultra-low
bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model
performance. The resulting tokenizer is adaptable to various bitrates and model
sizes for flexible deployment. Evaluated on automatic speech recognition and
audio classification, our method demonstrates its efficacy and potential for
broader task and architectural applicability through appropriate
regularization.

</details>


### [17] [Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries](https://arxiv.org/abs/2507.12723)
*Minyoung Kim,Sehwan Park,Sungmin Cha,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 论文提出了一种跨模态水印框架，用于从合成的视听伪造中恢复真实音频并定位篡改，以对抗虚假信息。


<details>
  <summary>Details</summary>
Motivation: 现有的方法只能检测或定位视听伪造，无法恢复真实音频，限制了对抗虚假信息的效果。

Method: 提出跨模态水印框架，在伪造前将真实音频嵌入视觉信息中，以实现真实音频恢复和篡改定位。

Result: 实验表明，该方法在对抗多种伪造（如语音克隆和唇同步）时表现优异。

Conclusion: 该方法为视听伪造提供了有效的防御手段，能恢复真实音频并定位篡改。

Abstract: Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.

</details>


### [18] [Sample-Constrained Black Box Optimization for Audio Personalization](https://arxiv.org/abs/2507.12773)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: 论文提出了一种混合查询方法，通过结合用户对音频样本的整体评分和对滤波器元素的单独评分，利用稀疏高斯过程回归优化音频个性化，以最大化用户满意度。


<details>
  <summary>Details</summary>
Motivation: 解决音频个性化中的黑盒优化问题，用户满意度函数未知，传统方法仅依赖整体评分，而混合查询方法能更高效地找到最优滤波器。

Method: 采用稀疏高斯过程回归（GPR），结合用户对音频样本的整体评分和对滤波器元素的单独评分，设计混合查询策略。

Result: 通过模拟和真实实验验证，混合查询方法优于单一查询类型，能显著提高用户满意度。

Conclusion: 混合查询方法为黑盒优化开辟了新方向，其解决方案可应用于音频个性化以外的其他领域。

Abstract: We consider the problem of personalizing audio to maximize user experience.
Briefly, we aim to find a filter $h^*$, which applied to any music or speech,
will maximize the user's satisfaction. This is a black-box optimization problem
since the user's satisfaction function is unknown. Substantive work has been
done on this topic where the key idea is to play audio samples to the user,
each shaped by a different filter $h_i$, and query the user for their
satisfaction scores $f(h_i)$. A family of ``surrogate" functions is then
designed to fit these scores and the optimization method gradually refines
these functions to arrive at the filter $\hat{h}^*$ that maximizes
satisfaction. In certain applications, we observe that a second type of
querying is possible where users can tell us the individual elements $h^*[j]$
of the optimal filter $h^*$. Consider an analogy from cooking where the goal is
to cook a recipe that maximizes user satisfaction. A user can be asked to score
various cooked recipes (e.g., tofu fried rice) or to score individual
ingredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$
queries, where a query can be of either type, our goal is to find the recipe
that will maximize this user's satisfaction. Our proposal builds on Sparse
Gaussian Process Regression (GPR) and shows how a hybrid approach can
outperform any one type of querying. Our results are validated through
simulations and real world experiments, where volunteers gave feedback on
music/speech audio and were able to achieve high satisfaction levels. We
believe this idea of hybrid querying opens new problems in black-box
optimization and solutions can benefit other applications beyond audio
personalization.

</details>


### [19] [Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features](https://arxiv.org/abs/2507.12793)
*J. M. Chan Sri Manukalpa,H. S. Bopage,W. A. M. Jayawardena,P. K. P. G. Panduwawala*

Main category: cs.SD

TL;DR: 提出了一种基于深度学习的非侵入性声学分类框架，用于早期白蚁检测，混合CNN-LSTM模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统白蚁检测方法侵入性强且效果不佳，需要一种非侵入、高效的早期检测方法。

Method: 采用混合CNN-LSTM架构，提取MFCC特征，训练模型分类白蚁声学信号。

Result: 模型准确率94.5%，精确率93.2%，召回率95.8%，优于单独CNN或LSTM模型。

Conclusion: 该研究为非侵入性白蚁检测提供了自动化解决方案，未来可结合IoT扩展应用。

Abstract: Structural pests, such as termites, pose a serious threat to wooden
buildings, resulting in significant economic losses due to their hidden and
progressive damage. Traditional detection methods, such as visual inspections
and chemical treatments, are invasive, labor intensive, and ineffective for
early stage infestations. To bridge this gap, this study proposes a non
invasive deep learning based acoustic classification framework for early
termite detection. We aim to develop a robust, scalable model that
distinguishes termite generated acoustic signals from background noise. We
introduce a hybrid Convolutional Neural Network Long Short Term Memory
architecture that captures both spatial and temporal features of termite
activity. Audio data were collected from termite infested and clean wooden
samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN
LSTM model to classify the signals. Experimental results show high performance,
with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis
reveals that the hybrid model outperforms standalone CNN and LSTM
architectures, underscoring its combined strength. Notably, the model yields
low false-negative rates, which is essential for enabling timely intervention.
This research contributes a non invasive, automated solution for early termite
detection, with practical implications for improved pest monitoring, minimized
structural damage, and better decision making by homeowners and pest control
professionals. Future work may integrate IoT for real time alerts and extend
detection to other structural pests.

</details>


### [20] [Autoregressive Speech Enhancement via Acoustic Tokens](https://arxiv.org/abs/2507.12825)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.SD

TL;DR: 论文研究了语音增强中离散表示（如声学标记）的性能，提出了一种新的自回归架构，并发现声学标记在保留说话人身份方面优于语义标记，但仍不及连续表示。


<details>
  <summary>Details</summary>
Motivation: 现有语音增强方法主要依赖监督回归，而离散表示（如音频标记化）的研究较少，且现有方法多关注语义标记，忽略了声学细节和自回归建模的潜力。

Method: 1）全面研究声学标记的性能，包括比特率和噪声强度的影响；2）提出一种新的基于转换器的自回归架构。

Result: 实验表明，声学标记在保留说话人身份方面优于语义标记，自回归方法进一步提升了性能，但离散表示仍不及连续表示。

Conclusion: 离散表示在语音增强中具有潜力，但仍需进一步研究以缩小与连续表示的差距。

Abstract: In speech processing pipelines, improving the quality and intelligibility of
real-world recordings is crucial. While supervised regression is the primary
method for speech enhancement, audio tokenization is emerging as a promising
alternative for a smooth integration with other modalities. However, research
on speech enhancement using discrete representations is still limited. Previous
work has mainly focused on semantic tokens, which tend to discard key acoustic
details such as speaker identity. Additionally, these studies typically employ
non-autoregressive models, assuming conditional independence of outputs and
overlooking the potential improvements offered by autoregressive modeling. To
address these gaps we: 1) conduct a comprehensive study of the performance of
acoustic tokens for speech enhancement, including the effect of bitrate and
noise strength; 2) introduce a novel transducer-based autoregressive
architecture specifically designed for this task. Experiments on VoiceBank and
Libri1Mix datasets show that acoustic tokens outperform semantic tokens in
terms of preserving speaker identity, and that our autoregressive approach can
further improve performance. Nevertheless, we observe that discrete
representations still fall short compared to continuous ones, highlighting the
need for further research in this area.

</details>


### [21] [Best Practices and Considerations for Child Speech Corpus Collection and Curation in Educational, Clinical, and Forensic Scenarios](https://arxiv.org/abs/2507.12870)
*John Hansen,Satwik Dutta,Ellen Grand*

Main category: cs.SD

TL;DR: 研究旨在为儿童语音语料库的开发提供最佳实践和考虑因素，涵盖数据收集、协作建立及语料库质量检查。


<details>
  <summary>Details</summary>
Motivation: 儿童语音能力动态变化及数据隐私问题使得构建技术就绪的儿童语音语料库具有挑战性。

Method: 描述数据收集的WHO、WHAT、WHEN、WHERE，并指导建立协作与信任，以及研究伦理协议。

Result: 提供了语料库质量检查、分类和标注的指南。

Conclusion: 研究为跨领域（教育、临床、法医）的儿童语音数据应用提供了实用框架。

Abstract: A child's spoken ability continues to change until their adult age. Until
7-8yrs, their speech sound development and language structure evolve rapidly.
This dynamic shift in their spoken communication skills and data privacy make
it challenging to curate technology-ready speech corpora for children. This
study aims to bridge this gap and provide researchers and practitioners with
the best practices and considerations for developing such a corpus based on an
intended goal. Although primarily focused on educational goals, applications of
child speech data have spread across fields including clinical and forensics
fields. Motivated by this goal, we describe the WHO, WHAT, WHEN, and WHERE of
data collection inspired by prior collection efforts and our
experience/knowledge. We also provide a guide to establish collaboration,
trust, and for navigating the human subjects research protocol. This study
concludes with guidelines for corpus quality check, triage, and annotation.

</details>


### [22] [Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes](https://arxiv.org/abs/2507.12932)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.SD

TL;DR: Enkidu是一种新型用户导向隐私保护框架，通过黑盒知识和少量用户数据训练生成通用频域扰动，有效防御语音深度伪造攻击，同时保持实时性和高效性。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造技术的快速发展威胁用户音频隐私，现有防御方法存在适应性差、计算成本高等问题。

Method: 利用黑盒知识和少量用户数据生成频域噪声补丁，实现实时轻量级保护。

Result: Enkidu在内存和运行时效率上显著优于现有方法，并在实验中表现出高效防御能力。

Conclusion: Enkidu提供了一种高效、可扩展的解决方案，有效对抗语音深度伪造威胁。

Abstract: The rapid advancement of voice deepfake technologies has raised serious
concerns about user audio privacy, as attackers increasingly exploit publicly
available voice data to generate convincing fake audio for malicious purposes
such as identity theft, financial fraud, and misinformation campaigns. While
existing defense methods offer partial protection, they face critical
limitations, including weak adaptability to unseen user data, poor scalability
to long audio, rigid reliance on white-box knowledge, and high computational
and temporal costs during the encryption process. To address these challenges
and defend against personalized voice deepfake threats, we propose Enkidu, a
novel user-oriented privacy-preserving framework that leverages universal
frequential perturbations generated through black-box knowledge and few-shot
training on a small amount of user data. These highly malleable
frequency-domain noise patches enable real-time, lightweight protection with
strong generalization across variable-length audio and robust resistance to
voice deepfake attacks, all while preserving perceptual quality and speech
intelligibility. Notably, Enkidu achieves over 50 to 200 times processing
memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime
efficiency (real-time coefficient as low as 0.004) compared to six
state-of-the-art countermeasures. Extensive experiments across six mainstream
text-to-speech models and five cutting-edge automated speaker verification
models demonstrate the effectiveness, transferability, and practicality of
Enkidu in defending against both vanilla and adaptive voice deepfake attacks.

</details>


### [23] [Multi-Class-Token Transformer for Multitask Self-supervised Music Information Retrieval](https://arxiv.org/abs/2507.12996)
*Yuexuan Kong,Vincent Lostanlen,Romain Hennequin,Mathieu Lagrange,Gabriel Meseguer-Brocal*

Main category: cs.SD

TL;DR: 论文提出了一种结合对比学习和等变学习的自监督多任务学习方法（MT2），通过双类别标记的ViT-1D架构，在音乐信息检索任务中表现优于单一任务模型。


<details>
  <summary>Details</summary>
Motivation: 对比学习在标签任务（如乐器识别）中表现更好，而等变学习在特定任务（如调性估计）中表现优异，但两者各有局限性。MT2旨在结合两者的优势。

Method: 使用Vision Transformer（ViT-1D）架构，配备两个类别标记，分别优化等变学习（CPSD）和对比学习（NT-Xent），并通过同一模型进行多任务训练。

Result: MT2在多个任务中表现优于单一任务模型，且通过平均两个类别标记进一步提升了性能。其参数效率高，仅需MERT的1/18参数。

Conclusion: MT2展示了多类别标记多任务学习在音乐信息检索中的潜力，结合了对比学习和等变学习的互补优势。

Abstract: Contrastive learning and equivariant learning are effective methods for
self-supervised learning (SSL) for audio content analysis. Yet, their
application to music information retrieval (MIR) faces a dilemma: the former is
more effective on tagging (e.g., instrument recognition) but less effective on
structured prediction (e.g., tonality estimation); The latter can match
supervised methods on the specific task it is designed for, but it does not
generalize well to other tasks. In this article, we adopt a best-of-both-worlds
approach by training a deep neural network on both kinds of pretext tasks at
once. The proposed new architecture is a Vision Transformer with 1-D
spectrogram patches (ViT-1D), equipped with two class tokens, which are
specialized to different self-supervised pretext tasks but optimized through
the same model: hence the qualification of self-supervised multi-class-token
multitask (MT2). The former class token optimizes cross-power spectral density
(CPSD) for equivariant learning over the circle of fifths, while the latter
optimizes normalized temperature-scaled cross-entropy (NT-Xent) for contrastive
learning. MT2 combines the strengths of both pretext tasks and outperforms
consistently both single-class-token ViT-1D models trained with either
contrastive or equivariant learning. Averaging the two class tokens further
improves performance on several tasks, highlighting the complementary nature of
the representations learned by each class token. Furthermore, using the same
single-linear-layer probing method on the features of last layer, MT2
outperforms MERT on all tasks except for beat tracking; achieving this with 18x
fewer parameters thanks to its multitasking capabilities. Our SSL benchmark
demonstrates the versatility of our multi-class-token multitask learning
approach for MIR applications.

</details>


### [24] [SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks](https://arxiv.org/abs/2507.13170)
*Kutub Uddin,Awais Khan,Muhammad Umar Farooq,Khalid Malik*

Main category: cs.SD

TL;DR: 论文提出了一种名为SHIELD的协作学习方法，用于防御生成式反取证攻击，显著提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造（如deepfake）的传播带来了严重风险，现有检测方法易受反取证攻击，尤其是基于生成对抗网络的攻击。

Method: 提出SHIELD方法，结合辅助生成模型（DF生成模型）和三重模型，捕捉真实音频与受攻击音频之间的关联。

Result: SHIELD在多种生成模型下表现稳健，显著降低了反取证攻击的影响，检测准确率平均提升至98%以上。

Conclusion: SHIELD方法有效防御生成式反取证攻击，为音频深度伪造检测提供了可靠解决方案。

Abstract: Audio plays a crucial role in applications like speaker verification,
voice-enabled smart devices, and audio conferencing. However, audio
manipulations, such as deepfakes, pose significant risks by enabling the spread
of misinformation. Our empirical analysis reveals that existing methods for
detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,
particularly those attacked using generative adversarial networks. In this
article, we propose a novel collaborative learning method called SHIELD to
defend against generative AF attacks. To expose AF signatures, we integrate an
auxiliary generative model, called the defense (DF) generative model, which
facilitates collaborative learning by combining input and output. Furthermore,
we design a triplet model to capture correlations for real and AF attacked
audios with real-generated and attacked-generated audios using auxiliary
generative models. The proposed SHIELD strengthens the defense against
generative AF attacks and achieves robust performance across various generative
models. The proposed AF significantly reduces the average detection accuracy
from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,
and from 98.41% to 51.18% for HalfTruth for three different generative models.
The proposed SHIELD mechanism is robust against AF attacks and achieves an
average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,
and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and
HalfTruth datasets, respectively.

</details>


### [25] [Voxtral](https://arxiv.org/abs/2507.13264)
*Alexander H. Liu,Andy Ehrenberg,Andy Lo,Clément Denoix,Corentin Barreau,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Sanchit Gandhi,Soham Ghosh,Srijan Mishra,Thomas Foubert,Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devendra Singh Chaplot,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Hadrien Chabran,Jessica Chudnovsky,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Romain Sauvestre,Roman Soletskyi,Sagar Vaze,Sandeep Subramanian,Saurabh Garg,Shashwat Dalal,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SD

TL;DR: Voxtral Mini和Voxtral Small是多模态音频聊天模型，支持语音和文本输入，性能优异且可本地运行。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时理解语音和文本的高性能模型，并在本地设备上运行。

Method: 训练多模态模型，支持32K上下文窗口，处理长达40分钟的音频和多轮对话。

Result: Voxtral Small超越多个闭源模型，并在音频基准测试中表现优异。

Conclusion: Voxtral模型在语音理解任务中表现出色，并开源发布。

Abstract: We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.
Voxtral is trained to comprehend both spoken audio and text documents,
achieving state-of-the-art performance across a diverse range of audio
benchmarks, while preserving strong text capabilities. Voxtral Small
outperforms a number of closed-source models, while being small enough to run
locally. A 32K context window enables the model to handle audio files up to 40
minutes in duration and long multi-turn conversations. We also contribute three
benchmarks for evaluating speech understanding models on knowledge and trivia.
Both Voxtral models are released under Apache 2.0 license.

</details>

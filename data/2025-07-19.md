<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS](https://arxiv.org/abs/2507.12593)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种基于Zak-OTFS的差分通信方案，减少周期性导频传输需求，提高频谱效率和能量分配。


<details>
  <summary>Details</summary>
Motivation: Zak-OTFS在延迟-多普勒域中具有可预测性，但仍需周期性导频传输。本文旨在通过差分通信方案减少导频需求。

Method: 利用检测到的数据作为导频，通过DD域信道预测能力，将前一时刻的信道估计用于下一时刻的数据检测。

Result: 方案提高了数据符号的能量分配和频谱效率，相比现有方法具有更低的误码率和复杂度。

Conclusion: 差分通信方案在Zak-OTFS系统中有效减少导频需求，提升性能。

Abstract: Zak-transform based orthogonal time frequency space (Zak-OTFS) is a
delay-Doppler (DD) domain modulation scheme in which the signal processing is
carried out in the DD domain. The channel when viewed in the DD domain is
predictable. However, even with Zak-OTFS, pilots need to be sent periodically,
albeit at a lower rate. In this paper, we propose a differential communication
scheme for Zak-OTFS systems that alleviates the need for periodic pilot
transmission. Towards this, we analytically show that the detected data can be
used as a pilot and that the channel estimate obtained from the detected data
can enable further detection enabling the "differential" aspect of the
communication. Specifically, we leverage the prediction capability of the DD
channel in Zak-OTFS to use the channel estimate (obtained from detected data
symbols treated as pilots) in the previous instant to detect data in the next
instant and propagate this forward. The advantages are two fold. First, it
allows the data symbols to enjoy higher energy since the energy that would
otherwise be required for pilot symbols can also be allocated to data symbols.
Second, it allows for full spectral efficiency compared to point or embedded
pilots. Comparison with the full spectral efficiency achieving spread pilot
scheme shows that the proposed method achieves better bit-error rate at lower
complexity.

</details>


### [2] [Achieving Robust Channel Estimation Neural Networks by Designed Training Data](https://arxiv.org/abs/2507.12630)
*Dianxin Luan,John Thompson*

Main category: eess.SP

TL;DR: 提出了一种离线训练的神经网络设计标准，用于生成合成训练数据集，确保网络在新信道上达到特定MSE，无需实际信道信息或参数更新。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动方法在新数据上性能下降的问题，同时满足低延迟和有限计算资源的需求。

Method: 设计合成训练数据集的标准，并提出基准设计，确保对不同信道配置的智能操作。

Result: 神经网络在不同复杂度下均能实现对新信道的鲁棒泛化，仿真验证了其有效性。

Conclusion: 离线训练的神经网络设计标准可实现对新信道的鲁棒性能，且与网络架构无关。

Abstract: Channel estimation is crucial in cognitive communications, as it enables
intelligent spectrum sensing and adaptive transmission by providing accurate
information about the current channel state. However, in many papers neural
networks are frequently tested by training and testing on one example channel
or similar channels. This is because data-driven methods often degrade on new
data which they are not trained on, as they cannot extrapolate their training
knowledge. This is despite the fact physical channels are often assumed to be
time-variant. However, due to the low latency requirements and limited
computing resources, neural networks may not have enough time and computing
resources to execute online training to fine-tune the parameters. This
motivates us to design offline-trained neural networks that can perform
robustly over wireless channels, but without any actual channel information
being known at design time. In this paper, we propose design criteria to
generate synthetic training datasets for neural networks, which guarantee that
after training the resulting networks achieve a certain mean squared error
(MSE) on new and previously unseen channels. Therefore, neural network
solutions require no prior channel information or parameters update for
real-world implementations. Based on the proposed design criteria, we further
propose a benchmark design which ensures intelligent operation for different
channel profiles. To demonstrate general applicability, we use neural networks
with different levels of complexity to show that the generalization achieved
appears to be independent of neural network architecture. From simulations,
neural networks achieve robust generalization to wireless channels with both
fixed channel profiles and variable delay spreads.

</details>


### [3] [A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis](https://arxiv.org/abs/2507.12645)
*Mohammed Guhdar,Ramadhan J. Mstafa,Abdulhakeem O. Mohammed*

Main category: eess.SP

TL;DR: 提出了一种统一的深度学习框架，用于处理多种生物信号（如ECG和EEG），解决了信号差异和类别不平衡问题，并在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 需要统一处理不同生物信号（如ECG和EEG）的架构，并解决类别不平衡问题，以提高患者评估的准确性。

Method: 结合ResNet CNN和注意力机制，采用时间域拼接的数据增强策略，并使用Focal Loss处理类别不平衡。预处理包括小波去噪、基线去除和标准化。

Result: 在UCI Seizure EEG、MIT-BIH Arrhythmia和PTB Diagnostic ECG数据集上分别达到99.96%、99.78%和100%的准确率。

Conclusion: 该框架在多种信号类型和临床场景中表现出鲁棒性，且适合部署在低端或可穿戴设备上。

Abstract: The increasing need for accurate and unified analysis of diverse biological
signals, such as ECG and EEG, is paramount for comprehensive patient
assessment, especially in synchronous monitoring. Despite advances in
multi-sensor fusion, a critical gap remains in developing unified architectures
that effectively process and extract features from fundamentally different
physiological signals. Another challenge is the inherent class imbalance in
many biomedical datasets, often causing biased performance in traditional
methods. This study addresses these issues by proposing a novel and unified
deep learning framework that achieves state-of-the-art performance across
different signal types. Our method integrates a ResNet-based CNN with an
attention mechanism, enhanced by a novel data augmentation strategy:
time-domain concatenation of multiple augmented variants of each signal to
generate richer representations. Unlike prior work, we scientifically increase
signal complexity to achieve future-reaching capabilities, which resulted in
the best predictions compared to the state of the art. Preprocessing steps
included wavelet denoising, baseline removal, and standardization. Class
imbalance was effectively managed through the combined use of this advanced
data augmentation and the Focal Loss function. Regularization techniques were
applied during training to ensure generalization. We rigorously evaluated the
proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH
Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,
and 100%, respectively, demonstrating robustness across diverse signal types
and clinical contexts. Finally, the architecture requires ~130 MB of memory and
processes each sample in ~10 ms, suggesting suitability for deployment on
low-end or wearable devices.

</details>


### [4] [Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers](https://arxiv.org/abs/2507.12706)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 论文提出了一种基于多机器学习分类器一致投票的保守卫星选择策略，增强了ZSM定位方法，显著提高了城市GNSS环境中的定位成功率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中，GNSS定位常因建筑物导致的信号遮挡和多径效应而产生显著误差，需改进定位方法以提高可靠性。

Method: 使用RF、GBDT和SVM三种分类器对GPS信号进行LOS/NLOS分类，仅当所有分类器一致且置信度超过阈值时选择卫星。

Result: 实验表明，该方法显著提高了定位成功率和接收机包含率，尽管卫星数量减少导致定位边界略有增加。

Conclusion: 所提方法在城市GNSS环境中有效提升了定位可靠性。

Abstract: In urban environments, global navigation satellite system (GNSS) positioning
is often compromised by signal blockages and multipath effects caused by
buildings, leading to significant positioning errors. To address this issue,
this study proposes a robust enhancement of zonotope shadow matching
(ZSM)-based positioning by employing a conservative satellite selection
strategy using unanimous voting across multiple machine learning classifiers.
Three distinct models - random forest (RF), gradient boosting decision tree
(GBDT), and support vector machine (SVM) - were trained to perform
line-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global
positioning system (GPS) signal features. A satellite is selected for
positioning only when all classifiers unanimously agree on its classification
and their associated confidence scores exceed a threshold. Experiments with
real-world GPS data collected in dense urban areas demonstrate that the
proposed method significantly improves the positioning success rate and the
receiver containment rate, even with imperfect LOS/NLOS classification.
Although a slight increase in the position bound was observed due to the
reduced number of satellites used, overall positioning reliability was
substantially enhanced, indicating the effectiveness of the proposed approach
in urban GNSS environments.

</details>


### [5] [Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO](https://arxiv.org/abs/2507.12917)
*Xi Ding,Luca Kunz,E. Jorswieck*

Main category: eess.SP

TL;DR: 提出了一种基于SDR的全局最优波束成形框架，用于小规模无小区MIMO系统中的联合感知与通信。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏全局最优性或需要额外后处理步骤，需改进。

Method: 采用SDR优化框架，无需后处理即可保证全局最优解。

Result: 框架提供全局最优且计算高效的波束成形设计。

Conclusion: 该框架为下一代无线网络发展提供了重要参考。

Abstract: This paper studies optimal joint beamforming (BF) for joint sensing and
communication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While
prior works have explored JSAC optimization using methods such as successive
convex approximation (SCA) and semidefinite relaxation (SDR), many of these
approaches either lack global optimality or require additional rank-reduction
steps. In contrast, we propose an SDR-based optimization framework that
guarantees globally optimal solutions without post-processing. To benchmark its
performance, we introduce a standalone BF strategy that dedicates each access
point (AP) exclusively to either communication or sensing. The proposed
formulation builds upon a general multi-user system model, enabling future
extensions beyond the single-user setting. Overall, our framework offers a
globally optimal and computationally efficient BF design, providing valuable
insights for the development of next-generation wireless networks.

</details>


### [6] [Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation](https://arxiv.org/abs/2507.13037)
*Guangyao Liu,Tianqi Mao,Yanqun Tang,Jingjing Zhao,Zhenyu Xiao*

Main category: eess.SP

TL;DR: 本文提出了一种基于AFDM的多模式索引调制方案（MM-AFDM-IM），旨在提高AFDM的频谱和能量效率。通过动态选择星座模式和激活子载波，无需额外能耗即可传输更多信息位。仿真结果验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: AFDM在高移动性通信场景中具有潜力，但需要进一步提升频谱和能量效率。

Method: 开发了多模式索引调制方案（MM-AFDM-IM），动态选择星座模式和激活子载波以传输额外信息位。

Result: 仿真结果表明，MM-AFDM-IM在比特错误率（BER）上优于传统方案。

Conclusion: MM-AFDM-IM是一种高效且性能优越的调制方案，适用于高移动性通信场景。

Abstract: Affine frequency division multiplexing (AFDM), a promising multicarrier
technique utilizing chirp signals, has been envisioned as an effective solution
for high-mobility communication scenarios. In this paper, we develop a
multiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,
which aims to further improve the spectral and energy efficiencies of AFDM.
Specifically, multiple constellation alphabets are selected for different
chirp-based subcarriers (chirps). Aside from classical amplitude/phase
modulation, additional information bits can be conveyed by the dynamic patterns
of both constellation mode selection and chirp activation, without extra energy
consumption. Furthermore, we discuss the mode selection strategy and derive an
asymptotically tight upper bound on the bit error rate (BER) of the proposed
scheme under maximum-likelihood detection. Simulation results are provided to
demonstrate the superior performance of MM-AFDM-IM compared to conventional
benchmark schemes.

</details>


### [7] [Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects](https://arxiv.org/abs/2507.13080)
*Morteza Alijani,Wout Joseph,David Plets*

Main category: eess.SP

TL;DR: 本文探讨了无调制可见光定位（uVLP）技术，作为传统VLP的低成本替代方案，利用现有照明基础设施实现高精度室内定位。


<details>
  <summary>Details</summary>
Motivation: 传统VLP技术因调制LED带来的高成本和操作复杂性限制了其广泛应用，uVLP通过利用未调制光源解决了这些问题。

Method: 论文分析了uVLP的基本原理，将其与传统VLP比较，并根据接收器技术分类为强度法和成像法，同时提出了一种分类法。

Result: uVLP提供了一种低成本、高效率的室内定位方案，但仍需解决技术和部署上的挑战。

Conclusion: uVLP是未来室内定位的有前景方向，需进一步研究以实现其广泛应用。

Abstract: Visible Light Positioning (VLP) has emerged as a promising technology for
next-generation indoor positioning systems (IPS), particularly within the scope
of sixth-generation (6G) wireless networks. Its attractiveness stems from
leveraging existing lighting infrastructures equipped with light-emitting
diodes (LEDs), enabling cost-efficient deployments and achieving high-precision
positioning accuracy in the centimeter-todecimeter range. However, widespread
adoption of traditional VLP solutions faces significant barriers due to the
increased costs and operational complexity associated with modulating LEDs,
which consequently reduces illumination efficiency by lowering their radiant
flux. To address these limitations, recent research has introduced the concept
of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals
of Opportunity (LSOOP) emitted by unmodulated illumination sources such as
conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure
alternative for indoor positioning by eliminating the need for modulation
hardware and maintaining lighting efficiency. This paper delineates the
fundamental principles of uVLP, provides a comparative analysis of uVLP versus
conventional VLP methods, and classifies existing uVLP techniques according to
receiver technologies into intensity-based methods (e.g., photodiodes, solar
cells, etc.) and imaging-based methods. Additionally, we propose a
comprehensive taxonomy categorizing techniques into demultiplexed and
undemultiplexed approaches. Within this structured framework, we critically
review current advancements in uVLP, discuss prevailing challenges, and outline
promising research directions essential for developing robust, scalable, and
widely deployable uVLP solutions.

</details>


### [8] [Angle Estimation of a Single Source with Massive Uniform Circular Arrays](https://arxiv.org/abs/2507.13086)
*Mingyan Gong*

Main category: eess.SP

TL;DR: 提出了一种基于均匀圆形阵列（UCA）的简单二维DOA估计方法，适用于实时信号处理，并能处理非均匀噪声。


<details>
  <summary>Details</summary>
Motivation: 均匀线性阵列只能估计源方位角，而UCA能提供360度方位角和额外仰角信息，因此需要一种简单高效的二维DOA估计方法。

Method: 通过量化方位角并计算协方差比较，获得方位角估计，再通过显式公式计算仰角估计。

Result: 数值结果表明，该方法能有效估计方位角和仰角，并可作为高精度多维搜索的起点，且适用于非均匀噪声环境。

Conclusion: 该方法计算简单，适用于实时处理，并能扩展至高精度搜索和非均匀噪声场景。

Abstract: Estimating the directions of arrival (DOAs) of incoming plane waves is an
essential topic in array signal processing. Widely adopted uniform linear
arrays can only provide estimates of source azimuth. Thus, uniform circular
arrays (UCAs) are attractive in that they can provide $360^{\circ}$ azimuthal
coverage and additional elevation angle information. Considering that with a
massive UCA, its polar angles of array sensors can approximately represent
azimuth angles over $360^{\circ}$ using angle quantization, a simple
two-dimensional DOA estimation method for a single source is proposed. In this
method, the quantized azimuth angle estimate is obtained by only calculating
and comparing a number of covariances, based on which the elevation angle
estimate is then obtained by an explicit formula. Thus, the proposed method is
computationally simple and suitable for real-time signal processing. Numerical
results verify that the proposed method can obtain azimuth as well as elevation
angle estimates and the estimates can be used as starting points of
multidimensional searches for methods with higher accuracy. Additionally, the
proposed method can still work in the presence of nonuniform noise.

</details>


### [9] [Multifrequency system model for multiport time-modulated scatterers](https://arxiv.org/abs/2507.13130)
*Aleksandr D. Kuznetsov,Jari Holopainen,Ville Viikari*

Main category: eess.SP

TL;DR: 本文提出了一种基于多端口S参数的多频散射模型，适用于非周期性和时变调制结构，扩展了传统S矩阵模型的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测多频和时变调制散射结构的性能，特别是在非周期性和非数字调制情况下。

Method: 扩展了多端口S参数模型，纳入结构散射、互耦、非数字调制和非周期性配置。

Result: 模型验证了其准确性和实用性，适用于广泛的通信和传感系统。

Conclusion: 该模型为多频和时变调制散射结构提供了精确的分析和优化工具。

Abstract: Utilizing scatterers in communication engineering, such as reconfigurable
intelligent surfaces (RISs) and backscatter systems, requires physically
consistent models for accurate performance prediction. A multiport model, which
also accounts for structural scattering, has been developed for non-periodic
scatterers. However, many emerging systems operate at multiple frequencies or
generate intermodulation harmonics, particularly when incorporating space-time
modulation (STM) or dynamic load control. These functionalities demand advanced
modeling approaches capable of capturing scattering behavior across several
frequencies and directions simultaneously. This article extends a multiport
S-parameters-based model for predicting the scattering properties of
multifrequency operating structures. The model extends the applicability of
convenient S-matrix models to time-modulated multiport structures. Unlike known
approaches, this model incorporates structural scattering, mutual coupling, the
possibility of non-digital modulation, and non-periodic configurations,
enabling precise analysis and optimization for a broad range of communication
and sensing systems. Validation against experimental results for a space-time
modulated scattering structure demonstrates the accuracy and practical
applicability of the proposed model.

</details>


### [10] [Disentangling coincident cell events using deep transfer learning and compressive sensing](https://arxiv.org/abs/2507.13176)
*Moritz Leuthner,Rafael Vorländer,Oliver Hayden*

Main category: eess.SP

TL;DR: 提出了一种结合全卷积神经网络（FCN）和压缩感知（CS）的混合框架，用于解决单细胞分析中信号重叠问题，显著提高了事件恢复率和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 单细胞分析在诊断和细胞治疗中至关重要，但信号重叠会严重影响数据准确性，需要一种高效的方法来解决这一问题。

Method: 使用FCN估计重叠事件数量，并结合CS模块重建单个信号成分，从而恢复单细胞特征。

Result: 相比传统算法，该方法恢复了21%更多事件，分类准确率超过97%，且具有临床可解释性。

Conclusion: 该框架为非光学单细胞传感平台奠定了基础，扩展了流式细胞术在转化医学中的应用。

Abstract: Accurate single-cell analysis is critical for diagnostics, immunomonitoring,
and cell therapy, but coincident events - where multiple cells overlap in a
sensing zone - can severely compromise signal fidelity. We present a hybrid
framework combining a fully convolutional neural network (FCN) with compressive
sensing (CS) to disentangle such overlapping events in one-dimensional sensor
data. The FCN, trained on bead-derived datasets, accurately estimates
coincident event counts and generalizes to immunomagnetically labeled CD4+ and
CD14+ cells in whole blood without retraining. Using this count, the CS module
reconstructs individual signal components with high fidelity, enabling precise
recovery of single-cell features, including velocity, amplitude, and
hydrodynamic diameter. Benchmarking against conventional state-machine
algorithms shows superior performance - recovering up to 21% more events and
improving classification accuracy beyond 97%. Explinability via class
activation maps and parameterized Gaussian template fitting ensures
transparency and clinical interpretability. Demonstrated with magnetic flow
cytometry (MFC), the framework is compatible with other waveform-generating
modalities, including impedance cytometry, nanopore, and resistive pulse
sensing. This work lays the foundation for next-generation non-optical
single-cell sensing platforms that are automated, generalizable, and capable of
resolving overlapping events, broadening the utility of cytometry in
translational medicine and precision diagnostics, e.g. cell-interaction
studies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models](https://arxiv.org/abs/2507.12595)
*Orchid Chetia Phukan,Mohd Mujtaba Akhtar,Girish,Arun Balaji Buduru*

Main category: eess.AS

TL;DR: 论文提出了一种多语言语音基础模型（SFMs）用于情感伪造检测（EFD），并通过THAMA方法融合模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究假设多语言SFMs能更好地理解语音中的音调、音高和强度变化，从而更有效地检测情感伪造。

Method: 通过比较SOTA SFMs，验证多语言SFMs的优势，并提出THAMA方法（结合Tucker分解和Hadamard乘积）进行模型融合。

Result: 多语言SFMs在域内和跨域评估中表现优异，THAMA进一步提升了性能，超越单一模型和基线方法。

Conclusion: 多语言SFMs与THAMA的结合在EFD任务中实现了最佳性能，验证了假设并提供了有效的解决方案。

Abstract: In this work, we address EmoFake Detection (EFD). We hypothesize that
multilingual speech foundation models (SFMs) will be particularly effective for
EFD due to their pre-training across diverse languages, enabling a nuanced
understanding of variations in pitch, tone, and intensity. To validate this, we
conduct a comprehensive comparative analysis of state-of-the-art (SOTA) SFMs.
Our results shows the superiority of multilingual SFMs for same language
(in-domain) as well as cross-lingual (out-domain) evaluation. To our end, we
also propose, THAMA for fusion of foundation models (FMs) motivated by related
research where combining FMs have shown improved performance. THAMA leverages
the complementary conjunction of tucker decomposition and hadamard product for
effective fusion. With THAMA, synergized with cooperative multilingual SFMs
achieves topmost performance across in-domain and out-domain settings,
outperforming individual FMs, baseline fusion techniques, and prior SOTA
methods.

</details>


### [12] [DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization](https://arxiv.org/abs/2507.12890)
*Huakang Chen,Yuepeng Jiang,Guobin Ma,Chunbo Hao,Shuai Wang,Jixun Yao,Ziqian Ning,Meng Meng,Jian Luan,Lei Xie*

Main category: eess.AS

TL;DR: DiffRhythm+是一个改进的扩散模型，用于可控且灵活的全长歌曲生成，解决了数据不平衡和可控性问题，提升了音乐质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前全长歌曲生成系统存在数据不平衡、可控性不足和音乐质量不一致的问题，DiffRhythm+旨在解决这些限制。

Method: DiffRhythm+采用扩展且平衡的训练数据集，引入多模态风格条件策略，并通过用户偏好优化性能。

Result: 实验表明，DiffRhythm+在自然度、编曲复杂度和听众满意度上显著优于先前系统。

Conclusion: DiffRhythm+通过改进数据集和增强可控性，显著提升了全长歌曲生成的质量和灵活性。

Abstract: Songs, as a central form of musical art, exemplify the richness of human
intelligence and creativity. While recent advances in generative modeling have
enabled notable progress in long-form song generation, current systems for
full-length song synthesis still face major challenges, including data
imbalance, insufficient controllability, and inconsistent musical quality.
DiffRhythm, a pioneering diffusion-based model, advanced the field by
generating full-length songs with expressive vocals and accompaniment. However,
its performance was constrained by an unbalanced model training dataset and
limited controllability over musical style, resulting in noticeable quality
disparities and restricted creative flexibility. To address these limitations,
we propose DiffRhythm+, an enhanced diffusion-based framework for controllable
and flexible full-length song generation. DiffRhythm+ leverages a substantially
expanded and balanced training dataset to mitigate issues such as repetition
and omission of lyrics, while also fostering the emergence of richer musical
skills and expressiveness. The framework introduces a multi-modal style
conditioning strategy, enabling users to precisely specify musical styles
through both descriptive text and reference audio, thereby significantly
enhancing creative control and diversity. We further introduce direct
performance optimization aligned with user preferences, guiding the model
toward consistently preferred outputs across evaluation metrics. Extensive
experiments demonstrate that DiffRhythm+ achieves significant improvements in
naturalness, arrangement complexity, and listener satisfaction over previous
systems.

</details>


### [13] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: UniSLU是一个统一框架，用于联合建模多个SLU任务，通过统一表示和生成方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖独立模型架构，增加了复杂性且未能充分利用跨任务数据。

Method: 提出统一表示和生成方法，联合建模ASR、spoken NER和SA任务。

Result: 在公开数据集上表现优于基准方法，适合实际应用。

Conclusion: UniSLU框架有效提升SLU性能，代码将开源以促进研究。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


### [14] [AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers with Multi-Scale and Multi-Task Learning](https://arxiv.org/abs/2507.12972)
*Daning Zhang,Ying Wei*

Main category: eess.AS

TL;DR: AVFSNet是一个音频-视觉语音分离模型，通过多尺度编码和并行架构联合优化，解决了混合信号中未知说话者数量的挑战，并提升了噪声适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设已知说话者数量，而针对未知数量的研究泛化能力有限。AVFSNet旨在解决这一问题。

Method: 结合多尺度编码和并行架构，独立并行分离每个说话者，并利用视觉信息增强噪声适应性。

Result: 在多个评估指标上达到最优性能，并在多样化数据集上表现优异。

Conclusion: AVFSNet在未知说话者数量和噪声适应性方面表现出色，具有广泛的应用潜力。

Abstract: Separating target speech from mixed signals containing flexible speaker
quantities presents a challenging task. While existing methods demonstrate
strong separation performance and noise robustness, they predominantly assume
prior knowledge of speaker counts in mixtures. The limited research addressing
unknown speaker quantity scenarios exhibits significantly constrained
generalization capabilities in real acoustic environments. To overcome these
challenges, this paper proposes AVFSNet -- an audio-visual speech separation
model integrating multi-scale encoding and parallel architecture -- jointly
optimized for speaker counting and multi-speaker separation tasks. The model
independently separates each speaker in parallel while enhancing environmental
noise adaptability through visual information integration. Comprehensive
experimental evaluations demonstrate that AVFSNet achieves state-of-the-art
results across multiple evaluation metrics and delivers outstanding performance
on diverse datasets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [15] [Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates](https://arxiv.org/abs/2507.12563)
*Carlos De La Vega Martin,Rodrigo Diaz Fernandez,Mark Sandler*

Main category: cs.SD

TL;DR: 本文比较分析了基于神经网络的方法来解决非线性弹性板振动问题，用于物理建模音频合成，评估了多个最新模型在自回归长序列预测中的表现和局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的有限差分和有限元数值方法虽然精度高但计算量大，限制了在实时音频应用中的使用，因此需要探索基于神经网络的替代方法来解决非线性弹性板振动问题。

Method: 对多个最新的神经网络模型进行比较分析，这些模型在短序列上训练，然后以自回归方式预测长序列，用于模拟非线性弹性板的振动行为。

Result: 研究揭示了这些神经网络模型的一些局限性，并指出仅仅关注时域预测误差是不够的，需要更全面的评估指标来衡量模型性能。

Conclusion: 当前的神经网络方法在模拟非线性振动方面存在局限性，作者讨论了这些发现对实时音频合成的影响，并提出了改进神经网络方法的未来研究方向。

Abstract: Physical modelling synthesis aims to generate audio from physical simulations
of vibrating structures. Thin elastic plates are a common model for drum
membranes. Traditional numerical methods like finite differences and finite
elements offer high accuracy but are computationally demanding, limiting their
use in real-time audio applications. This paper presents a comparative analysis
of neural network-based approaches for solving the vibration of nonlinear
elastic plates. We evaluate several state-of-the-art models, trained on short
sequences, for prediction of long sequences in an autoregressive fashion. We
show some of the limitations of these models, and why is not enough to look at
the prediction error in the time domain. We discuss the implications for
real-time audio synthesis and propose future directions for improving neural
approaches to model nonlinear vibration.

</details>


### [16] [Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine](https://arxiv.org/abs/2507.12701)
*Anastasia Kuznetsova,Inseon Jang,Wootaek Lim,Minje Kim*

Main category: cs.SD

TL;DR: 提出了一种高效的音频编码方法（ACoM），专注于机器任务而非人类感知，通过残差向量量化（RVQ）实现超低比特率压缩。


<details>
  <summary>Details</summary>
Motivation: 传统音频编码注重高保真重建，而ACoM更关注压缩效率和下游任务性能，忽略感知细节。

Method: 利用任务特定损失和RVQ损失，压缩和量化已训练模型的中间特征表示，支持灵活比特率和模型大小。

Result: 在自动语音识别和音频分类任务中表现优异，比特率低于200bps且性能损失极小。

Conclusion: 该方法高效且灵活，适用于多种任务和架构，具有广泛的应用潜力。

Abstract: Neural audio codecs, leveraging quantization algorithms, have significantly
impacted various speech/audio tasks. While high-fidelity reconstruction is
paramount for human perception, audio coding for machines (ACoM) prioritizes
efficient compression and downstream task performance, disregarding perceptual
nuances. This work introduces an efficient ACoM method that can compress and
quantize any chosen intermediate feature representation of an already trained
speech/audio downstream model. Our approach employs task-specific loss guidance
alongside residual vector quantization (RVQ) losses, providing ultra-low
bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model
performance. The resulting tokenizer is adaptable to various bitrates and model
sizes for flexible deployment. Evaluated on automatic speech recognition and
audio classification, our method demonstrates its efficacy and potential for
broader task and architectural applicability through appropriate
regularization.

</details>


### [17] [Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries](https://arxiv.org/abs/2507.12723)
*Minyoung Kim,Sehwan Park,Sungmin Cha,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 论文提出了一种跨模态水印框架，用于从合成视听伪造中恢复真实音频并定位篡改，以对抗虚假信息。


<details>
  <summary>Details</summary>
Motivation: 合成视听伪造（SAVF）技术使虚假内容更逼真，增加了错误信息的风险。现有方法无法恢复真实音频，限制了其对抗虚假信息的效果。

Method: 提出了一种跨模态水印框架，将真实音频嵌入到视觉内容中，以便在篡改后恢复真实音频并定位篡改。

Result: 实验表明，该方法在恢复真实音频和定位篡改方面表现优异，能有效对抗语音克隆和唇同步等篡改。

Conclusion: 该框架为对抗合成视听伪造提供了一种有效的解决方案，兼具恢复真实音频和定位篡改的能力。

Abstract: Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.

</details>


### [18] [Sample-Constrained Black Box Optimization for Audio Personalization](https://arxiv.org/abs/2507.12773)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: 论文提出了一种混合查询方法，结合稀疏高斯过程回归（GPR），通过用户对音频过滤器的评分和直接反馈优化个性化音频体验。


<details>
  <summary>Details</summary>
Motivation: 解决个性化音频优化问题，通过用户反馈最大化满意度，探索混合查询方法的潜力。

Method: 采用稀疏高斯过程回归（GPR），结合用户对音频样本的评分和对过滤器元素的直接反馈，设计混合查询策略。

Result: 实验验证了混合查询方法的有效性，用户满意度显著提升。

Conclusion: 混合查询方法为黑盒优化问题提供了新思路，可推广至其他应用领域。

Abstract: We consider the problem of personalizing audio to maximize user experience.
Briefly, we aim to find a filter $h^*$, which applied to any music or speech,
will maximize the user's satisfaction. This is a black-box optimization problem
since the user's satisfaction function is unknown. Substantive work has been
done on this topic where the key idea is to play audio samples to the user,
each shaped by a different filter $h_i$, and query the user for their
satisfaction scores $f(h_i)$. A family of ``surrogate" functions is then
designed to fit these scores and the optimization method gradually refines
these functions to arrive at the filter $\hat{h}^*$ that maximizes
satisfaction. In certain applications, we observe that a second type of
querying is possible where users can tell us the individual elements $h^*[j]$
of the optimal filter $h^*$. Consider an analogy from cooking where the goal is
to cook a recipe that maximizes user satisfaction. A user can be asked to score
various cooked recipes (e.g., tofu fried rice) or to score individual
ingredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$
queries, where a query can be of either type, our goal is to find the recipe
that will maximize this user's satisfaction. Our proposal builds on Sparse
Gaussian Process Regression (GPR) and shows how a hybrid approach can
outperform any one type of querying. Our results are validated through
simulations and real world experiments, where volunteers gave feedback on
music/speech audio and were able to achieve high satisfaction levels. We
believe this idea of hybrid querying opens new problems in black-box
optimization and solutions can benefit other applications beyond audio
personalization.

</details>


### [19] [Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features](https://arxiv.org/abs/2507.12793)
*J. M. Chan Sri Manukalpa,H. S. Bopage,W. A. M. Jayawardena,P. K. P. G. Panduwawala*

Main category: cs.SD

TL;DR: 提出了一种基于深度学习的非侵入式声学分类框架，用于早期白蚁检测，模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统白蚁检测方法侵入性强且效率低，需非侵入式高效解决方案。

Method: 采用混合CNN-LSTM架构，提取MFCC特征，分类白蚁声学信号。

Result: 模型准确率94.5%，精确率93.2%，召回率95.8%，优于单独CNN或LSTM。

Conclusion: 该研究为非侵入式白蚁检测提供了自动化方案，未来可结合IoT扩展应用。

Abstract: Structural pests, such as termites, pose a serious threat to wooden
buildings, resulting in significant economic losses due to their hidden and
progressive damage. Traditional detection methods, such as visual inspections
and chemical treatments, are invasive, labor intensive, and ineffective for
early stage infestations. To bridge this gap, this study proposes a non
invasive deep learning based acoustic classification framework for early
termite detection. We aim to develop a robust, scalable model that
distinguishes termite generated acoustic signals from background noise. We
introduce a hybrid Convolutional Neural Network Long Short Term Memory
architecture that captures both spatial and temporal features of termite
activity. Audio data were collected from termite infested and clean wooden
samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN
LSTM model to classify the signals. Experimental results show high performance,
with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis
reveals that the hybrid model outperforms standalone CNN and LSTM
architectures, underscoring its combined strength. Notably, the model yields
low false-negative rates, which is essential for enabling timely intervention.
This research contributes a non invasive, automated solution for early termite
detection, with practical implications for improved pest monitoring, minimized
structural damage, and better decision making by homeowners and pest control
professionals. Future work may integrate IoT for real time alerts and extend
detection to other structural pests.

</details>


### [20] [Autoregressive Speech Enhancement via Acoustic Tokens](https://arxiv.org/abs/2507.12825)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.SD

TL;DR: 本文研究了语音增强中离散表示（特别是声学标记）的性能，提出了一种新的自回归架构，并比较了其与语义标记和连续表示的效果。


<details>
  <summary>Details</summary>
Motivation: 语音增强中离散表示的研究有限，现有方法多关注语义标记而忽略声学细节，且缺乏自回归建模的探索。

Method: 1）研究声学标记的性能（比特率和噪声强度的影响）；2）提出一种基于转换器的自回归架构。

Result: 声学标记在保留说话人身份上优于语义标记，自回归方法进一步提升了性能，但仍不及连续表示。

Conclusion: 离散表示在语音增强中仍有不足，需进一步研究。

Abstract: In speech processing pipelines, improving the quality and intelligibility of
real-world recordings is crucial. While supervised regression is the primary
method for speech enhancement, audio tokenization is emerging as a promising
alternative for a smooth integration with other modalities. However, research
on speech enhancement using discrete representations is still limited. Previous
work has mainly focused on semantic tokens, which tend to discard key acoustic
details such as speaker identity. Additionally, these studies typically employ
non-autoregressive models, assuming conditional independence of outputs and
overlooking the potential improvements offered by autoregressive modeling. To
address these gaps we: 1) conduct a comprehensive study of the performance of
acoustic tokens for speech enhancement, including the effect of bitrate and
noise strength; 2) introduce a novel transducer-based autoregressive
architecture specifically designed for this task. Experiments on VoiceBank and
Libri1Mix datasets show that acoustic tokens outperform semantic tokens in
terms of preserving speaker identity, and that our autoregressive approach can
further improve performance. Nevertheless, we observe that discrete
representations still fall short compared to continuous ones, highlighting the
need for further research in this area.

</details>


### [21] [Best Practices and Considerations for Child Speech Corpus Collection and Curation in Educational, Clinical, and Forensic Scenarios](https://arxiv.org/abs/2507.12870)
*John Hansen,Satwik Dutta,Ellen Grand*

Main category: cs.SD

TL;DR: 研究探讨儿童语音数据收集的最佳实践，涵盖数据收集的“谁、什么、何时、何地”以及协作、信任建立和质量检查指南。


<details>
  <summary>Details</summary>
Motivation: 儿童语音能力动态变化且涉及隐私问题，导致构建技术适用的语音语料库具有挑战性。研究旨在填补这一空白，为跨领域应用提供指导。

Method: 基于先前收集经验和知识，描述数据收集的“谁、什么、何时、何地”，并提供协作、信任建立及研究协议导航指南。

Result: 提出了儿童语音语料库开发的最佳实践，包括数据收集、协作和质量检查的具体步骤。

Conclusion: 研究为儿童语音数据收集提供了全面指南，支持教育、临床和法医学等领域的应用。

Abstract: A child's spoken ability continues to change until their adult age. Until
7-8yrs, their speech sound development and language structure evolve rapidly.
This dynamic shift in their spoken communication skills and data privacy make
it challenging to curate technology-ready speech corpora for children. This
study aims to bridge this gap and provide researchers and practitioners with
the best practices and considerations for developing such a corpus based on an
intended goal. Although primarily focused on educational goals, applications of
child speech data have spread across fields including clinical and forensics
fields. Motivated by this goal, we describe the WHO, WHAT, WHEN, and WHERE of
data collection inspired by prior collection efforts and our
experience/knowledge. We also provide a guide to establish collaboration,
trust, and for navigating the human subjects research protocol. This study
concludes with guidelines for corpus quality check, triage, and annotation.

</details>


### [22] [Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes](https://arxiv.org/abs/2507.12932)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.SD

TL;DR: Enkidu是一种新型用户导向的隐私保护框架，通过黑盒知识和少量用户数据训练生成通用频域扰动，有效防御语音深度伪造攻击，同时保持高效率和语音质量。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造技术的快速发展威胁用户音频隐私，现有防御方法存在适应性差、计算成本高等问题。

Method: 利用黑盒知识和少量用户数据生成频域噪声补丁，实现实时轻量级保护。

Result: Enkidu在内存和运行时效率上显著优于现有方法，并在实验中证明了对多种攻击的有效防御。

Conclusion: Enkidu提供了一种高效、可扩展且实用的解决方案，适用于防御语音深度伪造攻击。

Abstract: The rapid advancement of voice deepfake technologies has raised serious
concerns about user audio privacy, as attackers increasingly exploit publicly
available voice data to generate convincing fake audio for malicious purposes
such as identity theft, financial fraud, and misinformation campaigns. While
existing defense methods offer partial protection, they face critical
limitations, including weak adaptability to unseen user data, poor scalability
to long audio, rigid reliance on white-box knowledge, and high computational
and temporal costs during the encryption process. To address these challenges
and defend against personalized voice deepfake threats, we propose Enkidu, a
novel user-oriented privacy-preserving framework that leverages universal
frequential perturbations generated through black-box knowledge and few-shot
training on a small amount of user data. These highly malleable
frequency-domain noise patches enable real-time, lightweight protection with
strong generalization across variable-length audio and robust resistance to
voice deepfake attacks, all while preserving perceptual quality and speech
intelligibility. Notably, Enkidu achieves over 50 to 200 times processing
memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime
efficiency (real-time coefficient as low as 0.004) compared to six
state-of-the-art countermeasures. Extensive experiments across six mainstream
text-to-speech models and five cutting-edge automated speaker verification
models demonstrate the effectiveness, transferability, and practicality of
Enkidu in defending against both vanilla and adaptive voice deepfake attacks.

</details>


### [23] [Multi-Class-Token Transformer for Multitask Self-supervised Music Information Retrieval](https://arxiv.org/abs/2507.12996)
*Yuexuan Kong,Vincent Lostanlen,Romain Hennequin,Mathieu Lagrange,Gabriel Meseguer-Brocal*

Main category: cs.SD

TL;DR: 本文提出了一种结合对比学习和等变学习的自监督多任务学习方法（MT2），通过双类别令牌的ViT-1D架构，在音乐信息检索任务中表现优于单一任务模型。


<details>
  <summary>Details</summary>
Motivation: 对比学习和等变学习在音乐信息检索中各有所长，但单独使用时存在局限性。本文旨在结合两者的优势，提出一种更通用的自监督学习方法。

Method: 采用双类别令牌的ViT-1D架构，分别优化对比学习（NT-Xent）和等变学习（CPSD）任务，并通过多任务学习结合两者的优势。

Result: MT2在多个任务上优于单一任务模型，且通过平均双类别令牌进一步提升了性能。其参数效率高，仅需18分之一参数即可在多数任务上超越MERT。

Conclusion: MT2展示了多类别令牌多任务学习在音乐信息检索中的通用性和高效性，为自监督学习提供了新思路。

Abstract: Contrastive learning and equivariant learning are effective methods for
self-supervised learning (SSL) for audio content analysis. Yet, their
application to music information retrieval (MIR) faces a dilemma: the former is
more effective on tagging (e.g., instrument recognition) but less effective on
structured prediction (e.g., tonality estimation); The latter can match
supervised methods on the specific task it is designed for, but it does not
generalize well to other tasks. In this article, we adopt a best-of-both-worlds
approach by training a deep neural network on both kinds of pretext tasks at
once. The proposed new architecture is a Vision Transformer with 1-D
spectrogram patches (ViT-1D), equipped with two class tokens, which are
specialized to different self-supervised pretext tasks but optimized through
the same model: hence the qualification of self-supervised multi-class-token
multitask (MT2). The former class token optimizes cross-power spectral density
(CPSD) for equivariant learning over the circle of fifths, while the latter
optimizes normalized temperature-scaled cross-entropy (NT-Xent) for contrastive
learning. MT2 combines the strengths of both pretext tasks and outperforms
consistently both single-class-token ViT-1D models trained with either
contrastive or equivariant learning. Averaging the two class tokens further
improves performance on several tasks, highlighting the complementary nature of
the representations learned by each class token. Furthermore, using the same
single-linear-layer probing method on the features of last layer, MT2
outperforms MERT on all tasks except for beat tracking; achieving this with 18x
fewer parameters thanks to its multitasking capabilities. Our SSL benchmark
demonstrates the versatility of our multi-class-token multitask learning
approach for MIR applications.

</details>


### [24] [SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks](https://arxiv.org/abs/2507.13170)
*Kutub Uddin,Awais Khan,Muhammad Umar Farooq,Khalid Malik*

Main category: cs.SD

TL;DR: 论文提出了一种名为SHIELD的协作学习方法，用于防御生成式反取证（AF）攻击，通过集成辅助生成模型和三重模型，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造（如deepfake）的传播对信息安全构成威胁，现有检测方法易受生成式AF攻击影响，因此需要更鲁棒的防御机制。

Method: 提出SHIELD方法，结合辅助生成模型（DF生成模型）和三重模型，通过协作学习捕捉真实音频与AF攻击音频的特征关联。

Result: SHIELD在ASVspoof2019、In-the-Wild和HalfTruth数据集上显著提升了检测准确率，分别达到98.13%、98.58%和99.57%（匹配场景）。

Conclusion: SHIELD能有效抵御生成式AF攻击，为音频深度伪造检测提供了鲁棒的解决方案。

Abstract: Audio plays a crucial role in applications like speaker verification,
voice-enabled smart devices, and audio conferencing. However, audio
manipulations, such as deepfakes, pose significant risks by enabling the spread
of misinformation. Our empirical analysis reveals that existing methods for
detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,
particularly those attacked using generative adversarial networks. In this
article, we propose a novel collaborative learning method called SHIELD to
defend against generative AF attacks. To expose AF signatures, we integrate an
auxiliary generative model, called the defense (DF) generative model, which
facilitates collaborative learning by combining input and output. Furthermore,
we design a triplet model to capture correlations for real and AF attacked
audios with real-generated and attacked-generated audios using auxiliary
generative models. The proposed SHIELD strengthens the defense against
generative AF attacks and achieves robust performance across various generative
models. The proposed AF significantly reduces the average detection accuracy
from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,
and from 98.41% to 51.18% for HalfTruth for three different generative models.
The proposed SHIELD mechanism is robust against AF attacks and achieves an
average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,
and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and
HalfTruth datasets, respectively.

</details>


### [25] [Voxtral](https://arxiv.org/abs/2507.13264)
*Alexander H. Liu,Andy Ehrenberg,Andy Lo,Clément Denoix,Corentin Barreau,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Sanchit Gandhi,Soham Ghosh,Srijan Mishra,Thomas Foubert,Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devendra Singh Chaplot,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Hadrien Chabran,Jessica Chudnovsky,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Romain Sauvestre,Roman Soletskyi,Sagar Vaze,Sandeep Subramanian,Saurabh Garg,Shashwat Dalal,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SD

TL;DR: Voxtral Mini和Voxtral Small是多模态音频聊天模型，支持语音和文本输入，性能优异且可本地运行。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时理解语音和文本的高效多模态模型，并在音频任务中实现领先性能。

Method: 训练Voxtral模型以处理语音和文本，支持32K上下文窗口，适用于长音频和多轮对话。

Result: Voxtral Small性能优于多个闭源模型，支持40分钟音频处理，并发布了三个语音理解评测基准。

Conclusion: Voxtral模型在多模态任务中表现出色，开源且适用于本地部署。

Abstract: We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.
Voxtral is trained to comprehend both spoken audio and text documents,
achieving state-of-the-art performance across a diverse range of audio
benchmarks, while preserving strong text capabilities. Voxtral Small
outperforms a number of closed-source models, while being small enough to run
locally. A 32K context window enables the model to handle audio files up to 40
minutes in duration and long multi-turn conversations. We also contribute three
benchmarks for evaluating speech understanding models on knowledge and trivia.
Both Voxtral models are released under Apache 2.0 license.

</details>

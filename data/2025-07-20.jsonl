{"id": "2507.12563", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12563", "abs": "https://arxiv.org/abs/2507.12563", "authors": ["Carlos De La Vega Martin", "Rodrigo Diaz Fernandez", "Mark Sandler"], "title": "Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates", "comment": null, "summary": "Physical modelling synthesis aims to generate audio from physical simulations\nof vibrating structures. Thin elastic plates are a common model for drum\nmembranes. Traditional numerical methods like finite differences and finite\nelements offer high accuracy but are computationally demanding, limiting their\nuse in real-time audio applications. This paper presents a comparative analysis\nof neural network-based approaches for solving the vibration of nonlinear\nelastic plates. We evaluate several state-of-the-art models, trained on short\nsequences, for prediction of long sequences in an autoregressive fashion. We\nshow some of the limitations of these models, and why is not enough to look at\nthe prediction error in the time domain. We discuss the implications for\nreal-time audio synthesis and propose future directions for improving neural\napproaches to model nonlinear vibration."}
{"id": "2507.12701", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12701", "abs": "https://arxiv.org/abs/2507.12701", "authors": ["Anastasia Kuznetsova", "Inseon Jang", "Wootaek Lim", "Minje Kim"], "title": "Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine", "comment": null, "summary": "Neural audio codecs, leveraging quantization algorithms, have significantly\nimpacted various speech/audio tasks. While high-fidelity reconstruction is\nparamount for human perception, audio coding for machines (ACoM) prioritizes\nefficient compression and downstream task performance, disregarding perceptual\nnuances. This work introduces an efficient ACoM method that can compress and\nquantize any chosen intermediate feature representation of an already trained\nspeech/audio downstream model. Our approach employs task-specific loss guidance\nalongside residual vector quantization (RVQ) losses, providing ultra-low\nbitrates (i.e., less than 200 bps) with a minimal loss of the downstream model\nperformance. The resulting tokenizer is adaptable to various bitrates and model\nsizes for flexible deployment. Evaluated on automatic speech recognition and\naudio classification, our method demonstrates its efficacy and potential for\nbroader task and architectural applicability through appropriate\nregularization."}
{"id": "2507.12723", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12723", "abs": "https://arxiv.org/abs/2507.12723", "authors": ["Minyoung Kim", "Sehwan Park", "Sungmin Cha", "Paul Hongsuck Seo"], "title": "Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries", "comment": "5 pages, 2 figures, Interspeech 2025", "summary": "Recent advances in voice cloning and lip synchronization models have enabled\nSynthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are\nmanipulated to mimic a target speaker. This significantly increases the risk of\nmisinformation by making fake content seem real. To address this issue,\nexisting methods detect or localize manipulations but cannot recover the\nauthentic audio that conveys the semantic content of the message. This\nlimitation reduces their effectiveness in combating audiovisual misinformation.\nIn this work, we introduce the task of Authentic Audio Recovery (AAR) and\nTamper Localization in Audio (TLA) from SAVFs and propose a cross-modal\nwatermarking framework to embed authentic audio into visuals before\nmanipulation. This enables AAR, TLA, and a robust defense against\nmisinformation. Extensive experiments demonstrate the strong performance of our\nmethod in AAR and TLA against various manipulations, including voice cloning\nand lip synchronization."}
{"id": "2507.12773", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12773", "abs": "https://arxiv.org/abs/2507.12773", "authors": ["Rajalaxmi Rajagopalan", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Sample-Constrained Black Box Optimization for Audio Personalization", "comment": "Published in AAAI 2024", "summary": "We consider the problem of personalizing audio to maximize user experience.\nBriefly, we aim to find a filter $h^*$, which applied to any music or speech,\nwill maximize the user's satisfaction. This is a black-box optimization problem\nsince the user's satisfaction function is unknown. Substantive work has been\ndone on this topic where the key idea is to play audio samples to the user,\neach shaped by a different filter $h_i$, and query the user for their\nsatisfaction scores $f(h_i)$. A family of ``surrogate\" functions is then\ndesigned to fit these scores and the optimization method gradually refines\nthese functions to arrive at the filter $\\hat{h}^*$ that maximizes\nsatisfaction. In certain applications, we observe that a second type of\nquerying is possible where users can tell us the individual elements $h^*[j]$\nof the optimal filter $h^*$. Consider an analogy from cooking where the goal is\nto cook a recipe that maximizes user satisfaction. A user can be asked to score\nvarious cooked recipes (e.g., tofu fried rice) or to score individual\ningredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$\nqueries, where a query can be of either type, our goal is to find the recipe\nthat will maximize this user's satisfaction. Our proposal builds on Sparse\nGaussian Process Regression (GPR) and shows how a hybrid approach can\noutperform any one type of querying. Our results are validated through\nsimulations and real world experiments, where volunteers gave feedback on\nmusic/speech audio and were able to achieve high satisfaction levels. We\nbelieve this idea of hybrid querying opens new problems in black-box\noptimization and solutions can benefit other applications beyond audio\npersonalization."}
{"id": "2507.12593", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.12593", "abs": "https://arxiv.org/abs/2507.12593", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Robert Calderbank"], "title": "Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS", "comment": "6 pages, 4 figures, submitted to IEEE Wireless Communications Letters\n  for possible publication. Copyright maybe transferred without notice", "summary": "Zak-transform based orthogonal time frequency space (Zak-OTFS) is a\ndelay-Doppler (DD) domain modulation scheme in which the signal processing is\ncarried out in the DD domain. The channel when viewed in the DD domain is\npredictable. However, even with Zak-OTFS, pilots need to be sent periodically,\nalbeit at a lower rate. In this paper, we propose a differential communication\nscheme for Zak-OTFS systems that alleviates the need for periodic pilot\ntransmission. Towards this, we analytically show that the detected data can be\nused as a pilot and that the channel estimate obtained from the detected data\ncan enable further detection enabling the \"differential\" aspect of the\ncommunication. Specifically, we leverage the prediction capability of the DD\nchannel in Zak-OTFS to use the channel estimate (obtained from detected data\nsymbols treated as pilots) in the previous instant to detect data in the next\ninstant and propagate this forward. The advantages are two fold. First, it\nallows the data symbols to enjoy higher energy since the energy that would\notherwise be required for pilot symbols can also be allocated to data symbols.\nSecond, it allows for full spectral efficiency compared to point or embedded\npilots. Comparison with the full spectral efficiency achieving spread pilot\nscheme shows that the proposed method achieves better bit-error rate at lower\ncomplexity."}
{"id": "2507.12595", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12595", "abs": "https://arxiv.org/abs/2507.12595", "authors": ["Orchid Chetia Phukan", "Mohd Mujtaba Akhtar", "Girish", "Arun Balaji Buduru"], "title": "Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models", "comment": null, "summary": "In this work, we address EmoFake Detection (EFD). We hypothesize that\nmultilingual speech foundation models (SFMs) will be particularly effective for\nEFD due to their pre-training across diverse languages, enabling a nuanced\nunderstanding of variations in pitch, tone, and intensity. To validate this, we\nconduct a comprehensive comparative analysis of state-of-the-art (SOTA) SFMs.\nOur results shows the superiority of multilingual SFMs for same language\n(in-domain) as well as cross-lingual (out-domain) evaluation. To our end, we\nalso propose, THAMA for fusion of foundation models (FMs) motivated by related\nresearch where combining FMs have shown improved performance. THAMA leverages\nthe complementary conjunction of tucker decomposition and hadamard product for\neffective fusion. With THAMA, synergized with cooperative multilingual SFMs\nachieves topmost performance across in-domain and out-domain settings,\noutperforming individual FMs, baseline fusion techniques, and prior SOTA\nmethods."}
{"id": "2507.12793", "categories": ["cs.SD", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12793", "abs": "https://arxiv.org/abs/2507.12793", "authors": ["J. M. Chan Sri Manukalpa", "H. S. Bopage", "W. A. M. Jayawardena", "P. K. P. G. Panduwawala"], "title": "Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features", "comment": "This is a preprint article", "summary": "Structural pests, such as termites, pose a serious threat to wooden\nbuildings, resulting in significant economic losses due to their hidden and\nprogressive damage. Traditional detection methods, such as visual inspections\nand chemical treatments, are invasive, labor intensive, and ineffective for\nearly stage infestations. To bridge this gap, this study proposes a non\ninvasive deep learning based acoustic classification framework for early\ntermite detection. We aim to develop a robust, scalable model that\ndistinguishes termite generated acoustic signals from background noise. We\nintroduce a hybrid Convolutional Neural Network Long Short Term Memory\narchitecture that captures both spatial and temporal features of termite\nactivity. Audio data were collected from termite infested and clean wooden\nsamples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN\nLSTM model to classify the signals. Experimental results show high performance,\nwith 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis\nreveals that the hybrid model outperforms standalone CNN and LSTM\narchitectures, underscoring its combined strength. Notably, the model yields\nlow false-negative rates, which is essential for enabling timely intervention.\nThis research contributes a non invasive, automated solution for early termite\ndetection, with practical implications for improved pest monitoring, minimized\nstructural damage, and better decision making by homeowners and pest control\nprofessionals. Future work may integrate IoT for real time alerts and extend\ndetection to other structural pests."}
{"id": "2507.12630", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12630", "abs": "https://arxiv.org/abs/2507.12630", "authors": ["Dianxin Luan", "John Thompson"], "title": "Achieving Robust Channel Estimation Neural Networks by Designed Training Data", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking (TCCN)", "summary": "Channel estimation is crucial in cognitive communications, as it enables\nintelligent spectrum sensing and adaptive transmission by providing accurate\ninformation about the current channel state. However, in many papers neural\nnetworks are frequently tested by training and testing on one example channel\nor similar channels. This is because data-driven methods often degrade on new\ndata which they are not trained on, as they cannot extrapolate their training\nknowledge. This is despite the fact physical channels are often assumed to be\ntime-variant. However, due to the low latency requirements and limited\ncomputing resources, neural networks may not have enough time and computing\nresources to execute online training to fine-tune the parameters. This\nmotivates us to design offline-trained neural networks that can perform\nrobustly over wireless channels, but without any actual channel information\nbeing known at design time. In this paper, we propose design criteria to\ngenerate synthetic training datasets for neural networks, which guarantee that\nafter training the resulting networks achieve a certain mean squared error\n(MSE) on new and previously unseen channels. Therefore, neural network\nsolutions require no prior channel information or parameters update for\nreal-world implementations. Based on the proposed design criteria, we further\npropose a benchmark design which ensures intelligent operation for different\nchannel profiles. To demonstrate general applicability, we use neural networks\nwith different levels of complexity to show that the generalization achieved\nappears to be independent of neural network architecture. From simulations,\nneural networks achieve robust generalization to wireless channels with both\nfixed channel profiles and variable delay spreads."}
{"id": "2507.12890", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12890", "abs": "https://arxiv.org/abs/2507.12890", "authors": ["Huakang Chen", "Yuepeng Jiang", "Guobin Ma", "Chunbo Hao", "Shuai Wang", "Jixun Yao", "Ziqian Ning", "Meng Meng", "Jian Luan", "Lei Xie"], "title": "DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization", "comment": null, "summary": "Songs, as a central form of musical art, exemplify the richness of human\nintelligence and creativity. While recent advances in generative modeling have\nenabled notable progress in long-form song generation, current systems for\nfull-length song synthesis still face major challenges, including data\nimbalance, insufficient controllability, and inconsistent musical quality.\nDiffRhythm, a pioneering diffusion-based model, advanced the field by\ngenerating full-length songs with expressive vocals and accompaniment. However,\nits performance was constrained by an unbalanced model training dataset and\nlimited controllability over musical style, resulting in noticeable quality\ndisparities and restricted creative flexibility. To address these limitations,\nwe propose DiffRhythm+, an enhanced diffusion-based framework for controllable\nand flexible full-length song generation. DiffRhythm+ leverages a substantially\nexpanded and balanced training dataset to mitigate issues such as repetition\nand omission of lyrics, while also fostering the emergence of richer musical\nskills and expressiveness. The framework introduces a multi-modal style\nconditioning strategy, enabling users to precisely specify musical styles\nthrough both descriptive text and reference audio, thereby significantly\nenhancing creative control and diversity. We further introduce direct\nperformance optimization aligned with user preferences, guiding the model\ntoward consistently preferred outputs across evaluation metrics. Extensive\nexperiments demonstrate that DiffRhythm+ achieves significant improvements in\nnaturalness, arrangement complexity, and listener satisfaction over previous\nsystems."}
{"id": "2507.12825", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12825", "abs": "https://arxiv.org/abs/2507.12825", "authors": ["Luca Della Libera", "Cem Subakan", "Mirco Ravanelli"], "title": "Autoregressive Speech Enhancement via Acoustic Tokens", "comment": "5 pages, 2 figures", "summary": "In speech processing pipelines, improving the quality and intelligibility of\nreal-world recordings is crucial. While supervised regression is the primary\nmethod for speech enhancement, audio tokenization is emerging as a promising\nalternative for a smooth integration with other modalities. However, research\non speech enhancement using discrete representations is still limited. Previous\nwork has mainly focused on semantic tokens, which tend to discard key acoustic\ndetails such as speaker identity. Additionally, these studies typically employ\nnon-autoregressive models, assuming conditional independence of outputs and\noverlooking the potential improvements offered by autoregressive modeling. To\naddress these gaps we: 1) conduct a comprehensive study of the performance of\nacoustic tokens for speech enhancement, including the effect of bitrate and\nnoise strength; 2) introduce a novel transducer-based autoregressive\narchitecture specifically designed for this task. Experiments on VoiceBank and\nLibri1Mix datasets show that acoustic tokens outperform semantic tokens in\nterms of preserving speaker identity, and that our autoregressive approach can\nfurther improve performance. Nevertheless, we observe that discrete\nrepresentations still fall short compared to continuous ones, highlighting the\nneed for further research in this area."}
{"id": "2507.12645", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12645", "abs": "https://arxiv.org/abs/2507.12645", "authors": ["Mohammed Guhdar", "Ramadhan J. Mstafa", "Abdulhakeem O. Mohammed"], "title": "A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis", "comment": null, "summary": "The increasing need for accurate and unified analysis of diverse biological\nsignals, such as ECG and EEG, is paramount for comprehensive patient\nassessment, especially in synchronous monitoring. Despite advances in\nmulti-sensor fusion, a critical gap remains in developing unified architectures\nthat effectively process and extract features from fundamentally different\nphysiological signals. Another challenge is the inherent class imbalance in\nmany biomedical datasets, often causing biased performance in traditional\nmethods. This study addresses these issues by proposing a novel and unified\ndeep learning framework that achieves state-of-the-art performance across\ndifferent signal types. Our method integrates a ResNet-based CNN with an\nattention mechanism, enhanced by a novel data augmentation strategy:\ntime-domain concatenation of multiple augmented variants of each signal to\ngenerate richer representations. Unlike prior work, we scientifically increase\nsignal complexity to achieve future-reaching capabilities, which resulted in\nthe best predictions compared to the state of the art. Preprocessing steps\nincluded wavelet denoising, baseline removal, and standardization. Class\nimbalance was effectively managed through the combined use of this advanced\ndata augmentation and the Focal Loss function. Regularization techniques were\napplied during training to ensure generalization. We rigorously evaluated the\nproposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH\nArrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,\nand 100%, respectively, demonstrating robustness across diverse signal types\nand clinical contexts. Finally, the architecture requires ~130 MB of memory and\nprocesses each sample in ~10 ms, suggesting suitability for deployment on\nlow-end or wearable devices."}
{"id": "2507.12951", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12951", "abs": "https://arxiv.org/abs/2507.12951", "authors": ["Zhichao Sheng", "Shilin Zhou", "Chen Gong", "Zhenghua Li"], "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets", "comment": "13 pages, 3 figures", "summary": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research."}
{"id": "2507.12870", "categories": ["cs.SD", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12870", "abs": "https://arxiv.org/abs/2507.12870", "authors": ["John Hansen", "Satwik Dutta", "Ellen Grand"], "title": "Best Practices and Considerations for Child Speech Corpus Collection and Curation in Educational, Clinical, and Forensic Scenarios", "comment": "5 pages, 0 figures, accepted at the 10th Workshop on Speech and\n  Language Technology in Education (SLaTE 2025), a Satellite Workshop of the\n  2025 Interspeech Conference", "summary": "A child's spoken ability continues to change until their adult age. Until\n7-8yrs, their speech sound development and language structure evolve rapidly.\nThis dynamic shift in their spoken communication skills and data privacy make\nit challenging to curate technology-ready speech corpora for children. This\nstudy aims to bridge this gap and provide researchers and practitioners with\nthe best practices and considerations for developing such a corpus based on an\nintended goal. Although primarily focused on educational goals, applications of\nchild speech data have spread across fields including clinical and forensics\nfields. Motivated by this goal, we describe the WHO, WHAT, WHEN, and WHERE of\ndata collection inspired by prior collection efforts and our\nexperience/knowledge. We also provide a guide to establish collaboration,\ntrust, and for navigating the human subjects research protocol. This study\nconcludes with guidelines for corpus quality check, triage, and annotation."}
{"id": "2507.12706", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12706", "abs": "https://arxiv.org/abs/2507.12706", "authors": ["Sanghyun Kim", "Jiwon Seo"], "title": "Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers", "comment": "Submitted to IEEE ITSC 2025", "summary": "In urban environments, global navigation satellite system (GNSS) positioning\nis often compromised by signal blockages and multipath effects caused by\nbuildings, leading to significant positioning errors. To address this issue,\nthis study proposes a robust enhancement of zonotope shadow matching\n(ZSM)-based positioning by employing a conservative satellite selection\nstrategy using unanimous voting across multiple machine learning classifiers.\nThree distinct models - random forest (RF), gradient boosting decision tree\n(GBDT), and support vector machine (SVM) - were trained to perform\nline-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global\npositioning system (GPS) signal features. A satellite is selected for\npositioning only when all classifiers unanimously agree on its classification\nand their associated confidence scores exceed a threshold. Experiments with\nreal-world GPS data collected in dense urban areas demonstrate that the\nproposed method significantly improves the positioning success rate and the\nreceiver containment rate, even with imperfect LOS/NLOS classification.\nAlthough a slight increase in the position bound was observed due to the\nreduced number of satellites used, overall positioning reliability was\nsubstantially enhanced, indicating the effectiveness of the proposed approach\nin urban GNSS environments."}
{"id": "2507.12972", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12972", "abs": "https://arxiv.org/abs/2507.12972", "authors": ["Daning Zhang", "Ying Wei"], "title": "AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers with Multi-Scale and Multi-Task Learning", "comment": null, "summary": "Separating target speech from mixed signals containing flexible speaker\nquantities presents a challenging task. While existing methods demonstrate\nstrong separation performance and noise robustness, they predominantly assume\nprior knowledge of speaker counts in mixtures. The limited research addressing\nunknown speaker quantity scenarios exhibits significantly constrained\ngeneralization capabilities in real acoustic environments. To overcome these\nchallenges, this paper proposes AVFSNet -- an audio-visual speech separation\nmodel integrating multi-scale encoding and parallel architecture -- jointly\noptimized for speaker counting and multi-speaker separation tasks. The model\nindependently separates each speaker in parallel while enhancing environmental\nnoise adaptability through visual information integration. Comprehensive\nexperimental evaluations demonstrate that AVFSNet achieves state-of-the-art\nresults across multiple evaluation metrics and delivers outstanding performance\non diverse datasets."}
{"id": "2507.12932", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.12932", "abs": "https://arxiv.org/abs/2507.12932", "authors": ["Zhou Feng", "Jiahao Chen", "Chunyi Zhou", "Yuwen Pu", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes", "comment": "Accepted by ACM MM 2025", "summary": "The rapid advancement of voice deepfake technologies has raised serious\nconcerns about user audio privacy, as attackers increasingly exploit publicly\navailable voice data to generate convincing fake audio for malicious purposes\nsuch as identity theft, financial fraud, and misinformation campaigns. While\nexisting defense methods offer partial protection, they face critical\nlimitations, including weak adaptability to unseen user data, poor scalability\nto long audio, rigid reliance on white-box knowledge, and high computational\nand temporal costs during the encryption process. To address these challenges\nand defend against personalized voice deepfake threats, we propose Enkidu, a\nnovel user-oriented privacy-preserving framework that leverages universal\nfrequential perturbations generated through black-box knowledge and few-shot\ntraining on a small amount of user data. These highly malleable\nfrequency-domain noise patches enable real-time, lightweight protection with\nstrong generalization across variable-length audio and robust resistance to\nvoice deepfake attacks, all while preserving perceptual quality and speech\nintelligibility. Notably, Enkidu achieves over 50 to 200 times processing\nmemory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime\nefficiency (real-time coefficient as low as 0.004) compared to six\nstate-of-the-art countermeasures. Extensive experiments across six mainstream\ntext-to-speech models and five cutting-edge automated speaker verification\nmodels demonstrate the effectiveness, transferability, and practicality of\nEnkidu in defending against both vanilla and adaptive voice deepfake attacks."}
{"id": "2507.12917", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12917", "abs": "https://arxiv.org/abs/2507.12917", "authors": ["Xi Ding", "Luca Kunz", "E. Jorswieck"], "title": "Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO", "comment": null, "summary": "This paper studies optimal joint beamforming (BF) for joint sensing and\ncommunication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While\nprior works have explored JSAC optimization using methods such as successive\nconvex approximation (SCA) and semidefinite relaxation (SDR), many of these\napproaches either lack global optimality or require additional rank-reduction\nsteps. In contrast, we propose an SDR-based optimization framework that\nguarantees globally optimal solutions without post-processing. To benchmark its\nperformance, we introduce a standalone BF strategy that dedicates each access\npoint (AP) exclusively to either communication or sensing. The proposed\nformulation builds upon a general multi-user system model, enabling future\nextensions beyond the single-user setting. Overall, our framework offers a\nglobally optimal and computationally efficient BF design, providing valuable\ninsights for the development of next-generation wireless networks."}
{"id": "2507.12563", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12563", "abs": "https://arxiv.org/abs/2507.12563", "authors": ["Carlos De La Vega Martin", "Rodrigo Diaz Fernandez", "Mark Sandler"], "title": "Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates", "comment": null, "summary": "Physical modelling synthesis aims to generate audio from physical simulations\nof vibrating structures. Thin elastic plates are a common model for drum\nmembranes. Traditional numerical methods like finite differences and finite\nelements offer high accuracy but are computationally demanding, limiting their\nuse in real-time audio applications. This paper presents a comparative analysis\nof neural network-based approaches for solving the vibration of nonlinear\nelastic plates. We evaluate several state-of-the-art models, trained on short\nsequences, for prediction of long sequences in an autoregressive fashion. We\nshow some of the limitations of these models, and why is not enough to look at\nthe prediction error in the time domain. We discuss the implications for\nreal-time audio synthesis and propose future directions for improving neural\napproaches to model nonlinear vibration."}
{"id": "2507.12996", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12996", "abs": "https://arxiv.org/abs/2507.12996", "authors": ["Yuexuan Kong", "Vincent Lostanlen", "Romain Hennequin", "Mathieu Lagrange", "Gabriel Meseguer-Brocal"], "title": "Multi-Class-Token Transformer for Multitask Self-supervised Music Information Retrieval", "comment": null, "summary": "Contrastive learning and equivariant learning are effective methods for\nself-supervised learning (SSL) for audio content analysis. Yet, their\napplication to music information retrieval (MIR) faces a dilemma: the former is\nmore effective on tagging (e.g., instrument recognition) but less effective on\nstructured prediction (e.g., tonality estimation); The latter can match\nsupervised methods on the specific task it is designed for, but it does not\ngeneralize well to other tasks. In this article, we adopt a best-of-both-worlds\napproach by training a deep neural network on both kinds of pretext tasks at\nonce. The proposed new architecture is a Vision Transformer with 1-D\nspectrogram patches (ViT-1D), equipped with two class tokens, which are\nspecialized to different self-supervised pretext tasks but optimized through\nthe same model: hence the qualification of self-supervised multi-class-token\nmultitask (MT2). The former class token optimizes cross-power spectral density\n(CPSD) for equivariant learning over the circle of fifths, while the latter\noptimizes normalized temperature-scaled cross-entropy (NT-Xent) for contrastive\nlearning. MT2 combines the strengths of both pretext tasks and outperforms\nconsistently both single-class-token ViT-1D models trained with either\ncontrastive or equivariant learning. Averaging the two class tokens further\nimproves performance on several tasks, highlighting the complementary nature of\nthe representations learned by each class token. Furthermore, using the same\nsingle-linear-layer probing method on the features of last layer, MT2\noutperforms MERT on all tasks except for beat tracking; achieving this with 18x\nfewer parameters thanks to its multitasking capabilities. Our SSL benchmark\ndemonstrates the versatility of our multi-class-token multitask learning\napproach for MIR applications."}
{"id": "2507.13037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13037", "abs": "https://arxiv.org/abs/2507.13037", "authors": ["Guangyao Liu", "Tianqi Mao", "Yanqun Tang", "Jingjing Zhao", "Zhenyu Xiao"], "title": "Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation", "comment": null, "summary": "Affine frequency division multiplexing (AFDM), a promising multicarrier\ntechnique utilizing chirp signals, has been envisioned as an effective solution\nfor high-mobility communication scenarios. In this paper, we develop a\nmultiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,\nwhich aims to further improve the spectral and energy efficiencies of AFDM.\nSpecifically, multiple constellation alphabets are selected for different\nchirp-based subcarriers (chirps). Aside from classical amplitude/phase\nmodulation, additional information bits can be conveyed by the dynamic patterns\nof both constellation mode selection and chirp activation, without extra energy\nconsumption. Furthermore, we discuss the mode selection strategy and derive an\nasymptotically tight upper bound on the bit error rate (BER) of the proposed\nscheme under maximum-likelihood detection. Simulation results are provided to\ndemonstrate the superior performance of MM-AFDM-IM compared to conventional\nbenchmark schemes."}
{"id": "2507.12701", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12701", "abs": "https://arxiv.org/abs/2507.12701", "authors": ["Anastasia Kuznetsova", "Inseon Jang", "Wootaek Lim", "Minje Kim"], "title": "Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine", "comment": null, "summary": "Neural audio codecs, leveraging quantization algorithms, have significantly\nimpacted various speech/audio tasks. While high-fidelity reconstruction is\nparamount for human perception, audio coding for machines (ACoM) prioritizes\nefficient compression and downstream task performance, disregarding perceptual\nnuances. This work introduces an efficient ACoM method that can compress and\nquantize any chosen intermediate feature representation of an already trained\nspeech/audio downstream model. Our approach employs task-specific loss guidance\nalongside residual vector quantization (RVQ) losses, providing ultra-low\nbitrates (i.e., less than 200 bps) with a minimal loss of the downstream model\nperformance. The resulting tokenizer is adaptable to various bitrates and model\nsizes for flexible deployment. Evaluated on automatic speech recognition and\naudio classification, our method demonstrates its efficacy and potential for\nbroader task and architectural applicability through appropriate\nregularization."}
{"id": "2507.13170", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13170", "abs": "https://arxiv.org/abs/2507.13170", "authors": ["Kutub Uddin", "Awais Khan", "Muhammad Umar Farooq", "Khalid Malik"], "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks", "comment": null, "summary": "Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively."}
{"id": "2507.13080", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13080", "abs": "https://arxiv.org/abs/2507.13080", "authors": ["Morteza Alijani", "Wout Joseph", "David Plets"], "title": "Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Visible Light Positioning (VLP) has emerged as a promising technology for\nnext-generation indoor positioning systems (IPS), particularly within the scope\nof sixth-generation (6G) wireless networks. Its attractiveness stems from\nleveraging existing lighting infrastructures equipped with light-emitting\ndiodes (LEDs), enabling cost-efficient deployments and achieving high-precision\npositioning accuracy in the centimeter-todecimeter range. However, widespread\nadoption of traditional VLP solutions faces significant barriers due to the\nincreased costs and operational complexity associated with modulating LEDs,\nwhich consequently reduces illumination efficiency by lowering their radiant\nflux. To address these limitations, recent research has introduced the concept\nof unmodulated Visible Light Positioning (uVLP), which exploits Light Signals\nof Opportunity (LSOOP) emitted by unmodulated illumination sources such as\nconventional LEDs. This paradigm offers a cost-effective, lowinfrastructure\nalternative for indoor positioning by eliminating the need for modulation\nhardware and maintaining lighting efficiency. This paper delineates the\nfundamental principles of uVLP, provides a comparative analysis of uVLP versus\nconventional VLP methods, and classifies existing uVLP techniques according to\nreceiver technologies into intensity-based methods (e.g., photodiodes, solar\ncells, etc.) and imaging-based methods. Additionally, we propose a\ncomprehensive taxonomy categorizing techniques into demultiplexed and\nundemultiplexed approaches. Within this structured framework, we critically\nreview current advancements in uVLP, discuss prevailing challenges, and outline\npromising research directions essential for developing robust, scalable, and\nwidely deployable uVLP solutions."}
{"id": "2507.12723", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12723", "abs": "https://arxiv.org/abs/2507.12723", "authors": ["Minyoung Kim", "Sehwan Park", "Sungmin Cha", "Paul Hongsuck Seo"], "title": "Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries", "comment": "5 pages, 2 figures, Interspeech 2025", "summary": "Recent advances in voice cloning and lip synchronization models have enabled\nSynthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are\nmanipulated to mimic a target speaker. This significantly increases the risk of\nmisinformation by making fake content seem real. To address this issue,\nexisting methods detect or localize manipulations but cannot recover the\nauthentic audio that conveys the semantic content of the message. This\nlimitation reduces their effectiveness in combating audiovisual misinformation.\nIn this work, we introduce the task of Authentic Audio Recovery (AAR) and\nTamper Localization in Audio (TLA) from SAVFs and propose a cross-modal\nwatermarking framework to embed authentic audio into visuals before\nmanipulation. This enables AAR, TLA, and a robust defense against\nmisinformation. Extensive experiments demonstrate the strong performance of our\nmethod in AAR and TLA against various manipulations, including voice cloning\nand lip synchronization."}
{"id": "2507.13264", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13264", "abs": "https://arxiv.org/abs/2507.13264", "authors": ["Alexander H. Liu", "Andy Ehrenberg", "Andy Lo", "Clément Denoix", "Corentin Barreau", "Guillaume Lample", "Jean-Malo Delignon", "Khyathi Raghavi Chandu", "Patrick von Platen", "Pavankumar Reddy Muddireddy", "Sanchit Gandhi", "Soham Ghosh", "Srijan Mishra", "Thomas Foubert", "Abhinav Rastogi", "Adam Yang", "Albert Q. Jiang", "Alexandre Sablayrolles", "Amélie Héliou", "Amélie Martin", "Anmol Agarwal", "Antoine Roux", "Arthur Darcet", "Arthur Mensch", "Baptiste Bout", "Baptiste Rozière", "Baudouin De Monicault", "Chris Bamford", "Christian Wallenwein", "Christophe Renaudin", "Clémence Lanfranchi", "Darius Dabert", "Devendra Singh Chaplot", "Devon Mizelle", "Diego de las Casas", "Elliot Chane-Sane", "Emilien Fugier", "Emma Bou Hanna", "Gabrielle Berrada", "Gauthier Delerce", "Gauthier Guinet", "Georgii Novikov", "Guillaume Martin", "Himanshu Jaju", "Jan Ludziejewski", "Jason Rute", "Jean-Hadrien Chabran", "Jessica Chudnovsky", "Joachim Studnia", "Joep Barmentlo", "Jonas Amar", "Josselin Somerville Roberts", "Julien Denize", "Karan Saxena", "Karmesh Yadav", "Kartik Khandelwal", "Kush Jain", "Lélio Renard Lavaud", "Léonard Blier", "Lingxiao Zhao", "Louis Martin", "Lucile Saulnier", "Luyu Gao", "Marie Pellat", "Mathilde Guillaumin", "Mathis Felardos", "Matthieu Dinot", "Maxime Darrin", "Maximilian Augustin", "Mickaël Seznec", "Neha Gupta", "Nikhil Raghuraman", "Olivier Duchenne", "Patricia Wang", "Patryk Saffer", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Philomène Chagniot", "Pierre Stock", "Pravesh Agrawal", "Rémi Delacourt", "Romain Sauvestre", "Roman Soletskyi", "Sagar Vaze", "Sandeep Subramanian", "Saurabh Garg", "Shashwat Dalal", "Siddharth Gandhi", "Sumukh Aithal", "Szymon Antoniak", "Teven Le Scao", "Thibault Schueller", "Thibaut Lavril", "Thomas Robert", "Thomas Wang", "Timothée Lacroix", "Tom Bewley", "Valeriia Nemychnikova", "Victor Paltz", "Virgile Richard", "Wen-Ding Li", "William Marshall", "Xuanyu Zhang", "Yihan Wan", "Yunhao Tang"], "title": "Voxtral", "comment": "17 pages", "summary": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.\nVoxtral is trained to comprehend both spoken audio and text documents,\nachieving state-of-the-art performance across a diverse range of audio\nbenchmarks, while preserving strong text capabilities. Voxtral Small\noutperforms a number of closed-source models, while being small enough to run\nlocally. A 32K context window enables the model to handle audio files up to 40\nminutes in duration and long multi-turn conversations. We also contribute three\nbenchmarks for evaluating speech understanding models on knowledge and trivia.\nBoth Voxtral models are released under Apache 2.0 license."}
{"id": "2507.13086", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13086", "abs": "https://arxiv.org/abs/2507.13086", "authors": ["Mingyan Gong"], "title": "Angle Estimation of a Single Source with Massive Uniform Circular Arrays", "comment": null, "summary": "Estimating the directions of arrival (DOAs) of incoming plane waves is an\nessential topic in array signal processing. Widely adopted uniform linear\narrays can only provide estimates of source azimuth. Thus, uniform circular\narrays (UCAs) are attractive in that they can provide $360^{\\circ}$ azimuthal\ncoverage and additional elevation angle information. Considering that with a\nmassive UCA, its polar angles of array sensors can approximately represent\nazimuth angles over $360^{\\circ}$ using angle quantization, a simple\ntwo-dimensional DOA estimation method for a single source is proposed. In this\nmethod, the quantized azimuth angle estimate is obtained by only calculating\nand comparing a number of covariances, based on which the elevation angle\nestimate is then obtained by an explicit formula. Thus, the proposed method is\ncomputationally simple and suitable for real-time signal processing. Numerical\nresults verify that the proposed method can obtain azimuth as well as elevation\nangle estimates and the estimates can be used as starting points of\nmultidimensional searches for methods with higher accuracy. Additionally, the\nproposed method can still work in the presence of nonuniform noise."}
{"id": "2507.12773", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12773", "abs": "https://arxiv.org/abs/2507.12773", "authors": ["Rajalaxmi Rajagopalan", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Sample-Constrained Black Box Optimization for Audio Personalization", "comment": "Published in AAAI 2024", "summary": "We consider the problem of personalizing audio to maximize user experience.\nBriefly, we aim to find a filter $h^*$, which applied to any music or speech,\nwill maximize the user's satisfaction. This is a black-box optimization problem\nsince the user's satisfaction function is unknown. Substantive work has been\ndone on this topic where the key idea is to play audio samples to the user,\neach shaped by a different filter $h_i$, and query the user for their\nsatisfaction scores $f(h_i)$. A family of ``surrogate\" functions is then\ndesigned to fit these scores and the optimization method gradually refines\nthese functions to arrive at the filter $\\hat{h}^*$ that maximizes\nsatisfaction. In certain applications, we observe that a second type of\nquerying is possible where users can tell us the individual elements $h^*[j]$\nof the optimal filter $h^*$. Consider an analogy from cooking where the goal is\nto cook a recipe that maximizes user satisfaction. A user can be asked to score\nvarious cooked recipes (e.g., tofu fried rice) or to score individual\ningredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$\nqueries, where a query can be of either type, our goal is to find the recipe\nthat will maximize this user's satisfaction. Our proposal builds on Sparse\nGaussian Process Regression (GPR) and shows how a hybrid approach can\noutperform any one type of querying. Our results are validated through\nsimulations and real world experiments, where volunteers gave feedback on\nmusic/speech audio and were able to achieve high satisfaction levels. We\nbelieve this idea of hybrid querying opens new problems in black-box\noptimization and solutions can benefit other applications beyond audio\npersonalization."}
{"id": "2507.12890", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12890", "abs": "https://arxiv.org/abs/2507.12890", "authors": ["Huakang Chen", "Yuepeng Jiang", "Guobin Ma", "Chunbo Hao", "Shuai Wang", "Jixun Yao", "Ziqian Ning", "Meng Meng", "Jian Luan", "Lei Xie"], "title": "DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization", "comment": null, "summary": "Songs, as a central form of musical art, exemplify the richness of human\nintelligence and creativity. While recent advances in generative modeling have\nenabled notable progress in long-form song generation, current systems for\nfull-length song synthesis still face major challenges, including data\nimbalance, insufficient controllability, and inconsistent musical quality.\nDiffRhythm, a pioneering diffusion-based model, advanced the field by\ngenerating full-length songs with expressive vocals and accompaniment. However,\nits performance was constrained by an unbalanced model training dataset and\nlimited controllability over musical style, resulting in noticeable quality\ndisparities and restricted creative flexibility. To address these limitations,\nwe propose DiffRhythm+, an enhanced diffusion-based framework for controllable\nand flexible full-length song generation. DiffRhythm+ leverages a substantially\nexpanded and balanced training dataset to mitigate issues such as repetition\nand omission of lyrics, while also fostering the emergence of richer musical\nskills and expressiveness. The framework introduces a multi-modal style\nconditioning strategy, enabling users to precisely specify musical styles\nthrough both descriptive text and reference audio, thereby significantly\nenhancing creative control and diversity. We further introduce direct\nperformance optimization aligned with user preferences, guiding the model\ntoward consistently preferred outputs across evaluation metrics. Extensive\nexperiments demonstrate that DiffRhythm+ achieves significant improvements in\nnaturalness, arrangement complexity, and listener satisfaction over previous\nsystems."}
{"id": "2507.13130", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.13130", "abs": "https://arxiv.org/abs/2507.13130", "authors": ["Aleksandr D. Kuznetsov", "Jari Holopainen", "Ville Viikari"], "title": "Multifrequency system model for multiport time-modulated scatterers", "comment": "11 pages, 11 figures; this work has been submitted to the IEEE for\n  possible publication", "summary": "Utilizing scatterers in communication engineering, such as reconfigurable\nintelligent surfaces (RISs) and backscatter systems, requires physically\nconsistent models for accurate performance prediction. A multiport model, which\nalso accounts for structural scattering, has been developed for non-periodic\nscatterers. However, many emerging systems operate at multiple frequencies or\ngenerate intermodulation harmonics, particularly when incorporating space-time\nmodulation (STM) or dynamic load control. These functionalities demand advanced\nmodeling approaches capable of capturing scattering behavior across several\nfrequencies and directions simultaneously. This article extends a multiport\nS-parameters-based model for predicting the scattering properties of\nmultifrequency operating structures. The model extends the applicability of\nconvenient S-matrix models to time-modulated multiport structures. Unlike known\napproaches, this model incorporates structural scattering, mutual coupling, the\npossibility of non-digital modulation, and non-periodic configurations,\nenabling precise analysis and optimization for a broad range of communication\nand sensing systems. Validation against experimental results for a space-time\nmodulated scattering structure demonstrates the accuracy and practical\napplicability of the proposed model."}
{"id": "2507.12793", "categories": ["cs.SD", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12793", "abs": "https://arxiv.org/abs/2507.12793", "authors": ["J. M. Chan Sri Manukalpa", "H. S. Bopage", "W. A. M. Jayawardena", "P. K. P. G. Panduwawala"], "title": "Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features", "comment": "This is a preprint article", "summary": "Structural pests, such as termites, pose a serious threat to wooden\nbuildings, resulting in significant economic losses due to their hidden and\nprogressive damage. Traditional detection methods, such as visual inspections\nand chemical treatments, are invasive, labor intensive, and ineffective for\nearly stage infestations. To bridge this gap, this study proposes a non\ninvasive deep learning based acoustic classification framework for early\ntermite detection. We aim to develop a robust, scalable model that\ndistinguishes termite generated acoustic signals from background noise. We\nintroduce a hybrid Convolutional Neural Network Long Short Term Memory\narchitecture that captures both spatial and temporal features of termite\nactivity. Audio data were collected from termite infested and clean wooden\nsamples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN\nLSTM model to classify the signals. Experimental results show high performance,\nwith 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis\nreveals that the hybrid model outperforms standalone CNN and LSTM\narchitectures, underscoring its combined strength. Notably, the model yields\nlow false-negative rates, which is essential for enabling timely intervention.\nThis research contributes a non invasive, automated solution for early termite\ndetection, with practical implications for improved pest monitoring, minimized\nstructural damage, and better decision making by homeowners and pest control\nprofessionals. Future work may integrate IoT for real time alerts and extend\ndetection to other structural pests."}
{"id": "2507.12951", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12951", "abs": "https://arxiv.org/abs/2507.12951", "authors": ["Zhichao Sheng", "Shilin Zhou", "Chen Gong", "Zhenghua Li"], "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets", "comment": "13 pages, 3 figures", "summary": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research."}
{"id": "2507.13176", "categories": ["eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13176", "abs": "https://arxiv.org/abs/2507.13176", "authors": ["Moritz Leuthner", "Rafael Vorländer", "Oliver Hayden"], "title": "Disentangling coincident cell events using deep transfer learning and compressive sensing", "comment": null, "summary": "Accurate single-cell analysis is critical for diagnostics, immunomonitoring,\nand cell therapy, but coincident events - where multiple cells overlap in a\nsensing zone - can severely compromise signal fidelity. We present a hybrid\nframework combining a fully convolutional neural network (FCN) with compressive\nsensing (CS) to disentangle such overlapping events in one-dimensional sensor\ndata. The FCN, trained on bead-derived datasets, accurately estimates\ncoincident event counts and generalizes to immunomagnetically labeled CD4+ and\nCD14+ cells in whole blood without retraining. Using this count, the CS module\nreconstructs individual signal components with high fidelity, enabling precise\nrecovery of single-cell features, including velocity, amplitude, and\nhydrodynamic diameter. Benchmarking against conventional state-machine\nalgorithms shows superior performance - recovering up to 21% more events and\nimproving classification accuracy beyond 97%. Explinability via class\nactivation maps and parameterized Gaussian template fitting ensures\ntransparency and clinical interpretability. Demonstrated with magnetic flow\ncytometry (MFC), the framework is compatible with other waveform-generating\nmodalities, including impedance cytometry, nanopore, and resistive pulse\nsensing. This work lays the foundation for next-generation non-optical\nsingle-cell sensing platforms that are automated, generalizable, and capable of\nresolving overlapping events, broadening the utility of cytometry in\ntranslational medicine and precision diagnostics, e.g. cell-interaction\nstudies."}
{"id": "2507.12825", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12825", "abs": "https://arxiv.org/abs/2507.12825", "authors": ["Luca Della Libera", "Cem Subakan", "Mirco Ravanelli"], "title": "Autoregressive Speech Enhancement via Acoustic Tokens", "comment": "5 pages, 2 figures", "summary": "In speech processing pipelines, improving the quality and intelligibility of\nreal-world recordings is crucial. While supervised regression is the primary\nmethod for speech enhancement, audio tokenization is emerging as a promising\nalternative for a smooth integration with other modalities. However, research\non speech enhancement using discrete representations is still limited. Previous\nwork has mainly focused on semantic tokens, which tend to discard key acoustic\ndetails such as speaker identity. Additionally, these studies typically employ\nnon-autoregressive models, assuming conditional independence of outputs and\noverlooking the potential improvements offered by autoregressive modeling. To\naddress these gaps we: 1) conduct a comprehensive study of the performance of\nacoustic tokens for speech enhancement, including the effect of bitrate and\nnoise strength; 2) introduce a novel transducer-based autoregressive\narchitecture specifically designed for this task. Experiments on VoiceBank and\nLibri1Mix datasets show that acoustic tokens outperform semantic tokens in\nterms of preserving speaker identity, and that our autoregressive approach can\nfurther improve performance. Nevertheless, we observe that discrete\nrepresentations still fall short compared to continuous ones, highlighting the\nneed for further research in this area."}
{"id": "2507.12972", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.12972", "abs": "https://arxiv.org/abs/2507.12972", "authors": ["Daning Zhang", "Ying Wei"], "title": "AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers with Multi-Scale and Multi-Task Learning", "comment": null, "summary": "Separating target speech from mixed signals containing flexible speaker\nquantities presents a challenging task. While existing methods demonstrate\nstrong separation performance and noise robustness, they predominantly assume\nprior knowledge of speaker counts in mixtures. The limited research addressing\nunknown speaker quantity scenarios exhibits significantly constrained\ngeneralization capabilities in real acoustic environments. To overcome these\nchallenges, this paper proposes AVFSNet -- an audio-visual speech separation\nmodel integrating multi-scale encoding and parallel architecture -- jointly\noptimized for speaker counting and multi-speaker separation tasks. The model\nindependently separates each speaker in parallel while enhancing environmental\nnoise adaptability through visual information integration. Comprehensive\nexperimental evaluations demonstrate that AVFSNet achieves state-of-the-art\nresults across multiple evaluation metrics and delivers outstanding performance\non diverse datasets."}
{"id": "2507.12870", "categories": ["cs.SD", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12870", "abs": "https://arxiv.org/abs/2507.12870", "authors": ["John Hansen", "Satwik Dutta", "Ellen Grand"], "title": "Best Practices and Considerations for Child Speech Corpus Collection and Curation in Educational, Clinical, and Forensic Scenarios", "comment": "5 pages, 0 figures, accepted at the 10th Workshop on Speech and\n  Language Technology in Education (SLaTE 2025), a Satellite Workshop of the\n  2025 Interspeech Conference", "summary": "A child's spoken ability continues to change until their adult age. Until\n7-8yrs, their speech sound development and language structure evolve rapidly.\nThis dynamic shift in their spoken communication skills and data privacy make\nit challenging to curate technology-ready speech corpora for children. This\nstudy aims to bridge this gap and provide researchers and practitioners with\nthe best practices and considerations for developing such a corpus based on an\nintended goal. Although primarily focused on educational goals, applications of\nchild speech data have spread across fields including clinical and forensics\nfields. Motivated by this goal, we describe the WHO, WHAT, WHEN, and WHERE of\ndata collection inspired by prior collection efforts and our\nexperience/knowledge. We also provide a guide to establish collaboration,\ntrust, and for navigating the human subjects research protocol. This study\nconcludes with guidelines for corpus quality check, triage, and annotation."}
{"id": "2507.13170", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13170", "abs": "https://arxiv.org/abs/2507.13170", "authors": ["Kutub Uddin", "Awais Khan", "Muhammad Umar Farooq", "Khalid Malik"], "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks", "comment": null, "summary": "Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively."}
{"id": "2507.13264", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13264", "abs": "https://arxiv.org/abs/2507.13264", "authors": ["Alexander H. Liu", "Andy Ehrenberg", "Andy Lo", "Clément Denoix", "Corentin Barreau", "Guillaume Lample", "Jean-Malo Delignon", "Khyathi Raghavi Chandu", "Patrick von Platen", "Pavankumar Reddy Muddireddy", "Sanchit Gandhi", "Soham Ghosh", "Srijan Mishra", "Thomas Foubert", "Abhinav Rastogi", "Adam Yang", "Albert Q. Jiang", "Alexandre Sablayrolles", "Amélie Héliou", "Amélie Martin", "Anmol Agarwal", "Antoine Roux", "Arthur Darcet", "Arthur Mensch", "Baptiste Bout", "Baptiste Rozière", "Baudouin De Monicault", "Chris Bamford", "Christian Wallenwein", "Christophe Renaudin", "Clémence Lanfranchi", "Darius Dabert", "Devendra Singh Chaplot", "Devon Mizelle", "Diego de las Casas", "Elliot Chane-Sane", "Emilien Fugier", "Emma Bou Hanna", "Gabrielle Berrada", "Gauthier Delerce", "Gauthier Guinet", "Georgii Novikov", "Guillaume Martin", "Himanshu Jaju", "Jan Ludziejewski", "Jason Rute", "Jean-Hadrien Chabran", "Jessica Chudnovsky", "Joachim Studnia", "Joep Barmentlo", "Jonas Amar", "Josselin Somerville Roberts", "Julien Denize", "Karan Saxena", "Karmesh Yadav", "Kartik Khandelwal", "Kush Jain", "Lélio Renard Lavaud", "Léonard Blier", "Lingxiao Zhao", "Louis Martin", "Lucile Saulnier", "Luyu Gao", "Marie Pellat", "Mathilde Guillaumin", "Mathis Felardos", "Matthieu Dinot", "Maxime Darrin", "Maximilian Augustin", "Mickaël Seznec", "Neha Gupta", "Nikhil Raghuraman", "Olivier Duchenne", "Patricia Wang", "Patryk Saffer", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Philomène Chagniot", "Pierre Stock", "Pravesh Agrawal", "Rémi Delacourt", "Romain Sauvestre", "Roman Soletskyi", "Sagar Vaze", "Sandeep Subramanian", "Saurabh Garg", "Shashwat Dalal", "Siddharth Gandhi", "Sumukh Aithal", "Szymon Antoniak", "Teven Le Scao", "Thibault Schueller", "Thibaut Lavril", "Thomas Robert", "Thomas Wang", "Timothée Lacroix", "Tom Bewley", "Valeriia Nemychnikova", "Victor Paltz", "Virgile Richard", "Wen-Ding Li", "William Marshall", "Xuanyu Zhang", "Yihan Wan", "Yunhao Tang"], "title": "Voxtral", "comment": "17 pages", "summary": "We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.\nVoxtral is trained to comprehend both spoken audio and text documents,\nachieving state-of-the-art performance across a diverse range of audio\nbenchmarks, while preserving strong text capabilities. Voxtral Small\noutperforms a number of closed-source models, while being small enough to run\nlocally. A 32K context window enables the model to handle audio files up to 40\nminutes in duration and long multi-turn conversations. We also contribute three\nbenchmarks for evaluating speech understanding models on knowledge and trivia.\nBoth Voxtral models are released under Apache 2.0 license."}

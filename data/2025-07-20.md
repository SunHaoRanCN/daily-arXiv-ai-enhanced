<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS](https://arxiv.org/abs/2507.12593)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种基于Zak-OTFS的差分通信方案，减少周期性导频传输需求，提高频谱效率和能量利用率。


<details>
  <summary>Details</summary>
Motivation: Zak-OTFS在延迟-多普勒域的信道具有可预测性，但仍需周期性导频传输。本文旨在通过差分通信方案减少导频需求。

Method: 利用检测到的数据符号作为导频，通过DD域信道的预测能力，将前一时刻的信道估计用于下一时刻的数据检测，实现差分通信。

Result: 方案提高了数据符号的能量利用率，实现了全频谱效率，且比现有方案具有更低的误码率和复杂度。

Conclusion: 提出的差分通信方案在Zak-OTFS系统中有效减少了导频传输需求，同时提升了性能和效率。

Abstract: Zak-transform based orthogonal time frequency space (Zak-OTFS) is a
delay-Doppler (DD) domain modulation scheme in which the signal processing is
carried out in the DD domain. The channel when viewed in the DD domain is
predictable. However, even with Zak-OTFS, pilots need to be sent periodically,
albeit at a lower rate. In this paper, we propose a differential communication
scheme for Zak-OTFS systems that alleviates the need for periodic pilot
transmission. Towards this, we analytically show that the detected data can be
used as a pilot and that the channel estimate obtained from the detected data
can enable further detection enabling the "differential" aspect of the
communication. Specifically, we leverage the prediction capability of the DD
channel in Zak-OTFS to use the channel estimate (obtained from detected data
symbols treated as pilots) in the previous instant to detect data in the next
instant and propagate this forward. The advantages are two fold. First, it
allows the data symbols to enjoy higher energy since the energy that would
otherwise be required for pilot symbols can also be allocated to data symbols.
Second, it allows for full spectral efficiency compared to point or embedded
pilots. Comparison with the full spectral efficiency achieving spread pilot
scheme shows that the proposed method achieves better bit-error rate at lower
complexity.

</details>


### [2] [Achieving Robust Channel Estimation Neural Networks by Designed Training Data](https://arxiv.org/abs/2507.12630)
*Dianxin Luan,John Thompson*

Main category: eess.SP

TL;DR: 论文提出了一种离线训练的神经网络设计准则，用于生成合成训练数据集，确保网络在新信道上的均方误差（MSE）表现稳定，无需实时调整参数。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在新数据上表现不佳，且物理信道通常是时变的，而低延迟和有限计算资源限制了在线训练的可能性。

Method: 提出设计准则生成合成训练数据集，并设计基准方案确保对不同信道配置的智能操作。

Result: 神经网络在不同复杂度下均能实现对新信道的鲁棒泛化，仿真验证了其在固定和可变延迟扩展信道上的表现。

Conclusion: 离线训练的神经网络设计准则有效解决了新信道上的泛化问题，无需实时调整参数，适用于实际无线通信场景。

Abstract: Channel estimation is crucial in cognitive communications, as it enables
intelligent spectrum sensing and adaptive transmission by providing accurate
information about the current channel state. However, in many papers neural
networks are frequently tested by training and testing on one example channel
or similar channels. This is because data-driven methods often degrade on new
data which they are not trained on, as they cannot extrapolate their training
knowledge. This is despite the fact physical channels are often assumed to be
time-variant. However, due to the low latency requirements and limited
computing resources, neural networks may not have enough time and computing
resources to execute online training to fine-tune the parameters. This
motivates us to design offline-trained neural networks that can perform
robustly over wireless channels, but without any actual channel information
being known at design time. In this paper, we propose design criteria to
generate synthetic training datasets for neural networks, which guarantee that
after training the resulting networks achieve a certain mean squared error
(MSE) on new and previously unseen channels. Therefore, neural network
solutions require no prior channel information or parameters update for
real-world implementations. Based on the proposed design criteria, we further
propose a benchmark design which ensures intelligent operation for different
channel profiles. To demonstrate general applicability, we use neural networks
with different levels of complexity to show that the generalization achieved
appears to be independent of neural network architecture. From simulations,
neural networks achieve robust generalization to wireless channels with both
fixed channel profiles and variable delay spreads.

</details>


### [3] [A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis](https://arxiv.org/abs/2507.12645)
*Mohammed Guhdar,Ramadhan J. Mstafa,Abdulhakeem O. Mohammed*

Main category: eess.SP

TL;DR: 提出了一种统一的深度学习框架，用于处理多种生物信号（如ECG和EEG），解决了信号差异和类别不平衡问题，并在多个基准数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能有效处理不同生理信号的统一架构，且生物医学数据中的类别不平衡问题导致传统方法性能偏差。

Method: 结合ResNet CNN和注意力机制，采用时间域信号拼接的数据增强策略，并使用Focal Loss处理类别不平衡。预处理包括小波去噪、基线去除和标准化。

Result: 在UCI Seizure EEG、MIT-BIH Arrhythmia和PTB Diagnostic ECG数据集上分别达到99.96%、99.78%和100%的准确率。

Conclusion: 该框架在性能和效率上均表现优异，适合部署在低端或可穿戴设备上。

Abstract: The increasing need for accurate and unified analysis of diverse biological
signals, such as ECG and EEG, is paramount for comprehensive patient
assessment, especially in synchronous monitoring. Despite advances in
multi-sensor fusion, a critical gap remains in developing unified architectures
that effectively process and extract features from fundamentally different
physiological signals. Another challenge is the inherent class imbalance in
many biomedical datasets, often causing biased performance in traditional
methods. This study addresses these issues by proposing a novel and unified
deep learning framework that achieves state-of-the-art performance across
different signal types. Our method integrates a ResNet-based CNN with an
attention mechanism, enhanced by a novel data augmentation strategy:
time-domain concatenation of multiple augmented variants of each signal to
generate richer representations. Unlike prior work, we scientifically increase
signal complexity to achieve future-reaching capabilities, which resulted in
the best predictions compared to the state of the art. Preprocessing steps
included wavelet denoising, baseline removal, and standardization. Class
imbalance was effectively managed through the combined use of this advanced
data augmentation and the Focal Loss function. Regularization techniques were
applied during training to ensure generalization. We rigorously evaluated the
proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH
Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,
and 100%, respectively, demonstrating robustness across diverse signal types
and clinical contexts. Finally, the architecture requires ~130 MB of memory and
processes each sample in ~10 ms, suggesting suitability for deployment on
low-end or wearable devices.

</details>


### [4] [Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers](https://arxiv.org/abs/2507.12706)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 论文提出了一种基于多机器学习分类器一致投票的保守卫星选择策略，增强了ZSM定位方法，显著提高了城市GNSS环境中的定位成功率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GNSS定位常因建筑物遮挡和多路径效应导致误差，需改进定位方法以提高可靠性。

Method: 使用RF、GBDT和SVM三种模型进行LOS/NLOS分类，仅在所有分类器一致且置信度超过阈值时选择卫星。

Result: 实验表明，该方法显著提高了定位成功率和接收器包含率，尽管卫星数量减少导致定位边界略有增加。

Conclusion: 该方法在城市GNSS环境中有效提升了定位可靠性。

Abstract: In urban environments, global navigation satellite system (GNSS) positioning
is often compromised by signal blockages and multipath effects caused by
buildings, leading to significant positioning errors. To address this issue,
this study proposes a robust enhancement of zonotope shadow matching
(ZSM)-based positioning by employing a conservative satellite selection
strategy using unanimous voting across multiple machine learning classifiers.
Three distinct models - random forest (RF), gradient boosting decision tree
(GBDT), and support vector machine (SVM) - were trained to perform
line-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global
positioning system (GPS) signal features. A satellite is selected for
positioning only when all classifiers unanimously agree on its classification
and their associated confidence scores exceed a threshold. Experiments with
real-world GPS data collected in dense urban areas demonstrate that the
proposed method significantly improves the positioning success rate and the
receiver containment rate, even with imperfect LOS/NLOS classification.
Although a slight increase in the position bound was observed due to the
reduced number of satellites used, overall positioning reliability was
substantially enhanced, indicating the effectiveness of the proposed approach
in urban GNSS environments.

</details>


### [5] [Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO](https://arxiv.org/abs/2507.12917)
*Xi Ding,Luca Kunz,E. Jorswieck*

Main category: eess.SP

TL;DR: 本文提出了一种在小规模无小区MIMO系统中实现联合感知与通信的全局最优波束成形方法，避免了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有联合感知与通信优化方法缺乏全局最优性或需要额外步骤，本文旨在解决这些问题。

Method: 采用基于半定松弛的优化框架，确保全局最优解，无需后处理。

Result: 提出的方法在计算效率和性能上优于传统策略。

Conclusion: 该框架为下一代无线网络的发展提供了全局最优且高效的波束成形设计。

Abstract: This paper studies optimal joint beamforming (BF) for joint sensing and
communication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While
prior works have explored JSAC optimization using methods such as successive
convex approximation (SCA) and semidefinite relaxation (SDR), many of these
approaches either lack global optimality or require additional rank-reduction
steps. In contrast, we propose an SDR-based optimization framework that
guarantees globally optimal solutions without post-processing. To benchmark its
performance, we introduce a standalone BF strategy that dedicates each access
point (AP) exclusively to either communication or sensing. The proposed
formulation builds upon a general multi-user system model, enabling future
extensions beyond the single-user setting. Overall, our framework offers a
globally optimal and computationally efficient BF design, providing valuable
insights for the development of next-generation wireless networks.

</details>


### [6] [Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation](https://arxiv.org/abs/2507.13037)
*Guangyao Liu,Tianqi Mao,Yanqun Tang,Jingjing Zhao,Zhenyu Xiao*

Main category: eess.SP

TL;DR: 提出了一种基于AFDM的多模式索引调制方案（MM-AFDM-IM），通过动态选择星座模式和激活子载波来提高频谱和能量效率。


<details>
  <summary>Details</summary>
Motivation: 针对高移动性通信场景，AFDM技术具有潜力，但需要进一步提升其频谱和能量效率。

Method: 设计了多模式索引调制方案，动态选择星座模式和激活子载波，无需额外能耗。

Result: 仿真结果表明，MM-AFDM-IM在误码率上优于传统方案。

Conclusion: MM-AFDM-IM是一种高效的多载波技术，适用于高移动性通信场景。

Abstract: Affine frequency division multiplexing (AFDM), a promising multicarrier
technique utilizing chirp signals, has been envisioned as an effective solution
for high-mobility communication scenarios. In this paper, we develop a
multiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,
which aims to further improve the spectral and energy efficiencies of AFDM.
Specifically, multiple constellation alphabets are selected for different
chirp-based subcarriers (chirps). Aside from classical amplitude/phase
modulation, additional information bits can be conveyed by the dynamic patterns
of both constellation mode selection and chirp activation, without extra energy
consumption. Furthermore, we discuss the mode selection strategy and derive an
asymptotically tight upper bound on the bit error rate (BER) of the proposed
scheme under maximum-likelihood detection. Simulation results are provided to
demonstrate the superior performance of MM-AFDM-IM compared to conventional
benchmark schemes.

</details>


### [7] [Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects](https://arxiv.org/abs/2507.13080)
*Morteza Alijani,Wout Joseph,David Plets*

Main category: eess.SP

TL;DR: 本文探讨了无调制可见光定位（uVLP）技术，分析了其与传统VLP的对比，并分类了现有uVLP技术。


<details>
  <summary>Details</summary>
Motivation: 传统VLP技术因调制LED带来的成本和效率问题限制了广泛应用，uVLP利用未调制光源提供低成本解决方案。

Method: 通过比较分析和分类现有uVLP技术，提出基于接收器技术的分类法，并构建技术分类框架。

Result: uVLP提供了一种低成本、高效率的室内定位替代方案，但仍需解决技术和可扩展性问题。

Conclusion: uVLP是未来室内定位的有前景方向，需进一步研究以克服挑战并实现广泛应用。

Abstract: Visible Light Positioning (VLP) has emerged as a promising technology for
next-generation indoor positioning systems (IPS), particularly within the scope
of sixth-generation (6G) wireless networks. Its attractiveness stems from
leveraging existing lighting infrastructures equipped with light-emitting
diodes (LEDs), enabling cost-efficient deployments and achieving high-precision
positioning accuracy in the centimeter-todecimeter range. However, widespread
adoption of traditional VLP solutions faces significant barriers due to the
increased costs and operational complexity associated with modulating LEDs,
which consequently reduces illumination efficiency by lowering their radiant
flux. To address these limitations, recent research has introduced the concept
of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals
of Opportunity (LSOOP) emitted by unmodulated illumination sources such as
conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure
alternative for indoor positioning by eliminating the need for modulation
hardware and maintaining lighting efficiency. This paper delineates the
fundamental principles of uVLP, provides a comparative analysis of uVLP versus
conventional VLP methods, and classifies existing uVLP techniques according to
receiver technologies into intensity-based methods (e.g., photodiodes, solar
cells, etc.) and imaging-based methods. Additionally, we propose a
comprehensive taxonomy categorizing techniques into demultiplexed and
undemultiplexed approaches. Within this structured framework, we critically
review current advancements in uVLP, discuss prevailing challenges, and outline
promising research directions essential for developing robust, scalable, and
widely deployable uVLP solutions.

</details>


### [8] [Angle Estimation of a Single Source with Massive Uniform Circular Arrays](https://arxiv.org/abs/2507.13086)
*Mingyan Gong*

Main category: eess.SP

TL;DR: 提出了一种基于均匀圆阵（UCA）的简单二维DOA估计方法，适用于实时信号处理，并能处理非均匀噪声。


<details>
  <summary>Details</summary>
Motivation: 均匀线阵只能估计方位角，而UCA能提供360度方位角和仰角信息，因此需要一种简单高效的二维DOA估计方法。

Method: 通过量化方位角并计算协方差来估计方位角，再通过显式公式估计仰角。

Result: 数值结果表明该方法能有效估计方位角和仰角，并可作为高精度方法的初始点。

Conclusion: 该方法计算简单，适用于实时处理，且对非均匀噪声具有鲁棒性。

Abstract: Estimating the directions of arrival (DOAs) of incoming plane waves is an
essential topic in array signal processing. Widely adopted uniform linear
arrays can only provide estimates of source azimuth. Thus, uniform circular
arrays (UCAs) are attractive in that they can provide $360^{\circ}$ azimuthal
coverage and additional elevation angle information. Considering that with a
massive UCA, its polar angles of array sensors can approximately represent
azimuth angles over $360^{\circ}$ using angle quantization, a simple
two-dimensional DOA estimation method for a single source is proposed. In this
method, the quantized azimuth angle estimate is obtained by only calculating
and comparing a number of covariances, based on which the elevation angle
estimate is then obtained by an explicit formula. Thus, the proposed method is
computationally simple and suitable for real-time signal processing. Numerical
results verify that the proposed method can obtain azimuth as well as elevation
angle estimates and the estimates can be used as starting points of
multidimensional searches for methods with higher accuracy. Additionally, the
proposed method can still work in the presence of nonuniform noise.

</details>


### [9] [Multifrequency system model for multiport time-modulated scatterers](https://arxiv.org/abs/2507.13130)
*Aleksandr D. Kuznetsov,Jari Holopainen,Ville Viikari*

Main category: eess.SP

TL;DR: 论文提出了一种多端口S参数模型，用于预测多频操作结构的散射特性，适用于非周期性配置和非数字调制。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测多频操作结构的散射行为，特别是在空间时间调制（STM）或动态负载控制下。

Method: 扩展了多端口S参数模型，考虑了结构散射、互耦、非数字调制和非周期性配置。

Result: 模型验证了其准确性和实用性，适用于广泛的通信和传感系统。

Conclusion: 该模型为多频操作结构提供了精确的分析和优化工具。

Abstract: Utilizing scatterers in communication engineering, such as reconfigurable
intelligent surfaces (RISs) and backscatter systems, requires physically
consistent models for accurate performance prediction. A multiport model, which
also accounts for structural scattering, has been developed for non-periodic
scatterers. However, many emerging systems operate at multiple frequencies or
generate intermodulation harmonics, particularly when incorporating space-time
modulation (STM) or dynamic load control. These functionalities demand advanced
modeling approaches capable of capturing scattering behavior across several
frequencies and directions simultaneously. This article extends a multiport
S-parameters-based model for predicting the scattering properties of
multifrequency operating structures. The model extends the applicability of
convenient S-matrix models to time-modulated multiport structures. Unlike known
approaches, this model incorporates structural scattering, mutual coupling, the
possibility of non-digital modulation, and non-periodic configurations,
enabling precise analysis and optimization for a broad range of communication
and sensing systems. Validation against experimental results for a space-time
modulated scattering structure demonstrates the accuracy and practical
applicability of the proposed model.

</details>


### [10] [Disentangling coincident cell events using deep transfer learning and compressive sensing](https://arxiv.org/abs/2507.13176)
*Moritz Leuthner,Rafael Vorländer,Oliver Hayden*

Main category: eess.SP

TL;DR: 提出了一种结合全卷积神经网络（FCN）和压缩感知（CS）的混合框架，用于解决单细胞分析中的信号重叠问题，显著提高了事件恢复和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 单细胞分析在诊断和细胞治疗中至关重要，但信号重叠会严重影响数据准确性，需要一种高效的方法来解决这一问题。

Method: 使用FCN估计重叠事件数量，结合CS模块重构单个信号，并通过磁流式细胞术（MFC）验证。

Result: 相比传统算法，该方法恢复了21%更多事件，分类准确率超过97%。

Conclusion: 该框架为下一代非光学单细胞传感平台奠定了基础，扩展了细胞术在精准诊断中的应用。

Abstract: Accurate single-cell analysis is critical for diagnostics, immunomonitoring,
and cell therapy, but coincident events - where multiple cells overlap in a
sensing zone - can severely compromise signal fidelity. We present a hybrid
framework combining a fully convolutional neural network (FCN) with compressive
sensing (CS) to disentangle such overlapping events in one-dimensional sensor
data. The FCN, trained on bead-derived datasets, accurately estimates
coincident event counts and generalizes to immunomagnetically labeled CD4+ and
CD14+ cells in whole blood without retraining. Using this count, the CS module
reconstructs individual signal components with high fidelity, enabling precise
recovery of single-cell features, including velocity, amplitude, and
hydrodynamic diameter. Benchmarking against conventional state-machine
algorithms shows superior performance - recovering up to 21% more events and
improving classification accuracy beyond 97%. Explinability via class
activation maps and parameterized Gaussian template fitting ensures
transparency and clinical interpretability. Demonstrated with magnetic flow
cytometry (MFC), the framework is compatible with other waveform-generating
modalities, including impedance cytometry, nanopore, and resistive pulse
sensing. This work lays the foundation for next-generation non-optical
single-cell sensing platforms that are automated, generalizable, and capable of
resolving overlapping events, broadening the utility of cytometry in
translational medicine and precision diagnostics, e.g. cell-interaction
studies.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Enhancing In-Domain and Out-Domain EmoFake Detection via Cooperative Multilingual Speech Foundation Models](https://arxiv.org/abs/2507.12595)
*Orchid Chetia Phukan,Mohd Mujtaba Akhtar,Girish,Arun Balaji Buduru*

Main category: eess.AS

TL;DR: 本文提出了一种用于情感伪造检测（EFD）的方法，利用多语言语音基础模型（SFMs）的优势，并通过THAMA模型融合技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 情感伪造检测需要理解语音中的细微变化（如音调、音色和强度），多语言SFMs因其跨语言预训练而具备这种能力。

Method: 通过比较SOTA的SFMs，验证多语言SFMs的优越性，并提出THAMA模型融合技术，结合Tucker分解和Hadamard乘积。

Result: 多语言SFMs在域内和跨域评估中表现最佳，THAMA进一步提升了性能，超越单个FM、基线融合技术和SOTA方法。

Conclusion: 多语言SFMs结合THAMA模型融合技术，为情感伪造检测提供了高效解决方案。

Abstract: In this work, we address EmoFake Detection (EFD). We hypothesize that
multilingual speech foundation models (SFMs) will be particularly effective for
EFD due to their pre-training across diverse languages, enabling a nuanced
understanding of variations in pitch, tone, and intensity. To validate this, we
conduct a comprehensive comparative analysis of state-of-the-art (SOTA) SFMs.
Our results shows the superiority of multilingual SFMs for same language
(in-domain) as well as cross-lingual (out-domain) evaluation. To our end, we
also propose, THAMA for fusion of foundation models (FMs) motivated by related
research where combining FMs have shown improved performance. THAMA leverages
the complementary conjunction of tucker decomposition and hadamard product for
effective fusion. With THAMA, synergized with cooperative multilingual SFMs
achieves topmost performance across in-domain and out-domain settings,
outperforming individual FMs, baseline fusion techniques, and prior SOTA
methods.

</details>


### [12] [DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization](https://arxiv.org/abs/2507.12890)
*Huakang Chen,Yuepeng Jiang,Guobin Ma,Chunbo Hao,Shuai Wang,Jixun Yao,Ziqian Ning,Meng Meng,Jian Luan,Lei Xie*

Main category: eess.AS

TL;DR: DiffRhythm+是一个改进的扩散模型，用于可控且灵活的全长歌曲生成，解决了数据不平衡和风格控制问题。


<details>
  <summary>Details</summary>
Motivation: 当前全长歌曲生成系统存在数据不平衡、可控性不足和音乐质量不一致的问题，DiffRhythm+旨在解决这些限制。

Method: DiffRhythm+采用扩展且平衡的训练数据集，引入多模态风格调节策略，并通过用户偏好优化性能。

Result: 实验表明，DiffRhythm+在自然度、编曲复杂度和听众满意度上显著优于现有系统。

Conclusion: DiffRhythm+通过改进数据集和增强控制能力，提升了全长歌曲生成的质量和多样性。

Abstract: Songs, as a central form of musical art, exemplify the richness of human
intelligence and creativity. While recent advances in generative modeling have
enabled notable progress in long-form song generation, current systems for
full-length song synthesis still face major challenges, including data
imbalance, insufficient controllability, and inconsistent musical quality.
DiffRhythm, a pioneering diffusion-based model, advanced the field by
generating full-length songs with expressive vocals and accompaniment. However,
its performance was constrained by an unbalanced model training dataset and
limited controllability over musical style, resulting in noticeable quality
disparities and restricted creative flexibility. To address these limitations,
we propose DiffRhythm+, an enhanced diffusion-based framework for controllable
and flexible full-length song generation. DiffRhythm+ leverages a substantially
expanded and balanced training dataset to mitigate issues such as repetition
and omission of lyrics, while also fostering the emergence of richer musical
skills and expressiveness. The framework introduces a multi-modal style
conditioning strategy, enabling users to precisely specify musical styles
through both descriptive text and reference audio, thereby significantly
enhancing creative control and diversity. We further introduce direct
performance optimization aligned with user preferences, guiding the model
toward consistently preferred outputs across evaluation metrics. Extensive
experiments demonstrate that DiffRhythm+ achieves significant improvements in
naturalness, arrangement complexity, and listener satisfaction over previous
systems.

</details>


### [13] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: 论文提出UniSLU框架，统一建模多个SLU任务，提升任务交互和数据集利用率，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SLU方法依赖独立模型架构，增加复杂性且限制跨任务交互，未能充分利用异构数据集。

Method: 提出统一表示和生成方法，联合建模ASR、spoken NER和SA任务，结合大语言模型。

Result: 在公开数据集上表现优于基准方法，适用于实际语音多媒体场景。

Conclusion: UniSLU框架有效提升SLU性能，代码和模型将开源以促进研究。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


### [14] [AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers with Multi-Scale and Multi-Task Learning](https://arxiv.org/abs/2507.12972)
*Daning Zhang,Ying Wei*

Main category: eess.AS

TL;DR: AVFSNet是一个结合音频和视觉信息的语音分离模型，用于处理未知说话人数量的混合信号，同时优化说话人计数和多说话人分离任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设已知说话人数量，而针对未知数量的研究泛化能力有限。AVFSNet旨在解决这一问题。

Method: AVFSNet采用多尺度编码和平行架构，结合视觉信息增强噪声适应性，并行分离每个说话人。

Result: 实验表明，AVFSNet在多个评估指标上达到最先进水平，并在不同数据集上表现优异。

Conclusion: AVFSNet有效解决了未知说话人数量场景下的语音分离问题，具有强大的泛化能力。

Abstract: Separating target speech from mixed signals containing flexible speaker
quantities presents a challenging task. While existing methods demonstrate
strong separation performance and noise robustness, they predominantly assume
prior knowledge of speaker counts in mixtures. The limited research addressing
unknown speaker quantity scenarios exhibits significantly constrained
generalization capabilities in real acoustic environments. To overcome these
challenges, this paper proposes AVFSNet -- an audio-visual speech separation
model integrating multi-scale encoding and parallel architecture -- jointly
optimized for speaker counting and multi-speaker separation tasks. The model
independently separates each speaker in parallel while enhancing environmental
noise adaptability through visual information integration. Comprehensive
experimental evaluations demonstrate that AVFSNet achieves state-of-the-art
results across multiple evaluation metrics and delivers outstanding performance
on diverse datasets.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [15] [Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates](https://arxiv.org/abs/2507.12563)
*Carlos De La Vega Martin,Rodrigo Diaz Fernandez,Mark Sandler*

Main category: cs.SD

TL;DR: 本文比较分析了基于神经网络的方法来解决非线性弹性板振动问题，用于物理建模音频合成，评估了多个最先进模型在自回归预测长序列方面的表现，并指出了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的有限差分和有限元等数值方法虽然精度高，但计算量大，限制了它们在实时音频应用中的使用。因此需要探索基于神经网络的替代方法来模拟非线性弹性板振动，以实现高效的物理建模音频合成。

Method: 对多个最先进的神经网络模型进行比较分析，这些模型在短序列上训练，然后以自回归方式预测长序列。评估了这些模型在非线性弹性板振动建模方面的性能表现。

Result: 研究揭示了现有神经网络方法的一些局限性，特别指出仅仅关注时域预测误差是不够的。这些发现对理解神经网络在物理建模中的适用性具有重要意义。

Conclusion: 当前的神经网络方法在建模非线性振动方面存在限制，需要改进神经网络方法来更好地处理非线性振动建模问题，以实现更有效的实时音频合成应用。

Abstract: Physical modelling synthesis aims to generate audio from physical simulations
of vibrating structures. Thin elastic plates are a common model for drum
membranes. Traditional numerical methods like finite differences and finite
elements offer high accuracy but are computationally demanding, limiting their
use in real-time audio applications. This paper presents a comparative analysis
of neural network-based approaches for solving the vibration of nonlinear
elastic plates. We evaluate several state-of-the-art models, trained on short
sequences, for prediction of long sequences in an autoregressive fashion. We
show some of the limitations of these models, and why is not enough to look at
the prediction error in the time domain. We discuss the implications for
real-time audio synthesis and propose future directions for improving neural
approaches to model nonlinear vibration.

</details>


### [16] [Task-Specific Audio Coding for Machines: Machine-Learned Latent Features Are Codes for That Machine](https://arxiv.org/abs/2507.12701)
*Anastasia Kuznetsova,Inseon Jang,Wootaek Lim,Minje Kim*

Main category: cs.SD

TL;DR: 本文提出了一种高效的音频编码方法（ACoM），专注于机器任务而非人类感知，通过任务特定损失和残差向量量化（RVQ）实现超低比特率（<200 bps），同时保持下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经音频编解码器注重高保真重建，而音频编码机器（ACoM）更关注高效压缩和下游任务性能，忽略感知细节。本文旨在填补这一需求。

Method: 采用任务特定损失和残差向量量化（RVQ）损失，压缩和量化已训练模型的中间特征表示，支持超低比特率和灵活部署。

Result: 在自动语音识别和音频分类任务中验证了方法的有效性，展示了广泛的适用性和性能保持能力。

Conclusion: 该方法为音频编码机器提供了一种高效、灵活的解决方案，适用于多种任务和架构。

Abstract: Neural audio codecs, leveraging quantization algorithms, have significantly
impacted various speech/audio tasks. While high-fidelity reconstruction is
paramount for human perception, audio coding for machines (ACoM) prioritizes
efficient compression and downstream task performance, disregarding perceptual
nuances. This work introduces an efficient ACoM method that can compress and
quantize any chosen intermediate feature representation of an already trained
speech/audio downstream model. Our approach employs task-specific loss guidance
alongside residual vector quantization (RVQ) losses, providing ultra-low
bitrates (i.e., less than 200 bps) with a minimal loss of the downstream model
performance. The resulting tokenizer is adaptable to various bitrates and model
sizes for flexible deployment. Evaluated on automatic speech recognition and
audio classification, our method demonstrates its efficacy and potential for
broader task and architectural applicability through appropriate
regularization.

</details>


### [17] [Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries](https://arxiv.org/abs/2507.12723)
*Minyoung Kim,Sehwan Park,Sungmin Cha,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 论文提出了一种跨模态水印框架，用于从合成的视听伪造中恢复真实音频并定位篡改，以对抗虚假信息。


<details>
  <summary>Details</summary>
Motivation: 现有的方法只能检测或定位视听伪造，无法恢复真实音频，限制了对抗虚假信息的效果。

Method: 提出了一种跨模态水印框架，将真实音频嵌入到视觉信息中，以便在伪造后恢复音频并定位篡改。

Result: 实验表明，该方法在恢复真实音频和定位篡改方面表现优异，对抗多种伪造手段（如语音克隆和唇同步）。

Conclusion: 该方法为视听伪造提供了有效的防御手段，能够恢复真实音频并定位篡改，对抗虚假信息。

Abstract: Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.

</details>


### [18] [Sample-Constrained Black Box Optimization for Audio Personalization](https://arxiv.org/abs/2507.12773)
*Rajalaxmi Rajagopalan,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: 论文提出了一种混合查询方法，结合用户对音频样本和单个滤波器元素的评分，通过稀疏高斯过程回归优化滤波器，以最大化用户满意度。


<details>
  <summary>Details</summary>
Motivation: 解决音频个性化中的黑盒优化问题，用户满意度函数未知，传统方法仅依赖用户对音频样本的评分，而混合查询方法能更高效地找到最优滤波器。

Method: 采用稀疏高斯过程回归（GPR），结合用户对音频样本和滤波器元素的评分（混合查询），逐步优化滤波器。

Result: 通过模拟和真实实验验证，混合查询方法优于单一查询方式，用户满意度显著提升。

Conclusion: 混合查询方法为黑盒优化开辟了新方向，适用于音频个性化及其他类似应用。

Abstract: We consider the problem of personalizing audio to maximize user experience.
Briefly, we aim to find a filter $h^*$, which applied to any music or speech,
will maximize the user's satisfaction. This is a black-box optimization problem
since the user's satisfaction function is unknown. Substantive work has been
done on this topic where the key idea is to play audio samples to the user,
each shaped by a different filter $h_i$, and query the user for their
satisfaction scores $f(h_i)$. A family of ``surrogate" functions is then
designed to fit these scores and the optimization method gradually refines
these functions to arrive at the filter $\hat{h}^*$ that maximizes
satisfaction. In certain applications, we observe that a second type of
querying is possible where users can tell us the individual elements $h^*[j]$
of the optimal filter $h^*$. Consider an analogy from cooking where the goal is
to cook a recipe that maximizes user satisfaction. A user can be asked to score
various cooked recipes (e.g., tofu fried rice) or to score individual
ingredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$
queries, where a query can be of either type, our goal is to find the recipe
that will maximize this user's satisfaction. Our proposal builds on Sparse
Gaussian Process Regression (GPR) and shows how a hybrid approach can
outperform any one type of querying. Our results are validated through
simulations and real world experiments, where volunteers gave feedback on
music/speech audio and were able to achieve high satisfaction levels. We
believe this idea of hybrid querying opens new problems in black-box
optimization and solutions can benefit other applications beyond audio
personalization.

</details>


### [19] [Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features](https://arxiv.org/abs/2507.12793)
*J. M. Chan Sri Manukalpa,H. S. Bopage,W. A. M. Jayawardena,P. K. P. G. Panduwawala*

Main category: cs.SD

TL;DR: 提出一种基于深度学习的非侵入式声学分类框架，用于早期白蚁检测，结合CNN和LSTM的混合模型，性能优于单独模型。


<details>
  <summary>Details</summary>
Motivation: 传统白蚁检测方法侵入性强且效率低，需开发非侵入式、高效的早期检测方案以减少经济损失。

Method: 采用混合CNN-LSTM架构，提取Mel频率倒谱系数，训练模型分类白蚁声信号。

Result: 模型准确率94.5%，精确率93.2%，召回率95.8%，假阴性率低。

Conclusion: 该研究为非侵入式白蚁检测提供自动化解决方案，未来可结合IoT实现实时预警。

Abstract: Structural pests, such as termites, pose a serious threat to wooden
buildings, resulting in significant economic losses due to their hidden and
progressive damage. Traditional detection methods, such as visual inspections
and chemical treatments, are invasive, labor intensive, and ineffective for
early stage infestations. To bridge this gap, this study proposes a non
invasive deep learning based acoustic classification framework for early
termite detection. We aim to develop a robust, scalable model that
distinguishes termite generated acoustic signals from background noise. We
introduce a hybrid Convolutional Neural Network Long Short Term Memory
architecture that captures both spatial and temporal features of termite
activity. Audio data were collected from termite infested and clean wooden
samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN
LSTM model to classify the signals. Experimental results show high performance,
with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis
reveals that the hybrid model outperforms standalone CNN and LSTM
architectures, underscoring its combined strength. Notably, the model yields
low false-negative rates, which is essential for enabling timely intervention.
This research contributes a non invasive, automated solution for early termite
detection, with practical implications for improved pest monitoring, minimized
structural damage, and better decision making by homeowners and pest control
professionals. Future work may integrate IoT for real time alerts and extend
detection to other structural pests.

</details>


### [20] [Autoregressive Speech Enhancement via Acoustic Tokens](https://arxiv.org/abs/2507.12825)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.SD

TL;DR: 论文研究了语音增强中离散表示（尤其是声学标记）的性能，提出了一种新型的自回归架构，实验表明声学标记在保留说话人身份方面优于语义标记，但仍不及连续表示。


<details>
  <summary>Details</summary>
Motivation: 提升真实世界录音的质量和清晰度是语音处理的关键，而离散表示（如声学标记）的研究仍有限，且现有方法常忽略声学细节和自回归建模的潜力。

Method: 1) 全面研究声学标记在语音增强中的性能，包括比特率和噪声强度的影响；2) 提出一种新型的基于转换器的自回归架构。

Result: 实验显示声学标记在保留说话人身份方面优于语义标记，自回归方法进一步提升了性能，但离散表示仍不及连续表示。

Conclusion: 离散表示在语音增强中具有潜力，但需进一步研究以缩小与连续表示的差距。

Abstract: In speech processing pipelines, improving the quality and intelligibility of
real-world recordings is crucial. While supervised regression is the primary
method for speech enhancement, audio tokenization is emerging as a promising
alternative for a smooth integration with other modalities. However, research
on speech enhancement using discrete representations is still limited. Previous
work has mainly focused on semantic tokens, which tend to discard key acoustic
details such as speaker identity. Additionally, these studies typically employ
non-autoregressive models, assuming conditional independence of outputs and
overlooking the potential improvements offered by autoregressive modeling. To
address these gaps we: 1) conduct a comprehensive study of the performance of
acoustic tokens for speech enhancement, including the effect of bitrate and
noise strength; 2) introduce a novel transducer-based autoregressive
architecture specifically designed for this task. Experiments on VoiceBank and
Libri1Mix datasets show that acoustic tokens outperform semantic tokens in
terms of preserving speaker identity, and that our autoregressive approach can
further improve performance. Nevertheless, we observe that discrete
representations still fall short compared to continuous ones, highlighting the
need for further research in this area.

</details>


### [21] [Best Practices and Considerations for Child Speech Corpus Collection and Curation in Educational, Clinical, and Forensic Scenarios](https://arxiv.org/abs/2507.12870)
*John Hansen,Satwik Dutta,Ellen Grand*

Main category: cs.SD

TL;DR: 研究旨在为儿童语音语料库的开发提供最佳实践和考虑因素，涵盖数据收集、协作、信任建立及语料质量检查。


<details>
  <summary>Details</summary>
Motivation: 儿童语音能力动态变化及数据隐私问题使得构建技术就绪的语音语料库具有挑战性，研究旨在填补这一空白。

Method: 描述数据收集的WHO、WHAT、WHEN、WHERE，提供协作与信任建立指南，并导航人类研究协议。

Result: 提供了语料库质量检查、分类和标注的指南。

Conclusion: 研究为儿童语音语料库的开发提供了全面的实践指导，适用于教育、临床和法医等领域。

Abstract: A child's spoken ability continues to change until their adult age. Until
7-8yrs, their speech sound development and language structure evolve rapidly.
This dynamic shift in their spoken communication skills and data privacy make
it challenging to curate technology-ready speech corpora for children. This
study aims to bridge this gap and provide researchers and practitioners with
the best practices and considerations for developing such a corpus based on an
intended goal. Although primarily focused on educational goals, applications of
child speech data have spread across fields including clinical and forensics
fields. Motivated by this goal, we describe the WHO, WHAT, WHEN, and WHERE of
data collection inspired by prior collection efforts and our
experience/knowledge. We also provide a guide to establish collaboration,
trust, and for navigating the human subjects research protocol. This study
concludes with guidelines for corpus quality check, triage, and annotation.

</details>


### [22] [Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes](https://arxiv.org/abs/2507.12932)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.SD

TL;DR: Enkidu是一个用户导向的隐私保护框架，通过黑盒知识和少量用户数据训练生成频率扰动，有效防御语音深度伪造攻击，同时保持高效和实时性。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造技术的快速发展威胁用户音频隐私，现有防御方法存在适应性差、计算成本高等问题。

Method: 利用黑盒知识和少量用户数据生成频率域噪声补丁，实现实时轻量级保护。

Result: Enkidu在内存和运行时间效率上显著优于现有方法，并在实验中表现出强大的防御能力。

Conclusion: Enkidu是一种高效、实用的解决方案，能够抵御多种语音深度伪造攻击。

Abstract: The rapid advancement of voice deepfake technologies has raised serious
concerns about user audio privacy, as attackers increasingly exploit publicly
available voice data to generate convincing fake audio for malicious purposes
such as identity theft, financial fraud, and misinformation campaigns. While
existing defense methods offer partial protection, they face critical
limitations, including weak adaptability to unseen user data, poor scalability
to long audio, rigid reliance on white-box knowledge, and high computational
and temporal costs during the encryption process. To address these challenges
and defend against personalized voice deepfake threats, we propose Enkidu, a
novel user-oriented privacy-preserving framework that leverages universal
frequential perturbations generated through black-box knowledge and few-shot
training on a small amount of user data. These highly malleable
frequency-domain noise patches enable real-time, lightweight protection with
strong generalization across variable-length audio and robust resistance to
voice deepfake attacks, all while preserving perceptual quality and speech
intelligibility. Notably, Enkidu achieves over 50 to 200 times processing
memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime
efficiency (real-time coefficient as low as 0.004) compared to six
state-of-the-art countermeasures. Extensive experiments across six mainstream
text-to-speech models and five cutting-edge automated speaker verification
models demonstrate the effectiveness, transferability, and practicality of
Enkidu in defending against both vanilla and adaptive voice deepfake attacks.

</details>


### [23] [Multi-Class-Token Transformer for Multitask Self-supervised Music Information Retrieval](https://arxiv.org/abs/2507.12996)
*Yuexuan Kong,Vincent Lostanlen,Romain Hennequin,Mathieu Lagrange,Gabriel Meseguer-Brocal*

Main category: cs.SD

TL;DR: 论文提出了一种结合对比学习和等变学习的自监督多任务学习方法（MT2），通过双类别令牌的ViT-1D架构，在音乐信息检索任务中优于单一任务模型。


<details>
  <summary>Details</summary>
Motivation: 解决对比学习和等变学习在音乐信息检索中的局限性：前者在标签任务（如乐器识别）表现好但结构化预测（如调性估计）效果差；后者在特定任务上表现优异但泛化能力不足。

Method: 采用双类别令牌的ViT-1D架构，同时优化对比学习（NT-Xent）和等变学习（CPSD）任务。

Result: MT2在多个任务上优于单一任务模型，且通过平均双令牌特征进一步提升了性能，参数更少（比MERT少18倍）。

Conclusion: 多类别令牌多任务学习方法在音乐信息检索中具有广泛适用性和高效性。

Abstract: Contrastive learning and equivariant learning are effective methods for
self-supervised learning (SSL) for audio content analysis. Yet, their
application to music information retrieval (MIR) faces a dilemma: the former is
more effective on tagging (e.g., instrument recognition) but less effective on
structured prediction (e.g., tonality estimation); The latter can match
supervised methods on the specific task it is designed for, but it does not
generalize well to other tasks. In this article, we adopt a best-of-both-worlds
approach by training a deep neural network on both kinds of pretext tasks at
once. The proposed new architecture is a Vision Transformer with 1-D
spectrogram patches (ViT-1D), equipped with two class tokens, which are
specialized to different self-supervised pretext tasks but optimized through
the same model: hence the qualification of self-supervised multi-class-token
multitask (MT2). The former class token optimizes cross-power spectral density
(CPSD) for equivariant learning over the circle of fifths, while the latter
optimizes normalized temperature-scaled cross-entropy (NT-Xent) for contrastive
learning. MT2 combines the strengths of both pretext tasks and outperforms
consistently both single-class-token ViT-1D models trained with either
contrastive or equivariant learning. Averaging the two class tokens further
improves performance on several tasks, highlighting the complementary nature of
the representations learned by each class token. Furthermore, using the same
single-linear-layer probing method on the features of last layer, MT2
outperforms MERT on all tasks except for beat tracking; achieving this with 18x
fewer parameters thanks to its multitasking capabilities. Our SSL benchmark
demonstrates the versatility of our multi-class-token multitask learning
approach for MIR applications.

</details>


### [24] [SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks](https://arxiv.org/abs/2507.13170)
*Kutub Uddin,Awais Khan,Muhammad Umar Farooq,Khalid Malik*

Main category: cs.SD

TL;DR: 论文提出了一种名为SHIELD的协作学习方法，用于防御生成式反取证（AF）攻击，通过集成辅助生成模型和三重模型来捕捉真实与AF攻击音频的关联，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 音频在多种应用中至关重要，但深度伪造音频的传播带来了严重风险。现有检测方法对生成式AF攻击的脆弱性促使了SHIELD的开发。

Method: 提出SHIELD方法，集成防御生成模型（DF）和三重模型，通过协作学习捕捉真实与AF攻击音频的关联。

Result: SHIELD显著提升了检测准确率，在ASVspoof2019、In-the-Wild和HalfTruth数据集上分别达到98.13%、98.58%和99.57%的平均准确率。

Conclusion: SHIELD方法有效防御生成式AF攻击，具有鲁棒性和高准确率，适用于多种生成模型。

Abstract: Audio plays a crucial role in applications like speaker verification,
voice-enabled smart devices, and audio conferencing. However, audio
manipulations, such as deepfakes, pose significant risks by enabling the spread
of misinformation. Our empirical analysis reveals that existing methods for
detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,
particularly those attacked using generative adversarial networks. In this
article, we propose a novel collaborative learning method called SHIELD to
defend against generative AF attacks. To expose AF signatures, we integrate an
auxiliary generative model, called the defense (DF) generative model, which
facilitates collaborative learning by combining input and output. Furthermore,
we design a triplet model to capture correlations for real and AF attacked
audios with real-generated and attacked-generated audios using auxiliary
generative models. The proposed SHIELD strengthens the defense against
generative AF attacks and achieves robust performance across various generative
models. The proposed AF significantly reduces the average detection accuracy
from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,
and from 98.41% to 51.18% for HalfTruth for three different generative models.
The proposed SHIELD mechanism is robust against AF attacks and achieves an
average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,
and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and
HalfTruth datasets, respectively.

</details>


### [25] [Voxtral](https://arxiv.org/abs/2507.13264)
*Alexander H. Liu,Andy Ehrenberg,Andy Lo,Clément Denoix,Corentin Barreau,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Sanchit Gandhi,Soham Ghosh,Srijan Mishra,Thomas Foubert,Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devendra Singh Chaplot,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Hadrien Chabran,Jessica Chudnovsky,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Romain Sauvestre,Roman Soletskyi,Sagar Vaze,Sandeep Subramanian,Saurabh Garg,Shashwat Dalal,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SD

TL;DR: Voxtral Mini和Voxtral Small是多模态音频聊天模型，支持语音和文本输入，性能优异且可本地运行。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时理解语音和文本的高效模型，并在音频和文本任务中保持领先性能。

Method: 训练多模态模型Voxtral，支持32K上下文窗口，处理长达40分钟的音频和多轮对话。

Result: Voxtral Small性能超越多个闭源模型，并发布三个语音理解评测基准。

Conclusion: Voxtral模型开源，适用于长音频和多轮对话，性能卓越。

Abstract: We present Voxtral Mini and Voxtral Small, two multimodal audio chat models.
Voxtral is trained to comprehend both spoken audio and text documents,
achieving state-of-the-art performance across a diverse range of audio
benchmarks, while preserving strong text capabilities. Voxtral Small
outperforms a number of closed-source models, while being small enough to run
locally. A 32K context window enables the model to handle audio files up to 40
minutes in duration and long multi-turn conversations. We also contribute three
benchmarks for evaluating speech understanding models on knowledge and trivia.
Both Voxtral models are released under Apache 2.0 license.

</details>

{"id": "2507.13463", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13463", "abs": "https://arxiv.org/abs/2507.13463", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "title": "Joint Motion, Angle, and Range Estimation in Near-Field under Array Calibration Imperfections", "comment": null, "summary": "Ultra-massive multiple-input multiple-output MIMO (UM-MIMO) leverages large\nantenna arrays at high frequencies, transitioning communication paradigm into\nthe radiative near-field (NF), where spherical wavefronts enable full-vector\nestimation of both target location and velocity. However, location and motion\nparameters become inherently coupled in this regime, making their joint\nestimation computationally demanding. To overcome this, we propose a novel\napproach that projects the received two-dimensional space-time signal onto the\nangle-Doppler domain using a two-dimensional discrete Fourier transform\n(2D-DFT). Our analysis reveals that the resulting angular spread is centered at\nthe target's true angle, with its width determined by the target's range.\nSimilarly, transverse motion induces a Doppler spread centered at the true\nradial velocity, with the width of Doppler spread proportional to the\ntransverse velocity. Exploiting these spectral characteristics, we develop a\nlow-complexity algorithm that provides coarse estimates of angle, range, and\nvelocity, which are subsequently refined using one-dimensional multiple signal\nclassification (MUSIC) applied independently to each parameter. The proposed\nmethod enables accurate and efficient estimation of NF target motion\nparameters. Simulation results demonstrate a normalized mean squared error\n(NMSE) of -40 dB for location and velocity estimates compared to maximum\nlikelihood estimation, while significantly reducing computational complexity."}
{"id": "2507.13520", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13520", "abs": "https://arxiv.org/abs/2507.13520", "authors": ["Sizhen Bian", "Mengxi Liu", "Paul Lukowicz"], "title": "Passive Body-Area Electrostatic Field (Human Body Capacitance) for Ubiquitous Computing", "comment": null, "summary": "Passive body-area electrostatic field sensing, also referred to as human body\ncapacitance (HBC), is an energy-efficient and non-intrusive sensing modality\nthat exploits the human body's inherent electrostatic properties to perceive\nhuman behaviors. This paper presents a focused overview of passive HBC sensing,\nincluding its underlying principles, historical evolution, hardware\narchitectures, and applications across research domains. Key challenges, such\nas susceptibility to environmental variation, are discussed to trigger\nmitigation techniques. Future research opportunities in sensor fusion and\nhardware enhancement are highlighted. To support continued innovation, this\nwork provides open-source resources and aims to empower researchers and\ndevelopers to leverage passive electrostatic sensing for next-generation\nwearable and ambient intelligence systems."}
{"id": "2507.13526", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13526", "abs": "https://arxiv.org/abs/2507.13526", "authors": ["Gedeon Ghislain Nkwewo Ngoufo", "Khaled Humadi", "Elham Baladi", "Gunes Karabulut Kurt"], "title": "Space Shift Keying-Enabled ISAC for Efficient Debris Detection and Communication in LEO Satellite Networks", "comment": null, "summary": "The proliferation of space debris in low Earth orbit (LEO) presents critical\nchallenges for orbital safety, particularly for satellite constellations.\nIntegrated sensing and communication (ISAC) systems provide a promising dual\nfunction solution by enabling both environmental sensing and data\ncommunication. This study explores the use of space shift keying (SSK)\nmodulation within ISAC frameworks, evaluating its performance when combined\nwith sinusoidal and chirp radar waveforms. SSK is particularly attractive due\nto its low hardware complexity and robust communication performance. Our\nresults demonstrate that both waveforms achieve comparable bit error rate (BER)\nperformance under SSK, validating its effectiveness for ISAC applications.\nHowever, waveform selection significantly affects sensing capability: while the\nsinusoidal waveform supports simpler implementation, its high ambiguity limits\nrange detection. In contrast, the chirp waveform enables range estimation and\nprovides a modest improvement in velocity detection accuracy. These findings\nhighlight the strength of SSK as a modulation scheme for ISAC and emphasize the\nimportance of selecting appropriate waveforms to optimize sensing accuracy\nwithout compromising communication performance. This insight supports the\ndesign of efficient and scalable ISAC systems for space applications,\nparticularly in the context of orbital debris monitoring."}
{"id": "2507.13554", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13554", "abs": "https://arxiv.org/abs/2507.13554", "authors": ["Meles Weldegebriel", "Zihan Li", "Dustin Maas", "Greg Hellbourg", "Ning Zhang", "Neal Patwari"], "title": "Sensing and Stopping Interfering Secondary Users: Validation of an Efficient Spectrum Sharing System", "comment": null, "summary": "We present the design and validation of Stoppable Secondary Use (StopSec), a\nprivacy-preserving protocol with the capability to identify a secondary user\n(SU) causing interference to a primary user (PU) and to act quickly to stop the\ninterference. All users are served by a database that provides a feedback\nmechanism from a PU to an interfering SU. We introduce a new lightweight and\nrobust method to watermark an SU's OFDM packet. Through extensive over-the-air\nreal-time experiments, we evaluate StopSec in terms of interference detection,\nidentification, and stopping latency, as well as impact on SUs. We show that\nthe watermarking method avoids negative impact to the secondary data link and\nis robust to real-world time-varying channels. Interfering SUs can be stopped\nin under 150 milliseconds, and when multiple users are simultaneously\ninterfering, they can all be stopped. Even when the interference is 10 dB lower\nthan the noise power, StopSec successfully stops interfering SUs within a few\nseconds of their appearance in the channel. StopSec can be an effective\nspectrum sharing protocol for cases when interference to a PU must be quickly\nand automatically stopped."}
{"id": "2507.13626", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.13626", "abs": "https://arxiv.org/abs/2507.13626", "authors": ["Cheng-Hung Hu", "Yusuke Yasud", "Akifumi Yoshimoto", "Tomoki Toda"], "title": "Unifying Listener Scoring Scales: Comparison Learning Framework for Speech Quality Assessment and Continuous Speech Emotion Recognition", "comment": "Accepted to Interspeech 2025", "summary": "Speech Quality Assessment (SQA) and Continuous Speech Emotion Recognition\n(CSER) are two key tasks in speech technology, both relying on listener\nratings. However, these ratings are inherently biased due to individual\nlistener factors. Previous approaches have introduced a mean listener scoring\nscale and modeled all listener scoring scales in the training set. However, the\nmean listener approach is prone to distortion from averaging ordinal data,\nleading to potential biases. Moreover, learning multiple listener scoring\nscales while inferring based only on the mean listener scale limits\neffectiveness. In contrast, our method focuses on modeling a unified listener\nscoring scale, using comparison scores to correctly capture the scoring\nrelationships between utterances. Experimental results show that our method\neffectively improves prediction performance in both SQA and CSER tasks, proving\nits effectiveness and robustness."}
{"id": "2507.13572", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13572", "abs": "https://arxiv.org/abs/2507.13572", "authors": ["Yixiao Zhang", "Haonan Chen", "Ju-Chiang Wang", "Jitong Chen"], "title": "Temporal Adaptation of Pre-trained Foundation Models for Music Structure Analysis", "comment": "Accepted to WASPAA 2025. Project Page:\n  https://sites.google.com/view/temporal-adaptation-for-msa/", "summary": "Audio-based music structure analysis (MSA) is an essential task in Music\nInformation Retrieval that remains challenging due to the complexity and\nvariability of musical form. Recent advances highlight the potential of\nfine-tuning pre-trained music foundation models for MSA tasks. However, these\nmodels are typically trained with high temporal feature resolution and short\naudio windows, which limits their efficiency and introduces bias when applied\nto long-form audio. This paper presents a temporal adaptation approach for\nfine-tuning music foundation models tailored to MSA. Our method enables\nefficient analysis of full-length songs in a single forward pass by\nincorporating two key strategies: (1) audio window extension and (2)\nlow-resolution adaptation. Experiments on the Harmonix Set and RWC-Pop datasets\nshow that our method significantly improves both boundary detection and\nstructural function prediction, while maintaining comparable memory usage and\ninference speed."}
{"id": "2507.13637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13637", "abs": "https://arxiv.org/abs/2507.13637", "authors": ["Jun Jiang", "Yuan Gao", "Xinyi Wu", "Shugong Xu"], "title": "Towards channel foundation models (CFMs): Motivations, methodologies and opportunities", "comment": "13 pages", "summary": "Artificial intelligence (AI) has emerged as a pivotal enabler for\nnext-generation wireless communication systems. However, conventional AI-based\nmodels encounter several limitations, such as heavy reliance on labeled data,\nlimited generalization capability, and task-specific design. To address these\nchallenges, this paper introduces, for the first time, the concept of channel\nfoundation models (CFMs)-a novel and unified framework designed to tackle a\nwide range of channel-related tasks through a pretrained, universal channel\nfeature extractor. By leveraging advanced AI architectures and self-supervised\nlearning techniques, CFMs are capable of effectively exploiting large-scale\nunlabeled data without the need for extensive manual annotation. We further\nanalyze the evolution of AI methodologies, from supervised learning and\nmulti-task learning to self-supervised learning, emphasizing the distinct\nadvantages of the latter in facilitating the development of CFMs. Additionally,\nwe provide a comprehensive review of existing studies on self-supervised\nlearning in this domain, categorizing them into generative, discriminative and\nthe combined paradigms. Given that the research on CFMs is still at an early\nstage, we identify several promising future research directions, focusing on\nmodel architecture innovation and the construction of high-quality, diverse\nchannel datasets."}
{"id": "2507.14044", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14044", "abs": "https://arxiv.org/abs/2507.14044", "authors": ["Tsun-An Hsieh", "Minje Kim"], "title": "TGIF: Talker Group-Informed Familiarization of Target Speaker Extraction", "comment": null, "summary": "State-of-the-art target speaker extraction (TSE) systems are typically\ndesigned to generalize to any given mixing environment, necessitating a model\nwith a large enough capacity as a generalist. Personalized speech enhancement\ncould be a specialized solution that adapts to single-user scenarios, but it\noverlooks the practical need for customization in cases where only a small\nnumber of talkers are involved, e.g., TSE for a specific family. We address\nthis gap with the proposed concept, talker group-informed familiarization\n(TGIF) of TSE, where the TSE system specializes in a particular group of users,\nwhich is challenging due to the inherent absence of a clean speech target. To\nthis end, we employ a knowledge distillation approach, where a group-specific\nstudent model learns from the pseudo-clean targets generated by a large teacher\nmodel. This tailors the student model to effectively extract the target speaker\nfrom the particular talker group while maintaining computational efficiency.\nExperimental results demonstrate that our approach outperforms the baseline\ngeneric models by adapting to the unique speech characteristics of a given\nspeaker group. Our newly proposed TGIF concept underscores the potential of\ndeveloping specialized solutions for diverse and real-world applications, such\nas on-device TSE on a family-owned device."}
{"id": "2507.13863", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13863", "abs": "https://arxiv.org/abs/2507.13863", "authors": ["Eric Grinstein", "Ashutosh Pandey", "Cole Li", "Shanmukha Srinivas", "Juan Azcarreta", "Jacob Donley", "Sanha Lee", "Ali Aroudi", "Cagdas Bilen"], "title": "Controlling the Parameterized Multi-channel Wiener Filter using a tiny neural network", "comment": "Accepted to WASPAA 2025", "summary": "Noise suppression and speech distortion are two important aspects to be\nbalanced when designing multi-channel Speech Enhancement (SE) algorithms.\nAlthough neural network models have achieved state-of-the-art noise\nsuppression, their non-linear operations often introduce high speech\ndistortion. Conversely, classical signal processing algorithms such as the\nParameterized Multi-channel Wiener Filter ( PMWF) beamformer offer explicit\nmechanisms for controlling the suppression/distortion trade-off. In this work,\nwe present NeuralPMWF, a system where the PMWF is entirely controlled using a\nlow-latency, low-compute neural network, resulting in a low-complexity system\noffering high noise reduction and low speech distortion. Experimental results\nshow that our proposed approach results in significantly better perceptual and\nobjective speech enhancement in comparison to several competitive baselines\nusing similar computational resources."}
{"id": "2507.13748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13748", "abs": "https://arxiv.org/abs/2507.13748", "authors": ["Patrick Matalla", "Joel Dittmer", "Md Salek Mahmud", "Christian Koos", "Sebastian Randel"], "title": "Elastic Buffer Design for Real-Time All-Digital Clock Recovery Enabling Free-Running Receiver Clock with Negative and Positive Clock Frequency Offsets", "comment": null, "summary": "We present an elastic buffer design that enables all-digital clock recovery\nimplementation with free-running receiver clock featuring negative and positive\nclock frequency offsets. Error-free real-time data transmission is demonstrated\nfrom -400 ppm to +400 ppm."}
{"id": "2507.13572", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13572", "abs": "https://arxiv.org/abs/2507.13572", "authors": ["Yixiao Zhang", "Haonan Chen", "Ju-Chiang Wang", "Jitong Chen"], "title": "Temporal Adaptation of Pre-trained Foundation Models for Music Structure Analysis", "comment": "Accepted to WASPAA 2025. Project Page:\n  https://sites.google.com/view/temporal-adaptation-for-msa/", "summary": "Audio-based music structure analysis (MSA) is an essential task in Music\nInformation Retrieval that remains challenging due to the complexity and\nvariability of musical form. Recent advances highlight the potential of\nfine-tuning pre-trained music foundation models for MSA tasks. However, these\nmodels are typically trained with high temporal feature resolution and short\naudio windows, which limits their efficiency and introduces bias when applied\nto long-form audio. This paper presents a temporal adaptation approach for\nfine-tuning music foundation models tailored to MSA. Our method enables\nefficient analysis of full-length songs in a single forward pass by\nincorporating two key strategies: (1) audio window extension and (2)\nlow-resolution adaptation. Experiments on the Harmonix Set and RWC-Pop datasets\nshow that our method significantly improves both boundary detection and\nstructural function prediction, while maintaining comparable memory usage and\ninference speed."}
{"id": "2507.14129", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14129", "abs": "https://arxiv.org/abs/2507.14129", "authors": ["Shikhar Bharadwaj", "Samuele Cornell", "Kwanghee Choi", "Satoru Fukayama", "Hye-jin Shim", "Soham Deshmukh", "Shinji Watanabe"], "title": "OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder", "comment": null, "summary": "Masked token prediction has emerged as a powerful pre-training objective\nacross language, vision, and speech, offering the potential to unify these\ndiverse modalities through a single pre-training task. However, its application\nfor general audio understanding remains underexplored, with BEATs being the\nonly notable example. BEATs has seen limited modifications due to the absence\nof open-source pre-training code. Furthermore, BEATs was trained only on\nAudioSet, restricting its broader downstream applicability. To address these\ngaps, we present OpenBEATs, an open-source framework that extends BEATs via\nmulti-domain audio pre-training. We conduct comprehensive evaluations across\nsix types of tasks, twenty five datasets, and three audio domains, including\naudio reasoning tasks such as audio question answering, entailment, and\ncaptioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics\ndatasets, two environmental sound datasets and five reasoning datasets,\nperforming better than models exceeding a billion parameters at one-fourth\ntheir parameter size. These results demonstrate the effectiveness of\nmulti-domain datasets and masked token prediction task to learn general-purpose\naudio representations. To promote further research and reproducibility, we\nrelease all pre-training and evaluation code, pretrained and fine-tuned\ncheckpoints, and training logs at https://shikhar-s.github.io/OpenBEATs"}
{"id": "2507.13766", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13766", "abs": "https://arxiv.org/abs/2507.13766", "authors": ["Kai Wu", "Zhongqin Wang", "Shu-Lin Chen", "J. Andrew Zhang", "Y. Jay Guo"], "title": "ISAC: From Human to Environmental Sensing", "comment": "15 pages, 8 figures", "summary": "Integrated Sensing and Communications (ISAC) is poised to become one of the\ndefining capabilities of the sixth generation (6G) wireless communications\nsystems, enabling the network infrastructure to jointly support high-throughput\ncommunications and situational awareness. While recent advances have explored\nISAC for both human-centric applications and environmental monitoring, existing\nresearch remains fragmented across these domains. This paper provides the first\nunified review of ISAC-enabled sensing for both human activities and\nenvironment, focusing on signal-level mechanisms, sensing features, and\nreal-world feasibility. We begin by characterising how diverse physical\nphenomena, ranging from human vital sign and motion to precipitation and flood\ndynamics, impact wireless signal propagation, producing measurable signatures\nin channel state information (CSI), Doppler profiles, and signal statistics. A\ncomprehensive analysis is then presented across two domains: human sensing\napplications including localisation, activity recognition, and vital sign\nmonitoring; and environmental sensing for rainfall, soil moisture, and water\nlevel. Experimental results from Long-Term Evolution (LTE) sensing under\nnon-line-of-sight (NLOS) conditions are incorporated to highlight the\nfeasibility in infrastructure-limited scenarios. Open challenges in signal\nfusion, domain adaptation, and generalisable sensing architectures are\ndiscussed to facilitate future research toward scalable and autonomous ISAC."}
{"id": "2507.13863", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13863", "abs": "https://arxiv.org/abs/2507.13863", "authors": ["Eric Grinstein", "Ashutosh Pandey", "Cole Li", "Shanmukha Srinivas", "Juan Azcarreta", "Jacob Donley", "Sanha Lee", "Ali Aroudi", "Cagdas Bilen"], "title": "Controlling the Parameterized Multi-channel Wiener Filter using a tiny neural network", "comment": "Accepted to WASPAA 2025", "summary": "Noise suppression and speech distortion are two important aspects to be\nbalanced when designing multi-channel Speech Enhancement (SE) algorithms.\nAlthough neural network models have achieved state-of-the-art noise\nsuppression, their non-linear operations often introduce high speech\ndistortion. Conversely, classical signal processing algorithms such as the\nParameterized Multi-channel Wiener Filter ( PMWF) beamformer offer explicit\nmechanisms for controlling the suppression/distortion trade-off. In this work,\nwe present NeuralPMWF, a system where the PMWF is entirely controlled using a\nlow-latency, low-compute neural network, resulting in a low-complexity system\noffering high noise reduction and low speech distortion. Experimental results\nshow that our proposed approach results in significantly better perceptual and\nobjective speech enhancement in comparison to several competitive baselines\nusing similar computational resources."}
{"id": "2507.13626", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.13626", "abs": "https://arxiv.org/abs/2507.13626", "authors": ["Cheng-Hung Hu", "Yusuke Yasud", "Akifumi Yoshimoto", "Tomoki Toda"], "title": "Unifying Listener Scoring Scales: Comparison Learning Framework for Speech Quality Assessment and Continuous Speech Emotion Recognition", "comment": "Accepted to Interspeech 2025", "summary": "Speech Quality Assessment (SQA) and Continuous Speech Emotion Recognition\n(CSER) are two key tasks in speech technology, both relying on listener\nratings. However, these ratings are inherently biased due to individual\nlistener factors. Previous approaches have introduced a mean listener scoring\nscale and modeled all listener scoring scales in the training set. However, the\nmean listener approach is prone to distortion from averaging ordinal data,\nleading to potential biases. Moreover, learning multiple listener scoring\nscales while inferring based only on the mean listener scale limits\neffectiveness. In contrast, our method focuses on modeling a unified listener\nscoring scale, using comparison scores to correctly capture the scoring\nrelationships between utterances. Experimental results show that our method\neffectively improves prediction performance in both SQA and CSER tasks, proving\nits effectiveness and robustness."}
{"id": "2507.13826", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.13826", "abs": "https://arxiv.org/abs/2507.13826", "authors": ["Kimitaka Sumi", "Takuya Sakamoto"], "title": "Simulation for Noncontact Radar-Based Physiological Sensing Using Depth-Camera-Derived Human 3D Model with Electromagnetic Scattering Analysis", "comment": "10 pages, 9 figures, 6 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study proposes a method for simulating signals received by\nfrequency-modulated continuous-wave radar during respiratory monitoring, using\nhuman body geometry and displacement data acquired via a depth camera. Unlike\nprevious studies that rely on simplified models of body geometry or\ndisplacement, the proposed approach models high-frequency scattering centers\nbased on realistic depth-camera-measured body shapes and motions. Experiments\nwere conducted with six participants under varying conditions, including\nvarying target distances, seating orientations, and radar types, with\nsimultaneous acquisition from the radar and depth camera. Relative to\nconventional model-based methods, the proposed technique achieved improvements\nof 7.5%, 58.2%, and 3.2% in the correlation coefficients of radar images,\ndisplacements, and spectrograms, respectively. This work contributes to the\ngeneration of radar-based physiological datasets through simulation and\nenhances our understanding of factors affecting the accuracy of non-contact\nsensing."}
{"id": "2507.14129", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14129", "abs": "https://arxiv.org/abs/2507.14129", "authors": ["Shikhar Bharadwaj", "Samuele Cornell", "Kwanghee Choi", "Satoru Fukayama", "Hye-jin Shim", "Soham Deshmukh", "Shinji Watanabe"], "title": "OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder", "comment": null, "summary": "Masked token prediction has emerged as a powerful pre-training objective\nacross language, vision, and speech, offering the potential to unify these\ndiverse modalities through a single pre-training task. However, its application\nfor general audio understanding remains underexplored, with BEATs being the\nonly notable example. BEATs has seen limited modifications due to the absence\nof open-source pre-training code. Furthermore, BEATs was trained only on\nAudioSet, restricting its broader downstream applicability. To address these\ngaps, we present OpenBEATs, an open-source framework that extends BEATs via\nmulti-domain audio pre-training. We conduct comprehensive evaluations across\nsix types of tasks, twenty five datasets, and three audio domains, including\naudio reasoning tasks such as audio question answering, entailment, and\ncaptioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics\ndatasets, two environmental sound datasets and five reasoning datasets,\nperforming better than models exceeding a billion parameters at one-fourth\ntheir parameter size. These results demonstrate the effectiveness of\nmulti-domain datasets and masked token prediction task to learn general-purpose\naudio representations. To promote further research and reproducibility, we\nrelease all pre-training and evaluation code, pretrained and fine-tuned\ncheckpoints, and training logs at https://shikhar-s.github.io/OpenBEATs"}
{"id": "2507.13829", "categories": ["eess.SP", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.13829", "abs": "https://arxiv.org/abs/2507.13829", "authors": ["Arnaud Poinas", "Rémi Bardenet"], "title": "On two fundamental properties of the zeros of spectrograms of noisy signals", "comment": null, "summary": "The spatial distribution of the zeros of the spectrogram is significantly\naltered when a signal is added to white Gaussian noise. The zeros tend to\ndelineate the support of the signal, and deterministic structures form in the\npresence of interference, as if the zeros were trapped. While sophisticated\nmethods have been proposed to detect signals as holes in the pattern of\nspectrogram zeros, few formal arguments have been made to support the\ndelineation and trapping effects. Through detailed computations for simple toy\nsignals, we show that two basic mathematical arguments, the intensity of zeros\nand Rouch\\'e's theorem, allow discussing delineation and trapping, and the\ninfluence of parameters like the signal-to-noise ratio. In particular,\ninterfering chirps, even nearly superimposed, yield an easy-to-detect\ndeterministic structure among zeros."}
{"id": "2507.13938", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13938", "abs": "https://arxiv.org/abs/2507.13938", "authors": ["Hyun Seok Lee"], "title": "Device-Free Localization Using Commercial UWB Transceivers", "comment": "8 pages, 10 figures, preprint", "summary": "Recently, commercial ultra-wideband (UWB) transceivers have enabled not only\nmeasuring device-to-device distance but also tracking the position of a\npedestrian who does not carry a UWB device. UWB-based device-free localization\nthat does not require dedicated radar equipment is compatible with existing\nanchor infrastructure and can be reused to reduce hardware deployment costs.\nHowever, it is difficult to estimate the target's position accurately in\nreal-world scenarios due to the low signal-to-noise ratio (SNR) and the\ncluttered environment. In this paper, we propose a deep learning (DL)-assisted\nparticle filter to overcome these challenges. First, the channel impulse\nresponse (CIR) variance is analyzed to capture the variability induced by the\ntarget's movement. Then, a DL-based one-dimensional attention U-Net is used to\nextract only the reflection components caused by the target and suppress the\nnoise components within the CIR variance profile. Finally, multiple\npreprocessed CIR variance profiles are used as input to a particle filter to\nestimate the target's position. Experimental results demonstrate that the\nproposed system is a practical and cost-effective solution for IoT and\nautomotive applications with a root mean square error (RMSE) of about 15 cm and\nan average processing time of 4 ms. Furthermore, comparisons with existing\nstate-of-the-art methods show that the proposed method provides the best\nperformance with reasonable computational costs."}
{"id": "2507.14018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14018", "abs": "https://arxiv.org/abs/2507.14018", "authors": ["Zeyuan Zhang", "Yue Xiu", "Phee Lep Yeoh", "Guangyi Liu", "Zixing Wu", "Ning Wei"], "title": "Distortion-Aware Hybrid Beamforming for Integrated Sensing and Communication", "comment": null, "summary": "This paper investigates a practical partially-connected hybrid beamforming\ntransmitter for integrated sensing and communication (ISAC) with distortion\nfrom nonlinear power amplification. For this ISAC system, we formulate a\ncommunication rate and sensing mutual information maximization problem driven\nby our distortion-aware hybrid beamforming design. To address this non-convex\nproblem, we first solve for a fully digital beamforming matrix by alternatively\nsolving three sub-problems using manifold optimization (MO) and our derived\nclosed-form solutions. The analog and digital beamforming matrices are then\nobtained through a decomposition algorithm. Numerical results demonstrate that\nthe proposed algorithm can improve overall ISAC performance compared to\ntraditional beamforming methods."}
{"id": "2507.14035", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14035", "abs": "https://arxiv.org/abs/2507.14035", "authors": ["Sai Xu", "Kai-Kit Wong", "Yanan Du", "Hanjiang Hong", "Chan-Byoung Chae", "Baiyang Liu", "Kin-Fai Tong"], "title": "Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and Software for Port Selection and Beamforming", "comment": null, "summary": "This paper proposes a hardware-software co-design approach to efficiently\noptimize beamforming and port selection in fluid antenna systems (FASs). To\nbegin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input\nmultiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)\nmaximization problem is formulated. Second, a method that integrates graph\nneural networks (GNNs) with random port selection (RPS) is proposed to jointly\noptimize beamforming and port selection, while also assessing the benefits and\nlimitations of random selection. Third, an instruction-driven deep learning\naccelerator based on a field-programmable gate array (FPGA) is developed to\nminimize inference latency. To further enhance efficiency, a scheduling\nalgorithm is introduced to reduce redundant computations and minimize the idle\ntime of computing cores. Simulation results demonstrate that the proposed\nGNN-RPS approach achieves competitive communication performance. Furthermore,\nexperimental evaluations indicate that the FPGA-based accelerator maintains low\nlatency while simultaneously executing beamforming inference for multiple port\nselections."}

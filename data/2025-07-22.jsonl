{"id": "2507.14346", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.14346", "abs": "https://arxiv.org/abs/2507.14346", "authors": ["Xuanru Zhou", "Jiachen Lian", "Cheol Jun Cho", "Tejas Prabhune", "Shuhe Li", "William Li", "Rodrigo Ortiz", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Towards Accurate Phonetic Error Detection Through Phoneme Similarity Modeling", "comment": "2025 Interspeech", "summary": "Phonetic error detection, a core subtask of automatic pronunciation\nassessment, identifies pronunciation deviations at the phoneme level. Speech\nvariability from accents and dysfluencies challenges accurate phoneme\nrecognition, with current models failing to capture these discrepancies\neffectively. We propose a verbatim phoneme recognition framework using\nmulti-task training with novel phoneme similarity modeling that transcribes\nwhat speakers actually say rather than what they're supposed to say. We develop\nand open-source \\textit{VCTK-accent}, a simulated dataset containing phonetic\nerrors, and propose two novel metrics for assessing pronunciation differences.\nOur work establishes a new benchmark for phonetic error detection."}
{"id": "2507.14451", "categories": ["eess.AS", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.14451", "abs": "https://arxiv.org/abs/2507.14451", "authors": ["Satwik Dutta", "Shruthigna Chandupatla", "John Hansen"], "title": "Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition of Children for On-device Edge Applications", "comment": "5 pages, 5 figures, accepted for presentation at the 2025 Workshop on\n  Child Computer Interaction (WOCCI 2025), a Satellite Workshop of the 2025\n  Interspeech Conference", "summary": "Reliability on cloud providers for ASR inference to support child-centered\nvoice-based applications is becoming challenging due to regulatory and privacy\nchallenges. Motivated by a privacy-preserving design, this study aims to\ndevelop a lightweight & efficient Whisper ASR system capable of running on a\nRaspberry Pi. Upon evaluation of the MyST corpus and by examining various\nfiltering strategies to fine-tune the `tiny.en' model, a Word Error Rate (WER)\nof 15.9% was achieved (11.8% filtered). A low-rank compression reduces the\nencoder size by 0.51M with 1.26x faster inference in GPU, with 11% relative WER\nincrease. During inference on Pi, the compressed version required ~2 GFLOPS\nfewer computations. The RTF for both the models ranged between [0.23-0.41] for\nvarious input audio durations. Analyzing the RAM usage and CPU temperature\nshowed that the PI was capable of handling both the tiny models, however it was\nnoticed that small models initiated additional overhead/thermal throttling."}
{"id": "2507.14534", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.14534", "abs": "https://arxiv.org/abs/2507.14534", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "comment": null, "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo."}
{"id": "2507.14898", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14898", "abs": "https://arxiv.org/abs/2507.14898", "authors": ["Susmita Bhattacharjee", "Jagabandhu Mishra", "H. S. Shekhawat", "S. R. Mahadeva Prasanna"], "title": "Parameter-Efficient Fine-Tuning of Foundation Models for CLP Speech Classification", "comment": "6 pages, 5 figures, conference", "summary": "We propose the use of parameter-efficient fine-tuning (PEFT) of foundation\nmodels for cleft lip and palate (CLP) detection and severity classification. In\nCLP, nasalization increases with severity due to the abnormal passage between\nthe oral and nasal tracts; this causes oral stops to be replaced by glottal\nstops and alters formant trajectories and vowel space. Since foundation models\nare trained for grapheme prediction or long-term quantized representation\nprediction, they may better discriminate CLP severity when fine-tuned on\ndomain-specific data. We conduct experiments on two datasets: English (NMCPC)\nand Kannada (AIISH). We perform a comparative analysis using embeddings from\nself-supervised models Wav2Vec2 and WavLM, and the weakly supervised Whisper,\neach paired with SVM classifiers, and compare them with traditional handcrafted\nfeatures eGeMAPS and ComParE. Finally, we fine-tune the best-performing Whisper\nmodel using PEFT techniques: Low-Rank Adapter (LoRA) and Decomposed Low-Rank\nAdapter (DoRA). Our results demonstrate that the proposed approach achieves\nrelative improvements of 26.4% and 63.4% in macro-average F1 score over the\nbest foundation model and handcrafted feature baselines on the NMCPC dataset,\nand improvements of 6.1% and 52.9% on the AIISH dataset, respectively."}
{"id": "2507.14237", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14237", "abs": "https://arxiv.org/abs/2507.14237", "authors": ["Louis Bahrman", "Mathieu Fontaine", "Gaël Richard"], "title": "U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model", "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "This paper explores the outcome of training state-ofthe-art dereverberation\nmodels with supervision settings ranging from weakly-supervised to fully\nunsupervised, relying solely on reverberant signals and an acoustic model for\ntraining. Most of the existing deep learning approaches typically require\npaired dry and reverberant data, which are difficult to obtain in practice. We\ndevelop instead a sequential learning strategy motivated by a bayesian\nformulation of the dereverberation problem, wherein acoustic parameters and dry\nsignals are estimated from reverberant inputs using deep neural networks,\nguided by a reverberation matching loss. Our most data-efficient variant\nrequires only 100 reverberation-parameter-labelled samples to outperform an\nunsupervised baseline, demonstrating the effectiveness and practicality of the\nproposed method in low-resource scenarios."}
{"id": "2507.14138", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14138", "abs": "https://arxiv.org/abs/2507.14138", "authors": ["Vaishnavi C K", "Sricharan Vijayarangan", "Sri Gayathri G", "Danush Adhithya N", "Alex Joseph", "Preejith SP", "Mohanasankar Sivaprakasam"], "title": "Optimizing VO2max Prediction in Gamified Cardiac Assessment: Leveraging Effective Feature Selection and Refined Protocols for Robust Models", "comment": "Accepted and Presented in IEEE IECBES 2024", "summary": "VO2max is a critical indicator of cardiopulmonary fitness, reflecting the\nmaximum amount of oxygen the body can utilize during intense exercise.\nAccurately measuring VO2max is essential for assessing cardiovascular health\nand predicting outcomes in clinical settings. However, current methods for\nVO2max estimation, such as Cardiopulmonary Exercise Testing (CPET), require\nexpensive equipment and the supervision of trained personnel, limiting\naccessibility for large-scale screening. Preliminary efforts have been made to\ncreate a more accessible method, such as the Cardiopulmonary Spot Jog Test\n(CPSJT). Unfortunately, these early attempts yielded high error margins,\nrendering them unsuitable for widespread use. In our study, we address these\nshortcomings by refining the CPSJT protocol to improve prediction accuracy. A\ncrucial contribution is improved feature extraction which include gender, body\nmass index, aerobic duration, and anaerobic duration. This targeted approach\nhelps in streamlining the model to enhance prediction precision while\nminimizing the risk of overfitting. In a cohort of 44 participants from the\nIndian population, we assessed the performance of various machine learning\nmodels using these features. With Stratified 5-Fold Cross-Validation, the Root\nMean Squared Error (RMSE) values were 5.78 for Linear Regression, 5.15 for\nRandom Forest, and 5.17 for Support Vector Regression. All models demonstrated\nstrong test correlations and low RMSE values, underscoring their robust and\nreliable performance."}
{"id": "2507.14988", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14988", "abs": "https://arxiv.org/abs/2507.14988", "authors": ["Yinghao Aaron Li", "Xilin Jiang", "Fei Tao", "Cheng Niu", "Kaifeng Xu", "Juntong Song", "Nima Mesgarani"], "title": "DMOSpeech 2: Reinforcement Learning for Duration Prediction in Metric-Optimized Speech Synthesis", "comment": null, "summary": "Diffusion-based text-to-speech (TTS) systems have made remarkable progress in\nzero-shot speech synthesis, yet optimizing all components for perceptual\nmetrics remains challenging. Prior work with DMOSpeech demonstrated direct\nmetric optimization for speech generation components, but duration prediction\nremained unoptimized. This paper presents DMOSpeech 2, which extends metric\noptimization to the duration predictor through a reinforcement learning\napproach. The proposed system implements a novel duration policy framework\nusing group relative preference optimization (GRPO) with speaker similarity and\nword error rate as reward signals. By optimizing this previously unoptimized\ncomponent, DMOSpeech 2 creates a more complete metric-optimized synthesis\npipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid\napproach leveraging a teacher model for initial denoising steps before\ntransitioning to the student model, significantly improving output diversity\nwhile maintaining efficiency. Comprehensive evaluations demonstrate superior\nperformance across all metrics compared to previous systems, while reducing\nsampling steps by half without quality degradation. These advances represent a\nsignificant step toward speech synthesis systems with metric optimization\nacross multiple components. The audio samples, code and pre-trained models are\navailable at https://dmospeech2.github.io/."}
{"id": "2507.14638", "categories": ["cs.SD", "eess.AS", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.14638", "abs": "https://arxiv.org/abs/2507.14638", "authors": ["Fabian C. Moss", "Jan Hajič jr.", "Adrian Nachtwey", "Laurent Pugin"], "title": "The Rest is Silence: Leveraging Unseen Species Models for Computational Musicology", "comment": null, "summary": "For many decades, musicologists have engaged in creating large databases\nserving different purposes for musicological research and scholarship. With the\nrise of fields like music information retrieval and digital musicology, there\nis now a constant and growing influx of musicologically relevant datasets and\ncorpora. In historical or observational settings, however, these datasets are\nnecessarily incomplete, and the true extent of a collection of interest remains\nunknown -- silent. Here, we apply, for the first time, so-called Unseen Species\nmodels (USMs) from ecology to areas of musicological activity. After\nintroducing the models formally, we show in four case studies how USMs can be\napplied to musicological data to address quantitative questions like: How many\ncomposers are we missing in RISM? What percentage of medieval sources of\nGregorian chant have we already cataloged? How many differences in music prints\ndo we expect to find between editions? How large is the coverage of songs from\ngenres of a folk music tradition? And, finally, how close are we in estimating\nthe size of the harmonic vocabulary of a large number of composers?"}
{"id": "2507.14141", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14141", "abs": "https://arxiv.org/abs/2507.14141", "authors": ["Danny Dongyeop Han", "Ahhyun Lucy Lee", "Taeyang Lee", "Yonghyeon Gwon", "Sebin Lee", "Seongjin Lee", "David Keetae Park", "Shinjae Yoo", "Jiook Cha", "Chun Kee Chung"], "title": "DIVER-0 : A Fully Channel Equivariant EEG Foundation Model", "comment": "11 pages, 1 figures, ICML 2025 Workshop on GenBio", "summary": "Electroencephalography (EEG) is a non-invasive technique widely used in\nbrain-computer interfaces and clinical applications, yet existing EEG\nfoundation models face limitations in modeling spatio-temporal brain dynamics\nand lack channel permutation equivariance, preventing robust generalization\nacross diverse electrode configurations. To address these challenges, we\npropose DIVER-0, a novel EEG foundation model that demonstrates how full\nspatio-temporal attention-rather than segregated spatial or temporal\nprocessing-achieves superior performance when properly designed with Rotary\nPosition Embedding (RoPE) for temporal relationships and binary attention\nbiases for channel differentiation. We also introduce Sliding Temporal\nConditional Positional Encoding (STCPE), which improves upon existing\nconditional positional encoding approaches by maintaining both temporal\ntranslation equivariance and channel permutation equivariance, enabling robust\nadaptation to arbitrary electrode configurations unseen during pretraining.\nExperimental results demonstrate that DIVER-0 achieves competitive performance\nwith only 10% of pretraining data while maintaining consistent results across\nall channel permutation conditions, validating its effectiveness for\ncross-dataset generalization and establishing key design principles for\nhandling the inherent heterogeneity of neural recording setups."}
{"id": "2507.15229", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15229", "abs": "https://arxiv.org/abs/2507.15229", "authors": ["Zhong-Qiu Wang", "Ruizhe Pang"], "title": "Mixture to Beamformed Mixture: Leveraging Beamformed Mixture as Weak-Supervision for Speech Enhancement and Noise-Robust ASR", "comment": "in submission", "summary": "In multi-channel speech enhancement and robust automatic speech recognition\n(ASR), beamforming can typically improve the signal-to-noise ratio (SNR) of the\ntarget speaker and produce reliable enhancement with little distortion to\ntarget speech. With this observation, we propose to leverage beamformed\nmixture, which has a higher SNR of the target speaker than the input mixture,\nas a weak supervision to train deep neural networks (DNNs) to enhance the input\nmixture. This way, we can train enhancement models using pairs of real-recorded\nmixture and its beamformed mixture, and potentially realize better\ngeneralization to real mixtures, compared with only training the models on\nsimulated mixtures, which usually mismatch real mixtures. Evaluation results on\nthe real-recorded CHiME-4 dataset show the effectiveness of the proposed\nalgorithm."}
{"id": "2507.14647", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14647", "abs": "https://arxiv.org/abs/2507.14647", "authors": ["Go Nishikawa", "Wataru Nakata", "Yuki Saito", "Kanami Imamura", "Hiroshi Saruwatari", "Tomohiko Nakamura"], "title": "Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer", "comment": "4 pages, 2 figures", "summary": "We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3:\nmean opinion score (MOS) prediction for speech with multiple sampling\nfrequencies (SFs). Our submitted model integrates an SF-independent (SFI)\nconvolutional layer into a self-supervised learning (SSL) model to achieve SFI\nspeech feature extraction for MOS prediction. We present some strategies to\nimprove the MOS prediction performance of our model: distilling knowledge from\na pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset.\nOur submission to the AMC 2025 Track 3 ranked the first in one evaluation\nmetric and the fourth in the final ranking. We also report the results of our\nablation study to investigate essential factors of our model."}
{"id": "2507.14144", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14144", "abs": "https://arxiv.org/abs/2507.14144", "authors": ["Cyril Falcon", "Hassan Mortada", "Mathéo Clavaud", "Jean-Philippe Michel"], "title": "Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman", "comment": "4 pages, in French language. 4 figures. Accepted for publication in\n  GRETSI 2025 proceedings", "summary": "The Recursive KalmanNet, recently introduced by the authors, is a recurrent\nneural network guided by a Kalman filter, capable of estimating the state\nvariables and error covariance of stochastic dynamic systems from noisy\nmeasurements, without prior knowledge of the noise characteristics. This paper\nexplores its generalization capabilities in out-of-distribution scenarios,\nwhere the temporal dynamics of the test measurements differ from those\nencountered during training.\n  Le Recursive KalmanNet, r\\'ecemment introduit par les auteurs, est un\nr\\'eseau de neurones r\\'ecurrent guid\\'e par un filtre de Kalman, capable\nd'estimer les variables d'\\'etat et la covariance des erreurs des syst\\`emes\ndynamiques stochastiques \\`a partir de mesures bruit\\'ees, sans connaissance\npr\\'ealable des caract\\'eristiques des bruits. Cet article explore ses\ncapacit\\'es de g\\'en\\'eralisation dans des sc\\'enarios hors distribution, o\\`u\nles dynamiques temporelles des mesures de test diff\\`erent de celles\nrencontr\\'ees \\`a l'entra\\^inement."}
{"id": "2507.15517", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15517", "abs": "https://arxiv.org/abs/2507.15517", "authors": ["Sapir Goldring", "Zamir Ben Hur", "David Lou Alon", "Boaz Rafaely"], "title": "Binaural Signal Matching with Wearable Arrays for Near-Field Sources", "comment": "Published at Forum Acusticum 2025", "summary": "Binaural reproduction methods aim to recreate an acoustic scene for a\nlistener over headphones, offering immersive experiences in applications such\nas Virtual Reality (VR) and teleconferencing. Among the existing approaches,\nthe Binaural Signal Matching (BSM) algorithm has demonstrated high quality\nreproduction due to its signal-independent formulation and the flexibility of\nunconstrained array geometry. However, this method assumes far-field sources\nand has not yet been investigated for near-field scenarios. This study\nevaluates the performance of BSM for near-field sources. Analysis of a\nsemi-circular array around a rigid sphere, modeling head-mounted devices, show\nthat far-field BSM performs adequately for sources up to approximately tens of\ncentimeters from the array. However, for sources closer than this range, the\nbinaural error increases significantly. Incorporating a near-field BSM design,\nwhich accounts for the source distance, significantly reduces the error,\nparticularly for these very-close distances, highlighting the benefits of\nnear-field modeling in improving reproduction accuracy."}
{"id": "2507.15101", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15101", "abs": "https://arxiv.org/abs/2507.15101", "authors": ["Menglu Li", "Xiao-Ping Zhang", "Lian Zhao"], "title": "Frame-level Temporal Difference Learning for Partial Deepfake Speech Detection", "comment": "5 pages, 4 figures, 4 tables. Accepted to IEEE SPL", "summary": "Detecting partial deepfake speech is essential due to its potential for\nsubtle misinformation. However, existing methods depend on costly frame-level\nannotations during training, limiting real-world scalability. Also, they focus\non detecting transition artifacts between bonafide and deepfake segments. As\ndeepfake generation techniques increasingly smooth these transitions, detection\nhas become more challenging. To address this, our work introduces a new\nperspective by analyzing frame-level temporal differences and reveals that\ndeepfake speech exhibits erratic directional changes and unnatural local\ntransitions compared to bonafide speech. Based on this finding, we propose a\nTemporal Difference Attention Module (TDAM) that redefines partial deepfake\ndetection as identifying unnatural temporal variations, without relying on\nexplicit boundary annotations. A dual-level hierarchical difference\nrepresentation captures temporal irregularities at both fine and coarse scales,\nwhile adaptive average pooling preserves essential patterns across\nvariable-length inputs to minimize information loss. Our TDAM-AvgPool model\nachieves state-of-the-art performance, with an EER of 0.59% on the PartialSpoof\ndataset and 0.03% on the HAD dataset, which significantly outperforms the\nexisting methods without requiring frame-level supervision."}
{"id": "2507.14146", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14146", "abs": "https://arxiv.org/abs/2507.14146", "authors": ["Kleanthis Avramidis", "Emily Zhou", "Tiantian Feng", "Hossein Hamidi Shishavan", "Frederico Marcolino Quintao Severgnini", "Danny J. Lohan", "Paul Schmalenberg", "Ercan M. Dede", "Shrikanth Narayanan"], "title": "Estimating Markers of Driving Stress through Multimodal Physiological Monitoring", "comment": "11 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "Understanding and mitigating driving stress is vital for preventing accidents\nand advancing both road safety and driver well-being. While vehicles are\nequipped with increasingly sophisticated safety systems, many limits exist in\ntheir ability to account for variable driving behaviors and environmental\ncontexts. In this study we examine how short-term stressor events impact\ndrivers' physiology and their behavioral responses behind the wheel. Leveraging\na controlled driving simulation setup, we collected physiological signals from\n31 adult participants and designed a multimodal machine learning system to\nestimate the presence of stressors. Our analysis explores the model sensitivity\nand temporal dynamics against both known and novel emotional inducers, and\nexamines the relationship between predicted stress and observable patterns of\nvehicle control. Overall, this study demonstrates the potential of linking\nphysiological signals with contextual and behavioral cues in order to improve\nreal-time estimation of driving stress."}
{"id": "2507.14237", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14237", "abs": "https://arxiv.org/abs/2507.14237", "authors": ["Louis Bahrman", "Mathieu Fontaine", "Gaël Richard"], "title": "U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model", "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "This paper explores the outcome of training state-ofthe-art dereverberation\nmodels with supervision settings ranging from weakly-supervised to fully\nunsupervised, relying solely on reverberant signals and an acoustic model for\ntraining. Most of the existing deep learning approaches typically require\npaired dry and reverberant data, which are difficult to obtain in practice. We\ndevelop instead a sequential learning strategy motivated by a bayesian\nformulation of the dereverberation problem, wherein acoustic parameters and dry\nsignals are estimated from reverberant inputs using deep neural networks,\nguided by a reverberation matching loss. Our most data-efficient variant\nrequires only 100 reverberation-parameter-labelled samples to outperform an\nunsupervised baseline, demonstrating the effectiveness and practicality of the\nproposed method in low-resource scenarios."}
{"id": "2507.15214", "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15214", "abs": "https://arxiv.org/abs/2507.15214", "authors": ["Natalia Tomashenko", "Emmanuel Vincent", "Marc Tommasi"], "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems", "comment": "Accepted at Interspeech-2025", "summary": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature."}
{"id": "2507.14147", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.14147", "abs": "https://arxiv.org/abs/2507.14147", "authors": ["Kevin Monteiro", "Sam Nallaperuma-Herzberg", "Martina Mason", "Steve Niederer"], "title": "Graph Convolutional Neural Networks to Model the Brain for Insomnia", "comment": "12 pages, 6 figures. This version has been accepted as a full paper\n  at the 2025 AI in Healthcare (AIiH) Conference", "summary": "Insomnia affects a vast population of the world and can have a wide range of\ncauses. Existing treatments for insomnia have been linked with many side\neffects like headaches, dizziness, etc. As such, there is a clear need for\nimproved insomnia treatment. Brain modelling has helped with assessing the\neffects of brain pathology on brain network dynamics and with supporting\nclinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.\nHowever, such models have not been developed for insomnia. Therefore, this\nproject attempts to understand the characteristics of the brain of individuals\nexperiencing insomnia using continuous long-duration EEG data. Brain networks\nare derived based on functional connectivity and spatial distance between EEG\nchannels. The power spectral density of the channels is then computed for the\nmajor brain wave frequency bands. A graph convolutional neural network (GCNN)\nmodel is then trained to capture the functional characteristics associated with\ninsomnia and configured for the classification task to judge performance.\nResults indicated a 50-second non-overlapping sliding window was the most\nsuitable choice for EEG segmentation. This approach achieved a classification\naccuracy of 70% at window level and 68% at subject level. Additionally, the\nomission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in\nmodel performance than the removal of other channels. These channel electrodes\nare positioned near brain regions known to exhibit atypical levels of\nfunctional connectivity in individuals with insomnia, which can explain such\nresults."}
{"id": "2507.14638", "categories": ["cs.SD", "eess.AS", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.14638", "abs": "https://arxiv.org/abs/2507.14638", "authors": ["Fabian C. Moss", "Jan Hajič jr.", "Adrian Nachtwey", "Laurent Pugin"], "title": "The Rest is Silence: Leveraging Unseen Species Models for Computational Musicology", "comment": null, "summary": "For many decades, musicologists have engaged in creating large databases\nserving different purposes for musicological research and scholarship. With the\nrise of fields like music information retrieval and digital musicology, there\nis now a constant and growing influx of musicologically relevant datasets and\ncorpora. In historical or observational settings, however, these datasets are\nnecessarily incomplete, and the true extent of a collection of interest remains\nunknown -- silent. Here, we apply, for the first time, so-called Unseen Species\nmodels (USMs) from ecology to areas of musicological activity. After\nintroducing the models formally, we show in four case studies how USMs can be\napplied to musicological data to address quantitative questions like: How many\ncomposers are we missing in RISM? What percentage of medieval sources of\nGregorian chant have we already cataloged? How many differences in music prints\ndo we expect to find between editions? How large is the coverage of songs from\ngenres of a folk music tradition? And, finally, how close are we in estimating\nthe size of the harmonic vocabulary of a large number of composers?"}
{"id": "2507.15221", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15221", "abs": "https://arxiv.org/abs/2507.15221", "authors": ["Haiying Xu", "Haoze Liu", "Mingshi Li", "Siyu Cai", "Guangxuan Zheng", "Yuhuang Jia", "Jinghua Zhao", "Yong Qin"], "title": "EchoVoices: Preserving Generational Voices and Memories for Seniors and Children", "comment": null, "summary": "Recent breakthroughs in intelligent speech and digital human technologies\nhave primarily targeted mainstream adult users, often overlooking the distinct\nvocal patterns and interaction styles of seniors and children. These\ndemographics possess distinct vocal characteristics, linguistic styles, and\ninteraction patterns that challenge conventional ASR, TTS, and LLM systems. To\naddress this, we introduce EchoVoices, an end-to-end digital human pipeline\ndedicated to creating persistent digital personas for seniors and children,\nensuring their voices and memories are preserved for future generations. Our\nsystem integrates three core innovations: a k-NN-enhanced Whisper model for\nrobust speech recognition of atypical speech; an age-adaptive VITS model for\nhigh-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that\nautomatically generates persona cards and leverages a RAG-based memory system\nfor conversational consistency. Our experiments, conducted on the SeniorTalk\nand ChildMandarin datasets, demonstrate significant improvements in recognition\naccuracy, synthesis quality, and speaker similarity. EchoVoices provides a\ncomprehensive framework for preserving generational voices, offering a new\nmeans of intergenerational connection and the creation of lasting digital\nlegacies."}
{"id": "2507.14148", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14148", "abs": "https://arxiv.org/abs/2507.14148", "authors": ["Daniele Pugliese", "Giovanni Iacovelli", "Alessio Fascista", "Domenico Striccoli", "Oleksandr Romanov", "Luigi Alfredo Grieco", "Gennaro Boggia"], "title": "Visible Light Indoor Positioning with a Single LED and Distributed Single-Element OIRS: An Iterative Approach with Adaptive Beam Steering", "comment": null, "summary": "The integration of Optical Intelligent Reflective Surfaces (OIRSs) into\nVisible Light Communication (VLC) systems is gaining momentum as a valid\nalternative to RF technologies, harnessing the existing lighting\ninfrastructures and the vast unlicensed optical spectrum to enable higher\nspectral efficiency, improved resilience to Line-of-Sight (LoS) blockages, and\nenhanced positioning capabilities. This paper investigates the problem of\nlocalizing a low-cost Photo Detector (PD) in a VLC-based indoor environment\nconsisting of only a single Light Emitting Diode (LED) as an active anchor, and\nmultiple spatially distributed single-element OIRSs. We formulate the problem\nwithin an indirect, computationally efficient localization framework: first,\nthe optimal Maximum Likelihood (ML) estimators of the LoS and Non-Line-of-Sight\n(NLoS) distances are derived, using a suitable OIRS activation strategy to\nprevent interferences. To overcome the grid-based optimization required by the\nML NLoS estimator, we devise a novel algorithm based on an unstructured noise\nvariance transformation, which admits a closed-form solution. The set of\nestimated LoS/NLoS distances are then used within a low-complexity localization\nalgorithm combining an Iterative Weighted Least Squares (IWLS) procedure, whose\nweights are set according to the inverse of the Cram\\'er-Rao Lower Bound\n(CRLB), with an adaptive beam steering strategy that allows the OIRSs network\nto dynamically align with the PD, without any prior knowledge of its position.\nAccordingly, we derive the CRLB for both LoS/NLoS distance estimation and PD\nposition estimation. Simulation results demonstrate the effectiveness of our\napproach in terms of localization accuracy, robustness against OIRSs\nmisalignment conditions, and low number of iterations required to attain the\ntheoretical bounds."}
{"id": "2507.14647", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14647", "abs": "https://arxiv.org/abs/2507.14647", "authors": ["Go Nishikawa", "Wataru Nakata", "Yuki Saito", "Kanami Imamura", "Hiroshi Saruwatari", "Tomohiko Nakamura"], "title": "Multi-Sampling-Frequency Naturalness MOS Prediction Using Self-Supervised Learning Model with Sampling-Frequency-Independent Layer", "comment": "4 pages, 2 figures", "summary": "We introduce our submission to the AudioMOS Challenge (AMC) 2025 Track 3:\nmean opinion score (MOS) prediction for speech with multiple sampling\nfrequencies (SFs). Our submitted model integrates an SF-independent (SFI)\nconvolutional layer into a self-supervised learning (SSL) model to achieve SFI\nspeech feature extraction for MOS prediction. We present some strategies to\nimprove the MOS prediction performance of our model: distilling knowledge from\na pretrained non-SFI-SSL model and pretraining with a large-scale MOS dataset.\nOur submission to the AMC 2025 Track 3 ranked the first in one evaluation\nmetric and the fourth in the final ranking. We also report the results of our\nablation study to investigate essential factors of our model."}
{"id": "2507.15272", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15272", "abs": "https://arxiv.org/abs/2507.15272", "authors": ["Ayush Singh Bhadoriya", "Abhishek Nikunj Shinde", "Isha Pandey", "Ganesh Ramakrishnan"], "title": "A2TTS: TTS for Low Resource Indian Languages", "comment": null, "summary": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil."}
{"id": "2507.14151", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14151", "abs": "https://arxiv.org/abs/2507.14151", "authors": ["Giuliana Monachino", "Nicolò La Porta", "Beatrice Zanchi", "Luigi Fiorillo", "Alvise Dei Rossi", "Georgiy Farina", "Francesca Dalia Faraci"], "title": "Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models", "comment": null, "summary": "Foundation Models (FMs) are large-scale machine learning models trained on\nextensive, diverse datasets that can be adapted to a wide range of downstream\ntasks with minimal fine-tuning. In the last two years, interest in FMs has also\ngrown for applications in the cardiological field to analyze the\nelectrocardiogram (ECG) signals. One of the key properties of FMs is their\ntransferability to a wide range of downstream scenarios. With the spread of\nwearable and portable devices, keen interest in learning from reduced-channel\nconfigurations has arisen. However, the adaptation of ECG FMs to downstream\nscenarios with fewer available channels still has to be properly investigated.\nIn this work, we propose Self-DANA, a novel, easy-to-integrate solution that\nmakes self-supervised architectures adaptable to a reduced number of input\nchannels, ensuring resource efficiency and high performance. We also introduce\nRandom Lead Selection, a novel augmentation technique to pre-train models in a\nmore robust and channel-agnostic way. Our experimental results on five\nreduced-channel configurations demonstrate that Self-DANA significantly\nenhances resource efficiency while reaching state-of-the-art performance. It\nrequires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about\n17% less average epoch CPU time, and about 24% less average epoch GPU time."}
{"id": "2507.15101", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15101", "abs": "https://arxiv.org/abs/2507.15101", "authors": ["Menglu Li", "Xiao-Ping Zhang", "Lian Zhao"], "title": "Frame-level Temporal Difference Learning for Partial Deepfake Speech Detection", "comment": "5 pages, 4 figures, 4 tables. Accepted to IEEE SPL", "summary": "Detecting partial deepfake speech is essential due to its potential for\nsubtle misinformation. However, existing methods depend on costly frame-level\nannotations during training, limiting real-world scalability. Also, they focus\non detecting transition artifacts between bonafide and deepfake segments. As\ndeepfake generation techniques increasingly smooth these transitions, detection\nhas become more challenging. To address this, our work introduces a new\nperspective by analyzing frame-level temporal differences and reveals that\ndeepfake speech exhibits erratic directional changes and unnatural local\ntransitions compared to bonafide speech. Based on this finding, we propose a\nTemporal Difference Attention Module (TDAM) that redefines partial deepfake\ndetection as identifying unnatural temporal variations, without relying on\nexplicit boundary annotations. A dual-level hierarchical difference\nrepresentation captures temporal irregularities at both fine and coarse scales,\nwhile adaptive average pooling preserves essential patterns across\nvariable-length inputs to minimize information loss. Our TDAM-AvgPool model\nachieves state-of-the-art performance, with an EER of 0.59% on the PartialSpoof\ndataset and 0.03% on the HAD dataset, which significantly outperforms the\nexisting methods without requiring frame-level supervision."}
{"id": "2507.15294", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.15294", "abs": "https://arxiv.org/abs/2507.15294", "authors": ["Junjie Li", "Wenxuan Wu", "Shuai Wang", "Zexu Pan", "Kong Aik Lee", "Helen Meng", "Haizhou Li"], "title": "MeMo: Attentional Momentum for Real-time Audio-visual Speaker Extraction under Impaired Visual Conditions", "comment": null, "summary": "Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate a target\nspeaker's voice from multi-speaker environments by leveraging visual cues as\nguidance. However, the performance of AV-TSE systems heavily relies on the\nquality of these visual cues. In extreme scenarios where visual cues are\nmissing or severely degraded, the system may fail to accurately extract the\ntarget speaker. In contrast, humans can maintain attention on a target speaker\neven in the absence of explicit auxiliary information. Motivated by such human\ncognitive ability, we propose a novel framework called MeMo, which incorporates\ntwo adaptive memory banks to store attention-related information. MeMo is\nspecifically designed for real-time scenarios: once initial attention is\nestablished, the system maintains attentional momentum over time, even when\nvisual cues become unavailable. We conduct comprehensive experiments to verify\nthe effectiveness of MeMo. Experimental results demonstrate that our proposed\nframework achieves SI-SNR improvements of at least 2 dB over the corresponding\nbaseline."}
{"id": "2507.14152", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.14152", "abs": "https://arxiv.org/abs/2507.14152", "authors": ["Frank Efe Erukainure", "Feidra Gjata", "Matin Ataei Kachouei", "Henry Cox", "Md. Azahar Ali"], "title": "Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors", "comment": "34 pages, 9 figures", "summary": "River water quality monitoring is important for aquatic life, livestock, and\nhumans because clean water is critical to meeting food demand during the global\nfood crisis. Excessive contaminants, including phosphate, deplete dissolved\noxygen and trigger eutrophication, leading to serious health and ecological\nproblems. Continuous sensors that track phosphate levels can therefore help\nprevent eutrophication. In this work we present a lithography-free phosphate\nsensor (P-sensor) that detects phosphate in river water at parts-per-billion\nlevels. The device uses a solid-state indicator electrode formed by 3D-printed\nperiodic polymer patterns (8 um feature size) coated with a thin phosphate\nion-selective membrane. The P-sensor detects as little as 1 ppb phosphate\nacross 0 - 475 ppm with a response time under 30 seconds. We validated the\nsensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at\nsites upstream and downstream of a sewage treatment plant and benchmarked the\nresults against a commercial phosphate meter. A feed-forward neural network was\ntrained to predict phosphate levels, achieving a mean-squared error below 1e-3,\nzero standard deviation, and a Pearson correlation coefficient of 0.997 for\nriver samples. These results demonstrate a practical tool for continuous\nwater-quality monitoring that can inform stakeholders and policymakers and\nultimately improve public health."}
{"id": "2507.15214", "categories": ["cs.SD", "cs.CL", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15214", "abs": "https://arxiv.org/abs/2507.15214", "authors": ["Natalia Tomashenko", "Emmanuel Vincent", "Marc Tommasi"], "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems", "comment": "Accepted at Interspeech-2025", "summary": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature."}
{"id": "2507.15396", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15396", "abs": "https://arxiv.org/abs/2507.15396", "authors": ["Hui-Guan Yuan", "Ryandhimas E. Zezario", "Shafique Ahmed", "Hsin-Min Wang", "Kai-Lung Hua", "Yu Tsao"], "title": "Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation", "comment": null, "summary": "Hearing loss simulation models are essential for hearing aid deployment.\nHowever, existing models have high computational complexity and latency, which\nlimits real-time applications and lack direct integration with speech\nprocessing systems. To address these issues, we propose Neuro-MSBG, a\nlightweight end-to-end model with a personalized audiogram encoder for\neffective time-frequency modeling. Experiments show that Neuro-MSBG supports\nparallel inference and retains the intelligibility and perceptual quality of\nthe original MSBG, with a Spearman's rank correlation coefficient (SRCC) of\n0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for\nPerceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation\nruntime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second\ninput), further demonstrating its efficiency and practicality."}
{"id": "2507.14153", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14153", "abs": "https://arxiv.org/abs/2507.14153", "authors": ["Daniel Cieślak", "Barbara Szyca", "Weronika Bajko", "Liwia Florkiewicz", "Kinga Grzęda", "Mariusz Kaczmarek", "Helena Kamieniecka", "Hubert Lis", "Weronika Matwiejuk", "Anna Prus", "Michalina Razik", "Inga Rozumowicz", "Wiktoria Ziembakowska"], "title": "Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM", "comment": "International Conference on Hybrid Artificial Intelligence Systems\n  (HAIS 2024)", "summary": "Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to\nits progressive nature and complex symptoms. This study introduces a novel\napproach utilizing surface electromyography (sEMG) to objectively assess PD\nseverity, focusing on the biceps brachii muscle. Initial analysis of sEMG data\nfrom five PD patients and five healthy controls revealed significant\nneuromuscular differences. A traditional Support Vector Machine (SVM) model\nachieved up to 83% accuracy, while enhancements with a Graph Convolutional\nNetwork-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.\nDespite the preliminary nature of these results, the study outlines a detailed\nexperimental methodology for future research with larger cohorts to validate\nthese findings and integrate the approach into clinical practice. The proposed\napproach holds promise for advancing PD severity assessment and improving\npatient care in Parkinson's disease management."}
{"id": "2507.15221", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15221", "abs": "https://arxiv.org/abs/2507.15221", "authors": ["Haiying Xu", "Haoze Liu", "Mingshi Li", "Siyu Cai", "Guangxuan Zheng", "Yuhuang Jia", "Jinghua Zhao", "Yong Qin"], "title": "EchoVoices: Preserving Generational Voices and Memories for Seniors and Children", "comment": null, "summary": "Recent breakthroughs in intelligent speech and digital human technologies\nhave primarily targeted mainstream adult users, often overlooking the distinct\nvocal patterns and interaction styles of seniors and children. These\ndemographics possess distinct vocal characteristics, linguistic styles, and\ninteraction patterns that challenge conventional ASR, TTS, and LLM systems. To\naddress this, we introduce EchoVoices, an end-to-end digital human pipeline\ndedicated to creating persistent digital personas for seniors and children,\nensuring their voices and memories are preserved for future generations. Our\nsystem integrates three core innovations: a k-NN-enhanced Whisper model for\nrobust speech recognition of atypical speech; an age-adaptive VITS model for\nhigh-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that\nautomatically generates persona cards and leverages a RAG-based memory system\nfor conversational consistency. Our experiments, conducted on the SeniorTalk\nand ChildMandarin datasets, demonstrate significant improvements in recognition\naccuracy, synthesis quality, and speaker similarity. EchoVoices provides a\ncomprehensive framework for preserving generational voices, offering a new\nmeans of intergenerational connection and the creation of lasting digital\nlegacies."}
{"id": "2507.15558", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15558", "abs": "https://arxiv.org/abs/2507.15558", "authors": ["Dzmitry Saladukha", "Ivan Koriabkin", "Kanstantsin Artsiom", "Aliaksei Rak", "Nikita Ryzhikov"], "title": "Multichannel Keyword Spotting for Noisy Conditions", "comment": "Accepted to Interspeech 2025", "summary": "This article presents a method for improving a keyword spotter (KWS)\nalgorithm in noisy environments. Although beamforming (BF) and adaptive noise\ncancellation (ANC) techniques are robust in some conditions, they may degrade\nthe performance of the activation system by distorting or suppressing useful\nsignals. The authors propose a neural network architecture that uses several\ninput channels and an attention mechanism that allows the network to determine\nthe most useful channel or their combination. The improved quality of the\nalgorithm was demonstrated on two datasets: from a laboratory with controlled\nconditions and from smart speakers in natural conditions. The proposed\nalgorithm was compared against several baselines in terms of the quality of\nnoise reduction metrics, KWS metrics, and computing resources in comparison\nwith existing solutions."}
{"id": "2507.14155", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14155", "abs": "https://arxiv.org/abs/2507.14155", "authors": ["Pramesh Gautam", "Sushmita Sapkota", "Carsten Bockelmann", "Shashi Raj Pandey", "Armin Dekorsy"], "title": "Extreme Value Theory-based Distributed Interference Prediction for 6G Industrial Sub-networks", "comment": null, "summary": "Interference prediction that accounts for extreme and rare events remains a\nkey challenge for ultra-densely deployed sub-networks (SNs) requiring\nhyper-reliable low-latency communication (HRLLC), particularly under dynamic\nmobility, rapidly varying channel statistics, and sporadic traffic. This paper\nproposes a novel calibrated interference tail prediction framework, a hybrid\nstatistical and machine learning (ML) approach that integrates an inverted\nquantile patch transformer (iQPTransformer) within extreme value theory (EVT).\nIt captures interference dynamics and tail behavior while quantifying\nuncertainty to provide statistical coverage guarantees. Its effectiveness is\ndemonstrated by leveraging the estimated interference tail distribution to\ndesign predictive, risk-aware resource allocation. In resource-constrained SN\nscenarios, we introduce the split-iQPTransformer, enabling collaborative\ntraining by distributing neural network components between sensor-actuator (SA)\npairs and the SN controller, while maintaining minimal performance disparity\ncompared to the centralized iQPTransformer. The framework effectively handles\ndeep fading, random traffic, and time-division duplexing (TDD) misalignments\nand is resilient to rare and extreme interference events. Extensive evaluations\nare performed under two mobility models and two realistic SN traffic patterns,\nusing a spatially consistent 3GPP channel model across all scenarios.\nExperimental results show consistent achievement of block error rate (BLER)\ntargets beyond the 95th percentile in the hyper-reliable regime, significantly\noutperforming baseline approaches."}
{"id": "2507.15272", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15272", "abs": "https://arxiv.org/abs/2507.15272", "authors": ["Ayush Singh Bhadoriya", "Abhishek Nikunj Shinde", "Isha Pandey", "Ganesh Ramakrishnan"], "title": "A2TTS: TTS for Low Resource Indian Languages", "comment": null, "summary": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil."}
{"id": "2507.14346", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.14346", "abs": "https://arxiv.org/abs/2507.14346", "authors": ["Xuanru Zhou", "Jiachen Lian", "Cheol Jun Cho", "Tejas Prabhune", "Shuhe Li", "William Li", "Rodrigo Ortiz", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Towards Accurate Phonetic Error Detection Through Phoneme Similarity Modeling", "comment": "2025 Interspeech", "summary": "Phonetic error detection, a core subtask of automatic pronunciation\nassessment, identifies pronunciation deviations at the phoneme level. Speech\nvariability from accents and dysfluencies challenges accurate phoneme\nrecognition, with current models failing to capture these discrepancies\neffectively. We propose a verbatim phoneme recognition framework using\nmulti-task training with novel phoneme similarity modeling that transcribes\nwhat speakers actually say rather than what they're supposed to say. We develop\nand open-source \\textit{VCTK-accent}, a simulated dataset containing phonetic\nerrors, and propose two novel metrics for assessing pronunciation differences.\nOur work establishes a new benchmark for phonetic error detection."}
{"id": "2507.14163", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14163", "abs": "https://arxiv.org/abs/2507.14163", "authors": ["Renxiang Qiu", "Raghavendra Selvan"], "title": "UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification", "comment": "Accepted to be presented at the 35th IEEE International Workshop on\n  Machine Learning for Signal Processing (IEEE MLSP 2025). Source code\n  available at https://github.com/HughYau/UniPhyNet", "summary": "We present UniPhyNet, a novel neural network architecture to classify\ncognitive load using multimodal physiological data -- specifically EEG, ECG and\nEDA signals -- without the explicit need for extracting hand-crafted features.\nUniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type\nblocks enhanced with channel block attention module to focus on the informative\nfeatures while a bidirectional gated recurrent unit is used to capture temporal\ndependencies. This architecture processes and combines signals in both unimodal\nand multimodal configurations via intermediate fusion of learned feature maps.\nOn the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy\nfrom 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based\nmodels, demonstrating its effectiveness as an end-to-end solution for\nreal-world cognitive state monitoring."}
{"id": "2507.15396", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15396", "abs": "https://arxiv.org/abs/2507.15396", "authors": ["Hui-Guan Yuan", "Ryandhimas E. Zezario", "Shafique Ahmed", "Hsin-Min Wang", "Kai-Lung Hua", "Yu Tsao"], "title": "Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation", "comment": null, "summary": "Hearing loss simulation models are essential for hearing aid deployment.\nHowever, existing models have high computational complexity and latency, which\nlimits real-time applications and lack direct integration with speech\nprocessing systems. To address these issues, we propose Neuro-MSBG, a\nlightweight end-to-end model with a personalized audiogram encoder for\neffective time-frequency modeling. Experiments show that Neuro-MSBG supports\nparallel inference and retains the intelligibility and perceptual quality of\nthe original MSBG, with a Spearman's rank correlation coefficient (SRCC) of\n0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for\nPerceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation\nruntime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second\ninput), further demonstrating its efficiency and practicality."}
{"id": "2507.14451", "categories": ["eess.AS", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.14451", "abs": "https://arxiv.org/abs/2507.14451", "authors": ["Satwik Dutta", "Shruthigna Chandupatla", "John Hansen"], "title": "Adapting Whisper for Lightweight and Efficient Automatic Speech Recognition of Children for On-device Edge Applications", "comment": "5 pages, 5 figures, accepted for presentation at the 2025 Workshop on\n  Child Computer Interaction (WOCCI 2025), a Satellite Workshop of the 2025\n  Interspeech Conference", "summary": "Reliability on cloud providers for ASR inference to support child-centered\nvoice-based applications is becoming challenging due to regulatory and privacy\nchallenges. Motivated by a privacy-preserving design, this study aims to\ndevelop a lightweight & efficient Whisper ASR system capable of running on a\nRaspberry Pi. Upon evaluation of the MyST corpus and by examining various\nfiltering strategies to fine-tune the `tiny.en' model, a Word Error Rate (WER)\nof 15.9% was achieved (11.8% filtered). A low-rank compression reduces the\nencoder size by 0.51M with 1.26x faster inference in GPU, with 11% relative WER\nincrease. During inference on Pi, the compressed version required ~2 GFLOPS\nfewer computations. The RTF for both the models ranged between [0.23-0.41] for\nvarious input audio durations. Analyzing the RAM usage and CPU temperature\nshowed that the PI was capable of handling both the tiny models, however it was\nnoticed that small models initiated additional overhead/thermal throttling."}
{"id": "2507.14164", "categories": ["eess.SP", "cs.AI", "cs.LG", "I.2; J.3"], "pdf": "https://arxiv.org/pdf/2507.14164", "abs": "https://arxiv.org/abs/2507.14164", "authors": ["Samuel Ruipérez-Campillo", "Alain Ryser", "Thomas M. Sutter", "Ruibin Feng", "Prasanth Ganesan", "Brototo Deb", "Kelly A. Brennan", "Maxime Pedron", "Albert J. Rogers", "Maarten Z. H. Kolk", "Fleur V. Y. Tjong", "Sanjiv M. Narayan", "Julia E. Vogt"], "title": "A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy", "comment": "9 pages, 2 figures, 3 tables, the last two authors are shared senior\n  authors", "summary": "In the field of cardiac electrophysiology (EP), effectively reducing noise in\nintra-cardiac signals is crucial for the accurate diagnosis and treatment of\narrhythmias and cardiomyopathies. However, traditional noise reduction\ntechniques fall short in addressing the diverse noise patterns from various\nsources, often non-linear and non-stationary, present in these signals. This\nwork introduces a Variational Autoencoder (VAE) model, aimed at improving the\nquality of intra-ventricular monophasic action potential (MAP) signal\nrecordings. By constructing representations of clean signals from a dataset of\n5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our\napproach demonstrates superior denoising performance when compared to\nconventional filtering methods commonly employed in clinical settings. We\nassess the effectiveness of our VAE model using various metrics, indicating its\nsuperior capability to denoise signals across different noise types, including\ntime-varying non-linear noise frequently found in clinical settings. These\nresults reveal that VAEs can eliminate diverse sources of noise in single\nbeats, outperforming state-of-the-art denoising techniques and potentially\nimproving treatment efficacy in cardiac EP."}
{"id": "2507.15558", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15558", "abs": "https://arxiv.org/abs/2507.15558", "authors": ["Dzmitry Saladukha", "Ivan Koriabkin", "Kanstantsin Artsiom", "Aliaksei Rak", "Nikita Ryzhikov"], "title": "Multichannel Keyword Spotting for Noisy Conditions", "comment": "Accepted to Interspeech 2025", "summary": "This article presents a method for improving a keyword spotter (KWS)\nalgorithm in noisy environments. Although beamforming (BF) and adaptive noise\ncancellation (ANC) techniques are robust in some conditions, they may degrade\nthe performance of the activation system by distorting or suppressing useful\nsignals. The authors propose a neural network architecture that uses several\ninput channels and an attention mechanism that allows the network to determine\nthe most useful channel or their combination. The improved quality of the\nalgorithm was demonstrated on two datasets: from a laboratory with controlled\nconditions and from smart speakers in natural conditions. The proposed\nalgorithm was compared against several baselines in terms of the quality of\nnoise reduction metrics, KWS metrics, and computing resources in comparison\nwith existing solutions."}
{"id": "2507.14534", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.14534", "abs": "https://arxiv.org/abs/2507.14534", "authors": ["Yu Zhang", "Baotong Tian", "Zhiyao Duan"], "title": "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion", "comment": null, "summary": "Zero-shot online voice conversion (VC) holds significant promise for\nreal-time communications and entertainment. However, current VC models struggle\nto preserve semantic fidelity under real-time constraints, deliver\nnatural-sounding conversions, and adapt effectively to unseen speaker\ncharacteristics. To address these challenges, we introduce Conan, a chunkwise\nonline zero-shot voice conversion model that preserves the content of the\nsource while matching the voice timbre and styles of reference speech. Conan\ncomprises three core components: 1) a Stream Content Extractor that leverages\nEmformer for low-latency streaming content encoding; 2) an Adaptive Style\nEncoder that extracts fine-grained stylistic features from reference speech for\nenhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully\ncausal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations\ndemonstrate that Conan outperforms baseline models in subjective and objective\nmetrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo."}
{"id": "2507.14165", "categories": ["eess.SP", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14165", "abs": "https://arxiv.org/abs/2507.14165", "authors": ["Philip Wiese", "Victor Kartsch", "Marco Guermandi", "Luca Benini"], "title": "A Multi-Modal IoT Node for Energy-Efficient Environmental Monitoring with Edge AI Processing", "comment": "7 pages, 4 figures, 2 tables. This paper has been accepted at 2025\n  IEEE International Conference on Omni-layer Intelligent Systems (COINS)", "summary": "The widespread adoption of Internet of Things (IoT) technologies has\nsignificantly advanced environmental monitoring (EM) by enabling cost-effective\nand scalable sensing solutions. Concurrently, machine learning (ML) and\nartificial intelligence (AI) are introducing powerful tools for the efficient\nand accurate analysis of complex environmental data. However, current IoT\nplatforms for environmental sensing are typically limited to a narrow set of\nsensors, preventing a comprehensive assessment of environmental conditions and\nlacking sufficient computational capabilities to support the deployment of\nadvanced ML and AI algorithms on the edge. To overcome these limitations, we\nintroduce a compact (17x38 mm2), multi-modal, MCU-based environmental IoT node\nintegrating 11 sensors, including CO2 concentration, volatile organic compounds\n(VOCs), light intensity, UV radiation, pressure, temperature, humidity, visual\nsensing via an RGB camera, and precise geolocation through a GNSS module. It\nfeatures GAP9, a parallel ultra-low-power system-on-chip, enabling real-time,\nenergy-efficient edge processing of advanced ML models directly on-device. We\nimplemented a YOLOv5-based occupancy detection pipeline (0.3 M parameters, 42\nMOP per inference), demonstrating 42% energy savings over raw data streaming.\nAdditionally, we present a smart indoor air quality (IAQ) monitoring setup that\ncombines occupancy detection with adaptive sample rates, achieving operational\ntimes of up to 143 h on a single compact 600 mAh, 3.7 V battery. Our platform\nlays the groundwork for innovative applications such as predictive indoor IAQ,\nenabling efficient AI-driven on-edge forecasting for energy-efficient and\nautonomous, proactive pollution-mitigation control strategies"}
{"id": "2507.14166", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14166", "abs": "https://arxiv.org/abs/2507.14166", "authors": ["Sankalp Jajee", "Gaurav Kumar", "Homayoun Valafar"], "title": "Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering", "comment": "8 pages, 5 figures", "summary": "Preclinical sleep research remains constrained by labor intensive, manual\nvigilance state classification and inter rater variability, limiting throughput\nand reproducibility. This study presents an automated framework developed by\nTeam Neural Prognosticators to classify electroencephalogram (EEG) recordings\nof small rodents into three critical vigilance states paradoxical sleep (REM),\nslow wave sleep (SWS), and wakefulness. The system integrates advanced signal\nprocessing with machine learning, leveraging engineered features from both time\nand frequency domains, including spectral power across canonical EEG bands\n(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and\ncross-frequency coupling metrics. These features capture distinct\nneurophysiological signatures such as high frequency desynchronization during\nwakefulness, delta oscillations in SWS, and REM specific bursts. Validated\nduring the 2024 Big Data Health Science Case Competition (University of South\nCarolina Big Data Health Science Center, 2024), our XGBoost model achieved\n91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of\n83.5%, outperforming all baseline methods. Our approach represents a critical\nadvancement in automated sleep state classification and a valuable tool for\naccelerating discoveries in sleep science and the development of targeted\ninterventions for chronic sleep disorders. As a publicly available code (BDHSC)\nresource is set to contribute significantly to advancements."}
{"id": "2507.14167", "categories": ["eess.SP", "cs.IR", "cs.LG", "62H05, 65-11, 94-11", "E.0; H.1.1; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.14167", "abs": "https://arxiv.org/abs/2507.14167", "authors": ["Lucas Heublein", "Christian Wielenberg", "Thorsten Nowak", "Tobias Feigl", "Christopher Mutschler", "Felix Ott"], "title": "Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization", "comment": "6 pages, 10 figures", "summary": "Jamming devices disrupt signals from the global navigation satellite system\n(GNSS) and pose a significant threat by compromising the reliability of\naccurate positioning. Consequently, the detection and localization of these\ninterference signals are essential to achieve situational awareness, mitigating\ntheir impact, and implementing effective counter-measures. Classical Angle of\nArrival (AoA) methods exhibit reduced accuracy in multipath environments due to\nsignal reflections and scattering, leading to localization errors.\nAdditionally, AoA-based techniques demand substantial computational resources\nfor array signal processing. In this paper, we propose a novel approach for\ndetecting and classifying interference while estimating the distance, azimuth,\nand elevation of jamming sources. Our benchmark study evaluates 128 vision\nencoder and time-series models to identify the highest-performing methods for\neach task. We introduce an attention-based fusion framework that integrates\nin-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed\nspectrograms while incorporating 22 AoA features to enhance localization\naccuracy. Furthermore, we present a novel dataset of moving jamming devices\nrecorded in an indoor environment with dynamic multipath conditions and\ndemonstrate superior performance compared to state-of-the-art methods."}
{"id": "2507.14169", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14169", "abs": "https://arxiv.org/abs/2507.14169", "authors": ["Pramesh Gautam", "Ravi Sharan B A G", "Paolo Baracca", "Carsten Bockelmann", "Thorsten Wild", "Armin Dekorsy"], "title": "CQI-Based Interference Prediction for Link Adaptation in Industrial Sub-networks", "comment": null, "summary": "We propose a novel interference prediction scheme to improve link adaptation\n(LA) in densely deployed industrial sub-networks (SNs) with high-reliability\nand low-latency communication (HRLLC) requirements. The proposed method aims to\nimprove the LA framework by predicting and leveraging the heavy-tailed\ninterference probability density function (pdf). Interference is modeled as a\nlatent vector of available channel quality indicator (CQI), using a vector\ndiscrete-time state-space model (vDSSM) at the SN controller, where the CQI is\nsubjected to compression, quantization, and delay-induced errors. To robustly\nestimate interference power values under these impairments, we employ a\nlow-complexity, outlier-robust, sparse Student-t process regression (SPTPR)\nmethod. This is integrated into a modified unscented Kalman filter, which\nrecursively refines predicted interference using CQI, enabling accurate\nestimation and compensating protocol feedback delays, crucial for accurate LA.\nNumerical results show that the proposed method achieves over 10x lower\ncomplexity compared to a similar non-parametric baseline. It also maintains a\nBLER below the 90th percentile target of 1e-6 while delivering performance\ncomparable to a state-of-the-art supervised technique using only CQI reports."}
{"id": "2507.14173", "categories": ["eess.SP", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14173", "abs": "https://arxiv.org/abs/2507.14173", "authors": ["Karim Alghoul", "Hussein Al Osman", "Abdulmotaleb El Saddik"], "title": "Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model", "comment": "Accepted by IEEE International Instrumentation and Measurement\n  Technology Conference (I2MTC) 2025", "summary": "Human computer interaction has become integral to modern life, driven by\nadvancements in machine learning technologies. Affective computing, in\nparticular, has focused on systems that recognize, interpret, and respond to\nhuman emotions, often using wearable devices, which provide continuous data\nstreams of physiological signals. Among various physiological signals, the\nphotoplethysmogram (PPG) has gained prominence due to its ease of acquisition\nfrom widely available devices. However, the generalization of PPG-based emotion\nrecognition models across individuals remains an unresolved challenge. This\npaper introduces a novel hybrid architecture that combines Convolutional Neural\nNetworks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal\nConvolutional Networks (TCNs) to address this issue. The proposed model\nintegrates the strengths of these architectures to improve robustness and\ngeneralization. Raw PPG signals are fed into the CNN for feature extraction.\nThese features are processed separately by LSTM and TCN. The outputs from these\ncomponents are concatenated to generate a final feature representation, which\nserves as the input for classifying valence and arousal, the primary dimensions\nof emotion. Experiments using the Photoplethysmogram Dataset for Emotional\nAnalysis (PPGE) demonstrate that the proposed hybrid model achieves better\nmodel generalization than standalone CNN and LSTM architectures. Our results\nshow that the proposed solution outperforms the state-of-the-art CNN\narchitecture, as well as a CNN-LSTM model, in emotion recognition tasks with\nPPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we\nhighlight the model's effectiveness in handling subject variability."}
{"id": "2507.14184", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14184", "abs": "https://arxiv.org/abs/2507.14184", "authors": ["ZhengXiao He", "Jinghao Wen", "Huayu Li", "Ao Li"], "title": "NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment", "comment": null, "summary": "We present a novel and interpretable framework for electrocardiogram\n(ECG)-based disease detection that combines hyperdimensional computing (HDC)\nwith learnable neural encoding. Unlike conventional HDC approaches that rely on\nstatic, random projections, our method introduces a rhythm-aware and trainable\nencoding pipeline based on RR intervals, a physiological signal segmentation\nstrategy that aligns with cardiac cycles. The core of our design is a\nneural-distilled HDC architecture, featuring a learnable RR-block encoder and a\nBinaryLinear hyperdimensional projection layer, optimized jointly with\ncross-entropy and proxy-based metric loss. This hybrid framework preserves the\nsymbolic interpretability of HDC while enabling task-adaptive representation\nlearning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model\nsignificantly outperforms traditional HDC and classical ML baselines, achieving\n73.09\\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable\nrobustness on PTB-XL. Our framework offers an efficient and scalable solution\nfor edge-compatible ECG classification, with strong potential for interpretable\nand personalized health monitoring."}
{"id": "2507.14185", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14185", "abs": "https://arxiv.org/abs/2507.14185", "authors": ["Abdullah Ahmed", "Jeremy Gummeson"], "title": "Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices", "comment": null, "summary": "Latent spaces offer an efficient and effective means of summarizing data\nwhile implicitly preserving meta-information through relational encoding. We\nleverage these meta-embeddings to develop a modality-agnostic, unified encoder.\nOur method employs sensor-latent fusion to analyze and correlate multimodal\nphysiological signals. Using a compressed sensing approach with\nautoencoder-based latent space fusion, we address the computational challenges\nof biosignal analysis on resource-constrained devices. Experimental results\nshow that our unified encoder is significantly faster, lighter, and more\nscalable than modality-specific alternatives, without compromising\nrepresentational accuracy."}
{"id": "2507.14187", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14187", "abs": "https://arxiv.org/abs/2507.14187", "authors": ["Xiaojuan Zhang", "Tianyu Jiang", "Haoxiang Zong", "Chen Zhang", "Chendan Li", "Marta Molinas"], "title": "AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms", "comment": null, "summary": "The impedance network (IN) model is gaining popularity in the oscillation\nanalysis of wind farms. However, the construction of such an IN model requires\nimpedance curves of each wind turbine under their respective operating\nconditions, making its online application difficult due to the transmission of\nnumerous high-density impedance curves. To address this issue, this paper\nproposes an AI-based impedance encoding-decoding method to facilitate the\nonline construction of IN model. First, an impedance encoder is trained to\ncompress impedance curves by setting the number of neurons much smaller than\nthat of frequency points. Then, the compressed data of each turbine are\nuploaded to the wind farm and an impedance decoder is trained to reconstruct\noriginal impedance curves. At last, based on the nodal admittance matrix (NAM)\nmethod, the IN model of the wind farm can be obtained. The proposed method is\nvalidated via model training and real-time simulations, demonstrating that the\nencoded impedance vectors enable fast transmission and accurate reconstruction\nof the original impedance curves."}
{"id": "2507.14190", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14190", "abs": "https://arxiv.org/abs/2507.14190", "authors": ["Mingcheng Liao", "Zebang Feng", "Miao Fan", "Shengtong Xu", "Haoyi Xiong"], "title": "Traffic Signal Phase and Timing Estimation with Large-Scale Floating Car Data", "comment": "Accepted by ITSC'25", "summary": "Effective modern transportation systems depend critically on accurate Signal\nPhase and Timing (SPaT) estimation. However, acquiring ground-truth SPaT\ninformation faces significant hurdles due to communication challenges with\ntransportation departments and signal installers. As a result, Floating Car\nData (FCD) has become the primary source for large-scale SPaT analyses. Current\nFCD approaches often simplify the problem by assuming fixed schedules and basic\nintersection designs for specific times and locations. These methods fail to\naccount for periodic signal changes, diverse intersection structures, and the\ninherent limitations of real-world data, thus lacking a comprehensive framework\nthat is universally applicable. Addressing this limitation, we propose an\nindustrial-grade FCD analysis suite that manages the entire process, from\ninitial data preprocessing to final SPaT estimation. Our approach estimates\nsignal phases, identifies time-of-day (TOD) periods, and determines the\ndurations of red and green lights. The framework's notable stability and\nrobustness across diverse conditions, regardless of road geometry, is a key\nfeature. Furthermore, we provide a cleaned, de-identified FCD dataset and\nsupporting parameters to facilitate future research. Currently operational\nwithin our navigation platform, the system analyses over 15 million FCD records\ndaily, supporting over two million traffic signals in mainland China, with more\nthan 75\\% of estimations demonstrating less than five seconds of error."}
{"id": "2507.14191", "categories": ["eess.SP", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14191", "abs": "https://arxiv.org/abs/2507.14191", "authors": ["Cliver Oliver Turpo Benique"], "title": "School Attendance Control System Based on RFID Technology with Raspberry Pi and Arduino: EDURFID", "comment": "27 pages, 4 figures. Educational technology system for rural schools\n  in Peru. Implements RFID-based attendance control using open-source hardware\n  (Raspberry Pi, Arduino). System validation conducted at T\\'upac Amaru\n  Secondary Educational Institution, Coasa, Puno", "summary": "This paper presents EDURFID, an automated school attendance control system\nbased on RFID technology designed for rural educational institutions in Peru.\nThe system integrates open-source hardware (Raspberry Pi 5, Arduino UNO R3)\nwith RC522 RFID modules operating at 13.56 MHz, implementing a web architecture\ndeveloped in Python Django. The system demonstrates 100% precision in RFID\nreadings with 0.03-second response time, achieving 94% cost reduction compared\nto commercial solutions. Validation at T\\'upac Amaru Secondary Educational\nInstitution showed successful automation of attendance processes, saving 50\ndaily minutes of administrative time while providing real-time reporting\ncapabilities."}
{"id": "2507.14194", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14194", "abs": "https://arxiv.org/abs/2507.14194", "authors": ["David J Poland"], "title": "Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics", "comment": "Preliminary version of a predictive maintenance framework using\n  spiking neural networks and entropy-based analysis. To be expanded in future\n  publications with hardware implementations and real-time drift detection\n  modules. arXiv admin note: substantial text overlap with arXiv:2501.05087", "summary": "This paper presents a novel framework for pattern prediction and system\nprognostics centered on Spatiotemporal Permutation Entropy analysis integrated\nwith Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address\nthe challenge of understanding complex dynamical patterns in multidimensional\nsystems through an approach that combines entropy-based complexity measures\nwith advanced neural architectures. The system leverages dual computational\nstages: first implementing spatiotemporal entropy extraction optimized for\nmultiscale temporal and spatial data streams, followed by an integrated BEQRNN\nlayer that enables probabilistic pattern prediction with uncertainty\nquantification. This architecture achieves 81.17% accuracy in spatiotemporal\npattern classification with prediction horizons up to 200 time steps and\nmaintains robust performance across diverse regimes. Field testing across\nchaotic attractors, reaction-diffusion systems, and industrial datasets shows a\n79% increase in critical transition detection accuracy and 81.22% improvement\nin long-term prediction reliability. The framework's effectiveness in\nprocessing complex, multimodal entropy features demonstrates significant\npotential for real-time prognostic applications."}
{"id": "2507.14195", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14195", "abs": "https://arxiv.org/abs/2507.14195", "authors": ["Elzbieta Gruzewska", "Pooja Rao", "Sebastien Baur", "Matthew Baugh", "Mathias M. J. Bellaiche", "Sharanya Srinivas", "Octavio Ponce", "Matthew Thompson", "Pramod Rudrapatna", "Michael A. Sanchez", "Lawrence Z. Cai", "Timothy JA Chico", "Robert F. Storey", "Emily Maz", "Umesh Telang", "Shravya Shetty", "Mayank Daswani"], "title": "UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach", "comment": "31 pages, 11 tables, 9 figures, 14 supplementary tables, 4\n  supplementary figures", "summary": "Radar technology presents untapped potential for continuous, contactless, and\npassive heart rate monitoring via consumer electronics like mobile phones.\nHowever the variety of available radar systems and lack of standardization\nmeans that a large new paired dataset collection is required for each radar\nsystem. This study demonstrates transfer learning between frequency-modulated\ncontinuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,\nboth increasingly integrated into consumer devices. FMCW radar utilizes a\ncontinuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW\nradar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3\nreceiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz\nbandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we\nachieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage\nerror (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119\nparticipants, an average of 8 hours per participant). This model maintained\nperformance (under 5 MAE/10% MAPE) across various body positions and heart rate\nranges, with a 98.9% recall. We then fine-tuned a variant of this model,\ntrained on single-antenna and single-range bin FMCW data, using a small (N=376,\navg 6 minutes per participant) IR-UWB dataset. This transfer learning approach\nyielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE\nreduction over the IR-UWB baseline. This demonstration of transfer learning\nbetween radar systems for heart rate monitoring has the potential to accelerate\nits introduction into existing consumer devices."}
{"id": "2507.14196", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14196", "abs": "https://arxiv.org/abs/2507.14196", "authors": ["Zahra Teimouri-Jervekani", "Fahimeh Nasimi", "Mohammadreza Yazdchi", "Ghazal MogharehZadeh", "Javad Tezerji", "Farzan Niknejad Mazandarani", "Maryam Mohebbi"], "title": "Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs", "comment": null, "summary": "Background and Objective: Differentiating wide complex tachycardia (WCT) is\nclinically critical yet challenging due to morphological similarities in\nelectrocardiogram (ECG) signals between life-threatening ventricular\ntachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).\nMisdiagnosis carries fatal risks. We propose a computationally efficient deep\nlearning solution to improve diagnostic accuracy and provide model\ninterpretability for clinical deployment.\n  Methods: A novel lightweight parallel deep architecture is introduced. Each\npipeline processes individual ECG leads using two 1D-CNN blocks to extract\nlocal features. Feature maps are concatenated across leads, followed by LSTM\nlayers to capture temporal dependencies. Final classification employs fully\nconnected layers. Explainability is achieved via Shapley Additive Explanations\n(SHAP) for local/global interpretation. The model was evaluated on a 35-subject\nECG database using standard performance metrics.\n  Results: The model achieved $95.63\\%$ accuracy ($95\\%$ CI: $93.07-98.19\\%$),\nwith sensitivity=$95.10\\%$, specificity=$96.06\\%$, and F1-score=$95.12\\%$. It\noutperformed state-of-the-art methods in both accuracy and computational\nefficiency, requiring minimal CNN blocks per pipeline. SHAP analysis\ndemonstrated clinically interpretable feature contributions.\n  Conclusions: Our end-to-end framework delivers high-precision WCT\nclassification with minimal computational overhead. The integration of SHAP\nenhances clinical trust by elucidating decision logic, supporting rapid,\ninformed diagnosis. This approach shows significant promise for real-world ECG\nanalysis tools."}
{"id": "2507.14206", "categories": ["eess.SP", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14206", "abs": "https://arxiv.org/abs/2507.14206", "authors": ["Zhijiang Tang", "Jiaxin Qi", "Yuhua Zheng", "Jianqiang Huang"], "title": "A Comprehensive Benchmark for Electrocardiogram Time-Series", "comment": "Accepted to ACM MM 2025", "summary": "Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial\nfor assessing cardiac health and diagnosing various diseases. Given its\ntime-series format, ECG data is often incorporated into pre-training datasets\nfor large-scale time-series model training. However, existing studies often\noverlook its unique characteristics and specialized downstream applications,\nwhich differ significantly from other time-series data, leading to an\nincomplete understanding of its properties. In this paper, we present an\nin-depth investigation of ECG signals and establish a comprehensive benchmark,\nwhich includes (1) categorizing its downstream applications into four distinct\nevaluation tasks, (2) identifying limitations in traditional evaluation metrics\nfor ECG analysis, and introducing a novel metric; (3) benchmarking\nstate-of-the-art time-series models and proposing a new architecture. Extensive\nexperiments demonstrate that our proposed benchmark is comprehensive and\nrobust. The results validate the effectiveness of the proposed metric and model\narchitecture, which establish a solid foundation for advancing research in ECG\nsignal analysis."}
{"id": "2507.14208", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14208", "abs": "https://arxiv.org/abs/2507.14208", "authors": ["Mohammadreza F. Imani", "Alexander L. Colson", "Leslie K. Miller", "Jorge A. Valdez", "Jose C. Sanchez", "Richard F. Rader"], "title": "Toward intelligent wireless networks in computer chassis", "comment": "9 pages, 5 figures", "summary": "Processing the exponentially growing amount of data produced daily requires\nefficient communication between different processing units in a computer.\nTraditionally, wired interconnects have been used to maintain these data links\ndue to their energy efficiency and ability to support high data rates. However,\nas computing demands continue to increase in size and speed, these wired\ninterconnects can become longer and less effective. One possible solution is to\nenhance the wired interconnects with short-range wireless communication (SRWC),\nwhich offers flexible resource allocation and the ability to broadcast data.\nHowever, implementing SRWC inside a computer chassis presents challenges due to\nmultiple scattering. This scattering stretches the channel impulse response\n(CIR), leading to inter-symbol interference (ISI) and limiting data rates. To\naddress this issue, we propose transforming the computer chassis into a smart\nradio environment by utilizing a reconfigurable intelligent surface (RIS). The\nRIS elements adjust the phase of reflected waves so that the multipath\ncomponents combine at the receiver in a way that creates a pulse-like CIR. This\napproach has been experimentally validated within a typical computer chassis.\nThe results of this study pave the way for integrating RIS-enabled SRWC to\nenhance wireless links in both current and future data processing units."}
{"id": "2507.14210", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14210", "abs": "https://arxiv.org/abs/2507.14210", "authors": ["Jiayuan Wei", "Qingwei Jiang", "Wen Fang", "Mingqing Liu", "Qingwen Liu", "Wen Chen", "Qingqing Wu"], "title": "System Design and Performance Analysis for RIS-assisted Terahertz Self-Alignment Beamforming", "comment": null, "summary": "The widespread deployment of Internet of Things(IoT) devices underscores the\nneed for sustainable wireless solutions capable of simultaneously transferring\nboth energy and information. Terahertz (THz) band-enabled simultaneous wireless\ninformation and power transfer (SWIPT) systems offer ultra-high data rates and\nexpansive bandwidth. However, THz waves are inherently susceptible to severe\npath loss and beam misalignment due to their narrow-beam characteristics. In\nthis context, this paper proposes a reconfigurable intelligent\nsurface(RIS)-assisted transmitter architecture for the THz-SWIPT system, which\nenables end-to-end self-alignment for steady-state transmission. The proposed\nsystem incorporates phase conjugate circuits to achieve self-aligned\nbeamforming, facilitating the dynamic tracking of mobile IoT devices without\nthe need for beam training. Additionally, active amplification within the RIS\narrays compensates for cascaded channel attenuation through an iterative power\ncycle, thereby enhancing the energy transmission efficiency. Theoretical models\nand simulations indicate that the proposed system significantly mitigates\nsidelobe interference, achieving a transmission efficiency of up to 73.26% over\na 2 meter distance with self-alignment."}
{"id": "2507.14216", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14216", "abs": "https://arxiv.org/abs/2507.14216", "authors": ["Manish Kumar", "Tzu-Hsuan Chou", "Byunghyun Lee", "Nicolò Michelusi", "David J. Love", "Yaguang Zhang", "James V. Krogmeier"], "title": "Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems", "comment": "This paper has been submitted to IEEE Transactions on Wireless\n  Communications", "summary": "Low-latency localization is critical in cellular networks to support\nreal-time applications requiring precise positioning. In this paper, we propose\na distributed machine learning (ML) framework for fingerprint-based\nlocalization tailored to cell-free massive multiple-input multiple-output\n(MIMO) systems, an emerging architecture for 6G networks. The proposed\nframework enables each access point (AP) to independently train a Gaussian\nprocess regression model using local angle-of-arrival and received signal\nstrength fingerprints. These models provide probabilistic position estimates\nfor the user equipment (UE), which are then fused by the UE with minimal\ncomputational overhead to derive a final location estimate. This decentralized\napproach eliminates the need for fronthaul communication between the APs and\nthe central processing unit (CPU), thereby reducing latency. Additionally,\ndistributing computational tasks across the APs alleviates the processing\nburden on the CPU compared to traditional centralized localization schemes.\nSimulation results demonstrate that the proposed distributed framework achieves\nlocalization accuracy comparable to centralized methods, despite lacking the\nbenefits of centralized data aggregation. Moreover, it effectively reduces\nuncertainty of the location estimates, as evidenced by the 95\\% covariance\nellipse. The results highlight the potential of distributed ML for enabling\nlow-latency, high-accuracy localization in future 6G networks."}
{"id": "2507.14220", "categories": ["eess.SP", "cs.LG", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2507.14220", "abs": "https://arxiv.org/abs/2507.14220", "authors": ["Haitian Hu", "Wei Zhang", "Feng Feng", "Zhiguo Zhang", "Qi-Jun Zhang"], "title": "Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters", "comment": null, "summary": "This article introduces an advanced space mapping (SM) technique that applies\na shared electromagnetic (EM)-based coarse model for multistate tuning-driven\nmultiphysics optimization of tunable filters. The SM method combines the\ncomputational efficiency of EM single-physics simulations with the precision of\nmultiphysics simulations. The shared coarse model is based on EM single-physics\nresponses corresponding to various nontunable design parameters values.\nConversely, the fine model is implemented to delineate the behavior of\nmultiphysics responses concerning both nontunable and tunable design parameter\nvalues. The proposed overall surrogate model comprises multiple subsurrogate\nmodels, each consisting of one shared coarse model and two distinct mapping\nneural networks. The responses from the shared coarse model in the EM\nsingle-physics filed offer a suitable approximation for the fine responses in\nthe multiphysics filed, whereas the mapping neural networks facilitate\ntransition from the EM single-physics field to the multiphysics field. Each\nsubsurrogate model maintains consistent nontunable design parameter values but\npossesses unique tunable design parameter values. By developing multiple\nsubsurrogate models, optimization can be simultaneously performed for each\ntuning state. Nontunable design parameter values are constrained by all tuning\nstates, whereas tunable design parameter values are confined to their\nrespective tuning states. This optimization technique simultaneously accounts\nfor all the tuning states to fulfill the necessary multiple tuning state\nrequirements. Multiple EM and multiphysics training samples are generated\nconcurrently to develop the surrogate model. Compared with existing direct\nmultiphysics parameterized modeling techniques, our proposed method achieves\nsuperior multiphysics modeling accuracy with fewer training samples and reduced\ncomputational costs."}
{"id": "2507.14224", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14224", "abs": "https://arxiv.org/abs/2507.14224", "authors": ["Benoît Brebion", "Alban Gallard", "Katrin Sippel", "Amer Zaylaa", "Hubert Preissl", "Sahar Moghimi", "Fabrice Wallois", "Yaël Frégier"], "title": "Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG", "comment": null, "summary": "Background and objective: Brain activity in premature newborns has\ntraditionally been studied using electroencephalography (EEG), leading to\nsubstantial advances in our understanding of early neural development. However,\nsince brain development takes root at the fetal stage, a critical window of\nthis process remains largely unknown. The only technique capable of recording\nneural activity in the intrauterine environment is fetal magnetoencephalography\n(fMEG), but this approach presents challenges in terms of data quality and\nscarcity. Using artificial intelligence, the present research aims to transfer\nthe well-established knowledge from EEG studies to fMEG to improve\nunderstanding of prenatal brain development, laying the foundations for better\ndetection and treatment of potential pathologies. Methods: We developed an\nunpaired diffusion translation method based on dual diffusion bridges, which\nnotably includes numerical integration improvements to obtain more qualitative\nresults at a lower computational cost. Models were trained on our unpaired\ndataset of bursts of spontaneous activity from 30 high-resolution premature\nnewborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that\nour method achieves significant improvement upon previous results obtained with\nGenerative Adversarial Networks (GANs), by almost 5% on the mean squared error\nin the time domain, and completely eliminating the mode collapse problem in the\nfrequency domain, thus achieving near-perfect signal fidelity. Conclusion: We\nset a new state of the art in the EEG-fMEG unpaired translation problem, as our\ndeveloped tool completely paves the way for early brain activity analysis.\nOverall, we also believe that our method could be reused for other unpaired\nsignal translation applications."}
{"id": "2507.14228", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14228", "abs": "https://arxiv.org/abs/2507.14228", "authors": ["Xiaobin Zhu", "Minling Zhang", "Guofa Cai", "Jiguang He", "Georges Kaddoum"], "title": "Design of A New Multiple-Chirp-Rate Index Modulation for LoRa Networks", "comment": "13 pages,11 figures,3 tables", "summary": "We propose a multiple chirp rate index modulation (MCR-IM) system based on\nZadoff-Chu (ZC) sequences that overcomes the problems of low transmission rate\nand large-scale access in classical LoRa networks. We demonstrate the extremely\nlow cross-correlation of MCR-IM signals across different spread factors,\nshowing that the proposed MCR-IM system also inherits the characteristics of ZC\nsequences modulation. Moreover, we derive an approximate closed-form expression\nfor the bit-error rate (BER) of the proposed MCR-IM system over Nakagami-m\nfading channels. Simulation results confirm the accuracy of the derived\nclosed-form expression and demonstrate that the MCR-IM system achieves higher\nlevels of spectral efficiency (SE) compared to existing systems. In this\ncontext, assigning multiple chirp rates to each user results in a reduction in\nthe number of parallel channels. To mitigate this issue, we propose a peak\ndetection based successive interference cancellation (PD-SIC) algorithm to\naccommodate more users. Compared to orthogonal scatter chirp spreading spectrum\nsystem that names OrthoRa, the MCR-IM system with PD-SIC algorithm achieves\nlower BER levels. For a similar number of collision signals, the throughput of\nthe MCR-IM system is enhanced by 16% to 21%. Owing to these advantages, the\nproposed MCR-IM is well suited for large-scale, high-rate LoRa network\napplications."}
{"id": "2507.14299", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14299", "abs": "https://arxiv.org/abs/2507.14299", "authors": ["Yu Bai", "Yifan Zhang", "Boxuan Xie", "Zheng Chang", "Yanru Zhang", "Riku Jantti", "Zhu Han"], "title": "Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) equipped with integrated sensing and\ncommunication (ISAC) capabilities are envisioned to play a pivotal role in\nfuture wireless networks due to their enhanced flexibility and efficiency.\nHowever, jointly optimizing UAV trajectory planning, multi-user communication,\nand target sensing under stringent resource constraints and time-critical\nconditions remains a significant challenge. To address this, we propose an Age\nof Information (AoI)-centric UAV-ISAC system that simultaneously performs\ntarget sensing and serves multiple ground users, emphasizing information\nfreshness as the core performance metric. We formulate a long-term average AoI\nminimization problem that jointly optimizes the UAV's flight trajectory and\nbeamforming. To tackle the high-dimensional, non-convexity of this problem, we\ndevelop a deep reinforcement learning (DRL)-based algorithm capable of\nproviding real-time decisions on UAV movement and beamforming for both radar\nsensing and multi-user communication. Specifically, a Kalman filter is employed\nfor accurate target state prediction, regularized zero-forcing is utilized to\nmitigate inter-user interference, and the Soft Actor-Critic algorithm is\napplied for training the DRL agent on continuous actions. The proposed\nframework adaptively balances the trade-offs between sensing accuracy and\ncommunication quality. Extensive simulation results demonstrate that our\nproposed method consistently achieves lower average AoI compared to baseline\napproaches."}
{"id": "2507.14309", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14309", "abs": "https://arxiv.org/abs/2507.14309", "authors": ["Mert Torun", "Alireza Parsay", "Yasamin Mostofi"], "title": "Fast and Robust Stationary Crowd Counting with Commodity WiFi", "comment": null, "summary": "This paper introduces a novel method for estimating the size of seated crowds\nwith commodity WiFi signals, by leveraging natural body fidgeting behaviors as\na passive sensing cue. Departing from prior binary fidget representations, our\napproach leverages the bandwidth of the received signal as a finer-grained and\nrobust indicator of crowd counts. More specifically, we propose a mathematical\nmodel that relates the probability density function (PDF) of the signal\nbandwidth to the crowd size, using a principled derivation based on the PDF of\nan individual's fidget-induced bandwidth. To characterize the individual\nfidgeting PDF, we use publicly available online videos, each of a seated\nindividual, from which we extract body motion profiles using vision techniques,\nfollowed by a speed-to-bandwidth conversion inspired by Carson's Rule from\nanalog FM radio design. Finally, to enhance robustness in real-world\ndeployments where unrelated motions may occur nearby, we further introduce an\nanomaly detection module that filters out non-fidget movements. We validate our\nsystem through 42 experiments across two indoor environments with crowd sizes\nup to and including 13 people, achieving a mean absolute error of 1.04 and a\nnormalized mean square error of 0.15, with an average convergence time of 51\nseconds, significantly reducing the convergence time as compared to the state\nof the art. Additional simulation results demonstrate scalability to larger\ncrowd sizes. Overall, our results show that our pipeline enables fast, robust,\nand highly accurate counting of seated crowds."}
{"id": "2507.14310", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14310", "abs": "https://arxiv.org/abs/2507.14310", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "title": "Optimizing Network Performance and Resource Allocation in HAPS-UAV Integrated Sensing and Communication Systems for 6G", "comment": null, "summary": "This paper proposes an innovative approach by leveraging uncrewed aerial\nvehicles (UAVs) as base stations (BSs) and a high-altitude platform station\n(HAPS) as the central processing unit (CPU) in an integrated sensing and\ncommunication (ISAC) system for 6G networks. We explore the challenges,\napplications, and advantages of ISAC systems in next-generation networks,\nhighlighting the significance of optimizing position and power control. Our\napproach integrates HAPS and UAVs to enhance wireless coverage, particularly in\nremote areas. UAVs function as dual-purpose access points (APs), using their\nmaneuverability and line-of-sight (LoS) aerial-to-ground (A2G) links to\ntransmit combined communication and sensing signals. The scheme operates in two\ntime slots: in the first slot, UAVs transmit dedicated signals to communication\nusers (CUs) and potential targets. UAVs detect targets in specific ground\nlocations and, after signal transmission, receive reflected signals from\ntargets. In the second slot, UAVs relay these signals to HAPS, which performs\nbeamforming to align signals for each CU from various UAVs. UAVs decode\ninformation from HAPS and adjust transmissions to maximize the beam pattern\nefficiency toward the desired targets. We formulate a multi-objective\noptimization problem to maximize both the minimum\nsignal-to-interference-plus-noise ratio (SINR) for CUs and the echo signal\npower from the targets. This is achieved by finding the optimal power\nallocation for CUs in each UAV, subject to constraints on the maximum total\npower in each UAV and the transmitted beam pattern gain. Simulation results\ndemonstrate the effectiveness of this approach in enhancing network\nperformance, resource allocation, fairness, and system optimization. Using HAPS\nas the CPU, computational tasks are offloaded from UAVs, which conserves energy\nand improves network performance."}
{"id": "2507.14469", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14469", "abs": "https://arxiv.org/abs/2507.14469", "authors": ["Shuxian Wu", "Shun Yao", "Xingyu Du", "Chin-Yu Chang", "Roy H. Olsson III"], "title": "Spatially tailored spin wave excitation for spurious-free, low-loss magnetostatic wave filters with ultra-wide frequency tunability", "comment": null, "summary": "Yttrium iron garnet magnetostatic wave (MSW) radio frequency (RF) cavity\nfilters are promising for sixth-generation (6G) communication systems due to\ntheir wide frequency tunability. However, the presence of severe spurious modes\narising from the finite cavity dimensions severely degrades the filter\nperformance. We present a half-cone transducer that spatially tailors spin wave\nexcitation to selectively enhance the primary cavity modes comprising the MSW\nfilter passband, while strongly suppressing the undesired spurious modes.\nTheoretical analysis, numerical simulations and experiments verify the\neffectiveness of the spatially tailored technique. We utilize the half-cone\ntransducer to demonstrate a spurious-free, single-cavity half-cone MSW filter\n(HC-MSWF) with an insertion loss (IL) of 2.4-3.2 dB over a frequency tuning\nrange of 6.3-16.8 GHz. Extending our study, we further demonstrate a\nspurious-free, dual-cavity HC-MSWF with an unprecedented tuning range of 21.7\nGHz (9.8-31.5 GHz) while maintaining a low IL of 2.9-3.8 dB. This significant\nadvance in performance will enable highly reconfigurable and robust 6G\nnetworks."}
{"id": "2507.14622", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14622", "abs": "https://arxiv.org/abs/2507.14622", "authors": ["Wahab Khawaja", "Ismail Guvenc", "Rune Hylsberg Jacobsen"], "title": "Propagation Channel Modeling for LEO Satellite Missions Using Ray-Tracing Simulations", "comment": "This manuscript is submitted to MILCOM 2025 conference", "summary": "This work presents a high-resolution, ray-tracing-based channel modeling for\nLow Earth Orbit (LEO) satellite-to-ground links in a suburban environment at\nX-band. Using simulations conducted in Wireless InSite, we develop a parametric\nchannel model that characterizes both large- and small-scale fading effects\nacross different satellite elevation angles. Large-scale fading incorporates\nattenuation due to terrain-induced shadowing and dynamic environmental factors\nsuch as weather conditions, and is compared with 3GPP NTN channel model.\nAdditionally, we quantify link degradation resulting from ground station (GS)\nantenna misalignment, considering both fixed single-element and electronically\nsteerable phased-array antennas. Small-scale fading is modeled by fitting a\nshadowed and non-shadowed Rician distribution to the fading statistics at\nvarious satellite elevations. To the best of our knowledge, this is the first\nstudy to propose a comprehensive elevation-aware channel model for\nsatellite-to-ground propagation at X-band, integrating ray-traced environmental\ndynamics, elevation-dependent fading, and phased-array beam misalignment\neffects."}
{"id": "2507.14804", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14804", "abs": "https://arxiv.org/abs/2507.14804", "authors": ["Jingjing Zhao", "Qian Xu", "Kaiquan Cai", "Yanbo Zhu", "Xidong Mu", "Yuanwei Liu"], "title": "Movable-Element STARS-Aided Secure Communications", "comment": null, "summary": "A novel movable-element (ME) enabled simultaneously transmitting and\nreflecting surface (ME-STARS)-aided secure communication system is\ninvestigated. Against the full-space eavesdropping, MEs are deployed at the\nSTARS for enhancing the physical layer security by exploiting higher spatial\ndegrees of freedom. Specifically, a sum secrecy rate maximization problem is\nformulated, which jointly optimizes the passive beamforming and the MEs\npositions at the ME-STARS, as well as the active beamforming at the base\nstation. To solve the resultant non-convex optimization problem involving\nhighly-coupled variables, an alternating optimization-based iterative algorithm\nis developed, decomposing the original problem into three subproblems. In\nparticular, for the MEs position optimization subproblem, a gradient ascent\nalgorithm is employed to iteratively refine the MEs' locations within the\nconfined region. Moreover, the the active and passive beamforming subproblems\nare solved by employing successive convex approximation. Numerical results\nunveil that: 1) ME-STARS significantly improves the secrecy performance\ncompared to the conventional STARS with fixed-position elements; and 2) The\nsecrecy rate achieved by the ME-STARS gets saturated within limited movable\nregion size."}
{"id": "2507.14831", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14831", "abs": "https://arxiv.org/abs/2507.14831", "authors": ["Mengyu Qian", "Xidong Mu", "Li You", "Michail Matthaiou"], "title": "Pinching-Antenna-based Communications: Spectral Efficiency Analysis and Deployment Strategies", "comment": "13 pages, 8 figures", "summary": "A multiple-waveguide pinching-antenna (PA)-based multi-user communication\nsystem is investigated. With a given number of PAs, two deployment strategies\nare considered, namely the centralized PA deployment, where all PAs are\nswitched between waveguides to serve users in a time-division manner to avail\nof beamforming gain, and the distributed PA deployment, where a single PA is\ndeployed on each waveguide to simultaneously serve multiple users by leveraging\nthe multiplexing gain. The spectral efficiency (SE) achieved by each deployment\nstrategy is analyzed: i) For the centralized deployment, the positioning\nstrategy of PAs on each waveguide is determined first with the aim of\nmaximizing the channel gain of the corresponding nearest served user. Based on\nthis, the corresponding system SE is derived. ii) For the distributed\ndeployment, the system SE under the maximum ratio transmission (MRT) is first\nobtained. To obtain an analytically tractable form, the stationary phase method\nis utilized to approximate the system SE. The approximation result reveals that\nthe average inter-user interference can be negligible with a large waveguide\nspacing and thus the simple MRT is appealing for PA-based multi-user\ncommunications. Furthermore, the system SEs achieved by the two strategies are\ncompared in both the high and low signal-to-noise ratio (SNR) regimes. Our\nanalysis suggests that at high SNRs, the distributed deployment is superior to\nachieve the maximal system SE, while the centralized deployment is more\nsuitable for the low-SNR regime. Finally, the theoretical analysis is verified\nthrough simulations."}
{"id": "2507.14856", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14856", "abs": "https://arxiv.org/abs/2507.14856", "authors": ["Victor Shatov", "Steffen Schieler", "Charlotte Muth", "José Miguel Mateos-Ramos", "Ivo Bizon", "Florian Euchner", "Sebastian Semper", "Stephan ten Brink", "Gerhard Fettweis", "Christian Häger", "Henk Wymeersch", "Laurent Schmalen", "Reiner Thomä", "Norman Franchi"], "title": "Integrated Radio Sensing Capabilities for 6G Networks: AI/ML Perspective", "comment": "30 pages, 18 figures", "summary": "The sixth-generation wireless communications (6G) is often labeled as\n\"connected intelligence\". Radio sensing, aligned with machine learning (ML) and\nartificial intelligence (AI), promises, among other benefits, breakthroughs in\nthe system's ability to perceive the environment and effectively utilize this\nawareness. This article offers a tutorial-style survey of AI and ML approaches\nto enhance the sensing capabilities of next-generation wireless networks. To\nthis end, while staying in the framework of integrated sensing and\ncommunication (ISAC), we expand the term \"sensing\" from radar, via spectrum\nsensing, to miscellaneous applications of radio sensing like non-cooperative\ntransmitter localization. We formulate the problems, explain the\nstate-of-the-art approaches, and detail AI-based techniques to tackle various\nobjectives in the context of wireless sensing. We discuss the advantages,\nenablers, and challenges of integrating various sensing capabilities into an\nenvisioned AI-powered multimodal multi-task network. In addition to the\ntutorial-style core of this work based on direct authors' involvement in 6G\nresearch problems, we review the related literature, and provide both a good\nstart for those entering this field of research, and a topical overview for a\ngeneral reader with a background in wireless communications"}
{"id": "2507.14888", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14888", "abs": "https://arxiv.org/abs/2507.14888", "authors": ["Zhuo Wang"], "title": "Stabilization of the bias point in MZM modulators", "comment": null, "summary": "This article mainly introduces the role of MZM in practical communication\nsystems, the materials used to make MZM modulators such as lithium niobate, and\nits working principle. It also explains why it changes due to environmental\nfactors. This leads to the introduction of a method that controls the stable\npoints of MZM by algorithmically controlling the voltage, and the algorithm is\nverified through experiments. Finally, a summary and outlook on the future\ndevelopment of MZM are provided."}
{"id": "2507.14937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14937", "abs": "https://arxiv.org/abs/2507.14937", "authors": ["Hugh L Kennedy"], "title": "Phase-optimised linearly-constrained minimum-variance beamformers", "comment": "Initial draft", "summary": "A novel procedure for the determination of the optimal group-delay for a\nLinearly-Constrained Minimum-Variance (LCMV) beamformer is proposed. Two ways\nof selecting the optimal delay are recommended: the first is the solution that\nminimizes the noise power; the second is the solution that minimizes the\nprocessing delay. The potential of this hitherto unexplored degree of design\nfreedom is explored using simulated Very-High-Frequency (VHF) communication,\nand Ultra-High-Frequency (UHF) bistatic radar, applications."}
{"id": "2507.14945", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14945", "abs": "https://arxiv.org/abs/2507.14945", "authors": ["Bin Wang", "Jun Fang", "Jieru Du", "Shihai Shao"], "title": "Jamming-Resistant AAV Communications: A Multichannel-Aided Approach", "comment": null, "summary": "Jamming cancellation is essential to reliable unmanned autonomous vehicle\n(AAV) communications in the presence of malicious jammers. In this paper, we\ndevelop a practical multichannel-aided jamming cancellation method to realize\nsecure AAV communications. The proposed method is capable of simultaneously\nachieving timing/frequency synchronization as well as jamming cancellation.\nMore importantly, our method does not need the signal's/jammer's channel state\ninformation. It only utilizes the knowledge of the legitimate sender's preamble\nsequence that is available in existing communication protocols. We also analyze\nthe length of the preamble sequence required for successful synchronization and\nsignal recovery. Experimental results on the built hardware platform show that,\nwith a two-antenna receiver, the proposed method can successfully decode the\nsignal of interest even when the jamming signal is $40$dB stronger than the\ncommunication signal."}
{"id": "2507.14951", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14951", "abs": "https://arxiv.org/abs/2507.14951", "authors": ["Hongzhi Zhu", "Wei Xu", "Xiaohu You"], "title": "Latent-attention Based Transformer for Near ML Polar Decoding in Short-code Regime", "comment": null, "summary": "Transformer architectures have emerged as promising deep learning (DL) tools\nfor modeling complex sequence-to-sequence interactions in channel decoding.\nHowever, current transformer-based decoders for error correction codes (ECCs)\ndemonstrate inferior performance and generalization capabilities compared to\nconventional algebraic decoders, especially in short-code regimes. In this\nwork, we propose a novel latent-attention based transformer (LAT) decoder for\npolar codes that addresses the limitations on performance and generalization\nthrough three pivotal innovations. First, we develop a latent-attention\nmechanism that supersedes the conventional self-attention mechanism. This\narchitectural modification enables independent learning of the Query and Key\nmatrices for code-aware attention computation, decoupling them from the Value\nmatrix to emphasize position-wise decoding interactions while reducing context\ncorrelation interference. Second, we devise an advanced training framework\nincorporating three synergistic components: entropy-aware importance sampling\nthat emphasizes low-probability regions in the signal constellation space,\nexperience reflow that introduces empirical labels to improve characterization\nof decoding boundaries, and dynamic label smoothing for likelihood-based\nregularization. Third, we propose a code-aware mask scheme which allows dynamic\nadaptation for varying code configurations. Numerical evaluations demonstrate\nthat the proposed LAT decoder achieves near maximum-likelihood (ML) performance\nin terms of both bit error rate (BER) and block error rate (BLER) for\nshort-length polar codes. Furthermore, the architecture exhibits robust\ngeneralization capabilities across diverse code rates and code lengths."}
{"id": "2507.14982", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14982", "abs": "https://arxiv.org/abs/2507.14982", "authors": ["Kareem M. Attiah", "Wei Yu"], "title": "How Many Simultaneous Beamformers are Needed for Integrated Sensing and Communications?", "comment": "26 pages, 7 figures, Submitted to T-IT for future publication", "summary": "Consider a downlink integrated sensing and communications (ISAC) system in\nwhich a base station employs linear beamforming to communicate to $K$ users,\nwhile simultaneously uses sensing beams to perform a sensing task of estimating\n$L$ real parameters. How many beamformers are needed to achieve the best\nperformance for both sensing and communications? This paper establishes bounds\non the minimum number of downlink beamformers, in which sensing performance is\nmeasured in terms of the Cram\\'{e}r-Rao bound for parameter estimation and\ncommunications performance is measured in terms of the\nsignal-to-interference-and-noise ratios. We show that an ISAC system requires\nat most $K + \\sqrt{\\frac{L(L+1)}{2}}$ beamformers if the remote users have the\nability to cancel the interference caused by the sensing beams. If cancelling\ninterference due to the sensing beams is not possible, the bound becomes\n$\\sqrt{K^2 + \\frac{L(L+1)}{2}}$. Interestingly, in the latter case, the bound\non the number of beamformers is less than the sum of the bounds for each task\nindividually. These results can be extended to sensing tasks for which the\nperformance is measured as a function of $d$ quadratic terms in the\nbeamformers. In this case, the bound becomes $K + \\sqrt{d}$ and $\\sqrt{K^2 +\nd}$, respectively. Specifically, for estimating complex path losses and\nangles-of-arrival of $N_\\text{tr}$ targets while communicating to $K$ users,\nthe bound on the minimum number of beamformers scales linearly in $K$ and in\n$N_\\text{tr}$, assuming interference from sensing can be cancelled. When\ninterference cancellation is not possible, the following exact characterization\nfor the case of $N_\\text{tr} = 1$ can be obtained: when $K=0$ or $1$, two\nbeamformers should be used; when $K \\ge 2$, exactly $K$ beamformers should be\nused, i.e., communication beamformers alone are already sufficient."}
{"id": "2507.15116", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15116", "abs": "https://arxiv.org/abs/2507.15116", "authors": ["Zichao Zhang", "Melda Yuksel", "Gokhan M. Guvensen", "Halim Yanikomeroglu"], "title": "PAPR Analysis for MIMO FTN Signaling with Gaussian Symbols", "comment": null, "summary": "Faster-than-Nyquist signaling serves as a promising solution for improving\nspectral efficiency in future generations of communications. However, its\nnature of fast acceleration brings highly overlapped pulses that lead to worse\npeak-to-average power ratio (PAPR) performance. In this paper, we investigate\nthe PAPR behavior of MIMO FTN using Gaussian symbols under optimal power\nallocation for two power constraints: fixed transmit power and fixed received\nsignal-to-noise-ratio (SNR). Our findings reveal that PAPR is mainly determined\nby the acceleration factor and the power constraint, but power allocation\noptimization does not change the PAPR behavior for Gaussian signaling."}
{"id": "2507.15118", "categories": ["eess.SP", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.15118", "abs": "https://arxiv.org/abs/2507.15118", "authors": ["Szymon Mazurek", "Stephen Moore", "Alessandro Crimi"], "title": "Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings", "comment": null, "summary": "Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce\nneurologists and costly diagnostic tools. We propose a graph-based deep\nlearning framework to detect epilepsy from low-cost Electroencephalography\n(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus\nis on fair, accessible automatic assessment and explainability to shed light on\nepilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,\nclassify them, and identify interchannel relationships and temporal dynamics\nusing graph attention networks (GAT). To emphasize connectivity biomarkers, we\nadapt the inherently node-focused GAT to analyze edges. We also designed signal\npreprocessing for low-fidelity recordings and a lightweight GAT architecture\ntrained on Google Colab and deployed on RaspberryPi devices. Results: The\napproach achieves promising classification performance, outperforming a\nstandard classifier based on random forest and graph convolutional networks in\nterms of accuracy and robustness over multiple sessions, but also highlighting\nspecific connections in the fronto-temporal region. Conclusions: The results\nhighlight the potential of GATs to provide insightful and scalable diagnostic\nsupport for epilepsy in underserved regions, paving the way for affordable and\naccessible neurodiagnostic tools."}
{"id": "2507.15255", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15255", "abs": "https://arxiv.org/abs/2507.15255", "authors": ["Deyun Zhang", "Xiang Lan", "Shijia Geng", "Qinghao Zhao", "Sumei Fan", "Mengling Feng", "Shenda Hong"], "title": "MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations", "comment": null, "summary": "Electrocardiogram (ECG) plays a foundational role in modern cardiovascular\ncare, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and\nconduction disorders. While machine learning has achieved expert-level\nperformance in ECG interpretation, the development of clinically deployable\nmultimodal AI systems remains constrained, primarily due to the lack of\npublicly available datasets that simultaneously incorporate raw signals,\ndiagnostic images, and interpretation text. Most existing ECG datasets provide\nonly single-modality data or, at most, dual modalities, making it difficult to\nbuild models that can understand and integrate diverse ECG information in\nreal-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext\nECG-Text-Image), the first large-scale ECG dataset that synchronizes raw\nwaveform data, high-resolution plotted images, and detailed textual\ninterpretations generated by large language models. In addition, MEETI includes\nbeat-level quantitative ECG parameters extracted from each lead, offering\nstructured parameters that support fine-grained analysis and model\ninterpretability. Each MEETI record is aligned across four components: (1) the\nraw ECG waveform, (2) the corresponding plotted image, (3) extracted feature\nparameters, and (4) detailed interpretation text. This alignment is achieved\nusing consistent, unique identifiers. This unified structure supports\ntransformer-based multimodal learning and supports fine-grained, interpretable\nreasoning about cardiac health. By bridging the gap between traditional signal\nanalysis, image-based interpretation, and language-driven understanding, MEETI\nestablished a robust foundation for the next generation of explainable,\nmultimodal cardiovascular AI. It offers the research community a comprehensive\nbenchmark for developing and evaluating ECG-based AI systems."}
{"id": "2507.15256", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15256", "abs": "https://arxiv.org/abs/2507.15256", "authors": ["Zihao Hu", "Jia Yan", "Ying-Jun Angela Zhang", "Jun Zhang", "Khaled B. Letaief"], "title": "Optimal Transceiver Design in Over-the-Air Federated Distillation", "comment": "13 pages, 7 figures, submitted to IEEE Transactions on Wireless\n  Communications", "summary": "The rapid proliferation and growth of artificial intelligence (AI) has led to\nthe development of federated learning (FL). FL allows wireless devices (WDs) to\ncooperatively learn by sharing only local model parameters, without needing to\nshare the entire dataset. However, the emergence of large AI models has made\nexisting FL approaches inefficient, due to the significant communication\noverhead required. In this paper, we propose a novel over-the-air federated\ndistillation (FD) framework by synergizing the strength of FL and knowledge\ndistillation to avoid the heavy local model transmission. Instead of sharing\nthe model parameters, only the WDs' model outputs, referred to as knowledge,\nare shared and aggregated over-the-air by exploiting the superposition property\nof the multiple-access channel. We shall study the transceiver design in\nover-the-air FD, aiming to maximize the learning convergence rate while meeting\nthe power constraints of the transceivers. The main challenge lies in the\nintractability of the learning performance analysis, as well as the non-convex\nnature and the optimization spanning the whole FD training period. To tackle\nthis problem, we first derive an analytical expression of the convergence rate\nin over-the-air FD. Then, the closed-form optimal solutions of the WDs'\ntransmit power and the estimator for over-the-air aggregation are obtained\ngiven the receiver combining strategy. Accordingly, we put forth an efficient\napproach to find the optimal receiver beamforming vector via semidefinite\nrelaxation. We further prove that there is no optimality gap between the\noriginal and relaxed problem for the receiver beamforming design. Numerical\nresults will show that the proposed over-the-air FD approach achieves a\nsignificant reduction in communication overhead, with only a minor compromise\nin testing accuracy compared to conventional FL benchmarks."}
{"id": "2507.15291", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15291", "abs": "https://arxiv.org/abs/2507.15291", "authors": ["Osman Tokluoglu", "Enver Cavus", "Ebrahim Bedeer", "Halim Yanikomeroglu"], "title": "A Novel Domain-Aware CNN Architecture for Faster-than-Nyquist Signaling Detection", "comment": "6 pages, 8 figures", "summary": "This paper proposes a convolutional neural network (CNN)-based detector for\nfaster-than-Nyquist (FTN) signaling that employs structured fixed kernel layers\nwith domain-informed masking to mitigate intersymbol interference (ISI). Unlike\nstandard CNNs with sliding kernels, the proposed method utilizes fixed-position\nkernels to directly capture ISI effects at varying distances from the central\nsymbol. A hierarchical filter allocation strategy is also introduced, assigning\nmore filters to earlier layers for strong ISI patterns and fewer to later\nlayers for weaker ones. This design improves detection accuracy while reducing\nredundant operations. Simulation results show that the detector achieves\nnear-optimal bit error rate (BER) performance for $\\tau \\geq 0.7$, closely\nmatching the BCJR algorithm, and offers computational gains of up to $46\\%$ and\n$84\\%$ over M-BCJR for BPSK and QPSK, respectively. Comparative analysis with\nother methods further highlights the efficiency and effectiveness of the\nproposed approach. To the best of our knowledge, this is the first application\nof a fixed-kernel CNN architecture tailored for FTN detection in the\nliterature."}
{"id": "2507.15306", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15306", "abs": "https://arxiv.org/abs/2507.15306", "authors": ["Midhila Madhusoodanan", "Mahesh Raveendranatha Panicker", "Pisharody Harikrishnan Gopalakrishnan", "Abhilash Rakkunedeth Hareendranathan"], "title": "BEAM-Net: A Deep Learning Framework with Bone Enhancement Attention Mechanism for High Resolution High Frame Rate Ultrasound Beamforming", "comment": null, "summary": "Pocket-sized, low-cost point-of-care ultrasound (POCUS) devices are\nincreasingly used in musculoskeletal (MSK) applications for structural\nexamination of bone tissue. However, the image quality in MSK ultrasound is\noften limited by speckle noise, low resolution, poor contrast, and anisotropic\nreflections, making bone images difficult to interpret without additional\npost-processing. Typically, medical ultrasound systems use delay and sum\nbeamforming (DASB) for image reconstruction, which is not specifically\noptimized for bone structures. To address these limitations, we propose\nBEAM-Net, a novel end-to-end deep neural network (DNN) that performs\nhigh-frame-rate ultrasound beamforming with integrated bone enhancement, using\nsingle-plane-wave (SPW) radio frequency (RF) data as input. Our approach embeds\na Bone Probability Map (BPM), which acts as an attention mechanism to enforce\nhigher structural similarity around bony regions in the image. The proposed\napproach is the first of its kind to incorporate bone enhancement directly into\nultrasound beamforming using deep learning. BEAM-Net was trained and evaluated\non in-vivo MSK and synthetic RF ultrasound datasets. This paper introduces the\nEdge Preservation Index (EPI) as a new region-focused metric for evaluating\nstructural fidelity in bone-enhanced ultrasound images. The performance of\nBEAM-Net was compared with conventional DASB and existing deep learning\narchitectures using the EPI, Contrast Ratio (CR), Signal-to-Noise ratio (SNR),\nSpeckle Similarity Index (SSI), and Structural Similarity Index (SSIM).\nBEAM-Net showed substantial gains over SPW-DASB, achieving 51.4-51% higher CR\nand 94.2-73.3% higher SNR on in-vivo MSK and synthetic RF datasets. It\noutperformed multiple steered plane wave DASB (MPW-DASB), with 19.8-24.0%\nimprovements in CR and SNR on in-vivo MSK and 2.5-12.8% improvements on\nsynthetic data."}
{"id": "2507.15364", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15364", "abs": "https://arxiv.org/abs/2507.15364", "authors": ["Ruifeng Zheng", "Cong Chen", "Shuang Wang", "Yiming Liu", "Lin You", "Jindong Lu", "Ruizhe Zhu", "Guodao Zhang", "Kejie Huang"], "title": "EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network", "comment": null, "summary": "Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure\nonsets can significantly impact patients' quality of life and health. However,\nwearable seizure-predicting devices are still limited, partly due to the bulky\nsize of EEG-collecting devices. To relieve the problem, we proposed a novel\ntwo-stage channel-aware Set Transformer Network that could perform seizure\nprediction with fewer EEG channel sensors. We also tested a seizure-independent\ndivision method which could prevent the adjacency of training and test data.\nExperiments were performed on the CHB-MIT dataset which includes 22 patients\nwith 88 merged seizures. The mean sensitivity before channel selection was\n76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,\ndominant channels emerged in 20 out of 22 patients; the average number of\nchannels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%\nwith an FPR of 0.11/hour. Furthermore, experimental results on the\nseizure-independent division supported our assertion that a more rigorous\nseizure-independent division should be used for patients with abundant EEG\nrecordings."}
{"id": "2507.15373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15373", "abs": "https://arxiv.org/abs/2507.15373", "authors": ["Tiantian Xu", "Zhenyao He", "Jindan Xu", "Wei Xu", "Jianfeng Wang", "Derrick Wing Kwan Ng"], "title": "Robust ISAC Transceiver Beamforming Design under Low-Resolution AD/DA Converters", "comment": null, "summary": "In this letter, we investigate the robust beamforming design for an\nintegrated sensing and communication (ISAC) system featuring low-resolution\ndigital-to-analog converters (DACs) and analog-to-digital converters (ADCs).\nTaking into account quantization noise, we aim at maximizing the radar\nsignal-to-quantization-plus-noise ratio (SQNR) while guaranteeing the minimum\nrequired signal-to-quantization-plus-interference-plus-noise ratio (SQINR) for\ncommunication users. To address this nonconvex design problem, we first examine\na scenario involving a point target and uniform-resolution DACs, where the\nglobally optimal solution is obtained by applying the semidefinite relaxation\n(SDR) technique. For more general scenarios, including those with mixed-DACs\nand/or an extended target, we develop a low-complexity\nmajorization-minimization (MM)-based algorithm to tackle the problem\niteratively. Compared to the non-robust algorithm, the proposed algorithm\ndemonstrates improved detection performance under practical quantization.\nSimulation results confirm the robustness and efficacy of our proposed\nalgorithm in low-resolution quantization scenarios."}
{"id": "2507.15475", "categories": ["eess.SP", "math.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15475", "abs": "https://arxiv.org/abs/2507.15475", "authors": ["Karl-Ludwig Besser"], "title": "On the Distribution of a Two-Dimensional Random Walk with Restricted Angles", "comment": "12 pages, 13 figures", "summary": "In this paper, we derive the distribution of a two-dimensional (complex)\nrandom walk in which the angle of each step is restricted to a subset of the\ncircle. This setting appears in various domains, such as in over-the-air\ncomputation in signal processing. In particular, we derive the exact joint and\nmarginal distributions for two steps, numerical solutions for a general number\nof steps, and approximations for a large number of steps. Furthermore, we\nprovide an exact characterization of the support for an arbitrary number of\nsteps. The results in this work provide a reference for future work involving\nsuch problems."}
{"id": "2507.15515", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15515", "abs": "https://arxiv.org/abs/2507.15515", "authors": ["Xuhui Zhang", "Wenchao Liu", "Jinke Ren", "Chunjie Wang", "Huijun Xing", "Yanyan Shen", "Shuguang Cui"], "title": "Movable-Antenna Empowered AAV-Enabled Data Collection over Low-Altitude Wireless Networks", "comment": "This manuscript has been submitted to IEEE", "summary": "Movable-antennas (MAs) are revolutionizing spatial signal processing by\nproviding flexible beamforming in next-generation wireless systems. This paper\ninvestigates an MA-empowered autonomous aerial vehicle (AAV) system in\nlow-altitude wireless networks (LAWNs) for uplink data collection from ground\nusers. We aim to maximize the sum achievable rate by jointly optimizing the AAV\ntrajectory, receive beamforming, and MA positions. An efficient alternating\noptimization (AO) algorithm that incorporates successive convex approximation,\nweighted minimum mean square error, and particle swarm optimization is\ndeveloped. The analysis of the computational complexity and convergence\nfeatures is provided. Extensive simulations demonstrate superior performance in\nterms of the sum achievable rate and the service reliability comparing to\nseveral benchmark schemes. These results demonstrate the distinctive advantages\nof the proposed scheme: enhanced spectral efficiency via adaptive beam-user\nalignment and improved collection reliability through spatial interference\nmanagement, highlighting the implementation potential of the MA-empowered\nLAWNs."}
{"id": "2507.15555", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15555", "abs": "https://arxiv.org/abs/2507.15555", "authors": ["Nianzu Li", "Peiran Wu", "Lipeng Zhu", "Weidong Mei", "Boyu Ning", "Derrick Wing Kwan Ng"], "title": "Sum-Rate Maximization for Movable-Antenna Array Enhanced Downlink NOMA Systems", "comment": null, "summary": "Movable antenna (MA) systems have recently attracted significant attention in\nthe field of wireless communications owing to their exceptional capability to\nproactively reconfigure wireless channels via flexible antenna movements. In\nthis paper, we investigate the resource allocation design for an MA\narray-enhanced downlink non-orthogonal multiple access (NOMA) system, where a\nbase station deploys multiple MAs to serve multiple single-antenna users. Our\ngoal is to maximize the sum rate of all users by jointly optimizing the\ntransmit beamforming, positions of MAs, successive interference cancellation\n(SIC) decoding order, and users' corresponding decoding indicator matrix, while\nadhering to constraints on the maximum transmit power and finite MA moving\nregion. The formulated problem is inherently highly non-convex, rendering it\nchallenging to acquire a globally optimal solution. As a compromise, we propose\na low-complexity two-stage optimization algorithm to obtain an effective\nsuboptimal solution. Specifically, in stage one, the SIC decoding order is\nfirst determined by solving a channel gain maximization problem. Then, in stage\ntwo, with the given SIC decoding order, the beamforming vectors, MA positions,\nand users' decoding indicator matrix are iteratively optimized by capitalizing\non alternating optimization, successive convex approximation (SCA), and genetic\nalgorithm (GA). Simulation results unveil that the sum-rate performance of the\nproposed MA-enabled downlink NOMA system significantly outperforms that of\nconventional fixed-position antenna (FPA) systems. Moreover, the results also\nshow that the antenna position optimization in the proposed algorithm can\nfurther enhance the advantages of NOMA over space division multiple access\n(SDMA)."}
{"id": "2507.15621", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15621", "abs": "https://arxiv.org/abs/2507.15621", "authors": ["Imran Ali Khan", "Saif Khan Mohammed", "Ronny Hadani", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Zak-OTFS based Multiuser Uplink in Doubly-Spread Channels", "comment": null, "summary": "Wireless users with different characteristics will be expected to share\nspectrum in next generation communication networks. One of the great strengths\nof wireless networks based on Orthogonal Frequency Division Multiplexing (OFDM)\nis the ease with which different non-overlapping time-frequency (TF) resources\ncan be allocated to different users by simply shifting each user's signal in\ntime and frequency. However, a significant weaknesses of OFDM is the\ninflexibility of sub-carrier spacing. Since OFDM does not allow users to have\ndifferent sub-carrier spacing, a single user subject to inter-carrier\ninterference causes carrier spacing to increase for all users. Zak-OTFS is an\nalternative delay-Doppler (DD) domain modulation scheme, where, in contrast to\nOFDM, the Input-Output (I/O) relation is predictable. We match the strength of\nOFDM by designing a novel DD domain method of shaping the transmitted Zak-OTFS\npulse on the uplink that enables flexible non-overlapping TF resource\nallocation. The base station (BS) receives a superposition of uplink signals\nand applies individual matched filters to obtain the data specific to\nindividual users. We develop theoretical measures of interference between\nusers, and present numerical simulations for a vehicular channel model\nrepresentative of next generation propagation environments. We demonstrate\nsingle-user performance in a multiuser Zak-OTFS uplink system without needing\nto provision guard bands between TF resources allocated to different users.\nThese performance results demonstrate that the benefits of a predictable\nZak-OTFS waveform can be realized within an architecture for uplink\ncommunication."}
{"id": "2507.15800", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15800", "abs": "https://arxiv.org/abs/2507.15800", "authors": ["Yinchao Yang", "Jingxuan Zhou", "Zhaohui Yang", "Mohammad Shikh-Bahaei"], "title": "Fluid Antenna-enabled Near-Field Integrated Sensing, Computing and Semantic Communication for Emerging Applications", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking", "summary": "The integration of sensing and communication (ISAC) is a key enabler for\nnext-generation technologies. With high-frequency bands and large-scale antenna\narrays, the Rayleigh distance extends, necessitating near-field (NF) models\nwhere waves are spherical. Although NF-ISAC improves both sensing and\ncommunication, it also poses challenges such as high data volume and potential\nprivacy risks. To address these, we propose a novel framework: near-field\nintegrated sensing, computing, and semantic communication (NF-ISCSC), which\nleverages semantic communication to transmit contextual information only,\nthereby reducing data overhead and improving efficiency. However, semantic\ncommunication is sensitive to channel variations, requiring adaptive\nmechanisms. To this end, fluid antennas (FAs) are introduced to support the\nNF-ISCSC system, enabling dynamic adaptability to changing channels. The\nproposed FA-enabled NF-ISCSC framework considers multiple communication users\nand extended targets comprising several scatterers. A joint optimization\nproblem is formulated to maximize data rate while accounting for sensing\nquality, computational load, and power budget. Using an alternating\noptimization (AO) approach, the original problem is divided into three\nsub-problems: ISAC beamforming, FA positioning, and semantic extraction ratio.\nBeamforming is optimized using the successive convex approximation method. FA\npositioning is solved via a projected Broyden-Fletcher-Goldfarb-Shanno (BFGS)\nalgorithm, and the semantic extraction ratio is optimized using bisection\nsearch. Simulation results demonstrate that the proposed framework achieves\nhigher data rates and better privacy preservation."}
{"id": "2507.14237", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14237", "abs": "https://arxiv.org/abs/2507.14237", "authors": ["Louis Bahrman", "Mathieu Fontaine", "Gaël Richard"], "title": "U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model", "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "This paper explores the outcome of training state-ofthe-art dereverberation\nmodels with supervision settings ranging from weakly-supervised to fully\nunsupervised, relying solely on reverberant signals and an acoustic model for\ntraining. Most of the existing deep learning approaches typically require\npaired dry and reverberant data, which are difficult to obtain in practice. We\ndevelop instead a sequential learning strategy motivated by a bayesian\nformulation of the dereverberation problem, wherein acoustic parameters and dry\nsignals are estimated from reverberant inputs using deep neural networks,\nguided by a reverberation matching loss. Our most data-efficient variant\nrequires only 100 reverberation-parameter-labelled samples to outperform an\nunsupervised baseline, demonstrating the effectiveness and practicality of the\nproposed method in low-resource scenarios."}
{"id": "2507.14898", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14898", "abs": "https://arxiv.org/abs/2507.14898", "authors": ["Susmita Bhattacharjee", "Jagabandhu Mishra", "H. S. Shekhawat", "S. R. Mahadeva Prasanna"], "title": "Parameter-Efficient Fine-Tuning of Foundation Models for CLP Speech Classification", "comment": "6 pages, 5 figures, conference", "summary": "We propose the use of parameter-efficient fine-tuning (PEFT) of foundation\nmodels for cleft lip and palate (CLP) detection and severity classification. In\nCLP, nasalization increases with severity due to the abnormal passage between\nthe oral and nasal tracts; this causes oral stops to be replaced by glottal\nstops and alters formant trajectories and vowel space. Since foundation models\nare trained for grapheme prediction or long-term quantized representation\nprediction, they may better discriminate CLP severity when fine-tuned on\ndomain-specific data. We conduct experiments on two datasets: English (NMCPC)\nand Kannada (AIISH). We perform a comparative analysis using embeddings from\nself-supervised models Wav2Vec2 and WavLM, and the weakly supervised Whisper,\neach paired with SVM classifiers, and compare them with traditional handcrafted\nfeatures eGeMAPS and ComParE. Finally, we fine-tune the best-performing Whisper\nmodel using PEFT techniques: Low-Rank Adapter (LoRA) and Decomposed Low-Rank\nAdapter (DoRA). Our results demonstrate that the proposed approach achieves\nrelative improvements of 26.4% and 63.4% in macro-average F1 score over the\nbest foundation model and handcrafted feature baselines on the NMCPC dataset,\nand improvements of 6.1% and 52.9% on the AIISH dataset, respectively."}

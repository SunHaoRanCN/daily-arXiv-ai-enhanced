{"id": "2507.15970", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15970", "abs": "https://arxiv.org/abs/2507.15970", "authors": ["Tarikul Islam Tamiti", "Nursad Mamun", "Anomadarshi Barua"], "title": "Nonlinear Framework for Speech Bandwidth Extension", "comment": null, "summary": "Recovering high-frequency components lost to bandwidth constraints is crucial\nfor applications ranging from telecommunications to high-fidelity audio on\nlimited resources. We introduce NDSI-BWE, a new adversarial Band Width\nExtension (BWE) framework that leverage four new discriminators inspired by\nnonlinear dynamical system to capture diverse temporal behaviors: a\nMulti-Resolution Lyapunov Discriminator (MRLD) for determining sensitivity to\ninitial conditions by capturing deterministic chaos, a Multi-Scale Recurrence\nDiscriminator (MS-RD) for self-similar recurrence dynamics, a Multi-Scale\nDetrended Fractal Analysis Discriminator (MSDFA) for long range slow variant\nscale invariant relationship, a Multi-Resolution Poincar\\'e Plot Discriminator\n(MR-PPD) for capturing hidden latent space relationship, a Multi-Period\nDiscriminator (MPD) for cyclical patterns, a Multi-Resolution Amplitude\nDiscriminator (MRAD) and Multi-Resolution Phase Discriminator (MRPD) for\ncapturing intricate amplitude-phase transition statistics. By using depth-wise\nconvolution at the core of the convolutional block with in each discriminators,\nNDSI-BWE attains an eight-times parameter reduction. These seven discriminators\nguide a complex-valued ConformerNeXt based genetor with a dual stream\nLattice-Net based architecture for simultaneous refinement of magnitude and\nphase. The genertor leverage the transformer based conformer's global\ndependency modeling and ConvNeXt block's local temporal modeling capability.\nAcross six objective evaluation metrics and subjective based texts comprises of\nfive human judges, NDSI-BWE establishes a new SoTA in BWE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NDSI-BWE\uff0c\u4e00\u4e2a\u65b0\u7684\u5bf9\u6297\u6027\u5e26\u5bbd\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u4e03\u4e2a\u53d7\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u542f\u53d1\u7684\u5224\u522b\u5668\u6765\u6062\u590d\u4e22\u5931\u7684\u9ad8\u9891\u6210\u5206\uff0c\u5728\u5e26\u5bbd\u6269\u5c55\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5728\u7535\u4fe1\u548c\u9ad8\u4fdd\u771f\u97f3\u9891\u7b49\u53d7\u9650\u8d44\u6e90\u5e94\u7528\u4e2d\uff0c\u6062\u590d\u56e0\u5e26\u5bbd\u9650\u5236\u800c\u4e22\u5931\u7684\u9ad8\u9891\u6210\u5206\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u5e26\u5bbd\u6269\u5c55\u65b9\u6cd5\u5728\u6355\u83b7\u590d\u6742\u7684\u65f6\u95f4\u52a8\u6001\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faNDSI-BWE\u6846\u67b6\uff0c\u5305\u542b\u4e03\u4e2a\u57fa\u4e8e\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u5224\u522b\u5668\uff1a\u591a\u5206\u8fa8\u7387\u674e\u96c5\u666e\u8bfa\u592b\u5224\u522b\u5668(MRLD)\u3001\u591a\u5c3a\u5ea6\u9012\u5f52\u5224\u522b\u5668(MS-RD)\u3001\u591a\u5c3a\u5ea6\u53bb\u8d8b\u52bf\u5206\u5f62\u5206\u6790\u5224\u522b\u5668(MSDFA)\u3001\u591a\u5206\u8fa8\u7387\u5e9e\u52a0\u83b1\u56fe\u5224\u522b\u5668(MR-PPD)\u3001\u591a\u5468\u671f\u5224\u522b\u5668(MPD)\u3001\u591a\u5206\u8fa8\u7387\u5e45\u5ea6\u5224\u522b\u5668(MRAD)\u548c\u591a\u5206\u8fa8\u7387\u76f8\u4f4d\u5224\u522b\u5668(MRPD)\u3002\u751f\u6210\u5668\u91c7\u7528\u57fa\u4e8eConformerNeXt\u7684\u590d\u503c\u67b6\u6784\u548c\u53cc\u6d41Lattice-Net\u8bbe\u8ba1\uff0c\u540c\u65f6\u7ec6\u5316\u5e45\u5ea6\u548c\u76f8\u4f4d\u3002\u901a\u8fc7\u6df1\u5ea6\u5377\u79ef\u5b9e\u73b0\u516b\u500d\u53c2\u6570\u51cf\u5c11\u3002", "result": "\u5728\u516d\u4e2a\u5ba2\u89c2\u8bc4\u4f30\u6307\u6807\u548c\u4e94\u540d\u4eba\u7c7b\u8bc4\u5224\u5458\u7684\u4e3b\u89c2\u6d4b\u8bd5\u4e2d\uff0cNDSI-BWE\u5728\u5e26\u5bbd\u6269\u5c55\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u6807\u51c6\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "NDSI-BWE\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u53d7\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u542f\u53d1\u7684\u5224\u522b\u5668\u548c\u5148\u8fdb\u7684\u751f\u6210\u5668\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5e26\u5bbd\u6269\u5c55\u4e2d\u7684\u9ad8\u9891\u6062\u590d\u95ee\u9898\uff0c\u4e3a\u8be5\u9886\u57df\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u4e3a\u7535\u4fe1\u548c\u97f3\u9891\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15991", "categories": ["cs.SD", "cs.DB", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15991", "abs": "https://arxiv.org/abs/2507.15991", "authors": ["David Fiala", "Laurent Pugin", "Marnix van Berchum", "Martha Thomae", "K\u00e9vin Roger"], "title": "A new XML conversion process for mensural music encoding : CMME\\_to\\_MEI (via Verovio)", "comment": null, "summary": "The Ricercar Lab - the musicological research team at the Center for advanced\nStudies in the Renaissance at the University of Tours - has decided to make\navailable in open access, thanks to the support of the French digital\ninfrastructure Biblissima, a large corpus of about 3500 XML files of 15th-c.\nmusic. This corpus was produced by the German musicologist Clemens Goldberg who\nencoded since 2010 onwards the musical content of 34 major 15th-c. music\nmanuscripts and other complementary files, in order to offer on his\nfoundation's website PDF files of complete collections of works by Du Fay,\nBinchois, Okeghem, Busnoys and most of their major contemporaries, focusing on\ntheir secular output. This corpus was encoded in an XML format named CMME\n(Computerized Mensural Music Editing), specifically conceived for mensural\nmusic by Theodor Dumitrescu in the 2000s, together with editorial and\npublication tools which have not been updated since then. This article focuses\non the development of a set of conversion tools for these CMME files to meet\nmore up-to-date standards of music encoding, namely MEI. A workshop was\norganised in September 2024 at the Campus Condorcet in Paris, gathering experts\nwith a wide range of knowledge on mensural music notation, XML formats and\nprogramming. A converter was developped directly in the open-source rendering\nlibrary Verovio, allowing the conversion from CMME to MEI mensural. A\nconversion to MEI CMN was implemented afterwards, enabling to load these files\nin common engraving softwares such as MuseScore with minimal loss of\ninformation. With the availability of a direct import of CMME-XML into Verovio,\nthe corpus of existing CMME files gets a new life. Furthermore, since the\nstand-alone CMME editor still works fine and no alternative is available yet\nfor native MEI, the converter offers a new pipeline for encoding and editing\nmensural music.", "AI": {"tldr": "\u6cd5\u56fd\u56fe\u5c14\u5927\u5b66\u97f3\u4e50\u5b66\u7814\u7a76\u56e2\u961f\u5f00\u653e\u4e86\u4e00\u4e2a\u5305\u542b\u7ea63500\u4e2aXML\u6587\u4ef6\u768415\u4e16\u7eaa\u97f3\u4e50\u8bed\u6599\u5e93\uff0c\u5e76\u5f00\u53d1\u4e86\u5c06CMME\u683c\u5f0f\u8f6c\u6362\u4e3aMEI\u683c\u5f0f\u7684\u5de5\u5177\uff0c\u4e3a\u53e4\u4ee3\u8bb0\u8c31\u6cd5\u97f3\u4e50\u7684\u6570\u5b57\u5316\u7f16\u7801\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684CMME\uff08\u8ba1\u7b97\u673a\u5316\u5b9a\u91cf\u97f3\u4e50\u7f16\u8f91\uff09\u683c\u5f0f\u867d\u7136\u4e13\u95e8\u4e3a\u5b9a\u91cf\u97f3\u4e50\u8bbe\u8ba1\uff0c\u4f46\u5176\u7f16\u8f91\u548c\u53d1\u5e03\u5de5\u5177\u81ea2000\u5e74\u4ee3\u4ee5\u6765\u672a\u66fe\u66f4\u65b0\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u97f3\u4e50\u7f16\u7801\u6807\u51c6\u7684\u8981\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u8f6c\u6362\u5de5\u5177\u4ee5\u9002\u5e94\u66f4\u5148\u8fdb\u7684MEI\uff08\u97f3\u4e50\u7f16\u7801\u5021\u8bae\uff09\u6807\u51c6\u3002", "method": "\u57282024\u5e749\u6708\u5df4\u9ece\u5eb7\u591a\u585e\u6821\u533a\u4e3e\u529e\u7684\u7814\u8ba8\u4f1a\u4e0a\uff0c\u6c47\u96c6\u4e86\u5b9a\u91cf\u97f3\u4e50\u8bb0\u8c31\u6cd5\u3001XML\u683c\u5f0f\u548c\u7f16\u7a0b\u65b9\u9762\u7684\u4e13\u5bb6\uff0c\u76f4\u63a5\u5728\u5f00\u6e90\u6e32\u67d3\u5e93Verovio\u4e2d\u5f00\u53d1\u4e86\u8f6c\u6362\u5668\uff0c\u5b9e\u73b0\u4eceCMME\u5230MEI\u5b9a\u91cf\u683c\u5f0f\u7684\u8f6c\u6362\uff0c\u968f\u540e\u5b9e\u73b0\u4e86\u5230MEI CMN\u683c\u5f0f\u7684\u8f6c\u6362\uff0c\u4f7f\u8fd9\u4e9b\u6587\u4ef6\u80fd\u591f\u5728MuseScore\u7b49\u5e38\u89c1\u7f16\u8c31\u8f6f\u4ef6\u4e2d\u52a0\u8f7d\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86CMME\u5230MEI\u7684\u8f6c\u6362\u5de5\u5177\uff0c\u4f7f\u73b0\u6709\u7684CMME\u6587\u4ef6\u8bed\u6599\u5e93\u83b7\u5f97\u4e86\u65b0\u751f\u547d\u529b\uff0c\u5e76\u4e14\u7531\u4e8e\u72ec\u7acb\u7684CMME\u7f16\u8f91\u5668\u4ecd\u80fd\u6b63\u5e38\u5de5\u4f5c\u4e14\u5c1a\u65e0MEI\u7684\u672c\u5730\u66ff\u4ee3\u65b9\u6848\uff0c\u8be5\u8f6c\u6362\u5668\u4e3a\u7f16\u7801\u548c\u7f16\u8f91\u5b9a\u91cf\u97f3\u4e50\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1CMME\u5230MEI\u7684\u8f6c\u6362\u5de5\u5177\uff0c\u4e0d\u4ec5\u4f7f15\u4e16\u7eaa\u97f3\u4e50\u7684\u5927\u578b\u6570\u5b57\u8bed\u6599\u5e93\u80fd\u591f\u9002\u5e94\u73b0\u4ee3\u7f16\u7801\u6807\u51c6\uff0c\u8fd8\u4e3a\u5b9a\u91cf\u97f3\u4e50\u7684\u6570\u5b57\u5316\u7814\u7a76\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u97f3\u4e50\u5b66\u6570\u5b57\u4eba\u6587\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.16136", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16136", "abs": "https://arxiv.org/abs/2507.16136", "authors": ["Eduardo Pacheco", "Atila Orhon", "Berkin Durmus", "Blaise Munyampirwa", "Andrey Leonov"], "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization", "comment": null, "summary": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SDBench\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u8bf4\u8bdd\u4eba\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u96c6\u6210\u4e8613\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86SpeakerKit\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u6bd4Pyannote v3\u5feb9.6\u500d\u7684\u63a8\u7406\u901f\u5ea6\u4e14\u4fdd\u6301\u76f8\u5f53\u7684\u9519\u8bef\u7387", "motivation": "\u73b0\u6709\u7684\u8bf4\u8bdd\u4eba\u5206\u5272\u7cfb\u7edf\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff0c\u4e14\u7cfb\u7edf\u95f4\u6bd4\u8f83\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u9700\u8981\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u6765\u5b9e\u73b0\u53ef\u91cd\u73b0\u7684\u8bc4\u4f30\u548c\u516c\u5e73\u6bd4\u8f83", "method": "\u6784\u5efa\u4e86SDBench\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u96c6\u621013\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e00\u81f4\u6027\u548c\u7ec6\u7c92\u5ea6\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177\uff1b\u57fa\u4e8ePyannote v3\u5f00\u53d1\u4e86\u4e13\u6ce8\u4e8e\u63a8\u7406\u6548\u7387\u7684SpeakerKit\u7cfb\u7edf\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u4f18\u5316", "result": "SpeakerKit\u7cfb\u7edf\u6bd4Pyannote v3\u5feb9.6\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u9519\u8bef\u7387\uff1b\u5bf96\u4e2a\u5148\u8fdb\u7cfb\u7edf\uff08\u5305\u62ecDeepgram\u3001AWS Transcribe\u3001Pyannote AI API\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e4b\u95f4\u7684\u91cd\u8981\u6743\u8861\u5173\u7cfb", "conclusion": "SDBench\u4e3a\u8bf4\u8bdd\u4eba\u5206\u5272\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u652f\u6301\u53ef\u91cd\u73b0\u8bc4\u4f30\u548c\u65b0\u7cfb\u7edf\u7684\u4fbf\u6377\u96c6\u6210\uff1b\u901a\u8fc7\u8be5\u5e73\u53f0\u5f00\u53d1\u7684SpeakerKit\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u5728\u7cfb\u7edf\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2507.16190", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16190", "abs": "https://arxiv.org/abs/2507.16190", "authors": ["Haoyin Yan", "Jie Zhang", "Chengqian Jiang", "Shuang Zhang"], "title": "LABNet: A Lightweight Attentive Beamforming Network for Ad-hoc Multichannel Microphone Invariant Real-Time Speech Enhancement", "comment": null, "summary": "Multichannel speech enhancement (SE) aims to restore clean speech from noisy\nmeasurements by leveraging spatiotemporal signal features. In ad-hoc array\nconditions, microphone invariance (MI) requires systems to handle different\nmicrophone numbers and array geometries. From a practical perspective,\nmultichannel recordings inevitably increase the computational burden for\nedge-device applications, highlighting the necessity of lightweight and\nefficient deployments. In this work, we propose a lightweight attentive\nbeamforming network (LABNet) to integrate MI in a low-complexity real-time SE\nsystem. We design a three-stage framework for efficient intra-channel modeling\nand inter-channel interaction. A cross-channel attention module is developed to\naggregate features from each channel selectively. Experimental results\ndemonstrate our LABNet achieves impressive performance with ultra-light\nresource overhead while maintaining the MI, indicating great potential for\nad-hoc array processing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u6ce2\u675f\u6210\u5f62\u7f51\u7edc(LABNet)\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u591a\u901a\u9053\u8bed\u97f3\u589e\u5f3a\uff0c\u540c\u65f6\u4fdd\u6301\u9ea6\u514b\u98ce\u4e0d\u53d8\u6027\uff0c\u5177\u6709\u8d85\u4f4e\u8d44\u6e90\u5f00\u9500\u548c\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5728ad-hoc\u9635\u5217\u6761\u4ef6\u4e0b\uff0c\u591a\u901a\u9053\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\u9700\u8981\u5904\u7406\u4e0d\u540c\u6570\u91cf\u7684\u9ea6\u514b\u98ce\u548c\u9635\u5217\u51e0\u4f55\u7ed3\u6784(\u9ea6\u514b\u98ce\u4e0d\u53d8\u6027)\uff0c\u540c\u65f6\u591a\u901a\u9053\u5f55\u97f3\u4f1a\u589e\u52a0\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u56e0\u6b64\u9700\u8981\u8f7b\u91cf\u7ea7\u548c\u9ad8\u6548\u7684\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\u7684\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u6ce2\u675f\u6210\u5f62\u7f51\u7edc(LABNet)\uff0c\u5305\u62ec\u9ad8\u6548\u7684\u901a\u9053\u5185\u5efa\u6a21\u548c\u901a\u9053\u95f4\u4ea4\u4e92\uff0c\u5e76\u5f00\u53d1\u4e86\u8de8\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u6765\u9009\u62e9\u6027\u5730\u805a\u5408\u6765\u81ea\u5404\u901a\u9053\u7684\u7279\u5f81\u3002", "result": "LABNet\u5728\u4fdd\u6301\u9ea6\u514b\u98ce\u4e0d\u53d8\u6027\u7684\u540c\u65f6\uff0c\u4ee5\u8d85\u8f7b\u91cf\u7684\u8d44\u6e90\u5f00\u9500\u5b9e\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "LABNet\u5728ad-hoc\u9635\u5217\u5904\u7406\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u8bed\u97f3\u589e\u5f3a\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u4e0d\u540c\u9ea6\u514b\u98ce\u914d\u7f6e\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2507.15914", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15914", "abs": "https://arxiv.org/abs/2507.15914", "authors": ["Hanwen Liu", "Yifeng Gong", "Zuwei Yan", "Zeheng Zhuang", "Jiaxuan Lu"], "title": "MSGM: A Multi-Scale Spatiotemporal Graph Mamba for EEG Emotion Recognition", "comment": null, "summary": "EEG-based emotion recognition struggles with capturing multi-scale\nspatiotemporal dynamics and ensuring computational efficiency for real-time\napplications. Existing methods often oversimplify temporal granularity and\nspatial hierarchies, limiting accuracy. To overcome these challenges, we\npropose the Multi-Scale Spatiotemporal Graph Mamba (MSGM), a novel framework\nintegrating multi-window temporal segmentation, bimodal spatial graph modeling,\nand efficient fusion via the Mamba architecture. By segmenting EEG signals\nacross diverse temporal scales and constructing global-local graphs with\nneuroanatomical priors, MSGM effectively captures fine-grained emotional\nfluctuations and hierarchical brain connectivity. A multi-depth Graph\nConvolutional Network (GCN) and token embedding fusion module, paired with\nMamba's state-space modeling, enable dynamic spatiotemporal interaction at\nlinear complexity. Notably, with just one MSST-Mamba layer, MSGM surpasses\nleading methods in the field on the SEED, THU-EP, and FACED datasets,\noutperforming baselines in subject-independent emotion classification while\nachieving robust accuracy and millisecond-level inference on the NVIDIA Jetson\nXavier NX.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u65f6\u7a7a\u56feMamba(MSGM)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7a97\u53e3\u65f6\u95f4\u5206\u5272\u3001\u53cc\u6a21\u6001\u7a7a\u95f4\u56fe\u5efa\u6a21\u548cMamba\u67b6\u6784\u7684\u9ad8\u6548\u878d\u5408\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u79c0\u7684EEG\u60c5\u611f\u8bc6\u522b\u6027\u80fd\uff0c\u4ec5\u7528\u4e00\u4e2aMSST-Mamba\u5c42\u5c31\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eEEG\u7684\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5728\u6355\u83b7\u591a\u5c3a\u5ea6\u65f6\u7a7a\u52a8\u6001\u548c\u786e\u4fdd\u5b9e\u65f6\u5e94\u7528\u7684\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5f80\u5f80\u8fc7\u5ea6\u7b80\u5316\u65f6\u95f4\u7c92\u5ea6\u548c\u7a7a\u95f4\u5c42\u6b21\u7ed3\u6784\uff0c\u4ece\u800c\u9650\u5236\u4e86\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u65f6\u7a7a\u56feMamba(MSGM)\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u591a\u7a97\u53e3\u65f6\u95f4\u5206\u5272\u3001\u57fa\u4e8e\u795e\u7ecf\u89e3\u5256\u5b66\u5148\u9a8c\u7684\u53cc\u6a21\u6001\u7a7a\u95f4\u56fe\u5efa\u6a21(\u5168\u5c40-\u5c40\u90e8\u56fe)\u3001\u591a\u6df1\u5ea6\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u548ctoken\u5d4c\u5165\u878d\u5408\u6a21\u5757\uff0c\u4ee5\u53caMamba\u7684\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u6765\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u52a8\u6001\u65f6\u7a7a\u4ea4\u4e92\u3002", "result": "\u5728SEED\u3001THU-EP\u548cFACED\u6570\u636e\u96c6\u4e0a\uff0cMSGM\u4ec5\u4f7f\u7528\u4e00\u4e2aMSST-Mamba\u5c42\u5c31\u8d85\u8d8a\u4e86\u9886\u57df\u5185\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u5728\u4e3b\u4f53\u72ec\u7acb\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5728NVIDIA Jetson Xavier NX\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u51c6\u786e\u6027\u548c\u6beb\u79d2\u7ea7\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "MSGM\u901a\u8fc7\u6709\u6548\u6355\u83b7\u7ec6\u7c92\u5ea6\u60c5\u611f\u6ce2\u52a8\u548c\u5c42\u6b21\u5316\u5927\u8111\u8fde\u63a5\u6027\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86EEG\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16104", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16104", "abs": "https://arxiv.org/abs/2507.16104", "authors": ["Gene-Ping Yang", "Sebastian Braun"], "title": "Distributed Asynchronous Device Speech Enhancement via Windowed Cross-Attention", "comment": null, "summary": "The increasing number of microphone-equipped personal devices offers great\nflexibility and potential using them as ad-hoc microphone arrays in dynamic\nmeeting environments. However, most existing approaches are designed for\ntime-synchronized microphone setups, a condition that may not hold in\nreal-world meeting scenarios, where time latency and clock drift vary across\ndevices. Under such conditions, we found transform-average-concatenate (TAC), a\npopular module for neural multi-microphone processing, insufficient in handling\ntime-asynchronous microphones. In response, we propose a windowed\ncross-attention module capable of dynamically aligning features between all\nmicrophones. This module is invariant to both the permutation and the number of\nmicrophones and can be easily integrated into existing models. Furthermore, we\npropose an optimal training target for multi-talker environments. We evaluated\nour approach in a multi-microphone noisy reverberant setup with unknown time\nlatency and clock drift of each microphone. Experimental results show that our\nmethod outperforms TAC on both iFaSNet and CRUSE models, offering faster\nconvergence and improved learning, demonstrating the efficacy of the windowed\ncross-attention module for asynchronous microphone setups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a97\u53e3\u5316\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u4e0d\u540c\u6b65\u7684\u591a\u9ea6\u514b\u98ce\u97f3\u9891\u5904\u7406\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709TAC\u6a21\u5757\u5728\u771f\u5b9e\u4f1a\u8bae\u573a\u666f\u4e2d\u56e0\u8bbe\u5907\u95f4\u65f6\u5ef6\u548c\u65f6\u949f\u6f02\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u591a\u9ea6\u514b\u98ce\u5904\u7406\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u65f6\u95f4\u540c\u6b65\u7684\u9ea6\u514b\u98ce\u8bbe\u7f6e\u8bbe\u8ba1\uff0c\u4f46\u5728\u771f\u5b9e\u4f1a\u8bae\u573a\u666f\u4e2d\uff0c\u4e0d\u540c\u8bbe\u5907\u95f4\u5b58\u5728\u65f6\u5ef6\u548c\u65f6\u949f\u6f02\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u6d41\u884c\u7684TAC\u6a21\u5757\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65f6\u95f4\u5f02\u6b65\u7684\u9ea6\u514b\u98ce\u9635\u5217", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a97\u53e3\u5316\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u80fd\u591f\u52a8\u6001\u5bf9\u9f50\u6240\u6709\u9ea6\u514b\u98ce\u95f4\u7684\u7279\u5f81\u3002\u8be5\u6a21\u5757\u5bf9\u9ea6\u514b\u98ce\u7684\u6392\u5217\u548c\u6570\u91cf\u90fd\u5177\u6709\u4e0d\u53d8\u6027\uff0c\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u3002\u540c\u65f6\u4e3a\u591a\u8bf4\u8bdd\u4eba\u73af\u5883\u63d0\u51fa\u4e86\u6700\u4f18\u8bad\u7ec3\u76ee\u6807", "result": "\u5728\u5177\u6709\u672a\u77e5\u65f6\u5ef6\u548c\u65f6\u949f\u6f02\u79fb\u7684\u591a\u9ea6\u514b\u98ce\u566a\u58f0\u6df7\u54cd\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728iFaSNet\u548cCRUSE\u6a21\u578b\u4e0a\u90fd\u4f18\u4e8eTAC\u6a21\u5757\uff0c\u6536\u655b\u66f4\u5feb\uff0c\u5b66\u4e60\u6548\u679c\u66f4\u597d", "conclusion": "\u7a97\u53e3\u5316\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6b65\u9ea6\u514b\u98ce\u8bbe\u7f6e\u4e2d\u7684\u97f3\u9891\u5904\u7406\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u4f1a\u8bae\u73af\u5883\u4e2d\u7684\u591a\u8bbe\u5907\u97f3\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16220", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16220", "abs": "https://arxiv.org/abs/2507.16220", "authors": ["Xuechen Liu", "Wanying Ge", "Xin Wang", "Junichi Yamagishi"], "title": "LENS-DF: Deepfake Detection and Temporal Localization for Long-Form Noisy Speech", "comment": "Accepted by IEEE International Joint Conference on Biometrics (IJCB)\n  2025, Osaka, Japan", "summary": "This study introduces LENS-DF, a novel and comprehensive recipe for training\nand evaluating audio deepfake detection and temporal localization under\ncomplicated and realistic audio conditions. The generation part of the recipe\noutputs audios from the input dataset with several critical characteristics,\nsuch as longer duration, noisy conditions, and containing multiple speakers, in\na controllable fashion. The corresponding detection and localization protocol\nuses models. We conduct experiments based on self-supervised learning front-end\nand simple back-end. The results indicate that models trained using data\ngenerated with LENS-DF consistently outperform those trained via conventional\nrecipes, demonstrating the effectiveness and usefulness of LENS-DF for robust\naudio deepfake detection and localization. We also conduct ablation studies on\nthe variations introduced, investigating their impact on and relevance to\nrealistic challenges in the field.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faLENS-DF\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u65f6\u95f4\u5b9a\u4f4d\u7684\u7efc\u5408\u65b9\u6848\uff0c\u80fd\u591f\u5728\u590d\u6742\u73b0\u5b9e\u97f3\u9891\u6761\u4ef6\u4e0b\u751f\u6210\u5177\u6709\u66f4\u957f\u65f6\u957f\u3001\u566a\u58f0\u73af\u5883\u548c\u591a\u8bf4\u8bdd\u4eba\u7279\u5f81\u7684\u6570\u636e\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6848\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u9762\u5bf9\u590d\u6742\u73b0\u5b9e\u97f3\u9891\u6761\u4ef6\uff08\u5982\u957f\u65f6\u957f\u3001\u566a\u58f0\u73af\u5883\u3001\u591a\u8bf4\u8bdd\u4eba\u7b49\uff09\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u52a0\u5168\u9762\u548c\u73b0\u5b9e\u7684\u8bad\u7ec3\u8bc4\u4f30\u65b9\u6848\u6765\u63d0\u5347\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faLENS-DF\u65b9\u6848\uff0c\u5305\u542b\u751f\u6210\u90e8\u5206\u548c\u68c0\u6d4b\u5b9a\u4f4d\u534f\u8bae\u4e24\u4e2a\u7ec4\u4ef6\u3002\u751f\u6210\u90e8\u5206\u80fd\u591f\u53ef\u63a7\u5730\u4ece\u8f93\u5165\u6570\u636e\u96c6\u4ea7\u751f\u5177\u6709\u5173\u952e\u7279\u5f81\uff08\u66f4\u957f\u65f6\u957f\u3001\u566a\u58f0\u6761\u4ef6\u3001\u591a\u8bf4\u8bdd\u4eba\uff09\u7684\u97f3\u9891\uff1b\u68c0\u6d4b\u5b9a\u4f4d\u534f\u8bae\u4f7f\u7528\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u524d\u7aef\u548c\u7b80\u5355\u540e\u7aef\u7684\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528LENS-DF\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u6027\u80fdconsistently\u4f18\u4e8e\u4f7f\u7528\u4f20\u7edf\u65b9\u6848\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5404\u79cd\u53d8\u5316\u56e0\u7d20\u5bf9\u73b0\u5b9e\u6311\u6218\u7684\u5f71\u54cd\u548c\u76f8\u5173\u6027\u3002", "conclusion": "LENS-DF\u662f\u4e00\u4e2a\u6709\u6548\u4e14\u5b9e\u7528\u7684\u7efc\u5408\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u65f6\u95f4\u5b9a\u4f4d\u5728\u590d\u6742\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2507.15969", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15969", "abs": "https://arxiv.org/abs/2507.15969", "authors": ["Shu Sun", "Yulu Guo", "Meixia Tao", "Wei Feng", "Jun Chen", "Ruifeng Gao", "Ye Li", "Jue Wang", "Theodore S. Rappaport"], "title": "Modeling and Analysis of Land-to-Ship Maritime Wireless Channels at 5.8 GHz", "comment": null, "summary": "Maritime channel modeling is crucial for designing robust communication\nsystems in marine environments, where factors like waves and wind impact signal\npropagation. This article investigates land-to-ship maritime wireless channel\ncharacteristics at 5.8 GHz based upon an extensive measurement campaign, with\nconcurrent hydrological and meteorological information collection. First, a\nnovel large-scale path loss model with physical foundation and high accuracy is\nproposed for dynamic marine environments. Then, we introduce the concept of\nsea-wave-induced fixed-point (SWIFT) fading, a peculiar phenomenon in maritime\nscenarios that captures the impact of sea surface fluctuations on received\npower. An enhanced two-ray model incorporating vessel rotational motion is\npropounded to simulate the SWIFT fading, showing good alignment with measured\ndata, particularly for modest antenna movements. Next, the small-scale fading\nis studied by leveraging a variety of models including the two-wave with\ndiffuse power (TWDP) and asymmetric Laplace distributions, with the latter\nperforming well in most cases, while TWDP better captures bimodal fading in\nrough seas. Furthermore, maritime channel sparsity is examined via the Gini\nindex and Rician $K$ factor, and temporal dispersion is characterized. The\nresulting channel models and parameter characteristics offer valuable insights\nfor maritime wireless system design and deployment.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u5927\u89c4\u6a21\u6d4b\u91cf\u6d3b\u52a8\u7814\u7a76\u4e865.8 GHz\u9891\u6bb5\u9646\u5730\u5230\u8239\u8236\u7684\u6d77\u4e0a\u65e0\u7ebf\u4fe1\u9053\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5927\u5c3a\u5ea6\u8def\u5f84\u635f\u8017\u6a21\u578b\u548c\u6d77\u6d6a\u8bf1\u5bfc\u56fa\u5b9a\u70b9\u8870\u843d\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6a21\u578b\u5206\u6790\u4e86\u5c0f\u5c3a\u5ea6\u8870\u843d\u7279\u6027\uff0c\u4e3a\u6d77\u4e0a\u65e0\u7ebf\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "motivation": "\u6d77\u4e0a\u901a\u4fe1\u73af\u5883\u4e2d\u6ce2\u6d6a\u548c\u98ce\u7b49\u56e0\u7d20\u4f1a\u5f71\u54cd\u4fe1\u53f7\u4f20\u64ad\uff0c\u9700\u8981\u5efa\u7acb\u51c6\u786e\u7684\u6d77\u4e0a\u4fe1\u9053\u6a21\u578b\u6765\u8bbe\u8ba1\u7a33\u5065\u7684\u6d77\u4e0a\u901a\u4fe1\u7cfb\u7edf\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u63cf\u8ff0\u52a8\u6001\u6d77\u6d0b\u73af\u5883\u4e0b\u7684\u4fe1\u9053\u7279\u6027\uff0c\u7279\u522b\u662f\u6d77\u9762\u8d77\u4f0f\u5bf9\u63a5\u6536\u529f\u7387\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a215.8 GHz\u9891\u6bb5\u9646\u5730\u5230\u8239\u8236\u6d4b\u91cf\u6d3b\u52a8\uff0c\u540c\u65f6\u6536\u96c6\u6c34\u6587\u548c\u6c14\u8c61\u4fe1\u606f\uff1b\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u57fa\u7840\u7684\u5927\u5c3a\u5ea6\u8def\u5f84\u635f\u8017\u6a21\u578b\uff1b\u5f15\u5165\u6d77\u6d6a\u8bf1\u5bfc\u56fa\u5b9a\u70b9\uff08SWIFT\uff09\u8870\u843d\u6982\u5ff5\u5e76\u5efa\u7acb\u5305\u542b\u8239\u8236\u65cb\u8f6c\u8fd0\u52a8\u7684\u589e\u5f3a\u53cc\u5c04\u7ebf\u6a21\u578b\uff1b\u4f7f\u7528TWDP\u548c\u975e\u5bf9\u79f0\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7b49\u591a\u79cd\u6a21\u578b\u5206\u6790\u5c0f\u5c3a\u5ea6\u8870\u843d\uff1b\u901a\u8fc7\u57fa\u5c3c\u6307\u6570\u548c\u83b1\u65afK\u56e0\u5b50\u7814\u7a76\u4fe1\u9053\u7a00\u758f\u6027\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u9ad8\u7cbe\u5ea6\u7684\u52a8\u6001\u6d77\u6d0b\u73af\u5883\u5927\u5c3a\u5ea6\u8def\u5f84\u635f\u8017\u6a21\u578b\uff1bSWIFT\u8870\u843d\u6a21\u578b\u4e0e\u6d4b\u91cf\u6570\u636e\u9ad8\u5ea6\u543b\u5408\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5929\u7ebf\u8fd0\u52a8\u5e45\u5ea6\u8f83\u5c0f\u7684\u60c5\u51b5\uff1b\u975e\u5bf9\u79f0\u62c9\u666e\u62c9\u65af\u5206\u5e03\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u800cTWDP\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u4e86\u6076\u52a3\u6d77\u51b5\u4e0b\u7684\u53cc\u5cf0\u8870\u843d\uff1b\u83b7\u5f97\u4e86\u6d77\u4e0a\u4fe1\u9053\u7a00\u758f\u6027\u548c\u65f6\u95f4\u8272\u6563\u7279\u6027\u7684\u91cf\u5316\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u7684\u4fe1\u9053\u6a21\u578b\u548c\u53c2\u6570\u7279\u6027\u4e3a\u6d77\u4e0a\u65e0\u7ebf\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u6d77\u9762\u52a8\u6001\u5bf9\u4fe1\u53f7\u4f20\u64ad\u5f71\u54cd\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u6d77\u4e0a\u901a\u4fe1\u7cfb\u7edf\u4f18\u5316\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.16456", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.16456", "abs": "https://arxiv.org/abs/2507.16456", "authors": ["Sujith Pulikodan", "Sahapthan K", "Prasanta Kumar Ghosh", "Visruth Sanka", "Nihar Desai"], "title": "An approach to measuring the performance of Automatic Speech Recognition (ASR) models in the context of Large Language Model (LLM) powered applications", "comment": "Accepted at INTERSPEECH 2025", "summary": "Automatic Speech Recognition (ASR) plays a crucial role in human-machine\ninteraction and serves as an interface for a wide range of applications.\nTraditionally, ASR performance has been evaluated using Word Error Rate (WER),\na metric that quantifies the number of insertions, deletions, and substitutions\nin the generated transcriptions. However, with the increasing adoption of large\nand powerful Large Language Models (LLMs) as the core processing component in\nvarious applications, the significance of different types of ASR errors in\ndownstream tasks warrants further exploration. In this work, we analyze the\ncapabilities of LLMs to correct errors introduced by ASRs and propose a new\nmeasure to evaluate ASR performance for LLM-powered applications.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7ea0\u6b63\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u9519\u8bef\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ASR\u6027\u80fd\u8bc4\u4f30\u6307\u6807\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5728LLM\u9a71\u52a8\u5e94\u7528\u4e2d\u7684ASR\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684ASR\u6027\u80fd\u8bc4\u4f30\u4e3b\u8981\u4f7f\u7528\u8bcd\u9519\u8bef\u7387(WER)\uff0c\u4f46\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u4e0d\u540c\u7c7b\u578b\u7684ASR\u9519\u8bef\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u7a0b\u5ea6\u4e0d\u540c\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9002\u5408LLM\u9a71\u52a8\u5e94\u7528\u7684ASR\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7ea0\u6b63ASR\u9519\u8bef\u7684\u80fd\u529b\uff0c\u7814\u7a76\u4e0d\u540c\u7c7b\u578bASR\u9519\u8bef(\u63d2\u5165\u3001\u5220\u9664\u3001\u66ff\u6362)\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u65b0\u7684ASR\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ASR\u6027\u80fd\u8bc4\u4f30\u6307\u6807\uff0c\u8be5\u6307\u6807\u4e13\u95e8\u9488\u5bf9LLM\u9a71\u52a8\u7684\u5e94\u7528\u573a\u666f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620ASR\u9519\u8bef\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "conclusion": "\u5728LLM\u5e7f\u6cdb\u5e94\u7528\u7684\u80cc\u666f\u4e0b\uff0c\u4f20\u7edf\u7684WER\u6307\u6807\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30ASR\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5408LLM\u9a71\u52a8\u5e94\u7528\u7684ASR\u8bc4\u4f30\u65b9\u6cd5\u6765\u6307\u5bfc\u7cfb\u7edf\u4f18\u5316\u3002"}}
{"id": "2507.16235", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16235", "abs": "https://arxiv.org/abs/2507.16235", "authors": ["Kaspar Soltero", "Tadeu Siqueira", "Stefanie Gutschmidt"], "title": "Robust Bioacoustic Detection via Richly Labelled Synthetic Soundscape Augmentation", "comment": "12 pages, 4 figures", "summary": "Passive Acoustic Monitoring (PAM) analysis is often hindered by the intensive\nmanual effort needed to create labelled training data. This study introduces a\nsynthetic data framework to generate large volumes of richly labelled training\ndata from very limited source material, improving the robustness of bioacoustic\ndetection models. Our framework synthesises realistic soundscapes by combining\nclean background noise with isolated target vocalisations (little owl),\nautomatically generating dynamic labels like bounding boxes during synthesis. A\nmodel fine-tuned on this data generalised well to real-world soundscapes, with\nperformance remaining high even when the diversity of source vocalisations was\ndrastically reduced, indicating the model learned generalised features without\noverfitting. This demonstrates that synthetic data generation is a highly\neffective strategy for training robust bioacoustic detectors from small source\ndatasets. The approach significantly reduces manual labelling effort,\novercoming a key bottleneck in computational bioacoustics and enhancing\necological assessment capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6e05\u6d01\u80cc\u666f\u566a\u58f0\u4e0e\u76ee\u6807\u9e1f\u7c7b\u53eb\u58f0\u7ed3\u5408\uff0c\u81ea\u52a8\u751f\u6210\u5927\u91cf\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u58f0\u5b66\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u88ab\u52a8\u58f0\u5b66\u76d1\u6d4b\u4e2d\u624b\u5de5\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u88ab\u52a8\u58f0\u5b66\u76d1\u6d4b(PAM)\u5206\u6790\u53d7\u5230\u521b\u5efa\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\u7684\u963b\u788d\uff0c\u8fd9\u662f\u8ba1\u7b97\u751f\u7269\u58f0\u5b66\u9886\u57df\u7684\u4e00\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u9650\u5236\u4e86\u751f\u6001\u8bc4\u4f30\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u6846\u67b6\uff0c\u5c06\u6e05\u6d01\u7684\u80cc\u666f\u566a\u58f0\u4e0e\u5b64\u7acb\u7684\u76ee\u6807\u9e1f\u7c7b\u53eb\u58f0(\u5c0f\u9e2e)\u7ed3\u5408\uff0c\u5728\u5408\u6210\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u751f\u6210\u52a8\u6001\u6807\u7b7e\uff08\u5982\u8fb9\u754c\u6846\uff09\uff0c\u4ece\u975e\u5e38\u6709\u9650\u7684\u6e90\u6750\u6599\u751f\u6210\u5927\u91cf\u4e30\u5bcc\u6807\u6ce8\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u58f0\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5373\u4f7f\u6e90\u9e1f\u53eb\u58f0\u7684\u591a\u6837\u6027\u5927\u5e45\u51cf\u5c11\uff0c\u6027\u80fd\u4ecd\u7136\u4fdd\u6301\u8f83\u9ad8\u6c34\u5e73\uff0c\u8868\u660e\u6a21\u578b\u5b66\u4e60\u5230\u4e86\u6cdb\u5316\u7279\u5f81\u800c\u6ca1\u6709\u8fc7\u62df\u5408\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u4ece\u5c0f\u578b\u6e90\u6570\u636e\u96c6\u8bad\u7ec3\u9c81\u68d2\u751f\u7269\u58f0\u5b66\u68c0\u6d4b\u5668\u7684\u9ad8\u6548\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u514b\u670d\u4e86\u8ba1\u7b97\u751f\u7269\u58f0\u5b66\u7684\u5173\u952e\u74f6\u9888\uff0c\u589e\u5f3a\u4e86\u751f\u6001\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2507.16132", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16132", "abs": "https://arxiv.org/abs/2507.16132", "authors": ["Yue Xiu", "Wanting Lyu", "You Li", "Ran Yang", "Phee Lep Yeoh", "Wei Zhang", "Guangyi Liu", "Ning Wei"], "title": "Meta-Reinforcement Learning Optimization for Movable Antenna-aided Full-Duplex CF-DFRC Systems with Carrier Frequency Offset", "comment": null, "summary": "By enabling spectrum sharing between radar and communication operations, the\ncell-free dual-functional radar-communication (CF-DFRC) system is a promising\ncandidate to significantly improve spectrum efficiency in future\nsixth-generation (6G) wireless networks. However, in wideband scenarios,\nsynchronization errors caused by carrier frequency offset (CFO) can severely\nreduce both communication capacity and sensing accuracy. To address this\nchallenge, this paper integrates movable antennas (MAs) into the CF-DFRC\nframework, leveraging their spatial flexibility and adaptive beamforming to\ndynamically mitigate CFO-induced impairments. To fully exploit the advantages\nof MAs in wideband scenarios with CFO, we aim to maximize the worst-case\nsum-rate of communication and sensing by jointly optimizing MA positions,\n{beamforming}, and CFO parameters, subject to transmit power and MA positioning\nconstraints. Due to the non-convex nature of the problem, we propose a robust\nmeta reinforcement learning (MRL)-based two-stage alternating optimization\nstrategy. In the first stage, we employ manifold optimization (MO) with penalty\ndual decomposition (PDD) to solve the CFO-robust worst-case subproblem. In the\nsecond stage, we adopt to jointly optimize {the MA positions and beamforming\nvectors} in a data-driven manner {for dynamic wireless environments}.\nSimulation results show that the proposed MRL approach significantly\noutperforms conventional deep reinforcement learning (DRL) schemes in both\ncommunication and sensing performance under CFO impairments. Furthermore,\ncompared to fixed-position antennas (FPAs), the MA-aided CF-DFRC system\nexhibits", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\u7684\u65e0\u8702\u7a9d\u53cc\u529f\u80fd\u96f7\u8fbe\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5143\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\u6765\u7f13\u89e3\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u5bf9\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd", "motivation": "\u5728\u5bbd\u5e26\u573a\u666f\u4e0b\uff0c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u4f1a\u4e25\u91cd\u964d\u4f4e\u65e0\u8702\u7a9d\u53cc\u529f\u80fd\u96f7\u8fbe\u901a\u4fe1\u7cfb\u7edf\u7684\u901a\u4fe1\u5bb9\u91cf\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u9700\u8981\u5229\u7528\u53ef\u79fb\u52a8\u5929\u7ebf\u7684\u7a7a\u95f4\u7075\u6d3b\u6027\u6765\u52a8\u6001\u7f13\u89e3\u8fd9\u79cd\u5f71\u54cd", "method": "\u63d0\u51fa\u57fa\u4e8e\u5143\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u7528\u5e26\u60e9\u7f5a\u5bf9\u5076\u5206\u89e3\u7684\u6d41\u5f62\u4f18\u5316\u6c42\u89e3CFO\u9c81\u68d2\u6700\u574f\u60c5\u51b5\u5b50\u95ee\u9898\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u8054\u5408\u4f18\u5316\u53ef\u79fb\u52a8\u5929\u7ebf\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\u5411\u91cf", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728CFO\u5f71\u54cd\u4e0b\u7684\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u4e0e\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u76f8\u6bd4\uff0c\u53ef\u79fb\u52a8\u5929\u7ebf\u8f85\u52a9\u7684\u7cfb\u7edf\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5c06\u53ef\u79fb\u52a8\u5929\u7ebf\u96c6\u6210\u5230\u65e0\u8702\u7a9d\u53cc\u529f\u80fd\u96f7\u8fbe\u901a\u4fe1\u6846\u67b6\u4e2d\uff0c\u5e76\u91c7\u7528\u5143\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u7684\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7684\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd"}}
{"id": "2507.16343", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16343", "abs": "https://arxiv.org/abs/2507.16343", "authors": ["Pengfei Cai", "Yan Song", "Qing Gu", "Nan Jiang", "Haoyu Song", "Ian McLoughlin"], "title": "Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal Queries", "comment": "Accepted by MM 2025", "summary": "Most existing sound event detection~(SED) algorithms operate under a\nclosed-set assumption, restricting their detection capabilities to predefined\nclasses. While recent efforts have explored language-driven zero-shot SED by\nexploiting audio-language models, their performance is still far from\nsatisfactory due to the lack of fine-grained alignment and cross-modal feature\nfusion. In this work, we propose the Detect Any Sound Model (DASM), a\nquery-based framework for open-vocabulary SED guided by multi-modal queries.\nDASM formulates SED as a frame-level retrieval task, where audio features are\nmatched against query vectors derived from text or audio prompts. To support\nthis formulation, DASM introduces a dual-stream decoder that explicitly\ndecouples event recognition and temporal localization: a cross-modality event\ndecoder performs query-feature fusion and determines the presence of sound\nevents at the clip-level, while a context network models temporal dependencies\nfor frame-level localization. Additionally, an inference-time attention masking\nstrategy is proposed to leverage semantic relations between base and novel\nclasses, substantially enhancing generalization to novel classes. Experiments\non the AudioSet Strong dataset demonstrate that DASM effectively balances\nlocalization accuracy with generalization to novel classes, outperforming\nCLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in\nthe closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot\nevaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the\nsupervised CRNN baseline. The project page is available at\nhttps://cai525.github.io/Transformer4SED/demo_page/DASM/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DASM\uff08Detect Any Sound Model\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8e\u67e5\u8be2\u7684\u5f00\u653e\u8bcd\u6c47\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u67e5\u8be2\u5f15\u5bfc\uff0c\u5c06SED\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e27\u7ea7\u68c0\u7d22\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u53cc\u6d41\u89e3\u7801\u5668\u5206\u79bb\u4e8b\u4ef6\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u7b97\u6cd5\u5927\u591a\u57fa\u4e8e\u5c01\u95ed\u96c6\u5047\u8bbe\uff0c\u53ea\u80fd\u68c0\u6d4b\u9884\u5b9a\u4e49\u7c7b\u522b\u7684\u58f0\u97f3\u4e8b\u4ef6\u3002\u867d\u7136\u6700\u8fd1\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u8bed\u8a00\u9a71\u52a8\u7684\u96f6\u6837\u672cSED\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u6027\u80fd\u4ecd\u4e0d\u4ee4\u4eba\u6ee1\u610f\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u68c0\u6d4b\u4efb\u610f\u58f0\u97f3\u7c7b\u522b\u7684\u5f00\u653e\u8bcd\u6c47SED\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDASM\u6846\u67b6\uff0c\u5c06SED\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e27\u7ea7\u68c0\u7d22\u4efb\u52a1\uff0c\u901a\u8fc7\u6587\u672c\u6216\u97f3\u9891\u63d0\u793a\u751f\u6210\u67e5\u8be2\u5411\u91cf\u4e0e\u97f3\u9891\u7279\u5f81\u8fdb\u884c\u5339\u914d\u3002\u91c7\u7528\u53cc\u6d41\u89e3\u7801\u5668\u67b6\u6784\uff1a\u8de8\u6a21\u6001\u4e8b\u4ef6\u89e3\u7801\u5668\u8fdb\u884c\u67e5\u8be2-\u7279\u5f81\u878d\u5408\u5e76\u5728\u7247\u6bb5\u7ea7\u786e\u5b9a\u58f0\u97f3\u4e8b\u4ef6\u5b58\u5728\u6027\uff1b\u4e0a\u4e0b\u6587\u7f51\u7edc\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u5e27\u7ea7\u5b9a\u4f4d\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u63a8\u7406\u65f6\u6ce8\u610f\u529b\u63a9\u7801\u7b56\u7565\uff0c\u5229\u7528\u57fa\u7840\u7c7b\u548c\u65b0\u7c7b\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u589e\u5f3a\u5bf9\u65b0\u7c7b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728AudioSet Strong\u6570\u636e\u96c6\u4e0a\uff0cDASM\u5728\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u4e0b\u6bd4\u57fa\u4e8eCLAP\u7684\u65b9\u6cd5\u63d0\u53477.8 PSDS\uff0c\u5728\u5c01\u95ed\u96c6\u8bbe\u7f6e\u4e0b\u6bd4\u57fa\u7ebf\u63d0\u53476.9 PSDS\u3002\u5728DESED\u6570\u636e\u96c6\u7684\u8de8\u6570\u636e\u96c6\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\uff0cDASM\u83b7\u5f9742.2\u7684PSDS1\u5206\u6570\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u6709\u76d1\u7763\u7684CRNN\u57fa\u7ebf\u3002", "conclusion": "DASM\u901a\u8fc7\u67e5\u8be2\u9a71\u52a8\u7684\u6846\u67b6\u548c\u53cc\u6d41\u89e3\u7801\u5668\u67b6\u6784\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5bf9\u65b0\u7c7b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5f00\u653e\u8bcd\u6c47\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47SED\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16210", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16210", "abs": "https://arxiv.org/abs/2507.16210", "authors": ["Li-Hsiang Shen", "Yi-Hsuan Chiu"], "title": "Joint Active and Passive Beamforming for Energy-Efficient STARS with Quantization and Element Selection in ISAC Systems", "comment": null, "summary": "This paper investigates a simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STARS)-aided integrated sensing and\ncommunication (ISAC) systems in support of full-space energy-efficient data\ntransmissions and target sensing. We formulate an energy efficiency (EE)\nmaximization problem jointly optimizing dual-functional radar-communication\n(DFRC)-empowered base station (BS) ISAC beamforming and STARS configurations of\namplitudes, phase-shifts, quantization levels as well as element selection.\nFurthermore, relaxed/independent/coupled STARS are considered to examine\narchitectural flexibility. To tackle the non-convex and mixed-integer problem,\nwe propose a joint active-passive beamforming, quantization and element\nselection (AQUES) scheme based on alternating optimization: Lagrangian dual and\nDinkelbach's transformation deals with fractions, whereas successive convex\napproximation (SCA) convexifies the problem; Penalty dual decomposition (PDD)\nframework and penalty-based convex-concave programming (PCCP) procedure solves\namplitude and phase-shifts; Heuristic search decides the quantization level;\nInteger relaxation deals with the element selection. Simulation results\ndemonstrate that STARS-ISAC with the proposed AQUES scheme significantly\nenhances EE while meeting communication rates and sensing quality requirements.\nThe coupled STARS further highlights its superior EE performance over\nindependent and relaxed STARS thanks to its reduced hardware complexity.\nMoreover, AQUES outperforms existing configurations and benchmark methods in\nthe open literature across various network parameters and deployment scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u540c\u65f6\u53d1\u5c04\u53cd\u5c04\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(STARS)\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u901a\u4fe1(ISAC)\u7cfb\u7edf\uff0c\u63d0\u51faAQUES\u65b9\u6848\u4f18\u5316\u80fd\u6548\uff0c\u5b9e\u73b0\u5168\u7a7a\u95f4\u80fd\u6548\u6570\u636e\u4f20\u8f93\u548c\u76ee\u6807\u611f\u77e5", "motivation": "\u4f20\u7edfISAC\u7cfb\u7edf\u5728\u5168\u7a7a\u95f4\u8986\u76d6\u548c\u80fd\u6548\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5229\u7528STARS\u6280\u672f\u5b9e\u73b0\u540c\u65f6\u53d1\u5c04\u548c\u53cd\u5c04\u529f\u80fd\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\u548c\u611f\u77e5\u901a\u4fe1\u6027\u80fd", "method": "\u63d0\u51fa\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u7684\u8054\u5408\u4e3b\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\u3001\u91cf\u5316\u548c\u5143\u7d20\u9009\u62e9(AQUES)\u65b9\u6848\uff1a\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u548cDinkelbach\u53d8\u6362\u5904\u7406\u5206\u6570\u95ee\u9898\uff1b\u91c7\u7528\u8fde\u7eed\u51f8\u8fd1\u4f3c(SCA)\u8fdb\u884c\u51f8\u5316\uff1b\u901a\u8fc7\u60e9\u7f5a\u5bf9\u5076\u5206\u89e3(PDD)\u6846\u67b6\u548c\u60e9\u7f5a\u51f8\u51f9\u89c4\u5212(PCCP)\u6c42\u89e3\u5e45\u5ea6\u548c\u76f8\u79fb\uff1b\u542f\u53d1\u5f0f\u641c\u7d22\u786e\u5b9a\u91cf\u5316\u7ea7\u522b\uff1b\u6574\u6570\u677e\u5f1b\u5904\u7406\u5143\u7d20\u9009\u62e9", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660eSTARS-ISAC\u7cfb\u7edf\u914d\u5408AQUES\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\uff0c\u540c\u65f6\u6ee1\u8db3\u901a\u4fe1\u901f\u7387\u548c\u611f\u77e5\u8d28\u91cf\u8981\u6c42\u3002\u8026\u5408STARS\u7531\u4e8e\u786c\u4ef6\u590d\u6742\u5ea6\u964d\u4f4e\uff0c\u76f8\u6bd4\u72ec\u7acb\u548c\u677e\u5f1bSTARS\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u80fd\u6548\u6027\u80fd\u3002AQUES\u5728\u5404\u79cd\u7f51\u7edc\u53c2\u6570\u548c\u90e8\u7f72\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u914d\u7f6e\u548c\u57fa\u51c6\u65b9\u6cd5", "conclusion": "STARS\u6280\u672f\u7ed3\u5408\u4f18\u5316\u7684AQUES\u65b9\u6848\u80fd\u591f\u6709\u6548\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\uff0c\u5b9e\u73b0\u5168\u7a7a\u95f4\u8986\u76d6\u7684\u9ad8\u6548\u611f\u77e5\u901a\u4fe1\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u7684\u7eff\u8272\u901a\u4fe1\u548c\u667a\u80fd\u611f\u77e5\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16564", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16564", "abs": "https://arxiv.org/abs/2507.16564", "authors": ["Yuxuan He", "Xiaoran Yang", "Ningning Pan", "Gongping Huang"], "title": "TTMBA: Towards Text To Multiple Sources Binaural Audio Generation", "comment": "5 pages,3 figures,2 tables", "summary": "Most existing text-to-audio (TTA) generation methods produce mono outputs,\nneglecting essential spatial information for immersive auditory experiences. To\naddress this issue, we propose a cascaded method for text-to-multisource\nbinaural audio generation (TTMBA) with both temporal and spatial control.\nFirst, a pretrained large language model (LLM) segments the text into a\nstructured format with time and spatial details for each sound event. Next, a\npretrained mono audio generation network creates multiple mono audios with\nvarying durations for each event. These mono audios are transformed into\nbinaural audios using a binaural rendering neural network based on spatial data\nfrom the LLM. Finally, the binaural audios are arranged by their start times,\nresulting in multisource binaural audio. Experimental results demonstrate the\nsuperiority of the proposed method in terms of both audio generation quality\nand spatial perceptual accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea7\u8054\u65b9\u6cd5\u6765\u751f\u6210\u5177\u6709\u65f6\u95f4\u548c\u7a7a\u95f4\u63a7\u5236\u7684\u6587\u672c\u5230\u591a\u6e90\u53cc\u8033\u97f3\u9891(TTMBA)\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u53ea\u4ea7\u751f\u5355\u58f0\u9053\u8f93\u51fa\u800c\u5ffd\u89c6\u7a7a\u95f4\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u97f3\u9891(TTA)\u751f\u6210\u65b9\u6cd5\u5927\u591a\u4ea7\u751f\u5355\u58f0\u9053\u8f93\u51fa\uff0c\u5ffd\u7565\u4e86\u6c89\u6d78\u5f0f\u542c\u89c9\u4f53\u9a8c\u6240\u5fc5\u9700\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u7684\u53cc\u8033\u97f3\u9891\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u65b9\u6cd5\uff1a1)\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u5c06\u6587\u672c\u5206\u5272\u6210\u5305\u542b\u6bcf\u4e2a\u58f0\u97f3\u4e8b\u4ef6\u65f6\u95f4\u548c\u7a7a\u95f4\u7ec6\u8282\u7684\u7ed3\u6784\u5316\u683c\u5f0f\uff1b2)\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5355\u58f0\u9053\u97f3\u9891\u751f\u6210\u7f51\u7edc\u4e3a\u6bcf\u4e2a\u4e8b\u4ef6\u521b\u5efa\u4e0d\u540c\u6301\u7eed\u65f6\u95f4\u7684\u591a\u4e2a\u5355\u58f0\u9053\u97f3\u9891\uff1b3)\u57fa\u4e8eLLM\u7684\u7a7a\u95f4\u6570\u636e\uff0c\u4f7f\u7528\u53cc\u8033\u6e32\u67d3\u795e\u7ecf\u7f51\u7edc\u5c06\u5355\u58f0\u9053\u97f3\u9891\u8f6c\u6362\u4e3a\u53cc\u8033\u97f3\u9891\uff1b4)\u6309\u5f00\u59cb\u65f6\u95f4\u6392\u5217\u53cc\u8033\u97f3\u9891\uff0c\u751f\u6210\u591a\u6e90\u53cc\u8033\u97f3\u9891\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u97f3\u9891\u751f\u6210\u8d28\u91cf\u548c\u7a7a\u95f4\u611f\u77e5\u51c6\u786e\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ea7\u8054\u7684\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u6587\u672c\u5230\u591a\u6e90\u53cc\u8033\u97f3\u9891\u7684\u751f\u6210\uff0c\u540c\u65f6\u5177\u5907\u65f6\u95f4\u548c\u7a7a\u95f4\u63a7\u5236\u80fd\u529b\uff0c\u5728\u97f3\u9891\u8d28\u91cf\u548c\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.16211", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16211", "abs": "https://arxiv.org/abs/2507.16211", "authors": ["Li-Hsiang Shen"], "title": "Liquid Intelligent Metasurface for Fluid Antennas-Assisted Networks", "comment": null, "summary": "This paper proposes a novel liquid intelligent metasurface (LIM)-assisted\ndownlink multi-user multiple-input single-output (MISO) system, wherein both\nthe base station (BS) and the metasurface are respectively equipped with fluid\nantennas (FA) and liquid elements. Unlike conventional reconfigurable\nmetasurface-assisted systems with static geometries, the proposed architecture\nenables joint electromagnetic and spatial reconfigurability by allowing both\nthe FA-empowered BS (FAS) and LIM to dynamically adjust their small-scale\npositions in addition to beamforming and phase-shift controls. We formulate a\nsum-rate maximization problem that jointly optimizes the BS beamforming, LIM\nphase-shifts, and the positions of fluid antennas and liquid elements. The\nproblem is highly non-convex due to coupling between variables, fractional\nexpressions, unit-modulus constraints as well as spatial correlation functions.\nTo address these challenges, we adopt alternating optimization and introduce\nauxiliary variables and employ successive convex approximation (SCA) as well as\nthe penalty convex-concave procedure (PCCP) to solve the respective\nsubproblems. Simulation results have demonstrated that the proposed FAS-LIM\narchitecture significantly outperforms benchmark methods employing conventional\nfixed metasurface and fixed antenna arrays in terms of various parameter\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6db2\u4f53\u667a\u80fd\u8d85\u8868\u9762\u8f85\u52a9\u4e0b\u884c\u591a\u7528\u6237MISO\u7cfb\u7edf\uff0c\u901a\u8fc7\u6d41\u4f53\u5929\u7ebf\u548c\u6db2\u4f53\u5143\u4ef6\u5b9e\u73b0\u7535\u78c1\u548c\u7a7a\u95f4\u7684\u8054\u5408\u91cd\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u53ef\u91cd\u6784\u8d85\u8868\u9762\u8f85\u52a9\u7cfb\u7edf\u91c7\u7528\u9759\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u4e3a\u4e86\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u7535\u78c1\u91cd\u6784\u548c\u7a7a\u95f4\u91cd\u6784\u7684\u65b0\u67b6\u6784\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u591a\u7528\u6237\u901a\u4fe1\u7cfb\u7edf\u7684\u548c\u901f\u7387\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6db2\u4f53\u667a\u80fd\u8d85\u8868\u9762(LIM)\u8f85\u52a9\u7684\u4e0b\u884c\u591a\u7528\u6237MISO\u7cfb\u7edf\uff0c\u57fa\u7ad9\u548c\u8d85\u8868\u9762\u5206\u522b\u914d\u5907\u6d41\u4f53\u5929\u7ebf\u548c\u6db2\u4f53\u5143\u4ef6\u3002\u5efa\u7acb\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u3001LIM\u76f8\u79fb\u4ee5\u53ca\u6d41\u4f53\u5929\u7ebf\u548c\u6db2\u4f53\u5143\u4ef6\u7684\u4f4d\u7f6e\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\uff0c\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\uff0c\u4f7f\u7528\u9010\u6b21\u51f8\u8fd1\u4f3c(SCA)\u548c\u60e9\u7f5a\u51f8-\u51f9\u8fc7\u7a0b(PCCP)\u6c42\u89e3\u5404\u5b50\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684FAS-LIM\u67b6\u6784\u5728\u5404\u79cd\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u91c7\u7528\u4f20\u7edf\u56fa\u5b9a\u8d85\u8868\u9762\u548c\u56fa\u5b9a\u5929\u7ebf\u9635\u5217\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u7cfb\u7edf\u548c\u901f\u7387\u6027\u80fd\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6db2\u4f53\u667a\u80fd\u8d85\u8868\u9762\u67b6\u6784\u901a\u8fc7\u5b9e\u73b0\u7535\u78c1\u548c\u7a7a\u95f4\u7684\u8054\u5408\u91cd\u6784\u80fd\u529b\uff0c\u4e3a\u591a\u7528\u6237MISO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6027\u80fd\u589e\u5f3a\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u6d41\u4f53\u5929\u7ebf\u548c\u6db2\u4f53\u5143\u4ef6\u5728\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.16724", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16724", "abs": "https://arxiv.org/abs/2507.16724", "authors": ["Jinbo Hu", "Yin Cao", "Ming Wu", "Feiran Yang", "Jun Yang"], "title": "SALM: Spatial Audio Language Model with Structured Embeddings for Understanding and Editing", "comment": "5 pages, 1 figure", "summary": "Spatial audio understanding is essential for accurately perceiving and\ninterpreting acoustic environments. However, existing audio-language models\nstruggle with processing spatial audio and perceiving spatial acoustic scenes.\nWe introduce the Spatial Audio Language Model (SALM), a novel framework that\nbridges spatial audio and language via multi-modal contrastive learning. SALM\nconsists of a text encoder and a dual-branch audio encoder, decomposing spatial\nsound into semantic and spatial components through structured audio embeddings.\nKey features of SALM include seamless alignment of spatial and text\nrepresentations, separate and joint extraction of spatial and semantic\ninformation, zero-shot direction classification and robust support for spatial\naudio editing. Experimental results demonstrate that SALM effectively captures\nand aligns cross-modal representations. Furthermore, it supports advanced\nediting capabilities, such as altering directional audio using text-based\nembeddings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7a7a\u95f4\u97f3\u9891\u8bed\u8a00\u6a21\u578b(SALM)\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5c06\u7a7a\u95f4\u97f3\u9891\u4e0e\u8bed\u8a00\u8fde\u63a5\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u65b9\u5411\u5206\u7c7b\u548c\u57fa\u4e8e\u6587\u672c\u7684\u7a7a\u95f4\u97f3\u9891\u7f16\u8f91\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7a7a\u95f4\u97f3\u9891\u548c\u611f\u77e5\u7a7a\u95f4\u58f0\u5b66\u573a\u666f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u97f3\u9891\u73af\u5883\u7684\u51c6\u786e\u7406\u89e3\u548c\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u7a7a\u95f4\u97f3\u9891\u8bed\u8a00\u6a21\u578b(SALM)\u6846\u67b6\uff0c\u5305\u542b\u6587\u672c\u7f16\u7801\u5668\u548c\u53cc\u5206\u652f\u97f3\u9891\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u97f3\u9891\u5d4c\u5165\u5c06\u7a7a\u95f4\u58f0\u97f3\u5206\u89e3\u4e3a\u8bed\u4e49\u548c\u7a7a\u95f4\u7ec4\u4ef6\uff0c\u91c7\u7528\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u7a7a\u95f4\u97f3\u9891\u4e0e\u8bed\u8a00\u7684\u6865\u63a5\u3002", "result": "SALM\u80fd\u591f\u6709\u6548\u6355\u83b7\u548c\u5bf9\u9f50\u8de8\u6a21\u6001\u8868\u793a\uff0c\u652f\u6301\u96f6\u6837\u672c\u65b9\u5411\u5206\u7c7b\uff0c\u5e76\u5177\u5907\u9ad8\u7ea7\u7f16\u8f91\u80fd\u529b\uff0c\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u5d4c\u5165\u6765\u6539\u53d8\u65b9\u5411\u6027\u97f3\u9891\u3002", "conclusion": "SALM\u6210\u529f\u5b9e\u73b0\u4e86\u7a7a\u95f4\u97f3\u9891\u4e0e\u8bed\u8a00\u7684\u65e0\u7f1d\u5bf9\u9f50\uff0c\u80fd\u591f\u5206\u522b\u548c\u8054\u5408\u63d0\u53d6\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3a\u7a7a\u95f4\u97f3\u9891\u7406\u89e3\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16375", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16375", "abs": "https://arxiv.org/abs/2507.16375", "authors": ["Peng Liu", "Xinyi Wang", "Zesong Fei", "Yuan Wu", "Jie Xu", "Arumugam Nallanathan"], "title": "Latency Minimization Oriented Radio and Computation Resource Allocations for 6G V2X Networks with ISCC", "comment": "15 pages, 13 figures", "summary": "Incorporating mobile edge computing (MEC) and integrated sensing and\ncommunication (ISAC) has emerged as a promising technology to enable integrated\nsensing, communication, and computing (ISCC) in the sixth generation (6G)\nnetworks. ISCC is particularly attractive for vehicle-to-everything (V2X)\napplications, where vehicles perform ISAC to sense the environment and\nsimultaneously offload the sensing data to roadside base stations (BSs) for\nremote processing. In this paper, we investigate a particular ISCC-enabled V2X\nsystem consisting of multiple multi-antenna BSs serving a set of single-antenna\nvehicles, in which the vehicles perform their respective ISAC operations (for\nsimultaneous sensing and offloading to the associated BS) over orthogonal\nsub-bands. With the focus on fairly minimizing the sensing completion latency\nfor vehicles while ensuring the detection probability constraints, we jointly\noptimize the allocations of radio resources (i.e., the sub-band allocation,\ntransmit power control at vehicles, and receive beamforming at BSs) as well as\ncomputation resources at BS MEC servers. To solve the formulated complex\nmixed-integer nonlinear programming (MINLP) problem, we propose an alternating\noptimization algorithm. In this algorithm, we determine the sub-band allocation\nvia the branch-and-bound method, optimize the transmit power control via\nsuccessive convex approximation (SCA), and derive the receive beamforming and\ncomputation resource allocation at BSs in closed form based on generalized\nRayleigh entropy and fairness criteria, respectively. Simulation results\ndemonstrate that the proposed joint resource allocation design significantly\nreduces the maximum task completion latency among all vehicles. Furthermore, we\nalso demonstrate several interesting trade-offs between the system performance\nand resource utilizations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e866G\u7f51\u7edc\u4e2dISCC\u8d4b\u80fd\u7684V2X\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u65e0\u7ebf\u7535\u8d44\u6e90\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u516c\u5e73\u5730\u6700\u5c0f\u5316\u8f66\u8f86\u611f\u77e5\u5b8c\u6210\u5ef6\u8fdf\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5c06\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97(MEC)\u548c\u96c6\u6210\u611f\u77e5\u901a\u4fe1(ISAC)\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u96c6\u6210\u611f\u77e5\u3001\u901a\u4fe1\u548c\u8ba1\u7b97(ISCC)\uff0c\u7279\u522b\u662f\u5728\u8f66\u8054\u7f51(V2X)\u5e94\u7528\u4e2d\uff0c\u8f66\u8f86\u9700\u8981\u540c\u65f6\u8fdb\u884c\u73af\u5883\u611f\u77e5\u548c\u6570\u636e\u5378\u8f7d\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u7684\u6df7\u5408\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff1a1)\u901a\u8fc7\u5206\u652f\u5b9a\u754c\u6cd5\u786e\u5b9a\u5b50\u5e26\u5206\u914d\uff1b2)\u901a\u8fc7\u8fde\u7eed\u51f8\u8fd1\u4f3c\u4f18\u5316\u53d1\u5c04\u529f\u7387\u63a7\u5236\uff1b3)\u57fa\u4e8e\u5e7f\u4e49\u745e\u5229\u71b5\u548c\u516c\u5e73\u6027\u51c6\u5219\u5206\u522b\u4ee5\u95ed\u5f0f\u5f62\u5f0f\u63a8\u5bfc\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u8d44\u6e90\u5206\u914d\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u6240\u6709\u8f66\u8f86\u4e2d\u7684\u6700\u5927\u4efb\u52a1\u5b8c\u6210\u5ef6\u8fdf\uff0c\u5e76\u5c55\u793a\u4e86\u7cfb\u7edf\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u51e0\u4e2a\u6709\u8da3\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86ISCC\u8d4b\u80fdV2X\u7cfb\u7edf\u4e2d\u65e0\u7ebf\u7535\u8d44\u6e90\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u8054\u5408\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u68c0\u6d4b\u6982\u7387\u7ea6\u675f\u7684\u540c\u65f6\u516c\u5e73\u5730\u6700\u5c0f\u5316\u4e86\u8f66\u8f86\u611f\u77e5\u5b8c\u6210\u5ef6\u8fdf\uff0c\u4e3a6G\u7f51\u7edc\u4e2d\u7684\u8f66\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002"}}
{"id": "2507.16550", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16550", "abs": "https://arxiv.org/abs/2507.16550", "authors": ["George C. Alexandropoulos", "Nir Shlezinger", "Ioannis Gavras", "Haiyang Zhang"], "title": "Hybrid RISs for Simultaneous Tunable Reflections and Sensing", "comment": null, "summary": "The concept of smart wireless environments envisions dynamic programmable\npropagation of information-bearing signals through the deployment of\nReconfigurable Intelligent Surfaces (RISs). Typical RIS implementations include\nmetasurfaces with passive unit elements capable to reflect their incident waves\nin controllable ways. However, this solely reflective operation induces\nsignificant challenges in the RIS orchestration from the wireless network. For\nexample, channel estimation, which is essential for coherent RIS-empowered\nwireless communications, is quite challenging with the available solely\nreflecting RIS designs. This chapter reviews the emerging concept of Hybrid\nReflecting and Sensing RISs (HRISs), which enables metasurfaces to reflect the\nimpinging signal in a controllable manner, while simultaneously sensing a\nportion of it. The sensing capability of HRISs facilitates various network\nmanagement functionalities, including channel parameter estimation and\nlocalization, while, most importantly, giving rise to computationally\nautonomous and self-configuring RISs. The implementation details of HRISs are\nfirst presented, which are then followed by a convenient mathematical model for\ncharacterizing their dual functionality. Then, two indicative applications of\nHRISs are discussed, one for simultaneous communications and sensing and\nanother that showcases their usefulness for estimating the individual channels\nin the uplink of a multi-user HRIS-empowered communication system. For both of\nthese applications, performance evaluation results are included validating the\nrole of HRISs for sensing as well as integrated sensing and communications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u6df7\u5408\u53cd\u5c04\u611f\u77e5\u667a\u80fd\u8868\u9762(HRISs)\u7684\u6982\u5ff5\uff0c\u8fd9\u662f\u4e00\u79cd\u65e2\u80fd\u53ef\u63a7\u53cd\u5c04\u4fe1\u53f7\u53c8\u80fd\u540c\u65f6\u611f\u77e5\u90e8\u5206\u4fe1\u53f7\u7684\u65b0\u578b\u667a\u80fd\u8868\u9762\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRIS\u5728\u4fe1\u9053\u4f30\u8ba1\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7684\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(RIS)\u4ec5\u5177\u5907\u53cd\u5c04\u529f\u80fd\uff0c\u5728\u65e0\u7ebf\u7f51\u7edc\u534f\u8c03\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4fe1\u9053\u4f30\u8ba1\u7b49\u76f8\u5e72\u901a\u4fe1\u5fc5\u9700\u529f\u80fd\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u5177\u5907\u611f\u77e5\u80fd\u529b\u7684\u65b0\u578b\u667a\u80fd\u8868\u9762\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u6df7\u5408\u53cd\u5c04\u611f\u77e5\u667a\u80fd\u8868\u9762(HRISs)\u6982\u5ff5\uff0c\u4f7f\u8d85\u8868\u9762\u80fd\u591f\u4ee5\u53ef\u63a7\u65b9\u5f0f\u53cd\u5c04\u5165\u5c04\u4fe1\u53f7\u7684\u540c\u65f6\u611f\u77e5\u5176\u4e2d\u4e00\u90e8\u5206\u4fe1\u53f7\u3002\u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86HRISs\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u5efa\u7acb\u4e86\u63cf\u8ff0\u5176\u53cc\u91cd\u529f\u80fd\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e24\u4e2a\u5e94\u7528\u6848\u4f8b\uff1a\u540c\u65f6\u901a\u4fe1\u611f\u77e5\u548c\u591a\u7528\u6237\u4e0a\u884c\u94fe\u8def\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "HRISs\u7684\u611f\u77e5\u80fd\u529b\u6709\u6548\u652f\u6301\u4e86\u5404\u79cd\u7f51\u7edc\u7ba1\u7406\u529f\u80fd\uff0c\u5305\u62ec\u4fe1\u9053\u53c2\u6570\u4f30\u8ba1\u548c\u5b9a\u4f4d\uff0c\u6700\u91cd\u8981\u7684\u662f\u5b9e\u73b0\u4e86\u8ba1\u7b97\u81ea\u4e3b\u548c\u81ea\u914d\u7f6e\u7684RIS\u3002\u6027\u80fd\u8bc4\u4f30\u7ed3\u679c\u9a8c\u8bc1\u4e86HRISs\u5728\u611f\u77e5\u4ee5\u53ca\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u65b9\u9762\u7684\u4f5c\u7528\u3002", "conclusion": "\u6df7\u5408\u53cd\u5c04\u611f\u77e5\u667a\u80fd\u8868\u9762(HRISs)\u901a\u8fc7\u540c\u65f6\u5177\u5907\u53cd\u5c04\u548c\u611f\u77e5\u53cc\u91cd\u529f\u80fd\uff0c\u514b\u670d\u4e86\u4f20\u7edfRIS\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u667a\u80fd\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u52a8\u6001\u53ef\u7f16\u7a0b\u4fe1\u53f7\u4f20\u64ad\u63d0\u4f9b\u4e86\u66f4\u5b8c\u5584\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u901a\u4fe1\u611f\u77e5\u4e00\u4f53\u5316\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.16733", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16733", "abs": "https://arxiv.org/abs/2507.16733", "authors": ["Dayu Fan", "Rui Meng", "Xiaodong Xu", "Yiming Liu", "Guoshun Nan", "Chenyuan Feng", "Shujun Han", "Song Gao", "Bingxuan Xu", "Dusit Niyato", "Tony Q. S. Quek", "Ping Zhang"], "title": "Generative Diffusion Models for Wireless Networks: Fundamental, Architecture, and State-of-the-Art", "comment": "30 pages, 11 figures", "summary": "With the rapid development of Generative Artificial Intelligence (GAI)\ntechnology, Generative Diffusion Models (GDMs) have shown significant\nempowerment potential in the field of wireless networks due to advantages, such\nas noise resistance, training stability, controllability, and multimodal\ngeneration. Although there have been multiple studies focusing on GDMs for\nwireless networks, there is still a lack of comprehensive reviews on their\ntechnological evolution. Motivated by this, we systematically explore the\napplication of GDMs in wireless networks. Firstly, starting from mathematical\nprinciples, we analyze technical advantages of GDMs and present six\nrepresentative models. Furthermore, we propose the multi-layer wireless network\narchitecture including sensing layer, transmission layer, application layer,\nand security plane. We also introduce the core mechanisms of GDM at each of the\nlayers. Subsequently, we conduct a rigorous review on existing GDM-based\nschemes, with a focus on analyzing their innovative points, the role of GDMs,\nstrengths, and weaknesses. Ultimately, we extract key challenges and provide\npotential solutions, with the aim of providing directional guidance for future\nresearch in this field.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u751f\u6210\u6269\u6563\u6a21\u578b(GDMs)\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u6280\u672f\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u591a\u5c42\u65e0\u7ebf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u7cfb\u7edf\u56de\u987e\u4e86\u73b0\u6709\u7684\u57fa\u4e8eGDM\u7684\u65b9\u6848\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u65e0\u7ebf\u7f51\u7edc\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u8d4b\u80fd\u6f5c\u529b\uff0c\u5177\u6709\u6297\u566a\u58f0\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u53ef\u63a7\u6027\u548c\u591a\u6a21\u6001\u751f\u6210\u7b49\u4f18\u52bf\u3002\u5c3d\u7ba1\u5df2\u6709\u591a\u9879\u7814\u7a76\u5173\u6ce8\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684GDMs\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u5bf9\u5176\u6280\u672f\u6f14\u8fdb\u7684\u5168\u9762\u7efc\u8ff0\u3002", "method": "\u4ece\u6570\u5b66\u539f\u7406\u51fa\u53d1\u5206\u6790GDMs\u7684\u6280\u672f\u4f18\u52bf\uff0c\u5c55\u793a\u516d\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff1b\u63d0\u51fa\u5305\u542b\u611f\u77e5\u5c42\u3001\u4f20\u8f93\u5c42\u3001\u5e94\u7528\u5c42\u548c\u5b89\u5168\u5e73\u9762\u7684\u591a\u5c42\u65e0\u7ebf\u7f51\u7edc\u67b6\u6784\uff1b\u4ecb\u7ecdGDM\u5728\u5404\u5c42\u7684\u6838\u5fc3\u673a\u5236\uff1b\u5bf9\u73b0\u6709\u57fa\u4e8eGDM\u7684\u65b9\u6848\u8fdb\u884c\u4e25\u683c\u56de\u987e\uff0c\u91cd\u70b9\u5206\u6790\u5176\u521b\u65b0\u70b9\u3001GDMs\u7684\u4f5c\u7528\u3001\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "result": "\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86GDMs\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u6280\u672f\u67b6\u6784\u6846\u67b6\uff0c\u5168\u9762\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6848\u7684\u7279\u70b9\u548c\u6027\u80fd\uff0c\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u63d0\u53d6\u4e86\u5173\u952e\u6311\u6218\u5e76\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u6027\u6307\u5bfc\uff0c\u63a8\u52a8\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u5e94\u7528\u3002"}}

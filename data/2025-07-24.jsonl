{"id": "2507.16832", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16832", "abs": "https://arxiv.org/abs/2507.16832", "authors": ["Peter Plantinga", "Briac Cordelle", "Dominique Louër", "Mirco Ravanelli", "Denise Klein"], "title": "Does Language Matter for Early Detection of Parkinson's Disease from Speech?", "comment": "Accepted to IEEE Workshop on Machine Learning for Signal Processing\n  (MLSP) 2025", "summary": "Using speech samples as a biomarker is a promising avenue for detecting and\nmonitoring the progression of Parkinson's disease (PD), but there is\nconsiderable disagreement in the literature about how best to collect and\nanalyze such data. Early research in detecting PD from speech used a sustained\nvowel phonation (SVP) task, while some recent research has explored recordings\nof more cognitively demanding tasks. To assess the role of language in PD\ndetection, we tested pretrained models with varying data types and pretraining\nobjectives and found that (1) text-only models match the performance of\nvocal-feature models, (2) multilingual Whisper outperforms self-supervised\nmodels whereas monolingual Whisper does worse, and (3) AudioSet pretraining\nimproves performance on SVP but not spontaneous speech. These findings together\nhighlight the critical role of language for the early detection of Parkinson's\ndisease."}
{"id": "2507.16834", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16834", "abs": "https://arxiv.org/abs/2507.16834", "authors": ["Jordan Madden", "Matthew Stone", "Dimitri Johnson", "Daniel Geddez"], "title": "Towards Robust Speech Recognition for Jamaican Patois Music Transcription", "comment": null, "summary": "Although Jamaican Patois is a widely spoken language, current speech\nrecognition systems perform poorly on Patois music, producing inaccurate\ncaptions that limit accessibility and hinder downstream applications. In this\nwork, we take a data-centric approach to this problem by curating more than 40\nhours of manually transcribed Patois music. We use this dataset to fine-tune\nstate-of-the-art automatic speech recognition (ASR) models, and use the results\nto develop scaling laws for the performance of Whisper models on Jamaican\nPatois audio. We hope that this work will have a positive impact on the\naccessibility of Jamaican Patois music and the future of Jamaican Patois\nlanguage modeling."}
{"id": "2507.16835", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16835", "abs": "https://arxiv.org/abs/2507.16835", "authors": ["Nima Yazdani", "Ali Ansari", "Aruj Mahajan", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi"], "title": "Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems", "comment": null, "summary": "Voice-based conversational AI systems increasingly rely on cascaded\narchitectures combining speech-to-text (STT), large language models (LLMs), and\ntext-to-speech (TTS) components. However, systematic evaluation of different\ncomponent combinations in production settings remains understudied. We present\na large-scale empirical comparison of STT x LLM x TTS stacks using data from\nover 300,000 AI-conducted job interviews. We develop an automated evaluation\nframework using LLM-as-a-Judge to assess conversational quality, technical\naccuracy, and skill assessment capabilities. Our analysis of four production\nconfigurations reveals that Google STT paired with GPT-4.1 significantly\noutperforms alternatives in both conversational and technical quality metrics.\nSurprisingly, we find that objective quality metrics correlate weakly with user\nsatisfaction scores, suggesting that user experience in voice-based AI systems\ndepends on factors beyond technical performance. Our findings provide practical\nguidance for selecting components in multimodal conversational AI systems and\ncontribute a validated evaluation methodology for voice-based interactions."}
{"id": "2507.16836", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16836", "abs": "https://arxiv.org/abs/2507.16836", "authors": ["Peter Plantinga", "Jen-Kai Chen", "Roozbeh Sattari", "Mirco Ravanelli", "Denise Klein"], "title": "From Black Box to Biomarker: Sparse Autoencoders for Interpreting Speech Models of Parkinson's Disease", "comment": "14 pages, 5 figures, submitted to NeurIPS 2025", "summary": "Speech holds promise as a cost-effective and non-invasive biomarker for\nneurological conditions such as Parkinson's disease (PD). While deep learning\nsystems trained on raw audio can find subtle signals not available from\nhand-crafted features, their black-box nature hinders clinical adoption. To\naddress this, we apply sparse autoencoders (SAEs) to uncover interpretable\ninternal representations from a speech-based PD detection system. We introduce\na novel mask-based activation for adapting SAEs to small biomedical datasets,\ncreating sparse disentangled dictionary representations. These dictionary\nentries are found to have strong associations with characteristic articulatory\ndeficits in PD speech, such as reduced spectral flux and increased spectral\nflatness in the low-energy regions highlighted by the model attention. We\nfurther show that the spectral flux is related to volumetric measurements of\nthe putamen from MRI scans, demonstrating the potential of SAEs to reveal\nclinically relevant biomarkers for disease monitoring and diagnosis."}
{"id": "2507.16843", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16843", "abs": "https://arxiv.org/abs/2507.16843", "authors": ["Zhongsheng Wang", "Sijie Wang", "Jia Wang", "Yung-I Liang", "Yuxi Zhang", "Jiamou Liu"], "title": "Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems", "comment": "Accepted by ICONIP 2024", "summary": "In the design of customer relationship management (CRM) systems, accurately\nidentifying customer types and offering personalized services are key to\nenhancing customer satisfaction and loyalty. However, this process faces the\nchallenge of discerning customer voices and intentions, and general pre-trained\nautomatic speech recognition (ASR) models make it difficult to effectively\naddress industry-specific speech recognition tasks. To address this issue, we\ninnovatively proposed a solution for fine-tuning industry-specific ASR models,\nwhich significantly improved the performance of the fine-tuned ASR models in\nindustry applications. Experimental results show that our method substantially\nimproves the crucial auxiliary role of the ASR model in industry CRM systems,\nand this approach has also been adopted in actual industrial applications."}
{"id": "2507.17003", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17003", "abs": "https://arxiv.org/abs/2507.17003", "authors": ["Seunggeun Kim", "Ziyi Wang", "Sungyoung Lee", "Youngmin Oh", "Hanqing Zhu", "Doyun Kim", "David Z. Pan"], "title": "PPAAS: PVT and Pareto Aware Analog Sizing via Goal-conditioned Reinforcement Learning", "comment": "Accepted to the 44th International Conference on Computer-Aided\n  Design (ICCAD 2025); 9 pages, 10 figures", "summary": "Device sizing is a critical yet challenging step in analog and mixed-signal\ncircuit design, requiring careful optimization to meet diverse performance\nspecifications. This challenge is further amplified under process, voltage, and\ntemperature (PVT) variations, which cause circuit behavior to shift across\ndifferent corners. While reinforcement learning (RL) has shown promise in\nautomating sizing for fixed targets, training a generalized policy that can\nadapt to a wide range of design specifications under PVT variations requires\nmuch more training samples and resources. To address these challenges, we\npropose a \\textbf{Goal-conditioned RL framework} that enables efficient policy\ntraining for analog device sizing across PVT corners, with strong\ngeneralization capability. To improve sample efficiency, we introduce\nPareto-front Dominance Goal Sampling, which constructs an automatic curriculum\nby sampling goals from the Pareto frontier of previously achieved goals. This\nstrategy is further enhanced by integrating Conservative Hindsight Experience\nReplay, which assigns relabeled goals with conservative virtual rewards to\nstabilize training and accelerate convergence. To reduce simulation overhead,\nour framework incorporates a Skip-on-Fail simulation strategy, which skips\nfull-corner simulations when nominal-corner simulation fails to meet target\nspecifications. Experiments on benchmark circuits demonstrate $\\sim$1.6$\\times$\nimprovement in sample efficiency and $\\sim$4.1$\\times$ improvement in\nsimulation efficiency compared to existing sizing methods. Code and benchmarks\nare publicly available at https://github.com/SeunggeunKimkr/PPAAS"}
{"id": "2507.16838", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16838", "abs": "https://arxiv.org/abs/2507.16838", "authors": ["Xinwei Cao", "Zijian Fan", "Torbjørn Svendsen", "Giampiero Salvi"], "title": "Segmentation-free Goodness of Pronunciation", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment."}
{"id": "2507.17297", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17297", "abs": "https://arxiv.org/abs/2507.17297", "authors": ["Tobias Morocutti", "Jonathan Greif", "Paul Primus", "Florian Schmid", "Gerhard Widmer"], "title": "On Temporal Guidance and Iterative Refinement in Audio Source Separation", "comment": null, "summary": "Spatial semantic segmentation of sound scenes (S5) involves the accurate\nidentification of active sound classes and the precise separation of their\nsources from complex acoustic mixtures. Conventional systems rely on a\ntwo-stage pipeline - audio tagging followed by label-conditioned source\nseparation - but are often constrained by the absence of fine-grained temporal\ninformation critical for effective separation. In this work, we address this\nlimitation by introducing a novel approach for S5 that enhances the synergy\nbetween the event detection and source separation stages. Our key contributions\nare threefold. First, we fine-tune a pre-trained Transformer to detect active\nsound classes. Second, we utilize a separate instance of this fine-tuned\nTransformer to perform sound event detection (SED), providing the separation\nmodule with detailed, time-varying guidance. Third, we implement an iterative\nrefinement mechanism that progressively enhances separation quality by\nrecursively reusing the separator's output from previous iterations. These\nadvancements lead to significant improvements in both audio tagging and source\nseparation performance, as demonstrated by our system's second-place finish in\nTask 4 of the DCASE Challenge 2025. Our implementation and model checkpoints\nare available in our GitHub repository: https://github.com/theMoro/dcase25task4 ."}
{"id": "2507.17106", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17106", "abs": "https://arxiv.org/abs/2507.17106", "authors": ["Jiazhao Wang", "Wenchao Jiang"], "title": "Efficient and Distortion-less Spectrum Multiplexer via Neural Network-based Filter Banks", "comment": null, "summary": "Spectrum multiplexer enables simultaneous transmission of multiple\nnarrow-band IoT signals through gateway devices, thereby enhancing overall\nspectrum utilization. We propose a novel solution based on filter banks that\noffer increased efficiency and minimal distortion compared with conventional\nmethods. We follow a model-driven approach to integrate the neural networks\ninto the filter bank design by interpreting the neural network models as filter\nbanks. The proposed NN-based filter banks can leverage advanced learning\ncapabilities to achieve distortionless multiplexing and harness hardware\nacceleration for high efficiency. Then, we evaluate the performance of the\nspectrum multiplexer implemented by NN-based filter banks for various types of\nsignals and environmental conditions. The results show that it can achieve a\nlow distortion level down to $-39$dB normalized mean squared error.\nFurthermore, it achieves up to $35$ times execution efficiency gain and $10$dB\nSNR gain compared with the conventional methods. The field applications show\nthat it can handle both the heterogeneous and homogeneous IoT networks,\nresulting in high packet reception ratio at the standard receivers up to\n$98\\%$."}
{"id": "2507.16845", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.16845", "abs": "https://arxiv.org/abs/2507.16845", "authors": ["Xiaoran Xua", "In-Ho Rab", "Ravi Sankarc"], "title": "Enhancing Lung Disease Diagnosis via Semi-Supervised Machine Learning", "comment": null, "summary": "Lung diseases, including lung cancer and COPD, are significant health\nconcerns globally. Traditional diagnostic methods can be costly,\ntime-consuming, and invasive. This study investigates the use of semi\nsupervised learning methods for lung sound signal detection using a model\ncombination of MFCC+CNN. By introducing semi supervised learning modules such\nas Mix Match, Co-Refinement, and Co Refurbishing, we aim to enhance the\ndetection performance while reducing dependence on manual annotations. With the\nadd-on semi-supervised modules, the accuracy rate of the MFCC+CNN model is\n92.9%, an increase of 3.8% to the baseline model. The research contributes to\nthe field of lung disease sound detection by addressing challenges such as\nindividual differences, feature insufficient labeled data."}
{"id": "2507.17326", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17326", "abs": "https://arxiv.org/abs/2507.17326", "authors": ["Milena Davudova", "Ziyuan Cai", "Valentina Giunchiglia", "Dragos C. Gruia", "Giulia Sanguedolce", "Adam Hampshire", "Fatemeh Geranmayeh"], "title": "Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task", "comment": null, "summary": "Detailed assessment of language impairment following stroke remains a\ncognitively complex and clinician-intensive task, limiting timely and scalable\ndiagnosis. Automatic Speech Recognition (ASR) foundation models offer a\npromising pathway to augment human evaluation through intelligent systems, but\ntheir effectiveness in the context of speech and language impairment remains\nuncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR\nfoundation model, can be applied to transcribe and analyze speech from patients\nwith stroke during a commonly used picture-naming task. We assess both verbatim\ntranscription accuracy and the model's ability to support downstream prediction\nof language function, which has major implications for outcomes after stroke.\nOur results show that the baseline Whisper model performs poorly on single-word\nspeech utterances. Nevertheless, fine-tuning Whisper significantly improves\ntranscription accuracy (reducing Word Error Rate by 87.72% in healthy speech\nand 71.22% in speech from patients). Further, learned representations from the\nmodel enable accurate prediction of speech quality (average F1 Macro of 0.74\nfor healthy, 0.75 for patients). However, evaluations on an unseen (TORGO)\ndataset reveal limited generalizability, highlighting the inability of Whisper\nto perform zero-shot transcription of single-word utterances on out-of-domain\nclinical speech and emphasizing the need to adapt models to specific clinical\npopulations. While challenges remain in cross-domain generalization, these\nfindings highlight the potential of foundation models, when appropriately\nfine-tuned, to advance automated speech and language assessment and\nrehabilitation for stroke-related impairments."}
{"id": "2507.17153", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17153", "abs": "https://arxiv.org/abs/2507.17153", "authors": ["Junjie Fang", "Chao Zhang", "Jiancheng An", "Hongwen Yu", "Qingqing Wu", "Mérouane Debbah", "Chau Yuen"], "title": "Stacked Intelligent Metasurface Assisted Multiuser Communications: From a Rate Fairness Perspective", "comment": null, "summary": "Stacked intelligent metasurface (SIM) extends the concept of single-layer\nreconfigurable holographic surfaces (RHS) by incorporating a multi-layered\nstructure, thereby providing enhanced control over electromagnetic wave\npropagation and improved signal processing capabilities. This study\ninvestigates the potential of SIM in enhancing the rate fairness in multiuser\ndownlink systems by addressing two key optimization problems: maximizing the\nminimum rate (MR) and maximizing the geometric mean of rates (GMR). {The former\nstrives to enhance the minimum user rate, thereby ensuring fairness among\nusers, while the latter relaxes fairness requirements to strike a better\ntrade-off between user fairness and system sum-rate (SR).} For the MR\nmaximization, we adopt a consensus alternating direction method of multipliers\n(ADMM)-based approach, which decomposes the approximated problem into\nsub-problems with closed-form solutions. {For GMR maximization, we develop an\nalternating optimization (AO)-based algorithm that also yields closed-form\nsolutions and can be seamlessly adapted for SR maximization. Numerical results\nvalidate the effectiveness and convergence of the proposed algorithms.}\nComparative evaluations show that MR maximization ensures near-perfect\nfairness, while GMR maximization balances fairness and system SR. Furthermore,\nthe two proposed algorithms respectively outperform existing related works in\nterms of MR and SR performance. Lastly, SIM with lower power consumption\nachieves performance comparable to that of multi-antenna digital beamforming."}
{"id": "2507.16875", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16875", "abs": "https://arxiv.org/abs/2507.16875", "authors": ["Isha Pandey", "Pranav Gaikwad", "Amruta Parulekar", "Ganesh Ramakrishnan"], "title": "Technical report: Impact of Duration Prediction on Speaker-specific TTS for Indian Languages", "comment": null, "summary": "High-quality speech generation for low-resource languages, such as many\nIndian languages, remains a significant challenge due to limited data and\ndiverse linguistic structures. Duration prediction is a critical component in\nmany speech generation pipelines, playing a key role in modeling prosody and\nspeech rhythm. While some recent generative approaches choose to omit explicit\nduration modeling, often at the cost of longer training times. We retain and\nexplore this module to better understand its impact in the linguistically rich\nand data-scarce landscape of India. We train a non-autoregressive Continuous\nNormalizing Flow (CNF) based speech model using publicly available Indian\nlanguage data and evaluate multiple duration prediction strategies for\nzero-shot, speaker-specific generation. Our comparative analysis on\nspeech-infilling tasks reveals nuanced trade-offs: infilling based predictors\nimprove intelligibility in some languages, while speaker-prompted predictors\nbetter preserve speaker characteristics in others. These findings inform the\ndesign and selection of duration strategies tailored to specific languages and\ntasks, underscoring the continued value of interpretable components like\nduration prediction in adapting advanced generative architectures to\nlow-resource, multilingual settings."}
{"id": "2507.17563", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17563", "abs": "https://arxiv.org/abs/2507.17563", "authors": ["Qing Wang", "Zehan Li", "Hang Lv", "Hongjie Chen", "Yaodong Song", "Jian Kang", "Jie Lian", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "BoSS: Beyond-Semantic Speech", "comment": null, "summary": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication."}
{"id": "2507.17154", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.17154", "abs": "https://arxiv.org/abs/2507.17154", "authors": ["Ruihua Wang", "Mingtong Chen", "Zhengbao Yang"], "title": "Design of a Noval Wearable ECG Monitoring Device", "comment": null, "summary": "The aim of this project is to develop a new wireless powered wearable ECG\nmonitoring device. The main goal of the project is to provide a wireless,\nsmall-sized ECG monitoring device that can be worn for a long period of time by\nthe monitored person. Electrocardiogram ECG reflects physiological and\npathological information about heart activity and is commonly used to diagnose\nheart disease. Existing wearable smart ECG solutions suffer from high power\nconsumption in both ECG diagnosis and transmission for high accuracy.\nMonitoring of ECG devices is mainly done by data extraction and acquisition,\npre-processing, feature extraction, processing and analysis, visualisation and\nauxiliary procedures. During the pre-processing of the information, different\nkinds of noise generated during the signal collection need to be taken into\naccount. The quality of the signal-to-noise ratio can usually be improved by\noptimising algorithms and reducing the noise power. The choice of electrodes\nusually has a direct impact on the signal-to-noise ratio and the user\nexperience, and conventional Ag/AgCl gel electrodes are not suitable for\nlong-term and dynamic monitoring as they are prone to skin irritation,\ninflammation and allergic reactions. Therefore, a completely new way of\ncombining electrodes and wires will be used in the report. The electrodes and\nwires are cut in one piece from a silver-plated fabric. The wire portion is cut\ninto a curved structure close to an S shape to ensure that it has good\nductility for comfort and signal integrity during daily movement of the\ngarment."}
{"id": "2507.17208", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17208", "abs": "https://arxiv.org/abs/2507.17208", "authors": ["Ryo Terashima", "Yuma Shirahata", "Masaya Kawamura"], "title": "SLASH: Self-Supervised Speech Pitch Estimation Leveraging DSP-derived Absolute Pitch", "comment": "Accepted to INTERSPEECH 2025", "summary": "We present SLASH, a pitch estimation method of speech signals based on\nself-supervised learning (SSL). To enhance the performance of conventional\nSSL-based approaches that primarily depend on the relative pitch difference\nderived from pitch shifting, our method incorporates absolute pitch values by\n1) introducing a prior pitch distribution derived from digital signal\nprocessing (DSP), and 2) optimizing absolute pitch through gradient descent\nwith a loss between the target and differentiable DSP-derived spectrograms. To\nstabilize the optimization, a novel spectrogram generation method is used that\nskips complicated waveform generation. In addition, the aperiodic components in\nspeech are accurately predicted through differentiable DSP, enhancing the\nmethod's applicability to speech signal processing. Experimental results showed\nthat the proposed method outperformed both baseline DSP and SSL-based pitch\nestimation methods, attributed to the effective integration of SSL and DSP."}
{"id": "2507.17682", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17682", "abs": "https://arxiv.org/abs/2507.17682", "authors": ["Daiqi Liu", "Tomás Arias-Vergara", "Jana Hutter", "Andreas Maier", "Paula Andrea Pérez-Toro"], "title": "Audio-Vision Contrastive Learning for Phonological Class Recognition", "comment": "conference to TSD 2025", "summary": "Accurate classification of articulatory-phonological features plays a vital\nrole in understanding human speech production and developing robust speech\ntechnologies, particularly in clinical contexts where targeted phonemic\nanalysis and therapy can improve disease diagnosis accuracy and personalized\nrehabilitation. In this work, we propose a multimodal deep learning framework\nthat combines real-time magnetic resonance imaging (rtMRI) and speech signals\nto classify three key articulatory dimensions: manner of articulation, place of\narticulation, and voicing. We perform classification on 15 phonological classes\nderived from the aforementioned articulatory dimensions and evaluate the system\nwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,\nmultimodal middle fusion, and contrastive learning-based audio-vision fusion.\nExperimental results on the USC-TIMIT dataset show that our contrastive\nlearning-based approach achieves state-of-the-art performance, with an average\nF1-score of 0.81, representing an absolute increase of 0.23 over the unimodal\nbaseline. The results confirm the effectiveness of contrastive representation\nlearning for multimodal articulatory analysis. Our code and processed dataset\nwill be made publicly available at\nhttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research."}
{"id": "2507.17196", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17196", "abs": "https://arxiv.org/abs/2507.17196", "authors": ["Hyelin Nam", "Jihong Park", "Jinho Choi", "Seong-Lyun Kim"], "title": "Hybrid Semantic-Complementary Transmission for High-Fidelity Image Reconstruction", "comment": null, "summary": "Recent advances in semantic communication (SC) have introduced neural network\n(NN)-based transceivers that convey semantic representation (SR) of signals\nsuch as images. However, these NNs are trained over diverse image distributions\nand thus often fail to reconstruct fine-grained image-specific details. To\novercome this limited reconstruction fidelity, we propose an extended SC\nframework, hybrid semantic communication (HSC), which supplements SR with\ncomplementary representation (CR) capturing residual image-specific\ninformation. The CR is constructed at the transmitter, and is combined with the\nactual SC outcome at the receiver to yield a high-fidelity recomposed image.\nWhile the transmission load of SR is fixed due to its NN-based structure, the\nload of CR can be flexibly adjusted to achieve a desirable fidelity. This\ncontrollability directly influences the final reconstruction error, for which\nwe derive a closed-form expression and the corresponding optimal CR. Simulation\nresults demonstrate that HSC substantially reduces MSE compared to the baseline\nSC without CR transmission across various channels and NN architectures."}
{"id": "2507.17540", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.17540", "abs": "https://arxiv.org/abs/2507.17540", "authors": ["Piotr Masztalski", "Michał Romaniuk", "Jakub Żak", "Mateusz Matuszewski", "Konrad Kowalczyk"], "title": "Clustering-based hard negative sampling for supervised contrastive speaker verification", "comment": "Accepted to INTERSPEECH 2025", "summary": "In speaker verification, contrastive learning is gaining popularity as an\nalternative to the traditionally used classification-based approaches.\nContrastive methods can benefit from an effective use of hard negative pairs,\nwhich are different-class samples particularly challenging for a verification\nmodel due to their similarity. In this paper, we propose CHNS - a\nclustering-based hard negative sampling method, dedicated for supervised\ncontrastive speaker representation learning. Our approach clusters embeddings\nof similar speakers, and adjusts batch composition to obtain an optimal ratio\nof hard and easy negatives during contrastive loss calculation. Experimental\nevaluation shows that CHNS outperforms a baseline supervised contrastive\napproach with and without loss-based hard negative sampling, as well as a\nstate-of-the-art classification-based approach to speaker verification by as\nmuch as 18 % relative EER and minDCF on the VoxCeleb dataset using two\nlightweight model architectures."}
{"id": "2507.16845", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.16845", "abs": "https://arxiv.org/abs/2507.16845", "authors": ["Xiaoran Xua", "In-Ho Rab", "Ravi Sankarc"], "title": "Enhancing Lung Disease Diagnosis via Semi-Supervised Machine Learning", "comment": null, "summary": "Lung diseases, including lung cancer and COPD, are significant health\nconcerns globally. Traditional diagnostic methods can be costly,\ntime-consuming, and invasive. This study investigates the use of semi\nsupervised learning methods for lung sound signal detection using a model\ncombination of MFCC+CNN. By introducing semi supervised learning modules such\nas Mix Match, Co-Refinement, and Co Refurbishing, we aim to enhance the\ndetection performance while reducing dependence on manual annotations. With the\nadd-on semi-supervised modules, the accuracy rate of the MFCC+CNN model is\n92.9%, an increase of 3.8% to the baseline model. The research contributes to\nthe field of lung disease sound detection by addressing challenges such as\nindividual differences, feature insufficient labeled data."}
{"id": "2507.17224", "categories": ["eess.SP", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.17224", "abs": "https://arxiv.org/abs/2507.17224", "authors": ["Feng Cao", "Zishuo Feng"], "title": "HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes", "comment": "9 pages, 3 figures, 6 tables", "summary": "Extracellular recordings are brief voltage fluctuations recorded near\nneurons, widely used in neuroscience as the basis for decoding brain activity\nat single-neuron resolution. Spike sorting, which assigns each spike to its\nsource neuron, is a critical step in brain sensing pipelines. However, it\nremains challenging under low signal-to-noise ratio (SNR), electrode drift, and\ncross-session variability. In this paper, we propose HuiduRep, a robust\nself-supervised representation learning framework that extracts discriminative\nand generalizable features from extracellular spike waveforms. By combining\ncontrastive learning with a denoising autoencoder, HuiduRep learns latent\nrepresentations that are robust to noise and drift. Built on HuiduRep, we\ndevelop a spike sorting pipeline that clusters spike representations without\nsupervision. Experiments on hybrid and real-world datasets demonstrate that\nHuiduRep achieves strong robustness and the pipeline matches or outperforms\nstate-of-the-art tools such as KiloSort4 and MountainSort5. These findings\ndemonstrate the potential of self-supervised spike representation learning as a\nfoundational tool for robust and generalizable processing of extracellular\nrecordings."}
{"id": "2507.17735", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.17735", "abs": "https://arxiv.org/abs/2507.17735", "authors": ["Qibing Bai", "Sho Inoue", "Shuai Wang", "Zhongjie Jiang", "Yannan Wang", "Haizhou Li"], "title": "Accent Normalization Using Self-Supervised Discrete Tokens with Non-Parallel Data", "comment": "Accepted to INTERSPEECH 2025", "summary": "Accent normalization converts foreign-accented speech into native-like speech\nwhile preserving speaker identity. We propose a novel pipeline using\nself-supervised discrete tokens and non-parallel training data. The system\nextracts tokens from source speech, converts them through a dedicated model,\nand synthesizes the output using flow matching. Our method demonstrates\nsuperior performance over a frame-to-frame baseline in naturalness,\naccentedness reduction, and timbre preservation across multiple English\naccents. Through token-level phonetic analysis, we validate the effectiveness\nof our token-based approach. We also develop two duration preservation methods,\nsuitable for applications such as dubbing."}
{"id": "2507.17540", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.17540", "abs": "https://arxiv.org/abs/2507.17540", "authors": ["Piotr Masztalski", "Michał Romaniuk", "Jakub Żak", "Mateusz Matuszewski", "Konrad Kowalczyk"], "title": "Clustering-based hard negative sampling for supervised contrastive speaker verification", "comment": "Accepted to INTERSPEECH 2025", "summary": "In speaker verification, contrastive learning is gaining popularity as an\nalternative to the traditionally used classification-based approaches.\nContrastive methods can benefit from an effective use of hard negative pairs,\nwhich are different-class samples particularly challenging for a verification\nmodel due to their similarity. In this paper, we propose CHNS - a\nclustering-based hard negative sampling method, dedicated for supervised\ncontrastive speaker representation learning. Our approach clusters embeddings\nof similar speakers, and adjusts batch composition to obtain an optimal ratio\nof hard and easy negatives during contrastive loss calculation. Experimental\nevaluation shows that CHNS outperforms a baseline supervised contrastive\napproach with and without loss-based hard negative sampling, as well as a\nstate-of-the-art classification-based approach to speaker verification by as\nmuch as 18 % relative EER and minDCF on the VoxCeleb dataset using two\nlightweight model architectures."}
{"id": "2507.17261", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17261", "abs": "https://arxiv.org/abs/2507.17261", "authors": ["Rui Ding", "Fuhui Zhou", "Yuhang Wu", "Qihui Wu", "Tony Q. S. Quek"], "title": "Joint Resource Optimization Over Licensed and Unlicensed Spectrum in Spectrum Sharing UAV Networks Against Jamming Attacks", "comment": null, "summary": "Unmanned aerial vehicle (UAV) communication is of crucial importance in\nrealizing heterogeneous practical wireless application scenarios. However, the\ndensely populated users and diverse services with high data rate demands has\ntriggered an increasing scarcity of UAV spectrum utilization. To tackle this\nproblem, it is promising to incorporate the underutilized unlicensed spectrum\nwith the licensed spectrum to boost network capacity. However, the openness of\nunlicensed spectrum makes UAVs susceptible to security threats from potential\njammers. Therefore, a spectrum sharing UAV network coexisting with licensed\ncellular network and unlicensed Wi-Fi network is considered with the\nanti-jamming technique in this paper. The sum rate maximization of the\nsecondary network is studied by jointly optimizing the transmit power,\nsubchannel allocation, and UAV trajectory. We first decompose the challenging\nnon-convex problem into two subproblems, 1) the joint power and subchannel\nallocation and 2) UAV trajectory design subproblems. A low-complexity iterative\nalgorithm is proposed in a alternating optimization manner over these two\nsubproblems to solve the formulated problem. Specifically, the Lagrange dual\ndecomposition is exploited to jointly optimize the transmit power and\nsubchannel allocation iteratively. Then, an efficient iterative algorithm\ncapitalizing on successive convex approximation is designed to get a suboptimal\nsolution for UAV trajectory. Simulation results demonstrate that our proposed\nalgorithm can significantly improve the sum transmission rate compared with the\nbenchmark schemes."}
{"id": "2507.16843", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.16843", "abs": "https://arxiv.org/abs/2507.16843", "authors": ["Zhongsheng Wang", "Sijie Wang", "Jia Wang", "Yung-I Liang", "Yuxi Zhang", "Jiamou Liu"], "title": "Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems", "comment": "Accepted by ICONIP 2024", "summary": "In the design of customer relationship management (CRM) systems, accurately\nidentifying customer types and offering personalized services are key to\nenhancing customer satisfaction and loyalty. However, this process faces the\nchallenge of discerning customer voices and intentions, and general pre-trained\nautomatic speech recognition (ASR) models make it difficult to effectively\naddress industry-specific speech recognition tasks. To address this issue, we\ninnovatively proposed a solution for fine-tuning industry-specific ASR models,\nwhich significantly improved the performance of the fine-tuned ASR models in\nindustry applications. Experimental results show that our method substantially\nimproves the crucial auxiliary role of the ASR model in industry CRM systems,\nand this approach has also been adopted in actual industrial applications."}
{"id": "2507.17735", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.17735", "abs": "https://arxiv.org/abs/2507.17735", "authors": ["Qibing Bai", "Sho Inoue", "Shuai Wang", "Zhongjie Jiang", "Yannan Wang", "Haizhou Li"], "title": "Accent Normalization Using Self-Supervised Discrete Tokens with Non-Parallel Data", "comment": "Accepted to INTERSPEECH 2025", "summary": "Accent normalization converts foreign-accented speech into native-like speech\nwhile preserving speaker identity. We propose a novel pipeline using\nself-supervised discrete tokens and non-parallel training data. The system\nextracts tokens from source speech, converts them through a dedicated model,\nand synthesizes the output using flow matching. Our method demonstrates\nsuperior performance over a frame-to-frame baseline in naturalness,\naccentedness reduction, and timbre preservation across multiple English\naccents. Through token-level phonetic analysis, we validate the effectiveness\nof our token-based approach. We also develop two duration preservation methods,\nsuitable for applications such as dubbing."}
{"id": "2507.17284", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17284", "abs": "https://arxiv.org/abs/2507.17284", "authors": ["Chaehyun Jung", "TaeJun Ha", "Hyeonuk Kim", "Jeonghun Park"], "title": "State Estimation with 1-Bit Observations and Imperfect Models: Bussgang Meets Kalman in Neural Networks", "comment": null, "summary": "State estimation from noisy observations is a fundamental problem in many\napplications of signal processing. Traditional methods, such as the extended\nKalman filter, work well under fully-known Gaussian models, while recent hybrid\ndeep learning frameworks, combining model-based and data-driven approaches, can\nalso handle partially known models and non-Gaussian noise. However, existing\nstudies commonly assume the absence of quantization distortion, which is\ninevitable, especially with non-ideal analog-to-digital converters. In this\nwork, we consider a state estimation problem with 1-bit quantization. 1-bit\nquantization causes significant quantization distortion and severe information\nloss, rendering conventional state estimation strategies unsuitable. To address\nthis, inspired by the Bussgang decomposition technique, we first develop the\nBussgang-aided Kalman filter by assuming perfectly known models. The proposed\nmethod suitably captures quantization distortion into the state estimation\nprocess. In addition, we propose a computationally efficient variant, referred\nto as the reduced Bussgang-aided Kalman filter and, building upon it, introduce\na deep learning-based approach for handling partially known models, termed the\nBussgang-aided KalmanNet. In particular, the Bussgang-aided KalmanNet jointly\nuses a dithering technique and a gated recurrent unit (GRU) architecture to\neffectively mitigate the effects of 1-bit quantization and model mismatch.\nThrough simulations on the Lorenz-Attractor model and the Michigan NCLT\ndataset, we demonstrate that our proposed methods achieve accurate state\nestimation performance even under highly nonlinear, mismatched models and 1-bit\nobservations."}
{"id": "2507.17297", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17297", "abs": "https://arxiv.org/abs/2507.17297", "authors": ["Tobias Morocutti", "Jonathan Greif", "Paul Primus", "Florian Schmid", "Gerhard Widmer"], "title": "On Temporal Guidance and Iterative Refinement in Audio Source Separation", "comment": null, "summary": "Spatial semantic segmentation of sound scenes (S5) involves the accurate\nidentification of active sound classes and the precise separation of their\nsources from complex acoustic mixtures. Conventional systems rely on a\ntwo-stage pipeline - audio tagging followed by label-conditioned source\nseparation - but are often constrained by the absence of fine-grained temporal\ninformation critical for effective separation. In this work, we address this\nlimitation by introducing a novel approach for S5 that enhances the synergy\nbetween the event detection and source separation stages. Our key contributions\nare threefold. First, we fine-tune a pre-trained Transformer to detect active\nsound classes. Second, we utilize a separate instance of this fine-tuned\nTransformer to perform sound event detection (SED), providing the separation\nmodule with detailed, time-varying guidance. Third, we implement an iterative\nrefinement mechanism that progressively enhances separation quality by\nrecursively reusing the separator's output from previous iterations. These\nadvancements lead to significant improvements in both audio tagging and source\nseparation performance, as demonstrated by our system's second-place finish in\nTask 4 of the DCASE Challenge 2025. Our implementation and model checkpoints\nare available in our GitHub repository: https://github.com/theMoro/dcase25task4 ."}
{"id": "2507.17292", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17292", "abs": "https://arxiv.org/abs/2507.17292", "authors": ["Yu Zhang", "Qin Yi", "Leila Musavian", "Tongyang Xu", "Zilong Liu"], "title": "Non-Orthogonal AFDM: A Promising Spectrum-Efficient Waveform for 6G High-Mobility Communications", "comment": "This work has been accepted by IEEE PIMRC 2025", "summary": "This paper proposes a spectrum-efficient nonorthogonal affine frequency\ndivision multiplexing (AFDM) waveform for reliable high-mobility communications\nin the upcoming sixth-generation (6G) mobile systems. Our core idea is to\nintroduce a compression factor to enable controllable subcarrier overlapping in\nchirp-based AFDM modulation. To mitigate intercarrier interference (ICI), we\nintroduce linear precoding at the transmitter and an iterative detection scheme\nat the receiver. Simulation results demonstrate that these techniques can\neffectively reduce interference and maintain robust bit error rate (BER)\nperformance even under aggressive compression factors and high-mobility channel\nconditions. The proposed non-orthogonal AFDM waveform offers a promising\nsolution for next-generation wireless networks, balancing spectrum efficiency\nand Doppler resilience in highly dynamic environments."}
{"id": "2507.17563", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17563", "abs": "https://arxiv.org/abs/2507.17563", "authors": ["Qing Wang", "Zehan Li", "Hang Lv", "Hongjie Chen", "Yaodong Song", "Jian Kang", "Jie Lian", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "BoSS: Beyond-Semantic Speech", "comment": null, "summary": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication."}
{"id": "2507.17352", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17352", "abs": "https://arxiv.org/abs/2507.17352", "authors": ["Chunmei Xu", "Siqi Zhang", "Yi Ma", "Rahim Tafazolli"], "title": "LightCom: A Generative AI-Augmented Framework for QoE-Oriented Communications", "comment": null, "summary": "Data-intensive and immersive applications, such as virtual reality, impose\nstringent quality of experience (QoE) requirements that challenge traditional\nquality of service (QoS)-driven communication systems. This paper presents\nLightCom, a lightweight encoding and generative AI (GenAI)-augmented decoding\nframework, designed for QoE-oriented communications under low signal-to-noise\nratio (SNR) conditions. LightCom simplifies transmitter design by applying\nbasic low-pass filtering for source coding and minimal channel coding,\nsignificantly reducing processing complexity and energy consumption. At the\nreceiver, GenAI models reconstruct high-fidelity content from highly compressed\nand degraded signals by leveraging generative priors to infer semantic and\nstructural information beyond traditional decoding capabilities. The key design\nprinciples are analyzed, along with the sufficiency and error-resilience of the\nsource representation. We also develop importance-aware power allocation\nstrategies to enhance QoE and extend perceived coverage. Simulation results\ndemonstrate that LightCom achieves up to a $14$ dB improvement in robustness\nand a $9$ dB gain in perceived coverage, outperforming traditional QoS-driven\nsystems relying on sophisticated source and channel coding. This paradigm shift\nmoves communication systems towards human-centric QoE metrics rather than\nbit-level fidelity, paving the way for more efficient and resilient wireless\nnetworks."}
{"id": "2507.17682", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17682", "abs": "https://arxiv.org/abs/2507.17682", "authors": ["Daiqi Liu", "Tomás Arias-Vergara", "Jana Hutter", "Andreas Maier", "Paula Andrea Pérez-Toro"], "title": "Audio-Vision Contrastive Learning for Phonological Class Recognition", "comment": "conference to TSD 2025", "summary": "Accurate classification of articulatory-phonological features plays a vital\nrole in understanding human speech production and developing robust speech\ntechnologies, particularly in clinical contexts where targeted phonemic\nanalysis and therapy can improve disease diagnosis accuracy and personalized\nrehabilitation. In this work, we propose a multimodal deep learning framework\nthat combines real-time magnetic resonance imaging (rtMRI) and speech signals\nto classify three key articulatory dimensions: manner of articulation, place of\narticulation, and voicing. We perform classification on 15 phonological classes\nderived from the aforementioned articulatory dimensions and evaluate the system\nwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,\nmultimodal middle fusion, and contrastive learning-based audio-vision fusion.\nExperimental results on the USC-TIMIT dataset show that our contrastive\nlearning-based approach achieves state-of-the-art performance, with an average\nF1-score of 0.81, representing an absolute increase of 0.23 over the unimodal\nbaseline. The results confirm the effectiveness of contrastive representation\nlearning for multimodal articulatory analysis. Our code and processed dataset\nwill be made publicly available at\nhttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research."}
{"id": "2507.17393", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17393", "abs": "https://arxiv.org/abs/2507.17393", "authors": ["Omar Osman", "Abdullah Qayyum", "Maziar Nekovee"], "title": "Partially Reflected Surface (PRS)-Loaded Graphene-Based Patch Antenna for 6G", "comment": null, "summary": "This work investigates a slotted patch antenna integrated with a partially\nreflected surface (PRS) to operate in the TeraHertz (THz) frequency range for\n6G. The antenna is based on graphene material, on a Rogers RT Duroid 6010\nsubstrate. The proposed antenna achieves a bandwidth of 70 GHz (750 GHz to 820\nGHz). The PRS sheet consists of 5x4 unit cells, which are optimised to enhance\nthe overall realized gain of the antenna. The overall realized gain has\nincreased by 1.07 dBi. Also, the PRS enhanced the antenna radiation pattern,\nshowing stable properties over the operating bandwidth. The improved antenna\nperformance is validated via simulations."}
{"id": "2507.17396", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17396", "abs": "https://arxiv.org/abs/2507.17396", "authors": ["Junlang Huang", "Hao Chen", "Zhong Guan"], "title": "Learning from Scratch: Structurally-masked Transformer for Next Generation Lib-free Simulation", "comment": null, "summary": "This paper proposes a neural framework for power and timing prediction of\nmulti-stage data path, distinguishing itself from traditional lib-based\nanalytical methods dependent on driver characterization and load\nsimplifications. To the best of our knowledge, this is the first\nlanguage-based, netlist-aware neural network designed explicitly for standard\ncells. Our approach employs two pre-trained neural models of waveform\nprediction and delay estimation that directly infer transient waveforms and\npropagation delays from SPICE netlists, conditioned on critical physical\nparameters such as load capacitance, input slew, and gate size. This method\naccurately captures both intrinsic and coupling-induced delay effects without\nrequiring simplification or interpolation. For multi-stage timing prediction,\nwe implement a recursive propagation strategy where predicted waveforms from\neach stage feed into subsequent stages, cumulatively capturing delays across\nthe logic chain. This approach ensures precise timing alignment and complete\nwaveform visibility throughout complex signal pathways. The waveform prediction\nutilizes a hybrid CNN-Transformer architecture with netlist-aware node-level\nencoding, addressing traditional Transformers' fixed input dimensionality\nconstraints. Additionally, specialized subnetworks separately handle primary\ndelay estimation and crosstalk correction. Experimental results demonstrate\nSPICE-level accuracy, consistently achieving RMSE below 0.0098 across diverse\nindustrial circuits. The proposed framework provides a scalable, structurally\nadaptable neural alternative to conventional power and timing engines,\ndemonstrating high fidelity to physical circuit behaviors."}
{"id": "2507.17419", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17419", "abs": "https://arxiv.org/abs/2507.17419", "authors": ["Abdullah Qayyum", "Maziar Nekovee"], "title": "Power Allocation and RIS Elements Optimisation for Reconfigurable Intelligent Surfaces assisted RSMA", "comment": null, "summary": "This paper proposes power allocation and the number of reconfigurable\nintelligent surfaces (RIS) elements optimisation in a RIS-assisted rate\nsplitting multiple access (RSMA) system. The optimised RIS-RSMA (ORIS-RSMA)\nmethod determines the optimal number of RIS elements and the power allocation\nfactors for both common and private parts of a message. Additionally, it\nmaximises the sum rate while ensuring that a target common rate is satisfied.\nThe performance of the proposed ORIS-RSMA is compared to that of the\nconventional RIS-RSMA and RSMA. Simulation results show that ORIS-RSMA achieves\na higher sum rate."}
{"id": "2507.17441", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17441", "abs": "https://arxiv.org/abs/2507.17441", "authors": ["Zinat Behdad", "Ozlem Tugfe Demir", "Ki Won Sung", "Cicek Cavdar"], "title": "Detecting Multiple Targets with Distributed Sensing and Communication in Cell-Free Massive MIMO", "comment": "6 pages", "summary": "This paper investigates multi-target detection in an integrated sensing and\ncommunication (ISAC) system within a cell-free massive MIMO (CF-mMIMO)\nframework. We adopt a user-centric approach for communication user equipments\n(UEs) and a distributed sensing approach for multi-target detection. A\nheuristic access point (AP) mode selection algorithm and a channel-aware\ndistributed sensing scheme are proposed, where local measurements at receive\nAPs (RX-APs) are weighted based on the received signals signal-to-interference\nratio (SIR). A maximum a posteriori ratio test (MAPRT) detector is applied\nunder two awareness levels at RX-APs. To balance the communication-sensing\ntrade-off, we develop a power allocation algorithm to jointly maximize the\nminimum detection probability and communication\nsignal-to-interference-plus-noise ratio (SINR) while satisfying power\nconstraints. The proposed scheme outperforms non-weighted methods. Adding test\nstatistics from more RX-APs can degrade sensing performance due to weaker\nchannels, but this effect can be mitigated by optimizing the weighting\nexponent. Additionally, assigning more sensing RX-APs to a sensing area results\nin approximately 10 dB loss in minimum communication SINR due to limited\ncommunication resources."}
{"id": "2507.17505", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.17505", "abs": "https://arxiv.org/abs/2507.17505", "authors": ["José P. González-Coma", "F. Javier López-Martínez"], "title": "Slow Fluid Antenna Multiple Access with Multiport Receivers", "comment": "5 pages, 3 figures. This work has been submitted to the IEEE for\n  publication", "summary": "We investigate whether equipping fluid-antenna (FA) receivers with multiple\n($L>1$) radiofrequency (RF) chains can improve the performance of the slow\nfluid-antenna multiple access (FAMA) technique, which enables open-loop\nconnectivity with channel state information (CSI) available only at the\nreceiver side. We analyze the case of slow-FAMA users equipped with multiport\nreceivers, so that $L$ ports of the FA are selected and combined to reduce\ninterference. We show that a joint design of the port selection matrix and the\ncombining vector at each receiver yields significant performance gains over\nreference schemes, demonstrating the potential of multiport reception in FA\nsystems with a limited number of RF chains."}
{"id": "2507.17506", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17506", "abs": "https://arxiv.org/abs/2507.17506", "authors": ["Imad Bouhou", "Stefano Fortunati", "Leila Gharsalli", "Alexandre Renaux"], "title": "Joint Multi-Target Detection-Tracking in Cognitive Massive MIMO Radar via POMCP", "comment": null, "summary": "This correspondence presents a power-aware cognitive radar framework for\njoint detection and tracking of multiple targets in a massive multiple-input\nmultiple-output (MIMO) radar environment. Building on a previous single-target\nalgorithm based on Partially Observable Monte Carlo Planning (POMCP), we extend\nit to the multi-target case by assigning each target an independent POMCP tree,\nenabling scalable and efficient planning.\n  Departing from uniform power allocation-which is often suboptimal with\nvarying signal-to-noise ratios (SNRs)-our approach predicts each target's\nfuture angular position and expected received power, based on its estimated\nrange and radar cross-section (RCS). These predictions guide adaptive waveform\ndesign via a constrained optimization problem that allocates transmit energy to\nenhance the detectability of weaker or distant targets, while ensuring\nsufficient power for high-SNR targets. The reward function in the underlying\npartially observable Markov decision process (POMDP) is also modified to\nprioritize accurate spatial and power estimation.\n  Simulations involving multiple targets with different SNRs confirm the\neffectiveness of our method. The proposed framework for the cognitive radar\nimproves detection probability for low-SNR targets and achieves more accurate\ntracking compared to approaches using uniform or orthogonal waveforms. These\nresults demonstrate the potential of the POMCP-based framework for adaptive,\nefficient multi-target radar systems."}
{"id": "2507.17623", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17623", "abs": "https://arxiv.org/abs/2507.17623", "authors": ["Guangteng Liu", "Xiayue Liu", "Zhixiang Xu", "Yufeng Yuan", "Hui Zhao", "Yuxuan Liu", "Yufei Jiang"], "title": "SA-WiSense: A Blind-Spot-Free Respiration Sensing Framework for Single-Antenna Wi-Fi Devices", "comment": "12pages, 10figures", "summary": "Wi-Fi sensing offers a promising technique for contactless human respiration\nmonitoring. A key challenge, however, is the blind spot problem caused by\nrandom phase offsets that corrupt the complementarity of respiratory signals.\nTo address the challenge, we propose a single-antenna-Wi-Fi-sensing\n(SA-WiSense) framework to improve accuracy of human respiration monitoring,\nrobust against random phase offsets. The proposed SA-WiSense framework is\ncost-efficient, as only a single antenna is used rather than multiple antennas\nas in the previous works. Therefore, the proposed framework is applicable to\nInternet of Thing (IoT), where most of sensors are equipped with a single\nantenna. On one hand, we propose a cross-subcarrier channel state information\n(CSI) ratio (CSCR) based blind spot mitigation approach for IoT, where the\nratios of two values of CSI between subcarriers are leveraged to mitigate\nrandom phase offsets. We prove that the random phase offsets can be cancelled\nby the proposed CSCR approach, thereby restoring the inherent complementarity\nof signals for blind-spot-free sensing. On the other hand, we propose a genetic\nalgorithm (GA) based subcarrier selection (GASS) approach by formulating an\noptimization problem in terms of the sensing-signal-to-noise ratio (SSNR) of\nCSCR between subcarriers. GA is utilized to solve the formulated optimization\nproblem. We use commodity ESP32 microcontrollers to build an experiment test.\nThe proposed works are validated to achieve an detection rate of 91.2% for\nrespiration monitoring at distances up to 8.0 meters, substantially more\naccurate than the state-of-the-art methods with a single antenna."}
{"id": "2507.17645", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17645", "abs": "https://arxiv.org/abs/2507.17645", "authors": ["Alessio Lukaj", "Keigo Masuoka", "Takumi Takahashi", "Giuseppe Thadeu Freitas de Abreu", "Hideki Ochiai"], "title": "Quaternion-Domain Super MDS for Robust 3D Localization", "comment": "12 pages, 8 figures", "summary": "This paper proposes a novel low-complexity three-dimensional (3D)\nlocalization algorithm for wireless sensor networks, termed quanternion-domain\nsuper multi-dimensional scaling (QD-SMDS). The algorithm is based on a\nreformulation of the SMDS, originally developed in the real domain, using\nquaternion algebra. By representing 3D coordinates as quaternions, the method\nconstructs a rank-1 Gram edge kernel (GEK) matrix that integrates both relative\ndistance and angular information between nodes, which enhances the noise\nreduction effect achieved through low-rank truncation employing singular value\ndecomposition (SVD), thereby improving robustness against information loss. To\nfurther reduce computational complexity, we also propose a variant of QD-SMDS\nthat eliminates the need for the computationally expensive SVD by leveraging\nthe inherent structure of the quaternion-domain GEK matrix. This alternative\ndirectly estimates node coordinates using only matrix multiplications within\nthe quaternion domain. Simulation results demonstrate that the proposed method\nsignificantly improves localization accuracy compared to the original SMDS\nalgorithm, especially in scenarios with substantial measurement errors. The\nproposed method also achieves comparable localization accuracy without\nrequiring SVD."}

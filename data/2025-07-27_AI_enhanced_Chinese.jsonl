{"id": "2507.17966", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17966", "abs": "https://arxiv.org/abs/2507.17966", "authors": ["Mohsen Bayat", "Sanoopkumar P. S.", "Arman Farhang"], "title": "Time and Frequency Synchronization for Multiuser OTFS in Uplink", "comment": null, "summary": "In this paper, we propose time and frequency synchronization techniques for\nuplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work\nfocuses on accurately estimating and correcting timing offsets (TOs) and\ncarrier frequency offsets (CFOs). Specifically, TO estimation is essential for\nlocating users' pilots on the delay-time plane, while CFO estimation enhances\nchannel estimation accuracy. First, we propose a TO estimation technique for an\nexisting multiuser pilot structure in MU-OTFS. We replace the impulse pilot\n(IMP) in this pilot structure with a more practical pilot with a cyclic prefix\n(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs\ndifferent Zadoff-Chu (ZC) sequences, which enables pilot separation via\ncorrelation at the receiver side. Consequently, we introduce a\ncorrelation-based TO estimation technique for uplink MU-OTFS using this pilot\nstructure. Next, a spectrally efficient and practical pilot pattern is\nproposed, where each user transmits a PCP within a shared pilot region on the\ndelay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO\nestimation technique utilizes a bank of filters to separate different users'\nsignals and accurately estimate their TOs. Then, we derive a mathematical\nthreshold range to enhance TO estimation accuracy by finding the first major\npeak in the correlation function rather than relying solely on the highest\npeak. After locating the received users' pilot signals using one of the\nproposed TO estimation techniques, our proposed CFO estimation technique\nreduces the multi-dimensional maximum likelihood (ML) search problem into\nmultiple one-dimensional search problems. In this technique, we apply the\nChebyshev polynomials of the first kind basis expansion model (CPF-BEM) to\neffectively handle the time-variations of the channel in obtaining the CFO\nestimates for all the users.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u4e0a\u884c\u591a\u7528\u6237OTFS\u7cfb\u7edf\u7684\u65f6\u95f4\u548c\u9891\u7387\u540c\u6b65\u6280\u672f\uff0c\u5305\u62ec\u5b9a\u65f6\u504f\u79fb\uff08TO\uff09\u548c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\uff08CFO\uff09\u7684\u4f30\u8ba1\u4e0e\u6821\u6b63\u65b9\u6cd5\u3002", "motivation": "\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u4f30\u8ba1\u548c\u6821\u6b63TO\u548cCFO\u5bf9\u591a\u7528\u6237OTFS\u7cfb\u7edf\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662fTO\u4f30\u8ba1\u7528\u4e8e\u5b9a\u4f4d\u7528\u6237\u5bfc\u9891\uff0cCFO\u4f30\u8ba1\u7528\u4e8e\u63d0\u9ad8\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8eSU-PCP\u5bfc\u9891\u7ed3\u6784\u7684TO\u4f30\u8ba1\u6280\u672f\uff1b2. \u63d0\u51faMU-PCP\u5bfc\u9891\u6a21\u5f0f\uff0c\u5229\u7528\u6ee4\u6ce2\u5668\u7ec4\u5206\u79bb\u7528\u6237\u4fe1\u53f7\u5e76\u4f30\u8ba1TO\uff1b3. \u63d0\u51fa\u57fa\u4e8eChebyshev\u591a\u9879\u5f0f\u7684CFO\u4f30\u8ba1\u6280\u672f\uff0c\u5c06\u591a\u7ef4ML\u641c\u7d22\u95ee\u9898\u7b80\u5316\u4e3a\u591a\u4e2a\u4e00\u7ef4\u641c\u7d22\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6539\u8fdb\u5bfc\u9891\u7ed3\u6784\u548c\u4f30\u8ba1\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684TO\u548cCFO\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u540c\u6b65\u6280\u672f\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u591a\u7528\u6237OTFS\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17982", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17982", "abs": "https://arxiv.org/abs/2507.17982", "authors": ["Pablo Ram\u00edrez-Espinosa", "Cleof\u00e1s Segura-G\u00f3mez", "\u00c1ngel Palomares-Caballero", "F. Javier L\u00f3pez-Mart\u00ednez", "David Morales-Jim\u00e9nez"], "title": "Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model", "comment": null, "summary": "Fluid antenna systems (FASs) have become a popular topic in the wireless\ncommunity as an effective yet simple means of exploiting spatial diversity. Due\nto the limitations of physically moving radiating elements, electronically\nreconfigurable antennas are emerging as practical implementations of FASs,\nsince changing the radiation pattern is functionally equivalent to physically\nmoving the device. However, electronically reconfigurable antennas pose a\nchallenge in terms of analytical modeling, often requiring full-wave\nsimulations or measurements for their characterization; this severely limits\nthe extraction of theoretical insights useful for system design. Motivated by\nthese difficulties and the growing interest in FASs, we propose in this paper a\ncomplete analytical model for metasurface-based embodiments of FASs.\nSpecifically, we advocate for the implementation of the FAS concept through\ndynamic metasurface antennas (DMAs), hitherto proposed as array replacements in\nmultiple-input multiple-output (MIMO) systems. We leverage circuit theory to\nrewrite the conventional signal model of FASs in terms of admittance matrices\naccounting for the electromagnetic effects inherent to metasurfaces. The model\nis validated with full-wave simulations, showing good agreement. We further\nillustrate how to apply the model for standard performance analysis, and\nprovide closed-form expressions for key metrics, including the resulting signal\ncovariance matrix. Results confirm that practical DMA-based FASs can achieve\nsimilar performance to that of idealized implementations of position-flexible\nantennas.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u8868\u9762\u7684\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u7684\u5b8c\u6574\u89e3\u6790\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8d85\u8868\u9762\u5929\u7ebf\uff08DMA\uff09\u5b9e\u73b0\uff0c\u5229\u7528\u7535\u8def\u7406\u8bba\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5168\u6ce2\u4eff\u771f\u9a8c\u8bc1\u3002", "motivation": "\u7531\u4e8e\u7535\u5b50\u53ef\u91cd\u6784\u5929\u7ebf\u5728\u5206\u6790\u5efa\u6a21\u4e0a\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u65e5\u76ca\u589e\u957f\u7684\u5173\u6ce8\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u6790\u6a21\u578b\u3002", "method": "\u91c7\u7528\u52a8\u6001\u8d85\u8868\u9762\u5929\u7ebf\uff08DMA\uff09\u5b9e\u73b0FAS\uff0c\u5229\u7528\u7535\u8def\u7406\u8bba\u91cd\u65b0\u6784\u5efa\u4fe1\u53f7\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5168\u6ce2\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u9a8c\u8bc1\u663e\u793a\u4e0e\u4eff\u771f\u7ed3\u679c\u4e00\u81f4\uff0c\u4e14DMA\u5b9e\u73b0\u7684FAS\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u5316\u7684\u4f4d\u7f6e\u7075\u6d3b\u5929\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u6790\u6a21\u578b\u4e3aFAS\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86DMA\u5b9e\u73b0\u7684FAS\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.18035", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18035", "abs": "https://arxiv.org/abs/2507.18035", "authors": ["Hyeonho Noh", "Hyeonsu Lyu", "Hyun Jong Yang"], "title": "Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming", "comment": null, "summary": "This paper explores an integrated sensing and communication (ISAC) network\nempowered by multiple active simultaneously transmitting and reflecting\nreconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes\ndownlink communication to multiple users while concurrently interrogating a\nsensing target. We jointly optimize the BS transmit beamformer and the\nreflection/transmission coefficients of every active STAR-RIS in order to\nmaximize the aggregate communication sum-rate, subject to (i) a stringent\nsensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an\nupper bound on the leakage of confidential information, and (iii) individual\nhardware and total power constraints at both the BS and the STAR-RISs. The\nresulting highly non-convex program is tackled with an efficient alternating\noptimization (AO) framework. First, the original formulation is reformulated\ninto an equivalent yet more tractable representation and partitioned into\nsubproblems. The BS beamformer is updated in closed form via the\nKarush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and\ntransmission vectors are refined through successive convex approximation (SCA),\nyielding a semidefinite program that is then solved via semidefinite\nrelaxation. Comprehensive simulations demonstrate that the proposed algorithm\ndelivers substantial sum-rate gains over passive-RIS and single STAR-RIS\nbaselines, all the while rigorously meeting the prescribed sensing and security\nconstraints.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u7531\u591a\u4e2a\u4e3b\u52a8\u540c\u65f6\u4f20\u8f93\u548c\u53cd\u5c04\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08STAR-RIS\uff09\u652f\u6301\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7f51\u7edc\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cSTAR-RIS\u53cd\u5c04/\u4f20\u8f93\u7cfb\u6570\uff0c\u6700\u5927\u5316\u901a\u4fe1\u603b\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u3001\u5b89\u5168\u548c\u529f\u7387\u7ea6\u675f\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528STAR-RIS\u6280\u672f\u63d0\u5347ISAC\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684\u611f\u77e5\u3001\u5b89\u5168\u548c\u786c\u4ef6\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u5206\u522b\u901a\u8fc7KKT\u6761\u4ef6\u548cSCA\u65b9\u6cd5\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cSTAR-RIS\u53c2\u6570\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u901a\u4fe1\u603b\u901f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u88ab\u52a8RIS\u548c\u5355STAR-RIS\u57fa\u7ebf\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u548c\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u4e3b\u52a8STAR-RIS\u5728ISAC\u7f51\u7edc\u4e2d\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\uff0c\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2507.18096", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18096", "abs": "https://arxiv.org/abs/2507.18096", "authors": ["Jihong Huang", "Rong Yang", "Wei Gao", "Xingqun Zhan", "Zheng Yao"], "title": "Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation", "comment": null, "summary": "Direct Position Estimation (DPE) is a method that directly estimate position,\nvelocity, and time (PVT) information from cross ambiguity function (CAF) of the\nGNSS signals, significantly enhancing receiver robustness in urban\nenvironments. However, there is still a lack of theoretical characterization on\nmultipath errors in the context of DPE theory. Geometric observations highlight\nthe unique characteristics of DPE errors stemming from multipath and thermal\nnoise as estimation bias and variance respectively. Expanding upon the\ntheoretical framework of DPE noise variance through geometric analysis, this\npaper focuses on a geometric representation of multipath errors by quantifying\nthe deviations in CAF and PVT solutions caused by off-centering bias relative\nto the azimuth and elevation angles. A satellite circular multipath bias (SCMB)\nmodel is introduced, amalgamating CAF and PVT errors from multiple satellite\nchannels. The boundaries for maximum or minimum PVT bias are established\nthrough discussions encompassing various multipath conditions. The correctness\nof the multipath geometrical portrait is confirmed through both Monte Carlo\nsimulations and urban canyon tests. The findings indicate that the maximum PVT\nbias depends on the largest multipath errors observed across various satellite\nchannels. Additionally, the PVT bias increases with satellite elevation angles,\ninfluenced by the CAF multipath bias projection. This serves as a reference for\nselecting DPE satellites from a geometric standpoint, underscoring the\nimportance of choosing a balanced combination of high and low elevation angles\nto achieve an optimal satellite geometry configuration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u91cf\u5316\u4e86\u591a\u5f84\u8bef\u5dee\u5bf9\u76f4\u63a5\u4f4d\u7f6e\u4f30\u8ba1\uff08DPE\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u5efa\u7acb\u4e86\u536b\u661f\u5706\u5f62\u591a\u5f84\u504f\u7f6e\uff08SCMB\uff09\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6b63\u786e\u6027\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9DPE\u7406\u8bba\u4e2d\u591a\u5f84\u8bef\u5dee\u7684\u7406\u8bba\u63cf\u8ff0\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u91cf\u5316\u591a\u5f84\u8bef\u5dee\u5bf9CAF\u548cPVT\u89e3\u7684\u5f71\u54cd\uff0c\u63d0\u51faSCMB\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u57ce\u5e02\u5ce1\u8c37\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "\u6700\u5927PVT\u504f\u7f6e\u53d6\u51b3\u4e8e\u591a\u5f84\u8bef\u5dee\u7684\u6700\u5927\u503c\uff0c\u4e14PVT\u504f\u7f6e\u968f\u536b\u661f\u4ef0\u89d2\u589e\u52a0\u800c\u589e\u5927\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ece\u51e0\u4f55\u89d2\u5ea6\u9009\u62e9DPE\u536b\u661f\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u9ad8\u4f4e\u4ef0\u89d2\u536b\u661f\u7ec4\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.17765", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17765", "abs": "https://arxiv.org/abs/2507.17765", "authors": ["Arindam Ghosh", "Mark Fuhs", "Bongjun Kim", "Anurag Chowdhury", "Monika Woszczyna"], "title": "ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding", "comment": "Interspeech 2025 Submission", "summary": "From an application standpoint, speaker-role diarization (RD), such as doctor\nvs. patient, host vs. guest, etc. is often more useful than traditional speaker\ndiarization (SD), which assigns generic labels like speaker-1, speaker-2 etc.\nIn the context of joint automatic speech recognition (ASR) + SD (who spoke\nwhat?), recent end-to-end models employ an auxiliary SD transducer,\nsynchronized with the ASR transducer, to predict speakers per word. In this\npaper, we extend this framework to RD with three key contributions: (1) we\nsimplify the training via forced alignment and cross-entropy loss instead of\nRNNT loss, (2) we show that word prediction and role prediction require\ndifferent amounts of predictor's context, leading to separate task-specific\npredictors, unlike existing shared-predictor models, and (3) we propose a way\nto leverage RD posterior activity to influence ASR decoding and reduce\nsmall-word deletion errors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u8bf4\u8bdd\u8005\u89d2\u8272\u5206\u7c7b\uff08RD\uff09\uff0c\u901a\u8fc7\u7b80\u5316\u8bad\u7ec3\u3001\u5206\u79bb\u4efb\u52a1\u7279\u5b9a\u9884\u6d4b\u5668\u4ee5\u53ca\u5229\u7528RD\u540e\u9a8c\u6d3b\u52a8\u4f18\u5316ASR\u89e3\u7801\u3002", "motivation": "\u4f20\u7edf\u8bf4\u8bdd\u8005\u5206\u7c7b\uff08SD\uff09\u4ec5\u63d0\u4f9b\u901a\u7528\u6807\u7b7e\uff08\u5982\u8bf4\u8bdd\u80051\u3001\u8bf4\u8bdd\u80052\uff09\uff0c\u800c\u89d2\u8272\u5206\u7c7b\uff08\u5982\u533b\u751f\u4e0e\u60a3\u8005\u3001\u4e3b\u6301\u4eba\u4e0e\u5609\u5bbe\uff09\u66f4\u5177\u5e94\u7528\u4ef7\u503c\u3002", "method": "1. \u4f7f\u7528\u5f3a\u5236\u5bf9\u9f50\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u7b80\u5316\u8bad\u7ec3\uff1b2. \u4e3a\u8bcd\u9884\u6d4b\u548c\u89d2\u8272\u9884\u6d4b\u8bbe\u8ba1\u72ec\u7acb\u7684\u9884\u6d4b\u5668\uff1b3. \u5229\u7528RD\u540e\u9a8c\u6d3b\u52a8\u4f18\u5316ASR\u89e3\u7801\u3002", "result": "\u6a21\u578b\u5728\u89d2\u8272\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u51cf\u5c11\u4e86ASR\u89e3\u7801\u4e2d\u7684\u5c0f\u8bcd\u5220\u9664\u9519\u8bef\u3002", "conclusion": "\u5206\u79bb\u4efb\u52a1\u9884\u6d4b\u5668\u548c\u5229\u7528RD\u540e\u9a8c\u6d3b\u52a8\u80fd\u663e\u8457\u63d0\u5347\u8054\u5408ASR+RD\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.17851", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17851", "abs": "https://arxiv.org/abs/2507.17851", "authors": ["Xiaoxu Zhu", "Junhua Li"], "title": "Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability", "comment": "20 pages, 9 figures, 2 tables", "summary": "Speech pretrained models contain task-specific information across different\nlayers, but decoupling content and timbre information remains challenging as\nremoving speaker-specific information often causes content loss. Current\nresearch lacks direct metrics to quantify timbre residual in model encodings,\nrelying on indirect evaluation through downstream tasks. This paper addresses\nthese challenges through interpretability-based speaker disentanglement in\nspeech pretraining models. We quantitatively evaluate timbre residual in model\nembeddings and improve speaker disentanglement using interpretive\nrepresentations. Our contributions include: (1) InterpTRQE-SptME Benchmark - a\ntimbre residual recognition framework using interpretability. The benchmark\nconcatenates content embeddings with timbre embeddings for speaker\nclassification, then applies Gradient SHAP Explainer to quantify timbre\nresidual. We evaluate seven speech pretraining model variations. (2)\nInterpTF-SptME method - an interpretability-based timbre filtering approach\nusing SHAP Noise and SHAP Cropping techniques. This model-agnostic method\ntransforms intermediate encodings to remove timbre while preserving content.\nExperiments on VCTK dataset with HuBERT LARGE demonstrate successful content\npreservation and significant speaker disentanglement optimization. Results show\nthe SHAP Noise method can reduce timbre residual from 18.05% to near 0% while\nmaintaining content integrity, contributing to enhanced performance in\ncontent-related speech processing tasks and preventing timbre privacy leakage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u8bed\u97f3\u9884\u8bad\u7ec3\u6a21\u578b\u97f3\u8272\u89e3\u8026\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u97f3\u8272\u6b8b\u7559\u548c\u6539\u8fdb\u89e3\u8026\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u4fdd\u7559\u548c\u97f3\u8272\u5206\u79bb\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u97f3\u8272\u4e0e\u5185\u5bb9\u4fe1\u606f\u96be\u4ee5\u89e3\u8026\u7684\u95ee\u9898\uff0c\u907f\u514d\u97f3\u8272\u4fe1\u606f\u6cc4\u9732\u5e76\u63d0\u5347\u5185\u5bb9\u5904\u7406\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faInterpTRQE-SptME\u57fa\u51c6\u548cInterpTF-SptME\u65b9\u6cd5\uff0c\u5229\u7528SHAP\u6280\u672f\u91cf\u5316\u97f3\u8272\u6b8b\u7559\u5e76\u8fc7\u6ee4\u97f3\u8272\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSHAP Noise\u65b9\u6cd5\u53ef\u5c06\u97f3\u8272\u6b8b\u7559\u4ece18.05%\u964d\u81f3\u63a5\u8fd10%\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u97f3\u8272\u89e3\u8026\uff0c\u63d0\u5347\u4e86\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u6027\u80fd\u5e76\u9632\u6b62\u97f3\u8272\u9690\u79c1\u6cc4\u9732\u3002"}}
{"id": "2507.18149", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18149", "abs": "https://arxiv.org/abs/2507.18149", "authors": ["Dongdong Zou", "Wei Wang", "Jiawen Yao", "Zhongxing Tian", "Zeyu Feng", "Huan Huang", "Fan Li", "Gordon Ning Liu", "Gangxiang Shen", "Yi Cai"], "title": "Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems", "comment": null, "summary": "Probabilistic shaping (PS) has attracted significant attention in\nintensity-modulation and direct-detection (IM-DD) systems. However, due to the\nunique system model and inherent constraints, the effective application of the\nPS technique is still an open question in IM-DD systems, particularly in\nsystems with memory effects. In this paper, a novel indirect PS scheme tailored\nfor peak power constrained (PPC) IM-DD systems is proposed. The key idea lies\nin strategically controlling the signal envelope to mitigate memory-induced\nimpairments, such as nonlinearity, overshoot, peak-to-average power ratio\nenhancement, etc. The proposed scheme incorporates a dynamic selective mapping\n(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol\nmapping in which the current symbol is not only determined by the current bits\npattern but also by previously generated symbols within a specified memory\nlength. At the receiver side, a turbo equalizer with a modified M-BCJR\nalgorithm is proposed to achieve the recovery of ambiguous bits induced by\nDSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the\nproposed scheme exhibits 1dB receiver sensitivity improvement over 2km\nsingle-mode fiber transmission. In addition, the proposed scheme has also been\ndemonstrated to be compatible with the typical probabilistic amplitude shaping\narchitecture, enabling a simple and fine-granularity rate adaptation\ncapability. To the best of our knowledge, this work opens a new sight for the\napplication of the PS technique in PPC IM-DD systems with memory effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5cf0\u503c\u529f\u7387\u53d7\u9650IM-DD\u7cfb\u7edf\u7684\u95f4\u63a5\u6982\u7387\u6574\u5f62\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6027\u6620\u5c04\u548c\u4fee\u6539\u7684M-BCJR\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eIM-DD\u7cfb\u7edf\u7684\u72ec\u7279\u6a21\u578b\u548c\u5185\u5b58\u6548\u5e94\uff0c\u6982\u7387\u6574\u5f62\u6280\u672f\u7684\u6709\u6548\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a8\u6001\u9009\u62e9\u6027\u6620\u5c04\uff08DSLM\uff09\u673a\u5236\u63a7\u5236\u4fe1\u53f7\u5305\u7edc\uff0c\u5e76\u7ed3\u5408\u4fee\u6539\u7684M-BCJR\u7b97\u6cd5\u7684turbo\u5747\u8861\u5668\u3002", "result": "\u572856GBaud PAM8\u7cfb\u7edf\u4e2d\uff0c\u63a5\u6536\u7075\u654f\u5ea6\u63d0\u5347\u4e861dB\uff0c\u4e14\u517c\u5bb9\u5178\u578b\u6982\u7387\u5e45\u5ea6\u6574\u5f62\u67b6\u6784\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u5185\u5b58\u6548\u5e94\u4e0b\u7684PPC IM-DD\u7cfb\u7edf\u5e94\u7528\u6982\u7387\u6574\u5f62\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.17799", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.17799", "abs": "https://arxiv.org/abs/2507.17799", "authors": ["Davide Ghia", "Gabriele Ciravegna", "Alkis Koudounas", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli"], "title": "A Concept-based approach to Voice Disorder Detection", "comment": null, "summary": "Voice disorders affect a significant portion of the population, and the\nability to diagnose them using automated, non-invasive techniques would\nrepresent a substantial advancement in healthcare, improving the quality of\nlife of patients. Recent studies have demonstrated that artificial intelligence\nmodels, particularly Deep Neural Networks (DNNs), can effectively address this\ntask. However, due to their complexity, the decision-making process of such\nmodels often remain opaque, limiting their trustworthiness in clinical\ncontexts. This paper investigates an alternative approach based on Explainable\nAI (XAI), a field that aims to improve the interpretability of DNNs by\nproviding different forms of explanations. Specifically, this works focuses on\nconcept-based models such as Concept Bottleneck Model (CBM) and Concept\nEmbedding Model (CEM) and how they can achieve performance comparable to\ntraditional deep learning methods, while offering a more transparent and\ninterpretable decision framework.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6539\u8fdb\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u8bed\u97f3\u969c\u788d\u8bca\u65ad\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u548c\u6982\u5ff5\u5d4c\u5165\u6a21\u578b\uff08CEM\uff09\u3002", "motivation": "\u8bed\u97f3\u969c\u788d\u5f71\u54cd\u5e7f\u6cdb\uff0c\u81ea\u52a8\u975e\u4fb5\u5165\u6027\u8bca\u65ad\u6280\u672f\u80fd\u663e\u8457\u6539\u5584\u533b\u7597\u8d28\u91cf\u548c\u60a3\u8005\u751f\u6d3b\u3002\u4f20\u7edfDNN\u867d\u6709\u6548\u4f46\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u4fe1\u4efb\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u548c\u6982\u5ff5\u5d4c\u5165\u6a21\u578b\uff08CEM\uff09\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u548c\u89e3\u91ca\u6027\u3002", "result": "\u6982\u5ff5\u6a21\u578b\uff08\u5982CBM\u548cCEM\uff09\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u900f\u660e\u7684\u51b3\u7b56\u6846\u67b6\u3002", "conclusion": "\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff08\u5982CBM\u548cCEM\uff09\u5728\u8bed\u97f3\u969c\u788d\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u53c8\u80fd\u589e\u5f3a\u4e34\u5e8a\u4fe1\u4efb\u3002"}}
{"id": "2507.17937", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17937", "abs": "https://arxiv.org/abs/2507.17937", "authors": ["Jaechul Roh", "Zachary Novack", "Yuefeng Peng", "Niloofar Mireshghallah", "Taylor Berg-Kirkpatrick", "Amir Houmansadr"], "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", "comment": null, "summary": "Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis\nfrom text, yet their vulnerability to training data memorization remains\nunderexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel\nattack where lyrics are semantically altered while preserving their acoustic\nstructure through homophonic substitutions (e.g., Eminem's famous \"mom's\nspaghetti\" $\\rightarrow$ \"Bob's confetti\"). Despite these distortions, we\nuncover a powerful form of sub-lexical memorization: models like SUNO and YuE\nregenerate outputs strikingly similar to known training content, achieving high\nsimilarity across audio-domain metrics, including CLAP, AudioJudge, and\nCoverID. This vulnerability persists across multiple languages and genres. More\nsurprisingly, we discover that phoneme-altered lyrics alone can trigger visual\nmemorization in text-to-video models. When prompted with phonetically modified\nlyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original\nmusic video -- including character appearance and scene composition -- despite\nno visual cues in the prompt. We term this phenomenon phonetic-to-visual\nregurgitation. Together, these findings expose a critical vulnerability in\ntranscript-conditioned multimodal generation: phonetic prompting alone can\nunlock memorized audiovisual content, raising urgent questions about copyright,\nsafety, and content provenance in modern generative systems. Example\ngenerations are available on our demo page (jrohsc.github.io/music_attack/).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6b4c\u8bcd\u5230\u6b4c\u66f2\u751f\u6210\u6a21\u578b\uff08LS2\uff09\u5bf9\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPT\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u97f3\u9891\u548c\u89c6\u89c9\u9886\u57df\u7684\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u6b4c\u8bcd\u5230\u6b4c\u66f2\u751f\u6210\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bed\u4e49\u88ab\u4fee\u6539\u4f46\u58f0\u5b66\u7ed3\u6784\u4fdd\u7559\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528APT\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u97f3\u66ff\u6362\u4fee\u6539\u6b4c\u8bcd\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u97f3\u9891\u548c\u89c6\u89c9\u9886\u57df\u7684\u8bb0\u5fc6\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u97f3\u9891\u9886\u57df\uff08\u5982SUNO\u548cYuE\uff09\u548c\u89c6\u89c9\u9886\u57df\uff08\u5982Veo 3\uff09\u5747\u8868\u73b0\u51fa\u5bf9\u8bad\u7ec3\u5185\u5bb9\u7684\u8bb0\u5fc6\uff0c\u89e6\u53d1\u76f8\u4f3c\u8f93\u51fa\u3002", "conclusion": "\u63ed\u793a\u4e86\u8f6c\u5f55\u6761\u4ef6\u591a\u6a21\u6001\u751f\u6210\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f15\u53d1\u5bf9\u7248\u6743\u3001\u5b89\u5168\u548c\u5185\u5bb9\u6765\u6e90\u7684\u62c5\u5fe7\u3002"}}
{"id": "2507.18166", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.18166", "abs": "https://arxiv.org/abs/2507.18166", "authors": ["Jonas Elmiger", "Gian Marti", "Christoph Studer"], "title": "GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing", "comment": null, "summary": "Modern positioning relies on radio signals from global navigation satellite\nsystems (GNSS). Their low receive power renders these radio signals susceptible\nto jamming attacks, in which malicious transmitters emit strong interference to\ndisrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,\nin which malicious transmitters mimic legitimate satellites by transmitting\nspurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna\nGNSS receivers that mitigates jammers as well as spoofers without requiring any\nprior knowledge of the receiver position or attack type: Jammers are mitigated\nduring signal acquisition using a recently developed adaptive spatial filtering\ntechnique. Spoofers are identified and rejected after signal acquisition using\na novel approach that tests the consistency of acquired signals by comparing\ntheir respective direction of arrival (DoA) and pseudorange estimates in a test\nthat is invariant with respect to the unknown receiver position. We demonstrate\nthe efficacy of our method using extensive simulations of a GPS L1 C/A system\nunder spoofing and jamming attacks.", "AI": {"tldr": "SCHIEBER\u662f\u4e00\u79cd\u591a\u5929\u7ebfGNSS\u63a5\u6536\u5668\u65b9\u6cd5\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u7f13\u89e3\u5e72\u6270\u548c\u6b3a\u9a97\u653b\u51fb\u3002", "motivation": "GNSS\u4fe1\u53f7\u6613\u53d7\u5e72\u6270\u548c\u6b3a\u9a97\u653b\u51fb\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\uff0cSCHIEBER\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u7a7a\u95f4\u6ee4\u6ce2\u6280\u672f\u7f13\u89e3\u5e72\u6270\uff0c\u901a\u8fc7\u6bd4\u8f83\u4fe1\u53f7\u5230\u8fbe\u65b9\u5411\u548c\u4f2a\u8ddd\u4f30\u8ba1\u8bc6\u522b\u6b3a\u9a97\u4fe1\u53f7\u3002", "result": "\u5728GPS L1 C/A\u7cfb\u7edf\u7684\u6a21\u62df\u4e2d\uff0cSCHIEBER\u6709\u6548\u7f13\u89e3\u4e86\u5e72\u6270\u548c\u6b3a\u9a97\u653b\u51fb\u3002", "conclusion": "SCHIEBER\u4e3aGNSS\u63a5\u6536\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u6297\u5e72\u6270\u548c\u6297\u6b3a\u9a97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18161", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.18161", "abs": "https://arxiv.org/abs/2507.18161", "authors": ["Samuele Cornell", "Christoph Boeddeker", "Taejin Park", "He Huang", "Desh Raj", "Matthew Wiesner", "Yoshiki Masuyama", "Xuankai Chang", "Zhong-Qiu Wang", "Stefano Squartini", "Paola Garcia", "Shinji Watanabe"], "title": "Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges", "comment": null, "summary": "The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on\nmulti-channel, generalizable, joint automatic speech recognition (ASR) and\ndiarization of conversational speech. With participation from 9 teams\nsubmitting 32 diverse systems, these challenges have contributed to\nstate-of-the-art research in the field. This paper outlines the challenges'\ndesign, evaluation metrics, datasets, and baseline systems while analyzing key\ntrends from participant submissions. From this analysis it emerges that: 1)\nMost participants use end-to-end (e2e) ASR systems, whereas hybrid systems were\nprevalent in previous CHiME challenges. This transition is mainly due to the\navailability of robust large-scale pre-trained models, which lowers the data\nburden for e2e-ASR. 2) Despite recent advances in neural speech separation and\nenhancement (SSE), all teams still heavily rely on guided source separation,\nsuggesting that current neural SSE techniques are still unable to reliably deal\nwith complex scenarios and different recording setups. 3) All best systems\nemploy diarization refinement via target-speaker diarization techniques.\nAccurate speaker counting in the first diarization pass is thus crucial to\navoid compounding errors and CHiME-8 DASR participants especially focused on\nthis part. 4) Downstream evaluation via meeting summarization can correlate\nweakly with transcription quality due to the remarkable effectiveness of\nlarge-language models in handling errors. On the NOTSOFAR-1 scenario, even\nsystems with over 50\\% time-constrained minimum permutation WER can perform\nroughly on par with the most effective ones (around 11\\%). 5) Despite recent\nprogress, accurately transcribing spontaneous speech in challenging acoustic\nenvironments remains difficult, even when using computationally intensive\nsystem ensembles.", "AI": {"tldr": "CHiME-7\u548c8\u6311\u6218\u8d5b\u805a\u7126\u4e8e\u591a\u901a\u9053\u3001\u53ef\u6cdb\u5316\u7684\u8054\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u5bf9\u8bdd\u8bed\u97f3\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u3002\u53c2\u4e0e\u8005\u4e3b\u8981\u4f7f\u7528\u7aef\u5230\u7aefASR\u7cfb\u7edf\uff0c\u4f9d\u8d56\u5f15\u5bfc\u6e90\u5206\u79bb\u6280\u672f\uff0c\u5e76\u5f3a\u8c03\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u4f18\u5316\u3002\u5c3d\u7ba1\u8fdb\u5c55\u663e\u8457\uff0c\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u63a8\u52a8\u591a\u901a\u9053\u3001\u53ef\u6cdb\u5316\u7684\u8054\u5408ASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6280\u672f\u7684\u7814\u7a76\uff0c\u5206\u6790\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\u548c\u53d1\u5c55\u8d8b\u52bf\u3002", "method": "\u901a\u8fc7\u6311\u6218\u8d5b\u8bbe\u8ba1\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5206\u6790\u53c2\u4e0e\u8005\u63d0\u4ea4\u7684\u7cfb\u7edf\u53ca\u5176\u8d8b\u52bf\u3002", "result": "\u7aef\u5230\u7aefASR\u7cfb\u7edf\u6210\u4e3a\u4e3b\u6d41\uff0c\u5f15\u5bfc\u6e90\u5206\u79bb\u4ecd\u5360\u4e3b\u5bfc\uff0c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4f18\u5316\u662f\u5173\u952e\u3002\u5c3d\u7ba1\u6280\u672f\u8fdb\u6b65\uff0c\u590d\u6742\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u4ecd\u56f0\u96be\u3002", "conclusion": "CHiME\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u7aef\u5230\u7aefASR\u7684\u4f18\u52bf\u548c\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u9700\u6c42\u3002"}}
{"id": "2507.17941", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17941", "abs": "https://arxiv.org/abs/2507.17941", "authors": ["Quoc Thinh Vo", "David Han"], "title": "Resnet-conformer network with shared weights and attention mechanism for sound event localization, detection, and distance estimation", "comment": "This paper has been submitted as a technical report outlining our\n  approach to Task 3A of the Detection and Classification of Acoustic Scenes\n  and Events (DCASE) 2024 and can be found in DCASE2024 technical reports", "summary": "This technical report outlines our approach to Task 3A of the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2024, focusing on Sound\nEvent Localization and Detection (SELD). SELD provides valuable insights by\nestimating sound event localization and detection, aiding in various machine\ncognition tasks such as environmental inference, navigation, and other sound\nlocalization-related applications. This year's challenge evaluates models using\neither audio-only (Track A) or audiovisual (Track B) inputs on annotated\nrecordings of real sound scenes. A notable change this year is the introduction\nof distance estimation, with evaluation metrics adjusted accordingly for a\ncomprehensive assessment. Our submission is for Task A of the Challenge, which\nfocuses on the audio-only track. Our approach utilizes log-mel spectrograms,\nintensity vectors, and employs multiple data augmentations. We proposed an\nEINV2-based [1] network architecture, achieving improved results: an F-score of\n40.2%, Angular Error (DOA) of 17.7 degrees, and Relative Distance Error (RDE)\nof 0.32 on the test set of the Development Dataset [2 ,3].", "AI": {"tldr": "\u62a5\u544a\u4ecb\u7ecd\u4e86DCASE 2024\u4efb\u52a13A\u7684\u97f3\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\uff08SELD\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u97f3\u9891\u8f93\u5165\uff08Track A\uff09\u548cEINV2\u7f51\u7edc\u67b6\u6784\uff0c\u53d6\u5f97\u4e86F-score 40.2%\u3001DOA 17.7\u5ea6\u548cRDE 0.32\u7684\u7ed3\u679c\u3002", "motivation": "SELD\u5728\u673a\u5668\u8ba4\u77e5\u4efb\u52a1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5982\u73af\u5883\u63a8\u65ad\u548c\u5bfc\u822a\u3002\u4eca\u5e74\u7684\u6311\u6218\u65b0\u589e\u4e86\u8ddd\u79bb\u4f30\u8ba1\uff0c\u4ee5\u66f4\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u3002", "method": "\u4f7f\u7528log-mel\u9891\u8c31\u56fe\u548c\u5f3a\u5ea6\u5411\u91cf\uff0c\u7ed3\u5408\u591a\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u51fa\u57fa\u4e8eEINV2\u7684\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728\u5f00\u53d1\u6570\u636e\u96c6\u6d4b\u8bd5\u96c6\u4e0a\uff0cF-score\u4e3a40.2%\uff0cDOA\u8bef\u5dee17.7\u5ea6\uff0cRDE\u8bef\u5dee0.32\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u97f3\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.18167", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18167", "abs": "https://arxiv.org/abs/2507.18167", "authors": ["Yuxuan Wen", "Xiaoming Chen", "Maojun Zhang", "Zhaoyang Zhang"], "title": "ICWLM: A Multi-Task Wireless Large Model via In-Context Learning", "comment": null, "summary": "The rapid evolution of wireless communication technologies, particularly\nmassive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),\nintroduces significant network complexity and computational demands.\nSignificant research efforts have been made to improve physical layer\nperformance by resorting to deep learning (DL) methods, which, however, are\nusually task-specific and struggle with data scarcity and generalization. To\naddress these challenges, we propose a novel In-Context Wireless Large Model\n(ICWLM), a wireless-native foundation model designed for simultaneous\nmulti-task learning at the physical layer. Unlike conventional methods that\nadapt wireless data to pre-trained large language models (LLMs), ICWLM is\ntrained directly on large-scale, mixed wireless datasets from scratch. It\njointly solves multiple classical physical layer problems, including multi-user\nprecoding (sum-rate maximization and max-min SINR) and channel prediction. A\nkey innovation of ICWLM is its utilization of in-context learning (ICL),\nenabling the model to adapt to varying system configurations and channel\nconditions with minimal demonstration pairs, eliminating the need for extensive\nretraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm\nto dynamically balance the individual task losses during multi-task training,\nensuring efficient and stable learning across diverse objectives. Extensive\nsimulation results demonstrate that ICWLM achieves competitive performance\ncompared to task-specific methods while exhibiting remarkable generalization\ncapabilities to unseen system configurations. This work offers a promising\nparadigm for developing unified and adaptive AI models for future wireless\nnetworks, potentially reducing deployment complexity and enhancing intelligent\nresource management.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u7ebf\u539f\u751f\u57fa\u7840\u6a21\u578bICWLM\uff0c\u7528\u4e8e\u7269\u7406\u5c42\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff08\u5982mMIMO\u548cmmWave\uff09\u5e26\u6765\u4e86\u7f51\u7edc\u590d\u6742\u6027\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4efb\u52a1\u7279\u5b9a\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "ICWLM\u76f4\u63a5\u4ece\u5927\u89c4\u6a21\u6df7\u5408\u65e0\u7ebf\u6570\u636e\u8bad\u7ec3\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u9002\u5e94\u4e0d\u540c\u7cfb\u7edf\u914d\u7f6e\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u6743\u91cd\u5e73\u5747\uff08DWA\uff09\u7b97\u6cd5\u5e73\u8861\u591a\u4efb\u52a1\u635f\u5931\u3002", "result": "ICWLM\u5728\u591a\u9879\u7269\u7406\u5c42\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u9002\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u7cfb\u7edf\u914d\u7f6e\u3002", "conclusion": "ICWLM\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u81ea\u9002\u5e94\u7684AI\u6a21\u578b\u8303\u5f0f\uff0c\u964d\u4f4e\u4e86\u90e8\u7f72\u590d\u6742\u6027\u5e76\u63d0\u5347\u4e86\u8d44\u6e90\u7ba1\u7406\u667a\u80fd\u6027\u3002"}}
{"id": "2507.18181", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.18181", "abs": "https://arxiv.org/abs/2507.18181", "authors": ["Linye Wei", "Shuzhang Zhong", "Songqiang Xu", "Runsheng Wang", "Ru Huang", "Meng Li"], "title": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding", "comment": null, "summary": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpecASR\u7684\u65b0\u578b\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u964d\u4f4e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u5b9e\u65f6\u5ef6\u8fdf\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728ASR\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u591a\u65b9\u8a00\u652f\u6301\uff0c\u4f46\u5176\u9ad8\u89e3\u7801\u5ef6\u8fdf\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651ASR\u4efb\u52a1\u7279\u6027\uff0c\u901f\u5ea6\u63d0\u5347\u6709\u9650\u3002", "method": "SpecASR\u901a\u8fc7\u81ea\u9002\u5e94\u8349\u7a3f\u5e8f\u5217\u751f\u6210\u3001\u8349\u7a3f\u5e8f\u5217\u91cd\u7528\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u7a00\u758f\u4ee4\u724c\u6811\u751f\u6210\u7b97\u6cd5\uff0c\u4f18\u5316\u89e3\u7801\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpecASR\u5728\u4fdd\u6301\u8bc6\u522b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u6bd4\u57fa\u7ebf\u81ea\u56de\u5f52\u89e3\u7801\u548c\u63a8\u6d4b\u89e3\u7801\u5206\u522b\u63d0\u901f3.04x-3.79x\u548c1.25x-1.84x\u3002", "conclusion": "SpecASR\u663e\u8457\u964d\u4f4e\u4e86ASR\u7684\u5b9e\u65f6\u5ef6\u8fdf\uff0c\u4e3aLLM\u5728\u5b9e\u65f6\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18051", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18051", "abs": "https://arxiv.org/abs/2507.18051", "authors": ["Hongfei Xue", "Kaixun Huang", "Zhikai Zhou", "Shen Huang", "Shidong Shang"], "title": "The TEA-ASLP System for Multilingual Conversational Speech Recognition and Speech Diarization in MLC-SLM 2025 Challenge", "comment": "Interspeech 2025 workshop", "summary": "This paper presents the TEA-ASLP's system submitted to the MLC-SLM 2025\nChallenge, addressing multilingual conversational automatic speech recognition\n(ASR) in Task I and speech diarization ASR in Task II. For Task I, we enhance\nIdeal-LLM model by integrating known language identification and a multilingual\nMOE LoRA structure, along with using CTC-predicted tokens as prompts to improve\nautoregressive generation. The model is trained on approximately 180k hours of\nmultilingual ASR data. In Task II, we replace the baseline English-Chinese\nspeaker diarization model with a more suitable English-only version. Our\napproach achieves a 30.8% reduction in word error rate (WER) compared to the\nbaseline speech language model, resulting in a final WER of 9.60% in Task I and\na time-constrained minimum-permutation WER of 17.49% in Task II, earning first\nand second place in the respective challenge tasks.", "AI": {"tldr": "TEA-ASLP\u7cfb\u7edf\u5728MLC-SLM 2025\u6311\u6218\u8d5b\u4e2d\uff0c\u901a\u8fc7\u6539\u8fdbIdeal-LLM\u6a21\u578b\u548c\u4f18\u5316\u8bf4\u8bdd\u4eba\u65e5\u5fd7ASR\uff0c\u5206\u522b\u5728\u4efb\u52a1I\u548c\u4efb\u52a1II\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u5bf9\u8bdd\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7ASR\u7684\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4efb\u52a1I\uff1a\u96c6\u6210\u8bed\u8a00\u8bc6\u522b\u548c\u591a\u8bed\u8a00MOE LoRA\u7ed3\u6784\uff0c\u4f7f\u7528CTC\u9884\u6d4b\u6807\u8bb0\u4f5c\u4e3a\u63d0\u793a\uff1b\u4efb\u52a1II\uff1a\u66ff\u6362\u57fa\u7ebf\u6a21\u578b\u4e3a\u66f4\u9002\u5408\u7684\u82f1\u8bed\u7248\u672c\u3002", "result": "\u4efb\u52a1I\u7684WER\u964d\u4f4e30.8%\uff0c\u6700\u7ec8\u4e3a9.60%\uff1b\u4efb\u52a1II\u7684WER\u4e3a17.49%\uff0c\u5206\u522b\u83b7\u5f97\u7b2c\u4e00\u548c\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u8bed\u8a00ASR\u548c\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.18370", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18370", "abs": "https://arxiv.org/abs/2507.18370", "authors": ["Morriel Kasher", "Michael Tinston", "Predrag Spasojevic"], "title": "Quantized Signal Recovery with Interference via Parametrized Look-Up Tables", "comment": "13 pages, 18 figures", "summary": "Efficient all-digital post-correction of low-resolution analog-to-digital\nconverters can be achieved by using Look-Up Tables (LUTs). The performance of a\nLUT can be optimized by incorporating a parametric model for the expected input\nsignal, noise level, and interference signals. We evaluate three analytical\nestimators for integration with parametrized LUTs, especially with applications\nto low-resolution, non-linear, or wideband quantizers. We also propose several\napproximations to improve tractability of the estimation problem for\nPhase-Shift Keyed input signals and Linear Frequency Modulated interference\nsignals. Simulated results validate the ability of our estimator to recover the\ninstantaneous value of the desired input signal in real-time with a high degree\nof accuracy. This includes cancellation of harmonic distortion that aliases\ninto the desired signal bandwidth from front-end saturation due to high-power\nout-of-band interference. Our estimators are shown to achieve a significant\ngain over conventional linear-filtering techniques while also being robust to\nchanges in input parameters, non-linear quantizers, and time-variant\ninterference sources. For a tone input quantized to 3 bits and estimated with a\nfixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error\nand $>$20 dBc improvement in Spurious-Free Dynamic Range.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u67e5\u627e\u8868\uff08LUTs\uff09\u548c\u53c2\u6570\u5316\u6a21\u578b\u4f18\u5316\u4f4e\u5206\u8fa8\u7387\u6a21\u6570\u8f6c\u6362\u5668\uff08ADCs\uff09\u540e\u6821\u6b63\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5206\u6790\u4f30\u8ba1\u5668\u3002\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u6062\u590d\u8f93\u5165\u4fe1\u53f7\u548c\u6291\u5236\u8c10\u6ce2\u5931\u771f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027\u6ee4\u6ce2\u6280\u672f\u3002", "motivation": "\u4f4e\u5206\u8fa8\u7387\u3001\u975e\u7ebf\u6027\u6216\u5bbd\u5e26\u91cf\u5316\u5668\u7684\u6027\u80fd\u4f18\u5316\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u9ad8\u529f\u7387\u5e26\u5916\u5e72\u6270\u65f6\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6821\u6b63\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u53c2\u6570\u5316\u6a21\u578b\uff08\u8f93\u5165\u4fe1\u53f7\u3001\u566a\u58f0\u548c\u5e72\u6270\u4fe1\u53f7\u7684\u9884\u671f\uff09\u4f18\u5316LUTs\uff0c\u63d0\u51fa\u4e09\u79cd\u5206\u6790\u4f30\u8ba1\u5668\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u4fe1\u53f7\u7c7b\u578b\uff08\u5982PSK\u548cLFM\uff09\u63d0\u51fa\u8fd1\u4f3c\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u6062\u590d\u8f93\u5165\u4fe1\u53f7\uff0c\u6291\u5236\u8c10\u6ce2\u5931\u771f\uff0c\u5e76\u57283\u4f4d\u91cf\u5316\u5668\u548c12\u62bd\u5934\u6a21\u578b\u4e0b\u5b9e\u73b0MSE\u6539\u5584>10 dB\u548cSFDR\u6539\u5584>20 dBc\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5bf9\u8f93\u5165\u53c2\u6570\u53d8\u5316\u3001\u975e\u7ebf\u6027\u91cf\u5316\u5668\u548c\u65f6\u53d8\u5e72\u6270\u6e90\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.18350", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.18350", "abs": "https://arxiv.org/abs/2507.18350", "authors": ["Chengyuan Qin", "Wenmeng Xiong", "Jing Zhou", "Maoshen Jia", "Changchun Bao"], "title": "Speech Enhancement with Dual-path Multi-Channel Linear Prediction Filter and Multi-norm Beamforming", "comment": "Paper accepted by Interspeech 2025", "summary": "In this paper, we propose a speech enhancement method us ing dual-path\nMulti-Channel Linear Prediction (MCLP) filters\n  and multi-norm beamforming. Specifically, the MCLP part in\n  the proposed method is designed with dual-path filters in both\n  time and frequency dimensions. For the beamforming part, we\n  minimize the power of the microphone array output as well as\n  the l1 norm of the denoised signals while preserving source sig nals from the\ntarget directions. An efficient method to select the\n  prediction orders in the dual-path filters is also proposed, which\n  is robust for signals with different reverberation time (T60) val ues and can\nbe applied to other MCLP-based methods. Eval uations demonstrate that our\nproposed method outperforms the\n  baseline methods for speech enhancement, particularly in high\n  reverberation scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u8def\u5f84\u591a\u901a\u9053\u7ebf\u6027\u9884\u6d4b\uff08MCLP\uff09\u6ee4\u6ce2\u5668\u548c\u591a\u8303\u6570\u6ce2\u675f\u5f62\u6210\u7684\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u5f3a\u6df7\u54cd\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u5728\u9ad8\u6df7\u54cd\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53cc\u8def\u5f84MCLP\u6ee4\u6ce2\u5668\uff08\u65f6\u95f4\u548c\u9891\u7387\u7ef4\u5ea6\uff09\u548c\u591a\u8303\u6570\u6ce2\u675f\u5f62\u6210\uff0c\u6700\u5c0f\u5316\u9ea6\u514b\u98ce\u9635\u5217\u8f93\u51fa\u529f\u7387\u548c\u53bb\u566a\u4fe1\u53f7\u7684l1\u8303\u6570\uff0c\u540c\u65f6\u4fdd\u7559\u76ee\u6807\u65b9\u5411\u4fe1\u53f7\u3002", "result": "\u5728\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5f3a\u6df7\u54cd\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u97f3\u589e\u5f3a\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u6df7\u54cd\u73af\u5883\u3002"}}
{"id": "2507.18452", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18452", "abs": "https://arxiv.org/abs/2507.18452", "authors": ["Jiaming Zhou", "Hongjie Chen", "Shiwan Zhao", "Jian Kang", "Jie Li", "Enzhi Wang", "Yujie Guo", "Haoqin Sun", "Hui Wang", "Aobo Kong", "Yong Qin", "Xuelong Li"], "title": "DIFFA: Large Language Diffusion Models Can Listen and Understand", "comment": null, "summary": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git.", "AI": {"tldr": "DIFFA\u662f\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u53e3\u8bed\u7406\u89e3\uff0c\u901a\u8fc7\u53cc\u9002\u914d\u5668\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u97f3\u9891\u6a21\u6001\u4e2d\u7684\u5e94\u7528\uff0c\u5f25\u8865\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u4e0e\u53cc\u9002\u914d\u5668\u67b6\u6784\uff0c\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1aASR\u76ee\u6807\u5bf9\u9f50\u8bed\u4e49\u8868\u793a\uff0c\u518d\u901a\u8fc7\u5408\u6210\u97f3\u9891-\u6587\u672c\u5bf9\u5b66\u4e60\u6307\u4ee4\u8ddf\u968f\u3002", "result": "\u5728MMSU\u3001MMAU\u548cVoiceBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u591a\u4e2a\u81ea\u56de\u5f52\u5f00\u6e90\u57fa\u7ebf\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u97f3\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.18587", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18587", "abs": "https://arxiv.org/abs/2507.18587", "authors": ["J\u00e9r\u00f4me Emery", "Ali Hasanzadeh Karkan", "Jean-Fran\u00e7ois Frigon", "Fran\u00e7ois Leduc-Primeau"], "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff", "comment": "6 pages, 3 figures. Accepted to the IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "summary": "Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21MIMO\u9884\u7f16\u7801\uff0c\u65e8\u5728\u964d\u4f4e\u53d1\u5c04\u673a\u80fd\u8017\u5e76\u52a8\u6001\u9002\u5e94\u7528\u6237\u901f\u7387\u9700\u6c42\u3002\u8be5\u6a21\u578b\u5728\u96f6\u6837\u672c\u90e8\u7f72\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u9ad8\u8d28\u91cf\u672c\u5730\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u80fd\u8017\u5e76\u52a8\u6001\u9002\u5e94\u7528\u6237\u9700\u6c42\u3002", "method": "\u4f7f\u7528Transformer\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u9884\u7f16\u7801\uff0c\u5f15\u5165\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u627e\u5230\u4e0e\u76ee\u6807\u5206\u5e03\u76f8\u4f3c\u7684\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728\u76f8\u540c\u80fd\u8017\u4e0b\uff0c\u96f6\u6837\u672c\u90e8\u7f72\u7684\u57fa\u7840\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u96f6\u5f3a\u5236\u65b9\u6cd5\uff0c\u63a5\u8fd1\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u6027\u80fd\uff0c\u4e14\u590d\u6742\u5ea6\u964d\u4f4e8\u500d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u548c\u8bad\u7ec3\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u5728\u5b9e\u9645\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u652f\u6301\u52a8\u6001\u914d\u7f6e\u7528\u6237\u901f\u7387\u9700\u6c42\u4ee5\u63d0\u9ad8\u80fd\u6548\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2507.18446", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.18446", "abs": "https://arxiv.org/abs/2507.18446", "authors": ["Ivan Medennikov", "Taejin Park", "Weiqing Wang", "He Huang", "Kunal Dhawan", "Jinhan Wang", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering", "comment": "Accepted to Interspeech 2025", "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSortformer\u7684\u6d41\u5f0f\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684\u5230\u8fbe\u987a\u5e8f\u8bf4\u8bdd\u4eba\u7f13\u5b58\uff08AOSC\uff09\u5b9e\u73b0\u9ad8\u6548\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6846\u67b6\u5728\u5b9e\u65f6\u591a\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528AOSC\u6309\u5230\u8fbe\u65f6\u95f4\u987a\u5e8f\u5b58\u50a8\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u52a8\u6001\u66f4\u65b0\u7f13\u5b58\u4ee5\u4f18\u5316\u5b58\u50a8\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u573a\u666f\u3002", "conclusion": "Streaming Sortformer\u4e3a\u5b9e\u65f6\u591a\u8bf4\u8bdd\u4eba\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u6d41\u5f0f\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5904\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}

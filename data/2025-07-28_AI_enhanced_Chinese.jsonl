{"id": "2507.18673", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18673", "abs": "https://arxiv.org/abs/2507.18673", "authors": ["Morriel Kasher", "Michael Tinston", "Predrag Spasojevic"], "title": "Design and Implementation of Parametrized Look-Up Tables for Post-Correction of Oversampling Low-Resolution ADCs", "comment": "13 pages, 20 figures. arXiv admin note: text overlap with\n  arXiv:2507.18370", "summary": "We propose a framework for the design, optimization, and implementation of\nLook-Up Tables (LUTs) used to recover noisy, oversampled, quantized signals\ngiven a parametric input model. The LUTs emulate the spectral effects of\npre-quantization dithering through an all-digital solution applied after\nquantization. This methodology decomposes the intractable LUT design problem\ninto four distinct stages, each of which is addressed analytically using a\nmodel-driven approach without reliance on training. Three dithering methods are\nstudied to improve spectral purity metrics. Two novel indexing schemes are\nproposed to limit the LUT memory overhead shown to compress the LUT size by\nover four orders of magnitude with marginal performance loss. The LUT design is\ntested with an oversampled noisy sinusoidal input quantized to 3 bits and shown\nto improve its Spurious-Free Dynamic Range (SFDR) by over 19 dBc with only 324\nbytes of memory while maintaining the same 3-bit fixed-point precision at the\ndigital output. This correction can be implemented using two-level\ncombinational logic ensuring ultra-low latency and, hence, suitable for\nlow-resolution wideband devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bbe\u8ba1\u3001\u4f18\u5316\u548c\u5b9e\u73b0\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6062\u590d\u566a\u58f0\u3001\u8fc7\u91c7\u6837\u3001\u91cf\u5316\u4fe1\u53f7\uff0c\u901a\u8fc7\u5168\u6570\u5b57\u89e3\u51b3\u65b9\u6848\u6a21\u62df\u9884\u91cf\u5316\u6296\u52a8\u7684\u9891\u8c31\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u91cf\u5316\u4fe1\u53f7\u6062\u590d\u4e2d\u7684\u9891\u8c31\u7eaf\u51c0\u5ea6\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u3002", "method": "\u5c06LUT\u8bbe\u8ba1\u95ee\u9898\u5206\u89e3\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u91c7\u7528\u4e09\u79cd\u6296\u52a8\u65b9\u6cd5\u63d0\u5347\u9891\u8c31\u7eaf\u51c0\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u7d22\u5f15\u65b9\u6848\u4ee5\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u3002", "result": "\u57283\u4f4d\u91cf\u5316\u4fe1\u53f7\u4e2d\uff0cSFDR\u63d0\u5347\u4e8619 dBc\uff0c\u5185\u5b58\u5360\u7528\u4ec5324\u5b57\u8282\uff0c\u4e14\u4fdd\u63013\u4f4d\u5b9a\u70b9\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u5bbd\u5e26\u8bbe\u5907\uff0c\u5177\u6709\u8d85\u4f4e\u5ef6\u8fdf\u7279\u6027\u3002"}}
{"id": "2507.18730", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18730", "abs": "https://arxiv.org/abs/2507.18730", "authors": ["Yufeng Zhou", "Wen Chen", "Qingqing Wu", "Xusheng Zhu", "Zhendong Li", "Kunlun Wang", "Qiong Wu"], "title": "Exploiting Movable Antennas in NOMA Networks: Joint Beamforming, Power Allocation and Antenna Position Optimization", "comment": null, "summary": "This paper investigates the movable antenna (MA)- assisted downlink\nnon-orthogonal multiple access (NOMA) network to maximize system throughput. In\nthe considered scenario, both the base station (BS) and users are equipped with\nMA, and a predetermined successive interference cancellation (SIC) decoding\norder is adopted. Based on the field-response channel model, we formulate a\ncomplex, non-convex problem to jointly optimize the BS beamforming, power\nallocation, and MA positions at both the transmitter and receivers. To address\nthis, we propose an efficient algorithm based on an alternating optimization\n(AO) framework, which decomposes the original problem into three distinct\nsubproblems. By employing sequential parametric convex approximation (SPCA) and\nsuccessive convex approximation (SCA) techniques, the non-convex constraints\nwithin each subproblem are transformed into tractable. This methodology ensures\nthe algorithm converges to a stable, locally optimal solution. Numerical\nresults validate that the proposed system, which fully exploits the degrees of\nfreedom from antenna mobility at both ends, significantly outperforms\nbenchmarks in terms of throughput.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7684\u4e0b\u884c\u975e\u6b63\u4ea4\u591a\u5740\uff08NOMA\uff09\u7f51\u7edc\uff0c\u65e8\u5728\u6700\u5927\u5316\u7cfb\u7edf\u541e\u5410\u91cf\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u3001\u529f\u7387\u5206\u914d\u53ca\u5929\u7ebf\u4f4d\u7f6e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u3002", "motivation": "\u901a\u8fc7\u5229\u7528\u53ef\u79fb\u52a8\u5929\u7ebf\u5728\u57fa\u7ad9\u548c\u7528\u6237\u7aef\u7684\u81ea\u7531\u5ea6\uff0c\u63d0\u5347\u975e\u6b63\u4ea4\u591a\u5740\u7f51\u7edc\u7684\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u5229\u7528SPCA\u548cSCA\u6280\u672f\u5904\u7406\u975e\u51f8\u7ea6\u675f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7cfb\u7edf\u5728\u541e\u5410\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u901a\u4fe1\u53c2\u6570\uff0c\u53ef\u79fb\u52a8\u5929\u7ebf\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347NOMA\u7f51\u7edc\u7684\u6027\u80fd\u3002"}}
{"id": "2507.18733", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18733", "abs": "https://arxiv.org/abs/2507.18733", "authors": ["Yuan Guo", "Wen Chen", "Qingqing Wu", "Yanze Zhu", "Yang Liu", "Zhendong Li", "Ying Wang"], "title": "Max-Min Rate Optimization for Multigroup Multicast MISO Systems Via Novel Transmissive RIS Transceiver", "comment": null, "summary": "This paper investigates a novel transmissive reconfigurable intelligent\nsurface (RIS) transceiver architectureenabled multigroup multicast downlink\ncommunication system. Under this setup, an optimization problem is formulated\nto maximize the minimum rate of users across all groups, subject to the maximum\navailable power of each RIS unit. Due to the nondifferentiable nature of the\nobjective function, the max-min rate problem is challenging to solve. To tackle\nthis difficult problem, we develop an iterative solution by leveraging the\nsuccessive convex approximation (SCA) and the penalty function method. However,\nthe above approach has high computational complexity and may lead to\ncompromised performance. To overcome these drawbacks, we design an efficient\nsecond-order cone programming (SOCP)-based method using the weighted minimum\nmean squared error (WMMSE) framework to reduce computational complexity.\nFurthermore, to further reduce the computational complexity, we also propose a\nlow-complexity and solver-free algorithm that analytically updates all\nvariables by combining the smooth approximation theory and the\nmajorization-minimization (MM) method. Numerical results are provided to verify\nthe convergence and effectiveness of our proposed three algorithms. It is also\ndemonstrated that the SOCP-based method outperforms the penalty-based algorithm\nin terms of both the achieved min rate and the computational complexity. In\ncontrast, the lowcomplexity design achieves significantly lower complexity with\nonly slightly degraded performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u900f\u5c04\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u6536\u53d1\u5668\u67b6\u6784\u7684\u591a\u7ec4\u591a\u64ad\u4e0b\u884c\u901a\u4fe1\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u7b97\u6cd5\u4ee5\u4f18\u5316\u7528\u6237\u6700\u5c0f\u901f\u7387\uff0c\u5e76\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u548c\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u7ec4\u591a\u64ad\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u76ee\u6807\u51fd\u6570\u7684\u4e0d\u53ef\u5fae\u6027\u5bfc\u81f4\u7684\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "1. \u4f7f\u7528\u9010\u6b21\u51f8\u8fd1\u4f3c\uff08SCA\uff09\u548c\u60e9\u7f5a\u51fd\u6570\u6cd5\u7684\u8fed\u4ee3\u89e3\u6cd5\uff1b2. \u57fa\u4e8e\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08WMMSE\uff09\u6846\u67b6\u7684\u4e8c\u9636\u9525\u89c4\u5212\uff08SOCP\uff09\u65b9\u6cd5\uff1b3. \u7ed3\u5408\u5e73\u6ed1\u8fd1\u4f3c\u7406\u8bba\u548c\u6700\u5927\u5316-\u6700\u5c0f\u5316\uff08MM\uff09\u65b9\u6cd5\u7684\u4f4e\u590d\u6742\u5ea6\u65e0\u6c42\u89e3\u5668\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cSOCP\u65b9\u6cd5\u5728\u6700\u5c0f\u901f\u7387\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u60e9\u7f5a\u6cd5\uff0c\u800c\u4f4e\u590d\u6742\u5ea6\u8bbe\u8ba1\u5728\u6027\u80fd\u7565\u6709\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u79cd\u7b97\u6cd5\u5747\u6709\u6548\uff0cSOCP\u65b9\u6cd5\u6027\u80fd\u6700\u4f18\uff0c\u4f4e\u590d\u6742\u5ea6\u8bbe\u8ba1\u9002\u5408\u5bf9\u6027\u80fd\u8981\u6c42\u4e0d\u9ad8\u7684\u573a\u666f\u3002"}}
{"id": "2507.18764", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18764", "abs": "https://arxiv.org/abs/2507.18764", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "title": "Max-Min Fairness-Oriented Beamforming Design in HAPS-Enabled ISAC for 6G Networks", "comment": null, "summary": "This paper presents a high-altitude platform station (HAPS)-enabled\nintegrated sensing and communication (ISAC) system designed for\nsixth-generation (6G) networks. Positioned in the stratosphere, HAPS serves as\na super-macro base station, leveraging advanced beamforming techniques to\nenable communication and sensing simultaneously. This research addresses the\nneed for equitable service distribution in 6G networks by focusing on fairness\nwithin the HAPS-ISAC system. It tackles a non-convex optimization problem that\nbalances sensing beampattern gain and signal-to-interference-plus-noise ratio\n(SINR) requirements among communication users (CUs) using a max-min fairness\napproach while adhering to power constraints. The proposed HAPS-ISAC framework\nensures efficient resource allocation, reliable coverage, and improved sensing\naccuracy. Simulation results validate the potential of HAPS-ISAC as a pivotal\nenabler for 6G networks and integrated communication-sensing systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7a7a\u5e73\u53f0\u7ad9\uff08HAPS\uff09\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e6G\u7f51\u7edc\uff0c\u901a\u8fc7\u4f18\u5316\u8d44\u6e90\u5206\u914d\u5b9e\u73b0\u516c\u5e73\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u89e3\u51b36G\u7f51\u7edc\u4e2d\u516c\u5e73\u670d\u52a1\u5206\u914d\u7684\u9700\u6c42\uff0c\u540c\u65f6\u517c\u987e\u901a\u4fe1\u548c\u611f\u77e5\u529f\u80fd\u3002", "method": "\u5229\u7528HAPS\u4f5c\u4e3a\u8d85\u7ea7\u5b8f\u57fa\u7ad9\uff0c\u91c7\u7528\u6ce2\u675f\u6210\u5f62\u6280\u672f\uff0c\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u95ee\u9898\u5e73\u8861\u611f\u77e5\u6ce2\u675f\u589e\u76ca\u548c\u901a\u4fe1\u7528\u6237\u7684SINR\u9700\u6c42\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHAPS-ISAC\u6846\u67b6\u80fd\u9ad8\u6548\u5206\u914d\u8d44\u6e90\uff0c\u63d0\u4f9b\u53ef\u9760\u8986\u76d6\u5e76\u63d0\u5347\u611f\u77e5\u7cbe\u5ea6\u3002", "conclusion": "HAPS-ISAC\u662f6G\u7f51\u7edc\u548c\u96c6\u6210\u901a\u4fe1-\u611f\u77e5\u7cfb\u7edf\u7684\u5173\u952e\u63a8\u52a8\u8005\u3002"}}
{"id": "2507.18723", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18723", "abs": "https://arxiv.org/abs/2507.18723", "authors": ["Vishakh Begari"], "title": "SCORE-SET: A dataset of GuitarPro files for Music Phrase Generation and Sequence Learning", "comment": "6 pages, 6 figures", "summary": "A curated dataset of Guitar Pro tablature files (.gp5 format), tailored for\ntasks involving guitar music generation, sequence modeling, and\nperformance-aware learning is provided. The dataset is derived from MIDI notes\nin MAESTRO and GiantMIDI which have been adapted into rhythm guitar tracks.\nThese tracks are further processed to include a variety of expression settings\ntypical of guitar performance, such as bends, slides, vibrato, and palm muting,\nto better reflect the nuances of real-world guitar playing.", "AI": {"tldr": "\u63d0\u4f9b\u4e13\u4e3a\u5409\u4ed6\u97f3\u4e50\u751f\u6210\u3001\u5e8f\u5217\u5efa\u6a21\u548c\u6027\u80fd\u611f\u77e5\u5b66\u4e60\u4efb\u52a1\u5b9a\u5236\u7684Guitar Pro\u6587\u4ef6\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u5409\u4ed6\u97f3\u4e50\u751f\u6210\u548c\u5efa\u6a21\u4efb\u52a1\u63d0\u4f9b\u66f4\u8d34\u8fd1\u771f\u5b9e\u6f14\u594f\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u5409\u4ed6\u8868\u73b0\u6280\u5de7\u3002", "method": "\u4eceMAESTRO\u548cGiantMIDI\u7684MIDI\u97f3\u7b26\u4e2d\u63d0\u53d6\u8282\u594f\u5409\u4ed6\u97f3\u8f68\uff0c\u5e76\u6dfb\u52a0\u5409\u4ed6\u8868\u73b0\u6280\u5de7\uff08\u5982\u5f2f\u97f3\u3001\u6ed1\u97f3\u3001\u98a4\u97f3\u7b49\uff09\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u5409\u4ed6\u8868\u73b0\u6280\u5de7\u7684\u6570\u636e\u96c6\uff0c\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u5409\u4ed6\u6f14\u594f\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5409\u4ed6\u97f3\u4e50\u751f\u6210\u548c\u5efa\u6a21\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u3001\u66f4\u771f\u5b9e\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.19040", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19040", "abs": "https://arxiv.org/abs/2507.19040", "authors": ["Yizhou Peng", "Yi-Wen Chao", "Dianwen Ng", "Yukun Ma", "Chongjia Ni", "Bin Ma", "Eng Siong Chng"], "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems", "comment": "Accepted to Interspeech 2025. 5 pages", "summary": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5168\u53cc\u5de5\u5bf9\u8bdd\u7cfb\u7edf\uff08FDSDS\uff09\u6027\u80fd\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u6d4b\u4e2d\u7f3a\u4e4f\u5bf9\u7528\u6237\u4e2d\u65ad\u573a\u666f\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u7684\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u8f6e\u6d41\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u800c\u5168\u53cc\u5de5\u7cfb\u7edf\u652f\u6301\u5b9e\u65f6\u4e2d\u65ad\u548c\u53cd\u9988\uff0c\u4f46\u73b0\u6709\u8bc4\u6d4b\u7f3a\u4e4f\u76f8\u5173\u6307\u6807\u3002", "method": "\u5229\u7528LLMs\u3001TTS\u548cASR\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u5957\u8bc4\u4f30FDSDS\u5904\u7406\u7528\u6237\u4e2d\u65ad\u3001\u5ef6\u8fdf\u7ba1\u7406\u548c\u9c81\u68d2\u6027\u7684\u65b0\u6307\u6807\u3002", "result": "\u6d4b\u8bd5\u4e86\u4e09\u4e2a\u5f00\u6e90FDSDS\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u5728\u9891\u7e41\u4e2d\u65ad\u548c\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5982\u65e0\u6cd5\u54cd\u5e94\u7528\u6237\u4e2d\u65ad\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u5168\u53cc\u5de5\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.18793", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18793", "abs": "https://arxiv.org/abs/2507.18793", "authors": ["Kuranage Roche Rayan Ranasinghe", "Jiancheng An", "Iv\u00e1n Alexander Morales Sandoval", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "Chau Yuen", "M\u00e9rouane Debbah"], "title": "Flexible Intelligent Metasurfaces in High-Mobility MIMO Integrated Sensing and Communications", "comment": "Submitted to an IEEE journal", "summary": "We propose a novel doubly-dispersive (DD) multiple-input multiple-output\n(MIMO) channel model incorporating flexible intelligent metasurfaces (FIMs),\nwhich is suitable for integrated sensing and communications (ISAC) in\nhigh-mobility scenarios. We then discuss how the proposed FIM-parameterized DD\n(FPDD) channel model can be applied in a logical manner to ISAC waveforms that\nare known to perform well in DD environments, namely, orthogonal frequency\ndivision multiplexing (OFDM), orthogonal time frequency space (OTFS), and\naffine frequency division multiplexing (AFDM). Leveraging the proposed model,\nwe formulate an achievable rate maximization problem with a strong sensing\nconstraint for all the aforementioned waveforms, which we then solve via a\ngradient ascent algorithm with closed-form gradients presented as a bonus. Our\nnumerical results indicate that the achievable rate is significantly impacted\nby the emerging FIM technology with careful parametrization essential in\nobtaining strong ISAC performance across all waveforms suitable to mitigating\nthe effects of DD channels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u8272\u6563\uff08DD\uff09MIMO\u4fe1\u9053\u6a21\u578b\uff0c\u7ed3\u5408\u7075\u6d3b\u667a\u80fd\u8d85\u8868\u9762\uff08FIM\uff09\uff0c\u9002\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u53cc\u8272\u6563\u4fe1\u9053\u73af\u5883\u4e0b\u4f18\u5316\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u6027\u80fd\uff0c\u5229\u7528FIM\u6280\u672f\u63d0\u5347\u7cfb\u7edf\u8868\u73b0\u3002", "method": "\u63d0\u51faFIM\u53c2\u6570\u5316\u7684DD\uff08FPDD\uff09\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eOFDM\u3001OTFS\u548cAFDM\u6ce2\u5f62\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u6c42\u89e3\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cFIM\u6280\u672f\u5bf9\u53ef\u8fbe\u5230\u7684\u901f\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u53c2\u6570\u5316\u5bf9\u63d0\u5347ISAC\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "FIM\u6280\u672f\u7ed3\u5408\u9002\u5f53\u7684\u53c2\u6570\u5316\uff0c\u53ef\u4ee5\u6709\u6548\u4f18\u5316\u53cc\u8272\u6563\u4fe1\u9053\u4e2d\u7684ISAC\u6027\u80fd\u3002"}}
{"id": "2507.18897", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18897", "abs": "https://arxiv.org/abs/2507.18897", "authors": ["Rongkun Xue", "Yazhe Niu", "Shuai Hu", "Zixin Yin", "Yongqiang Yao", "Jing Yang"], "title": "HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling", "comment": null, "summary": "Discrete speech tokenization is a fundamental component in speech codecs.\nHowever, in large-scale speech-to-speech systems, the complexity of parallel\nstreams from multiple quantizers and the computational cost of\nhigh-time-dimensional codecs pose significant challenges. In this paper, we\nintroduce HH-Codec, a neural codec that achieves extreme compression at 24\ntokens per second for 24 kHz audio while relying on single-quantizer inference.\nOur approach involves a carefully designed Vector Quantization space for Spoken\nLanguage Modeling, optimizing compression efficiency while minimizing\ninformation loss. Building on this, we propose an asymmetric encoder-decoder\narchitecture (Audio-VQ-Mel-Audio) that leverages dual supervision and\nprogressive training to enhance reconstruction stability and fidelity. HH-Codec\nachieves state-of-the-art performance in speech reconstruction with an\nultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in\ncodebook utilization and generative model adaptation, with extensive ablations\nvalidating the necessity of each module. HH-Codec is available at\nhttps://github.com/opendilab/HH-Codec.", "AI": {"tldr": "HH-Codec\u662f\u4e00\u79cd\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5355\u91cf\u5316\u5668\u63a8\u7406\u5b9e\u73b024 kHz\u97f3\u9891\u7684\u6781\u7aef\u538b\u7f29\uff0824 tokens/\u79d2\uff09\uff0c\u5e76\u57280.3 kbps\u7684\u8d85\u4f4e\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8bed\u97f3\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u8bed\u97f3\u5230\u8bed\u97f3\u7cfb\u7edf\u4e2d\u591a\u91cf\u5316\u5668\u5e76\u884c\u6d41\u590d\u6742\u6027\u548c\u9ad8\u65f6\u95f4\u7ef4\u5ea6\u7f16\u89e3\u7801\u5668\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4f18\u5316\u7684\u5411\u91cf\u91cf\u5316\u7a7a\u95f4\uff0c\u91c7\u7528\u4e0d\u5bf9\u79f0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff08Audio-VQ-Mel-Audio\uff09\uff0c\u7ed3\u5408\u53cc\u91cd\u76d1\u7763\u548c\u6e10\u8fdb\u8bad\u7ec3\u63d0\u5347\u91cd\u5efa\u7a33\u5b9a\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u57280.3 kbps\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u6700\u4f18\u8bed\u97f3\u91cd\u5efa\uff0c\u5e76\u5728\u4ee3\u7801\u672c\u5229\u7528\u7387\u548c\u751f\u6210\u6a21\u578b\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HH-Codec\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u548c\u8bed\u97f3\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u97f3\u7cfb\u7edf\u3002"}}
{"id": "2507.19137", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.19137", "abs": "https://arxiv.org/abs/2507.19137", "authors": ["Alice Zhang", "Skanda Muralidhar", "Daniel Gatica-Perez", "Mathew Magimai-Doss"], "title": "Assessment of Personality Dimensions Across Situations Using Conversational Speech", "comment": null, "summary": "Prior research indicates that users prefer assistive technologies whose\npersonalities align with their own. This has sparked interest in automatic\npersonality perception (APP), which aims to predict an individual's perceived\npersonality traits. Previous studies in APP have treated personalities as\nstatic traits, independent of context. However, perceived personalities can\nvary by context and situation as shown in psychological research. In this\nstudy, we investigate the relationship between conversational speech and\nperceived personality for participants engaged in two work situations (a\nneutral interview and a stressful client interaction). Our key findings are: 1)\nperceived personalities differ significantly across interactions, 2) loudness,\nsound level, and spectral flux features are indicative of perceived\nextraversion, agreeableness, conscientiousness, and openness in neutral\ninteractions, while neuroticism correlates with these features in stressful\ncontexts, 3) handcrafted acoustic features and non-verbal features outperform\nspeaker embeddings in inference of perceived personality, and 4) stressful\ninteractions are more predictive of neuroticism, aligning with existing\npsychological research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u6237\u504f\u597d\u4e0e\u81ea\u8eab\u6027\u683c\u5339\u914d\u7684\u8f85\u52a9\u6280\u672f\u3002\u81ea\u52a8\u6027\u683c\u611f\u77e5\uff08APP\uff09\u7814\u7a76\u901a\u5e38\u5c06\u6027\u683c\u89c6\u4e3a\u9759\u6001\u7279\u8d28\uff0c\u4f46\u5b9e\u9645\u6027\u683c\u611f\u77e5\u4f1a\u56e0\u60c5\u5883\u53d8\u5316\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e24\u79cd\u5de5\u4f5c\u60c5\u5883\u4e0b\uff08\u4e2d\u6027\u9762\u8bd5\u548c\u538b\u529b\u5ba2\u6237\u4e92\u52a8\uff09\u5bf9\u8bdd\u8bed\u97f3\u4e0e\u611f\u77e5\u6027\u683c\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u6027\u683c\u611f\u77e5\u7684\u52a8\u6001\u6027\uff0c\u9a8c\u8bc1\u60c5\u5883\u5bf9\u6027\u683c\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u4e2d\u6027\u9762\u8bd5\u548c\u538b\u529b\u5ba2\u6237\u4e92\u52a8\u4e2d\u7684\u5bf9\u8bdd\u8bed\u97f3\uff0c\u63d0\u53d6\u58f0\u5b66\u548c\u975e\u8bed\u8a00\u7279\u5f81\uff0c\u6bd4\u8f83\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u6027\u683c\u611f\u77e5\u5dee\u5f02\u3002", "result": "1) \u6027\u683c\u611f\u77e5\u56e0\u60c5\u5883\u663e\u8457\u4e0d\u540c\uff1b2) \u4e0d\u540c\u60c5\u5883\u4e0b\u58f0\u5b66\u7279\u5f81\u4e0e\u6027\u683c\u7279\u8d28\u76f8\u5173\uff1b3) \u624b\u5de5\u7279\u5f81\u4f18\u4e8e\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff1b4) \u538b\u529b\u60c5\u5883\u66f4\u80fd\u9884\u6d4b\u795e\u7ecf\u8d28\u3002", "conclusion": "\u6027\u683c\u611f\u77e5\u662f\u52a8\u6001\u7684\uff0c\u60c5\u5883\u5bf9\u6027\u683c\u611f\u77e5\u6709\u663e\u8457\u5f71\u54cd\uff0c\u58f0\u5b66\u548c\u975e\u8bed\u8a00\u7279\u5f81\u662f\u6709\u6548\u7684\u9884\u6d4b\u6307\u6807\u3002"}}
{"id": "2507.18927", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18927", "abs": "https://arxiv.org/abs/2507.18927", "authors": ["Xin Cheng", "Yu He", "Menglu Li", "Ruoguang Li", "Feng Shu", "Guangjie Han"], "title": "A Fingerprint Database Generation Method for RIS-Assisted Indoor Positioning", "comment": null, "summary": "Reconfigurable intelligent surface (RIS) has emerged as a promising\ntechnology to enhance indoor wireless communication and sensing performance.\nHowever, the construction of reliable received signal strength (RSS)-based\nfingerprint databases for RIS-assisted indoor positioning remains an open\nchallenge due to the lack of realistic and spatially consistent channel\nmodeling methods. In this paper, we propose a novel method with open-source\ncodes for generating RIS-assisted RSS fingerprint databases. Our method\ncaptures the complex RIS-assisted multipath behaviors by extended cluster-based\nchannel modeling and the physical and electromagnetic properties of RIS and\ntransmitter (Tx). And the spatial consistency is incorporated when simulating\nthe fingerprint data collection across neighboring positions. Furthermore, the\nproposed method offers exceptional flexibility in configuring RIS and Tx\nparameters. Extensive simulations are conducted to evaluate the fingerprint\ndatabase generated by the proposed method. Moreover, the positioning\nperformance on the database using K-nearest neighbors (KNN) and deep neural\nnetwork (DNN) is analyzed, providing valuable insights for the system design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210RIS\u8f85\u52a9RSS\u6307\u7eb9\u6570\u636e\u5e93\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "RIS\u6280\u672f\u867d\u80fd\u63d0\u5347\u5ba4\u5185\u65e0\u7ebf\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684RSS\u6307\u7eb9\u6570\u636e\u5e93\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u7684\u57fa\u4e8e\u7c07\u7684\u4fe1\u9053\u5efa\u6a21\u548cRIS/Tx\u7684\u7269\u7406\u7535\u78c1\u7279\u6027\uff0c\u6a21\u62df\u590d\u6742\u591a\u5f84\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7KNN\u548cDNN\u5206\u6790\u4e86\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aRIS\u8f85\u52a9\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2507.19037", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19037", "abs": "https://arxiv.org/abs/2507.19037", "authors": ["Yiwen Guan", "Viet Anh Trinh", "Vivek Voleti", "Jacob Whitehill"], "title": "MLLM-based Speech Recognition: When and How is Multimodality Beneficial?", "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have opened new\npossibilities for unified modeling of speech, text, images, and other\nmodalities. Building on our prior work, this paper examines the conditions and\nmodel architectures under which multiple input modalities can improve automatic\nspeech recognition (ASR) accuracy in noisy environments. Through experiments on\nsynthetic and real-world data, we find that (1) harnessing more modalities\nusually improves ASR accuracy, as each modality provides complementary\ninformation, but the improvement depends on the amount of auditory noise. (2)\nSynchronized modalities (e.g., lip movements) are more useful at high noise\nlevels whereas unsynchronized modalities (e.g., image context) are most helpful\nat moderate noise levels. (3) Higher-quality visual representations\nconsistently improve ASR accuracy, highlighting the importance of developing\nmore powerful visual encoders. (4) Mamba exhibits similar trends regarding the\nbenefits of multimodality as do Transformers. (5) The input order of modalities\nas well as their weights in the loss function can significantly impact\naccuracy. These findings both offer practical insights and help to deepen our\nunderstanding of multi-modal speech recognition under challenging conditions.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u566a\u58f0\u73af\u5883\u4e2d\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u51c6\u786e\u6027\u7684\u6761\u4ef6\u548c\u67b6\u6784\u7814\u7a76\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u8f93\u5165\u5728\u566a\u58f0\u73af\u5883\u4e0b\u5982\u4f55\u63d0\u5347ASR\u51c6\u786e\u6027\uff0c\u4ee5\u5f25\u8865\u5355\u4e00\u6a21\u6001\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u6001\uff08\u5982\u8bed\u97f3\u3001\u6587\u672c\u3001\u56fe\u50cf\uff09\u5bf9ASR\u7684\u5f71\u54cd\u3002", "result": "\u591a\u6a21\u6001\u901a\u5e38\u63d0\u5347ASR\u51c6\u786e\u6027\uff0c\u4f46\u6548\u679c\u4f9d\u8d56\u4e8e\u566a\u58f0\u6c34\u5e73\uff1b\u540c\u6b65\u4e0e\u975e\u540c\u6b65\u6a21\u6001\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u6548\u679c\u4e0d\u540c\uff1b\u89c6\u89c9\u8868\u5f81\u8d28\u91cf\u5bf9ASR\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u591a\u6a21\u6001\u4f5c\u7528\u7684\u7406\u89e3\u3002"}}
{"id": "2507.19204", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.19204", "abs": "https://arxiv.org/abs/2507.19204", "authors": ["Simon Malan", "Benjamin van Niekerk", "Herman Kamper"], "title": "Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?", "comment": "5 figures, 5 tables", "summary": "We investigate the problem of segmenting unlabeled speech into word-like\nunits and clustering these to create a lexicon. Prior work can be categorized\ninto two frameworks. Bottom-up methods first determine boundaries and then\ncluster the fixed segmented words into a lexicon. In contrast, top-down methods\nincorporate information from the clustered words to inform boundary selection.\nHowever, it is unclear whether top-down information is necessary to improve\nsegmentation. To explore this, we look at two similar approaches that differ in\nwhether top-down clustering informs boundary selection. Our simple bottom-up\nstrategy predicts word boundaries using the dissimilarity between adjacent\nself-supervised features, then clusters the resulting segments to construct a\nlexicon. Our top-down system is an updated version of the ES-KMeans dynamic\nprogramming method that iteratively uses K-means to update its boundaries. On\nthe five-language ZeroSpeech benchmarks, both approaches achieve comparable\nstate-of-the-art results, with the bottom-up system being nearly five times\nfaster. Through detailed analyses, we show that the top-down influence of\nES-KMeans can be beneficial (depending on factors like the candidate\nboundaries), but in many cases the simple bottom-up method performs just as\nwell. For both methods, we show that the clustering step is a limiting factor.\nTherefore, we recommend that future work focus on improved clustering\ntechniques and learning more discriminative word-like representations. Project\ncode repository: https://github.com/s-malan/prom-seg-clus.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u672a\u6807\u8bb0\u7684\u8bed\u97f3\u5206\u5272\u4e3a\u7c7b\u4f3c\u5355\u8bcd\u7684\u5355\u5143\u5e76\u805a\u7c7b\u5f62\u6210\u8bcd\u5178\u3002\u6bd4\u8f83\u4e86\u81ea\u5e95\u5411\u4e0a\u548c\u81ea\u9876\u5411\u4e0b\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e24\u8005\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u81ea\u5e95\u5411\u4e0a\u65b9\u6cd5\u66f4\u5feb\u3002\u805a\u7c7b\u6b65\u9aa4\u662f\u6027\u80fd\u74f6\u9888\uff0c\u5efa\u8bae\u672a\u6765\u7814\u7a76\u6539\u8fdb\u805a\u7c7b\u6280\u672f\u548c\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u8868\u793a\u3002", "motivation": "\u63a2\u7d22\u81ea\u9876\u5411\u4e0b\u4fe1\u606f\u662f\u5426\u5bf9\u8bed\u97f3\u5206\u5272\u6539\u8fdb\u5fc5\u8981\uff0c\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\u7684\u6548\u679c\u3002", "method": "\u81ea\u5e95\u5411\u4e0a\u65b9\u6cd5\u57fa\u4e8e\u76f8\u90bb\u81ea\u76d1\u7763\u7279\u5f81\u7684\u5dee\u5f02\u9884\u6d4b\u8fb9\u754c\u5e76\u805a\u7c7b\uff1b\u81ea\u9876\u5411\u4e0b\u65b9\u6cd5\u4f7f\u7528ES-KMeans\u52a8\u6001\u89c4\u5212\u8fed\u4ee3\u66f4\u65b0\u8fb9\u754c\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728ZeroSpeech\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u76f8\u5f53\uff0c\u81ea\u5e95\u5411\u4e0a\u65b9\u6cd5\u5feb\u4e94\u500d\u3002\u805a\u7c7b\u6b65\u9aa4\u662f\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "\u81ea\u9876\u5411\u4e0b\u65b9\u6cd5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6709\u76ca\uff0c\u4f46\u81ea\u5e95\u5411\u4e0a\u65b9\u6cd5\u901a\u5e38\u8db3\u591f\u3002\u672a\u6765\u5e94\u6539\u8fdb\u805a\u7c7b\u6280\u672f\u548c\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2507.18943", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18943", "abs": "https://arxiv.org/abs/2507.18943", "authors": ["Abishek Shrestha", "Damith Herath", "Angie Fearon", "Maryam Ghahramani"], "title": "Assessing the Reliability and Validity of a Balance Mat for Measuring Postural Stability: A Combined Robot-Human Approach", "comment": null, "summary": "Postural sway assessment is important for detecting balance problems and\nidentifying people at risk of falls. Force plates (FP) are considered the gold\nstandard postural sway assessment method in laboratory conditions, but their\nlack of portability and requirement of high-level expertise limit their\nwidespread usage. This study evaluates the reliability and validity of a novel\nBalance Mat (BM) device, a low-cost portable alternative that uses optical\nfibre technology. The research includes two studies: a robot study and a human\nstudy. In the robot study, a UR10 robotic arm was used to obtain controlled\nsway patterns to assess the reliability and sensitivity of the BM. In the human\nstudy, 51 healthy young participants performed balance tasks on the BM in\ncombination with an FP to evaluate the BM's validity. Sway metrics such as sway\nmean, sway absolute mean, sway root mean square (RMS), sway path, sway range,\nand sway velocity were calculated from both BM and FP and compared. Reliability\nwas evaluated using the intra-class correlation coefficient (ICC), where values\ngreater than 0.9 were considered excellent and values between 0.75 and 0.9 were\nconsidered good. Results from the robot study demonstrated good to excellent\nICC values in both single and double-leg stances. The human study showed\nmoderate to strong correlations for sway path and range. Using Bland-Altman\nplots for agreement analysis revealed proportional bias between the BM and the\nFP where the BM overestimated sway metrics compared to the FP. Calibration was\nused to improve the agreement between the devices. The device demonstrated\nconsistent sway measurement across varied stance conditions, establishing both\nreliability and validity following appropriate calibration.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u65b0\u578b\u4f4e\u6210\u672c\u4fbf\u643a\u5f0f\u5e73\u8861\u57ab\uff08BM\uff09\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u5b9e\u9a8c\u548c\u4eba\u4f53\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u529b\u677f\uff08FP\uff09\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "motivation": "\u529b\u677f\uff08FP\uff09\u662f\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e0b\u8bc4\u4f30\u59ff\u52bf\u6447\u6446\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u7f3a\u4e4f\u4fbf\u643a\u6027\u548c\u9ad8\u4e13\u4e1a\u6027\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1BM\u4f5c\u4e3a\u4fbf\u643a\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u673a\u5668\u4eba\u5b9e\u9a8c\u4f7f\u7528UR10\u673a\u68b0\u81c2\u751f\u6210\u53d7\u63a7\u6447\u6446\u6a21\u5f0f\u8bc4\u4f30BM\u7684\u53ef\u9760\u6027\u548c\u7075\u654f\u5ea6\uff1b\u4eba\u4f53\u5b9e\u9a8c\u8ba951\u540d\u5065\u5eb7\u5e74\u8f7b\u53c2\u4e0e\u8005\u5728BM\u548cFP\u4e0a\u5b8c\u6210\u5e73\u8861\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e24\u8005\u7684\u6447\u6446\u6307\u6807\u3002", "result": "\u673a\u5668\u4eba\u5b9e\u9a8c\u663e\u793aBM\u5728\u5355\u817f\u548c\u53cc\u817f\u7ad9\u7acb\u65f6\u53ef\u9760\u6027\u826f\u597d\uff08ICC>0.75\uff09\u3002\u4eba\u4f53\u5b9e\u9a8c\u4e2d\uff0cBM\u4e0eFP\u7684\u6447\u6446\u8def\u5f84\u548c\u8303\u56f4\u76f8\u5173\u6027\u4e2d\u7b49\u81f3\u5f3a\uff0c\u4f46BM\u503e\u5411\u4e8e\u9ad8\u4f30\u6307\u6807\uff0c\u9700\u6821\u51c6\u6539\u8fdb\u4e00\u81f4\u6027\u3002", "conclusion": "BM\u5728\u9002\u5f53\u6821\u51c6\u540e\u8868\u73b0\u51fa\u53ef\u9760\u7684\u6447\u6446\u6d4b\u91cf\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3aFP\u4fbf\u643a\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.19062", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19062", "abs": "https://arxiv.org/abs/2507.19062", "authors": ["Zhaoxi Mu", "Rilin Chen", "Andong Li", "Meng Yu", "Xinyu Yang", "Dong Yu"], "title": "From Continuous to Discrete: Cross-Domain Collaborative General Speech Enhancement via Hierarchical Language Models", "comment": "ACMMM 2025", "summary": "This paper introduces OmniGSE, a novel general speech enhancement (GSE)\nframework designed to mitigate the diverse distortions that speech signals\nencounter in real-world scenarios. These distortions include background noise,\nreverberation, bandwidth limitations, signal clipping, and network packet loss.\nExisting methods typically focus on optimizing for a single type of distortion,\noften struggling to effectively handle the simultaneous presence of multiple\ndistortions in complex scenarios. OmniGSE bridges this gap by integrating the\nstrengths of discriminative and generative approaches through a two-stage\narchitecture that enables cross-domain collaborative optimization. In the first\nstage, continuous features are enhanced using a lightweight channel-split\nNAC-RoFormer. In the second stage, discrete tokens are generated to reconstruct\nhigh-quality speech through language models. Specifically, we designed a\nhierarchical language model structure consisting of a RootLM and multiple\nBranchLMs. The RootLM models general acoustic features across codebook layers,\nwhile the BranchLMs explicitly capture the progressive relationships between\ndifferent codebook levels. Experimental results demonstrate that OmniGSE\nsurpasses existing models across multiple benchmarks, particularly excelling in\nscenarios involving compound distortions. These findings underscore the\nframework's potential for robust and versatile speech enhancement in real-world\napplications.", "AI": {"tldr": "OmniGSE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u901a\u7528\u8bed\u97f3\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u7ed3\u5408\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u6709\u6548\u5904\u7406\u591a\u79cd\u8bed\u97f3\u5931\u771f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00\u5931\u771f\u4f18\u5316\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u573a\u666f\u4e2d\u7684\u591a\u91cd\u5931\u771f\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u8f7b\u91cf\u7ea7NAC-RoFormer\u589e\u5f3a\u8fde\u7eed\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5206\u5c42\u8bed\u8a00\u6a21\u578b\uff08RootLM\u548cBranchLMs\uff09\u751f\u6210\u79bb\u6563\u6807\u8bb0\u4ee5\u91cd\u5efa\u9ad8\u8d28\u91cf\u8bed\u97f3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u5408\u5931\u771f\u573a\u666f\u4e0b\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "OmniGSE\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u7a33\u5065\u4e14\u591a\u529f\u80fd\u8bed\u97f3\u589e\u5f3a\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.19208", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19208", "abs": "https://arxiv.org/abs/2507.19208", "authors": ["Robert Metzger", "Mattes Ohlenbusch", "Christian Rollwage", "Simon Doclo"], "title": "Comparison of Knowledge Distillation Methods for Low-complexity Multi-microphone Speech Enhancement using the FT-JNF Architecture", "comment": "Accepted at the ITG Conference on Speech Communication 2025 in Berlin", "summary": "Multi-microphone speech enhancement using deep neural networks (DNNs) has\nsignificantly progressed in recent years. However, many proposed DNN-based\nspeech enhancement algorithms cannot be implemented on devices with limited\nhardware resources. Only lowering the complexity of such systems by reducing\nthe number of parameters often results in worse performance. Knowledge\nDistillation (KD) is a promising approach for reducing DNN model size while\npreserving performance. In this paper, we consider the recently proposed\nFrequency-Time Joint Non-linear Filter (FT-JNF) architecture and investigate\nseveral KD methods to train smaller (student) models from a large pre-trained\n(teacher) model. Five KD methods are evaluated using direct output matching,\nthe self-similarity of intermediate layers, and fused multi-layer losses.\nExperimental results on a simulated dataset using a compact array with five\nmicrophones show that three KD methods substantially improve the performance of\nstudent models compared to training without KD. A student model with only 25%\nof the teacher model's parameters achieves comparable PESQ scores at 0 dB SNR.\nFurthermore, a reduction of up to 96% in model size can be achieved with only a\nminimal decrease in PESQ scores.", "AI": {"tldr": "\u591a\u9ea6\u514b\u98ce\u8bed\u97f3\u589e\u5f3a\u7684DNN\u65b9\u6cd5\u8fd1\u5e74\u6765\u8fdb\u5c55\u663e\u8457\uff0c\u4f46\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u96be\u4ee5\u5b9e\u73b0\u3002\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u53ef\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u5e76\u4fdd\u6301\u6027\u80fd\u3002\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cdKD\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u4e09\u79cd\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c25%\u53c2\u6570\u7684\u6a21\u578b\u57280 dB SNR\u4e0bPESQ\u5206\u6570\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3DNN\u8bed\u97f3\u589e\u5f3a\u7b97\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u73b0\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9891\u7387-\u65f6\u95f4\u8054\u5408\u975e\u7ebf\u6027\u6ee4\u6ce2\u5668\uff08FT-JNF\uff09\u67b6\u6784\uff0c\u8bc4\u4f30\u4e94\u79cd\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5305\u62ec\u76f4\u63a5\u8f93\u51fa\u5339\u914d\u3001\u4e2d\u95f4\u5c42\u81ea\u76f8\u4f3c\u6027\u548c\u878d\u5408\u591a\u5c42\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e09\u79cdKD\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\uff0c25%\u53c2\u6570\u7684\u6a21\u578b\u57280 dB SNR\u4e0bPESQ\u5206\u6570\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\uff0c\u6a21\u578b\u5c3a\u5bf8\u6700\u591a\u53ef\u51cf\u5c1196%\u4e14PESQ\u5206\u6570\u4e0b\u964d\u6781\u5c0f\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u662f\u51cf\u5c0fDNN\u6a21\u578b\u5c3a\u5bf8\u5e76\u4fdd\u6301\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002"}}
{"id": "2507.18980", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18980", "abs": "https://arxiv.org/abs/2507.18980", "authors": ["Bin Wang", "Jun Fang", "Yue Xiao", "Martin Haardt"], "title": "Max-Min Beamforming for Large-Scale Cell-Free Massive MIMO: A Randomized ADMM Algorithm", "comment": null, "summary": "We consider the problem of max-min beamforming (MMB) for cell-free massive\nmulti-input multi-output (MIMO) systems, where the objective is to maximize the\nminimum achievable rate among all users. Existing MMB methods are mainly based\non deterministic optimization methods, which are computationally inefficient\nwhen the problem size grows large. To address this issue, we, in this paper,\npropose a randomized alternating direction method of multiplier (ADMM)\nalgorithm for large-scale MMB problems. We first propose a novel formulation\nthat transforms the highly challenging feasibility-checking problem into a\nlinearly constrained optimization problem. An efficient randomized ADMM is then\ndeveloped for solving the linearly constrained problem. Unlike standard ADMM,\nrandomized ADMM only needs to solve a small number of subproblems at each\niteration to ensure convergence, thus achieving a substantial complexity\nreduction. Our theoretical analysis reveals that the proposed algorithm\nexhibits an O(1/\\bar{t}) convergence rate (\\bar{t} represents the number of\niterations), which is on the same order as its deterministic counterpart.\nNumerical results show that the proposed algorithm offers a significant\ncomplexity advantage over existing methods in solving the MMB problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673aADMM\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u6700\u5927-\u6700\u5c0f\u6ce2\u675f\u6210\u5f62\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u786e\u5b9a\u6027\u4f18\u5316\u65b9\u6cd5\u5728\u95ee\u9898\u89c4\u6a21\u589e\u5927\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u53ef\u884c\u6027\u68c0\u67e5\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u968f\u673aADMM\u7b97\u6cd5\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u9700\u89e3\u51b3\u5c11\u91cf\u5b50\u95ee\u9898\u3002", "result": "\u7b97\u6cd5\u5177\u6709O(1/\\bar{t})\u6536\u655b\u901f\u5ea6\uff0c\u6570\u503c\u7ed3\u679c\u663e\u793a\u5176\u590d\u6742\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u968f\u673aADMM\u7b97\u6cd5\u4e3a\u5927\u89c4\u6a21MMB\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19202", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19202", "abs": "https://arxiv.org/abs/2507.19202", "authors": ["Nao Tokui", "Tom Baker"], "title": "Latent Granular Resynthesis using Neural Audio Codecs", "comment": "Accepted at ISMIR 2025 Late Breaking Demos", "summary": "We introduce a novel technique for creative audio resynthesis that operates\nby reworking the concept of granular synthesis at the latent vector level. Our\napproach creates a \"granular codebook\" by encoding a source audio corpus into\nlatent vector segments, then matches each latent grain of a target audio signal\nto its closest counterpart in the codebook. The resulting hybrid sequence is\ndecoded to produce audio that preserves the target's temporal structure while\nadopting the source's timbral characteristics. This technique requires no model\ntraining, works with diverse audio materials, and naturally avoids the\ndiscontinuities typical of traditional concatenative synthesis through the\ncodec's implicit interpolation during decoding. We include supplementary\nmaterial at https://github.com/naotokui/latentgranular/ , as well as a\nproof-of-concept implementation to allow users to experiment with their own\nsounds at https://huggingface.co/spaces/naotokui/latentgranular .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u5411\u91cf\u7c92\u5ea6\u7684\u97f3\u9891\u91cd\u5408\u6210\u6280\u672f\uff0c\u901a\u8fc7\u6784\u5efa\u201c\u7c92\u5ea6\u7801\u672c\u201d\u5339\u914d\u76ee\u6807\u97f3\u9891\u7684\u6f5c\u5728\u9897\u7c92\uff0c\u5b9e\u73b0\u4fdd\u7559\u76ee\u6807\u65f6\u95f4\u7ed3\u6784\u7684\u540c\u65f6\u878d\u5408\u6e90\u97f3\u8272\u7279\u5f81\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u62fc\u63a5\u5408\u6210\u4e2d\u7684\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u540c\u65f6\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u591a\u6837\u97f3\u9891\u6750\u6599\u3002", "method": "\u5c06\u6e90\u97f3\u9891\u7f16\u7801\u4e3a\u6f5c\u5728\u5411\u91cf\u6bb5\u6784\u5efa\u7801\u672c\uff0c\u5339\u914d\u76ee\u6807\u97f3\u9891\u7684\u6f5c\u5728\u9897\u7c92\u5e76\u89e3\u7801\u751f\u6210\u6df7\u5408\u97f3\u9891\u3002", "result": "\u751f\u6210\u7684\u97f3\u9891\u4fdd\u7559\u4e86\u76ee\u6807\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u540c\u65f6\u5177\u6709\u6e90\u97f3\u8272\u7684\u7279\u5f81\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8fde\u7eed\u6027\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u97f3\u9891\u91cd\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u97f3\u9891\u6750\u6599\u3002"}}
{"id": "2507.19369", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.19369", "abs": "https://arxiv.org/abs/2507.19369", "authors": ["Yoav Ellinson", "Sharon Gannot"], "title": "Binaural Target Speaker Extraction using HRTFs and a Complex-Valued Neural Network", "comment": null, "summary": "In this work, we aim to imitate the human ability to selectively attend to a\nsingle speaker, even in the presence of multiple simultaneous talkers. We\npropose a novel approach for binaural target speaker extraction that leverages\nthe listener's Head-Related Transfer Function (HRTF) to isolate the desired\nspeaker. Notably, our method does not rely on speaker embeddings, making it\nspeaker-independent and enabling strong generalization across multiple speech\ndatasets in different languages.\n  We employ a fully complex-valued neural network that operates directly on the\ncomplex-valued Short-Time Fourier Transform (STFT) of the mixed audio signals.\nThis deviates from conventional approaches that use spectrograms or treat the\nreal and imaginary components of the STFT as separate real-valued inputs.\n  We first evaluate the method in an anechoic, noise-free scenario, where it\ndemonstrates excellent extraction performance while effectively preserving the\nbinaural cues of the target signal. We then test a modified variant under mild\nreverberation conditions. This version remains robust in reverberant\nenvironments, maintaining speech clarity, preserving source directionality, and\nsimultaneously reducing reverberation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528HRTF\u7684\u53cc\u8033\u76ee\u6807\u8bf4\u8bdd\u4eba\u63d0\u53d6\u65b9\u6cd5\uff0c\u65e0\u9700\u8bf4\u8bdd\u4eba\u5d4c\u5165\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u5728\u591a\u4eba\u540c\u65f6\u8bf4\u8bdd\u65f6\u9009\u62e9\u6027\u5173\u6ce8\u5355\u4e00\u8bf4\u8bdd\u4eba\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5168\u590d\u6570\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u5904\u7406\u590d\u6570STFT\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u5206\u91cf\u5206\u79bb\u5904\u7406\u3002", "result": "\u5728\u65e0\u566a\u58f0\u548c\u8f7b\u5ea6\u6df7\u54cd\u73af\u5883\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4fdd\u6301\u76ee\u6807\u4fe1\u53f7\u7684\u53cc\u8033\u7ebf\u7d22\u548c\u65b9\u5411\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u80fd\u6709\u6548\u51cf\u5c11\u6df7\u54cd\u3002"}}
{"id": "2507.19149", "categories": ["eess.SP", "I.2.6; C.2.1; C.2.3; C.4"], "pdf": "https://arxiv.org/pdf/2507.19149", "abs": "https://arxiv.org/abs/2507.19149", "authors": ["Helena Serpi", "Christina", "Politi"], "title": "Machine Learning based Radio Environment Map Estimation for Indoor Visible Light Communication", "comment": "10 pages, 10 figures", "summary": "An innovative method for radio map estimation in optical wireless\ncommunications is proposed that is based on Machine Learning rather than\nsimulation techniques. Multi-Layer Perceptron (MLP) representation of indoor\nVisible Light Communication (VLC) systems is suggested, and signal propagation\nis estimated. The simulation and performance predictions are accurate, fast and\nrequire a reduced set of training sample size with respect to other\ncounterparts, making this solution very suitable for real time estimation of an\nindoor VLC system. It is shown that by tweaking MLP parameters, such as sample\nsize, number of epochs and batch size, one can balance the desired level of\ninference accuracy with training time and optimize the model's performance to\nmeet real-time requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5149\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u65e0\u7ebf\u7535\u5730\u56fe\u4f30\u8ba1\uff0c\u53d6\u4ee3\u4e86\u4f20\u7edf\u7684\u4eff\u771f\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u6280\u672f\u8017\u65f6\u4e14\u590d\u6742\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5feb\u901f\u3001\u51c6\u786e\u4e14\u9002\u7528\u4e8e\u5b9e\u65f6\u4f30\u8ba1\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u5efa\u6a21\u5ba4\u5185\u53ef\u89c1\u5149\u901a\u4fe1\uff08VLC\uff09\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u8c03\u6574MLP\u53c2\u6570\uff08\u5982\u6837\u672c\u91cf\u3001\u8bad\u7ec3\u5468\u671f\u548c\u6279\u91cf\u5927\u5c0f\uff09\u4f18\u5316\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u6027\u80fd\u9884\u6d4b\u4e0a\u8868\u73b0\u51c6\u786e\u3001\u5feb\u901f\uff0c\u4e14\u6240\u9700\u8bad\u7ec3\u6837\u672c\u91cf\u8f83\u5c11\uff0c\u9002\u5408\u5b9e\u65f6\u4f30\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574MLP\u53c2\u6570\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u7cbe\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f18\u5316\u6a21\u578b\u6027\u80fd\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u3002"}}
{"id": "2507.19225", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19225", "abs": "https://arxiv.org/abs/2507.19225", "authors": ["Fang Kang", "Yin Cao", "Haoyu Chen"], "title": "Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation", "comment": null, "summary": "Recent studies in speech-driven talking face generation achieve promising\nresults, but their reliance on fixed-driven speech limits further applications\n(e.g., face-voice mismatch). Thus, we extend the task to a more challenging\nsetting: given a face image and text to speak, generating both talking face\nanimation and its corresponding speeches. Accordingly, we propose a novel\nframework, Face2VoiceSync, with several novel contributions: 1) Voice-Face\nAlignment, ensuring generated voices match facial appearance; 2) Diversity \\&\nManipulation, enabling generated voice control over paralinguistic features\nspace; 3) Efficient Training, using a lightweight VAE to bridge visual and\naudio large-pretrained models, with significantly fewer trainable parameters\nthan existing methods; 4) New Evaluation Metric, fairly assessing the diversity\nand identity consistency. Experiments show Face2VoiceSync achieves both visual\nand audio state-of-the-art performances on a single 40GB GPU.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFace2VoiceSync\u6846\u67b6\uff0c\u89e3\u51b3\u8bed\u97f3\u9a71\u52a8\u7684\u4eba\u8138\u52a8\u753b\u751f\u6210\u4e2d\u7684\u56fa\u5b9a\u8bed\u97f3\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u4eba\u8138\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u540c\u6b65\u7684\u8bed\u97f3\u548c\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u9a71\u52a8\u7684\u4eba\u8138\u52a8\u753b\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u8bed\u97f3\uff0c\u9650\u5236\u4e86\u5e94\u7528\uff08\u5982\u4eba\u8138-\u8bed\u97f3\u4e0d\u5339\u914d\uff09\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u6269\u5c55\u4efb\u52a1\u81f3\u66f4\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\uff1a\u4ece\u4eba\u8138\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u540c\u6b65\u7684\u8bed\u97f3\u548c\u52a8\u753b\u3002", "method": "\u63d0\u51faFace2VoiceSync\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u8bed\u97f3-\u4eba\u8138\u5bf9\u9f50\uff1b2) \u591a\u6837\u6027\u4e0e\u64cd\u63a7\uff1b3) \u9ad8\u6548\u8bad\u7ec3\uff1b4) \u65b0\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFace2VoiceSync\u5728\u89c6\u89c9\u548c\u97f3\u9891\u6027\u80fd\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u4ec5\u9700\u5355\u4e2a40GB GPU\u3002", "conclusion": "Face2VoiceSync\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u97f3\u9a71\u52a8\u4eba\u8138\u52a8\u753b\u751f\u6210\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u591a\u6837\u5316\u7684\u8bed\u97f3\u548c\u52a8\u753b\u540c\u6b65\u751f\u6210\u3002"}}
{"id": "2507.19173", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.19173", "abs": "https://arxiv.org/abs/2507.19173", "authors": ["Lorenzo Cazzella", "Francesco Linsalata", "Damiano Badini", "Matteo Matteucci", "Maurizio Magarini", "Umberto Spagnolini"], "title": "High-Fidelity RF Mapping: Assessing Environmental Modeling in 6G Network Digital Twins", "comment": null, "summary": "The design of accurate Digital Twins (DTs) of electromagnetic environments\nstrictly depends on the fidelity of the underlying environmental modeling.\nEvaluating the differences among diverse levels of modeling accuracy is key to\ndetermine the relevance of the model features towards both efficient and\naccurate DT simulations. In this paper, we propose two metrics, the Hausdorff\nray tracing (HRT) and chamfer ray tracing (CRT) distances, to consistently\ncompare the temporal, angular and power features between two ray tracing\nsimulations performed on 3D scenarios featured by environmental changes. To\nevaluate the introduced metrics, we considered a high-fidelity digital twin\nmodel of an area of Milan, Italy and we enriched it with two different types of\nenvironmental changes: (i) the inclusion of parked vehicles meshes, and (ii)\nthe segmentation of the buildings facade faces to separate the windows mesh\ncomponents from the rest of the building. We performed grid-based and vehicular\nray tracing simulations at 28 GHz carrier frequency on the obtained scenarios\nintegrating the NVIDIA Sionna RT ray tracing simulator with the SUMO vehicular\ntraffic simulator. Both the HRT and CRT metrics highlighted the areas of the\nscenarios where the simulated radio propagation features differ owing to the\nintroduced mesh integrations, while the vehicular ray tracing simulations\nallowed to uncover the distance patterns arising along realistic vehicular\ntrajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5ea6\u91cf\u65b9\u6cd5\uff08HRT\u548cCRT\uff09\uff0c\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u73af\u5883\u5efa\u6a21\u7cbe\u5ea6\u4e0b\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u7684\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u7c73\u5170\u7684\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8bc4\u4f30\u73af\u5883\u5efa\u6a21\u7cbe\u5ea6\u5bf9\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u6a21\u62df\u7684\u5f71\u54cd\uff0c\u4ee5\u786e\u5b9a\u6a21\u578b\u7279\u5f81\u5bf9\u9ad8\u6548\u548c\u51c6\u786e\u6a21\u62df\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faHRT\u548cCRT\u4e24\u79cd\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408NVIDIA Sionna RT\u548cSUMO\u6a21\u62df\u5668\uff0c\u572828 GHz\u9891\u7387\u4e0b\u8fdb\u884c\u7f51\u683c\u548c\u8f66\u8f86\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u3002", "result": "HRT\u548cCRT\u6210\u529f\u8bc6\u522b\u4e86\u56e0\u73af\u5883\u53d8\u5316\u5bfc\u81f4\u7684\u6a21\u62df\u5dee\u5f02\uff0c\u8f66\u8f86\u6a21\u62df\u63ed\u793a\u4e86\u6cbf\u771f\u5b9e\u8f68\u8ff9\u7684\u8ddd\u79bb\u6a21\u5f0f\u3002", "conclusion": "HRT\u548cCRT\u662f\u8bc4\u4f30\u73af\u5883\u5efa\u6a21\u7cbe\u5ea6\u5bf9DT\u6a21\u62df\u5f71\u54cd\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.19308", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19308", "abs": "https://arxiv.org/abs/2507.19308", "authors": ["Lorenzo Concina", "Jordi Luque", "Alessio Brutti", "Marco Matassoni", "Yuchen Zhang"], "title": "The Eloquence team submission for task 1 of MLC-SLM challenge", "comment": "Technical Report for MLC-SLM Challenge of Interspeech2025", "summary": "In this paper, we present our studies and experiments carried out for the\ntask 1 of the Challenge and Workshop on Multilingual Conversational Speech\nLanguage Model (MLC-SLM), which focuses on advancing multilingual\nconversational speech recognition through the development of speech language\nmodels architectures. Given the increasing relevance of real-world\nconversational data for building robust Spoken Dialogue Systems, we explore\nthree approaches to multilingual ASR. First, we conduct an evaluation of the\nofficial baseline to better understand its strengths and limitations, by\ntraining two projectors (linear and qformer) with different foundation models.\nSecond we leverage the SLAM-ASR framework to train a custom multilingual linear\nprojector. Finally we investigate the role of contrastive learning and the\nextended conversational context in enhancing the robustness of recognition.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\uff0c\u901a\u8fc7\u8bc4\u4f30\u57fa\u7ebf\u3001\u8bad\u7ec3\u81ea\u5b9a\u4e49\u591a\u8bed\u8a00\u7ebf\u6027\u6295\u5f71\u5668\u4ee5\u53ca\u63a2\u7d22\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u5c55\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u6570\u636e\u5728\u6784\u5efa\u9c81\u68d2\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u589e\u52a0\uff0c\u7814\u7a76\u591a\u8bed\u8a00ASR\u7684\u9700\u6c42\u65e5\u76ca\u7a81\u51fa\u3002", "method": "1. \u8bc4\u4f30\u5b98\u65b9\u57fa\u7ebf\u5e76\u8bad\u7ec3\u4e24\u79cd\u6295\u5f71\u5668\uff08\u7ebf\u6027\u548cqformer\uff09\uff1b2. \u4f7f\u7528SLAM-ASR\u6846\u67b6\u8bad\u7ec3\u81ea\u5b9a\u4e49\u591a\u8bed\u8a00\u7ebf\u6027\u6295\u5f71\u5668\uff1b3. \u7814\u7a76\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u5c55\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e09\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u591a\u8bed\u8a00\u5bf9\u8bdd\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.19181", "categories": ["eess.SP", "cs.DM", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19181", "abs": "https://arxiv.org/abs/2507.19181", "authors": ["Giacomo Elefante", "Gianluca Giacchi", "Michael Multerer", "Jacopo Quizi"], "title": "Bespoke multiresolution analysis of graph signals", "comment": null, "summary": "We present a novel framework for discrete multiresolution analysis of graph\nsignals. The main analytical tool is the samplet transform, originally defined\nin the Euclidean framework as a discrete wavelet-like construction, tailored to\nthe analysis of scattered data. The first contribution of this work is defining\nsamplets on graphs. To this end, we subdivide the graph into a fixed number of\npatches, embed each patch into a Euclidean space, where we construct samplets,\nand eventually pull the construction back to the graph. This ensures\northogonality, locality, and the vanishing moments property with respect to\nproperly defined polynomial spaces on graphs. Compared to classical Haar\nwavelets, this framework broadens the class of graph signals that can\nefficiently be compressed and analyzed. Along this line, we provide a\ndefinition of a class of signals that can be compressed using our construction.\nWe support our findings with different examples of signals defined on graphs\nwhose vertices lie on smooth manifolds. For efficient numerical implementation,\nwe combine heavy edge clustering, to partition the graph into meaningful\npatches, with landmark \\texttt{Isomap}, which provides low-dimensional\nembeddings for each patch. Our results demonstrate the method's robustness,\nscalability, and ability to yield sparse representations with controllable\napproximation error, significantly outperforming traditional Haar wavelet\napproaches in terms of compression efficiency and multiresolution fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u4fe1\u53f7\u7684\u591a\u5206\u8fa8\u7387\u5206\u6790\u6846\u67b6\uff0c\u5229\u7528\u6837\u672c\u53d8\u6362\uff08samplet transform\uff09\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u548c\u5206\u6790\uff0c\u4f18\u4e8e\u4f20\u7edfHaar\u5c0f\u6ce2\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfHaar\u5c0f\u6ce2\u65b9\u6cd5\u5728\u56fe\u4fe1\u53f7\u5206\u6790\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u6765\u6269\u5c55\u53ef\u538b\u7f29\u4fe1\u53f7\u7684\u8303\u56f4\u3002", "method": "\u5c06\u56fe\u5212\u5206\u4e3a\u591a\u4e2a\u5b50\u56fe\uff0c\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u6784\u5efa\u6837\u672c\u53d8\u6362\uff0c\u518d\u6620\u5c04\u56de\u56fe\u7ed3\u6784\uff0c\u7ed3\u5408\u91cd\u8fb9\u805a\u7c7b\u548cIsomap\u5d4c\u5165\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7a00\u758f\u8868\u793a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u538b\u7f29\u6548\u7387\u548c\u5206\u8fa8\u7387\u4fdd\u771f\u5ea6\u663e\u8457\u4f18\u4e8eHaar\u5c0f\u6ce2\u3002", "conclusion": "\u6837\u672c\u53d8\u6362\u6846\u67b6\u4e3a\u56fe\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5e73\u6ed1\u6d41\u5f62\u4e0a\u7684\u4fe1\u53f7\u5904\u7406\u3002"}}
{"id": "2507.19327", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19327", "abs": "https://arxiv.org/abs/2507.19327", "authors": ["Niklas Dieckow", "Katharina Ostaszewski", "Philip Heinisch", "Henriette Struckmann", "Hendrik Ranocha"], "title": "Real-time rail vehicle localisation using spatially resolved magnetic field measurements", "comment": null, "summary": "This work presents two complementary real-time rail vehicle localization\nmethods based on magnetic field measurements and a pre-recorded magnetic map.\nThe first uses a particle filter reweighted via magnetic similarity, employing\na heavy-tailed non-Gaussian kernel for enhanced stability. The second is a\nstateless sequence alignment technique that transforms real-time magnetic\nsignals into the spatial domain and matches them to the map using a similarity\nmeasure. Experiments with operational train data show that the particle filter\nachieves track-selective, sub-5-meter accuracy over 21.6 km, though its\nperformance degrades at low speeds and during cold starts. Accuracy tests were\nconstrained by the GNSS-based reference system. In contrast, the\nalignment-based method excels in cold-start scenarios, localizing within 30 m\nin 92 % of tests (100 % using top-3 matches). A hybrid approach combines both\nmethods$\\unicode{x2014}$alignment-based initialization followed by particle\nfilter tracking. Runtime analysis confirms real-time capability on\nconsumer-grade hardware. The system delivers accurate, robust localization\nsuitable for safety-critical rail applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u78c1\u573a\u6d4b\u91cf\u7684\u5b9e\u65f6\u94c1\u8def\u8f66\u8f86\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u7c92\u5b50\u6ee4\u6ce2\u548c\u5e8f\u5217\u5bf9\u9f50\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u94c1\u8def\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u51c6\u786e\u3001\u7a33\u5b9a\u7684\u5b9e\u65f6\u8f66\u8f86\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u7c92\u5b50\u6ee4\u6ce2\u901a\u8fc7\u78c1\u573a\u76f8\u4f3c\u6027\u91cd\u52a0\u6743\uff0c\u4f7f\u7528\u91cd\u5c3e\u975e\u9ad8\u65af\u6838\u589e\u5f3a\u7a33\u5b9a\u6027\uff1b2. \u65e0\u72b6\u6001\u5e8f\u5217\u5bf9\u9f50\u6280\u672f\u5c06\u5b9e\u65f6\u4fe1\u53f7\u8f6c\u6362\u4e3a\u7a7a\u95f4\u57df\u5e76\u4e0e\u5730\u56fe\u5339\u914d\u3002", "result": "\u7c92\u5b50\u6ee4\u6ce2\u572821.6\u516c\u91cc\u8303\u56f4\u5185\u5b9e\u73b05\u7c73\u5185\u7cbe\u5ea6\uff0c\u4f46\u4f4e\u901f\u548c\u51b7\u542f\u52a8\u65f6\u6027\u80fd\u4e0b\u964d\uff1b\u5bf9\u9f50\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u65f6\u8868\u73b0\u4f18\u5f02\uff0c92%\u6d4b\u8bd5\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u5c0f\u4e8e30\u7c73\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff0c\u786c\u4ef6\u8981\u6c42\u4f4e\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u94c1\u8def\u573a\u666f\u3002"}}

{"id": "2507.19546", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19546", "abs": "https://arxiv.org/abs/2507.19546", "authors": ["Yansong Du", "Yutong Deng", "Yuting Zhou", "Feiyu Jiao", "Bangyao Wang", "Zhancong Xu", "Zhaoxiang Jiang", "Xun Guan"], "title": "Multipath Interference Suppression in Indirect Time-of-Flight Imaging via a Novel Compressed Sensing Framework", "comment": "15 pages, 10 figures", "summary": "We propose a novel compressed sensing method to improve the depth\nreconstruction accuracy and multi-target separation capability of indirect\nTime-of-Flight (iToF) systems. Unlike traditional approaches that rely on\nhardware modifications, complex modulation, or cumbersome data-driven\nreconstruction, our method operates with a single modulation frequency and\nconstructs the sensing matrix using multiple phase shifts and narrow-duty-cycle\ncontinuous waves. During matrix construction, we further account for pixel-wise\nrange variation caused by lens distortion, making the sensing matrix better\naligned with actual modulation response characteristics. To enhance sparse\nrecovery, we apply K-Means clustering to the distance response dictionary and\nconstrain atom selection within each cluster during the OMP process, which\neffectively reduces the search space and improves solution stability.\nExperimental results demonstrate that the proposed method outperforms\ntraditional approaches in both reconstruction accuracy and robustness, without\nrequiring any additional hardware changes."}
{"id": "2507.19763", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19763", "abs": "https://arxiv.org/abs/2507.19763", "authors": ["Zhuoyin Dai", "Jingran Xu", "Xiaoli Xu", "Ruoguang Li", "Yong Zeng", "Jiangbin Lyu"], "title": "Coverage Probability and Average Rate Analysis of Hybrid Cellular and Cell-free Network", "comment": null, "summary": "Cell-free wireless networks deploy distributed access points (APs) to\nsimultaneously serve user equipments (UEs) across the service region and are\nregarded as one of the most promising network architectural paradigms. Despite\nrecent advances in the performance analysis and optimization of cellfree\nwireless networks, it remains an open question whether large-scale deployment\nof APs in existing wireless networks can cost-effectively achieve communication\ncapacity growth. Besides, the realization of a cell-free network is considered\nto be a gradual long-term evolutionary process in which cell-free APs will be\nincrementally introduced into existing cellular networks, and form a hybrid\ncommunication network with the existing cellular base stations (BSs). Such a\ncollaboration will bridge the gap between the established cellular network and\nthe innovative cellfree network. Therefore, hybrid cellular and cell-free\nnetworks (HCCNs) emerge as a practical and feasible solution for advancing\ncell-free network development, and it is worthwhile to further explore its\nperformance limits. This paper presents a stochastic geometry-based hybrid\ncellular and cell-free network model to analyze the distributions of signal and\ninterference and reveal their mutual coupling. Specifically, in order to\nbenefit the UEs from both the cellular BSs and the cell-free APs, a conjugate\nbeamforming design is employed, and the aggregated signal is analyzed using\nmoment matching. Then, the coverage probability of the hybrid network is\ncharacterized by deriving the Laplace transforms and their higher-order\nderivatives of interference components. Furthermore, the average achievable\nrate of the hybrid network over channel fading is derived based on the\ninterference coupling analysis."}
{"id": "2507.19785", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19785", "abs": "https://arxiv.org/abs/2507.19785", "authors": ["Gevindu Ganganath", "Pasindu Sankalpa", "Samal Punsara", "Demitha Pasindu", "Chamira U. S. Edussooriya", "Ranga Rodrigo", "Udaya S. K. P. Miriya Thanthrige"], "title": "Radar and Acoustic Sensor Fusion using a Transformer Encoder for Robust Drone Detection and Classification", "comment": "Submitted to IEEE Sensors Letters", "summary": "The use of drones in a wide range of applications is steadily increasing.\nHowever, this has also raised critical security concerns such as unauthorized\ndrone intrusions into restricted zones. Therefore, robust and accurate drone\ndetection and classification mechanisms are required despite significant\nchallenges due to small size of drones, low-altitude flight, and environmental\nnoise. In this letter, we propose a multi-modal approach combining radar and\nacoustic sensing for detecting and classifying drones. We employ radar due to\nits long-range capabilities, and robustness to different weather conditions. We\nutilize raw acoustic signals without converting them to other domains such as\nspectrograms or Mel-frequency cepstral coefficients. This enables us to use\nfewer number of parameters compared to the stateof-the-art approaches.\nFurthermore, we explore the effectiveness of the transformer encoder\narchitecture in fusing these sensors. Experimental results obtained in outdoor\nsettings verify the superior performance of the proposed approach compared to\nthe state-of-the-art methods."}
{"id": "2507.19812", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19812", "abs": "https://arxiv.org/abs/2507.19812", "authors": ["Dezhi Wang", "Chongwen Huang", "Xiaojun Yuan", "Sami Muhaidat", "Lei Liu", "Xiaoming Chen", "Zhaoyang Zhang", "Chau Yuen", "Mérouane Debbah"], "title": "Channel Estimation in Massive MIMO Systems with Orthogonal Delay-Doppler Division Multiplexing", "comment": null, "summary": "Orthogonal delay-Doppler division multiplexing~(ODDM) modulation has recently\nbeen regarded as a promising technology to provide reliable communications in\nhigh-mobility situations. Accurate and low-complexity channel estimation is one\nof the most critical challenges for massive multiple input multiple\noutput~(MIMO) ODDM systems, mainly due to the extremely large antenna arrays\nand high-mobility environments. To overcome these challenges, this paper\naddresses the issue of channel estimation in downlink massive MIMO-ODDM systems\nand proposes a low-complexity algorithm based on memory approximate message\npassing~(MAMP) to estimate the channel state information~(CSI). Specifically,\nwe first establish the effective channel model of the massive MIMO-ODDM\nsystems, where the magnitudes of the elements in the equivalent channel vector\nfollow a Bernoulli-Gaussian distribution. Further, as the number of antennas\ngrows, the elements in the equivalent coefficient matrix tend to become\ncompletely random. Leveraging these characteristics, we utilize the MAMP method\nto determine the gains, delays, and Doppler effects of the multi-path channel,\nwhile the channel angles are estimated through the discrete Fourier transform\nmethod. Finally, numerical results show that the proposed channel estimation\nalgorithm approaches the Bayesian optimal results when the number of antennas\ntends to infinity and improves the channel estimation accuracy by about 30%\ncompared with the existing algorithms in terms of the normalized mean square\nerror."}
{"id": "2507.20023", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20023", "abs": "https://arxiv.org/abs/2507.20023", "authors": ["Vikas Tokala", "Eric Grinstein", "Mike Brookes", "Simon Doclo", "Jesper Jensen", "Patrick A. Naylor"], "title": "Binaural Speech Enhancement Using Complex Convolutional Recurrent Networks", "comment": null, "summary": "From hearing aids to augmented and virtual reality devices, binaural speech\nenhancement algorithms have been established as state-of-the-art techniques to\nimprove speech intelligibility and listening comfort. In this paper, we present\nan end-to-end binaural speech enhancement method using a complex recurrent\nconvolutional network with an encoder-decoder architecture and a complex LSTM\nrecurrent block placed between the encoder and decoder. A loss function that\nfocuses on the preservation of spatial information in addition to speech\nintelligibility improvement and noise reduction is introduced. The network\nestimates individual complex ratio masks for the left and right-ear channels of\na binaural hearing device in the time-frequency domain. We show that, compared\nto other baseline algorithms, the proposed method significantly improves the\nestimated speech intelligibility and reduces the noise while preserving the\nspatial information of the binaural signals in acoustic situations with a\nsingle target speaker and isotropic noise of various types."}
{"id": "2507.19557", "categories": ["cs.SD", "eess.AS", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.19557", "abs": "https://arxiv.org/abs/2507.19557", "authors": ["Haowen Li", "Ziyi Yang", "Mou Wang", "Ee-Leng Tan", "Junwei Yeow", "Santi Peksi", "Woon-Seng Gan"], "title": "Joint Feature and Output Distillation for Low-complexity Acoustic Scene Classification", "comment": "4 pages, submitted to DCASE2025 Challenge Task 1", "summary": "This report presents a dual-level knowledge distillation framework with\nmulti-teacher guidance for low-complexity acoustic scene classification (ASC)\nin DCASE2025 Task 1. We propose a distillation strategy that jointly transfers\nboth soft logits and intermediate feature representations. Specifically, we\npre-trained PaSST and CP-ResNet models as teacher models. Logits from teachers\nare averaged to generate soft targets, while one CP-ResNet is selected for\nfeature-level distillation. This enables the compact student model (CP-Mobile)\nto capture both semantic distribution and structural information from teacher\nguidance. Experiments on the TAU Urban Acoustic Scenes 2022 Mobile dataset\n(development set) demonstrate that our submitted systems achieve up to 59.30\\%\naccuracy."}
{"id": "2507.19837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19837", "abs": "https://arxiv.org/abs/2507.19837", "authors": ["Jiacheng Wang", "Changyuan Zhao", "Zehui Xiong", "Tao Xiang", "Dusit Niyato", "Xianbin Wang", "Shiwen Mao", "Dong In Kim"], "title": "Feature Engineering for Wireless Communications and Networking: Concepts, Methodologies, and Applications", "comment": "7 pages, 5 figures", "summary": "AI-enabled wireless communications have attracted tremendous research\ninterest in recent years, particularly with the rise of novel paradigms such as\nlow-altitude integrated sensing and communication (ISAC) networks. Within these\nsystems, feature engineering plays a pivotal role by transforming raw wireless\ndata into structured representations suitable for AI models. Hence, this paper\noffers a comprehensive investigation of feature engineering techniques in\nAI-driven wireless communications. Specifically, we begin with a detailed\nanalysis of fundamental principles and methodologies of feature engineering.\nNext, we present its applications in wireless communication systems, with\nspecial emphasis on ISAC networks. Finally, we introduce a generative AI-based\nframework, which can reconstruct signal feature spectrum under malicious\nattacks in low-altitude ISAC networks. The case study shows that it can\neffectively reconstruct the signal spectrum, achieving an average structural\nsimilarity index improvement of 4%, thereby supporting downstream sensing and\ncommunication applications."}
{"id": "2507.20027", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20027", "abs": "https://arxiv.org/abs/2507.20027", "authors": ["Vikas Tokala", "Eric Grinstein", "Rory Brooks", "Mike Brookes", "Simon Doclo", "Jesper Jensen", "Patrick A. Naylor"], "title": "Binaural Localization Model for Speech in Noise", "comment": null, "summary": "Binaural acoustic source localization is important to human listeners for\nspatial awareness, communication and safety. In this paper, an end-to-end\nbinaural localization model for speech in noise is presented. A lightweight\nconvolutional recurrent network that localizes sound in the frontal azimuthal\nplane for noisy reverberant binaural signals is introduced. The model\nincorporates additive internal ear noise to represent the frequency-dependent\nhearing threshold of a typical listener. The localization performance of the\nmodel is compared with the steered response power algorithm, and the use of the\nmodel as a measure of interaural cue preservation for binaural speech\nenhancement methods is studied. A listening test was performed to compare the\nperformance of the model with human localization of speech in noisy conditions."}
{"id": "2507.19835", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.19835", "abs": "https://arxiv.org/abs/2507.19835", "authors": ["Chunshi Wang", "Hongxing Li", "Yawei Luo"], "title": "SonicGauss: Position-Aware Physical Sound Synthesis for 3D Gaussian Representations", "comment": "Accepted by ACMMM'25", "summary": "While 3D Gaussian representations (3DGS) have proven effective for modeling\nthe geometry and appearance of objects, their potential for capturing other\nphysical attributes-such as sound-remains largely unexplored. In this paper, we\npresent a novel framework dubbed SonicGauss for synthesizing impact sounds from\n3DGS representations by leveraging their inherent geometric and material\nproperties. Specifically, we integrate a diffusion-based sound synthesis model\nwith a PointTransformer-based feature extractor to infer material\ncharacteristics and spatial-acoustic correlations directly from Gaussian\nellipsoids. Our approach supports spatially varying sound responses conditioned\non impact locations and generalizes across a wide range of object categories.\nExperiments on the ObjectFolder dataset and real-world recordings demonstrate\nthat our method produces realistic, position-aware auditory feedback. The\nresults highlight the framework's robustness and generalization ability,\noffering a promising step toward bridging 3D visual representations and\ninteractive sound synthesis. Project page: https://chunshi.wang/SonicGauss"}
{"id": "2507.19910", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19910", "abs": "https://arxiv.org/abs/2507.19910", "authors": ["Jun Wu", "Weijie Yuan", "Qingqing Cheng", "Haijia Jin"], "title": "Toward Dual-Functional LAWN: Control-Aware System Design for Aerodynamics-Aided UAV Formations", "comment": null, "summary": "Integrated sensing and communication (ISAC) has emerged as a pivotal\ntechnology for advancing low-altitude wireless networks (LAWNs), serving as a\ncritical enabler for next-generation communication systems. This paper\ninvestigates the system design for energy-saving unmanned aerial vehicle (UAV)\nformations in dual-functional LAWNs, where a ground base station (GBS)\nsimultaneously wirelessly controls multiple UAV formations and performs sensing\ntasks. To enhance flight endurance, we exploit the aerodynamic upwash effects\nand propose a distributed energy-saving formation framework based on the\nadapt-then-combine (ATC) diffusion least mean square (LMS) algorithm.\nSpecifically, each UAV updates the local position estimate by invoking the LMS\nalgorithm, followed by refining it through cooperative information exchange\nwith neighbors. This enables an optimized aerodynamic structure that minimizes\nthe formation's overall energy consumption. To ensure control stability and\nfairness, we formulate a maximum linear quadratic regulator (LQR) minimization\nproblem, which is subject to both the available power budget and the required\nsensing beam pattern gain. To address this non-convex problem, we develop a\ntwo-step approach by first deriving a closed-form expression of LQR as a\nfunction of arbitrary beamformers. Subsequently, an efficient iterative\nalgorithm that integrates successive convex approximation (SCA) and\nsemidefinite relaxation (SDR) techniques is proposed to obtain a sub-optimal\ndual-functional beamforming solution. Extensive simulation results confirm that\nthe 'V'-shaped formation is the most energy-efficient configuration and\ndemonstrate the superiority of our proposed design over benchmark schemes in\nimproving control performance."}
{"id": "2507.20530", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20530", "abs": "https://arxiv.org/abs/2507.20530", "authors": ["Gyeong-Tae Lee", "Hyeonuk Nam", "Yong-Hwa Park"], "title": "Binaural Sound Event Localization and Detection based on HRTF Cues for Humanoid Robots", "comment": "Submitted to IEEE/ACM TASLP", "summary": "This paper introduces Binaural Sound Event Localization and Detection\n(BiSELD), a task that aims to jointly detect and localize multiple sound events\nusing binaural audio, inspired by the spatial hearing mechanism of humans. To\nsupport this task, we present a synthetic benchmark dataset, called the\nBinaural Set, which simulates realistic auditory scenes using measured\nhead-related transfer functions (HRTFs) and diverse sound events. To\neffectively address the BiSELD task, we propose a new input feature\nrepresentation called the Binaural Time-Frequency Feature (BTFF), which encodes\ninteraural time difference (ITD), interaural level difference (ILD), and\nhigh-frequency spectral cues (SC) from binaural signals. BTFF is composed of\neight channels, including left and right mel-spectrograms, velocity-maps,\nSC-maps, and ITD-/ILD-maps, designed to cover different spatial cues across\nfrequency bands and spatial axes. A CRNN-based model, BiSELDnet, is then\ndeveloped to learn both spectro-temporal patterns and HRTF-based localization\ncues from BTFF. Experiments on the Binaural Set show that each BTFF sub-feature\nenhances task performance: V-map improves detection, ITD-/ILD-maps enable\naccurate horizontal localization, and SC-map captures vertical spatial cues.\nThe final system achieves a SELD error of 0.110 with 87.1% F-score and\n4.4{\\deg} localization error, demonstrating the effectiveness of the proposed\nframework in mimicking human-like auditory perception."}
{"id": "2507.19991", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19991", "abs": "https://arxiv.org/abs/2507.19991", "authors": ["Hei Shing Cheung", "Boya Zhang"], "title": "Efficient Vocal-Conditioned Music Generation via Soft Alignment Attention and Latent Diffusion", "comment": "6 page, 3 figures", "summary": "We present a lightweight latent diffusion model for vocal-conditioned musical\naccompaniment generation that addresses critical limitations in existing music\nAI systems. Our approach introduces a novel soft alignment attention mechanism\nthat adaptively combines local and global temporal dependencies based on\ndiffusion timesteps, enabling efficient capture of multi-scale musical\nstructure. Operating in the compressed latent space of a pre-trained\nvariational autoencoder, the model achieves a 220 times parameter reduction\ncompared to state-of-the-art systems while delivering 52 times faster\ninference. Experimental evaluation demonstrates competitive performance with\nonly 15M parameters, outperforming OpenAI Jukebox in production quality and\ncontent unity while maintaining reasonable musical coherence. The\nultra-lightweight architecture enables real-time deployment on consumer\nhardware, making AI-assisted music creation accessible for interactive\napplications and resource-constrained environments."}
{"id": "2507.19936", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19936", "abs": "https://arxiv.org/abs/2507.19936", "authors": ["Zhongnian Li", "Chao Zheng", "Jian Xiao", "Ji Wang", "Gongpu Wang", "Ming Zeng", "Octavia A. Dobre"], "title": "Deep Learning Based Joint Channel Estimation and Positioning for Sparse XL-MIMO OFDM Systems", "comment": "5 pages,8 figures", "summary": "This paper investigates joint channel estimation and positioning in\nnear-field sparse extra-large multiple-input multiple-output (XL-MIMO)\northogonal frequency division multiplexing (OFDM) systems. To achieve\ncooperative gains between channel estimation and positioning, we propose a deep\nlearning-based two-stage framework comprising positioning and channel\nestimation. In the positioning stage, the user's coordinates are predicted and\nutilized in the channel estimation stage, thereby enhancing the accuracy of\nchannel estimation. Within this framework, we propose a U-shaped Mamba\narchitecture for channel estimation and positioning, termed as CP-Mamba. This\nnetwork integrates the strengths of the Mamba model with the structural\nadvantages of U-shaped convolutional networks, enabling effective capture of\nlocal spatial features and long-range temporal dependencies of the channel.\nNumerical simulation results demonstrate that the proposed two-stage approach\nwith CP-Mamba architecture outperforms existing baseline methods. Moreover,\nsparse arrays (SA) exhibit significantly superior performance in both channel\nestimation and positioning accuracy compared to conventional compact arrays."}
{"id": "2507.20666", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20666", "abs": "https://arxiv.org/abs/2507.20666", "authors": ["Harsh Purohit", "Tomoya Nishida", "Kota Dohi", "Takashi Endo", "Yohei Kawaguchi"], "title": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection", "comment": null, "summary": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems."}
{"id": "2507.20036", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20036", "abs": "https://arxiv.org/abs/2507.20036", "authors": ["James Taylor", "Wolfgang Mack"], "title": "Improving Audio Classification by Transitioning from Zero- to Few-Shot", "comment": "Submitted to Interspeech 2025", "summary": "State-of-the-art audio classification often employs a zero-shot approach,\nwhich involves comparing audio embeddings with embeddings from text describing\nthe respective audio class. These embeddings are usually generated by neural\nnetworks trained through contrastive learning to align audio and text\nrepresentations. Identifying the optimal text description for an audio class is\nchallenging, particularly when the class comprises a wide variety of sounds.\nThis paper examines few-shot methods designed to improve classification\naccuracy beyond the zero-shot approach. Specifically, audio embeddings are\ngrouped by class and processed to replace the inherently noisy text embeddings.\nOur results demonstrate that few-shot classification typically outperforms the\nzero-shot baseline."}
{"id": "2507.19984", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19984", "abs": "https://arxiv.org/abs/2507.19984", "authors": ["Irfan Muhammad", "Priyadarshi Mukherjee", "Wee Kiat New", "Hirley Alves", "Ioannis Krikidis", "Kai-Kit Wong"], "title": "Dependability Theory-based Statistical QoS Provisioning of Fluid Antenna Systems", "comment": null, "summary": "Fluid antenna systems (FAS) have recently emerged as a promising technology\nfor next-generation wireless networks, offering real-time spatial\nreconfiguration to enhance reliability, throughput, and energy efficiency.\nNevertheless, existing studies often overlook the temporal dynamics of channel\nfading and their implications for mission-critical operations. In this paper,\nwe propose a dependability-theoretic framework for statistical\nquality-of-service (QoS) provisioning of FAS under finite blocklength (FBL)\nconstraints. Specifically, we derive new closed-form expressions for the\nlevel-crossing rate (LCR) and average fade duration (AFD) of an $N$-port FAS\nover Nakagami-$m$ fading channels. Leveraging these second-order statistics, we\ndefine two key dependability metrics such as mission reliability and mean\ntime-to-first-failure (MTTFF), to quantify the probability of uninterrupted\noperation over a defined mission duration. We further extend the classical\neffective capacity (EC) concept to incorporate mission reliability in the FBL\nregime, yielding a mission EC (mEC). To capture energy efficiency under bursty\ntraffic and latency constraints, we also develop the mission effective energy\nefficiency (mEEE) metric and formulate its maximization as a non-convex\nfractional optimization problem. This problem is then solved via a modified\nDinkelbach's method with an embedded line search. Extensive simulations uncover\ncritical trade-offs among port count, QoS exponent, signal-to-noise ratio, and\nmission duration, offering insights for the design of ultra-reliable,\nlow-latency, and energy-efficient industrial internet-of-things (IIoT) systems."}
{"id": "2507.20926", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.20926", "abs": "https://arxiv.org/abs/2507.20926", "authors": ["Kangqi Jing", "Wenbin Zhang", "Yu Gao"], "title": "End-to-End DOA-Guided Speech Extraction in Noisy Multi-Talker Scenarios", "comment": "Accepted by INTERSPEECH 2025", "summary": "Target Speaker Extraction (TSE) plays a critical role in enhancing speech\nsignals in noisy and multi-speaker environments. This paper presents an\nend-to-end TSE model that incorporates Direction of Arrival (DOA) and beamwidth\nembeddings to extract speech from a specified spatial region centered around\nthe DOA. Our approach efficiently captures spatial and temporal features,\nenabling robust performance in highly complex scenarios with multiple\nsimultaneous speakers. Experimental results demonstrate that the proposed model\nnot only significantly enhances the target speech within the defined beamwidth\nbut also effectively suppresses interference from other directions, producing a\nclear and isolated target voice. Furthermore, the model achieves remarkable\nimprovements in downstream Automatic Speech Recognition (ASR) tasks, making it\nparticularly suitable for real-world applications."}
{"id": "2507.20052", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20052", "abs": "https://arxiv.org/abs/2507.20052", "authors": ["Nouhaila Fraihi", "Ouassim Karrakchou", "Mounir Ghogho"], "title": "Improving Deep Learning-based Respiratory Sound Analysis with Frequency Selection and Attention Mechanism", "comment": null, "summary": "Accurate classification of respiratory sounds requires deep learning models\nthat effectively capture fine-grained acoustic features and long-range temporal\ndependencies. Convolutional Neural Networks (CNNs) are well-suited for\nextracting local time-frequency patterns but are limited in modeling global\ncontext. In contrast, transformer-based models can capture long-range\ndependencies, albeit with higher computational demands. To address these\nlimitations, we propose a compact CNN-Temporal Self-Attention (CNN-TSA) network\nthat integrates lightweight self-attention into an efficient CNN backbone.\nCentral to our approach is a Frequency Band Selection (FBS) module that\nsuppresses noisy and non-informative frequency regions, substantially improving\naccuracy and reducing FLOPs by up to 50%. We also introduce age-specific models\nto enhance robustness across diverse patient groups. Evaluated on the\nSPRSound-2022/2023 and ICBHI-2017 lung sound datasets, CNN-TSA with FBS sets\nnew benchmarks on SPRSound and achieves state-of-the-art performance on ICBHI,\nall with a significantly smaller computational footprint. Furthermore,\nintegrating FBS into an existing transformer baseline yields a new record on\nICBHI, confirming FBS as an effective drop-in enhancement. These results\ndemonstrate that our framework enables reliable, real-time respiratory sound\nanalysis suitable for deployment in resource-constrained settings."}
{"id": "2507.19996", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19996", "abs": "https://arxiv.org/abs/2507.19996", "authors": ["Saeed Razavikia", "Mohammad Bokaei", "Arash Amini", "Stefano Rini", "Carlo Fischione"], "title": "DOA Estimation via Optimal Weighted Low-Rank Matrix Completion", "comment": null, "summary": "This paper presents a novel method for estimating the direction of arrival\n(DOA) for a non-uniform and sparse linear sensor array using the weighted\nlifted structure low-rank matrix completion. The proposed method uses a single\nsnapshot sample in which a single array of data is observed. The method is\nrooted in a weighted lifted-structured low-rank matrix recovery framework. The\nmethod involves four key steps: (i) lifting the antenna samples to form a\nlow-rank stature, then (ii) designing left and right weight matrices to reflect\nthe sample informativeness, (iii) estimating a noise-free uniform array output\nthrough completion of the weighted lifted samples, and (iv) obtaining the DOAs\nfrom the restored uniform linear array samples.\n  We study the complexity of steps (i) to (iii) above, where we analyze the\nrequired sample for the array interpolation of step (iii) for DOA estimation.\nWe demonstrate that the proposed choice of weight matrices achieves a\nnear-optimal sample complexity. This complexity aligns with the problem's\ndegree of freedom, equivalent to the number of DOAs adjusted for logarithmic\nfactors. Numerical evaluations show the proposed method's superiority against\nthe non-weighted counterpart and atomic norm minimization-based methods.\nNotably, our proposed method significantly improves, with approximately a 10 dB\nreduction in normalized mean-squared error over the non-weighted method at\nlow-noise conditions."}
{"id": "2507.19557", "categories": ["cs.SD", "eess.AS", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.19557", "abs": "https://arxiv.org/abs/2507.19557", "authors": ["Haowen Li", "Ziyi Yang", "Mou Wang", "Ee-Leng Tan", "Junwei Yeow", "Santi Peksi", "Woon-Seng Gan"], "title": "Joint Feature and Output Distillation for Low-complexity Acoustic Scene Classification", "comment": "4 pages, submitted to DCASE2025 Challenge Task 1", "summary": "This report presents a dual-level knowledge distillation framework with\nmulti-teacher guidance for low-complexity acoustic scene classification (ASC)\nin DCASE2025 Task 1. We propose a distillation strategy that jointly transfers\nboth soft logits and intermediate feature representations. Specifically, we\npre-trained PaSST and CP-ResNet models as teacher models. Logits from teachers\nare averaged to generate soft targets, while one CP-ResNet is selected for\nfeature-level distillation. This enables the compact student model (CP-Mobile)\nto capture both semantic distribution and structural information from teacher\nguidance. Experiments on the TAU Urban Acoustic Scenes 2022 Mobile dataset\n(development set) demonstrate that our submitted systems achieve up to 59.30\\%\naccuracy."}
{"id": "2507.20128", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20128", "abs": "https://arxiv.org/abs/2507.20128", "authors": ["Shenghua Yuan", "Xing Tang", "Jiatao Chen", "Tianming Xie", "Jing Wang", "Bing Shi"], "title": "Diffusion-based Symbolic Music Generation with Structured State Space Models", "comment": "9 pages,3figures", "summary": "Recent advancements in diffusion models have significantly improved symbolic\nmusic generation. However, most approaches rely on transformer-based\narchitectures with self-attention mechanisms, which are constrained by\nquadratic computational complexity, limiting scalability for long sequences. To\naddress this, we propose Symbolic Music Diffusion with Mamba (SMDIM), a novel\ndiffusion-based architecture integrating Structured State Space Models (SSMs)\nfor efficient global context modeling and the Mamba-FeedForward-Attention Block\n(MFA) for precise local detail preservation. The MFA Block combines the linear\ncomplexity of Mamba layers, the non-linear refinement of FeedForward layers,\nand the fine-grained precision of self-attention mechanisms, achieving a\nbalance between scalability and musical expressiveness. SMDIM achieves\nnear-linear complexity, making it highly efficient for long-sequence tasks.\nEvaluated on diverse datasets, including FolkDB, a collection of traditional\nChinese folk music that represents an underexplored domain in symbolic music\ngeneration, SMDIM outperforms state-of-the-art models in both generation\nquality and computational efficiency. Beyond symbolic music, SMDIM's\narchitectural design demonstrates adaptability to a broad range of\nlong-sequence generation tasks, offering a scalable and efficient solution for\ncoherent sequence modeling."}
{"id": "2507.20189", "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.20189", "abs": "https://arxiv.org/abs/2507.20189", "authors": ["Chengkai Wang", "Di Wu", "Yunsheng Liao", "Wenyao Zheng", "Ziyi Zeng", "Xurong Gao", "Hemmings Wu", "Zhoule Zhu", "Jie Yang", "Lihua Zhong", "Weiwei Cheng", "Yun-Hsuan Chen", "Mohamad Sawan"], "title": "NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated Methamphetamine Addiction Analysis", "comment": null, "summary": "Methamphetamine dependence poses a significant global health challenge, yet\nits assessment and the evaluation of treatments like repetitive transcranial\nmagnetic stimulation (rTMS) frequently depend on subjective self-reports, which\nmay introduce uncertainties. While objective neuroimaging modalities such as\nelectroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)\noffer alternatives, their individual limitations and the reliance on\nconventional, often hand-crafted, feature extraction can compromise the\nreliability of derived biomarkers. To overcome these limitations, we propose\nNeuroCLIP, a novel deep learning framework integrating simultaneously recorded\nEEG and fNIRS data through a progressive learning strategy. This approach\noffers a robust and trustworthy biomarker for methamphetamine addiction.\nValidation experiments show that NeuroCLIP significantly improves\ndiscriminative capabilities among the methamphetamine-dependent individuals and\nhealthy controls compared to models using either EEG or only fNIRS alone.\nFurthermore, the proposed framework facilitates objective, brain-based\nevaluation of rTMS treatment efficacy, demonstrating measurable shifts in\nneural patterns towards healthy control profiles after treatment. Critically,\nwe establish the trustworthiness of the multimodal data-driven biomarker by\nshowing its strong correlation with psychometrically validated craving scores.\nThese findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP\noffers enhanced robustness and reliability over single-modality approaches,\nproviding a valuable tool for addiction neuroscience research and potentially\nimproving clinical assessments."}
{"id": "2507.20140", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.20140", "abs": "https://arxiv.org/abs/2507.20140", "authors": ["Taesoo Kim", "Jinju Kim", "Dongchan Kim", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot Text-to-Speech", "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025), Vancouver, Canada. PMLR 267, 2025. Authors Jinju Kim and Taesoo\n  Kim contributed equally", "summary": "The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has\nenabled high-fidelity voice synthesis from minimal audio cues, raising\nsignificant privacy and ethical concerns. Despite the threats to voice privacy,\nresearch to selectively remove the knowledge to replicate unwanted individual\nvoices from pre-trained model parameters has not been explored. In this paper,\nwe address the new challenge of speaker identity unlearning for ZS-TTS systems.\nTo meet this goal, we propose the first machine unlearning frameworks for\nZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the\nmodel forgets designated speaker identities while retaining its ability to\ngenerate accurate speech for other speakers. Our proposed methods incorporate\nrandomness to prevent consistent replication of forget speakers' voices,\nassuring unlearned identities remain untraceable. Additionally, we propose a\nnew evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses\nthe model's ability to disregard prompts associated with forgotten speakers,\neffectively neutralizing its knowledge of these voices. The experiments\nconducted on the state-of-the-art model demonstrate that TGU prevents the model\nfrom replicating forget speakers' voices while maintaining high quality for\nother speakers. The demo is available at https://speechunlearn.github.io/"}
{"id": "2507.20140", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.20140", "abs": "https://arxiv.org/abs/2507.20140", "authors": ["Taesoo Kim", "Jinju Kim", "Dongchan Kim", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot Text-to-Speech", "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025), Vancouver, Canada. PMLR 267, 2025. Authors Jinju Kim and Taesoo\n  Kim contributed equally", "summary": "The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has\nenabled high-fidelity voice synthesis from minimal audio cues, raising\nsignificant privacy and ethical concerns. Despite the threats to voice privacy,\nresearch to selectively remove the knowledge to replicate unwanted individual\nvoices from pre-trained model parameters has not been explored. In this paper,\nwe address the new challenge of speaker identity unlearning for ZS-TTS systems.\nTo meet this goal, we propose the first machine unlearning frameworks for\nZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the\nmodel forgets designated speaker identities while retaining its ability to\ngenerate accurate speech for other speakers. Our proposed methods incorporate\nrandomness to prevent consistent replication of forget speakers' voices,\nassuring unlearned identities remain untraceable. Additionally, we propose a\nnew evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses\nthe model's ability to disregard prompts associated with forgotten speakers,\neffectively neutralizing its knowledge of these voices. The experiments\nconducted on the state-of-the-art model demonstrate that TGU prevents the model\nfrom replicating forget speakers' voices while maintaining high quality for\nother speakers. The demo is available at https://speechunlearn.github.io/"}
{"id": "2507.20283", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20283", "abs": "https://arxiv.org/abs/2507.20283", "authors": ["Haotian Tian", "Lixiang Lian", "Jiaqi Cao", "Sijie Ji"], "title": "Information-Preserving CSI Feedback: Invertible Networks with Endogenous Quantization and Channel Error Mitigation", "comment": null, "summary": "Deep learning has emerged as a promising solution for efficient channel state\ninformation (CSI) feedback in frequency division duplex (FDD) massive MIMO\nsystems. Conventional deep learning-based methods typically rely on a deep\nautoencoder to compress the CSI, which leads to irreversible information loss\nand degrades reconstruction accuracy. This paper introduces InvCSINet, an\ninformation-preserving CSI feedback framework based on invertible neural\nnetworks (INNs). By leveraging the bijective nature of INNs, the model ensures\ninformation-preserving compression and reconstruction with shared model\nparameters. To address practical challenges such as quantization and\nchannel-induced errors, we endogenously integrate an adaptive quantization\nmodule, a differentiable bit-channel distortion module and an information\ncompensation module into the INN architecture. This design enables the network\nto learn and compensate the information loss during CSI compression,\nquantization, and noisy transmission, thereby preserving the CSI integrity\nthroughout the feedback process. Simulation results validate the effectiveness\nof the proposed scheme, demonstrating superior CSI recovery performance and\nrobustness to practical impairments with a lightweight architecture."}
{"id": "2507.20169", "categories": ["cs.SD", "eess.AS", "I.2.7; H.5.5"], "pdf": "https://arxiv.org/pdf/2507.20169", "abs": "https://arxiv.org/abs/2507.20169", "authors": ["Shaowen Wang", "Xinyuan Chen", "Yao Xu"], "title": "Self-Improvement for Audio Large Language Model using Unlabeled Speech", "comment": "To appear in Interspeech 2025. 6 pages, 1 figure", "summary": "Recent audio LLMs have emerged rapidly, demonstrating strong generalization\nacross various speech tasks. However, given the inherent complexity of speech\nsignals, these models inevitably suffer from performance degradation in\nspecific target domains. To address this, we focus on enhancing audio LLMs in\ntarget domains without any labeled data. We propose a self-improvement method\ncalled SI-SDA, leveraging the information embedded in large-model decoding to\nevaluate the quality of generated pseudo labels and then perform domain\nadaptation based on reinforcement learning optimization. Experimental results\nshow that our method consistently and significantly improves audio LLM\nperformance, outperforming existing baselines in WER and BLEU across multiple\npublic datasets of automatic speech recognition (ASR), spoken\nquestion-answering (SQA), and speech-to-text translation (S2TT). Furthermore,\nour approach exhibits high data efficiency, underscoring its potential for\nreal-world deployment."}
{"id": "2507.20169", "categories": ["cs.SD", "eess.AS", "I.2.7; H.5.5"], "pdf": "https://arxiv.org/pdf/2507.20169", "abs": "https://arxiv.org/abs/2507.20169", "authors": ["Shaowen Wang", "Xinyuan Chen", "Yao Xu"], "title": "Self-Improvement for Audio Large Language Model using Unlabeled Speech", "comment": "To appear in Interspeech 2025. 6 pages, 1 figure", "summary": "Recent audio LLMs have emerged rapidly, demonstrating strong generalization\nacross various speech tasks. However, given the inherent complexity of speech\nsignals, these models inevitably suffer from performance degradation in\nspecific target domains. To address this, we focus on enhancing audio LLMs in\ntarget domains without any labeled data. We propose a self-improvement method\ncalled SI-SDA, leveraging the information embedded in large-model decoding to\nevaluate the quality of generated pseudo labels and then perform domain\nadaptation based on reinforcement learning optimization. Experimental results\nshow that our method consistently and significantly improves audio LLM\nperformance, outperforming existing baselines in WER and BLEU across multiple\npublic datasets of automatic speech recognition (ASR), spoken\nquestion-answering (SQA), and speech-to-text translation (S2TT). Furthermore,\nour approach exhibits high data efficiency, underscoring its potential for\nreal-world deployment."}
{"id": "2507.20392", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20392", "abs": "https://arxiv.org/abs/2507.20392", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ozgur Ozdemir", "Mani Bharathi Pandian", "Ismail Guvenc"], "title": "Reliability of Wi-Fi, LTE, and 5G-Based UAV RC Links in ISM Bands: Uplink Interference Asymmetry Analysis and HARQ Design", "comment": null, "summary": "Command and control of uncrewed aerial vehicles (UAVs) is often realized\nthrough air-to-ground (A2G) remote control (RC) links that operate in ISM\nbands. While wireless fidelity (Wi-Fi) technology is commonly used for UAV RC\nlinks, ISM-based long-term evolution (LTE) and fifth-generation (5G)\ntechnologies have also been recently considered for the same purpose. A major\nproblem for UAV RC links in the ISM bands is that other types of interference\nsources, such as legacy Wi-Fi and Bluetooth transmissions, may degrade the link\nquality. Such interference problems are a higher concern for the UAV in the air\nthan the RC unit on the ground due to the UAV being in line-of-sight (LoS) with\na larger number of interference sources. To obtain empirical evidence of the\nasymmetric interference conditions in downlink (DL) and uplink (UL), we first\nconducted a measurement campaign using a helikite platform in urban and rural\nareas at NC State University. The results from this measurement campaign show\nthat the aggregate interference can be up to 16.66 dB at higher altitudes up to\n170 m, compared with the interference observed at a ground receiver. As a\nresult of this asymmetric UL interference, lost hybrid automatic repeat request\n(HARQ) indicators (ACK/NACK) in the UL may degrade the DL throughput. To\ninvestigate this, we study various HARQ mechanisms, including HARQ Type-I with\nno combining, HARQ Type-I with chase combining, HARQ Type-III with incremental\nredundancy, and burst transmission with chase combining. To evaluate the impact\nof asymmetric UL interference on throughput performance, we consider three\nsteps of evaluation process: 1) standalone physical DL shared channel (PDSCH)\nthroughput evaluation with perfect ACK/NACK assumption; 2) standalone physical\nUL control channel (PUCCH) decoding reliability evaluation; and 3) PDSCH DL\nthroughput evaluation with asymmetric UL ACK/NACK transmission."}
{"id": "2507.20417", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.20417", "abs": "https://arxiv.org/abs/2507.20417", "authors": ["Yassine El Kheir", "Arnab Das", "Enes Erdem Erdogan", "Fabian Ritter-Guttierez", "Tim Polzehl", "Sebastian Möller"], "title": "Two Views, One Truth: Spectral and Self-Supervised Features Fusion for Robust Speech Deepfake Detection", "comment": "ACCEPTED WASPAA 2025", "summary": "Recent advances in synthetic speech have made audio deepfakes increasingly\nrealistic, posing significant security risks. Existing detection methods that\nrely on a single modality, either raw waveform embeddings or spectral based\nfeatures, are vulnerable to non spoof disturbances and often overfit to known\nforgery algorithms, resulting in poor generalization to unseen attacks. To\naddress these shortcomings, we investigate hybrid fusion frameworks that\nintegrate self supervised learning (SSL) based representations with handcrafted\nspectral descriptors (MFCC , LFCC, CQCC). By aligning and combining\ncomplementary information across modalities, these fusion approaches capture\nsubtle artifacts that single feature approaches typically overlook. We explore\nseveral fusion strategies, including simple concatenation, cross attention,\nmutual cross attention, and a learnable gating mechanism, to optimally blend\nSSL features with fine grained spectral cues. We evaluate our approach on four\nchallenging public benchmarks and report generalization performance. All fusion\nvariants consistently outperform an SSL only baseline, with the cross attention\nstrategy achieving the best generalization with a 38% relative reduction in\nequal error rate (EER). These results confirm that joint modeling of waveform\nand spectral views produces robust, domain agnostic representations for audio\ndeepfake detection."}
{"id": "2507.20417", "categories": ["cs.SD", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.20417", "abs": "https://arxiv.org/abs/2507.20417", "authors": ["Yassine El Kheir", "Arnab Das", "Enes Erdem Erdogan", "Fabian Ritter-Guttierez", "Tim Polzehl", "Sebastian Möller"], "title": "Two Views, One Truth: Spectral and Self-Supervised Features Fusion for Robust Speech Deepfake Detection", "comment": "ACCEPTED WASPAA 2025", "summary": "Recent advances in synthetic speech have made audio deepfakes increasingly\nrealistic, posing significant security risks. Existing detection methods that\nrely on a single modality, either raw waveform embeddings or spectral based\nfeatures, are vulnerable to non spoof disturbances and often overfit to known\nforgery algorithms, resulting in poor generalization to unseen attacks. To\naddress these shortcomings, we investigate hybrid fusion frameworks that\nintegrate self supervised learning (SSL) based representations with handcrafted\nspectral descriptors (MFCC , LFCC, CQCC). By aligning and combining\ncomplementary information across modalities, these fusion approaches capture\nsubtle artifacts that single feature approaches typically overlook. We explore\nseveral fusion strategies, including simple concatenation, cross attention,\nmutual cross attention, and a learnable gating mechanism, to optimally blend\nSSL features with fine grained spectral cues. We evaluate our approach on four\nchallenging public benchmarks and report generalization performance. All fusion\nvariants consistently outperform an SSL only baseline, with the cross attention\nstrategy achieving the best generalization with a 38% relative reduction in\nequal error rate (EER). These results confirm that joint modeling of waveform\nand spectral views produces robust, domain agnostic representations for audio\ndeepfake detection."}
{"id": "2507.20408", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20408", "abs": "https://arxiv.org/abs/2507.20408", "authors": ["Samiul Based Shuvo", "Taufiq Hasan"], "title": "A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification", "comment": null, "summary": "Automated analysis of lung sound auscultation is essential for monitoring\nrespiratory health, especially in regions facing a shortage of skilled\nhealthcare workers. While respiratory sound classification has been widely\nstudied in adults, its ap plication in pediatric populations, particularly in\nchildren aged <6 years, remains an underexplored area. The developmental\nchanges in pediatric lungs considerably alter the acoustic proper ties of\nrespiratory sounds, necessitating specialized classification approaches\ntailored to this age group. To address this, we propose a multistage hybrid\nCNN-Transformer framework that combines CNN-extracted features with an\nattention-based architecture to classify pediatric respiratory diseases using\nscalogram images from both full recordings and individual breath events. Our\nmodel achieved an overall score of 0.9039 in binary event classifi cation and\n0.8448 in multiclass event classification by employing class-wise focal loss to\naddress data imbalance. At the recording level, the model attained scores of\n0.720 for ternary and 0.571 for multiclass classification. These scores\noutperform the previous best models by 3.81% and 5.94%, respectively. This\napproach offers a promising solution for scalable pediatric respiratory disease\ndiagnosis, especially in resource-limited settings."}
{"id": "2507.20485", "categories": ["cs.SD", "eess.AS", "68-06", "J.2"], "pdf": "https://arxiv.org/pdf/2507.20485", "abs": "https://arxiv.org/abs/2507.20485", "authors": ["Hideki Kawahara", "Kohei Yatabe", "Ken-Ichi Sakakibara"], "title": "Sound Safeguarding for Acoustic Measurement Using Any Sounds: Tools and Applications", "comment": "2 pages, 2 figures, IEEE GCCE 2025 Demo session, Accepted", "summary": "We demonstrate tools and applications developed based on the method of \"sound\nsafeguarding,\" which enables any sound to be used for acoustic measurements. We\ndeveloped tools for preparation, interactive and real-time measurement, and\nreport generation. We extended and modified the method during its development\nbased on its application in various practical situations. We have open-sourced\nthese tools and encourage prospective users to use them to improve their\nacoustic environments."}
{"id": "2507.20485", "categories": ["cs.SD", "eess.AS", "68-06", "J.2"], "pdf": "https://arxiv.org/pdf/2507.20485", "abs": "https://arxiv.org/abs/2507.20485", "authors": ["Hideki Kawahara", "Kohei Yatabe", "Ken-Ichi Sakakibara"], "title": "Sound Safeguarding for Acoustic Measurement Using Any Sounds: Tools and Applications", "comment": "2 pages, 2 figures, IEEE GCCE 2025 Demo session, Accepted", "summary": "We demonstrate tools and applications developed based on the method of \"sound\nsafeguarding,\" which enables any sound to be used for acoustic measurements. We\ndeveloped tools for preparation, interactive and real-time measurement, and\nreport generation. We extended and modified the method during its development\nbased on its application in various practical situations. We have open-sourced\nthese tools and encourage prospective users to use them to improve their\nacoustic environments."}
{"id": "2507.20489", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20489", "abs": "https://arxiv.org/abs/2507.20489", "authors": ["Sanghyeok Kim", "Jinu Gong", "Joonhyuk Kang"], "title": "Energy-Efficient Secure Communications via Joint Optimization of UAV Trajectory and Movable-Antenna Array Beamforming", "comment": "5 pages, 2 figures", "summary": "This paper investigates the potential of unmanned aerial vehicles (UAVs)\nequipped with movable-antenna (MA) arrays to strengthen security in wireless\ncommunication systems. We propose a novel framework that jointly optimizes the\nUAV trajectory and the reconfigurable beamforming of the MA array to maximize\nsecrecy energy efficiency, while ensuring reliable communication with\nlegitimate users. By exploiting the spatial degrees of freedom enabled by the\nMA array, the system can form highly directional beams and deep nulls, thereby\nsignificantly improving physical layer security. Numerical results demonstrate\nthat the proposed approach achieves superior secrecy energy efficiency,\nattributed to the enhanced spatial flexibility provided by the movable antenna\narchitecture."}
{"id": "2507.20624", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20624", "abs": "https://arxiv.org/abs/2507.20624", "authors": ["Aogu Wada", "Tomohiko Nakamura", "Hiroshi Saruwatari"], "title": "Hyperbolic Embeddings for Order-Aware Classification of Audio Effect Chains", "comment": "7 pages, 3 figures, accepted for the 28th International Conference on\n  Digital Audio Effects (DAFx25)", "summary": "Audio effects (AFXs) are essential tools in music production, frequently\napplied in chains to shape timbre and dynamics. The order of AFXs in a chain\nplays a crucial role in determining the final sound, particularly when\nnon-linear (e.g., distortion) or time-variant (e.g., chorus) processors are\ninvolved. Despite its importance, most AFX-related studies have primarily\nfocused on estimating effect types and their parameters from a wet signal. To\naddress this gap, we formulate AFX chain recognition as the task of jointly\nestimating AFX types and their order from a wet signal. We propose a\nneural-network-based method that embeds wet signals into a hyperbolic space and\nclassifies their AFX chains. Hyperbolic space can represent tree-structured\ndata more efficiently than Euclidean space due to its exponential expansion\nproperty. Since AFX chains can be represented as trees, with AFXs as nodes and\nedges encoding effect order, hyperbolic space is well-suited for modeling the\nexponentially growing and non-commutative nature of ordered AFX combinations,\nwhere changes in effect order can result in different final sounds. Experiments\nusing guitar sounds demonstrate that, with an appropriate curvature, the\nproposed method outperforms its Euclidean counterpart. Further analysis based\non AFX type and chain length highlights the effectiveness of the proposed\nmethod in capturing AFX order."}
{"id": "2507.20587", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.20587", "abs": "https://arxiv.org/abs/2507.20587", "authors": ["Zhongyao Luo", "Hao Wu", "Zhao Ge", "Ming Tang"], "title": "Real-Time Distributed Optical Fiber Vibration Recognition via Extreme Lightweight Model and Cross-Domain Distillation", "comment": "12 pages, 8 figures", "summary": "Distributed optical fiber vibration sensing (DVS) systems offer a promising\nsolution for large-scale monitoring and intrusion event recognition. However,\ntheir practical deployment remains hindered by two major challenges:\ndegradation of recognition accuracy in dynamic conditions, and the\ncomputational bottleneck of real-time processing for mass sensing data. This\npaper presents a new solution to these challenges, through a FPGA-accelerated\nextreme lightweight model along with a newly proposed knowledge distillation\nframework. The proposed three-layer depthwise separable convolution network\ncontains only 4141 parameters, which is the most compact architecture in this\nfield to date, and achieves a maximum processing speed of 0.019 ms for each\nsample covering a 12.5 m fiber length over 0.256 s. This performance\ncorresponds to real-time processing capabilities for sensing fibers extending\nup to 168.68 km. To improve generalizability under changing environments, the\nproposed cross-domain distillation framework guided by physical priors is used\nhere to embed frequency-domain insights into the time-domain model. This allows\nfor time-frequency representation learning without increasing complexity and\nboosts recognition accuracy from 51.93% to 95.72% under unseen environmental\nconditions. The proposed methodology provides key advancements including a\nframework combining interpretable signal processing technique with deep\nlearning and a reference architecture for real-time processing and\nedge-computing in DVS systems, and more general distributed optical fiber\nsensing (DOFS) area. It mitigates the trade-off between sensing range and\nreal-time capability, bridging the gap between theoretical capabilities and\npractical deployment requirements. Furthermore, this work reveals a new\ndirection for building more efficient, robust and explainable artificial\nintelligence systems for DOFS technologies."}
{"id": "2507.20731", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20731", "abs": "https://arxiv.org/abs/2507.20731", "authors": ["Andong Li", "Tong Lei", "Zhihang Sun", "Rilin Chen", "Erwei Yin", "Xiaodong Li", "Chengshi Zheng"], "title": "Learning Neural Vocoder from Range-Null Space Decomposition", "comment": "10 pages, 7 figures, IJCAI2025", "summary": "Despite the rapid development of neural vocoders in recent years, they\nusually suffer from some intrinsic challenges like opaque modeling, and\nparameter-performance trade-off. In this study, we propose an innovative\ntime-frequency (T-F) domain-based neural vocoder to resolve the above-mentioned\nchallenges. To be specific, we bridge the connection between the classical\nsignal range-null decomposition (RND) theory and vocoder task, and the\nreconstruction of target spectrogram can be decomposed into the superimposition\nbetween the range-space and null-space, where the former is enabled by a linear\ndomain shift from the original mel-scale domain to the target linear-scale\ndomain, and the latter is instantiated via a learnable network for further\nspectral detail generation. Accordingly, we propose a novel dual-path\nframework, where the spectrum is hierarchically encoded/decoded, and the cross-\nand narrow-band modules are elaborately devised for efficient sub-band and\nsequential modeling. Comprehensive experiments are conducted on the LJSpeech\nand LibriTTS benchmarks. Quantitative and qualitative results show that while\nenjoying lightweight network parameters, the proposed approach yields\nstate-of-the-art performance among existing advanced methods. Our code and the\npretrained model weights are available at\nhttps://github.com/Andong-Li-speech/RNDVoC."}
{"id": "2507.20648", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20648", "abs": "https://arxiv.org/abs/2507.20648", "authors": ["Christos Ntemkas", "Antonios Argyriou"], "title": "RFI and Jamming Detection in Antenna Arrays with an LSTM Autoencoder", "comment": null, "summary": "Radio frequency interference (RFI) and malicious jammers are a significant\nproblem in our wireless world. Detecting RFI or jamming is typically performed\nwith model-based statistical detection or AI-empowered algorithms that use an\ninput baseband data or time-frequency representations like spectrograms. In\nthis work we depart from the previous approaches and we leverage data in\nantenna array systems. We use Fourier imaging to localize spatially the sources\nand then deploy a deep LSTM autoencoder that detects RFI and jamming as\nanomalies. Our results for different power levels of the RFI/jamming sources,\nand the signal of interest, reveal that our detector offers high performance\nwithout needing any pre-existing knowledge regarding the RFI or jamming signal."}
{"id": "2507.20880", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20880", "abs": "https://arxiv.org/abs/2507.20880", "authors": ["Renhang Liu", "Chia-Yu Hung", "Navonil Majumder", "Taylor Gautreaux", "Amir Ali Bagherzadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment", "comment": "https://github.com/declare-lab/jamify", "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes."}
{"id": "2507.20651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20651", "abs": "https://arxiv.org/abs/2507.20651", "authors": ["Jichao Zhang", "Xiao-Lei Zhang", "Kunde Yang"], "title": "Angle-distance decomposition based on deep learning for active sonar detection", "comment": null, "summary": "Underwater target detection using active sonar constitutes a critical\nresearch area in marine sciences and engineering. However, traditional signal\nprocessing methods face significant challenges in complex underwater\nenvironments due to noise, reverberation, and interference. To address these\nissues, this paper presents a deep learning-based active sonar target detection\nmethod that decomposes the detection process into separate angle and distance\nestimation tasks. Active sonar target detection employs deep learning models to\npredict target distance and angle, with the final target position determined by\nintegrating these estimates. Limited underwater acoustic data hinders effective\nmodel training, but transfer learning and simulation offer practical solutions\nto this challenge. Experimental results verify that the method achieves\neffective and robust performance under challenging conditions."}
{"id": "2507.20900", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.20900", "abs": "https://arxiv.org/abs/2507.20900", "authors": ["Yonghyun Kim", "Wayne Chi", "Anastasios N. Angelopoulos", "Wei-Lin Chiang", "Koichi Saito", "Shinji Watanabe", "Yuki Mitsufuji", "Chris Donahue"], "title": "Music Arena: Live Evaluation for Text-to-Music", "comment": null, "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org"}
{"id": "2507.20657", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20657", "abs": "https://arxiv.org/abs/2507.20657", "authors": ["Margarita Loupa", "Antonios Argyriou", "Yanwei Liu"], "title": "The micro-Doppler Attack Against AI-based Human Activity Classification from Wireless Signals", "comment": null, "summary": "A subset of Human Activity Classification (HAC) systems are based on AI\nalgorithms that use passively collected wireless signals. This paper presents\nthe micro-Doppler attack targeting HAC from wireless orthogonal frequency\ndivision multiplexing (OFDM) signals. The attack is executed by inserting\nartificial variations in a transmitted OFDM waveform to alter its micro-Doppler\nsignature when it reflects off a human target. We investigate two variants of\nour scheme that manipulate the waveform at different time scales resulting in\naltered receiver spectrograms. HAC accuracy with a deep convolutional neural\nnetwork (CNN) can be reduced to less than 10%."}
{"id": "2507.20530", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20530", "abs": "https://arxiv.org/abs/2507.20530", "authors": ["Gyeong-Tae Lee", "Hyeonuk Nam", "Yong-Hwa Park"], "title": "Binaural Sound Event Localization and Detection based on HRTF Cues for Humanoid Robots", "comment": "Submitted to IEEE/ACM TASLP", "summary": "This paper introduces Binaural Sound Event Localization and Detection\n(BiSELD), a task that aims to jointly detect and localize multiple sound events\nusing binaural audio, inspired by the spatial hearing mechanism of humans. To\nsupport this task, we present a synthetic benchmark dataset, called the\nBinaural Set, which simulates realistic auditory scenes using measured\nhead-related transfer functions (HRTFs) and diverse sound events. To\neffectively address the BiSELD task, we propose a new input feature\nrepresentation called the Binaural Time-Frequency Feature (BTFF), which encodes\ninteraural time difference (ITD), interaural level difference (ILD), and\nhigh-frequency spectral cues (SC) from binaural signals. BTFF is composed of\neight channels, including left and right mel-spectrograms, velocity-maps,\nSC-maps, and ITD-/ILD-maps, designed to cover different spatial cues across\nfrequency bands and spatial axes. A CRNN-based model, BiSELDnet, is then\ndeveloped to learn both spectro-temporal patterns and HRTF-based localization\ncues from BTFF. Experiments on the Binaural Set show that each BTFF sub-feature\nenhances task performance: V-map improves detection, ITD-/ILD-maps enable\naccurate horizontal localization, and SC-map captures vertical spatial cues.\nThe final system achieves a SELD error of 0.110 with 87.1% F-score and\n4.4{\\deg} localization error, demonstrating the effectiveness of the proposed\nframework in mimicking human-like auditory perception."}
{"id": "2507.20664", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20664", "abs": "https://arxiv.org/abs/2507.20664", "authors": ["Kohei Shimomura", "Chi-Hsuan Lee", "Takuya Sakamoto"], "title": "A Nonlinear Spectral Approach for Radar-Based Heartbeat Estimation via Autocorrelation of Higher Harmonics", "comment": "4 pages, 4 figures, 3 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study presents a nonlinear signal processing method for accurate\nradar-based heartbeat interval estimation by exploiting the periodicity of\nhigher-order harmonics inherent in heartbeat signals. Unlike conventional\napproaches that employ selective frequency filtering or track individual\nharmonics, the proposed method enhances the global periodic structure of the\nspectrum via nonlinear correlation processing. Specifically, smoothing and\nsecond-derivative operations are first applied to the radar displacement signal\nto suppress noise and accentuate higher-order heartbeat harmonics. Rather than\nisolating specific frequency components, we compute localized autocorrelations\nof the Fourier spectrum around the harmonic frequencies. The incoherent\nsummation of these autocorrelations yields a pseudo-spectrum in which the\nfundamental heartbeat periodicity is distinctly emphasized. This nonlinear\napproach mitigates the effects of respiratory harmonics and noise, enabling\nrobust interbeat interval estimation. Experiments with radar measurements from\nfive participants demonstrate that the proposed method reduces root-mean-square\nerror by 20% and improves the correlation coefficient by 0.20 relative to\nconventional techniques."}
{"id": "2507.20666", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.20666", "abs": "https://arxiv.org/abs/2507.20666", "authors": ["Harsh Purohit", "Tomoya Nishida", "Kota Dohi", "Takashi Endo", "Yohei Kawaguchi"], "title": "MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection", "comment": null, "summary": "This paper proposes a method for generating machine-type-specific anomalies\nto evaluate the relative performance of unsupervised anomalous sound detection\n(UASD) systems across different machine types, even in the absence of real\nanomaly sound data. Conventional keyword-based data augmentation methods often\nproduce unrealistic sounds due to their reliance on manually defined labels,\nlimiting scalability as machine types and anomaly patterns diversify. Advanced\naudio generative models, such as MIMII-Gen, show promise but typically depend\non anomalous training data, making them less effective when diverse anomalous\nexamples are unavailable. To address these limitations, we propose a novel\nsynthesis approach leveraging large language models (LLMs) to interpret textual\ndescriptions of faults and automatically select audio transformation functions,\nconverting normal machine sounds into diverse and plausible anomalous sounds.\nWe validate this approach by evaluating a UASD system trained only on normal\nsounds from five machine types, using both real and synthetic anomaly data.\nExperimental results reveal consistent trends in relative detection difficulty\nacross machine types between synthetic and real anomalies. This finding\nsupports our hypothesis and highlights the effectiveness of the proposed\nLLM-based synthesis approach for relative evaluation of UASD systems."}
{"id": "2507.20789", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20789", "abs": "https://arxiv.org/abs/2507.20789", "authors": ["Hung Nguyen-Kha", "Vu Nguyen Ha", "Ti Ti Nguyen", "Eva Lagunas", "Symeon Chatzinotas", "Joel Grotz"], "title": "DT-Aided Resource Management in Spectrum Sharing Integrated Satellite-Terrestrial Networks", "comment": null, "summary": "The integrated satellite-terrestrial networks (ISTNs) through spectrum\nsharing have emerged as a promising solution to improve spectral efficiency and\nmeet increasing wireless demand. However, this coexistence introduces\nsignificant challenges, including inter-system interference (ISI) and the low\nEarth orbit satellite (LSat) movements. To capture the actual environment for\nresource management, we propose a time-varying digital twin (DT)-aided\nframework for ISTNs incorporating 3D map that enables joint optimization of\nbandwidth (BW) allocation, traffic steering, and resource allocation, and aims\nto minimize congestion. The problem is formulated as a mixed-integer nonlinear\nprogramming (MINLP), addressed through a two-phase algorithm based on\nsuccessive convex approximation (SCA) and compressed sensing approaches.\nNumerical results demonstrate the proposed method's superior performance in\nqueue length minimization compared to benchmarks."}
{"id": "2507.20825", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20825", "abs": "https://arxiv.org/abs/2507.20825", "authors": ["Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "Chirp-Permuted AFDM: A New Degree of Freedom for Next-Generation Versatile Waveform Design", "comment": null, "summary": "We present a novel multicarrier waveform, termed chirp-permuted affine\nfrequency division multiplexing (CP-AFDM), which introduces a unique\nchirp-permutation domain on top of the chirp subcarriers of the conventional\nAFDM. Rigorous analysis of the signal model and waveform properties, supported\nby numerical simulations, demonstrates that the proposed CP-AFDM preserves all\ncore characteristics of affine frequency division multiplexing (AFDM) -\nincluding robustness to doubly-dispersive channels, peak-to-average power ratio\n(PAPR), and full delay-Doppler representation - while further enhancing\nambiguity function resolution and peak-to-sidelobe ratio (PSLR) in the Doppler\ndomain. These improvements establish CP-AFDM as a highly attractive candidate\nfor emerging sixth generation (6G) use cases demanding both reliability and\nsensing-awareness. Moreover, by exploiting the vast degree of freedom in the\nchirp-permutation domain, two exemplary multifunctional applications are\nintroduced: an index modulation (IM) technique over the permutation domain\nwhich achieves significant spectral efficiency gains, and a physical-layer\nsecurity scheme that ensures practically perfect security through\npermutation-based keying, without requiring additional transmit energy or\nsignaling overhead."}
{"id": "2507.20942", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20942", "abs": "https://arxiv.org/abs/2507.20942", "authors": ["Taewon Jeong", "Lucas Giroto", "Umut Utku Erdem", "Christian Karle", "Jiyeon Choi", "Thomas Zwick", "Benjamin Nuss"], "title": "Interference Analysis and Successive Interference Cancellation for Multistatic OFDM-based ISAC Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Multistatic integrated sensing and communications (ISAC) systems, which use\ndistributed transmitters and receivers, offer enhanced spatial coverage and\nsensing accuracy compared to stand-alone ISAC configurations. However, these\nsystems face challenges due to interference between co-existing ISAC nodes,\nespecially during simultaneous operation. In this paper, we analyze the impact\nof this mutual interference arising from the co-existence in a multistatic ISAC\nscenario, where a mono- and a bistatic ISAC system share the same spectral\nresources. We first classify differenct types of interference in the power\ndomain. Then, we discuss how the interference can affect both sensing and\ncommunications in terms of bit error rate (BER), error vector magnitude (EVM),\nand radar image under varied transmit power and RCS configurations through\nsimulations. Along with interfernce analysis, we propose a low-complexity\nsuccessive interference cancellation method that adaptively cancels either the\nmonostatic reflection or the bistatic line-of-sight signal based on a\nmonostatic radar image signal-to-interference-plus-noise ratio (SINR). The\nproposed framework is evaluated with both simulations and proof-of-concept\nmeasurements using an ISAC testbed with a radar echo generator for object\nemulation. The results have shown that the proposed method reduces BER and\nimproves EVM as well as radar image SINR across a wide range of SINR\nconditions. These results demonstrate that accurate component-wise cancellation\ncan be achieved with low computational overhead, making the method suitable for\npractical applications."}
{"id": "2507.20952", "categories": ["eess.SP", "94C30", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.20952", "abs": "https://arxiv.org/abs/2507.20952", "authors": ["Jimmy Fernandez Landivar", "Andrea Zanella", "Ihsane Gryech", "Sofie Pollin", "Hazem Sallouha"], "title": "Analytical Modeling of Batteryless IoT Sensors Powered by Ambient Energy Harvesting", "comment": "6 pages, 6 figures, 1 table, accepted for publication in the 36th\n  IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC 2025), Istanbul, T\\\"urkiye", "summary": "This paper presents a comprehensive mathematical model to characterize the\nenergy dynamics of batteryless IoT sensor nodes powered entirely by ambient\nenergy harvesting. The model captures both the energy harvesting and\nconsumption phases, explicitly incorporating power management tasks to enable\nprecise estimation of device behavior across diverse environmental conditions.\nThe proposed model is applicable to a wide range of IoT devices and supports\nintelligent power management units designed to maximize harvested energy under\nfluctuating environmental conditions. We validated our model against a\nprototype batteryless IoT node, conducting experiments under three distinct\nillumination scenarios. Results show a strong correlation between analytical\nand measured supercapacitor voltage profiles, confirming the proposed model's\naccuracy."}
{"id": "2507.20023", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20023", "abs": "https://arxiv.org/abs/2507.20023", "authors": ["Vikas Tokala", "Eric Grinstein", "Mike Brookes", "Simon Doclo", "Jesper Jensen", "Patrick A. Naylor"], "title": "Binaural Speech Enhancement Using Complex Convolutional Recurrent Networks", "comment": null, "summary": "From hearing aids to augmented and virtual reality devices, binaural speech\nenhancement algorithms have been established as state-of-the-art techniques to\nimprove speech intelligibility and listening comfort. In this paper, we present\nan end-to-end binaural speech enhancement method using a complex recurrent\nconvolutional network with an encoder-decoder architecture and a complex LSTM\nrecurrent block placed between the encoder and decoder. A loss function that\nfocuses on the preservation of spatial information in addition to speech\nintelligibility improvement and noise reduction is introduced. The network\nestimates individual complex ratio masks for the left and right-ear channels of\na binaural hearing device in the time-frequency domain. We show that, compared\nto other baseline algorithms, the proposed method significantly improves the\nestimated speech intelligibility and reduces the noise while preserving the\nspatial information of the binaural signals in acoustic situations with a\nsingle target speaker and isotropic noise of various types."}
{"id": "2507.20027", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.20027", "abs": "https://arxiv.org/abs/2507.20027", "authors": ["Vikas Tokala", "Eric Grinstein", "Rory Brooks", "Mike Brookes", "Simon Doclo", "Jesper Jensen", "Patrick A. Naylor"], "title": "Binaural Localization Model for Speech in Noise", "comment": null, "summary": "Binaural acoustic source localization is important to human listeners for\nspatial awareness, communication and safety. In this paper, an end-to-end\nbinaural localization model for speech in noise is presented. A lightweight\nconvolutional recurrent network that localizes sound in the frontal azimuthal\nplane for noisy reverberant binaural signals is introduced. The model\nincorporates additive internal ear noise to represent the frequency-dependent\nhearing threshold of a typical listener. The localization performance of the\nmodel is compared with the steered response power algorithm, and the use of the\nmodel as a measure of interaural cue preservation for binaural speech\nenhancement methods is studied. A listening test was performed to compare the\nperformance of the model with human localization of speech in noisy conditions."}

{"id": "2507.22141", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.22141", "abs": "https://arxiv.org/abs/2507.22141", "authors": ["Atiquzzaman Mondal", "Waheeb Tashan", "Ayat Al-Olaimat", "Hüseyin Arslan"], "title": "Efficient handover based on Near-field and Far-field RIS for seamless connectivity", "comment": "11 pages, 10 figures, IEEE Transactions on Mobile Computing", "summary": "Reconfigurable Intelligent Surfaces (RIS) is becoming a transformative\ntechnology for the upcoming 6G communication networks, providing a way for\nsmartly maneuvering the electromagnetic waves to enhance coverage and\nconnectivity. This paper presents an efficient handover (HO) management scheme\nleveraging RIS in the Fresnel region i.e., in both the near-field (NF) and\nfar-field (FF) regions to reduce signaling overhead and optimize mobility\nmanagement. For this, we analyzed the signal strength variations in the\nconsidered RIS-aided networks, considering the radiative NF and FF regions, and\nderive the probability density function (PDF) of the RIS-UE distance in the NF\nregion to quantify RIS reflection gains along the user equipment (UE)\ntrajectory. We propose a new HO algorithm incorporating several HO categories\nlike hard handover (HHO), soft handover (SHO), RIS-aided cell breathing\n(RIS-CB), and RIS-aided ping-pong avoidance (RIS-PP) strategies. The proposed\nalgorithm uses bit error rate (BER) as a key parameter to predict the\nminimization of unnecessary HOs by using RIS-aided pathways to retain\nconnectivity with the serving base station (BS), which minimizes the\nrequirement for frequent target BS searching and ultimately optimizes the HO.\nBy restricting measurement reports and HO requests, the suggested method\nimproves spectrum efficiency (SE) and energy efficiency (EE), especially in\ncrowded cellular networks. Numerical results highlight significant reductions\nin HO rates and signaling load, ensuring seamless connectivity and improved\nquality of service (QoS) in 6G systems."}
{"id": "2507.22263", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22263", "abs": "https://arxiv.org/abs/2507.22263", "authors": ["K. A. Shahriar", "E. H. Bhuiyan", "Q. Luo", "M. E. H. Chowdhury", "X. J. Zhou"], "title": "Deep Learning for Gradient and BCG Artifacts Removal in EEG During Simultaneous fMRI", "comment": "15 pages and 13 figures", "summary": "Simultaneous EEG-fMRI recording combines high temporal and spatial resolution\nfor tracking neural activity. However, its usefulness is greatly limited by\nartifacts from magnetic resonance (MR), especially gradient artifacts (GA) and\nballistocardiogram (BCG) artifacts, which interfere with the EEG signal. To\naddress this issue, we used a denoising autoencoder (DAR), a deep learning\nframework designed to reduce MR-related artifacts in EEG recordings. Using\npaired data that includes both artifact-contaminated and MR-corrected EEG from\nthe CWL EEG-fMRI dataset, DAR uses a 1D convolutional autoencoder to learn a\ndirect mapping from noisy to clear signal segments. Compared to traditional\nartifact removal methods like principal component analysis (PCA), independent\ncomponent analysis (ICA), average artifact subtraction (AAS), and wavelet\nthresholding, DAR shows better performance. It achieves a root-mean-squared\nerror (RMSE) of 0.0218 $\\pm$ 0.0152, a structural similarity index (SSIM) of\n0.8885 $\\pm$ 0.0913, and a signal-to-noise ratio (SNR) gain of 14.63 dB.\nStatistical analysis with paired t-tests confirms that these improvements are\nsignificant (p<0.001; Cohen's d>1.2). A leave-one-subject-out (LOSO)\ncross-validation protocol shows that the model generalizes well, yielding an\naverage RMSE of 0.0635 $\\pm$ 0.0110 and an SSIM of 0.6658 $\\pm$ 0.0880 across\nunseen subjects. Additionally, saliency-based visualizations demonstrate that\nDAR highlights areas with dense artifacts, which makes its decisions easier to\ninterpret. Overall, these results position DAR as a potential and\nunderstandable solution for real-time EEG artifact removal in simultaneous\nEEG-fMRI applications."}
{"id": "2507.22343", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22343", "abs": "https://arxiv.org/abs/2507.22343", "authors": ["Yifan Yu", "Shengjie Xiu", "Daniel P. Palomar"], "title": "Robust Filtering and Learning in State-Space Models: Skewness and Heavy Tails Via Asymmetric Laplace Distribution", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "State-space models are pivotal for dynamic system analysis but often struggle\nwith outlier data that deviates from Gaussian distributions, frequently\nexhibiting skewness and heavy tails. This paper introduces a robust extension\nutilizing the asymmetric Laplace distribution, specifically tailored to capture\nthese complex characteristics. We propose an efficient variational Bayes\nalgorithm and a novel single-loop parameter estimation strategy, significantly\nenhancing the efficiency of the filtering, smoothing, and parameter estimation\nprocesses. Our comprehensive experiments demonstrate that our methods provide\nconsistently robust performance across various noise settings without the need\nfor manual hyperparameter adjustments. In stark contrast, existing models\ngenerally rely on specific noise conditions and necessitate extensive manual\ntuning. Moreover, our approach uses far fewer computational resources, thereby\nvalidating the model's effectiveness and underscoring its potential for\npractical applications in fields such as robust control and financial modeling."}
{"id": "2507.22400", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.22400", "abs": "https://arxiv.org/abs/2507.22400", "authors": ["Salih Gümüsbuğa", "Ozan Alp Topal", "Özlem Tuğfe Demir"], "title": "Green One-Bit Quantized Precoding in Cell-Free Massive MIMO", "comment": "5 pages, 4 figures, to be presented at 2025 International Conference\n  on Future Communications and Networks (FCN)", "summary": "Cell-free massive MIMO (multiple-input multiple-output) is expected to be one\nof the key technologies in sixth-generation (6G) and beyond wireless\ncommunications, offering enhanced spectral efficiency for cell-edge user\nequipments by employing joint transmission and reception with a large number of\nantennas distributed throughout the region. However, high-resolution RF chains\nassociated with these antennas significantly increase power consumption. To\naddress this issue, the use of low-resolution analog-to-digital and\ndigital-to-analog converters (ADCs/DACs) has emerged as a promising approach to\nbalance power efficiency and performance in massive MIMO networks. In this\nwork, we propose a novel quantized precoding algorithm tailored for cell-free\nmassive MIMO systems, where the proposed method dynamically deactivates\nunnecessary antennas based on the structure of each symbol vector, thereby\nenhancing energy efficiency. Simulation results demonstrate that our algorithm\noutperforms existing methods such as squared-infinity norm Douglas-Rachford\nsplitting (SQUID) and regularized zero forcing (RZF), achieving superior\nperformance while effectively reducing power consumption."}
{"id": "2507.22208", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22208", "abs": "https://arxiv.org/abs/2507.22208", "authors": ["Shreyansh Pathak", "Sonu Shreshtha", "Richa Singh", "Mayank Vatsa"], "title": "Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice Biometrics", "comment": "9 pages, 2 figures, 5 tables, Accepted at IJCB 2025 (Osaka, Japan)", "summary": "The widespread adoption of voice-enabled authentication and audio biometric\nsystems have significantly increased privacy vulnerabilities associated with\nsensitive speech data. Compliance with privacy regulations such as GDPR's right\nto be forgotten and India's DPDP Act necessitates targeted and efficient\nerasure of individual-specific voice signatures from already-trained biometric\nmodels. Existing unlearning methods designed for visual data inadequately\nhandle the sequential, temporal, and high-dimensional nature of audio signals,\nleading to ineffective or incomplete speaker and accent erasure. To address\nthis, we introduce QPAudioEraser, a quantum-inspired audio unlearning\nframework. Our our-phase approach involves: (1) weight initialization using\ndestructive interference to nullify target features, (2) superposition-based\nlabel transformations that obscure class identity, (3) an\nuncertainty-maximizing quantum loss function, and (4) entanglement-inspired\nmixing of correlated weights to retain model knowledge. Comprehensive\nevaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech\nCommands, LibriSpeech, and Speech Accent Archive datasets validate\nQPAudioEraser's superior performance. The framework achieves complete erasure\nof target data (0% Forget Accuracy) while incurring minimal impact on model\nutility, with a performance degradation on retained data as low as 0.05%.\nQPAudioEraser consistently surpasses conventional baselines across\nsingle-class, multi-class, sequential, and accent-level erasure scenarios,\nestablishing the proposed approach as a robust privacy-preserving solution."}
{"id": "2507.22094", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22094", "abs": "https://arxiv.org/abs/2507.22094", "authors": ["Nicholas Mehlman", "Jean-Christophe Gagnon-Audet", "Michael Shvartsman", "Kelvin Niu", "Alexander H. Miller", "Shagun Sodhani"], "title": "Scaling and Distilling Transformer Models for sEMG", "comment": "Accepted at TMLR 2025 (https://openreview.net/forum?id=hFPWThwUiZ),\n  11 pages", "summary": "Surface electromyography (sEMG) signals offer a promising avenue for\ndeveloping innovative human-computer interfaces by providing insights into\nmuscular activity. However, the limited volume of training data and\ncomputational constraints during deployment have restricted the investigation\nof scaling up the model size for solving sEMG tasks. In this paper, we\ndemonstrate that vanilla transformer models can be effectively scaled up on\nsEMG data and yield improved cross-user performance up to 110M parameters,\nsurpassing the model size regime investigated in other sEMG research (usually\n<10M parameters). We show that >100M-parameter models can be effectively\ndistilled into models 50x smaller with minimal loss of performance (<1.5%\nabsolute). This results in efficient and expressive models suitable for complex\nreal-time sEMG tasks in real-world environments."}
{"id": "2507.22513", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22513", "abs": "https://arxiv.org/abs/2507.22513", "authors": ["Lizhou Liu", "Xiaohui Chen", "Zihan Tang", "Mengyao Ma", "Wenyi Zhang"], "title": "PINN and GNN-based RF Map Construction for Wireless Communication Systems", "comment": null, "summary": "Radio frequency (RF) map is a promising technique for capturing the\ncharacteristics of multipath signal propagation, offering critical support for\nchannel modeling, coverage analysis, and beamforming in wireless communication\nnetworks. This paper proposes a novel RF map construction method based on a\ncombination of physics-informed neural network (PINN) and graph neural network\n(GNN). The PINN incorporates physical constraints derived from electromagnetic\npropagation laws to guide the learning process, while the GNN models spatial\ncorrelations among receiver locations. By parameterizing multipath signals into\nreceived power, delay, and angle of arrival (AoA), and integrating both\nphysical priors and spatial dependencies, the proposed method achieves accurate\nprediction of multipath parameters. Experimental results demonstrate that the\nmethod enables high-precision RF map construction under sparse sampling\nconditions and delivers robust performance in both indoor and complex outdoor\nenvironments, outperforming baseline methods in terms of generalization and\naccuracy."}
{"id": "2507.22322", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22322", "abs": "https://arxiv.org/abs/2507.22322", "authors": ["Hogeon Yu"], "title": "A Two-Step Learning Framework for Enhancing Sound Event Localization and Detection", "comment": "5pages, 2figures", "summary": "Sound Event Localization and Detection (SELD) is crucial in spatial audio\nprocessing, enabling systems to detect sound events and estimate their 3D\ndirections. Existing SELD methods use single- or dual-branch architectures:\nsingle-branch models share SED and DoA representations, causing optimization\nconflicts, while dual-branch models separate tasks but limit information\nexchange. To address this, we propose a two-step learning framework. First, we\nintroduce a tracwise reordering format to maintain temporal consistency,\npreventing event reassignments across tracks. Next, we train SED and DoA\nnetworks to prevent interference and ensure task-specific feature learning.\nFinally, we effectively fuse DoA and SED features to enhance SELD performance\nwith better spatial and event representation. Experiments on the 2023 DCASE\nchallenge Task 3 dataset validate our framework, showing its ability to\novercome single- and dual-branch limitations and improve event classification\nand localization."}
{"id": "2507.22157", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22157", "abs": "https://arxiv.org/abs/2507.22157", "authors": ["Hamed Jafarzadeh Asl", "Mahsa Ghazvini Nejad", "Amin Edraki", "Masoud Asgharian", "Vahid Partovi Nia"], "title": "Tiny Noise-Robust Voice Activity Detector for Voice Assistants", "comment": "Hamed Jafarzadeh Asl and Mahsa Ghazvini Nejad contributed equally to\n  this work", "summary": "Voice Activity Detection (VAD) in the presence of background noise remains a\nchallenging problem in speech processing. Accurate VAD is essential in\nautomatic speech recognition, voice-to-text, conversational agents, etc, where\nnoise can severely degrade the performance. A modern application includes the\nvoice assistant, specially mounted on Artificial Intelligence of Things (AIoT)\ndevices such as cell phones, smart glasses, earbuds, etc, where the voice\nsignal includes background noise. Therefore, VAD modules must remain\nlight-weight due to their practical on-device limitation. The existing models\noften struggle with low signal-to-noise ratios across diverse acoustic\nenvironments. A simple VAD often detects human voice in a clean environment,\nbut struggles to detect the human voice in noisy conditions. We propose a\nnoise-robust VAD that comprises a light-weight VAD, with data pre-processing\nand post-processing added modules to handle the background noise. This approach\nsignificantly enhances the VAD accuracy in noisy environments and requires\nneither a larger model, nor fine-tuning. Experimental results demonstrate that\nour approach achieves a notable improvement compared to baselines, particularly\nin environments with high background noise interference. This modified VAD\nadditionally improving clean speech detection."}
{"id": "2507.22567", "categories": ["eess.SP", "cs.CV", "68T45", "I.5.4"], "pdf": "https://arxiv.org/pdf/2507.22567", "abs": "https://arxiv.org/abs/2507.22567", "authors": ["Weicheng Gao"], "title": "Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination", "comment": "5 pages, 5 figures, 2 tables", "summary": "This work is completed on a whim after discussions with my junior colleague.\nThe motion direction angle affects the micro-Doppler spectrum width, thus\ndetermining the human motion direction can provide important prior information\nfor downstream tasks such as gait recognition. However, Doppler-Time map\n(DTM)-based methods still have room for improvement in achieving feature\naugmentation and motion determination simultaneously. In response, a low-cost\nbut accurate radar-based human motion direction determination (HMDD) method is\nexplored in this paper. In detail, the radar-based human gait DTMs are first\ngenerated, and then the feature augmentation is achieved using feature linking\nmodel. Subsequently, the HMDD is implemented through a lightweight and fast\nVision Transformer-Convolutional Neural Network hybrid model structure. The\neffectiveness of the proposed method is verified through open-source dataset.\nThe open-source code of this work is released at:\nhttps://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination."}
{"id": "2507.22612", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22612", "abs": "https://arxiv.org/abs/2507.22612", "authors": ["Junjie Cao"], "title": "Adaptive Duration Model for Text Speech Alignment", "comment": "4 pages, 3 figures, 2 tables", "summary": "Speech-to-text alignment is a critical component of neural text to-speech\n(TTS) models. Autoregressive TTS models typically use an attention mechanism to\nlearn these alignments on-line. However, these alignments tend to be brittle\nand often fail to generalize to long utterances and out-of-domain text, leading\nto missing or repeating words. Most non-autoregressive end to-end TTS models\nrely on durations extracted from external sources, using additional duration\nmodels for alignment. In this paper, we propose a novel duration prediction\nframework that can give compromising phoneme-level duration distribution with\ngiven text. In our experiments, the proposed duration model has more precise\nprediction and condition adaptation ability compared to previous baseline\nmodels. Numerically, it has roughly a 11.3 percents immprovement on alignment\naccuracy, and makes the performance of zero-shot TTS models more robust to the\nmismatch between prompt audio and input audio."}
{"id": "2507.22534", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22534", "abs": "https://arxiv.org/abs/2507.22534", "authors": ["Michele Panariello", "Sarina Meyer", "Pierre Champion", "Xiaoxiao Miao", "Massimiliano Todisco", "Ngoc Thang Vu", "Nicholas Evans"], "title": "The Risks and Detection of Overestimated Privacy Protection in Voice Anonymisation", "comment": "Accepted at SPSC 2025 - 5th Symposium on Security and Privacy in\n  Speech Communication", "summary": "Voice anonymisation aims to conceal the voice identity of speakers in speech\nrecordings. Privacy protection is usually estimated from the difficulty of\nusing a speaker verification system to re-identify the speaker\npost-anonymisation. Performance assessments are therefore dependent on the\nverification model as well as the anonymisation system. There is hence\npotential for privacy protection to be overestimated when the verification\nsystem is poorly trained, perhaps with mismatched data. In this paper, we\ndemonstrate the insidious risk of overestimating anonymisation performance and\nshow examples of exaggerated performance reported in the literature. For the\nworst case we identified, performance is overestimated by 74% relative. We then\nintroduce a means to detect when performance assessment might be untrustworthy\nand show that it can identify all overestimation scenarios presented in the\npaper. Our solution is openly available as a fork of the 2024 VoicePrivacy\nChallenge evaluation toolkit."}
{"id": "2507.22573", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22573", "abs": "https://arxiv.org/abs/2507.22573", "authors": ["Niclas Führling", "Ivan Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "Gonzalo Seco-Granados", "David González G.", "Osvaldo Gonsa"], "title": "Fundamental Limits of Rigid Body Localization", "comment": null, "summary": "We consider a novel approach to formulate the Cram\\'er-Rao Lower Bound (CRLB)\nfor the rigid body localization (RBL) problem, which allows us to assess the\nfundamental accuracy limits on the estimation of the translation and rotation\nof a rigid body with respect to a known reference. To that end, we adopt an\ninformation-centric construction of the Fisher information matrix (FIM), which\nallows to capture the contribution of each measurement towards the FIM, both in\nterms of input measurement types, as well as of their error distributions.\nTaking advantage of this approach, we derive a generic framework for the CRLB\nformulation, which is applicable to any type of rigid body localization\nscenario, extending the conventional FIM formulation suitable for point targets\nto the case of a rigid body whose location include both translation vector and\nthe rotation matrix (or alternative the rotation angles), with respect to a\nreference. Closed-form expressions for all CRLBs are given, including the bound\nincorporating an orthonormality constraint onto the rotation matrix. Numerical\nresults illustrate that the derived expression correctly lower-bounds the\nerrors of estimated localization parameters obtained via various related\nstate-of-the-art (SotA) estimators, revealing their accuracies and suggesting\nthat SotA RBL algorithms can still be improved."}
{"id": "2507.22746", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22746", "abs": "https://arxiv.org/abs/2507.22746", "authors": ["Yanqing Liu", "Ruiqing Xue", "Chong Zhang", "Yufei Liu", "Gang Wang", "Bohan Li", "Yao Qian", "Lei He", "Shujie Liu", "Sheng Zhao"], "title": "Next Tokens Denoising for Speech Synthesis", "comment": null, "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts."}
{"id": "2507.22599", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22599", "abs": "https://arxiv.org/abs/2507.22599", "authors": ["Xiajie Zhou", "Candy Olivia Mawalim", "Masashi Unoki"], "title": "Modeling Multi-Level Hearing Loss for Speech Intelligibility Prediction", "comment": "5 pages, 2 figures, to appear in WASPAA 2025", "summary": "The diverse perceptual consequences of hearing loss severely impede speech\ncommunication, but standard clinical audiometry, which is focused on\nthreshold-based frequency sensitivity, does not adequately capture deficits in\nfrequency and temporal resolution. To address this limitation, we propose a\nspeech intelligibility prediction method that explicitly simulates auditory\ndegradations according to hearing loss severity by broadening cochlear filters\nand applying low-pass modulation filtering to temporal envelopes. Speech\nsignals are subsequently analyzed using the spectro-temporal modulation (STM)\nrepresentations, which reflect how auditory resolution loss alters the\nunderlying modulation structure. In addition, normalized cross-correlation\n(NCC) matrices quantify the similarity between the STM representations of clean\nspeech and speech in noise. These auditory-informed features are utilized to\ntrain a Vision Transformer-based regression model that integrates the STM maps\nand NCC embeddings to estimate speech intelligibility scores. Evaluations on\nthe Clarity Prediction Challenge corpus show that the proposed method\noutperforms the Hearing-Aid Speech Perception Index v2 (HASPI v2) in both mild\nand moderate-to-severe hearing loss groups, with a relative root mean squared\nerror reduction of 16.5% for the mild group and a 6.1% reduction for the\nmoderate-to-severe group. These results highlight the importance of explicitly\nmodeling listener-specific frequency and temporal resolution degradations to\nimprove speech intelligibility prediction and provide interpretability in\nauditory distortions."}
{"id": "2507.22616", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22616", "abs": "https://arxiv.org/abs/2507.22616", "authors": ["Ronit Sohanpal", "Jiaqian Yang", "Eric Sillekens", "Henrique Buglia", "Mingming Tan", "Dini Pratiwi", "Robert I. Killey", "Polina Bayvel"], "title": "Measurement and Analysis of the Power Consumption of Hybrid-Amplified SCL-band Links", "comment": "2025 European Conference on Optical Communication (ECOC)", "summary": "We studied the power consumption of hybrid-amplified SCL-band links using\ncommercial benchtop amplifiers and Raman pumps. We show a reduction in energy\nper bit for multi-span hybrid Raman amplified links of up to 26% versus lumped\namplification."}
{"id": "2507.22628", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.22628", "abs": "https://arxiv.org/abs/2507.22628", "authors": ["Tao Zhuang", "Longbiao He", "Feng Niu", "Jia-Xin Zhong", "Jing Lu"], "title": "A k-space approach to modeling multi-channel parametric array loudspeaker systems", "comment": null, "summary": "Multi-channel parametric array loudspeaker (MCPAL) systems offer enhanced\nflexibility and promise for generating highly directional audio beams in\nreal-world applications. However, efficient and accurate prediction of their\ngenerated sound fields remains a major challenge due to the complex nonlinear\nbehavior and multi-channel signal processing involved. To overcome this\nobstacle, we propose a k-space approach for modeling arbitrary MCPAL systems\narranged on a baffled planar surface. In our method, the linear ultrasound\nfield is first solved using the angular spectrum approach, and the quasilinear\naudio sound field is subsequently computed efficiently in k-space. By\nleveraging three-dimensional fast Fourier transforms, our approach not only\nachieves high computational and memory efficiency but also maintains accuracy\nwithout relying on the paraxial approximation. For typical configurations\nstudied, the proposed method demonstrates a speed-up of more than four orders\nof magnitude compared to the direct integration method. Our proposed approach\npaved the way for simulating and designing advanced MCPAL systems."}
{"id": "2507.22628", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.22628", "abs": "https://arxiv.org/abs/2507.22628", "authors": ["Tao Zhuang", "Longbiao He", "Feng Niu", "Jia-Xin Zhong", "Jing Lu"], "title": "A k-space approach to modeling multi-channel parametric array loudspeaker systems", "comment": null, "summary": "Multi-channel parametric array loudspeaker (MCPAL) systems offer enhanced\nflexibility and promise for generating highly directional audio beams in\nreal-world applications. However, efficient and accurate prediction of their\ngenerated sound fields remains a major challenge due to the complex nonlinear\nbehavior and multi-channel signal processing involved. To overcome this\nobstacle, we propose a k-space approach for modeling arbitrary MCPAL systems\narranged on a baffled planar surface. In our method, the linear ultrasound\nfield is first solved using the angular spectrum approach, and the quasilinear\naudio sound field is subsequently computed efficiently in k-space. By\nleveraging three-dimensional fast Fourier transforms, our approach not only\nachieves high computational and memory efficiency but also maintains accuracy\nwithout relying on the paraxial approximation. For typical configurations\nstudied, the proposed method demonstrates a speed-up of more than four orders\nof magnitude compared to the direct integration method. Our proposed approach\npaved the way for simulating and designing advanced MCPAL systems."}
{"id": "2507.22656", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22656", "abs": "https://arxiv.org/abs/2507.22656", "authors": ["Zhiming Zhu", "Shu Xu", "Jiexin Zhang", "Chunguo Li", "Yongming Huang", "Luxi Yang"], "title": "A Multi-Scale Spatial Attention Network for Near-field MIMO Channel Estimation", "comment": null, "summary": "The deployment of extremely large-scale array (ELAA) brings higher spectral\nefficiency and spatial degree of freedom, but triggers issues on near-field\nchannel estimation.\n  Existing near-field channel estimation schemes primarily exploit sparsity in\nthe transform domain.\n  However, these schemes are sensitive to the transform matrix selection and\nthe stopping criteria.\n  Inspired by the success of deep learning (DL) in far-field channel\nestimation, this paper proposes a novel spatial-attention-based method for\nreconstructing extremely large-scale MIMO (XL-MIMO) channel.\n  Initially, the spatial antenna correlations of near-field channels are\nanalyzed as an expectation over the angle-distance space, which demonstrate\ncorrelation range of an antenna element varies with its position.\n  Due to the strong correlation between adjacent antenna elements, interactions\nof inter-subchannel are applied to describe inherent correlation of near-field\nchannels instead of inter-element.\n  Subsequently, a multi-scale spatial attention network (MsSAN) with the\ninter-subchannel correlation learning capabilities is proposed tailed to\nnear-field MIMO channel estimation.\n  We employ the multi-scale architecture to refine the subchannel size in\nMsSAN.\n  Specially, we inventively introduce the sum of dot products as spatial\nattention (SA) instead of cross-covariance to weight subchannel features at\ndifferent scales in the SA module.\n  Simulation results are presented to validate the proposed MsSAN achieves\nremarkable the inter-subchannel correlation learning capabilities and\noutperforms others in terms of near-field channel reconstruction."}
{"id": "2507.22208", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22208", "abs": "https://arxiv.org/abs/2507.22208", "authors": ["Shreyansh Pathak", "Sonu Shreshtha", "Richa Singh", "Mayank Vatsa"], "title": "Quantum-Inspired Audio Unlearning: Towards Privacy-Preserving Voice Biometrics", "comment": "9 pages, 2 figures, 5 tables, Accepted at IJCB 2025 (Osaka, Japan)", "summary": "The widespread adoption of voice-enabled authentication and audio biometric\nsystems have significantly increased privacy vulnerabilities associated with\nsensitive speech data. Compliance with privacy regulations such as GDPR's right\nto be forgotten and India's DPDP Act necessitates targeted and efficient\nerasure of individual-specific voice signatures from already-trained biometric\nmodels. Existing unlearning methods designed for visual data inadequately\nhandle the sequential, temporal, and high-dimensional nature of audio signals,\nleading to ineffective or incomplete speaker and accent erasure. To address\nthis, we introduce QPAudioEraser, a quantum-inspired audio unlearning\nframework. Our our-phase approach involves: (1) weight initialization using\ndestructive interference to nullify target features, (2) superposition-based\nlabel transformations that obscure class identity, (3) an\nuncertainty-maximizing quantum loss function, and (4) entanglement-inspired\nmixing of correlated weights to retain model knowledge. Comprehensive\nevaluations with ResNet18, ViT, and CNN architectures across AudioMNIST, Speech\nCommands, LibriSpeech, and Speech Accent Archive datasets validate\nQPAudioEraser's superior performance. The framework achieves complete erasure\nof target data (0% Forget Accuracy) while incurring minimal impact on model\nutility, with a performance degradation on retained data as low as 0.05%.\nQPAudioEraser consistently surpasses conventional baselines across\nsingle-class, multi-class, sequential, and accent-level erasure scenarios,\nestablishing the proposed approach as a robust privacy-preserving solution."}
{"id": "2507.22727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22727", "abs": "https://arxiv.org/abs/2507.22727", "authors": ["Jionghui Wang", "Hongwei Wang", "Jun Fang", "Lingxiang Li", "Zhi Chen"], "title": "Compressive Near-Field Wideband Channel Estimation for THz Extremely Large-scale MIMO Systems", "comment": null, "summary": "We consider the channel acquisition problem for a wideband terahertz (THz)\ncommunication system, where an extremely large-scale array is deployed to\nmitigate severe path attenuation. In channel modeling, we account for both the\nnear-field spherical wavefront and the wideband beam-splitting phenomena,\nresulting in a wideband near-field channel. We propose a frequency-independent\northogonal dictionary that generalizes the standard discrete Fourier transform\n(DFT) matrix by introducing an additional parameter to capture the near-field\nproperty. This dictionary enables the wideband near-field channel to be\nefficiently represented with a two-dimensional (2D) block-sparse structure.\nLeveraging this specific sparse structure, the wideband near-field channel\nestimation problem can be effectively addressed within a customized compressive\nsensing framework. Numerical results demonstrate the significant advantages of\nour proposed 2D block-sparsity-aware method over conventional\npolar-domain-based approaches for near-field wideband channel estimation."}
{"id": "2507.22322", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22322", "abs": "https://arxiv.org/abs/2507.22322", "authors": ["Hogeon Yu"], "title": "A Two-Step Learning Framework for Enhancing Sound Event Localization and Detection", "comment": "5pages, 2figures", "summary": "Sound Event Localization and Detection (SELD) is crucial in spatial audio\nprocessing, enabling systems to detect sound events and estimate their 3D\ndirections. Existing SELD methods use single- or dual-branch architectures:\nsingle-branch models share SED and DoA representations, causing optimization\nconflicts, while dual-branch models separate tasks but limit information\nexchange. To address this, we propose a two-step learning framework. First, we\nintroduce a tracwise reordering format to maintain temporal consistency,\npreventing event reassignments across tracks. Next, we train SED and DoA\nnetworks to prevent interference and ensure task-specific feature learning.\nFinally, we effectively fuse DoA and SED features to enhance SELD performance\nwith better spatial and event representation. Experiments on the 2023 DCASE\nchallenge Task 3 dataset validate our framework, showing its ability to\novercome single- and dual-branch limitations and improve event classification\nand localization."}
{"id": "2507.22612", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22612", "abs": "https://arxiv.org/abs/2507.22612", "authors": ["Junjie Cao"], "title": "Adaptive Duration Model for Text Speech Alignment", "comment": "4 pages, 3 figures, 2 tables", "summary": "Speech-to-text alignment is a critical component of neural text to-speech\n(TTS) models. Autoregressive TTS models typically use an attention mechanism to\nlearn these alignments on-line. However, these alignments tend to be brittle\nand often fail to generalize to long utterances and out-of-domain text, leading\nto missing or repeating words. Most non-autoregressive end to-end TTS models\nrely on durations extracted from external sources, using additional duration\nmodels for alignment. In this paper, we propose a novel duration prediction\nframework that can give compromising phoneme-level duration distribution with\ngiven text. In our experiments, the proposed duration model has more precise\nprediction and condition adaptation ability compared to previous baseline\nmodels. Numerically, it has roughly a 11.3 percents immprovement on alignment\naccuracy, and makes the performance of zero-shot TTS models more robust to the\nmismatch between prompt audio and input audio."}
{"id": "2507.22746", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.22746", "abs": "https://arxiv.org/abs/2507.22746", "authors": ["Yanqing Liu", "Ruiqing Xue", "Chong Zhang", "Yufei Liu", "Gang Wang", "Bohan Li", "Yao Qian", "Lei He", "Shujie Liu", "Sheng Zhao"], "title": "Next Tokens Denoising for Speech Synthesis", "comment": null, "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts."}

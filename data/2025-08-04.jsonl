{"id": "2508.00317", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00317", "abs": "https://arxiv.org/abs/2508.00317", "authors": ["Wen-Chin Huang"], "title": "Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities", "comment": "APSIPA ASC 2025 perspective paper", "summary": "Speech quality assessment (SQA) refers to the evaluation of speech quality,\nand developing an accurate automatic SQA method that reflects human perception\nhas become increasingly important, in order to keep up with the generative AI\nboom. In recent years, SQA has progressed to a point that researchers started\nto faithfully use automatic SQA in research papers as a rigorous measurement of\ngoodness for speech generation systems. We believe that the scientific\nchallenges and open-source activities of late have stimulated the growth in\nthis field. In this paper, we review recent challenges as well as open-source\nimplementations and toolkits for SQA, and highlight the importance of\nmaintaining such activities to facilitate the development of not only SQA\nitself but also generative AI for speech."}
{"id": "2508.00733", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00733", "abs": "https://arxiv.org/abs/2508.00733", "authors": ["Le Wang", "Jun Wang", "Feng Deng", "Chen Zhang", "Kun Gai", "Di Zhang"], "title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation", "comment": "12 pages, 2 figures", "summary": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality."}
{"id": "2508.00240", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.00240", "abs": "https://arxiv.org/abs/2508.00240", "authors": ["Ismael Nawfal", "Symeon Delikaris Manias", "Mehrez Souden", "Juha Merimaa", "Joshua Atkins", "Elisabeth McMullin", "Shadi Pirhosseinloo", "Daniel Phillips"], "title": "Ambisonics Super-Resolution Using A Waveform-Domain Neural Network", "comment": null, "summary": "Ambisonics is a spatial audio format describing a sound field. First-order\nAmbisonics (FOA) is a popular format comprising only four channels. This\nlimited channel count comes at the expense of spatial accuracy. Ideally one\nwould be able to take the efficiency of a FOA format without its limitations.\nWe have devised a data-driven spatial audio solution that retains the\nefficiency of the FOA format but achieves quality that surpasses conventional\nrenderers. Utilizing a fully convolutional time-domain audio neural network\n(Conv-TasNet), we created a solution that takes a FOA input and provides a\nhigher order Ambisonics (HOA) output. This data driven approach is novel when\ncompared to typical physics and psychoacoustic based renderers. Quantitative\nevaluations showed a 0.6dB average positional mean squared error difference\nbetween predicted and actual 3rd order HOA. The median qualitative rating\nshowed an 80% improvement in perceived quality over the traditional rendering\napproach."}
{"id": "2508.00307", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00307", "abs": "https://arxiv.org/abs/2508.00307", "authors": ["Belman Jahir Rodriguez", "Sergio F. Chevtchenko", "Marcelo Herrera Martinez", "Yeshwant Bethy", "Saeed Afshar"], "title": "Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization", "comment": null, "summary": "We introduce a U-net model for 360{\\deg} acoustic source localization\nformulated as a spherical semantic segmentation task. Rather than regressing\ndiscrete direction-of-arrival (DoA) angles, our model segments beamformed audio\nmaps (azimuth and elevation) into regions of active sound presence. Using\ndelay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate\nsignals aligned with drone GPS telemetry to create binary supervision masks. A\nmodified U-Net, trained on frequency-domain representations of these maps,\nlearns to identify spatially distributed source regions while addressing class\nimbalance via the Tversky loss. Because the network operates on beamformed\nenergy maps, the approach is inherently array-independent and can adapt to\ndifferent microphone configurations without retraining from scratch. The\nsegmentation outputs are post-processed by computing centroids over activated\nregions, enabling robust DoA estimates. Our dataset includes real-world\nopen-field recordings of a DJI Air 3 drone, synchronized with 360{\\deg} video\nand flight logs across multiple dates and locations. Experimental results show\nthat U-net generalizes across environments, providing improved angular\nprecision, offering a new paradigm for dense spatial audio understanding beyond\ntraditional Sound Source Localization (SSL)."}
{"id": "2508.00123", "categories": ["eess.AS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00123", "abs": "https://arxiv.org/abs/2508.00123", "authors": ["Changhong Wang", "Michel Olvera", "Gaël Richard"], "title": "Melody-Lyrics Matching with Contrastive Alignment Loss", "comment": "10 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "The connection between music and lyrics is far beyond semantic bonds.\nConceptual pairs in the two modalities such as rhythm and rhyme, note duration\nand syllabic stress, and structure correspondence, raise a compelling yet\nseldom-explored direction in the field of music information retrieval. In this\npaper, we present melody-lyrics matching (MLM), a new task which retrieves\npotential lyrics for a given symbolic melody from text sources. Rather than\ngenerating lyrics from scratch, MLM essentially exploits the relationships\nbetween melody and lyrics. We propose a self-supervised representation learning\nframework with contrastive alignment loss for melody and lyrics. This has the\npotential to leverage the abundance of existing songs with paired melody and\nlyrics. No alignment annotations are required. Additionally, we introduce\nsylphone, a novel representation for lyrics at syllable-level activated by\nphoneme identity and vowel stress. We demonstrate that our method can match\nmelody with coherent and singable lyrics with empirical results and intuitive\nexamples. We open source code and provide matching examples on the companion\nwebpage: https://github.com/changhongw/mlm."}
{"id": "2508.00093", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00093", "abs": "https://arxiv.org/abs/2508.00093", "authors": ["Lucas Alves Zischler", "Chiara Lasagni", "Paolo Serena", "Alberto Bononi", "Giammarco Di Sciullo", "Divya A. Shaji", "Antonio Mecozzi", "Cristian Antonelli"], "title": "Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering", "comment": "Submitted for the Journal of Lightwave Technology", "summary": "Wideband systems experience significant inter-channel stimulated Raman\nscattering (ISRS) and channel-dependent losses. Due to the non-uniform\nattenuation profile, the combined effects of ISRS and fiber loss can only be\naccurately estimated using numerical methods. In this work, we present an\napproximate closed-form expression for the channels' power profile accounting\nfor these combined effects. We validate the proposed expression against\nnumerical solutions in the case of CLU transmission, showing high accuracy for\nboth single-span and multi-span fiber-optic links. Additionally, we derive an\ninverse expression, formulated as a function of the output power, which can be\nutilized to target a desired optical signal-to-noise ratio (OSNR) profile\nthrough pre-emphasis of the launched channel powers."}
{"id": "2508.00479", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00479", "abs": "https://arxiv.org/abs/2508.00479", "authors": ["Noah Shore"], "title": "Wavelet-Based Time-Frequency Fingerprinting for Feature Extraction of Traditional Irish Music", "comment": "Master's thesis. The focus of the thesis is on the underlying\n  techniques for signal fingerprinting", "summary": "This work presents a wavelet-based approach to time-frequency fingerprinting\nfor time series feature extraction, with a focus on audio identification from\nlive recordings of traditional Irish tunes. The challenges of identifying\nfeatures in time-series data are addressed by employing a continuous wavelet\ntransform to extract spectral features and wavelet coherence analysis is used\nto compare recorded audio spectrograms to synthetically generated tunes. The\nsynthetic tunes are derived from ABC notation, which is a common symbolic\nrepresentation for Irish music. Experimental results demonstrate that the\nwavelet-based method can accurately and efficiently identify recorded tunes.\nThis research study also details the performance of the wavelet coherence\nmodel, highlighting its strengths over other methods of time-frequency\ndecomposition. Additionally, we discuss and deploy the model on several\napplications beyond music, including in EEG signal analysis and financial time\nseries forecasting."}
{"id": "2508.00240", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.00240", "abs": "https://arxiv.org/abs/2508.00240", "authors": ["Ismael Nawfal", "Symeon Delikaris Manias", "Mehrez Souden", "Juha Merimaa", "Joshua Atkins", "Elisabeth McMullin", "Shadi Pirhosseinloo", "Daniel Phillips"], "title": "Ambisonics Super-Resolution Using A Waveform-Domain Neural Network", "comment": null, "summary": "Ambisonics is a spatial audio format describing a sound field. First-order\nAmbisonics (FOA) is a popular format comprising only four channels. This\nlimited channel count comes at the expense of spatial accuracy. Ideally one\nwould be able to take the efficiency of a FOA format without its limitations.\nWe have devised a data-driven spatial audio solution that retains the\nefficiency of the FOA format but achieves quality that surpasses conventional\nrenderers. Utilizing a fully convolutional time-domain audio neural network\n(Conv-TasNet), we created a solution that takes a FOA input and provides a\nhigher order Ambisonics (HOA) output. This data driven approach is novel when\ncompared to typical physics and psychoacoustic based renderers. Quantitative\nevaluations showed a 0.6dB average positional mean squared error difference\nbetween predicted and actual 3rd order HOA. The median qualitative rating\nshowed an 80% improvement in perceived quality over the traditional rendering\napproach."}
{"id": "2508.00274", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00274", "abs": "https://arxiv.org/abs/2508.00274", "authors": ["Yunfei Liu", "Mingxuan Liu", "Wupeng Xie", "Xinzhu Liu", "Wenxue Liu", "Yangang Sun", "Xin Qiu", "Cui Yuan", "Jinhai Li"], "title": "RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder", "comment": null, "summary": "Automatic modulation classification (AMC) is a basic technology in\nintelligent wireless communication systems. It is important for tasks such as\nspectrum monitoring, cognitive radio, and secure communications. In recent\nyears, deep learning methods have made great progress in AMC. However,\nmainstream methods still face two key problems. First, they often use\ntime-frequency images instead of raw signals. This causes loss of key\nmodulation features and reduces adaptability to different communication\nconditions. Second, most methods rely on supervised learning. This needs a\nlarge amount of labeled data, which is hard to get in real-world environments.\nTo solve these problems, we propose a self-supervised learning framework called\nRIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from\nunlabeled data. It takes raw IQ sequences as input. By applying random masking\nand reconstruction, it captures important time-domain features such as\namplitude, phase, etc. This helps the model learn useful and transferable\nrepresentations. RIS-MAE is tested on four datasets. The results show that it\nperforms better than existing methods in few-shot and cross-domain tasks.\nNotably, it achieves high classification accuracy on previously unseen datasets\nwith only a small number of fine-tuning samples, confirming its generalization\nability and potential for real-world deployment."}
{"id": "2508.00501", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.00501", "abs": "https://arxiv.org/abs/2508.00501", "authors": ["Paolo Ostan", "Francesca Del Gaudio", "Federico Miotello", "Mirco Pezzoli", "Fabio Antonacci"], "title": "VR-PTOLEMAIC: A Virtual Environment for the Perceptual Testing of Spatial Audio Algorithms", "comment": "to appear in EAA Forum Acusticum 2025", "summary": "The perceptual evaluation of spatial audio algorithms is an important step in\nthe development of immersive audio applications, as it ensures that synthesized\nsound fields meet quality standards in terms of listening experience, spatial\nperception and auditory realism. To support these evaluations, virtual reality\ncan offer a powerful platform by providing immersive and interactive testing\nenvironments. In this paper, we present VR-PTOLEMAIC, a virtual reality\nevaluation system designed for assessing spatial audio algorithms. The system\nimplements the MUSHRA (MUlti-Stimulus test with Hidden Reference and Anchor)\nevaluation methodology into a virtual environment. In particular, users can\nposition themselves in each of the 25 simulated listening positions of a\nvirtually recreated seminar room and evaluate simulated acoustic responses with\nrespect to the actually recorded second-order ambisonic room impulse responses,\nall convolved with various source signals. We evaluated the usability of the\nproposed framework through an extensive testing campaign in which assessors\nwere asked to compare the reconstruction capabilities of various sound field\nreconstruction algorithms. Results show that the VR platform effectively\nsupports the assessment of spatial audio algorithms, with generally positive\nfeedback on user experience and immersivity."}
{"id": "2508.00307", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00307", "abs": "https://arxiv.org/abs/2508.00307", "authors": ["Belman Jahir Rodriguez", "Sergio F. Chevtchenko", "Marcelo Herrera Martinez", "Yeshwant Bethy", "Saeed Afshar"], "title": "Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization", "comment": null, "summary": "We introduce a U-net model for 360{\\deg} acoustic source localization\nformulated as a spherical semantic segmentation task. Rather than regressing\ndiscrete direction-of-arrival (DoA) angles, our model segments beamformed audio\nmaps (azimuth and elevation) into regions of active sound presence. Using\ndelay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate\nsignals aligned with drone GPS telemetry to create binary supervision masks. A\nmodified U-Net, trained on frequency-domain representations of these maps,\nlearns to identify spatially distributed source regions while addressing class\nimbalance via the Tversky loss. Because the network operates on beamformed\nenergy maps, the approach is inherently array-independent and can adapt to\ndifferent microphone configurations without retraining from scratch. The\nsegmentation outputs are post-processed by computing centroids over activated\nregions, enabling robust DoA estimates. Our dataset includes real-world\nopen-field recordings of a DJI Air 3 drone, synchronized with 360{\\deg} video\nand flight logs across multiple dates and locations. Experimental results show\nthat U-net generalizes across environments, providing improved angular\nprecision, offering a new paradigm for dense spatial audio understanding beyond\ntraditional Sound Source Localization (SSL)."}
{"id": "2508.00326", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00326", "abs": "https://arxiv.org/abs/2508.00326", "authors": ["Chengwang Ji", "Kehui Li", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "title": "Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems", "comment": null, "summary": "Reconfigurable distributed antenna and reflecting surface (RDARS) is a\npromising architecture for future sixth-generation (6G) wireless networks. In\nparticular, the dynamic working mode configuration for the RDARS-aided system\nbrings an extra selection gain compared to the existing reconfigurable\nintelligent surface (RIS)-aided system and distributed antenna system (DAS). In\nthis paper, we consider the RDARS-aided downlink multiple-input multiple-output\n(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly\noptimizing the beamforming matrices at the based station (BS) and RDARS, as\nwell as mode switching matrix at RDARS. The optimization problem is challenging\nto be solved due to the non-convex objective function and mixed integer binary\nconstraint. To this end, a penalty term-based weight minimum mean square error\n(PWM) algorithm is proposed by integrating the majorization-minimization (MM)\nand weight minimum mean square error (WMMSE) methods. To further escape the\nlocal optimum point in the PWM algorithm, a model-driven DL method is\nintegrated into this algorithm, where the key variables related to the\nconvergence of PWM algorithm are trained to accelerate the convergence speed\nand improve the system performance. Simulation results are provided to show\nthat the PWM-based beamforming network (PWM-BFNet) can reduce the number of\niterations by half and achieve performance improvements of 26.53% and 103.2% at\nthe scenarios of high total transmit power and a large number of RDARS transmit\nelements (TEs), respectively."}
{"id": "2508.00479", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00479", "abs": "https://arxiv.org/abs/2508.00479", "authors": ["Noah Shore"], "title": "Wavelet-Based Time-Frequency Fingerprinting for Feature Extraction of Traditional Irish Music", "comment": "Master's thesis. The focus of the thesis is on the underlying\n  techniques for signal fingerprinting", "summary": "This work presents a wavelet-based approach to time-frequency fingerprinting\nfor time series feature extraction, with a focus on audio identification from\nlive recordings of traditional Irish tunes. The challenges of identifying\nfeatures in time-series data are addressed by employing a continuous wavelet\ntransform to extract spectral features and wavelet coherence analysis is used\nto compare recorded audio spectrograms to synthetically generated tunes. The\nsynthetic tunes are derived from ABC notation, which is a common symbolic\nrepresentation for Irish music. Experimental results demonstrate that the\nwavelet-based method can accurately and efficiently identify recorded tunes.\nThis research study also details the performance of the wavelet coherence\nmodel, highlighting its strengths over other methods of time-frequency\ndecomposition. Additionally, we discuss and deploy the model on several\napplications beyond music, including in EEG signal analysis and financial time\nseries forecasting."}
{"id": "2508.00409", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00409", "abs": "https://arxiv.org/abs/2508.00409", "authors": ["Mohammad Soleymani", "Ignacio Santamaria", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "title": "STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink", "comment": "Accepted at 28th International Workshop on Smart Antennas 2025", "summary": "Rate splitting multiple access (RSMA) is intrinsically amalgamated with\nsimultaneously transmitting and reflecting (STAR) reconfigurable intelligent\nsurfaces (RIS) to enhance energy efficiency (EE) of the finite block length\n(FBL) multiple-input multiple-output (MIMO) downlink. An alternating\noptimization-based algorithm is proposed to jointly optimize the transmit\nbeamforming matrices, STAR-RIS configurations, and rate-splitting parameters.\nSTAR-RIS attains 360-degree full-plane coverage, while RSMA provides a\nprominent gain by efficiently managing interference. Numerical results reveal a\nstrong synergy between RSMA and STAR-RIS, demonstreating significant EE gains\nover reflective RIS and spatial division multiple access (SDMA)."}
{"id": "2508.00501", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.00501", "abs": "https://arxiv.org/abs/2508.00501", "authors": ["Paolo Ostan", "Francesca Del Gaudio", "Federico Miotello", "Mirco Pezzoli", "Fabio Antonacci"], "title": "VR-PTOLEMAIC: A Virtual Environment for the Perceptual Testing of Spatial Audio Algorithms", "comment": "to appear in EAA Forum Acusticum 2025", "summary": "The perceptual evaluation of spatial audio algorithms is an important step in\nthe development of immersive audio applications, as it ensures that synthesized\nsound fields meet quality standards in terms of listening experience, spatial\nperception and auditory realism. To support these evaluations, virtual reality\ncan offer a powerful platform by providing immersive and interactive testing\nenvironments. In this paper, we present VR-PTOLEMAIC, a virtual reality\nevaluation system designed for assessing spatial audio algorithms. The system\nimplements the MUSHRA (MUlti-Stimulus test with Hidden Reference and Anchor)\nevaluation methodology into a virtual environment. In particular, users can\nposition themselves in each of the 25 simulated listening positions of a\nvirtually recreated seminar room and evaluate simulated acoustic responses with\nrespect to the actually recorded second-order ambisonic room impulse responses,\nall convolved with various source signals. We evaluated the usability of the\nproposed framework through an extensive testing campaign in which assessors\nwere asked to compare the reconstruction capabilities of various sound field\nreconstruction algorithms. Results show that the VR platform effectively\nsupports the assessment of spatial audio algorithms, with generally positive\nfeedback on user experience and immersivity."}
{"id": "2508.00456", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00456", "abs": "https://arxiv.org/abs/2508.00456", "authors": ["Ji Wang", "Bin Tang", "Jian Xiao", "Qimei Cui", "Xingwang Li", "Tony Q. S. Quek"], "title": "When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework", "comment": null, "summary": "As the real propagation environment becomes in creasingly complex and\ndynamic, millimeter wave beam prediction faces huge challenges. However, the\npowerful cross modal representation capability of vision-language model (VLM)\nprovides a promising approach. The traditional methods that rely on real-time\nchannel state information (CSI) are computationally expensive and often fail to\nmaintain accuracy in such environments. In this paper, we present a VLM-driven\ncontrastive learning based multimodal beam prediction framework that integrates\nmultimodal data via modality-specific encoders. To enforce cross-modal\nconsistency, we adopt a contrastive pretraining strategy to align image and\nLiDAR features in the latent space. We use location information as text prompts\nand connect it to the text encoder to introduce language modality, which\nfurther improves cross-modal consistency. Experiments on the DeepSense-6G\ndataset show that our VLM backbone provides additional semantic grounding.\nCompared with existing methods, the overall distance-based accuracy score\n(DBA-Score) of 0.9016, corresponding to 1.46% average improvement."}
{"id": "2508.00509", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00509", "abs": "https://arxiv.org/abs/2508.00509", "authors": ["Paolo Ostan", "Carlo Centofanti", "Mirco Pezzoli", "Alberto Bernardini", "Claudia Rinaldi", "Fabio Antonacci"], "title": "Dynamic Real-Time Ambisonics Order Adaptation for Immersive Networked Music Performances", "comment": "to appear in EUSIPCO 2025", "summary": "Advanced remote applications such as Networked Music Performance (NMP)\nrequire solutions to guarantee immersive real-world-like interaction among\nusers. Therefore, the adoption of spatial audio formats, such as Ambisonics, is\nfundamental to let the user experience an immersive acoustic scene. The\naccuracy of the sound scene reproduction increases with the order of the\nAmbisonics enconding, resulting in an improved immersivity at the cost of a\ngreater number of audio channels, which in turn escalates both bandwidth\nrequirements and susceptibility to network impairments (e.g., latency, jitter,\nand packet loss). These factors pose a significant challenge for interactive\nmusic sessions, which demand high spatial fidelity and low end-to-end delay. We\npropose a real-time adaptive higher-order Ambisonics strategy that continuously\nmonitors network throughput and dynamically scales the Ambisonics order. When\navailable bandwidth drops below a preset threshold, the order is lowered to\nprevent audio dropouts; it then reverts to higher orders once conditions\nrecover, thus balancing immersion and reliability. A MUSHRA-based evaluation\nindicates that this adaptive approach is promising to guarantee user experience\nin bandwidth-limited NMP scenarios."}
{"id": "2508.00494", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00494", "abs": "https://arxiv.org/abs/2508.00494", "authors": ["Youngsun Kong", "Farnoush Baghestani", "I-Ping Chen", "Ki Chon"], "title": "Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency", "comment": "Accepted and presented at the 47th Annual International Conference of\n  the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "summary": "Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has\nbeen a promising non-invasive surrogate for accurate and effective assessment\nof the sympathetic nervous system (SNS). Typically, SKNA extraction requires a\nhigher sampling frequency than the typical ECG recording requirement (> 2 kHz)\nbecause analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,\nECG recording systems commonly provide a sampling frequency of 1 kHz or lower,\nparticularly for wearable devices. Our recent power spectral analysis exhibited\nthat 150-500 Hz frequency bands are dominant during sympathetic stimulation.\nTherefore, we hypothesize that SKNA can be extracted from ECG sampled at a\nlower sampling frequency. We collected ECG signals from 16 participants during\nSNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical\nanalyses of significance, classification performance, and reliability indicate\nno significant difference between SKNA indices derived from ECG signals sampled\nat 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,\nwhich are limited to low sampling rates due to resource constraints or outdated\nguidelines, can be used to reliably collect SKNA if muscle artifact\ncontamination is minimal."}
{"id": "2508.00317", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00317", "abs": "https://arxiv.org/abs/2508.00317", "authors": ["Wen-Chin Huang"], "title": "Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities", "comment": "APSIPA ASC 2025 perspective paper", "summary": "Speech quality assessment (SQA) refers to the evaluation of speech quality,\nand developing an accurate automatic SQA method that reflects human perception\nhas become increasingly important, in order to keep up with the generative AI\nboom. In recent years, SQA has progressed to a point that researchers started\nto faithfully use automatic SQA in research papers as a rigorous measurement of\ngoodness for speech generation systems. We believe that the scientific\nchallenges and open-source activities of late have stimulated the growth in\nthis field. In this paper, we review recent challenges as well as open-source\nimplementations and toolkits for SQA, and highlight the importance of\nmaintaining such activities to facilitate the development of not only SQA\nitself but also generative AI for speech."}
{"id": "2508.00603", "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00603", "abs": "https://arxiv.org/abs/2508.00603", "authors": ["Hong-Cheng Liang", "Man-Wai Mak", "Kong Aik Lee"], "title": "Subband Architecture Aided Selective Fixed-Filter Active Noise Control", "comment": null, "summary": "The feedforward selective fixed-filter method selects the most suitable\npre-trained control filter based on the spectral features of the detected\nreference signal, effectively avoiding slow convergence in conventional\nadaptive algorithms. However, it can only handle limited types of noises, and\nthe performance degrades when the input noise exhibits non-uniform power\nspectral density. To address these limitations, this paper devises a novel\nselective fixed-filter scheme based on a delayless subband structure. In the\noff-line training stage, subband control filters are pre-trained for different\nfrequency ranges and stored in a dedicated sub-filter database. During the\non-line control stage, the incoming noise is decomposed using a polyphase FFT\nfilter bank, and a frequency-band-matching mechanism assigns each subband\nsignal the most appropriate control filter. Subsequently, a weight stacking\ntechnique is employed to combine all subband weights into a fullband filter,\nenabling real-time noise suppression. Experimental results demonstrate that the\nproposed scheme provides fast convergence, effective noise reduction, and\nstrong robustness in handling more complicated noisy environments."}
{"id": "2508.00603", "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00603", "abs": "https://arxiv.org/abs/2508.00603", "authors": ["Hong-Cheng Liang", "Man-Wai Mak", "Kong Aik Lee"], "title": "Subband Architecture Aided Selective Fixed-Filter Active Noise Control", "comment": null, "summary": "The feedforward selective fixed-filter method selects the most suitable\npre-trained control filter based on the spectral features of the detected\nreference signal, effectively avoiding slow convergence in conventional\nadaptive algorithms. However, it can only handle limited types of noises, and\nthe performance degrades when the input noise exhibits non-uniform power\nspectral density. To address these limitations, this paper devises a novel\nselective fixed-filter scheme based on a delayless subband structure. In the\noff-line training stage, subband control filters are pre-trained for different\nfrequency ranges and stored in a dedicated sub-filter database. During the\non-line control stage, the incoming noise is decomposed using a polyphase FFT\nfilter bank, and a frequency-band-matching mechanism assigns each subband\nsignal the most appropriate control filter. Subsequently, a weight stacking\ntechnique is employed to combine all subband weights into a fullband filter,\nenabling real-time noise suppression. Experimental results demonstrate that the\nproposed scheme provides fast convergence, effective noise reduction, and\nstrong robustness in handling more complicated noisy environments."}
{"id": "2508.00800", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00800", "abs": "https://arxiv.org/abs/2508.00800", "authors": ["Rui Chen", "Wen-Xuan Long", "Bing-Qian Wang", "Yuan He", "Rui-Jin Sun", "Nan Cheng", "Gan Zheng", "Dusit Niyato"], "title": "Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding", "comment": "38 pages, 18 figures", "summary": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks."}
{"id": "2508.00733", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00733", "abs": "https://arxiv.org/abs/2508.00733", "authors": ["Le Wang", "Jun Wang", "Feng Deng", "Chen Zhang", "Kun Gai", "Di Zhang"], "title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation", "comment": "12 pages, 2 figures", "summary": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality."}
{"id": "2508.00307", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00307", "abs": "https://arxiv.org/abs/2508.00307", "authors": ["Belman Jahir Rodriguez", "Sergio F. Chevtchenko", "Marcelo Herrera Martinez", "Yeshwant Bethy", "Saeed Afshar"], "title": "Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization", "comment": null, "summary": "We introduce a U-net model for 360{\\deg} acoustic source localization\nformulated as a spherical semantic segmentation task. Rather than regressing\ndiscrete direction-of-arrival (DoA) angles, our model segments beamformed audio\nmaps (azimuth and elevation) into regions of active sound presence. Using\ndelay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate\nsignals aligned with drone GPS telemetry to create binary supervision masks. A\nmodified U-Net, trained on frequency-domain representations of these maps,\nlearns to identify spatially distributed source regions while addressing class\nimbalance via the Tversky loss. Because the network operates on beamformed\nenergy maps, the approach is inherently array-independent and can adapt to\ndifferent microphone configurations without retraining from scratch. The\nsegmentation outputs are post-processed by computing centroids over activated\nregions, enabling robust DoA estimates. Our dataset includes real-world\nopen-field recordings of a DJI Air 3 drone, synchronized with 360{\\deg} video\nand flight logs across multiple dates and locations. Experimental results show\nthat U-net generalizes across environments, providing improved angular\nprecision, offering a new paradigm for dense spatial audio understanding beyond\ntraditional Sound Source Localization (SSL)."}
{"id": "2508.00479", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00479", "abs": "https://arxiv.org/abs/2508.00479", "authors": ["Noah Shore"], "title": "Wavelet-Based Time-Frequency Fingerprinting for Feature Extraction of Traditional Irish Music", "comment": "Master's thesis. The focus of the thesis is on the underlying\n  techniques for signal fingerprinting", "summary": "This work presents a wavelet-based approach to time-frequency fingerprinting\nfor time series feature extraction, with a focus on audio identification from\nlive recordings of traditional Irish tunes. The challenges of identifying\nfeatures in time-series data are addressed by employing a continuous wavelet\ntransform to extract spectral features and wavelet coherence analysis is used\nto compare recorded audio spectrograms to synthetically generated tunes. The\nsynthetic tunes are derived from ABC notation, which is a common symbolic\nrepresentation for Irish music. Experimental results demonstrate that the\nwavelet-based method can accurately and efficiently identify recorded tunes.\nThis research study also details the performance of the wavelet coherence\nmodel, highlighting its strengths over other methods of time-frequency\ndecomposition. Additionally, we discuss and deploy the model on several\napplications beyond music, including in EEG signal analysis and financial time\nseries forecasting."}

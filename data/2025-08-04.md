<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 8]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.SD](#cs.SD) [Total: 2]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering](https://arxiv.org/abs/2508.00093)
*Lucas Alves Zischler,Chiara Lasagni,Paolo Serena,Alberto Bononi,Giammarco Di Sciullo,Divya A. Shaji,Antonio Mecozzi,Cristian Antonelli*

Main category: eess.SP

TL;DR: 提出了一种近似闭式表达式，用于估计宽带系统中ISRS和光纤损耗的联合效应，并通过数值验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 宽带系统中的ISRS和通道相关损耗难以准确估计，需要数值方法。

Method: 提出了一种近似闭式表达式，并验证了其在单跨和多跨光纤链路中的准确性。

Result: 表达式在CLU传输中表现出高准确性，并推导了可用于目标OSNR剖面的逆表达式。

Conclusion: 该方法为宽带系统中的功率剖面估计和目标OSNR设计提供了有效工具。

Abstract: Wideband systems experience significant inter-channel stimulated Raman
scattering (ISRS) and channel-dependent losses. Due to the non-uniform
attenuation profile, the combined effects of ISRS and fiber loss can only be
accurately estimated using numerical methods. In this work, we present an
approximate closed-form expression for the channels' power profile accounting
for these combined effects. We validate the proposed expression against
numerical solutions in the case of CLU transmission, showing high accuracy for
both single-span and multi-span fiber-optic links. Additionally, we derive an
inverse expression, formulated as a function of the output power, which can be
utilized to target a desired optical signal-to-noise ratio (OSNR) profile
through pre-emphasis of the launched channel powers.

</details>


### [2] [RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder](https://arxiv.org/abs/2508.00274)
*Yunfei Liu,Mingxuan Liu,Wupeng Xie,Xinzhu Liu,Wenxue Liu,Yangang Sun,Xin Qiu,Cui Yuan,Jinhai Li*

Main category: eess.SP

TL;DR: 提出了一种自监督学习框架RIS-MAE，用于自动调制分类，解决了传统方法依赖时频图像和大量标注数据的问题，通过掩码自编码器从原始IQ序列中学习特征，在少样本和跨域任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动调制分类（AMC）是智能无线通信系统的基础技术，但主流方法依赖时频图像和大量标注数据，导致关键特征丢失且适应性不足。

Method: 提出RIS-MAE框架，使用掩码自编码器从原始IQ序列中学习特征，通过随机掩码和重建捕获时域特征（如幅度、相位）。

Result: 在四个数据集上测试，RIS-MAE在少样本和跨域任务中优于现有方法，仅需少量微调样本即可在新数据集上实现高分类准确率。

Conclusion: RIS-MAE展示了强大的泛化能力和实际部署潜力，为AMC提供了一种高效的自监督解决方案。

Abstract: Automatic modulation classification (AMC) is a basic technology in
intelligent wireless communication systems. It is important for tasks such as
spectrum monitoring, cognitive radio, and secure communications. In recent
years, deep learning methods have made great progress in AMC. However,
mainstream methods still face two key problems. First, they often use
time-frequency images instead of raw signals. This causes loss of key
modulation features and reduces adaptability to different communication
conditions. Second, most methods rely on supervised learning. This needs a
large amount of labeled data, which is hard to get in real-world environments.
To solve these problems, we propose a self-supervised learning framework called
RIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from
unlabeled data. It takes raw IQ sequences as input. By applying random masking
and reconstruction, it captures important time-domain features such as
amplitude, phase, etc. This helps the model learn useful and transferable
representations. RIS-MAE is tested on four datasets. The results show that it
performs better than existing methods in few-shot and cross-domain tasks.
Notably, it achieves high classification accuracy on previously unseen datasets
with only a small number of fine-tuning samples, confirming its generalization
ability and potential for real-world deployment.

</details>


### [3] [Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems](https://arxiv.org/abs/2508.00326)
*Chengwang Ji,Kehui Li,Haiquan Lu,Qiaoyan Peng,Jintao Wang,Shaodan Ma*

Main category: eess.SP

TL;DR: 论文提出了一种基于RDARS架构的6G无线网络优化方法，通过联合优化基站和RDARS的波束成形矩阵以及RDARS的模式切换矩阵，最大化加权和速率（WSR）。采用PWM算法结合MM和WMMSE方法，并引入模型驱动的深度学习以提升性能。


<details>
  <summary>Details</summary>
Motivation: RDARS架构为6G网络提供了动态工作模式配置的优势，相比RIS和DAS系统具有额外选择增益。论文旨在解决其下行MIMO系统中WSR最大化问题。

Method: 提出PWM算法，结合MM和WMMSE方法处理非凸目标函数和混合整数约束。进一步集成模型驱动的深度学习以优化收敛速度和性能。

Result: 仿真显示PWM-BFNet算法减少了一半迭代次数，在高发射功率和大RDARS传输元素场景下分别实现了26.53%和103.2%的性能提升。

Conclusion: RDARS架构结合PWM-BFNet算法显著提升了6G网络的性能，验证了其在实际应用中的潜力。

Abstract: Reconfigurable distributed antenna and reflecting surface (RDARS) is a
promising architecture for future sixth-generation (6G) wireless networks. In
particular, the dynamic working mode configuration for the RDARS-aided system
brings an extra selection gain compared to the existing reconfigurable
intelligent surface (RIS)-aided system and distributed antenna system (DAS). In
this paper, we consider the RDARS-aided downlink multiple-input multiple-output
(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly
optimizing the beamforming matrices at the based station (BS) and RDARS, as
well as mode switching matrix at RDARS. The optimization problem is challenging
to be solved due to the non-convex objective function and mixed integer binary
constraint. To this end, a penalty term-based weight minimum mean square error
(PWM) algorithm is proposed by integrating the majorization-minimization (MM)
and weight minimum mean square error (WMMSE) methods. To further escape the
local optimum point in the PWM algorithm, a model-driven DL method is
integrated into this algorithm, where the key variables related to the
convergence of PWM algorithm are trained to accelerate the convergence speed
and improve the system performance. Simulation results are provided to show
that the PWM-based beamforming network (PWM-BFNet) can reduce the number of
iterations by half and achieve performance improvements of 26.53% and 103.2% at
the scenarios of high total transmit power and a large number of RDARS transmit
elements (TEs), respectively.

</details>


### [4] [STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink](https://arxiv.org/abs/2508.00409)
*Mohammad Soleymani,Ignacio Santamaria,Eduard Jorswieck,Robert Schober,Lajos Hanzo*

Main category: eess.SP

TL;DR: RSMA与STAR-RIS结合提升FBL MIMO下行链路的能效，通过联合优化算法实现显著能效增益。


<details>
  <summary>Details</summary>
Motivation: 提升有限块长度MIMO下行链路的能效，解决传统反射RIS和SDMA的局限性。

Method: 提出基于交替优化的算法，联合优化发射波束成形矩阵、STAR-RIS配置和速率分割参数。

Result: 数值结果显示RSMA与STAR-RIS协同作用显著，能效优于反射RIS和SDMA。

Conclusion: RSMA与STAR-RIS结合是提升能效的有效方案，具有实际应用潜力。

Abstract: Rate splitting multiple access (RSMA) is intrinsically amalgamated with
simultaneously transmitting and reflecting (STAR) reconfigurable intelligent
surfaces (RIS) to enhance energy efficiency (EE) of the finite block length
(FBL) multiple-input multiple-output (MIMO) downlink. An alternating
optimization-based algorithm is proposed to jointly optimize the transmit
beamforming matrices, STAR-RIS configurations, and rate-splitting parameters.
STAR-RIS attains 360-degree full-plane coverage, while RSMA provides a
prominent gain by efficiently managing interference. Numerical results reveal a
strong synergy between RSMA and STAR-RIS, demonstreating significant EE gains
over reflective RIS and spatial division multiple access (SDMA).

</details>


### [5] [When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework](https://arxiv.org/abs/2508.00456)
*Ji Wang,Bin Tang,Jian Xiao,Qimei Cui,Xingwang Li,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的多模态波束预测框架，通过对比学习整合多模态数据，显著提升了毫米波波束预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于真实传播环境日益复杂和动态化，传统基于实时信道状态信息（CSI）的方法计算成本高且难以保持准确性，因此需要更高效的方法。

Method: 采用VLM驱动的对比学习框架，通过模态特定编码器整合多模态数据，并利用对比预训练策略对齐图像和LiDAR特征。引入文本提示（位置信息）以增强跨模态一致性。

Result: 在DeepSense-6G数据集上的实验表明，该方法实现了0.9016的DBA-Score，平均提升1.46%。

Conclusion: VLM驱动的多模态框架显著提升了毫米波波束预测的准确性和效率。

Abstract: As the real propagation environment becomes in creasingly complex and
dynamic, millimeter wave beam prediction faces huge challenges. However, the
powerful cross modal representation capability of vision-language model (VLM)
provides a promising approach. The traditional methods that rely on real-time
channel state information (CSI) are computationally expensive and often fail to
maintain accuracy in such environments. In this paper, we present a VLM-driven
contrastive learning based multimodal beam prediction framework that integrates
multimodal data via modality-specific encoders. To enforce cross-modal
consistency, we adopt a contrastive pretraining strategy to align image and
LiDAR features in the latent space. We use location information as text prompts
and connect it to the text encoder to introduce language modality, which
further improves cross-modal consistency. Experiments on the DeepSense-6G
dataset show that our VLM backbone provides additional semantic grounding.
Compared with existing methods, the overall distance-based accuracy score
(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.

</details>


### [6] [Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency](https://arxiv.org/abs/2508.00494)
*Youngsun Kong,Farnoush Baghestani,I-Ping Chen,Ki Chon*

Main category: eess.SP

TL;DR: 研究表明，皮肤神经活动（SKNA）可以从低采样频率（如0.5 kHz或1 kHz）的心电图（ECG）信号中提取，无需传统的高采样频率（>2 kHz）。


<details>
  <summary>Details</summary>
Motivation: 传统SKNA提取需要高采样频率（>2 kHz），但许多ECG设备（尤其是可穿戴设备）采样频率较低（≤1 kHz），限制了SKNA的应用。

Method: 通过从16名参与者的ECG信号中提取SKNA，并对比0.5 kHz、1 kHz和4 kHz采样频率下的结果。

Result: 统计分析显示，不同采样频率（0.5 kHz、1 kHz、4 kHz）下提取的SKNA指标无显著差异。

Conclusion: 低采样频率的ECG设备（如0.5 kHz或1 kHz）可以可靠地收集SKNA，前提是肌肉伪影污染最小。

Abstract: Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has
been a promising non-invasive surrogate for accurate and effective assessment
of the sympathetic nervous system (SNS). Typically, SKNA extraction requires a
higher sampling frequency than the typical ECG recording requirement (> 2 kHz)
because analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,
ECG recording systems commonly provide a sampling frequency of 1 kHz or lower,
particularly for wearable devices. Our recent power spectral analysis exhibited
that 150-500 Hz frequency bands are dominant during sympathetic stimulation.
Therefore, we hypothesize that SKNA can be extracted from ECG sampled at a
lower sampling frequency. We collected ECG signals from 16 participants during
SNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical
analyses of significance, classification performance, and reliability indicate
no significant difference between SKNA indices derived from ECG signals sampled
at 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,
which are limited to low sampling rates due to resource constraints or outdated
guidelines, can be used to reliably collect SKNA if muscle artifact
contamination is minimal.

</details>


### [7] [Subband Architecture Aided Selective Fixed-Filter Active Noise Control](https://arxiv.org/abs/2508.00603)
*Hong-Cheng Liang,Man-Wai Mak,Kong Aik Lee*

Main category: eess.SP

TL;DR: 提出了一种基于延迟子带结构的选择性固定滤波器方案，解决了传统方法对非均匀功率谱密度噪声性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统选择性固定滤波器方法仅能处理有限类型噪声，且在非均匀功率谱密度噪声下性能下降。

Method: 离线训练阶段预训练子带控制滤波器并存储；在线控制阶段通过多相FFT滤波器组分解噪声，使用频带匹配机制选择滤波器，并通过权重叠加技术合成全带滤波器。

Result: 实验表明，该方案具有快速收敛、有效降噪和强鲁棒性。

Conclusion: 新方案能有效处理更复杂的噪声环境。

Abstract: The feedforward selective fixed-filter method selects the most suitable
pre-trained control filter based on the spectral features of the detected
reference signal, effectively avoiding slow convergence in conventional
adaptive algorithms. However, it can only handle limited types of noises, and
the performance degrades when the input noise exhibits non-uniform power
spectral density. To address these limitations, this paper devises a novel
selective fixed-filter scheme based on a delayless subband structure. In the
off-line training stage, subband control filters are pre-trained for different
frequency ranges and stored in a dedicated sub-filter database. During the
on-line control stage, the incoming noise is decomposed using a polyphase FFT
filter bank, and a frequency-band-matching mechanism assigns each subband
signal the most appropriate control filter. Subsequently, a weight stacking
technique is employed to combine all subband weights into a fullband filter,
enabling real-time noise suppression. Experimental results demonstrate that the
proposed scheme provides fast convergence, effective noise reduction, and
strong robustness in handling more complicated noisy environments.

</details>


### [8] [Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding](https://arxiv.org/abs/2508.00800)
*Rui Chen,Wen-Xuan Long,Bing-Qian Wang,Yuan He,Rui-Jin Sun,Nan Cheng,Gan Zheng,Dusit Niyato*

Main category: eess.SP

TL;DR: 综述了多波束高吞吐量卫星（HTS）系统的最新进展，包括硬件基础、资源分配、预编码方法，并讨论了相关挑战。


<details>
  <summary>Details</summary>
Motivation: 卫星通信是6G关键技术，需满足日益增长的流量需求，因此需总结HTS系统的现状及挑战。

Method: 总结了HTS硬件基础、灵活的无线电资源分配方法及多波束预编码技术。

Result: 提出了优化资源利用和干扰消除的方法，并指出了Q/V频段链路中断、同步问题等挑战。

Conclusion: 研究这些课题将提升HTS系统性能，为地面网络覆盖不足地区提供高速数据。

Abstract: With its wide coverage and uninterrupted service, satellite communication is
a critical technology for next-generation 6G communications. High throughput
satellite (HTS) systems, utilizing multipoint beam and frequency multiplexing
techniques, enable satellite communication capacity of up to Tbps to meet the
growing traffic demand. Therefore, it is imperative to review
the-state-of-the-art of multibeam HTS systems and identify their associated
challenges and perspectives. Firstly, we summarize the multibeam HTS hardware
foundations, including ground station systems, on-board payloads, and user
terminals. Subsequently, we review the flexible on-board radio resource
allocation approaches of bandwidth, power, time slot, and joint allocation
schemes of HTS systems to optimize resource utilization and cater to
non-uniform service demand. Additionally, we survey multibeam precoding methods
for the HTS system to achieve full-frequency reuse and interference
cancellation, which are classified according to different deployments such as
single gateway precoding, multiple gateway precoding, on-board precoding, and
hybrid on-board/on-ground precoding. Finally, we disscuss the challenges
related to Q/V band link outage, time and frequency synchronization of
gateways, the accuracy of channel state information (CSI), payload light-weight
development, and the application of deep learning (DL). Research on these
topics will contribute to enhancing the performance of HTS systems and finally
delivering high-speed data to areas underserved by terrestrial networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [9] [Ambisonics Super-Resolution Using A Waveform-Domain Neural Network](https://arxiv.org/abs/2508.00240)
*Ismael Nawfal,Symeon Delikaris Manias,Mehrez Souden,Juha Merimaa,Joshua Atkins,Elisabeth McMullin,Shadi Pirhosseinloo,Daniel Phillips*

Main category: eess.AS

TL;DR: 提出了一种基于数据驱动的空间音频解决方案，利用卷积时域音频神经网络（Conv-TasNet）将一阶Ambisonics（FOA）输入转换为高阶Ambisonics（HOA）输出，显著提升了音质和空间准确性。


<details>
  <summary>Details</summary>
Motivation: 解决一阶Ambisonics（FOA）因通道数量有限导致的空间精度不足问题，同时保留其高效性。

Method: 使用卷积时域音频神经网络（Conv-TasNet），将FOA输入转换为HOA输出。

Result: 定量评估显示预测与实际3阶HOA之间的平均位置均方误差差为0.6dB；定性评估显示感知质量比传统渲染方法提升了80%。

Conclusion: 该方法在保留FOA高效性的同时，显著提升了音质和空间准确性，为空间音频渲染提供了新的数据驱动解决方案。

Abstract: Ambisonics is a spatial audio format describing a sound field. First-order
Ambisonics (FOA) is a popular format comprising only four channels. This
limited channel count comes at the expense of spatial accuracy. Ideally one
would be able to take the efficiency of a FOA format without its limitations.
We have devised a data-driven spatial audio solution that retains the
efficiency of the FOA format but achieves quality that surpasses conventional
renderers. Utilizing a fully convolutional time-domain audio neural network
(Conv-TasNet), we created a solution that takes a FOA input and provides a
higher order Ambisonics (HOA) output. This data driven approach is novel when
compared to typical physics and psychoacoustic based renderers. Quantitative
evaluations showed a 0.6dB average positional mean squared error difference
between predicted and actual 3rd order HOA. The median qualitative rating
showed an 80% improvement in perceived quality over the traditional rendering
approach.

</details>


### [10] [Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization](https://arxiv.org/abs/2508.00307)
*Belman Jahir Rodriguez,Sergio F. Chevtchenko,Marcelo Herrera Martinez,Yeshwant Bethy,Saeed Afshar*

Main category: eess.AS

TL;DR: 提出了一种基于U-net的360度声源定位方法，通过球形语义分割任务实现，避免了传统的离散方向估计。


<details>
  <summary>Details</summary>
Motivation: 传统声源定位方法依赖离散方向估计，限制了精度和适应性。本文旨在通过语义分割实现更密集的空间音频理解。

Method: 使用延迟求和波束形成生成音频图，通过改进的U-net和Tversky损失进行训练，输出后处理为区域质心。

Result: 实验表明，该方法在不同环境下具有通用性，提高了角度精度。

Conclusion: 该方法为密集空间音频理解提供了新范式，超越了传统声源定位方法。

Abstract: We introduce a U-net model for 360{\deg} acoustic source localization
formulated as a spherical semantic segmentation task. Rather than regressing
discrete direction-of-arrival (DoA) angles, our model segments beamformed audio
maps (azimuth and elevation) into regions of active sound presence. Using
delay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate
signals aligned with drone GPS telemetry to create binary supervision masks. A
modified U-Net, trained on frequency-domain representations of these maps,
learns to identify spatially distributed source regions while addressing class
imbalance via the Tversky loss. Because the network operates on beamformed
energy maps, the approach is inherently array-independent and can adapt to
different microphone configurations without retraining from scratch. The
segmentation outputs are post-processed by computing centroids over activated
regions, enabling robust DoA estimates. Our dataset includes real-world
open-field recordings of a DJI Air 3 drone, synchronized with 360{\deg} video
and flight logs across multiple dates and locations. Experimental results show
that U-net generalizes across environments, providing improved angular
precision, offering a new paradigm for dense spatial audio understanding beyond
traditional Sound Source Localization (SSL).

</details>


### [11] [Melody-Lyrics Matching with Contrastive Alignment Loss](https://arxiv.org/abs/2508.00123)
*Changhong Wang,Michel Olvera,Gaël Richard*

Main category: eess.AS

TL;DR: 论文提出了一种名为旋律-歌词匹配（MLM）的新任务，通过自监督表示学习框架和对比对齐损失，利用现有歌曲中的旋律和歌词关系，无需对齐标注，实现了旋律与连贯、可唱歌词的匹配。


<details>
  <summary>Details</summary>
Motivation: 探索音乐与歌词之间超越语义的联系，如节奏与押韵、音符时长与音节重音等，填补音乐信息检索领域的空白。

Method: 提出自监督表示学习框架，使用对比对齐损失，并引入音节级别的歌词表示（sylphone），无需对齐标注。

Result: 方法能够匹配旋律与连贯、可唱的歌词，并通过实证结果和直观示例验证了其有效性。

Conclusion: MLM任务和提出的框架为音乐与歌词关系的研究提供了新方向，开源代码和示例进一步支持了研究的可复现性。

Abstract: The connection between music and lyrics is far beyond semantic bonds.
Conceptual pairs in the two modalities such as rhythm and rhyme, note duration
and syllabic stress, and structure correspondence, raise a compelling yet
seldom-explored direction in the field of music information retrieval. In this
paper, we present melody-lyrics matching (MLM), a new task which retrieves
potential lyrics for a given symbolic melody from text sources. Rather than
generating lyrics from scratch, MLM essentially exploits the relationships
between melody and lyrics. We propose a self-supervised representation learning
framework with contrastive alignment loss for melody and lyrics. This has the
potential to leverage the abundance of existing songs with paired melody and
lyrics. No alignment annotations are required. Additionally, we introduce
sylphone, a novel representation for lyrics at syllable-level activated by
phoneme identity and vowel stress. We demonstrate that our method can match
melody with coherent and singable lyrics with empirical results and intuitive
examples. We open source code and provide matching examples on the companion
webpage: https://github.com/changhongw/mlm.

</details>


### [12] [Wavelet-Based Time-Frequency Fingerprinting for Feature Extraction of Traditional Irish Music](https://arxiv.org/abs/2508.00479)
*Noah Shore*

Main category: eess.AS

TL;DR: 提出了一种基于小波变换的时频指纹方法，用于时间序列特征提取，特别针对爱尔兰传统音乐的音频识别。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列数据中特征识别的挑战，尤其是在实时录音中识别传统爱尔兰曲调的需求。

Method: 采用连续小波变换提取频谱特征，并利用小波相干性分析比较录音音频谱图与基于ABC符号生成的合成曲调。

Result: 实验表明，该方法能准确高效地识别录音曲调，并在其他领域（如EEG信号分析和金融时间序列预测）中表现优异。

Conclusion: 小波相干性模型在时频分解中优于其他方法，具有广泛的应用潜力。

Abstract: This work presents a wavelet-based approach to time-frequency fingerprinting
for time series feature extraction, with a focus on audio identification from
live recordings of traditional Irish tunes. The challenges of identifying
features in time-series data are addressed by employing a continuous wavelet
transform to extract spectral features and wavelet coherence analysis is used
to compare recorded audio spectrograms to synthetically generated tunes. The
synthetic tunes are derived from ABC notation, which is a common symbolic
representation for Irish music. Experimental results demonstrate that the
wavelet-based method can accurately and efficiently identify recorded tunes.
This research study also details the performance of the wavelet coherence
model, highlighting its strengths over other methods of time-frequency
decomposition. Additionally, we discuss and deploy the model on several
applications beyond music, including in EEG signal analysis and financial time
series forecasting.

</details>


### [13] [VR-PTOLEMAIC: A Virtual Environment for the Perceptual Testing of Spatial Audio Algorithms](https://arxiv.org/abs/2508.00501)
*Paolo Ostan,Francesca Del Gaudio,Federico Miotello,Mirco Pezzoli,Fabio Antonacci*

Main category: eess.AS

TL;DR: VR-PTOLEMAIC是一个虚拟现实评估系统，用于评估空间音频算法，通过沉浸式环境支持MUSHRA方法，用户反馈良好。


<details>
  <summary>Details</summary>
Motivation: 开发沉浸式音频应用需要确保合成声场在听感、空间感知和听觉真实性上达标，因此需要有效的评估方法。

Method: 系统将MUSHRA评估方法集成到虚拟环境中，用户可在25个模拟听音位置评估声场重建算法。

Result: 测试表明VR平台有效支持空间音频算法评估，用户体验和沉浸感反馈积极。

Conclusion: VR-PTOLEMAIC为空间音频算法评估提供了高效且用户友好的解决方案。

Abstract: The perceptual evaluation of spatial audio algorithms is an important step in
the development of immersive audio applications, as it ensures that synthesized
sound fields meet quality standards in terms of listening experience, spatial
perception and auditory realism. To support these evaluations, virtual reality
can offer a powerful platform by providing immersive and interactive testing
environments. In this paper, we present VR-PTOLEMAIC, a virtual reality
evaluation system designed for assessing spatial audio algorithms. The system
implements the MUSHRA (MUlti-Stimulus test with Hidden Reference and Anchor)
evaluation methodology into a virtual environment. In particular, users can
position themselves in each of the 25 simulated listening positions of a
virtually recreated seminar room and evaluate simulated acoustic responses with
respect to the actually recorded second-order ambisonic room impulse responses,
all convolved with various source signals. We evaluated the usability of the
proposed framework through an extensive testing campaign in which assessors
were asked to compare the reconstruction capabilities of various sound field
reconstruction algorithms. Results show that the VR platform effectively
supports the assessment of spatial audio algorithms, with generally positive
feedback on user experience and immersivity.

</details>


### [14] [Dynamic Real-Time Ambisonics Order Adaptation for Immersive Networked Music Performances](https://arxiv.org/abs/2508.00509)
*Paolo Ostan,Carlo Centofanti,Mirco Pezzoli,Alberto Bernardini,Claudia Rinaldi,Fabio Antonacci*

Main category: eess.AS

TL;DR: 提出了一种实时自适应高阶Ambisonics策略，动态调整Ambisonics阶数以平衡沉浸感和网络可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决网络音乐表演中高空间保真度和低延迟的需求，同时应对网络带宽波动带来的挑战。

Method: 实时监测网络吞吐量，动态调整Ambisonics阶数，带宽不足时降低阶数防止音频中断，条件恢复时提高阶数。

Result: MUSHRA评估表明该方法在带宽受限场景下能有效保障用户体验。

Conclusion: 自适应策略在网络音乐表演中平衡了沉浸感和可靠性，具有应用潜力。

Abstract: Advanced remote applications such as Networked Music Performance (NMP)
require solutions to guarantee immersive real-world-like interaction among
users. Therefore, the adoption of spatial audio formats, such as Ambisonics, is
fundamental to let the user experience an immersive acoustic scene. The
accuracy of the sound scene reproduction increases with the order of the
Ambisonics enconding, resulting in an improved immersivity at the cost of a
greater number of audio channels, which in turn escalates both bandwidth
requirements and susceptibility to network impairments (e.g., latency, jitter,
and packet loss). These factors pose a significant challenge for interactive
music sessions, which demand high spatial fidelity and low end-to-end delay. We
propose a real-time adaptive higher-order Ambisonics strategy that continuously
monitors network throughput and dynamically scales the Ambisonics order. When
available bandwidth drops below a preset threshold, the order is lowered to
prevent audio dropouts; it then reverts to higher orders once conditions
recover, thus balancing immersion and reliability. A MUSHRA-based evaluation
indicates that this adaptive approach is promising to guarantee user experience
in bandwidth-limited NMP scenarios.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [15] [Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities](https://arxiv.org/abs/2508.00317)
*Wen-Chin Huang*

Main category: cs.SD

TL;DR: 本文回顾了语音质量评估（SQA）的近期挑战和开源工具，强调了这些活动对推动SQA和语音生成AI发展的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的兴起，开发能准确反映人类感知的自动SQA方法变得愈发重要。

Method: 回顾了SQA的近期挑战和开源实现及工具包。

Result: 自动SQA已在研究论文中作为语音生成系统质量的严格测量标准。

Conclusion: 维持开源活动和科学挑战对SQA和语音生成AI的发展至关重要。

Abstract: Speech quality assessment (SQA) refers to the evaluation of speech quality,
and developing an accurate automatic SQA method that reflects human perception
has become increasingly important, in order to keep up with the generative AI
boom. In recent years, SQA has progressed to a point that researchers started
to faithfully use automatic SQA in research papers as a rigorous measurement of
goodness for speech generation systems. We believe that the scientific
challenges and open-source activities of late have stimulated the growth in
this field. In this paper, we review recent challenges as well as open-source
implementations and toolkits for SQA, and highlight the importance of
maintaining such activities to facilitate the development of not only SQA
itself but also generative AI for speech.

</details>


### [16] [AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation](https://arxiv.org/abs/2508.00733)
*Le Wang,Jun Wang,Feng Deng,Chen Zhang,Kun Gai,Di Zhang*

Main category: cs.SD

TL;DR: AudioGen-Omni是一个基于多模态扩散变换器（MMDit）的统一方法，能够生成与输入视频同步的高保真音频、语音和歌曲。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨模态音频生成任务中的语义限制和效率问题。

Method: 采用联合训练范式，结合视频-文本-音频数据，使用统一的歌词-转录编码器和AdaLN联合注意力机制。

Result: 在音频质量、语义对齐和唇同步准确性方面表现优异，推理时间短（1.91秒生成8秒音频）。

Conclusion: AudioGen-Omni在跨模态音频生成任务中实现了高效和通用性，达到了最先进的性能。

Abstract: We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.

</details>

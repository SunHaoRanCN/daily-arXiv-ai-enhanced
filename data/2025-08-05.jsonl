{"id": "2508.01034", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01034", "abs": "https://arxiv.org/abs/2508.01034", "authors": ["Rishith Sadashiv T N", "Abhishek Bedge", "Saisha Suresh Bore", "Jagabandhu Mishra", "Mrinmoy Bhattacharjee", "S R Mahadeva Prasanna"], "title": "Fusion of Modulation Spectrogram and SSL with Multi-head Attention for Fake Speech Detection", "comment": null, "summary": "Fake speech detection systems have become a necessity to combat against\nspeech deepfakes. Current systems exhibit poor generalizability on\nout-of-domain speech samples due to lack to diverse training data. In this\npaper, we attempt to address domain generalization issue by proposing a novel\nspeech representation using self-supervised (SSL) speech embeddings and the\nModulation Spectrogram (MS) feature. A fusion strategy is used to combine both\nspeech representations to introduce a new front-end for the classification\ntask. The proposed SSL+MS fusion representation is passed to the AASIST\nback-end network. Experiments are conducted on monolingual and multilingual\nfake speech datasets to evaluate the efficacy of the proposed model\narchitecture in cross-dataset and multilingual cases. The proposed model\nachieves a relative performance improvement of 37% and 20% on the ASVspoof 2019\nand MLAAD datasets, respectively, in in-domain settings compared to the\nbaseline. In the out-of-domain scenario, the model trained on ASVspoof 2019\nshows a 36% relative improvement when evaluated on the MLAAD dataset. Across\nall evaluated languages, the proposed model consistently outperforms the\nbaseline, indicating enhanced domain generalization."}
{"id": "2508.01467", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01467", "abs": "https://arxiv.org/abs/2508.01467", "authors": ["Haohan Shi", "Xiyu Shi", "Safak Dogan", "Tianjin Huang", "Yunxiao Zhang"], "title": "Multi-Granularity Adaptive Time-Frequency Attention Framework for Audio Deepfake Detection under Real-World Communication Degradations", "comment": null, "summary": "The rise of highly convincing synthetic speech poses a growing threat to\naudio communications. Although existing Audio Deepfake Detection (ADD) methods\nhave demonstrated good performance under clean conditions, their effectiveness\ndrops significantly under degradations such as packet losses and speech codec\ncompression in real-world communication environments. In this work, we propose\nthe first unified framework for robust ADD under such degradations, which is\ndesigned to effectively accommodate multiple types of Time-Frequency (TF)\nrepresentations. The core of our framework is a novel Multi-Granularity\nAdaptive Attention (MGAA) architecture, which employs a set of customizable\nmulti-scale attention heads to capture both global and local receptive fields\nacross varying TF granularities. A novel adaptive fusion mechanism subsequently\nadjusts and fuses these attention branches based on the saliency of TF regions,\nallowing the model to dynamically reallocate its focus according to the\ncharacteristics of the degradation. This enables the effective localization and\namplification of subtle forgery traces. Extensive experiments demonstrate that\nthe proposed framework consistently outperforms state-of-the-art baselines\nacross various real-world communication degradation scenarios, including six\nspeech codecs and five levels of packet losses. In addition, comparative\nanalysis reveals that the MGAA-enhanced features significantly improve\nseparability between real and fake audio classes and sharpen decision\nboundaries. These results highlight the robustness and practical deployment\npotential of our framework in real-world communication environments."}
{"id": "2508.01576", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01576", "abs": "https://arxiv.org/abs/2508.01576", "authors": ["Jeanelle Dao", "Jadelynn Dao"], "title": "Lumename: Wearable Device for Hearing Impaired with Personalized ML-Based Auditory Detection and Haptic-Visual Alerts", "comment": null, "summary": "According to the World Health Organization, 430 million people experience\ndisabling hearing loss. For them, recognizing spoken commands such as one's\nname is difficult. To address this issue, Lumename, a real-time smartwatch,\nutilizes on-device machine learning to detect a user-customized name before\ngenerating a haptic-visual alert. During training, to overcome the need for\nlarge datasets, Lumename uses novel audio modulation techniques to augment\nsamples from one user and generate additional samples to represent diverse\ngenders and ages. Constrained random iterations were used to find optimal\nparameters within the model architecture. This approach resulted in a\nlow-resource and low-power TinyML model that could quickly infer various\nkeyword samples while remaining 91.67\\% accurate on a custom-built smartwatch\nbased on an Arduino Nano 33 BLE Sense."}
{"id": "2508.01637", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01637", "abs": "https://arxiv.org/abs/2508.01637", "authors": ["Jiusi Zheng", "Vishwas Shetty", "Natarajan Balaji Shankar", "Abeer Alwan"], "title": "An Age-Agnostic System for Robust Speaker Verification", "comment": "Accepted to the Interspeech 2025 Workshop on Child Computer\n  Interaction", "summary": "In speaker verification (SV), the acoustic mismatch between children's and\nadults' speech leads to suboptimal performance when adult-trained SV systems\nare applied to children's speaker verification (C-SV). While domain adaptation\ntechniques can enhance performance on C-SV tasks, they often do so at the\nexpense of significant degradation in performance on adults' SV (A-SV) tasks.\nIn this study, we propose an Age Agnostic Speaker Verification (AASV) system\nthat achieves robust performance across both C-SV and A-SV tasks. Our approach\nemploys a domain classifier to disentangle age-related attributes from speech\nand subsequently expands the embedding space using the extracted domain\ninformation, forming a unified speaker representation that is robust and highly\ndiscriminative across age groups. Experiments on the OGI and VoxCeleb datasets\ndemonstrate the effectiveness of our approach in bridging SV performance\ndisparities, laying the foundation for inclusive and age-adaptive SV systems."}
{"id": "2508.01166", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.01166", "abs": "https://arxiv.org/abs/2508.01166", "authors": ["Bingshen Mu", "Hexin Liu", "Hongfei Xue", "Kun Wei", "Lei Xie"], "title": "Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR", "comment": null, "summary": "Automatic Speech Recognition (ASR) aims to convert human speech content into\ncorresponding text. In conversational scenarios, effectively utilizing context\ncan enhance its accuracy. Large Language Models' (LLMs) exceptional\nlong-context understanding and reasoning abilities enable LLM-based ASR\n(LLM-ASR) to leverage historical context for recognizing conversational speech,\nwhich has a high degree of contextual relevance. However, existing\nconversational LLM-ASR methods use a fixed number of preceding utterances or\nthe entire conversation history as context, resulting in significant ASR\nconfusion and computational costs due to massive irrelevant and redundant\ninformation. This paper proposes a multi-modal retrieval-and-selection method\nnamed MARS that augments conversational LLM-ASR by enabling it to retrieve and\nselect the most relevant acoustic and textual historical context for the\ncurrent utterance. Specifically, multi-modal retrieval obtains a set of\ncandidate historical contexts, each exhibiting high acoustic or textual\nsimilarity to the current utterance. Multi-modal selection calculates the\nacoustic and textual similarities for each retrieved candidate historical\ncontext and, by employing our proposed near-ideal ranking method to consider\nboth similarities, selects the best historical context. Evaluations on the\nInterspeech 2025 Multilingual Conversational Speech Language Model Challenge\ndataset show that the LLM-ASR, when trained on only 1.5K hours of data and\nequipped with the MARS, outperforms the state-of-the-art top-ranking system\ntrained on 179K hours of data."}
{"id": "2508.01007", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01007", "abs": "https://arxiv.org/abs/2508.01007", "authors": ["Hanyoung Park", "Ji-Woong Choi"], "title": "Binary Hypothesis Testing-Based Low-Complexity Beamspace Channel Estimation for mmWave Massive MIMO Systems", "comment": "Submitted to a journal", "summary": "Millimeter-wave (mmWave) communications have gained attention as a key\ntechnology for high-capacity wireless systems, owing to the wide available\nbandwidth. However, mmWave signals suffer from their inherent characteristics\nsuch as severe path loss, poor scattering, and limited diffraction, which\nnecessitate the use of large antenna arrays and directional beamforming,\ntypically implemented through massive MIMO architectures. Accurate channel\nestimation is critical in such systems, but its computational complexity\nincreases proportionally with the number of antennas. This may become a\nsignificant burden in mmWave systems where channels exhibit rapid fluctuations\nand require frequent updates. In this paper, we propose a low-complexity\nchannel denoiser based on Bayesian binary hypothesis testing and beamspace\nsparsity. By modeling each sparse beamspace component as a mixture of signal\nand noise under a Bernoulli-complex Gaussian prior, we formulate a likelihood\nratio test to detect signal-relevant elements. Then, a hard-thresholding rule\nis applied to suppress noise-dominant components in the noisy channel vector.\nDespite its extremely low computational complexity, the proposed method\nachieves channel estimation accuracy that is comparable to that of complex\niterative or learning-based approaches. This effectiveness is supported by both\ntheoretical analysis and numerical evaluation, suggesting that the method can\nbe a viable option for mmWave systems with strict resource constraints."}
{"id": "2508.01847", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.01847", "abs": "https://arxiv.org/abs/2508.01847", "authors": ["Avishkar Behera", "Riya Ann Easow", "Venkatesh Parvathala", "K. Sri Rama Murty"], "title": "Test-Time Training for Speech Enhancement", "comment": "Accepted to Interspeech 2025. 5 pages, 2 figures", "summary": "This paper introduces a novel application of Test-Time Training (TTT) for\nSpeech Enhancement, addressing the challenges posed by unpredictable noise\nconditions and domain shifts. This method combines a main speech enhancement\ntask with a self-supervised auxiliary task in a Y-shaped architecture. The\nmodel dynamically adapts to new domains during inference time by optimizing the\nproposed self-supervised tasks like noise-augmented signal reconstruction or\nmasked spectrogram prediction, bypassing the need for labeled data. We further\nintroduce various TTT strategies offering a trade-off between adaptation and\nefficiency. Evaluations across synthetic and real-world datasets show\nconsistent improvements across speech quality metrics, outperforming the\nbaseline model. This work highlights the effectiveness of TTT in speech\nenhancement, providing insights for future research in adaptive and robust\nspeech processing."}
{"id": "2508.01172", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01172", "abs": "https://arxiv.org/abs/2508.01172", "authors": ["Fan Wu", "Kaicheng Zhao", "Elgar Fleisch", "Filipe Barata"], "title": "GeHirNet: A Gender-Aware Hierarchical Model for Voice Pathology Classification", "comment": null, "summary": "AI-based voice analysis shows promise for disease diagnostics, but existing\nclassifiers often fail to accurately identify specific pathologies because of\ngender-related acoustic variations and the scarcity of data for rare diseases.\nWe propose a novel two-stage framework that first identifies gender-specific\npathological patterns using ResNet-50 on Mel spectrograms, then performs\ngender-conditioned disease classification. We address class imbalance through\nmulti-scale resampling and time warping augmentation. Evaluated on a merged\ndataset from four public repositories, our two-stage architecture with time\nwarping achieves state-of-the-art performance (97.63\\% accuracy, 95.25\\% MCC),\nwith a 5\\% MCC improvement over single-stage baseline. This work advances voice\npathology classification while reducing gender bias through hierarchical\nmodeling of vocal characteristics."}
{"id": "2508.01044", "categories": ["eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.01044", "abs": "https://arxiv.org/abs/2508.01044", "authors": ["Mehdi Zafari", "Rang Liu", "A. Lee Swindlehurst"], "title": "Coordinated Decentralized Resource Optimization for Cell-Free ISAC Systems", "comment": "Accepted to the 2025 IEEE Asilomar Conference on Signals, Systems,\n  and Computers. This work was supported by the National Science Foundation\n  (NSF) under Grant CCF-2322191. A revised version with final results will be\n  uploaded after camera-ready submission", "summary": "Integrated Sensing and Communication (ISAC) is emerging as a key enabler for\n6G wireless networks, allowing the joint use of spectrum and infrastructure for\nboth communication and sensing. While prior ISAC solutions have addressed\nresource optimization, including power allocation, beamforming, and waveform\ndesign, they often rely on centralized architectures with full network\nknowledge, limiting their scalability in distributed systems. In this paper, we\npropose two coordinated decentralized optimization algorithms for beamforming\nand power allocation tailored to cell-free ISAC networks. The first algorithm\nemploys locally designed fixed beamformers at access points (APs), combined\nwith a centralized power allocation scheme computed at a central server (CS).\nThe second algorithm jointly optimizes beamforming and power control through a\nfully decentralized consensus ADMM framework. Both approaches rely on local\ninformation at APs and limited coordination with the CS. Simulation results\nobtained using our proposed Python-based simulation framework evaluate their\nfronthaul overhead and system-level performance, demonstrating their\npracticality for scalable ISAC deployment in decentralized, cell-free\narchitectures."}
{"id": "2508.02112", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02112", "abs": "https://arxiv.org/abs/2508.02112", "authors": ["Thilo von Neumann", "Christoph Boeddeker", "Marc Delcroix", "Reinhold Haeb-Umbach"], "title": "Word Error Rate Definitions and Algorithms for Long-Form Multi-talker Speech Recognition", "comment": "Accepted for IEEE Transactions on Audio Speech and Language\n  Processing (TASLP), vol. 33", "summary": "The predominant metric for evaluating speech recognizers, the Word Error Rate\n(WER) has been extended in different ways to handle transcripts produced by\nlong-form multi-talker speech recognizers. These systems process long\ntranscripts containing multiple speakers and complex speaking patterns so that\nthe classical WER cannot be applied. There are speaker-attributed approaches\nthat count speaker confusion errors, such as the concatenated\nminimum-permutation WER cpWER and the time-constrained cpWER (tcpWER), and\nspeaker-agnostic approaches, which aim to ignore speaker confusion errors, such\nas the Optimal Reference Combination WER (ORC-WER) and the MIMO-WER. These WERs\nevaluate different aspects and error types (e.g., temporal misalignment). A\ndetailed comparison has not been made. We therefore present a unified\ndescription of the existing WERs and highlight when to use which metric. To\nfurther analyze how many errors are caused by speaker confusion, we propose the\nDiarization-invariant cpWER (DI-cpWER). It ignores speaker attribution errors\nand its difference to cpWER reflects the impact of speaker confusions on the\nWER. Since error types cannot reliably be classified automatically, we discuss\nways to visualize sequence alignments between the reference and hypothesis\ntranscripts to facilitate the spotting of errors by a human judge. Since some\nWER definitions have high computational complexity, we introduce a greedy\nalgorithm to approximate the ORC-WER and DI-cpWER with high precision ($<0.1\\%$\ndeviation in our experiments) and polynomial complexity instead of exponential.\nTo improve the plausibility of the metrics, we also incorporate the time\nconstraint from the tcpWER into ORC-WER and MIMO-WER, also significantly\nreducing the computational complexity."}
{"id": "2508.01178", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01178", "abs": "https://arxiv.org/abs/2508.01178", "authors": ["Yi Jiang", "Wei Wang", "Xianwen Guo", "Huiyun Liu", "Hanrui Wang", "Youri Xu", "Haoqi Gu", "Zhongqian Xie", "Chuanjiang Luo"], "title": "Advancing the Foundation Model for Music Understanding", "comment": null, "summary": "The field of Music Information Retrieval (MIR) is fragmented, with\nspecialized models excelling at isolated tasks. In this work, we challenge this\nparadigm by introducing a unified foundation model named MuFun for holistic\nmusic understanding. Our model features a novel architecture that jointly\nprocesses instrumental and lyrical content, and is trained on a large-scale\ndataset covering diverse tasks such as genre classification, music tagging, and\nquestion answering. To facilitate robust evaluation, we also propose a new\nbenchmark for multi-faceted music understanding called MuCUE (Music\nComprehensive Understanding Evaluation). Experiments show our model\nsignificantly outperforms existing audio large language models across the MuCUE\ntasks, demonstrating its state-of-the-art effectiveness and generalization\nability."}
{"id": "2508.01121", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01121", "abs": "https://arxiv.org/abs/2508.01121", "authors": ["Joshua Wong", "Kin Tsang"], "title": "A Highly Available GTFS-RT Positions System", "comment": null, "summary": "We develop a system for real-time public transportation data, deciding to use\nthe data standard GTFS-RT (GTFS Realtime), an open data format for public\ntransit data. We give an overview of the design of a physical GPS sensor\ndevice, its firmware, and processes. Next, we give the algorithms used to\ntranslate raw sensor data into a public GTFS-RT data feed. We deploy this feed\nover a highly available cluster across multiple regions to maintain high\navailability."}
{"id": "2508.02228", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02228", "abs": "https://arxiv.org/abs/2508.02228", "authors": ["Eyal Cohen", "Bhiksha Raj", "Joseph Keshet"], "title": "Guiding an Automatic Speech Recognition Decoder Using Large Language Models", "comment": "11 pages, 2 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Automatic Speech Recognition (ASR) consists of an acoustic model (AM) and a\nlanguage model (LM). The AM estimates the probability of an acoustic signal\nbased on a sequence of linguistic units, typically phones, characters, or\ntokens, while the LM assesses the likelihood of a specific sequence of words or\ntokens. Although Large Language Models (LLMs) have demonstrated significant\npotential across various tasks, integrating them into ASR remains an open\nchallenge. By decomposing the maximum a posteriori (MAP) estimator of words (or\ntokens) given the acoustic signal, we derive an iterative procedure that\nfacilitates a novel integration of the AM and LLM, while maintaining their\nseparability. This approach enables each component to be independently trained\nand improved using its own data, thereby maximizing the system's performance by\nleveraging the strengths of both models without requiring joint optimization.\nWe illustrate the effectiveness of our method in comparison to three language\nmodels: N-gram, GCNN, and TransformerLM across multiple datasets spanning\nvarious speech styles, including ALLSSTAR, WSJ0, and TED-LIUM 3. Our\nexperiments involved two acoustic models (wav2vec 2.0 and HuBERT) and three\nLLMs (GPT-2, LLaMA 2, and Falcon). Notably, our method demonstrates particular\nefficacy in addressing complex speech sentences, acronyms, and domain-specific\nvocabulary."}
{"id": "2508.01277", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.01277", "abs": "https://arxiv.org/abs/2508.01277", "authors": ["Raphael Schwinger", "Paria Vali Zadeh", "Lukas Rauch", "Mats Kurz", "Tom Hauschild", "Sam Lapp", "Sven Tomforde"], "title": "Foundation Models for Bioacoustics -- a Comparative Review", "comment": "Preprint", "summary": "Automated bioacoustic analysis is essential for biodiversity monitoring and\nconservation, requiring advanced deep learning models that can adapt to diverse\nbioacoustic tasks. This article presents a comprehensive review of large-scale\npretrained bioacoustic foundation models and systematically investigates their\ntransferability across multiple bioacoustic classification tasks. We overview\nbioacoustic representation learning including major pretraining data sources\nand benchmarks. On this basis, we review bioacoustic foundation models by\nthoroughly analysing design decisions such as model architecture, pretraining\nscheme, and training paradigm. Additionally, we evaluate selected foundation\nmodels on classification tasks from the BEANS and BirdSet benchmarks, comparing\nthe generalisability of learned representations under both linear and attentive\nprobing strategies. Our comprehensive experimental analysis reveals that\nBirdMAE, trained on large-scale bird song data with a self-supervised\nobjective, achieves the best performance on the BirdSet benchmark. On BEANS,\nBEATs$_{NLM}$, the extracted encoder of the NatureLM-audio large audio model,\nis slightly better. Both transformer-based models require attentive probing to\nextract the full performance of their representations. ConvNext$_{BS}$ and\nPerch models trained with supervision on large-scale bird song data remain\ncompetitive for passive acoustic monitoring classification tasks of BirdSet in\nlinear probing settings. Training a new linear classifier has clear advantages\nover evaluating these models without further training. While on BEANS, the\nbaseline model BEATs trained with self-supervision on AudioSet outperforms\nbird-specific models when evaluated with attentive probing. These findings\nprovide valuable guidance for practitioners selecting appropriate models to\nadapt them to new bioacoustic classification tasks via probing."}
{"id": "2508.01283", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.01283", "abs": "https://arxiv.org/abs/2508.01283", "authors": ["Xuehan Wang", "Jinhong Yuan", "Jintao Wang", "Zhi Sun"], "title": "On the Characterization and Evaluation of Doppler Squint in Wideband ODDM Systems", "comment": "6 pages, 4 figures. This paper has been accepted by IEEE Globecom\n  2025", "summary": "The recently proposed orthogonal delay-Doppler division multiplexing (ODDM)\nmodulation has been demonstrated to enjoy excellent reliability over\ndoubly-dispersive channels. However, most of the prior analysis tends to ignore\nthe interactive dispersion caused by the wideband property of ODDM signal,\nwhich possibly leads to performance degradation. To solve this problem, we\ninvestigate the input-output relation of ODDM systems considering the wideband\neffect, which is also known as the Doppler squint effect (DSE) in the\nliterature. The extra delay-Doppler (DD) dispersion caused by the DSE is first\nexplicitly explained by employing the time-variant frequency response of\nmultipath channels. Its characterization is then derived for both reduced\ncyclic prefix (RCP) and zero padded (ZP)-based wideband ODDM systems, where the\nextra DD spread and more complicated power leakage outside the peak region are\npresented theoretically. Numerical results are finally provided to confirm the\nsignificance of DSE. The derivations in this paper are beneficial for\ndeveloping accurate signal processing techniques in ODDM-based integrated\nsensing and communication systems."}
{"id": "2508.02295", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.02295", "abs": "https://arxiv.org/abs/2508.02295", "authors": ["Yangyang Qu", "Michele Panariello", "Massimiliano Todisco", "Nicholas Evans"], "title": "Reference-free Adversarial Sex Obfuscation in Speech", "comment": null, "summary": "Sex conversion in speech involves privacy risks from data collection and\noften leaves residual sex-specific cues in outputs, even when target speaker\nreferences are unavailable. We introduce RASO for Reference-free Adversarial\nSex Obfuscation. Innovations include a sex-conditional adversarial learning\nframework to disentangle linguistic content from sex-related acoustic markers\nand explicit regularisation to align fundamental frequency distributions and\nformant trajectories with sex-neutral characteristics learned from sex-balanced\ntraining data. RASO preserves linguistic content and, even when assessed under\na semi-informed attack model, it significantly outperforms a competing approach\nto sex obfuscation."}
{"id": "2508.01394", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01394", "abs": "https://arxiv.org/abs/2508.01394", "authors": ["Tongxi Wang", "Yang Yu", "Qing Wang", "Junlang Qian"], "title": "Via Score to Performance: Efficient Human-Controllable Long Song Generation with Bar-Level Symbolic Notation", "comment": null, "summary": "Song generation is regarded as the most challenging problem in music AIGC;\nnonetheless, existing approaches have yet to fully overcome four persistent\nlimitations: controllability, generalizability, perceptual quality, and\nduration. We argue that these shortcomings stem primarily from the prevailing\nparadigm of attempting to learn music theory directly from raw audio, a task\nthat remains prohibitively difficult for current models. To address this, we\npresent Bar-level AI Composing Helper (BACH), the first model explicitly\ndesigned for song generation through human-editable symbolic scores. BACH\nintroduces a tokenization strategy and a symbolic generative procedure tailored\nto hierarchical song structure. Consequently, it achieves substantial gains in\nthe efficiency, duration, and perceptual quality of song generation.\nExperiments demonstrate that BACH, with a small model size, establishes a new\nSOTA among all publicly reported song generation systems, even surpassing\ncommercial solutions such as Suno. Human evaluations further confirm its\nsuperiority across multiple subjective metrics."}
{"id": "2508.01510", "categories": ["eess.SP", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01510", "abs": "https://arxiv.org/abs/2508.01510", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "title": "DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset", "comment": null, "summary": "A fully customisable chip-on board (COB) LED design to evoke two brain\nresponses simultaneously (steady state visual evoked potential (SSVEP) and\ntransient evoked potential, P300) is discussed in this paper. Considering\ndifferent possible modalities in braincomputer interfacing (BCI), SSVEP is\nwidely accepted as it requires a lesser number of electroencephalogram (EEG)\nelectrodes and minimal training time. The aim of this work was to produce a\nhybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced\nfatigue and improved classification performance. The system comprises of four\nindependent radial green visual stimuli controlled individually by a 32-bit\nmicrocontroller platform to evoke SSVEP and four red LEDs flashing at random\nintervals to generate P300 events. The system can also record the P300 event\ntimestamps that can be used in classification, to improve the accuracy and\nreliability. The hybrid stimulus was tested for realtime classification\naccuracy by controlling a LEGO robot to move in four directions."}
{"id": "2508.02483", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02483", "abs": "https://arxiv.org/abs/2508.02483", "authors": ["Jule Pohlhausen", "Jörg Bitzer"], "title": "Revisiting the Privacy of Low-Frequency Speech Signals: Exploring Resampling Methods, Evaluation Scenarios, and Speaker Characteristics", "comment": "Accepted at SPSC 2025 - 5th Symposium on Security and Privacy in\n  Speech Communication", "summary": "While audio recordings in real life provide insights into social dynamics and\nconversational behavior, they also raise concerns about the privacy of\npersonal, sensitive data. This article explores the effectiveness of\nrestricting recordings to low-frequency audio to protect spoken content. For\nresampling the audio signals to different sampling rates, we compare the effect\nof employing anti-aliasing filtering. Privacy enhancement is measured by an\nincreased word error rate of automatic speech recognition models. The impact on\nutility performance is measured with voice activity detection models. Our\nexperimental results show that for clean recordings, models trained with a\nsampling rate of up to 800 Hz transcribe the majority of words correctly. For\nboth models, we analyzed the impact of the speaker's sex and pitch, and we\ndemonstrated that missing anti-aliasing filters more strongly compromise speech\nprivacy."}
{"id": "2508.01488", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01488", "abs": "https://arxiv.org/abs/2508.01488", "authors": ["Alain Riou", "Bernardo Torres", "Ben Hayes", "Stefan Lattner", "Gaëtan Hadjeres", "Gaël Richard", "Geoffroy Peeters"], "title": "PESTO: Real-Time Pitch Estimation with Self-supervised Transposition-equivariant Objective", "comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval", "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."}
{"id": "2508.01689", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01689", "abs": "https://arxiv.org/abs/2508.01689", "authors": ["Yichen Jin", "Zongze Li", "Zeyi Ren", "Qingfeng Lin", "Yik-Chung Wu"], "title": "Balancing Latency and Model Accuracy for Fluid Antenna-Assisted LM-Embedded MIMO Network", "comment": "6pages, 6figures, accepted by Globecom 2025", "summary": "This paper addresses the challenge of large model (LM)-embedded wireless\nnetwork for handling the trade-off problem of model accuracy and network\nlatency. To guarantee a high-quality of users' service, the network latency\nshould be minimized while maintaining an acceptable inference accuracy. To meet\nthis requirement, LM quantization is proposed to reduce the latency. However,\nthe excessive quantization may destroy the accuracy of LM inference. To this\nend, a promising fluid antenna (FA) technology is investigated for enhancing\nthe transmission capacity, leading to a lower network latency in the\nLM-embedded multiple-input multiple-output (MIMO) network. To design the\nFA-assisted LM-embedded network with the lower latency and higher accuracy\nrequirements, the latency and peak signal to noise ratio (PSNR) are considered\nin the objective function. Then, an efficient optimization algorithm is\nproposed under the block coordinate descent framework. Simulation results are\nprovided to show the convergence behavior of the proposed algorithm, and the\nperformance gains from the proposed FA-assisted LMembedded network over the\nother benchmark networks in terms of network latency and PSNR."}
{"id": "2508.01277", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.01277", "abs": "https://arxiv.org/abs/2508.01277", "authors": ["Raphael Schwinger", "Paria Vali Zadeh", "Lukas Rauch", "Mats Kurz", "Tom Hauschild", "Sam Lapp", "Sven Tomforde"], "title": "Foundation Models for Bioacoustics -- a Comparative Review", "comment": "Preprint", "summary": "Automated bioacoustic analysis is essential for biodiversity monitoring and\nconservation, requiring advanced deep learning models that can adapt to diverse\nbioacoustic tasks. This article presents a comprehensive review of large-scale\npretrained bioacoustic foundation models and systematically investigates their\ntransferability across multiple bioacoustic classification tasks. We overview\nbioacoustic representation learning including major pretraining data sources\nand benchmarks. On this basis, we review bioacoustic foundation models by\nthoroughly analysing design decisions such as model architecture, pretraining\nscheme, and training paradigm. Additionally, we evaluate selected foundation\nmodels on classification tasks from the BEANS and BirdSet benchmarks, comparing\nthe generalisability of learned representations under both linear and attentive\nprobing strategies. Our comprehensive experimental analysis reveals that\nBirdMAE, trained on large-scale bird song data with a self-supervised\nobjective, achieves the best performance on the BirdSet benchmark. On BEANS,\nBEATs$_{NLM}$, the extracted encoder of the NatureLM-audio large audio model,\nis slightly better. Both transformer-based models require attentive probing to\nextract the full performance of their representations. ConvNext$_{BS}$ and\nPerch models trained with supervision on large-scale bird song data remain\ncompetitive for passive acoustic monitoring classification tasks of BirdSet in\nlinear probing settings. Training a new linear classifier has clear advantages\nover evaluating these models without further training. While on BEANS, the\nbaseline model BEATs trained with self-supervision on AudioSet outperforms\nbird-specific models when evaluated with attentive probing. These findings\nprovide valuable guidance for practitioners selecting appropriate models to\nadapt them to new bioacoustic classification tasks via probing."}
{"id": "2508.01493", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01493", "abs": "https://arxiv.org/abs/2508.01493", "authors": ["Bernardo Torres", "Alain Riou", "Gaël Richard", "Geoffroy Peeters"], "title": "Translation-Equivariant Self-Supervised Learning for Pitch Estimation with Optimal Transport", "comment": "Extended Abstracts for the Late-Breaking Demo Session of the 26th\n  International Society for Music Information Retrieval Conference", "summary": "In this paper, we propose an Optimal Transport objective for learning\none-dimensional translation-equivariant systems and demonstrate its\napplicability to single pitch estimation. Our method provides a theoretically\ngrounded, more numerically stable, and simpler alternative for training\nstate-of-the-art self-supervised pitch estimators."}
{"id": "2508.01709", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01709", "abs": "https://arxiv.org/abs/2508.01709", "authors": ["Ljupcho Milosheski", "Mihael Mohorčič", "Carolina Fortuna"], "title": "Spectrum Sensing with Deep Clustering: Label-Free Radio Access Technology Recognition", "comment": "IEEE Open Journal of the Communication Society", "summary": "The growth of the number of connected devices and network densification is\ndriving an increasing demand for radio network resources, particularly Radio\nFrequency (RF) spectrum. Given the dynamic and complex nature of contemporary\nwireless environments, characterized by a wide variety of devices and multiple\nRATs, spectrum sensing is envisioned to become a building component of future\n6G, including as a component within O-RAN or digital twins. However, the\ncurrent SotA research for RAT classification predominantly revolves around\nsupervised Convolutional Neural Network (CNN)-based approach that require\nextensive labeled dataset. Due to this, it is unclear how existing models\nbehave in environments for which training data is unavailable thus leaving open\nquestions regarding their generalization capabilities. In this paper, we\npropose a new spectrum sensing workflow in which the model training does not\nrequire any prior knowledge of the RATs transmitting in that area (i.e. no\nlabelled data) and the class assignment can be easily done through manual\nmapping. Furthermore, we adapt a SSL deep clustering architecture capable of\nautonomously extracting spectrum features from raw 1D Fast Fourier Transform\n(FFT) data. We evaluate the proposed architecture on three real-world datasets\nfrom three European cities, in the 868 MHz, 2.4 GHz and 5.9 GHz bands\ncontaining over 10 RATs and show that the developed model achieves superior\nperformance by up to 35 percentage points with 22% fewer trainable parameters\nand 50% less floating-point operations per second (FLOPS) compared to an SotA\nAE-based reference architecture."}
{"id": "2508.01493", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01493", "abs": "https://arxiv.org/abs/2508.01493", "authors": ["Bernardo Torres", "Alain Riou", "Gaël Richard", "Geoffroy Peeters"], "title": "Translation-Equivariant Self-Supervised Learning for Pitch Estimation with Optimal Transport", "comment": "Extended Abstracts for the Late-Breaking Demo Session of the 26th\n  International Society for Music Information Retrieval Conference", "summary": "In this paper, we propose an Optimal Transport objective for learning\none-dimensional translation-equivariant systems and demonstrate its\napplicability to single pitch estimation. Our method provides a theoretically\ngrounded, more numerically stable, and simpler alternative for training\nstate-of-the-art self-supervised pitch estimators."}
{"id": "2508.01498", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01498", "abs": "https://arxiv.org/abs/2508.01498", "authors": ["Rajarshi Ghosh", "Jayanth Athipatla"], "title": "ShrutiSense: Microtonal Modeling and Correction in Indian Classical Music", "comment": null, "summary": "Indian classical music relies on a sophisticated microtonal system of 22\nshrutis (pitch intervals), which provides expressive nuance beyond the 12-tone\nequal temperament system. Existing symbolic music processing tools fail to\naccount for these microtonal distinctions and culturally specific raga grammars\nthat govern melodic movement. We present ShrutiSense, a comprehensive symbolic\npitch processing system designed for Indian classical music, addressing two\ncritical tasks: (1) correcting westernized or corrupted pitch sequences, and\n(2) completing melodic sequences with missing values. Our approach employs\ncomplementary models for different tasks: a Shruti-aware finite-state\ntransducer (FST) that performs contextual corrections within the 22-shruti\nframework and a grammar-constrained Shruti hidden Markov model (GC-SHMM) that\nincorporates raga-specific transition rules for contextual completions.\nComprehensive evaluation on simulated data across five ragas demonstrates that\nShrutiSense (FST model) achieves 91.3% shruti classification accuracy for\ncorrection tasks, with example sequences showing 86.7-90.0% accuracy at\ncorruption levels of 0.2 to 0.4. The system exhibits robust performance under\npitch noise up to +/-50 cents, maintaining consistent accuracy across ragas\n(90.7-91.8%), thus preserving the cultural authenticity of Indian classical\nmusic expression."}
{"id": "2508.01719", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01719", "abs": "https://arxiv.org/abs/2508.01719", "authors": ["Haoyue Tan", "Yu Li", "Zhenxi Zhang", "Xiaoran Shi", "Feng Zhou"], "title": "ModFus-DM: Explore the Representation in Modulated Signal Diffusion Generated Models", "comment": null, "summary": "Automatic modulation classification (AMC) is essential for wireless\ncommunication systems in both military and civilian applications. However,\nexisting deep learning-based AMC methods often require large labeled signals\nand struggle with non-fixed signal lengths, distribution shifts, and limited\nlabeled signals. To address these challenges, we propose a modulation-driven\nfeature fusion via diffusion model (ModFus-DM), a novel unsupervised AMC\nframework that leverages the generative capacity of diffusion models for robust\nmodulation representation learning. We design a modulated signal diffusion\ngeneration model (MSDGM) to implicitly capture structural and semantic\ninformation through a progressive denoising process. Additionally, we propose\nthe diffusion-aware feature fusion (DAFFus) module, which adaptively aggregates\nmulti-scale diffusion features to enhance discriminative representation.\nExtensive experiments on RML2016.10A, RML2016.10B, RML2018.01A and RML2022\ndatasets demonstrate that ModFus-DM significantly outperforms existing methods\nin various challenging scenarios, such as limited-label settings, distribution\nshifts, variable-length signal recognition and channel fading scenarios.\nNotably, ModFus-DM achieves over 88.27% accuracy in 24-type recognition tasks\nat SNR $\\geq $ 12dB with only 10 labeled signals per type."}
{"id": "2508.01796", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01796", "abs": "https://arxiv.org/abs/2508.01796", "authors": ["Runxuan Yang", "Kai Li", "Guo Chen", "Xiaolin Hu"], "title": "Enhancing Spectrogram Realism in Singing Voice Synthesis via Explicit Bandwidth Extension Prior to Vocoder", "comment": "7 pages, 8 figures", "summary": "This paper addresses the challenge of enhancing the realism of\nvocoder-generated singing voice audio by mitigating the distinguishable\ndisparities between synthetic and real-life recordings, particularly in\nhigh-frequency spectrogram components. Our proposed approach combines two\ninnovations: an explicit linear spectrogram estimation step using denoising\ndiffusion process with DiT-based neural network architecture optimized for\ntime-frequency data, and a redesigned vocoder based on Vocos specialized in\nhandling large linear spectrograms with increased frequency bins. This\nintegrated method can produce audio with high-fidelity spectrograms that are\nchallenging for both human listeners and machine classifiers to differentiate\nfrom authentic recordings. Objective and subjective evaluations demonstrate\nthat our streamlined approach maintains high audio quality while achieving this\nrealism. This work presents a substantial advancement in overcoming the\nlimitations of current vocoding techniques, particularly in the context of\nadversarial attacks on fake spectrogram detection."}
{"id": "2508.01571", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01571", "abs": "https://arxiv.org/abs/2508.01571", "authors": ["Ziyu Wang", "Yuxuan Wu", "Roger B. Dannenberg", "Gus Xia"], "title": "Automatic Melody Reduction via Shortest Path Finding", "comment": "Accepted paper at ISMIR 2025.\n  https://ismir2025.ismir.net/accepted-papers", "summary": "Melody reduction, as an abstract representation of musical compositions,\nserves not only as a tool for music analysis but also as an intermediate\nrepresentation for structured music generation. Prior computational theories,\nsuch as the Generative Theory of Tonal Music, provide insightful\ninterpretations of music, but they are not fully automatic and usually limited\nto the classical genre. In this paper, we propose a novel and conceptually\nsimple computational method for melody reduction using a graph-based\nrepresentation inspired by principles from computational music theories, where\nthe reduction process is formulated as finding the shortest path. We evaluate\nour algorithm on pop, folk, and classical genres, and experimental results show\nthat the algorithm produces melody reductions that are more faithful to the\noriginal melody and more musically coherent than other common melody\ndownsampling methods. As a downstream task, we use melody reductions to\ngenerate symbolic music variations. Experiments show that our method achieves\nhigher quality than state-of-the-art style transfer methods."}
{"id": "2508.01771", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01771", "abs": "https://arxiv.org/abs/2508.01771", "authors": ["Nagla Abuzgaia", "Abdelhamid Salem", "Ahmed Elbarsha"], "title": "FAS Enabled UAV for Energy-Efficient WPCNs", "comment": null, "summary": "This letter presents an innovative scheme to enhance the communication rate\nand energy efficiency (EE) of Unmanned Aerial Vehicle (UAV) in wireless powered\ncommunication networks (WPCNs) by deploying the emerging fluid antenna system\n(FAS) technology onto the UAV. Our proposed approach leverages the dynamic port\nswitching capability of FAS, enabling the UAV to adaptively select the optimal\nantenna location that maximizes channel gain for both downlink wireless power\ntransfer (WPT) and uplink wireless data transfer (WDT). We derive both exact\nanalytical expression of the ergodic spectral rate, and asymptotic expression\nat high signal to noise ratio (SNR) regime under Nakagami-m correlated fading\nchannels. The Mont-Carlo simulation results confirms the accuracy of the\nanalytical expressions and demonstrate the substantial increase in energy\nefficiency of UAV with FAS compared to fixed antenna systems."}
{"id": "2508.02000", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.02000", "abs": "https://arxiv.org/abs/2508.02000", "authors": ["Xuanjun Chen", "Shih-Peng Cheng", "Jiawei Du", "Lin Zhang", "Xiaoxiao Miao", "Chung-Che Wang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling", "comment": "Work in progress", "summary": "Audio-visual temporal deepfake localization under the content-driven partial\nmanipulation remains a highly challenging task. In this scenario, the deepfake\nregions are usually only spanning a few frames, with the majority of the rest\nremaining identical to the original. To tackle this, we propose a Hierarchical\nBoundary Modeling Network (HBMNet), which includes three modules: an\nAudio-Visual Feature Encoder that extracts discriminative frame-level\nrepresentations, a Coarse Proposal Generator that predicts candidate boundary\nregions, and a Fine-grained Probabilities Generator that refines these\nproposals using bidirectional boundary-content probabilities. From the modality\nperspective, we enhance audio-visual learning through dedicated encoding and\nfusion, reinforced by frame-level supervision to boost discriminability. From\nthe temporal perspective, HBMNet integrates multi-scale cues and bidirectional\nboundary-content relationships. Experiments show that encoding and fusion\nprimarily improve precision, while frame-level supervision boosts recall. Each\nmodule (audio-visual fusion, temporal scales, bi-directionality) contributes\ncomplementary benefits, collectively enhancing localization performance. HBMNet\noutperforms BA-TFD and UMMAFormer and shows improved potential scalability with\nmore training data."}
{"id": "2508.01659", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01659", "abs": "https://arxiv.org/abs/2508.01659", "authors": ["Yuhang Jia", "Xu Zhang", "Yong Qin"], "title": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs", "comment": null, "summary": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of multimodal\nlarge language models (MLLMs). To further strengthen this alignment, recent\nworks have proposed Audio Difference Captioning (ADC), which takes multiple\naudio inputs and encourages the model to describe their differences, thereby\npromoting fine-grained audio discrimination. However, despite its effectiveness\nin enabling difference-telling and detailed discrimination, ADC introduces a\nnotable semantic gap between the input audios-often rich in diverse sound\nevents-and the relatively brief, difference-focused output captions. This\ndeviation from AC-style descriptions leads to a mismatch with the pretraining\nobjective, resulting in catastrophic forgetting during finetuning. To mitigate\nthis issue, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that encourages the model to capture the\nshared semantics across audio clips rather than emphasizing their detailed\ndifferences. Experimental results demonstrate that ACC not only effectively\nenhances audio-text understanding on primary captioning benchmarks but also\nbetter preserves general capabilities across diverse speech and music-related\ndownstream tasks, such as vocal sound classification (VSC), speech emotion\nrecognition (SER), musical instrument classification (MIC), and music genre\nclassification (MGC), compared to ADC. These findings validate that ACC\ncontributes to more robust cross-modal understanding and achieves a better\nbalance between generalization and task-specific performance in the context of\nMLLMs."}
{"id": "2508.01776", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.01776", "abs": "https://arxiv.org/abs/2508.01776", "authors": ["Cheima Hammami", "Luc Le Magoarou", "Philipp del Hougne"], "title": "Statistical Multiport-Network Modeling and Efficient Discrete Optimization of RIS", "comment": "5 pages including 3 figures", "summary": "This Letter fills the research gap on physics-consistent optimization for\nreconfigurable intelligent surfaces (RISs) with mutual coupling (MC) and\n1-bit-tunable elements, a common hardware constraint in existing RIS\nprototypes. We compare a model-based method (temperature-annealed\nback-propagation) and model-agnostic methods (coordinate descent, genetic\nalgorithm), and evaluate potential benefits of intelligently initializing these\nmethods. To facilitate our evaluation, we introduce a technique for generating\nstatistical ensembles of multiport-network model parameters, wherein a single\nhyper-parameter adjusts the MC strength. The technique is a generalization of\nRayleigh fading to radio environments with deterministic programmability, and\nit accounts for passivity constraints as well as the coherent-backscattering\neffect. We find that, except when MC is negligible, coordinate descent with\nrandom initialization yields the most favorable trade-off in terms of\nperformance, execution time and memory usage. We expect our findings to extend\nto beyond-diagonal RIS, stacked intelligent metasurfaces, dynamic metasurface\nantennas, and wave-domain physical neural networks."}
{"id": "2508.02071", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02071", "abs": "https://arxiv.org/abs/2508.02071", "authors": ["Yulun Wu", "Zhongweiyang Xu", "Jianchong Chen", "Zhong-Qiu Wang", "Romit Roy Choudhury"], "title": "Unsupervised Multi-channel Speech Dereverberation via Diffusion", "comment": null, "summary": "We consider the problem of multi-channel single-speaker blind\ndereverberation, where multi-channel mixtures are used to recover the clean\nanechoic speech. To solve this problem, we propose USD-DPS, {U}nsupervised\n{S}peech {D}ereverberation via {D}iffusion {P}osterior {S}ampling. USD-DPS uses\nan unconditional clean speech diffusion model as a strong prior to solve the\nproblem by posterior sampling. At each diffusion sampling step, we estimate all\nmicrophone channels' room impulse responses (RIRs), which are further used to\nenforce a multi-channel mixture consistency constraint for diffusion guidance.\nFor multi-channel RIR estimation, we estimate reference-channel RIR by\noptimizing RIR parameters of a sub-band RIR signal model, with the Adam\noptimizer. We estimate non-reference channels' RIRs analytically using forward\nconvolutive prediction (FCP). We found that this combination provides a good\nbalance between sampling efficiency and RIR prior modeling, which shows\nsuperior performance among unsupervised dereverberation approaches. An audio\ndemo page is provided in https://usddps.github.io/USDDPS_demo/."}
{"id": "2508.01691", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01691", "abs": "https://arxiv.org/abs/2508.01691", "authors": ["Tiantian Feng", "Kevin Huang", "Anfeng Xu", "Xuan Shi", "Thanathai Lertpetchpun", "Jihwan Lee", "Yoonjeong Lee", "Dani Byrd", "Shrikanth Narayanan"], "title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe", "comment": null, "summary": "We present Voxlect, a novel benchmark for modeling dialects and regional\nlanguages worldwide using speech foundation models. Specifically, we report\ncomprehensive benchmark evaluations on dialects and regional language varieties\nin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,\nSpanish, French, German, Brazilian Portuguese, and Italian. Our study used over\n2 million training utterances from 30 publicly available speech corpora that\nare provided with dialectal information. We evaluate the performance of several\nwidely used speech foundation models in classifying speech dialects. We assess\nthe robustness of the dialectal models under noisy conditions and present an\nerror analysis that highlights modeling results aligned with geographic\ncontinuity. In addition to benchmarking dialect classification, we demonstrate\nseveral downstream applications enabled by Voxlect. Specifically, we show that\nVoxlect can be applied to augment existing speech recognition datasets with\ndialect information, enabling a more detailed analysis of ASR performance\nacross dialectal variations. Voxlect is also used as a tool to evaluate the\nperformance of speech generation systems. Voxlect is publicly available with\nthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect."}
{"id": "2508.01824", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.01824", "abs": "https://arxiv.org/abs/2508.01824", "authors": ["Lin Cheng", "Bernardo A. Huberman"], "title": "A Heuristic Method for Simplified Resource Allocation based on Comparative Advantage in Wireless Access Systems", "comment": null, "summary": "This paper presents a heuristic method for simplifying resource allocation in\naccess systems, leveraging the concept of comparative advantage to reduce\ncomputational complexity while maintaining near-optimal performance. Using\npower-division non-orthogonal multiple access (PD-NOMA) as an example, we\ndemonstrate how this approach mitigates the challenge of power allocation in\nmulti-cell networks. Our method reduces the search space for optimization,\nsignificantly decreasing computational overhead while ensuring efficient\nspectrum utilization. In principle, the method reduces the dimensions of search\nspace by half. Extensive analysis and simulations validate its effectiveness,\nhighlighting its potential for practical deployment in next-generation wireless\nnetworks. The proposed framework can help streamline resource allocation in\ncomplex communication environments, enhancing system performance and\nscalability."}
{"id": "2508.02210", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02210", "abs": "https://arxiv.org/abs/2508.02210", "authors": ["George Close", "Kris Hong", "Thomas Hain", "Stefan Goetze"], "title": "WhiSQA: Non-Intrusive Speech Quality Prediction Using Whisper Encoder Features", "comment": "Accepted at SPECOM 2025", "summary": "There has been significant research effort developing neural-network-based\npredictors of SQ in recent years. While a primary objective has been to develop\nnon-intrusive, i.e.~reference-free, metrics to assess the performance of SE\nsystems, recent work has also investigated the direct inference of neural SQ\npredictors within the loss function of downstream speech tasks. To aid in the\ntraining of SQ predictors, several large datasets of audio with corresponding\nhuman labels of quality have been created. Recent work in this area has shown\nthat speech representations derived from large unsupervised or semi-supervised\nfoundational speech models are useful input feature representations for neural\nSQ prediction. In this work, a novel and robust SQ predictor is proposed based\non feature representations extracted from an ASR model, found to be a powerful\ninput feature for the SQ prediction task. The proposed system achieves higher\ncorrelation with human MOS ratings than recent approaches on all NISQA test\nsets and shows significantly better domain adaption compared to the commonly\nused DNSMOS metric."}
{"id": "2508.01796", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01796", "abs": "https://arxiv.org/abs/2508.01796", "authors": ["Runxuan Yang", "Kai Li", "Guo Chen", "Xiaolin Hu"], "title": "Enhancing Spectrogram Realism in Singing Voice Synthesis via Explicit Bandwidth Extension Prior to Vocoder", "comment": "7 pages, 8 figures", "summary": "This paper addresses the challenge of enhancing the realism of\nvocoder-generated singing voice audio by mitigating the distinguishable\ndisparities between synthetic and real-life recordings, particularly in\nhigh-frequency spectrogram components. Our proposed approach combines two\ninnovations: an explicit linear spectrogram estimation step using denoising\ndiffusion process with DiT-based neural network architecture optimized for\ntime-frequency data, and a redesigned vocoder based on Vocos specialized in\nhandling large linear spectrograms with increased frequency bins. This\nintegrated method can produce audio with high-fidelity spectrograms that are\nchallenging for both human listeners and machine classifiers to differentiate\nfrom authentic recordings. Objective and subjective evaluations demonstrate\nthat our streamlined approach maintains high audio quality while achieving this\nrealism. This work presents a substantial advancement in overcoming the\nlimitations of current vocoding techniques, particularly in the context of\nadversarial attacks on fake spectrogram detection."}
{"id": "2508.01828", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.01828", "abs": "https://arxiv.org/abs/2508.01828", "authors": ["Ahmad Dkhan", "Simon Tarboush", "Hadi Sarieddeen", "Tareq Y. Al-Naffouri"], "title": "RIS-Aided Near-Field Channel Estimation under Mutual Coupling and Spatial Correlation", "comment": null, "summary": "The integration of reconfigurable intelligent surfaces (RIS) with extremely\nlarge multiple-input multiple-output (MIMO) arrays at the base station has\nemerged as a key enabler for enhancing wireless network performance. However,\nthis setup introduces high-dimensional channel matrices, leading to increased\ncomputational complexity and pilot overhead in channel estimation. Mutual\ncoupling (MC) effects among densely packed unit cells, spatial correlation, and\nnear-field propagation conditions further complicate the estimation process.\nConventional estimators, such as linear minimum mean square error (MMSE),\nrequire channel statistics that are challenging to acquire for high-dimensional\narrays, while least squares (LS) estimators suffer from performance\nlimitations. To address these challenges, the reduced-subspace least squares\n(RS-LS) estimator leverages array geometry to enhance estimation accuracy. This\nwork advances the promising RS-LS estimation algorithm by explicitly\nincorporating MC effects into the more realistic and challenging near-field\npropagation environment within the increasingly relevant generalized RIS-aided\nMIMO framework. Additionally, we investigate the impact of MC on the spatial\ndegrees of freedom (DoF). Our analysis reveals that accounting for MC effects\nprovides a significant performance gain of approximately 5 dB at an SNR of 5\ndB, compared to conventional methods that ignore MC."}
{"id": "2508.02255", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02255", "abs": "https://arxiv.org/abs/2508.02255", "authors": ["Suhita Ghosh", "Melanie Jouaiti", "Jan-Ole Perschewski", "Sebastian Stober"], "title": "StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation", "comment": "Accepted in Interspeech 2025", "summary": "Detecting and segmenting dysfluencies is crucial for effective speech therapy\nand real-time feedback. However, most methods only classify dysfluencies at the\nutterance level. We introduce StutterCut, a semi-supervised framework that\nformulates dysfluency segmentation as a graph partitioning problem, where\nspeech embeddings from overlapping windows are represented as graph nodes. We\nrefine the connections between nodes using a pseudo-oracle classifier trained\non weak (utterance-level) labels, with its influence controlled by an\nuncertainty measure from Monte Carlo dropout. Additionally, we extend the\nweakly labelled FluencyBank dataset by incorporating frame-level dysfluency\nboundaries for four dysfluency types. This provides a more realistic benchmark\ncompared to synthetic datasets. Experiments on real and synthetic datasets show\nthat StutterCut outperforms existing methods, achieving higher F1 scores and\nmore precise stuttering onset detection."}
{"id": "2508.01897", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.01897", "abs": "https://arxiv.org/abs/2508.01897", "authors": ["Mingru Yang", "Yanmei Gu", "Qianhua He", "Yanxiong Li", "Peirong Zhang", "Yongqiang Chen", "Zhiming Wang", "Huijia Zhu", "Jian Liu", "Weiqiang Wang"], "title": "Generalizable Audio Deepfake Detection via Hierarchical Structure Learning and Feature Whitening in Poincaré sphere", "comment": "Accepted for publication on Interspeech 2025", "summary": "Audio deepfake detection (ADD) faces critical generalization challenges due\nto diverse real-world spoofing attacks and domain variations. However, existing\nmethods primarily rely on Euclidean distances, failing to adequately capture\nthe intrinsic hierarchical structures associated with attack categories and\ndomain factors. To address these issues, we design a novel framework\nPoin-HierNet to construct domain-invariant hierarchical representations in the\nPoincar\\'e sphere. Poin-HierNet includes three key components: 1) Poincar\\'e\nPrototype Learning (PPL) with several data prototypes aligning sample features\nand capturing multilevel hierarchies beyond human labels; 2) Hierarchical\nStructure Learning (HSL) leverages top prototypes to establish a tree-like\nhierarchical structure from data prototypes; and 3) Poincar\\'e Feature\nWhitening (PFW) enhances domain invariance by applying feature whitening to\nsuppress domain-sensitive features. We evaluate our approach on four datasets:\nASVspoof 2019 LA, ASVspoof 2021 LA, ASVspoof 2021 DF, and In-The-Wild.\nExperimental results demonstrate that Poin-HierNet exceeds state-of-the-art\nmethods in Equal Error Rate."}
{"id": "2508.02048", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02048", "abs": "https://arxiv.org/abs/2508.02048", "authors": ["Yoon Huh", "Bumjun Kim", "Wan Choi"], "title": "Feature Reconstruction Aided Federated Learning for Image Semantic Communication", "comment": "IEEE Globecom 2025", "summary": "Research in semantic communication has garnered considerable attention,\nparticularly in the area of image transmission, where joint source-channel\ncoding (JSCC)-based neural network (NN) modules are frequently employed.\nHowever, these systems often experience performance degradation over time due\nto an outdated knowledge base, highlighting the need for periodic updates. To\naddress this challenge in the context of training JSCC modules for image\ntransmission, we propose a federated learning (FL) algorithm with semantic\nfeature reconstruction (FR), named FedSFR. This algorithm more efficiently\nutilizes the available communication capacity by allowing some of the selected\nFL participants to transmit smaller feature vectors instead of local update\ninformation. Unlike conventional FL methods, our approach integrates FR at the\nparameter server (PS), stabilizing training and enhancing image transmission\nquality. Experimental results demonstrate that the proposed scheme\nsignificantly enhances both the stability and effectiveness of the FL process\ncompared to other algorithms. Furthermore, we mathematically derive the\nconvergence rate to validate the improved performance."}
{"id": "2508.01960", "categories": ["cs.SD", "cs.CL", "eess.AS", "68T10 (Primary) 68T45 (Secondary)", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.01960", "abs": "https://arxiv.org/abs/2508.01960", "authors": ["Anton Batliner", "Shahin Amiriparian", "Björn W. Schuller"], "title": "Non-Verbal Vocalisations and their Challenges: Emotion, Privacy, Sparseness, and Real Life", "comment": null, "summary": "Non-Verbal Vocalisations (NVVs) are short `non-word' utterances without\nproper linguistic (semantic) meaning but conveying connotations -- be this\nemotions/affects or other paralinguistic information. We start this\ncontribution with a historic sketch: how they were addressed in psychology and\nlinguistics in the last two centuries, how they were neglected later on, and\nhow they came to the fore with the advent of emotion research. We then give an\noverview of types of NVVs (formal aspects) and functions of NVVs, exemplified\nwith the typical NVV \\textit{ah}. Interesting as they are, NVVs come, however,\nwith a bunch of challenges that should be accounted for: Privacy and general\nethical considerations prevent them of being recorded in real-life (private)\nscenarios to a sufficient extent. Isolated, prompted (acted) exemplars do not\nnecessarily model NVVs in context; yet, this is the preferred strategy so far\nwhen modelling NVVs, especially in AI. To overcome these problems, we argue in\nfavour of corpus-based approaches. This guarantees a more realistic modelling;\nhowever, we are still faced with privacy and sparse data problems."}
{"id": "2508.02117", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02117", "abs": "https://arxiv.org/abs/2508.02117", "authors": ["Lin Chen", "Chang Cai", "Huiyuan Yang", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Scoring ISAC: Benchmarking Integrated Sensing and Communications via Score-Based Generative Modeling", "comment": null, "summary": "Integrated sensing and communications (ISAC) is a key enabler for\nnext-generation wireless systems, aiming to support both high-throughput\ncommunication and high-accuracy environmental sensing using shared spectrum and\nhardware. Theoretical performance metrics, such as mutual information (MI),\nminimum mean squared error (MMSE), and Bayesian Cram\\'{e}r--Rao bound (BCRB),\nplay a key role in evaluating ISAC system performance limits. However, in\npractice, hardware impairments, multipath propagation, interference, and scene\nconstraints often result in nonlinear, multimodal, and non-Gaussian\ndistributions, making it challenging to derive these metrics analytically.\nRecently, there has been a growing interest in applying score-based generative\nmodels to characterize these metrics from data, although not discussed for\nISAC. This paper provides a tutorial-style summary of recent advances in\nscore-based performance evaluation, with a focus on ISAC systems. We refer to\nthe summarized framework as scoring ISAC, which not only reflects the core\nmethodology based on score functions but also emphasizes the goal of scoring\n(i.e., evaluating) ISAC systems under realistic conditions. We present the\nconnections between classical performance metrics and the score functions and\nprovide the practical training techniques for learning score functions to\nestimate performance metrics. Proof-of-concept experiments on target detection\nand localization validate the accuracy of score-based performance estimators\nagainst ground-truth analytical expressions, illustrating their ability to\nreplicate and extend traditional analyses in more complex, realistic settings.\nThis framework demonstrates the great potential of score-based generative\nmodels in ISAC performance analysis, algorithm design, and system optimization."}
{"id": "2508.02000", "categories": ["cs.SD", "cs.CV", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.02000", "abs": "https://arxiv.org/abs/2508.02000", "authors": ["Xuanjun Chen", "Shih-Peng Cheng", "Jiawei Du", "Lin Zhang", "Xiaoxiao Miao", "Chung-Che Wang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling", "comment": "Work in progress", "summary": "Audio-visual temporal deepfake localization under the content-driven partial\nmanipulation remains a highly challenging task. In this scenario, the deepfake\nregions are usually only spanning a few frames, with the majority of the rest\nremaining identical to the original. To tackle this, we propose a Hierarchical\nBoundary Modeling Network (HBMNet), which includes three modules: an\nAudio-Visual Feature Encoder that extracts discriminative frame-level\nrepresentations, a Coarse Proposal Generator that predicts candidate boundary\nregions, and a Fine-grained Probabilities Generator that refines these\nproposals using bidirectional boundary-content probabilities. From the modality\nperspective, we enhance audio-visual learning through dedicated encoding and\nfusion, reinforced by frame-level supervision to boost discriminability. From\nthe temporal perspective, HBMNet integrates multi-scale cues and bidirectional\nboundary-content relationships. Experiments show that encoding and fusion\nprimarily improve precision, while frame-level supervision boosts recall. Each\nmodule (audio-visual fusion, temporal scales, bi-directionality) contributes\ncomplementary benefits, collectively enhancing localization performance. HBMNet\noutperforms BA-TFD and UMMAFormer and shows improved potential scalability with\nmore training data."}
{"id": "2508.02122", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02122", "abs": "https://arxiv.org/abs/2508.02122", "authors": ["Yuanyuan Zhang", "Rui Yang", "Yutao Yue", "Eng Gee Lim", "Zidong Wang"], "title": "An Overview of Algorithms for Contactless Cardiac Feature Extraction from Radar Signals: Advances and Challenges", "comment": null, "summary": "Contactless cardiac monitoring has vast potential to replace contact-based\nmonitoring in various future scenarios such as smart home and in-cabin\nmonitoring. Various contactless sensors can be potentially implemented for\ncardiac monitoring, such as cameras, acoustic sensors, Wi-Fi routers and\nradars. Among all these sensors, radar could achieve unobtrusive monitoring\nwith high accuracy and robustness at the same time. The research about\nradar-based cardiac monitoring can be generally divided into the radar\narchitecture design and signal-processing parts, where the former has been\nthoroughly reviewed in the literature but not the latter. To the best of the\nauthor knowledge, this is the first review paper that focuses on elaborating\nthe algorithms for extracting cardiac features from the received radar signal.\nIn addition, a new taxonomy is proposed to reveal the core feature of each\nalgorithm, with the pros and cons evaluated in detail. Furthermore, the public\ndatasets containing the received radar signal and ground-truth cardiac feature\nsignal are listed with detailed configurations, and the corresponding\nevaluations may help the researchers select the suitable dataset. At last,\nseveral unsolved challenges and future directions are suggested and discussed\nin detail to encourage future research on solving the main obstacles in this\nfield. In summary, this review can be served as a guide for researchers and\npractitioners to quickly understand the research trend and recent development\nof the cardiac feature extraction algorithms, and it is worth further\ninvestigating the relative area based on the proposed challenges and future\ndirections."}
{"id": "2508.02071", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02071", "abs": "https://arxiv.org/abs/2508.02071", "authors": ["Yulun Wu", "Zhongweiyang Xu", "Jianchong Chen", "Zhong-Qiu Wang", "Romit Roy Choudhury"], "title": "Unsupervised Multi-channel Speech Dereverberation via Diffusion", "comment": null, "summary": "We consider the problem of multi-channel single-speaker blind\ndereverberation, where multi-channel mixtures are used to recover the clean\nanechoic speech. To solve this problem, we propose USD-DPS, {U}nsupervised\n{S}peech {D}ereverberation via {D}iffusion {P}osterior {S}ampling. USD-DPS uses\nan unconditional clean speech diffusion model as a strong prior to solve the\nproblem by posterior sampling. At each diffusion sampling step, we estimate all\nmicrophone channels' room impulse responses (RIRs), which are further used to\nenforce a multi-channel mixture consistency constraint for diffusion guidance.\nFor multi-channel RIR estimation, we estimate reference-channel RIR by\noptimizing RIR parameters of a sub-band RIR signal model, with the Adam\noptimizer. We estimate non-reference channels' RIRs analytically using forward\nconvolutive prediction (FCP). We found that this combination provides a good\nbalance between sampling efficiency and RIR prior modeling, which shows\nsuperior performance among unsupervised dereverberation approaches. An audio\ndemo page is provided in https://usddps.github.io/USDDPS_demo/."}
{"id": "2508.02135", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02135", "abs": "https://arxiv.org/abs/2508.02135", "authors": ["Ahmet Kaplan", "Diana P. M. Osorio", "Erik G. Larsson"], "title": "Analysis of Broad Beam Beamforming for Collocated and Distributed MIMO", "comment": null, "summary": "Broad beam beamforming (BF) design in multiple-input multiple-output (MIMO)\ncan be convenient for initial access, synchronization, and sensing capabilities\nin cellular networks by avoiding overheads of sweeping methods while making\nefficient use of resources. Phase-only BF is key for maximizing power\nefficiency across antennas. A successful method to produce broad beams is the\nphase-only dual-polarization BF (DPBF). However, its efficiency has not been\nproved in non-line-of-sight (NLoS). Therefore, this paper contributes by\nevaluating DPBF in collocated and distributed MIMO configurations under both\nline-of-sight (LoS) and NLoS channel conditions. We model the reflection\ncoefficients for different materials in NLoS conditions and propose the use of\northogonal space-time block code to improve the coverage compared to the DPBF\nin collocated MIMO (C-MIMO). We further propose a DPBF method for distributed\nMIMO and show that it achieves better coverage than C-MIMO with DPBF."}
{"id": "2508.02175", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02175", "abs": "https://arxiv.org/abs/2508.02175", "authors": ["Liang Lin", "Miao Yu", "Kaiwen Luo", "Yibo Zhang", "Lilan Peng", "Dexian Wang", "Xuehai Tang", "Yuanhe Zhang", "Xikang Yang", "Zhenhong Zhou", "Kun Wang", "Yang Liu"], "title": "Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers", "comment": null, "summary": "As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth."}
{"id": "2508.02223", "categories": ["eess.SP", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2508.02223", "abs": "https://arxiv.org/abs/2508.02223", "authors": ["Mingyan Gong"], "title": "The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Maximum likelihood factor analysis has been used for direction of arrival\nestimation in unknown nonuniform noise and some iterative approaches have been\ndeveloped. In particular, the Factor Analysis for Anisotropic Noise (FAAN)\nmethod proposed by Stoica and Babu has excellent convergence properties. In\nthis letter, the Expectation/Conditional Maximization Either (ECME) algorithm,\nan extension of the expectation-maximization algorithm, is designed, which has\nalmost the same computational complexity at each iteration as the FAAN method.\nHowever, numerical results show that the ECME algorithm yields faster stable\nconvergence and is computationally more efficient."}
{"id": "2508.02210", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02210", "abs": "https://arxiv.org/abs/2508.02210", "authors": ["George Close", "Kris Hong", "Thomas Hain", "Stefan Goetze"], "title": "WhiSQA: Non-Intrusive Speech Quality Prediction Using Whisper Encoder Features", "comment": "Accepted at SPECOM 2025", "summary": "There has been significant research effort developing neural-network-based\npredictors of SQ in recent years. While a primary objective has been to develop\nnon-intrusive, i.e.~reference-free, metrics to assess the performance of SE\nsystems, recent work has also investigated the direct inference of neural SQ\npredictors within the loss function of downstream speech tasks. To aid in the\ntraining of SQ predictors, several large datasets of audio with corresponding\nhuman labels of quality have been created. Recent work in this area has shown\nthat speech representations derived from large unsupervised or semi-supervised\nfoundational speech models are useful input feature representations for neural\nSQ prediction. In this work, a novel and robust SQ predictor is proposed based\non feature representations extracted from an ASR model, found to be a powerful\ninput feature for the SQ prediction task. The proposed system achieves higher\ncorrelation with human MOS ratings than recent approaches on all NISQA test\nsets and shows significantly better domain adaption compared to the commonly\nused DNSMOS metric."}
{"id": "2508.02334", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02334", "abs": "https://arxiv.org/abs/2508.02334", "authors": ["Ahmet Sacid Sümer", "Ebubekir Memişoğlu", "Hüseyin Arslan"], "title": "Adaptive Phase-Shifted Pilot Design for Uplink Multiple Access in ISAC Systems", "comment": null, "summary": "In uplink integrated sensing and communication (ISAC) systems, pilot signal\ndesign is crucial for enabling accurate channel estimation and reliable radar\nsensing. In orthogonal frequency-division multiple access (OFDMA)-based\nframeworks, conventional pilot allocation schemes face a trade-off between\nspectral efficiency (SE) and sensing performance. Interleaved pilots improve\nuser equipment (UE) multiplexing through sparse allocation but reduce the\nmaximum unambiguous range. Conversely, orthogonal block-based pilots reduce\nrange ambiguity but degrade sensing resolution due to limited delay\ngranularity. To address this trade-off, the phase-shifted ISAC (PS-ISAC) scheme\nwas recently proposed for uplink multiple access in ISAC systems. However,\nPS-ISAC suffers from spectral inefficiency due to the fixed cyclic prefix (CP)\nconstraints. To overcome these limitations, we propose adaptive\nphase-shifted-ISAC (APS-ISAC), an enhanced pilot scheme that employs an\noverlapped block-pilot structure with UE-specific phase shifts determined by\nmaximum excess delay of each UE. This design enables UEs to share the same\ntime-frequency resources while preserving separable and contiguous channel\nimpulse responses (CIRs) at the base station (BS). Simulation results show that\nAPS-ISAC significantly outperforms conventional pilot allocation methods in\nterms of SE, approximately doubling the number of multiplexed UEs. It also\nachieves lower mean square error (MSE) under power constraints with reduced\ncomplexity. Furthermore, it yields maximum range resolution and unambiguous\nsensing performance. These results establish APS-ISAC as a scalable, spectrally\nefficient, ambiguity-resilient, and low-complexity pilot design paradigm for\nfuture uplink ISAC systems."}
{"id": "2508.02255", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02255", "abs": "https://arxiv.org/abs/2508.02255", "authors": ["Suhita Ghosh", "Melanie Jouaiti", "Jan-Ole Perschewski", "Sebastian Stober"], "title": "StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation", "comment": "Accepted in Interspeech 2025", "summary": "Detecting and segmenting dysfluencies is crucial for effective speech therapy\nand real-time feedback. However, most methods only classify dysfluencies at the\nutterance level. We introduce StutterCut, a semi-supervised framework that\nformulates dysfluency segmentation as a graph partitioning problem, where\nspeech embeddings from overlapping windows are represented as graph nodes. We\nrefine the connections between nodes using a pseudo-oracle classifier trained\non weak (utterance-level) labels, with its influence controlled by an\nuncertainty measure from Monte Carlo dropout. Additionally, we extend the\nweakly labelled FluencyBank dataset by incorporating frame-level dysfluency\nboundaries for four dysfluency types. This provides a more realistic benchmark\ncompared to synthetic datasets. Experiments on real and synthetic datasets show\nthat StutterCut outperforms existing methods, achieving higher F1 scores and\nmore precise stuttering onset detection."}
{"id": "2508.02349", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02349", "abs": "https://arxiv.org/abs/2508.02349", "authors": ["Jeanne I. M. Parmentier", "Rhana M. Aarts", "Elin Hernlund", "Marie Rhodin", "Berend Jan van der Zwaag"], "title": "Detecting and measuring respiratory events in horses during exercise with a microphone: deep learning vs. standard signal processing", "comment": null, "summary": "Monitoring respiration parameters such as respiratory rate could be\nbeneficial to understand the impact of training on equine health and\nperformance and ultimately improve equine welfare. In this work, we compare\ndeep learning-based methods to an adapted signal processing method to\nautomatically detect cyclic respiratory events and extract the dynamic\nrespiratory rate from microphone recordings during high intensity exercise in\nStandardbred trotters. Our deep learning models are able to detect exhalation\nsounds (median F1 score of 0.94) in noisy microphone signals and show promising\nresults on unlabelled signals at lower exercising intensity, where the\nexhalation sounds are less recognisable. Temporal convolutional networks were\nbetter at detecting exhalation events and estimating dynamic respiratory rates\n(median F1: 0.94, Mean Absolute Error (MAE) $\\pm$ Confidence Intervals (CI):\n1.44$\\pm$1.04 bpm, Limits Of Agreements (LOA): 0.63$\\pm$7.06 bpm) than long\nshort-term memory networks (median F1: 0.90, MAE$\\pm$CI: 3.11$\\pm$1.58 bpm) and\nsignal processing methods (MAE$\\pm$CI: 2.36$\\pm$1.11 bpm). This work is the\nfirst to automatically detect equine respiratory sounds and automatically\ncompute dynamic respiratory rates in exercising horses. In the future, our\nmodels will be validated on lower exercising intensity sounds and different\nmicrophone placements will be evaluated in order to find the best combination\nfor regular monitoring."}
{"id": "2508.02354", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02354", "abs": "https://arxiv.org/abs/2508.02354", "authors": ["Cuno Sankey-Olsen", "Rasmus Hvass Olesen", "Tobias Oliver Eberhard", "Andreas Triantafyllopoulos", "Björn Schuller", "Ilhan Aslan"], "title": "Detecting COPD Through Speech Analysis: A Dataset of Danish Speech and Machine Learning Approach", "comment": null, "summary": "Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating\ndisease affecting millions around the world. Its early detection using\nnon-invasive means could enable preventive interventions that improve quality\nof life and patient outcomes, with speech recently shown to be a valuable\nbiomarker. Yet, its validity across different linguistic groups remains to be\nseen. To that end, audio data were collected from 96 Danish participants\nconducting three speech tasks (reading, coughing, sustained vowels). Half of\nthe participants were diagnosed with different levels of COPD and the other\nhalf formed a healthy control group. Subsequently, we investigated different\nbaseline models using openSMILE features and learnt x-vector embeddings. We\nobtained a best accuracy of 67% using openSMILE features and logistic\nregression. Our findings support the potential of speech-based analysis as a\nnon-invasive, remote, and scalable screening tool as part of future COPD\nhealthcare solutions."}
{"id": "2508.02359", "categories": ["eess.SP", "cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02359", "abs": "https://arxiv.org/abs/2508.02359", "authors": ["Surej Mouli", "Ramaswamy Palaniappan"], "title": "Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue", "comment": null, "summary": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications."}
{"id": "2508.02391", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02391", "abs": "https://arxiv.org/abs/2508.02391", "authors": ["Yizhu Jin", "Zhen Ye", "Zeyue Tian", "Haohe Liu", "Qiuqiang Kong", "Yike Guo", "Wei Xue"], "title": "Inference-time Scaling for Diffusion-based Audio Super-resolution", "comment": null, "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nincluding audio super-resolution (SR). In many applications like movie\npost-production and album mastering, substantial computational budgets are\navailable for achieving superior audio quality. However, while existing\ndiffusion approaches typically increase sampling steps to improve quality, the\nperformance remains fundamentally limited by the stochastic nature of the\nsampling process, leading to high-variance and quality-limited outputs. Here,\nrather than simply increasing the number of sampling steps, we propose a\ndifferent paradigm through inference-time scaling for SR, which explores\nmultiple solution trajectories during the sampling process. Different\ntask-specific verifiers are developed, and two search algorithms, including the\nrandom search and zero-order search for SR, are introduced. By actively guiding\nthe exploration of the high-dimensional solution space through\nverifier-algorithm combinations, we enable more robust and higher-quality\noutputs. Through extensive validation across diverse audio domains (speech,\nmusic, sound effects) and frequency ranges, we demonstrate consistent\nperformance gains, achieving improvements of up to 9.70% in aesthetics, 5.88%\nin speaker similarity, 15.20% in word error rate, and 46.98% in spectral\ndistance for speech SR from 4kHz to 24kHz, showcasing the effectiveness of our\napproach. Audio samples are available at:\nhttps://racerk.github.io/tt-scale-audiosr/."}
{"id": "2508.02417", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02417", "abs": "https://arxiv.org/abs/2508.02417", "authors": ["Nazmun N Khan", "Taylor Sweet", "Chase A Harvey", "Calder Knapp", "Dean J. Krusienski", "David E Thompson"], "title": "The Role of Review Process Failures in Affective State Estimation: An Empirical Investigation of DEAP Dataset", "comment": "25 pages, 4 figures, This is a preprint version of the manuscript. It\n  is intended for submission to a peer-reviewed journal", "summary": "The reliability of affective state estimation using EEG data is in question,\ngiven the variability in reported performance and the lack of standardized\nevaluation protocols. To investigate this, we reviewed 101 studies, focusing on\nthe widely used DEAP dataset for emotion recognition. Our analysis revealed\nwidespread methodological issues that include data leakage from improper\nsegmentation, biased feature selection, flawed hyperparameter optimization,\nneglect of class imbalance, and insufficient methodological reporting. Notably,\nwe found that nearly 87% of the reviewed papers contained one or more of these\nerrors. Moreover, through experimental analysis, we observed that such\nmethodological flaws can inflate the classification accuracy by up to 46%.\nThese findings reveal fundamental gaps in standardized evaluation practices and\nhighlight critical deficiencies in the peer review process for machine learning\napplications in neuroscience, emphasizing the urgent need for stricter\nmethodological standards and evaluation protocols."}
{"id": "2508.02448", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02448", "abs": "https://arxiv.org/abs/2508.02448", "authors": ["Andreas Triantafyllopoulos", "Anton Batliner", "Björn W. Schuller"], "title": "Charting 15 years of progress in deep learning for speech emotion recognition: A replication study", "comment": "Code: https://github.com/CHI-TUM/ser-progress-replication Submitted\n  for review", "summary": "Speech emotion recognition (SER) has long benefited from the adoption of deep\nlearning methodologies. Deeper models -- with more layers and more trainable\nparameters -- are generally perceived as being `better' by the SER community.\nThis raises the question -- \\emph{how much better} are modern-era deep neural\nnetworks compared to their earlier iterations? Beyond that, the more important\nquestion of how to move forward remains as poignant as ever. SER is far from a\nsolved problem; therefore, identifying the most prominent avenues of future\nresearch is of paramount importance. In the present contribution, we attempt a\nquantification of progress in the 15 years of research beginning with the\nintroduction of the landmark 2009 INTERSPEECH Emotion Challenge. We conduct a\nlarge scale investigation of model architectures, spanning both audio-based\nmodels that rely on speech inputs and text-baed models that rely solely on\ntranscriptions. Our results point towards diminishing returns and a plateau\nafter the recent introduction of transformer architectures. Moreover, we\ndemonstrate how perceptions of progress are conditioned on the particular\nselection of models that are compared. Our findings have important\nrepercussions about the state-of-the-art in SER research and the paths forward"}
{"id": "2508.02447", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02447", "abs": "https://arxiv.org/abs/2508.02447", "authors": ["Shalini Tripathi", "Ankur Bansal", "Holger Claussen", "Lester Ho", "Chinmoy Kundu"], "title": "Secure Energy Efficient Wireless Transmission: A Finite v/s Infinite-Horizon RL Solution", "comment": null, "summary": "In this paper, a joint optimal allocation of transmit power at the source and\njamming power at the destination is proposed to maximize the average secrecy\nenergy efficiency (SEE) of a wireless network within a finite time duration.\nThe destination transmits the jamming signal to improve secrecy by utilizing\nfull-duplex capability. The source and destination both have energy harvesting\n(EH) capability with limited battery capacity. Due to the Markov nature of the\nsystem, the problem is formulated as a finite-horizon reinforcement learning\n(RL) problem. We propose the finite-horizon joint power allocation (FHJPA)\nalgorithm for the finite-horizon RL problem and compare it with a\nlow-complexity greedy algorithm (GA). An infinite-horizon joint power\nallocation (IHJPA) algorithm is also proposed for the corresponding\ninfinite-horizon problem. A comparative analysis of these algorithms is carried\nout in terms of SEE, expected total transmitted secure bits, and computational\ncomplexity. The results show that the FHJPA algorithm outperforms the GA and\nIHJPA algorithms due to its appropriate modelling in finite horizon\ntransmission. When the source node battery has sufficient energy, the GA can\nyield performance close to the FHJPA algorithm despite its low-complexity. When\nthe transmission time horizon increases, the accuracy of the infinite-horizon\nmodel improves, resulting in a reduced performance gap between FHJPA and IHJPA\nalgorithms. The computational time comparison shows that the FHJPA algorithm\ntakes $16.6$ percent less time than the IHJPA algorithm."}
{"id": "2508.02521", "categories": ["cs.SD", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02521", "abs": "https://arxiv.org/abs/2508.02521", "authors": ["Andrea Di Pierno", "Luca Guarnera", "Dario Allegra", "Sebastiano Battiato"], "title": "Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework", "comment": null, "summary": "The proliferation of audio deepfakes poses a growing threat to trust in\ndigital communications. While detection methods have advanced, attributing\naudio deepfakes to their source models remains an underexplored yet crucial\nchallenge. In this paper we introduce LAVA (Layered Architecture for Voice\nAttribution), a hierarchical framework for audio deepfake detection and model\nrecognition that leverages attention-enhanced latent representations extracted\nby a convolutional autoencoder trained solely on fake audio. Two specialized\nclassifiers operate on these features: Audio Deepfake Attribution (ADA), which\nidentifies the generation technology, and Audio Deepfake Model Recognition\n(ADMR), which recognize the specific generative model instance. To improve\nrobustness under open-set conditions, we incorporate confidence-based rejection\nthresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong\nperformance: the ADA classifier achieves F1-scores over 95% across all\ndatasets, and the ADMR module reaches 96.31% macro F1 across six classes.\nAdditional tests on unseen attacks from ASVpoof2019 LA and error propagation\nanalysis confirm LAVA's robustness and reliability. The framework advances the\nfield by introducing a supervised approach to deepfake attribution and model\nrecognition under open-set conditions, validated on public benchmarks and\naccompanied by publicly released models and code. Models and code are available\nat https://www.github.com/adipiz99/lava-framework."}
{"id": "2508.02471", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02471", "abs": "https://arxiv.org/abs/2508.02471", "authors": ["Anton Björkman", "Filip Elvander"], "title": "Inverse harmonic clustering for multi-pitch estimation: an optimal transport approach", "comment": "13 pages, 8 figures", "summary": "In this work, we consider the problem of multi-pitch estimation, i.e.,\nidentifying super-imposed truncated harmonic series from noisy measurements. We\nphrase this as recovering a harmonically-structured measure on the unit circle,\nwhere the structure is enforced using regularizers based on optimal transport\ntheory. In the resulting framework, a signal's spectral content is\nsimultaneously inferred and assigned, or transported, to a small set of\nharmonic series defined by their corresponding fundamental frequencies. In\ncontrast to existing methods from the compressed sensing paradigm, the proposed\nframework decouples regularization and dictionary design and mitigates\ncoherency problems. As a direct consequence, this also introduces robustness to\nthe phenomenon of inharmonicity. From this framework, we derive two estimation\nmethods, one for stochastic and one for deterministic signals, and propose\nefficient numerical algorithms implementing them. In numerical studies on both\nsynthetic and real data, the proposed methods are shown to achieve better\nestimation performance as compared to other methods from statistical signal\nprocessing literature. Furthermore, they perform comparably or better than\nnetwork-based methods, except when the latter are specially trained on the\ndata-type considered and are given access to considerably more data during\ninference."}
{"id": "2508.01847", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.01847", "abs": "https://arxiv.org/abs/2508.01847", "authors": ["Avishkar Behera", "Riya Ann Easow", "Venkatesh Parvathala", "K. Sri Rama Murty"], "title": "Test-Time Training for Speech Enhancement", "comment": "Accepted to Interspeech 2025. 5 pages, 2 figures", "summary": "This paper introduces a novel application of Test-Time Training (TTT) for\nSpeech Enhancement, addressing the challenges posed by unpredictable noise\nconditions and domain shifts. This method combines a main speech enhancement\ntask with a self-supervised auxiliary task in a Y-shaped architecture. The\nmodel dynamically adapts to new domains during inference time by optimizing the\nproposed self-supervised tasks like noise-augmented signal reconstruction or\nmasked spectrogram prediction, bypassing the need for labeled data. We further\nintroduce various TTT strategies offering a trade-off between adaptation and\nefficiency. Evaluations across synthetic and real-world datasets show\nconsistent improvements across speech quality metrics, outperforming the\nbaseline model. This work highlights the effectiveness of TTT in speech\nenhancement, providing insights for future research in adaptive and robust\nspeech processing."}
{"id": "2508.02559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02559", "abs": "https://arxiv.org/abs/2508.02559", "authors": ["Sijia Li", "Rui Sun", "Bing Xu", "Yuanwei Liu"], "title": "Cramér-Rao Bound for Direct Position Estimation in OFDM Based Cellular Systems", "comment": "5 pages, 3 figures, conference", "summary": "Although direct position estimation (DPE) has been demonstrated to offer\nenhanced robustness in GNSS receivers, its theoretical limits and performance\nin OFDM based positioning systems remain largely unexplored. In this paper, the\nCram\\'er-Rao bound (CRB) for DPE using OFDM based cellular signals is derived\nand benchmarked against the conventional two-step positioning method to assess\ntheir relative performance in non-line-of-sight (NLOS) dominated multipath\nenvironments. Numerical results reveal that 1) the DPE method consistently\noutperforms the two-step approach in OFDM systems under all evaluated\nconditions; 2) a large bandwidth is crucial in both methods, and increasing\nsubcarrier spacing is more beneficial for a fixed bandwidth; 3) utilizing\nmultiple OFDM symbols for positioning leads to substantial improvements in\nlocalization accuracy compared to relying on a single symbol. However, further\nincreasing the number of symbols yields marginal improvements while\nsignificantly increasing computational complexity."}
{"id": "2508.02295", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.02295", "abs": "https://arxiv.org/abs/2508.02295", "authors": ["Yangyang Qu", "Michele Panariello", "Massimiliano Todisco", "Nicholas Evans"], "title": "Reference-free Adversarial Sex Obfuscation in Speech", "comment": null, "summary": "Sex conversion in speech involves privacy risks from data collection and\noften leaves residual sex-specific cues in outputs, even when target speaker\nreferences are unavailable. We introduce RASO for Reference-free Adversarial\nSex Obfuscation. Innovations include a sex-conditional adversarial learning\nframework to disentangle linguistic content from sex-related acoustic markers\nand explicit regularisation to align fundamental frequency distributions and\nformant trajectories with sex-neutral characteristics learned from sex-balanced\ntraining data. RASO preserves linguistic content and, even when assessed under\na semi-informed attack model, it significantly outperforms a competing approach\nto sex obfuscation."}

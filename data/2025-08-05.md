<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 24]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 23]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Binary Hypothesis Testing-Based Low-Complexity Beamspace Channel Estimation for mmWave Massive MIMO Systems](https://arxiv.org/abs/2508.01007)
*Hanyoung Park,Ji-Woong Choi*

Main category: eess.SP

TL;DR: 提出了一种基于贝叶斯二元假设检验和波束空间稀疏性的低复杂度信道去噪方法，适用于毫米波通信系统。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信系统由于信号路径损耗大、散射差和衍射有限，需要大规模天线阵列和定向波束成形，但信道估计的计算复杂度高，成为系统负担。

Method: 通过伯努利-复高斯先验建模稀疏波束空间分量，构建似然比检验检测信号相关元素，并应用硬阈值规则抑制噪声主导分量。

Result: 尽管计算复杂度极低，该方法在信道估计精度上与复杂迭代或基于学习的方法相当。

Conclusion: 该方法为资源受限的毫米波系统提供了一种可行的信道估计方案。

Abstract: Millimeter-wave (mmWave) communications have gained attention as a key
technology for high-capacity wireless systems, owing to the wide available
bandwidth. However, mmWave signals suffer from their inherent characteristics
such as severe path loss, poor scattering, and limited diffraction, which
necessitate the use of large antenna arrays and directional beamforming,
typically implemented through massive MIMO architectures. Accurate channel
estimation is critical in such systems, but its computational complexity
increases proportionally with the number of antennas. This may become a
significant burden in mmWave systems where channels exhibit rapid fluctuations
and require frequent updates. In this paper, we propose a low-complexity
channel denoiser based on Bayesian binary hypothesis testing and beamspace
sparsity. By modeling each sparse beamspace component as a mixture of signal
and noise under a Bernoulli-complex Gaussian prior, we formulate a likelihood
ratio test to detect signal-relevant elements. Then, a hard-thresholding rule
is applied to suppress noise-dominant components in the noisy channel vector.
Despite its extremely low computational complexity, the proposed method
achieves channel estimation accuracy that is comparable to that of complex
iterative or learning-based approaches. This effectiveness is supported by both
theoretical analysis and numerical evaluation, suggesting that the method can
be a viable option for mmWave systems with strict resource constraints.

</details>


### [2] [Coordinated Decentralized Resource Optimization for Cell-Free ISAC Systems](https://arxiv.org/abs/2508.01044)
*Mehdi Zafari,Rang Liu,A. Lee Swindlehurst*

Main category: eess.SP

TL;DR: 本文提出了两种去中心化优化算法，用于无小区ISAC网络中的波束成形和功率分配，解决了现有集中式方法的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC解决方案依赖集中式架构，限制了分布式系统的可扩展性，因此需要去中心化优化方法。

Method: 提出两种算法：1) 本地固定波束成形与集中功率分配结合；2) 完全去中心化的共识ADMM框架联合优化波束成形和功率控制。

Result: 仿真结果表明，两种算法在减少前传开销和提升系统性能方面具有实用性。

Conclusion: 去中心化算法适用于可扩展的无小区ISAC部署。

Abstract: Integrated Sensing and Communication (ISAC) is emerging as a key enabler for
6G wireless networks, allowing the joint use of spectrum and infrastructure for
both communication and sensing. While prior ISAC solutions have addressed
resource optimization, including power allocation, beamforming, and waveform
design, they often rely on centralized architectures with full network
knowledge, limiting their scalability in distributed systems. In this paper, we
propose two coordinated decentralized optimization algorithms for beamforming
and power allocation tailored to cell-free ISAC networks. The first algorithm
employs locally designed fixed beamformers at access points (APs), combined
with a centralized power allocation scheme computed at a central server (CS).
The second algorithm jointly optimizes beamforming and power control through a
fully decentralized consensus ADMM framework. Both approaches rely on local
information at APs and limited coordination with the CS. Simulation results
obtained using our proposed Python-based simulation framework evaluate their
fronthaul overhead and system-level performance, demonstrating their
practicality for scalable ISAC deployment in decentralized, cell-free
architectures.

</details>


### [3] [A Highly Available GTFS-RT Positions System](https://arxiv.org/abs/2508.01121)
*Joshua Wong,Kin Tsang*

Main category: eess.SP

TL;DR: 开发了一个基于GTFS-RT标准的实时公共交通数据系统，包括GPS传感器设备设计、固件、数据处理算法，并通过多区域高可用集群部署。


<details>
  <summary>Details</summary>
Motivation: 利用GTFS-RT开放数据格式，为公共交通提供实时数据支持。

Method: 设计GPS传感器设备及固件，开发算法将原始数据转换为GTFS-RT格式，并通过多区域集群部署。

Result: 成功实现了实时公共交通数据的高可用性系统。

Conclusion: 该系统为公共交通提供了可靠的实时数据解决方案。

Abstract: We develop a system for real-time public transportation data, deciding to use
the data standard GTFS-RT (GTFS Realtime), an open data format for public
transit data. We give an overview of the design of a physical GPS sensor
device, its firmware, and processes. Next, we give the algorithms used to
translate raw sensor data into a public GTFS-RT data feed. We deploy this feed
over a highly available cluster across multiple regions to maintain high
availability.

</details>


### [4] [On the Characterization and Evaluation of Doppler Squint in Wideband ODDM Systems](https://arxiv.org/abs/2508.01283)
*Xuehan Wang,Jinhong Yuan,Jintao Wang,Zhi Sun*

Main category: eess.SP

TL;DR: 本文研究了正交延迟-多普勒分复用（ODDM）调制中的宽带效应（多普勒斜视效应，DSE），并分析了其对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 先前的研究忽略了ODDM信号的宽带特性引起的交互色散，可能导致性能下降。本文旨在解决这一问题。

Method: 通过分析多径信道的时变频率响应，研究了DSE引起的额外延迟-多普勒色散，并推导了其在RCP和ZP宽带ODDM系统中的特性。

Result: 理论分析和数值结果表明，DSE会导致额外的延迟-多普勒扩展和更复杂的功率泄漏。

Conclusion: 本文的推导有助于开发基于ODDM的集成感知与通信系统中的精确信号处理技术。

Abstract: The recently proposed orthogonal delay-Doppler division multiplexing (ODDM)
modulation has been demonstrated to enjoy excellent reliability over
doubly-dispersive channels. However, most of the prior analysis tends to ignore
the interactive dispersion caused by the wideband property of ODDM signal,
which possibly leads to performance degradation. To solve this problem, we
investigate the input-output relation of ODDM systems considering the wideband
effect, which is also known as the Doppler squint effect (DSE) in the
literature. The extra delay-Doppler (DD) dispersion caused by the DSE is first
explicitly explained by employing the time-variant frequency response of
multipath channels. Its characterization is then derived for both reduced
cyclic prefix (RCP) and zero padded (ZP)-based wideband ODDM systems, where the
extra DD spread and more complicated power leakage outside the peak region are
presented theoretically. Numerical results are finally provided to confirm the
significance of DSE. The derivations in this paper are beneficial for
developing accurate signal processing techniques in ODDM-based integrated
sensing and communication systems.

</details>


### [5] [DIY hybrid SSVEP-P300 LED stimuli for BCI platform using EMOTIV EEG headset](https://arxiv.org/abs/2508.01510)
*Surej Mouli,Ramaswamy Palaniappan*

Main category: eess.SP

TL;DR: 本文介绍了一种可定制化的COB LED设计，用于同时诱发两种脑电反应（SSVEP和P300），旨在减少疲劳并提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 结合SSVEP和P300的优势，开发一种混合BCI硬件平台，以减少电极数量、缩短训练时间并提高分类准确性。

Method: 使用四组独立控制的绿色视觉刺激诱发SSVEP，四组随机闪烁的红色LED诱发P300，并通过32位微控制器平台实现精确控制。

Result: 系统成功实现了实时分类，并通过控制LEGO机器人验证了其准确性。

Conclusion: 该混合刺激设计在减少疲劳和提高分类性能方面表现出色，为BCI应用提供了新思路。

Abstract: A fully customisable chip-on board (COB) LED design to evoke two brain
responses simultaneously (steady state visual evoked potential (SSVEP) and
transient evoked potential, P300) is discussed in this paper. Considering
different possible modalities in braincomputer interfacing (BCI), SSVEP is
widely accepted as it requires a lesser number of electroencephalogram (EEG)
electrodes and minimal training time. The aim of this work was to produce a
hybrid BCI hardware platform to evoke SSVEP and P300 precisely with reduced
fatigue and improved classification performance. The system comprises of four
independent radial green visual stimuli controlled individually by a 32-bit
microcontroller platform to evoke SSVEP and four red LEDs flashing at random
intervals to generate P300 events. The system can also record the P300 event
timestamps that can be used in classification, to improve the accuracy and
reliability. The hybrid stimulus was tested for realtime classification
accuracy by controlling a LEGO robot to move in four directions.

</details>


### [6] [Balancing Latency and Model Accuracy for Fluid Antenna-Assisted LM-Embedded MIMO Network](https://arxiv.org/abs/2508.01689)
*Yichen Jin,Zongze Li,Zeyi Ren,Qingfeng Lin,Yik-Chung Wu*

Main category: eess.SP

TL;DR: 本文提出了一种基于流体天线（FA）技术的优化方法，用于解决大型模型（LM）嵌入无线网络中模型精度与网络延迟之间的权衡问题。通过量化LM和利用FA技术提升传输容量，实现了低延迟和高精度的目标。


<details>
  <summary>Details</summary>
Motivation: 在LM嵌入的无线网络中，如何在保证高推理精度的同时最小化网络延迟是一个关键挑战。过度的量化会损害模型精度，而FA技术可以提升传输容量，从而降低延迟。

Method: 提出了一种基于块坐标下降框架的高效优化算法，将延迟和峰值信噪比（PSNR）纳入目标函数，设计FA辅助的LM嵌入网络。

Result: 仿真结果表明，所提算法具有良好的收敛性，并且在网络延迟和PSNR方面优于其他基准网络。

Conclusion: FA辅助的LM嵌入网络能够有效平衡延迟和精度，为未来无线网络中的LM部署提供了可行的解决方案。

Abstract: This paper addresses the challenge of large model (LM)-embedded wireless
network for handling the trade-off problem of model accuracy and network
latency. To guarantee a high-quality of users' service, the network latency
should be minimized while maintaining an acceptable inference accuracy. To meet
this requirement, LM quantization is proposed to reduce the latency. However,
the excessive quantization may destroy the accuracy of LM inference. To this
end, a promising fluid antenna (FA) technology is investigated for enhancing
the transmission capacity, leading to a lower network latency in the
LM-embedded multiple-input multiple-output (MIMO) network. To design the
FA-assisted LM-embedded network with the lower latency and higher accuracy
requirements, the latency and peak signal to noise ratio (PSNR) are considered
in the objective function. Then, an efficient optimization algorithm is
proposed under the block coordinate descent framework. Simulation results are
provided to show the convergence behavior of the proposed algorithm, and the
performance gains from the proposed FA-assisted LMembedded network over the
other benchmark networks in terms of network latency and PSNR.

</details>


### [7] [Spectrum Sensing with Deep Clustering: Label-Free Radio Access Technology Recognition](https://arxiv.org/abs/2508.01709)
*Ljupcho Milosheski,Mihael Mohorčič,Carolina Fortuna*

Main category: eess.SP

TL;DR: 论文提出了一种无需标记数据的频谱感知工作流，采用SSL深度聚类架构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着设备数量和网络密度的增加，对射频频谱资源的需求激增，但现有基于监督学习的RAT分类方法依赖大量标记数据，泛化能力存疑。

Method: 提出无需先验知识的频谱感知工作流，采用SSL深度聚类架构从原始FFT数据中自主提取特征。

Result: 在三个真实数据集上验证，性能提升高达35%，参数减少22%，FLOPS降低50%。

Conclusion: 该方法为6G频谱感知提供了高效、泛化能力强的解决方案。

Abstract: The growth of the number of connected devices and network densification is
driving an increasing demand for radio network resources, particularly Radio
Frequency (RF) spectrum. Given the dynamic and complex nature of contemporary
wireless environments, characterized by a wide variety of devices and multiple
RATs, spectrum sensing is envisioned to become a building component of future
6G, including as a component within O-RAN or digital twins. However, the
current SotA research for RAT classification predominantly revolves around
supervised Convolutional Neural Network (CNN)-based approach that require
extensive labeled dataset. Due to this, it is unclear how existing models
behave in environments for which training data is unavailable thus leaving open
questions regarding their generalization capabilities. In this paper, we
propose a new spectrum sensing workflow in which the model training does not
require any prior knowledge of the RATs transmitting in that area (i.e. no
labelled data) and the class assignment can be easily done through manual
mapping. Furthermore, we adapt a SSL deep clustering architecture capable of
autonomously extracting spectrum features from raw 1D Fast Fourier Transform
(FFT) data. We evaluate the proposed architecture on three real-world datasets
from three European cities, in the 868 MHz, 2.4 GHz and 5.9 GHz bands
containing over 10 RATs and show that the developed model achieves superior
performance by up to 35 percentage points with 22% fewer trainable parameters
and 50% less floating-point operations per second (FLOPS) compared to an SotA
AE-based reference architecture.

</details>


### [8] [ModFus-DM: Explore the Representation in Modulated Signal Diffusion Generated Models](https://arxiv.org/abs/2508.01719)
*Haoyue Tan,Yu Li,Zhenxi Zhang,Xiaoran Shi,Feng Zhou*

Main category: eess.SP

TL;DR: 提出了一种基于扩散模型的无监督自动调制分类框架ModFus-DM，解决了现有深度学习方法在非固定信号长度、分布偏移和有限标记信号下的问题。


<details>
  <summary>Details</summary>
Motivation: 自动调制分类（AMC）在无线通信系统中至关重要，但现有深度学习方法需要大量标记信号且难以应对非固定信号长度、分布偏移和标记信号有限的问题。

Method: 设计了调制信号扩散生成模型（MSDGM）和扩散感知特征融合模块（DAFFus），通过渐进去噪过程和多尺度特征融合提升表示学习能力。

Result: 在多个数据集上显著优于现有方法，特别是在有限标记、分布偏移和变长信号识别等挑战性场景下，24类识别任务在SNR≥12dB时准确率超过88.27%。

Conclusion: ModFus-DM是一种高效的无监督AMC框架，在多种复杂场景下表现出色，为无线通信系统提供了更稳健的解决方案。

Abstract: Automatic modulation classification (AMC) is essential for wireless
communication systems in both military and civilian applications. However,
existing deep learning-based AMC methods often require large labeled signals
and struggle with non-fixed signal lengths, distribution shifts, and limited
labeled signals. To address these challenges, we propose a modulation-driven
feature fusion via diffusion model (ModFus-DM), a novel unsupervised AMC
framework that leverages the generative capacity of diffusion models for robust
modulation representation learning. We design a modulated signal diffusion
generation model (MSDGM) to implicitly capture structural and semantic
information through a progressive denoising process. Additionally, we propose
the diffusion-aware feature fusion (DAFFus) module, which adaptively aggregates
multi-scale diffusion features to enhance discriminative representation.
Extensive experiments on RML2016.10A, RML2016.10B, RML2018.01A and RML2022
datasets demonstrate that ModFus-DM significantly outperforms existing methods
in various challenging scenarios, such as limited-label settings, distribution
shifts, variable-length signal recognition and channel fading scenarios.
Notably, ModFus-DM achieves over 88.27% accuracy in 24-type recognition tasks
at SNR $\geq $ 12dB with only 10 labeled signals per type.

</details>


### [9] [FAS Enabled UAV for Energy-Efficient WPCNs](https://arxiv.org/abs/2508.01771)
*Nagla Abuzgaia,Abdelhamid Salem,Ahmed Elbarsha*

Main category: eess.SP

TL;DR: 论文提出了一种利用流体天线系统（FAS）提升无人机在无线供能通信网络中通信速率和能量效率的创新方案。


<details>
  <summary>Details</summary>
Motivation: 通过动态端口切换能力，无人机可以自适应选择最佳天线位置，以最大化下行无线能量传输和上行无线数据传输的信道增益。

Method: 利用FAS技术，推导了Nakagami-m相关衰落信道下遍历频谱率的精确解析表达式和高信噪比下的渐近表达式。

Result: 蒙特卡洛仿真验证了解析表达式的准确性，并表明FAS相比固定天线系统显著提升了无人机的能量效率。

Conclusion: FAS技术为无人机在无线供能通信网络中提供了更高的通信速率和能量效率。

Abstract: This letter presents an innovative scheme to enhance the communication rate
and energy efficiency (EE) of Unmanned Aerial Vehicle (UAV) in wireless powered
communication networks (WPCNs) by deploying the emerging fluid antenna system
(FAS) technology onto the UAV. Our proposed approach leverages the dynamic port
switching capability of FAS, enabling the UAV to adaptively select the optimal
antenna location that maximizes channel gain for both downlink wireless power
transfer (WPT) and uplink wireless data transfer (WDT). We derive both exact
analytical expression of the ergodic spectral rate, and asymptotic expression
at high signal to noise ratio (SNR) regime under Nakagami-m correlated fading
channels. The Mont-Carlo simulation results confirms the accuracy of the
analytical expressions and demonstrate the substantial increase in energy
efficiency of UAV with FAS compared to fixed antenna systems.

</details>


### [10] [Statistical Multiport-Network Modeling and Efficient Discrete Optimization of RIS](https://arxiv.org/abs/2508.01776)
*Cheima Hammami,Luc Le Magoarou,Philipp del Hougne*

Main category: eess.SP

TL;DR: 论文研究了物理一致优化方法在具有互耦合和1比特可调元件的可重构智能表面（RIS）中的应用，比较了基于模型和无模型方法，并评估了智能初始化的潜在优势。


<details>
  <summary>Details</summary>
Motivation: 填补现有RIS原型中硬件约束（如互耦合和1比特可调元件）下的物理一致优化研究空白。

Method: 比较了基于模型的方法（温度退火反向传播）和无模型方法（坐标下降、遗传算法），并引入了一种生成多端口网络模型参数统计集合的技术。

Result: 发现当互耦合不可忽略时，随机初始化的坐标下降方法在性能、执行时间和内存使用方面表现最佳。

Conclusion: 研究结果可推广到非对角RIS、堆叠智能超表面、动态超表面天线和波域物理神经网络等领域。

Abstract: This Letter fills the research gap on physics-consistent optimization for
reconfigurable intelligent surfaces (RISs) with mutual coupling (MC) and
1-bit-tunable elements, a common hardware constraint in existing RIS
prototypes. We compare a model-based method (temperature-annealed
back-propagation) and model-agnostic methods (coordinate descent, genetic
algorithm), and evaluate potential benefits of intelligently initializing these
methods. To facilitate our evaluation, we introduce a technique for generating
statistical ensembles of multiport-network model parameters, wherein a single
hyper-parameter adjusts the MC strength. The technique is a generalization of
Rayleigh fading to radio environments with deterministic programmability, and
it accounts for passivity constraints as well as the coherent-backscattering
effect. We find that, except when MC is negligible, coordinate descent with
random initialization yields the most favorable trade-off in terms of
performance, execution time and memory usage. We expect our findings to extend
to beyond-diagonal RIS, stacked intelligent metasurfaces, dynamic metasurface
antennas, and wave-domain physical neural networks.

</details>


### [11] [A Heuristic Method for Simplified Resource Allocation based on Comparative Advantage in Wireless Access Systems](https://arxiv.org/abs/2508.01824)
*Lin Cheng,Bernardo A. Huberman*

Main category: eess.SP

TL;DR: 提出一种启发式方法，通过比较优势简化资源分配，降低计算复杂度并保持近优性能。


<details>
  <summary>Details</summary>
Motivation: 解决多小区网络中功率分配的高计算复杂度问题。

Method: 利用比较优势概念，减少优化搜索空间，以PD-NOMA为例展示方法。

Result: 搜索空间维度减半，计算开销显著降低，频谱利用率高效。

Conclusion: 方法有效且适用于下一代无线网络，提升系统性能和可扩展性。

Abstract: This paper presents a heuristic method for simplifying resource allocation in
access systems, leveraging the concept of comparative advantage to reduce
computational complexity while maintaining near-optimal performance. Using
power-division non-orthogonal multiple access (PD-NOMA) as an example, we
demonstrate how this approach mitigates the challenge of power allocation in
multi-cell networks. Our method reduces the search space for optimization,
significantly decreasing computational overhead while ensuring efficient
spectrum utilization. In principle, the method reduces the dimensions of search
space by half. Extensive analysis and simulations validate its effectiveness,
highlighting its potential for practical deployment in next-generation wireless
networks. The proposed framework can help streamline resource allocation in
complex communication environments, enhancing system performance and
scalability.

</details>


### [12] [RIS-Aided Near-Field Channel Estimation under Mutual Coupling and Spatial Correlation](https://arxiv.org/abs/2508.01828)
*Ahmad Dkhan,Simon Tarboush,Hadi Sarieddeen,Tareq Y. Al-Naffouri*

Main category: eess.SP

TL;DR: 论文提出了一种改进的RS-LS估计器，将互耦效应纳入近场传播环境，显著提升了信道估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO与RIS集成中高维信道矩阵带来的计算复杂性和导频开销问题，以及互耦效应和近场传播条件对信道估计的挑战。

Method: 提出改进的RS-LS估计器，结合互耦效应和近场传播环境，并分析其对空间自由度的影响。

Result: 在5 dB SNR下，相比忽略互耦的传统方法，性能提升约5 dB。

Conclusion: 改进的RS-LS估计器在复杂环境中显著提升了信道估计的准确性。

Abstract: The integration of reconfigurable intelligent surfaces (RIS) with extremely
large multiple-input multiple-output (MIMO) arrays at the base station has
emerged as a key enabler for enhancing wireless network performance. However,
this setup introduces high-dimensional channel matrices, leading to increased
computational complexity and pilot overhead in channel estimation. Mutual
coupling (MC) effects among densely packed unit cells, spatial correlation, and
near-field propagation conditions further complicate the estimation process.
Conventional estimators, such as linear minimum mean square error (MMSE),
require channel statistics that are challenging to acquire for high-dimensional
arrays, while least squares (LS) estimators suffer from performance
limitations. To address these challenges, the reduced-subspace least squares
(RS-LS) estimator leverages array geometry to enhance estimation accuracy. This
work advances the promising RS-LS estimation algorithm by explicitly
incorporating MC effects into the more realistic and challenging near-field
propagation environment within the increasingly relevant generalized RIS-aided
MIMO framework. Additionally, we investigate the impact of MC on the spatial
degrees of freedom (DoF). Our analysis reveals that accounting for MC effects
provides a significant performance gain of approximately 5 dB at an SNR of 5
dB, compared to conventional methods that ignore MC.

</details>


### [13] [Feature Reconstruction Aided Federated Learning for Image Semantic Communication](https://arxiv.org/abs/2508.02048)
*Yoon Huh,Bumjun Kim,Wan Choi*

Main category: eess.SP

TL;DR: 提出了一种名为FedSFR的联邦学习算法，通过语义特征重构优化图像传输中的JSCC模块训练，显著提升了稳定性和传输质量。


<details>
  <summary>Details</summary>
Motivation: 解决JSCC模块因知识库过时而导致的性能下降问题，提升图像传输效率。

Method: 采用联邦学习结合语义特征重构（FR），允许部分参与者传输更小的特征向量而非本地更新信息。

Result: 实验表明，FedSFR显著提升了FL过程的稳定性和图像传输质量，并数学推导了收敛速率以验证性能提升。

Conclusion: FedSFR通过语义特征重构和联邦学习的结合，有效优化了图像传输中的JSCC模块训练。

Abstract: Research in semantic communication has garnered considerable attention,
particularly in the area of image transmission, where joint source-channel
coding (JSCC)-based neural network (NN) modules are frequently employed.
However, these systems often experience performance degradation over time due
to an outdated knowledge base, highlighting the need for periodic updates. To
address this challenge in the context of training JSCC modules for image
transmission, we propose a federated learning (FL) algorithm with semantic
feature reconstruction (FR), named FedSFR. This algorithm more efficiently
utilizes the available communication capacity by allowing some of the selected
FL participants to transmit smaller feature vectors instead of local update
information. Unlike conventional FL methods, our approach integrates FR at the
parameter server (PS), stabilizing training and enhancing image transmission
quality. Experimental results demonstrate that the proposed scheme
significantly enhances both the stability and effectiveness of the FL process
compared to other algorithms. Furthermore, we mathematically derive the
convergence rate to validate the improved performance.

</details>


### [14] [Scoring ISAC: Benchmarking Integrated Sensing and Communications via Score-Based Generative Modeling](https://arxiv.org/abs/2508.02117)
*Lin Chen,Chang Cai,Huiyuan Yang,Xiaojun Yuan,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: 本文介绍了基于分数的生成模型在集成感知与通信（ISAC）系统性能评估中的应用，提出了一种称为“评分ISAC”的框架，用于在复杂现实条件下评估ISAC系统。


<details>
  <summary>Details</summary>
Motivation: ISAC系统在实际应用中面临硬件损伤、多径传播等非线性、多模态和非高斯分布问题，传统性能指标难以解析推导。

Method: 利用基于分数的生成模型从数据中学习性能指标，连接经典性能指标与分数函数，并提供实际训练技术。

Result: 实验验证了基于分数的性能估计器在目标检测和定位中的准确性，能够复现并扩展传统分析。

Conclusion: 基于分数的生成模型在ISAC性能分析、算法设计和系统优化中具有巨大潜力。

Abstract: Integrated sensing and communications (ISAC) is a key enabler for
next-generation wireless systems, aiming to support both high-throughput
communication and high-accuracy environmental sensing using shared spectrum and
hardware. Theoretical performance metrics, such as mutual information (MI),
minimum mean squared error (MMSE), and Bayesian Cram\'{e}r--Rao bound (BCRB),
play a key role in evaluating ISAC system performance limits. However, in
practice, hardware impairments, multipath propagation, interference, and scene
constraints often result in nonlinear, multimodal, and non-Gaussian
distributions, making it challenging to derive these metrics analytically.
Recently, there has been a growing interest in applying score-based generative
models to characterize these metrics from data, although not discussed for
ISAC. This paper provides a tutorial-style summary of recent advances in
score-based performance evaluation, with a focus on ISAC systems. We refer to
the summarized framework as scoring ISAC, which not only reflects the core
methodology based on score functions but also emphasizes the goal of scoring
(i.e., evaluating) ISAC systems under realistic conditions. We present the
connections between classical performance metrics and the score functions and
provide the practical training techniques for learning score functions to
estimate performance metrics. Proof-of-concept experiments on target detection
and localization validate the accuracy of score-based performance estimators
against ground-truth analytical expressions, illustrating their ability to
replicate and extend traditional analyses in more complex, realistic settings.
This framework demonstrates the great potential of score-based generative
models in ISAC performance analysis, algorithm design, and system optimization.

</details>


### [15] [An Overview of Algorithms for Contactless Cardiac Feature Extraction from Radar Signals: Advances and Challenges](https://arxiv.org/abs/2508.02122)
*Yuanyuan Zhang,Rui Yang,Yutao Yue,Eng Gee Lim,Zidong Wang*

Main category: eess.SP

TL;DR: 本文是一篇综述论文，首次专注于从雷达信号中提取心脏特征的算法，提出新分类法并评估优缺点，列举公开数据集，讨论未解挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 无接触心脏监测在未来智能家居和车内监测中有广泛应用潜力，雷达因其高精度和鲁棒性成为理想选择，但信号处理部分尚未被充分综述。

Method: 提出新分类法揭示算法核心特征，详细评估优缺点，列举公开数据集及其配置。

Result: 为研究人员提供快速了解心脏特征提取算法趋势和最新发展的指南，并指出未来研究方向。

Conclusion: 本文可作为研究者和从业者的指南，鼓励进一步研究解决该领域的主要障碍。

Abstract: Contactless cardiac monitoring has vast potential to replace contact-based
monitoring in various future scenarios such as smart home and in-cabin
monitoring. Various contactless sensors can be potentially implemented for
cardiac monitoring, such as cameras, acoustic sensors, Wi-Fi routers and
radars. Among all these sensors, radar could achieve unobtrusive monitoring
with high accuracy and robustness at the same time. The research about
radar-based cardiac monitoring can be generally divided into the radar
architecture design and signal-processing parts, where the former has been
thoroughly reviewed in the literature but not the latter. To the best of the
author knowledge, this is the first review paper that focuses on elaborating
the algorithms for extracting cardiac features from the received radar signal.
In addition, a new taxonomy is proposed to reveal the core feature of each
algorithm, with the pros and cons evaluated in detail. Furthermore, the public
datasets containing the received radar signal and ground-truth cardiac feature
signal are listed with detailed configurations, and the corresponding
evaluations may help the researchers select the suitable dataset. At last,
several unsolved challenges and future directions are suggested and discussed
in detail to encourage future research on solving the main obstacles in this
field. In summary, this review can be served as a guide for researchers and
practitioners to quickly understand the research trend and recent development
of the cardiac feature extraction algorithms, and it is worth further
investigating the relative area based on the proposed challenges and future
directions.

</details>


### [16] [Analysis of Broad Beam Beamforming for Collocated and Distributed MIMO](https://arxiv.org/abs/2508.02135)
*Ahmet Kaplan,Diana P. M. Osorio,Erik G. Larsson*

Main category: eess.SP

TL;DR: 本文研究了双极化波束成形（DPBF）在共置和分布式MIMO中的性能，提出了改进覆盖的方法。


<details>
  <summary>Details</summary>
Motivation: 评估DPBF在非视距（NLoS）条件下的效率，并改进其在共置和分布式MIMO中的覆盖性能。

Method: 建模NLoS条件下的反射系数，提出正交空时分组码改进共置MIMO的覆盖，并提出分布式MIMO的DPBF方法。

Result: 分布式MIMO的DPBF方法比共置MIMO的DPBF覆盖更好。

Conclusion: DPBF在分布式MIMO中表现更优，为实际应用提供了改进方案。

Abstract: Broad beam beamforming (BF) design in multiple-input multiple-output (MIMO)
can be convenient for initial access, synchronization, and sensing capabilities
in cellular networks by avoiding overheads of sweeping methods while making
efficient use of resources. Phase-only BF is key for maximizing power
efficiency across antennas. A successful method to produce broad beams is the
phase-only dual-polarization BF (DPBF). However, its efficiency has not been
proved in non-line-of-sight (NLoS). Therefore, this paper contributes by
evaluating DPBF in collocated and distributed MIMO configurations under both
line-of-sight (LoS) and NLoS channel conditions. We model the reflection
coefficients for different materials in NLoS conditions and propose the use of
orthogonal space-time block code to improve the coverage compared to the DPBF
in collocated MIMO (C-MIMO). We further propose a DPBF method for distributed
MIMO and show that it achieves better coverage than C-MIMO with DPBF.

</details>


### [17] [The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise](https://arxiv.org/abs/2508.02223)
*Mingyan Gong*

Main category: eess.SP

TL;DR: ECME算法在非均匀噪声下的DOA估计中表现优于FAAN方法，具有更快的收敛速度和更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决非均匀噪声环境下方向到达（DOA）估计的问题，并改进现有方法的收敛性和计算效率。

Method: 设计了一种基于期望/条件最大化（ECME）的算法，作为FAAN方法的扩展，计算复杂度相近但性能更优。

Result: 数值结果表明，ECME算法比FAAN方法收敛更快且计算效率更高。

Conclusion: ECME算法在非均匀噪声DOA估计中具有显著优势，适合实际应用。

Abstract: Maximum likelihood factor analysis has been used for direction of arrival
estimation in unknown nonuniform noise and some iterative approaches have been
developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN)
method proposed by Stoica and Babu has excellent convergence properties. In
this letter, the Expectation/Conditional Maximization Either (ECME) algorithm,
an extension of the expectation-maximization algorithm, is designed, which has
almost the same computational complexity at each iteration as the FAAN method.
However, numerical results show that the ECME algorithm yields faster stable
convergence and is computationally more efficient.

</details>


### [18] [Adaptive Phase-Shifted Pilot Design for Uplink Multiple Access in ISAC Systems](https://arxiv.org/abs/2508.02334)
*Ahmet Sacid Sümer,Ebubekir Memişoğlu,Hüseyin Arslan*

Main category: eess.SP

TL;DR: APS-ISAC是一种新型的导频设计方法，通过自适应相位偏移和重叠块导频结构，显著提升了上行ISAC系统的频谱效率和感知性能。


<details>
  <summary>Details</summary>
Motivation: 传统导频分配方法在频谱效率和感知性能之间存在权衡，PS-ISAC虽提出解决方案但受限于固定循环前缀约束。

Method: 提出自适应相位偏移ISAC（APS-ISAC），采用重叠块导频结构和用户设备（UE）特定的相位偏移，以最大化UE复用并保持可分离的连续信道脉冲响应。

Result: 仿真结果表明，APS-ISAC在频谱效率上显著优于传统方法，UE复用数量翻倍，且在功率约束下具有更低的均方误差和复杂度。

Conclusion: APS-ISAC是一种可扩展、频谱高效、抗模糊且低复杂度的导频设计范式，适用于未来上行ISAC系统。

Abstract: In uplink integrated sensing and communication (ISAC) systems, pilot signal
design is crucial for enabling accurate channel estimation and reliable radar
sensing. In orthogonal frequency-division multiple access (OFDMA)-based
frameworks, conventional pilot allocation schemes face a trade-off between
spectral efficiency (SE) and sensing performance. Interleaved pilots improve
user equipment (UE) multiplexing through sparse allocation but reduce the
maximum unambiguous range. Conversely, orthogonal block-based pilots reduce
range ambiguity but degrade sensing resolution due to limited delay
granularity. To address this trade-off, the phase-shifted ISAC (PS-ISAC) scheme
was recently proposed for uplink multiple access in ISAC systems. However,
PS-ISAC suffers from spectral inefficiency due to the fixed cyclic prefix (CP)
constraints. To overcome these limitations, we propose adaptive
phase-shifted-ISAC (APS-ISAC), an enhanced pilot scheme that employs an
overlapped block-pilot structure with UE-specific phase shifts determined by
maximum excess delay of each UE. This design enables UEs to share the same
time-frequency resources while preserving separable and contiguous channel
impulse responses (CIRs) at the base station (BS). Simulation results show that
APS-ISAC significantly outperforms conventional pilot allocation methods in
terms of SE, approximately doubling the number of multiplexed UEs. It also
achieves lower mean square error (MSE) under power constraints with reduced
complexity. Furthermore, it yields maximum range resolution and unambiguous
sensing performance. These results establish APS-ISAC as a scalable, spectrally
efficient, ambiguity-resilient, and low-complexity pilot design paradigm for
future uplink ISAC systems.

</details>


### [19] [Detecting and measuring respiratory events in horses during exercise with a microphone: deep learning vs. standard signal processing](https://arxiv.org/abs/2508.02349)
*Jeanne I. M. Parmentier,Rhana M. Aarts,Elin Hernlund,Marie Rhodin,Berend Jan van der Zwaag*

Main category: eess.SP

TL;DR: 论文比较了深度学习和信号处理方法，用于从马匹高强度运动时的麦克风录音中自动检测呼吸事件和提取动态呼吸率。结果表明，时间卷积网络表现最佳。


<details>
  <summary>Details</summary>
Motivation: 监测呼吸参数（如呼吸率）有助于了解训练对马匹健康和表现的影响，从而改善马匹福利。

Method: 比较了深度学习模型（时间卷积网络和长短时记忆网络）与信号处理方法，用于检测呼吸事件和计算动态呼吸率。

Result: 时间卷积网络在检测呼气声音和估计动态呼吸率方面表现最佳（F1分数中位数0.94，MAE±CI：1.44±1.04 bpm）。

Conclusion: 该研究首次实现了马匹运动时呼吸声音的自动检测和动态呼吸率的自动计算，未来将进一步验证模型在低强度运动中的表现和不同麦克风放置的效果。

Abstract: Monitoring respiration parameters such as respiratory rate could be
beneficial to understand the impact of training on equine health and
performance and ultimately improve equine welfare. In this work, we compare
deep learning-based methods to an adapted signal processing method to
automatically detect cyclic respiratory events and extract the dynamic
respiratory rate from microphone recordings during high intensity exercise in
Standardbred trotters. Our deep learning models are able to detect exhalation
sounds (median F1 score of 0.94) in noisy microphone signals and show promising
results on unlabelled signals at lower exercising intensity, where the
exhalation sounds are less recognisable. Temporal convolutional networks were
better at detecting exhalation events and estimating dynamic respiratory rates
(median F1: 0.94, Mean Absolute Error (MAE) $\pm$ Confidence Intervals (CI):
1.44$\pm$1.04 bpm, Limits Of Agreements (LOA): 0.63$\pm$7.06 bpm) than long
short-term memory networks (median F1: 0.90, MAE$\pm$CI: 3.11$\pm$1.58 bpm) and
signal processing methods (MAE$\pm$CI: 2.36$\pm$1.11 bpm). This work is the
first to automatically detect equine respiratory sounds and automatically
compute dynamic respiratory rates in exercising horses. In the future, our
models will be validated on lower exercising intensity sounds and different
microphone placements will be evaluated in order to find the best combination
for regular monitoring.

</details>


### [20] [Toward a reliable PWM-based light-emitting diode visual stimulus for improved SSVEP response with minimal visual fatigue](https://arxiv.org/abs/2508.02359)
*Surej Mouli,Ramaswamy Palaniappan*

Main category: eess.SP

TL;DR: 研究探讨了通过调整视觉刺激的高占空比来减少SSVEP使用中的视觉疲劳，并提高性能。


<details>
  <summary>Details</summary>
Motivation: SSVEP在脑机接口等应用中具有高信息传输率，但视觉疲劳和PWM精度问题限制了其长期使用。

Method: 使用50%至95%占空比的PWM刺激，记录EEG数据，测试10名受试者的反应。

Result: 高占空比（85%）显著减少视觉疲劳，且SSVEP响应性能达到峰值。

Conclusion: 85%占空比的视觉刺激可能为SSVEP的实用化提供解决方案。

Abstract: Steady state visual evoked response (SSVEP) is widely used in visual-based
diagnosis and applications such as brain computer interfacing due to its high
information transfer rate and the capability to activate commands through
simple gaze control. However, one major impediment in using flashing visual
stimulus to obtain SSVEP is eye fatigue that prevents continued long term use
preventing practical deployment. This combined with the difficulty in
establishing precise pulse-width modulation (PWM) that results in poorer
accuracy warrants the development of appropriate approach to solve these
issues. Various studies have suggested the usage of high frequencies of visual
stimulus to reduce the visual fatigue for the user but this results in poor
response performance. Here, the authors study the use of extremely high
duty-cycles in the stimulus in the hope of solving these constraints.
Electroencephalogram data was recorded with PWM duty-cycles of 50 to 95%
generated by a precise custom-made light-emitting diode hardware and tested ten
subjects responded that increasing duty-cycles had less visual strain for all
the frequency values and the SSVEP exhibited a subject-independent peak
response for duty-cycle of 85%. This could pave the way for increased usage of
SSVEP for practical applications.

</details>


### [21] [The Role of Review Process Failures in Affective State Estimation: An Empirical Investigation of DEAP Dataset](https://arxiv.org/abs/2508.02417)
*Nazmun N Khan,Taylor Sweet,Chase A Harvey,Calder Knapp,Dean J. Krusienski,David E Thompson*

Main category: eess.SP

TL;DR: 研究发现EEG情感状态估计的可靠性存在问题，87%的论文存在方法学错误，导致分类准确率被夸大高达46%。


<details>
  <summary>Details</summary>
Motivation: 探讨EEG数据在情感状态估计中的可靠性，揭示现有研究中的方法学问题。

Method: 回顾101项研究，分析DEAP数据集的使用情况，识别常见方法学错误。

Result: 87%的论文存在方法学错误，错误可能使分类准确率夸大46%。

Conclusion: 需制定更严格的方法学标准和评估协议，改进神经科学中机器学习的同行评审过程。

Abstract: The reliability of affective state estimation using EEG data is in question,
given the variability in reported performance and the lack of standardized
evaluation protocols. To investigate this, we reviewed 101 studies, focusing on
the widely used DEAP dataset for emotion recognition. Our analysis revealed
widespread methodological issues that include data leakage from improper
segmentation, biased feature selection, flawed hyperparameter optimization,
neglect of class imbalance, and insufficient methodological reporting. Notably,
we found that nearly 87% of the reviewed papers contained one or more of these
errors. Moreover, through experimental analysis, we observed that such
methodological flaws can inflate the classification accuracy by up to 46%.
These findings reveal fundamental gaps in standardized evaluation practices and
highlight critical deficiencies in the peer review process for machine learning
applications in neuroscience, emphasizing the urgent need for stricter
methodological standards and evaluation protocols.

</details>


### [22] [Secure Energy Efficient Wireless Transmission: A Finite v/s Infinite-Horizon RL Solution](https://arxiv.org/abs/2508.02447)
*Shalini Tripathi,Ankur Bansal,Holger Claussen,Lester Ho,Chinmoy Kundu*

Main category: eess.SP

TL;DR: 提出了一种联合优化源节点和目的节点功率分配的方法，以最大化无线网络在有限时间内的平均保密能量效率（SEE）。


<details>
  <summary>Details</summary>
Motivation: 通过全双工技术提升无线网络的保密性能，同时考虑能量收集和有限电池容量的约束。

Method: 将问题建模为有限时域强化学习（RL）问题，提出FHJPA算法，并与贪婪算法（GA）和无限时域算法（IHJPA）进行比较。

Result: FHJPA算法在SEE和计算效率上优于GA和IHJPA算法，GA在电池充足时表现接近FHJPA。

Conclusion: FHJPA算法在有限时域传输中表现最佳，而IHJPA在时域延长时性能差距减小。

Abstract: In this paper, a joint optimal allocation of transmit power at the source and
jamming power at the destination is proposed to maximize the average secrecy
energy efficiency (SEE) of a wireless network within a finite time duration.
The destination transmits the jamming signal to improve secrecy by utilizing
full-duplex capability. The source and destination both have energy harvesting
(EH) capability with limited battery capacity. Due to the Markov nature of the
system, the problem is formulated as a finite-horizon reinforcement learning
(RL) problem. We propose the finite-horizon joint power allocation (FHJPA)
algorithm for the finite-horizon RL problem and compare it with a
low-complexity greedy algorithm (GA). An infinite-horizon joint power
allocation (IHJPA) algorithm is also proposed for the corresponding
infinite-horizon problem. A comparative analysis of these algorithms is carried
out in terms of SEE, expected total transmitted secure bits, and computational
complexity. The results show that the FHJPA algorithm outperforms the GA and
IHJPA algorithms due to its appropriate modelling in finite horizon
transmission. When the source node battery has sufficient energy, the GA can
yield performance close to the FHJPA algorithm despite its low-complexity. When
the transmission time horizon increases, the accuracy of the infinite-horizon
model improves, resulting in a reduced performance gap between FHJPA and IHJPA
algorithms. The computational time comparison shows that the FHJPA algorithm
takes $16.6$ percent less time than the IHJPA algorithm.

</details>


### [23] [Inverse harmonic clustering for multi-pitch estimation: an optimal transport approach](https://arxiv.org/abs/2508.02471)
*Anton Björkman,Filip Elvander*

Main category: eess.SP

TL;DR: 该论文提出了一种基于最优传输理论的多音高估计方法，通过解耦正则化和字典设计，提高了对非谐波现象的鲁棒性，并在合成和真实数据上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决多音高估计问题，即从噪声测量中识别叠加的截断谐波序列，同时克服现有方法在谐波结构恢复中的局限性。

Method: 将问题建模为在单位圆上恢复谐波结构化的测度，利用最优传输理论的正则化方法，同时推断信号的频谱内容并将其分配到少量谐波序列中。

Result: 提出的方法在合成和真实数据上表现优于传统统计信号处理方法，与基于网络的方法相当或更好，除非后者经过专门训练且数据量更大。

Conclusion: 该方法通过解耦正则化和字典设计，提高了多音高估计的鲁棒性和性能，为谐波结构恢复提供了新思路。

Abstract: In this work, we consider the problem of multi-pitch estimation, i.e.,
identifying super-imposed truncated harmonic series from noisy measurements. We
phrase this as recovering a harmonically-structured measure on the unit circle,
where the structure is enforced using regularizers based on optimal transport
theory. In the resulting framework, a signal's spectral content is
simultaneously inferred and assigned, or transported, to a small set of
harmonic series defined by their corresponding fundamental frequencies. In
contrast to existing methods from the compressed sensing paradigm, the proposed
framework decouples regularization and dictionary design and mitigates
coherency problems. As a direct consequence, this also introduces robustness to
the phenomenon of inharmonicity. From this framework, we derive two estimation
methods, one for stochastic and one for deterministic signals, and propose
efficient numerical algorithms implementing them. In numerical studies on both
synthetic and real data, the proposed methods are shown to achieve better
estimation performance as compared to other methods from statistical signal
processing literature. Furthermore, they perform comparably or better than
network-based methods, except when the latter are specially trained on the
data-type considered and are given access to considerably more data during
inference.

</details>


### [24] [Cramér-Rao Bound for Direct Position Estimation in OFDM Based Cellular Systems](https://arxiv.org/abs/2508.02559)
*Sijia Li,Rui Sun,Bing Xu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文研究了基于OFDM信号的直接位置估计（DPE）的理论极限和性能，推导了其Cramér-Rao界（CRB），并与传统两步定位法在NLOS多径环境中的性能进行了对比。


<details>
  <summary>Details</summary>
Motivation: 探索DPE在OFDM定位系统中的理论极限和性能，填补现有研究的空白。

Method: 推导DPE的CRB，并与两步定位法在NLOS多径环境中进行性能对比。

Result: 1) DPE在OFDM系统中始终优于两步法；2) 大带宽对两种方法均关键，增加子载波间距更有利；3) 使用多个OFDM符号可显著提升定位精度，但过多符号会带来边际收益和计算复杂度增加。

Conclusion: DPE在OFDM系统中具有显著优势，但需权衡符号数量和计算复杂度。

Abstract: Although direct position estimation (DPE) has been demonstrated to offer
enhanced robustness in GNSS receivers, its theoretical limits and performance
in OFDM based positioning systems remain largely unexplored. In this paper, the
Cram\'er-Rao bound (CRB) for DPE using OFDM based cellular signals is derived
and benchmarked against the conventional two-step positioning method to assess
their relative performance in non-line-of-sight (NLOS) dominated multipath
environments. Numerical results reveal that 1) the DPE method consistently
outperforms the two-step approach in OFDM systems under all evaluated
conditions; 2) a large bandwidth is crucial in both methods, and increasing
subcarrier spacing is more beneficial for a fixed bandwidth; 3) utilizing
multiple OFDM symbols for positioning leads to substantial improvements in
localization accuracy compared to relying on a single symbol. However, further
increasing the number of symbols yields marginal improvements while
significantly increasing computational complexity.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [25] [Fusion of Modulation Spectrogram and SSL with Multi-head Attention for Fake Speech Detection](https://arxiv.org/abs/2508.01034)
*Rishith Sadashiv T N,Abhishek Bedge,Saisha Suresh Bore,Jagabandhu Mishra,Mrinmoy Bhattacharjee,S R Mahadeva Prasanna*

Main category: eess.AS

TL;DR: 提出了一种结合自监督语音嵌入和调制谱特征的新方法，用于提升伪造语音检测系统的跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前伪造语音检测系统因缺乏多样化训练数据，在跨领域语音样本上表现不佳。

Method: 使用自监督语音嵌入和调制谱特征融合，结合AASIST后端网络进行分类。

Result: 在ASVspoof 2019和MLAAD数据集上分别提升37%和20%，跨领域测试提升36%。

Conclusion: 该方法显著提升了跨领域和多语言环境下的伪造语音检测性能。

Abstract: Fake speech detection systems have become a necessity to combat against
speech deepfakes. Current systems exhibit poor generalizability on
out-of-domain speech samples due to lack to diverse training data. In this
paper, we attempt to address domain generalization issue by proposing a novel
speech representation using self-supervised (SSL) speech embeddings and the
Modulation Spectrogram (MS) feature. A fusion strategy is used to combine both
speech representations to introduce a new front-end for the classification
task. The proposed SSL+MS fusion representation is passed to the AASIST
back-end network. Experiments are conducted on monolingual and multilingual
fake speech datasets to evaluate the efficacy of the proposed model
architecture in cross-dataset and multilingual cases. The proposed model
achieves a relative performance improvement of 37% and 20% on the ASVspoof 2019
and MLAAD datasets, respectively, in in-domain settings compared to the
baseline. In the out-of-domain scenario, the model trained on ASVspoof 2019
shows a 36% relative improvement when evaluated on the MLAAD dataset. Across
all evaluated languages, the proposed model consistently outperforms the
baseline, indicating enhanced domain generalization.

</details>


### [26] [Multi-Granularity Adaptive Time-Frequency Attention Framework for Audio Deepfake Detection under Real-World Communication Degradations](https://arxiv.org/abs/2508.01467)
*Haohan Shi,Xiyu Shi,Safak Dogan,Tianjin Huang,Yunxiao Zhang*

Main category: eess.AS

TL;DR: 提出了一种统一的音频深度伪造检测框架，通过多粒度自适应注意力机制应对现实通信环境中的退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频深度伪造检测方法在干净条件下表现良好，但在现实通信环境中的退化（如丢包和语音编解码压缩）下效果显著下降。

Method: 设计了多粒度自适应注意力（MGAA）架构，利用多尺度注意力头捕捉全局和局部感受野，并通过自适应融合机制动态调整注意力。

Result: 在多种现实通信退化场景下表现优于现有方法，显著提高了真假音频类别的可分性和决策边界清晰度。

Conclusion: 该框架具有鲁棒性和实际部署潜力，适用于现实通信环境。

Abstract: The rise of highly convincing synthetic speech poses a growing threat to
audio communications. Although existing Audio Deepfake Detection (ADD) methods
have demonstrated good performance under clean conditions, their effectiveness
drops significantly under degradations such as packet losses and speech codec
compression in real-world communication environments. In this work, we propose
the first unified framework for robust ADD under such degradations, which is
designed to effectively accommodate multiple types of Time-Frequency (TF)
representations. The core of our framework is a novel Multi-Granularity
Adaptive Attention (MGAA) architecture, which employs a set of customizable
multi-scale attention heads to capture both global and local receptive fields
across varying TF granularities. A novel adaptive fusion mechanism subsequently
adjusts and fuses these attention branches based on the saliency of TF regions,
allowing the model to dynamically reallocate its focus according to the
characteristics of the degradation. This enables the effective localization and
amplification of subtle forgery traces. Extensive experiments demonstrate that
the proposed framework consistently outperforms state-of-the-art baselines
across various real-world communication degradation scenarios, including six
speech codecs and five levels of packet losses. In addition, comparative
analysis reveals that the MGAA-enhanced features significantly improve
separability between real and fake audio classes and sharpen decision
boundaries. These results highlight the robustness and practical deployment
potential of our framework in real-world communication environments.

</details>


### [27] [Lumename: Wearable Device for Hearing Impaired with Personalized ML-Based Auditory Detection and Haptic-Visual Alerts](https://arxiv.org/abs/2508.01576)
*Jeanelle Dao,Jadelynn Dao*

Main category: eess.AS

TL;DR: Lumename是一款实时智能手表，通过设备端机器学习检测用户自定义的名字，并生成触觉-视觉警报，帮助听力障碍者识别语音命令。


<details>
  <summary>Details</summary>
Motivation: 全球有4.3亿人患有听力障碍，识别语音命令（如名字）对他们来说很困难。

Method: 使用新颖的音频调制技术增强用户样本，生成多样化的性别和年龄样本，并通过约束随机迭代优化模型参数。

Result: 开发了一个低资源、低功耗的TinyML模型，在基于Arduino Nano 33 BLE Sense的智能手表上实现了91.67%的准确率。

Conclusion: Lumename为听力障碍者提供了一种高效的语音命令识别解决方案。

Abstract: According to the World Health Organization, 430 million people experience
disabling hearing loss. For them, recognizing spoken commands such as one's
name is difficult. To address this issue, Lumename, a real-time smartwatch,
utilizes on-device machine learning to detect a user-customized name before
generating a haptic-visual alert. During training, to overcome the need for
large datasets, Lumename uses novel audio modulation techniques to augment
samples from one user and generate additional samples to represent diverse
genders and ages. Constrained random iterations were used to find optimal
parameters within the model architecture. This approach resulted in a
low-resource and low-power TinyML model that could quickly infer various
keyword samples while remaining 91.67\% accurate on a custom-built smartwatch
based on an Arduino Nano 33 BLE Sense.

</details>


### [28] [An Age-Agnostic System for Robust Speaker Verification](https://arxiv.org/abs/2508.01637)
*Jiusi Zheng,Vishwas Shetty,Natarajan Balaji Shankar,Abeer Alwan*

Main category: eess.AS

TL;DR: 提出了一种年龄无关的说话人验证系统（AASV），通过域分类器分离年龄相关特征，扩展嵌入空间，实现跨年龄组的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 解决儿童说话人验证（C-SV）中因声学不匹配导致的性能下降问题，同时避免成人说话人验证（A-SV）性能的显著退化。

Method: 使用域分类器分离年龄相关特征，扩展嵌入空间，形成统一的说话人表示。

Result: 在OGI和VoxCeleb数据集上验证了方法的有效性，缩小了跨年龄组的性能差异。

Conclusion: AASV系统为包容性和年龄自适应的说话人验证系统奠定了基础。

Abstract: In speaker verification (SV), the acoustic mismatch between children's and
adults' speech leads to suboptimal performance when adult-trained SV systems
are applied to children's speaker verification (C-SV). While domain adaptation
techniques can enhance performance on C-SV tasks, they often do so at the
expense of significant degradation in performance on adults' SV (A-SV) tasks.
In this study, we propose an Age Agnostic Speaker Verification (AASV) system
that achieves robust performance across both C-SV and A-SV tasks. Our approach
employs a domain classifier to disentangle age-related attributes from speech
and subsequently expands the embedding space using the extracted domain
information, forming a unified speaker representation that is robust and highly
discriminative across age groups. Experiments on the OGI and VoxCeleb datasets
demonstrate the effectiveness of our approach in bridging SV performance
disparities, laying the foundation for inclusive and age-adaptive SV systems.

</details>


### [29] [Test-Time Training for Speech Enhancement](https://arxiv.org/abs/2508.01847)
*Avishkar Behera,Riya Ann Easow,Venkatesh Parvathala,K. Sri Rama Murty*

Main category: eess.AS

TL;DR: 本文提出了一种新颖的测试时训练（TTT）方法用于语音增强，通过Y形架构结合主任务和自监督辅助任务，动态适应新领域，无需标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决语音增强中不可预测的噪声条件和领域偏移问题，提升模型适应性。

Method: 采用Y形架构，结合主任务和自监督辅助任务（如噪声增强信号重建或掩码频谱图预测），动态优化模型。

Result: 在合成和真实数据集上，语音质量指标持续提升，优于基线模型。

Conclusion: TTT在语音增强中效果显著，为自适应和鲁棒语音处理提供了新思路。

Abstract: This paper introduces a novel application of Test-Time Training (TTT) for
Speech Enhancement, addressing the challenges posed by unpredictable noise
conditions and domain shifts. This method combines a main speech enhancement
task with a self-supervised auxiliary task in a Y-shaped architecture. The
model dynamically adapts to new domains during inference time by optimizing the
proposed self-supervised tasks like noise-augmented signal reconstruction or
masked spectrogram prediction, bypassing the need for labeled data. We further
introduce various TTT strategies offering a trade-off between adaptation and
efficiency. Evaluations across synthetic and real-world datasets show
consistent improvements across speech quality metrics, outperforming the
baseline model. This work highlights the effectiveness of TTT in speech
enhancement, providing insights for future research in adaptive and robust
speech processing.

</details>


### [30] [Word Error Rate Definitions and Algorithms for Long-Form Multi-talker Speech Recognition](https://arxiv.org/abs/2508.02112)
*Thilo von Neumann,Christoph Boeddeker,Marc Delcroix,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 论文探讨了长格式多说话人语音识别系统的评估指标，提出了统一的WER描述，并引入DI-cpWER以分析说话人混淆错误的影响。此外，还提出了可视化方法和高效算法来优化计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统WER无法适用于长格式多说话人语音识别系统，需要扩展和改进评估指标以更全面地反映系统性能。

Method: 提出了DI-cpWER以忽略说话人混淆错误，并引入可视化方法和贪婪算法来优化计算复杂度。

Result: DI-cpWER能有效分析说话人混淆错误的影响，贪婪算法显著降低了ORC-WER和DI-cpWER的计算复杂度。

Conclusion: 论文为多说话人语音识别系统提供了更全面的评估框架，并提出了高效的计算方法。

Abstract: The predominant metric for evaluating speech recognizers, the Word Error Rate
(WER) has been extended in different ways to handle transcripts produced by
long-form multi-talker speech recognizers. These systems process long
transcripts containing multiple speakers and complex speaking patterns so that
the classical WER cannot be applied. There are speaker-attributed approaches
that count speaker confusion errors, such as the concatenated
minimum-permutation WER cpWER and the time-constrained cpWER (tcpWER), and
speaker-agnostic approaches, which aim to ignore speaker confusion errors, such
as the Optimal Reference Combination WER (ORC-WER) and the MIMO-WER. These WERs
evaluate different aspects and error types (e.g., temporal misalignment). A
detailed comparison has not been made. We therefore present a unified
description of the existing WERs and highlight when to use which metric. To
further analyze how many errors are caused by speaker confusion, we propose the
Diarization-invariant cpWER (DI-cpWER). It ignores speaker attribution errors
and its difference to cpWER reflects the impact of speaker confusions on the
WER. Since error types cannot reliably be classified automatically, we discuss
ways to visualize sequence alignments between the reference and hypothesis
transcripts to facilitate the spotting of errors by a human judge. Since some
WER definitions have high computational complexity, we introduce a greedy
algorithm to approximate the ORC-WER and DI-cpWER with high precision ($<0.1\%$
deviation in our experiments) and polynomial complexity instead of exponential.
To improve the plausibility of the metrics, we also incorporate the time
constraint from the tcpWER into ORC-WER and MIMO-WER, also significantly
reducing the computational complexity.

</details>


### [31] [Guiding an Automatic Speech Recognition Decoder Using Large Language Models](https://arxiv.org/abs/2508.02228)
*Eyal Cohen,Bhiksha Raj,Joseph Keshet*

Main category: eess.AS

TL;DR: 提出了一种将大型语言模型（LLM）与自动语音识别（ASR）系统集成的新方法，通过分解最大后验估计器实现独立训练和优化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种任务中表现出色，但其与ASR系统的集成仍具挑战性。本文旨在解决这一问题，同时保持AM和LLM的独立性。

Method: 通过分解最大后验估计器，提出了一种迭代方法，使AM和LLM能够独立训练和优化，无需联合优化。

Result: 实验表明，该方法在多种数据集和语音风格中表现优异，尤其在处理复杂句子、缩写和领域特定词汇时效果显著。

Conclusion: 该方法成功实现了LLM与ASR的高效集成，同时保持了模型的独立性和性能优化。

Abstract: Automatic Speech Recognition (ASR) consists of an acoustic model (AM) and a
language model (LM). The AM estimates the probability of an acoustic signal
based on a sequence of linguistic units, typically phones, characters, or
tokens, while the LM assesses the likelihood of a specific sequence of words or
tokens. Although Large Language Models (LLMs) have demonstrated significant
potential across various tasks, integrating them into ASR remains an open
challenge. By decomposing the maximum a posteriori (MAP) estimator of words (or
tokens) given the acoustic signal, we derive an iterative procedure that
facilitates a novel integration of the AM and LLM, while maintaining their
separability. This approach enables each component to be independently trained
and improved using its own data, thereby maximizing the system's performance by
leveraging the strengths of both models without requiring joint optimization.
We illustrate the effectiveness of our method in comparison to three language
models: N-gram, GCNN, and TransformerLM across multiple datasets spanning
various speech styles, including ALLSSTAR, WSJ0, and TED-LIUM 3. Our
experiments involved two acoustic models (wav2vec 2.0 and HuBERT) and three
LLMs (GPT-2, LLaMA 2, and Falcon). Notably, our method demonstrates particular
efficacy in addressing complex speech sentences, acronyms, and domain-specific
vocabulary.

</details>


### [32] [Reference-free Adversarial Sex Obfuscation in Speech](https://arxiv.org/abs/2508.02295)
*Yangyang Qu,Michele Panariello,Massimiliano Todisco,Nicholas Evans*

Main category: eess.AS

TL;DR: RASO是一种无参考的对抗性别混淆方法，通过条件对抗学习和显式正则化消除语音中的性别特征，同时保留语言内容。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换中的隐私风险及残留性别特征问题。

Method: 采用性别条件对抗学习框架和显式正则化，对齐基频分布和共振峰轨迹。

Result: RASO在保留语言内容的同时，显著优于其他性别混淆方法。

Conclusion: RASO有效消除语音中的性别特征，提升隐私保护能力。

Abstract: Sex conversion in speech involves privacy risks from data collection and
often leaves residual sex-specific cues in outputs, even when target speaker
references are unavailable. We introduce RASO for Reference-free Adversarial
Sex Obfuscation. Innovations include a sex-conditional adversarial learning
framework to disentangle linguistic content from sex-related acoustic markers
and explicit regularisation to align fundamental frequency distributions and
formant trajectories with sex-neutral characteristics learned from sex-balanced
training data. RASO preserves linguistic content and, even when assessed under
a semi-informed attack model, it significantly outperforms a competing approach
to sex obfuscation.

</details>


### [33] [Revisiting the Privacy of Low-Frequency Speech Signals: Exploring Resampling Methods, Evaluation Scenarios, and Speaker Characteristics](https://arxiv.org/abs/2508.02483)
*Jule Pohlhausen,Jörg Bitzer*

Main category: eess.AS

TL;DR: 研究探讨了通过限制音频采样率（低至800Hz）来保护语音隐私的有效性，分析了抗混叠滤波的影响，并评估了隐私保护对语音识别和语音活动检测的影响。


<details>
  <summary>Details</summary>
Motivation: 现实生活中的音频录音涉及隐私问题，研究旨在通过低采样率保护敏感语音内容。

Method: 通过不同采样率重采样音频信号，比较抗混叠滤波的效果，用语音识别和语音活动检测模型评估隐私和实用性。

Result: 在800Hz采样率下，语音识别模型仍能正确转录大部分内容；抗混叠滤波缺失会显著降低隐私保护效果。

Conclusion: 低采样率结合抗混叠滤波可有效平衡语音隐私保护和实用性。

Abstract: While audio recordings in real life provide insights into social dynamics and
conversational behavior, they also raise concerns about the privacy of
personal, sensitive data. This article explores the effectiveness of
restricting recordings to low-frequency audio to protect spoken content. For
resampling the audio signals to different sampling rates, we compare the effect
of employing anti-aliasing filtering. Privacy enhancement is measured by an
increased word error rate of automatic speech recognition models. The impact on
utility performance is measured with voice activity detection models. Our
experimental results show that for clean recordings, models trained with a
sampling rate of up to 800 Hz transcribe the majority of words correctly. For
both models, we analyzed the impact of the speaker's sex and pitch, and we
demonstrated that missing anti-aliasing filters more strongly compromise speech
privacy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [34] [Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR](https://arxiv.org/abs/2508.01166)
*Bingshen Mu,Hexin Liu,Hongfei Xue,Kun Wei,Lei Xie*

Main category: cs.SD

TL;DR: 论文提出了一种多模态检索与选择方法MARS，用于提升基于大语言模型的自动语音识别（LLM-ASR）在对话场景中的性能，通过动态选择最相关的历史上下文，显著提高了识别准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有对话LLM-ASR方法使用固定数量的历史上下文或全部对话历史，导致无关和冗余信息干扰，增加了ASR混淆和计算成本。

Method: 提出MARS方法，通过多模态检索和选择动态获取与当前话语最相关的声学和文本历史上下文，并结合声学和文本相似度进行排名选择最佳上下文。

Result: 在Interspeech 2025数据集上，仅使用1.5K小时数据训练的LLM-ASR结合MARS，性能优于使用179K小时数据训练的最先进系统。

Conclusion: MARS方法有效提升了对话LLM-ASR的性能，减少了无关信息干扰，同时降低了计算成本。

Abstract: Automatic Speech Recognition (ASR) aims to convert human speech content into
corresponding text. In conversational scenarios, effectively utilizing context
can enhance its accuracy. Large Language Models' (LLMs) exceptional
long-context understanding and reasoning abilities enable LLM-based ASR
(LLM-ASR) to leverage historical context for recognizing conversational speech,
which has a high degree of contextual relevance. However, existing
conversational LLM-ASR methods use a fixed number of preceding utterances or
the entire conversation history as context, resulting in significant ASR
confusion and computational costs due to massive irrelevant and redundant
information. This paper proposes a multi-modal retrieval-and-selection method
named MARS that augments conversational LLM-ASR by enabling it to retrieve and
select the most relevant acoustic and textual historical context for the
current utterance. Specifically, multi-modal retrieval obtains a set of
candidate historical contexts, each exhibiting high acoustic or textual
similarity to the current utterance. Multi-modal selection calculates the
acoustic and textual similarities for each retrieved candidate historical
context and, by employing our proposed near-ideal ranking method to consider
both similarities, selects the best historical context. Evaluations on the
Interspeech 2025 Multilingual Conversational Speech Language Model Challenge
dataset show that the LLM-ASR, when trained on only 1.5K hours of data and
equipped with the MARS, outperforms the state-of-the-art top-ranking system
trained on 179K hours of data.

</details>


### [35] [GeHirNet: A Gender-Aware Hierarchical Model for Voice Pathology Classification](https://arxiv.org/abs/2508.01172)
*Fan Wu,Kaicheng Zhao,Elgar Fleisch,Filipe Barata*

Main category: cs.SD

TL;DR: 提出一种两阶段框架，通过性别特异性病理模式识别和性别条件疾病分类，结合多尺度重采样和时间扭曲增强，显著提高语音病理分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有分类器因性别相关声学变异和罕见疾病数据稀缺，难以准确识别特定病理。

Method: 使用ResNet-50分析梅尔频谱图，先识别性别特异性病理模式，再进行性别条件分类；采用多尺度重采样和时间扭曲增强解决类别不平衡。

Result: 在合并数据集上达到97.63%准确率和95.25% MCC，比单阶段基线提升5% MCC。

Conclusion: 该框架通过分层建模减少性别偏见，推动了语音病理分类的发展。

Abstract: AI-based voice analysis shows promise for disease diagnostics, but existing
classifiers often fail to accurately identify specific pathologies because of
gender-related acoustic variations and the scarcity of data for rare diseases.
We propose a novel two-stage framework that first identifies gender-specific
pathological patterns using ResNet-50 on Mel spectrograms, then performs
gender-conditioned disease classification. We address class imbalance through
multi-scale resampling and time warping augmentation. Evaluated on a merged
dataset from four public repositories, our two-stage architecture with time
warping achieves state-of-the-art performance (97.63\% accuracy, 95.25\% MCC),
with a 5\% MCC improvement over single-stage baseline. This work advances voice
pathology classification while reducing gender bias through hierarchical
modeling of vocal characteristics.

</details>


### [36] [Advancing the Foundation Model for Music Understanding](https://arxiv.org/abs/2508.01178)
*Yi Jiang,Wei Wang,Xianwen Guo,Huiyun Liu,Hanrui Wang,Youri Xu,Haoqi Gu,Zhongqian Xie,Chuanjiang Luo*

Main category: cs.SD

TL;DR: 提出了一种名为MuFun的统一基础模型，用于全面音乐理解，通过联合处理乐器和歌词内容，并在多任务数据集上训练，显著优于现有音频大语言模型。


<details>
  <summary>Details</summary>
Motivation: 音乐信息检索领域存在碎片化问题，专门化模型仅擅长孤立任务，因此需要一种统一模型来提升整体音乐理解能力。

Method: 设计了MuFun模型，结合乐器和歌词内容处理，并在大规模多任务数据集上进行训练。同时提出了新的评估基准MuCUE。

Result: 实验表明，MuFun在MuCUE任务上显著优于现有音频大语言模型，展现了其先进性和泛化能力。

Conclusion: MuFun模型为音乐理解提供了一种统一的解决方案，并在多任务上表现出色，为未来研究提供了新方向。

Abstract: The field of Music Information Retrieval (MIR) is fragmented, with
specialized models excelling at isolated tasks. In this work, we challenge this
paradigm by introducing a unified foundation model named MuFun for holistic
music understanding. Our model features a novel architecture that jointly
processes instrumental and lyrical content, and is trained on a large-scale
dataset covering diverse tasks such as genre classification, music tagging, and
question answering. To facilitate robust evaluation, we also propose a new
benchmark for multi-faceted music understanding called MuCUE (Music
Comprehensive Understanding Evaluation). Experiments show our model
significantly outperforms existing audio large language models across the MuCUE
tasks, demonstrating its state-of-the-art effectiveness and generalization
ability.

</details>


### [37] [Foundation Models for Bioacoustics -- a Comparative Review](https://arxiv.org/abs/2508.01277)
*Raphael Schwinger,Paria Vali Zadeh,Lukas Rauch,Mats Kurz,Tom Hauschild,Sam Lapp,Sven Tomforde*

Main category: cs.SD

TL;DR: 本文综述了大规模预训练生物声学基础模型，并评估了它们在多个生物声学分类任务中的迁移能力，发现BirdMAE和BEATs$_{NLM}$表现最佳。


<details>
  <summary>Details</summary>
Motivation: 生物声学自动分析对生物多样性监测和保护至关重要，需要能适应多样化任务的深度学习模型。

Method: 综述了生物声学表示学习、预训练数据源和基准，分析了模型架构、预训练方案和训练范式，并在BEANS和BirdSet基准上评估了模型性能。

Result: BirdMAE在BirdSet上表现最佳，BEATs$_{NLM}$在BEANS上略优；Transformer模型需注意力探测以发挥全部性能。

Conclusion: 研究为实践者选择适合的生物声学分类任务模型提供了指导。

Abstract: Automated bioacoustic analysis is essential for biodiversity monitoring and
conservation, requiring advanced deep learning models that can adapt to diverse
bioacoustic tasks. This article presents a comprehensive review of large-scale
pretrained bioacoustic foundation models and systematically investigates their
transferability across multiple bioacoustic classification tasks. We overview
bioacoustic representation learning including major pretraining data sources
and benchmarks. On this basis, we review bioacoustic foundation models by
thoroughly analysing design decisions such as model architecture, pretraining
scheme, and training paradigm. Additionally, we evaluate selected foundation
models on classification tasks from the BEANS and BirdSet benchmarks, comparing
the generalisability of learned representations under both linear and attentive
probing strategies. Our comprehensive experimental analysis reveals that
BirdMAE, trained on large-scale bird song data with a self-supervised
objective, achieves the best performance on the BirdSet benchmark. On BEANS,
BEATs$_{NLM}$, the extracted encoder of the NatureLM-audio large audio model,
is slightly better. Both transformer-based models require attentive probing to
extract the full performance of their representations. ConvNext$_{BS}$ and
Perch models trained with supervision on large-scale bird song data remain
competitive for passive acoustic monitoring classification tasks of BirdSet in
linear probing settings. Training a new linear classifier has clear advantages
over evaluating these models without further training. While on BEANS, the
baseline model BEATs trained with self-supervision on AudioSet outperforms
bird-specific models when evaluated with attentive probing. These findings
provide valuable guidance for practitioners selecting appropriate models to
adapt them to new bioacoustic classification tasks via probing.

</details>


### [38] [Via Score to Performance: Efficient Human-Controllable Long Song Generation with Bar-Level Symbolic Notation](https://arxiv.org/abs/2508.01394)
*Tongxi Wang,Yang Yu,Qing Wang,Junlang Qian*

Main category: cs.SD

TL;DR: BACH模型通过符号化乐谱生成歌曲，解决了现有方法在可控性、泛化性、感知质量和时长上的不足，成为当前最先进的歌曲生成系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从原始音频学习音乐理论存在困难，导致可控性、泛化性、感知质量和时长问题。

Method: 提出BACH模型，采用符号化乐谱和分层歌曲结构的生成策略。

Result: BACH在效率、时长和感知质量上显著提升，超越现有公开系统和商业方案。

Conclusion: BACH通过符号化乐谱实现了更高效的歌曲生成，成为新的SOTA。

Abstract: Song generation is regarded as the most challenging problem in music AIGC;
nonetheless, existing approaches have yet to fully overcome four persistent
limitations: controllability, generalizability, perceptual quality, and
duration. We argue that these shortcomings stem primarily from the prevailing
paradigm of attempting to learn music theory directly from raw audio, a task
that remains prohibitively difficult for current models. To address this, we
present Bar-level AI Composing Helper (BACH), the first model explicitly
designed for song generation through human-editable symbolic scores. BACH
introduces a tokenization strategy and a symbolic generative procedure tailored
to hierarchical song structure. Consequently, it achieves substantial gains in
the efficiency, duration, and perceptual quality of song generation.
Experiments demonstrate that BACH, with a small model size, establishes a new
SOTA among all publicly reported song generation systems, even surpassing
commercial solutions such as Suno. Human evaluations further confirm its
superiority across multiple subjective metrics.

</details>


### [39] [PESTO: Real-Time Pitch Estimation with Self-supervised Transposition-equivariant Objective](https://arxiv.org/abs/2508.01488)
*Alain Riou,Bernardo Torres,Ben Hayes,Stefan Lattner,Gaëtan Hadjeres,Gaël Richard,Geoffroy Peeters*

Main category: cs.SD

TL;DR: PESTO是一种自监督学习方法，用于单音高估计，采用Siamese架构和Toeplitz全连接层，无需标注数据即可训练，性能优异且轻量。


<details>
  <summary>Details</summary>
Motivation: 解决单音高估计问题，同时避免对标注数据的依赖，并实现轻量化和实时性。

Method: 使用Siamese架构处理VQT帧，通过Toeplitz层实现平移等变性，采用类基平移等变目标训练。

Result: 在音乐和语音数据集上表现优异，超越自监督基线，与监督方法竞争，且泛化能力强。

Conclusion: PESTO轻量、低延迟，适合实时应用，并通过流式VQT实现增强实用性。

Abstract: In this paper, we introduce PESTO, a self-supervised learning approach for
single-pitch estimation using a Siamese architecture. Our model processes
individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch
distributions. The neural network is designed to be equivariant to
translations, notably thanks to a Toeplitz fully-connected layer. In addition,
we construct pitch-shifted pairs by translating and cropping the VQT frames and
train our model with a novel class-based transposition-equivariant objective,
eliminating the need for annotated data. Thanks to this architecture and
training objective, our model achieves remarkable performances while being very
lightweight ($130$k parameters). Evaluations on music and speech datasets
(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms
self-supervised baselines but also competes with supervised methods, exhibiting
superior cross-dataset generalization. Finally, we enhance PESTO's practical
utility by developing a streamable VQT implementation using cached
convolutions. Combined with our model's low latency (less than 10 ms) and
minimal parameter count, this makes PESTO particularly suitable for real-time
applications.

</details>


### [40] [Translation-Equivariant Self-Supervised Learning for Pitch Estimation with Optimal Transport](https://arxiv.org/abs/2508.01493)
*Bernardo Torres,Alain Riou,Gaël Richard,Geoffroy Peeters*

Main category: cs.SD

TL;DR: 提出一种基于最优传输目标的方法，用于学习一维平移等变系统，并应用于单音高估计。


<details>
  <summary>Details</summary>
Motivation: 为训练自监督音高估计器提供理论支持、数值稳定且更简单的替代方案。

Method: 使用最优传输目标学习一维平移等变系统。

Result: 方法在单音高估计中表现优异。

Conclusion: 该方法为自监督音高估计提供了一种更优的解决方案。

Abstract: In this paper, we propose an Optimal Transport objective for learning
one-dimensional translation-equivariant systems and demonstrate its
applicability to single pitch estimation. Our method provides a theoretically
grounded, more numerically stable, and simpler alternative for training
state-of-the-art self-supervised pitch estimators.

</details>


### [41] [ShrutiSense: Microtonal Modeling and Correction in Indian Classical Music](https://arxiv.org/abs/2508.01498)
*Rajarshi Ghosh,Jayanth Athipatla*

Main category: cs.SD

TL;DR: ShrutiSense是一个专为印度古典音乐设计的符号化音高处理系统，用于纠正西方化或损坏的音高序列，并补全缺失值。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐处理工具未能考虑印度古典音乐中的微音程（22 shrutis）和文化特定的raga语法。

Method: 结合Shruti感知的有限状态转换器（FST）和语法约束的Shruti隐马尔可夫模型（GC-SHMM）。

Result: 在模拟数据上，FST模型shruti分类准确率达91.3%，且在音高噪声下表现稳健。

Conclusion: ShrutiSense能有效保持印度古典音乐的文化真实性。

Abstract: Indian classical music relies on a sophisticated microtonal system of 22
shrutis (pitch intervals), which provides expressive nuance beyond the 12-tone
equal temperament system. Existing symbolic music processing tools fail to
account for these microtonal distinctions and culturally specific raga grammars
that govern melodic movement. We present ShrutiSense, a comprehensive symbolic
pitch processing system designed for Indian classical music, addressing two
critical tasks: (1) correcting westernized or corrupted pitch sequences, and
(2) completing melodic sequences with missing values. Our approach employs
complementary models for different tasks: a Shruti-aware finite-state
transducer (FST) that performs contextual corrections within the 22-shruti
framework and a grammar-constrained Shruti hidden Markov model (GC-SHMM) that
incorporates raga-specific transition rules for contextual completions.
Comprehensive evaluation on simulated data across five ragas demonstrates that
ShrutiSense (FST model) achieves 91.3% shruti classification accuracy for
correction tasks, with example sequences showing 86.7-90.0% accuracy at
corruption levels of 0.2 to 0.4. The system exhibits robust performance under
pitch noise up to +/-50 cents, maintaining consistent accuracy across ragas
(90.7-91.8%), thus preserving the cultural authenticity of Indian classical
music expression.

</details>


### [42] [Enhancing Spectrogram Realism in Singing Voice Synthesis via Explicit Bandwidth Extension Prior to Vocoder](https://arxiv.org/abs/2508.01796)
*Runxuan Yang,Kai Li,Guo Chen,Xiaolin Hu*

Main category: cs.SD

TL;DR: 论文提出了一种结合去噪扩散过程和DiT架构的方法，改进声码器生成的高频频谱真实性，使其难以与真实录音区分。


<details>
  <summary>Details</summary>
Motivation: 解决声码器生成的歌声音频在高频频谱上与真实录音的明显差异问题。

Method: 结合去噪扩散过程的线性频谱估计和基于Vocos的改进声码器，处理高频频谱。

Result: 生成的音频频谱高保真，难以被人类或机器识别为合成。

Conclusion: 该方法显著提升了声码器技术的真实性，特别是在对抗假频谱检测方面。

Abstract: This paper addresses the challenge of enhancing the realism of
vocoder-generated singing voice audio by mitigating the distinguishable
disparities between synthetic and real-life recordings, particularly in
high-frequency spectrogram components. Our proposed approach combines two
innovations: an explicit linear spectrogram estimation step using denoising
diffusion process with DiT-based neural network architecture optimized for
time-frequency data, and a redesigned vocoder based on Vocos specialized in
handling large linear spectrograms with increased frequency bins. This
integrated method can produce audio with high-fidelity spectrograms that are
challenging for both human listeners and machine classifiers to differentiate
from authentic recordings. Objective and subjective evaluations demonstrate
that our streamlined approach maintains high audio quality while achieving this
realism. This work presents a substantial advancement in overcoming the
limitations of current vocoding techniques, particularly in the context of
adversarial attacks on fake spectrogram detection.

</details>


### [43] [Automatic Melody Reduction via Shortest Path Finding](https://arxiv.org/abs/2508.01571)
*Ziyu Wang,Yuxuan Wu,Roger B. Dannenberg,Gus Xia*

Main category: cs.SD

TL;DR: 提出了一种基于图表示的新颖旋律简化方法，通过最短路径实现，适用于多种音乐风格，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有旋律简化方法多为手动且局限于古典音乐，缺乏全自动且通用的解决方案。

Method: 采用基于图的表示方法，将旋律简化问题转化为最短路径问题。

Result: 在流行、民谣和古典音乐中，该方法生成的简化旋律更忠实且音乐性更强，且在下游任务中优于现有风格迁移方法。

Conclusion: 该方法为旋律简化提供了一种有效的全自动解决方案，适用于多种音乐风格，并展示了在下游任务中的潜力。

Abstract: Melody reduction, as an abstract representation of musical compositions,
serves not only as a tool for music analysis but also as an intermediate
representation for structured music generation. Prior computational theories,
such as the Generative Theory of Tonal Music, provide insightful
interpretations of music, but they are not fully automatic and usually limited
to the classical genre. In this paper, we propose a novel and conceptually
simple computational method for melody reduction using a graph-based
representation inspired by principles from computational music theories, where
the reduction process is formulated as finding the shortest path. We evaluate
our algorithm on pop, folk, and classical genres, and experimental results show
that the algorithm produces melody reductions that are more faithful to the
original melody and more musically coherent than other common melody
downsampling methods. As a downstream task, we use melody reductions to
generate symbolic music variations. Experiments show that our method achieves
higher quality than state-of-the-art style transfer methods.

</details>


### [44] [Localizing Audio-Visual Deepfakes via Hierarchical Boundary Modeling](https://arxiv.org/abs/2508.02000)
*Xuanjun Chen,Shih-Peng Cheng,Jiawei Du,Lin Zhang,Xiaoxiao Miao,Chung-Che Wang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 论文提出了一种名为HBMNet的分层边界建模网络，用于解决内容驱动的部分篡改下的音视频时序深度伪造定位问题。


<details>
  <summary>Details</summary>
Motivation: 在内容驱动的部分篡改场景中，深度伪造区域通常仅跨越少数帧，其余大部分内容保持不变，这使得定位任务极具挑战性。

Method: HBMNet包含三个模块：音视频特征编码器提取帧级特征，粗粒度提案生成器预测候选边界区域，细粒度概率生成器通过双向边界-内容概率优化提案。

Result: 实验表明，音视频编码与融合主要提升精度，帧级监督提高召回率。HBMNet在性能上优于BA-TFD和UMMAFormer，并显示出更强的可扩展性。

Conclusion: HBMNet通过多模态和多尺度学习，显著提升了音视频时序深度伪造的定位性能。

Abstract: Audio-visual temporal deepfake localization under the content-driven partial
manipulation remains a highly challenging task. In this scenario, the deepfake
regions are usually only spanning a few frames, with the majority of the rest
remaining identical to the original. To tackle this, we propose a Hierarchical
Boundary Modeling Network (HBMNet), which includes three modules: an
Audio-Visual Feature Encoder that extracts discriminative frame-level
representations, a Coarse Proposal Generator that predicts candidate boundary
regions, and a Fine-grained Probabilities Generator that refines these
proposals using bidirectional boundary-content probabilities. From the modality
perspective, we enhance audio-visual learning through dedicated encoding and
fusion, reinforced by frame-level supervision to boost discriminability. From
the temporal perspective, HBMNet integrates multi-scale cues and bidirectional
boundary-content relationships. Experiments show that encoding and fusion
primarily improve precision, while frame-level supervision boosts recall. Each
module (audio-visual fusion, temporal scales, bi-directionality) contributes
complementary benefits, collectively enhancing localization performance. HBMNet
outperforms BA-TFD and UMMAFormer and shows improved potential scalability with
more training data.

</details>


### [45] [From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs](https://arxiv.org/abs/2508.01659)
*Yuhang Jia,Xu Zhang,Yong Qin*

Main category: cs.SD

TL;DR: 论文提出Audio Commonality Captioning (ACC)方法，通过捕捉音频间的共享语义而非差异，解决了Audio Difference Captioning (ADC)在微调时导致的语义鸿沟和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: ADC虽然能促进细粒度音频区分，但其差异导向的输出与预训练目标不匹配，导致微调时的灾难性遗忘。

Method: 提出ACC方法，鼓励模型捕捉音频间的共享语义而非差异。

Result: 实验表明，ACC在音频文本理解任务上表现优于ADC，且能更好地保留通用能力。

Conclusion: ACC在MLLMs中实现了更稳健的跨模态理解，平衡了泛化能力和任务特定性能。

Abstract: Audio Captioning (AC) plays a pivotal role in enhancing audio-text
cross-modal understanding during the pretraining and finetuning of multimodal
large language models (MLLMs). To further strengthen this alignment, recent
works have proposed Audio Difference Captioning (ADC), which takes multiple
audio inputs and encourages the model to describe their differences, thereby
promoting fine-grained audio discrimination. However, despite its effectiveness
in enabling difference-telling and detailed discrimination, ADC introduces a
notable semantic gap between the input audios-often rich in diverse sound
events-and the relatively brief, difference-focused output captions. This
deviation from AC-style descriptions leads to a mismatch with the pretraining
objective, resulting in catastrophic forgetting during finetuning. To mitigate
this issue, we propose Audio Commonality Captioning (ACC), a comparably
challenging but gentler alternative that encourages the model to capture the
shared semantics across audio clips rather than emphasizing their detailed
differences. Experimental results demonstrate that ACC not only effectively
enhances audio-text understanding on primary captioning benchmarks but also
better preserves general capabilities across diverse speech and music-related
downstream tasks, such as vocal sound classification (VSC), speech emotion
recognition (SER), musical instrument classification (MIC), and music genre
classification (MGC), compared to ADC. These findings validate that ACC
contributes to more robust cross-modal understanding and achieves a better
balance between generalization and task-specific performance in the context of
MLLMs.

</details>


### [46] [Unsupervised Multi-channel Speech Dereverberation via Diffusion](https://arxiv.org/abs/2508.02071)
*Yulun Wu,Zhongweiyang Xu,Jianchong Chen,Zhong-Qiu Wang,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: 论文提出了一种名为USD-DPS的无监督语音去混响方法，利用扩散后验采样和多通道一致性约束，结合RIR估计技术，实现了高效的去混响效果。


<details>
  <summary>Details</summary>
Motivation: 解决多通道单说话者盲去混响问题，恢复干净的语音信号。

Method: 使用无条件干净语音扩散模型作为先验，通过后验采样解决问题；结合多通道混合一致性约束和RIR估计技术。

Result: 在无监督去混响方法中表现出优越性能。

Conclusion: USD-DPS在多通道语音去混响任务中具有高效性和优越性。

Abstract: We consider the problem of multi-channel single-speaker blind
dereverberation, where multi-channel mixtures are used to recover the clean
anechoic speech. To solve this problem, we propose USD-DPS, {U}nsupervised
{S}peech {D}ereverberation via {D}iffusion {P}osterior {S}ampling. USD-DPS uses
an unconditional clean speech diffusion model as a strong prior to solve the
problem by posterior sampling. At each diffusion sampling step, we estimate all
microphone channels' room impulse responses (RIRs), which are further used to
enforce a multi-channel mixture consistency constraint for diffusion guidance.
For multi-channel RIR estimation, we estimate reference-channel RIR by
optimizing RIR parameters of a sub-band RIR signal model, with the Adam
optimizer. We estimate non-reference channels' RIRs analytically using forward
convolutive prediction (FCP). We found that this combination provides a good
balance between sampling efficiency and RIR prior modeling, which shows
superior performance among unsupervised dereverberation approaches. An audio
demo page is provided in https://usddps.github.io/USDDPS_demo/.

</details>


### [47] [Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe](https://arxiv.org/abs/2508.01691)
*Tiantian Feng,Kevin Huang,Anfeng Xu,Xuan Shi,Thanathai Lertpetchpun,Jihwan Lee,Yoonjeong Lee,Dani Byrd,Shrikanth Narayanan*

Main category: cs.SD

TL;DR: Voxlect是一个用于全球方言和地区语言建模的新基准，基于语音基础模型，评估了多种语言的方言分类性能，并展示了其在下游任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究方言和地区语言的多样性，为语音基础模型提供全面的评估基准。

Method: 利用30个公开语音语料库中的200万条训练数据，评估多种语音基础模型的方言分类性能，并分析噪声条件下的鲁棒性。

Result: Voxlect能够有效分类方言，并支持语音识别和生成系统的性能评估。

Conclusion: Voxlect为方言建模提供了实用工具，并展示了其在语音技术中的广泛应用潜力。

Abstract: We present Voxlect, a novel benchmark for modeling dialects and regional
languages worldwide using speech foundation models. Specifically, we report
comprehensive benchmark evaluations on dialects and regional language varieties
in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,
Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over
2 million training utterances from 30 publicly available speech corpora that
are provided with dialectal information. We evaluate the performance of several
widely used speech foundation models in classifying speech dialects. We assess
the robustness of the dialectal models under noisy conditions and present an
error analysis that highlights modeling results aligned with geographic
continuity. In addition to benchmarking dialect classification, we demonstrate
several downstream applications enabled by Voxlect. Specifically, we show that
Voxlect can be applied to augment existing speech recognition datasets with
dialect information, enabling a more detailed analysis of ASR performance
across dialectal variations. Voxlect is also used as a tool to evaluate the
performance of speech generation systems. Voxlect is publicly available with
the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.

</details>


### [48] [WhiSQA: Non-Intrusive Speech Quality Prediction Using Whisper Encoder Features](https://arxiv.org/abs/2508.02210)
*George Close,Kris Hong,Thomas Hain,Stefan Goetze*

Main category: cs.SD

TL;DR: 提出了一种基于ASR模型特征表示的新型SQ预测器，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发非侵入式SQ预测器，并探索其在语音任务损失函数中的直接推断。

Method: 利用ASR模型提取的特征表示作为输入，构建SQ预测器。

Result: 在所有NISQA测试集上，与人类MOS评分的相关性高于现有方法，且域适应能力显著优于DNSMOS指标。

Conclusion: 基于ASR特征的SQ预测器具有更高的性能和适应性。

Abstract: There has been significant research effort developing neural-network-based
predictors of SQ in recent years. While a primary objective has been to develop
non-intrusive, i.e.~reference-free, metrics to assess the performance of SE
systems, recent work has also investigated the direct inference of neural SQ
predictors within the loss function of downstream speech tasks. To aid in the
training of SQ predictors, several large datasets of audio with corresponding
human labels of quality have been created. Recent work in this area has shown
that speech representations derived from large unsupervised or semi-supervised
foundational speech models are useful input feature representations for neural
SQ prediction. In this work, a novel and robust SQ predictor is proposed based
on feature representations extracted from an ASR model, found to be a powerful
input feature for the SQ prediction task. The proposed system achieves higher
correlation with human MOS ratings than recent approaches on all NISQA test
sets and shows significantly better domain adaption compared to the commonly
used DNSMOS metric.

</details>


### [49] [StutterCut: Uncertainty-Guided Normalised Cut for Dysfluency Segmentation](https://arxiv.org/abs/2508.02255)
*Suhita Ghosh,Melanie Jouaiti,Jan-Ole Perschewski,Sebastian Stober*

Main category: cs.SD

TL;DR: StutterCut是一个半监督框架，将不流畅性分割问题转化为图分割问题，利用伪预言分类器和不确定性度量优化节点连接，并在FluencyBank数据集上扩展帧级标注，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅在话语级别分类不流畅性，无法满足实时反馈和精准分割的需求。

Method: 将语音嵌入表示为图节点，利用伪预言分类器和蒙特卡洛dropout的不确定性度量优化节点连接。

Result: 在真实和合成数据集上，StutterCut的F1分数更高，不流畅性起始检测更精准。

Conclusion: StutterCut通过半监督图分割方法，显著提升了不流畅性分割的准确性和实用性。

Abstract: Detecting and segmenting dysfluencies is crucial for effective speech therapy
and real-time feedback. However, most methods only classify dysfluencies at the
utterance level. We introduce StutterCut, a semi-supervised framework that
formulates dysfluency segmentation as a graph partitioning problem, where
speech embeddings from overlapping windows are represented as graph nodes. We
refine the connections between nodes using a pseudo-oracle classifier trained
on weak (utterance-level) labels, with its influence controlled by an
uncertainty measure from Monte Carlo dropout. Additionally, we extend the
weakly labelled FluencyBank dataset by incorporating frame-level dysfluency
boundaries for four dysfluency types. This provides a more realistic benchmark
compared to synthetic datasets. Experiments on real and synthetic datasets show
that StutterCut outperforms existing methods, achieving higher F1 scores and
more precise stuttering onset detection.

</details>


### [50] [Generalizable Audio Deepfake Detection via Hierarchical Structure Learning and Feature Whitening in Poincaré sphere](https://arxiv.org/abs/2508.01897)
*Mingru Yang,Yanmei Gu,Qianhua He,Yanxiong Li,Peirong Zhang,Yongqiang Chen,Zhiming Wang,Huijia Zhu,Jian Liu,Weiqiang Wang*

Main category: cs.SD

TL;DR: 论文提出Poin-HierNet框架，利用Poincaré球构建域不变层次表示，解决音频深度伪造检测中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖欧几里得距离，无法有效捕捉攻击类别和域因素的内在层次结构。

Method: Poin-HierNet包含三个关键组件：Poincaré原型学习（PPL）、层次结构学习（HSL）和Poincaré特征白化（PFW）。

Result: 在四个数据集上实验表明，Poin-HierNet在等错误率上优于现有方法。

Conclusion: Poin-HierNet通过层次表示和域不变特征，显著提升了音频深度伪造检测的泛化能力。

Abstract: Audio deepfake detection (ADD) faces critical generalization challenges due
to diverse real-world spoofing attacks and domain variations. However, existing
methods primarily rely on Euclidean distances, failing to adequately capture
the intrinsic hierarchical structures associated with attack categories and
domain factors. To address these issues, we design a novel framework
Poin-HierNet to construct domain-invariant hierarchical representations in the
Poincar\'e sphere. Poin-HierNet includes three key components: 1) Poincar\'e
Prototype Learning (PPL) with several data prototypes aligning sample features
and capturing multilevel hierarchies beyond human labels; 2) Hierarchical
Structure Learning (HSL) leverages top prototypes to establish a tree-like
hierarchical structure from data prototypes; and 3) Poincar\'e Feature
Whitening (PFW) enhances domain invariance by applying feature whitening to
suppress domain-sensitive features. We evaluate our approach on four datasets:
ASVspoof 2019 LA, ASVspoof 2021 LA, ASVspoof 2021 DF, and In-The-Wild.
Experimental results demonstrate that Poin-HierNet exceeds state-of-the-art
methods in Equal Error Rate.

</details>


### [51] [Non-Verbal Vocalisations and their Challenges: Emotion, Privacy, Sparseness, and Real Life](https://arxiv.org/abs/2508.01960)
*Anton Batliner,Shahin Amiriparian,Björn W. Schuller*

Main category: cs.SD

TL;DR: 论文探讨了非语言发声（NVVs）的历史、类型、功能及研究挑战，并提倡基于语料库的方法以更真实地建模。


<details>
  <summary>Details</summary>
Motivation: 研究NVVs的历史背景、类型和功能，并解决当前研究中隐私和数据稀疏的问题。

Method: 通过历史回顾、类型和功能分析，提出基于语料库的研究方法。

Result: NVVs研究面临隐私和稀疏数据的挑战，语料库方法能更真实地建模。

Conclusion: 提倡使用语料库方法解决NVVs研究中的隐私和数据问题，以实现更真实的建模。

Abstract: Non-Verbal Vocalisations (NVVs) are short `non-word' utterances without
proper linguistic (semantic) meaning but conveying connotations -- be this
emotions/affects or other paralinguistic information. We start this
contribution with a historic sketch: how they were addressed in psychology and
linguistics in the last two centuries, how they were neglected later on, and
how they came to the fore with the advent of emotion research. We then give an
overview of types of NVVs (formal aspects) and functions of NVVs, exemplified
with the typical NVV \textit{ah}. Interesting as they are, NVVs come, however,
with a bunch of challenges that should be accounted for: Privacy and general
ethical considerations prevent them of being recorded in real-life (private)
scenarios to a sufficient extent. Isolated, prompted (acted) exemplars do not
necessarily model NVVs in context; yet, this is the preferred strategy so far
when modelling NVVs, especially in AI. To overcome these problems, we argue in
favour of corpus-based approaches. This guarantees a more realistic modelling;
however, we are still faced with privacy and sparse data problems.

</details>


### [52] [Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers](https://arxiv.org/abs/2508.02175)
*Liang Lin,Miao Yu,Kaiwen Luo,Yibo Zhang,Lilan Peng,Dexian Wang,Xuehai Tang,Yuanhe Zhang,Xikang Yang,Zhenhong Zhou,Kun Wang,Yang Liu*

Main category: cs.SD

TL;DR: 论文研究了音频大语言模型（ALLM）对基于声学特征的隐蔽后门攻击的脆弱性，提出了HIN攻击框架，并通过AudioSafe基准测试揭示了ALLM的显著安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着音频大语言模型（ALLM）在语音处理中的广泛应用，其安全性问题亟待解决。音频的独特性使得现有文本和视觉安全研究无法直接适用，因此需要专门研究ALLM对声学特征攻击的脆弱性。

Method: 提出了Hidden in the Noise（HIN）后门攻击框架，通过修改音频波形（如时间动态调整和频谱噪声注入）嵌入隐蔽触发模式。使用AudioSafe基准测试评估ALLM对九种风险类型的鲁棒性。

Result: 实验发现：(I) 环境噪声和语速变化等声学特征攻击成功率超过90%；(II) ALLM对不同声学特征的敏感性差异显著，对音量触发反应最小；(III) 中毒样本仅引起损失曲线微小波动，攻击隐蔽性强。

Conclusion: ALLM对基于声学特征的后门攻击存在严重漏洞，HIN框架和AudioSafe基准为未来音频模型安全研究提供了重要工具和方向。

Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech
processing, their safety implications demand urgent attention. While
considerable research has explored textual and vision safety, audio's distinct
characteristics present significant challenges. This paper first investigates:
Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In
response to this issue, we introduce Hidden in the Noise (HIN), a novel
backdoor attack framework designed to exploit subtle, audio-specific features.
HIN applies acoustic modifications to raw audio waveforms, such as alterations
to temporal dynamics and strategic injection of spectrally tailored noise.
These changes introduce consistent patterns that an ALLM's acoustic feature
encoder captures, embedding robust triggers within the audio stream. To
evaluate ALLM robustness against audio-feature-based triggers, we develop the
AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments
on AudioSafe and three established safety datasets reveal critical
vulnerabilities in existing ALLMs: (I) audio features like environment noise
and speech rate variations achieve over 90% average attack success rate. (II)
ALLMs exhibit significant sensitivity differences across acoustic features,
particularly showing minimal response to volume as a trigger, and (III)
poisoned sample inclusion causes only marginal loss curve fluctuations,
highlighting the attack's stealth.

</details>


### [53] [Detecting COPD Through Speech Analysis: A Dataset of Danish Speech and Machine Learning Approach](https://arxiv.org/abs/2508.02354)
*Cuno Sankey-Olsen,Rasmus Hvass Olesen,Tobias Oliver Eberhard,Andreas Triantafyllopoulos,Björn Schuller,Ilhan Aslan*

Main category: cs.SD

TL;DR: 研究探讨了语音作为COPD非侵入性筛查工具的潜力，在丹麦参与者中测试了不同语音任务和模型，最高准确率为67%。


<details>
  <summary>Details</summary>
Motivation: COPD是一种严重影响生活质量的疾病，早期非侵入性检测可能改善患者预后，语音被证明是一种有价值的生物标志物。

Method: 收集96名丹麦参与者的语音数据（阅读、咳嗽、持续元音），使用openSMILE特征和x-vector嵌入模型进行分析。

Result: 使用openSMILE特征和逻辑回归模型获得67%的最高准确率。

Conclusion: 语音分析有望成为未来COPD医疗解决方案中非侵入性、远程和可扩展的筛查工具。

Abstract: Chronic Obstructive Pulmonary Disease (COPD) is a serious and debilitating
disease affecting millions around the world. Its early detection using
non-invasive means could enable preventive interventions that improve quality
of life and patient outcomes, with speech recently shown to be a valuable
biomarker. Yet, its validity across different linguistic groups remains to be
seen. To that end, audio data were collected from 96 Danish participants
conducting three speech tasks (reading, coughing, sustained vowels). Half of
the participants were diagnosed with different levels of COPD and the other
half formed a healthy control group. Subsequently, we investigated different
baseline models using openSMILE features and learnt x-vector embeddings. We
obtained a best accuracy of 67% using openSMILE features and logistic
regression. Our findings support the potential of speech-based analysis as a
non-invasive, remote, and scalable screening tool as part of future COPD
healthcare solutions.

</details>


### [54] [Inference-time Scaling for Diffusion-based Audio Super-resolution](https://arxiv.org/abs/2508.02391)
*Yizhu Jin,Zhen Ye,Zeyue Tian,Haohe Liu,Qiuqiang Kong,Yike Guo,Wei Xue*

Main category: cs.SD

TL;DR: 提出了一种通过推理时缩放和验证器-算法组合改进音频超分辨率的方法，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在音频超分辨率中因随机采样过程导致输出质量受限，需改进方法以提升性能。

Method: 采用推理时缩放和多轨迹探索，结合任务特定验证器和搜索算法（随机搜索和零阶搜索）优化采样过程。

Result: 在多种音频领域验证中，性能显著提升，如语音超分辨率的美学评分提升9.70%，频谱距离改善46.98%。

Conclusion: 提出的方法通过主动引导高维解空间探索，实现了更稳健和高质量的音频超分辨率输出。

Abstract: Diffusion models have demonstrated remarkable success in generative tasks,
including audio super-resolution (SR). In many applications like movie
post-production and album mastering, substantial computational budgets are
available for achieving superior audio quality. However, while existing
diffusion approaches typically increase sampling steps to improve quality, the
performance remains fundamentally limited by the stochastic nature of the
sampling process, leading to high-variance and quality-limited outputs. Here,
rather than simply increasing the number of sampling steps, we propose a
different paradigm through inference-time scaling for SR, which explores
multiple solution trajectories during the sampling process. Different
task-specific verifiers are developed, and two search algorithms, including the
random search and zero-order search for SR, are introduced. By actively guiding
the exploration of the high-dimensional solution space through
verifier-algorithm combinations, we enable more robust and higher-quality
outputs. Through extensive validation across diverse audio domains (speech,
music, sound effects) and frequency ranges, we demonstrate consistent
performance gains, achieving improvements of up to 9.70% in aesthetics, 5.88%
in speaker similarity, 15.20% in word error rate, and 46.98% in spectral
distance for speech SR from 4kHz to 24kHz, showcasing the effectiveness of our
approach. Audio samples are available at:
https://racerk.github.io/tt-scale-audiosr/.

</details>


### [55] [Charting 15 years of progress in deep learning for speech emotion recognition: A replication study](https://arxiv.org/abs/2508.02448)
*Andreas Triantafyllopoulos,Anton Batliner,Björn W. Schuller*

Main category: cs.SD

TL;DR: 论文探讨了语音情感识别（SER）领域深度学习模型的进展，发现现代深度神经网络相比早期版本提升有限，且进步感知依赖于模型选择。


<details>
  <summary>Details</summary>
Motivation: 量化SER领域15年来的进展，识别未来研究方向。

Method: 大规模调查模型架构，包括基于音频和文本的模型，分析其性能提升趋势。

Result: 发现性能提升逐渐减少，Transformer架构后趋于平稳。

Conclusion: SER研究面临瓶颈，未来需探索新方向。

Abstract: Speech emotion recognition (SER) has long benefited from the adoption of deep
learning methodologies. Deeper models -- with more layers and more trainable
parameters -- are generally perceived as being `better' by the SER community.
This raises the question -- \emph{how much better} are modern-era deep neural
networks compared to their earlier iterations? Beyond that, the more important
question of how to move forward remains as poignant as ever. SER is far from a
solved problem; therefore, identifying the most prominent avenues of future
research is of paramount importance. In the present contribution, we attempt a
quantification of progress in the 15 years of research beginning with the
introduction of the landmark 2009 INTERSPEECH Emotion Challenge. We conduct a
large scale investigation of model architectures, spanning both audio-based
models that rely on speech inputs and text-baed models that rely solely on
transcriptions. Our results point towards diminishing returns and a plateau
after the recent introduction of transformer architectures. Moreover, we
demonstrate how perceptions of progress are conditioned on the particular
selection of models that are compared. Our findings have important
repercussions about the state-of-the-art in SER research and the paths forward

</details>


### [56] [Towards Reliable Audio Deepfake Attribution and Model Recognition: A Multi-Level Autoencoder-Based Framework](https://arxiv.org/abs/2508.02521)
*Andrea Di Pierno,Luca Guarnera,Dario Allegra,Sebastiano Battiato*

Main category: cs.SD

TL;DR: 论文提出LAVA框架，用于音频深度伪造的检测和模型识别，通过分层结构和注意力增强的潜在表示，在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造的泛滥威胁数字通信的信任，现有方法在模型溯源方面研究不足。

Method: LAVA框架使用卷积自编码器提取注意力增强的潜在表示，结合两个分类器（ADA和ADMR）进行技术识别和模型实例识别，并引入置信度阈值提升鲁棒性。

Result: 在ASVspoof2021等数据集上，ADA分类器F1分数超过95%，ADMR模块在六类中达到96.31%宏F1。

Conclusion: LAVA为开放集条件下的深度伪造溯源和模型识别提供了监督方法，并在公开基准上验证了其鲁棒性和可靠性。

Abstract: The proliferation of audio deepfakes poses a growing threat to trust in
digital communications. While detection methods have advanced, attributing
audio deepfakes to their source models remains an underexplored yet crucial
challenge. In this paper we introduce LAVA (Layered Architecture for Voice
Attribution), a hierarchical framework for audio deepfake detection and model
recognition that leverages attention-enhanced latent representations extracted
by a convolutional autoencoder trained solely on fake audio. Two specialized
classifiers operate on these features: Audio Deepfake Attribution (ADA), which
identifies the generation technology, and Audio Deepfake Model Recognition
(ADMR), which recognize the specific generative model instance. To improve
robustness under open-set conditions, we incorporate confidence-based rejection
thresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong
performance: the ADA classifier achieves F1-scores over 95% across all
datasets, and the ADMR module reaches 96.31% macro F1 across six classes.
Additional tests on unseen attacks from ASVpoof2019 LA and error propagation
analysis confirm LAVA's robustness and reliability. The framework advances the
field by introducing a supervised approach to deepfake attribution and model
recognition under open-set conditions, validated on public benchmarks and
accompanied by publicly released models and code. Models and code are available
at https://www.github.com/adipiz99/lava-framework.

</details>

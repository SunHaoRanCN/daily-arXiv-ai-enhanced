{"id": "2508.02687", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02687", "abs": "https://arxiv.org/abs/2508.02687", "authors": ["Yijia Hao", "Maarten Strackx", "Miguel Gandara", "Sandy Cochran", "Bo Liu"], "title": "An AI-driven EDA Algorithm-Empowered VCO and LDO Co-Design Method", "comment": null, "summary": "Traditionally, the output noise and power supply rejection of low-dropout\nregulators (LDOs) are optimized to minimize power supply fluctuations, reducing\ntheir impact on the low-frequency noise of target voltage-controlled\noscillators (VCOs). However, this sequential design approach does not fully\naddress the trade-offs between high-frequency and LDO-induced low-frequency\nphase noise. To overcome this limitation, this paper presents a co-design\nmethod for low phase-noise LC-tank VCOs powered by LDOs. It is difficult to\ncarry out the co-design using traditional manual design techniques. Hence, an\nefficient AI-driven EDA algorithm is used. To validate the proposed method, a\n5.6 GHz LC-tank VCO with an integrated LDO is designed using a 65 nm CMOS\nprocess. Simulations show that the co-design method improves phase noise by 1.2\ndB at a 1 MHz offset and reduces dynamic power consumption by 28.8%, with FoM\nincreased by 2.4 dBc/Hz compared to the conventional sequential design method."}
{"id": "2508.02689", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02689", "abs": "https://arxiv.org/abs/2508.02689", "authors": ["Jiawei Wang", "Yu Guan", "Chen Chen", "Ligang Zhou", "Laurence T. Yang", "Sai Gu"], "title": "On Improving PPG-Based Sleep Staging: A Pilot Study", "comment": null, "summary": "Sleep monitoring through accessible wearable technology is crucial to\nimproving well-being in ubiquitous computing. Although\nphotoplethysmography(PPG) sensors are widely adopted in consumer devices,\nachieving consistently reliable sleep staging using PPG alone remains a\nnon-trivial challenge. In this work, we explore multiple strategies to enhance\nthe performance of PPG-based sleep staging. Specifically, we compare\nconventional single-stream model with dual-stream cross-attention strategies,\nbased on which complementary information can be learned via PPG and PPG-derived\nmodalities such as augmented PPG or synthetic ECG. To study the effectiveness\nof the aforementioned approaches in four-stage sleep monitoring task, we\nconducted experiments on the world's largest sleep staging dataset, i.e., the\nMulti-Ethnic Study of Atherosclerosis(MESA). We found that substantial\nperformance gain can be achieved by combining PPG and its auxiliary information\nunder the dual-stream cross-attention architecture. Source code of this project\ncan be found at https://github.com/DavyWJW/sleep-staging-models"}
{"id": "2508.02693", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02693", "abs": "https://arxiv.org/abs/2508.02693", "authors": ["Xinwei Yue", "Xinning Guo", "Xidong Mu", "Jingjing Zhao", "Peng Yang", "Junsheng Mu", "Zhiping Lu"], "title": "Federated Learning in Active STARS-Aided Uplink Networks", "comment": null, "summary": "Active simultaneously transmitting and reflecting surfaces (ASTARS) have\nattracted growing research interest due to its ability to alleviate\nmultiplicative fading and reshape the electromagnetic environment across the\nentire space. In this paper, we utilise ASTARS to assist the federated learning\n(FL) uplink model transfer and further reduce the number of uploaded parameter\ncounts through over-the-air (OTA) computing techniques. The impact of model\naggregation errors on ASTARS-aided FL uplink networks is characterized. We\nderive an upper bound on the aggregation error of the OTA-FL model and quantify\nthe training loss due to communication errors. Then, we define the performance\nof OTA-FL as a joint optimization problem that encompasses both the assignment\nof received beams and the phase shifting of ASTARS, aiming to achieve the\nmaximum learning efficiency and high-quality signal transmission. Numerical\nresults demonstrate that: i) The FL accuracy in ASTARS uplink networks are\nenhanced compared to that in state-of-the-art networks; ii) The ASTARS enabled\nFL system achieves the better learning accuracy using fewer active units than\nother baseline, especially when the dataset is more discrete; and iii) FL\naccuracy improves with higher amplification power, but excessive amplification\nmakes thermal noise the dominant source of error."}
{"id": "2508.02698", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02698", "abs": "https://arxiv.org/abs/2508.02698", "authors": ["Sameera Bharadwaja H.", "D. K. Mehra"], "title": "A Completely Blind Channel Estimation Technique for OFDM Using Constellation Splitting", "comment": null, "summary": "The problem of second-order statistics (SOS)-based blind channel estimation\nin OFDM systems is addressed in this paper. Almost all SOS-based methods\nproposed so far suffer from a complex-scalar estimation ambiguity, which is\nresolved by using pilots or reference symbols. We propose an algorithm to\nresolve this ambiguity in blind manner using frequency-domain linear\nnon-redundant precoding and constellation-splitting among the alternate\nsubcarriers. The performance of the proposed scheme is evaluated via numerical\nsimulations in MATLAB environment. Simulation results show that the proposed\napproach performs as good as its semi-blind counterpart for M-ary PAM systems."}
{"id": "2508.02801", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02801", "abs": "https://arxiv.org/abs/2508.02801", "authors": ["Hyung Gun Chi", "Florian Pesce", "Wonil Chang", "Oggi Rudovic", "Arturo Argueta", "Stefan Braun", "Vineet Garg", "Ahmed Hussen Abdelaziz"], "title": "Adaptive Knowledge Distillation for Device-Directed Speech Detection", "comment": "5 pages, 2 figures, Interspeech accepted", "summary": "Device-directed speech detection (DDSD) is a binary classification task that\nseparates the user's queries to a voice assistant (VA) from background speech\nor side conversations. This is important for achieving naturalistic user\nexperience. To this end, we propose knowledge distillation (KD) to enhance DDSD\naccuracy while ensuring efficient deployment. Specifically, we introduce a\nnovel adaptive KD method that transfers knowledge from general representations\nof an ASR large pre-trained acoustic encoder (teacher). We apply task-specific\nadapters, on top of the (frozen) teacher encoder, trained jointly with the\nstudent model on DDSD. We demonstrate that the proposed adaptive KD outperforms\nthe student model without distillation in the keyword and keyword-free\n(follow-up) invocations, with an improvement of +26% and +19% in terms of Equal\nError Rate, respectively. We also show that this approach generalizes across\nthe transformer and conformer-based model architectures."}
{"id": "2508.02849", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.02849", "abs": "https://arxiv.org/abs/2508.02849", "authors": ["Chunyu Qiang", "Haoyu Wang", "Cheng Gong", "Tianrui Wang", "Ruibo Fu", "Tao Wang", "Ruilong Chen", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "title": "SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec", "comment": null, "summary": "Speech codecs serve as a crucial bridge in unifying speech and text language\nmodels. Existing codec methods face several challenges in semantic encoding,\nsuch as residual paralinguistic information (e.g., timbre, emotion),\ninsufficient semantic completeness, limited reconstruction capability, and lack\nof support for streaming. To address these challenges, we propose\nSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that\ndisentangles semantic and paralinguistic information in a single-codebook\nspace. To ensure semantic completeness and reconstruction fidelity,\nparalinguistic encoding is introduced to bridge the information gap between\nsemantic and acoustic encoding. A semantic-only efficient quantization method\nbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is\nproposed. This approach alleviates the long-tail distribution problem of tokens\nwhile maintaining high codebook utilization. A semantic disentanglement method\nbased on contrastive learning is proposed, which aligns text and speech in a\njoint multimodal frame-level space, effectively removing paralinguistic\ninformation from semantic encoding. An acoustic-constrained multi-stage\noptimization strategy is proposed to ensure robust and stable convergence.\nFigure~\\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA\n(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.\nThe code and model weights for SecoustiCodec will be open-sourced upon the\ncompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,\ncode, and model weights."}
{"id": "2508.02703", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02703", "abs": "https://arxiv.org/abs/2508.02703", "authors": ["Evangelos Sariyanidi", "John D. Herrington", "Lisa Yankowitz", "Pratik Chaudhari", "Theodore D. Satterthwaite", "Casey J. Zampella", "Robert T. Schultz", "Russell T. Shinohara", "Birkan Tunc"], "title": "Measuring Dependencies between Biological Signals with Temporal Self-supervision, and its Limitations", "comment": "To be submitted to NeurIPS 2025 AI for Science Workshop", "summary": "Measuring the statistical dependence between observed signals is a primary\ntool for scientific discovery. However, biological systems often exhibit\ncomplex non-linear interactions that currently cannot be captured without a\npriori knowledge regarding the nature of dependence. We introduce a\nself-supervised approach, concurrence, which is inspired by the observation\nthat if two signals are dependent, then one should be able to distinguish\nbetween temporally aligned vs. misaligned segments extracted from them.\nExperiments with fMRI, physiological and behavioral signals show that, to our\nknowledge, concurrence is the first approach that can expose relationships\nacross such a wide spectrum of signals and extract scientifically relevant\ndifferences without ad-hoc parameter tuning or reliance on a priori\ninformation, providing a potent tool for scientific discoveries across fields.\nHowever, depencencies caused by extraneous factors remain an open problem, thus\nresearchers should validate that exposed relationships truely pertain to the\nquestion(s) of interest."}
{"id": "2508.03041", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03041", "abs": "https://arxiv.org/abs/2508.03041", "authors": ["Malek Itani", "Ashton Graves", "Sefik Emre Eskimez", "Shyamnath Gollakota"], "title": "Neural Speech Extraction with Human Feedback", "comment": "Interspeech 2025", "summary": "We present the first neural target speech extraction (TSE) system that uses\nhuman feedback for iterative refinement. Our approach allows users to mark\nspecific segments of the TSE output, generating an edit mask. The refinement\nsystem then improves the marked sections while preserving unmarked regions.\nSince large-scale datasets of human-marked errors are difficult to collect, we\ngenerate synthetic datasets using various automated masking functions and train\nmodels on each. Evaluations show that models trained with noise power-based\nmasking (in dBFS) and probabilistic thresholding perform best, aligning with\nhuman annotations. In a study with 22 participants, users showed a preference\nfor refined outputs over baseline TSE. Our findings demonstrate that\nhuman-in-the-loop refinement is a promising approach for improving the\nperformance of neural speech extraction."}
{"id": "2508.02974", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02974", "abs": "https://arxiv.org/abs/2508.02974", "authors": ["Julien Hauret", "Thomas Joubaud", "Éric Bavu"], "title": "Real-time speech enhancement in noise for throat microphone using neural audio codec as foundation model", "comment": "2 pages, 2 figures", "summary": "We present a real-time speech enhancement demo using speech captured with a\nthroat microphone. This demo aims to showcase the complete pipeline, from\nrecording to deep learning-based post-processing, for speech captured in noisy\nenvironments with a body-conducted microphone. The throat microphone records\nskin vibrations, which naturally attenuate external noise, but this robustness\ncomes at the cost of reduced audio bandwidth. To address this challenge, we\nfine-tune Kyutai's Mimi--a neural audio codec supporting real-time\ninference--on Vibravox, a dataset containing paired air-conducted and throat\nmicrophone recordings. We compare this enhancement strategy against\nstate-of-the-art models and demonstrate its superior performance. The inference\nruns in an interactive interface that allows users to toggle enhancement,\nvisualize spectrograms, and monitor processing latency."}
{"id": "2508.02710", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02710", "abs": "https://arxiv.org/abs/2508.02710", "authors": ["Beatriz Macas Ordóñez", "Diego Vinicio Orellana Villavicencio", "José Manuel Ferrández", "Paula Bonomini"], "title": "Evaluation of Deep Learning Models for LBBB Classification in ECG Signals", "comment": "Accepted for presentation in the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "summary": "This study explores different neural network architectures to evaluate their\nability to extract spatial and temporal patterns from electrocardiographic\n(ECG) signals and classify them into three groups: healthy subjects, Left\nBundle Branch Block (LBBB), and Strict Left Bundle Branch Block (sLBBB).\n  Clinical Relevance, Innovative technologies enable the selection of\ncandidates for Cardiac Resynchronization Therapy (CRT) by optimizing the\nclassification of subjects with Left Bundle Branch Block (LBBB)."}
{"id": "2508.03047", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03047", "abs": "https://arxiv.org/abs/2508.03047", "authors": ["Malek Itani", "Tuochao Chen", "Shyamnath Gollakota"], "title": "TF-MLPNet: Tiny Real-Time Neural Speech Separation", "comment": "The 6th Clarity Workshop on Improving Speech-in-Noise for Hearing\n  Devices (Clarity 2025)", "summary": "Speech separation on hearable devices can enable transformative augmented and\nenhanced hearing capabilities. However, state-of-the-art speech separation\nnetworks cannot run in real-time on tiny, low-power neural accelerators\ndesigned for hearables, due to their limited compute capabilities. We present\nTF-MLPNet, the first speech separation network capable of running in real-time\non such low-power accelerators while outperforming existing streaming models\nfor blind speech separation and target speech extraction. Our network operates\nin the time-frequency domain, processing frequency sequences with stacks of\nfully connected layers that alternate along the channel and frequency\ndimensions, and independently processing the time sequence at each frequency\nbin using convolutional layers. Results show that our mixed-precision\nquantization-aware trained (QAT) model can process 6 ms audio chunks in\nreal-time on the GAP9 processor, achieving a 3.5-4x runtime reduction compared\nto prior speech separation models."}
{"id": "2508.03065", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.03065", "abs": "https://arxiv.org/abs/2508.03065", "authors": ["Dong Yang"], "title": "Fast Algorithm for Moving Sound Source", "comment": null, "summary": "Modern neural network-based speech processing systems need reverberation\nresistance, relying on large amounts of reverberation data for training.\nExisting methods simulate dynamic scenarios by sampling static systems or\nsupplement with measured data, but struggle to simulate motion data conforming\nto physical laws. To address insufficient training data for speech enhancement\nmodels in moving scenarios, this paper proposes Yang's motion spatio-temporal\nsampling reconstruction theory, enabling efficient simulation of motion-induced\ncontinuous time-varying reverberation. It breaks through the limitations of\ntraditional static Image-Source Method (ISM) in time-varying systems by\ndecomposing the moving image source's impulse response into linear\ntime-invariant modulation and discrete time-varying fractional delay,\nestablishing a physics-compliant moving sound field model. Based on the\nband-limited nature of motion displacement, a hierarchical sampling strategy is\nadopted: high sampling rates for low-order images to retain details, and low\nrates for high-order ones to reduce complexity, combined with a fast synthesis\narchitecture for real-time simulation. Experiments show that compared to\nopen-source model GSound, the theory more accurately restores amplitude and\nphase changes in moving scenarios, solving the industry challenge of motion\nsound source data simulation. It provides high-quality dynamic training data\nfor speech enhancement models and improves the robustness of multi-channel\nend-to-end voice tracking algorithms."}
{"id": "2508.02712", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02712", "abs": "https://arxiv.org/abs/2508.02712", "authors": ["Pallock Halder", "Satyajit Mojumder"], "title": "Physics-guided denoiser network for enhanced additive manufacturing data quality", "comment": "28 pages, 13 figures, 5 tables", "summary": "Modern engineering systems are increasingly equipped with sensors for\nreal-time monitoring and decision-making. However, the data collected by these\nsensors is often noisy and difficult to interpret, limiting its utility for\ncontrol and diagnostics. In this work, we propose a physics-informed denoising\nframework that integrates energy-based model and Fisher score regularization to\njointly reduce data noise and enforce physical consistency with a physics-based\nmodel. The approach is first validated on benchmark problems, including the\nsimple harmonic oscillator, Burgers' equation, and Laplace's equation, across\nvarying noise levels. We then apply the denoising framework to real thermal\nemission data from laser powder bed fusion (LPBF) additive manufacturing\nexperiments, using a trained Physics-Informed Neural Network (PINN) surrogate\nmodel of the LPBF process to guide denoising. Results show that the proposed\nmethod outperforms baseline neural network denoisers, effectively reducing\nnoise under a range of LPBF processing conditions. This physics-guided\ndenoising strategy enables robust, real-time interpretation of low-cost sensor\ndata, facilitating predictive control and improved defect mitigation in\nadditive manufacturing."}
{"id": "2508.03123", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03123", "abs": "https://arxiv.org/abs/2508.03123", "authors": ["Jingyi Chen", "Ju Seung Byun", "Micha Elsner", "Pichao Wang", "Andrew Perrault"], "title": "Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback", "comment": "4 pages, 1 figure, INTERSPEECH 2025. arXiv admin note: text overlap\n  with arXiv:2405.14632", "summary": "Diffusion models produce high-fidelity speech but are inefficient for\nreal-time use due to long denoising steps and challenges in modeling intonation\nand rhythm. To improve this, we propose Diffusion Loss-Guided Policy\nOptimization (DLPO), an RLHF framework for TTS diffusion models. DLPO\nintegrates the original training loss into the reward function, preserving\ngenerative capabilities while reducing inefficiencies. Using naturalness scores\nas feedback, DLPO aligns reward optimization with the diffusion model's\nstructure, improving speech quality. We evaluate DLPO on WaveGrad 2, a\nnon-autoregressive diffusion-based TTS model. Results show significant\nimprovements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective\nevaluations, with DLPO audio preferred 67\\% of the time. These findings\ndemonstrate DLPO's potential for efficient, high-quality diffusion TTS in\nreal-time, resource-limited settings."}
{"id": "2508.03087", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03087", "abs": "https://arxiv.org/abs/2508.03087", "authors": ["Ryo Matsuda", "Juliano G. C. Ribeiro", "Hitoshi Akiyama", "Jorge Trevino"], "title": "Kernel ridge regression based sound field estimation using a rigid spherical microphone array", "comment": "This paper has been accepted to the IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA) 2025", "summary": "We propose a sound field estimation method based on kernel ridge regression\nusing a rigid spherical microphone array. Kernel ridge regression with\nphysically constrained kernel functions, and further with kernel functions\nadapted to observed sound fields, have proven to be powerful tools. However,\nsuch methods generally assume an open-sphere microphone array configuration,\ni.e., no scatterers exist within the observation or estimation region.\nAlternatively, some approaches assume the presence of scatterers and attempt to\neliminate their influence through a least-squares formulation. Even then, these\nmethods typically do not incorporate the boundary conditions of the scatterers,\nwhich are not presumed to be known. In contrast, we exploit the fact the\nscatterer here is a rigid sphere. Meaning, both the virtual scattering source\nlocations and the boundary conditions are well-defined. Based on this, we\nformulate the scattered sound field within the kernel ridge regression\nframework and propose a novel sound field representation incorporating a\nboundary constraint. The effectiveness of the proposed method is demonstrated\nthrough numerical simulations and real-world experiments using a newly\ndeveloped spherical microphone array."}
{"id": "2508.02713", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02713", "abs": "https://arxiv.org/abs/2508.02713", "authors": ["Pengxu Lin", "An-An Lu", "Xiqi Gao"], "title": "Precoder Design for User-Centric Network Massive MIMO: A Symplectic Optimization Approach", "comment": null, "summary": "In this paper, we utilize symplectic optimization to design a precoder for\nuser-centric network (UCN) massive multiple-input multiple-output (MIMO)\nsystems, where a subset of base stations (BSs) serves each user terminal (UT)\ninstead of using all BSs. In UCN massive MIMO systems, the dimension of the\nprecoders is reduced compared to conventional network massive MIMO. It\nsimplifies the implementation of precoders in practical systems. However, the\nmatrix inversion in traditional linear precoders still requires high\ncomputational complexity. To avoid the matrix inversion, we employ the\nsymplectic optimization framework, where optimization problems are solved based\non dissipative Hamiltonian dynamical systems. To better fit symplectic\noptimization, we transform the received model into the real field and\nreformulate the weighted sum-rate (WSR) maximization problem. The objective\nfunction of the optimization problem is viewed as the potential energy of the\ndynamical system. Due to energy dissipation, the continuous dynamical system\nalways converges to a state with minimal potential energy. By discretizing the\ncontinuous system while preserving the symplectic structure, we obtain an\niterative method for the precoder design. The complexity analysis of the\nproposed symplectic method is also provided to show its high computational\nefficiency. Simulation results demonstrate that the proposed precoder design\nbased on symplectic optimization outperforms the weighted minimum mean-square\nerror (WMMSE) precoder in the UCN massive MIMO system."}
{"id": "2508.03166", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03166", "abs": "https://arxiv.org/abs/2508.03166", "authors": ["Mohammed Salah Al-Radhi", "Géza Németh", "Branislav Gerazov"], "title": "MiSTR: Multi-Modal iEEG-to-Speech Synthesis with Transformer-Based Prosody Prediction and Neural Phase Reconstruction", "comment": "5 pages, 2 figures, 1 table. Accepted for presentation at Interspeech\n  2025", "summary": "Speech synthesis from intracranial EEG (iEEG) signals offers a promising\navenue for restoring communication in individuals with severe speech\nimpairments. However, achieving intelligible and natural speech remains\nchallenging due to limitations in feature representation, prosody modeling, and\nphase reconstruction. We introduce MiSTR, a deep-learning framework that\nintegrates: 1) Wavelet-based feature extraction to capture fine-grained\ntemporal, spectral, and neurophysiological representations of iEEG signals, 2)\nA Transformer-based decoder for prosody-aware spectrogram prediction, and 3) A\nneural phase vocoder enforcing harmonic consistency via adaptive spectral\ncorrection. Evaluated on a public iEEG dataset, MiSTR achieves state-of-the-art\nspeech intelligibility, with a mean Pearson correlation of 0.91 between\nreconstructed and original Mel spectrograms, improving over existing neural\nspeech synthesis baselines."}
{"id": "2508.03190", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03190", "abs": "https://arxiv.org/abs/2508.03190", "authors": ["Bronya Roni Chernyak", "Yael Segal", "Yosi Shrem", "Joseph Keshet"], "title": "PatchDSU: Uncertainty Modeling for Out of Distribution Generalization in Keyword Spotting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Deep learning models excel at many tasks but rely on the assumption that\ntraining and test data follow the same distribution. This assumption often does\nnot hold in real-world speech systems, where distribution shifts are common due\nto varying environments, recording conditions, and speaker diversity.\n  The method of Domain Shifts with Uncertainty (DSU) augments the input of each\nneural network layer based on the input feature statistics. It addresses the\nproblem of out-of-domain generalization by assuming feature statistics follow a\nmultivariate Gaussian distribution and substitutes the input with sampled\nfeatures from this distribution. While effective for computer vision, applying\nDSU to speech presents challenges due to the nature of the data. Unlike static\nvisual data, speech is a temporal signal commonly represented by a spectrogram\n- the change of frequency over time. This representation cannot be treated as a\nsimple image, and the resulting sparsity can lead to skewed feature statistics\nwhen applied to the entire input.\n  To tackle out-of-distribution issues in keyword spotting, we propose\nPatchDSU, which extends DSU by splitting the input into patches and\nindependently augmenting each patch. We evaluated PatchDSU and DSU alongside\nother methods on the Google Speech Commands, Librispeech, and TED-LIUM.\nAdditionally, we evaluated performance under white Gaussian and MUSAN music\nnoise conditions. We also explored out-of-domain generalization by analyzing\nmodel performance on datasets they were not trained on. Overall, in most cases,\nboth PatchDSU and DSU outperform other methods. Notably, PatchDSU demonstrates\nmore consistent improvements across the evaluated scenarios compared to other\napproaches."}
{"id": "2508.02718", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02718", "abs": "https://arxiv.org/abs/2508.02718", "authors": ["Zahra Mohammadi", "Siamak Mohammadi"], "title": "SleepLiteCNN: Energy-Efficient Sleep Apnea Subtype Classification with 1-Second Resolution Using Single-Lead ECG", "comment": null, "summary": "Apnea is a common sleep disorder characterized by breathing interruptions\nlasting at least ten seconds and occurring more than five times per hour.\nAccurate, high-temporal-resolution detection of sleep apnea subtypes -\nObstructive, Central, and Mixed - is crucial for effective treatment and\nmanagement. This paper presents an energy-efficient method for classifying\nthese subtypes using a single-lead electrocardiogram (ECG) with high temporal\nresolution to address the real-time needs of wearable devices. We evaluate a\nwide range of classical machine learning algorithms and deep learning\narchitectures on 1-second ECG windows, comparing their accuracy, complexity,\nand energy consumption. Based on this analysis, we introduce SleepLiteCNN, a\ncompact and energy-efficient convolutional neural network specifically designed\nfor wearable platforms. SleepLiteCNN achieves over 95% accuracy and a 92%\nmacro-F1 score, while requiring just 1.8 microjoules per inference after 8-bit\nquantization. Field Programmable Gate Array (FPGA) synthesis further\ndemonstrates significant reductions in hardware resource usage, confirming its\nsuitability for continuous, real-time monitoring in energy-constrained\nenvironments. These results establish SleepLiteCNN as a practical and effective\nsolution for wearable device sleep apnea subtype detection."}
{"id": "2508.03365", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03365", "abs": "https://arxiv.org/abs/2508.03365", "authors": ["Bodam Kim", "Hiskias Dingeto", "Taeyoun Kwon", "Dasol Choi", "DongGeon Lee", "Haon Park", "JaeHoon Lee", "Jongho Shin"], "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs", "comment": null, "summary": "As large language models become increasingly integrated into daily life,\naudio has emerged as a key interface for human-AI interaction. However, this\nconvenience also introduces new vulnerabilities, making audio a potential\nattack surface for adversaries. Our research introduces WhisperInject, a\ntwo-stage adversarial audio attack framework that can manipulate\nstate-of-the-art audio language models to generate harmful content. Our method\nuses imperceptible perturbations in audio inputs that remain benign to human\nlisteners. The first stage uses a novel reward-based optimization method,\nReinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the\ntarget model to circumvent its own safety protocols and generate harmful native\nresponses. This native harmful response then serves as the target for Stage 2,\nPayload Injection, where we use Projected Gradient Descent (PGD) to optimize\nsubtle perturbations that are embedded into benign audio carriers, such as\nweather queries or greeting messages. Validated under the rigorous\nStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation\nframework, our experiments demonstrate a success rate exceeding 86% across\nQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a\nnew class of practical, audio-native threats, moving beyond theoretical\nexploits to reveal a feasible and covert method for manipulating AI behavior."}
{"id": "2508.02801", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02801", "abs": "https://arxiv.org/abs/2508.02801", "authors": ["Hyung Gun Chi", "Florian Pesce", "Wonil Chang", "Oggi Rudovic", "Arturo Argueta", "Stefan Braun", "Vineet Garg", "Ahmed Hussen Abdelaziz"], "title": "Adaptive Knowledge Distillation for Device-Directed Speech Detection", "comment": "5 pages, 2 figures, Interspeech accepted", "summary": "Device-directed speech detection (DDSD) is a binary classification task that\nseparates the user's queries to a voice assistant (VA) from background speech\nor side conversations. This is important for achieving naturalistic user\nexperience. To this end, we propose knowledge distillation (KD) to enhance DDSD\naccuracy while ensuring efficient deployment. Specifically, we introduce a\nnovel adaptive KD method that transfers knowledge from general representations\nof an ASR large pre-trained acoustic encoder (teacher). We apply task-specific\nadapters, on top of the (frozen) teacher encoder, trained jointly with the\nstudent model on DDSD. We demonstrate that the proposed adaptive KD outperforms\nthe student model without distillation in the keyword and keyword-free\n(follow-up) invocations, with an improvement of +26% and +19% in terms of Equal\nError Rate, respectively. We also show that this approach generalizes across\nthe transformer and conformer-based model architectures."}
{"id": "2508.02724", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02724", "abs": "https://arxiv.org/abs/2508.02724", "authors": ["Yahia Dalbah", "Marcel Worring", "Yen-Chia Hsu"], "title": "Veli: Unsupervised Method and Unified Benchmark for Low-Cost Air Quality Sensor Correction", "comment": "Main content: 7 pages, 9 Figures, 3 Tables. Appendix: 4 pages, 6\n  Figures", "summary": "Urban air pollution is a major health crisis causing millions of premature\ndeaths annually, underscoring the urgent need for accurate and scalable\nmonitoring of air quality (AQ). While low-cost sensors (LCS) offer a scalable\nalternative to expensive reference-grade stations, their readings are affected\nby drift, calibration errors, and environmental interference. To address these\nchallenges, we introduce Veli (Reference-free Variational Estimation via Latent\nInference), an unsupervised Bayesian model that leverages variational inference\nto correct LCS readings without requiring co-location with reference stations,\neliminating a major deployment barrier. Specifically, Veli constructs a\ndisentangled representation of the LCS readings, effectively separating the\ntrue pollutant reading from the sensor noise. To build our model and address\nthe lack of standardized benchmarks in AQ monitoring, we also introduce the Air\nQuality Sensor Data Repository (AQ-SDR). AQ-SDR is the largest AQ sensor\nbenchmark to date, with readings from 23,737 LCS and reference stations across\nmultiple regions. Veli demonstrates strong generalization across both\nin-distribution and out-of-distribution settings, effectively handling sensor\ndrift and erratic sensor behavior. Code for model and dataset will be made\npublic when this paper is published."}
{"id": "2508.03448", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03448", "abs": "https://arxiv.org/abs/2508.03448", "authors": ["Jan Melechovsky", "Ambuj Mehrish", "Dorien Herremans"], "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering", "comment": null, "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach."}
{"id": "2508.03041", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03041", "abs": "https://arxiv.org/abs/2508.03041", "authors": ["Malek Itani", "Ashton Graves", "Sefik Emre Eskimez", "Shyamnath Gollakota"], "title": "Neural Speech Extraction with Human Feedback", "comment": "Interspeech 2025", "summary": "We present the first neural target speech extraction (TSE) system that uses\nhuman feedback for iterative refinement. Our approach allows users to mark\nspecific segments of the TSE output, generating an edit mask. The refinement\nsystem then improves the marked sections while preserving unmarked regions.\nSince large-scale datasets of human-marked errors are difficult to collect, we\ngenerate synthetic datasets using various automated masking functions and train\nmodels on each. Evaluations show that models trained with noise power-based\nmasking (in dBFS) and probabilistic thresholding perform best, aligning with\nhuman annotations. In a study with 22 participants, users showed a preference\nfor refined outputs over baseline TSE. Our findings demonstrate that\nhuman-in-the-loop refinement is a promising approach for improving the\nperformance of neural speech extraction."}
{"id": "2508.02742", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02742", "abs": "https://arxiv.org/abs/2508.02742", "authors": ["Chunyu Liu", "Hao Zhang", "Wei Wu", "Fuhui Zhou", "Qihui Wu", "Derrick Wing Kwan Ng", "Chan-Byoung Chae"], "title": "SpectrumFM: A New Paradigm for Spectrum Cognition", "comment": "This paper has been accepted for presentation at the 2025 IEEE Global\n  Communications Conference (GLOBECOM 2025), Cognitive Radio and AI-Enabled\n  Network Symposium", "summary": "The enhancement of spectrum efficiency and the realization of secure spectrum\nutilization are critically dependent on spectrum cognition. However, existing\nspectrum cognition methods often exhibit limited generalization and suboptimal\naccuracy when deployed across diverse spectrum environments and tasks. To\novercome these challenges, we propose a spectrum foundation model, termed\nSpectrumFM, which provides a new paradigm for spectrum cognition. An innovative\nspectrum encoder that exploits the convolutional neural networks and the\nmulti-head self attention mechanisms is proposed to effectively capture both\nfine-grained local signal structures and high-level global dependencies in the\nspectrum data. To enhance its adaptability, two novel self-supervised learning\ntasks, namely masked reconstruction and next-slot signal prediction, are\ndeveloped for pre-training SpectrumFM, enabling the model to learn rich and\ntransferable representations. Furthermore, low-rank adaptation (LoRA)\nparameter-efficient fine-tuning is exploited to enable SpectrumFM to seamlessly\nadapt to various downstream spectrum cognition tasks, including spectrum\nsensing (SS), anomaly detection (AD), and wireless technology classification\n(WTC). Extensive experiments demonstrate the superiority of SpectrumFM over\nstate-of-the-art methods. Specifically, it improves detection probability in\nthe SS task by 30% at -4 dB signal-to-noise ratio (SNR), boosts the area under\nthe curve (AUC) in the AD task by over 10%, and enhances WTC accuracy by 9.6%."}
{"id": "2508.03543", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03543", "abs": "https://arxiv.org/abs/2508.03543", "authors": ["Tianxin Xie", "Shan Yang", "Chenxing Li", "Dong Yu", "Li Liu"], "title": "EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable Text-to-Speech via Activation Steering", "comment": null, "summary": "Text-to-speech (TTS) has shown great progress in recent years. However, most\nexisting TTS systems offer only coarse and rigid emotion control, typically via\ndiscrete emotion labels or a carefully crafted and detailed emotional text\nprompt, making fine-grained emotion manipulation either inaccessible or\nunstable. These models also require extensive, high-quality datasets for\ntraining. To address these limitations, we propose EmoSteer-TTS, a novel\ntraining-free approach, to achieve fine-grained speech emotion control\n(conversion, interpolation, erasure) by activation steering. We first\nempirically observe that modifying a subset of the internal activations within\na flow matching-based TTS model can effectively alter the emotional tone of\nsynthesized speech. Building on this insight, we then develop a training-free\nand efficient algorithm, including activation extraction, emotional token\nsearching, and inference-time steering, which can be seamlessly integrated into\na wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In\naddition, to derive effective steering vectors, we construct a curated\nemotional speech dataset with diverse speakers. Extensive experiments\ndemonstrate that EmoSteer-TTS enables fine-grained, interpretable, and\ncontinuous control over speech emotion, outperforming the state-of-the-art\n(SOTA). To the best of our knowledge, this is the first method that achieves\ntraining-free and continuous fine-grained emotion control in TTS."}
{"id": "2508.03047", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03047", "abs": "https://arxiv.org/abs/2508.03047", "authors": ["Malek Itani", "Tuochao Chen", "Shyamnath Gollakota"], "title": "TF-MLPNet: Tiny Real-Time Neural Speech Separation", "comment": "The 6th Clarity Workshop on Improving Speech-in-Noise for Hearing\n  Devices (Clarity 2025)", "summary": "Speech separation on hearable devices can enable transformative augmented and\nenhanced hearing capabilities. However, state-of-the-art speech separation\nnetworks cannot run in real-time on tiny, low-power neural accelerators\ndesigned for hearables, due to their limited compute capabilities. We present\nTF-MLPNet, the first speech separation network capable of running in real-time\non such low-power accelerators while outperforming existing streaming models\nfor blind speech separation and target speech extraction. Our network operates\nin the time-frequency domain, processing frequency sequences with stacks of\nfully connected layers that alternate along the channel and frequency\ndimensions, and independently processing the time sequence at each frequency\nbin using convolutional layers. Results show that our mixed-precision\nquantization-aware trained (QAT) model can process 6 ms audio chunks in\nreal-time on the GAP9 processor, achieving a 3.5-4x runtime reduction compared\nto prior speech separation models."}
{"id": "2508.02799", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02799", "abs": "https://arxiv.org/abs/2508.02799", "authors": ["Jessica Sanson", "Rahul C. Shah", "Maximilian Pinaroc", "Valerio Frascolla"], "title": "Extracting Range-Doppler Information of Moving Targets from Wi-Fi Channel State Information", "comment": null, "summary": "This paper presents, for the first time, a method to extract both range and\nDoppler information from commercial Wi-Fi Channel State Information (CSI) using\na monostatic (single transceiver) setup. Utilizing the CSI phase in Wi-Fi\nsensing from a Network Interface Card (NIC) not designed for full-duplex\noperation is challenging due to (1) Hardware asynchronization, which introduces\nsignificant phase errors, and (2) Proximity of transmit (Tx) and receive (Rx)\nantennas, which creates strong coupling that overwhelms the motion signal of\ninterest. We propose a new signal processing approach that addresses both\nchallenges via three key innovations: Time offset cancellation, Phase alignment\ncorrection, and Tx/Rx coupling mitigation. Our method achieves cm-level\naccuracy in range and Doppler estimation for moving targets, validated using a\ncommercial Intel Wi-Fi AX211 NIC. Our results show successful detection and\ntracking of moving objects in realistic environments, establishing the\nfeasibility of high-precision sensing using standard Wi-Fi packet\ncommunications and off-the-shelf hardware without requiring any modification or\nspecialized full-duplex capabilities."}
{"id": "2508.02849", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.02849", "abs": "https://arxiv.org/abs/2508.02849", "authors": ["Chunyu Qiang", "Haoyu Wang", "Cheng Gong", "Tianrui Wang", "Ruibo Fu", "Tao Wang", "Ruilong Chen", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "title": "SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec", "comment": null, "summary": "Speech codecs serve as a crucial bridge in unifying speech and text language\nmodels. Existing codec methods face several challenges in semantic encoding,\nsuch as residual paralinguistic information (e.g., timbre, emotion),\ninsufficient semantic completeness, limited reconstruction capability, and lack\nof support for streaming. To address these challenges, we propose\nSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that\ndisentangles semantic and paralinguistic information in a single-codebook\nspace. To ensure semantic completeness and reconstruction fidelity,\nparalinguistic encoding is introduced to bridge the information gap between\nsemantic and acoustic encoding. A semantic-only efficient quantization method\nbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is\nproposed. This approach alleviates the long-tail distribution problem of tokens\nwhile maintaining high codebook utilization. A semantic disentanglement method\nbased on contrastive learning is proposed, which aligns text and speech in a\njoint multimodal frame-level space, effectively removing paralinguistic\ninformation from semantic encoding. An acoustic-constrained multi-stage\noptimization strategy is proposed to ensure robust and stable convergence.\nFigure~\\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA\n(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.\nThe code and model weights for SecoustiCodec will be open-sourced upon the\ncompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,\ncode, and model weights."}
{"id": "2508.03123", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03123", "abs": "https://arxiv.org/abs/2508.03123", "authors": ["Jingyi Chen", "Ju Seung Byun", "Micha Elsner", "Pichao Wang", "Andrew Perrault"], "title": "Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback", "comment": "4 pages, 1 figure, INTERSPEECH 2025. arXiv admin note: text overlap\n  with arXiv:2405.14632", "summary": "Diffusion models produce high-fidelity speech but are inefficient for\nreal-time use due to long denoising steps and challenges in modeling intonation\nand rhythm. To improve this, we propose Diffusion Loss-Guided Policy\nOptimization (DLPO), an RLHF framework for TTS diffusion models. DLPO\nintegrates the original training loss into the reward function, preserving\ngenerative capabilities while reducing inefficiencies. Using naturalness scores\nas feedback, DLPO aligns reward optimization with the diffusion model's\nstructure, improving speech quality. We evaluate DLPO on WaveGrad 2, a\nnon-autoregressive diffusion-based TTS model. Results show significant\nimprovements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective\nevaluations, with DLPO audio preferred 67\\% of the time. These findings\ndemonstrate DLPO's potential for efficient, high-quality diffusion TTS in\nreal-time, resource-limited settings."}
{"id": "2508.02847", "categories": ["eess.SP", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02847", "abs": "https://arxiv.org/abs/2508.02847", "authors": ["Ke Xu", "Chaitanya Krishna Prasad Vallabh", "Souran Manoochehri"], "title": "Integrating Machine Learning with Multimodal Monitoring System Utilizing Acoustic and Vision Sensing to Evaluate Geometric Variations in Laser Directed Energy Deposition", "comment": null, "summary": "Laser directed energy deposition (DED) additive manufacturing struggles with\nconsistent part quality due to complex melt pool dynamics and process\nvariations. While much research targets defect detection, little work has\nvalidated process monitoring systems for evaluating melt pool dynamics and\nprocess quality. This study presents a novel multimodal monitoring framework,\nsynergistically integrating contact-based acoustic emission (AE) sensing with\ncoaxial camera vision to enable layer-wise identification and evaluation of\ngeometric variations in DED parts. The experimental study used three part\nconfigurations: a baseline part without holes, a part with a 3mm diameter\nthrough-hole, and one with a 5mm through-hole to test the system's discerning\ncapabilities. Raw sensor data was preprocessed: acoustic signals were filtered\nfor time-domain and frequency-domain feature extraction, while camera data\nunderwent melt pool segmentation and morphological feature extraction. Multiple\nmachine learning algorithms (including SVM, random forest, and XGBoost) were\nevaluated to find the optimal model for classifying layer-wise geometric\nvariations. The integrated multimodal strategy achieved a superior\nclassification performance of 94.4%, compared to 87.8% for AE only and 86.7%\nfor the camera only. Validation confirmed the integrated system effectively\ncaptures both structural vibration signatures and surface morphological changes\ntied to the geometric variations. While this study focuses on specific\ngeometries, the demonstrated capability to discriminate between features\nestablishes a technical foundation for future applications in characterizing\npart variations like geometric inaccuracies and manufacturing-induced defects."}
{"id": "2508.03065", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.03065", "abs": "https://arxiv.org/abs/2508.03065", "authors": ["Dong Yang"], "title": "Fast Algorithm for Moving Sound Source", "comment": null, "summary": "Modern neural network-based speech processing systems need reverberation\nresistance, relying on large amounts of reverberation data for training.\nExisting methods simulate dynamic scenarios by sampling static systems or\nsupplement with measured data, but struggle to simulate motion data conforming\nto physical laws. To address insufficient training data for speech enhancement\nmodels in moving scenarios, this paper proposes Yang's motion spatio-temporal\nsampling reconstruction theory, enabling efficient simulation of motion-induced\ncontinuous time-varying reverberation. It breaks through the limitations of\ntraditional static Image-Source Method (ISM) in time-varying systems by\ndecomposing the moving image source's impulse response into linear\ntime-invariant modulation and discrete time-varying fractional delay,\nestablishing a physics-compliant moving sound field model. Based on the\nband-limited nature of motion displacement, a hierarchical sampling strategy is\nadopted: high sampling rates for low-order images to retain details, and low\nrates for high-order ones to reduce complexity, combined with a fast synthesis\narchitecture for real-time simulation. Experiments show that compared to\nopen-source model GSound, the theory more accurately restores amplitude and\nphase changes in moving scenarios, solving the industry challenge of motion\nsound source data simulation. It provides high-quality dynamic training data\nfor speech enhancement models and improves the robustness of multi-channel\nend-to-end voice tracking algorithms."}
{"id": "2508.03166", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03166", "abs": "https://arxiv.org/abs/2508.03166", "authors": ["Mohammed Salah Al-Radhi", "Géza Németh", "Branislav Gerazov"], "title": "MiSTR: Multi-Modal iEEG-to-Speech Synthesis with Transformer-Based Prosody Prediction and Neural Phase Reconstruction", "comment": "5 pages, 2 figures, 1 table. Accepted for presentation at Interspeech\n  2025", "summary": "Speech synthesis from intracranial EEG (iEEG) signals offers a promising\navenue for restoring communication in individuals with severe speech\nimpairments. However, achieving intelligible and natural speech remains\nchallenging due to limitations in feature representation, prosody modeling, and\nphase reconstruction. We introduce MiSTR, a deep-learning framework that\nintegrates: 1) Wavelet-based feature extraction to capture fine-grained\ntemporal, spectral, and neurophysiological representations of iEEG signals, 2)\nA Transformer-based decoder for prosody-aware spectrogram prediction, and 3) A\nneural phase vocoder enforcing harmonic consistency via adaptive spectral\ncorrection. Evaluated on a public iEEG dataset, MiSTR achieves state-of-the-art\nspeech intelligibility, with a mean Pearson correlation of 0.91 between\nreconstructed and original Mel spectrograms, improving over existing neural\nspeech synthesis baselines."}
{"id": "2508.02856", "categories": ["eess.SP", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.02856", "abs": "https://arxiv.org/abs/2508.02856", "authors": ["Seyed Bagher Hashemi Natanzi", "Hossein Mohammadi", "Bo Tang", "Vuk Marojevic"], "title": "Secure mmWave Beamforming with Proactive-ISAC Defense Against Beam-Stealing Attacks", "comment": null, "summary": "Millimeter-wave (mmWave) communication systems face increasing susceptibility\nto advanced beam-stealing attacks, posing a significant physical layer security\nthreat. This paper introduces a novel framework employing an advanced Deep\nReinforcement Learning (DRL) agent for proactive and adaptive defense against\nthese sophisticated attacks. A key innovation is leveraging Integrated Sensing\nand Communications (ISAC) capabilities for active, intelligent threat\nassessment. The DRL agent, built on a Proximal Policy Optimization (PPO)\nalgorithm, dynamically controls ISAC probing actions to investigate suspicious\nactivities. We introduce an intensive curriculum learning strategy that\nguarantees the agent experiences successful detection during training to\novercome the complex exploration challenges inherent to such a\nsecurity-critical task. Consequently, the agent learns a robust and adaptive\npolicy that intelligently balances security and communication performance.\nNumerical results demonstrate that our framework achieves a mean attacker\ndetection rate of 92.8% while maintaining an average user SINR of over 13 dB."}
{"id": "2508.03365", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03365", "abs": "https://arxiv.org/abs/2508.03365", "authors": ["Bodam Kim", "Hiskias Dingeto", "Taeyoun Kwon", "Dasol Choi", "DongGeon Lee", "Haon Park", "JaeHoon Lee", "Jongho Shin"], "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs", "comment": null, "summary": "As large language models become increasingly integrated into daily life,\naudio has emerged as a key interface for human-AI interaction. However, this\nconvenience also introduces new vulnerabilities, making audio a potential\nattack surface for adversaries. Our research introduces WhisperInject, a\ntwo-stage adversarial audio attack framework that can manipulate\nstate-of-the-art audio language models to generate harmful content. Our method\nuses imperceptible perturbations in audio inputs that remain benign to human\nlisteners. The first stage uses a novel reward-based optimization method,\nReinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the\ntarget model to circumvent its own safety protocols and generate harmful native\nresponses. This native harmful response then serves as the target for Stage 2,\nPayload Injection, where we use Projected Gradient Descent (PGD) to optimize\nsubtle perturbations that are embedded into benign audio carriers, such as\nweather queries or greeting messages. Validated under the rigorous\nStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation\nframework, our experiments demonstrate a success rate exceeding 86% across\nQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a\nnew class of practical, audio-native threats, moving beyond theoretical\nexploits to reveal a feasible and covert method for manipulating AI behavior."}
{"id": "2508.02950", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.02950", "abs": "https://arxiv.org/abs/2508.02950", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Robert Calderbank"], "title": "Zak-OTFS for Faster-Than-Nyquist Signaling in the Presence of Mobility & Delay Spread", "comment": "5 pages, 2 figures. Submitted to IEEE", "summary": "Orthogonal signaling limits the number of information symbols transmitted in\nbandwidth $B$ and time $T$ to be $BT$. This corresponds to the Nyquist\nsignaling and is achieved by mounting information symbols on $BT$-dimensional\nbasis spanning the $BT$-dimensional space spaced $\\frac{1}{B}$ and\n$\\frac{1}{T}$ apart. Faster-than-Nyquist signaling involves transmitting more\nthan $BT$ informational symbols in a $BT$-dimensional space. This leads to loss\nof orthogonality. This is achieved by time and/or bandwidth expansion resulting\nfrom packing more information symbols in the same $BT$-dimensional space\n(spacing less than $\\frac{1}{B}$ and/or $\\frac{1}{T}$). In this paper, we take\na different approach to faster-than-Nyquist signaling. We propose to\nsuperimpose the information symbols on one another maintaining the original\nspacing in the Nyquist signaling. We carry this out in the delay-Doppler (DD)\ndomain using Zak-transform based orthogonal time frequency space (Zak-OTFS)\nmodulation. In Zak-OTFS, the channel varies slowly. Further Zak-OTFS also\nallows construction of mutually unbiased bases the interference between which\nappear like Gaussian noise. The proposed scheme leverages the slow variation in\nthe DD channel to construct a precoder that mitigates the effect of the\ndoubly-spread channel. Further, in the proposed scheme we mount information\nsymbols on two mutually unbiased bases which allows superposition of\ninformation symbols. This simplifies receiver processing to detection in\nGaussian noise since each basis appears to the other as Gaussian noise. This\nreduction makes it possible to use trellis coded modulation to enhance\nbit-error performance. Numerical results demonstrate that the\nfaster-than-Nyquist signaling scheme achieves similar uncoded performance as\nthat of Nyquist signaling and with coding the performance is better than\nNyquist signaling at high signal-to-noise ratios."}
{"id": "2508.03448", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03448", "abs": "https://arxiv.org/abs/2508.03448", "authors": ["Jan Melechovsky", "Ambuj Mehrish", "Dorien Herremans"], "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering", "comment": null, "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach."}
{"id": "2508.03011", "categories": ["eess.SP", "cs.RO", "I.2.9; C.3"], "pdf": "https://arxiv.org/pdf/2508.03011", "abs": "https://arxiv.org/abs/2508.03011", "authors": ["Hsun-Yu Lee", "Jie Lin", "Fang-Jing Wu"], "title": "Generating Light-based Fingerprints for Indoor Localization", "comment": "5 pages, 12 figures; presented at the 2024 MC & WASN Conference (Best\n  Paper Candidate)", "summary": "Accurate indoor localization underpins applications ranging from wayfinding\nand emergency response to asset tracking and smart-building services.\nRadio-frequency solutions (e.g. Wi-Fi, RFID, UWB) are widely adopted but remain\nvulnerable to multipath fading, interference, and uncontrollable coverage\nvariation. We explore an orthogonal modality -- visible light communication\n(VLC) -- and demonstrate that the spectral signatures captured by a low-cost\nAS7341 sensor can serve as robust location fingerprints.\n  We introduce a two-stage framework that (i) trains a multi-layer perceptron\n(MLP) on real spectral measurements and (ii) enlarges the training corpus with\nsynthetic samples produced by TabGAN. The augmented dataset reduces the mean\nlocalization error from 62.9cm to 49.3cm -- a 20% improvement -- while\nrequiring only 5% additional data-collection effort. Experimental results\nobtained on 42 reference points in a U-shaped laboratory confirm that GAN-based\naugmentation mitigates data-scarcity issues and enhances generalization."}
{"id": "2508.03021", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03021", "abs": "https://arxiv.org/abs/2508.03021", "authors": ["Zhengyu Wang", "Tiebin Mi", "Gui Zhou", "Robert C. Qiu"], "title": "Metasurface-Enabled Extremely Large-Scale Antenna Systems: Transceiver Architecture, Physical Modeling, and Channel Estimation", "comment": null, "summary": "Extremely large-scale antenna arrays (ELAAs) have emerged as a pivotal\ntechnology for addressing the unprecedented performance demands of\nnext-generation wireless communication systems. To enhance their practicality,\nwe propose metasurface-enabled extremely large-scale antenna (MELA) systems --\nnovel transceiver architectures that employ reconfigurable transmissive\nmetasurfaces to facilitate efficient over-the-air RF-to-antenna coupling and\nphase control. This architecture eliminates the need for bulky switch matrices\nand costly phase-shifter networks typically required in conventional solutions.\nPhysically grounded models are developed to characterize electromagnetic field\npropagation through individual transmissive unit cells, capturing the\nfundamental physics of wave transformation and transmission. Additionally,\ndistance-dependent approximate models are introduced, exhibiting structural\nproperties conducive to efficient parameter estimation and signal processing.\nBased on the channel model, a two stage channel estimation framework is\nproposed for the scenarios comprising users in the hybrid near- and far-fields.\nIn the first stage, a dictionary-driven beamspace filtering technique enables\nrapid angular-domain scanning. In the refinement stage, the rotational symmetry\nof subarrays is exploited to design super-resolution estimators that jointly\nrecover angular and range parameters. An analytical expression for the\nhalf-power beamwidth of MELA is derived, revealing its near-optimal spatial\nresolution relative to conventional ELAA architectures. Numerical experiments\nfurther validate the high-resolution of the proposed channel estimation\nalgorithm and the fidelity of the electromagnetic model, positioning the MELA\narchitecture as a highly competitive and forward-looking solution for practical\nELAA deployment."}
{"id": "2508.03084", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03084", "abs": "https://arxiv.org/abs/2508.03084", "authors": ["Lingyan Zhang", "Yuanfeng Qiu", "Dachuan Li", "Shaohua Wu", "Tingting Zhang", "Qinyu Zhang"], "title": "Scenario-Agnostic Deep-Learning-Based Localization with Contrastive Self-Supervised Pre-training", "comment": null, "summary": "Wireless localization has become a promising technology for offering\nintelligent location-based services. Although its localization accuracy is\nimproved under specific scenarios, the short of environmental dynamic\nvulnerability still hinders this approach from being fully practical\napplications. In this paper, we propose CSSLoc, a novel framework on\ncontrastive self-supervised pre-training to learn generic representations for\naccurate localization in various scenarios. Without the location information\nsupervision, CSSLoc attempts to learn an insightful metric on the similarity\ndiscrimination of radio data, in such a scenario-agnostic manner that the\nsimilar samples are closely clustered together and different samples are\nseparated in the representation space. Furthermore, the trained feature encoder\ncan be directly transferred for downstream localization tasks, and the location\npredictor is trained to estimate accurate locations with the robustness of\nenvironmental dynamics. With extensive experimental results, CSSLoc can\noutperform classical and state-of-the-art DNN-based localization schemes in\ntypical indoor scenarios, pushing deep-learning-based localization from\nspecificity to generality."}
{"id": "2508.03120", "categories": ["eess.SP", "cs.ET", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03120", "abs": "https://arxiv.org/abs/2508.03120", "authors": ["Jiangyou Zhu", "Hongyu Deng", "He Chen"], "title": "Can Large Language Models Identify Materials from Radar Signals?", "comment": null, "summary": "Accurately identifying the material composition of objects is a critical\ncapability for AI robots powered by large language models (LLMs) to perform\ncontext-aware manipulation. Radar technologies offer a promising sensing\nmodality for material recognition task. When combined with deep learning, radar\ntechnologies have demonstrated strong potential in identifying the material of\nvarious objects. However, existing radar-based solutions are often constrained\nto closed-set object categories and typically require task-specific data\ncollection to train deep learning models, largely limiting their practical\napplicability. This raises an important question: Can we leverage the powerful\nreasoning capabilities of pre-trained LLMs to directly infer material\ncomposition from raw radar signals? Answering this question is non-trivial due\nto the inherent redundancy of radar signals and the fact that pre-trained LLMs\nhave no prior exposure to raw radar data during training. To address this, we\nintroduce LLMaterial, the first study to investigate the feasibility of using\nLLM to identify materials directly from radar signals. First, we introduce a\nphysics-informed signal processing pipeline that distills high-redundancy radar\nraw data into a set of compact intermediate parameters that encapsulate the\nmaterial's intrinsic characteristics. Second, we adopt a retrieval-augmented\ngeneration (RAG) strategy to provide the LLM with domain-specific knowledge,\nenabling it to interpret and reason over the extracted intermediate parameters.\nLeveraging this integration, the LLM is empowered to perform step-by-step\nreasoning on the condensed radar features, achieving open-set material\nrecognition directly from raw radar signals. Preliminary results show that\nLLMaterial can effectively distinguish among a variety of common materials,\nhighlighting its strong potential for real-world material identification\napplications."}
{"id": "2508.03131", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03131", "abs": "https://arxiv.org/abs/2508.03131", "authors": ["Na Liu", "Chengliang Dai", "Qiuyue Wu", "Qiuqi Li", "Guoxiong Cai"], "title": "Model Order Reduction for Large-scale Circuits Using Higher Order Dynamic Mode Decomposition", "comment": null, "summary": "Model order reduction (MOR) has long been a mainstream strategy to accelerate\nlarge-scale transient circuit simulation. Dynamic Mode Decomposition (DMD)\nrepresents a novel data-driven characterization method, extracting dominant\ndynamical modes directly from time-domain simulation data without requiring\nexplicit system equations. This paper first deduces the DMD algorithm and then\nproposes high order dynamic mode decomposition (HODMD) incorporating delayed\nembedding technique, specifically targeting computational efficiency in\nlarge-scale circuit simulations. Compared with the DMD method, the HODMD method\novercomes the problem that the output signal cannot be reconstructed when the\nspatial resolution is insufficient. The proposed HODMD algorithm is applicable\nto general circuits and does not impose any constraints on the topology of the\npertinent circuit or type of the components. Three representative numerical\ntest cases are presented to systematically validate both the computational\nefficiency and accuracy of the proposed HODMD method."}
{"id": "2508.03248", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03248", "abs": "https://arxiv.org/abs/2508.03248", "authors": ["Yoon Huh", "Bumjun Kim", "Wan Choi"], "title": "Federated Learning with Feature Reconstruction for Vector Quantization based Semantic Communication", "comment": null, "summary": "Recent advancements in semantic communication have primarily focused on image\ntransmission, where neural network (NN)-based joint source-channel coding\n(JSCC) modules play a central role. However, such systems often experience\nsemantic communication errors due to mismatched knowledge bases between users\nand performance degradation from outdated models, necessitating regular model\nupdates. To address these challenges in vector quantization (VQ)-based image\nsemantic communication systems, we propose FedSFR, a novel federated learning\n(FL) framework that incorporates semantic feature reconstruction (FR). FedSFR\nintroduces an FR step at the parameter server (PS) and allows a subset of\nclients to transmit compact feature vectors in lieu of sending full local model\nupdates, thereby improving training stability and communication efficiency. To\nenable effective FR learning, we design a loss function tailored for VQ-based\nimage semantic communication and demonstrate its validity as a surrogate for\nimage reconstruction error. Additionally, we provide a rigorous convergence\nanalysis and present a differentially private variant of FedSFR, along with\nformal privacy analysis. Experimental results on two benchmark datasets\nvalidate the superiority of FedSFR over existing baselines, especially in\ncapacity-constrained settings, confirming both its effectiveness and\nrobustness."}
{"id": "2508.03274", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03274", "abs": "https://arxiv.org/abs/2508.03274", "authors": ["Ramaswamy Palaniappan", "Surej Mouli", "Howard Bowman", "Ian McLoughlin"], "title": "Investigating the Cognitive Response of Brake Lights in Initiating Braking Action Using EEG", "comment": "arXiv admin note: text overlap with arXiv:2010.10584", "summary": "Half of all road accidents result from either lack of driver attention or\nfrom maintaining insufficient separation between vehicles. Collision from the\nrear, in particular, has been identified as the most common class of accident\nin the UK, and its influencing factors have been widely studied for many years.\nRear-mounted stop lamps, illuminated when braking, are the primary mechanism to\nalert following drivers to the need to reduce speed or brake. This paper\ndevelops a novel brain response approach to measuring subject reaction to\ndifferent brake light designs. A variety of off-the-shelf brake light\nassemblies are tested in a physical simulated driving environment to assess the\ncognitive reaction times of 22 subjects. Eight pairs of LED-based and two pairs\nof incandescent bulb-based brake light assemblies are used and\nelectroencephalogram (EEG) data recorded. Channel Pz is utilised to extract the\nP3 component evoked during the decision making process that occurs in the brain\nwhen a participant decides to lift their foot from the accelerator and depress\nthe brake. EEG analysis shows that both incandescent bulb-based lights are\nstatistically slower to evoke cognitive responses than all tested LED-based\nlights. Between the LED designs, differences are evident, but not statistically\nsignificant, attributed to the significant amount of movement artifact in the\nEEG signal."}
{"id": "2508.03279", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03279", "abs": "https://arxiv.org/abs/2508.03279", "authors": ["Vasileios Kouvakis", "Stylianos E. Trevlakis", "Ioannis Arapakis", "Alexandros-Apostolos A. Boulogeorgos"], "title": "Spiking Neural Networks for Resource Allocation in UAV-Enabled Wireless Networks", "comment": null, "summary": "This work presents a new spiking neural network (SNN)-based approach for user\nequipment-base station (UE-BS) association in non-terrestrial networks (NTNs).\nWith the introduction of UAV's in wireless networks, the system architecture\nbecomes heterogeneous, resulting in the need for dynamic and efficient\nmanagement to avoid congestion and sustain overall performance. The presented\nframework compares two SNN-based optimization strategies. Specifically, a\ntop-down centralized approach with complete network visibility and a bottom-up\ndistributed approach for individual network nodes. The SNN is based on leak\nintegrate-and-fire neurons with temporal components, which can perform fast and\nefficient event-driven inference. Realistic ray-tracing simulations are\nconducted, which showcase that the bottom-up model attains over 90\\% accuracy,\nwhile the top-down model maintains 80-100\\% accuracy. Both approaches reveal a\ntrade-off between individually optimal solutions and UE-BS association\nfeasibility, thus revealing the effectiveness of both approaches depending on\ndeployment scenarios."}
{"id": "2508.03327", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03327", "abs": "https://arxiv.org/abs/2508.03327", "authors": ["Xingyu Huang", "Ruining Fan", "Mouli Chakraborty", "Avishek Nag", "Anshu Mukherjee"], "title": "Quantum Deep Learning for Massive MIMO User Scheduling", "comment": null, "summary": "We introduce a hybrid Quantum Neural Networks (QNN) architecture for the\nefficient user scheduling in 5G/Beyond 5G (B5G) massive Multiple Input Multiple\nOutput (MIMO) systems, addressing the scalability issues of traditional\nmethods. By leveraging statistical Channel State Information (CSI), our model\nreduces computational overhead and enhances spectral efficiency. It integrates\nclassical neural networks with a variational quantum circuit kernel,\noutperforming classical Convolutional Neural Networks (CNNs) and maintaining\nrobust performance in noisy channels. This demonstrates the potential of\nquantum-enhanced Machine Learning (ML) for wireless scheduling."}
{"id": "2508.03391", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03391", "abs": "https://arxiv.org/abs/2508.03391", "authors": ["Seunghyeon Jeon", "Seonjung Kim", "Gyeongrae Im", "Yo-Seb Jeon"], "title": "Beam-Hopping Pattern Design for Grant-Free Random Access in LEO Satellite Communications", "comment": null, "summary": "Increasing demand for massive device connectivity in underserved regions\ndrives the development of advanced low Earth orbit (LEO) satellite\ncommunication systems. Beam-hopping LEO systems without connection\nestablishment provide a promising solution for achieving both demand-aware\nresource allocation and low access latency. This paper investigates\nbeam-hopping pattern design for the grant-free random access systems to\ndynamically allocate satellite resources according to traffic demands across\nserving cells. We formulate a binary optimization problem that aims to maximize\nthe minimum successful transmission probability across cells, given limited\nsatellite beam generation capacity. To solve this problem, we propose novel\nbeam-hopping design algorithms that alternately enhance the collision avoidance\nrate and decoding success probability within an alternating optimization\nframework. Specifically, the algorithms employ a bisection method to optimize\nillumination allocation for each cell based on demand, while using the\nalternating direction method of multipliers (ADMM) to optimize beam-hopping\npatterns for maximizing decoding success probability. Furthermore, we enhance\nthe ADMM by replacing the strict binary constraint with two equivalent\ncontinuous-valued constraints. Simulation results demonstrate the superiority\nof the proposed algorithms compared to other beam-hopping methods and verify\nrobustness in managing traffic demand imbalance."}
{"id": "2508.03423", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03423", "abs": "https://arxiv.org/abs/2508.03423", "authors": ["Isabella W. G. da Silva", "Zahra Mobini", "Hien Q. Ngo", "Hyundong Shin", "Michail Matthaiou"], "title": "How to Proactively Monitor Untrusted Communications with Cell-Free Massive MIMO?", "comment": null, "summary": "This paper studies a cell-free massive multiple-input multiple-output\n(CF-mMIMO) proactive monitoring system in which multiple multi-antenna\nmonitoring nodes (MNs) are assigned to either observe the transmissions from an\nuntrusted transmitter (UT) or to jam the reception at the untrusted receiver\n(UR). We propose an effective channel state information (CSI) acquisition\nscheme for the monitoring system. In our approach, the MNs leverage the pilot\nsignals transmitted during the uplink and downlink phases of the untrusted link\nand estimate the effective channels corresponding to the UT and UR via a\nminimum mean-squared error (MMSE) estimation scheme. We derive new spectral\nefficiency (SE) expressions for the untrusted link and the monitoring system.\nFor the latter, the SE is derived for two CSI availability cases at the central\nprocessing unit (CPU); namely case-1: imperfect CSI knowledge at both MNs and\nCPU, case-2: imperfect CSI knowledge at the MNs and no CSI knowledge at the\nCPU. To improve the monitoring performance, we propose a novel joint mode\nassignment and jamming power control optimization method to maximize the\nmonitoring success probability (MSP) based on the Bayesian optimization\nframework. Numerical results show that (a) our CF-mMIMO proactive monitoring\nsystem relying on the proposed CSI acquisition and optimization approach\nsignificantly outperforms the considered benchmarks; (b) the MSP performance of\nour CF-mMIMO proactive monitoring system is greater than 0.8, regardless of the\nnumber of antennas at the untrusted nodes or the precoding scheme for the\nuntrusted transmission link."}
{"id": "2508.03460", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.03460", "abs": "https://arxiv.org/abs/2508.03460", "authors": ["Anubhab Chowdhury", "Sai Subramanyam Thoota", "Erik G. Larsson"], "title": "Joint Sensing and Bi-Directional Communication with Dynamic TDD Enabled Cell-Free MIMO", "comment": "Accepted for publication in the IEEE Transactions on Wireless\n  Communications 14 pages, 9 figures (one with two subfigures)", "summary": "This paper studies integrated sensing and communication (ISAC) with dynamic\ntime division duplex (DTDD) cell-free (CF) massive multiple-input\nmultiple-output~(mMIMO) systems. DTDD enables the CF mMIMO system to\nconcurrently serve both uplink~(UL) and downlink~(DL) users with spatially\nseparated \\emph{half-duplex~(HD)} access points~(APs) using the same\ntime-frequency resources. Further, to facilitate ISAC, the UL APs are utilized\nfor both UL data and target echo reception, while the DL APs jointly transmit\nthe precoded DL data streams and target signal. In this context, we present\ncentralized and distributed generalized likelihood-ratio tests~(GLRTs) for\ntarget detection treating UL users' signals as sensing interference. We then\nquantify the optimality and complexity trade-off between distributed and\ncentralized GLRTs and benchmark the respective estimators with the Bayesian\nCram\\'er-Rao lower bound for target radar-cross section~(RCS). Then, we present\na unified framework for joint UL users' data detection and RCS estimation.\nNext, for communication, we derive the signal-to-noise-plus-interference~(SINR)\noptimal combiner accounting for the cross-link and radar interference for UL\ndata processing. In DL, we use regularized zero-forcing for the users and\npropose two types of precoders for the target: one ``user-centric\" that\nnullifies the interference caused by the target signal to the DL users and one\n``target-centric\" based on the dominant eigenvector of the composite channel\nbetween the target and the APs. Finally, numerical studies corroborate with our\ntheoretical findings and reveal that the \\emph{GLRT is robust to inter-AP\ninterference, and DTDD doubles the $90\\%$-likely sum UL-DL SE compared to\ntraditional TDD-based CF-mMIMO ISAC systems}; while using HD hardware."}
{"id": "2508.03584", "categories": ["eess.SP", "cs.AI", "cs.ET", "cs.NI", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2508.03584", "abs": "https://arxiv.org/abs/2508.03584", "authors": ["Fatih Gulec", "Hamdan Awan", "Nigel Wallbridge", "Andrew W. Eckford"], "title": "Decoding and Engineering the Phytobiome Communication for Smart Agriculture", "comment": "Under revision for IEEE Communications Magazine", "summary": "Smart agriculture applications, integrating technologies like the Internet of\nThings and machine learning/artificial intelligence (ML/AI) into agriculture,\nhold promise to address modern challenges of rising food demand, environmental\npollution, and water scarcity. Alongside the concept of the phytobiome, which\ndefines the area including the plant, its environment, and associated\norganisms, and the recent emergence of molecular communication (MC), there\nexists an important opportunity to advance agricultural science and practice\nusing communication theory. In this article, we motivate to use the\ncommunication engineering perspective for developing a holistic understanding\nof the phytobiome communication and bridge the gap between the phytobiome\ncommunication and smart agriculture. Firstly, an overview of phytobiome\ncommunication via molecular and electrophysiological signals is presented and a\nmulti-scale framework modeling the phytobiome as a communication network is\nconceptualized. Then, how this framework is used to model electrophysiological\nsignals is demonstrated with plant experiments. Furthermore, possible smart\nagriculture applications, such as smart irrigation and targeted delivery of\nagrochemicals, through engineering the phytobiome communication are proposed.\nThese applications merge ML/AI methods with the Internet of Bio-Nano-Things\nenabled by MC and pave the way towards more efficient, sustainable, and\neco-friendly agricultural production. Finally, the implementation challenges,\nopen research issues, and industrial outlook for these applications are\ndiscussed."}

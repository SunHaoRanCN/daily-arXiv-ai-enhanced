{"id": "2508.03764", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03764", "abs": "https://arxiv.org/abs/2508.03764", "authors": ["Justin Luong", "Hao Xue", "Flora D. Salim"], "title": "CoughViT: A Self-Supervised Vision Transformer for Cough Audio Representation Learning", "comment": "Accepted to ISWC", "summary": "Physicians routinely assess respiratory sounds during the diagnostic process,\nproviding insight into the condition of a patient's airways. In recent years,\nAI-based diagnostic systems operating on respiratory sounds, have demonstrated\nsuccess in respiratory disease detection. These systems represent a crucial\nadvancement in early and accessible diagnosis which is essential for timely\ntreatment. However, label and data scarcity remain key challenges, especially\nfor conditions beyond COVID-19, limiting diagnostic performance and reliable\nevaluation. In this paper, we propose CoughViT, a novel pre-training framework\nfor learning general-purpose cough sound representations, to enhance diagnostic\nperformance in tasks with limited data. To address label scarcity, we employ\nmasked data modelling to train a feature encoder in a self-supervised learning\nmanner. We evaluate our approach against other pre-training strategies on three\ndiagnostically important cough classification tasks. Experimental results show\nthat our representations match or exceed current state-of-the-art supervised\naudio representations in enhancing performance on downstream tasks."}
{"id": "2508.03780", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03780", "abs": "https://arxiv.org/abs/2508.03780", "authors": ["Katharina Hoedt", "Arthur Flexer", "Gerhard Widmer"], "title": "Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition", "comment": "8 pages, published in Proceedings of the 22nd Sound and Music\n  Computing Conference 2025 (SMC-25)", "summary": "One of the desired key properties of deep learning models is the ability to\ngeneralise to unseen samples. When provided with new samples that are\n(perceptually) similar to one or more training samples, deep learning models\nare expected to produce correspondingly similar outputs. Models that succeed in\npredicting similar outputs for similar inputs are often called robust. Deep\nlearning models, on the other hand, have been shown to be highly vulnerable to\nminor (adversarial) perturbations of the input, which manage to drastically\nchange a model's output and simultaneously expose its reliance on spurious\ncorrelations. In this work, we investigate whether inherently interpretable\ndeep models, i.e., deep models that were designed to focus more on meaningful\nand interpretable features, are more robust to irrelevant perturbations in the\ndata, compared to their black-box counterparts. We test our hypothesis by\ncomparing the robustness of an interpretable and a black-box music emotion\nrecognition (MER) model when challenged with adversarial examples. Furthermore,\nwe include an adversarially trained model, which is optimised to be more\nrobust, in the comparison. Our results indicate that inherently more\ninterpretable models can indeed be more robust than their black-box\ncounterparts, and achieve similar levels of robustness as adversarially trained\nmodels, at lower computational cost."}
{"id": "2508.03983", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03983", "abs": "https://arxiv.org/abs/2508.03983", "authors": ["Heinrich Dinkel", "Gang Li", "Jizhong Liu", "Jian Luan", "Yadong Niu", "Xingwei Sun", "Tianzi Wang", "Qiyang Xiao", "Junbo Zhang", "Jiahao Zhou"], "title": "MiDashengLM: Efficient Audio Understanding with General Audio Captions", "comment": null, "summary": "Current approaches for large audio language models (LALMs) often rely on\nclosed data sources or proprietary models, limiting their generalization and\naccessibility. This paper introduces MiDashengLM, a novel open audio-language\nmodel designed for efficient and comprehensive audio understanding through the\nuse of general audio captions using our novel ACAVCaps training dataset.\nMiDashengLM exclusively relies on publicly available pretraining and supervised\nfine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At\nits core, MiDashengLM integrates Dasheng, an open-source audio encoder,\nspecifically engineered to process diverse auditory information effectively.\nUnlike previous works primarily focused on Automatic Speech Recognition (ASR)\nbased audio-text alignment, our strategy centers on general audio captions,\nfusing speech, sound and music information into one textual representation,\nenabling a holistic textual representation of complex audio scenes. Lastly,\nMiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT)\nand up to 20x higher throughput than comparable models. Checkpoints are\navailable online at https://huggingface.co/mispeech/midashenglm-7b and\nhttps://github.com/xiaomi-research/dasheng-lm."}
{"id": "2508.04096", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04096", "abs": "https://arxiv.org/abs/2508.04096", "authors": ["Bingshen Mu", "Yiwen Shao", "Kun Wei", "Dong Yu", "Lei Xie"], "title": "Efficient Scaling for LLM-based ASR", "comment": "Accepted by ASRU 2025", "summary": "Large language model (LLM)-based automatic speech recognition (ASR) achieves\nstrong performance but often incurs high computational costs. This work\ninvestigates how to obtain the best LLM-ASR performance efficiently. Through\ncomprehensive and controlled experiments, we find that pretraining the speech\nencoder before integrating it with the LLM leads to significantly better\nscaling efficiency than the standard practice of joint post-training of\nLLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training\nstrategy, EFIN: Encoder First Integration. Among all training strategies\nevaluated, EFIN consistently delivers better performance (relative to 21.1%\nCERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore,\nwe derive a scaling law that approximates ASR error rates as a computation\nfunction, providing practical guidance for LLM-ASR scaling."}
{"id": "2508.03937", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03937", "abs": "https://arxiv.org/abs/2508.03937", "authors": ["Zongli Ye", "Jiachen Lian", "Akshaj Gupta", "Xuanru Zhou", "Krish Patel", "Haodong Li", "Hwi Joo Park", "Chenxu Guo", "Shuhe Li", "Sam Wang", "Cheol Jun Cho", "Zoe Ezzes", "Jet M. J. Vonk", "Brittany T. Morin", "Rian Bogley", "Lisa Wauters", "Zachary A. Miller", "Maria Luisa Gorno-Tempini", "Gopala Anumanchipalli"], "title": "LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription Robustness", "comment": "2025 ASRU", "summary": "Phonetic speech transcription is crucial for fine-grained linguistic analysis\nand downstream speech applications. While Connectionist Temporal Classification\n(CTC) is a widely used approach for such tasks due to its efficiency, it often\nfalls short in recognition performance, especially under unclear and nonfluent\nspeech. In this work, we propose LCS-CTC, a two-stage framework for\nphoneme-level speech recognition that combines a similarity-aware local\nalignment algorithm with a constrained CTC training objective. By predicting\nfine-grained frame-phoneme cost matrices and applying a modified Longest Common\nSubsequence (LCS) algorithm, our method identifies high-confidence alignment\nzones which are used to constrain the CTC decoding path space, thereby reducing\noverfitting and improving generalization ability, which enables both robust\nrecognition and text-free forced alignment. Experiments on both LibriSpeech and\nPPA demonstrate that LCS-CTC consistently outperforms vanilla CTC baselines,\nsuggesting its potential to unify phoneme modeling across fluent and non-fluent\nspeech."}
{"id": "2508.03698", "categories": ["eess.SP", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03698", "abs": "https://arxiv.org/abs/2508.03698", "authors": ["Se Won Oh", "Hyuntae Jeong", "Seungeun Chung", "Jeong Mook Lim", "Kyoung Ju Noh", "Sunkyung Lee", "Gyuwon Jung"], "title": "Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024", "comment": "This work is intended for submission to an IEEE conference. The\n  content is also relevant to the cs.HC category", "summary": "Improving human health and well-being requires an accurate and effective\nunderstanding of an individual's physical and mental state throughout daily\nlife. To support this goal, we utilized smartphones, smartwatches, and sleep\nsensors to collect data passively and continuously for 24 hours a day, with\nminimal interference to participants' usual behavior, enabling us to gather\nquantitative data on daily behaviors and sleep activities across multiple days.\nAdditionally, we gathered subjective self-reports of participants' fatigue,\nstress, and sleep quality through surveys conducted immediately before and\nafter sleep. This comprehensive lifelog dataset is expected to provide a\nfoundational resource for exploring meaningful insights into human daily life\nand lifestyle patterns, and a portion of the data has been anonymized and made\npublicly available for further research. In this paper, we introduce the ETRI\nLifelog Dataset 2024, detailing its structure and presenting potential\napplications, such as using machine learning models to predict sleep quality\nand stress."}
{"id": "2508.04195", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04195", "abs": "https://arxiv.org/abs/2508.04195", "authors": ["Huan Liao", "Qinke Ni", "Yuancheng Wang", "Yiheng Lu", "Haoyue Zhan", "Pengyuan Xie", "Qiang Zhang", "Zhizheng Wu"], "title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations", "comment": null, "summary": "Paralinguistic vocalizations-including non-verbal sounds like laughter and\nbreathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are\nintegral to natural spoken communication. Despite their importance in conveying\naffect, intent, and interactional cues, such cues remain largely overlooked in\nconventional automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. We present NVSpeech, an integrated and scalable pipeline that bridges\nthe recognition and synthesis of paralinguistic vocalizations, encompassing\ndataset construction, ASR modeling, and controllable TTS. (1) We introduce a\nmanually annotated dataset of 48,430 human-spoken utterances with 18 word-level\nparalinguistic categories. (2) We develop the paralinguistic-aware ASR model,\nwhich treats paralinguistic cues as inline decodable tokens (e.g., \"You're so\nfunny [Laughter]\"), enabling joint lexical and non-verbal transcription. This\nmodel is then used to automatically annotate a large corpus, the first\nlarge-scale Chinese dataset of 174,179 utterances (573 hours) with word-level\nalignment and paralingustic cues. (3) We finetune zero-shot TTS models on both\nhuman- and auto-labeled data to enable explicit control over paralinguistic\nvocalizations, allowing context-aware insertion at arbitrary token positions\nfor human-like speech synthesis. By unifying the recognition and generation of\nparalinguistic vocalizations, NVSpeech offers the first open, large-scale,\nword-level annotated pipeline for expressive speech modeling in Mandarin,\nintegrating recognition and synthesis in a scalable and controllable manner.\nDataset and audio demos are available at https://nvspeech170k.github.io/."}
{"id": "2508.04141", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04141", "abs": "https://arxiv.org/abs/2508.04141", "authors": ["Jingyuan Xing", "Zhipeng Li", "Jialong Mai", "Xiaofen Xing", "Xiangmin Xu"], "title": "Parallel GPT: Harmonizing the Independence and Interdependence of Acoustic and Semantic Information for Zero-Shot Text-to-Speech", "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP)", "summary": "Advances in speech representation and large language models have enhanced\nzero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS\nmodels face challenges in capturing the complex correlations between acoustic\nand semantic features, resulting in a lack of expressiveness and similarity.\nThe primary reason lies in the complex relationship between semantic and\nacoustic features, which manifests independent and interdependent aspects.This\npaper introduces a TTS framework that combines both autoregressive (AR) and\nnon-autoregressive (NAR) modules to harmonize the independence and\ninterdependence of acoustic and semantic information. The AR model leverages\nthe proposed Parallel Tokenizer to synthesize the top semantic and acoustic\ntokens simultaneously. In contrast, considering the interdependence, the\nCoupled NAR model predicts detailed tokens based on the general AR model's\noutput. Parallel GPT, built on this architecture, is designed to improve\nzero-shot text-to-speech synthesis through its parallel structure. Experiments\non English and Chinese datasets demonstrate that the proposed model\nsignificantly outperforms the quality and efficiency of the synthesis of\nexisting zero-shot TTS models. Speech demos are available at\nhttps://t1235-ch.github.io/pgpt/."}
{"id": "2508.03715", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03715", "abs": "https://arxiv.org/abs/2508.03715", "authors": ["Bertram Fuchs", "Mehdi Ejtehadi", "Ana Cisnal", "Jürgen Pannek", "Anke Scheel-Sailer", "Robert Riener", "Inge Eriks-Hoogland", "Diego Paez-Granados"], "title": "Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors", "comment": null, "summary": "Autonomic Dysreflexia (AD) is a potentially life-threatening condition\ncharacterized by sudden, severe blood pressure (BP) spikes in individuals with\nspinal cord injury (SCI). Early, accurate detection is essential to prevent\ncardiovascular complications, yet current monitoring methods are either\ninvasive or rely on subjective symptom reporting, limiting applicability in\ndaily file. This study presents a non-invasive, explainable machine learning\nframework for detecting AD using multimodal wearable sensors. Data were\ncollected from 27 individuals with chronic SCI during urodynamic studies,\nincluding electrocardiography (ECG), photoplethysmography (PPG), bioimpedance\n(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three\ncommercial devices. Objective AD labels were derived from synchronized\ncuff-based BP measurements. Following signal preprocessing and feature\nextraction, BorutaSHAP was used for robust feature selection, and SHAP values\nfor explainability. We trained modality- and device-specific weak learners and\naggregated them using a stacked ensemble meta-model. Cross-validation was\nstratified by participants to ensure generalizability. HR- and ECG-derived\nfeatures were identified as the most informative, particularly those capturing\nrhythm morphology and variability. The Nearest Centroid ensemble yielded the\nhighest performance (Macro F1 = 0.77+/-0.03), significantly outperforming\nbaseline models. Among modalities, HR achieved the highest area under the curve\n(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature\nfeatures contributed less to overall accuracy, consistent with missing data and\nlow specificity. The model proved robust to sensor dropout and aligned well\nwith clinical AD events. These results represent an important step toward\npersonalized, real-time monitoring for individuals with SCI."}
{"id": "2508.04529", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04529", "abs": "https://arxiv.org/abs/2508.04529", "authors": ["Han Yin", "Yang Xiao", "Rohan Kumar Das", "Jisheng Bai", "Ting Dang"], "title": "ESDD 2026: Environmental Sound Deepfake Detection Challenge Evaluation Plan", "comment": null, "summary": "Recent advances in audio generation systems have enabled the creation of\nhighly realistic and immersive soundscapes, which are increasingly used in film\nand virtual reality. However, these audio generators also raise concerns about\npotential misuse, such as generating deceptive audio content for fake videos\nand spreading misleading information. Existing datasets for environmental sound\ndeepfake detection (ESDD) are limited in scale and audio types. To address this\ngap, we have proposed EnvSDD, the first large-scale curated dataset designed\nfor ESDD, consisting of 45.25 hours of real and 316.7 hours of fake sound.\nBased on EnvSDD, we are launching the Environmental Sound Deepfake Detection\nChallenge. Specifically, we present two different tracks: ESDD in Unseen\nGenerators and Black-Box Low-Resource ESDD, covering various challenges\nencountered in real-life scenarios. The challenge will be held in conjunction\nwith the 2026 IEEE International Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP 2026)."}
{"id": "2508.04143", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04143", "abs": "https://arxiv.org/abs/2508.04143", "authors": ["Xi Xuan", "Yang Xiao", "Rohan Kumar Das", "Tomi Kinnunen"], "title": "Multilingual Source Tracing of Speech Deepfakes: A First Benchmark", "comment": "Accepted at Interspeech SPSC 2025 - 5th Symposium on Security and\n  Privacy in Speech Communication (Oral)", "summary": "Recent progress in generative AI has made it increasingly easy to create\nnatural-sounding deepfake speech from just a few seconds of audio. While these\ntools support helpful applications, they also raise serious concerns by making\nit possible to generate convincing fake speech in many languages. Current\nresearch has largely focused on detecting fake speech, but little attention has\nbeen given to tracing the source models used to generate it. This paper\nintroduces the first benchmark for multilingual speech deepfake source tracing,\ncovering both mono- and cross-lingual scenarios. We comparatively investigate\nDSP- and SSL-based modeling; examine how SSL representations fine-tuned on\ndifferent languages impact cross-lingual generalization performance; and\nevaluate generalization to unseen languages and speakers. Our findings offer\nthe first comprehensive insights into the challenges of identifying speech\ngeneration models when training and inference languages differ. The dataset,\nprotocol and code are available at\nhttps://github.com/xuanxixi/Multilingual-Source-Tracing."}
{"id": "2508.03906", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.03906", "abs": "https://arxiv.org/abs/2508.03906", "authors": ["Saif Khan Mohammed", "Saurabh Prakash", "Muhammad Ubadah", "Imran Ali Khan", "Ronny Hadani", "Shlomo Rakib", "Shachar Kons", "Yoav Hebron", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Zak-OTFS over CP-OFDM", "comment": null, "summary": "Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to\nachieve significantly better performance compared to the standardized\nCyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high\ndelay/Doppler spread scenarios envisaged in next generation communication\nsystems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)\ndomain, characterized by two parameters, (i) the pulse period along the delay\naxis (``delay period\") (Doppler period is related to the delay period), and\n(ii) the pulse shaping filter. An important practical challenge is enabling\nsupport for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper\nwe show that Zak-OTFS modulation with pulse shaping constrained to sinc\nfiltering (filter bandwidth equal to the communication bandwidth $B$) followed\nby time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is\nthe symbol duration and $T_{cp}$ is the CP duration), can be implemented as a\nlow-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS\nde-modulator with matched filtering constrained to sinc filtering (filter\nbandwidth $B$) followed by rectangular time windowing over duration $T$ can be\nimplemented as a low-complexity post-processing of the CP-OFDM de-modulator\noutput. This proposed ``Zak-OTFS over CP-OFDM\" architecture enables us to\nharness the benefits of Zak-OTFS in existing network infrastructure. We also\nshow that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with\nCP-OFDM being a special case when the delay period takes its minimum possible\nvalue equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum\ndelay period."}
{"id": "2508.04651", "categories": ["cs.SD", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04651", "abs": "https://arxiv.org/abs/2508.04651", "authors": ["Lyria Team", "Antoine Caillon", "Brian McWilliams", "Cassie Tarakajian", "Ian Simon", "Ilaria Manco", "Jesse Engel", "Noah Constant", "Pen Li", "Timo I. Denk", "Alberto Lalama", "Andrea Agostinelli", "Anna Huang", "Ethan Manilow", "George Brower", "Hakan Erdogan", "Heidi Lei", "Itai Rolnick", "Ivan Grishchenko", "Manu Orsini", "Matej Kastelic", "Mauricio Zuluaga", "Mauro Verzetti", "Michael Dooley", "Ondrej Skopek", "Rafael Ferrer", "Zalán Borsos", "Äaron van den Oord", "Douglas Eck", "Eli Collins", "Jason Baldridge", "Tom Hume", "Chris Donahue", "Kehang Han", "Adam Roberts"], "title": "Live Music Models", "comment": null, "summary": "We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance."}
{"id": "2508.04230", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04230", "abs": "https://arxiv.org/abs/2508.04230", "authors": ["Yacouba Kaloga", "Ina Kodrasi"], "title": "Towards interpretable emotion recognition: Identifying key features with machine learning", "comment": null, "summary": "Unsupervised methods, such as wav2vec2 and HuBERT, have achieved\nstate-of-the-art performance in audio tasks, leading to a shift away from\nresearch on interpretable features. However, the lack of interpretability in\nthese methods limits their applicability in critical domains like medicine,\nwhere understanding feature relevance is crucial. To better understand the\nfeatures of unsupervised models, it remains critical to identify the\ninterpretable features relevant to a given task. In this work, we focus on\nemotion recognition and use machine learning algorithms to identify and\ngeneralize the most important interpretable features for this task. While\nprevious studies have explored feature relevance in emotion recognition, they\nare often constrained by narrow contexts and present inconsistent findings. Our\napproach aims to overcome these limitations, providing a broader and more\nrobust framework for identifying the most important interpretable features."}
{"id": "2508.04046", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04046", "abs": "https://arxiv.org/abs/2508.04046", "authors": ["Xiao Tong", "Lei Lei", "Ang Li", "A. Lee Swindlehurst", "Symeon Chatzinotas"], "title": "Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints", "comment": null, "summary": "This paper investigates constructive interference (CI)-based waveform design\nfor phase shift keying and quadrature amplitude modulation symbols under\nrelaxed block-level power constraints in multi-user multiple-input\nsingle-output (MU-MIMO) communication systems. Existing linear CI-based\nprecoding methods, including symbol-level precoding (SLP) and block-level\nprecoding (BLP), suffer from performance limitations due to strict symbol-level\npower budgets or insufficient degrees of freedom over the block. To overcome\nthese challenges, we propose a nonlinear waveform optimization framework that\nintroduces additional optimization variables and maximizes the minimum CI\nmetric across the transmission block. The optimal waveform is derived in closed\nform using the function and Karush Kuhn Tucker conditions, and the solution is\nexplicitly expressed with respect to the dual variables. Moreover, the original\nproblems are equivalently reformulated as tractable quadratic programming (QP)\nproblems. To efficiently solve the derived QP problems, we develop an improved\nalternating direction method of multipliers (ADMM) algorithm by integrating a\nlinear-time projection technique, which significantly enhances the\ncomputational efficiency. Simulation results demonstrate that the proposed\nalgorithms substantially outperform the conventional CI-SLP and CI-BLP\napproaches, particularly under high-order modulations and large block lengths."}
{"id": "2508.04141", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04141", "abs": "https://arxiv.org/abs/2508.04141", "authors": ["Jingyuan Xing", "Zhipeng Li", "Jialong Mai", "Xiaofen Xing", "Xiangmin Xu"], "title": "Parallel GPT: Harmonizing the Independence and Interdependence of Acoustic and Semantic Information for Zero-Shot Text-to-Speech", "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing (TASLP)", "summary": "Advances in speech representation and large language models have enhanced\nzero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS\nmodels face challenges in capturing the complex correlations between acoustic\nand semantic features, resulting in a lack of expressiveness and similarity.\nThe primary reason lies in the complex relationship between semantic and\nacoustic features, which manifests independent and interdependent aspects.This\npaper introduces a TTS framework that combines both autoregressive (AR) and\nnon-autoregressive (NAR) modules to harmonize the independence and\ninterdependence of acoustic and semantic information. The AR model leverages\nthe proposed Parallel Tokenizer to synthesize the top semantic and acoustic\ntokens simultaneously. In contrast, considering the interdependence, the\nCoupled NAR model predicts detailed tokens based on the general AR model's\noutput. Parallel GPT, built on this architecture, is designed to improve\nzero-shot text-to-speech synthesis through its parallel structure. Experiments\non English and Chinese datasets demonstrate that the proposed model\nsignificantly outperforms the quality and efficiency of the synthesis of\nexisting zero-shot TTS models. Speech demos are available at\nhttps://t1235-ch.github.io/pgpt/."}
{"id": "2508.04283", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04283", "abs": "https://arxiv.org/abs/2508.04283", "authors": ["Chengwei Ouyang", "Kexin Fei", "Haoshuai Zhou", "Congxi Lu", "Linkai Li"], "title": "A Multi-stage Low-latency Enhancement System for Hearing Aids", "comment": "2 pages, 1 figure, 1 table. accepted to ICASSP 2023", "summary": "This paper proposes an end-to-end system for the ICASSP 2023 Clarity\nChallenge. In this work, we introduce four major novelties: (1) a novel\nmulti-stage system in both the magnitude and complex domains to better utilize\nphase information; (2) an asymmetric window pair to achieve higher frequency\nresolution with the 5ms latency constraint; (3) the integration of head\nrotation information and the mixture signals to achieve better enhancement; (4)\na post-processing module that achieves higher hearing aid speech perception\nindex (HASPI) scores with the hearing aid amplification stage provided by the\nbaseline system."}
{"id": "2508.04068", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04068", "abs": "https://arxiv.org/abs/2508.04068", "authors": ["Liu Xuanyu", "Gao Shijian", "Liu Boxun", "Cheng Xiang", "Yang Liuqing"], "title": "WiFo-CF: Wireless Foundation Model for CSI Feedback", "comment": null, "summary": "Deep learning-based channel state information (CSI) feedback schemes\ndemonstrate strong compression capabilities but are typically constrained to\nfixed system configurations, limiting their generalization and flexibility. To\naddress this challenge, WiFo-CF, a novel wireless foundation model tailored for\nCSI feedback, is proposed, uniquely accommodating heterogeneous configurations\nsuch as varying channel dimensions, feedback rates, and data distributions\nwithin a unified framework through its key innovations: (1) a multi-user,\nmulti-rate self-supervised pre-training strategy; and (2) a Mixture of Shared\nand Routed Expert (S-R MoE) architecture. Supporting the large-scale\npre-training of WiFo-CF is the first heterogeneous channel feedback dataset,\nwhose diverse patterns enable the model to achieve superior performance on both\nin-distribution and out-of-distribution data across simulated and real-world\nscenarios. Furthermore, the learned representations effectively facilitate\nadaptation to downstream tasks such as CSI-based indoor localization,\nvalidating WiFo-CF's scalability and deployment potential."}
{"id": "2508.04143", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04143", "abs": "https://arxiv.org/abs/2508.04143", "authors": ["Xi Xuan", "Yang Xiao", "Rohan Kumar Das", "Tomi Kinnunen"], "title": "Multilingual Source Tracing of Speech Deepfakes: A First Benchmark", "comment": "Accepted at Interspeech SPSC 2025 - 5th Symposium on Security and\n  Privacy in Speech Communication (Oral)", "summary": "Recent progress in generative AI has made it increasingly easy to create\nnatural-sounding deepfake speech from just a few seconds of audio. While these\ntools support helpful applications, they also raise serious concerns by making\nit possible to generate convincing fake speech in many languages. Current\nresearch has largely focused on detecting fake speech, but little attention has\nbeen given to tracing the source models used to generate it. This paper\nintroduces the first benchmark for multilingual speech deepfake source tracing,\ncovering both mono- and cross-lingual scenarios. We comparatively investigate\nDSP- and SSL-based modeling; examine how SSL representations fine-tuned on\ndifferent languages impact cross-lingual generalization performance; and\nevaluate generalization to unseen languages and speakers. Our findings offer\nthe first comprehensive insights into the challenges of identifying speech\ngeneration models when training and inference languages differ. The dataset,\nprotocol and code are available at\nhttps://github.com/xuanxixi/Multilingual-Source-Tracing."}
{"id": "2508.04333", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04333", "abs": "https://arxiv.org/abs/2508.04333", "authors": ["Gyeong-Tae Lee"], "title": "Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots", "comment": "200 pages", "summary": "Humanoid robots require simultaneous sound event type and direction\nestimation for situational awareness, but conventional two-channel input\nstruggles with elevation estimation and front-back confusion. This paper\nproposes a binaural sound event localization and detection (BiSELD) neural\nnetwork to address these challenges. BiSELDnet learns time-frequency patterns\nand head-related transfer function (HRTF) localization cues from binaural input\nfeatures. A novel eight-channel binaural time-frequency feature (BTFF) is\nintroduced, comprising left/right mel-spectrograms, V-maps, an interaural time\ndifference (ITD) map (below 1.5 kHz), an interaural level difference (ILD) map\n(above 5 kHz with front-back asymmetry), and spectral cue (SC) maps (above 5\nkHz for elevation). The effectiveness of BTFF was confirmed across\nomnidirectional, horizontal, and median planes. BiSELDnets, particularly one\nbased on the efficient Trinity module, were implemented to output time series\nof direction vectors for each sound event class, enabling simultaneous\ndetection and localization. Vector activation map (VAM) visualization was\nproposed to analyze network learning, confirming BiSELDnet's focus on the N1\nnotch frequency for elevation estimation. Comparative evaluations under urban\nbackground noise conditions demonstrated that the proposed BiSELD model\nsignificantly outperforms state-of-the-art (SOTA) SELD models with binaural\ninput."}
{"id": "2508.04075", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04075", "abs": "https://arxiv.org/abs/2508.04075", "authors": ["Yujie Liu", "Yong Liang Guan", "David González G.", "Halim Yanikomeroglu"], "title": "DFT-s-OFDM with Chirp Modulation", "comment": "This paper has been accepted by IEEE PIMRC 2025", "summary": "In this paper, a new waveform called discrete Fourier transform spread\northogonal frequency division multiplexing with chirp modulation\n(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.\nThe information bits are conveyed by not only Q-ary constellation symbols but\nalso the starting frequency of chirp signal. It could maintain the benefits\nprovided by the chirped discrete Fourier transform spread orthogonal frequency\ndivision multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio\n(PAPR), full frequency diversity exploitation, etc. Simulation results confirm\nthat the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while\nkeeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In\naddition, when maintaining the same spectral efficiency, the proposed\nDFT-s-OFDM-CM with the splitting of information bits into two streams enables\nthe use of lower-order constellation modulation and offers greater resilience\nto noise, resulting in a lower BER than the chirped DFT-s-OFDM."}
{"id": "2508.04230", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04230", "abs": "https://arxiv.org/abs/2508.04230", "authors": ["Yacouba Kaloga", "Ina Kodrasi"], "title": "Towards interpretable emotion recognition: Identifying key features with machine learning", "comment": null, "summary": "Unsupervised methods, such as wav2vec2 and HuBERT, have achieved\nstate-of-the-art performance in audio tasks, leading to a shift away from\nresearch on interpretable features. However, the lack of interpretability in\nthese methods limits their applicability in critical domains like medicine,\nwhere understanding feature relevance is crucial. To better understand the\nfeatures of unsupervised models, it remains critical to identify the\ninterpretable features relevant to a given task. In this work, we focus on\nemotion recognition and use machine learning algorithms to identify and\ngeneralize the most important interpretable features for this task. While\nprevious studies have explored feature relevance in emotion recognition, they\nare often constrained by narrow contexts and present inconsistent findings. Our\napproach aims to overcome these limitations, providing a broader and more\nrobust framework for identifying the most important interpretable features."}
{"id": "2508.04425", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04425", "abs": "https://arxiv.org/abs/2508.04425", "authors": ["Yexin Yang", "Shuai Wang", "Xun Gong", "Yanmin Qian", "Kai Yu"], "title": "Text adaptation for speaker verification with speaker-text factorized embeddings", "comment": "ICASSP 2020", "summary": "Text mismatch between pre-collected data, either training data or enrollment\ndata, and the actual test data can significantly hurt text-dependent speaker\nverification (SV) system performance. Although this problem can be solved by\ncarefully collecting data with the target speech content, such data collection\ncould be costly and inflexible. In this paper, we propose a novel text\nadaptation framework to address the text mismatch issue. Here, a speaker-text\nfactorization network is proposed to factorize the input speech into speaker\nembeddings and text embeddings and then integrate them into a single\nrepresentation in the later stage. Given a small amount of speaker-independent\nadaptation utterances, text embeddings of target speech content can be\nextracted and used to adapt the text-independent speaker embeddings to\ntext-customized speaker embeddings. Experiments on RSR2015 show that text\nadaptation can significantly improve the performance of text mismatch\nconditions."}
{"id": "2508.04128", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04128", "abs": "https://arxiv.org/abs/2508.04128", "authors": ["Di Wu", "Yifei Jia", "Siyuan Li", "Shiqi Zhao", "Jie Yang", "Mohamad Sawan"], "title": "Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving", "comment": null, "summary": "Neurophysiological decoding, fundamental to advancing brain-computer\ninterface (BCI) technologies, has significantly benefited from recent advances\nin deep learning. However, existing decoding approaches largely remain\nconstrained to single-task scenarios and individual subjects, limiting their\nbroader applicability and generalizability. Efforts towards creating\nlarge-scale neurophysiological foundation models have shown promise, but\ncontinue to struggle with significant challenges due to pervasive data\nheterogeneity across subjects and decoding tasks. Simply increasing model\nparameters and dataset size without explicitly addressing this heterogeneity\nfails to replicate the scaling successes seen in natural language processing.\nHere, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),\na general-purpose decoding framework explicitly designed to manage the\nubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE\nincorporates a brain-regional-temporal embedding mechanism combined with a\nmixture-of-experts approach, assigning neural signals from distinct brain\nregions to specialized regional experts on a unified embedding basis, thus\nexplicitly resolving both structural and functional heterogeneity.\nAdditionally, our region-masked autoencoding pre-training strategy further\nenhances representational consistency among subjects, complemented by a\ntask-disentangled information aggregation method tailored to effectively handle\ntask-specific neural variations. Evaluations conducted on intracranial\nrecordings from 11 subjects across five diverse tasks, including complex\nlanguage decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE\nsurpasses prior art and exhibits robust generalization for zero-shot decoding\non unseen subjects."}
{"id": "2508.04283", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04283", "abs": "https://arxiv.org/abs/2508.04283", "authors": ["Chengwei Ouyang", "Kexin Fei", "Haoshuai Zhou", "Congxi Lu", "Linkai Li"], "title": "A Multi-stage Low-latency Enhancement System for Hearing Aids", "comment": "2 pages, 1 figure, 1 table. accepted to ICASSP 2023", "summary": "This paper proposes an end-to-end system for the ICASSP 2023 Clarity\nChallenge. In this work, we introduce four major novelties: (1) a novel\nmulti-stage system in both the magnitude and complex domains to better utilize\nphase information; (2) an asymmetric window pair to achieve higher frequency\nresolution with the 5ms latency constraint; (3) the integration of head\nrotation information and the mixture signals to achieve better enhancement; (4)\na post-processing module that achieves higher hearing aid speech perception\nindex (HASPI) scores with the hearing aid amplification stage provided by the\nbaseline system."}
{"id": "2508.04430", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04430", "abs": "https://arxiv.org/abs/2508.04430", "authors": ["Yash Bhake", "Ankit Anand", "Preeti Rao"], "title": "Melodic and Metrical Elements of Expressiveness in Hindustani Vocal Music", "comment": "To appear in the proceedings of the 26th International Society for\n  Music Information Retrieval Conference (ISMIR), Daejeon Korea, 2025", "summary": "This paper presents an attempt to study the aesthetics of North Indian Khayal\nmusic with reference to the flexibility exercised by artists in performing\npopular compositions. We study expressive timing and pitch variations of the\ngiven lyrical content within and across performances and propose computational\nrepresentations that can discriminate between different performances of the\nsame song in terms of expression. We present the necessary audio processing and\nannotation procedures, and discuss our observations and insights from the\nanalysis of a dataset of two songs in two ragas each rendered by ten prominent\nartists."}
{"id": "2508.04144", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04144", "abs": "https://arxiv.org/abs/2508.04144", "authors": ["Hossein Maleki", "Carles Diaz-Vilor", "Ali Pezeshki", "Vahid Tarokh", "Hamid Jafarkhani"], "title": "Dual-Function Radar-Communication Beamforming with Outage Probability Metric", "comment": null, "summary": "The integrated design of communication and sensing may offer a potential\nsolution to address spectrum congestion. In this work, we develop a beamforming\nmethod for a dual-function radar-communication system, where the transmit\nsignal is used for both radar surveillance and communication with multiple\ndownlink users, despite imperfect channel state information (CSI). We focus on\ntwo scenarios of interest: radar-centric and communication-centric. In the\nradar-centric scenario, the primary goal is to optimize radar performance while\nattaining acceptable communication performance. To this end, we minimize a\nweighted sum of the mean-squared error in achieving a desired beampattern and a\nmean-squared cross correlation of the radar returns from directions of interest\n(DOI). We also seek to ensure that the probability of outage for the\ncommunication users remains below a desired threshold. In the\ncommunication-centric scenario, our main objective is to minimize the maximum\nprobability of outage among the communication users while keeping the\naforementioned radar metrics below a desired threshold. Both optimization\nproblems are stochastic and untractable. We first take advantage of central\nlimit theorem to obtain deterministic non-convex problems and then consider\nrelaxations of these problems in the form of semidefinite programs with rank-1\nconstraints. We provide numerical experiments demonstrating the effectiveness\nof the proposed designs."}
{"id": "2508.04333", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04333", "abs": "https://arxiv.org/abs/2508.04333", "authors": ["Gyeong-Tae Lee"], "title": "Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots", "comment": "200 pages", "summary": "Humanoid robots require simultaneous sound event type and direction\nestimation for situational awareness, but conventional two-channel input\nstruggles with elevation estimation and front-back confusion. This paper\nproposes a binaural sound event localization and detection (BiSELD) neural\nnetwork to address these challenges. BiSELDnet learns time-frequency patterns\nand head-related transfer function (HRTF) localization cues from binaural input\nfeatures. A novel eight-channel binaural time-frequency feature (BTFF) is\nintroduced, comprising left/right mel-spectrograms, V-maps, an interaural time\ndifference (ITD) map (below 1.5 kHz), an interaural level difference (ILD) map\n(above 5 kHz with front-back asymmetry), and spectral cue (SC) maps (above 5\nkHz for elevation). The effectiveness of BTFF was confirmed across\nomnidirectional, horizontal, and median planes. BiSELDnets, particularly one\nbased on the efficient Trinity module, were implemented to output time series\nof direction vectors for each sound event class, enabling simultaneous\ndetection and localization. Vector activation map (VAM) visualization was\nproposed to analyze network learning, confirming BiSELDnet's focus on the N1\nnotch frequency for elevation estimation. Comparative evaluations under urban\nbackground noise conditions demonstrated that the proposed BiSELD model\nsignificantly outperforms state-of-the-art (SOTA) SELD models with binaural\ninput."}
{"id": "2508.04512", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04512", "abs": "https://arxiv.org/abs/2508.04512", "authors": ["Franziska Braun", "Christopher Witzl", "Andreas Erzigkeit", "Hartmut Lehfeld", "Thomas Hillemacher", "Tobias Bocklet", "Korbinian Riedhammer"], "title": "Pitfalls and Limits in Automatic Dementia Assessment", "comment": "Accepted at INTERSPEECH 2025", "summary": "Current work on speech-based dementia assessment focuses on either feature\nextraction to predict assessment scales, or on the automation of existing test\nprocedures. Most research uses public data unquestioningly and rarely performs\na detailed error analysis, focusing primarily on numerical performance. We\nperform an in-depth analysis of an automated standardized dementia assessment,\nthe Syndrom-Kurz-Test. We find that while there is a high overall correlation\nwith human annotators, due to certain artifacts, we observe high correlations\nfor the severely impaired individuals, which is less true for the healthy or\nmildly impaired ones. Speech production decreases with cognitive decline,\nleading to overoptimistic correlations when test scoring relies on word naming.\nDepending on the test design, fallback handling introduces further biases that\nfavor certain groups. These pitfalls remain independent of group distributions\nin datasets and require differentiated analysis of target groups."}
{"id": "2508.04169", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04169", "abs": "https://arxiv.org/abs/2508.04169", "authors": ["Ruiyun Zhang", "Zhaolin Wang", "Zhiqing Wei", "Yuanwei Liu", "Zehui Xiong", "Zhiyong Feng"], "title": "Subspace Fitting Approach for Wideband Near-Field Localization", "comment": null, "summary": "Two subspace fitting approaches are proposed for wideband near-field\nlocalization. Unlike in conventional far-field systems, where distance and\nangle can be estimated separately, spherical wave propagation in near-field\nsystems couples these parameters. We therefore derive a frequency-domain\nnear-field signal model for multi-target wideband systems and develop a\nsubspace fitting-based MUSIC method that jointly estimates distance and angle.\nTo reduce complexity, a Fresnel approximation MUSIC algorithm is further\nintroduced to decouple the distance and angle parameters. Numerical results\nverify the effectiveness of both proposed approaches."}
{"id": "2508.04425", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04425", "abs": "https://arxiv.org/abs/2508.04425", "authors": ["Yexin Yang", "Shuai Wang", "Xun Gong", "Yanmin Qian", "Kai Yu"], "title": "Text adaptation for speaker verification with speaker-text factorized embeddings", "comment": "ICASSP 2020", "summary": "Text mismatch between pre-collected data, either training data or enrollment\ndata, and the actual test data can significantly hurt text-dependent speaker\nverification (SV) system performance. Although this problem can be solved by\ncarefully collecting data with the target speech content, such data collection\ncould be costly and inflexible. In this paper, we propose a novel text\nadaptation framework to address the text mismatch issue. Here, a speaker-text\nfactorization network is proposed to factorize the input speech into speaker\nembeddings and text embeddings and then integrate them into a single\nrepresentation in the later stage. Given a small amount of speaker-independent\nadaptation utterances, text embeddings of target speech content can be\nextracted and used to adapt the text-independent speaker embeddings to\ntext-customized speaker embeddings. Experiments on RSR2015 show that text\nadaptation can significantly improve the performance of text mismatch\nconditions."}
{"id": "2508.04585", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04585", "abs": "https://arxiv.org/abs/2508.04585", "authors": ["Yifan Hu", "Rui Liu", "Yi Ren", "Xiang Yin", "Haizhou Li"], "title": "UniTalker: Conversational Speech-Visual Synthesis", "comment": "15 pages, 8 figures", "summary": "Conversational Speech Synthesis (CSS) is a key task in the user-agent\ninteraction area, aiming to generate more expressive and empathetic speech for\nusers. However, it is well-known that \"listening\" and \"eye contact\" play\ncrucial roles in conveying emotions during real-world interpersonal\ncommunication. Existing CSS research is limited to perceiving only text and\nspeech within the dialogue context, which restricts its effectiveness.\nMoreover, speech-only responses further constrain the interactive experience.\nTo address these limitations, we introduce a Conversational Speech-Visual\nSynthesis (CSVS) task as an extension of traditional CSS. By leveraging\nmultimodal dialogue context, it provides users with coherent audiovisual\nresponses. To this end, we develop a CSVS system named UniTalker, which is a\nunified model that seamlessly integrates multimodal perception and multimodal\nrendering capabilities. Specifically, it leverages a large-scale language model\nto comprehensively understand multimodal cues in the dialogue context,\nincluding speaker, text, speech, and the talking-face animations. After that,\nit employs multi-task sequence prediction to first infer the target utterance's\nemotion and then generate empathetic speech and natural talking-face\nanimations. To ensure that the generated speech-visual content remains\nconsistent in terms of emotion, content, and duration, we introduce three key\noptimizations: 1) Designing a specialized neural landmark codec to tokenize and\nreconstruct facial expression sequences. 2) Proposing a bimodal speech-visual\nhard alignment decoding strategy. 3) Applying emotion-guided rendering during\nthe generation stage. Comprehensive objective and subjective experiments\ndemonstrate that our model synthesizes more empathetic speech and provides\nusers with more natural and emotionally consistent talking-face animations."}
{"id": "2508.04185", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04185", "abs": "https://arxiv.org/abs/2508.04185", "authors": ["Evangelos Koutsonas", "Xiaonan Mu", "Nan Qi", "Stylianos Trevlakis", "Theodoros A. Tsiftsis", "Alexandros-Apostolos A. Boulogeorgos"], "title": "Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems", "comment": null, "summary": "Integration of RIS in radio access networks requires signaling between edge\nunits and the RIS microcontroller (MC). Unfortunately, in several practical\nscenarios, the signaling latency is higher than the communication channel\ncoherence time, which causes outdated signaling at the RIS. To counterbalance\nthis, we introduce a simultaneous information and control signaling (SICS)\nprotocol that enables operation adaptation through wireless control signal\ntransmission. SICS assumes that the MC is equipped with a single antenna that\noperates at the same frequency as the RIS. RIS operates in simultaneous\ntransmission and reflection (STAR) mode, and the source employs non-orthogonal\nmultiple access (NOMA) to superposition the information signal to the control\nsignal. To maximize the achievable user data rate while ensuring the MC's\nability to decode the control signal, we formulate and solve the corresponding\noptimization problem that returns RIS's reflection and transmission\ncoefficients as well as the superposition coefficients of the NOMA scheme. Our\nresults reveal the robustness of the SICS approach."}
{"id": "2508.04430", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04430", "abs": "https://arxiv.org/abs/2508.04430", "authors": ["Yash Bhake", "Ankit Anand", "Preeti Rao"], "title": "Melodic and Metrical Elements of Expressiveness in Hindustani Vocal Music", "comment": "To appear in the proceedings of the 26th International Society for\n  Music Information Retrieval Conference (ISMIR), Daejeon Korea, 2025", "summary": "This paper presents an attempt to study the aesthetics of North Indian Khayal\nmusic with reference to the flexibility exercised by artists in performing\npopular compositions. We study expressive timing and pitch variations of the\ngiven lyrical content within and across performances and propose computational\nrepresentations that can discriminate between different performances of the\nsame song in terms of expression. We present the necessary audio processing and\nannotation procedures, and discuss our observations and insights from the\nanalysis of a dataset of two songs in two ragas each rendered by ten prominent\nartists."}
{"id": "2508.03780", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03780", "abs": "https://arxiv.org/abs/2508.03780", "authors": ["Katharina Hoedt", "Arthur Flexer", "Gerhard Widmer"], "title": "Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition", "comment": "8 pages, published in Proceedings of the 22nd Sound and Music\n  Computing Conference 2025 (SMC-25)", "summary": "One of the desired key properties of deep learning models is the ability to\ngeneralise to unseen samples. When provided with new samples that are\n(perceptually) similar to one or more training samples, deep learning models\nare expected to produce correspondingly similar outputs. Models that succeed in\npredicting similar outputs for similar inputs are often called robust. Deep\nlearning models, on the other hand, have been shown to be highly vulnerable to\nminor (adversarial) perturbations of the input, which manage to drastically\nchange a model's output and simultaneously expose its reliance on spurious\ncorrelations. In this work, we investigate whether inherently interpretable\ndeep models, i.e., deep models that were designed to focus more on meaningful\nand interpretable features, are more robust to irrelevant perturbations in the\ndata, compared to their black-box counterparts. We test our hypothesis by\ncomparing the robustness of an interpretable and a black-box music emotion\nrecognition (MER) model when challenged with adversarial examples. Furthermore,\nwe include an adversarially trained model, which is optimised to be more\nrobust, in the comparison. Our results indicate that inherently more\ninterpretable models can indeed be more robust than their black-box\ncounterparts, and achieve similar levels of robustness as adversarially trained\nmodels, at lower computational cost."}
{"id": "2508.04214", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.04214", "abs": "https://arxiv.org/abs/2508.04214", "authors": ["Yasaman Khorsandmanesh", "Emil Björnson", "Joakim Jaldén", "Bengt Lindoff"], "title": "Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems", "comment": "This paper will be presented in PIMRC 2025", "summary": "This paper considers a millimeter-wave wideband point-to-point MIMO system\nwith fully digital transceivers at the base station and the user equipment\n(UE), focusing on mobile UE scenarios. A main challenge when building a digital\nUE combining is the large volume of baseband samples to handle. To mitigate\ncomputational and hardware complexity, we propose a novel two-stage digital\ncombining scheme at the UE. The first stage reduces the $N_{\\text{r}}$ received\nsignals to $N_{\\text{c}}$ streams before baseband processing, leveraging\nchannel geometry for dimension reduction and updating at the beam coherence\ntime, which is longer than the channel coherence time of the small-scale\nfading. By contrast, the second-stage combining is updated per fading\nrealization. We develop a pilot-based channel estimation framework for this\nhardware setup based on maximum likelihoodestimation in both uplink and\ndownlink. Digital precoding and combining designs are proposed, and a spectral\nefficiency expression that incorporates imperfect channel knowledge is derived.\nThe numerical results demonstrate that the proposed approach outperforms hybrid\nbeamforming, showcasing the attractiveness of using two-stage fully digital\ntransceivers in future systems."}
{"id": "2508.03983", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.03983", "abs": "https://arxiv.org/abs/2508.03983", "authors": ["Heinrich Dinkel", "Gang Li", "Jizhong Liu", "Jian Luan", "Yadong Niu", "Xingwei Sun", "Tianzi Wang", "Qiyang Xiao", "Junbo Zhang", "Jiahao Zhou"], "title": "MiDashengLM: Efficient Audio Understanding with General Audio Captions", "comment": null, "summary": "Current approaches for large audio language models (LALMs) often rely on\nclosed data sources or proprietary models, limiting their generalization and\naccessibility. This paper introduces MiDashengLM, a novel open audio-language\nmodel designed for efficient and comprehensive audio understanding through the\nuse of general audio captions using our novel ACAVCaps training dataset.\nMiDashengLM exclusively relies on publicly available pretraining and supervised\nfine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At\nits core, MiDashengLM integrates Dasheng, an open-source audio encoder,\nspecifically engineered to process diverse auditory information effectively.\nUnlike previous works primarily focused on Automatic Speech Recognition (ASR)\nbased audio-text alignment, our strategy centers on general audio captions,\nfusing speech, sound and music information into one textual representation,\nenabling a holistic textual representation of complex audio scenes. Lastly,\nMiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT)\nand up to 20x higher throughput than comparable models. Checkpoints are\navailable online at https://huggingface.co/mispeech/midashenglm-7b and\nhttps://github.com/xiaomi-research/dasheng-lm."}
{"id": "2508.04222", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04222", "abs": "https://arxiv.org/abs/2508.04222", "authors": ["Thibaut Ceulemans", "Cel Thys", "Robbert Beerten", "Zhuangzhuang Cui", "Sofie Pollin"], "title": "Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP", "comment": "6 pages, 4 figures. Accepted for presentation at the 36th IEEE\n  International Symposium on Personal, Indoor and Mobile Radio Communications\n  (PIMRC 2025), Istanbul, T\\\"urkiye", "summary": "This work focuses on channel estimation in extremely large aperture array\n(ELAA) systems, where near-field propagation and spatial non-stationarity\nintroduce complexities that hinder the effectiveness of traditional estimation\ntechniques. A physics-based hybrid channel model is developed, incorporating\nnon-binary visibility region (VR) masks to simulate diffraction-induced power\nvariations across the antenna array. To address the estimation challenges posed\nby these channel conditions, a novel algorithm is proposed:\nVisibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching\nPursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework\nby integrating VR estimation through a hidden Markov model (HMM), using a novel\nemission formulation and Viterbi decoding. This allows the algorithm to\nadaptively mask steering vectors and account for spatial non-stationarity at\nthe antenna level. Simulation results demonstrate that the proposed method\nenhances estimation accuracy compared to existing techniques, particularly in\nlow-SNR and sparse scenarios, while maintaining a low computational complexity.\nThe algorithm presents robustness across a range of design parameters and\nchannel conditions, offering a practical solution for ELAA systems."}
{"id": "2508.04096", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04096", "abs": "https://arxiv.org/abs/2508.04096", "authors": ["Bingshen Mu", "Yiwen Shao", "Kun Wei", "Dong Yu", "Lei Xie"], "title": "Efficient Scaling for LLM-based ASR", "comment": "Accepted by ASRU 2025", "summary": "Large language model (LLM)-based automatic speech recognition (ASR) achieves\nstrong performance but often incurs high computational costs. This work\ninvestigates how to obtain the best LLM-ASR performance efficiently. Through\ncomprehensive and controlled experiments, we find that pretraining the speech\nencoder before integrating it with the LLM leads to significantly better\nscaling efficiency than the standard practice of joint post-training of\nLLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training\nstrategy, EFIN: Encoder First Integration. Among all training strategies\nevaluated, EFIN consistently delivers better performance (relative to 21.1%\nCERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore,\nwe derive a scaling law that approximates ASR error rates as a computation\nfunction, providing practical guidance for LLM-ASR scaling."}
{"id": "2508.04223", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04223", "abs": "https://arxiv.org/abs/2508.04223", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Shuai Liu", "Hongyang Du", "Geyong Min"], "title": "Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications", "comment": "submitted to IEEE Journal", "summary": "Digital task-oriented semantic communication (ToSC) aims to transmit only\ntask-relevant information, significantly reducing communication overhead.\nExisting ToSC methods typically rely on learned codebooks to encode semantic\nfeatures and map them to constellation symbols. However, these codebooks are\noften sparsely activated, resulting in low spectral efficiency and\nunderutilization of channel capacity. This highlights a key challenge: how to\ndesign a codebook that not only supports task-specific inference but also\napproaches the theoretical limits of channel capacity. To address this\nchallenge, we construct a spectral efficiency-aware codebook design framework\nthat explicitly incorporates the codebook activation probability into the\noptimization process. Beyond maximizing task performance, we introduce the\nWasserstein (WS) distance as a regularization metric to minimize the gap\nbetween the learned activation distribution and the optimal channel input\ndistribution. Furthermore, we reinterpret WS theory from a generative\nperspective to align with the semantic nature of ToSC. Combining the above two\naspects, we propose a WS-based adaptive hybrid distribution scheme, termed\nWS-DC, which learns compact, task-driven and channel-aware latent\nrepresentations. Experimental results demonstrate that WS-DC not only\noutperforms existing approaches in inference accuracy but also significantly\nimproves codebook efficiency, offering a promising direction toward\ncapacity-approaching semantic communication systems."}
{"id": "2508.04240", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04240", "abs": "https://arxiv.org/abs/2508.04240", "authors": ["Sitong Chen", "Beiqianyi Li", "Cuilin He", "Dongyang Li", "Mingyang Wu", "Xinke Shen", "Song Wang", "Xuetao Wei", "Xindi Wang", "Haiyan Wu", "Quanying Liu"], "title": "ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening", "comment": null, "summary": "EEG-based neural decoding requires large-scale benchmark datasets. Paired\nbrain-language data across speaking, listening, and reading modalities are\nessential for aligning neural activity with the semantic representation of\nlarge language models (LLMs). However, such datasets are rare, especially for\nnon-English languages. Here, we present ChineseEEG-2, a high-density EEG\ndataset designed for benchmarking neural decoding models under real-world\nlanguage tasks. Building on our previous ChineseEEG dataset, which focused on\nsilent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and\nPassive Listening (PL), using the same Chinese corpus. EEG and audio were\nsimultaneously recorded from four participants during ~10.7 hours of reading\naloud. These recordings were then played to eight other participants,\ncollecting ~21.6 hours of EEG during listening. This setup enables speech\ntemporal and semantic alignment across the RA and PL modalities. ChineseEEG-2\nincludes EEG signals, precise audio, aligned semantic embeddings from\npre-trained language models, and task labels. Together with ChineseEEG, this\ndataset supports joint semantic alignment learning across speaking, listening,\nand reading. It enables benchmarking of neural decoding algorithms and promotes\nbrain-LLM alignment under multimodal language tasks, especially in Chinese.\nChineseEEG-2 provides a benchmark dataset for next-generation neural semantic\ndecoding."}
{"id": "2508.04253", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04253", "abs": "https://arxiv.org/abs/2508.04253", "authors": ["Yiyan Ma", "Bo Ai", "Jinhong Yuan", "Shuangyang Li", "Qingqing Cheng", "Zhenguo Shi", "Weijie Yuan", "Zhiqiang Wei", "Akram Shafie", "Guoyu Ma", "Yunlong Lu", "Mi Yang", "Zhangdui Zhong"], "title": "Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond", "comment": null, "summary": "High-mobility scenarios will be a critical part of 6G systems. Since the\nwidely deployed orthogonal frequency division multiplexing (OFDM) waveform\nsuffers from subcarrier orthogonality loss under severe Doppler spread,\ndelay-Doppler domain multi-carrier (DDMC) modulation systems, such as\northogonal time frequency space (OTFS), have been extensively studied. While\nOTFS can exploit time-frequency (TF) domain channel diversity, it faces\nchallenges including high receiver complexity and inflexible TF resource\nallocation, making OFDM still the most promising waveform for 6G. In this\narticle, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme\nto enhance OFDM performance based on DDMC research insights. First, we design a\nDD-a-OFDM system structure, retaining the classical OFDM transceiver while\nincorporating DD domain channel estimation and TF domain equalization. Second,\nwe detail DD domain channel estimation using discrete TF pilots and prove that\nTF domain inter-carrier interference (ICI) could be transformed into DD domain\nGaussian interference. Third, we derive closed-form Cram\\'{e}r-Rao lower bounds\n(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood\n(ML) and peak detection-based channel estimators, along with a corresponding TF\ndomain equalizer. Numerical results verify the proposed design, showing that\nDD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and\noutperforms OTFS in channel estimation accuracy with lower pilot overhead."}
{"id": "2508.04291", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.04291", "abs": "https://arxiv.org/abs/2508.04291", "authors": ["Anbang Zhang", "Shuaishuai Guo", "Chenyuan Feng", "Hongyang Du", "Haojin Li", "Chen Sun", "Haijun Zhang"], "title": "Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication", "comment": "submitted to IEEE Journal", "summary": "Discrete representation has emerged as a powerful tool in task-oriented\nsemantic communication (ToSC), offering compact, interpretable, and efficient\nrepresentations well-suited for low-power edge intelligence scenarios. Its\ninherent digital nature aligns seamlessly with hardware-friendly deployment and\nrobust storage/transmission protocols. However, despite its strengths, current\nToSC frameworks often decouple semantic-aware discrete mapping from the\nunderlying channel characteristics and task demands. This mismatch leads to\nsuboptimal communication performance, degraded task utility, and limited\ngeneralization under variable wireless conditions. Moreover, conventional\ndesigns frequently overlook channel-awareness in codebook construction,\nrestricting the effectiveness of semantic symbol selection under constrained\nresources. To address these limitations, this paper proposes a channel-aware\ndiscrete semantic coding framework tailored for low-power edge networks.\nLeveraging a Wasserstein-regularized objective, our approach aligns discrete\ncode activations with optimal input distributions, thereby improving semantic\nfidelity, robustness, and task accuracy. Extensive experiments on the inference\ntasks across diverse signal-to-noise ratio (SNR) regimes show that our method\nachieves notable gains in accuracy and communication efficiency. This work\nprovides new insights into integrating discrete semantics and channel\noptimization, paving the way for the widespread adoption of semantic\ncommunication in future digital infrastructures."}
{"id": "2508.04322", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04322", "abs": "https://arxiv.org/abs/2508.04322", "authors": ["Ruopeng Xu", "Zhaohui Yang", "Zhaoyang Zhang", "Mohammad Shikh-Bahaei", "Kaibin Huang", "Dusit Niyato"], "title": "Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications", "comment": null, "summary": "In this paper, we propose an energy efficient wireless communication system\nbased on fluid antenna relay (FAR) to solve the problem of non-line-of-sight\n(NLoS) links caused by blockages with considering the physical properties.\nDriven by the demand for the sixth generation (6G) communication, fluid antenna\nsystems (FASs) have become a key technology due to their flexibility in\ndynamically adjusting antenna positions. Existing research on FAS primarily\nfocuses on line-of-sight (LoS) communication scenarios, and neglects the\nsituations where only NLoS links exist. To address the issues posted by NLoS\ncommunication, we design an FAR-assisted communication system combined with\namplify-and-forward (AF) protocol. In order to alleviate the high energy\nconsumption introduced by AF protocol while ensuring communication quality, we\nformulate an energy efficiency (EE) maximization problem. By optimizing the\npositions of the fluid antennas (FAs) on both sides of the FAR, we achieve\ncontrollable phase shifts of the signals transmitting through the blockage\nwhich causes the NLoS link. Besides, we establish a channel model that jointly\nconsiders the blockage-through matrix, large-scale fading, and small-scale\nfading. To maximize the EE of the system, we jointly optimize the FAR position,\nFA positions, power control, and beamforming design under given constraints,\nand propose an iterative algorithm to solve this formulated optimization\nproblem. Simulation results show that the proposed algorithm outperforms the\ntraditional schemes in terms of EE, achieving up to $23.39\\%$ and $39.94\\%$\nhigher EE than the conventional reconfigurable intelligent surface (RIS) scheme\nand traditional AF relay scheme, respectively."}
{"id": "2508.04331", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04331", "abs": "https://arxiv.org/abs/2508.04331", "authors": ["Mohamadreza Delbari", "Qikai Zhou", "Robin Neuder", "Alejandro Jiménez-Sáez", "Vahid Jamali"], "title": "Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination", "comment": "arXiv admin note: text overlap with arXiv:2411.12342", "summary": "Liquid crystal (LC) technology provides a low-power and scalable approach to\nimplement a reconfigurable intelligent surface (RIS). However, the LC-based\nRIS's phase-shift response is inherently frequency-dependent, which can lead to\nperformance degradation if not properly addressed. This issue becomes\nespecially critical in secure communication systems, where such variations may\nresult in considerable information leakage. To avoid the need for full channel\nstate information (CSI) acquisition and frequent RIS reconfiguration, we design\nRIS for a wideband orthogonal frequency division multiplexing (OFDM) system to\nilluminate a desired area containing legitimate users while avoiding leakage to\nregions where potential eavesdroppers may be located. Our simulation results\ndemonstrate that the proposed algorithm improves the secrecy rate compared to\nmethods that neglect frequency-dependent effects. In the considered setup, the\nproposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz\nbandwidth when the center frequency is 60 GHz."}
{"id": "2508.04570", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04570", "abs": "https://arxiv.org/abs/2508.04570", "authors": ["A. Tarik Leblebici", "Sumeyra Hassan", "Erdal Panayirci", "H. Vincent Poor"], "title": "Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming", "comment": "11 Pages, 13 Figures, submitted to Elsevier Physical Communication\n  Journal on 17th June 2025", "summary": "This paper proposes a joint communication and indoor positioning (JCP) system\nbased on visible light communication (VLC) designed for high-precision indoor\nenvironments. The framework supports 2D and 3D positioning using received\nsignal strength (RSS) from pilot transmissions, enhanced by the radical axis\ntheorem to improve accuracy under measurement uncertainties. Communication is\nachieved using spatial modulation (SM) with M-ary pulse amplitude modulation\n(PAM), where data is conveyed through the modulation symbol and the active\nlight-emitting diode (LED) index, improving spectral efficiency while\nmaintaining low complexity. A pilot-aided least squares (LS) estimator is\nemployed for joint channel and dimming coefficient estimation, enabling robust\nsymbol detection in multipath environments characterized by both line-of-sight\n(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician\nfading. The proposed system incorporates a dimming control mechanism to meet\nlighting requirements while maintaining reliable communication and positioning\nperformance. Simulation results demonstrate sub-centimeter localization\naccuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below\n10^{-6} for low-order PAM schemes. Additionally, comparative analysis across\nuser locations reveals that positioning and communication performance improve\nsignificantly near the geometric center of the LED layout. These findings\nvalidate the effectiveness of the proposed system for future 6G indoor networks\nrequiring integrated localization and communication under practical channel\nconditions."}

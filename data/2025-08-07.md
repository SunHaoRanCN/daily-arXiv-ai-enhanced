<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [eess.AS](#eess.AS) [Total: 10]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: 论文介绍了ETRI Lifelog Dataset 2024，通过智能设备被动收集数据，结合主观报告，旨在探索人类日常生活的模式和健康状态。


<details>
  <summary>Details</summary>
Motivation: 提升对人类健康和福祉的理解，需要准确捕捉个体日常的生理和心理状态。

Method: 使用智能手机、智能手表和睡眠传感器被动收集24小时数据，并结合睡眠前后的主观调查。

Result: 构建了一个全面的生活日志数据集，部分数据已公开供研究使用。

Conclusion: 该数据集为研究人类日常生活模式提供了基础资源，并展示了机器学习在预测睡眠质量和压力方面的潜在应用。

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


### [2] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 该研究提出了一种非侵入性、可解释的机器学习框架，用于通过多模态可穿戴传感器检测自主神经反射异常（AD），显著优于基线模型，并展示了高准确性和临床一致性。


<details>
  <summary>Details</summary>
Motivation: 自主神经反射异常（AD）是一种潜在危及生命的疾病，现有监测方法多为侵入性或依赖主观症状报告，限制了日常应用。因此，开发非侵入性、准确的检测方法至关重要。

Method: 研究使用多模态可穿戴传感器（如ECG、PPG、BioZ等）采集数据，通过BorutaSHAP进行特征选择，并训练模态和设备特定的弱学习器，最终通过堆叠集成元模型进行聚合。

Result: HR和ECG特征最具信息量，集成模型性能最高（Macro F1 = 0.77+/-0.03），HR模态的AUC达到0.93。模型对传感器丢失具有鲁棒性，并与临床事件一致。

Conclusion: 该研究为脊髓损伤患者的个性化、实时监测提供了重要进展，展示了多模态传感器和机器学习在AD检测中的潜力。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


### [3] [Zak-OTFS over CP-OFDM](https://arxiv.org/abs/2508.03906)
*Saif Khan Mohammed,Saurabh Prakash,Muhammad Ubadah,Imran Ali Khan,Ronny Hadani,Shlomo Rakib,Shachar Kons,Yoav Hebron,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: Zak-OTFS调制在高速延迟/多普勒扩展场景中优于CP-OFDM，提出了一种低复杂度的Zak-OTFS over CP-OFDM架构，兼容现有网络。


<details>
  <summary>Details</summary>
Motivation: 解决在现有CP-OFDM调制解调器中支持Zak-OTFS调制的实际挑战，以利用其在高延迟/多普勒扩展场景中的优势。

Method: 通过sinc滤波和时间窗口限制，将Zak-OTFS调制实现为CP-OFDM的低复杂度预编码器，解调器同理。

Result: 提出的Zak-OTFS over CP-OFDM架构兼容现有基础设施，且CP-OFDM是其特例。

Conclusion: Zak-OTFS over CP-OFDM是一种高效且兼容的调制方案，适用于下一代通信系统。

Abstract: Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to
achieve significantly better performance compared to the standardized
Cyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high
delay/Doppler spread scenarios envisaged in next generation communication
systems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)
domain, characterized by two parameters, (i) the pulse period along the delay
axis (``delay period") (Doppler period is related to the delay period), and
(ii) the pulse shaping filter. An important practical challenge is enabling
support for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper
we show that Zak-OTFS modulation with pulse shaping constrained to sinc
filtering (filter bandwidth equal to the communication bandwidth $B$) followed
by time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is
the symbol duration and $T_{cp}$ is the CP duration), can be implemented as a
low-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS
de-modulator with matched filtering constrained to sinc filtering (filter
bandwidth $B$) followed by rectangular time windowing over duration $T$ can be
implemented as a low-complexity post-processing of the CP-OFDM de-modulator
output. This proposed ``Zak-OTFS over CP-OFDM" architecture enables us to
harness the benefits of Zak-OTFS in existing network infrastructure. We also
show that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with
CP-OFDM being a special case when the delay period takes its minimum possible
value equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum
delay period.

</details>


### [4] [Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints](https://arxiv.org/abs/2508.04046)
*Xiao Tong,Lei Lei,Ang Li,A. Lee Swindlehurst,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 论文提出了一种非线性波形优化框架，用于多用户MIMO系统中基于构造干扰的波形设计，解决了现有线性方法的性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于构造干扰的线性预编码方法（如符号级预编码和块级预编码）因严格的功率约束或自由度不足而性能受限。

Method: 提出非线性波形优化框架，引入额外优化变量，最大化传输块中的最小构造干扰度量，并通过改进的ADMM算法高效求解。

Result: 仿真结果表明，所提算法在高阶调制和大块长度下显著优于传统方法。

Conclusion: 非线性波形优化框架和高效算法显著提升了多用户MIMO系统的性能。

Abstract: This paper investigates constructive interference (CI)-based waveform design
for phase shift keying and quadrature amplitude modulation symbols under
relaxed block-level power constraints in multi-user multiple-input
single-output (MU-MIMO) communication systems. Existing linear CI-based
precoding methods, including symbol-level precoding (SLP) and block-level
precoding (BLP), suffer from performance limitations due to strict symbol-level
power budgets or insufficient degrees of freedom over the block. To overcome
these challenges, we propose a nonlinear waveform optimization framework that
introduces additional optimization variables and maximizes the minimum CI
metric across the transmission block. The optimal waveform is derived in closed
form using the function and Karush Kuhn Tucker conditions, and the solution is
explicitly expressed with respect to the dual variables. Moreover, the original
problems are equivalently reformulated as tractable quadratic programming (QP)
problems. To efficiently solve the derived QP problems, we develop an improved
alternating direction method of multipliers (ADMM) algorithm by integrating a
linear-time projection technique, which significantly enhances the
computational efficiency. Simulation results demonstrate that the proposed
algorithms substantially outperform the conventional CI-SLP and CI-BLP
approaches, particularly under high-order modulations and large block lengths.

</details>


### [5] [WiFo-CF: Wireless Foundation Model for CSI Feedback](https://arxiv.org/abs/2508.04068)
*Liu Xuanyu,Gao Shijian,Liu Boxun,Cheng Xiang,Yang Liuqing*

Main category: eess.SP

TL;DR: WiFo-CF是一种新型无线基础模型，通过自监督预训练和专家混合架构，解决了传统CSI反馈方案在异构配置下的局限性，并展示了优异的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的CSI反馈方案在固定系统配置下表现良好，但缺乏对异构配置的适应能力，限制了其通用性和灵活性。

Method: 提出WiFo-CF模型，采用多用户、多速率自监督预训练策略和专家混合架构（S-R MoE），并利用首个异构信道反馈数据集进行大规模预训练。

Result: 模型在模拟和实际场景中均表现出色，支持异构配置，并能有效适应下游任务（如CSI室内定位）。

Conclusion: WiFo-CF具有优异的性能和部署潜力，为CSI反馈提供了灵活且通用的解决方案。

Abstract: Deep learning-based channel state information (CSI) feedback schemes
demonstrate strong compression capabilities but are typically constrained to
fixed system configurations, limiting their generalization and flexibility. To
address this challenge, WiFo-CF, a novel wireless foundation model tailored for
CSI feedback, is proposed, uniquely accommodating heterogeneous configurations
such as varying channel dimensions, feedback rates, and data distributions
within a unified framework through its key innovations: (1) a multi-user,
multi-rate self-supervised pre-training strategy; and (2) a Mixture of Shared
and Routed Expert (S-R MoE) architecture. Supporting the large-scale
pre-training of WiFo-CF is the first heterogeneous channel feedback dataset,
whose diverse patterns enable the model to achieve superior performance on both
in-distribution and out-of-distribution data across simulated and real-world
scenarios. Furthermore, the learned representations effectively facilitate
adaptation to downstream tasks such as CSI-based indoor localization,
validating WiFo-CF's scalability and deployment potential.

</details>


### [6] [DFT-s-OFDM with Chirp Modulation](https://arxiv.org/abs/2508.04075)
*Yujie Liu,Yong Liang Guan,David González G.,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 提出了一种新的波形DFT-s-OFDM-CM，结合了啁啾调制，提高了频谱效率并保持了低PAPR和频率多样性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线通信需要更高的频谱效率和更好的抗噪性能，同时保持低PAPR和频率多样性。

Method: 通过将信息比特分配到Q进制星座符号和啁啾信号的起始频率上，结合DFT-s-OFDM技术。

Result: 仿真结果显示，DFT-s-OFDM-CM在保持类似BER的同时提高了频谱效率，且在相同频谱效率下，通过分信息流降低了BER。

Conclusion: DFT-s-OFDM-CM是一种有前景的波形，适用于下一代无线通信。

Abstract: In this paper, a new waveform called discrete Fourier transform spread
orthogonal frequency division multiplexing with chirp modulation
(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.
The information bits are conveyed by not only Q-ary constellation symbols but
also the starting frequency of chirp signal. It could maintain the benefits
provided by the chirped discrete Fourier transform spread orthogonal frequency
division multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio
(PAPR), full frequency diversity exploitation, etc. Simulation results confirm
that the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while
keeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In
addition, when maintaining the same spectral efficiency, the proposed
DFT-s-OFDM-CM with the splitting of information bits into two streams enables
the use of lower-order constellation modulation and offers greater resilience
to noise, resulting in a lower BER than the chirped DFT-s-OFDM.

</details>


### [7] [Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving](https://arxiv.org/abs/2508.04128)
*Di Wu,Yifei Jia,Siyuan Li,Shiqi Zhao,Jie Yang,Mohamad Sawan*

Main category: eess.SP

TL;DR: Neuro-MoBRE是一种新型的神经生理解码框架，通过区域专家混合和嵌入机制解决数据异质性，在多任务和跨受试者场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经生理解码方法局限于单任务和个体受试者，缺乏通用性和泛化能力，需要解决数据异质性。

Method: Neuro-MoBRE结合脑区-时间嵌入机制和专家混合方法，通过区域掩码自编码预训练和任务解缠信息聚合处理异质性。

Result: 在11名受试者的颅内记录和五项任务中，Neuro-MoBRE优于现有方法，并展示了零样本解码的强泛化能力。

Conclusion: Neuro-MoBRE为解决神经生理数据异质性提供了有效框架，具有广泛的应用潜力。

Abstract: Neurophysiological decoding, fundamental to advancing brain-computer
interface (BCI) technologies, has significantly benefited from recent advances
in deep learning. However, existing decoding approaches largely remain
constrained to single-task scenarios and individual subjects, limiting their
broader applicability and generalizability. Efforts towards creating
large-scale neurophysiological foundation models have shown promise, but
continue to struggle with significant challenges due to pervasive data
heterogeneity across subjects and decoding tasks. Simply increasing model
parameters and dataset size without explicitly addressing this heterogeneity
fails to replicate the scaling successes seen in natural language processing.
Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),
a general-purpose decoding framework explicitly designed to manage the
ubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE
incorporates a brain-regional-temporal embedding mechanism combined with a
mixture-of-experts approach, assigning neural signals from distinct brain
regions to specialized regional experts on a unified embedding basis, thus
explicitly resolving both structural and functional heterogeneity.
Additionally, our region-masked autoencoding pre-training strategy further
enhances representational consistency among subjects, complemented by a
task-disentangled information aggregation method tailored to effectively handle
task-specific neural variations. Evaluations conducted on intracranial
recordings from 11 subjects across five diverse tasks, including complex
language decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE
surpasses prior art and exhibits robust generalization for zero-shot decoding
on unseen subjects.

</details>


### [8] [Dual-Function Radar-Communication Beamforming with Outage Probability Metric](https://arxiv.org/abs/2508.04144)
*Hossein Maleki,Carles Diaz-Vilor,Ali Pezeshki,Vahid Tarokh,Hamid Jafarkhani*

Main category: eess.SP

TL;DR: 提出了一种用于双功能雷达通信系统的波束成形方法，旨在优化雷达或通信性能，同时考虑不完美的信道状态信息。


<details>
  <summary>Details</summary>
Motivation: 解决频谱拥塞问题，通过集成通信和感知设计提高系统效率。

Method: 针对雷达中心和通信中心两种场景，分别优化性能指标，并通过半定规划松弛解决随机不可行问题。

Result: 数值实验验证了所提设计的有效性。

Conclusion: 该方法在双功能系统中实现了雷达和通信性能的平衡，为解决频谱拥塞提供了潜在解决方案。

Abstract: The integrated design of communication and sensing may offer a potential
solution to address spectrum congestion. In this work, we develop a beamforming
method for a dual-function radar-communication system, where the transmit
signal is used for both radar surveillance and communication with multiple
downlink users, despite imperfect channel state information (CSI). We focus on
two scenarios of interest: radar-centric and communication-centric. In the
radar-centric scenario, the primary goal is to optimize radar performance while
attaining acceptable communication performance. To this end, we minimize a
weighted sum of the mean-squared error in achieving a desired beampattern and a
mean-squared cross correlation of the radar returns from directions of interest
(DOI). We also seek to ensure that the probability of outage for the
communication users remains below a desired threshold. In the
communication-centric scenario, our main objective is to minimize the maximum
probability of outage among the communication users while keeping the
aforementioned radar metrics below a desired threshold. Both optimization
problems are stochastic and untractable. We first take advantage of central
limit theorem to obtain deterministic non-convex problems and then consider
relaxations of these problems in the form of semidefinite programs with rank-1
constraints. We provide numerical experiments demonstrating the effectiveness
of the proposed designs.

</details>


### [9] [Subspace Fitting Approach for Wideband Near-Field Localization](https://arxiv.org/abs/2508.04169)
*Ruiyun Zhang,Zhaolin Wang,Zhiqing Wei,Yuanwei Liu,Zehui Xiong,Zhiyong Feng*

Main category: eess.SP

TL;DR: 提出了两种用于宽带近场定位的子空间拟合方法，解决了传统远场系统中距离和角度耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 近场系统中球面波传播导致距离和角度参数耦合，传统方法无法单独估计，需要新的解决方案。

Method: 推导了多目标宽带系统的频域近场信号模型，开发了基于子空间拟合的MUSIC方法，并引入Fresnel近似MUSIC算法降低复杂度。

Result: 数值结果验证了两种方法的有效性。

Conclusion: 提出的方法能有效解决近场定位中距离和角度耦合的问题，且Fresnel近似算法降低了计算复杂度。

Abstract: Two subspace fitting approaches are proposed for wideband near-field
localization. Unlike in conventional far-field systems, where distance and
angle can be estimated separately, spherical wave propagation in near-field
systems couples these parameters. We therefore derive a frequency-domain
near-field signal model for multi-target wideband systems and develop a
subspace fitting-based MUSIC method that jointly estimates distance and angle.
To reduce complexity, a Fresnel approximation MUSIC algorithm is further
introduced to decouple the distance and angle parameters. Numerical results
verify the effectiveness of both proposed approaches.

</details>


### [10] [Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems](https://arxiv.org/abs/2508.04185)
*Evangelos Koutsonas,Xiaonan Mu,Nan Qi,Stylianos Trevlakis,Theodoros A. Tsiftsis,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: 论文提出了一种名为SICS的协议，通过无线控制信号传输解决RIS中信号延迟问题，优化了用户数据速率和RIS的反射/传输系数。


<details>
  <summary>Details</summary>
Motivation: 在RIS与边缘单元的信号传输中，信号延迟可能超过信道相干时间，导致信号过时。为了解决这一问题，需要一种新的协议来适应操作。

Method: 提出SICS协议，利用单天线MC与RIS同频工作，采用STAR模式和NOMA技术叠加信息与控制信号，并通过优化问题确定RIS系数和NOMA叠加系数。

Result: SICS方法表现出鲁棒性，能够有效提升用户数据速率，同时确保MC解码控制信号的能力。

Conclusion: SICS协议为解决RIS信号延迟问题提供了一种有效的解决方案，优化了系统性能。

Abstract: Integration of RIS in radio access networks requires signaling between edge
units and the RIS microcontroller (MC). Unfortunately, in several practical
scenarios, the signaling latency is higher than the communication channel
coherence time, which causes outdated signaling at the RIS. To counterbalance
this, we introduce a simultaneous information and control signaling (SICS)
protocol that enables operation adaptation through wireless control signal
transmission. SICS assumes that the MC is equipped with a single antenna that
operates at the same frequency as the RIS. RIS operates in simultaneous
transmission and reflection (STAR) mode, and the source employs non-orthogonal
multiple access (NOMA) to superposition the information signal to the control
signal. To maximize the achievable user data rate while ensuring the MC's
ability to decode the control signal, we formulate and solve the corresponding
optimization problem that returns RIS's reflection and transmission
coefficients as well as the superposition coefficients of the NOMA scheme. Our
results reveal the robustness of the SICS approach.

</details>


### [11] [Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems](https://arxiv.org/abs/2508.04214)
*Yasaman Khorsandmanesh,Emil Björnson,Joakim Jaldén,Bengt Lindoff*

Main category: eess.SP

TL;DR: 提出了一种针对毫米波宽带点对点MIMO系统的两阶段数字合并方案，以降低计算和硬件复杂度，并在移动用户设备场景中优于混合波束成形。


<details>
  <summary>Details</summary>
Motivation: 解决移动用户设备场景中数字合并处理大量基带样本的计算和硬件复杂度问题。

Method: 采用两阶段数字合并方案：第一阶段利用信道几何特性减少信号维度，第二阶段针对每个衰落实现更新。开发了基于最大似然估计的信道估计框架。

Result: 数值结果表明，该方法优于混合波束成形，展示了全数字收发器的潜力。

Conclusion: 两阶段全数字收发器方案在未来系统中具有吸引力，尤其在移动场景中表现优异。

Abstract: This paper considers a millimeter-wave wideband point-to-point MIMO system
with fully digital transceivers at the base station and the user equipment
(UE), focusing on mobile UE scenarios. A main challenge when building a digital
UE combining is the large volume of baseband samples to handle. To mitigate
computational and hardware complexity, we propose a novel two-stage digital
combining scheme at the UE. The first stage reduces the $N_{\text{r}}$ received
signals to $N_{\text{c}}$ streams before baseband processing, leveraging
channel geometry for dimension reduction and updating at the beam coherence
time, which is longer than the channel coherence time of the small-scale
fading. By contrast, the second-stage combining is updated per fading
realization. We develop a pilot-based channel estimation framework for this
hardware setup based on maximum likelihoodestimation in both uplink and
downlink. Digital precoding and combining designs are proposed, and a spectral
efficiency expression that incorporates imperfect channel knowledge is derived.
The numerical results demonstrate that the proposed approach outperforms hybrid
beamforming, showcasing the attractiveness of using two-stage fully digital
transceivers in future systems.

</details>


### [12] [Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP](https://arxiv.org/abs/2508.04222)
*Thibaut Ceulemans,Cel Thys,Robbert Beerten,Zhuangzhuang Cui,Sofie Pollin*

Main category: eess.SP

TL;DR: 论文提出了一种针对极大规模天线阵列（ELAA）系统的信道估计方法，通过结合物理模型和隐马尔可夫模型（HMM）改进稀疏恢复算法，显著提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法在ELAA系统中因近场传播和空间非平稳性而失效，需要新的解决方案。

Method: 提出VR-HMM-P-SOMP算法，结合非二元可见区域（VR）掩码和HMM，自适应调整导向向量以应对空间非平稳性。

Result: 仿真显示该方法在低信噪比和稀疏场景下优于现有技术，且计算复杂度低。

Conclusion: 该算法为ELAA系统提供了一种高效且鲁棒的信道估计方案。

Abstract: This work focuses on channel estimation in extremely large aperture array
(ELAA) systems, where near-field propagation and spatial non-stationarity
introduce complexities that hinder the effectiveness of traditional estimation
techniques. A physics-based hybrid channel model is developed, incorporating
non-binary visibility region (VR) masks to simulate diffraction-induced power
variations across the antenna array. To address the estimation challenges posed
by these channel conditions, a novel algorithm is proposed:
Visibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching
Pursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework
by integrating VR estimation through a hidden Markov model (HMM), using a novel
emission formulation and Viterbi decoding. This allows the algorithm to
adaptively mask steering vectors and account for spatial non-stationarity at
the antenna level. Simulation results demonstrate that the proposed method
enhances estimation accuracy compared to existing techniques, particularly in
low-SNR and sparse scenarios, while maintaining a low computational complexity.
The algorithm presents robustness across a range of design parameters and
channel conditions, offering a practical solution for ELAA systems.

</details>


### [13] [Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications](https://arxiv.org/abs/2508.04223)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Shuai Liu,Hongyang Du,Geyong Min*

Main category: eess.SP

TL;DR: 论文提出了一种基于Wasserstein距离的自适应混合分布方案（WS-DC），用于设计频谱效率感知的码本，以提升任务导向语义通信的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向语义通信（ToSC）方法中，码本稀疏激活导致频谱效率低和信道容量未充分利用，亟需设计一种既能支持任务推理又能接近信道容量理论极限的码本。

Method: 构建了频谱效率感知的码本设计框架，将码本激活概率纳入优化过程，并引入Wasserstein距离作为正则化指标，最小化学习到的激活分布与最优信道输入分布之间的差距。

Result: 实验表明，WS-DC在推理准确性和码本效率上均优于现有方法，为接近信道容量的语义通信系统提供了新方向。

Conclusion: WS-DC通过结合任务驱动和信道感知的潜在表示，显著提升了语义通信系统的性能，具有重要的应用潜力。

Abstract: Digital task-oriented semantic communication (ToSC) aims to transmit only
task-relevant information, significantly reducing communication overhead.
Existing ToSC methods typically rely on learned codebooks to encode semantic
features and map them to constellation symbols. However, these codebooks are
often sparsely activated, resulting in low spectral efficiency and
underutilization of channel capacity. This highlights a key challenge: how to
design a codebook that not only supports task-specific inference but also
approaches the theoretical limits of channel capacity. To address this
challenge, we construct a spectral efficiency-aware codebook design framework
that explicitly incorporates the codebook activation probability into the
optimization process. Beyond maximizing task performance, we introduce the
Wasserstein (WS) distance as a regularization metric to minimize the gap
between the learned activation distribution and the optimal channel input
distribution. Furthermore, we reinterpret WS theory from a generative
perspective to align with the semantic nature of ToSC. Combining the above two
aspects, we propose a WS-based adaptive hybrid distribution scheme, termed
WS-DC, which learns compact, task-driven and channel-aware latent
representations. Experimental results demonstrate that WS-DC not only
outperforms existing approaches in inference accuracy but also significantly
improves codebook efficiency, offering a promising direction toward
capacity-approaching semantic communication systems.

</details>


### [14] [ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening](https://arxiv.org/abs/2508.04240)
*Sitong Chen,Beiqianyi Li,Cuilin He,Dongyang Li,Mingyang Wu,Xinke Shen,Song Wang,Xuetao Wei,Xindi Wang,Haiyan Wu,Quanying Liu*

Main category: eess.SP

TL;DR: ChineseEEG-2是一个高密度EEG数据集，用于多模态语言任务下的神经解码模型基准测试，支持中文语言研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏非英语语言的多模态脑-语言配对数据，阻碍了神经活动与大型语言模型语义表示的对齐研究。

Method: 扩展了之前的ChineseEEG数据集，新增了朗读和被动听读两种模态，记录了EEG和音频数据，并进行了语义对齐。

Result: 提供了EEG信号、音频、语义嵌入和任务标签，支持多模态语言任务的神经解码算法基准测试。

Conclusion: ChineseEEG-2为下一代神经语义解码提供了基准数据集，促进了中文环境下的脑-LLM对齐研究。

Abstract: EEG-based neural decoding requires large-scale benchmark datasets. Paired
brain-language data across speaking, listening, and reading modalities are
essential for aligning neural activity with the semantic representation of
large language models (LLMs). However, such datasets are rare, especially for
non-English languages. Here, we present ChineseEEG-2, a high-density EEG
dataset designed for benchmarking neural decoding models under real-world
language tasks. Building on our previous ChineseEEG dataset, which focused on
silent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and
Passive Listening (PL), using the same Chinese corpus. EEG and audio were
simultaneously recorded from four participants during ~10.7 hours of reading
aloud. These recordings were then played to eight other participants,
collecting ~21.6 hours of EEG during listening. This setup enables speech
temporal and semantic alignment across the RA and PL modalities. ChineseEEG-2
includes EEG signals, precise audio, aligned semantic embeddings from
pre-trained language models, and task labels. Together with ChineseEEG, this
dataset supports joint semantic alignment learning across speaking, listening,
and reading. It enables benchmarking of neural decoding algorithms and promotes
brain-LLM alignment under multimodal language tasks, especially in Chinese.
ChineseEEG-2 provides a benchmark dataset for next-generation neural semantic
decoding.

</details>


### [15] [Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond](https://arxiv.org/abs/2508.04253)
*Yiyan Ma,Bo Ai,Jinhong Yuan,Shuangyang Li,Qingqing Cheng,Zhenguo Shi,Weijie Yuan,Zhiqiang Wei,Akram Shafie,Guoyu Ma,Yunlong Lu,Mi Yang,Zhangdui Zhong*

Main category: eess.SP

TL;DR: 论文提出了一种基于延迟-多普勒（DD）域信号处理的OFDM增强方案（DD-a-OFDM），通过结合DDMC研究的见解，提升了OFDM在高移动性场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 6G系统中高移动性场景对波形设计提出了挑战，OFDM在多普勒扩展下存在子载波正交性损失问题，而OTFS虽能利用时频（TF）域信道多样性，但复杂度高且资源分配不灵活。因此，需要一种既能保留OFDM优势又能提升性能的方案。

Method: 设计了DD-a-OFDM系统结构，保留了经典OFDM收发机，同时引入DD域信道估计和TF域均衡；详细描述了基于离散TF导频的DD域信道估计方法，并证明了TF域载波间干扰（ICI）可转化为DD域高斯干扰；推导了DD域信道估计的CRLB；开发了ML和峰值检测的信道估计器及对应的TF域均衡器。

Result: 数值结果表明，DD-a-OFDM相比经典OFDM降低了误码率（BER），并在信道估计精度上优于OTFS，且导频开销更低。

Conclusion: DD-a-OFDM是一种在高移动性场景下提升OFDM性能的有效方案，兼具低复杂度和灵活性。

Abstract: High-mobility scenarios will be a critical part of 6G systems. Since the
widely deployed orthogonal frequency division multiplexing (OFDM) waveform
suffers from subcarrier orthogonality loss under severe Doppler spread,
delay-Doppler domain multi-carrier (DDMC) modulation systems, such as
orthogonal time frequency space (OTFS), have been extensively studied. While
OTFS can exploit time-frequency (TF) domain channel diversity, it faces
challenges including high receiver complexity and inflexible TF resource
allocation, making OFDM still the most promising waveform for 6G. In this
article, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme
to enhance OFDM performance based on DDMC research insights. First, we design a
DD-a-OFDM system structure, retaining the classical OFDM transceiver while
incorporating DD domain channel estimation and TF domain equalization. Second,
we detail DD domain channel estimation using discrete TF pilots and prove that
TF domain inter-carrier interference (ICI) could be transformed into DD domain
Gaussian interference. Third, we derive closed-form Cram\'{e}r-Rao lower bounds
(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood
(ML) and peak detection-based channel estimators, along with a corresponding TF
domain equalizer. Numerical results verify the proposed design, showing that
DD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and
outperforms OTFS in channel estimation accuracy with lower pilot overhead.

</details>


### [16] [Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication](https://arxiv.org/abs/2508.04291)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Hongyang Du,Haojin Li,Chen Sun,Haijun Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种针对低功耗边缘网络的信道感知离散语义编码框架，通过Wasserstein正则化目标优化语义保真度和任务准确性。


<details>
  <summary>Details</summary>
Motivation: 当前任务导向语义通信（ToSC）框架中，离散映射与信道特性及任务需求脱节，导致性能不佳。

Method: 利用Wasserstein正则化目标，将离散码激活与最优输入分布对齐。

Result: 在多样化信噪比（SNR）场景下的推理任务中，方法显著提升了准确性和通信效率。

Conclusion: 该研究为离散语义与信道优化的结合提供了新思路，推动了语义通信在未来数字基础设施中的广泛应用。

Abstract: Discrete representation has emerged as a powerful tool in task-oriented
semantic communication (ToSC), offering compact, interpretable, and efficient
representations well-suited for low-power edge intelligence scenarios. Its
inherent digital nature aligns seamlessly with hardware-friendly deployment and
robust storage/transmission protocols. However, despite its strengths, current
ToSC frameworks often decouple semantic-aware discrete mapping from the
underlying channel characteristics and task demands. This mismatch leads to
suboptimal communication performance, degraded task utility, and limited
generalization under variable wireless conditions. Moreover, conventional
designs frequently overlook channel-awareness in codebook construction,
restricting the effectiveness of semantic symbol selection under constrained
resources. To address these limitations, this paper proposes a channel-aware
discrete semantic coding framework tailored for low-power edge networks.
Leveraging a Wasserstein-regularized objective, our approach aligns discrete
code activations with optimal input distributions, thereby improving semantic
fidelity, robustness, and task accuracy. Extensive experiments on the inference
tasks across diverse signal-to-noise ratio (SNR) regimes show that our method
achieves notable gains in accuracy and communication efficiency. This work
provides new insights into integrating discrete semantics and channel
optimization, paving the way for the widespread adoption of semantic
communication in future digital infrastructures.

</details>


### [17] [Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications](https://arxiv.org/abs/2508.04322)
*Ruopeng Xu,Zhaohui Yang,Zhaoyang Zhang,Mohammad Shikh-Bahaei,Kaibin Huang,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于流体天线中继（FAR）的能效无线通信系统，解决非视距（NLoS）链路问题，并通过优化天线位置和协议设计提升能效。


<details>
  <summary>Details</summary>
Motivation: 6G通信需求推动流体天线系统（FAS）发展，但现有研究多关注视距（LoS）场景，忽视了非视距（NLoS）链路问题。

Method: 设计FAR辅助通信系统，结合放大转发（AF）协议，优化天线位置、功率控制和波束成形，提出迭代算法解决能效最大化问题。

Result: 仿真结果显示，所提算法在能效上优于传统方案，比可重构智能表面（RIS）和传统AF中继方案分别提升23.39%和39.94%。

Conclusion: FAR系统通过优化设计和算法实现，显著提升了非视距链路的能效，为6G通信提供了有效解决方案。

Abstract: In this paper, we propose an energy efficient wireless communication system
based on fluid antenna relay (FAR) to solve the problem of non-line-of-sight
(NLoS) links caused by blockages with considering the physical properties.
Driven by the demand for the sixth generation (6G) communication, fluid antenna
systems (FASs) have become a key technology due to their flexibility in
dynamically adjusting antenna positions. Existing research on FAS primarily
focuses on line-of-sight (LoS) communication scenarios, and neglects the
situations where only NLoS links exist. To address the issues posted by NLoS
communication, we design an FAR-assisted communication system combined with
amplify-and-forward (AF) protocol. In order to alleviate the high energy
consumption introduced by AF protocol while ensuring communication quality, we
formulate an energy efficiency (EE) maximization problem. By optimizing the
positions of the fluid antennas (FAs) on both sides of the FAR, we achieve
controllable phase shifts of the signals transmitting through the blockage
which causes the NLoS link. Besides, we establish a channel model that jointly
considers the blockage-through matrix, large-scale fading, and small-scale
fading. To maximize the EE of the system, we jointly optimize the FAR position,
FA positions, power control, and beamforming design under given constraints,
and propose an iterative algorithm to solve this formulated optimization
problem. Simulation results show that the proposed algorithm outperforms the
traditional schemes in terms of EE, achieving up to $23.39\%$ and $39.94\%$
higher EE than the conventional reconfigurable intelligent surface (RIS) scheme
and traditional AF relay scheme, respectively.

</details>


### [18] [Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination](https://arxiv.org/abs/2508.04331)
*Mohamadreza Delbari,Qikai Zhou,Robin Neuder,Alejandro Jiménez-Sáez,Vahid Jamali*

Main category: eess.SP

TL;DR: 论文提出了一种基于液晶技术的可重构智能表面（RIS）设计，用于宽频带OFDM系统，以提高保密通信中的保密率。


<details>
  <summary>Details</summary>
Motivation: 液晶RIS的相位偏移响应具有频率依赖性，可能导致性能下降和信息泄露，尤其在保密通信系统中更为关键。

Method: 设计了一种RIS算法，用于宽频带OFDM系统，旨在照亮合法用户区域并避免信息泄露至潜在窃听者区域。

Result: 仿真结果表明，所提算法在8 GHz带宽下（中心频率60 GHz）实现了约2 bits/symbol的保密率，优于忽略频率依赖效应的方法。

Conclusion: 该方法有效解决了液晶RIS的频率依赖性问题，提升了保密通信系统的性能。

Abstract: Liquid crystal (LC) technology provides a low-power and scalable approach to
implement a reconfigurable intelligent surface (RIS). However, the LC-based
RIS's phase-shift response is inherently frequency-dependent, which can lead to
performance degradation if not properly addressed. This issue becomes
especially critical in secure communication systems, where such variations may
result in considerable information leakage. To avoid the need for full channel
state information (CSI) acquisition and frequent RIS reconfiguration, we design
RIS for a wideband orthogonal frequency division multiplexing (OFDM) system to
illuminate a desired area containing legitimate users while avoiding leakage to
regions where potential eavesdroppers may be located. Our simulation results
demonstrate that the proposed algorithm improves the secrecy rate compared to
methods that neglect frequency-dependent effects. In the considered setup, the
proposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz
bandwidth when the center frequency is 60 GHz.

</details>


### [19] [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](https://arxiv.org/abs/2508.04570)
*A. Tarik Leblebici,Sumeyra Hassan,Erdal Panayirci,H. Vincent Poor*

Main category: eess.SP

TL;DR: 提出了一种基于可见光通信的高精度室内联合通信与定位系统，结合空间调制和RSS技术，实现亚厘米级定位和高效通信。


<details>
  <summary>Details</summary>
Motivation: 为未来6G室内网络提供集成定位与通信的解决方案，满足高精度和可靠性的需求。

Method: 使用RSS和激进轴定理进行定位，空间调制和PAM实现通信，结合LS估计器和调光控制。

Result: 在高信噪比下实现亚厘米级定位，BER低于10^{-6}，性能在LED布局中心区域显著提升。

Conclusion: 该系统在复杂信道条件下有效，适用于未来6G网络的集成定位与通信需求。

Abstract: This paper proposes a joint communication and indoor positioning (JCP) system
based on visible light communication (VLC) designed for high-precision indoor
environments. The framework supports 2D and 3D positioning using received
signal strength (RSS) from pilot transmissions, enhanced by the radical axis
theorem to improve accuracy under measurement uncertainties. Communication is
achieved using spatial modulation (SM) with M-ary pulse amplitude modulation
(PAM), where data is conveyed through the modulation symbol and the active
light-emitting diode (LED) index, improving spectral efficiency while
maintaining low complexity. A pilot-aided least squares (LS) estimator is
employed for joint channel and dimming coefficient estimation, enabling robust
symbol detection in multipath environments characterized by both line-of-sight
(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician
fading. The proposed system incorporates a dimming control mechanism to meet
lighting requirements while maintaining reliable communication and positioning
performance. Simulation results demonstrate sub-centimeter localization
accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below
10^{-6} for low-order PAM schemes. Additionally, comparative analysis across
user locations reveals that positioning and communication performance improve
significantly near the geometric center of the LED layout. These findings
validate the effectiveness of the proposed system for future 6G indoor networks
requiring integrated localization and communication under practical channel
conditions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [20] [LCS-CTC: Leveraging Soft Alignments to Enhance Phonetic Transcription Robustness](https://arxiv.org/abs/2508.03937)
*Zongli Ye,Jiachen Lian,Akshaj Gupta,Xuanru Zhou,Krish Patel,Haodong Li,Hwi Joo Park,Chenxu Guo,Shuhe Li,Sam Wang,Cheol Jun Cho,Zoe Ezzes,Jet M. J. Vonk,Brittany T. Morin,Rian Bogley,Lisa Wauters,Zachary A. Miller,Maria Luisa Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: LCS-CTC是一种结合局部对齐算法和CTC训练目标的两阶段框架，用于音素级语音识别，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统CTC方法在非流利或模糊语音中表现不佳，需要一种更鲁棒的识别方法。

Method: 提出LCS-CTC框架，通过预测帧-音素成本矩阵和修改的LCS算法，约束CTC解码路径。

Result: 在LibriSpeech和PPA数据集上，LCS-CTC性能优于传统CTC。

Conclusion: LCS-CTC有望统一流利和非流利语音的音素建模。

Abstract: Phonetic speech transcription is crucial for fine-grained linguistic analysis
and downstream speech applications. While Connectionist Temporal Classification
(CTC) is a widely used approach for such tasks due to its efficiency, it often
falls short in recognition performance, especially under unclear and nonfluent
speech. In this work, we propose LCS-CTC, a two-stage framework for
phoneme-level speech recognition that combines a similarity-aware local
alignment algorithm with a constrained CTC training objective. By predicting
fine-grained frame-phoneme cost matrices and applying a modified Longest Common
Subsequence (LCS) algorithm, our method identifies high-confidence alignment
zones which are used to constrain the CTC decoding path space, thereby reducing
overfitting and improving generalization ability, which enables both robust
recognition and text-free forced alignment. Experiments on both LibriSpeech and
PPA demonstrate that LCS-CTC consistently outperforms vanilla CTC baselines,
suggesting its potential to unify phoneme modeling across fluent and non-fluent
speech.

</details>


### [21] [Parallel GPT: Harmonizing the Independence and Interdependence of Acoustic and Semantic Information for Zero-Shot Text-to-Speech](https://arxiv.org/abs/2508.04141)
*Jingyuan Xing,Zhipeng Li,Jialong Mai,Xiaofen Xing,Xiangmin Xu*

Main category: eess.AS

TL;DR: 论文提出了一种结合自回归（AR）和非自回归（NAR）模块的TTS框架，通过并行结构提升零样本文本到语音合成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有零样本TTS模型难以捕捉声学和语义特征的复杂相关性，导致表达性和相似性不足。

Method: 提出并行Tokenizer的AR模型和耦合NAR模型，分别处理独立和相互依赖的声学与语义信息。

Result: 在英语和中文数据集上，模型显著优于现有零样本TTS模型的质量和效率。

Conclusion: 该框架通过并行结构有效解决了声学与语义特征的复杂关系，提升了零样本TTS性能。

Abstract: Advances in speech representation and large language models have enhanced
zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS
models face challenges in capturing the complex correlations between acoustic
and semantic features, resulting in a lack of expressiveness and similarity.
The primary reason lies in the complex relationship between semantic and
acoustic features, which manifests independent and interdependent aspects.This
paper introduces a TTS framework that combines both autoregressive (AR) and
non-autoregressive (NAR) modules to harmonize the independence and
interdependence of acoustic and semantic information. The AR model leverages
the proposed Parallel Tokenizer to synthesize the top semantic and acoustic
tokens simultaneously. In contrast, considering the interdependence, the
Coupled NAR model predicts detailed tokens based on the general AR model's
output. Parallel GPT, built on this architecture, is designed to improve
zero-shot text-to-speech synthesis through its parallel structure. Experiments
on English and Chinese datasets demonstrate that the proposed model
significantly outperforms the quality and efficiency of the synthesis of
existing zero-shot TTS models. Speech demos are available at
https://t1235-ch.github.io/pgpt/.

</details>


### [22] [Multilingual Source Tracing of Speech Deepfakes: A First Benchmark](https://arxiv.org/abs/2508.04143)
*Xi Xuan,Yang Xiao,Rohan Kumar Das,Tomi Kinnunen*

Main category: eess.AS

TL;DR: 本文提出了首个多语言语音深度伪造来源追踪基准，研究了单语言和跨语言场景下的模型性能，并探讨了SSL表示对不同语言的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步使得伪造语音变得容易，但现有研究多集中于检测伪造语音，而忽视了追踪生成模型的来源。

Method: 通过比较DSP和SSL建模方法，研究SSL表示在不同语言上的微调对跨语言泛化性能的影响，并评估对未见语言和说话者的泛化能力。

Result: 研究首次全面揭示了训练和推理语言不同时识别语音生成模型的挑战。

Conclusion: 提供了数据集、协议和代码，为未来研究奠定了基础。

Abstract: Recent progress in generative AI has made it increasingly easy to create
natural-sounding deepfake speech from just a few seconds of audio. While these
tools support helpful applications, they also raise serious concerns by making
it possible to generate convincing fake speech in many languages. Current
research has largely focused on detecting fake speech, but little attention has
been given to tracing the source models used to generate it. This paper
introduces the first benchmark for multilingual speech deepfake source tracing,
covering both mono- and cross-lingual scenarios. We comparatively investigate
DSP- and SSL-based modeling; examine how SSL representations fine-tuned on
different languages impact cross-lingual generalization performance; and
evaluate generalization to unseen languages and speakers. Our findings offer
the first comprehensive insights into the challenges of identifying speech
generation models when training and inference languages differ. The dataset,
protocol and code are available at
https://github.com/xuanxixi/Multilingual-Source-Tracing.

</details>


### [23] [Towards interpretable emotion recognition: Identifying key features with machine learning](https://arxiv.org/abs/2508.04230)
*Yacouba Kaloga,Ina Kodrasi*

Main category: eess.AS

TL;DR: 该论文探讨了无监督方法（如wav2vec2和HuBERT）在音频任务中的局限性，尤其是缺乏可解释性，并提出了一种识别和推广情感识别任务中重要可解释特征的方法。


<details>
  <summary>Details</summary>
Motivation: 无监督方法在音频任务中表现出色，但缺乏可解释性限制了其在医学等关键领域的应用。因此，识别和理解这些模型中的可解释特征变得至关重要。

Method: 使用机器学习算法识别和推广情感识别任务中最重要的可解释特征。

Result: 提出了一种更广泛且稳健的框架，用于识别情感识别任务中的重要可解释特征。

Conclusion: 该研究为无监督模型的可解释性提供了新的视角，特别是在情感识别任务中，有助于推动关键领域的应用。

Abstract: Unsupervised methods, such as wav2vec2 and HuBERT, have achieved
state-of-the-art performance in audio tasks, leading to a shift away from
research on interpretable features. However, the lack of interpretability in
these methods limits their applicability in critical domains like medicine,
where understanding feature relevance is crucial. To better understand the
features of unsupervised models, it remains critical to identify the
interpretable features relevant to a given task. In this work, we focus on
emotion recognition and use machine learning algorithms to identify and
generalize the most important interpretable features for this task. While
previous studies have explored feature relevance in emotion recognition, they
are often constrained by narrow contexts and present inconsistent findings. Our
approach aims to overcome these limitations, providing a broader and more
robust framework for identifying the most important interpretable features.

</details>


### [24] [A Multi-stage Low-latency Enhancement System for Hearing Aids](https://arxiv.org/abs/2508.04283)
*Chengwei Ouyang,Kexin Fei,Haoshuai Zhou,Congxi Lu,Linkai Li*

Main category: eess.AS

TL;DR: 本文提出了一种端到端系统，用于ICASSP 2023 Clarity挑战赛，包含四个创新点：多阶段系统、非对称窗口对、头部旋转信息集成和后处理模块。


<details>
  <summary>Details</summary>
Motivation: 提升语音增强效果，特别是在听力辅助设备中，满足低延迟和高频率分辨率的需求。

Method: 采用多阶段系统（幅度和复数域）、非对称窗口对、头部旋转信息与混合信号结合，以及后处理模块。

Result: 系统在HASPI评分上表现更优，同时满足5ms延迟约束。

Conclusion: 提出的方法有效提升了语音增强性能，特别是在听力辅助应用中。

Abstract: This paper proposes an end-to-end system for the ICASSP 2023 Clarity
Challenge. In this work, we introduce four major novelties: (1) a novel
multi-stage system in both the magnitude and complex domains to better utilize
phase information; (2) an asymmetric window pair to achieve higher frequency
resolution with the 5ms latency constraint; (3) the integration of head
rotation information and the mixture signals to achieve better enhancement; (4)
a post-processing module that achieves higher hearing aid speech perception
index (HASPI) scores with the hearing aid amplification stage provided by the
baseline system.

</details>


### [25] [Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots](https://arxiv.org/abs/2508.04333)
*Gyeong-Tae Lee*

Main category: eess.AS

TL;DR: 提出了一种基于双耳输入的双耳声音事件定位与检测神经网络（BiSELDnet），通过八通道双耳时频特征（BTFF）提升声音事件类型和方向的估计能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统双通道输入在高度估计和前后混淆问题上表现不佳，人形机器人需要同时进行声音事件类型和方向估计以实现情境感知。

Method: 引入八通道双耳时频特征（BTFF），包括左右梅尔频谱图、V图、ITD图、ILD图和SC图，结合Trinity模块实现声音事件的检测与定位。

Result: 在全方位、水平和中位平面中验证了BTFF的有效性，BiSELDnet在噪声环境下显著优于现有SELD模型。

Conclusion: BiSELDnet通过BTFF和VAM可视化成功解决了高度估计和前后混淆问题，为机器人情境感知提供了高效解决方案。

Abstract: Humanoid robots require simultaneous sound event type and direction
estimation for situational awareness, but conventional two-channel input
struggles with elevation estimation and front-back confusion. This paper
proposes a binaural sound event localization and detection (BiSELD) neural
network to address these challenges. BiSELDnet learns time-frequency patterns
and head-related transfer function (HRTF) localization cues from binaural input
features. A novel eight-channel binaural time-frequency feature (BTFF) is
introduced, comprising left/right mel-spectrograms, V-maps, an interaural time
difference (ITD) map (below 1.5 kHz), an interaural level difference (ILD) map
(above 5 kHz with front-back asymmetry), and spectral cue (SC) maps (above 5
kHz for elevation). The effectiveness of BTFF was confirmed across
omnidirectional, horizontal, and median planes. BiSELDnets, particularly one
based on the efficient Trinity module, were implemented to output time series
of direction vectors for each sound event class, enabling simultaneous
detection and localization. Vector activation map (VAM) visualization was
proposed to analyze network learning, confirming BiSELDnet's focus on the N1
notch frequency for elevation estimation. Comparative evaluations under urban
background noise conditions demonstrated that the proposed BiSELD model
significantly outperforms state-of-the-art (SOTA) SELD models with binaural
input.

</details>


### [26] [Text adaptation for speaker verification with speaker-text factorized embeddings](https://arxiv.org/abs/2508.04425)
*Yexin Yang,Shuai Wang,Xun Gong,Yanmin Qian,Kai Yu*

Main category: eess.AS

TL;DR: 提出了一种文本适应框架，通过分解语音为说话人和文本嵌入，解决文本不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 预收集数据与实际测试数据的文本不匹配会显著影响说话人验证系统性能，而针对性数据收集成本高且不灵活。

Method: 提出说话人-文本分解网络，将语音分解为说话人和文本嵌入，并通过少量说话人无关的适应话语提取目标文本嵌入。

Result: 在RSR2015上的实验表明，文本适应能显著改善文本不匹配条件下的性能。

Conclusion: 文本适应框架有效解决了文本不匹配问题，提升了说话人验证系统的性能。

Abstract: Text mismatch between pre-collected data, either training data or enrollment
data, and the actual test data can significantly hurt text-dependent speaker
verification (SV) system performance. Although this problem can be solved by
carefully collecting data with the target speech content, such data collection
could be costly and inflexible. In this paper, we propose a novel text
adaptation framework to address the text mismatch issue. Here, a speaker-text
factorization network is proposed to factorize the input speech into speaker
embeddings and text embeddings and then integrate them into a single
representation in the later stage. Given a small amount of speaker-independent
adaptation utterances, text embeddings of target speech content can be
extracted and used to adapt the text-independent speaker embeddings to
text-customized speaker embeddings. Experiments on RSR2015 show that text
adaptation can significantly improve the performance of text mismatch
conditions.

</details>


### [27] [Melodic and Metrical Elements of Expressiveness in Hindustani Vocal Music](https://arxiv.org/abs/2508.04430)
*Yash Bhake,Ankit Anand,Preeti Rao*

Main category: eess.AS

TL;DR: 研究北印度Khayal音乐的美学，分析艺术家在表演中的灵活性和表达差异，提出计算表示方法区分不同表演。


<details>
  <summary>Details</summary>
Motivation: 探索Khayal音乐表演中艺术家对流行曲目的灵活处理及其美学表达。

Method: 研究表达时间和音高变化，提出计算表示方法，分析音频处理和标注程序。

Result: 通过十位艺术家对两首歌曲的表演数据集分析，得出区分表演表达的计算表示。

Conclusion: 计算表示能有效区分不同表演的表达差异，为音乐美学研究提供新视角。

Abstract: This paper presents an attempt to study the aesthetics of North Indian Khayal
music with reference to the flexibility exercised by artists in performing
popular compositions. We study expressive timing and pitch variations of the
given lyrical content within and across performances and propose computational
representations that can discriminate between different performances of the
same song in terms of expression. We present the necessary audio processing and
annotation procedures, and discuss our observations and insights from the
analysis of a dataset of two songs in two ragas each rendered by ten prominent
artists.

</details>


### [28] [Pitfalls and Limits in Automatic Dementia Assessment](https://arxiv.org/abs/2508.04512)
*Franziska Braun,Christopher Witzl,Andreas Erzigkeit,Hartmut Lehfeld,Thomas Hillemacher,Tobias Bocklet,Korbinian Riedhammer*

Main category: eess.AS

TL;DR: 论文分析了基于语音的痴呆评估中的常见问题，指出现有研究过于依赖数值性能而忽略错误分析，并揭示了测试设计和数据处理中的潜在偏差。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注特征提取或自动化测试，但缺乏对数据使用和错误分析的深入探讨，可能导致评估结果不准确。

Method: 通过对Syndrom-Kurz-Test的深入分析，研究了语音评估中的相关性及其在不同认知水平群体中的表现。

Result: 研究发现，整体相关性较高，但对健康或轻度受损个体的相关性较低，且测试设计和回退处理可能引入偏差。

Conclusion: 需对不同目标群体进行差异化分析，以避免测试设计和数据处理中的潜在偏差。

Abstract: Current work on speech-based dementia assessment focuses on either feature
extraction to predict assessment scales, or on the automation of existing test
procedures. Most research uses public data unquestioningly and rarely performs
a detailed error analysis, focusing primarily on numerical performance. We
perform an in-depth analysis of an automated standardized dementia assessment,
the Syndrom-Kurz-Test. We find that while there is a high overall correlation
with human annotators, due to certain artifacts, we observe high correlations
for the severely impaired individuals, which is less true for the healthy or
mildly impaired ones. Speech production decreases with cognitive decline,
leading to overoptimistic correlations when test scoring relies on word naming.
Depending on the test design, fallback handling introduces further biases that
favor certain groups. These pitfalls remain independent of group distributions
in datasets and require differentiated analysis of target groups.

</details>


### [29] [UniTalker: Conversational Speech-Visual Synthesis](https://arxiv.org/abs/2508.04585)
*Yifan Hu,Rui Liu,Yi Ren,Xiang Yin,Haizhou Li*

Main category: eess.AS

TL;DR: 论文提出了一种名为UniTalker的CSVS系统，扩展了传统的CSS任务，通过多模态感知和渲染生成更自然、情感一致的语音和面部动画。


<details>
  <summary>Details</summary>
Motivation: 现有CSS研究仅感知文本和语音，限制了情感表达和交互体验，因此需要引入视觉模态以提升效果。

Method: UniTalker利用大规模语言模型理解多模态对话上下文，并通过多任务序列预测生成情感一致的语音和面部动画，结合三种优化策略。

Result: 实验表明，UniTalker生成的语音和面部动画更具同理心且情感一致。

Conclusion: CSVS任务和UniTalker系统在多模态交互中显著提升了情感表达和用户体验。

Abstract: Conversational Speech Synthesis (CSS) is a key task in the user-agent
interaction area, aiming to generate more expressive and empathetic speech for
users. However, it is well-known that "listening" and "eye contact" play
crucial roles in conveying emotions during real-world interpersonal
communication. Existing CSS research is limited to perceiving only text and
speech within the dialogue context, which restricts its effectiveness.
Moreover, speech-only responses further constrain the interactive experience.
To address these limitations, we introduce a Conversational Speech-Visual
Synthesis (CSVS) task as an extension of traditional CSS. By leveraging
multimodal dialogue context, it provides users with coherent audiovisual
responses. To this end, we develop a CSVS system named UniTalker, which is a
unified model that seamlessly integrates multimodal perception and multimodal
rendering capabilities. Specifically, it leverages a large-scale language model
to comprehensively understand multimodal cues in the dialogue context,
including speaker, text, speech, and the talking-face animations. After that,
it employs multi-task sequence prediction to first infer the target utterance's
emotion and then generate empathetic speech and natural talking-face
animations. To ensure that the generated speech-visual content remains
consistent in terms of emotion, content, and duration, we introduce three key
optimizations: 1) Designing a specialized neural landmark codec to tokenize and
reconstruct facial expression sequences. 2) Proposing a bimodal speech-visual
hard alignment decoding strategy. 3) Applying emotion-guided rendering during
the generation stage. Comprehensive objective and subjective experiments
demonstrate that our model synthesizes more empathetic speech and provides
users with more natural and emotionally consistent talking-face animations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [30] [CoughViT: A Self-Supervised Vision Transformer for Cough Audio Representation Learning](https://arxiv.org/abs/2508.03764)
*Justin Luong,Hao Xue,Flora D. Salim*

Main category: cs.SD

TL;DR: 提出了一种名为CoughViT的自监督预训练框架，用于学习通用的咳嗽声音表示，以提升数据稀缺条件下的呼吸疾病诊断性能。


<details>
  <summary>Details</summary>
Motivation: 呼吸声音在疾病诊断中至关重要，但标签和数据稀缺限制了AI诊断系统的性能，尤其是在非COVID-19疾病中。

Method: 采用掩码数据建模的自监督学习方法训练特征编码器，并在三个咳嗽分类任务中与其他预训练策略进行比较。

Result: 实验表明，CoughViT的表征性能达到或超过当前最先进的监督音频表征。

Conclusion: CoughViT为数据稀缺条件下的呼吸疾病诊断提供了一种有效的解决方案。

Abstract: Physicians routinely assess respiratory sounds during the diagnostic process,
providing insight into the condition of a patient's airways. In recent years,
AI-based diagnostic systems operating on respiratory sounds, have demonstrated
success in respiratory disease detection. These systems represent a crucial
advancement in early and accessible diagnosis which is essential for timely
treatment. However, label and data scarcity remain key challenges, especially
for conditions beyond COVID-19, limiting diagnostic performance and reliable
evaluation. In this paper, we propose CoughViT, a novel pre-training framework
for learning general-purpose cough sound representations, to enhance diagnostic
performance in tasks with limited data. To address label scarcity, we employ
masked data modelling to train a feature encoder in a self-supervised learning
manner. We evaluate our approach against other pre-training strategies on three
diagnostically important cough classification tasks. Experimental results show
that our representations match or exceed current state-of-the-art supervised
audio representations in enhancing performance on downstream tasks.

</details>


### [31] [Are Inherently Interpretable Models More Robust? A Study In Music Emotion Recognition](https://arxiv.org/abs/2508.03780)
*Katharina Hoedt,Arthur Flexer,Gerhard Widmer*

Main category: cs.SD

TL;DR: 本文研究了可解释性深度学习模型是否比黑盒模型对无关扰动更鲁棒，并通过音乐情感识别模型验证了假设。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在面对微小对抗性扰动时表现脆弱，而可解释性模型可能更关注有意义特征，从而更鲁棒。

Method: 比较可解释性模型、黑盒模型和对抗训练模型在对抗样本下的鲁棒性。

Result: 可解释性模型比黑盒模型更鲁棒，且能达到与对抗训练模型相似的鲁棒性，计算成本更低。

Conclusion: 可解释性模型在鲁棒性和计算效率上具有优势。

Abstract: One of the desired key properties of deep learning models is the ability to
generalise to unseen samples. When provided with new samples that are
(perceptually) similar to one or more training samples, deep learning models
are expected to produce correspondingly similar outputs. Models that succeed in
predicting similar outputs for similar inputs are often called robust. Deep
learning models, on the other hand, have been shown to be highly vulnerable to
minor (adversarial) perturbations of the input, which manage to drastically
change a model's output and simultaneously expose its reliance on spurious
correlations. In this work, we investigate whether inherently interpretable
deep models, i.e., deep models that were designed to focus more on meaningful
and interpretable features, are more robust to irrelevant perturbations in the
data, compared to their black-box counterparts. We test our hypothesis by
comparing the robustness of an interpretable and a black-box music emotion
recognition (MER) model when challenged with adversarial examples. Furthermore,
we include an adversarially trained model, which is optimised to be more
robust, in the comparison. Our results indicate that inherently more
interpretable models can indeed be more robust than their black-box
counterparts, and achieve similar levels of robustness as adversarially trained
models, at lower computational cost.

</details>


### [32] [MiDashengLM: Efficient Audio Understanding with General Audio Captions](https://arxiv.org/abs/2508.03983)
*Heinrich Dinkel,Gang Li,Jizhong Liu,Jian Luan,Yadong Niu,Xingwei Sun,Tianzi Wang,Qiyang Xiao,Junbo Zhang,Jiahao Zhou*

Main category: cs.SD

TL;DR: MiDashengLM是一种新型开放音频语言模型，通过使用公开数据集和开源音频编码器Dasheng，实现了高效且全面的音频理解，并在速度和吞吐量上显著优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型（LALMs）依赖封闭数据或专有模型，限制了其泛化能力和可访问性。本文旨在解决这一问题。

Method: 利用公开的预训练和监督微调数据集，结合开源音频编码器Dasheng，专注于通用音频字幕而非传统的语音识别对齐。

Result: MiDashengLM在首次令牌时间（TTFT）上提速4倍，吞吐量提高20倍。

Conclusion: MiDashengLM通过开放数据和开源技术，实现了高效、透明的音频理解，为未来研究提供了可复现的基础。

Abstract: Current approaches for large audio language models (LALMs) often rely on
closed data sources or proprietary models, limiting their generalization and
accessibility. This paper introduces MiDashengLM, a novel open audio-language
model designed for efficient and comprehensive audio understanding through the
use of general audio captions using our novel ACAVCaps training dataset.
MiDashengLM exclusively relies on publicly available pretraining and supervised
fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At
its core, MiDashengLM integrates Dasheng, an open-source audio encoder,
specifically engineered to process diverse auditory information effectively.
Unlike previous works primarily focused on Automatic Speech Recognition (ASR)
based audio-text alignment, our strategy centers on general audio captions,
fusing speech, sound and music information into one textual representation,
enabling a holistic textual representation of complex audio scenes. Lastly,
MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT)
and up to 20x higher throughput than comparable models. Checkpoints are
available online at https://huggingface.co/mispeech/midashenglm-7b and
https://github.com/xiaomi-research/dasheng-lm.

</details>


### [33] [Efficient Scaling for LLM-based ASR](https://arxiv.org/abs/2508.04096)
*Bingshen Mu,Yiwen Shao,Kun Wei,Dong Yu,Lei Xie*

Main category: cs.SD

TL;DR: EFIN（Encoder First Integration）是一种多阶段LLM-ASR训练策略，通过预训练语音编码器再与LLM集成，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保证性能的同时降低LLM-ASR的高计算成本。

Method: 提出EFIN策略，先预训练语音编码器再与LLM集成，并通过实验验证其效果。

Result: EFIN在降低49.9% FLOPs的同时，性能提升21.1% CERR。

Conclusion: EFIN是一种高效且性能优越的LLM-ASR训练策略，并提出了计算与错误率的比例定律。

Abstract: Large language model (LLM)-based automatic speech recognition (ASR) achieves
strong performance but often incurs high computational costs. This work
investigates how to obtain the best LLM-ASR performance efficiently. Through
comprehensive and controlled experiments, we find that pretraining the speech
encoder before integrating it with the LLM leads to significantly better
scaling efficiency than the standard practice of joint post-training of
LLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training
strategy, EFIN: Encoder First Integration. Among all training strategies
evaluated, EFIN consistently delivers better performance (relative to 21.1%
CERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore,
we derive a scaling law that approximates ASR error rates as a computation
function, providing practical guidance for LLM-ASR scaling.

</details>


### [34] [NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations](https://arxiv.org/abs/2508.04195)
*Huan Liao,Qinke Ni,Yuancheng Wang,Yiheng Lu,Haoyue Zhan,Pengyuan Xie,Qiang Zhang,Zhizheng Wu*

Main category: cs.SD

TL;DR: NVSpeech是一个集成管道，用于识别和合成副语言声音（如笑声、呼吸声等），包括数据集构建、ASR建模和可控TTS，填补了传统ASR和TTS系统在副语言处理上的空白。


<details>
  <summary>Details</summary>
Motivation: 副语言声音（如笑声、呼吸声等）在自然语音交流中至关重要，但传统ASR和TTS系统往往忽略这些声音。NVSpeech旨在填补这一空白，提供一种集成解决方案。

Method: 1. 构建手动标注的48,430条副语言声音数据集；2. 开发副语言感知的ASR模型，将副语言作为可解码标记；3. 利用该模型自动标注大规模中文数据集；4. 微调零样本TTS模型，实现副语言声音的可控合成。

Result: NVSpeech提供了首个开放、大规模、词级标注的副语言处理管道，支持中文表达性语音建模，并实现了识别与合成的统一。

Conclusion: NVSpeech通过集成识别与合成，为副语言声音的处理提供了可扩展且可控的解决方案，推动了表达性语音建模的发展。

Abstract: Paralinguistic vocalizations-including non-verbal sounds like laughter and
breathing, as well as lexicalized interjections such as "uhm" and "oh"-are
integral to natural spoken communication. Despite their importance in conveying
affect, intent, and interactional cues, such cues remain largely overlooked in
conventional automatic speech recognition (ASR) and text-to-speech (TTS)
systems. We present NVSpeech, an integrated and scalable pipeline that bridges
the recognition and synthesis of paralinguistic vocalizations, encompassing
dataset construction, ASR modeling, and controllable TTS. (1) We introduce a
manually annotated dataset of 48,430 human-spoken utterances with 18 word-level
paralinguistic categories. (2) We develop the paralinguistic-aware ASR model,
which treats paralinguistic cues as inline decodable tokens (e.g., "You're so
funny [Laughter]"), enabling joint lexical and non-verbal transcription. This
model is then used to automatically annotate a large corpus, the first
large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level
alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both
human- and auto-labeled data to enable explicit control over paralinguistic
vocalizations, allowing context-aware insertion at arbitrary token positions
for human-like speech synthesis. By unifying the recognition and generation of
paralinguistic vocalizations, NVSpeech offers the first open, large-scale,
word-level annotated pipeline for expressive speech modeling in Mandarin,
integrating recognition and synthesis in a scalable and controllable manner.
Dataset and audio demos are available at https://nvspeech170k.github.io/.

</details>


### [35] [ESDD 2026: Environmental Sound Deepfake Detection Challenge Evaluation Plan](https://arxiv.org/abs/2508.04529)
*Han Yin,Yang Xiao,Rohan Kumar Das,Jisheng Bai,Ting Dang*

Main category: cs.SD

TL;DR: 论文提出EnvSDD数据集，用于环境声音深度伪造检测（ESDD），并启动相关挑战赛。


<details>
  <summary>Details</summary>
Motivation: 音频生成技术的进步带来潜在滥用风险，现有ESDD数据集规模有限。

Method: 构建EnvSDD数据集（45.25小时真实音频和316.7小时伪造音频），并设计两项挑战赛。

Result: EnvSDD填补了ESDD数据集的空白，挑战赛涵盖实际场景中的多种问题。

Conclusion: EnvSDD及挑战赛将推动环境声音深度伪造检测的研究。

Abstract: Recent advances in audio generation systems have enabled the creation of
highly realistic and immersive soundscapes, which are increasingly used in film
and virtual reality. However, these audio generators also raise concerns about
potential misuse, such as generating deceptive audio content for fake videos
and spreading misleading information. Existing datasets for environmental sound
deepfake detection (ESDD) are limited in scale and audio types. To address this
gap, we have proposed EnvSDD, the first large-scale curated dataset designed
for ESDD, consisting of 45.25 hours of real and 316.7 hours of fake sound.
Based on EnvSDD, we are launching the Environmental Sound Deepfake Detection
Challenge. Specifically, we present two different tracks: ESDD in Unseen
Generators and Black-Box Low-Resource ESDD, covering various challenges
encountered in real-life scenarios. The challenge will be held in conjunction
with the 2026 IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP 2026).

</details>


### [36] [Live Music Models](https://arxiv.org/abs/2508.04651)
*Lyria Team,Antoine Caillon,Brian McWilliams,Cassie Tarakajian,Ian Simon,Ilaria Manco,Jesse Engel,Noah Constant,Pen Li,Timo I. Denk,Alberto Lalama,Andrea Agostinelli,Anna Huang,Ethan Manilow,George Brower,Hakan Erdogan,Heidi Lei,Itai Rolnick,Ivan Grishchenko,Manu Orsini,Matej Kastelic,Mauricio Zuluaga,Mauro Verzetti,Michael Dooley,Ondrej Skopek,Rafael Ferrer,Zalán Borsos,Äaron van den Oord,Douglas Eck,Eli Collins,Jason Baldridge,Tom Hume,Chris Donahue,Kehang Han,Adam Roberts*

Main category: cs.SD

TL;DR: 介绍了一种实时生成音乐的模型Magenta RealTime和Lyria RealTime，支持文本或音频提示控制，性能优于其他开源模型。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助音乐创作的新范式，强调实时交互和用户控制。

Method: 开发了Magenta RealTime和Lyria RealTime两种模型，支持文本或音频提示控制音乐风格。

Result: Magenta RealTime在音乐质量指标上优于其他开源模型，且参数更少；Lyria RealTime提供更广泛的控制功能。

Conclusion: 这些模型展示了AI辅助音乐创作的新方向，强调实时交互和用户参与。

Abstract: We introduce a new class of generative models for music called live music
models that produce a continuous stream of music in real-time with synchronized
user control. We release Magenta RealTime, an open-weights live music model
that can be steered using text or audio prompts to control acoustic style. On
automatic metrics of music quality, Magenta RealTime outperforms other
open-weights music generation models, despite using fewer parameters and
offering first-of-its-kind live generation capabilities. We also release Lyria
RealTime, an API-based model with extended controls, offering access to our
most powerful model with wide prompt coverage. These models demonstrate a new
paradigm for AI-assisted music creation that emphasizes human-in-the-loop
interaction for live music performance.

</details>

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 8]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-Time Doppler and Ionospheric Dispersion Correction Techniques for Arbitrary Waveforms Utilizing GPU Compute](https://arxiv.org/abs/2508.04951)
*Daniel J. Vickers,A. H. Mack,Idahosa A. Osaretin*

Main category: eess.SP

TL;DR: 论文分析了雷达数字信号处理中的多普勒和电离层失真校正算法，提出了基于FFT和数值插值的方法，并在GPU上实现了实时处理。


<details>
  <summary>Details</summary>
Motivation: 传统雷达信号处理依赖专用硬件，限制了波形灵活性和系统复杂性。现代通用计算系统的发展使得实时数字信号处理成为可能。

Method: 提出了两种算法：基于FFT的电离层色散校正和基于sinc插值的多普勒色散校正，并在NVIDIA H100 GPU上实现。

Result: 两种算法在精度上与传统波形专用方法相当，且能在GPU上实时运行，提高了系统灵活性。

Conclusion: 这些波形无关的算法易于集成到现有软件定义无线电系统中，为雷达信号处理提供了高效解决方案。

Abstract: General requirements for radar digital signal processing are ionospheric
distortion and Doppler dispersion correction, which has historically required
radar-specific hardware to implement in real time. Although analog solutions
are computationally efficient, they often come with system design drawbacks
which limit waveform flexibility and can result in an overall increase of
system complexity. With improvements in modern general compute systems,
real-time digital signal processing is becoming more realizable using
non-radar-specific high-performance compute. In this paper, we present an
analysis of general Doppler and ionospheric correction algorithms for arbitrary
waveforms for radar digital signal processing. We also include considerations
for efficient implementation of these algorithms in software, specifically
using GPU hardware. This analysis includes metrics of performance such as
execution time and error correction accuracy. We also provide recommendations
for application in radar signal processing. We identify two algorithms for
dispersion correction: an FFT-based method for ionospheric dispersion and a
numerical interpolation method via sinc interpolation for Doppler dispersion.
Both of these algorithms are able to compensate for dispersion equivalent in
accuracy to waveform-specific analytical methods and were able to be performed
in real-time on a single NVIDIA H100 GPU. These methods are waveform agnostic
and applied directly to the samples, improving system flexibility and making
them easy to incorporate into existing software-defined radio systems.

</details>


### [2] [Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas](https://arxiv.org/abs/2508.04964)
*Zhaowei Wang,Yunsong Huang,Weicheng Liu,Hui-Ming Wang*

Main category: eess.SP

TL;DR: 提出了一种基于分布式可重构智能超表面天线（RIMSA）的无线传感方法，通过深度强化学习优化波束成形模式，并在干扰环境下实现高精度传感。


<details>
  <summary>Details</summary>
Motivation: 传统射频传感方法在不利传播信道（如衰落和噪声）下精度受限，需要一种更鲁棒的解决方案。

Method: 部署多个RIMSA接收器，利用深度强化学习优化波束成形模式，并设计神经网络将接收信号映射为传感结果。

Result: 仿真表明，分布式RIMSA系统比集中式实现更高效，且在干扰攻击下仍能保持高精度传感。

Conclusion: 分布式RIMSA系统通过智能优化和抗干扰设计，显著提升了无线传感的鲁棒性和准确性。

Abstract: The utilization of radio frequency (RF) signals for wireless sensing has
garnered increasing attention. However, the radio environment is unpredictable
and often unfavorable, the sensing accuracy of traditional RF sensing methods
is often affected by adverse propagation channels from the transmitter to the
receiver, such as fading and noise. In this paper, we propose employing
distributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect
the presence and location of objects where multiple RIMSA receivers (RIMSA Rxs)
are deployed on different places. By programming their beamforming patterns,
RIMSA Rxs can enhance the quality of received signals. The RF sensing problem
is modeled as a joint optimization problem of beamforming pattern and mapping
of received signals to sensing outcomes. To address this challenge, we
introduce a deep reinforcement learning (DRL) algorithm aimed at calculating
the optimal beamforming patterns and a neural network aimed at converting
received signals into sensing outcomes. In addition, the malicious attacker may
potentially launch jamming attack to disrupt sensing process. To enable
effective sensing in interferenceprone environment, we devise a combined loss
function that takes into account the Signal to Interference plus Noise Ratio
(SINR) of the received signals. The simulation results show that the proposed
distributed RIMSA system can achieve more efficient sensing performance and
better overcome environmental influences than centralized implementation.
Furthermore, the introduced method ensures high-accuracy sensing performance
even under jamming attack.

</details>


### [3] [Localized Kernel Methods for Signal Processing](https://arxiv.org/abs/2508.04978)
*Sippanon Kitimoon*

Main category: eess.SP

TL;DR: 论文提出了两种基于局部核的信号处理方法，用于噪声条件下的参数恢复。第一种方法针对多维指数模型的频率和幅度估计，第二种方法用于分离线性调频信号。两种方法均基于局部核和高效FFT实现，实验证明其鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决噪声条件下信号参数恢复的挑战，特别是在低信噪比和多维信号场景中。

Method: 1. 使用局部三角多项式核检测多维频率，并通过坐标投影和配准提高恢复精度；2. 构建局部核的SSO变体，通过FFT滤波和分段线性回归分离线性调频信号。

Result: 在低信噪比下优于传统方法（如MUSIC和ESPRIT），并能以较少样本实现高精度恢复；成功分离信噪比低至-30 dB的交叉和不连续调频信号。

Conclusion: 基于局部核的方法在噪声条件下表现优异，无需子空间分解或稀疏正则化，具有扩展潜力。

Abstract: This dissertation presents two signal processing methods using specially
designed localized kernels for parameter recovery under noisy condition. The
first method addresses the estimation of frequencies and amplitudes in
multidimensional exponential models. It utilizes localized trigonometric
polynomial kernels to detect the multivariate frequencies, followed by a more
detailed parameter estimation. We compare our method with MUSIC and ESPRIT,
which are classical subspace-based algorithms widely used for estimating the
parameters of exponential signals. In the univariate case, the method
outperforms MUSIC and ESPRIT under low signal-to-noise ratios. For the
multivariate case, we develop a coordinate-wise projection and registration
approach that achieves high recovery accuracy using significantly fewer samples
than other methods.
  The second method focuses on separating linear chirp components from
time-localized signal segments. A variant of the Signal Separation Operator
(SSO) is constructed using a localized kernel. Instantaneous frequency
estimates are obtained via FFT-based filtering, then clustered and fitted with
piecewise linear regression. The method operates without prior knowledge of the
number of components and is shown to recover intersecting and discontinuous
chirps at SNR levels as low as -30 dB.
  Both methods share an idea based on localized kernels and efficient FFT-based
implementation, and neither requires subspace decomposition or sparsity
regularization. Experimental results confirm the robustness and tractability of
the proposed approaches across a range of simulated data conditions. Potential
extensions include application to nonlinear chirps, adaptive kernel design, and
signal classification using extracted features.

</details>


### [4] [Power-Constrained and Quantized MIMO-RSMA Systems with Imperfect CSIT: Joint Precoding, Antenna Selection, and Power Control](https://arxiv.org/abs/2508.05080)
*Jiwon Sung,Seokjun Park,Jinseok Choi*

Main category: eess.SP

TL;DR: 提出了一种联合预编码、天线选择和功率控制算法，以最大化基站功率预算下的总频谱效率。


<details>
  <summary>Details</summary>
Motivation: 充分利用基站的功率预算潜力，优化多用户MIMO系统的频谱效率。

Method: 通过条件平均速率方法处理不完美CSIT，将问题分解为预编码方向和功率控制子问题，分别用拉格朗日驻点和梯度下降求解。

Result: 仿真验证了算法的有效性，并发现8-11位中等分辨率DAC比低分辨率DAC更高效。

Conclusion: 所提算法能有效提升系统性能，中等分辨率DAC在功率效率上更具优势。

Abstract: To utilize the full potential of the available power at a base station (BS),
we propose a joint precoding, antenna selection, and transmit power control
algorithm for a total power budget at the BS. We formulate a sum spectral
efficiency (SE) maximization problem for downlink multi-user multiple-input
multiple-output (MIMO) rate-splitting multiple access (RSMA) systems with
arbitrary-resolution digital-to-analog converters (DACs). We reformulate the
problem by defining the ergodic sum SE using the conditional average rate
approach to handle imperfect channel state information at the transmitter
(CSIT), and by using approximation techniques to make the problem more
tractable. Then, we decompose the problem into precoding direction and power
control subproblems. We solve the precoding direction subproblem by identifying
a superior Lagrangian stationary point, and the power control subproblem using
gradient descent. We also propose a complexity-reduction approach that is more
suitable for massive MIMO systems. Simulation results not only validate the
proposed algorithm but also reveal that when utilizing the full potential of
the power budget at the BS, medium-resolution DACs with 8-11 bits may actually
be more power-efficient than low-resolution DACs.

</details>


### [5] [Digital Twin Channel-Aided CSI Prediction: A Environment-based Subspace Extraction Approach for Achieving Low Overhead and Robustness](https://arxiv.org/abs/2508.05142)
*Yichen Cai,Jianhua Zhang,Li Yu,Zhen Zhang,Yuxiang Zhang,Lianzheng Shi,Yuelong Qiu*

Main category: eess.SP

TL;DR: 论文提出了一种基于环境特定信道子空间基（EB）的部分到全信道状态信息（CSI）预测方法（EB-P2WCP），用于实现低开销的信道预测。该方法结合数字孪生地图中的静态环境信息和实时CSI，通过设计的EB-P2WNet网络，显著降低了开销并提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了满足6G移动通信系统在复杂场景中对鲁棒性和高速通信的需求，减少系统开销，论文提出了一种基于感知和AI的数字孪生信道（DTC）技术。

Method: 利用数字孪生地图提取环境特定信道子空间基（EB）作为环境先验信息，结合实时估计的局部CSI，设计EB-P2WNet网络预测全空间-频率域信道。

Result: 仿真结果表明，该方法在低信噪比和导频比例条件下显著降低开销（最高50%），对多用户干扰具有鲁棒性，并能快速预测未来信道状态（1.3毫秒内）。

Conclusion: EB-P2WCP方法通过结合环境先验信息和实时CSI，实现了高效、鲁棒的信道预测，为6G通信系统提供了可行的低开销解决方案。

Abstract: To meet the robust and high-speed communication requirements of the
sixth-generation (6G) mobile communication system in complex scenarios,
sensing- and artificial intelligence (AI)-based digital twin channel (DTC)
techniques become a promising approach to reduce system overhead. In this
paper, we propose an environment-specific channel subspace basis (EB)-aided
partial-to-whole channel state information (CSI) prediction method (EB-P2WCP)
for realizing DTC-enabled low-overhead channel prediction. Specifically, EB is
utilized to characterize the static properties of the electromagnetic
environment, which is extracted from the digital twin map, serving as
environmental information prior to the prediction task. Then, we fuse EB with
real-time estimated local CSI to predict the entire spatial-frequency domain
channel for both the present and future time instances. Hence, an EB-based
partial-to-whole CSI prediction network (EB-P2WNet) is designed to achieve a
robust channel prediction scheme in various complex scenarios. Simulation
results indicate that incorporating EB provides significant benefits under low
signal-to-noise ratio and pilot ratio conditions, achieving a reduction of up
to 50% in pilot overhead. Additionally, the proposed method maintains
robustness against multi-user interference, tolerating 3-meter localization
errors with only a 0.5 dB NMSE increase, and predicts CSI for the next channel
coherent time within 1.3 milliseconds.

</details>


### [6] [Optimization of Liquid Lens-based Imaging Receiver for MIMO VLC Systems](https://arxiv.org/abs/2508.05204)
*Kapila W. S. Palitharathna,Christodoulos Skouroumounis,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 提出了一种基于液体透镜的成像接收器，用于MIMO可见光通信系统，通过动态调整液体透镜的焦距和方向角，降低MIMO信道增益的空间相关性，从而提升误码率性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态透镜在动态条件下（如用户移动和接收器随机方向）适应性不足，液体透镜因其动态调整能力成为更优选择。

Method: 开发了精确的数学模型描述系统信道增益，并提出了两种透镜调整方案（CLS和VULO）以最小化误码率。

Result: 数值结果表明，液体透镜系统在随机接收器方向条件下显著优于静态透镜系统，误码率从4×10^-2降至5×10^-4。

Conclusion: 液体透镜在动态环境中具有显著优势，能有效提升MIMO可见光通信系统的性能。

Abstract: In this paper, a liquid lens-based imaging receiver is proposed for
multiple-input multiple-output (MIMO) visible light communication (VLC)
systems. By dynamically adjusting the focal length and orientation angles of
the liquid lens, the spatial correlation between MIMO channel gains is reduced,
leading to enhanced bit-error rate (BER) performance. Unlike static lenses,
liquid lenses offer adaptability in dynamic conditions, including user mobility
and random receiver orientation. An accurate mathematical framework is
developed to model the channel gains of the proposed system, and an
optimization problem is formulated to minimize its BER. Due to the complexity
of the resulting channel model, two lens adjustment schemes, namely, ($i$) the
CLS scheme, and ($ii$) the VULO scheme are introduced. Numerical results
demonstrate that the proposed liquid lens-based system offers substantial BER
improvements over conventional static lens-based receivers across a wide range
of random receiver orientation conditions. Specifically, at a random receiver
orientation variance of $10^{\circ}$, the BER is improved from $4\times
10^{-2}$ to $5\times 10^{-4}$ by employing the proposed liquid lens.

</details>


### [7] [Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios](https://arxiv.org/abs/2508.05226)
*Junzhe Song,Ruisi He,Mi Yang,Zhengyu Zhang,Bingcheng Liu,Jiahui Han,Haoxiang Zhang,Bo Ai*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的ISAC框架，用于车辆环境重建，解决了动态场景跟踪精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC方法在动态场景重建中精度和时间一致性不足，限制了实际应用。

Method: 建立多模态测量数据集，开发多阶段深度学习网络（场景解码器、聚类中心解码器和点云解码器）进行环境重建。

Result: 实验显示方法在Chamfer Distance（0.29）和F Score@1%（0.87）上表现优异，复杂度分析证明其高效实用。

Conclusion: 该方法为未来智能交通提供了低成本环境重建的途径。

Abstract: Integrated Sensing and Communication (ISAC) technology plays a critical role
in future intelligent transportation systems, by enabling vehicles to perceive
and reconstruct the surrounding environment through reuse of wireless signals,
thereby reducing or even eliminating the need for additional sensors such as
LiDAR or radar. However, existing ISAC based reconstruction methods often lack
the ability to track dynamic scenes with sufficient accuracy and temporal
consistency, limiting the real world applicability. To address this limitation,
we propose a deep learning based framework for vehicular environment
reconstruction by using ISAC channels. We first establish a joint channel
environment dataset based on multi modal measurements from real world urban
street scenarios. Then, a multistage deep learning network is developed to
reconstruct the environment. Specifically, a scene decoder identifies the
environmental context such as buildings, trees and so on; a cluster center
decoder predicts coarse spatial layouts by localizing dominant scattering
centers; a point cloud decoder recovers fine grained geometry and structure of
surrounding environments. Experimental results demonstrate that the proposed
method achieves high-quality dynamic environment reconstruction with a Chamfer
Distance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis
demonstrates the efficiency and practical applicability of the method in real
time scenarios. This work provides a pathway toward low cost environment
reconstruction based on ISAC for future intelligent transportation.

</details>


### [8] [Unifying Common Signal Analyses with Instantaneous Time-Frequency Atoms](https://arxiv.org/abs/2508.05380)
*Steven Sandoval,Phillip L. De Leon*

Main category: eess.SP

TL;DR: 本文提出了一种基于瞬时时间-频率原子的方法，用于计算与多种常见信号分析相关的瞬时频谱（IS），并通过封闭形式表达式统一这些分析。


<details>
  <summary>Details</summary>
Motivation: 先前的研究提出了瞬时时间-频率分析的通用框架，但未提供具体计算瞬时频谱的方法。本文旨在填补这一空白。

Method: 使用瞬时时间-频率原子，将信号分析视为AM-FM分量的分解，并利用二次chirplet模板进行统一。

Result: 通过两参数二次chirplet，将各种IS组织成二维连续体，并通过示例信号验证了封闭形式IS的计算。

Conclusion: 本文展示了通用框架如何统一多种信号分析，并提供了具体的IS计算方法。

Abstract: In previous work, we presented a general framework for instantaneous
time-frequency analysis but did not provide any specific details of how to
compute a particular instantaneous spectrum (IS). In this work, we use
instantaneous time-frequency atoms to obtain an IS associated with common
signal analyses: time domain analysis, frequency domain analysis, fractional
Fourier transform, synchrosqueezed short-time Fourier transform, and
synchrosqueezed short-time fractional Fourier transform. By doing so, we
demonstrate how the general framework can be used to unify these analyses and
we develop closed-form expressions for the corresponding ISs. This is
accomplished by viewing these analyses as decompositions into AM--FM components
and recognizing that each uses a specialized (or limiting) form of a quadratic
chirplet as a template during analysis. With a two-parameter quadratic
chirplet, we can organize these ISs into a 2D continuum with points in the
plane corresponding to a decomposition related to one of the signal analyses.
Finally, using several example signals, we compute in closed-form the ISs for
the various analyses.

</details>


### [9] [Sub- μ W Battery-Less and Oscillator-Less Wi-Fi Backscattering Transmitter Reusing RF Signal for Harvesting, Communications, and Motion Detection](https://arxiv.org/abs/2508.05479)
*Marco Privitera,Andrea Ballo,Karim Ali Ahmed,Alfio Dario Grasso,Massimo Alioto*

Main category: eess.SP

TL;DR: 本文提出了一种亚微瓦级功耗的802.11b反向散射发射器，实现了射频能量收集、反向散射通信和位置/运动传感的多功能集成。


<details>
  <summary>Details</summary>
Motivation: 通过消除电池和外部运动传感器（如MEMS），实现设备的超小型化、长寿命和低成本。

Method: 采用双音入射波的二阶互调提取频率，消除本地振荡器，突破WiFi发射器的微瓦功耗限制。

Result: 实现了低至-19 dBm的累积能量收集/传输/传感灵敏度，并通过采集电压作为RSS代理实现位置/运动传感。

Conclusion: 该技术为无电池、低成本、超小型化的物联网设备提供了新解决方案。

Abstract: In this paper, a sub-uW power 802.11b backscattering transmitter is presented
to enable reuse of the same incident wave for three purposes: RF harvesting,
backscattering communications and position/motion sensing. The removal of the
battery and any off-chip motion sensor (e.g., MEMS) enables unprecedented level
of miniaturization and ubiquity, unrestricted device lifespan, low fabrication
and maintenance cost. The uW power wall for WiFi transmitters is broken for the
first time via local oscillator elimination, as achieved by extracting its
frequency through second-order intermodulation of a twotone incident wave. The
two-tone scheme also enables a cumulative harvesting/transmission/sensing
sensitivity down to Pmin -19 dBm. Position/motion sensing is enabled by using
the harvested voltage as a proxy for the Received Signal Strength (RSS),
allowing to sense the chip location with respect to the tone generator(s)
shared across tags in indoor neighborhoods.

</details>


### [10] [0.6-V, uW-Power 4-Stage OTA with Minimal Components and 100X Load Range](https://arxiv.org/abs/2508.05499)
*M. Privitera,A. D. Grasso,A. Ballo,M. Alioto*

Main category: eess.SP

TL;DR: 提出了一种用于超低功耗应用的四级运算跨导放大器（OTA），通过简化的频率补偿设计，减少了晶体管和被动元件数量，同时显著提高了功率效率。


<details>
  <summary>Details</summary>
Motivation: 传统四级OTA的补偿复杂且功率效率低，需要一种简化设计并提高效率的解决方案。

Method: 采用四阶段OTA设计，结合简化的频率补偿技术，减少晶体管和被动元件数量。

Result: 功率效率显著提升（大信号/小信号分别提高3.7倍和11.3倍），负载电容范围宽（最大/最小比>100倍），稳定性高。

Conclusion: 该设计在保持稳定性的同时，简化了四级OTA的补偿问题，并显著提高了功率效率。

Abstract: A four-stage operational transconductance amplifier (OTA) for ultra-low-power
applications is introduced in this paper. The proposed circuit inclusive of
frequency compensation requires minimal transistor count and passives,
overcoming the traditionally difficult compensation of 4-stage OTAs and
bringing it back to the simplicity of 3-stage OTAs. At the same time, the
proposed circuit achieves high power efficiency, as evidenced by the >3.7X
(>11.3X) improvement in the large-signal (small-signal) power efficiency figure
of merit FOML (FOMS), compared to prior 4-stage OTAs (sub-1 V multi-stage
OTAs). Thanks to the lower sensitivity of the phase margin to the load
capacitance, the proposed OTA remains stable under a wide range of loads
(double-sided as in any 3-4-stage OTA), achieving a max/min ratio of the load
capacitance of >100X.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices](https://arxiv.org/abs/2508.04857)
*Yael Segal-Feldman,Ann R. Bradlow,Matthew Goldrick,Joseph Keshet*

Main category: eess.AS

TL;DR: 本文提出了一种用于小型设备的开放词汇关键词检测模型，结合语音编码器、目标关键词编码器和检测网络，实现了高精度检测。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇关键词检测任务，尤其是在小型设备上的高效实现。

Method: 模型由语音编码器（Tiny Whisper或Tiny Conformer）、目标关键词编码器（超网络生成关键词特定权重）和检测网络（Perceiver模块）组成。

Result: 系统在检测性能和泛化能力上达到最新水平，最小模型（420万参数）性能优于更大模型。

Conclusion: 该模型高效且鲁棒，适用于开放词汇关键词检测任务。

Abstract: Open-vocabulary keyword spotting (KWS) refers to the task of detecting words
or terms within speech recordings, regardless of whether they were included in
the training data. This paper introduces an open-vocabulary keyword spotting
model with state-of-the-art detection accuracy for small-footprint devices. The
model is composed of a speech encoder, a target keyword encoder, and a
detection network. The speech encoder is either a tiny Whisper or a tiny
Conformer. The target keyword encoder is implemented as a hyper-network that
takes the desired keyword as a character string and generates a unique set of
weights for a convolutional layer, which can be considered as a
keyword-specific matched filter. The detection network uses the matched-filter
weights to perform a keyword-specific convolution, which guides the
cross-attention mechanism of a Perceiver module in determining whether the
target term appears in the recording. The results indicate that our system
achieves state-of-the-art detection performance and generalizes effectively to
out-of-domain conditions, including second-language (L2) speech. Notably, our
smallest model, with just 4.2 million parameters, matches or outperforms models
that are several times larger, demonstrating both efficiency and robustness.

</details>


### [12] [Closed-Form Successive Relative Transfer Function Vector Estimation based on Blind Oblique Projection Incorporating Noise Whitening](https://arxiv.org/abs/2508.04887)
*Henri Gode,Simon Doclo*

Main category: eess.AS

TL;DR: 本文提出了一种改进的盲斜投影（BOP）方法，用于在噪声和混响环境中在线估计多个声源的相对传递函数（RTF），解决了传统方法的高计算复杂性和低信噪比下的性能问题。


<details>
  <summary>Details</summary>
Motivation: 在噪声和混响环境中，传统BOP方法在估计多个声源的RTF时存在计算复杂度高、随机向量引入性能下降以及高信噪比假设的局限性。

Method: 提出了三种改进：1）推导BOP成本函数的闭式解以降低计算复杂度；2）使用正交附加向量替代随机向量；3）引入噪声处理技术以增强低信噪比下的鲁棒性。此外，提出了一种基于空间一致性的在线声源计数方法。

Result: 仿真实验表明，改进方法在真实混响噪声环境中对3个连续激活的声源RTF估计更准确，且无需先验声源活动信息。

Conclusion: 改进的BOP方法显著提升了RTF估计的效率和准确性，适用于复杂声学环境。

Abstract: Relative transfer functions (RTFs) of sound sources play a crucial role in
beamforming, enabling effective noise and interference suppression. This paper
addresses the challenge of online estimating the RTF vectors of multiple sound
sources in noisy and reverberant environments, for the specific scenario where
sources activate successively. While the RTF vector of the first source can be
estimated straightforwardly, the main challenge arises in estimating the RTF
vectors of subsequent sources during segments where multiple sources are
simultaneously active. The blind oblique projection (BOP) method has been
proposed to estimate the RTF vector of a newly activating source by optimally
blocking this source. However, this method faces several limitations: high
computational complexity due to its reliance on iterative gradient descent
optimization, the introduction of random additional vectors, which can
negatively impact performance, and the assumption of high signal-to-noise ratio
(SNR). To overcome these limitations, in this paper we propose three extensions
to the BOP method. First, we derive a closed-form solution for optimizing the
BOP cost function, significantly reducing computational complexity. Second, we
introduce orthogonal additional vectors instead of random vectors, enhancing
RTF vector estimation accuracy. Third, we incorporate noise handling techniques
inspired by covariance subtraction and whitening, increasing robustness in low
SNR conditions. To provide a frame-by-frame estimate of the source activity
pattern, required by both the conventional BOP method and the proposed method,
we propose a spatial-coherence-based online source counting method. Simulations
are performed with real-world reverberant noisy recordings featuring 3
successively activating speakers, with and without a-priori knowledge of the
source activity pattern.

</details>


### [13] [REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with Diffusion Transformers](https://arxiv.org/abs/2508.04996)
*Yuepeng Jiang,Ziqian Ning,Shuai Wang,Chengjia Wang,Mengxiao Bi,Pengcheng Zhu,Lei Xie,Zhonghua Fu*

Main category: eess.AS

TL;DR: REF-VC是一种抗噪声且富有表现力的语音转换系统，通过随机擦除策略和隐式对齐提升性能，并在实验中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决现实语音转换中环境噪声和用户对表现力输出的需求问题，传统方法在抗噪性和表现力上存在不足。

Method: 提出随机擦除策略减少SSL特征冗余，隐式对齐抑制非必要特征重建，并集成快捷模型加速推理。

Result: 在噪声集上优于Seed-VC等基线模型，在干净集上表现相当，且兼容歌唱语音转换。

Conclusion: REF-VC在抗噪性和表现力上取得平衡，具有实际应用潜力。

Abstract: In real-world voice conversion applications, environmental noise in source
speech and user demands for expressive output pose critical challenges.
Traditional ASR-based methods ensure noise robustness but suppress prosody,
while SSL-based models improve expressiveness but suffer from timbre leakage
and noise sensitivity. This paper proposes REF-VC, a noise-robust expressive
voice conversion system. Key innovations include: (1) A random erasing strategy
to mitigate the information redundancy inherent in SSL feature, enhancing noise
robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to
suppress non-essential feature reconstruction; (3) Integration of Shortcut
Models to accelerate flow matching inference, significantly reducing to 4
steps. Experimental results demonstrate that our model outperforms baselines
such as Seed-VC in zero-shot scenarios on the noisy set, while also performing
comparably to Seed-VC on the clean set. In addition, REF-VC can be compatible
with singing voice conversion within one model.

</details>


### [14] [MOVER: Combining Multiple Meeting Recognition Systems](https://arxiv.org/abs/2508.05055)
*Naoyuki Kamo,Tsubasa Ochiai,Marc Delcroix,Tomohiro Nakatani*

Main category: eess.AS

TL;DR: MOVER是一种新的会议识别系统组合方法，首次结合了不同说话人分割和语音识别输出的会议识别系统，通过五阶段流程显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（如DOVER和ROVER）仅能分别结合说话人分割或语音识别系统的输出，而MOVER填补了同时结合两者的空白。

Method: MOVER通过五阶段流程（包括说话人对齐、分段分组、词和时间组合等）结合不同时间间隔和说话人标签的假设。

Result: 在CHiME-8 DASR和NOTSOFAR-1任务中，MOVER相对现有技术分别提升了9.55%和8.51%的tcpWER。

Conclusion: MOVER成功结合了多样化的会议识别系统输出，显著提升了性能，填补了现有方法的不足。

Abstract: In this paper, we propose Meeting recognizer Output Voting Error Reduction
(MOVER), a novel system combination method for meeting recognition tasks.
Although there are methods to combine the output of diarization (e.g., DOVER)
or automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first
approach that can combine the outputs of meeting recognition systems that
differ in terms of both diarization and ASR. MOVER combines hypotheses with
different time intervals and speaker labels through a five-stage process that
includes speaker alignment, segment grouping, word and timing combination, etc.
Experimental results on the CHiME-8 DASR task and the multi-channel track of
the NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple
meeting recognition systems with diverse diarization and recognition outputs,
achieving relative tcpWER improvements of 9.55 % and 8.51 % over the
state-of-the-art systems for both tasks.

</details>


### [15] [Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS](https://arxiv.org/abs/2508.05102)
*Anuprabha M,Krishna Gurugubelli,Anil Kumar Vuppala*

Main category: eess.AS

TL;DR: 研究探讨F5-TTS在克隆构音障碍语音时的效果，发现其对语音清晰度的偏好强于说话人相似性和韵律保留，并分析了公平性问题。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音数据稀缺，现有语音合成技术可能引入偏见，需研究其效果和公平性。

Method: 使用TORGO数据集评估F5-TTS在克隆构音障碍语音时的表现，关注清晰度、说话人相似性和韵律保留，并采用公平性指标分析偏见。

Result: F5-TTS在构音障碍语音合成中更偏向语音清晰度，而非说话人相似性和韵律保留，存在公平性问题。

Conclusion: 研究结果有助于开发更公平的构音障碍语音合成技术，推动包容性语音技术的发展。

Abstract: Dysarthric speech poses significant challenges in developing assistive
technologies, primarily due to the limited availability of data. Recent
advances in neural speech synthesis, especially zero-shot voice cloning,
facilitate synthetic speech generation for data augmentation; however, they may
introduce biases towards dysarthric speech. In this paper, we investigate the
effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using
TORGO dataset, focusing on intelligibility, speaker similarity, and prosody
preservation. We also analyze potential biases using fairness metrics like
Disparate Impact and Parity Difference to assess disparities across dysarthric
severity levels. Results show that F5-TTS exhibits a strong bias toward speech
intelligibility over speaker and prosody preservation in dysarthric speech
synthesis. Insights from this study can help integrate fairness-aware
dysarthric speech synthesis, fostering the advancement of more inclusive speech
technologies.

</details>


### [16] [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
*Seraphina Fong,Marco Matassoni,Alessio Brutti*

Main category: eess.AS

TL;DR: 该论文研究了在低资源语言环境下使用语音大语言模型（Speech LLMs）进行自动语音识别（ASR），通过SLAM-ASR框架连接语音编码器和大语言模型，探讨了数据稀缺的挑战及多语言预训练模型的优势。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在低资源语言自动语音识别中的适用性，解决数据稀缺问题。

Method: 采用SLAM-ASR框架，通过轻量级可训练投影器连接语音编码器和大语言模型，评估训练数据需求及多语言预训练模型的效果。

Result: 多语言预训练模型（如EuroLLM、Salamandra）在低资源环境下表现优异，尤其是小训练集时。

Conclusion: 研究为优化低资源语言和多语言环境下的语音大语言模型提供了重要见解。

Abstract: Large language models (LLMs) have demonstrated potential in handling spoken
inputs for high-resource languages, reaching state-of-the-art performance in
various tasks. However, their applicability is still less explored in
low-resource settings. This work investigates the use of Speech LLMs for
low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a
trainable lightweight projector connects a speech encoder and a LLM. Firstly,
we assess training data volume requirements to match Whisper-only performance,
re-emphasizing the challenges of limited data. Secondly, we show that
leveraging mono- or multilingual projectors pretrained on high-resource
languages reduces the impact of data scarcity, especially with small training
sets. Using multilingual LLMs (EuroLLM, Salamandra) with
whisper-large-v3-turbo, we evaluate performance on several public benchmarks,
providing insights for future research on optimizing Speech LLMs for
low-resource languages and multilinguality.

</details>


### [17] [Privacy Disclosure of Similarity in Speech and Language Processing](https://arxiv.org/abs/2508.05250)
*Tom Bäckström,Mohammad Hassan Vali,My Nguyen,Silas Rech*

Main category: eess.AS

TL;DR: 论文提出了一种量化相似性排名隐私泄露的方法，通过估计其概率分布来评估身份识别中的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 由于数据噪声和相似性度量不准确，传统的身份识别方法可能不可靠，但相似性排名仍可能泄露隐私信息。

Method: 基于真实说话者的相似性排名直方图或Beta-二项分布建模，以熵（比特）表示隐私泄露。

Result: 实验表明，所有测试的说话者和作者特征均包含可识别个人的信息，其中说话者识别算法的嵌入信息最多。

Conclusion: 相似性排名泄露指标可用于比较不同生物特征的隐私泄露程度，并帮助全面评估隐私威胁。

Abstract: Speaker, author, and other biometric identification applications often
compare a sample's similarity to a database of templates to determine the
identity. Given that data may be noisy and similarity measures can be
inaccurate, such a comparison may not reliably identify the true identity as
the most similar. Still, even the similarity rank based on an inaccurate
similarity measure can disclose private information about the true identity. We
propose a methodology for quantifying the privacy disclosure of such a
similarity rank by estimating its probability distribution. It is based on
determining the histogram of the similarity rank of the true speaker, or when
data is scarce, modeling the histogram with the beta-binomial distribution. We
express the disclosure in terms of entropy (bits), such that the disclosure
from independent features are additive. Our experiments demonstrate that all
tested speaker and author characterizations contain personally identifying
information (PII) that can aid in identification, with embeddings from speaker
recognition algorithms containing the most information, followed by phone
embeddings, linguistic embeddings, and fundamental frequency. Our initial
experiments show that the disclosure of PII increases with the length of test
samples, but it is bounded by the length of database templates. The provided
metric, similarity rank disclosure, provides a way to compare the disclosure of
PII between biometric features and merge them to aid identification. It can
thus aid in the holistic evaluation of threats to privacy in speech and other
biometric technologies.

</details>


### [18] [Investigation of Speech and Noise Latent Representations in Single-channel VAE-based Speech Enhancement](https://arxiv.org/abs/2508.05293)
*Jiatong Li,Simon Doclo*

Main category: eess.AS

TL;DR: 研究探讨了变分自编码器（VAE）中语音和噪声潜在表示对语音增强性能的影响，发现分离的表示能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索不同潜在表示对基于VAE的单通道语音增强系统性能的影响。

Method: 使用预训练的VAE获取语音和噪声的潜在表示，并通过修改损失项研究其对增强性能的影响。

Result: 实验表明，语音和噪声潜在表示分离的VAE比重叠表示的标准VAE性能更优。

Conclusion: 清晰的语音和噪声潜在表示分离对提升语音增强性能至关重要。

Abstract: Recently, a variational autoencoder (VAE)-based single-channel speech
enhancement system using Bayesian permutation training has been proposed, which
uses two pretrained VAEs to obtain latent representations for speech and noise.
Based on these pretrained VAEs, a noisy VAE learns to generate speech and noise
latent representations from noisy speech for speech enhancement. Modifying the
pretrained VAE loss terms affects the pretrained speech and noise latent
representations. In this paper, we investigate how these different
representations affect speech enhancement performance. Experiments on the DNS3,
WSJ0-QUT, and VoiceBank-DEMAND datasets show that a latent space where speech
and noise representations are clearly separated significantly improves
performance over standard VAEs, which produce overlapping speech and noise
representations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [19] [Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS](https://arxiv.org/abs/2508.04721)
*Vignesh Ethiraj,Ashwath David,Sidhanth Menon,Divya Vijay*

Main category: cs.SD

TL;DR: 论文介绍了一种低延迟的电信AI语音代理管道，结合了四个专用模型，用于实时交互式电信应用，如呼叫中心自动化和智能客户支持。


<details>
  <summary>Details</summary>
Motivation: 为电信行业提供高效、低延迟的语音AI解决方案，以支持实时交互和知识驱动的语音交互。

Method: 结合了四个专用模型（TSLAM、T-VEC、TTE、T-Synth），实现流式ASR、对话智能、检索增强生成和实时TTS。

Result: 系统在500条电信问题数据集上测试，显示模型实时性能优异（RTF低于1.0），适用于企业级低延迟部署。

Conclusion: 该框架为下一代电信AI奠定了基础，支持自动化客户支持和诊断等应用。

Abstract: We introduce a low-latency telecom AI voice agent pipeline for real-time,
interactive telecommunications use, enabling advanced voice AI for call center
automation, intelligent IVR (Interactive Voice Response), and AI-driven
customer support. The solution is built for telecom, combining four specialized
models by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language
Model (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific
Automatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific
Text-to-Speech (TTS) model. These models enable highly responsive,
domain-adapted voice AI agents supporting knowledge-grounded spoken
interactions with low latency. The pipeline integrates streaming ASR (TTE),
conversational intelligence (TSLAM), retrieval augmented generation (RAG) over
telecom documents, and real-time TTS (T-Synth), setting a new benchmark for
telecom voice assistants. To evaluate the system, we built a dataset of 500
human-recorded telecom questions from RFCs, simulating real telecom agent
queries. This framework allows analysis of latency, domain relevance, and
real-time performance across the stack. Results show that TSLAM, TTE, and
T-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,
low-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and
T-Synth -- provide a foundation for next-generation telecom AI, enabling
automated customer support, diagnostics, and more.

</details>


### [20] [Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion](https://arxiv.org/abs/2508.04723)
*Sha Zhao,Song Yi,Yangxuan Zhou,Jiadong Pan,Jiquan Wang,Jie Xia,Shijian Li,Shurong Dong,Gang Pan*

Main category: cs.SD

TL;DR: MEEtBrain是一个便携式多模态框架，用于情绪分析，结合AI生成音乐和无线EEG-fNIRS采集，解决了现有研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 音乐情感计算在心理健康中很重要，但现有研究存在音乐刺激受限、单模态数据依赖和便携性不足的问题。

Method: 提出MEEtBrain框架，使用AI生成音乐刺激，并通过无线头带设备同步采集EEG和fNIRS数据。

Result: 收集了14小时的数据集，验证了框架的有效性，并公开数据集以促进研究。

Conclusion: MEEtBrain解决了现有技术的局限性，为音乐情感计算提供了更高效和便携的解决方案。

Abstract: Emotions critically influence mental health, driving interest in music-based
affective computing via neurophysiological signals with Brain-computer
Interface techniques. While prior studies leverage music's accessibility for
emotion induction, three key limitations persist: \textbf{(1) Stimulus
Constraints}: Music stimuli are confined to small corpora due to copyright and
curation costs, with selection biases from heuristic emotion-music mappings
that ignore individual affective profiles. \textbf{(2) Modality Specificity}:
Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights
from cross-modal signal fusion.\textbf{ (3) Portability Limitation}: Cumbersome
setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability
due to procedural complexity and portability barriers. To address these
limitations, we propose MEEtBrain, a portable and multimodal framework for
emotion analysis (valence/arousal), integrating AI-generated music stimuli with
synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the
music stimuli can be automatically generated by AI on a large scale,
eliminating subjective selection biases while ensuring music diversity. We use
our developed portable device that is designed in a lightweight headband-style
and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A
14-hour dataset from 20 participants was collected in the first recruitment to
validate the framework's efficacy, with AI-generated music eliciting target
emotions (valence/arousal). We are actively expanding our multimodal dataset
(44 participants in the latest dataset) and make it publicly available to
promote further research and practical applications. \textbf{The dataset is
available at https://zju-bmi-lab.github.io/ZBra.

</details>


### [21] [Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation](https://arxiv.org/abs/2508.05011)
*Huaicheng Zhang,Wei Tan,Guangzheng Li,Yixuan Zhang,Hangting Chen,Shun Lei,Chenyu Yang,Zhiyong Wu,Shuai Wang,Qijun Huang,Dong Yu*

Main category: cs.SD

TL;DR: 提出了一种基于强化学习的框架，通过偏好优化控制歌词到歌曲生成中的幻觉问题，显著减少了音素错误率。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在歌词到歌曲生成中存在内容幻觉问题，导致输出与输入歌词不一致，影响音乐连贯性。

Method: 开发了基于音素错误率和规则过滤的偏好数据集，并在强化学习框架中评估了三种优化策略：DPO、PPO和GRPO。

Result: DPO显著降低了7.4%的音素错误率，PPO和GRPO分别降低了4.9%和4.7%，同时保持了音乐质量。

Conclusion: 该框架为歌词到歌曲生成中的幻觉控制提供了系统性解决方案，并具有扩展到音乐风格和音乐性增强的潜力。

Abstract: Recent advances in audio-based generative language models have accelerated
AI-driven lyric-to-song generation. However, these models frequently suffer
from content hallucination, producing outputs misaligned with the input lyrics
and undermining musical coherence. Current supervised fine-tuning (SFT)
approaches, limited by passive label-fitting, exhibit constrained
self-improvement and poor hallucination mitigation. To address this core
challenge, we propose a novel reinforcement learning (RL) framework leveraging
preference optimization for hallucination control. Our key contributions
include: (1) Developing a robust hallucination preference dataset constructed
via phoneme error rate (PER) computation and rule-based filtering to capture
alignment with human expectations; (2) Implementing and evaluating three
distinct preference optimization strategies within the RL framework: Direct
Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group
Relative Policy Optimization (GRPO). DPO operates off-policy to enhance
positive token likelihood, achieving a significant 7.4% PER reduction. PPO and
GRPO employ an on-policy approach, training a PER-based reward model to
iteratively optimize sequences via reward maximization and KL-regularization,
yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective
and subjective evaluations confirm that our methods effectively suppress
hallucinations while preserving musical quality. Crucially, this work presents
a systematic, RL-based solution to hallucination control in lyric-to-song
generation. The framework's transferability also unlocks potential for music
style adherence and musicality enhancement, opening new avenues for future
generative song research.

</details>


### [22] [SpectroStream: A Versatile Neural Codec for General Audio](https://arxiv.org/abs/2508.05207)
*Yunpeng Li,Kehang Han,Brian McWilliams,Zalan Borsos,Marco Tagliasacchi*

Main category: cs.SD

TL;DR: SpectroStream是一种全频带多通道神经音频编解码器，扩展了SoundStream的能力，支持48 kHz立体声音乐的高质量重建，比特率为4-16 kbps。


<details>
  <summary>Details</summary>
Motivation: 扩展SoundStream的能力，以支持更高采样率和多通道音频的高质量重建。

Method: 采用时频域音频表示的新神经架构，并使用延迟融合策略处理多通道音频。

Result: 在48 kHz立体声音乐中实现了高质量重建，比特率为4-16 kbps。

Conclusion: SpectroStream在更高采样率和多通道音频处理方面表现出色，提供了更好的音频质量。

Abstract: We propose SpectroStream, a full-band multi-channel neural audio codec.
Successor to the well-established SoundStream, SpectroStream extends its
capability beyond 24 kHz monophonic audio and enables high-quality
reconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is
accomplished with a new neural architecture that leverages audio representation
in the time-frequency domain, which leads to better audio quality especially at
higher sample rate. The model also uses a delayed-fusion strategy to handle
multi-channel audio, which is crucial in balancing per-channel acoustic quality
and cross-channel phase consistency.

</details>


### [23] [Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces](https://arxiv.org/abs/2508.05306)
*Mathias Rose Bjare,Stefan Lattner,Gerhard Widmer*

Main category: cs.SD

TL;DR: 该研究探讨了使用自回归扩散模型（ADMs）计算的信息内容（IC）在音乐期望和惊讶建模中的有效性，发现其性能优于生成无限词汇变换器（GIVT）。


<details>
  <summary>Details</summary>
Motivation: 研究目的是验证扩散模型在音乐和音频特征建模中的潜力，尤其是在捕捉惊讶和音频分段边界方面的表现。

Method: 通过两种不同的扩散常微分方程（ODEs）计算IC，并在单音高惊讶和多轨音频分段检测任务中评估其性能。

Result: 扩散模型在两项任务中表现优于或等同于GIVT，且在不同噪声水平下捕捉不同音频粒度的惊讶特征。

Conclusion: 扩散模型在音乐惊讶建模中具有优势，且噪声水平的选择对任务性能有显著影响。

Abstract: Recently, the information content (IC) of predictions from a Generative
Infinite-Vocabulary Transformer (GIVT) has been used to model musical
expectancy and surprisal in audio. We investigate the effectiveness of such
modelling using IC calculated with autoregressive diffusion models (ADMs). We
empirically show that IC estimates of models based on two different diffusion
ordinary differential equations (ODEs) describe diverse data better, in terms
of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's
effectiveness in capturing surprisal aspects by examining two tasks: (1)
capturing monophonic pitch surprisal, and (2) detecting segment boundaries in
multi-track audio. In both tasks, the diffusion models match or exceed the
performance of a GIVT. We hypothesize that the surprisal estimated at different
diffusion process noise levels corresponds to the surprisal of music and audio
features present at different audio granularities. Testing our hypothesis, we
find that, for appropriate noise levels, the studied musical surprisal tasks'
results improve. Code is provided on github.com/SonyCSLParis/audioic.

</details>


### [24] [A Scalable Pipeline for Enabling Non-Verbal Speech Generation and Understanding](https://arxiv.org/abs/2508.05385)
*Runchuan Ye,Yixuan Zhou,Renjie Yu,Zijian Lin,Kehan Li,Xiang Li,Xin Liu,Guoyang Zeng,Zhiyong Wu*

Main category: cs.SD

TL;DR: 论文介绍了NonVerbalSpeech-38K数据集，用于非语言语音生成与理解，包含38,718个样本，验证了其在语音合成和标注任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有语音系统主要关注语言内容，缺乏对非语言声音（如笑声、叹息）的理解与生成能力，限制了语音界面的情感智能和沟通丰富性。

Method: 通过自动标注管道从真实媒体中收集并标注了38,718个样本，包含10类非语言声音，并利用F5-TTS和Qwen2-Audio等模型验证数据集。

Result: 数据集在非语言语音生成与理解任务中表现有效，提升了语音合成和标注的性能。

Conclusion: 研究提出了构建非语言语音数据集的实用方法，并发布了大尺度数据集，推动了人机交互的丰富性。

Abstract: Human spoken communication involves not only lexical content but also
non-verbal vocalizations (NVs) such as laughter, sighs, and coughs, which
convey emotions, intentions, and social signals. However, most existing speech
systems focus solely on verbal content and lack the ability to understand and
generate such non-verbal cues, reducing the emotional intelligence and
communicative richness of spoken interfaces. In this work, we introduce
$\textbf{NonVerbalSpeech-38K}$, a large and diverse dataset for non-verbal
speech generation and understanding, collected from real-world media and
annotated using an automatic pipeline. The dataset contains 38,718 samples
(about 131 hours) with 10 categories of non-verbal cues, such as laughter,
sniff, and throat clearing. We further validate the dataset by fine-tuning
state-of-the-art models, including F5-TTS and Qwen2-Audio, demonstrating its
effectiveness in non-verbal speech generation and understanding tasks. Our
contributions are threefold: (1) We propose a practical pipeline for building
natural and diverse non-verbal speech datasets; (2) We release a large-scale
dataset to advance research on non-verbal speech generation and understanding;
(3) We validate the dataset's effectiveness by demonstrating improvements in
both non-verbal speech synthesis and captioning, thereby facilitating richer
human-computer interaction.

</details>


### [25] [SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription](https://arxiv.org/abs/2508.05554)
*Raymond Grossman,Taejin Park,Kunal Dhawan,Andrew Titus,Sophia Zhi,Yulia Shchadilova,Weiqing Wang,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.SD

TL;DR: SPGISpeech 2.0是一个金融领域的语音数据集，新增3,780小时的专业转录收益电话音频，支持多说话者ASR，并验证了其在说话者标记ASR任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 提升语音识别任务的多样性，同时保留原始数据集的核心特性，适用于端到端ASR和多说话者ASR研究。

Method: 扩展数据集，新增音频片段及其转录文本，并包含通话和说话者信息。

Result: 验证了SPGISpeech 2.0在说话者标记ASR任务中的性能提升。

Conclusion: 该数据集有望推动语音识别技术的进步，并激发广泛的研究应用。

Abstract: We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged
transcription in the financial domain. SPGISpeech 2.0 improves the diversity of
applicable modeling tasks while maintaining the core characteristic of the
original SPGISpeech dataset: audio snippets and their corresponding fully
formatted text transcriptions, usable for end-to-end automatic speech
recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of
professionally transcribed earnings calls. Furthermore, the dataset contains
call and speaker information for each audio snippet facilitating multi-talker
ASR. We validate the utility of SPGISpeech 2.0 through improvements in
speaker-tagged ASR performance of popular speech recognition models after
fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect
SPGISpeech 2.0 to foster advancements in speech recognition technologies and
inspire a wide range of research applications.

</details>

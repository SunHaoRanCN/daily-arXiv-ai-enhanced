{"id": "2508.04951", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04951", "abs": "https://arxiv.org/abs/2508.04951", "authors": ["Daniel J. Vickers", "A. H. Mack", "Idahosa A. Osaretin"], "title": "Real-Time Doppler and Ionospheric Dispersion Correction Techniques for Arbitrary Waveforms Utilizing GPU Compute", "comment": "This is the author's preprint version of the work which will be\n  submitted to IEEE Access", "summary": "General requirements for radar digital signal processing are ionospheric\ndistortion and Doppler dispersion correction, which has historically required\nradar-specific hardware to implement in real time. Although analog solutions\nare computationally efficient, they often come with system design drawbacks\nwhich limit waveform flexibility and can result in an overall increase of\nsystem complexity. With improvements in modern general compute systems,\nreal-time digital signal processing is becoming more realizable using\nnon-radar-specific high-performance compute. In this paper, we present an\nanalysis of general Doppler and ionospheric correction algorithms for arbitrary\nwaveforms for radar digital signal processing. We also include considerations\nfor efficient implementation of these algorithms in software, specifically\nusing GPU hardware. This analysis includes metrics of performance such as\nexecution time and error correction accuracy. We also provide recommendations\nfor application in radar signal processing. We identify two algorithms for\ndispersion correction: an FFT-based method for ionospheric dispersion and a\nnumerical interpolation method via sinc interpolation for Doppler dispersion.\nBoth of these algorithms are able to compensate for dispersion equivalent in\naccuracy to waveform-specific analytical methods and were able to be performed\nin real-time on a single NVIDIA H100 GPU. These methods are waveform agnostic\nand applied directly to the samples, improving system flexibility and making\nthem easy to incorporate into existing software-defined radio systems."}
{"id": "2508.04964", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.04964", "abs": "https://arxiv.org/abs/2508.04964", "authors": ["Zhaowei Wang", "Yunsong Huang", "Weicheng Liu", "Hui-Ming Wang"], "title": "Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas", "comment": null, "summary": "The utilization of radio frequency (RF) signals for wireless sensing has\ngarnered increasing attention. However, the radio environment is unpredictable\nand often unfavorable, the sensing accuracy of traditional RF sensing methods\nis often affected by adverse propagation channels from the transmitter to the\nreceiver, such as fading and noise. In this paper, we propose employing\ndistributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect\nthe presence and location of objects where multiple RIMSA receivers (RIMSA Rxs)\nare deployed on different places. By programming their beamforming patterns,\nRIMSA Rxs can enhance the quality of received signals. The RF sensing problem\nis modeled as a joint optimization problem of beamforming pattern and mapping\nof received signals to sensing outcomes. To address this challenge, we\nintroduce a deep reinforcement learning (DRL) algorithm aimed at calculating\nthe optimal beamforming patterns and a neural network aimed at converting\nreceived signals into sensing outcomes. In addition, the malicious attacker may\npotentially launch jamming attack to disrupt sensing process. To enable\neffective sensing in interferenceprone environment, we devise a combined loss\nfunction that takes into account the Signal to Interference plus Noise Ratio\n(SINR) of the received signals. The simulation results show that the proposed\ndistributed RIMSA system can achieve more efficient sensing performance and\nbetter overcome environmental influences than centralized implementation.\nFurthermore, the introduced method ensures high-accuracy sensing performance\neven under jamming attack."}
{"id": "2508.04978", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04978", "abs": "https://arxiv.org/abs/2508.04978", "authors": ["Sippanon Kitimoon"], "title": "Localized Kernel Methods for Signal Processing", "comment": "PhD thesis", "summary": "This dissertation presents two signal processing methods using specially\ndesigned localized kernels for parameter recovery under noisy condition. The\nfirst method addresses the estimation of frequencies and amplitudes in\nmultidimensional exponential models. It utilizes localized trigonometric\npolynomial kernels to detect the multivariate frequencies, followed by a more\ndetailed parameter estimation. We compare our method with MUSIC and ESPRIT,\nwhich are classical subspace-based algorithms widely used for estimating the\nparameters of exponential signals. In the univariate case, the method\noutperforms MUSIC and ESPRIT under low signal-to-noise ratios. For the\nmultivariate case, we develop a coordinate-wise projection and registration\napproach that achieves high recovery accuracy using significantly fewer samples\nthan other methods.\n  The second method focuses on separating linear chirp components from\ntime-localized signal segments. A variant of the Signal Separation Operator\n(SSO) is constructed using a localized kernel. Instantaneous frequency\nestimates are obtained via FFT-based filtering, then clustered and fitted with\npiecewise linear regression. The method operates without prior knowledge of the\nnumber of components and is shown to recover intersecting and discontinuous\nchirps at SNR levels as low as -30 dB.\n  Both methods share an idea based on localized kernels and efficient FFT-based\nimplementation, and neither requires subspace decomposition or sparsity\nregularization. Experimental results confirm the robustness and tractability of\nthe proposed approaches across a range of simulated data conditions. Potential\nextensions include application to nonlinear chirps, adaptive kernel design, and\nsignal classification using extracted features."}
{"id": "2508.05080", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05080", "abs": "https://arxiv.org/abs/2508.05080", "authors": ["Jiwon Sung", "Seokjun Park", "Jinseok Choi"], "title": "Power-Constrained and Quantized MIMO-RSMA Systems with Imperfect CSIT: Joint Precoding, Antenna Selection, and Power Control", "comment": "13 pages, 7 figures", "summary": "To utilize the full potential of the available power at a base station (BS),\nwe propose a joint precoding, antenna selection, and transmit power control\nalgorithm for a total power budget at the BS. We formulate a sum spectral\nefficiency (SE) maximization problem for downlink multi-user multiple-input\nmultiple-output (MIMO) rate-splitting multiple access (RSMA) systems with\narbitrary-resolution digital-to-analog converters (DACs). We reformulate the\nproblem by defining the ergodic sum SE using the conditional average rate\napproach to handle imperfect channel state information at the transmitter\n(CSIT), and by using approximation techniques to make the problem more\ntractable. Then, we decompose the problem into precoding direction and power\ncontrol subproblems. We solve the precoding direction subproblem by identifying\na superior Lagrangian stationary point, and the power control subproblem using\ngradient descent. We also propose a complexity-reduction approach that is more\nsuitable for massive MIMO systems. Simulation results not only validate the\nproposed algorithm but also reveal that when utilizing the full potential of\nthe power budget at the BS, medium-resolution DACs with 8-11 bits may actually\nbe more power-efficient than low-resolution DACs."}
{"id": "2508.04721", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T50, 68T10, 94A12", "I.2.7; H.3.3; C.2.2"], "pdf": "https://arxiv.org/pdf/2508.04721", "abs": "https://arxiv.org/abs/2508.04721", "authors": ["Vignesh Ethiraj", "Ashwath David", "Sidhanth Menon", "Divya Vijay"], "title": "Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS", "comment": null, "summary": "We introduce a low-latency telecom AI voice agent pipeline for real-time,\ninteractive telecommunications use, enabling advanced voice AI for call center\nautomation, intelligent IVR (Interactive Voice Response), and AI-driven\ncustomer support. The solution is built for telecom, combining four specialized\nmodels by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language\nModel (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific\nAutomatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific\nText-to-Speech (TTS) model. These models enable highly responsive,\ndomain-adapted voice AI agents supporting knowledge-grounded spoken\ninteractions with low latency. The pipeline integrates streaming ASR (TTE),\nconversational intelligence (TSLAM), retrieval augmented generation (RAG) over\ntelecom documents, and real-time TTS (T-Synth), setting a new benchmark for\ntelecom voice assistants. To evaluate the system, we built a dataset of 500\nhuman-recorded telecom questions from RFCs, simulating real telecom agent\nqueries. This framework allows analysis of latency, domain relevance, and\nreal-time performance across the stack. Results show that TSLAM, TTE, and\nT-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,\nlow-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and\nT-Synth -- provide a foundation for next-generation telecom AI, enabling\nautomated customer support, diagnostics, and more."}
{"id": "2508.04857", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04857", "abs": "https://arxiv.org/abs/2508.04857", "authors": ["Yael Segal-Feldman", "Ann R. Bradlow", "Matthew Goldrick", "Joseph Keshet"], "title": "Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices", "comment": "pre-print", "summary": "Open-vocabulary keyword spotting (KWS) refers to the task of detecting words\nor terms within speech recordings, regardless of whether they were included in\nthe training data. This paper introduces an open-vocabulary keyword spotting\nmodel with state-of-the-art detection accuracy for small-footprint devices. The\nmodel is composed of a speech encoder, a target keyword encoder, and a\ndetection network. The speech encoder is either a tiny Whisper or a tiny\nConformer. The target keyword encoder is implemented as a hyper-network that\ntakes the desired keyword as a character string and generates a unique set of\nweights for a convolutional layer, which can be considered as a\nkeyword-specific matched filter. The detection network uses the matched-filter\nweights to perform a keyword-specific convolution, which guides the\ncross-attention mechanism of a Perceiver module in determining whether the\ntarget term appears in the recording. The results indicate that our system\nachieves state-of-the-art detection performance and generalizes effectively to\nout-of-domain conditions, including second-language (L2) speech. Notably, our\nsmallest model, with just 4.2 million parameters, matches or outperforms models\nthat are several times larger, demonstrating both efficiency and robustness."}
{"id": "2508.05142", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05142", "abs": "https://arxiv.org/abs/2508.05142", "authors": ["Yichen Cai", "Jianhua Zhang", "Li Yu", "Zhen Zhang", "Yuxiang Zhang", "Lianzheng Shi", "Yuelong Qiu"], "title": "Digital Twin Channel-Aided CSI Prediction: A Environment-based Subspace Extraction Approach for Achieving Low Overhead and Robustness", "comment": null, "summary": "To meet the robust and high-speed communication requirements of the\nsixth-generation (6G) mobile communication system in complex scenarios,\nsensing- and artificial intelligence (AI)-based digital twin channel (DTC)\ntechniques become a promising approach to reduce system overhead. In this\npaper, we propose an environment-specific channel subspace basis (EB)-aided\npartial-to-whole channel state information (CSI) prediction method (EB-P2WCP)\nfor realizing DTC-enabled low-overhead channel prediction. Specifically, EB is\nutilized to characterize the static properties of the electromagnetic\nenvironment, which is extracted from the digital twin map, serving as\nenvironmental information prior to the prediction task. Then, we fuse EB with\nreal-time estimated local CSI to predict the entire spatial-frequency domain\nchannel for both the present and future time instances. Hence, an EB-based\npartial-to-whole CSI prediction network (EB-P2WNet) is designed to achieve a\nrobust channel prediction scheme in various complex scenarios. Simulation\nresults indicate that incorporating EB provides significant benefits under low\nsignal-to-noise ratio and pilot ratio conditions, achieving a reduction of up\nto 50% in pilot overhead. Additionally, the proposed method maintains\nrobustness against multi-user interference, tolerating 3-meter localization\nerrors with only a 0.5 dB NMSE increase, and predicts CSI for the next channel\ncoherent time within 1.3 milliseconds."}
{"id": "2508.04723", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04723", "abs": "https://arxiv.org/abs/2508.04723", "authors": ["Sha Zhao", "Song Yi", "Yangxuan Zhou", "Jiadong Pan", "Jiquan Wang", "Jie Xia", "Shijian Li", "Shurong Dong", "Gang Pan"], "title": "Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion", "comment": "Accepted by ACM MM 2025", "summary": "Emotions critically influence mental health, driving interest in music-based\naffective computing via neurophysiological signals with Brain-computer\nInterface techniques. While prior studies leverage music's accessibility for\nemotion induction, three key limitations persist: \\textbf{(1) Stimulus\nConstraints}: Music stimuli are confined to small corpora due to copyright and\ncuration costs, with selection biases from heuristic emotion-music mappings\nthat ignore individual affective profiles. \\textbf{(2) Modality Specificity}:\nOverreliance on unimodal neural data (e.g., EEG) ignores complementary insights\nfrom cross-modal signal fusion.\\textbf{ (3) Portability Limitation}: Cumbersome\nsetups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability\ndue to procedural complexity and portability barriers. To address these\nlimitations, we propose MEEtBrain, a portable and multimodal framework for\nemotion analysis (valence/arousal), integrating AI-generated music stimuli with\nsynchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the\nmusic stimuli can be automatically generated by AI on a large scale,\neliminating subjective selection biases while ensuring music diversity. We use\nour developed portable device that is designed in a lightweight headband-style\nand uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A\n14-hour dataset from 20 participants was collected in the first recruitment to\nvalidate the framework's efficacy, with AI-generated music eliciting target\nemotions (valence/arousal). We are actively expanding our multimodal dataset\n(44 participants in the latest dataset) and make it publicly available to\npromote further research and practical applications. \\textbf{The dataset is\navailable at https://zju-bmi-lab.github.io/ZBra."}
{"id": "2508.04887", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04887", "abs": "https://arxiv.org/abs/2508.04887", "authors": ["Henri Gode", "Simon Doclo"], "title": "Closed-Form Successive Relative Transfer Function Vector Estimation based on Blind Oblique Projection Incorporating Noise Whitening", "comment": null, "summary": "Relative transfer functions (RTFs) of sound sources play a crucial role in\nbeamforming, enabling effective noise and interference suppression. This paper\naddresses the challenge of online estimating the RTF vectors of multiple sound\nsources in noisy and reverberant environments, for the specific scenario where\nsources activate successively. While the RTF vector of the first source can be\nestimated straightforwardly, the main challenge arises in estimating the RTF\nvectors of subsequent sources during segments where multiple sources are\nsimultaneously active. The blind oblique projection (BOP) method has been\nproposed to estimate the RTF vector of a newly activating source by optimally\nblocking this source. However, this method faces several limitations: high\ncomputational complexity due to its reliance on iterative gradient descent\noptimization, the introduction of random additional vectors, which can\nnegatively impact performance, and the assumption of high signal-to-noise ratio\n(SNR). To overcome these limitations, in this paper we propose three extensions\nto the BOP method. First, we derive a closed-form solution for optimizing the\nBOP cost function, significantly reducing computational complexity. Second, we\nintroduce orthogonal additional vectors instead of random vectors, enhancing\nRTF vector estimation accuracy. Third, we incorporate noise handling techniques\ninspired by covariance subtraction and whitening, increasing robustness in low\nSNR conditions. To provide a frame-by-frame estimate of the source activity\npattern, required by both the conventional BOP method and the proposed method,\nwe propose a spatial-coherence-based online source counting method. Simulations\nare performed with real-world reverberant noisy recordings featuring 3\nsuccessively activating speakers, with and without a-priori knowledge of the\nsource activity pattern."}
{"id": "2508.05204", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05204", "abs": "https://arxiv.org/abs/2508.05204", "authors": ["Kapila W. S. Palitharathna", "Christodoulos Skouroumounis", "Ioannis Krikidis"], "title": "Optimization of Liquid Lens-based Imaging Receiver for MIMO VLC Systems", "comment": "This paper has been accepted for presentation at the IEEE Global\n  Communications Conference (GLOBECOM 2025), which will be held in Taipei,\n  Taiwan, from December 8 to 12, 2025. arXiv admin note: substantial text\n  overlap with arXiv:2503.10316", "summary": "In this paper, a liquid lens-based imaging receiver is proposed for\nmultiple-input multiple-output (MIMO) visible light communication (VLC)\nsystems. By dynamically adjusting the focal length and orientation angles of\nthe liquid lens, the spatial correlation between MIMO channel gains is reduced,\nleading to enhanced bit-error rate (BER) performance. Unlike static lenses,\nliquid lenses offer adaptability in dynamic conditions, including user mobility\nand random receiver orientation. An accurate mathematical framework is\ndeveloped to model the channel gains of the proposed system, and an\noptimization problem is formulated to minimize its BER. Due to the complexity\nof the resulting channel model, two lens adjustment schemes, namely, ($i$) the\nCLS scheme, and ($ii$) the VULO scheme are introduced. Numerical results\ndemonstrate that the proposed liquid lens-based system offers substantial BER\nimprovements over conventional static lens-based receivers across a wide range\nof random receiver orientation conditions. Specifically, at a random receiver\norientation variance of $10^{\\circ}$, the BER is improved from $4\\times\n10^{-2}$ to $5\\times 10^{-4}$ by employing the proposed liquid lens."}
{"id": "2508.05011", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05011", "abs": "https://arxiv.org/abs/2508.05011", "authors": ["Huaicheng Zhang", "Wei Tan", "Guangzheng Li", "Yixuan Zhang", "Hangting Chen", "Shun Lei", "Chenyu Yang", "Zhiyong Wu", "Shuai Wang", "Qijun Huang", "Dong Yu"], "title": "Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation", "comment": null, "summary": "Recent advances in audio-based generative language models have accelerated\nAI-driven lyric-to-song generation. However, these models frequently suffer\nfrom content hallucination, producing outputs misaligned with the input lyrics\nand undermining musical coherence. Current supervised fine-tuning (SFT)\napproaches, limited by passive label-fitting, exhibit constrained\nself-improvement and poor hallucination mitigation. To address this core\nchallenge, we propose a novel reinforcement learning (RL) framework leveraging\npreference optimization for hallucination control. Our key contributions\ninclude: (1) Developing a robust hallucination preference dataset constructed\nvia phoneme error rate (PER) computation and rule-based filtering to capture\nalignment with human expectations; (2) Implementing and evaluating three\ndistinct preference optimization strategies within the RL framework: Direct\nPreference Optimization (DPO), Proximal Policy Optimization (PPO), and Group\nRelative Policy Optimization (GRPO). DPO operates off-policy to enhance\npositive token likelihood, achieving a significant 7.4% PER reduction. PPO and\nGRPO employ an on-policy approach, training a PER-based reward model to\niteratively optimize sequences via reward maximization and KL-regularization,\nyielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective\nand subjective evaluations confirm that our methods effectively suppress\nhallucinations while preserving musical quality. Crucially, this work presents\na systematic, RL-based solution to hallucination control in lyric-to-song\ngeneration. The framework's transferability also unlocks potential for music\nstyle adherence and musicality enhancement, opening new avenues for future\ngenerative song research."}
{"id": "2508.04996", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04996", "abs": "https://arxiv.org/abs/2508.04996", "authors": ["Yuepeng Jiang", "Ziqian Ning", "Shuai Wang", "Chengjia Wang", "Mengxiao Bi", "Pengcheng Zhu", "Lei Xie", "Zhonghua Fu"], "title": "REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with Diffusion Transformers", "comment": null, "summary": "In real-world voice conversion applications, environmental noise in source\nspeech and user demands for expressive output pose critical challenges.\nTraditional ASR-based methods ensure noise robustness but suppress prosody,\nwhile SSL-based models improve expressiveness but suffer from timbre leakage\nand noise sensitivity. This paper proposes REF-VC, a noise-robust expressive\nvoice conversion system. Key innovations include: (1) A random erasing strategy\nto mitigate the information redundancy inherent in SSL feature, enhancing noise\nrobustness and expressiveness; (2) Implicit alignment inspired by E2TTS to\nsuppress non-essential feature reconstruction; (3) Integration of Shortcut\nModels to accelerate flow matching inference, significantly reducing to 4\nsteps. Experimental results demonstrate that our model outperforms baselines\nsuch as Seed-VC in zero-shot scenarios on the noisy set, while also performing\ncomparably to Seed-VC on the clean set. In addition, REF-VC can be compatible\nwith singing voice conversion within one model."}
{"id": "2508.05226", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05226", "abs": "https://arxiv.org/abs/2508.05226", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Bingcheng Liu", "Jiahui Han", "Haoxiang Zhang", "Bo Ai"], "title": "Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) technology plays a critical role\nin future intelligent transportation systems, by enabling vehicles to perceive\nand reconstruct the surrounding environment through reuse of wireless signals,\nthereby reducing or even eliminating the need for additional sensors such as\nLiDAR or radar. However, existing ISAC based reconstruction methods often lack\nthe ability to track dynamic scenes with sufficient accuracy and temporal\nconsistency, limiting the real world applicability. To address this limitation,\nwe propose a deep learning based framework for vehicular environment\nreconstruction by using ISAC channels. We first establish a joint channel\nenvironment dataset based on multi modal measurements from real world urban\nstreet scenarios. Then, a multistage deep learning network is developed to\nreconstruct the environment. Specifically, a scene decoder identifies the\nenvironmental context such as buildings, trees and so on; a cluster center\ndecoder predicts coarse spatial layouts by localizing dominant scattering\ncenters; a point cloud decoder recovers fine grained geometry and structure of\nsurrounding environments. Experimental results demonstrate that the proposed\nmethod achieves high-quality dynamic environment reconstruction with a Chamfer\nDistance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis\ndemonstrates the efficiency and practical applicability of the method in real\ntime scenarios. This work provides a pathway toward low cost environment\nreconstruction based on ISAC for future intelligent transportation."}
{"id": "2508.05207", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05207", "abs": "https://arxiv.org/abs/2508.05207", "authors": ["Yunpeng Li", "Kehang Han", "Brian McWilliams", "Zalan Borsos", "Marco Tagliasacchi"], "title": "SpectroStream: A Versatile Neural Codec for General Audio", "comment": null, "summary": "We propose SpectroStream, a full-band multi-channel neural audio codec.\nSuccessor to the well-established SoundStream, SpectroStream extends its\ncapability beyond 24 kHz monophonic audio and enables high-quality\nreconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is\naccomplished with a new neural architecture that leverages audio representation\nin the time-frequency domain, which leads to better audio quality especially at\nhigher sample rate. The model also uses a delayed-fusion strategy to handle\nmulti-channel audio, which is crucial in balancing per-channel acoustic quality\nand cross-channel phase consistency."}
{"id": "2508.05055", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05055", "abs": "https://arxiv.org/abs/2508.05055", "authors": ["Naoyuki Kamo", "Tsubasa Ochiai", "Marc Delcroix", "Tomohiro Nakatani"], "title": "MOVER: Combining Multiple Meeting Recognition Systems", "comment": null, "summary": "In this paper, we propose Meeting recognizer Output Voting Error Reduction\n(MOVER), a novel system combination method for meeting recognition tasks.\nAlthough there are methods to combine the output of diarization (e.g., DOVER)\nor automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first\napproach that can combine the outputs of meeting recognition systems that\ndiffer in terms of both diarization and ASR. MOVER combines hypotheses with\ndifferent time intervals and speaker labels through a five-stage process that\nincludes speaker alignment, segment grouping, word and timing combination, etc.\nExperimental results on the CHiME-8 DASR task and the multi-channel track of\nthe NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple\nmeeting recognition systems with diverse diarization and recognition outputs,\nachieving relative tcpWER improvements of 9.55 % and 8.51 % over the\nstate-of-the-art systems for both tasks."}
{"id": "2508.05380", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05380", "abs": "https://arxiv.org/abs/2508.05380", "authors": ["Steven Sandoval", "Phillip L. De Leon"], "title": "Unifying Common Signal Analyses with Instantaneous Time-Frequency Atoms", "comment": null, "summary": "In previous work, we presented a general framework for instantaneous\ntime-frequency analysis but did not provide any specific details of how to\ncompute a particular instantaneous spectrum (IS). In this work, we use\ninstantaneous time-frequency atoms to obtain an IS associated with common\nsignal analyses: time domain analysis, frequency domain analysis, fractional\nFourier transform, synchrosqueezed short-time Fourier transform, and\nsynchrosqueezed short-time fractional Fourier transform. By doing so, we\ndemonstrate how the general framework can be used to unify these analyses and\nwe develop closed-form expressions for the corresponding ISs. This is\naccomplished by viewing these analyses as decompositions into AM--FM components\nand recognizing that each uses a specialized (or limiting) form of a quadratic\nchirplet as a template during analysis. With a two-parameter quadratic\nchirplet, we can organize these ISs into a 2D continuum with points in the\nplane corresponding to a decomposition related to one of the signal analyses.\nFinally, using several example signals, we compute in closed-form the ISs for\nthe various analyses."}
{"id": "2508.05306", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05306", "abs": "https://arxiv.org/abs/2508.05306", "authors": ["Mathias Rose Bjare", "Stefan Lattner", "Gerhard Widmer"], "title": "Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces", "comment": "9 pages, 1 figure, 5 tables. Accepted at the 25th International\n  Society for Music Information Retrieval Conference (ISMIR), Daejeon, South\n  Korea, 2025 2025", "summary": "Recently, the information content (IC) of predictions from a Generative\nInfinite-Vocabulary Transformer (GIVT) has been used to model musical\nexpectancy and surprisal in audio. We investigate the effectiveness of such\nmodelling using IC calculated with autoregressive diffusion models (ADMs). We\nempirically show that IC estimates of models based on two different diffusion\nordinary differential equations (ODEs) describe diverse data better, in terms\nof negative log-likelihood, than a GIVT. We evaluate diffusion model IC's\neffectiveness in capturing surprisal aspects by examining two tasks: (1)\ncapturing monophonic pitch surprisal, and (2) detecting segment boundaries in\nmulti-track audio. In both tasks, the diffusion models match or exceed the\nperformance of a GIVT. We hypothesize that the surprisal estimated at different\ndiffusion process noise levels corresponds to the surprisal of music and audio\nfeatures present at different audio granularities. Testing our hypothesis, we\nfind that, for appropriate noise levels, the studied musical surprisal tasks'\nresults improve. Code is provided on github.com/SonyCSLParis/audioic."}
{"id": "2508.05102", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05102", "abs": "https://arxiv.org/abs/2508.05102", "authors": ["Anuprabha M", "Krishna Gurugubelli", "Anil Kumar Vuppala"], "title": "Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS", "comment": "Accepted at Interspeech 2025", "summary": "Dysarthric speech poses significant challenges in developing assistive\ntechnologies, primarily due to the limited availability of data. Recent\nadvances in neural speech synthesis, especially zero-shot voice cloning,\nfacilitate synthetic speech generation for data augmentation; however, they may\nintroduce biases towards dysarthric speech. In this paper, we investigate the\neffectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using\nTORGO dataset, focusing on intelligibility, speaker similarity, and prosody\npreservation. We also analyze potential biases using fairness metrics like\nDisparate Impact and Parity Difference to assess disparities across dysarthric\nseverity levels. Results show that F5-TTS exhibits a strong bias toward speech\nintelligibility over speaker and prosody preservation in dysarthric speech\nsynthesis. Insights from this study can help integrate fairness-aware\ndysarthric speech synthesis, fostering the advancement of more inclusive speech\ntechnologies."}
{"id": "2508.05479", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05479", "abs": "https://arxiv.org/abs/2508.05479", "authors": ["Marco Privitera", "Andrea Ballo", "Karim Ali Ahmed", "Alfio Dario Grasso", "Massimo Alioto"], "title": "Sub- μ W Battery-Less and Oscillator-Less Wi-Fi Backscattering Transmitter Reusing RF Signal for Harvesting, Communications, and Motion Detection", "comment": null, "summary": "In this paper, a sub-uW power 802.11b backscattering transmitter is presented\nto enable reuse of the same incident wave for three purposes: RF harvesting,\nbackscattering communications and position/motion sensing. The removal of the\nbattery and any off-chip motion sensor (e.g., MEMS) enables unprecedented level\nof miniaturization and ubiquity, unrestricted device lifespan, low fabrication\nand maintenance cost. The uW power wall for WiFi transmitters is broken for the\nfirst time via local oscillator elimination, as achieved by extracting its\nfrequency through second-order intermodulation of a twotone incident wave. The\ntwo-tone scheme also enables a cumulative harvesting/transmission/sensing\nsensitivity down to Pmin -19 dBm. Position/motion sensing is enabled by using\nthe harvested voltage as a proxy for the Received Signal Strength (RSS),\nallowing to sense the chip location with respect to the tone generator(s)\nshared across tags in indoor neighborhoods."}
{"id": "2508.05385", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05385", "abs": "https://arxiv.org/abs/2508.05385", "authors": ["Runchuan Ye", "Yixuan Zhou", "Renjie Yu", "Zijian Lin", "Kehan Li", "Xiang Li", "Xin Liu", "Guoyang Zeng", "Zhiyong Wu"], "title": "A Scalable Pipeline for Enabling Non-Verbal Speech Generation and Understanding", "comment": null, "summary": "Human spoken communication involves not only lexical content but also\nnon-verbal vocalizations (NVs) such as laughter, sighs, and coughs, which\nconvey emotions, intentions, and social signals. However, most existing speech\nsystems focus solely on verbal content and lack the ability to understand and\ngenerate such non-verbal cues, reducing the emotional intelligence and\ncommunicative richness of spoken interfaces. In this work, we introduce\n$\\textbf{NonVerbalSpeech-38K}$, a large and diverse dataset for non-verbal\nspeech generation and understanding, collected from real-world media and\nannotated using an automatic pipeline. The dataset contains 38,718 samples\n(about 131 hours) with 10 categories of non-verbal cues, such as laughter,\nsniff, and throat clearing. We further validate the dataset by fine-tuning\nstate-of-the-art models, including F5-TTS and Qwen2-Audio, demonstrating its\neffectiveness in non-verbal speech generation and understanding tasks. Our\ncontributions are threefold: (1) We propose a practical pipeline for building\nnatural and diverse non-verbal speech datasets; (2) We release a large-scale\ndataset to advance research on non-verbal speech generation and understanding;\n(3) We validate the dataset's effectiveness by demonstrating improvements in\nboth non-verbal speech synthesis and captioning, thereby facilitating richer\nhuman-computer interaction."}
{"id": "2508.05149", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05149", "abs": "https://arxiv.org/abs/2508.05149", "authors": ["Seraphina Fong", "Marco Matassoni", "Alessio Brutti"], "title": "Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages", "comment": "Accepted at Interspeech 2025. 5 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) have demonstrated potential in handling spoken\ninputs for high-resource languages, reaching state-of-the-art performance in\nvarious tasks. However, their applicability is still less explored in\nlow-resource settings. This work investigates the use of Speech LLMs for\nlow-resource Automatic Speech Recognition using the SLAM-ASR framework, where a\ntrainable lightweight projector connects a speech encoder and a LLM. Firstly,\nwe assess training data volume requirements to match Whisper-only performance,\nre-emphasizing the challenges of limited data. Secondly, we show that\nleveraging mono- or multilingual projectors pretrained on high-resource\nlanguages reduces the impact of data scarcity, especially with small training\nsets. Using multilingual LLMs (EuroLLM, Salamandra) with\nwhisper-large-v3-turbo, we evaluate performance on several public benchmarks,\nproviding insights for future research on optimizing Speech LLMs for\nlow-resource languages and multilinguality."}
{"id": "2508.05499", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05499", "abs": "https://arxiv.org/abs/2508.05499", "authors": ["M. Privitera", "A. D. Grasso", "A. Ballo", "M. Alioto"], "title": "0.6-V, uW-Power 4-Stage OTA with Minimal Components and 100X Load Range", "comment": null, "summary": "A four-stage operational transconductance amplifier (OTA) for ultra-low-power\napplications is introduced in this paper. The proposed circuit inclusive of\nfrequency compensation requires minimal transistor count and passives,\novercoming the traditionally difficult compensation of 4-stage OTAs and\nbringing it back to the simplicity of 3-stage OTAs. At the same time, the\nproposed circuit achieves high power efficiency, as evidenced by the >3.7X\n(>11.3X) improvement in the large-signal (small-signal) power efficiency figure\nof merit FOML (FOMS), compared to prior 4-stage OTAs (sub-1 V multi-stage\nOTAs). Thanks to the lower sensitivity of the phase margin to the load\ncapacitance, the proposed OTA remains stable under a wide range of loads\n(double-sided as in any 3-4-stage OTA), achieving a max/min ratio of the load\ncapacitance of >100X."}
{"id": "2508.05554", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05554", "abs": "https://arxiv.org/abs/2508.05554", "authors": ["Raymond Grossman", "Taejin Park", "Kunal Dhawan", "Andrew Titus", "Sophia Zhi", "Yulia Shchadilova", "Weiqing Wang", "Jagadeesh Balam", "Boris Ginsburg"], "title": "SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription", "comment": "To be presented at Interspeech 2025", "summary": "We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged\ntranscription in the financial domain. SPGISpeech 2.0 improves the diversity of\napplicable modeling tasks while maintaining the core characteristic of the\noriginal SPGISpeech dataset: audio snippets and their corresponding fully\nformatted text transcriptions, usable for end-to-end automatic speech\nrecognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of\nprofessionally transcribed earnings calls. Furthermore, the dataset contains\ncall and speaker information for each audio snippet facilitating multi-talker\nASR. We validate the utility of SPGISpeech 2.0 through improvements in\nspeaker-tagged ASR performance of popular speech recognition models after\nfine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect\nSPGISpeech 2.0 to foster advancements in speech recognition technologies and\ninspire a wide range of research applications."}
{"id": "2508.05250", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05250", "abs": "https://arxiv.org/abs/2508.05250", "authors": ["Tom Bäckström", "Mohammad Hassan Vali", "My Nguyen", "Silas Rech"], "title": "Privacy Disclosure of Similarity in Speech and Language Processing", "comment": null, "summary": "Speaker, author, and other biometric identification applications often\ncompare a sample's similarity to a database of templates to determine the\nidentity. Given that data may be noisy and similarity measures can be\ninaccurate, such a comparison may not reliably identify the true identity as\nthe most similar. Still, even the similarity rank based on an inaccurate\nsimilarity measure can disclose private information about the true identity. We\npropose a methodology for quantifying the privacy disclosure of such a\nsimilarity rank by estimating its probability distribution. It is based on\ndetermining the histogram of the similarity rank of the true speaker, or when\ndata is scarce, modeling the histogram with the beta-binomial distribution. We\nexpress the disclosure in terms of entropy (bits), such that the disclosure\nfrom independent features are additive. Our experiments demonstrate that all\ntested speaker and author characterizations contain personally identifying\ninformation (PII) that can aid in identification, with embeddings from speaker\nrecognition algorithms containing the most information, followed by phone\nembeddings, linguistic embeddings, and fundamental frequency. Our initial\nexperiments show that the disclosure of PII increases with the length of test\nsamples, but it is bounded by the length of database templates. The provided\nmetric, similarity rank disclosure, provides a way to compare the disclosure of\nPII between biometric features and merge them to aid identification. It can\nthus aid in the holistic evaluation of threats to privacy in speech and other\nbiometric technologies."}
{"id": "2508.04857", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.04857", "abs": "https://arxiv.org/abs/2508.04857", "authors": ["Yael Segal-Feldman", "Ann R. Bradlow", "Matthew Goldrick", "Joseph Keshet"], "title": "Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices", "comment": "pre-print", "summary": "Open-vocabulary keyword spotting (KWS) refers to the task of detecting words\nor terms within speech recordings, regardless of whether they were included in\nthe training data. This paper introduces an open-vocabulary keyword spotting\nmodel with state-of-the-art detection accuracy for small-footprint devices. The\nmodel is composed of a speech encoder, a target keyword encoder, and a\ndetection network. The speech encoder is either a tiny Whisper or a tiny\nConformer. The target keyword encoder is implemented as a hyper-network that\ntakes the desired keyword as a character string and generates a unique set of\nweights for a convolutional layer, which can be considered as a\nkeyword-specific matched filter. The detection network uses the matched-filter\nweights to perform a keyword-specific convolution, which guides the\ncross-attention mechanism of a Perceiver module in determining whether the\ntarget term appears in the recording. The results indicate that our system\nachieves state-of-the-art detection performance and generalizes effectively to\nout-of-domain conditions, including second-language (L2) speech. Notably, our\nsmallest model, with just 4.2 million parameters, matches or outperforms models\nthat are several times larger, demonstrating both efficiency and robustness."}
{"id": "2508.05293", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05293", "abs": "https://arxiv.org/abs/2508.05293", "authors": ["Jiatong Li", "Simon Doclo"], "title": "Investigation of Speech and Noise Latent Representations in Single-channel VAE-based Speech Enhancement", "comment": "5 pages, 5 figures", "summary": "Recently, a variational autoencoder (VAE)-based single-channel speech\nenhancement system using Bayesian permutation training has been proposed, which\nuses two pretrained VAEs to obtain latent representations for speech and noise.\nBased on these pretrained VAEs, a noisy VAE learns to generate speech and noise\nlatent representations from noisy speech for speech enhancement. Modifying the\npretrained VAE loss terms affects the pretrained speech and noise latent\nrepresentations. In this paper, we investigate how these different\nrepresentations affect speech enhancement performance. Experiments on the DNS3,\nWSJ0-QUT, and VoiceBank-DEMAND datasets show that a latent space where speech\nand noise representations are clearly separated significantly improves\nperformance over standard VAEs, which produce overlapping speech and noise\nrepresentations."}
{"id": "2508.04721", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T50, 68T10, 94A12", "I.2.7; H.3.3; C.2.2"], "pdf": "https://arxiv.org/pdf/2508.04721", "abs": "https://arxiv.org/abs/2508.04721", "authors": ["Vignesh Ethiraj", "Ashwath David", "Sidhanth Menon", "Divya Vijay"], "title": "Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS", "comment": null, "summary": "We introduce a low-latency telecom AI voice agent pipeline for real-time,\ninteractive telecommunications use, enabling advanced voice AI for call center\nautomation, intelligent IVR (Interactive Voice Response), and AI-driven\ncustomer support. The solution is built for telecom, combining four specialized\nmodels by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language\nModel (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific\nAutomatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific\nText-to-Speech (TTS) model. These models enable highly responsive,\ndomain-adapted voice AI agents supporting knowledge-grounded spoken\ninteractions with low latency. The pipeline integrates streaming ASR (TTE),\nconversational intelligence (TSLAM), retrieval augmented generation (RAG) over\ntelecom documents, and real-time TTS (T-Synth), setting a new benchmark for\ntelecom voice assistants. To evaluate the system, we built a dataset of 500\nhuman-recorded telecom questions from RFCs, simulating real telecom agent\nqueries. This framework allows analysis of latency, domain relevance, and\nreal-time performance across the stack. Results show that TSLAM, TTE, and\nT-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,\nlow-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and\nT-Synth -- provide a foundation for next-generation telecom AI, enabling\nautomated customer support, diagnostics, and more."}
{"id": "2508.04723", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.04723", "abs": "https://arxiv.org/abs/2508.04723", "authors": ["Sha Zhao", "Song Yi", "Yangxuan Zhou", "Jiadong Pan", "Jiquan Wang", "Jie Xia", "Shijian Li", "Shurong Dong", "Gang Pan"], "title": "Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion", "comment": "Accepted by ACM MM 2025", "summary": "Emotions critically influence mental health, driving interest in music-based\naffective computing via neurophysiological signals with Brain-computer\nInterface techniques. While prior studies leverage music's accessibility for\nemotion induction, three key limitations persist: \\textbf{(1) Stimulus\nConstraints}: Music stimuli are confined to small corpora due to copyright and\ncuration costs, with selection biases from heuristic emotion-music mappings\nthat ignore individual affective profiles. \\textbf{(2) Modality Specificity}:\nOverreliance on unimodal neural data (e.g., EEG) ignores complementary insights\nfrom cross-modal signal fusion.\\textbf{ (3) Portability Limitation}: Cumbersome\nsetups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability\ndue to procedural complexity and portability barriers. To address these\nlimitations, we propose MEEtBrain, a portable and multimodal framework for\nemotion analysis (valence/arousal), integrating AI-generated music stimuli with\nsynchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the\nmusic stimuli can be automatically generated by AI on a large scale,\neliminating subjective selection biases while ensuring music diversity. We use\nour developed portable device that is designed in a lightweight headband-style\nand uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A\n14-hour dataset from 20 participants was collected in the first recruitment to\nvalidate the framework's efficacy, with AI-generated music eliciting target\nemotions (valence/arousal). We are actively expanding our multimodal dataset\n(44 participants in the latest dataset) and make it publicly available to\npromote further research and practical applications. \\textbf{The dataset is\navailable at https://zju-bmi-lab.github.io/ZBra."}
{"id": "2508.05011", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05011", "abs": "https://arxiv.org/abs/2508.05011", "authors": ["Huaicheng Zhang", "Wei Tan", "Guangzheng Li", "Yixuan Zhang", "Hangting Chen", "Shun Lei", "Chenyu Yang", "Zhiyong Wu", "Shuai Wang", "Qijun Huang", "Dong Yu"], "title": "Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation", "comment": null, "summary": "Recent advances in audio-based generative language models have accelerated\nAI-driven lyric-to-song generation. However, these models frequently suffer\nfrom content hallucination, producing outputs misaligned with the input lyrics\nand undermining musical coherence. Current supervised fine-tuning (SFT)\napproaches, limited by passive label-fitting, exhibit constrained\nself-improvement and poor hallucination mitigation. To address this core\nchallenge, we propose a novel reinforcement learning (RL) framework leveraging\npreference optimization for hallucination control. Our key contributions\ninclude: (1) Developing a robust hallucination preference dataset constructed\nvia phoneme error rate (PER) computation and rule-based filtering to capture\nalignment with human expectations; (2) Implementing and evaluating three\ndistinct preference optimization strategies within the RL framework: Direct\nPreference Optimization (DPO), Proximal Policy Optimization (PPO), and Group\nRelative Policy Optimization (GRPO). DPO operates off-policy to enhance\npositive token likelihood, achieving a significant 7.4% PER reduction. PPO and\nGRPO employ an on-policy approach, training a PER-based reward model to\niteratively optimize sequences via reward maximization and KL-regularization,\nyielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective\nand subjective evaluations confirm that our methods effectively suppress\nhallucinations while preserving musical quality. Crucially, this work presents\na systematic, RL-based solution to hallucination control in lyric-to-song\ngeneration. The framework's transferability also unlocks potential for music\nstyle adherence and musicality enhancement, opening new avenues for future\ngenerative song research."}
{"id": "2508.05207", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05207", "abs": "https://arxiv.org/abs/2508.05207", "authors": ["Yunpeng Li", "Kehang Han", "Brian McWilliams", "Zalan Borsos", "Marco Tagliasacchi"], "title": "SpectroStream: A Versatile Neural Codec for General Audio", "comment": null, "summary": "We propose SpectroStream, a full-band multi-channel neural audio codec.\nSuccessor to the well-established SoundStream, SpectroStream extends its\ncapability beyond 24 kHz monophonic audio and enables high-quality\nreconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is\naccomplished with a new neural architecture that leverages audio representation\nin the time-frequency domain, which leads to better audio quality especially at\nhigher sample rate. The model also uses a delayed-fusion strategy to handle\nmulti-channel audio, which is crucial in balancing per-channel acoustic quality\nand cross-channel phase consistency."}
{"id": "2508.05306", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05306", "abs": "https://arxiv.org/abs/2508.05306", "authors": ["Mathias Rose Bjare", "Stefan Lattner", "Gerhard Widmer"], "title": "Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces", "comment": "9 pages, 1 figure, 5 tables. Accepted at the 25th International\n  Society for Music Information Retrieval Conference (ISMIR), Daejeon, South\n  Korea, 2025 2025", "summary": "Recently, the information content (IC) of predictions from a Generative\nInfinite-Vocabulary Transformer (GIVT) has been used to model musical\nexpectancy and surprisal in audio. We investigate the effectiveness of such\nmodelling using IC calculated with autoregressive diffusion models (ADMs). We\nempirically show that IC estimates of models based on two different diffusion\nordinary differential equations (ODEs) describe diverse data better, in terms\nof negative log-likelihood, than a GIVT. We evaluate diffusion model IC's\neffectiveness in capturing surprisal aspects by examining two tasks: (1)\ncapturing monophonic pitch surprisal, and (2) detecting segment boundaries in\nmulti-track audio. In both tasks, the diffusion models match or exceed the\nperformance of a GIVT. We hypothesize that the surprisal estimated at different\ndiffusion process noise levels corresponds to the surprisal of music and audio\nfeatures present at different audio granularities. Testing our hypothesis, we\nfind that, for appropriate noise levels, the studied musical surprisal tasks'\nresults improve. Code is provided on github.com/SonyCSLParis/audioic."}
{"id": "2508.05385", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05385", "abs": "https://arxiv.org/abs/2508.05385", "authors": ["Runchuan Ye", "Yixuan Zhou", "Renjie Yu", "Zijian Lin", "Kehan Li", "Xiang Li", "Xin Liu", "Guoyang Zeng", "Zhiyong Wu"], "title": "A Scalable Pipeline for Enabling Non-Verbal Speech Generation and Understanding", "comment": null, "summary": "Human spoken communication involves not only lexical content but also\nnon-verbal vocalizations (NVs) such as laughter, sighs, and coughs, which\nconvey emotions, intentions, and social signals. However, most existing speech\nsystems focus solely on verbal content and lack the ability to understand and\ngenerate such non-verbal cues, reducing the emotional intelligence and\ncommunicative richness of spoken interfaces. In this work, we introduce\n$\\textbf{NonVerbalSpeech-38K}$, a large and diverse dataset for non-verbal\nspeech generation and understanding, collected from real-world media and\nannotated using an automatic pipeline. The dataset contains 38,718 samples\n(about 131 hours) with 10 categories of non-verbal cues, such as laughter,\nsniff, and throat clearing. We further validate the dataset by fine-tuning\nstate-of-the-art models, including F5-TTS and Qwen2-Audio, demonstrating its\neffectiveness in non-verbal speech generation and understanding tasks. Our\ncontributions are threefold: (1) We propose a practical pipeline for building\nnatural and diverse non-verbal speech datasets; (2) We release a large-scale\ndataset to advance research on non-verbal speech generation and understanding;\n(3) We validate the dataset's effectiveness by demonstrating improvements in\nboth non-verbal speech synthesis and captioning, thereby facilitating richer\nhuman-computer interaction."}
{"id": "2508.05554", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.05554", "abs": "https://arxiv.org/abs/2508.05554", "authors": ["Raymond Grossman", "Taejin Park", "Kunal Dhawan", "Andrew Titus", "Sophia Zhi", "Yulia Shchadilova", "Weiqing Wang", "Jagadeesh Balam", "Boris Ginsburg"], "title": "SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription", "comment": "To be presented at Interspeech 2025", "summary": "We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged\ntranscription in the financial domain. SPGISpeech 2.0 improves the diversity of\napplicable modeling tasks while maintaining the core characteristic of the\noriginal SPGISpeech dataset: audio snippets and their corresponding fully\nformatted text transcriptions, usable for end-to-end automatic speech\nrecognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of\nprofessionally transcribed earnings calls. Furthermore, the dataset contains\ncall and speaker information for each audio snippet facilitating multi-talker\nASR. We validate the utility of SPGISpeech 2.0 through improvements in\nspeaker-tagged ASR performance of popular speech recognition models after\nfine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect\nSPGISpeech 2.0 to foster advancements in speech recognition technologies and\ninspire a wide range of research applications."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 8]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-Time Doppler and Ionospheric Dispersion Correction Techniques for Arbitrary Waveforms Utilizing GPU Compute](https://arxiv.org/abs/2508.04951)
*Daniel J. Vickers,A. H. Mack,Idahosa A. Osaretin*

Main category: eess.SP

TL;DR: 论文分析了通用的多普勒和电离层校正算法，提出基于FFT和数值插值的方法，可在GPU上实时处理，提高雷达信号处理的灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统雷达信号处理依赖专用硬件，存在波形灵活性不足和系统复杂度高的问题，现代通用计算系统的发展为实时数字信号处理提供了新可能。

Method: 提出两种算法：基于FFT的电离层色散校正和基于sinc插值的多普勒色散校正，利用GPU实现实时处理。

Result: 两种算法在NVIDIA H100 GPU上实现了与波形专用方法相当的校正精度和实时性能。

Conclusion: 这些波形无关的方法提高了系统灵活性，易于集成到现有软件定义无线电系统中。

Abstract: General requirements for radar digital signal processing are ionospheric
distortion and Doppler dispersion correction, which has historically required
radar-specific hardware to implement in real time. Although analog solutions
are computationally efficient, they often come with system design drawbacks
which limit waveform flexibility and can result in an overall increase of
system complexity. With improvements in modern general compute systems,
real-time digital signal processing is becoming more realizable using
non-radar-specific high-performance compute. In this paper, we present an
analysis of general Doppler and ionospheric correction algorithms for arbitrary
waveforms for radar digital signal processing. We also include considerations
for efficient implementation of these algorithms in software, specifically
using GPU hardware. This analysis includes metrics of performance such as
execution time and error correction accuracy. We also provide recommendations
for application in radar signal processing. We identify two algorithms for
dispersion correction: an FFT-based method for ionospheric dispersion and a
numerical interpolation method via sinc interpolation for Doppler dispersion.
Both of these algorithms are able to compensate for dispersion equivalent in
accuracy to waveform-specific analytical methods and were able to be performed
in real-time on a single NVIDIA H100 GPU. These methods are waveform agnostic
and applied directly to the samples, improving system flexibility and making
them easy to incorporate into existing software-defined radio systems.

</details>


### [2] [Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas](https://arxiv.org/abs/2508.04964)
*Zhaowei Wang,Yunsong Huang,Weicheng Liu,Hui-Ming Wang*

Main category: eess.SP

TL;DR: 提出了一种基于分布式可重构智能超表面天线（RIMSA）的无线传感方法，通过深度强化学习优化波束成形，提高信号质量，并在干扰环境下保持高精度传感。


<details>
  <summary>Details</summary>
Motivation: 传统射频传感方法在不利传播信道（如衰落和噪声）下准确性受限，需要一种更鲁棒的解决方案。

Method: 部署多个RIMSA接收器，利用深度强化学习优化波束成形模式，并通过神经网络将接收信号映射为传感结果。

Result: 仿真表明，分布式RIMSA系统比集中式实现更高效，且在干扰攻击下仍能保持高精度传感。

Conclusion: 分布式RIMSA系统结合DRL和神经网络，显著提升了无线传感的鲁棒性和准确性。

Abstract: The utilization of radio frequency (RF) signals for wireless sensing has
garnered increasing attention. However, the radio environment is unpredictable
and often unfavorable, the sensing accuracy of traditional RF sensing methods
is often affected by adverse propagation channels from the transmitter to the
receiver, such as fading and noise. In this paper, we propose employing
distributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect
the presence and location of objects where multiple RIMSA receivers (RIMSA Rxs)
are deployed on different places. By programming their beamforming patterns,
RIMSA Rxs can enhance the quality of received signals. The RF sensing problem
is modeled as a joint optimization problem of beamforming pattern and mapping
of received signals to sensing outcomes. To address this challenge, we
introduce a deep reinforcement learning (DRL) algorithm aimed at calculating
the optimal beamforming patterns and a neural network aimed at converting
received signals into sensing outcomes. In addition, the malicious attacker may
potentially launch jamming attack to disrupt sensing process. To enable
effective sensing in interferenceprone environment, we devise a combined loss
function that takes into account the Signal to Interference plus Noise Ratio
(SINR) of the received signals. The simulation results show that the proposed
distributed RIMSA system can achieve more efficient sensing performance and
better overcome environmental influences than centralized implementation.
Furthermore, the introduced method ensures high-accuracy sensing performance
even under jamming attack.

</details>


### [3] [Localized Kernel Methods for Signal Processing](https://arxiv.org/abs/2508.04978)
*Sippanon Kitimoon*

Main category: eess.SP

TL;DR: 论文提出了两种基于局部核的信号处理方法，用于噪声条件下的参数恢复。第一种方法针对多维指数模型的频率和幅度估计，第二种方法用于分离线性调频信号。两种方法均基于局部核和高效FFT实现，无需子空间分解或稀疏正则化。


<details>
  <summary>Details</summary>
Motivation: 解决噪声条件下信号参数恢复的挑战，特别是在低信噪比和多维情况下。

Method: 1. 使用局部三角多项式核检测多维频率，并通过坐标投影和配准提高恢复精度；2. 构建局部核的SSO变体，通过FFT滤波和分段线性回归分离调频信号。

Result: 在低信噪比下优于经典算法（如MUSIC和ESPRIT），并能恢复低至-30 dB的交叉和不连续调频信号。

Conclusion: 提出的方法具有鲁棒性和高效性，可扩展到非线性调频信号和自适应核设计。

Abstract: This dissertation presents two signal processing methods using specially
designed localized kernels for parameter recovery under noisy condition. The
first method addresses the estimation of frequencies and amplitudes in
multidimensional exponential models. It utilizes localized trigonometric
polynomial kernels to detect the multivariate frequencies, followed by a more
detailed parameter estimation. We compare our method with MUSIC and ESPRIT,
which are classical subspace-based algorithms widely used for estimating the
parameters of exponential signals. In the univariate case, the method
outperforms MUSIC and ESPRIT under low signal-to-noise ratios. For the
multivariate case, we develop a coordinate-wise projection and registration
approach that achieves high recovery accuracy using significantly fewer samples
than other methods.
  The second method focuses on separating linear chirp components from
time-localized signal segments. A variant of the Signal Separation Operator
(SSO) is constructed using a localized kernel. Instantaneous frequency
estimates are obtained via FFT-based filtering, then clustered and fitted with
piecewise linear regression. The method operates without prior knowledge of the
number of components and is shown to recover intersecting and discontinuous
chirps at SNR levels as low as -30 dB.
  Both methods share an idea based on localized kernels and efficient FFT-based
implementation, and neither requires subspace decomposition or sparsity
regularization. Experimental results confirm the robustness and tractability of
the proposed approaches across a range of simulated data conditions. Potential
extensions include application to nonlinear chirps, adaptive kernel design, and
signal classification using extracted features.

</details>


### [4] [Power-Constrained and Quantized MIMO-RSMA Systems with Imperfect CSIT: Joint Precoding, Antenna Selection, and Power Control](https://arxiv.org/abs/2508.05080)
*Jiwon Sung,Seokjun Park,Jinseok Choi*

Main category: eess.SP

TL;DR: 提出了一种联合预编码、天线选择和发射功率控制算法，以最大化基站功率预算下的总频谱效率。


<details>
  <summary>Details</summary>
Motivation: 充分利用基站的功率潜力，解决多用户MIMO系统中频谱效率最大化问题。

Method: 通过条件平均速率方法处理不完美CSIT，分解问题为预编码方向和功率控制子问题，分别用拉格朗日驻点和梯度下降求解。

Result: 仿真验证了算法有效性，并发现8-11位中分辨率DAC比低分辨率DAC更高效。

Conclusion: 提出的算法能有效提升频谱效率，中分辨率DAC在功率效率上更具优势。

Abstract: To utilize the full potential of the available power at a base station (BS),
we propose a joint precoding, antenna selection, and transmit power control
algorithm for a total power budget at the BS. We formulate a sum spectral
efficiency (SE) maximization problem for downlink multi-user multiple-input
multiple-output (MIMO) rate-splitting multiple access (RSMA) systems with
arbitrary-resolution digital-to-analog converters (DACs). We reformulate the
problem by defining the ergodic sum SE using the conditional average rate
approach to handle imperfect channel state information at the transmitter
(CSIT), and by using approximation techniques to make the problem more
tractable. Then, we decompose the problem into precoding direction and power
control subproblems. We solve the precoding direction subproblem by identifying
a superior Lagrangian stationary point, and the power control subproblem using
gradient descent. We also propose a complexity-reduction approach that is more
suitable for massive MIMO systems. Simulation results not only validate the
proposed algorithm but also reveal that when utilizing the full potential of
the power budget at the BS, medium-resolution DACs with 8-11 bits may actually
be more power-efficient than low-resolution DACs.

</details>


### [5] [Digital Twin Channel-Aided CSI Prediction: A Environment-based Subspace Extraction Approach for Achieving Low Overhead and Robustness](https://arxiv.org/abs/2508.05142)
*Yichen Cai,Jianhua Zhang,Li Yu,Zhen Zhang,Yuxiang Zhang,Lianzheng Shi,Yuelong Qiu*

Main category: eess.SP

TL;DR: 提出了一种基于环境特定信道子空间基（EB）的部分到全信道状态信息（CSI）预测方法（EB-P2WCP），用于实现低开销的信道预测，显著减少系统开销。


<details>
  <summary>Details</summary>
Motivation: 为满足6G移动通信系统在复杂场景下的鲁棒和高速通信需求，利用感知和AI的数字孪生信道（DTC）技术降低系统开销。

Method: 利用EB表征电磁环境的静态特性，结合实时估计的局部CSI，预测整个空间-频率域信道，设计EB-P2WNet网络实现鲁棒预测。

Result: 在低信噪比和导频比条件下，EB显著减少50%导频开销，抗多用户干扰能力强，容忍3米定位误差，预测时间仅1.3毫秒。

Conclusion: EB-P2WCP方法在复杂场景下实现高效低开销信道预测，为6G通信提供实用解决方案。

Abstract: To meet the robust and high-speed communication requirements of the
sixth-generation (6G) mobile communication system in complex scenarios,
sensing- and artificial intelligence (AI)-based digital twin channel (DTC)
techniques become a promising approach to reduce system overhead. In this
paper, we propose an environment-specific channel subspace basis (EB)-aided
partial-to-whole channel state information (CSI) prediction method (EB-P2WCP)
for realizing DTC-enabled low-overhead channel prediction. Specifically, EB is
utilized to characterize the static properties of the electromagnetic
environment, which is extracted from the digital twin map, serving as
environmental information prior to the prediction task. Then, we fuse EB with
real-time estimated local CSI to predict the entire spatial-frequency domain
channel for both the present and future time instances. Hence, an EB-based
partial-to-whole CSI prediction network (EB-P2WNet) is designed to achieve a
robust channel prediction scheme in various complex scenarios. Simulation
results indicate that incorporating EB provides significant benefits under low
signal-to-noise ratio and pilot ratio conditions, achieving a reduction of up
to 50% in pilot overhead. Additionally, the proposed method maintains
robustness against multi-user interference, tolerating 3-meter localization
errors with only a 0.5 dB NMSE increase, and predicts CSI for the next channel
coherent time within 1.3 milliseconds.

</details>


### [6] [Optimization of Liquid Lens-based Imaging Receiver for MIMO VLC Systems](https://arxiv.org/abs/2508.05204)
*Kapila W. S. Palitharathna,Christodoulos Skouroumounis,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 提出了一种基于液体透镜的成像接收器，用于MIMO可见光通信系统，通过动态调整透镜参数降低空间相关性，显著提升误码率性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态透镜在动态条件下（如用户移动或接收器随机方向）适应性不足，液体透镜的动态调整能力可解决这一问题。

Method: 开发了数学模型描述系统信道增益，并提出了两种透镜调整方案（CLS和VULO）以优化误码率。

Result: 在接收器方向方差为10°时，误码率从4×10−2显著降至5×10−4。

Conclusion: 液体透镜显著优于传统静态透镜，尤其在动态条件下表现突出。

Abstract: In this paper, a liquid lens-based imaging receiver is proposed for
multiple-input multiple-output (MIMO) visible light communication (VLC)
systems. By dynamically adjusting the focal length and orientation angles of
the liquid lens, the spatial correlation between MIMO channel gains is reduced,
leading to enhanced bit-error rate (BER) performance. Unlike static lenses,
liquid lenses offer adaptability in dynamic conditions, including user mobility
and random receiver orientation. An accurate mathematical framework is
developed to model the channel gains of the proposed system, and an
optimization problem is formulated to minimize its BER. Due to the complexity
of the resulting channel model, two lens adjustment schemes, namely, ($i$) the
CLS scheme, and ($ii$) the VULO scheme are introduced. Numerical results
demonstrate that the proposed liquid lens-based system offers substantial BER
improvements over conventional static lens-based receivers across a wide range
of random receiver orientation conditions. Specifically, at a random receiver
orientation variance of $10^{\circ}$, the BER is improved from $4\times
10^{-2}$ to $5\times 10^{-4}$ by employing the proposed liquid lens.

</details>


### [7] [Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios](https://arxiv.org/abs/2508.05226)
*Junzhe Song,Ruisi He,Mi Yang,Zhengyu Zhang,Bingcheng Liu,Jiahui Han,Haoxiang Zhang,Bo Ai*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的ISAC框架，用于车辆环境重建，通过多阶段网络实现高质量动态场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC方法在动态场景跟踪和时态一致性上不足，限制了实际应用。

Method: 建立多模态数据集，开发多阶段深度学习网络（场景解码器、聚类中心解码器、点云解码器）进行环境重建。

Result: 实验显示方法在Chamfer Distance和F Score@1%上表现优异，复杂度分析验证了实时适用性。

Conclusion: 该方法为未来智能交通提供了低成本环境重建的可行路径。

Abstract: Integrated Sensing and Communication (ISAC) technology plays a critical role
in future intelligent transportation systems, by enabling vehicles to perceive
and reconstruct the surrounding environment through reuse of wireless signals,
thereby reducing or even eliminating the need for additional sensors such as
LiDAR or radar. However, existing ISAC based reconstruction methods often lack
the ability to track dynamic scenes with sufficient accuracy and temporal
consistency, limiting the real world applicability. To address this limitation,
we propose a deep learning based framework for vehicular environment
reconstruction by using ISAC channels. We first establish a joint channel
environment dataset based on multi modal measurements from real world urban
street scenarios. Then, a multistage deep learning network is developed to
reconstruct the environment. Specifically, a scene decoder identifies the
environmental context such as buildings, trees and so on; a cluster center
decoder predicts coarse spatial layouts by localizing dominant scattering
centers; a point cloud decoder recovers fine grained geometry and structure of
surrounding environments. Experimental results demonstrate that the proposed
method achieves high-quality dynamic environment reconstruction with a Chamfer
Distance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis
demonstrates the efficiency and practical applicability of the method in real
time scenarios. This work provides a pathway toward low cost environment
reconstruction based on ISAC for future intelligent transportation.

</details>


### [8] [Unifying Common Signal Analyses with Instantaneous Time-Frequency Atoms](https://arxiv.org/abs/2508.05380)
*Steven Sandoval,Phillip L. De Leon*

Main category: eess.SP

TL;DR: 本文提出了一种基于瞬时时频原子的方法，用于计算与多种信号分析相关的瞬时频谱（IS），并通过二次线性调频信号将这些分析统一为一个二维连续体。


<details>
  <summary>Details</summary>
Motivation: 之前的工作提出了瞬时时频分析的通用框架，但未提供具体计算瞬时频谱的方法。本文旨在填补这一空白。

Method: 使用瞬时时频原子，结合二次线性调频信号，将多种信号分析（如时域分析、频域分析等）统一为一个二维连续体。

Result: 开发了闭式表达式，计算了多种信号分析的IS，并通过示例信号验证了方法的有效性。

Conclusion: 该方法成功地将多种信号分析统一为一个框架，并提供了计算IS的具体实现。

Abstract: In previous work, we presented a general framework for instantaneous
time-frequency analysis but did not provide any specific details of how to
compute a particular instantaneous spectrum (IS). In this work, we use
instantaneous time-frequency atoms to obtain an IS associated with common
signal analyses: time domain analysis, frequency domain analysis, fractional
Fourier transform, synchrosqueezed short-time Fourier transform, and
synchrosqueezed short-time fractional Fourier transform. By doing so, we
demonstrate how the general framework can be used to unify these analyses and
we develop closed-form expressions for the corresponding ISs. This is
accomplished by viewing these analyses as decompositions into AM--FM components
and recognizing that each uses a specialized (or limiting) form of a quadratic
chirplet as a template during analysis. With a two-parameter quadratic
chirplet, we can organize these ISs into a 2D continuum with points in the
plane corresponding to a decomposition related to one of the signal analyses.
Finally, using several example signals, we compute in closed-form the ISs for
the various analyses.

</details>


### [9] [Sub- μ W Battery-Less and Oscillator-Less Wi-Fi Backscattering Transmitter Reusing RF Signal for Harvesting, Communications, and Motion Detection](https://arxiv.org/abs/2508.05479)
*Marco Privitera,Andrea Ballo,Karim Ali Ahmed,Alfio Dario Grasso,Massimo Alioto*

Main category: eess.SP

TL;DR: 论文提出了一种亚微瓦级功耗的802.11b反向散射发射器，实现了射频能量收集、反向散射通信和位置/运动传感的多功能集成。


<details>
  <summary>Details</summary>
Motivation: 通过消除电池和外部运动传感器（如MEMS），实现设备的超小型化、长寿命和低成本。

Method: 采用双音入射波的二阶互调提取频率，消除本地振荡器，突破WiFi发射器的微瓦功耗限制。

Result: 实现了低至-19 dBm的累积能量收集/传输/传感灵敏度，并通过能量收集电压作为RSS代理实现位置/运动传感。

Conclusion: 该技术为无电池、超低功耗的物联网设备提供了新的解决方案。

Abstract: In this paper, a sub-uW power 802.11b backscattering transmitter is presented
to enable reuse of the same incident wave for three purposes: RF harvesting,
backscattering communications and position/motion sensing. The removal of the
battery and any off-chip motion sensor (e.g., MEMS) enables unprecedented level
of miniaturization and ubiquity, unrestricted device lifespan, low fabrication
and maintenance cost. The uW power wall for WiFi transmitters is broken for the
first time via local oscillator elimination, as achieved by extracting its
frequency through second-order intermodulation of a twotone incident wave. The
two-tone scheme also enables a cumulative harvesting/transmission/sensing
sensitivity down to Pmin -19 dBm. Position/motion sensing is enabled by using
the harvested voltage as a proxy for the Received Signal Strength (RSS),
allowing to sense the chip location with respect to the tone generator(s)
shared across tags in indoor neighborhoods.

</details>


### [10] [0.6-V, uW-Power 4-Stage OTA with Minimal Components and 100X Load Range](https://arxiv.org/abs/2508.05499)
*M. Privitera,A. D. Grasso,A. Ballo,M. Alioto*

Main category: eess.SP

TL;DR: 本文提出了一种用于超低功耗应用的四级运算跨导放大器（OTA），通过简化的频率补偿设计，减少了晶体管和被动元件的数量，同时提高了功率效率。


<details>
  <summary>Details</summary>
Motivation: 传统四级OTA的补偿设计复杂，且功率效率较低，本文旨在简化设计并提升性能。

Method: 采用四阶段OTA设计，结合简化的频率补偿技术，减少晶体管和被动元件数量。

Result: 与现有四级OTA相比，大信号（小信号）功率效率FOML（FOMS）提高了>3.7X（>11.3X），负载电容范围稳定性显著提升。

Conclusion: 所提出的OTA设计在简化补偿的同时，显著提升了功率效率和负载适应性，适用于超低功耗应用。

Abstract: A four-stage operational transconductance amplifier (OTA) for ultra-low-power
applications is introduced in this paper. The proposed circuit inclusive of
frequency compensation requires minimal transistor count and passives,
overcoming the traditionally difficult compensation of 4-stage OTAs and
bringing it back to the simplicity of 3-stage OTAs. At the same time, the
proposed circuit achieves high power efficiency, as evidenced by the >3.7X
(>11.3X) improvement in the large-signal (small-signal) power efficiency figure
of merit FOML (FOMS), compared to prior 4-stage OTAs (sub-1 V multi-stage
OTAs). Thanks to the lower sensitivity of the phase margin to the load
capacitance, the proposed OTA remains stable under a wide range of loads
(double-sided as in any 3-4-stage OTA), achieving a max/min ratio of the load
capacitance of >100X.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices](https://arxiv.org/abs/2508.04857)
*Yael Segal-Feldman,Ann R. Bradlow,Matthew Goldrick,Joseph Keshet*

Main category: eess.AS

TL;DR: 该论文提出了一种用于小型设备的开放词汇关键词检测模型，结合语音编码器、关键词编码器和检测网络，实现了最先进的检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇关键词检测任务，即在语音记录中检测未包含在训练数据中的词汇或术语。

Method: 模型由语音编码器（Tiny Whisper或Tiny Conformer）、目标关键词编码器（超网络生成卷积层权重）和检测网络（Perceiver模块的交叉注意力机制）组成。

Result: 系统实现了最先进的检测性能，并在域外条件（如第二语言语音）下表现出色。最小模型（420万参数）性能优于更大模型。

Conclusion: 该模型高效且鲁棒，适用于小型设备，并在开放词汇关键词检测任务中表现卓越。

Abstract: Open-vocabulary keyword spotting (KWS) refers to the task of detecting words
or terms within speech recordings, regardless of whether they were included in
the training data. This paper introduces an open-vocabulary keyword spotting
model with state-of-the-art detection accuracy for small-footprint devices. The
model is composed of a speech encoder, a target keyword encoder, and a
detection network. The speech encoder is either a tiny Whisper or a tiny
Conformer. The target keyword encoder is implemented as a hyper-network that
takes the desired keyword as a character string and generates a unique set of
weights for a convolutional layer, which can be considered as a
keyword-specific matched filter. The detection network uses the matched-filter
weights to perform a keyword-specific convolution, which guides the
cross-attention mechanism of a Perceiver module in determining whether the
target term appears in the recording. The results indicate that our system
achieves state-of-the-art detection performance and generalizes effectively to
out-of-domain conditions, including second-language (L2) speech. Notably, our
smallest model, with just 4.2 million parameters, matches or outperforms models
that are several times larger, demonstrating both efficiency and robustness.

</details>


### [12] [Closed-Form Successive Relative Transfer Function Vector Estimation based on Blind Oblique Projection Incorporating Noise Whitening](https://arxiv.org/abs/2508.04887)
*Henri Gode,Simon Doclo*

Main category: eess.AS

TL;DR: 本文提出三种改进盲斜投影（BOP）方法的扩展，用于在噪声和混响环境中在线估计多个声源的相对传递函数（RTF）。改进包括闭式解、正交附加向量和噪声处理技术，提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在噪声和混响环境中，现有BOP方法存在计算复杂度高、性能受随机附加向量影响以及高信噪比假设的局限性，需改进以提升RTF估计的效率和鲁棒性。

Method: 提出三种BOP扩展：闭式解优化成本函数、正交附加向量替代随机向量、噪声处理技术（协方差减法和白化）。并开发基于空间相干性的在线声源计数方法。

Result: 仿真实验表明，改进方法在真实混响噪声环境中显著提高了RTF估计的准确性和鲁棒性，尤其适用于低信噪比条件。

Conclusion: 改进的BOP方法有效解决了现有局限性，为多声源RTF在线估计提供了更高效和鲁棒的解决方案。

Abstract: Relative transfer functions (RTFs) of sound sources play a crucial role in
beamforming, enabling effective noise and interference suppression. This paper
addresses the challenge of online estimating the RTF vectors of multiple sound
sources in noisy and reverberant environments, for the specific scenario where
sources activate successively. While the RTF vector of the first source can be
estimated straightforwardly, the main challenge arises in estimating the RTF
vectors of subsequent sources during segments where multiple sources are
simultaneously active. The blind oblique projection (BOP) method has been
proposed to estimate the RTF vector of a newly activating source by optimally
blocking this source. However, this method faces several limitations: high
computational complexity due to its reliance on iterative gradient descent
optimization, the introduction of random additional vectors, which can
negatively impact performance, and the assumption of high signal-to-noise ratio
(SNR). To overcome these limitations, in this paper we propose three extensions
to the BOP method. First, we derive a closed-form solution for optimizing the
BOP cost function, significantly reducing computational complexity. Second, we
introduce orthogonal additional vectors instead of random vectors, enhancing
RTF vector estimation accuracy. Third, we incorporate noise handling techniques
inspired by covariance subtraction and whitening, increasing robustness in low
SNR conditions. To provide a frame-by-frame estimate of the source activity
pattern, required by both the conventional BOP method and the proposed method,
we propose a spatial-coherence-based online source counting method. Simulations
are performed with real-world reverberant noisy recordings featuring 3
successively activating speakers, with and without a-priori knowledge of the
source activity pattern.

</details>


### [13] [REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with Diffusion Transformers](https://arxiv.org/abs/2508.04996)
*Yuepeng Jiang,Ziqian Ning,Shuai Wang,Chengjia Wang,Mengxiao Bi,Pengcheng Zhu,Lei Xie,Zhonghua Fu*

Main category: eess.AS

TL;DR: REF-VC是一种抗噪声且富有表现力的语音转换系统，通过随机擦除策略和隐式对齐提升性能，并在实验中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决实际语音转换应用中环境噪声和用户对表现力输出的需求问题，传统方法在噪声鲁棒性和表现力之间存在权衡。

Method: 提出REF-VC系统，采用随机擦除策略减少SSL特征冗余，隐式对齐抑制非必要特征重建，并集成Shortcut Models加速推理。

Result: 在零噪声场景下优于Seed-VC等基线模型，同时在干净数据集上表现相当，且兼容歌唱语音转换。

Conclusion: REF-VC在噪声鲁棒性和表现力方面取得平衡，并展示了多功能性。

Abstract: In real-world voice conversion applications, environmental noise in source
speech and user demands for expressive output pose critical challenges.
Traditional ASR-based methods ensure noise robustness but suppress prosody,
while SSL-based models improve expressiveness but suffer from timbre leakage
and noise sensitivity. This paper proposes REF-VC, a noise-robust expressive
voice conversion system. Key innovations include: (1) A random erasing strategy
to mitigate the information redundancy inherent in SSL feature, enhancing noise
robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to
suppress non-essential feature reconstruction; (3) Integration of Shortcut
Models to accelerate flow matching inference, significantly reducing to 4
steps. Experimental results demonstrate that our model outperforms baselines
such as Seed-VC in zero-shot scenarios on the noisy set, while also performing
comparably to Seed-VC on the clean set. In addition, REF-VC can be compatible
with singing voice conversion within one model.

</details>


### [14] [MOVER: Combining Multiple Meeting Recognition Systems](https://arxiv.org/abs/2508.05055)
*Naoyuki Kamo,Tsubasa Ochiai,Marc Delcroix,Tomohiro Nakatani*

Main category: eess.AS

TL;DR: MOVER是一种新的会议识别系统组合方法，首次结合了不同说话人分割和语音识别输出的系统，通过五阶段流程显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能单独结合说话人分割或语音识别系统的输出，而MOVER填补了同时结合两者的空白。

Method: MOVER通过五阶段流程（包括说话人对齐、分段分组、词和时间组合等）结合不同时间间隔和说话人标签的假设。

Result: 在CHiME-8 DASR和NOTSOFAR-1任务中，MOVER相比现有最佳系统分别实现了9.55%和8.51%的相对tcpWER提升。

Conclusion: MOVER成功结合了多样化的会议识别系统输出，显著提升了性能，为会议识别任务提供了新的解决方案。

Abstract: In this paper, we propose Meeting recognizer Output Voting Error Reduction
(MOVER), a novel system combination method for meeting recognition tasks.
Although there are methods to combine the output of diarization (e.g., DOVER)
or automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first
approach that can combine the outputs of meeting recognition systems that
differ in terms of both diarization and ASR. MOVER combines hypotheses with
different time intervals and speaker labels through a five-stage process that
includes speaker alignment, segment grouping, word and timing combination, etc.
Experimental results on the CHiME-8 DASR task and the multi-channel track of
the NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple
meeting recognition systems with diverse diarization and recognition outputs,
achieving relative tcpWER improvements of 9.55 % and 8.51 % over the
state-of-the-art systems for both tasks.

</details>


### [15] [Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS](https://arxiv.org/abs/2508.05102)
*Anuprabha M,Krishna Gurugubelli,Anil Kumar Vuppala*

Main category: eess.AS

TL;DR: 研究探讨F5-TTS在克隆构音障碍语音时的效果，发现其偏向语音清晰度而非说话者相似性和韵律保留，并提出公平性评估方法。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音数据稀缺，现有语音合成技术可能引入偏见，需研究其公平性和效果。

Method: 使用TORGO数据集评估F5-TTS在克隆构音障碍语音时的表现，分析清晰度、说话者相似性和韵律保留，并采用公平性指标评估偏见。

Result: F5-TTS在构音障碍语音合成中更注重清晰度，而忽略说话者相似性和韵律保留，存在偏见。

Conclusion: 研究为开发更公平的构音障碍语音合成技术提供了见解，推动包容性语音技术的发展。

Abstract: Dysarthric speech poses significant challenges in developing assistive
technologies, primarily due to the limited availability of data. Recent
advances in neural speech synthesis, especially zero-shot voice cloning,
facilitate synthetic speech generation for data augmentation; however, they may
introduce biases towards dysarthric speech. In this paper, we investigate the
effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using
TORGO dataset, focusing on intelligibility, speaker similarity, and prosody
preservation. We also analyze potential biases using fairness metrics like
Disparate Impact and Parity Difference to assess disparities across dysarthric
severity levels. Results show that F5-TTS exhibits a strong bias toward speech
intelligibility over speaker and prosody preservation in dysarthric speech
synthesis. Insights from this study can help integrate fairness-aware
dysarthric speech synthesis, fostering the advancement of more inclusive speech
technologies.

</details>


### [16] [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
*Seraphina Fong,Marco Matassoni,Alessio Brutti*

Main category: eess.AS

TL;DR: 本文研究了在低资源语言环境下使用语音大语言模型（Speech LLMs）进行自动语音识别（ASR），通过SLAM-ASR框架连接语音编码器和大语言模型，探讨了数据量需求及多语言预训练模型的优势。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在低资源语言自动语音识别中的适用性，解决数据稀缺问题。

Method: 使用SLAM-ASR框架，通过轻量级可训练投影器连接语音编码器和大语言模型，评估数据量需求及多语言预训练模型的效果。

Result: 多语言预训练模型（如EuroLLM、Salamandra）在小训练集下显著减轻数据稀缺影响，性能接近Whisper-only。

Conclusion: 多语言预训练模型为低资源语言ASR提供了优化方向，未来研究可进一步探索其潜力。

Abstract: Large language models (LLMs) have demonstrated potential in handling spoken
inputs for high-resource languages, reaching state-of-the-art performance in
various tasks. However, their applicability is still less explored in
low-resource settings. This work investigates the use of Speech LLMs for
low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a
trainable lightweight projector connects a speech encoder and a LLM. Firstly,
we assess training data volume requirements to match Whisper-only performance,
re-emphasizing the challenges of limited data. Secondly, we show that
leveraging mono- or multilingual projectors pretrained on high-resource
languages reduces the impact of data scarcity, especially with small training
sets. Using multilingual LLMs (EuroLLM, Salamandra) with
whisper-large-v3-turbo, we evaluate performance on several public benchmarks,
providing insights for future research on optimizing Speech LLMs for
low-resource languages and multilinguality.

</details>


### [17] [Privacy Disclosure of Similarity in Speech and Language Processing](https://arxiv.org/abs/2508.05250)
*Tom Bäckström,Mohammad Hassan Vali,My Nguyen,Silas Rech*

Main category: eess.AS

TL;DR: 该论文提出了一种量化相似性排名隐私泄露的方法，通过估计其概率分布来评估生物特征识别中的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 生物特征识别中，相似性排名可能泄露隐私信息，但现有方法未能量化这种泄露。

Method: 基于真实说话者的相似性排名直方图或Beta-二项分布建模，以熵（比特）表示隐私泄露。

Result: 实验表明，所有测试的生物特征（如说话者识别嵌入、电话嵌入等）均包含可识别个人的信息，且泄露随样本长度增加而增加。

Conclusion: 相似性排名泄露指标可用于比较不同生物特征的隐私风险，并辅助隐私威胁的全面评估。

Abstract: Speaker, author, and other biometric identification applications often
compare a sample's similarity to a database of templates to determine the
identity. Given that data may be noisy and similarity measures can be
inaccurate, such a comparison may not reliably identify the true identity as
the most similar. Still, even the similarity rank based on an inaccurate
similarity measure can disclose private information about the true identity. We
propose a methodology for quantifying the privacy disclosure of such a
similarity rank by estimating its probability distribution. It is based on
determining the histogram of the similarity rank of the true speaker, or when
data is scarce, modeling the histogram with the beta-binomial distribution. We
express the disclosure in terms of entropy (bits), such that the disclosure
from independent features are additive. Our experiments demonstrate that all
tested speaker and author characterizations contain personally identifying
information (PII) that can aid in identification, with embeddings from speaker
recognition algorithms containing the most information, followed by phone
embeddings, linguistic embeddings, and fundamental frequency. Our initial
experiments show that the disclosure of PII increases with the length of test
samples, but it is bounded by the length of database templates. The provided
metric, similarity rank disclosure, provides a way to compare the disclosure of
PII between biometric features and merge them to aid identification. It can
thus aid in the holistic evaluation of threats to privacy in speech and other
biometric technologies.

</details>


### [18] [Investigation of Speech and Noise Latent Representations in Single-channel VAE-based Speech Enhancement](https://arxiv.org/abs/2508.05293)
*Jiatong Li,Simon Doclo*

Main category: eess.AS

TL;DR: 研究探讨了变分自编码器（VAE）中语音和噪声潜在表示对语音增强性能的影响，发现清晰的分离表示能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索不同潜在表示如何影响基于VAE的语音增强系统性能。

Method: 使用预训练的VAE获取语音和噪声的潜在表示，并通过修改损失项研究其影响。

Result: 实验表明，语音和噪声潜在表示分离清晰的VAE性能优于表示重叠的标准VAE。

Conclusion: 清晰的潜在表示分离是提升VAE语音增强性能的关键。

Abstract: Recently, a variational autoencoder (VAE)-based single-channel speech
enhancement system using Bayesian permutation training has been proposed, which
uses two pretrained VAEs to obtain latent representations for speech and noise.
Based on these pretrained VAEs, a noisy VAE learns to generate speech and noise
latent representations from noisy speech for speech enhancement. Modifying the
pretrained VAE loss terms affects the pretrained speech and noise latent
representations. In this paper, we investigate how these different
representations affect speech enhancement performance. Experiments on the DNS3,
WSJ0-QUT, and VoiceBank-DEMAND datasets show that a latent space where speech
and noise representations are clearly separated significantly improves
performance over standard VAEs, which produce overlapping speech and noise
representations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [19] [Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS](https://arxiv.org/abs/2508.04721)
*Vignesh Ethiraj,Ashwath David,Sidhanth Menon,Divya Vijay*

Main category: cs.SD

TL;DR: 论文介绍了一种低延迟的电信AI语音代理管道，结合了四个专门模型，用于实时交互式电信应用，如呼叫中心自动化和智能客户支持。


<details>
  <summary>Details</summary>
Motivation: 为电信行业提供高效、低延迟的语音AI解决方案，支持实时交互和知识驱动的对话。

Method: 结合四个专门模型（TSLAM、T-VEC、TTE、T-Synth），集成流式ASR、对话智能、检索增强生成和实时TTS。

Result: 系统在500条电信问题数据集上测试，TSLAM、TTE和T-Synth的实时因子（RTF）低于1.0，适合企业级低延迟部署。

Conclusion: 该AI代理管道为下一代电信AI奠定了基础，支持自动化客户支持和诊断等功能。

Abstract: We introduce a low-latency telecom AI voice agent pipeline for real-time,
interactive telecommunications use, enabling advanced voice AI for call center
automation, intelligent IVR (Interactive Voice Response), and AI-driven
customer support. The solution is built for telecom, combining four specialized
models by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language
Model (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific
Automatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific
Text-to-Speech (TTS) model. These models enable highly responsive,
domain-adapted voice AI agents supporting knowledge-grounded spoken
interactions with low latency. The pipeline integrates streaming ASR (TTE),
conversational intelligence (TSLAM), retrieval augmented generation (RAG) over
telecom documents, and real-time TTS (T-Synth), setting a new benchmark for
telecom voice assistants. To evaluate the system, we built a dataset of 500
human-recorded telecom questions from RFCs, simulating real telecom agent
queries. This framework allows analysis of latency, domain relevance, and
real-time performance across the stack. Results show that TSLAM, TTE, and
T-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,
low-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and
T-Synth -- provide a foundation for next-generation telecom AI, enabling
automated customer support, diagnostics, and more.

</details>


### [20] [Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion](https://arxiv.org/abs/2508.04723)
*Sha Zhao,Song Yi,Yangxuan Zhou,Jiadong Pan,Jiquan Wang,Jie Xia,Shijian Li,Shurong Dong,Gang Pan*

Main category: cs.SD

TL;DR: MEEtBrain是一个便携式多模态框架，用于情绪分析，结合AI生成音乐和无线EEG-fNIRS采集，解决了现有研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 音乐对情绪的影响引发了对基于音乐的神经生理信号情感计算的兴趣，但现有研究存在音乐刺激受限、模态单一和便携性差的问题。

Method: 提出MEEtBrain框架，利用AI生成音乐刺激，并通过无线头带同步采集EEG和fNIRS信号。

Result: 收集了14小时的多模态数据，验证了框架的有效性，并公开数据集以促进研究。

Conclusion: MEEtBrain通过AI生成音乐和多模态信号采集，解决了现有研究的局限性，具有实际应用潜力。

Abstract: Emotions critically influence mental health, driving interest in music-based
affective computing via neurophysiological signals with Brain-computer
Interface techniques. While prior studies leverage music's accessibility for
emotion induction, three key limitations persist: \textbf{(1) Stimulus
Constraints}: Music stimuli are confined to small corpora due to copyright and
curation costs, with selection biases from heuristic emotion-music mappings
that ignore individual affective profiles. \textbf{(2) Modality Specificity}:
Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights
from cross-modal signal fusion.\textbf{ (3) Portability Limitation}: Cumbersome
setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability
due to procedural complexity and portability barriers. To address these
limitations, we propose MEEtBrain, a portable and multimodal framework for
emotion analysis (valence/arousal), integrating AI-generated music stimuli with
synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the
music stimuli can be automatically generated by AI on a large scale,
eliminating subjective selection biases while ensuring music diversity. We use
our developed portable device that is designed in a lightweight headband-style
and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A
14-hour dataset from 20 participants was collected in the first recruitment to
validate the framework's efficacy, with AI-generated music eliciting target
emotions (valence/arousal). We are actively expanding our multimodal dataset
(44 participants in the latest dataset) and make it publicly available to
promote further research and practical applications. \textbf{The dataset is
available at https://zju-bmi-lab.github.io/ZBra.

</details>


### [21] [Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation](https://arxiv.org/abs/2508.05011)
*Huaicheng Zhang,Wei Tan,Guangzheng Li,Yixuan Zhang,Hangting Chen,Shun Lei,Chenyu Yang,Zhiyong Wu,Shuai Wang,Qijun Huang,Dong Yu*

Main category: cs.SD

TL;DR: 论文提出了一种基于强化学习的框架，通过偏好优化控制歌词到歌曲生成中的幻觉问题，显著减少了音素错误率。


<details>
  <summary>Details</summary>
Motivation: 当前基于音频的生成语言模型在歌词到歌曲生成中存在内容幻觉问题，导致输出与输入歌词不一致，影响音乐连贯性。现有监督微调方法效果有限。

Method: 提出了一种强化学习框架，包括三种偏好优化策略：DPO、PPO和GRPO，利用音素错误率（PER）计算和规则过滤构建数据集。

Result: DPO、PPO和GRPO分别实现了7.4%、4.9%和4.7%的PER降低，有效抑制幻觉并保持音乐质量。

Conclusion: 该框架为歌词到歌曲生成中的幻觉控制提供了系统性解决方案，并具有扩展到音乐风格和音乐性增强的潜力。

Abstract: Recent advances in audio-based generative language models have accelerated
AI-driven lyric-to-song generation. However, these models frequently suffer
from content hallucination, producing outputs misaligned with the input lyrics
and undermining musical coherence. Current supervised fine-tuning (SFT)
approaches, limited by passive label-fitting, exhibit constrained
self-improvement and poor hallucination mitigation. To address this core
challenge, we propose a novel reinforcement learning (RL) framework leveraging
preference optimization for hallucination control. Our key contributions
include: (1) Developing a robust hallucination preference dataset constructed
via phoneme error rate (PER) computation and rule-based filtering to capture
alignment with human expectations; (2) Implementing and evaluating three
distinct preference optimization strategies within the RL framework: Direct
Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group
Relative Policy Optimization (GRPO). DPO operates off-policy to enhance
positive token likelihood, achieving a significant 7.4% PER reduction. PPO and
GRPO employ an on-policy approach, training a PER-based reward model to
iteratively optimize sequences via reward maximization and KL-regularization,
yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective
and subjective evaluations confirm that our methods effectively suppress
hallucinations while preserving musical quality. Crucially, this work presents
a systematic, RL-based solution to hallucination control in lyric-to-song
generation. The framework's transferability also unlocks potential for music
style adherence and musicality enhancement, opening new avenues for future
generative song research.

</details>


### [22] [SpectroStream: A Versatile Neural Codec for General Audio](https://arxiv.org/abs/2508.05207)
*Yunpeng Li,Kehang Han,Brian McWilliams,Zalan Borsos,Marco Tagliasacchi*

Main category: cs.SD

TL;DR: SpectroStream是一种全频带多通道神经音频编解码器，扩展了SoundStream的能力，支持48 kHz立体声音乐的高质量重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有编解码器在24 kHz单声道音频以上和48 kHz立体声音乐高质量重建方面的不足。

Method: 采用新的神经架构，利用时频域音频表示，并使用延迟融合策略处理多通道音频。

Result: 在4-16 kbps比特率下实现高质量音频重建，尤其在高采样率下表现更优。

Conclusion: SpectroStream在高质量音频重建和多通道处理方面表现出色。

Abstract: We propose SpectroStream, a full-band multi-channel neural audio codec.
Successor to the well-established SoundStream, SpectroStream extends its
capability beyond 24 kHz monophonic audio and enables high-quality
reconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is
accomplished with a new neural architecture that leverages audio representation
in the time-frequency domain, which leads to better audio quality especially at
higher sample rate. The model also uses a delayed-fusion strategy to handle
multi-channel audio, which is crucial in balancing per-channel acoustic quality
and cross-channel phase consistency.

</details>


### [23] [Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces](https://arxiv.org/abs/2508.05306)
*Mathias Rose Bjare,Stefan Lattner,Gerhard Widmer*

Main category: cs.SD

TL;DR: 论文研究了使用自回归扩散模型（ADMs）计算信息内容（IC）来建模音乐期望和意外性，发现其效果优于生成无限词汇变换器（GIVT）。


<details>
  <summary>Details</summary>
Motivation: 探索自回归扩散模型在音乐和音频特征建模中的有效性，尤其是信息内容（IC）对音乐意外性和音频分段边界的捕捉能力。

Method: 使用两种不同的扩散常微分方程（ODEs）计算IC，并通过负对数似然评估其性能。任务包括单音高意外性建模和多轨音频分段边界检测。

Result: 扩散模型在两项任务中表现优于或等同于GIVT，且在不同噪声水平下捕捉不同音频粒度的意外性。

Conclusion: 扩散模型能有效捕捉音乐和音频的意外性，且噪声水平的选择对任务性能有显著影响。

Abstract: Recently, the information content (IC) of predictions from a Generative
Infinite-Vocabulary Transformer (GIVT) has been used to model musical
expectancy and surprisal in audio. We investigate the effectiveness of such
modelling using IC calculated with autoregressive diffusion models (ADMs). We
empirically show that IC estimates of models based on two different diffusion
ordinary differential equations (ODEs) describe diverse data better, in terms
of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's
effectiveness in capturing surprisal aspects by examining two tasks: (1)
capturing monophonic pitch surprisal, and (2) detecting segment boundaries in
multi-track audio. In both tasks, the diffusion models match or exceed the
performance of a GIVT. We hypothesize that the surprisal estimated at different
diffusion process noise levels corresponds to the surprisal of music and audio
features present at different audio granularities. Testing our hypothesis, we
find that, for appropriate noise levels, the studied musical surprisal tasks'
results improve. Code is provided on github.com/SonyCSLParis/audioic.

</details>


### [24] [A Scalable Pipeline for Enabling Non-Verbal Speech Generation and Understanding](https://arxiv.org/abs/2508.05385)
*Runchuan Ye,Yixuan Zhou,Renjie Yu,Zijian Lin,Kehan Li,Xiang Li,Xin Liu,Guoyang Zeng,Zhiyong Wu*

Main category: cs.SD

TL;DR: 论文提出并验证了一个大规模非语言语音数据集NonVerbalSpeech-38K，用于提升语音系统的情感智能和交互丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有语音系统主要关注语言内容，缺乏对非语言声音（如笑声、叹息）的理解和生成能力，限制了交互的情感表达。

Method: 通过自动标注流程从真实媒体中收集并标注38,718个样本（10类非语言声音），并利用F5-TTS和Qwen2-Audio等模型验证数据集的有效性。

Result: 数据集在非语言语音生成和理解任务中表现出色，提升了语音合成和标注的性能。

Conclusion: 该数据集为非语言语音研究提供了实用工具，推动了更丰富的人机交互。

Abstract: Human spoken communication involves not only lexical content but also
non-verbal vocalizations (NVs) such as laughter, sighs, and coughs, which
convey emotions, intentions, and social signals. However, most existing speech
systems focus solely on verbal content and lack the ability to understand and
generate such non-verbal cues, reducing the emotional intelligence and
communicative richness of spoken interfaces. In this work, we introduce
$\textbf{NonVerbalSpeech-38K}$, a large and diverse dataset for non-verbal
speech generation and understanding, collected from real-world media and
annotated using an automatic pipeline. The dataset contains 38,718 samples
(about 131 hours) with 10 categories of non-verbal cues, such as laughter,
sniff, and throat clearing. We further validate the dataset by fine-tuning
state-of-the-art models, including F5-TTS and Qwen2-Audio, demonstrating its
effectiveness in non-verbal speech generation and understanding tasks. Our
contributions are threefold: (1) We propose a practical pipeline for building
natural and diverse non-verbal speech datasets; (2) We release a large-scale
dataset to advance research on non-verbal speech generation and understanding;
(3) We validate the dataset's effectiveness by demonstrating improvements in
both non-verbal speech synthesis and captioning, thereby facilitating richer
human-computer interaction.

</details>


### [25] [SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription](https://arxiv.org/abs/2508.05554)
*Raymond Grossman,Taejin Park,Kunal Dhawan,Andrew Titus,Sophia Zhi,Yulia Shchadilova,Weiqing Wang,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.SD

TL;DR: SPGISpeech 2.0是一个金融领域的语音数据集，新增3,780小时的专业转录收益电话录音，支持多说话人ASR任务，并验证了其在说话人标记ASR性能上的提升。


<details>
  <summary>Details</summary>
Motivation: 提供更多样化的建模任务支持，同时保持原始数据集的核心特性，推动语音识别技术的发展。

Method: 扩展数据集，新增收益电话录音，并包含通话和说话人信息，用于多说话人ASR任务。

Result: 验证了SPGISpeech 2.0在说话人标记ASR性能上的提升。

Conclusion: SPGISpeech 2.0有望促进语音识别技术的进步，并激发广泛的研究应用。

Abstract: We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged
transcription in the financial domain. SPGISpeech 2.0 improves the diversity of
applicable modeling tasks while maintaining the core characteristic of the
original SPGISpeech dataset: audio snippets and their corresponding fully
formatted text transcriptions, usable for end-to-end automatic speech
recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of
professionally transcribed earnings calls. Furthermore, the dataset contains
call and speaker information for each audio snippet facilitating multi-talker
ASR. We validate the utility of SPGISpeech 2.0 through improvements in
speaker-tagged ASR performance of popular speech recognition models after
fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect
SPGISpeech 2.0 to foster advancements in speech recognition technologies and
inspire a wide range of research applications.

</details>

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 8]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-Time Doppler and Ionospheric Dispersion Correction Techniques for Arbitrary Waveforms Utilizing GPU Compute](https://arxiv.org/abs/2508.04951)
*Daniel J. Vickers,A. H. Mack,Idahosa A. Osaretin*

Main category: eess.SP

TL;DR: 论文分析了雷达数字信号处理中通用的多普勒和电离层校正算法，提出基于GPU的高效实现方法，验证了实时处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统雷达信号处理依赖专用硬件，存在波形灵活性不足和系统复杂度高的问题，现代通用计算系统为实时数字信号处理提供了新可能。

Method: 提出了两种校正算法：基于FFT的电离层校正和基于数值插值的多普勒校正，并在NVIDIA H100 GPU上实现实时处理。

Result: 两种算法校正精度与专用方法相当，且能在单GPU上实时运行，波形无关性提高了系统灵活性。

Conclusion: 研究为雷达信号处理提供了高效、灵活的软件实现方案，适用于现有软件定义无线电系统。

Abstract: General requirements for radar digital signal processing are ionospheric
distortion and Doppler dispersion correction, which has historically required
radar-specific hardware to implement in real time. Although analog solutions
are computationally efficient, they often come with system design drawbacks
which limit waveform flexibility and can result in an overall increase of
system complexity. With improvements in modern general compute systems,
real-time digital signal processing is becoming more realizable using
non-radar-specific high-performance compute. In this paper, we present an
analysis of general Doppler and ionospheric correction algorithms for arbitrary
waveforms for radar digital signal processing. We also include considerations
for efficient implementation of these algorithms in software, specifically
using GPU hardware. This analysis includes metrics of performance such as
execution time and error correction accuracy. We also provide recommendations
for application in radar signal processing. We identify two algorithms for
dispersion correction: an FFT-based method for ionospheric dispersion and a
numerical interpolation method via sinc interpolation for Doppler dispersion.
Both of these algorithms are able to compensate for dispersion equivalent in
accuracy to waveform-specific analytical methods and were able to be performed
in real-time on a single NVIDIA H100 GPU. These methods are waveform agnostic
and applied directly to the samples, improving system flexibility and making
them easy to incorporate into existing software-defined radio systems.

</details>


### [2] [Anti-Jamming Sensing with Distributed Reconfigurable Intelligent Metasurface Antennas](https://arxiv.org/abs/2508.04964)
*Zhaowei Wang,Yunsong Huang,Weicheng Liu,Hui-Ming Wang*

Main category: eess.SP

TL;DR: 提出了一种利用分布式可重构智能超表面天线（RIMSA）和深度强化学习（DRL）优化无线传感的方法，以应对恶劣传播环境和恶意干扰。


<details>
  <summary>Details</summary>
Motivation: 传统射频（RF）传感方法在恶劣传播通道（如衰落和噪声）下精度受限，且易受恶意干扰影响。

Method: 部署多个RIMSA接收器，通过编程其波束成形模式增强信号质量，并采用DRL算法优化波束成形和信号映射。

Result: 仿真结果显示，分布式RIMSA系统比集中式实现更高效，且在干扰环境下仍能保持高精度传感。

Conclusion: 该方法显著提升了无线传感的鲁棒性和准确性，适用于复杂环境。

Abstract: The utilization of radio frequency (RF) signals for wireless sensing has
garnered increasing attention. However, the radio environment is unpredictable
and often unfavorable, the sensing accuracy of traditional RF sensing methods
is often affected by adverse propagation channels from the transmitter to the
receiver, such as fading and noise. In this paper, we propose employing
distributed Reconfigurable Intelligent Metasurface Antennas (RIMSA) to detect
the presence and location of objects where multiple RIMSA receivers (RIMSA Rxs)
are deployed on different places. By programming their beamforming patterns,
RIMSA Rxs can enhance the quality of received signals. The RF sensing problem
is modeled as a joint optimization problem of beamforming pattern and mapping
of received signals to sensing outcomes. To address this challenge, we
introduce a deep reinforcement learning (DRL) algorithm aimed at calculating
the optimal beamforming patterns and a neural network aimed at converting
received signals into sensing outcomes. In addition, the malicious attacker may
potentially launch jamming attack to disrupt sensing process. To enable
effective sensing in interferenceprone environment, we devise a combined loss
function that takes into account the Signal to Interference plus Noise Ratio
(SINR) of the received signals. The simulation results show that the proposed
distributed RIMSA system can achieve more efficient sensing performance and
better overcome environmental influences than centralized implementation.
Furthermore, the introduced method ensures high-accuracy sensing performance
even under jamming attack.

</details>


### [3] [Localized Kernel Methods for Signal Processing](https://arxiv.org/abs/2508.04978)
*Sippanon Kitimoon*

Main category: eess.SP

TL;DR: 本文提出了两种基于局部核的信号处理方法，用于噪声条件下的参数恢复。第一种方法针对多维指数模型的频率和幅度估计，优于经典算法MUSIC和ESPRIT；第二种方法分离线性啁啾信号，无需先验信息且适应低信噪比。


<details>
  <summary>Details</summary>
Motivation: 解决噪声环境下信号参数恢复的挑战，特别是在低信噪比和多维情况下。

Method: 1. 使用局部三角多项式核检测多维频率，结合坐标投影和配准方法；2. 基于局部核的SSO变体，通过FFT滤波和分段线性回归分离啁啾信号。

Result: 在低信噪比下优于经典算法，且多维情况下样本需求更少；啁啾信号分离方法适应-30 dB的低信噪比。

Conclusion: 基于局部核的方法在噪声环境下表现出鲁棒性和高效性，未来可扩展至非线性啁啾和自适应核设计。

Abstract: This dissertation presents two signal processing methods using specially
designed localized kernels for parameter recovery under noisy condition. The
first method addresses the estimation of frequencies and amplitudes in
multidimensional exponential models. It utilizes localized trigonometric
polynomial kernels to detect the multivariate frequencies, followed by a more
detailed parameter estimation. We compare our method with MUSIC and ESPRIT,
which are classical subspace-based algorithms widely used for estimating the
parameters of exponential signals. In the univariate case, the method
outperforms MUSIC and ESPRIT under low signal-to-noise ratios. For the
multivariate case, we develop a coordinate-wise projection and registration
approach that achieves high recovery accuracy using significantly fewer samples
than other methods.
  The second method focuses on separating linear chirp components from
time-localized signal segments. A variant of the Signal Separation Operator
(SSO) is constructed using a localized kernel. Instantaneous frequency
estimates are obtained via FFT-based filtering, then clustered and fitted with
piecewise linear regression. The method operates without prior knowledge of the
number of components and is shown to recover intersecting and discontinuous
chirps at SNR levels as low as -30 dB.
  Both methods share an idea based on localized kernels and efficient FFT-based
implementation, and neither requires subspace decomposition or sparsity
regularization. Experimental results confirm the robustness and tractability of
the proposed approaches across a range of simulated data conditions. Potential
extensions include application to nonlinear chirps, adaptive kernel design, and
signal classification using extracted features.

</details>


### [4] [Power-Constrained and Quantized MIMO-RSMA Systems with Imperfect CSIT: Joint Precoding, Antenna Selection, and Power Control](https://arxiv.org/abs/2508.05080)
*Jiwon Sung,Seokjun Park,Jinseok Choi*

Main category: eess.SP

TL;DR: 论文提出了一种联合预编码、天线选择和发射功率控制算法，以充分利用基站的总功率预算，优化下行多用户MIMO RSMA系统的频谱效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用基站的功率预算，提升多用户MIMO RSMA系统的频谱效率，尤其是在不完美信道状态信息（CSIT）和任意分辨率DAC的情况下。

Method: 通过条件平均速率方法定义遍历和频谱效率，将问题分解为预编码方向和功率控制子问题，分别用拉格朗日驻点和梯度下降求解，并提出了适用于大规模MIMO的复杂度降低方法。

Result: 仿真结果表明，算法有效且在中分辨率DAC（8-11位）下比低分辨率DAC更节能。

Conclusion: 提出的算法能有效优化系统性能，中分辨率DAC在功率效率上更具优势。

Abstract: To utilize the full potential of the available power at a base station (BS),
we propose a joint precoding, antenna selection, and transmit power control
algorithm for a total power budget at the BS. We formulate a sum spectral
efficiency (SE) maximization problem for downlink multi-user multiple-input
multiple-output (MIMO) rate-splitting multiple access (RSMA) systems with
arbitrary-resolution digital-to-analog converters (DACs). We reformulate the
problem by defining the ergodic sum SE using the conditional average rate
approach to handle imperfect channel state information at the transmitter
(CSIT), and by using approximation techniques to make the problem more
tractable. Then, we decompose the problem into precoding direction and power
control subproblems. We solve the precoding direction subproblem by identifying
a superior Lagrangian stationary point, and the power control subproblem using
gradient descent. We also propose a complexity-reduction approach that is more
suitable for massive MIMO systems. Simulation results not only validate the
proposed algorithm but also reveal that when utilizing the full potential of
the power budget at the BS, medium-resolution DACs with 8-11 bits may actually
be more power-efficient than low-resolution DACs.

</details>


### [5] [Digital Twin Channel-Aided CSI Prediction: A Environment-based Subspace Extraction Approach for Achieving Low Overhead and Robustness](https://arxiv.org/abs/2508.05142)
*Yichen Cai,Jianhua Zhang,Li Yu,Zhen Zhang,Yuxiang Zhang,Lianzheng Shi,Yuelong Qiu*

Main category: eess.SP

TL;DR: 提出了一种基于环境特定信道子空间基（EB）的部分到全信道状态信息（CSI）预测方法（EB-P2WCP），用于实现低开销的信道预测。该方法利用EB表征电磁环境的静态特性，并结合实时CSI预测整个信道，显著降低了开销。


<details>
  <summary>Details</summary>
Motivation: 为满足6G移动通信系统在复杂场景下的高鲁棒性和高速通信需求，减少系统开销。

Method: 利用EB表征环境静态特性，结合实时CSI预测整个信道，设计了EB-P2WNet网络。

Result: 在低信噪比和导频比条件下显著降低开销（最高50%），对多用户干扰鲁棒，预测速度快（1.3毫秒）。

Conclusion: EB-P2WCP方法在复杂场景下实现了高效、鲁棒的低开销信道预测。

Abstract: To meet the robust and high-speed communication requirements of the
sixth-generation (6G) mobile communication system in complex scenarios,
sensing- and artificial intelligence (AI)-based digital twin channel (DTC)
techniques become a promising approach to reduce system overhead. In this
paper, we propose an environment-specific channel subspace basis (EB)-aided
partial-to-whole channel state information (CSI) prediction method (EB-P2WCP)
for realizing DTC-enabled low-overhead channel prediction. Specifically, EB is
utilized to characterize the static properties of the electromagnetic
environment, which is extracted from the digital twin map, serving as
environmental information prior to the prediction task. Then, we fuse EB with
real-time estimated local CSI to predict the entire spatial-frequency domain
channel for both the present and future time instances. Hence, an EB-based
partial-to-whole CSI prediction network (EB-P2WNet) is designed to achieve a
robust channel prediction scheme in various complex scenarios. Simulation
results indicate that incorporating EB provides significant benefits under low
signal-to-noise ratio and pilot ratio conditions, achieving a reduction of up
to 50% in pilot overhead. Additionally, the proposed method maintains
robustness against multi-user interference, tolerating 3-meter localization
errors with only a 0.5 dB NMSE increase, and predicts CSI for the next channel
coherent time within 1.3 milliseconds.

</details>


### [6] [Optimization of Liquid Lens-based Imaging Receiver for MIMO VLC Systems](https://arxiv.org/abs/2508.05204)
*Kapila W. S. Palitharathna,Christodoulos Skouroumounis,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 提出了一种基于液体透镜的成像接收器，用于MIMO可见光通信系统，通过动态调整透镜参数降低信道增益的空间相关性，显著提升误码率性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态透镜在动态条件下（如用户移动或接收器随机朝向）适应性不足，液体透镜的动态调整能力可解决这一问题。

Method: 开发了数学模型描述信道增益，提出两种透镜调整方案（CLS和VULO），并通过优化问题最小化误码率。

Result: 在接收器随机朝向方差为10°时，误码率从4×10−2显著降低至5×10−4。

Conclusion: 液体透镜显著优于静态透镜，尤其在动态条件下表现突出。

Abstract: In this paper, a liquid lens-based imaging receiver is proposed for
multiple-input multiple-output (MIMO) visible light communication (VLC)
systems. By dynamically adjusting the focal length and orientation angles of
the liquid lens, the spatial correlation between MIMO channel gains is reduced,
leading to enhanced bit-error rate (BER) performance. Unlike static lenses,
liquid lenses offer adaptability in dynamic conditions, including user mobility
and random receiver orientation. An accurate mathematical framework is
developed to model the channel gains of the proposed system, and an
optimization problem is formulated to minimize its BER. Due to the complexity
of the resulting channel model, two lens adjustment schemes, namely, ($i$) the
CLS scheme, and ($ii$) the VULO scheme are introduced. Numerical results
demonstrate that the proposed liquid lens-based system offers substantial BER
improvements over conventional static lens-based receivers across a wide range
of random receiver orientation conditions. Specifically, at a random receiver
orientation variance of $10^{\circ}$, the BER is improved from $4\times
10^{-2}$ to $5\times 10^{-4}$ by employing the proposed liquid lens.

</details>


### [7] [Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios](https://arxiv.org/abs/2508.05226)
*Junzhe Song,Ruisi He,Mi Yang,Zhengyu Zhang,Bingcheng Liu,Jiahui Han,Haoxiang Zhang,Bo Ai*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的ISAC框架，用于车辆环境重建，解决了现有方法在动态场景跟踪精度和时间一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC方法在动态场景重建中精度和时间一致性不足，限制了实际应用。

Method: 通过多模态测量建立联合信道环境数据集，开发多阶段深度学习网络（场景解码器、聚类中心解码器和点云解码器）进行环境重建。

Result: 实验结果显示，该方法在Chamfer Distance和F Score@1%上表现优异（0.29和0.87），且复杂度分析表明其实时适用性。

Conclusion: 该研究为未来智能交通提供了一种低成本的ISAC环境重建方案。

Abstract: Integrated Sensing and Communication (ISAC) technology plays a critical role
in future intelligent transportation systems, by enabling vehicles to perceive
and reconstruct the surrounding environment through reuse of wireless signals,
thereby reducing or even eliminating the need for additional sensors such as
LiDAR or radar. However, existing ISAC based reconstruction methods often lack
the ability to track dynamic scenes with sufficient accuracy and temporal
consistency, limiting the real world applicability. To address this limitation,
we propose a deep learning based framework for vehicular environment
reconstruction by using ISAC channels. We first establish a joint channel
environment dataset based on multi modal measurements from real world urban
street scenarios. Then, a multistage deep learning network is developed to
reconstruct the environment. Specifically, a scene decoder identifies the
environmental context such as buildings, trees and so on; a cluster center
decoder predicts coarse spatial layouts by localizing dominant scattering
centers; a point cloud decoder recovers fine grained geometry and structure of
surrounding environments. Experimental results demonstrate that the proposed
method achieves high-quality dynamic environment reconstruction with a Chamfer
Distance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis
demonstrates the efficiency and practical applicability of the method in real
time scenarios. This work provides a pathway toward low cost environment
reconstruction based on ISAC for future intelligent transportation.

</details>


### [8] [Unifying Common Signal Analyses with Instantaneous Time-Frequency Atoms](https://arxiv.org/abs/2508.05380)
*Steven Sandoval,Phillip L. De Leon*

Main category: eess.SP

TL;DR: 本文提出了一种基于瞬时时间-频率原子的方法，用于计算与多种信号分析相关的瞬时频谱（IS），并通过封闭形式表达式统一这些分析。


<details>
  <summary>Details</summary>
Motivation: 先前的研究提出了瞬时时间-频率分析的通用框架，但未提供具体计算瞬时频谱的方法。本文旨在填补这一空白。

Method: 使用瞬时时间-频率原子，将时间域分析、频率域分析、分数傅里叶变换等信号分析方法统一为AM-FM分量分解，并基于二次chirplet模板开发封闭形式的IS表达式。

Result: 通过二维连续体组织这些IS，展示了不同信号分析方法的关联性，并通过示例信号验证了封闭形式IS的计算。

Conclusion: 本文成功地将多种信号分析方法统一为一个框架，并提供了具体的IS计算方法，为信号处理领域提供了新的工具。

Abstract: In previous work, we presented a general framework for instantaneous
time-frequency analysis but did not provide any specific details of how to
compute a particular instantaneous spectrum (IS). In this work, we use
instantaneous time-frequency atoms to obtain an IS associated with common
signal analyses: time domain analysis, frequency domain analysis, fractional
Fourier transform, synchrosqueezed short-time Fourier transform, and
synchrosqueezed short-time fractional Fourier transform. By doing so, we
demonstrate how the general framework can be used to unify these analyses and
we develop closed-form expressions for the corresponding ISs. This is
accomplished by viewing these analyses as decompositions into AM--FM components
and recognizing that each uses a specialized (or limiting) form of a quadratic
chirplet as a template during analysis. With a two-parameter quadratic
chirplet, we can organize these ISs into a 2D continuum with points in the
plane corresponding to a decomposition related to one of the signal analyses.
Finally, using several example signals, we compute in closed-form the ISs for
the various analyses.

</details>


### [9] [Sub- μ W Battery-Less and Oscillator-Less Wi-Fi Backscattering Transmitter Reusing RF Signal for Harvesting, Communications, and Motion Detection](https://arxiv.org/abs/2508.05479)
*Marco Privitera,Andrea Ballo,Karim Ali Ahmed,Alfio Dario Grasso,Massimo Alioto*

Main category: eess.SP

TL;DR: 提出了一种超低功耗的802.11b反向散射发射器，实现了射频能量收集、反向散射通信和位置/运动传感的多功能集成。


<details>
  <summary>Details</summary>
Motivation: 通过消除电池和外部运动传感器（如MEMS），实现设备微型化、无限寿命和低成本。

Method: 利用双音入射波的二阶互调提取频率，消除本地振荡器，实现超低功耗。

Result: 首次突破WiFi发射器的微瓦功耗限制，最低灵敏度达-19 dBm。

Conclusion: 该技术为室内环境中的标签提供了多功能、低成本且持久的解决方案。

Abstract: In this paper, a sub-uW power 802.11b backscattering transmitter is presented
to enable reuse of the same incident wave for three purposes: RF harvesting,
backscattering communications and position/motion sensing. The removal of the
battery and any off-chip motion sensor (e.g., MEMS) enables unprecedented level
of miniaturization and ubiquity, unrestricted device lifespan, low fabrication
and maintenance cost. The uW power wall for WiFi transmitters is broken for the
first time via local oscillator elimination, as achieved by extracting its
frequency through second-order intermodulation of a twotone incident wave. The
two-tone scheme also enables a cumulative harvesting/transmission/sensing
sensitivity down to Pmin -19 dBm. Position/motion sensing is enabled by using
the harvested voltage as a proxy for the Received Signal Strength (RSS),
allowing to sense the chip location with respect to the tone generator(s)
shared across tags in indoor neighborhoods.

</details>


### [10] [0.6-V, uW-Power 4-Stage OTA with Minimal Components and 100X Load Range](https://arxiv.org/abs/2508.05499)
*M. Privitera,A. D. Grasso,A. Ballo,M. Alioto*

Main category: eess.SP

TL;DR: 提出了一种用于超低功耗应用的四级运算跨导放大器（OTA），通过简化频率补偿设计，显著提升了功率效率，并在宽负载范围内保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统四级OTA的补偿设计复杂且功率效率低，需简化设计并提升性能。

Method: 采用四阶段OTA设计，优化频率补偿，减少晶体管数量和无源元件。

Result: 功率效率显著提升（大信号和小信号分别提高3.7倍和11.3倍），负载电容范围宽（最大/最小比>100倍）。

Conclusion: 该设计简化了四级OTA的补偿问题，同时实现了高功率效率和稳定性。

Abstract: A four-stage operational transconductance amplifier (OTA) for ultra-low-power
applications is introduced in this paper. The proposed circuit inclusive of
frequency compensation requires minimal transistor count and passives,
overcoming the traditionally difficult compensation of 4-stage OTAs and
bringing it back to the simplicity of 3-stage OTAs. At the same time, the
proposed circuit achieves high power efficiency, as evidenced by the >3.7X
(>11.3X) improvement in the large-signal (small-signal) power efficiency figure
of merit FOML (FOMS), compared to prior 4-stage OTAs (sub-1 V multi-stage
OTAs). Thanks to the lower sensitivity of the phase margin to the load
capacitance, the proposed OTA remains stable under a wide range of loads
(double-sided as in any 3-4-stage OTA), achieving a max/min ratio of the load
capacitance of >100X.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Keyword Spotting with Hyper-Matched Filters for Small Footprint Devices](https://arxiv.org/abs/2508.04857)
*Yael Segal-Feldman,Ann R. Bradlow,Matthew Goldrick,Joseph Keshet*

Main category: eess.AS

TL;DR: 本文提出了一种用于小型设备的开放词汇关键词检测模型，结合语音编码器、目标关键词编码器和检测网络，实现了最先进的检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇关键词检测任务，尤其是在训练数据中未包含的词汇，同时适用于小型设备。

Method: 模型由语音编码器（tiny Whisper或Conformer）、目标关键词编码器（超网络生成关键词特定权重）和检测网络（Perceiver模块）组成。

Result: 系统在检测性能和泛化能力上达到最先进水平，最小模型（420万参数）性能优于更大模型。

Conclusion: 该模型高效且鲁棒，适用于开放词汇检测，尤其在小型设备上表现优异。

Abstract: Open-vocabulary keyword spotting (KWS) refers to the task of detecting words
or terms within speech recordings, regardless of whether they were included in
the training data. This paper introduces an open-vocabulary keyword spotting
model with state-of-the-art detection accuracy for small-footprint devices. The
model is composed of a speech encoder, a target keyword encoder, and a
detection network. The speech encoder is either a tiny Whisper or a tiny
Conformer. The target keyword encoder is implemented as a hyper-network that
takes the desired keyword as a character string and generates a unique set of
weights for a convolutional layer, which can be considered as a
keyword-specific matched filter. The detection network uses the matched-filter
weights to perform a keyword-specific convolution, which guides the
cross-attention mechanism of a Perceiver module in determining whether the
target term appears in the recording. The results indicate that our system
achieves state-of-the-art detection performance and generalizes effectively to
out-of-domain conditions, including second-language (L2) speech. Notably, our
smallest model, with just 4.2 million parameters, matches or outperforms models
that are several times larger, demonstrating both efficiency and robustness.

</details>


### [12] [Closed-Form Successive Relative Transfer Function Vector Estimation based on Blind Oblique Projection Incorporating Noise Whitening](https://arxiv.org/abs/2508.04887)
*Henri Gode,Simon Doclo*

Main category: eess.AS

TL;DR: 本文提出三种改进方法，以解决盲斜投影（BOP）在估计多声源相对传递函数（RTF）时的高计算复杂度、随机向量引入和低信噪比（SNR）问题。


<details>
  <summary>Details</summary>
Motivation: 在噪声和混响环境中，多声源的RTF在线估计是一个挑战，尤其是当声源相继激活时。现有BOP方法存在计算复杂、随机向量影响性能和低SNR假设的局限。

Method: 1. 推导BOP成本函数的闭式解以减少计算复杂度；2. 使用正交附加向量替代随机向量；3. 引入噪声处理技术（协方差减法和白化）以提高低SNR下的鲁棒性。此外，提出基于空间一致性的在线声源计数方法。

Result: 仿真实验表明，改进方法在真实混响噪声环境中对3个相继激活的声源RTF估计更准确，且无需先验声源活动信息。

Conclusion: 提出的扩展方法显著提升了BOP的性能，适用于复杂声学环境中的多声源RTF在线估计。

Abstract: Relative transfer functions (RTFs) of sound sources play a crucial role in
beamforming, enabling effective noise and interference suppression. This paper
addresses the challenge of online estimating the RTF vectors of multiple sound
sources in noisy and reverberant environments, for the specific scenario where
sources activate successively. While the RTF vector of the first source can be
estimated straightforwardly, the main challenge arises in estimating the RTF
vectors of subsequent sources during segments where multiple sources are
simultaneously active. The blind oblique projection (BOP) method has been
proposed to estimate the RTF vector of a newly activating source by optimally
blocking this source. However, this method faces several limitations: high
computational complexity due to its reliance on iterative gradient descent
optimization, the introduction of random additional vectors, which can
negatively impact performance, and the assumption of high signal-to-noise ratio
(SNR). To overcome these limitations, in this paper we propose three extensions
to the BOP method. First, we derive a closed-form solution for optimizing the
BOP cost function, significantly reducing computational complexity. Second, we
introduce orthogonal additional vectors instead of random vectors, enhancing
RTF vector estimation accuracy. Third, we incorporate noise handling techniques
inspired by covariance subtraction and whitening, increasing robustness in low
SNR conditions. To provide a frame-by-frame estimate of the source activity
pattern, required by both the conventional BOP method and the proposed method,
we propose a spatial-coherence-based online source counting method. Simulations
are performed with real-world reverberant noisy recordings featuring 3
successively activating speakers, with and without a-priori knowledge of the
source activity pattern.

</details>


### [13] [REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with Diffusion Transformers](https://arxiv.org/abs/2508.04996)
*Yuepeng Jiang,Ziqian Ning,Shuai Wang,Chengjia Wang,Mengxiao Bi,Pengcheng Zhu,Lei Xie,Zhonghua Fu*

Main category: eess.AS

TL;DR: REF-VC是一种抗噪声且富有表现力的语音转换系统，通过随机擦除策略和隐式对齐提升性能，并在实验中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现实语音转换应用中，环境噪声和用户对富有表现力输出的需求是主要挑战，传统方法无法同时满足抗噪性和表现力。

Method: 提出REF-VC系统，采用随机擦除策略减少SSL特征冗余，隐式对齐抑制非必要特征重建，并集成Shortcut Models加速推理。

Result: 在零噪声场景下优于Seed-VC等基线模型，同时在干净数据集上表现相当，且兼容歌唱语音转换。

Conclusion: REF-VC在抗噪声和表现力方面均表现出色，具有实际应用潜力。

Abstract: In real-world voice conversion applications, environmental noise in source
speech and user demands for expressive output pose critical challenges.
Traditional ASR-based methods ensure noise robustness but suppress prosody,
while SSL-based models improve expressiveness but suffer from timbre leakage
and noise sensitivity. This paper proposes REF-VC, a noise-robust expressive
voice conversion system. Key innovations include: (1) A random erasing strategy
to mitigate the information redundancy inherent in SSL feature, enhancing noise
robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to
suppress non-essential feature reconstruction; (3) Integration of Shortcut
Models to accelerate flow matching inference, significantly reducing to 4
steps. Experimental results demonstrate that our model outperforms baselines
such as Seed-VC in zero-shot scenarios on the noisy set, while also performing
comparably to Seed-VC on the clean set. In addition, REF-VC can be compatible
with singing voice conversion within one model.

</details>


### [14] [MOVER: Combining Multiple Meeting Recognition Systems](https://arxiv.org/abs/2508.05055)
*Naoyuki Kamo,Tsubasa Ochiai,Marc Delcroix,Tomohiro Nakatani*

Main category: eess.AS

TL;DR: MOVER是一种新的会议识别系统组合方法，首次结合了不同说话人分割和语音识别输出的系统，通过五阶段流程显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能单独结合说话人分割或语音识别输出，而MOVER填补了同时结合两者的空白。

Method: MOVER通过五阶段流程（包括说话人对齐、分段分组、词和时间组合等）结合不同假设。

Result: 在CHiME-8 DASR和NOTSOFAR-1任务中，MOVER相对现有最佳系统的tcpWER分别提升了9.55%和8.51%。

Conclusion: MOVER成功结合了多样化的会议识别系统，显著提升了性能。

Abstract: In this paper, we propose Meeting recognizer Output Voting Error Reduction
(MOVER), a novel system combination method for meeting recognition tasks.
Although there are methods to combine the output of diarization (e.g., DOVER)
or automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first
approach that can combine the outputs of meeting recognition systems that
differ in terms of both diarization and ASR. MOVER combines hypotheses with
different time intervals and speaker labels through a five-stage process that
includes speaker alignment, segment grouping, word and timing combination, etc.
Experimental results on the CHiME-8 DASR task and the multi-channel track of
the NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple
meeting recognition systems with diverse diarization and recognition outputs,
achieving relative tcpWER improvements of 9.55 % and 8.51 % over the
state-of-the-art systems for both tasks.

</details>


### [15] [Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS](https://arxiv.org/abs/2508.05102)
*Anuprabha M,Krishna Gurugubelli,Anil Kumar Vuppala*

Main category: eess.AS

TL;DR: 研究探讨了F5-TTS在克隆构音障碍语音时的效果，发现其对语音清晰度的偏好强于说话者相似性和韵律保留，并分析了公平性偏差。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音数据有限，现有语音合成技术可能引入偏差，需研究其效果及公平性。

Method: 使用F5-TTS和TORGO数据集，评估清晰度、说话者相似性和韵律保留，并采用公平性指标分析偏差。

Result: F5-TTS在构音障碍语音合成中更偏向语音清晰度，而非说话者相似性和韵律保留。

Conclusion: 研究为开发更公平、包容的语音技术提供了见解。

Abstract: Dysarthric speech poses significant challenges in developing assistive
technologies, primarily due to the limited availability of data. Recent
advances in neural speech synthesis, especially zero-shot voice cloning,
facilitate synthetic speech generation for data augmentation; however, they may
introduce biases towards dysarthric speech. In this paper, we investigate the
effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using
TORGO dataset, focusing on intelligibility, speaker similarity, and prosody
preservation. We also analyze potential biases using fairness metrics like
Disparate Impact and Parity Difference to assess disparities across dysarthric
severity levels. Results show that F5-TTS exhibits a strong bias toward speech
intelligibility over speaker and prosody preservation in dysarthric speech
synthesis. Insights from this study can help integrate fairness-aware
dysarthric speech synthesis, fostering the advancement of more inclusive speech
technologies.

</details>


### [16] [Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages](https://arxiv.org/abs/2508.05149)
*Seraphina Fong,Marco Matassoni,Alessio Brutti*

Main category: eess.AS

TL;DR: 研究探讨了语音大语言模型（Speech LLMs）在低资源自动语音识别（ASR）中的应用，通过SLAM-ASR框架连接语音编码器与大语言模型，分析了数据量和多语言预训练对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言中表现优异，但在低资源语言中的适用性尚未充分探索。本研究旨在填补这一空白。

Method: 使用SLAM-ASR框架，通过轻量级投影器连接语音编码器和大语言模型，评估数据量需求及多语言预训练的效果。

Result: 研究发现，多语言预训练能缓解数据稀缺问题，尤其在小型训练集上表现显著。

Conclusion: 研究为优化低资源语言和多语言语音大语言模型提供了重要见解。

Abstract: Large language models (LLMs) have demonstrated potential in handling spoken
inputs for high-resource languages, reaching state-of-the-art performance in
various tasks. However, their applicability is still less explored in
low-resource settings. This work investigates the use of Speech LLMs for
low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a
trainable lightweight projector connects a speech encoder and a LLM. Firstly,
we assess training data volume requirements to match Whisper-only performance,
re-emphasizing the challenges of limited data. Secondly, we show that
leveraging mono- or multilingual projectors pretrained on high-resource
languages reduces the impact of data scarcity, especially with small training
sets. Using multilingual LLMs (EuroLLM, Salamandra) with
whisper-large-v3-turbo, we evaluate performance on several public benchmarks,
providing insights for future research on optimizing Speech LLMs for
low-resource languages and multilinguality.

</details>


### [17] [Privacy Disclosure of Similarity in Speech and Language Processing](https://arxiv.org/abs/2508.05250)
*Tom Bäckström,Mohammad Hassan Vali,My Nguyen,Silas Rech*

Main category: eess.AS

TL;DR: 论文提出了一种量化相似性排名隐私泄露的方法，通过估计其概率分布来评估个人身份信息的泄露程度。


<details>
  <summary>Details</summary>
Motivation: 由于数据可能存在噪声且相似性度量不准确，传统的身份识别方法可能不可靠，但相似性排名仍可能泄露隐私信息。

Method: 基于真实说话者相似性排名的直方图或Beta-二项分布建模，以熵（比特）表示隐私泄露。

Result: 实验表明，所有测试的说话者和作者特征均包含个人身份信息，其中说话者识别算法的嵌入信息最多。

Conclusion: 相似性排名泄露指标可用于比较不同生物特征隐私泄露程度，并辅助隐私威胁的全面评估。

Abstract: Speaker, author, and other biometric identification applications often
compare a sample's similarity to a database of templates to determine the
identity. Given that data may be noisy and similarity measures can be
inaccurate, such a comparison may not reliably identify the true identity as
the most similar. Still, even the similarity rank based on an inaccurate
similarity measure can disclose private information about the true identity. We
propose a methodology for quantifying the privacy disclosure of such a
similarity rank by estimating its probability distribution. It is based on
determining the histogram of the similarity rank of the true speaker, or when
data is scarce, modeling the histogram with the beta-binomial distribution. We
express the disclosure in terms of entropy (bits), such that the disclosure
from independent features are additive. Our experiments demonstrate that all
tested speaker and author characterizations contain personally identifying
information (PII) that can aid in identification, with embeddings from speaker
recognition algorithms containing the most information, followed by phone
embeddings, linguistic embeddings, and fundamental frequency. Our initial
experiments show that the disclosure of PII increases with the length of test
samples, but it is bounded by the length of database templates. The provided
metric, similarity rank disclosure, provides a way to compare the disclosure of
PII between biometric features and merge them to aid identification. It can
thus aid in the holistic evaluation of threats to privacy in speech and other
biometric technologies.

</details>


### [18] [Investigation of Speech and Noise Latent Representations in Single-channel VAE-based Speech Enhancement](https://arxiv.org/abs/2508.05293)
*Jiatong Li,Simon Doclo*

Main category: eess.AS

TL;DR: 研究探讨了变分自编码器（VAE）中潜在表示对语音增强性能的影响，发现清晰的语音和噪声表示分离能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索不同潜在表示对基于VAE的语音增强系统性能的影响，特别是语音和噪声表示的分离程度。

Method: 使用预训练的VAE获取语音和噪声的潜在表示，并通过修改损失项研究其对增强性能的影响。

Result: 实验表明，语音和噪声表示清晰分离的潜在空间比重叠表示显著提升了性能。

Conclusion: 清晰的语音和噪声潜在表示分离对基于VAE的语音增强系统至关重要。

Abstract: Recently, a variational autoencoder (VAE)-based single-channel speech
enhancement system using Bayesian permutation training has been proposed, which
uses two pretrained VAEs to obtain latent representations for speech and noise.
Based on these pretrained VAEs, a noisy VAE learns to generate speech and noise
latent representations from noisy speech for speech enhancement. Modifying the
pretrained VAE loss terms affects the pretrained speech and noise latent
representations. In this paper, we investigate how these different
representations affect speech enhancement performance. Experiments on the DNS3,
WSJ0-QUT, and VoiceBank-DEMAND datasets show that a latent space where speech
and noise representations are clearly separated significantly improves
performance over standard VAEs, which produce overlapping speech and noise
representations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [19] [Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS](https://arxiv.org/abs/2508.04721)
*Vignesh Ethiraj,Ashwath David,Sidhanth Menon,Divya Vijay*

Main category: cs.SD

TL;DR: 论文介绍了一种低延迟的电信AI语音代理管道，结合了四个专用模型，用于实时交互式电信应用，如呼叫中心自动化和智能客户支持。


<details>
  <summary>Details</summary>
Motivation: 为电信行业提供高效、低延迟的语音AI解决方案，以支持实时交互和自动化客户服务。

Method: 通过整合四个专用模型（TSLAM、T-VEC、TTE、T-Synth）构建管道，实现流式语音识别、对话智能、检索增强生成和实时语音合成。

Result: 实验表明，模型在延迟、领域相关性和实时性能上表现优异，RTF低于1.0，适用于企业级部署。

Conclusion: 该管道为下一代电信AI奠定了基础，支持自动化客户服务和诊断等应用。

Abstract: We introduce a low-latency telecom AI voice agent pipeline for real-time,
interactive telecommunications use, enabling advanced voice AI for call center
automation, intelligent IVR (Interactive Voice Response), and AI-driven
customer support. The solution is built for telecom, combining four specialized
models by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language
Model (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific
Automatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific
Text-to-Speech (TTS) model. These models enable highly responsive,
domain-adapted voice AI agents supporting knowledge-grounded spoken
interactions with low latency. The pipeline integrates streaming ASR (TTE),
conversational intelligence (TSLAM), retrieval augmented generation (RAG) over
telecom documents, and real-time TTS (T-Synth), setting a new benchmark for
telecom voice assistants. To evaluate the system, we built a dataset of 500
human-recorded telecom questions from RFCs, simulating real telecom agent
queries. This framework allows analysis of latency, domain relevance, and
real-time performance across the stack. Results show that TSLAM, TTE, and
T-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,
low-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and
T-Synth -- provide a foundation for next-generation telecom AI, enabling
automated customer support, diagnostics, and more.

</details>


### [20] [Wearable Music2Emotion : Assessing Emotions Induced by AI-Generated Music through Portable EEG-fNIRS Fusion](https://arxiv.org/abs/2508.04723)
*Sha Zhao,Song Yi,Yangxuan Zhou,Jiadong Pan,Jiquan Wang,Jie Xia,Shijian Li,Shurong Dong,Gang Pan*

Main category: cs.SD

TL;DR: MEEtBrain是一个便携式多模态框架，通过AI生成音乐刺激和无线头带同步采集EEG-fNIRS信号，解决了现有音乐情感计算中的刺激限制、模态单一性和便携性问题。


<details>
  <summary>Details</summary>
Motivation: 音乐情感计算在心理健康领域具有重要意义，但现有研究存在刺激限制、模态单一性和便携性问题，亟需解决。

Method: 提出MEEtBrain框架，结合AI生成音乐和便携式无线头带设备，同步采集EEG-fNIRS信号，用于情感分析（效价/唤醒）。

Result: 初步实验收集了20名参与者14小时的数据，验证了框架的有效性，并扩展至44名参与者，数据集已公开。

Conclusion: MEEtBrain通过多模态和便携式设计，为音乐情感计算提供了更高效、多样化的解决方案。

Abstract: Emotions critically influence mental health, driving interest in music-based
affective computing via neurophysiological signals with Brain-computer
Interface techniques. While prior studies leverage music's accessibility for
emotion induction, three key limitations persist: \textbf{(1) Stimulus
Constraints}: Music stimuli are confined to small corpora due to copyright and
curation costs, with selection biases from heuristic emotion-music mappings
that ignore individual affective profiles. \textbf{(2) Modality Specificity}:
Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights
from cross-modal signal fusion.\textbf{ (3) Portability Limitation}: Cumbersome
setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability
due to procedural complexity and portability barriers. To address these
limitations, we propose MEEtBrain, a portable and multimodal framework for
emotion analysis (valence/arousal), integrating AI-generated music stimuli with
synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the
music stimuli can be automatically generated by AI on a large scale,
eliminating subjective selection biases while ensuring music diversity. We use
our developed portable device that is designed in a lightweight headband-style
and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A
14-hour dataset from 20 participants was collected in the first recruitment to
validate the framework's efficacy, with AI-generated music eliciting target
emotions (valence/arousal). We are actively expanding our multimodal dataset
(44 participants in the latest dataset) and make it publicly available to
promote further research and practical applications. \textbf{The dataset is
available at https://zju-bmi-lab.github.io/ZBra.

</details>


### [21] [Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation](https://arxiv.org/abs/2508.05011)
*Huaicheng Zhang,Wei Tan,Guangzheng Li,Yixuan Zhang,Hangting Chen,Shun Lei,Chenyu Yang,Zhiyong Wu,Shuai Wang,Qijun Huang,Dong Yu*

Main category: cs.SD

TL;DR: 论文提出了一种基于强化学习的框架，通过偏好优化控制歌词到歌曲生成中的幻觉问题，显著减少了音素错误率。


<details>
  <summary>Details</summary>
Motivation: 当前音频生成语言模型在歌词到歌曲生成中存在内容幻觉问题，导致输出与输入歌词不一致，影响音乐连贯性。现有监督微调方法改进有限。

Method: 开发了一个基于音素错误率和规则过滤的偏好数据集，并在强化学习框架中评估了三种优化策略：DPO、PPO和GRPO。

Result: DPO显著减少7.4%的音素错误率，PPO和GRPO分别减少4.9%和4.7%。主观和客观评估均证实了方法的有效性。

Conclusion: 该框架为歌词到歌曲生成中的幻觉控制提供了系统性解决方案，并具有潜在的音乐风格和音乐性增强能力。

Abstract: Recent advances in audio-based generative language models have accelerated
AI-driven lyric-to-song generation. However, these models frequently suffer
from content hallucination, producing outputs misaligned with the input lyrics
and undermining musical coherence. Current supervised fine-tuning (SFT)
approaches, limited by passive label-fitting, exhibit constrained
self-improvement and poor hallucination mitigation. To address this core
challenge, we propose a novel reinforcement learning (RL) framework leveraging
preference optimization for hallucination control. Our key contributions
include: (1) Developing a robust hallucination preference dataset constructed
via phoneme error rate (PER) computation and rule-based filtering to capture
alignment with human expectations; (2) Implementing and evaluating three
distinct preference optimization strategies within the RL framework: Direct
Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group
Relative Policy Optimization (GRPO). DPO operates off-policy to enhance
positive token likelihood, achieving a significant 7.4% PER reduction. PPO and
GRPO employ an on-policy approach, training a PER-based reward model to
iteratively optimize sequences via reward maximization and KL-regularization,
yielding PER reductions of 4.9% and 4.7%, respectively. Comprehensive objective
and subjective evaluations confirm that our methods effectively suppress
hallucinations while preserving musical quality. Crucially, this work presents
a systematic, RL-based solution to hallucination control in lyric-to-song
generation. The framework's transferability also unlocks potential for music
style adherence and musicality enhancement, opening new avenues for future
generative song research.

</details>


### [22] [SpectroStream: A Versatile Neural Codec for General Audio](https://arxiv.org/abs/2508.05207)
*Yunpeng Li,Kehang Han,Brian McWilliams,Zalan Borsos,Marco Tagliasacchi*

Main category: cs.SD

TL;DR: SpectroStream是一个全频带多通道神经音频编解码器，扩展了SoundStream的能力，支持48 kHz立体声音乐的高质量重建，比特率为4-16 kbps。


<details>
  <summary>Details</summary>
Motivation: 解决现有编解码器在48 kHz立体声音乐高质量重建方面的不足，尤其是在高采样率下的音频质量提升。

Method: 采用新的神经架构，利用时频域音频表示，并使用延迟融合策略处理多通道音频。

Result: 实现了在4-16 kbps比特率下对48 kHz立体声音乐的高质量重建。

Conclusion: SpectroStream在音频质量和多通道处理方面表现优异，适用于高采样率音频编码。

Abstract: We propose SpectroStream, a full-band multi-channel neural audio codec.
Successor to the well-established SoundStream, SpectroStream extends its
capability beyond 24 kHz monophonic audio and enables high-quality
reconstruction of 48 kHz stereo music at bit rates of 4--16 kbps. This is
accomplished with a new neural architecture that leverages audio representation
in the time-frequency domain, which leads to better audio quality especially at
higher sample rate. The model also uses a delayed-fusion strategy to handle
multi-channel audio, which is crucial in balancing per-channel acoustic quality
and cross-channel phase consistency.

</details>


### [23] [Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces](https://arxiv.org/abs/2508.05306)
*Mathias Rose Bjare,Stefan Lattner,Gerhard Widmer*

Main category: cs.SD

TL;DR: 该论文研究了使用自回归扩散模型（ADMs）计算信息内容（IC）来建模音乐期望和惊讶感的效果，发现其优于生成无限词汇变换器（GIVT）。


<details>
  <summary>Details</summary>
Motivation: 探索自回归扩散模型在音乐和音频特征建模中的有效性，尤其是其信息内容（IC）在捕捉惊讶感方面的表现。

Method: 使用两种不同的扩散常微分方程（ODEs）计算IC，并通过负对数似然评估其性能。

Result: 在单音高惊讶感和多轨音频片段边界检测任务中，扩散模型的表现优于或等同于GIVT。

Conclusion: 扩散模型在不同噪声水平下估计的惊讶感对应不同音频粒度特征，适当噪声水平可提升任务表现。

Abstract: Recently, the information content (IC) of predictions from a Generative
Infinite-Vocabulary Transformer (GIVT) has been used to model musical
expectancy and surprisal in audio. We investigate the effectiveness of such
modelling using IC calculated with autoregressive diffusion models (ADMs). We
empirically show that IC estimates of models based on two different diffusion
ordinary differential equations (ODEs) describe diverse data better, in terms
of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's
effectiveness in capturing surprisal aspects by examining two tasks: (1)
capturing monophonic pitch surprisal, and (2) detecting segment boundaries in
multi-track audio. In both tasks, the diffusion models match or exceed the
performance of a GIVT. We hypothesize that the surprisal estimated at different
diffusion process noise levels corresponds to the surprisal of music and audio
features present at different audio granularities. Testing our hypothesis, we
find that, for appropriate noise levels, the studied musical surprisal tasks'
results improve. Code is provided on github.com/SonyCSLParis/audioic.

</details>


### [24] [A Scalable Pipeline for Enabling Non-Verbal Speech Generation and Understanding](https://arxiv.org/abs/2508.05385)
*Runchuan Ye,Yixuan Zhou,Renjie Yu,Zijian Lin,Kehan Li,Xiang Li,Xin Liu,Guoyang Zeng,Zhiyong Wu*

Main category: cs.SD

TL;DR: 论文介绍了NonVerbalSpeech-38K数据集，用于非语言语音生成和理解，包含38,718个样本，10类非语言线索，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有语音系统主要关注语言内容，缺乏对非语言线索（如笑声、叹息等）的理解和生成能力，影响了语音界面的情感智能和沟通丰富性。

Method: 提出了一个自动化的数据收集和标注流程，构建了包含10类非语言线索的大规模数据集NonVerbalSpeech-38K，并通过微调F5-TTS和Qwen2-Audio等模型验证其有效性。

Result: 实验表明，该数据集在非语言语音生成和理解任务中表现良好，提升了语音合成和描述的准确性。

Conclusion: 该研究为非语言语音研究提供了实用工具和数据集，促进了更丰富的人机交互。

Abstract: Human spoken communication involves not only lexical content but also
non-verbal vocalizations (NVs) such as laughter, sighs, and coughs, which
convey emotions, intentions, and social signals. However, most existing speech
systems focus solely on verbal content and lack the ability to understand and
generate such non-verbal cues, reducing the emotional intelligence and
communicative richness of spoken interfaces. In this work, we introduce
$\textbf{NonVerbalSpeech-38K}$, a large and diverse dataset for non-verbal
speech generation and understanding, collected from real-world media and
annotated using an automatic pipeline. The dataset contains 38,718 samples
(about 131 hours) with 10 categories of non-verbal cues, such as laughter,
sniff, and throat clearing. We further validate the dataset by fine-tuning
state-of-the-art models, including F5-TTS and Qwen2-Audio, demonstrating its
effectiveness in non-verbal speech generation and understanding tasks. Our
contributions are threefold: (1) We propose a practical pipeline for building
natural and diverse non-verbal speech datasets; (2) We release a large-scale
dataset to advance research on non-verbal speech generation and understanding;
(3) We validate the dataset's effectiveness by demonstrating improvements in
both non-verbal speech synthesis and captioning, thereby facilitating richer
human-computer interaction.

</details>


### [25] [SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription](https://arxiv.org/abs/2508.05554)
*Raymond Grossman,Taejin Park,Kunal Dhawan,Andrew Titus,Sophia Zhi,Yulia Shchadilova,Weiqing Wang,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.SD

TL;DR: SPGISpeech 2.0是一个金融领域的语音数据集，新增3,780小时专业转录的财报电话会议音频，支持多说话人ASR任务。


<details>
  <summary>Details</summary>
Motivation: 提升语音识别模型的多样性，特别是针对金融领域的多说话人场景。

Method: 扩展原始SPGISpeech数据集，增加音频片段和对应的格式化文本转录，并标注说话人和通话信息。

Result: 验证了SPGISpeech 2.0在说话人标记ASR任务中的有效性，模型性能得到提升。

Conclusion: SPGISpeech 2.0将推动语音识别技术的发展，并激发广泛的研究应用。

Abstract: We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged
transcription in the financial domain. SPGISpeech 2.0 improves the diversity of
applicable modeling tasks while maintaining the core characteristic of the
original SPGISpeech dataset: audio snippets and their corresponding fully
formatted text transcriptions, usable for end-to-end automatic speech
recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of
professionally transcribed earnings calls. Furthermore, the dataset contains
call and speaker information for each audio snippet facilitating multi-talker
ASR. We validate the utility of SPGISpeech 2.0 through improvements in
speaker-tagged ASR performance of popular speech recognition models after
fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect
SPGISpeech 2.0 to foster advancements in speech recognition technologies and
inspire a wide range of research applications.

</details>

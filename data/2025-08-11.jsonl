{"id": "2508.05801", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05801", "abs": "https://arxiv.org/abs/2508.05801", "authors": ["Yingbo Hua"], "title": "A Remark on the AAA Method for Secret-Key Generation in Mobile Networks", "comment": null, "summary": "A broadly applicable method for secret-key generation is named for its\naccumulative, adaptable and additive (AAA) properties. This paper first shows a\nrobustness of its performance. Namely, even if there is an inter correlation or\na leakage caused intra correlation among the superimposed packets, provided\nthere is a nonzero probability for each packet to be missed in full or in part\nby Eve, then the equivocation of the key generated by the AAA method always\nbecomes perfect as the number of superpositions becomes infinite. Also shown in\nthis paper is a comparison between the AAA method and an ideal method based on\nreciprocal channel estimation, which reveals several advantages of the AAA\nmethod."}
{"id": "2508.05882", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.05882", "abs": "https://arxiv.org/abs/2508.05882", "authors": ["Yingbo Hua"], "title": "STEEP -- An Alternative To Quantum Key Distribution", "comment": null, "summary": "Secret-message transmission by echoing encrypted probes (STEEP) is discussed\nas an alternative to quantum key distribution (QKD). The former only needs\nclassic or non-quantum channels while the latter needs both quantum and classic\nchannels for secret-key generation. STEEP is shown to yield a secrecy rate\nsufficient for one-time pads encryption in many practical situations including\nin-air channels or undersea optical cables. Other advantages of STEEP over QKD\ninclude cost, complexity, compatibility, and robustness against constant\neavesdropping."}
{"id": "2508.05959", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.05959", "abs": "https://arxiv.org/abs/2508.05959", "authors": ["Amirhossein Taherpour", "Somayeh Khani", "Abbas Taherpour", "Tamer Khattab"], "title": "IRS-Assisted IoT Activity Detection Under Asynchronous Transmission and Heterogeneous Powers: Detectors and Performance Analysis", "comment": null, "summary": "This paper addresses the problem of activity detection in distributed\nInternet of Things (IoT) networks, where devices employ asynchronous\ntransmissions with heterogeneous power levels to report their local\nobservations. The system leverages an intelligent reflecting surface (IRS) to\nenhance detection reliability, with optional incorporation of a direct\nline-of-sight (LoS) path. We formulate the detection problem as a binary\nhypothesis test and develop four detectors: an optimal detector alongside three\ncomputationally efficient detectors designed for practical scenarios with\ndifferent levels of prior knowledge about noise variance, channel state\ninformation, and device transmit powers. For each detector, we derive\nclosed-form expressions for both detection and false alarm probabilities,\nestablishing theoretical performance benchmarks. Extensive simulations validate\nour analytical results and systematically evaluate the impact of key system\nparameters including the number of antennas, samples, users, and IRS elements\non detection performance. The proposed framework effectively bridges\ntheoretical optimality with implementation practicality, providing a scalable\nsolution for IRS-assisted IoT networks in emerging 6G systems."}
{"id": "2508.06022", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06022", "abs": "https://arxiv.org/abs/2508.06022", "authors": ["Zeping Sui", "Qu Luo", "Zilong Liu", "Murat Temiz", "Leila Musavian", "Christos Masouros", "Yong Liang Guan", "Pei Xiao", "Lajos Hanzo"], "title": "Multi-Functional Chirp Signalling for Next-Generation Multi-Carrier Wireless Networks: Communications, Sensing and ISAC Perspectives", "comment": "8 pages, 5 figures, submitted to IEEE Wireless Communications", "summary": "To meet the increasingly demanding quality-of-service requirements of the\nnext-generation multi-carrier mobile networks, it is essential to design\nmulti-functional signalling schemes facilitating efficient, flexible, and\nreliable communication and sensing in complex wireless environments. As a\ncompelling candidate, we advocate chirp signalling, beneficially amalgamating\nsequences (e.g., Zadoff-Chu sequences) with waveforms (e.g., chirp spread\nspectrum and frequency-modulated continuous wave (FMCW) radar), given their\nresilience against doubly selective channels. Besides chirp sequences, a wide\nrange of chirp waveforms is considered, ranging from FMCW to affine\nfrequency-division multiplexing (AFDM), to create a promising chirp\nmulticarrier waveform. This study also highlights the advantages of such\nwaveforms in supporting reliable high-mobility communications, plus integrated\nsensing and communications (ISAC). Finally, we outline several emerging\nresearch directions for chirp signalling designs."}
{"id": "2508.05878", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05878", "abs": "https://arxiv.org/abs/2508.05878", "authors": ["Martyna Majchrzak", "Jacek Mańdziuk"], "title": "Training chord recognition models on artificially generated audio", "comment": null, "summary": "One of the challenging problems in Music Information Retrieval is the\nacquisition of enough non-copyrighted audio recordings for model training and\nevaluation. This study compares two Transformer-based neural network models for\nchord sequence recognition in audio recordings and examines the effectiveness\nof using an artificially generated dataset for this purpose. The models are\ntrained on various combinations of Artificial Audio Multitracks (AAM),\nSchubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated\nwith three metrics: Root, MajMin and Chord Content Metric (CCM). The\nexperiments prove that even though there are certainly differences in\ncomplexity and structure between artificially generated and human-composed\nmusic, the former can be useful in certain scenarios. Specifically, AAM can\nenrich a smaller training dataset of music composed by a human or can even be\nused as a standalone training set for a model that predicts chord sequences in\npop music, if no other data is available."}
{"id": "2508.05835", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.05835", "abs": "https://arxiv.org/abs/2508.05835", "authors": ["Edresson Casanova", "Paarth Neekhara", "Ryan Langman", "Shehzeen Hussain", "Subhankar Ghosh", "Xuesong Yang", "Ante Jukić", "Jason Li", "Boris Ginsburg"], "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference", "comment": "Accepted to Interspeech 2025", "summary": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference."}
{"id": "2508.06037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06037", "abs": "https://arxiv.org/abs/2508.06037", "authors": ["Tien Ngoc Ha", "Daniel Romero"], "title": "Bayesian Radio Map Estimation: Fundamentals and Implementation via Diffusion Models", "comment": null, "summary": "Radio map estimation (RME) is the problem of inferring the value of a certain\nmetric (e.g. signal power) across an area of interest given a collection of\nmeasurements. While most works tackle this problem from a purely non-Bayesian\nperspective, some Bayesian estimators have been proposed. However, the latter\nfocus on estimating the map itself, the Bayesian standpoint is adopted mainly\nto exploit prior information or to capture uncertainty. This paper pursues a\nmore general formulation, where the goal is to determine the posterior\ndistribution of the map given the measurements. Besides handling uncertainty\nand allowing standard Bayesian estimates, solving this problem is seen to\nenable minimum mean square error estimation of arbitrary map functionals (e.g.\ncapacity, bit error rate, or coverage area to name a few) while training only\nfor power estimation. A general Bayesian estimator is proposed based on\nconditional diffusion models and both the Bayesian and non-Bayesian paradigms\nare compared analytically and numerically to determine when the Bayesian\napproach is preferable."}
{"id": "2508.05978", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05978", "abs": "https://arxiv.org/abs/2508.05978", "authors": ["Wei Chen", "Binzhu Sha", "Dan Luo", "Jing Yang", "Zhuo Wang", "Fan Fan", "Zhiyong Wu"], "title": "DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching", "comment": "Accepted by INTERSPEECH 2025", "summary": "Singing Voice Conversion (SVC) transfers a source singer's timbre to a target\nwhile keeping melody and lyrics. The key challenge in any-to-any SVC is\nadapting unseen speaker timbres to source audio without quality degradation.\nExisting methods either face timbre leakage or fail to achieve satisfactory\ntimbre similarity and quality in the generated audio. To address these\nchallenges, we propose DAFMSVC, where the self-supervised learning (SSL)\nfeatures from the source audio are replaced with the most similar SSL features\nfrom the target audio to prevent timbre leakage. It also incorporates a dual\ncross-attention mechanism for the adaptive fusion of speaker embeddings,\nmelody, and linguistic content. Additionally, we introduce a flow matching\nmodule for high quality audio generation from the fused features. Experimental\nresults show that DAFMSVC significantly enhances timbre similarity and\nnaturalness, outperforming state-of-the-art methods in both subjective and\nobjective evaluations."}
{"id": "2508.06271", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06271", "abs": "https://arxiv.org/abs/2508.06271", "authors": ["Xingchen Li", "Boyi Kang", "Ziqian Wang", "Zihan Zhang", "Mingshuai Liu", "Zhonghua Fu", "Lei Xie"], "title": "EchoFree: Towards Ultra Lightweight and Efficient Neural Acoustic Echo Cancellation", "comment": null, "summary": "In recent years, neural networks (NNs) have been widely applied in acoustic\necho cancellation (AEC). However, existing approaches struggle to meet\nreal-world low-latency and computational requirements while maintaining\nperformance. To address this challenge, we propose EchoFree, an ultra\nlightweight neural AEC framework that combines linear filtering with a neural\npost filter. Specifically, we design a neural post-filter operating on\nBark-scale spectral features. Furthermore, we introduce a two-stage\noptimization strategy utilizing self-supervised learning (SSL) models to\nimprove model performance. We evaluate our method on the blind test set of the\nICASSP 2023 AEC Challenge. The results demonstrate that our model, with only\n278K parameters and 30 MMACs computational complexity, outperforms existing\nlow-complexity AEC models and achieves performance comparable to that of\nstate-of-the-art lightweight model DeepVQE-S. The audio examples are available."}
{"id": "2508.06054", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06054", "abs": "https://arxiv.org/abs/2508.06054", "authors": ["Yiheng Wang", "Shutao Zhang", "Ye Xue", "Tsung-Hui Chang"], "title": "Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling", "comment": null, "summary": "This paper presents MM-LSCM, a self-supervised multi-modal neural radio\nradiance field framework for localized statistical channel modeling (LSCM) for\nnext-generation network optimization. Traditional LSCM methods rely solely on\nRSRP data, limiting their ability to model environmental structures that affect\nsignal propagation. To address this, we propose a dual-branch neural\narchitecture that integrates RSRP data and LiDAR point cloud information,\nenhancing spatial awareness and predictive accuracy. MM-LSCM leverages\nvolume-rendering-based multi-modal synthesis to align radio propagation with\nenvironmental obstacles and employs a self-supervised training approach,\neliminating the need for costly labeled data. Experimental results demonstrate\nthat MM-LSCM significantly outperforms conventional methods in channel\nreconstruction accuracy and robustness to noise, making it a promising solution\nfor real-world wireless network optimization."}
{"id": "2508.06098", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06098", "abs": "https://arxiv.org/abs/2508.06098", "authors": ["Xiquan Li", "Junxi Liu", "Yuzhe Liang", "Zhikang Niu", "Wenxi Chen", "Xie Chen"], "title": "MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows", "comment": "9 pages, 3 figures", "summary": "Recent developments in diffusion- and flow- based models have significantly\nadvanced Text-to-Audio Generation (TTA). While achieving great synthesis\nquality and controllability, current TTA systems still suffer from slow\ninference speed, which significantly limits their practical applicability. This\npaper presents MeanAudio, a novel MeanFlow-based model tailored for fast and\nfaithful text-to-audio generation. Built on a Flux-style latent transformer,\nMeanAudio regresses the average velocity field during training, enabling fast\ngeneration by mapping directly from the start to the endpoint of the flow\ntrajectory. By incorporating classifier-free guidance (CFG) into the training\ntarget, MeanAudio incurs no additional cost in the guided sampling process. To\nfurther stabilize training, we propose an instantaneous-to-mean curriculum with\nflow field mix-up, which encourages the model to first learn the foundational\ninstantaneous dynamics, and then gradually adapt to mean flows. This strategy\nproves critical for enhancing training efficiency and generation quality.\nExperimental results demonstrate that MeanAudio achieves state-of-the-art\nperformance in single-step audio generation. Specifically, it achieves a real\ntime factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup\nover SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates\nstrong performance in multi-step generation, enabling smooth and coherent\ntransitions across successive synthesis steps."}
{"id": "2508.06284", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06284", "abs": "https://arxiv.org/abs/2508.06284", "authors": ["Fredrik Cumlin", "Xinyu Liang", "Anubhab Ghosh", "Saikat Chatterjee"], "title": "Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment", "comment": "ECAI workshop paper", "summary": "Non-intrusive speech quality assessment (SQA) systems suffer from limited\ntraining data and costly human annotations, hindering their generalization to\nreal-time conferencing calls. In this work, we propose leveraging large\nlanguage models (LLMs) as pseudo-raters for speech quality to address these\ndata bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129\nspeech clips with simulated degradations labeled by a fine-tuned auditory LLM\n(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled\ndata, using LLM-labeled data, and a two-stage approach (pretraining on LLM\nlabels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.\nWe test on several datasets across languages and quality degradations. While\nLLM-labeled training yields mixed results compared to human-labeled training,\nwe provide empirical evidence that the two-stage approach improves the\ngeneralization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on\nNISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings\ndemonstrate the potential of using LLMs as scalable pseudo-raters for speech\nquality assessment, offering a cost-effective solution to the data limitation\nproblem."}
{"id": "2508.06141", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06141", "abs": "https://arxiv.org/abs/2508.06141", "authors": ["Marco Bertuletti", "Yichao Zhang", "Mahdi Abdollahpour", "Samuel Riedel", "Alessandro Vanelli-Coralli"], "title": "Fast End-to-End Simulation and Exploration of Many-RISCV-Core Baseband Transceivers for Software-Defined Radio-Access Networks", "comment": "7 pages", "summary": "The fast-rising demand for wireless bandwidth requires rapid evolution of\nhigh-performance baseband processing infrastructure. Programmable many-core\nprocessors for software-defined radio (SDR) have emerged as high-performance\nbaseband processing engines, offering the flexibility required to capture\nevolving wireless standards and technologies. This trend must be supported by a\ndesign framework enabling functional validation and end-to-end performance\nanalysis of SDR hardware within realistic radio environment models. We propose\na static binary translation based simulator augmented with a fast, approximate\ntiming model of the hardware and coupled to wireless channel models to simulate\nthe most performance-critical physical layer functions implemented in software\non a many (1024) RISC-V cores cluster customized for SDR. Our framework\nsimulates the detection of a 5G OFDM-symbol on a server-class processor in\n9.5s-3min, on a single thread, depending on the input MIMO size (three orders\nof magnitude faster than RTL simulation). The simulation is easily parallelized\nto 128 threads with 73-121x speedup compared to a single thread."}
{"id": "2508.06262", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06262", "abs": "https://arxiv.org/abs/2508.06262", "authors": ["Wenjie Tian", "Xinfa Zhu", "Hanke Xie", "Zhen Ye", "Wei Xue", "Lei Xie"], "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis", "comment": null, "summary": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus."}
{"id": "2508.06310", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06310", "abs": "https://arxiv.org/abs/2508.06310", "authors": ["Yihsuan Wu", "Yukai Chiu", "Michael Anthony", "Mingsian R. Bai"], "title": "Egonoise Resilient Source Localization and Speech Enhancement for Drones Using a Hybrid Model and Learning-Based Approach", "comment": null, "summary": "Drones are becoming increasingly important in search and rescue missions, and\neven military operations. While the majority of drones are equipped with camera\nvision capabilities, the realm of drone audition remains underexplored due to\nthe inherent challenge of mitigating the egonoise generated by the rotors. In\nthis paper, we present a novel technique to address this extremely low\nsignal-to-noise ratio (SNR) problem encountered by the microphone-embedded\ndrones. The technique is implemented using a hybrid approach that combines\nArray Signal Processing (ASP) and Deep Neural Networks (DNN) to enhance the\nspeech signals captured by a six-microphone uniform circular array mounted on a\nquadcopter. The system performs localization of the target speaker through\nbeamsteering in conjunction with speech enhancement through a Generalized\nSidelobe Canceller-DeepFilterNet 2 (GSC-DF2) system. To validate the system,\nthe DREGON dataset and measured data are employed. Objective evaluations of the\nproposed hybrid approach demonstrated its superior performance over four\nbaseline methods in the SNR condition as low as -30 dB."}
{"id": "2508.06176", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06176", "abs": "https://arxiv.org/abs/2508.06176", "authors": ["Marco Bertuletti", "Yichao Zhang", "Alessandro Vanelli-Coralli", "Luca Benini"], "title": "A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio Uplinks", "comment": null, "summary": "Following the scale-up of new radio (NR) complexity in 5G and beyond, the\nphysical layer's computing load on base stations is increasing under a strictly\nconstrained latency and power budget; base stations must process > 20-Gb/s\nuplink wireless data rate on the fly, in < 10 W. At the same time, the\nprogrammability and reconfigurability of base station components are the key\nrequirements; it reduces the time and cost of new networks' deployment, it\nlowers the acceptance threshold for industry players to enter the market, and\nit ensures return on investments in a fast-paced evolution of standards. In\nthis article, we present the design of a many-core cluster for 5G and beyond\nbase station processing. Our design features 1024, streamlined RISC-V cores\nwith domain-specific FP extensions, and 4-MiB shared memory. It provides the\nnecessary computational capabilities for software-defined processing of the\nlower physical layer of 5G physical uplink shared channel (PUSCH), satisfying\nhigh-end throughput requirements (66 Gb/s for a transition time interval (TTI),\n9.4-302 Gb/s depending on the processing stage). The throughput metrics for the\nimplemented functions are ten times higher than in state-of-the-art (SoTA)\napplication-specific instruction processors (ASIPs). The energy efficiency on\nkey NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\\deg}C, and 0.8 V, on a\nplaced and routed instance in 12-nm CMOS technology, is competitive with SoTA\narchitectures. The PUSCH processing runs end-to-end on a single cluster in 1.7\nms, at <6-W average power consumption, achieving 12 Gb/s/W."}
{"id": "2508.06321", "categories": ["cs.SD", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06321", "abs": "https://arxiv.org/abs/2508.06321", "authors": ["Durjoy Chandra Paul", "Gaurob Saha", "Md Amjad Hossain"], "title": "EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition", "comment": "To be published in ICCCNT 2025 (16th International Conference on\n  Computing Communication and Networking Technologies)", "summary": "Recognizing emotional signals in speech has a significant impact on enhancing\nthe effectiveness of human-computer interaction (HCI). This study introduces\nEmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term\nMemory (LSTM) layers with one-dimensional Convolutional Neural Networks\n(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and\nvariety of the features that are taken from speech signals have a significant\nimpact on how well SER systems perform. A comprehensive speech data\naugmentation strategy was used to combine both traditional methods, such as\nnoise addition, pitch shifting, and time stretching, with a novel\ncombination-based augmentation pipeline to enhance generalization and reduce\noverfitting. Each audio sample was transformed into a high-dimensional feature\nvector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient\n(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a\nweighted accuracy of 95.78\\% and unweighted accuracy of 92.52\\% on the IEMOCAP\ndataset and, with ELU activation, has a weighted accuracy of 96.75\\% and\nunweighted accuracy of 91.28\\%. On the RAVDESS dataset, we get a weighted\naccuracy of 94.53\\% and 94.98\\% unweighted accuracy for ReLU activation and\n93.72\\% weighted accuracy and 94.64\\% unweighted accuracy for ELU activation.\nThese results highlight EmoAugNet's effectiveness in improving the robustness\nand performance of SER systems through integated data augmentation and hybrid\nmodeling."}
{"id": "2508.06356", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06356", "abs": "https://arxiv.org/abs/2508.06356", "authors": ["Sarina Meyer", "Ngoc Thang Vu"], "title": "Use Cases for Voice Anonymization", "comment": "Accepted at SPSC 2025 - 5th Symposium on Security and Privacy in\n  Speech Communication", "summary": "The performance of a voice anonymization system is typically measured\naccording to its ability to hide the speaker's identity and keep the data's\nutility for downstream tasks. This means that the requirements the\nanonymization should fulfill depend on the context in which it is used and may\ndiffer greatly between use cases. However, these use cases are rarely specified\nin research papers. In this paper, we study the implications of use\ncase-specific requirements on the design of voice anonymization methods. We\nperform an extensive literature analysis and user study to collect possible use\ncases and to understand the expectations of the general public towards such\ntools. Based on these studies, we propose the first taxonomy of use cases for\nvoice anonymization, and derive a set of requirements and design criteria for\nmethod development and evaluation. Using this scheme, we propose to focus more\non use case-oriented research and development of voice anonymization systems."}
{"id": "2508.06275", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06275", "abs": "https://arxiv.org/abs/2508.06275", "authors": ["SaiKrishna Saketh Yellapragada", "Esa Ollila", "Mario Costa"], "title": "Efficient Deep Neural Receiver with Post-Training Quantization", "comment": null, "summary": "Deep learning has recently garnered significant interest in wireless\ncommunications due to its superior performance compared to traditional\nmodel-based algorithms. Deep convolutional neural networks (CNNs) have\ndemonstrated notable improvements in block error rate (BLER) under various\nchannel models and mobility scenarios. However, the high computational\ncomplexity and resource demands of deep CNNs pose challenges for deployment in\nresource-constrained edge systems. The 3rd Generation Partnership Project\n(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)\nintegration in enabling advanced radio-access networks for 6G systems. The hard\nreal-time processing demands of 5G and 6G require efficient techniques such as\npost-training quantization (PTQ), quantization-aware training (QAT), pruning,\nand hybrid approaches to meet latency requirements. In this paper, we focus on\nPTQ to reduce model complexity by lowering the bit-width of weights, thereby\nenhancing computational efficiency. Our analysis employs symmetric uniform\nquantization, applying both per-tensor and per-channel PTQ to a neural receiver\nachieving performance comparable to full-precision models. Specifically, 8-bit\nper-channel quantization maintains BLER performance with minimal degradation,\nwhile 4-bit quantization shows great promise but requires further optimization\nto achieve target BLER levels. These results highlight the potential of\nultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems."}
{"id": "2508.06372", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06372", "abs": "https://arxiv.org/abs/2508.06372", "authors": ["Han Yin", "Yafeng Chen", "Chong Deng", "Luyao Cheng", "Hui Wang", "Chao-Hong Tan", "Qian Chen", "Wen Wang", "Xiangang Li"], "title": "SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models", "comment": null, "summary": "The Speaker Diarization and Recognition (SDR) task aims to predict \"who spoke\nwhen and what\" within an audio clip, which is a crucial task in various\nreal-world multi-speaker scenarios such as meeting transcription and dialogue\nsystems. Existing SDR systems typically adopt a cascaded framework, combining\nmultiple modules such as speaker diarization (SD) and automatic speech\nrecognition (ASR). The cascaded systems suffer from several limitations, such\nas error propagation, difficulty in handling overlapping speech, and lack of\njoint optimization for exploring the synergy between SD and ASR tasks. To\naddress these limitations, we introduce SpeakerLM, a unified multimodal large\nlanguage model for SDR that jointly performs SD and ASR in an end-to-end\nmanner. Moreover, to facilitate diverse real-world scenarios, we incorporate a\nflexible speaker registration mechanism into SpeakerLM, enabling SDR under\ndifferent speaker registration settings. SpeakerLM is progressively developed\nwith a multi-stage training strategy on large-scale real data. Extensive\nexperiments show that SpeakerLM demonstrates strong data scaling capability and\ngeneralizability, outperforming state-of-the-art cascaded baselines on both\nin-domain and out-of-domain public SDR benchmarks. Furthermore, experimental\nresults show that the proposed speaker registration mechanism effectively\nensures robust SDR performance of SpeakerLM across diverse speaker registration\nconditions and varying numbers of registered speakers."}
{"id": "2508.06405", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06405", "abs": "https://arxiv.org/abs/2508.06405", "authors": ["Guilherme Zucatelli", "Ricardo Barioni", "Gabriela Dantas"], "title": "Acoustic Non-Stationarity Objective Assessment with Hard Label Criteria for Supervised Learning Models", "comment": "Manuscript under review", "summary": "Objective non-stationarity measures are resource intensive and impose\ncritical limitations for real-time processing solutions. In this paper, a novel\nHard Label Criteria (HLC) algorithm is proposed to generate a global\nnon-stationarity label for acoustic signals, enabling supervised learning\nstrategies to be trained as stationarity estimators. The HLC is first evaluated\non state-of-the-art general-purpose acoustic models, demonstrating that these\nmodels encode stationarity information. Furthermore, the first-of-its-kind\nHLC-based Network for Acoustic Non-Stationarity Assessment (NANSA) is proposed.\nNANSA models outperform competing approaches, achieving up to 99\\%\nclassification accuracy, while solving the computational infeasibility of\ntraditional objective measures."}
{"id": "2508.06340", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06340", "abs": "https://arxiv.org/abs/2508.06340", "authors": ["Danish Mehmood Mughal", "Daniyal Munir", "Qazi Arbab Ahmed", "Hans D. Schotten", "Thorsten Jungeblut", "Sang-Hyo Kim", "Min Young Chung"], "title": "MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications", "comment": "Accepted for presentation at IEEE CSCN 2025", "summary": "Reconfigurable intelligent surfaces (RIS) enhance wireless communication by\ndynamically shaping the propagation environment, but their integration\nintroduces hardware-level security risks. This paper presents the concept of\nMalicious RIS (MALRIS), where compromised components behave adversarially, even\nunder passive operation. The focus of this work is on practical threats such as\nmanufacturing time tampering, malicious firmware, and partial element control.\nTwo representative attacks, power-splitting and element-splitting, are modeled\nto assess their impact. Simulations in a RIS-assisted system reveal that even a\nlimited hardware compromise can significantly degrade performance metrics such\nas bit error rate, throughput, and secrecy metrics. By exposing this overlooked\nthreat surface, this work aims to promote awareness and support secure,\ntrustworthy RIS deployment in future wireless networks."}
{"id": "2508.06391", "categories": ["cs.SD", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06391", "abs": "https://arxiv.org/abs/2508.06391", "authors": ["Péter Mihajlik", "Éva Székely", "Piroska Barta", "Máté Soma Kádár", "Gergely Dobsinszki", "László Tóth"], "title": "Improved Dysarthric Speech to Text Conversion via TTS Personalization", "comment": null, "summary": "We present a case study on developing a customized speech-to-text system for\na Hungarian speaker with severe dysarthria. State-of-the-art automatic speech\nrecognition (ASR) models struggle with zero-shot transcription of dysarthric\nspeech, yielding high error rates. To improve performance with limited real\ndysarthric data, we fine-tune an ASR model using synthetic speech generated via\na personalized text-to-speech (TTS) system. We introduce a method for\ngenerating synthetic dysarthric speech with controlled severity by leveraging\npremorbidity recordings of the given speaker and speaker embedding\ninterpolation, enabling ASR fine-tuning on a continuum of impairments.\nFine-tuning on both real and synthetic dysarthric speech reduces the character\nerror rate (CER) from 36-51% (zero-shot) to 7.3%. Our monolingual\nFastConformer_Hu ASR model significantly outperforms Whisper-turbo when\nfine-tuned on the same data, and the inclusion of synthetic speech contributes\nto an 18% relative CER reduction. These results highlight the potential of\npersonalized ASR systems for improving accessibility for individuals with\nsevere speech impairments."}
{"id": "2508.06262", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06262", "abs": "https://arxiv.org/abs/2508.06262", "authors": ["Wenjie Tian", "Xinfa Zhu", "Hanke Xie", "Zhen Ye", "Wei Xue", "Lei Xie"], "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis", "comment": null, "summary": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus."}
{"id": "2508.06428", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06428", "abs": "https://arxiv.org/abs/2508.06428", "authors": ["Zhiwen Zhou"], "title": "Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for Low-Altitude UAV with Zero Sensing Resource Allocation", "comment": null, "summary": "Low-altitude unmanned aerial vehicles (UAVs) are expected to play an\nimportant role for low-altitude economy with a wide range of applications like\nprecise agriculture, aerial delivery and surveillance. Integrated sensing and\ncommunication (ISAC) is a key technology to enable the large-scale deployment\nand routine usage of UAVs by providing both communication and sensing services\nefficiently. For UAV ISAC systems, as UAV often acts as both a communication\nuser equipment (UE) and a sensing target, traditional ISAC systems that usually\nallocate dedicated TF resources for sensing are inefficient due to the severe\ndegradation of communication spectral efficiency. To address this issue, in\nthis paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal\nfrequency division multiplexing (OFDM)-based ISAC framework for UAVs that\neliminates the need for dedicated sensing TF resources, achieving zero TF\nsensing overhead. By designing the transmit beamforming to meet the\nrequirements for both communication and sensing tasks, our proposed approach\nenables the communication TF resources to be fully reused for sensing, thereby\nenhancing both the communication sum rate and the sensing performance in terms\nof resolution, unambiguous range, and accuracy. Additionally, we introduce a\nlow-complexity target searching beamforming algorithm and a two-stage\nsuper-resolution sensing algorithm, which ensure efficient implementation.\nSimulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not\nonly improves the communication sum rate but also outperforms traditional ISAC\nsystems in sensing performance, making it a promising solution for future ISAC\nsystems to support low-altitude UAVs."}
{"id": "2508.06393", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06393", "abs": "https://arxiv.org/abs/2508.06393", "authors": ["Md Asif Jalal", "Luca Remaggi", "Vasileios Moschopoulos", "Thanasis Kotsiopoulos", "Vandana Rajan", "Karthikeyan Saravanan", "Anastasis Drosou", "Junho Heo", "Hyuk Oh", "Seokyeong Jeong"], "title": "Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling", "comment": "Accepted to Interspeech 2025", "summary": "Traditional speech separation and speaker diarization approaches rely on\nprior knowledge of target speakers or a predetermined number of participants in\naudio signals. To address these limitations, recent advances focus on\ndeveloping enrollment-free methods capable of identifying targets without\nexplicit speaker labeling. This work introduces a new approach to train\nsimultaneous speech separation and diarization using automatic identification\nof target speaker embeddings, within mixtures. Our proposed model employs a\ndual-stage training pipeline designed to learn robust speaker representation\nfeatures that are resilient to background noise interference. Furthermore, we\npresent an overlapping spectral loss function specifically tailored for\nenhancing diarization accuracy during overlapped speech frames. Experimental\nresults show significant performance gains compared to the current SOTA\nbaseline, achieving 71% relative improvement in DER and 69% in cpWER."}
{"id": "2508.06405", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06405", "abs": "https://arxiv.org/abs/2508.06405", "authors": ["Guilherme Zucatelli", "Ricardo Barioni", "Gabriela Dantas"], "title": "Acoustic Non-Stationarity Objective Assessment with Hard Label Criteria for Supervised Learning Models", "comment": "Manuscript under review", "summary": "Objective non-stationarity measures are resource intensive and impose\ncritical limitations for real-time processing solutions. In this paper, a novel\nHard Label Criteria (HLC) algorithm is proposed to generate a global\nnon-stationarity label for acoustic signals, enabling supervised learning\nstrategies to be trained as stationarity estimators. The HLC is first evaluated\non state-of-the-art general-purpose acoustic models, demonstrating that these\nmodels encode stationarity information. Furthermore, the first-of-its-kind\nHLC-based Network for Acoustic Non-Stationarity Assessment (NANSA) is proposed.\nNANSA models outperform competing approaches, achieving up to 99\\%\nclassification accuracy, while solving the computational infeasibility of\ntraditional objective measures."}
{"id": "2508.05835", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.05835", "abs": "https://arxiv.org/abs/2508.05835", "authors": ["Edresson Casanova", "Paarth Neekhara", "Ryan Langman", "Shehzeen Hussain", "Subhankar Ghosh", "Xuesong Yang", "Ante Jukić", "Jason Li", "Boris Ginsburg"], "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference", "comment": "Accepted to Interspeech 2025", "summary": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference."}

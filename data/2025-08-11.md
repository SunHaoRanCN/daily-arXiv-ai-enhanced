<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.SD](#cs.SD) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Remark on the AAA Method for Secret-Key Generation in Mobile Networks](https://arxiv.org/abs/2508.05801)
*Yingbo Hua*

Main category: eess.SP

TL;DR: 论文提出了一种具有累积性、适应性和可加性（AAA）特性的密钥生成方法，展示了其性能的鲁棒性，并在与理想方法的比较中突出了AAA方法的优势。


<details>
  <summary>Details</summary>
Motivation: 研究一种广泛适用的密钥生成方法，确保其在存在干扰或泄漏的情况下仍能保持性能。

Method: 通过分析叠加包的数量趋近无穷时密钥的混淆性，验证AAA方法的鲁棒性，并与基于互易信道估计的理想方法进行比较。

Result: AAA方法在叠加包数量趋近无穷时，密钥的混淆性趋于完美；与理想方法相比，AAA方法具有多项优势。

Conclusion: AAA方法在密钥生成中表现出鲁棒性和优越性，适用于多种场景。

Abstract: A broadly applicable method for secret-key generation is named for its
accumulative, adaptable and additive (AAA) properties. This paper first shows a
robustness of its performance. Namely, even if there is an inter correlation or
a leakage caused intra correlation among the superimposed packets, provided
there is a nonzero probability for each packet to be missed in full or in part
by Eve, then the equivocation of the key generated by the AAA method always
becomes perfect as the number of superpositions becomes infinite. Also shown in
this paper is a comparison between the AAA method and an ideal method based on
reciprocal channel estimation, which reveals several advantages of the AAA
method.

</details>


### [2] [STEEP -- An Alternative To Quantum Key Distribution](https://arxiv.org/abs/2508.05882)
*Yingbo Hua*

Main category: eess.SP

TL;DR: STEEP是一种替代量子密钥分发（QKD）的秘密消息传输方法，仅需经典或非量子信道，适用于多种实际场景。


<details>
  <summary>Details</summary>
Motivation: 探讨一种比QKD更简单、成本更低且兼容性更强的秘密消息传输方法。

Method: 通过回显加密探针（STEEP）实现秘密消息传输，无需量子信道。

Result: STEEP在许多实际场景中（如空中信道或海底光缆）能提供足够的保密速率，支持一次性密码本加密。

Conclusion: STEEP在成本、复杂性、兼容性和抗持续窃听方面优于QKD。

Abstract: Secret-message transmission by echoing encrypted probes (STEEP) is discussed
as an alternative to quantum key distribution (QKD). The former only needs
classic or non-quantum channels while the latter needs both quantum and classic
channels for secret-key generation. STEEP is shown to yield a secrecy rate
sufficient for one-time pads encryption in many practical situations including
in-air channels or undersea optical cables. Other advantages of STEEP over QKD
include cost, complexity, compatibility, and robustness against constant
eavesdropping.

</details>


### [3] [IRS-Assisted IoT Activity Detection Under Asynchronous Transmission and Heterogeneous Powers: Detectors and Performance Analysis](https://arxiv.org/abs/2508.05959)
*Amirhossein Taherpour,Somayeh Khani,Abbas Taherpour,Tamer Khattab*

Main category: eess.SP

TL;DR: 论文提出了一种在分布式物联网网络中检测活动的方法，利用智能反射面（IRS）提升检测可靠性，并设计了四种检测器，结合理论分析与仿真验证。


<details>
  <summary>Details</summary>
Motivation: 解决分布式物联网网络中设备异步传输和异质功率下的活动检测问题，提升检测可靠性。

Method: 将检测问题建模为二元假设检验，设计了四种检测器（包括最优检测器和三种计算高效的检测器），并推导了检测和虚警概率的闭式表达式。

Result: 通过仿真验证了理论分析，并评估了关键系统参数对检测性能的影响。

Conclusion: 提出的框架在理论最优性和实现实用性之间架起了桥梁，为6G系统中的IRS辅助物联网网络提供了可扩展的解决方案。

Abstract: This paper addresses the problem of activity detection in distributed
Internet of Things (IoT) networks, where devices employ asynchronous
transmissions with heterogeneous power levels to report their local
observations. The system leverages an intelligent reflecting surface (IRS) to
enhance detection reliability, with optional incorporation of a direct
line-of-sight (LoS) path. We formulate the detection problem as a binary
hypothesis test and develop four detectors: an optimal detector alongside three
computationally efficient detectors designed for practical scenarios with
different levels of prior knowledge about noise variance, channel state
information, and device transmit powers. For each detector, we derive
closed-form expressions for both detection and false alarm probabilities,
establishing theoretical performance benchmarks. Extensive simulations validate
our analytical results and systematically evaluate the impact of key system
parameters including the number of antennas, samples, users, and IRS elements
on detection performance. The proposed framework effectively bridges
theoretical optimality with implementation practicality, providing a scalable
solution for IRS-assisted IoT networks in emerging 6G systems.

</details>


### [4] [Multi-Functional Chirp Signalling for Next-Generation Multi-Carrier Wireless Networks: Communications, Sensing and ISAC Perspectives](https://arxiv.org/abs/2508.06022)
*Zeping Sui,Qu Luo,Zilong Liu,Murat Temiz,Leila Musavian,Christos Masouros,Yong Liang Guan,Pei Xiao,Lajos Hanzo*

Main category: eess.SP

TL;DR: 论文提出利用啁啾信号设计多功能通信方案，以应对下一代多载波移动网络的高服务质量需求，并探讨其在高速移动通信和集成感知与通信中的优势。


<details>
  <summary>Details</summary>
Motivation: 为满足下一代多载波移动网络对高效、灵活和可靠通信与感知的需求，研究啁啾信号的设计与应用。

Method: 结合Zadoff-Chu序列与啁啾扩展频谱、FMCW雷达等波形，设计多功能啁啾多载波波形。

Result: 啁啾信号在双选择性信道中表现出强鲁棒性，适用于高速移动通信和ISAC。

Conclusion: 啁啾信号是未来通信系统的有潜力候选方案，并提出了进一步研究方向。

Abstract: To meet the increasingly demanding quality-of-service requirements of the
next-generation multi-carrier mobile networks, it is essential to design
multi-functional signalling schemes facilitating efficient, flexible, and
reliable communication and sensing in complex wireless environments. As a
compelling candidate, we advocate chirp signalling, beneficially amalgamating
sequences (e.g., Zadoff-Chu sequences) with waveforms (e.g., chirp spread
spectrum and frequency-modulated continuous wave (FMCW) radar), given their
resilience against doubly selective channels. Besides chirp sequences, a wide
range of chirp waveforms is considered, ranging from FMCW to affine
frequency-division multiplexing (AFDM), to create a promising chirp
multicarrier waveform. This study also highlights the advantages of such
waveforms in supporting reliable high-mobility communications, plus integrated
sensing and communications (ISAC). Finally, we outline several emerging
research directions for chirp signalling designs.

</details>


### [5] [Bayesian Radio Map Estimation: Fundamentals and Implementation via Diffusion Models](https://arxiv.org/abs/2508.06037)
*Tien Ngoc Ha,Daniel Romero*

Main category: eess.SP

TL;DR: 论文提出了一种基于条件扩散模型的通用贝叶斯估计器，用于无线电地图估计（RME），并比较了贝叶斯和非贝叶斯方法的优劣。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法主要关注地图本身的估计，而本文旨在通过后验分布更全面地处理不确定性，并支持任意地图功能的最小均方误差估计。

Method: 采用条件扩散模型构建通用贝叶斯估计器，分析并数值比较贝叶斯和非贝叶斯方法。

Result: 贝叶斯方法在不确定性处理和功能估计方面表现更优，尤其在训练仅针对功率估计时仍能有效估计其他功能。

Conclusion: 贝叶斯方法在特定场景下更具优势，尤其是在需要处理不确定性和复杂功能估计时。

Abstract: Radio map estimation (RME) is the problem of inferring the value of a certain
metric (e.g. signal power) across an area of interest given a collection of
measurements. While most works tackle this problem from a purely non-Bayesian
perspective, some Bayesian estimators have been proposed. However, the latter
focus on estimating the map itself, the Bayesian standpoint is adopted mainly
to exploit prior information or to capture uncertainty. This paper pursues a
more general formulation, where the goal is to determine the posterior
distribution of the map given the measurements. Besides handling uncertainty
and allowing standard Bayesian estimates, solving this problem is seen to
enable minimum mean square error estimation of arbitrary map functionals (e.g.
capacity, bit error rate, or coverage area to name a few) while training only
for power estimation. A general Bayesian estimator is proposed based on
conditional diffusion models and both the Bayesian and non-Bayesian paradigms
are compared analytically and numerically to determine when the Bayesian
approach is preferable.

</details>


### [6] [Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling](https://arxiv.org/abs/2508.06054)
*Yiheng Wang,Shutao Zhang,Ye Xue,Tsung-Hui Chang*

Main category: eess.SP

TL;DR: MM-LSCM是一种自监督多模态神经无线电辐射场框架，用于下一代网络优化的局部统计信道建模，结合RSRP数据和LiDAR点云信息，显著提升了信道重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统LSCM方法仅依赖RSRP数据，无法充分建模影响信号传播的环境结构，因此需要一种更全面的方法。

Method: 提出双分支神经架构，整合RSRP数据和LiDAR点云信息，利用基于体积渲染的多模态合成和环境自监督训练。

Result: 实验表明，MM-LSCM在信道重建精度和抗噪性上显著优于传统方法。

Conclusion: MM-LSCM为实际无线网络优化提供了高效且无需昂贵标注数据的解决方案。

Abstract: This paper presents MM-LSCM, a self-supervised multi-modal neural radio
radiance field framework for localized statistical channel modeling (LSCM) for
next-generation network optimization. Traditional LSCM methods rely solely on
RSRP data, limiting their ability to model environmental structures that affect
signal propagation. To address this, we propose a dual-branch neural
architecture that integrates RSRP data and LiDAR point cloud information,
enhancing spatial awareness and predictive accuracy. MM-LSCM leverages
volume-rendering-based multi-modal synthesis to align radio propagation with
environmental obstacles and employs a self-supervised training approach,
eliminating the need for costly labeled data. Experimental results demonstrate
that MM-LSCM significantly outperforms conventional methods in channel
reconstruction accuracy and robustness to noise, making it a promising solution
for real-world wireless network optimization.

</details>


### [7] [Fast End-to-End Simulation and Exploration of Many-RISCV-Core Baseband Transceivers for Software-Defined Radio-Access Networks](https://arxiv.org/abs/2508.06141)
*Marco Bertuletti,Yichao Zhang,Mahdi Abdollahpour,Samuel Riedel,Alessandro Vanelli-Coralli*

Main category: eess.SP

TL;DR: 提出了一种基于静态二进制翻译的模拟器框架，用于高效模拟SDR硬件在真实无线环境中的性能，支持1024个RISC-V核心的集群。


<details>
  <summary>Details</summary>
Motivation: 无线带宽需求快速增长，需要高性能基带处理基础设施，而可编程多核处理器为软件定义无线电（SDR）提供了灵活性。

Method: 采用静态二进制翻译模拟器，结合快速近似时序模型和无线信道模型，模拟SDR硬件性能。

Result: 在单线程下模拟5G OFDM符号检测仅需9.5秒至3分钟，比RTL模拟快三个数量级；并行至128线程时可提速73-121倍。

Conclusion: 该框架为SDR硬件设计提供了高效的功能验证和端到端性能分析工具。

Abstract: The fast-rising demand for wireless bandwidth requires rapid evolution of
high-performance baseband processing infrastructure. Programmable many-core
processors for software-defined radio (SDR) have emerged as high-performance
baseband processing engines, offering the flexibility required to capture
evolving wireless standards and technologies. This trend must be supported by a
design framework enabling functional validation and end-to-end performance
analysis of SDR hardware within realistic radio environment models. We propose
a static binary translation based simulator augmented with a fast, approximate
timing model of the hardware and coupled to wireless channel models to simulate
the most performance-critical physical layer functions implemented in software
on a many (1024) RISC-V cores cluster customized for SDR. Our framework
simulates the detection of a 5G OFDM-symbol on a server-class processor in
9.5s-3min, on a single thread, depending on the input MIMO size (three orders
of magnitude faster than RTL simulation). The simulation is easily parallelized
to 128 threads with 73-121x speedup compared to a single thread.

</details>


### [8] [A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio Uplinks](https://arxiv.org/abs/2508.06176)
*Marco Bertuletti,Yichao Zhang,Alessandro Vanelli-Coralli,Luca Benini*

Main category: eess.SP

TL;DR: 本文设计了一个多核集群，用于5G及以后基站的物理层处理，具有1024个RISC-V核心和4-MiB共享内存，满足高吞吐量和能效要求。


<details>
  <summary>Details</summary>
Motivation: 5G及以后基站的物理层计算负载增加，需在严格的延迟和功耗限制下处理高数据速率，同时需要可编程性和可重构性以降低成本和时间。

Method: 设计了一个包含1024个RISC-V核心的多核集群，支持特定领域的浮点扩展，并实现了4-MiB共享内存。

Result: 该设计在5G物理上行共享信道（PUSCH）处理中实现了高吞吐量（66 Gb/s）和能效（12 Gb/s/W），优于现有技术。

Conclusion: 该多核集群设计满足5G及以后基站的高性能需求，具有显著的优势。

Abstract: Following the scale-up of new radio (NR) complexity in 5G and beyond, the
physical layer's computing load on base stations is increasing under a strictly
constrained latency and power budget; base stations must process > 20-Gb/s
uplink wireless data rate on the fly, in < 10 W. At the same time, the
programmability and reconfigurability of base station components are the key
requirements; it reduces the time and cost of new networks' deployment, it
lowers the acceptance threshold for industry players to enter the market, and
it ensures return on investments in a fast-paced evolution of standards. In
this article, we present the design of a many-core cluster for 5G and beyond
base station processing. Our design features 1024, streamlined RISC-V cores
with domain-specific FP extensions, and 4-MiB shared memory. It provides the
necessary computational capabilities for software-defined processing of the
lower physical layer of 5G physical uplink shared channel (PUSCH), satisfying
high-end throughput requirements (66 Gb/s for a transition time interval (TTI),
9.4-302 Gb/s depending on the processing stage). The throughput metrics for the
implemented functions are ten times higher than in state-of-the-art (SoTA)
application-specific instruction processors (ASIPs). The energy efficiency on
key NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\deg}C, and 0.8 V, on a
placed and routed instance in 12-nm CMOS technology, is competitive with SoTA
architectures. The PUSCH processing runs end-to-end on a single cluster in 1.7
ms, at <6-W average power consumption, achieving 12 Gb/s/W.

</details>


### [9] [Efficient Deep Neural Receiver with Post-Training Quantization](https://arxiv.org/abs/2508.06275)
*SaiKrishna Saketh Yellapragada,Esa Ollila,Mario Costa*

Main category: eess.SP

TL;DR: 论文探讨了在无线通信中应用深度学习，特别是通过后训练量化（PTQ）降低CNN模型的复杂度，以提升计算效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在无线通信中表现出色，但高计算复杂度和资源需求限制了其在边缘系统的部署。研究旨在通过PTQ技术解决这一问题。

Method: 采用对称均匀量化方法，对神经接收器进行每张量和每通道的后训练量化（PTQ），比较不同比特宽度的性能。

Result: 8位每通道量化能保持BLER性能，4位量化表现良好但需进一步优化。

Conclusion: 超低比特宽PTQ在6G系统中部署高效神经接收器具有潜力。

Abstract: Deep learning has recently garnered significant interest in wireless
communications due to its superior performance compared to traditional
model-based algorithms. Deep convolutional neural networks (CNNs) have
demonstrated notable improvements in block error rate (BLER) under various
channel models and mobility scenarios. However, the high computational
complexity and resource demands of deep CNNs pose challenges for deployment in
resource-constrained edge systems. The 3rd Generation Partnership Project
(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)
integration in enabling advanced radio-access networks for 6G systems. The hard
real-time processing demands of 5G and 6G require efficient techniques such as
post-training quantization (PTQ), quantization-aware training (QAT), pruning,
and hybrid approaches to meet latency requirements. In this paper, we focus on
PTQ to reduce model complexity by lowering the bit-width of weights, thereby
enhancing computational efficiency. Our analysis employs symmetric uniform
quantization, applying both per-tensor and per-channel PTQ to a neural receiver
achieving performance comparable to full-precision models. Specifically, 8-bit
per-channel quantization maintains BLER performance with minimal degradation,
while 4-bit quantization shows great promise but requires further optimization
to achieve target BLER levels. These results highlight the potential of
ultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.

</details>


### [10] [MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications](https://arxiv.org/abs/2508.06340)
*Danish Mehmood Mughal,Daniyal Munir,Qazi Arbab Ahmed,Hans D. Schotten,Thorsten Jungeblut,Sang-Hyo Kim,Min Young Chung*

Main category: eess.SP

TL;DR: 论文提出恶意可重构智能表面（MALRIS）概念，揭示硬件级安全风险，并通过模拟攻击展示其对通信性能的显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究RIS在硬件层面可能被恶意利用的安全风险，以促进未来无线网络的安全部署。

Method: 提出MALRIS概念，建模两种攻击（功率分割和单元分割），并通过模拟评估其对性能的影响。

Result: 模拟显示，即使有限的硬件被攻陷，也会显著降低误码率、吞吐量和保密性等性能指标。

Conclusion: 研究揭示了RIS被恶意利用的潜在威胁，呼吁提高安全意识并推动安全部署。

Abstract: Reconfigurable intelligent surfaces (RIS) enhance wireless communication by
dynamically shaping the propagation environment, but their integration
introduces hardware-level security risks. This paper presents the concept of
Malicious RIS (MALRIS), where compromised components behave adversarially, even
under passive operation. The focus of this work is on practical threats such as
manufacturing time tampering, malicious firmware, and partial element control.
Two representative attacks, power-splitting and element-splitting, are modeled
to assess their impact. Simulations in a RIS-assisted system reveal that even a
limited hardware compromise can significantly degrade performance metrics such
as bit error rate, throughput, and secrecy metrics. By exposing this overlooked
threat surface, this work aims to promote awareness and support secure,
trustworthy RIS deployment in future wireless networks.

</details>


### [11] [Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for Low-Altitude UAV with Zero Sensing Resource Allocation](https://arxiv.org/abs/2508.06428)
*Zhiwen Zhou*

Main category: eess.SP

TL;DR: 提出了一种基于MIMO-OFDM的无人机ISAC框架，消除专用感知资源需求，提升通信和感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统ISAC系统因专用感知资源分配导致通信频谱效率下降的问题。

Method: 设计发射波束成形以满足通信和感知需求，引入低复杂度目标搜索波束成形算法和两阶段超分辨率感知算法。

Result: 仿真显示该框架提升了通信总速率，并在感知性能上优于传统ISAC系统。

Conclusion: 该框架为零TF感知开销的未来ISAC系统提供了可行方案。

Abstract: Low-altitude unmanned aerial vehicles (UAVs) are expected to play an
important role for low-altitude economy with a wide range of applications like
precise agriculture, aerial delivery and surveillance. Integrated sensing and
communication (ISAC) is a key technology to enable the large-scale deployment
and routine usage of UAVs by providing both communication and sensing services
efficiently. For UAV ISAC systems, as UAV often acts as both a communication
user equipment (UE) and a sensing target, traditional ISAC systems that usually
allocate dedicated TF resources for sensing are inefficient due to the severe
degradation of communication spectral efficiency. To address this issue, in
this paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal
frequency division multiplexing (OFDM)-based ISAC framework for UAVs that
eliminates the need for dedicated sensing TF resources, achieving zero TF
sensing overhead. By designing the transmit beamforming to meet the
requirements for both communication and sensing tasks, our proposed approach
enables the communication TF resources to be fully reused for sensing, thereby
enhancing both the communication sum rate and the sensing performance in terms
of resolution, unambiguous range, and accuracy. Additionally, we introduce a
low-complexity target searching beamforming algorithm and a two-stage
super-resolution sensing algorithm, which ensure efficient implementation.
Simulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not
only improves the communication sum rate but also outperforms traditional ISAC
systems in sensing performance, making it a promising solution for future ISAC
systems to support low-altitude UAVs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [12] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukić,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: NanoCodec是一种低帧率音频编解码器，显著提高了语音LLM的训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有音频编解码器帧率高，导致自回归模型训练和推理速度慢，需要低帧率解决方案。

Method: 通过消融研究分析帧率、比特率和因果关系对编解码质量的影响，并开发NanoCodec。

Result: NanoCodec在12.5 FPS下实现高质量压缩，优于现有方法，成为低延迟高效语音LLM的新基准。

Conclusion: NanoCodec为语音LLM提供了一种高效、低延迟的音频处理方案。

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>


### [13] [EchoFree: Towards Ultra Lightweight and Efficient Neural Acoustic Echo Cancellation](https://arxiv.org/abs/2508.06271)
*Xingchen Li,Boyi Kang,Ziqian Wang,Zihan Zhang,Mingshuai Liu,Zhonghua Fu,Lei Xie*

Main category: eess.AS

TL;DR: EchoFree是一种超轻量级神经AEC框架，结合线性滤波和神经后滤波器，通过两阶段优化策略提升性能，在低复杂度和低延迟下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经AEC方法难以在低延迟和低计算复杂度下保持性能，因此需要一种更高效的解决方案。

Method: 提出EchoFree框架，结合线性滤波和基于Bark尺度频谱特征的神经后滤波器，采用两阶段优化策略（利用自监督学习模型）。

Result: 在ICASSP 2023 AEC挑战赛盲测集上，模型仅278K参数和30 MMACs计算复杂度，性能优于现有低复杂度AEC模型，接近DeepVQE-S。

Conclusion: EchoFree在低复杂度和低延迟下实现了高性能，为实际应用提供了可行的解决方案。

Abstract: In recent years, neural networks (NNs) have been widely applied in acoustic
echo cancellation (AEC). However, existing approaches struggle to meet
real-world low-latency and computational requirements while maintaining
performance. To address this challenge, we propose EchoFree, an ultra
lightweight neural AEC framework that combines linear filtering with a neural
post filter. Specifically, we design a neural post-filter operating on
Bark-scale spectral features. Furthermore, we introduce a two-stage
optimization strategy utilizing self-supervised learning (SSL) models to
improve model performance. We evaluate our method on the blind test set of the
ICASSP 2023 AEC Challenge. The results demonstrate that our model, with only
278K parameters and 30 MMACs computational complexity, outperforms existing
low-complexity AEC models and achieves performance comparable to that of
state-of-the-art lightweight model DeepVQE-S. The audio examples are available.

</details>


### [14] [Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment](https://arxiv.org/abs/2508.06284)
*Fredrik Cumlin,Xinyu Liang,Anubhab Ghosh,Saikat Chatterjee*

Main category: eess.AS

TL;DR: 论文提出利用大型语言模型（LLMs）作为语音质量的伪评分者，解决非侵入式语音质量评估（SQA）中数据不足的问题。通过构建LibriAugmented数据集，比较了三种训练策略，发现两阶段方法（预训练+微调）在泛化性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 非侵入式语音质量评估系统因训练数据有限和人工标注成本高而难以泛化到实时会议通话场景。

Method: 利用微调的听觉LLM（Vicuna-7b-v1.5）生成伪标签数据集LibriAugmented，比较三种训练策略：仅用人工标签、仅用LLM标签、两阶段方法（预训练+微调）。

Result: 两阶段方法在多个数据集上表现优于其他方法（如DNSMOS Pro在NISQA_TEST_LIVETALK上PCC为0.63 vs. 0.55）。

Conclusion: LLMs可作为语音质量评估的可扩展伪评分者，提供了一种经济高效的数据限制解决方案。

Abstract: Non-intrusive speech quality assessment (SQA) systems suffer from limited
training data and costly human annotations, hindering their generalization to
real-time conferencing calls. In this work, we propose leveraging large
language models (LLMs) as pseudo-raters for speech quality to address these
data bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129
speech clips with simulated degradations labeled by a fine-tuned auditory LLM
(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled
data, using LLM-labeled data, and a two-stage approach (pretraining on LLM
labels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.
We test on several datasets across languages and quality degradations. While
LLM-labeled training yields mixed results compared to human-labeled training,
we provide empirical evidence that the two-stage approach improves the
generalization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on
NISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings
demonstrate the potential of using LLMs as scalable pseudo-raters for speech
quality assessment, offering a cost-effective solution to the data limitation
problem.

</details>


### [15] [Egonoise Resilient Source Localization and Speech Enhancement for Drones Using a Hybrid Model and Learning-Based Approach](https://arxiv.org/abs/2508.06310)
*Yihsuan Wu,Yukai Chiu,Michael Anthony,Mingsian R. Bai*

Main category: eess.AS

TL;DR: 论文提出了一种结合阵列信号处理和深度神经网络的新技术，用于解决无人机麦克风在极低信噪比条件下的语音信号增强问题。


<details>
  <summary>Details</summary>
Motivation: 无人机在搜索救援和军事行动中日益重要，但其麦克风系统因旋翼噪声导致极低信噪比，语音信号增强技术尚待探索。

Method: 采用六麦克风均匀圆形阵列，结合阵列信号处理和深度神经网络（GSC-DF2系统）进行目标说话人定位和语音增强。

Result: 在低至-30 dB的信噪比条件下，所提混合方法优于四种基线方法。

Conclusion: 该方法显著提升了无人机在极低信噪比环境下的语音信号捕获能力，具有实际应用潜力。

Abstract: Drones are becoming increasingly important in search and rescue missions, and
even military operations. While the majority of drones are equipped with camera
vision capabilities, the realm of drone audition remains underexplored due to
the inherent challenge of mitigating the egonoise generated by the rotors. In
this paper, we present a novel technique to address this extremely low
signal-to-noise ratio (SNR) problem encountered by the microphone-embedded
drones. The technique is implemented using a hybrid approach that combines
Array Signal Processing (ASP) and Deep Neural Networks (DNN) to enhance the
speech signals captured by a six-microphone uniform circular array mounted on a
quadcopter. The system performs localization of the target speaker through
beamsteering in conjunction with speech enhancement through a Generalized
Sidelobe Canceller-DeepFilterNet 2 (GSC-DF2) system. To validate the system,
the DREGON dataset and measured data are employed. Objective evaluations of the
proposed hybrid approach demonstrated its superior performance over four
baseline methods in the SNR condition as low as -30 dB.

</details>


### [16] [Use Cases for Voice Anonymization](https://arxiv.org/abs/2508.06356)
*Sarina Meyer,Ngoc Thang Vu*

Main category: eess.AS

TL;DR: 本文研究了语音匿名化系统在不同用例中的需求差异，提出了首个语音匿名化用例分类法，并基于此提出了设计标准。


<details>
  <summary>Details</summary>
Motivation: 语音匿名化系统的性能评估通常缺乏对具体用例的明确说明，导致设计无法满足多样化需求。

Method: 通过文献分析和用户研究收集用例及公众期望，提出用例分类法，并制定设计标准。

Result: 提出了首个语音匿名化用例分类法，并明确了方法开发和评估的设计标准。

Conclusion: 建议未来研究应更注重用例导向的语音匿名化系统开发。

Abstract: The performance of a voice anonymization system is typically measured
according to its ability to hide the speaker's identity and keep the data's
utility for downstream tasks. This means that the requirements the
anonymization should fulfill depend on the context in which it is used and may
differ greatly between use cases. However, these use cases are rarely specified
in research papers. In this paper, we study the implications of use
case-specific requirements on the design of voice anonymization methods. We
perform an extensive literature analysis and user study to collect possible use
cases and to understand the expectations of the general public towards such
tools. Based on these studies, we propose the first taxonomy of use cases for
voice anonymization, and derive a set of requirements and design criteria for
method development and evaluation. Using this scheme, we propose to focus more
on use case-oriented research and development of voice anonymization systems.

</details>


### [17] [Acoustic Non-Stationarity Objective Assessment with Hard Label Criteria for Supervised Learning Models](https://arxiv.org/abs/2508.06405)
*Guilherme Zucatelli,Ricardo Barioni,Gabriela Dantas*

Main category: eess.AS

TL;DR: 提出了一种新的硬标签标准（HLC）算法，用于生成声学信号的全局非平稳性标签，并通过监督学习训练平稳性估计器。HLC在通用声学模型上验证后，进一步提出了基于HLC的网络（NANSA），其性能优于现有方法，分类准确率达99%，同时解决了传统方法的计算不可行性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的非平稳性测量方法计算资源密集，限制了实时处理的应用。本文旨在提出一种高效且准确的替代方案。

Method: 提出HLC算法生成非平稳性标签，并基于此开发了NANSA网络，用于声学非平稳性评估。

Result: HLC验证了通用声学模型能编码平稳性信息；NANSA分类准确率达99%，解决了传统方法的计算瓶颈。

Conclusion: HLC和NANSA为声学非平稳性评估提供了高效且准确的解决方案，适用于实时处理场景。

Abstract: Objective non-stationarity measures are resource intensive and impose
critical limitations for real-time processing solutions. In this paper, a novel
Hard Label Criteria (HLC) algorithm is proposed to generate a global
non-stationarity label for acoustic signals, enabling supervised learning
strategies to be trained as stationarity estimators. The HLC is first evaluated
on state-of-the-art general-purpose acoustic models, demonstrating that these
models encode stationarity information. Furthermore, the first-of-its-kind
HLC-based Network for Acoustic Non-Stationarity Assessment (NANSA) is proposed.
NANSA models outperform competing approaches, achieving up to 99\%
classification accuracy, while solving the computational infeasibility of
traditional objective measures.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [18] [Training chord recognition models on artificially generated audio](https://arxiv.org/abs/2508.05878)
*Martyna Majchrzak,Jacek Mańdziuk*

Main category: cs.SD

TL;DR: 比较两种基于Transformer的神经网络模型在音频和弦序列识别中的表现，探讨人工生成数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决音乐信息检索中非版权音频数据不足的问题。

Method: 使用人工生成音频（AAM）、舒伯特冬之旅数据集和McGill Billboard数据集训练模型，并通过Root、MajMin和CCM指标评估。

Result: 人工生成音乐虽与人类创作音乐有差异，但在特定场景下仍有效，可作为补充或独立训练集。

Conclusion: 人工生成数据集在缺乏其他数据时，可用于和弦序列预测模型的训练。

Abstract: One of the challenging problems in Music Information Retrieval is the
acquisition of enough non-copyrighted audio recordings for model training and
evaluation. This study compares two Transformer-based neural network models for
chord sequence recognition in audio recordings and examines the effectiveness
of using an artificially generated dataset for this purpose. The models are
trained on various combinations of Artificial Audio Multitracks (AAM),
Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated
with three metrics: Root, MajMin and Chord Content Metric (CCM). The
experiments prove that even though there are certainly differences in
complexity and structure between artificially generated and human-composed
music, the former can be useful in certain scenarios. Specifically, AAM can
enrich a smaller training dataset of music composed by a human or can even be
used as a standalone training set for a model that predicts chord sequences in
pop music, if no other data is available.

</details>


### [19] [DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching](https://arxiv.org/abs/2508.05978)
*Wei Chen,Binzhu Sha,Dan Luo,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu*

Main category: cs.SD

TL;DR: DAFMSVC是一种新的歌唱声音转换方法，通过替换自监督学习特征和引入双交叉注意力机制，显著提升了音色相似性和音频质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有歌唱声音转换方法中音色泄漏和音色相似性不足的问题。

Method: 使用自监督学习特征替换和双交叉注意力机制，结合流匹配模块生成高质量音频。

Result: 实验表明DAFMSVC在音色相似性和自然度上优于现有方法。

Conclusion: DAFMSVC有效解决了音色泄漏问题，提升了转换质量。

Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target
while keeping melody and lyrics. The key challenge in any-to-any SVC is
adapting unseen speaker timbres to source audio without quality degradation.
Existing methods either face timbre leakage or fail to achieve satisfactory
timbre similarity and quality in the generated audio. To address these
challenges, we propose DAFMSVC, where the self-supervised learning (SSL)
features from the source audio are replaced with the most similar SSL features
from the target audio to prevent timbre leakage. It also incorporates a dual
cross-attention mechanism for the adaptive fusion of speaker embeddings,
melody, and linguistic content. Additionally, we introduce a flow matching
module for high quality audio generation from the fused features. Experimental
results show that DAFMSVC significantly enhances timbre similarity and
naturalness, outperforming state-of-the-art methods in both subjective and
objective evaluations.

</details>


### [20] [MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows](https://arxiv.org/abs/2508.06098)
*Xiquan Li,Junxi Liu,Yuzhe Liang,Zhikang Niu,Wenxi Chen,Xie Chen*

Main category: cs.SD

TL;DR: MeanAudio是一种基于MeanFlow的快速文本到音频生成模型，通过回归平均速度场实现高效生成，并在单步生成中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音频生成系统虽然合成质量和可控性优秀，但推理速度慢，限制了实际应用。

Method: 采用Flux-style潜在变换器，结合无分类器引导和瞬时到平均的课程学习策略，优化训练效率和生成质量。

Result: MeanAudio在单步生成中实现0.013的实时因子（RTF），比现有扩散模型快100倍，并在多步生成中表现优异。

Conclusion: MeanAudio通过高效设计和训练策略，显著提升了文本到音频生成的速度和质量，具有广泛的应用潜力。

Abstract: Recent developments in diffusion- and flow- based models have significantly
advanced Text-to-Audio Generation (TTA). While achieving great synthesis
quality and controllability, current TTA systems still suffer from slow
inference speed, which significantly limits their practical applicability. This
paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and
faithful text-to-audio generation. Built on a Flux-style latent transformer,
MeanAudio regresses the average velocity field during training, enabling fast
generation by mapping directly from the start to the endpoint of the flow
trajectory. By incorporating classifier-free guidance (CFG) into the training
target, MeanAudio incurs no additional cost in the guided sampling process. To
further stabilize training, we propose an instantaneous-to-mean curriculum with
flow field mix-up, which encourages the model to first learn the foundational
instantaneous dynamics, and then gradually adapt to mean flows. This strategy
proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art
performance in single-step audio generation. Specifically, it achieves a real
time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup
over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates
strong performance in multi-step generation, enabling smooth and coherent
transitions across successive synthesis steps.

</details>


### [21] [Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis](https://arxiv.org/abs/2508.06262)
*Wenjie Tian,Xinfa Zhu,Hanke Xie,Zhen Ye,Wei Xue,Lei Xie*

Main category: cs.SD

TL;DR: Llasa+是一个基于Llasa的加速和流式TTS模型，通过多令牌预测模块和验证算法实现快速生成，同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 现有自回归结构和大型模型（如Llasa）在推理延迟和流式合成方面存在挑战，Llasa+旨在解决这些问题。

Method: 引入多令牌预测模块加速生成，设计验证算法避免错误传播，并开发因果解码器支持流式语音重建。

Result: 实验显示Llasa+在LibriTTS上训练时实现1.48倍加速且不牺牲生成质量。

Conclusion: Llasa+有效解决了推理延迟和流式合成问题，其框架可推广至其他基于LLM的模型。

Abstract: Recent progress in text-to-speech (TTS) has achieved impressive naturalness
and flexibility, especially with the development of large language model
(LLM)-based approaches. However, existing autoregressive (AR) structures and
large-scale models, such as Llasa, still face significant challenges in
inference latency and streaming synthesis. To deal with the limitations, we
introduce Llasa+, an accelerated and streaming TTS model built on Llasa.
Specifically, to accelerate the generation process, we introduce two
plug-and-play Multi-Token Prediction (MTP) modules following the frozen
backbone. These modules allow the model to predict multiple tokens in one AR
step. Additionally, to mitigate potential error propagation caused by
inaccurate MTP, we design a novel verification algorithm that leverages the
frozen backbone to validate the generated tokens, thus allowing Llasa+ to
achieve speedup without sacrificing generation quality. Furthermore, we design
a causal decoder that enables streaming speech reconstruction from tokens.
Extensive experiments show that Llasa+ achieves a 1.48X speedup without
sacrificing generation quality, despite being trained only on LibriTTS.
Moreover, the MTP-and-verification framework can be applied to accelerate any
LLM-based model. All codes and models are publicly available at
https://github.com/ASLP-lab/LLaSA_Plus.

</details>


### [22] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: EmoAugNet结合LSTM和1D-CNN，通过数据增强和混合建模提升语音情感识别（SER）性能，在两个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互（HCI）效果，通过改进语音情感识别系统的鲁棒性和性能。

Method: 使用混合深度学习框架（LSTM+1D-CNN），结合传统和创新的数据增强策略，提取RMSE、MFCC和ZCR特征。

Result: 在IEMOCAP和RAVDESS数据集上，加权准确率最高达96.75%，未加权准确率最高达94.98%。

Conclusion: EmoAugNet通过数据增强和混合建模显著提升了SER系统的性能。

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


### [23] [SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models](https://arxiv.org/abs/2508.06372)
*Han Yin,Yafeng Chen,Chong Deng,Luyao Cheng,Hui Wang,Chao-Hong Tan,Qian Chen,Wen Wang,Xiangang Li*

Main category: cs.SD

TL;DR: SpeakerLM是一个统一的多模态大型语言模型，用于端到端联合执行说话人分割和语音识别（SDR），解决了传统级联系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统级联SDR系统存在错误传播、难以处理重叠语音以及缺乏联合优化的问题，需要一种更高效的解决方案。

Method: 提出SpeakerLM，通过多阶段训练策略在大规模真实数据上开发，并引入灵活的说话人注册机制。

Result: 实验表明，SpeakerLM在数据扩展和泛化能力上表现优异，优于现有级联基线，且说话人注册机制在不同条件下表现稳健。

Conclusion: SpeakerLM为SDR任务提供了一种高效、统一的解决方案，具有广泛的实际应用潜力。

Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke
when and what" within an audio clip, which is a crucial task in various
real-world multi-speaker scenarios such as meeting transcription and dialogue
systems. Existing SDR systems typically adopt a cascaded framework, combining
multiple modules such as speaker diarization (SD) and automatic speech
recognition (ASR). The cascaded systems suffer from several limitations, such
as error propagation, difficulty in handling overlapping speech, and lack of
joint optimization for exploring the synergy between SD and ASR tasks. To
address these limitations, we introduce SpeakerLM, a unified multimodal large
language model for SDR that jointly performs SD and ASR in an end-to-end
manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a
flexible speaker registration mechanism into SpeakerLM, enabling SDR under
different speaker registration settings. SpeakerLM is progressively developed
with a multi-stage training strategy on large-scale real data. Extensive
experiments show that SpeakerLM demonstrates strong data scaling capability and
generalizability, outperforming state-of-the-art cascaded baselines on both
in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental
results show that the proposed speaker registration mechanism effectively
ensures robust SDR performance of SpeakerLM across diverse speaker registration
conditions and varying numbers of registered speakers.

</details>


### [24] [Improved Dysarthric Speech to Text Conversion via TTS Personalization](https://arxiv.org/abs/2508.06391)
*Péter Mihajlik,Éva Székely,Piroska Barta,Máté Soma Kádár,Gergely Dobsinszki,László Tóth*

Main category: cs.SD

TL;DR: 研究开发了一种针对匈牙利语重度构音障碍患者的定制化语音转文字系统，通过合成语音和真实语音微调ASR模型，显著降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在零样本转录构音障碍语音时错误率高，需改进以提升可访问性。

Method: 利用个性化TTS系统生成合成语音，结合真实语音微调ASR模型，控制构音障碍严重程度。

Result: 字符错误率从36-51%降至7.3%，合成语音贡献了18%的相对CER降低。

Conclusion: 个性化ASR系统对改善重度构音障碍患者的语音可访问性具有潜力。

Abstract: We present a case study on developing a customized speech-to-text system for
a Hungarian speaker with severe dysarthria. State-of-the-art automatic speech
recognition (ASR) models struggle with zero-shot transcription of dysarthric
speech, yielding high error rates. To improve performance with limited real
dysarthric data, we fine-tune an ASR model using synthetic speech generated via
a personalized text-to-speech (TTS) system. We introduce a method for
generating synthetic dysarthric speech with controlled severity by leveraging
premorbidity recordings of the given speaker and speaker embedding
interpolation, enabling ASR fine-tuning on a continuum of impairments.
Fine-tuning on both real and synthetic dysarthric speech reduces the character
error rate (CER) from 36-51% (zero-shot) to 7.3%. Our monolingual
FastConformer_Hu ASR model significantly outperforms Whisper-turbo when
fine-tuned on the same data, and the inclusion of synthetic speech contributes
to an 18% relative CER reduction. These results highlight the potential of
personalized ASR systems for improving accessibility for individuals with
severe speech impairments.

</details>


### [25] [Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling](https://arxiv.org/abs/2508.06393)
*Md Asif Jalal,Luca Remaggi,Vasileios Moschopoulos,Thanasis Kotsiopoulos,Vandana Rajan,Karthikeyan Saravanan,Anastasis Drosou,Junho Heo,Hyuk Oh,Seokyeong Jeong*

Main category: cs.SD

TL;DR: 提出了一种无需预先标注说话人的语音分离和说话人日志方法，通过双阶段训练和重叠频谱损失函数显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖说话人先验信息或固定参与者数量，限制了应用范围。

Method: 采用双阶段训练管道学习抗干扰的说话人表征特征，并设计重叠频谱损失函数优化重叠语音帧的日志准确性。

Result: 实验显示，相对于当前最优基线，DER和cpWER分别提升了71%和69%。

Conclusion: 该方法在无需说话人标注的情况下显著提升了语音分离和日志的性能。

Abstract: Traditional speech separation and speaker diarization approaches rely on
prior knowledge of target speakers or a predetermined number of participants in
audio signals. To address these limitations, recent advances focus on
developing enrollment-free methods capable of identifying targets without
explicit speaker labeling. This work introduces a new approach to train
simultaneous speech separation and diarization using automatic identification
of target speaker embeddings, within mixtures. Our proposed model employs a
dual-stage training pipeline designed to learn robust speaker representation
features that are resilient to background noise interference. Furthermore, we
present an overlapping spectral loss function specifically tailored for
enhancing diarization accuracy during overlapped speech frames. Experimental
results show significant performance gains compared to the current SOTA
baseline, achieving 71% relative improvement in DER and 69% in cpWER.

</details>

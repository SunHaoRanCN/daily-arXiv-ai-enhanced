{"id": "2508.06516", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06516", "abs": "https://arxiv.org/abs/2508.06516", "authors": ["Marine Delabaere", "Léa Miqueu", "Michael Moreno", "Gautier Bigois", "Hoang Duong", "Ella Fernandez", "Flavie Manent", "Maria Salgado-Herrera", "Bastien Pasdeloup", "Nicolas Farrugia", "Axel Marmoret"], "title": "AutoMashup: Automatic Music Mashups Creation", "comment": null, "summary": "We introduce AutoMashup, a system for automatic mashup creation based on\nsource separation, music analysis, and compatibility estimation. We propose\nusing COCOLA to assess compatibility between separated stems and investigate\nwhether general-purpose pretrained audio models (CLAP and MERT) can support\nzero-shot estimation of track pair compatibility. Our results show that mashup\ncompatibility is asymmetric -- it depends on the role assigned to each track\n(vocals or accompaniment) -- and that current embeddings fail to reproduce the\nperceptual coherence measured by COCOLA. These findings underline the\nlimitations of general-purpose audio representations for compatibility\nestimation in mashup creation."}
{"id": "2508.06890", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06890", "abs": "https://arxiv.org/abs/2508.06890", "authors": ["Jinsung Yoon", "Wooyeol Jeong", "Jio Gim", "Young-Joo Suh"], "title": "Maestro-EVC: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody", "comment": "Accepted at ASRU 2025", "summary": "Emotional voice conversion (EVC) aims to modify the emotional style of speech\nwhile preserving its linguistic content. In practical EVC, controllability, the\nability to independently control speaker identity and emotional style using\ndistinct references, is crucial. However, existing methods often struggle to\nfully disentangle these attributes and lack the ability to model fine-grained\nemotional expressions such as temporal dynamics. We propose Maestro-EVC, a\ncontrollable EVC framework that enables independent control of content, speaker\nidentity, and emotion by effectively disentangling each attribute from separate\nreferences. We further introduce a temporal emotion representation and an\nexplicit prosody modeling with prosody augmentation to robustly capture and\ntransfer the temporal dynamics of the target emotion, even under\nprosody-mismatched conditions. Experimental results confirm that Maestro-EVC\nachieves high-quality, controllable, and emotionally expressive speech\nsynthesis."}
{"id": "2508.07048", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07048", "abs": "https://arxiv.org/abs/2508.07048", "authors": ["Taeyoun Kwon", "Junhyuk Ahn", "Taegeun Yun", "Heeju Jwa", "Yoonchae Choi", "Siwon Park", "Nam-Joon Kim", "Jangchan Kim", "Hyun Gon Ryu", "Hyuk-Jae Lee"], "title": "Whisfusion: Parallel ASR Decoding via a Diffusion Transformer", "comment": "16 pages, 9 figures", "summary": "Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive\napplications such as real-time captioning and meeting transcription. However,\ntruly parallel ASR decoding remains challenging due to the sequential nature of\nautoregressive (AR) decoders and the context limitations of non-autoregressive\n(NAR) methods. While modern ASR encoders can process up to 30 seconds of audio\nat once, AR decoders still generate tokens sequentially, creating a latency\nbottleneck. We propose Whisfusion, the first framework to fuse a pre-trained\nWhisper encoder with a text diffusion decoder. This NAR architecture resolves\nthe AR latency bottleneck by processing the entire acoustic context in parallel\nat every decoding step. A lightweight cross-attention adapter trained via\nparameter-efficient fine-tuning (PEFT) bridges the two modalities. We also\nintroduce a batch-parallel, multi-step decoding strategy that improves accuracy\nby increasing the number of candidates with minimal impact on speed. Fine-tuned\nsolely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny\n(8.3% vs. 9.7%), and offers comparable latency on short audio. For longer\nutterances (>20s), it is up to 2.6x faster than the AR baseline, establishing a\nnew, efficient operating point for long-form ASR. The implementation and\ntraining scripts are available at https://github.com/taeyoun811/Whisfusion."}
{"id": "2508.07086", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07086", "abs": "https://arxiv.org/abs/2508.07086", "authors": ["Beilong Tang", "Xiaoxiao Miao", "Xin Wang", "Ming Li"], "title": "SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization", "comment": "8 pages, 3 figures, accepted by 2025 IEEE Automatic Speech\n  Recognition and Understanding Workshop (ASRU)", "summary": "Voice anonymization protects speaker privacy by concealing identity while\npreserving linguistic and paralinguistic content. Self-supervised learning\n(SSL) representations encode linguistic features but preserve speaker traits.\nWe propose a novel speaker-embedding-free framework called SEF-MK. Instead of\nusing a single k-means model trained on the entire dataset, SEF-MK anonymizes\nSSL representations for each utterance by randomly selecting one of multiple\nk-means models, each trained on a different subset of speakers. We explore this\napproach from both attacker and user perspectives. Extensive experiments show\nthat, compared to a single k-means model, SEF-MK with multiple k-means models\nbetter preserves linguistic and emotional content from the user's viewpoint.\nHowever, from the attacker's perspective, utilizing multiple k-means models\nboosts the effectiveness of privacy attacks. These insights can aid users in\ndesigning voice anonymization systems to mitigate attacker threats."}
{"id": "2508.06508", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06508", "abs": "https://arxiv.org/abs/2508.06508", "authors": ["Sameera Bharadwaja H.", "D. K. Mehra"], "title": "A Completely Blind Channel Estimation Technique for OFDM Using Generalized Constellation Splitting and Modified Phase-Directed Algorithm", "comment": null, "summary": "The problem of blind channel estimation for SISO-OFDM systems using\nsecond-order statistics (SOS) is addressed. A comparison of two prominent\nSOS-based techniques: subspace-decomposition and precoding-induced\ncorrelation-averaging techniques in terms of MSE performance is presented. The\ndrawback of these methods is that they suffer from a complex-scalar estimation\nambiguity which is resolved by using pilots or reference symbols. By using\npilots the whole purpose of blind techniques is contradicted. We propose an\nalgorithm to resolve this ambiguity in blind manner using generalized\nconstellation-splitting and modified phase-directed algorithm. The performance\nof the proposed scheme is evaluated via numerical simulations in MATLAB\nenvironment."}
{"id": "2508.06686", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06686", "abs": "https://arxiv.org/abs/2508.06686", "authors": ["Orchisama Das", "Gloria Dal Santo", "Sebastian J. Schlecht", "Vesa Valimaki", "Zoran Cvetkovic"], "title": "Differentiable Grouped Feedback Delay Networks for Learning Coupled Volume Acoustics", "comment": null, "summary": "Rendering dynamic reverberation in a complicated acoustic space for moving\nsources and listeners is challenging but crucial for enhancing user immersion\nin extended-reality (XR) applications. Capturing spatially varying room impulse\nresponses (RIRs) is costly and often impractical. Moreover, dynamic convolution\nwith measured RIRs is computationally expensive with high memory demands,\ntypically not available on wearable computing devices. Grouped Feedback Delay\nNetworks (GFDNs), on the other hand, allow efficient rendering of coupled room\nacoustics. However, its parameters need to be tuned to match the reverberation\nprofile of a coupled space. In this work, we propose the concept of\nDifferentiable GFDNs (DiffGFDNs), which have tunable parameters that are\noptimised to match the late reverberation profile of a set of RIRs captured\nfrom a space that exhibits multi-slope decay. Once trained on a finite set of\nmeasurements, the DiffGFDN generalises to unmeasured locations in the space. We\npropose a parallel processing pipeline that has multiple DiffGFDNs with\nfrequency-independent parameters processing each octave band. The parameters of\nthe DiffGFDN can be updated rapidly during inferencing as sources and listeners\nmove. We evaluate the proposed architecture against the Common Slopes (CS)\nmodel on a dataset of RIRs for three coupled rooms. The proposed architecture\ngenerates multi-slope late reverberation with low memory and computational\nrequirements, achieving better energy decay relief (EDR) error and slightly\nworse octave-band energy decay curve (EDC) errors compared to the CS model.\nFurthermore, DiffGFDN requires an order of magnitude fewer floating-point\noperations per sample than the CS renderer."}
{"id": "2508.07152", "categories": ["cs.SD", "cs.NA", "eess.AS", "math.NA", "physics.ao-ph", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.07152", "abs": "https://arxiv.org/abs/2508.07152", "authors": ["Jinbao Weng", "Yubo Qi", "Yanming Yang", "Hongtao Wen", "Hongtao Zhou", "Benqing Chen", "Dewei Xu", "Ruichao Xue", "Caigao Zeng"], "title": "Inversion of Arctic dual-channel sound speed profile based on random airgun signal", "comment": null, "summary": "For the unique dual-channel sound speed profiles of the Canadian Basin and\nthe Chukchi Plateau in the Arctic, based on the propagation characteristics of\nrefracted normal modes under dual-channel sound speed profiles, an inversion\nmethod using refracted normal modes for dual-channel sound speed profiles is\nproposed. This method proposes a dual-parameter representation method for\ndual-channel sound speed profiles, tailored to the characteristics of\ndual-channel sound speed profiles. A dispersion structure extraction method is\nproposed for the dispersion structure characteristics of refracted normal modes\nunder dual-channel sound speed profiles. Combining the parameter representation\nmethod of sound speed profiles and the dispersion structure extraction method,\nan inversion method for dual-channel sound speed profiles is proposed. For the\ncommon horizontal variation of sound speed profiles in long-distance acoustic\npropagation, a method for inverting horizontally varying dual-channel sound\nspeed profiles is proposed. Finally, this article verifies the effectiveness of\nthe dual-channel sound speed profile inversion method using the Arctic\nlow-frequency long-range acoustic propagation experiment. Compared with\nprevious sound speed profile inversion methods, the method proposed in this\narticle has the advantages of fewer inversion parameters and faster inversion\nspeed. It can be implemented using only a single hydrophone passively receiving\nrandom air gun signals, and it also solves the inversion problem of horizontal\nvariation of sound speed profiles. It has significant advantages such as low\ncost, easy deployment, and fast computation speed."}
{"id": "2508.06672", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06672", "abs": "https://arxiv.org/abs/2508.06672", "authors": ["Jacob S. Clements", "Zachary L. Clements"], "title": "GPU-accelerated Direct Geolocation of GNSS Interference", "comment": null, "summary": "In recent years, there has been a sharp increase in Global Navigation\nSatellite Systems (GNSS) interference, which has proven to be problematic in\nGNSS-dependent civilian applications. Many currently deployed GNSS receivers\nlack the proper countermeasures to defend themselves against interference,\nprompting the need for alternative defenses. Satellites in Low Earth Orbit\n(LEO) provide an opportunity for GNSS interference detection, classification,\nand localization. The direct geolocation approach has been shown to be\nwell-suited for low SNR regimes and in cases limited to short captures --\nexactly what is expected for receivers in LEO. Direct geolocation is a\nsingle-step search over a geographical grid that enables estimation of the\ntransmitter location directly from correlating raw observed signals. However, a\nkey limitation to this approach is the computational requirements. This\ncomputational burden is compounded for LEO-based receivers as the geographic\nsearch space is extensive. This paper alleviates the computational burden of\ndirect geolocation by exploiting the independence of position-domain\ncorrelation across candidate points and time steps: nearly all computation can\nbe accomplished in parallel on a graphics processing unit (GPU). This paper\npresents and evaluates the performance of GPU-accelerated direct geolocation\ncompared to traditional CPU processing."}
{"id": "2508.06840", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06840", "abs": "https://arxiv.org/abs/2508.06840", "authors": ["Seonggyu Lee", "Sein Cheong", "Sangwook Han", "Jong Won Shin"], "title": "FlowSE: Flow Matching-based Speech Enhancement", "comment": "Published in ICASSP 2025", "summary": "Diffusion probabilistic models have shown impressive performance for speech\nenhancement, but they typically require 25 to 60 function evaluations in the\ninference phase, resulting in heavy computational complexity. Recently, a\nfine-tuning method was proposed to correct the reverse process, which\nsignificantly lowered the number of function evaluations (NFE). Flow matching\nis a method to train continuous normalizing flows which model probability paths\nfrom known distributions to unknown distributions including those described by\ndiffusion processes. In this paper, we propose a speech enhancement based on\nconditional flow matching. The proposed method achieved the performance\ncomparable to those for the diffusion-based speech enhancement with the NFE of\n60 when the NFE was 5, and showed similar performance with the diffusion model\ncorrecting the reverse process at the same NFE from 1 to 5 without additional\nfine tuning procedure. We also have shown that the corresponding diffusion\nmodel derived from the conditional probability path with a modified optimal\ntransport conditional vector field demonstrated similar performances with the\nNFE of 5 without any fine-tuning procedure."}
{"id": "2508.07157", "categories": ["cs.SD", "cs.NA", "eess.AS", "math.NA", "physics.ao-ph", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.07157", "abs": "https://arxiv.org/abs/2508.07157", "authors": ["Jinbao Weng", "Yubo Qi", "Yanming Yang", "Hongtao Wen", "Hongtao Zhou", "Benqing Chen", "Dewei Xu", "Ruichao Xue", "Caigao Zeng"], "title": "Acoustic source depth estimation method based on a single hydrophone in Arctic underwater", "comment": null, "summary": "Based on the normal mode and ray theory, this article discusses the\ncharacteristics of surface sound source and reception at the surface layer, and\nexplores depth estimation methods based on normal modes and rays, and proposes\na depth estimation method based on the upper limit of modal frequency. Data\nverification is conducted to discuss the applicability and limitations of\ndifferent methods. For the surface refracted normal mode waveguide, modes can\nbe separated through warping transformation. Based on the characteristics of\nnormal mode amplitude variation with frequency and number, the sound source\ndepth can be estimated by matching amplitude information. Based on the spatial\nvariation characteristics of eigenfunctions with frequency, a sound source\ndepth estimation method matching the cutoff frequency of normal modes is\nproposed. For the deep Arctic sea, the sound ray arrival structure at the\nreceiving end is obtained through the analysis of deep inversion sound ray\ntrajectories, and the sound source depth can be estimated by matching the time\ndifference of ray arrivals. Experimental data is used to verify the sound field\npatterns and the effectiveness of the sound source depth estimation method."}
{"id": "2508.06794", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06794", "abs": "https://arxiv.org/abs/2508.06794", "authors": ["Rui Meng", "Xiaodong Xu", "Bizhu Wang", "Hao Sun", "Shida Xia", "Shujun Han", "Ping Zhang"], "title": "Physical Layer Authentication Based on Hierarchical Variational Auto-Encoder for Industrial Internet of Things", "comment": "17 pages, 13 figures", "summary": "Recently, Physical Layer Authentication (PLA) has attracted much attention\nsince it takes advantage of the channel randomness nature of transmission media\nto achieve communication confidentiality and authentication. In the complex\nenvironment, such as the Industrial Internet of Things (IIoT), machine learning\n(ML) is widely employed with PLA to extract and analyze complex channel\ncharacteristics for identity authentication. However, most PLA schemes for IIoT\nrequire attackers' prior channel information, leading to severe performance\ndegradation when the source of the received signals is unknown in the training\nstage. Thus, a channel impulse response (CIR)-based PLA scheme named\n\"Hierarchical Variational Auto-Encoder (HVAE)\" for IIoT is proposed in this\narticle, aiming at achieving high authentication performance without knowing\nattackers' prior channel information even when trained on a few data in the\ncomplex environment. HVAE consists of an Auto-Encoder (AE) module for CIR\ncharacteristics extraction and a Variational Auto-Encoder (VAE) module for\nimproving the representation ability of the CIR characteristic and outputting\nthe authentication results. Besides, a new objective function is constructed in\nwhich both the single-peak and the double-peak Gaussian distribution are taken\ninto consideration in the VAE module. Moreover, the simulations are conducted\nunder the static and mobile IIoT scenario, which verify the superiority of the\nproposed HVAE over three comparison PLA schemes even with a few training data."}
{"id": "2508.06842", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06842", "abs": "https://arxiv.org/abs/2508.06842", "authors": ["Seonggyu Lee", "Sein Cheong", "Sangwook Han", "Kihyuk Kim", "Jong Won Shi"], "title": "Speech Enhancement based on cascaded two flow", "comment": "Accepted at Interspeech 2025", "summary": "Speech enhancement (SE) based on diffusion probabilistic models has exhibited\nimpressive performance, while requiring a relatively high number of function\nevaluations (NFE). Recently, SE based on flow matching has been proposed, which\nshowed competitive performance with a small NFE. Early approaches adopted the\nnoisy speech as the only conditioning variable. There have been other\napproaches which utilize speech enhanced with a predictive model as another\nconditioning variable and to sample an initial value, but they require a\nseparate predictive model on top of the generative SE model. In this work, we\npropose to employ an identical model based on flow matching for both SE and\ngenerating enhanced speech used as an initial starting point and a conditioning\nvariable. Experimental results showed that the proposed method required the\nsame or fewer NFEs even with two cascaded generative methods while achieving\nequivalent or better performances to the previous baselines."}
{"id": "2508.07176", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07176", "abs": "https://arxiv.org/abs/2508.07176", "authors": ["Yuanjian Chen", "Yang Xiao", "Han Yin", "Yadong Guan", "Xubo Liu"], "title": "Noise-Robust Sound Event Detection and Counting via Language-Queried Sound Separation", "comment": null, "summary": "Most sound event detection (SED) systems perform well on clean datasets but\ndegrade significantly in noisy environments. Language-queried audio source\nseparation (LASS) models show promise for robust SED by separating target\nevents; existing methods require elaborate multi-stage training and lack\nexplicit guidance for target events. To address these challenges, we introduce\nevent appearance detection (EAD), a counting-based approach that counts event\noccurrences at both the clip and frame levels. Based on EAD, we propose a\nco-training-based multi-task learning framework for EAD and SED to enhance\nSED's performance in noisy environments. First, SED struggles to learn the same\npatterns as EAD. Then, a task-based constraint is designed to improve\nprediction consistency between SED and EAD. This framework provides more\nreliable clip-level predictions for LASS models and strengthens timestamp\ndetection capability. Experiments on DESED and WildDESED datasets demonstrate\nbetter performance compared to existing methods, with advantages becoming more\npronounced at higher noise levels."}
{"id": "2508.06829", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06829", "abs": "https://arxiv.org/abs/2508.06829", "authors": ["K. A. Shahriar"], "title": "Deep Domain-Adversarial Adaptation for Automatic Modulation Classification under Channel Variability", "comment": "5 pages, 3 figures", "summary": "Automatic Modulation Classification (AMC) plays a significant role in modern\ncognitive and intelligent radio systems, where accurate identification of\nmodulation is crucial for adaptive communication. The presence of heterogeneous\nwireless channel conditions, such as Rayleigh and Rician fading, poses\nsignificant challenges to the generalization ability of conventional AMC\nmodels. In this work, a domain-adversarial neural network (DANN) based deep\nlearning framework is proposed that explicitly mitigates channel-induced\ndistribution shifts between source and target domains. The approach is\nevaluated using a comprehensive simulated dataset containing five modulation\nschemes (BPSK, QPSK, 16QAM, 64QAM, 256QAM) across Rayleigh and Rician fading\nchannels at five frequency bands. Comparative experiments demonstrate that the\nDANN-based model achieves up to 14.93% absolute accuracy improvement in certain\nmodulation cases compared to a baseline supervised model trained solely on the\nsource domain. The findings establish the engineering feasibility of domain\nadversarial learning in AMC tasks under real-world channel variability and\noffer a robust direction for future research in adaptive spectrum intelligence"}
{"id": "2508.06928", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06928", "abs": "https://arxiv.org/abs/2508.06928", "authors": ["Vasudha Sathyapriyan", "Michael S. Pedersen", "Mike Brookes", "Jan Østergaard", "Patrick A. Naylor", "Jesper Jensen"], "title": "Head-steered channel selection method for hearing aid applications using remote microphones", "comment": "11 pages, 8 figures", "summary": "We propose a channel selection method for hearing aid applications using\nremote microphones, in the presence of multiple competing talkers. The proposed\nchannel selection method uses the hearing aid user's head-steering direction to\nidentify the remote channel originating from the frontal direction of the\nhearing aid user, which captures the target talker signal. We pose the channel\nselection task as a multiple hypothesis testing problem, and derive a maximum\nlikelihood solution. Under realistic, simplifying assumptions, the solution\nselects the remote channel which has the highest weighted squared absolute\ncorrelation coefficient with the output of the head-steered hearing aid\nbeamformer. We analyze the performance of the proposed channel selection method\nusing close-talking remote microphones and table microphone arrays. Through\nsimulations using realistic acoustic scenes, we show that the proposed channel\nselection method consistently outperforms existing methods in accurately\nfinding the remote channel that captures the target talker signal, in the\npresence of multiple competing talkers, without the use of any additional\nsensors."}
{"id": "2508.07363", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07363", "abs": "https://arxiv.org/abs/2508.07363", "authors": ["Hanyu Ding", "Wenlong Dong", "Qirong Mao"], "title": "Keyword Mamba: Spoken Keyword Spotting with State Space Models", "comment": "Under peer review", "summary": "Keyword spotting (KWS) is an essential task in speech processing. It is\nwidely used in voice assistants and smart devices. Deep learning models like\nCNNs, RNNs, and Transformers have performed well in KWS. However, they often\nstruggle to handle long-term patterns and stay efficient at the same time. In\nthis work, we present Keyword Mamba, a new architecture for KWS. It uses a\nneural state space model (SSM) called Mamba. We apply Mamba along the time axis\nand also explore how it can replace the self-attention part in Transformer\nmodels. We test our model on the Google Speech Commands datasets. The results\nshow that Keyword Mamba reaches strong accuracy with fewer parameters and lower\ncomputational cost. To our knowledge, this is the first time a state space\nmodel has been used for KWS. These results suggest that Mamba has strong\npotential in speech-related tasks."}
{"id": "2508.06868", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06868", "abs": "https://arxiv.org/abs/2508.06868", "authors": ["Bin Lyu", "Jiayu Guan", "Meng Hua", "Changsheng You", "Tianqi Mao", "Abbas Jamalipour"], "title": "Secure Transmission for Cell-Free Symbiotic Radio Communications with Movable Antenna: Continuous and Discrete Positioning Designs", "comment": "14 pages,6 figures", "summary": "In this paper, we study a movable antenna (MA) empowered secure transmission\nscheme for reconfigurable intelligent surface (RIS) aided cell-free symbiotic\nradio (SR) system. Specifically, the MAs deployed at distributed access points\n(APs) work collaboratively with the RIS to establish high-quality propagation\nlinks for both primary and secondary transmissions, as well as suppressing the\nrisk of eavesdropping on confidential primary information. We consider both\ncontinuous and discrete MA position cases and maximize the secrecy rate of\nprimary transmission under the secondary transmission constraints,\nrespectively. For the continuous position case, we propose a two-layer\niterative optimization method based on differential evolution with one-in-one\nrepresentation (DEO), to find a high-quality solution with relatively moderate\ncomputational complexity. For the discrete position case, we first extend the\nDEO based iterative framework by introducing the mapping and determination\noperations to handle the characteristic of discrete MA positions. To further\nreduce the computational complexity, we then design an alternating optimization\n(AO) iterative framework to solve all variables within a single layer. In\nparticular, we develop an efficient strategy to derive the sub-optimal solution\nfor the discrete MA positions, superseding the DEO-based method. Numerical\nresults validate the effectiveness of the proposed MA empowered secure\ntransmission scheme along with its optimization algorithms."}
{"id": "2508.07014", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07014", "abs": "https://arxiv.org/abs/2508.07014", "authors": ["Andrei Andrusenko", "Vladimir Bataev", "Lilit Grigoryan", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree", "comment": "Accepted by ASRU 2025", "summary": "Recognizing specific key phrases is an essential task for contextualized\nAutomatic Speech Recognition (ASR). However, most existing context-biasing\napproaches have limitations associated with the necessity of additional model\ntraining, significantly slow down the decoding process, or constrain the choice\nof the ASR system type. This paper proposes a universal ASR context-biasing\nframework that supports all major types: CTC, Transducers, and Attention\nEncoder-Decoder models. The framework is based on a GPU-accelerated word\nboosting tree, which enables it to be used in shallow fusion mode for greedy\nand beam search decoding without noticeable speed degradation, even with a vast\nnumber of key phrases (up to 20K items). The obtained results showed high\nefficiency of the proposed method, surpassing the considered open-source\ncontext-biasing approaches in accuracy and decoding speed. Our context-biasing\nframework is open-sourced as a part of the NeMo toolkit."}
{"id": "2508.07561", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07561", "abs": "https://arxiv.org/abs/2508.07561", "authors": ["Yiheng Jiang", "Tian Biao"], "title": "A Small-footprint Acoustic Echo Cancellation Solution for Mobile Full-Duplex Speech Interactions", "comment": "This paper is accepted to ICASSP 2025", "summary": "In full-duplex speech interaction systems, effective Acoustic Echo\nCancellation (AEC) is crucial for recovering echo-contaminated speech. This\npaper presents a neural network-based AEC solution to address challenges in\nmobile scenarios with varying hardware, nonlinear distortions and long latency.\nWe first incorporate diverse data augmentation strategies to enhance the\nmodel's robustness across various environments. Moreover, progressive learning\nis employed to incrementally improve AEC effectiveness, resulting in a\nconsiderable improvement in speech quality. To further optimize AEC's\ndownstream applications, we introduce a novel post-processing strategy\nemploying tailored parameters designed specifically for tasks such as Voice\nActivity Detection (VAD) and Automatic Speech Recognition (ASR), thus enhancing\ntheir overall efficacy. Finally, our method employs a small-footprint model\nwith streaming inference, enabling seamless deployment on mobile devices.\nEmpirical results demonstrate effectiveness of the proposed method in Echo\nReturn Loss Enhancement and Perceptual Evaluation of Speech Quality, alongside\nsignificant improvements in both VAD and ASR results."}
{"id": "2508.06952", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06952", "abs": "https://arxiv.org/abs/2508.06952", "authors": ["Haiyang Zhang", "Nir Shlezinger", "Giulia Torcolacci", "Francesco Guidi", "Anna Guerra", "Qianyu Yang", "Mohammadreza F. Imani", "Davide Dardari", "Yonina C. Eldar"], "title": "Extremely Large-Scale Dynamic Metasurface Antennas for 6G Near-Field Networks: Opportunities and Challenges", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "6G networks will need to support higher data rates, high-precision\nlocalization, and imaging capabilities. Near-field technologies, enabled by\nextremely large-scale (XL)-arrays, are expected to be essential physical-layer\nsolutions to meet these ambitious requirements. However, implementing XL-array\nsystems using traditional fully-digital or hybrid analog/digital architectures\nposes significant challenges due to high power consumption and implementation\ncosts. Emerging XL-dynamic metasurface antennas (XL-DMAs) provide a promising\nalternative, enabling ultra-low power and cost-efficient solutions, making them\nideal candidates for 6G near-field networks. In this article, we discuss the\nopportunities and challenges of XL-DMAs employed in 6G near-field networks. We\nfirst outline the fundamental principles of XL-DMAs and present the specifics\nof the near-field model of XL-DMAs. We then highlight several promising\napplications that might benefit from XL-DMAs, including near-field\ncommunication, localization, and imaging. Finally, we discuss several open\nproblems and potential future directions that should be addressed to fully\nexploit the capabilities of XL-DMAs in the next 6G near-field networks."}
{"id": "2508.07219", "categories": ["eess.AS", "cs.SD", "I.2.7; H.5.5; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.07219", "abs": "https://arxiv.org/abs/2508.07219", "authors": ["Minu Kim", "Kangwook Jang", "Hoirin Kim"], "title": "ParaNoise-SV: Integrated Approach for Noise-Robust Speaker Verification with Parallel Joint Learning of Speech Enhancement and Noise Extraction", "comment": "5 pages, 3 figures, accepted to Interspeech 2025", "summary": "Noise-robust speaker verification leverages joint learning of speech\nenhancement (SE) and speaker verification (SV) to improve robustness. However,\nprevailing approaches rely on implicit noise suppression, which struggles to\nseparate noise from speaker characteristics as they do not explicitly\ndistinguish noise from speech during training. Although integrating SE and SV\nhelps, it remains limited in handling noise effectively. Meanwhile, recent SE\nstudies suggest that explicitly modeling noise, rather than merely suppressing\nit, enhances noise resilience. Reflecting this, we propose ParaNoise-SV, with\ndual U-Nets combining a noise extraction (NE) network and a speech enhancement\n(SE) network. The NE U-Net explicitly models noise, while the SE U-Net refines\nspeech with guidance from NE through parallel connections, preserving\nspeaker-relevant features. Experimental results show that ParaNoise-SV achieves\na relatively 8.4% lower equal error rate (EER) than previous joint SE-SV\nmodels."}
{"id": "2508.07563", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07563", "abs": "https://arxiv.org/abs/2508.07563", "authors": ["Yiheng Jiang", "Haoxu Wang", "Yafeng Chen", "Gang Qiao", "Biao Tian"], "title": "Exploring Efficient Directional and Distance Cues for Regional Speech Separation", "comment": "This paper has been accepted by Interspeech 2025", "summary": "In this paper, we introduce a neural network-based method for regional speech\nseparation using a microphone array. This approach leverages novel spatial cues\nto extract the sound source not only from specified direction but also within\ndefined distance. Specifically, our method employs an improved delay-and-sum\ntechnique to obtain directional cues, substantially enhancing the signal from\nthe target direction. We further enhance separation by incorporating the\ndirect-to-reverberant ratio into the input features, enabling the model to\nbetter discriminate sources within and beyond a specified distance.\nExperimental results demonstrate that our proposed method leads to substantial\ngains across multiple objective metrics. Furthermore, our method achieves\nstate-of-the-art performance on the CHiME-8 MMCSG dataset, which was recorded\nin real-world conversational scenarios, underscoring its effectiveness for\nspeech separation in practical applications."}
{"id": "2508.06958", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06958", "abs": "https://arxiv.org/abs/2508.06958", "authors": ["Xin Cheng", "Guangjie Han", "Menglu Li", "Ruoguang Li", "Feng Shu"], "title": "Millimeter-Wave Position Sensing Using Reconfigurable Intelligent Surfaces: Positioning Error Bound and Phase Shift Configuration", "comment": null, "summary": "Millimeter-wave (mmWave) positioning has emerged as a promising technology\nfor next-generation intelligent systems. The advent of reconfigurable\nintelligent surfaces (RISs) has revolutionized high-precision mmWave\nlocalization by enabling dynamic manipulation of wireless propagation\nenvironments. This paper investigates a three-dimensional (3D) multi-input\nsingle-output (MISO) mmWave positioning system assisted by multiple RISs. We\nintroduce a measurement framework incorporating sequential RIS activation and\ndirectional beamforming to fully exploit virtual line-of-sight (VLoS) paths.\nThe theoretical performance limits are rigorously analyzed through derivation\nof the Fisher information and subsequent positioning error bound (PEB). To\nminimize the PEB, two distinct optimization approaches are proposed for\ncontinuous and discrete phase shift configurations of RISs. For continuous\nphase shifts, a Riemannian manifold-based optimization algorithm is proposed.\nFor discrete phase shifts, a heuristic algorithm incorporating the grey wolf\noptimizer is proposed. Extensive numerical simulations demonstrate the\neffectiveness of the proposed algorithms in reducing the PEB and validate the\nimprovement in positioning accuracy achieved by multiple RISs."}
{"id": "2508.07282", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07282", "abs": "https://arxiv.org/abs/2508.07282", "authors": ["Jing-Tong Tzeng", "Bo-Hao Su", "Ya-Tse Wu", "Hsing-Hang Chou", "Chi-Chun Lee"], "title": "Lessons Learnt: Revisit Key Training Strategies for Effective Speech Emotion Recognition in the Wild", "comment": "Accepted to Interspeech 2025", "summary": "In this study, we revisit key training strategies in machine learning often\noverlooked in favor of deeper architectures. Specifically, we explore balancing\nstrategies, activation functions, and fine-tuning techniques to enhance speech\nemotion recognition (SER) in naturalistic conditions. Our findings show that\nsimple modifications improve generalization with minimal architectural changes.\nOur multi-modal fusion model, integrating these optimizations, achieves a\nvalence CCC of 0.6953, the best valence score in Task 2: Emotional Attribute\nRegression. Notably, fine-tuning RoBERTa and WavLM separately in a\nsingle-modality setting, followed by feature fusion without training the\nbackbone extractor, yields the highest valence performance. Additionally, focal\nloss and activation functions significantly enhance performance without\nincreasing complexity. These results suggest that refining core components,\nrather than deepening models, leads to more robust SER in-the-wild."}
{"id": "2508.07751", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07751", "abs": "https://arxiv.org/abs/2508.07751", "authors": ["Zhanhong He", "David Cooper", "Defeng Huang", "Roberto Togneri"], "title": "Filling MIDI Velocity using U-Net Image Colorizer", "comment": "12 pages, submitted to CMMR2025 conference", "summary": "Modern music producers commonly use MIDI (Musical Instrument Digital\nInterface) to store their musical compositions. However, MIDI files created\nwith digital software may lack the expressive characteristics of human\nperformances, essentially leaving the velocity parameter - a control for note\nloudness - undefined, which defaults to a flat value. The task of filling MIDI\nvelocity is termed MIDI velocity prediction, which uses regression models to\nenhance music expressiveness by adjusting only this parameter. In this paper,\nwe introduce the U-Net, a widely adopted architecture in image colorization, to\nthis task. By conceptualizing MIDI data as images, we adopt window attention\nand develop a custom loss function to address the sparsity of MIDI-converted\nimages. Current dataset availability restricts our experiments to piano data.\nEvaluated on the MAESTRO v3 and SMD datasets, our proposed method for filling\nMIDI velocity outperforms previous approaches in both quantitative metrics and\nqualitative listening tests."}
{"id": "2508.07002", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07002", "abs": "https://arxiv.org/abs/2508.07002", "authors": ["Ze Wang", "Guoping Zhang", "Hongbo Xu"], "title": "Joint Beamforming Optimization for Pinching-Antenna Systems (PASS)-assisted Symbiotic Radio", "comment": null, "summary": "This paper investigates a novel downlink symbiotic radio (SR) framework\nempowered by the pinching antenna system (PASS), aiming to enhance both primary\nand secondary transmissions through reconfigurable antenna positioning. PASS\nconsists of multiple waveguides equipped with numerous low-cost pinching\nantennas (PAs), whose positions can be flexibly adjusted to simultaneously\nmanipulate large-scale path loss and signal phases.We formulate a joint\ntransmit and pinching beamforming optimization problem to maximize the\nachievable sum rate while satisfying the detection error probability constraint\nfor the IR and the feasible deployment region constraints for the PAs. This\nproblem is inherently nonconvex and highly coupled. To address it, two solution\nstrategies are developed. 1) A learning-aided gradient descent (LGD) algorithm\nis proposed, where the constrained problem is reformulated into a\ndifferentiable form and solved through end-to-end learning based on the\nprinciple of gradient descent. The PA position matrix is reparameterized to\ninherently satisfy minimum spacing constraints, while transmit power and\nwaveguide length limits are enforced via projection and normalization. 2) A\ntwo-stage optimization-based approach is designed, in which the transmit\nbeamforming is first optimized via successive convex approximation (SCA),\nfollowed by pinching beamforming optimization using a particle swarm\noptimization (PSO) search over candidate PA placements. The SCA-PSO algorithm\nachieves performance close to that of the element-wise method while\nsignificantly reducing computational complexity by exploring a randomly\ngenerated effective solution subspace, while further improving upon the LGD\nmethod by avoiding undesirable local optima."}
{"id": "2508.07285", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07285", "abs": "https://arxiv.org/abs/2508.07285", "authors": ["Mohammad Reza Peyghan", "Fatemeh Rajabi", "Saman Soleimani Roudi", "Saeedreza Zouashkiani", "Sajjad Amini", "Shahrokh Ghaemmaghami"], "title": "A Survey on Non-Intrusive ASR Refinement: From Output-Level Correction to Full-Model Distillation", "comment": null, "summary": "Automatic Speech Recognition (ASR) has become an integral component of modern\ntechnology, powering applications such as voice-activated assistants,\ntranscription services, and accessibility tools. Yet ASR systems continue to\nstruggle with the inherent variability of human speech, such as accents,\ndialects, and speaking styles, as well as environmental interference, including\nbackground noise. Moreover, domain-specific conversations often employ\nspecialized terminology, which can exacerbate transcription errors. These\nshortcomings not only degrade raw ASR accuracy but also propagate mistakes\nthrough subsequent natural language processing pipelines. Because redesigning\nan ASR model is costly and time-consuming, non-intrusive refinement techniques\nthat leave the model's architecture unchanged have become increasingly popular.\nIn this survey, we systematically review current non-intrusive refinement\napproaches and group them into five classes: fusion, re-scoring, correction,\ndistillation, and training adjustment. For each class, we outline the main\nmethods, advantages, drawbacks, and ideal application scenarios. Beyond method\nclassification, this work surveys adaptation techniques aimed at refining ASR\nin domain-specific contexts, reviews commonly used evaluation datasets along\nwith their construction processes, and proposes a standardized set of metrics\nto facilitate fair comparisons. Finally, we identify open research gaps and\nsuggest promising directions for future work. By providing this structured\noverview, we aim to equip researchers and practitioners with a clear foundation\nfor developing more robust, accurate ASR refinement pipelines."}
{"id": "2508.07944", "categories": ["cs.SD", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.07944", "abs": "https://arxiv.org/abs/2508.07944", "authors": ["Vojtěch Staněk", "Karel Srna", "Anton Firc", "Kamil Malinka"], "title": "SCDF: A Speaker Characteristics DeepFake Speech Dataset for Bias Analysis", "comment": null, "summary": "Despite growing attention to deepfake speech detection, the aspects of bias\nand fairness remain underexplored in the speech domain. To address this gap, we\nintroduce the Speaker Characteristics Deepfake (SCDF) dataset: a novel, richly\nannotated resource enabling systematic evaluation of demographic biases in\ndeepfake speech detection. SCDF contains over 237,000 utterances in a balanced\nrepresentation of both male and female speakers spanning five languages and a\nwide age range. We evaluate several state-of-the-art detectors and show that\nspeaker characteristics significantly influence detection performance,\nrevealing disparities across sex, language, age, and synthesizer type. These\nfindings highlight the need for bias-aware development and provide a foundation\nfor building non-discriminatory deepfake detection systems aligned with ethical\nand regulatory standards."}
{"id": "2508.07013", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07013", "abs": "https://arxiv.org/abs/2508.07013", "authors": ["Yufan Zhou", "Jingyi Li", "Wenkang Xu", "An Liu"], "title": "Robust Super-Resolution Compressive Sensing: A Two-timescale Alternating MAP Approach", "comment": null, "summary": "The problem of super-resolution compressive sensing (SR-CS) is crucial for\nvarious wireless sensing and communication applications. Existing methods often\nsuffer from limited resolution capabilities and sensitivity to\nhyper-parameters, hindering their ability to accurately recover sparse signals\nwhen the grid parameters do not lie precisely on a fixed grid and are close to\neach other. To overcome these limitations, this paper introduces a novel robust\nsuper-resolution compressive sensing algorithmic framework using a\ntwo-timescale alternating maximum a posteriori (MAP) approach. At the slow\ntimescale, the proposed framework iterates between a sparse signal estimation\nmodule and a grid update module. In the sparse signal estimation module, a\nhyperbolic-tangent prior distribution based variational Bayesian inference\n(tanh-VBI) algorithm with a strong sparsity promotion capability is adopted to\nestimate the posterior probability of the sparse vector and accurately identify\nactive grid components carrying primary energy under a dense grid.\nSubsequently, the grid update module utilizes the BFGS algorithm to refine\nthese low-dimensional active grid components at a faster timescale to achieve\nsuper-resolution estimation of the grid parameters with a low computational\ncost. The proposed scheme is applied to the channel extrapolation problem, and\nsimulation results demonstrate the superiority of the proposed scheme compared\nto baseline schemes."}
{"id": "2508.07302", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07302", "abs": "https://arxiv.org/abs/2508.07302", "authors": ["Tianlun Zuo", "Jingbin Hu", "Yuke Li", "Xinfa Zhu", "Hai Li", "Ying Yan", "Junhui Liu", "Danming Xie", "Lei Xie"], "title": "XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity Using Retrieval-Augmented Generation", "comment": null, "summary": "Zero-shot emotion transfer in cross-lingual speech synthesis refers to\ngenerating speech in a target language, where the emotion is expressed based on\nreference speech from a different source language.However, this task remains\nchallenging due to the scarcity of parallel multilingual emotional corpora, the\npresence of foreign accent artifacts, and the difficulty of separating emotion\nfrom language-specific prosodic features.In this paper, we propose XEmoRAG, a\nnovel framework to enable zero-shot emotion transfer from Chinese to Thai using\na large language model (LLM)-based model, without relying on parallel emotional\ndata.XEmoRAG extracts language-agnostic emotional embeddings from Chinese\nspeech and retrieves emotionally matched Thai utterances from a curated\nemotional database, enabling controllable emotion transfer without explicit\nemotion labels. Additionally, a flow-matching alignment module minimizes pitch\nand duration mismatches, ensuring natural prosody. It also blends Chinese\ntimbre into the Thai synthesis, enhancing rhythmic accuracy and emotional\nexpression, while preserving speaker characteristics and emotional\nconsistency.Experimental results show that XEmoRAG synthesizes expressive and\nnatural Thai speech using only Chinese reference audio, without requiring\nexplicit emotion labels.These results highlight XEmoRAG's capability to achieve\nflexible and low-resource emotional transfer across languages.Our demo is\navailable at https://tlzuo-lesley.github.io/Demo-page/."}
{"id": "2508.07973", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07973", "abs": "https://arxiv.org/abs/2508.07973", "authors": ["Sebastian Murgul", "Johannes Schimper", "Michael Heizmann"], "title": "Joint Transcription of Acoustic Guitar Strumming Directions and Chords", "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "Automatic transcription of guitar strumming is an underrepresented and\nchallenging task in Music Information Retrieval (MIR), particularly for\nextracting both strumming directions and chord progressions from audio signals.\nWhile existing methods show promise, their effectiveness is often hindered by\nlimited datasets. In this work, we extend a multimodal approach to guitar\nstrumming transcription by introducing a novel dataset and a deep\nlearning-based transcription model. We collect 90 min of real-world guitar\nrecordings using an ESP32 smartwatch motion sensor and a structured recording\nprotocol, complemented by a synthetic dataset of 4h of labeled strumming audio.\nA Convolutional Recurrent Neural Network (CRNN) model is trained to detect\nstrumming events, classify their direction, and identify the corresponding\nchords using only microphone audio. Our evaluation demonstrates significant\nimprovements over baseline onset detection algorithms, with a hybrid method\ncombining synthetic and real-world data achieving the highest accuracy for both\nstrumming action detection and chord classification. These results highlight\nthe potential of deep learning for robust guitar strumming transcription and\nopen new avenues for automatic rhythm guitar analysis."}
{"id": "2508.07131", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07131", "abs": "https://arxiv.org/abs/2508.07131", "authors": ["Yanqing Xu", "Zhiguo Ding", "Octavia A. Dobre", "Tsung-Hui Chang"], "title": "Pinching-Antenna System Design with LoS Blockage: Does In-Waveguide Attenuation Matter?", "comment": "14 pages, 6 figures", "summary": "In the literature of pinching-antenna systems, in-waveguide attenuation is\noften neglected to simplify system design and enable more tractable analysis.\nHowever, its effect on overall system performance has received limited\nattention in the existing literature. While a recent study has shown that, in\nline-of-sight (LoS)-dominated environments, the data rate loss incurred by\nomitting in-waveguide attenuation is negligible when the communication area is\nnot excessively large, its effect under more general conditions remains\nunclear. This work extends the analysis to more realistic scenarios involving\narbitrary levels of LoS blockage. We begin by examining a single-user case and\nderive an explicit expression for the average data rate loss caused by\nneglecting in-waveguide attenuation. The results demonstrate that, even for\nlarge service areas, the rate loss remains negligible under typical LoS\nblockage conditions. We then consider a more general multi-user scenario, where\nmultiple pinching antennas, each deployed on a separate waveguide, jointly\nserve multiple users. The objective is to maximize the average sum rate by\njointly optimize antenna positions and transmit beamformers to maximize the\naverage sum rate under probabilistic LoS blockage. To solve the resulting\nstochastic and nonconvex optimization problem, we propose a dynamic sample\naverage approximation (SAA) algorithm. At each iteration, this method replaces\nthe expected objective with an empirical average computed from dynamically\nregenerated random channel realizations, ensuring that the optimization\naccurately reflects the current antenna configuration. Extensive simulation\nresults are provided to the proposed algorithm and demonstrate the substantial\nperformance gains of pinching-antenna systems, particularly in environments\nwith significant LoS blockage."}
{"id": "2508.07315", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07315", "abs": "https://arxiv.org/abs/2508.07315", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Nikolay Karpov", "Andrei Andrusenko", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities", "comment": "Accepted to Automatic Speech Recognition and Understanding Workshop\n  (ASRU) 2025", "summary": "While beam search improves speech recognition quality over greedy decoding,\nstandard implementations are slow, often sequential, and CPU-bound. To fully\nleverage modern hardware capabilities, we present a novel open-source FlexCTC\ntoolkit for fully GPU-based beam decoding, designed for Connectionist Temporal\nClassification (CTC) models. Developed entirely in Python and PyTorch, it\noffers a fast, user-friendly, and extensible alternative to traditional C++,\nCUDA, or WFST-based decoders. The toolkit features a high-performance, fully\nbatched GPU implementation with eliminated CPU-GPU synchronization and\nminimized kernel launch overhead via CUDA Graphs. It also supports advanced\ncontextualization techniques, including GPU-powered N-gram language model\nfusion and phrase-level boosting. These features enable accurate and efficient\ndecoding, making them suitable for both research and production use."}
{"id": "2508.07987", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07987", "abs": "https://arxiv.org/abs/2508.07987", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription", "comment": "Accepted to the 6th Conference on AI Music Creativity (AIMC), 2025", "summary": "Automatic transcription of acoustic guitar fingerpicking performances remains\na challenging task due to the scarcity of labeled training data and legal\nconstraints connected with musical recordings. This work investigates a\nprocedural data generation pipeline as an alternative to real audio recordings\nfor training transcription models. Our approach synthesizes training data\nthrough four stages: knowledge-based fingerpicking tablature composition, MIDI\nperformance rendering, physical modeling using an extended Karplus-Strong\nalgorithm, and audio augmentation including reverb and distortion. We train and\nevaluate a CRNN-based note-tracking model on both real and synthetic datasets,\ndemonstrating that procedural data can be used to achieve reasonable\nnote-tracking results. Finetuning with a small amount of real data further\nenhances transcription accuracy, improving over models trained exclusively on\nreal recordings. These results highlight the potential of procedurally\ngenerated audio for data-scarce music information retrieval tasks."}
{"id": "2508.07148", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07148", "abs": "https://arxiv.org/abs/2508.07148", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Saif Khan Mohammed", "Venkatesh Khammammetti", "Robert Calderbank"], "title": "Low-Complexity Equalization of Zak-OTFS in the Frequency Domain", "comment": "13 pages, 12 figures. Submitted to npj Wireless Technology", "summary": "4G/5G wireless standards use orthogonal frequency division multiplexing\n(OFDM) which is robust to frequency selectivity. Equalization is possible with\na single tap filter, and low-complexity equalization makes OFDM an attractive\nphysical layer. However the performance of OFDM degrades with mobility, since\nDoppler spreads introduce inter-carrier interference (ICI) between subcarriers\nand they are no longer orthogonal. Zak-transform based orthogonal time\nfrequency space (Zak-OTFS) modulation has been shown to be robust to doubly\nselective channels. Zak-OTFS signals are formed in the delay-Doppler (DD)\ndomain, converted to time domain (TD) for transmission and reception, then\nreturned to the DD domain for processing. The received signal is a\nsuperposition of many attenuated copies since the doubly selective channel\nintroduces delay and Doppler shifts. The received symbols are more difficult to\nequalize since they are subject to interference along both delay and Doppler\naxes. In this paper, we propose a new low-complexity method of equalizing\nZak-OTFS in the frequency domain (FD). We derive the FD system model and show\nthat it is unitarily equivalent to the DD system model. We show that the\nchannel matrix in the FD is banded, making it possible to apply conjugate\ngradient methods to reduce the complexity of equalization. We show that\ncomplexity of FD equalization is linear in the dimension of a Zak-OTFS frame.\nFor comparison the complexity of naive MMSE equalization is cubic in the frame\ndimension. Through numerical simulations we show that FD equalization of\nZak-OTFS achieves similar performance as equalization in DD domain."}
{"id": "2508.07337", "categories": ["eess.AS", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07337", "abs": "https://arxiv.org/abs/2508.07337", "authors": ["Ivan Kukanov", "Jun Wah Ng"], "title": "KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features", "comment": "7 pages, accepted to the 33rd ACM International Conference on\n  Multimedia (MM'25)", "summary": "The rapid development of audio-driven talking head generators and advanced\nText-To-Speech (TTS) models has led to more sophisticated temporal deepfakes.\nThese advances highlight the need for robust methods capable of detecting and\nlocalizing deepfakes, even under novel, unseen attack scenarios. Current\nstate-of-the-art deepfake detectors, while accurate, are often computationally\nexpensive and struggle to generalize to novel manipulation techniques. To\naddress these challenges, we propose multimodal approaches for the\nAV-Deepfake1M 2025 challenge. For the visual modality, we leverage handcrafted\nfeatures to improve interpretability and adaptability. For the audio modality,\nwe adapt a self-supervised learning (SSL) backbone coupled with graph attention\nnetworks to capture rich audio representations, improving detection robustness.\nOur approach strikes a balance between performance and real-world deployment,\nfocusing on resilience and potential interpretability. On the AV-Deepfake1M++\ndataset, our multimodal system achieves AUC of 92.78% for deepfake\nclassification task and IoU of 0.3536 for temporal localization using only the\naudio modality."}
{"id": "2508.08027", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08027", "abs": "https://arxiv.org/abs/2508.08027", "authors": ["Ahmed Aboeitta", "Ahmed Sharshar", "Youssef Nafea", "Shady Shehata"], "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches", "comment": null, "summary": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction."}
{"id": "2508.07160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07160", "abs": "https://arxiv.org/abs/2508.07160", "authors": ["Deyu Lu", "Xiaoli Ma", "Yiyin Wang"], "title": "Vector Orthogonal Chirp Division Multiplexing Over Doubly Selective Channels", "comment": null, "summary": "In this letter, we extend orthogonal chirp division multiplexing (OCDM) to\nvector OCDM (VOCDM) to provide more design freedom to deal with doubly\nselective channels. The VOCDM modulation is implemented by performing M\nparallel N-size inverse discrete Fresnel transforms (IDFnT). Based on the\ncomplex exponential basis expansion model (CE-BEM) for doubly selective\nchannels, we derive the VOCDM input-output relationship, and show performance\ntradeoffs of VOCDM with respect to (w.r.t.) its modulation parameters M and N.\nSpecifically, we investigate the diversity and peak-to-average power ratio\n(PAPR) of VOCDM w.r.t. M and N. Under doubly selective channels, VOCDM exhibits\nsuperior diversity performance as long as the parameters M and N are configured\nto satisfy some constraints from the delay and the Doppler spreads of the\nchannel, respectively. Furthermore, the PAPR of VOCDM signals decreases with a\ndecreasing N. These theoretical findings are verified through numerical\nsimulations."}
{"id": "2508.07426", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07426", "abs": "https://arxiv.org/abs/2508.07426", "authors": ["Henry Li Xinyuan", "Zexin Cai", "Ashi Garg", "Kevin Duh", "Leibny Paola García-Perera", "Sanjeev Khudanpur", "Nicholas Andrews", "Matthew Wiesner"], "title": "Scalable Controllable Accented TTS", "comment": "Accepted at IEEE ASRU 2025", "summary": "We tackle the challenge of scaling accented TTS systems, expanding their\ncapabilities to include much larger amounts of training data and a wider\nvariety of accent labels, even for accents that are poorly represented or\nunlabeled in traditional TTS datasets. To achieve this, we employ two\nstrategies: 1. Accent label discovery via a speech geolocation model, which\nautomatically infers accent labels from raw speech data without relying solely\non human annotation; 2. Timbre augmentation through kNN voice conversion to\nincrease data diversity and model robustness. These strategies are validated on\nCommonVoice, where we fine-tune XTTS-v2 for accented TTS with accent labels\ndiscovered or enhanced using geolocation. We demonstrate that the resulting\naccented TTS model not only outperforms XTTS-v2 fine-tuned on self-reported\naccent labels in CommonVoice, but also existing accented TTS benchmarks."}
{"id": "2508.08039", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08039", "abs": "https://arxiv.org/abs/2508.08039", "authors": ["Shu Wu", "Chenxing Li", "Wenfu Wang", "Hao Zhang", "Hualei Wang", "Meng Yu", "Dong Yu"], "title": "Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning", "comment": "preprint", "summary": "Recent advancements in large language models, multimodal large language\nmodels, and large audio language models (LALMs) have significantly improved\ntheir reasoning capabilities through reinforcement learning with rule-based\nrewards. However, the explicit reasoning process has yet to show significant\nbenefits for audio question answering, and effectively leveraging deep\nreasoning remains an open challenge, with LALMs still falling short of\nhuman-level auditory-language reasoning. To address these limitations, we\npropose Audio-Thinker, a reinforcement learning framework designed to enhance\nthe reasoning capabilities of LALMs, with a focus on improving adaptability,\nconsistency, and effectiveness. Our approach introduces an adaptive think\naccuracy reward, enabling the model to adjust its reasoning strategies based on\ntask complexity dynamically. Furthermore, we incorporate an external reward\nmodel to evaluate the overall consistency and quality of the reasoning process,\ncomplemented by think-based rewards that help the model distinguish between\nvalid and flawed reasoning paths during training. Experimental results\ndemonstrate that our Audio-Thinker model outperforms existing\nreasoning-oriented LALMs across various benchmark tasks, exhibiting superior\nreasoning and generalization capabilities."}
{"id": "2508.07206", "categories": ["eess.SP", "cs.NA", "cs.SY", "eess.SY", "math.NA", "42C10, 94A12", "G.1.2; I.6.6"], "pdf": "https://arxiv.org/pdf/2508.07206", "abs": "https://arxiv.org/abs/2508.07206", "authors": ["Konstantin A. Rybakov", "Egor D. Shermatov"], "title": "Applying the Spectral Method for Modeling Linear Filters: Butterworth, Linkwitz-Riley, and Chebyshev filters", "comment": null, "summary": "This paper proposes a new technique for computer modeling linear filters\nbased on the spectral form of mathematical description of linear systems. It\nassumes that input and output signals of the filter are represented as\northogonal expansions, while filters themselves are described by\ntwo-dimensional non-stationary transfer functions. This technique allows one to\nmodel the output signal in continuous time, and it is successfully tested on\nthe Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders."}
{"id": "2508.07523", "categories": ["eess.AS", "cs.SD", "92C50 (Primary) 68Q25, 94A12 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.07523", "abs": "https://arxiv.org/abs/2508.07523", "authors": ["Bram Bremer", "Matthew Bigelow", "Stuart Anstee", "Gregory Cohen", "Andre van Schaik", "Ying Xu"], "title": "Real-time CARFAC Cochlea Model Acceleration on FPGA for Underwater Acoustic Sensing Systems", "comment": "5 pages, 6 figures", "summary": "This paper presents a real-time, energy-efficient embedded system\nimplementing an array of Cascade of Asymmetric Resonators with Fast-Acting\nCompression (CARFAC) cochlea models for underwater sound analysis. Built on the\nAMD Kria KV260 System-on-Module (SoM), the system integrates a Rust-based\nsoftware framework on the processor for real-time interfacing and\nsynchronization with multiple hydrophone inputs, and a hardware-accelerated\nimplementation of the CARFAC models on a Field-Programmable Gate Array (FPGA)\nfor real-time sound pre-processing. Compared to prior work, the CARFAC\naccelerator achieves improved scalability and processing speed while reducing\nresource usage through optimized time-multiplexing, pipelined design, and\nelimination of costly division circuits. Experimental results demonstrate 13.5%\nhardware utilization for a single 64-channel CARFAC instance and a whole board\npower consumption of 3.11 W when processing a 256 kHz input signal in real\ntime."}
{"id": "2508.07014", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07014", "abs": "https://arxiv.org/abs/2508.07014", "authors": ["Andrei Andrusenko", "Vladimir Bataev", "Lilit Grigoryan", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "TurboBias: Universal ASR Context-Biasing powered by GPU-accelerated Phrase-Boosting Tree", "comment": "Accepted by ASRU 2025", "summary": "Recognizing specific key phrases is an essential task for contextualized\nAutomatic Speech Recognition (ASR). However, most existing context-biasing\napproaches have limitations associated with the necessity of additional model\ntraining, significantly slow down the decoding process, or constrain the choice\nof the ASR system type. This paper proposes a universal ASR context-biasing\nframework that supports all major types: CTC, Transducers, and Attention\nEncoder-Decoder models. The framework is based on a GPU-accelerated word\nboosting tree, which enables it to be used in shallow fusion mode for greedy\nand beam search decoding without noticeable speed degradation, even with a vast\nnumber of key phrases (up to 20K items). The obtained results showed high\nefficiency of the proposed method, surpassing the considered open-source\ncontext-biasing approaches in accuracy and decoding speed. Our context-biasing\nframework is open-sourced as a part of the NeMo toolkit."}
{"id": "2508.07226", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07226", "abs": "https://arxiv.org/abs/2508.07226", "authors": ["Yueheng Li", "Xueyun Long", "Mario Pauli", "Suheng Tian", "Xiang Wan", "Benjamin Nuss", "Tiejun Cui", "Haixia Zhang", "Thomas Zwick"], "title": "Multi-RIS Deployment Optimization for mmWave ISAC Systems in Real-World Environments", "comment": "13 pages, 9 figures", "summary": "Reconfigurable intelligent surface-assisted integrated sensing and\ncommunication (RIS-ISAC) presents a promising system architecture to leverage\nthe wide bandwidth available at millimeter-wave (mmWave) frequencies, while\nmitigating severe signal propagation losses and reducing infrastructure costs.\nTo enhance ISAC functionalities in the future air-ground integrated network\napplications, RIS deployment must be carefully designed and evaluated, which\nforms the core motivation of this paper. To ensure practical relevance, a\nmulti-RIS-ISAC system is established, with its signal model at mmWave\nfrequencies demonstrated using ray-launching calibrated to real-world\nenvironments. On this basis, an energy-efficiency-driven optimization problem\nis formulated to minimize the multi-RIS size-to-coverage sum ratio,\ncomprehensively considering real-world RIS deployment constraints, positions,\norientations, as well as ISAC beamforming strategies at both the base station\nand the RISs. To solve the resulting non-convex mixed-integer problem, a\nsimplified reformulation based on equivalent gain scaling method is introduced.\nA two-step iterative algorithm is then proposed, in which the deployment\nparameters are determined under fixed RIS positions in the first step, and the\nRIS position set is updated in the second step to progressively approach the\noptimum solution. Simulation results based on realistic parameter benchmarks\npresent that the optimized RISs deployment significantly enhances communication\ncoverage and sensing accuracy with the minimum RIS sizes, outperforming\nexisting approaches."}
{"id": "2508.07558", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07558", "abs": "https://arxiv.org/abs/2508.07558", "authors": ["Ziqian Wang", "Zikai Liu", "Yike Zhu", "Xingchen Li", "Boyi Kang", "Jixun Yao", "Xianjun Xia", "Chuanzeng Huang", "Lei Xie"], "title": "UniFlow: Unifying Speech Front-End Tasks via Continuous Generative Modeling", "comment": "extended version", "summary": "Generative modeling has recently achieved remarkable success across image,\nvideo, and audio domains, demonstrating powerful capabilities for unified\nrepresentation learning. Yet speech front-end tasks such as speech enhancement\n(SE), target speaker extraction (TSE), acoustic echo cancellation (AEC), and\nlanguage-queried source separation (LASS) remain largely tackled by disparate,\ntask-specific solutions. This fragmentation leads to redundant engineering\neffort, inconsistent performance, and limited extensibility. To address this\ngap, we introduce UniFlow, a unified framework that employs continuous\ngenerative modeling to tackle diverse speech front-end tasks in a shared latent\nspace. Specifically, UniFlow utilizes a waveform variational autoencoder (VAE)\nto learn a compact latent representation of raw audio, coupled with a Diffusion\nTransformer (DiT) that predicts latent updates. To differentiate the speech\nprocessing task during the training, learnable condition embeddings indexed by\na task ID are employed to enable maximal parameter sharing while preserving\ntask-specific adaptability. To balance model performance and computational\nefficiency, we investigate and compare three generative objectives: denoising\ndiffusion, flow matching, and mean flow within the latent domain. We validate\nUniFlow on multiple public benchmarks, demonstrating consistent gains over\nstate-of-the-art baselines. UniFlow's unified latent formulation and\nconditional design make it readily extensible to new tasks, providing an\nintegrated foundation for building and scaling generative speech processing\npipelines. To foster future research, we will open-source our codebase."}
{"id": "2508.07219", "categories": ["eess.AS", "cs.SD", "I.2.7; H.5.5; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.07219", "abs": "https://arxiv.org/abs/2508.07219", "authors": ["Minu Kim", "Kangwook Jang", "Hoirin Kim"], "title": "ParaNoise-SV: Integrated Approach for Noise-Robust Speaker Verification with Parallel Joint Learning of Speech Enhancement and Noise Extraction", "comment": "5 pages, 3 figures, accepted to Interspeech 2025", "summary": "Noise-robust speaker verification leverages joint learning of speech\nenhancement (SE) and speaker verification (SV) to improve robustness. However,\nprevailing approaches rely on implicit noise suppression, which struggles to\nseparate noise from speaker characteristics as they do not explicitly\ndistinguish noise from speech during training. Although integrating SE and SV\nhelps, it remains limited in handling noise effectively. Meanwhile, recent SE\nstudies suggest that explicitly modeling noise, rather than merely suppressing\nit, enhances noise resilience. Reflecting this, we propose ParaNoise-SV, with\ndual U-Nets combining a noise extraction (NE) network and a speech enhancement\n(SE) network. The NE U-Net explicitly models noise, while the SE U-Net refines\nspeech with guidance from NE through parallel connections, preserving\nspeaker-relevant features. Experimental results show that ParaNoise-SV achieves\na relatively 8.4% lower equal error rate (EER) than previous joint SE-SV\nmodels."}
{"id": "2508.07265", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07265", "abs": "https://arxiv.org/abs/2508.07265", "authors": ["Bile Peng", "Vahid Jamali", "Eduard Jorswieck"], "title": "A Scalable Machine Learning Approach Enabled RIS Optimization with Implicit Channel Estimation", "comment": null, "summary": "The reconfigurable intelligent surface (RIS) is considered as a key enabler\nof the next-generation mobile radio systems. While attracting extensive\ninterest from academia and industry due to its passive nature and low cost,\nscalability of RIS elements and requirement for channel state information (CSI)\nare two major difficulties for the RIS to become a reality. In this work, we\nintroduce an unsupervised machine learning (ML) enabled optimization approach\nto configure the RIS. The dedicated neural network (NN) architecture RISnet is\ncombined with an implicit channel estimation method. The RISnet learns to map\nfrom received pilot signals to RIS configuration directly without explicit\nchannel estimation. Simulation results show that the proposed algorithm\noutperforms baselines significantly."}
{"id": "2508.07711", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07711", "abs": "https://arxiv.org/abs/2508.07711", "authors": ["Hui-Peng Du", "Yang Ai", "Rui-Chen Zheng", "Ye-Xin Lu", "Zhen-Hua Ling"], "title": "Is GAN Necessary for Mel-Spectrogram-based Neural Vocoder?", "comment": "Accepted by IEEE Signal Processing Letters", "summary": "Recently, mainstream mel-spectrogram-based neural vocoders rely on generative\nadversarial network (GAN) for high-fidelity speech generation, e.g., HiFi-GAN\nand BigVGAN. However, the use of GAN restricts training efficiency and model\ncomplexity. Therefore, this paper proposes a novel FreeGAN vocoder, aiming to\nanswer the question of whether GAN is necessary for mel-spectrogram-based\nneural vocoders. The FreeGAN employs an amplitude-phase serial prediction\nframework, eliminating the need for GAN training. It incorporates amplitude\nprior input, SNAKE-ConvNeXt v2 backbone and frequency-weighted anti-wrapping\nphase loss to compensate for the performance loss caused by the absence of GAN.\nExperimental results confirm that the speech quality of FreeGAN is comparable\nto that of advanced GAN-based vocoders, while significantly improving training\nefficiency and complexity. Other explicit-phase-prediction-based neural\nvocoders can also work without GAN, leveraging our proposed methods."}
{"id": "2508.07315", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07315", "abs": "https://arxiv.org/abs/2508.07315", "authors": ["Lilit Grigoryan", "Vladimir Bataev", "Nikolay Karpov", "Andrei Andrusenko", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "FlexCTC: GPU-powered CTC Beam Decoding with advanced Contextual Abilities", "comment": "Accepted to Automatic Speech Recognition and Understanding Workshop\n  (ASRU) 2025", "summary": "While beam search improves speech recognition quality over greedy decoding,\nstandard implementations are slow, often sequential, and CPU-bound. To fully\nleverage modern hardware capabilities, we present a novel open-source FlexCTC\ntoolkit for fully GPU-based beam decoding, designed for Connectionist Temporal\nClassification (CTC) models. Developed entirely in Python and PyTorch, it\noffers a fast, user-friendly, and extensible alternative to traditional C++,\nCUDA, or WFST-based decoders. The toolkit features a high-performance, fully\nbatched GPU implementation with eliminated CPU-GPU synchronization and\nminimized kernel launch overhead via CUDA Graphs. It also supports advanced\ncontextualization techniques, including GPU-powered N-gram language model\nfusion and phrase-level boosting. These features enable accurate and efficient\ndecoding, making them suitable for both research and production use."}
{"id": "2508.07305", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07305", "abs": "https://arxiv.org/abs/2508.07305", "authors": ["Mahdi Maleki", "Reza Agahzadeh Ayoubi", "Marouan Mizmizi", "Umberto Spagnolini"], "title": "Channel Charting in Smart Radio Environments", "comment": null, "summary": "This paper introduces the use of static electromagnetic skins (EMSs) to\nenable robust device localization via channel charting (CC) in realistic urban\nenvironments. We develop a rigorous optimization framework that leverages EMS\nto enhance channel dissimilarity and spatial fingerprinting, formulating EMS\nphase profile design as a codebook-based problem targeting the upper quantiles\nof key embedding metric, localization error, trustworthiness, and continuity.\nThrough 3D ray-traced simulations of a representative city scenario, we\ndemonstrate that optimized EMS configurations, in addition to significant\nimprovement of the average positioning error, reduce the 90th-percentile\nlocalization error from over 60 m (no EMS) to less than 25 m, while drastically\nimproving trustworthiness and continuity. To the best of our knowledge, this is\nthe first work to exploit Smart Radio Environment (SRE) with static EMS for\nenhancing CC, achieving substantial gains in localization performance under\nchallenging None-Line-of-Sight (NLoS) conditions."}
{"id": "2508.07757", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07757", "abs": "https://arxiv.org/abs/2508.07757", "authors": ["Zhanhong He", "Roberto Togneri", "Defeng", "Huang"], "title": "Score-Informed BiLSTM Correction for Refining MIDI Velocity in Automatic Piano Transcription", "comment": "4 pages; rejected paper by WASPAA2025", "summary": "MIDI is a modern standard for storing music, recording how musical notes are\nplayed. Many piano performances have corresponding MIDI scores available\nonline. Some of these are created by the original performer, recording on an\nelectric piano alongside the audio, while others are through manual\ntranscription. In recent years, automatic music transcription (AMT) has rapidly\nadvanced, enabling machines to transcribe MIDI from audio. However, these\ntranscriptions often require further correction. Assuming a perfect timing\ncorrection, we focus on the loudness correction in terms of MIDI velocity (a\nparameter in MIDI for loudness control). This task can be approached through\nscore-informed MIDI velocity estimation, which has undergone several\ndevelopments. While previous approaches introduced specifically built models to\nre-estimate MIDI velocity, thereby replacing AMT estimates, we propose a BiLSTM\ncorrection module to refine AMT-estimated velocity. Although we did not reach\nstate-of-the-art performance, we validated our method on the well-known AMT\nsystem, the high-resolution piano transcription (HPT), and achieved significant\nimprovements."}
{"id": "2508.07523", "categories": ["eess.AS", "cs.SD", "92C50 (Primary) 68Q25, 94A12 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.07523", "abs": "https://arxiv.org/abs/2508.07523", "authors": ["Bram Bremer", "Matthew Bigelow", "Stuart Anstee", "Gregory Cohen", "Andre van Schaik", "Ying Xu"], "title": "Real-time CARFAC Cochlea Model Acceleration on FPGA for Underwater Acoustic Sensing Systems", "comment": "5 pages, 6 figures", "summary": "This paper presents a real-time, energy-efficient embedded system\nimplementing an array of Cascade of Asymmetric Resonators with Fast-Acting\nCompression (CARFAC) cochlea models for underwater sound analysis. Built on the\nAMD Kria KV260 System-on-Module (SoM), the system integrates a Rust-based\nsoftware framework on the processor for real-time interfacing and\nsynchronization with multiple hydrophone inputs, and a hardware-accelerated\nimplementation of the CARFAC models on a Field-Programmable Gate Array (FPGA)\nfor real-time sound pre-processing. Compared to prior work, the CARFAC\naccelerator achieves improved scalability and processing speed while reducing\nresource usage through optimized time-multiplexing, pipelined design, and\nelimination of costly division circuits. Experimental results demonstrate 13.5%\nhardware utilization for a single 64-channel CARFAC instance and a whole board\npower consumption of 3.11 W when processing a 256 kHz input signal in real\ntime."}
{"id": "2508.07436", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07436", "abs": "https://arxiv.org/abs/2508.07436", "authors": ["Mehrbod Zarifi", "Mohamad Amin Jamshidi", "Zolfa Anvari", "Hamed Ghafarirad", "Mohammad Zareinejad"], "title": "Detection and Classification of Internal Leakage in Hydraulic Cylinders", "comment": "10 pages, 7 figures, presented at the 12th RSI International\n  Conference on Robotics and Mechatronics (ICRoM 2024), IEEE", "summary": "Hydraulic systems have been one of the most used technologies in many\nindustries due to their reliance on incompressible fluids that facilitate\nenergy and power transfer. Within such systems, hydraulic cylinders are prime\ndevices that convert hydraulic energy into mechanical energy. Some of the\ngenuine and very common problems related to hydraulic cylinders are leakages.\nLeakage in hydraulic systems can cause a drop in pressure, general\ninefficiency, and even complete failure of such systems. The various ways\nleakage can occur define the major categorization of leakage: internal and\nexternal leakage. External leakage is easily noticeable, while internal\nleakage, which involves fluid movement between pressure chambers, can be harder\nto detect and may gradually impact system performance without obvious signs.\nWhen leakage surpasses acceptable limits, it is classified as a fault or\nfailure. In such cases, leakage is divided into three categories: no leakage,\nlow leakage, and high leakage. It suggests a fault detection algorithm with the\nbasic responsibility of detecting minimum leakage within the Hydraulic system,\nand minimizing detection time is the core idea of this paper. In order to fully\ndevelop this idea, experimental data collection of Hydraulic systems is\nrequired. The collected data uses pressure sensors and other signals that are\nsingle-related. Due to the utilization of Long Short-Term Memory (LSTM)\nrecurrent neural networks, more complex data analysis was enabled, which the\nLSTM-based leakage detection algorithm successfully achieved, providing almost\n96% accuracy in classifying leakage types. Results demonstrate that the\nproposed method can perform real-time and online fault diagnosis for each\ncycle, reducing maintenance costs and prolonging the hydraulic system's\nlifespan."}
{"id": "2508.07829", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07829", "abs": "https://arxiv.org/abs/2508.07829", "authors": ["Hyeonuk Nam"], "title": "Auditory Intelligence: Understanding the World Through Sound", "comment": "Position paper without experimental/quantitative validation. Not\n  submitted to any journal/conference", "summary": "Recent progress in auditory intelligence has yielded high-performing systems\nfor sound event detection (SED), acoustic scene classification (ASC), automated\naudio captioning (AAC), and audio question answering (AQA). Yet these tasks\nremain largely constrained to surface-level recognition-capturing what happened\nbut not why, what it implies, or how it unfolds in context. I propose a\nconceptual reframing of auditory intelligence as a layered, situated process\nthat encompasses perception, reasoning, and interaction. To instantiate this\nview, I introduce four cognitively inspired task paradigms-ASPIRE, SODA, AUX,\nand AUGMENT-those structure auditory understanding across time-frequency\npattern captioning, hierarchical event/scene description, causal explanation,\nand goal-driven interpretation, respectively. Together, these paradigms provide\na roadmap toward more generalizable, explainable, and human-aligned auditory\nintelligence, and are intended to catalyze a broader discussion of what it\nmeans for machines to understand sound."}
{"id": "2508.07757", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07757", "abs": "https://arxiv.org/abs/2508.07757", "authors": ["Zhanhong He", "Roberto Togneri", "Defeng", "Huang"], "title": "Score-Informed BiLSTM Correction for Refining MIDI Velocity in Automatic Piano Transcription", "comment": "4 pages; rejected paper by WASPAA2025", "summary": "MIDI is a modern standard for storing music, recording how musical notes are\nplayed. Many piano performances have corresponding MIDI scores available\nonline. Some of these are created by the original performer, recording on an\nelectric piano alongside the audio, while others are through manual\ntranscription. In recent years, automatic music transcription (AMT) has rapidly\nadvanced, enabling machines to transcribe MIDI from audio. However, these\ntranscriptions often require further correction. Assuming a perfect timing\ncorrection, we focus on the loudness correction in terms of MIDI velocity (a\nparameter in MIDI for loudness control). This task can be approached through\nscore-informed MIDI velocity estimation, which has undergone several\ndevelopments. While previous approaches introduced specifically built models to\nre-estimate MIDI velocity, thereby replacing AMT estimates, we propose a BiLSTM\ncorrection module to refine AMT-estimated velocity. Although we did not reach\nstate-of-the-art performance, we validated our method on the well-known AMT\nsystem, the high-resolution piano transcription (HPT), and achieved significant\nimprovements."}
{"id": "2508.07513", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07513", "abs": "https://arxiv.org/abs/2508.07513", "authors": ["Emre Kurtoglu", "Mohammad Mahbubur Rahman"], "title": "Direction of Arrival Estimation with Virtual Antenna Array Using FMCW Radar Simulated Data", "comment": null, "summary": "The FMCW radars are widely used for automotive radar systems. The basic idea\nfor FMCW radars is to generate a linear frequency ramp as transmit signal. The\ndifference frequency, (i.e., beat frequency) between the transmitted and\nreceived signal is determined after down conversion. The FFT operation on beat\nfrequency signal can recognize targets at different range and velocity.\nIncreasing demand on safety functionality leads to the Direction of Arrival\n(DOA) estimation to resolve two closely located targets. Consequently, the\nproblem of angle estimation for 77GHz FMCW automotive radar simulated data has\nbeen investigated in this term project. In particular, we examined the\nperformances of FFT, MUSIC and compressed sensing in angle estimation task, and\nit was found that although FFT is the fastest algorithm, it has very poor\nangular resolution when compared with others which are both super resolution\nalgorithms. The code for this project report is available at\nhttps://github.com/ekurtgl/FMCW-MIMO-Radar-Simulation."}
{"id": "2508.07836", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07836", "abs": "https://arxiv.org/abs/2508.07836", "authors": ["Vishwas M. Shetty", "Jiusi Zheng", "Abeer Alwan"], "title": "G-IFT: A Gated Linear Unit adapter with Iterative Fine-Tuning for Low-Resource Children's Speaker Verification", "comment": "Accepted at WOCCI, 2025 - Interspeech workshop", "summary": "Speaker Verification (SV) systems trained on adults speech often underperform\non children's SV due to the acoustic mismatch, and limited children speech data\nmakes fine-tuning not very effective. In this paper, we propose an innovative\nframework, a Gated Linear Unit adapter with Iterative Fine-Tuning (G-IFT), to\nenhance knowledge transfer efficiency between the high-resource adults speech\ndomain and the low-resource children's speech domain. In this framework, a\nGated Linear Unit adapter is first inserted between the pre-trained speaker\nembedding model and the classifier. Then the classifier, adapter, and\npre-trained speaker embedding model are optimized sequentially in an iterative\nway. This framework is agnostic to the type of the underlying architecture of\nthe SV system. Our experiments on ECAPA-TDNN, ResNet, and X-vector\narchitectures using the OGI and MyST datasets demonstrate that the G-IFT\nframework yields consistent reductions in Equal Error Rates compared to\nbaseline methods."}
{"id": "2508.07829", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.07829", "abs": "https://arxiv.org/abs/2508.07829", "authors": ["Hyeonuk Nam"], "title": "Auditory Intelligence: Understanding the World Through Sound", "comment": "Position paper without experimental/quantitative validation. Not\n  submitted to any journal/conference", "summary": "Recent progress in auditory intelligence has yielded high-performing systems\nfor sound event detection (SED), acoustic scene classification (ASC), automated\naudio captioning (AAC), and audio question answering (AQA). Yet these tasks\nremain largely constrained to surface-level recognition-capturing what happened\nbut not why, what it implies, or how it unfolds in context. I propose a\nconceptual reframing of auditory intelligence as a layered, situated process\nthat encompasses perception, reasoning, and interaction. To instantiate this\nview, I introduce four cognitively inspired task paradigms-ASPIRE, SODA, AUX,\nand AUGMENT-those structure auditory understanding across time-frequency\npattern captioning, hierarchical event/scene description, causal explanation,\nand goal-driven interpretation, respectively. Together, these paradigms provide\na roadmap toward more generalizable, explainable, and human-aligned auditory\nintelligence, and are intended to catalyze a broader discussion of what it\nmeans for machines to understand sound."}
{"id": "2508.07572", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07572", "abs": "https://arxiv.org/abs/2508.07572", "authors": ["Yuanwei Liu", "Hao Jiang", "Xiaoxia Xu", "Zhaolin Wang", "Jia Guo", "Chongjun Ouyang", "Xidong Mu", "Zhiguo Ding", "Arumugam Nallanathan", "George K. Karagiannidis", "Robert Schober"], "title": "Pinching-Antenna Systems (PASS): A Tutorial", "comment": "Submitted to IEEE journal", "summary": "Pinching antenna systems (PASS) present a breakthrough among the\nflexible-antenna technologies, and distinguish themselves by facilitating\nlarge-scale antenna reconfiguration, line-of-sight creation, scalable\nimplementation, and near-field benefits, thus bringing wireless communications\nfrom the last mile to the last meter. A comprehensive tutorial is presented in\nthis paper. First, the fundamentals of PASS are discussed, including PASS\nsignal models, hardware models, power radiation models, and pinching antenna\nactivation methods. Building upon this, the information-theoretic capacity\nlimits achieved by PASS are characterized, and several typical performance\nmetrics of PASS-based communications are analyzed to demonstrate its\nsuperiority over conventional antenna technologies. Next, the pinching\nbeamforming design is investigated. The corresponding power scaling law is\nfirst characterized. For the joint transmit and pinching design in the general\nmultiple-waveguide case, 1) a pair of transmission strategies is proposed for\nPASS-based single-user communications to validate the superiority of PASS,\nnamely sub-connected and fully connected structures; and 2) three practical\nprotocols are proposed for facilitating PASS-based multi-user communications,\nnamely waveguide switching, waveguide division, and waveguide multiplexing. A\npossible implementation of PASS in wideband communications is further\nhighlighted. Moreover, the channel state information acquisition in PASS is\nelaborated with a pair of promising solutions. To overcome the high complexity\nand suboptimality inherent in conventional convex-optimization-based\napproaches, machine-learning-based methods for operating PASS are also\nexplored, focusing on selected deep neural network architectures and training\nalgorithms. Finally, several promising applications of PASS in next-generation\nwireless networks are highlighted."}
{"id": "2508.08155", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08155", "abs": "https://arxiv.org/abs/2508.08155", "authors": ["Shuai Wang", "Zhaokai Sun", "Zhennan Lin", "Chengyou Wang", "Zhou Pan", "Lei Xie"], "title": "MSU-Bench: Towards Understanding the Conversational Multi-talker Scenarios", "comment": null, "summary": "Spoken Language Understanding (SLU) has progressed from traditional\nsingle-task methods to large audio language model (LALM) solutions. Yet, most\nexisting speech benchmarks focus on single-speaker or isolated tasks,\noverlooking the challenges posed by multi-speaker conversations that are common\nin real-world scenarios. We introduce MSU-Bench, a comprehensive benchmark for\nevaluating multi-speaker conversational understanding with a speaker-centric\ndesign. Our hierarchical framework covers four progressive tiers:\nsingle-speaker static attribute understanding, single-speaker dynamic attribute\nunderstanding, multi-speaker background understanding, and multi-speaker\ninteraction understanding. This structure ensures all tasks are grounded in\nspeaker-centric contexts, from basic perception to complex reasoning across\nmultiple speakers. By evaluating state-of-the-art models on MSU-Bench, we\ndemonstrate that as task complexity increases across the benchmark's tiers, all\nmodels exhibit a significant performance decline. We also observe a persistent\ncapability gap between open-source models and closed-source commercial ones,\nparticularly in multi-speaker interaction reasoning. These findings validate\nthe effectiveness of MSU-Bench for assessing and advancing conversational\nunderstanding in realistic multi-speaker environments. Demos can be found in\nthe supplementary material."}
{"id": "2508.08155", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08155", "abs": "https://arxiv.org/abs/2508.08155", "authors": ["Shuai Wang", "Zhaokai Sun", "Zhennan Lin", "Chengyou Wang", "Zhou Pan", "Lei Xie"], "title": "MSU-Bench: Towards Understanding the Conversational Multi-talker Scenarios", "comment": null, "summary": "Spoken Language Understanding (SLU) has progressed from traditional\nsingle-task methods to large audio language model (LALM) solutions. Yet, most\nexisting speech benchmarks focus on single-speaker or isolated tasks,\noverlooking the challenges posed by multi-speaker conversations that are common\nin real-world scenarios. We introduce MSU-Bench, a comprehensive benchmark for\nevaluating multi-speaker conversational understanding with a speaker-centric\ndesign. Our hierarchical framework covers four progressive tiers:\nsingle-speaker static attribute understanding, single-speaker dynamic attribute\nunderstanding, multi-speaker background understanding, and multi-speaker\ninteraction understanding. This structure ensures all tasks are grounded in\nspeaker-centric contexts, from basic perception to complex reasoning across\nmultiple speakers. By evaluating state-of-the-art models on MSU-Bench, we\ndemonstrate that as task complexity increases across the benchmark's tiers, all\nmodels exhibit a significant performance decline. We also observe a persistent\ncapability gap between open-source models and closed-source commercial ones,\nparticularly in multi-speaker interaction reasoning. These findings validate\nthe effectiveness of MSU-Bench for assessing and advancing conversational\nunderstanding in realistic multi-speaker environments. Demos can be found in\nthe supplementary material."}
{"id": "2508.07651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07651", "abs": "https://arxiv.org/abs/2508.07651", "authors": ["Ziye Jia", "Yian Zhu", "Qihui Wu", "Lei Zhang", "Sen Yang", "Zhu Han"], "title": "Remote ID Based UAV Collision Avoidance Optimization for Low-Altitude Airspace Safety", "comment": null, "summary": "With the rapid development of unmanned aerial vehicles (UAVs), it is\nparamount to ensure safe and efficient operations in open airspaces. The remote\nidentification (Remote ID) is deemed an effective real-time UAV monitoring\nsystem by the federal aviation administration, which holds potentials for\nenabling inter-UAV communications. This paper deeply investigates the\napplication of Remote ID for UAV collision avoidance while minimizing\ncommunication delays. First, we propose a Remote ID based distributed multi-UAV\ncollision avoidance (DMUCA) framework to support the collision detection,\navoidance decision-making, and trajectory recovery. Next, the average\ntransmission delays for Remote ID messages are analyzed, incorporating the\npacket reception mechanisms and packet loss due to interference. The\noptimization problem is formulated to minimize the long-term average\ncommunication delay, where UAVs can flexibly select the Remote ID protocol to\nenhance the collision avoidance performance. To tackle the problem, we design a\nmulti-agent deep Q-network based adaptive communication configuration\nalgorithm, allowing UAVs to autonomously learn the optimal protocol\nconfigurations in dynamic environments. Finally, numerical results verify the\nfeasibility of the proposed DMUCA framework, and the proposed mechanism can\nreduce the average delay by 32% compared to the fixed protocol configuration."}
{"id": "2508.07363", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07363", "abs": "https://arxiv.org/abs/2508.07363", "authors": ["Hanyu Ding", "Wenlong Dong", "Qirong Mao"], "title": "Keyword Mamba: Spoken Keyword Spotting with State Space Models", "comment": "Under peer review", "summary": "Keyword spotting (KWS) is an essential task in speech processing. It is\nwidely used in voice assistants and smart devices. Deep learning models like\nCNNs, RNNs, and Transformers have performed well in KWS. However, they often\nstruggle to handle long-term patterns and stay efficient at the same time. In\nthis work, we present Keyword Mamba, a new architecture for KWS. It uses a\nneural state space model (SSM) called Mamba. We apply Mamba along the time axis\nand also explore how it can replace the self-attention part in Transformer\nmodels. We test our model on the Google Speech Commands datasets. The results\nshow that Keyword Mamba reaches strong accuracy with fewer parameters and lower\ncomputational cost. To our knowledge, this is the first time a state space\nmodel has been used for KWS. These results suggest that Mamba has strong\npotential in speech-related tasks."}
{"id": "2508.07696", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07696", "abs": "https://arxiv.org/abs/2508.07696", "authors": ["Joohyuk Park", "Yongjeong Oh", "Jihun Park", "Yo-Seb Jeon"], "title": "Importance-Aware Semantic Communication in MIMO-OFDM Systems Using Vision Transformer", "comment": null, "summary": "This paper presents a novel importance-aware quantization, subcarrier\nmapping, and power allocation (IA-QSMPA) framework for semantic communication\nin multiple-input multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) systems, empowered by a pretrained Vision Transformer (ViT). The\nproposed framework exploits attention-based importance extracted from a\npretrained ViT to jointly optimize quantization levels, subcarrier mapping, and\npower allocation. Specifically, IA-QSMPA maps semantically important features\nto high-quality subchannels and allocates resources in accordance with their\ncontribution to task performance and communication latency. To efficiently\nsolve the resulting nonconvex optimization problem, a block coordinate descent\nalgorithm is employed. The framework is further extended to operate under\nfinite blocklength transmission, where communication errors may occur. In this\nsetting, a segment-wise linear approximation of the channel dispersion penalty\nis introduced to enable efficient joint optimization under practical\nconstraints. Simulation results on a multi-view image classification task using\nthe MVP-N dataset demonstrate that IA-QSMPA significantly outperforms\nconventional methods in both ideal and finite blocklength transmission\nscenarios, achieving superior task performance and communication efficiency."}
{"id": "2508.07751", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07751", "abs": "https://arxiv.org/abs/2508.07751", "authors": ["Zhanhong He", "David Cooper", "Defeng Huang", "Roberto Togneri"], "title": "Filling MIDI Velocity using U-Net Image Colorizer", "comment": "12 pages, submitted to CMMR2025 conference", "summary": "Modern music producers commonly use MIDI (Musical Instrument Digital\nInterface) to store their musical compositions. However, MIDI files created\nwith digital software may lack the expressive characteristics of human\nperformances, essentially leaving the velocity parameter - a control for note\nloudness - undefined, which defaults to a flat value. The task of filling MIDI\nvelocity is termed MIDI velocity prediction, which uses regression models to\nenhance music expressiveness by adjusting only this parameter. In this paper,\nwe introduce the U-Net, a widely adopted architecture in image colorization, to\nthis task. By conceptualizing MIDI data as images, we adopt window attention\nand develop a custom loss function to address the sparsity of MIDI-converted\nimages. Current dataset availability restricts our experiments to piano data.\nEvaluated on the MAESTRO v3 and SMD datasets, our proposed method for filling\nMIDI velocity outperforms previous approaches in both quantitative metrics and\nqualitative listening tests."}
{"id": "2508.07717", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07717", "abs": "https://arxiv.org/abs/2508.07717", "authors": ["Yuchen Gao", "Xiao Xu", "Eckehard Steinbach", "Daniel E. Lucani", "Qi Zhang"], "title": "Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction", "comment": null, "summary": "This paper presents a multimodal framework that integrates touch signals\n(contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our\napproach enhances scene reconstruction, particularly under challenging\nconditions like low lighting, limited camera viewpoints, and occlusions.\nDifferent from the visual-only method, the proposed approach incorporates\nspatially selective touch measurements to refine both the geometry and\nappearance of the 3D Gaussian representation. To guide the touch exploration,\nwe introduce a two-stage sampling scheme that initially probes sparse regions\nand then concentrates on high-uncertainty boundaries identified from the\nreconstructed mesh. A geometric loss is proposed to ensure surface smoothness,\nresulting in improved geometry. Experimental results across diverse scenarios\nshow consistent improvements in geometric accuracy. In the most challenging\ncase with severe occlusion, the Chamfer Distance is reduced by over 15x,\ndemonstrating the effectiveness of integrating touch cues into 3D Gaussian\nSplatting. Furthermore, our approach maintains a fully online pipeline,\nunderscoring its feasibility in visually degraded environments."}
{"id": "2508.07973", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07973", "abs": "https://arxiv.org/abs/2508.07973", "authors": ["Sebastian Murgul", "Johannes Schimper", "Michael Heizmann"], "title": "Joint Transcription of Acoustic Guitar Strumming Directions and Chords", "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "Automatic transcription of guitar strumming is an underrepresented and\nchallenging task in Music Information Retrieval (MIR), particularly for\nextracting both strumming directions and chord progressions from audio signals.\nWhile existing methods show promise, their effectiveness is often hindered by\nlimited datasets. In this work, we extend a multimodal approach to guitar\nstrumming transcription by introducing a novel dataset and a deep\nlearning-based transcription model. We collect 90 min of real-world guitar\nrecordings using an ESP32 smartwatch motion sensor and a structured recording\nprotocol, complemented by a synthetic dataset of 4h of labeled strumming audio.\nA Convolutional Recurrent Neural Network (CRNN) model is trained to detect\nstrumming events, classify their direction, and identify the corresponding\nchords using only microphone audio. Our evaluation demonstrates significant\nimprovements over baseline onset detection algorithms, with a hybrid method\ncombining synthetic and real-world data achieving the highest accuracy for both\nstrumming action detection and chord classification. These results highlight\nthe potential of deep learning for robust guitar strumming transcription and\nopen new avenues for automatic rhythm guitar analysis."}
{"id": "2508.07909", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07909", "abs": "https://arxiv.org/abs/2508.07909", "authors": ["Bile Peng", "Karl-Ludwig Besser", "Shanpu Shen", "Finn Siegismund-Poschmann", "Ramprasad Raghunath", "Daniel M. Mittleman", "Vahid Jamali", "Eduard A. Jorswieck"], "title": "RIS-Assisted NOMA with Partial CSI and Mutual Coupling: A Machine Learning Approach", "comment": null, "summary": "Non-orthogonal multiple access (NOMA) is a promising multiple access\ntechnique. Its performance depends strongly on the wireless channel property,\nwhich can be enhanced by reconfigurable intelligent surfaces (RISs). In this\npaper, we jointly optimize base station (BS) precoding and RIS configuration\nwith unsupervised machine learning (ML), which looks for the optimal solution\nautonomously. In particular, we propose a dedicated neural network (NN)\narchitecture RISnet inspired by domain knowledge in communication. Compared to\nstate-of-the-art, the proposed approach combines analytical optimal BS\nprecoding and ML-enabled RIS, has a high scalability to control more than 1000\nRIS elements, has a low requirement for channel state information (CSI) in\ninput, and addresses the mutual coupling between RIS elements. Beyond the\nconsidered problem, this work is an early contribution to domain knowledge\nenabled ML, which exploit the domain expertise of communication systems to\ndesign better approaches than general ML methods."}
{"id": "2508.07987", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07987", "abs": "https://arxiv.org/abs/2508.07987", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription", "comment": "Accepted to the 6th Conference on AI Music Creativity (AIMC), 2025", "summary": "Automatic transcription of acoustic guitar fingerpicking performances remains\na challenging task due to the scarcity of labeled training data and legal\nconstraints connected with musical recordings. This work investigates a\nprocedural data generation pipeline as an alternative to real audio recordings\nfor training transcription models. Our approach synthesizes training data\nthrough four stages: knowledge-based fingerpicking tablature composition, MIDI\nperformance rendering, physical modeling using an extended Karplus-Strong\nalgorithm, and audio augmentation including reverb and distortion. We train and\nevaluate a CRNN-based note-tracking model on both real and synthetic datasets,\ndemonstrating that procedural data can be used to achieve reasonable\nnote-tracking results. Finetuning with a small amount of real data further\nenhances transcription accuracy, improving over models trained exclusively on\nreal recordings. These results highlight the potential of procedurally\ngenerated audio for data-scarce music information retrieval tasks."}
{"id": "2508.07967", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07967", "abs": "https://arxiv.org/abs/2508.07967", "authors": ["Haijia Jin", "Weijie Yuan", "Jun Wu", "Jiacheng Wang", "Dusit Niyato", "Xianbin Wang", "George K. Karagiannidis", "Zhiyun Lin", "Yi Gong", "Dong In Kim", "Athina Petropulu", "Maria Sabrina Greco", "Abbas Jamalipour", "Sumei Sun"], "title": "Advancing the Control of Low-Altitude Wireless Networks: Architecture, Design Principles, and Future Directions", "comment": null, "summary": "This article introduces a control-oriented low-altitude wireless network\n(LAWN) that integrates near-ground communications and remote estimation of the\ninternal system state. This integration supports reliable networked control in\ndynamic aerial-ground environments. First, we introduce the network's modular\narchitecture and key performance metrics. Then, we discuss core design\ntrade-offs across the control, communication, and estimation layers. A case\nstudy illustrates closed-loop coordination under wireless constraints. Finally,\nwe outline future directions for scalable, resilient LAWN deployments in\nreal-time and resource-constrained scenarios."}
{"id": "2508.08027", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08027", "abs": "https://arxiv.org/abs/2508.08027", "authors": ["Ahmed Aboeitta", "Ahmed Sharshar", "Youssef Nafea", "Shady Shehata"], "title": "Bridging ASR and LLMs for Dysarthric Speech Recognition: Benchmarking Self-Supervised and Generative Approaches", "comment": null, "summary": "Speech Recognition (ASR) due to phoneme distortions and high variability.\nWhile self-supervised ASR models like Wav2Vec, HuBERT, and Whisper have shown\npromise, their effectiveness in dysarthric speech remains unclear. This study\nsystematically benchmarks these models with different decoding strategies,\nincluding CTC, seq2seq, and LLM-enhanced decoding (BART,GPT-2, Vicuna). Our\ncontributions include (1) benchmarking ASR architectures for dysarthric speech,\n(2) introducing LLM-based decoding to improve intelligibility, (3) analyzing\ngeneralization across datasets, and (4) providing insights into recognition\nerrors across severity levels. Findings highlight that LLM-enhanced decoding\nimproves dysarthric ASR by leveraging linguistic constraints for phoneme\nrestoration and grammatical correction."}
{"id": "2508.08097", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08097", "abs": "https://arxiv.org/abs/2508.08097", "authors": ["Muhammad Asif", "Zain Ali", "Asim Ihsan", "Ali Ranjha", "Zhu Shoujin", "Manzoor Ahmed", "Xingwang Li", "Symeon Chatzinotas"], "title": "Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors", "comment": "13 pages, and 11 figures. Submitted to IEEE", "summary": "This work explores the integration of rate-splitting multiple access (RSMA),\nsimultaneous wireless information and power transfer (SWIPT), and\nbeyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the\nspectral-efficiency, energy-efficiency, coverage, and connectivity of future\nsixth-generation (6G) communication networks. Specifically, with a multiuser\nBD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding\nvectors, the common rate proportion of users, the power-splitting ratios, and\nscattering matrix of BD-RIS node, under the assumption of imperfect channel\nstate information (CSI). Additionally, to better capture practical hardware\nbehavior, we incorporate a nonlinear energy harvesting model under energy\nharvesting constraints. We design a robust optimization framework to maximize\nthe system sum-rate, while explicitly accounting for the worst-case impact of\nCSI uncertainties. Further, we introduce an alternating optimization framework\nthat partitions the problem into several blocks, which are optimized\niteratively. More specifically, the transmit precoding vectors are optimized by\nreformulating the problem as a convex semidefinite programming through\nsuccessive-convex approximation (SCA), whereas the power-splitting problem is\nsolved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the\nscattering matrix of the BD-RIS, we first employ SCA to reformulate the problem\ninto a convex form, and then design a manifold optimization strategy based on\nthe Conjugate-Gradient method. Finally, numerical simulation results reveal\nthat the proposed scheme provides significant performance improvements over\nexisting benchmarks and demonstrates rapid convergence within a reasonable\nnumber of iterations."}
{"id": "2508.08206", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08206", "abs": "https://arxiv.org/abs/2508.08206", "authors": ["Amirhossein Taherpour", "Abbas Taherpour", "Tamer Khattab"], "title": "Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers", "comment": null, "summary": "We propose a joint learning framework for Byzantine-resilient spectrum\nsensing and secure intelligent reflecting surface (IRS)--assisted opportunistic\naccess under channel state information (CSI) uncertainty. The sensing stage\nperforms logit-domain Bayesian updates with trimmed aggregation and\nattention-weighted consensus, and the base station (BS) fuses network beliefs\nwith a conservative minimum rule, preserving detection accuracy under a bounded\nnumber of Byzantine users. Conditioned on the sensing outcome, we pose downlink\ndesign as sum mean-squared error (MSE) minimization under transmit-power and\nsignal-leakage constraints and jointly optimize the BS precoder, IRS phase\nshifts, and user equalizers. With partial (or known) CSI, we develop an\naugmented-Lagrangian alternating algorithm with projected updates and provide\nprovable sublinear convergence, with accelerated rates under mild local\ncurvature. With unknown CSI, we perform constrained Bayesian optimization (BO)\nin a geometry-aware low-dimensional latent space using Gaussian process (GP)\nsurrogates; we prove regret bounds for a constrained upper confidence bound\n(UCB) variant of the BO module, and demonstrate strong empirical performance of\nthe implemented procedure. Simulations across diverse network conditions show\nhigher detection probability at fixed false-alarm rate under adversarial\nattacks, large reductions in sum MSE for honest users, strong suppression of\neavesdropper signal power, and fast convergence. The framework offers a\npractical path to secure opportunistic communication that adapts to CSI\navailability while coherently coordinating sensing and transmission through\njoint learning."}
{"id": "2508.06516", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06516", "abs": "https://arxiv.org/abs/2508.06516", "authors": ["Marine Delabaere", "Léa Miqueu", "Michael Moreno", "Gautier Bigois", "Hoang Duong", "Ella Fernandez", "Flavie Manent", "Maria Salgado-Herrera", "Bastien Pasdeloup", "Nicolas Farrugia", "Axel Marmoret"], "title": "AutoMashup: Automatic Music Mashups Creation", "comment": null, "summary": "We introduce AutoMashup, a system for automatic mashup creation based on\nsource separation, music analysis, and compatibility estimation. We propose\nusing COCOLA to assess compatibility between separated stems and investigate\nwhether general-purpose pretrained audio models (CLAP and MERT) can support\nzero-shot estimation of track pair compatibility. Our results show that mashup\ncompatibility is asymmetric -- it depends on the role assigned to each track\n(vocals or accompaniment) -- and that current embeddings fail to reproduce the\nperceptual coherence measured by COCOLA. These findings underline the\nlimitations of general-purpose audio representations for compatibility\nestimation in mashup creation."}
{"id": "2508.06840", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06840", "abs": "https://arxiv.org/abs/2508.06840", "authors": ["Seonggyu Lee", "Sein Cheong", "Sangwook Han", "Jong Won Shin"], "title": "FlowSE: Flow Matching-based Speech Enhancement", "comment": "Published in ICASSP 2025", "summary": "Diffusion probabilistic models have shown impressive performance for speech\nenhancement, but they typically require 25 to 60 function evaluations in the\ninference phase, resulting in heavy computational complexity. Recently, a\nfine-tuning method was proposed to correct the reverse process, which\nsignificantly lowered the number of function evaluations (NFE). Flow matching\nis a method to train continuous normalizing flows which model probability paths\nfrom known distributions to unknown distributions including those described by\ndiffusion processes. In this paper, we propose a speech enhancement based on\nconditional flow matching. The proposed method achieved the performance\ncomparable to those for the diffusion-based speech enhancement with the NFE of\n60 when the NFE was 5, and showed similar performance with the diffusion model\ncorrecting the reverse process at the same NFE from 1 to 5 without additional\nfine tuning procedure. We also have shown that the corresponding diffusion\nmodel derived from the conditional probability path with a modified optimal\ntransport conditional vector field demonstrated similar performances with the\nNFE of 5 without any fine-tuning procedure."}
{"id": "2508.06842", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06842", "abs": "https://arxiv.org/abs/2508.06842", "authors": ["Seonggyu Lee", "Sein Cheong", "Sangwook Han", "Kihyuk Kim", "Jong Won Shi"], "title": "Speech Enhancement based on cascaded two flow", "comment": "Accepted at Interspeech 2025", "summary": "Speech enhancement (SE) based on diffusion probabilistic models has exhibited\nimpressive performance, while requiring a relatively high number of function\nevaluations (NFE). Recently, SE based on flow matching has been proposed, which\nshowed competitive performance with a small NFE. Early approaches adopted the\nnoisy speech as the only conditioning variable. There have been other\napproaches which utilize speech enhanced with a predictive model as another\nconditioning variable and to sample an initial value, but they require a\nseparate predictive model on top of the generative SE model. In this work, we\npropose to employ an identical model based on flow matching for both SE and\ngenerating enhanced speech used as an initial starting point and a conditioning\nvariable. Experimental results showed that the proposed method required the\nsame or fewer NFEs even with two cascaded generative methods while achieving\nequivalent or better performances to the previous baselines."}

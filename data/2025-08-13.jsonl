{"id": "2508.08399", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08399", "abs": "https://arxiv.org/abs/2508.08399", "authors": ["Ryo Aihara", "Yoshiki Masuyama", "Gordon Wichern", "Fran√ßois G. Germain", "Jonathan Le Roux"], "title": "Exploring Disentangled Neural Speech Codecs from Self-Supervised Representations", "comment": null, "summary": "Neural audio codecs (NACs), which use neural networks to generate compact\naudio representations, have garnered interest for their applicability to many\ndownstream tasks -- especially quantized codecs due to their compatibility with\nlarge language models. However, unlike text, speech conveys not only linguistic\ncontent but also rich paralinguistic features. Encoding these elements in an\nentangled fashion may be suboptimal, as it limits flexibility. For instance,\nvoice conversion (VC) aims to convert speaker characteristics while preserving\nthe original linguistic content, which requires a disentangled representation.\nInspired by VC methods utilizing $k$-means quantization with self-supervised\nfeatures to disentangle phonetic information, we develop a discrete NAC capable\nof structured disentanglement. Experimental evaluations show that our approach\nachieves reconstruction performance on par with conventional NACs that do not\nexplicitly perform disentanglement, while also matching the effectiveness of\nconventional VC techniques."}
{"id": "2508.08585", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08585", "abs": "https://arxiv.org/abs/2508.08585", "authors": ["Yangui Fang", "Jing Peng", "Yu Xi", "Xu Li", "Haoyu Li", "Chengwei Zhang", "Guohui Zhong", "Kai Yu"], "title": "Joint decoding method for controllable contextual speech recognition based on Speech LLM", "comment": null, "summary": "Contextual speech recognition refers to the ability to identify preferences\nfor specific content based on contextual information. Recently, leveraging the\ncontextual understanding capabilities of Speech LLM to achieve contextual\nbiasing by injecting contextual information through prompts have emerged as a\nresearch hotspot.However, the direct information injection method via prompts\nrelies on the internal attention mechanism of the model, making it impossible\nto explicitly control the extent of information injection. To address this\nlimitation, we propose a joint decoding method to control the contextual\ninformation. This approach enables explicit control over the injected\ncontextual information and achieving superior recognition performance.\nAdditionally, Our method can also be used for sensitive word suppression\nrecognition.Furthermore, experimental results show that even Speech LLM not\npre-trained on long contextual data can acquire long contextual capabilities\nthrough our method."}
{"id": "2508.08715", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08715", "abs": "https://arxiv.org/abs/2508.08715", "authors": ["Xiaoxue Gao", "Huayun Zhang", "Nancy F. Chen"], "title": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs", "comment": "5 figures", "summary": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods."}
{"id": "2508.08890", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08890", "abs": "https://arxiv.org/abs/2508.08890", "authors": ["Mordehay Moradi", "Sharon Gannot"], "title": "Transient Noise Removal via Diffusion-based Speech Inpainting", "comment": "23 pages, 3 figures, signal processing paper on speech inpainting", "summary": "In this paper, we present PGDI, a diffusion-based speech inpainting framework\nfor restoring missing or severely corrupted speech segments. Unlike previous\nmethods that struggle with speaker variability or long gap lengths, PGDI can\naccurately reconstruct gaps of up to one second in length while preserving\nspeaker identity, prosody, and environmental factors such as reverberation.\nCentral to this approach is classifier guidance, specifically phoneme-level\nguidance, which substantially improves reconstruction fidelity. PGDI operates\nin a speaker-independent manner and maintains robustness even when long\nsegments are completely masked by strong transient noise, making it well-suited\nfor real-world applications, such as fireworks, door slams, hammer strikes, and\nconstruction noise. Through extensive experiments across diverse speakers and\ngap lengths, we demonstrate PGDI's superior inpainting performance and its\nability to handle challenging acoustic conditions. We consider both scenarios,\nwith and without access to the transcript during inference, showing that while\nthe availability of text further enhances performance, the model remains\neffective even in its absence. For audio samples, visit:\nhttps://mordehaym.github.io/PGDI/"}
{"id": "2508.08468", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08468", "abs": "https://arxiv.org/abs/2508.08468", "authors": ["Anis Hamadouche", "Haifeng Luo", "Mathini Sellathurai", "Tharm Ratnarajah"], "title": "Audio-Visual Speech Enhancement: Architectural Design and Deployment Strategies", "comment": null, "summary": "This paper introduces a new AI-based Audio-Visual Speech Enhancement (AVSE)\nsystem and presents a comparative performance analysis of different deployment\narchitectures. The proposed AVSE system employs convolutional neural networks\n(CNNs) for spectral feature extraction and long short-term memory (LSTM)\nnetworks for temporal modeling, enabling robust speech enhancement through\nmultimodal fusion of audio and visual cues. Multiple deployment scenarios are\ninvestigated, including cloud-based, edge-assisted, and standalone device\nimplementations. Their performance is evaluated in terms of speech quality\nimprovement, latency, and computational overhead. Real-world experiments are\nconducted across various network conditions, including Ethernet, Wi-Fi, 4G, and\n5G, to analyze the trade-offs between processing delay, communication latency,\nand perceptual speech quality. The results show that while cloud deployment\nachieves the highest enhancement quality, edge-assisted architectures offer the\nbest balance between latency and intelligibility, meeting real-time\nrequirements under 5G and Wi-Fi 6 conditions. These findings provide practical\nguidelines for selecting and optimizing AVSE deployment architectures in\ndiverse applications, including assistive hearing devices, telepresence, and\nindustrial communications."}
{"id": "2508.08257", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08257", "abs": "https://arxiv.org/abs/2508.08257", "authors": ["Zacharias Chen", "Alexa Cristelle Cahilig", "Sarah Dias", "Prithu Kolar", "Ravi Prakash", "Patrick J. Codd"], "title": "Where is the Boundary: Multimodal Sensor Fusion Test Bench for Tissue Boundary Delineation", "comment": "4 pages, 5 figures", "summary": "Robot-assisted neurological surgery is receiving growing interest due to the\nimproved dexterity, precision, and control of surgical tools, which results in\nbetter patient outcomes. However, such systems often limit surgeons' natural\nsensory feedback, which is crucial in identifying tissues -- particularly in\noncological procedures where distinguishing between healthy and tumorous tissue\nis vital. While imaging and force sensing have addressed the lack of sensory\nfeedback, limited research has explored multimodal sensing options for accurate\ntissue boundary delineation. We present a user-friendly, modular test bench\ndesigned to evaluate and integrate complementary multimodal sensors for tissue\nidentification. Our proposed system first uses vision-based guidance to\nestimate boundary locations with visual cues, which are then refined using data\nacquired by contact microphones and a force sensor. Real-time data acquisition\nand visualization are supported via an interactive graphical interface.\nExperimental results demonstrate that multimodal fusion significantly improves\nmaterial classification accuracy. The platform provides a scalable\nhardware-software solution for exploring sensor fusion in surgical applications\nand demonstrates the potential of multimodal approaches in real-time tissue\nboundary delineation."}
{"id": "2508.08924", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08924", "abs": "https://arxiv.org/abs/2508.08924", "authors": ["Rui Feng", "Yuang Chen", "Yu Hu", "Jun Du", "Jiahong Yuan"], "title": "EGGCodec: A Robust Neural Encodec Framework for EGG Reconstruction and F0 Extraction", "comment": "5 pages, 5 figures, to be appeared in IEEE Signal Processing Letters", "summary": "This letter introduces EGGCodec, a robust neural Encodec framework engineered\nfor electroglottography (EGG) signal reconstruction and F0 extraction. We\npropose a multi-scale frequency-domain loss function to capture the nuanced\nrelationship between original and reconstructed EGG signals, complemented by a\ntime-domain correlation loss to improve generalization and accuracy. Unlike\nconventional Encodec models that extract F0 directly from features, EGGCodec\nleverages reconstructed EGG signals, which more closely correspond to F0. By\nremoving the conventional GAN discriminator, we streamline EGGCodec's training\nprocess without compromising efficiency, incurring only negligible performance\ndegradation. Trained on a widely used EGG-inclusive dataset, extensive\nevaluations demonstrate that EGGCodec outperforms state-of-the-art F0\nextraction schemes, reducing mean absolute error (MAE) from 14.14 Hz to 13.69\nHz, and improving voicing decision error (VDE) by 38.2\\%. Moreover, extensive\nablation experiments validate the contribution of each component of EGGCodec."}
{"id": "2508.08550", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08550", "abs": "https://arxiv.org/abs/2508.08550", "authors": ["Chaoqun Cui", "Liangbin Huang", "Shijing Wang", "Zhe Tong", "Zhaolong Huang", "Xiao Zeng", "Xiaofeng Liu"], "title": "Fine-grained Video Dubbing Duration Alignment with Segment Supervised Preference Optimization", "comment": "This paper is accepted by ACL2025 (Main)", "summary": "Video dubbing aims to translate original speech in visual media programs from\nthe source language to the target language, relying on neural machine\ntranslation and text-to-speech technologies. Due to varying information\ndensities across languages, target speech often mismatches the source speech\nduration, causing audio-video synchronization issues that significantly impact\nviewer experience. In this study, we approach duration alignment in LLM-based\nvideo dubbing machine translation as a preference optimization problem. We\npropose the Segment Supervised Preference Optimization (SSPO) method, which\nemploys a segment-wise sampling strategy and fine-grained loss to mitigate\nduration mismatches between source and target lines. Experimental results\ndemonstrate that SSPO achieves superior performance in duration alignment\ntasks."}
{"id": "2508.08425", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08425", "abs": "https://arxiv.org/abs/2508.08425", "authors": ["Marwan Jalaleddine", "Jiajie Li", "Warren J. Gross"], "title": "Hardware-friendly IR-HARQ for Polar SCL Decoders", "comment": null, "summary": "To extend the applications of polar codes within next-generation wireless\ncommunication systems, it is essential to incorporate support for Incremental\nRedundancy (IR) Hybrid Automatic Repeat Request (HARQ) schemes. The baseline\nIR-HARQ scheme's reliance on set-based operations leads to irregular memory\naccess patterns, posing significant challenges for efficient hardware\nimplementation. Furthermore, the introduction of new bit types increases the\nnumber of fast nodes that are decoded without traversing the sub-tree,\nresulting in a substantial area overhead when implemented in hardware. To\naddress these issues and improve hardware compatibility, we propose\ntransforming the set-based operations within the polar IR-HARQ scheme into\nbinary vector operations. Additionally, we introduce a new fast node\nintegration approach that avoids increasing the number of fast nodes, thereby\nminimizing the associated area overhead. Our proposed scheme results in a\nmemory overhead of 25-27% compared to successive cancellation list (SCL)\ndecoding without IR-HARQ support."}
{"id": "2508.08925", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08925", "abs": "https://arxiv.org/abs/2508.08925", "authors": ["Zhining He", "Yang Xiao"], "title": "LPGNet: A Lightweight Network with Parallel Attention and Gated Fusion for Multimodal Emotion Recognition", "comment": "Under peering review", "summary": "Emotion recognition in conversations (ERC) aims to predict the emotional\nstate of each utterance by using multiple input types, such as text and audio.\nWhile Transformer-based models have shown strong performance in this task, they\noften face two major issues: high computational cost and heavy dependence on\nspeaker information. These problems reduce their ability to generalize in\nreal-world conversations. To solve these challenges, we propose LPGNet, a\nLightweight network with Parallel attention and Gated fusion for multimodal\nERC. The main part of LPGNet is the Lightweight Parallel Interaction Attention\n(LPIA) module. This module replaces traditional stacked Transformer layers with\nparallel dot-product attention, which can model both within-modality and\nbetween-modality relationships more efficiently. To improve emotional feature\nlearning, LPGNet also uses a dual-gated fusion method. This method filters and\ncombines features from different input types in a flexible and dynamic way. In\naddition, LPGNet removes speaker embeddings completely, which allows the model\nto work independently of speaker identity. Experiments on the IEMOCAP dataset\nshow that LPGNet reaches over 87% accuracy and F1-score in 4-class emotion\nclassification. It outperforms strong baseline models while using fewer\nparameters and showing better generalization across speakers."}
{"id": "2508.08559", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08559", "abs": "https://arxiv.org/abs/2508.08559", "authors": ["Alexandrine Fortier", "Sonal Joshi", "Thomas Thebaud", "Jesus Villalba Lopez", "Najim Dehak", "Patrick Cardinal"], "title": "Multi-Target Backdoor Attacks Against Speaker Recognition", "comment": "Accepted to IEEE Automatic Speech Recognition and Understanding\n  Workshop 2025", "summary": "In this work, we propose a multi-target backdoor attack against speaker\nidentification using position-independent clicking sounds as triggers. Unlike\nprevious single-target approaches, our method targets up to 50 speakers\nsimultaneously, achieving success rates of up to 95.04%. To simulate more\nrealistic attack conditions, we vary the signal-to-noise ratio between speech\nand trigger, demonstrating a trade-off between stealth and effectiveness. We\nfurther extend the attack to the speaker verification task by selecting the\nmost similar training speaker - based on cosine similarity - as the target. The\nattack is most effective when target and enrolled speaker pairs are highly\nsimilar, reaching success rates of up to 90% in such cases."}
{"id": "2508.08491", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08491", "abs": "https://arxiv.org/abs/2508.08491", "authors": ["Hongwei Hou", "Yafei Wang", "Xinping Yi", "Wenjin Wang", "Dirk T. M. Slock", "Shi Jin"], "title": "Tensor-Structured Bayesian Channel Prediction for Upper Mid-Band XL-MIMO Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The upper mid-band balances coverage and capacity for the future cellular\nsystems and also embraces XL-MIMO systems, offering enhanced spectral and\nenergy efficiency. However, these benefits are significantly degraded under\nmobility due to channel aging, and further exacerbated by the unique near-field\n(NF) and spatial non-stationarity (SnS) propagation in such systems. To address\nthis challenge, we propose a novel channel prediction approach that\nincorporates dedicated channel modeling, probabilistic representations, and\nBayesian inference algorithms for this emerging scenario. Specifically, we\ndevelop tensor-structured channel models in both the spatial-frequency-temporal\n(SFT) and beam-delay-Doppler (BDD) domains, which leverage temporal\ncorrelations among multiple pilot symbols for channel prediction. The factor\nmatrices of multi-linear transformations are parameterized by BDD domain grids\nand SnS factors, where beam domain grids are jointly determined by angles and\nslopes under spatial-chirp based NF representations. To enable tractable\ninference, we replace environment-dependent BDD domain grids with uniformly\nsampled ones, and introduce perturbation parameters in each domain to mitigate\ngrid mismatch. We further propose a hybrid beam domain strategy that integrates\nangle-only sampling with slope hyperparameterization to avoid the computational\nburden of explicit slope sampling. Based on the probabilistic models, we\ndevelop tensor-structured bi-layer inference (TS-BLI) algorithm under the\nexpectation-maximization (EM) framework, which reduces computational complexity\nvia tensor operations by leveraging the bi-layer factor graph for approximate\nE-step inference and an alternating strategy with closed-form updates in the\nM-step. Numerical simulations based on the near-practical channel simulator\ndemonstrate the superior channel prediction performance of the proposed\nalgorithm."}
{"id": "2508.08938", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08938", "abs": "https://arxiv.org/abs/2508.08938", "authors": ["Alexander Polok", "Santosh Kesiraju", "Karel Bene≈°", "Bolaji Yusuf", "Luk√°≈° Burget", "Jan ƒåernock√Ω"], "title": "DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition", "comment": "Accepted at IEEE ASRU 2025", "summary": "This paper presents a simple yet effective regularization for the internal\nlanguage model induced by the decoder in encoder-decoder ASR models, thereby\nimproving robustness and generalization in both in- and out-of-domain settings.\nThe proposed method, Decoder-Centric Regularization in Encoder-Decoder\n(DeCRED), adds auxiliary classifiers to the decoder, enabling next token\nprediction via intermediate logits. Empirically, DeCRED reduces the mean\ninternal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this\ntranslates into actual WER improvements over the baseline in 5 of 7 in-domain\nand 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and\n18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing\nthe baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%,\nrespectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium,\nshowing competitive WERs despite training on much less data with fewer\nparameters."}
{"id": "2508.08775", "categories": ["cs.SD", "cs.GR", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.08775", "abs": "https://arxiv.org/abs/2508.08775", "authors": ["Xutong Jin", "Guoping Wang", "Sheng Li"], "title": "SonicRadiation: A Hybrid Numerical Solution for Sound Radiation without Ghost Cells", "comment": "11 pages", "summary": "Interactive synthesis of physical sound effects is crucial in digital media\nproduction. Sound radiation simulation, a key component of physically based\nsound synthesis, has posed challenges in the context of complex object\nboundaries. Previous methods, such as ghost cell-based finite-difference\ntime-domain (FDTD) wave solver, have struggled to address these challenges,\nleading to large errors and failures in complex boundaries because of the\nlimitation of ghost cells. We present SonicRadiation, a hybrid numerical\nsolution capable of handling complex and dynamic object boundaries in sound\nradiation simulation without relying on ghost cells. We derive a consistent\nformulation to connect the physical quantities on grid cells in FDTD with the\nboundary elements in the time-domain boundary element method (TDBEM). Hereby,\nwe propose a boundary grid synchronization strategy to seamlessly integrate\nTDBEM with FDTD while maintaining high numerical accuracy. Our method holds\nboth advantages from the accuracy of TDBEM for the near-field and the\nefficiency of FDTD for the far-field. Experimental results demonstrate the\nsuperiority of our method in sound radiation simulation over previous\napproaches in terms of accuracy and efficiency, particularly in complex scenes,\nfurther validating its effectiveness."}
{"id": "2508.08506", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08506", "abs": "https://arxiv.org/abs/2508.08506", "authors": ["Mehdi Zafari", "Divyanshu Pandey", "Rahman Doost-Mohammady"], "title": "An Analytical and Experimental Study of Distributed Uplink Beamforming in the Presence of Carrier Frequency Offsets", "comment": "This work has been submitted to the IEEE Transactions on Vehicular\n  Technology for possible publication. This version includes revisions based on\n  the first-round review", "summary": "Realizing distributed multi-user beamforming (D-MUBF) in time division duplex\n(TDD)-based multi-user MIMO (MU-MIMO) systems faces significant challenges. One\nof the most fundamental challenges is achieving accurate over-the-air (OTA)\ntiming and frequency synchronization among distributed access points (APs),\nparticularly due to residual frequency offsets caused by local oscillator (LO)\ndrifts. Despite decades of research on synchronization for MU-MIMO, there are\nonly a few experimental studies that evaluate D-MUBF techniques under imperfect\nfrequency synchronization among distributed antennas. This paper presents an\nanalytical and experimental assessment of D-MUBF methods in the presence of\nfrequency synchronization errors. We provide closed-form expressions for\nsignal-to-interference-plus-noise ratio (SINR) as a function of channel\ncharacteristics and statistical properties of carrier frequency offset (CFO)\namong AP antennas. In addition, through experimental evaluations conducted with\nthe RENEW massive MIMO testbed, we collected comprehensive datasets across\nvarious experimental scenarios. These datasets comprise uplink pilot samples\nfor channel and CFO estimation, in addition to uplink multi-user data intended\nfor analyzing D-MUBF techniques. By examining these datasets, we assess the\nperformance of D-MUBF in the presence of CFO and compare the analytical\npredictions with empirical measurements. Furthermore, we make the datasets\npublicly available and provide insights on utilizing them for future research\nendeavors."}
{"id": "2508.08953", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08953", "abs": "https://arxiv.org/abs/2508.08953", "authors": ["Soo-Whan Chung", "Min-Seok Choi"], "title": "Listen through the Sound: Generative Speech Restoration Leveraging Acoustic Context Representation", "comment": "Accepted to INTERSPEECH 2025", "summary": "This paper introduces a novel approach to speech restoration by integrating a\ncontext-related conditioning strategy. Specifically, we employ the\ndiffusion-based generative restoration model, UNIVERSE++, as a backbone to\nevaluate the effectiveness of contextual representations. We incorporate\nacoustic context embeddings extracted from the CLAP model, which capture the\nenvironmental attributes of input audio. Additionally, we propose an Acoustic\nContext (ACX) representation that refines CLAP embeddings to better handle\nvarious distortion factors and their intensity in speech signals. Unlike\ncontent-based approaches that rely on linguistic and speaker attributes, ACX\nprovides contextual information that enables the restoration model to\ndistinguish and mitigate distortions better. Experimental results indicate that\ncontext-aware conditioning improves both restoration performance and its\nstability across diverse distortion conditions, reducing variability compared\nto content-based methods."}
{"id": "2508.08805", "categories": ["cs.SD", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08805", "abs": "https://arxiv.org/abs/2508.08805", "authors": ["Liam Pram", "Fabio Morreale"], "title": "Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems", "comment": "Extended version of the presentation at The First International\n  Conference in AI Music Studies 2024", "summary": "AI systems for music generation are increasingly common and easy to use,\ngranting people without any musical background the ability to create music.\nBecause of this, generative-AI has been marketed and celebrated as a means of\ndemocratizing music making. However, inclusivity often functions as marketable\nrhetoric rather than a genuine guiding principle in these industry settings. In\nthis paper, we look at four generative-AI music making systems available to the\npublic as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they\nare rhetoricized by their developers, and received by users. Our aim is to\ninvestigate ideologies that are driving the early-stage development and\nadoption of generative-AI in music making, with a particular focus on\ndemocratization. A combination of autoethnography and digital ethnography is\nused to examine patterns and incongruities in rhetoric when positioned against\nproduct functionality. The results are then collated to develop a nuanced,\ncontextual discussion. The shared ideology we map between producers and\nconsumers is individualist, globalist, techno-liberal, and ethically evasive.\nIt is a 'total ideology' which obfuscates individual responsibility, and\nthrough which the nature of music and musical practice is transfigured to suit\ngenerative outcomes."}
{"id": "2508.08571", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08571", "abs": "https://arxiv.org/abs/2508.08571", "authors": ["Anthony Joseph Perre", "Parker Huggins", "Alphan Sahin"], "title": "Learning Zero Constellations for Binary MOCZ in Fading Channels", "comment": "This work has been accepted for presentation at IEEE PIMRC Workshops\n  - Emerging Modulation Techniques Towards 6G Networks 2025", "summary": "In this work, we propose two methods to design zero constellations for binary\nmodulation on conjugate-reciprocal zeros (BMOCZ). In the first approach, we\ntreat constellation design as a multi-label binary classification problem and\nlearn the zero locations for a direct zero-testing (DiZeT) decoder. In the\nsecond approach, we introduce a neural network (NN)-based decoder and jointly\nlearn the decoder and zero constellation parameters. We show that the NN-based\ndecoder can directly generalize to flat-fading channels, despite being trained\nunder additive white Gaussian noise. Furthermore, the results of numerical\nsimulations demonstrate that learned zero constellations outperform the\ncanonical, Huffman BMOCZ constellation, with the proposed NN-based decoder\nachieving large performance gain at the expense of increased computational\ncomplexity."}
{"id": "2508.08962", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08962", "abs": "https://arxiv.org/abs/2508.08962", "authors": ["Xinyu Liang", "Fredrik Cumlin", "Victor Ungureanu", "Chandan K. A. Reddy", "Christian Schuldt", "Saikat Chatterjee"], "title": "Selection of Layers from Self-supervised Learning Models for Predicting Mean-Opinion-Score of Speech", "comment": "Accepted at IEEE ASRU 2025", "summary": "Self-supervised learning (SSL) models like Wav2Vec2, HuBERT, and WavLM have\nbeen widely used in speech processing. These transformer-based models consist\nof multiple layers, each capturing different levels of representation. While\nprior studies explored their layer-wise representations for efficiency and\nperformance, speech quality assessment (SQA) models predominantly rely on\nlast-layer features, leaving intermediate layers underexamined. In this work,\nwe systematically evaluate different layers of multiple SSL models for\npredicting mean-opinion-score (MOS). Features from each layer are fed into a\nlightweight regression network to assess effectiveness. Our experiments\nconsistently show early-layers features outperform or match those from the last\nlayer, leading to significant improvements over conventional approaches and\nstate-of-the-art MOS prediction models. These findings highlight the advantages\nof early-layer selection, offering enhanced performance and reduced system\ncomplexity."}
{"id": "2508.08892", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08892", "abs": "https://arxiv.org/abs/2508.08892", "authors": ["Yahya Sherif Solayman Mohamed Saleh", "Ahmed Mohammed Dabbous", "Lama Alkhaled", "Hum Yan Chai", "Muhammad Ehsan Rana", "Hamam Mokayed"], "title": "Sound Signal Synthesis with Auxiliary Classifier GAN, COVID-19 cough as an example", "comment": null, "summary": "One of the fastest-growing domains in AI is healthcare. Given its importance,\nit has been the interest of many researchers to deploy ML models into the\never-demanding healthcare domain to aid doctors and increase accessibility.\nDelivering reliable models, however, demands a sizable amount of data, and the\nrecent COVID-19 pandemic served as a reminder of the rampant and scary nature\nof healthcare that makes training models difficult. To alleviate such scarcity,\nmany published works attempted to synthesize radiological cough data to train\nbetter COVID-19 detection models on the respective radiological data. To\naccommodate the time sensitivity expected during a pandemic, this work focuses\non detecting COVID-19 through coughs using synthetic data to improve the\naccuracy of the classifier. The work begins by training a CNN on a balanced\nsubset of the Coughvid dataset, establishing a baseline classification test\naccuracy of 72%. The paper demonstrates how an Auxiliary Classification GAN\n(ACGAN) may be trained to conditionally generate novel synthetic Mel\nSpectrograms of both healthy and COVID-19 coughs. These coughs are used to\naugment the training dataset of the CNN classifier, allowing it to reach a new\ntest accuracy of 75%. The work highlights the expected messiness and\ninconsistency in training and offers insights into detecting and handling such\nshortcomings."}
{"id": "2508.08602", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08602", "abs": "https://arxiv.org/abs/2508.08602", "authors": ["Justin London"], "title": "Biomedical Signal Processing: EEG and ECG Classification with Discrete Wavelet Transforms, Energy Distribution, and Convolutional Neural Networks", "comment": null, "summary": "Biomedical signal processing extract meaningful information from\nphysiological signals like electrocardiograms (ECGs), electroencephalograms\n(EEGs), and electromyograms (EMGs) to diagnose, monitor, and treat medical\nconditions and diseases such as seizures, cardiomyopathy, and neuromuscular\ndisorders, respectively. Traditional manual physician analysis of electrical\nrecordings is prone to human error as subtle anomolies may not be detected.\nRecently, advanced deep learning has significantly improved the accuracy of\nbiomedical signal analysis. A multi-modal deep learning model is proposed that\nutilizes discrete wavelet transforms for signal pre-processing to reduce noise.\nA multi-modal image fusion and multimodal feature fusion framework is utilized\nthat converts numeric biomedical signals into 2D and 3D images for image\nprocessing using Gramian angular fields, recurrency plots, and Markov\ntransition fields. In this paper, deep learning models are applied to ECG, EEG,\nand human activity signals using actual medical datasets, brain, and heart\nrecordings. The results demonstrate that using a multi-modal approach using\nwavelet transforms improves the accuracy of disease and disorder\nclassification."}
{"id": "2508.08961", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08961", "abs": "https://arxiv.org/abs/2508.08961", "authors": ["Yuanyuan Wang", "Dongchao Yang", "Yiwen Shao", "Hangting Chen", "Jiankun Zhao", "Zhiyong Wu", "Helen Meng", "Xixin Wu"], "title": "DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models", "comment": null, "summary": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model."}
{"id": "2508.08957", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08957", "abs": "https://arxiv.org/abs/2508.08957", "authors": ["Chien-Chun Wang", "Kuan-Tang Huang", "Cheng-Yeh Yang", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen"], "title": "QAMRO: Quality-aware Adaptive Margin Ranking Optimization for Human-aligned Assessment of Audio Generation Systems", "comment": "Accepted to IEEE ASRU 2025", "summary": "Evaluating audio generation systems, including text-to-music (TTM),\ntext-to-speech (TTS), and text-to-audio (TTA), remains challenging due to the\nsubjective and multi-dimensional nature of human perception. Existing methods\ntreat mean opinion score (MOS) prediction as a regression problem, but standard\nregression losses overlook the relativity of perceptual judgments. To address\nthis limitation, we introduce QAMRO, a novel Quality-aware Adaptive Margin\nRanking Optimization framework that seamlessly integrates regression objectives\nfrom different perspectives, aiming to highlight perceptual differences and\nprioritize accurate ratings. Our framework leverages pre-trained audio-text\nmodels such as CLAP and Audiobox-Aesthetics, and is trained exclusively on the\nofficial AudioMOS Challenge 2025 dataset. It demonstrates superior alignment\nwith human evaluations across all dimensions, significantly outperforming\nrobust baseline models."}
{"id": "2508.08620", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08620", "abs": "https://arxiv.org/abs/2508.08620", "authors": ["Yang Lu", "Shengli Zhang", "Chang Liu", "Ruichen Zhang", "Bo Ai", "Dusit Niyato", "Wei Ni", "Xianbin Wang", "Abbas Jamalipour"], "title": "Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence: A Survey", "comment": null, "summary": "The rapid advancement of communication technologies has driven the evolution\nof communication networks towards both high-dimensional resource utilization\nand multifunctional integration. This evolving complexity poses significant\nchallenges in designing communication networks to satisfy the growing\nquality-of-service and time sensitivity of mobile applications in dynamic\nenvironments. Graph neural networks (GNNs) have emerged as fundamental deep\nlearning (DL) models for complex communication networks. GNNs not only augment\nthe extraction of features over network topologies but also enhance scalability\nand facilitate distributed computation. However, most existing GNNs follow a\ntraditional passive learning framework, which may fail to meet the needs of\nincreasingly diverse wireless systems. This survey proposes the employment of\nagentic artificial intelligence (AI) to organize and integrate GNNs, enabling\nscenario- and task-aware implementation towards edge general intelligence. To\ncomprehend the full capability of GNNs, we holistically review recent\napplications of GNNs in wireless communications and networking. Specifically,\nwe focus on the alignment between graph representations and network topologies,\nand between neural architectures and wireless tasks. We first provide an\noverview of GNNs based on prominent neural architectures, followed by the\nconcept of agentic GNNs. Then, we summarize and compare GNN applications for\nconventional systems and emerging technologies, including physical, MAC, and\nnetwork layer designs, integrated sensing and communication (ISAC),\nreconfigurable intelligent surface (RIS) and cell-free network architecture. We\nfurther propose a large language model (LLM) framework as an intelligent\nquestion-answering agent, leveraging this survey as a local knowledge base to\nenable GNN-related responses tailored to wireless communication research."}
{"id": "2508.09126", "categories": ["cs.SD", "cs.SE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.09126", "abs": "https://arxiv.org/abs/2508.09126", "authors": ["Christopher Mitcheltree", "Bogdan Teleaga", "Andrew Fyfe", "Naotake Masuda", "Matthias Sch√§fer", "Alfie Bradic", "Nao Tokui"], "title": "Neutone SDK: An Open Source Framework for Neural Audio Processing", "comment": "Accepted to AES International Conference on Artificial Intelligence\n  and Machine Learning for Audio 2025", "summary": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk"}
{"id": "2508.08961", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08961", "abs": "https://arxiv.org/abs/2508.08961", "authors": ["Yuanyuan Wang", "Dongchao Yang", "Yiwen Shao", "Hangting Chen", "Jiankun Zhao", "Zhiyong Wu", "Helen Meng", "Xixin Wu"], "title": "DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models", "comment": null, "summary": "Extending pre-trained Large Language Models (LLMs)'s speech understanding or\ngeneration abilities by introducing various effective speech tokens has\nattracted great attention in the speech community. However, building a unified\nspeech understanding and generation model still faces the following challenges:\n(1) Due to the huge modality gap between speech tokens and text tokens,\nextending text LLMs to unified speech LLMs relies on large-scale paired data\nfor fine-tuning, and (2) Generation and understanding tasks prefer information\nat different levels, e.g., generation benefits from detailed acoustic features,\nwhile understanding favors high-level semantics. This divergence leads to\ndifficult performance optimization in one unified model. To solve these\nchallenges, in this paper, we present two key insights in speech tokenization\nand speech language modeling. Specifically, we first propose an\nUnderstanding-driven Speech Tokenizer (USTokenizer), which extracts high-level\nsemantic information essential for accomplishing understanding tasks using text\nLLMs. In this way, USToken enjoys better modality commonality with text, which\nreduces the difficulty of modality alignment in adapting text LLMs to speech\nLLMs. Secondly, we present DualSpeechLM, a dual-token modeling framework that\nconcurrently models USToken as input and acoustic token as output within a\nunified, end-to-end framework, seamlessly integrating speech understanding and\ngeneration capabilities. Furthermore, we propose a novel semantic supervision\nloss and a Chain-of-Condition (CoC) strategy to stabilize model training and\nenhance speech generation performance. Experimental results demonstrate that\nour proposed approach effectively fosters a complementary relationship between\nunderstanding and generation tasks, highlighting the promising strategy of\nmutually enhancing both tasks in one unified model."}
{"id": "2508.08663", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08663", "abs": "https://arxiv.org/abs/2508.08663", "authors": ["Vidya Bhasker Shukla", "Italo Atzeni"], "title": "Sparse Near-Field Channel Estimation for XL-MIMO via Adaptive Filtering", "comment": "To be presented at WSA 2025", "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) systems\noperating at sub-THz carrier frequencies represent a promising solution to meet\nthe demands of next-generation wireless applications. This work focuses on\nsparse channel estimation for XL-MIMO systems operating in the near-field (NF)\nregime. Assuming a practical subarray-based architecture, we develop a NF\nchannel estimation framework based on adaptive filtering, referred to as\n\\textit{polar-domain zero-attracting least mean squares (PD-ZALMS)}. The\nproposed method achieves significantly superior channel estimation accuracy and\nlower computational complexity compared with the well-established polar-domain\northogonal matching pursuit. In addition, the proposed PD-ZALMS is shown to\noutperform the oracle least-squares channel estimator at low-to-moderate\nsignal-to-noise ratio."}
{"id": "2508.08967", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08967", "abs": "https://arxiv.org/abs/2508.08967", "authors": ["Kuan-Tang Huang", "Li-Wei Chen", "Hung-Shin Lee", "Berlin Chen", "Hsin-Min Wang"], "title": "Revealing the Role of Audio Channels in ASR Performance Degradation", "comment": "Accepted to IEEE ASRU 2025", "summary": "Pre-trained automatic speech recognition (ASR) models have demonstrated\nstrong performance on a variety of tasks. However, their performance can\ndegrade substantially when the input audio comes from different recording\nchannels. While previous studies have demonstrated this phenomenon, it is often\nattributed to the mismatch between training and testing corpora. This study\nargues that variations in speech characteristics caused by different recording\nchannels can fundamentally harm ASR performance. To address this limitation, we\npropose a normalization technique designed to mitigate the impact of channel\nvariation by aligning internal feature representations in the ASR model with\nthose derived from a clean reference channel. This approach significantly\nimproves ASR performance on previously unseen channels and languages,\nhighlighting its ability to generalize across channel and language differences."}
{"id": "2508.08686", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08686", "abs": "https://arxiv.org/abs/2508.08686", "authors": ["Ming Lyu", "Hao Chen", "Dan Wang", "Chen Qiu", "Guangyin Feng", "Nan Ma", "Xiaodong Xu"], "title": "VQ-VAE Based Digital Semantic Communication with Importance-Aware OFDM Transmission", "comment": "6 pages, 5 figures, conference", "summary": "Semantic communication (SemCom) significantly reduces redundant data and\nimproves transmission efficiency by extracting the latent features of\ninformation. However, most of the conventional deep learning-based SemCom\nsystems focus on analog transmission and lack in compatibility with practical\ndigital communications. This paper proposes a vector quantized-variational\nautoencoder (VQ-VAE) based digital SemCom system that directly transmits the\nsemantic features and incorporates the importance-aware orthogonal frequency\ndivision multiplexing (OFDM) transmission to enhance the SemCom performance,\nwhere the VQ-VAE generates a discrete codebook shared between the transmitter\nand receiver. At transmitter, the latent semantic features are firstly\nextracted by VQ-VAE, and then the shared codebook is adopted to match these\nfeatures, which are subsequently transformed into a discrete version to adapt\nthe digital transmission. To protect the semantic information, an\nimportance-aware OFDM transmission strategy is proposed to allocate the key\nfeatures near the OFDM reference signals, where the feature importance is\nderived from the gradient-based method. At the receiver, the features are\nrematched with the shared codebook to further correct errors. Finally,\nexperimental results demonstrate that our proposed scheme outperforms the\nconventional DeepSC and achieves better reconstruction performance under low\nSNR region."}
{"id": "2508.09126", "categories": ["cs.SD", "cs.SE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.09126", "abs": "https://arxiv.org/abs/2508.09126", "authors": ["Christopher Mitcheltree", "Bogdan Teleaga", "Andrew Fyfe", "Naotake Masuda", "Matthias Sch√§fer", "Alfie Bradic", "Nao Tokui"], "title": "Neutone SDK: An Open Source Framework for Neural Audio Processing", "comment": "Accepted to AES International Conference on Artificial Intelligence\n  and Machine Learning for Audio 2025", "summary": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk"}
{"id": "2508.08757", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08757", "abs": "https://arxiv.org/abs/2508.08757", "authors": ["Mateen Ashraf", "Shahab Jahanbazi", "Onel L. A. L√≥pez"], "title": "Evaluating Task Execution Performance Under Energy Measurement Overhead", "comment": null, "summary": "Energy-awareness for adapting task execution behavior can bring several\nbenefits in terms of performance improvement in energy harvesting (EH) Internet\nof Things (IoT) devices. However, the energy measurement cost of acquiring\nenergy information, which is traditionally ignored, can potentially neutralize\nor even reverse the potential benefits. This paper highlights operational\nparameters, such as energy measurement frequency and task execution frequency,\nwhich can be tuned to improve the task execution performance of an EH-IoT\ndevice. To this end, we consider energy-blind (EB) and energy-aware (EA) task\ndecision approaches and compare their task completion rate performance. We show\nthat, for specific hardware design parameters of an EH-IoT device, there exists\nan optimal energy measurement/task execution frequency that can maximize the\ntask completion rate in both approaches. Moreover, if these parameters are not\nchosen appropriately, then energy measurement costs can cause EA scheduling to\nunderperform compared to EB scheduling."}
{"id": "2508.08890", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08890", "abs": "https://arxiv.org/abs/2508.08890", "authors": ["Mordehay Moradi", "Sharon Gannot"], "title": "Transient Noise Removal via Diffusion-based Speech Inpainting", "comment": "23 pages, 3 figures, signal processing paper on speech inpainting", "summary": "In this paper, we present PGDI, a diffusion-based speech inpainting framework\nfor restoring missing or severely corrupted speech segments. Unlike previous\nmethods that struggle with speaker variability or long gap lengths, PGDI can\naccurately reconstruct gaps of up to one second in length while preserving\nspeaker identity, prosody, and environmental factors such as reverberation.\nCentral to this approach is classifier guidance, specifically phoneme-level\nguidance, which substantially improves reconstruction fidelity. PGDI operates\nin a speaker-independent manner and maintains robustness even when long\nsegments are completely masked by strong transient noise, making it well-suited\nfor real-world applications, such as fireworks, door slams, hammer strikes, and\nconstruction noise. Through extensive experiments across diverse speakers and\ngap lengths, we demonstrate PGDI's superior inpainting performance and its\nability to handle challenging acoustic conditions. We consider both scenarios,\nwith and without access to the transcript during inference, showing that while\nthe availability of text further enhances performance, the model remains\neffective even in its absence. For audio samples, visit:\nhttps://mordehaym.github.io/PGDI/"}
{"id": "2508.08771", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08771", "abs": "https://arxiv.org/abs/2508.08771", "authors": ["Atta Ullah", "Daniyal Munir", "Daniel Lindenschmitt", "Hans D. Schotten"], "title": "Wideband Coplanar Waveguide MIMO Antenna for 6G Millimeter-Wave Applications with Defected Ground Structure", "comment": "Accepted for presentation at IEEE AP-S/URSI 2025", "summary": "This research study introduces a novel small antenna with wideband capacity\nfor the higher frequency range. As a possible contender for 6G wireless\nnetworks, the proposed antenna is designed to target the 6G Millimeter-Wave\n(mmWave) operating bands spanning 25 GHz to 33.5 GHz. With a microstrip patch\nstructure fed by a coplanar waveguide (CPW) with the defected ground structure\n(DGS), a single antenna is introduced and then a design of 2 x 2 MIMO antenna\nis presented. The single antenna has 2 elements, while the 2 x 2 MIMO antenna\nhas 8 elements. It achieves remarkably well in terms of return loss of 8.5 GHz\nwideband, which is anticipated to be used for several applications in 6G mmWave\ntechnology."}
{"id": "2508.08925", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08925", "abs": "https://arxiv.org/abs/2508.08925", "authors": ["Zhining He", "Yang Xiao"], "title": "LPGNet: A Lightweight Network with Parallel Attention and Gated Fusion for Multimodal Emotion Recognition", "comment": "Under peering review", "summary": "Emotion recognition in conversations (ERC) aims to predict the emotional\nstate of each utterance by using multiple input types, such as text and audio.\nWhile Transformer-based models have shown strong performance in this task, they\noften face two major issues: high computational cost and heavy dependence on\nspeaker information. These problems reduce their ability to generalize in\nreal-world conversations. To solve these challenges, we propose LPGNet, a\nLightweight network with Parallel attention and Gated fusion for multimodal\nERC. The main part of LPGNet is the Lightweight Parallel Interaction Attention\n(LPIA) module. This module replaces traditional stacked Transformer layers with\nparallel dot-product attention, which can model both within-modality and\nbetween-modality relationships more efficiently. To improve emotional feature\nlearning, LPGNet also uses a dual-gated fusion method. This method filters and\ncombines features from different input types in a flexible and dynamic way. In\naddition, LPGNet removes speaker embeddings completely, which allows the model\nto work independently of speaker identity. Experiments on the IEMOCAP dataset\nshow that LPGNet reaches over 87% accuracy and F1-score in 4-class emotion\nclassification. It outperforms strong baseline models while using fewer\nparameters and showing better generalization across speakers."}
{"id": "2508.08782", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08782", "abs": "https://arxiv.org/abs/2508.08782", "authors": ["Wessel L. van Nierop", "Ois√≠n Nolan", "Tristan S. W. Stevens", "Ruud J. G. van Sloun"], "title": "Patient-Adaptive Focused Transmit Beamforming using Cognitive Ultrasound", "comment": null, "summary": "Focused transmit beamforming is the most commonly used acquisition scheme for\nechocardiograms, but suffers from relatively low frame rates, and in 3D, even\nlower volume rates. Fast imaging based on unfocused transmits has disadvantages\nsuch as motion decorrelation and limited harmonic imaging capabilities. This\nwork introduces a patient-adaptive focused transmit scheme that has the ability\nto drastically reduce the number of transmits needed to produce a high-quality\nultrasound image. The method relies on posterior sampling with a temporal\ndiffusion model to perceive and reconstruct the anatomy based on partial\nobservations, while subsequently taking an action to acquire the most\ninformative transmits. This active perception modality outperforms random and\nequispaced subsampling on the 2D EchoNet-Dynamic dataset and a 3D Philips\ndataset, where we actively select focused elevation planes. Furthermore, we\nshow it achieves better performance in terms of generalized contrast-to-noise\nratio when compared to the same number of diverging waves transmits on three\nin-house echocardiograms. Additionally, we can estimate ejection fraction using\nonly 2% of the total transmits and show that the method is robust to outlier\npatients. Finally, our method can be run in real-time on GPU accelerators from\n2023. The code is publicly available at https://tue-bmd.github.io/ulsa/"}
{"id": "2508.08953", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.08953", "abs": "https://arxiv.org/abs/2508.08953", "authors": ["Soo-Whan Chung", "Min-Seok Choi"], "title": "Listen through the Sound: Generative Speech Restoration Leveraging Acoustic Context Representation", "comment": "Accepted to INTERSPEECH 2025", "summary": "This paper introduces a novel approach to speech restoration by integrating a\ncontext-related conditioning strategy. Specifically, we employ the\ndiffusion-based generative restoration model, UNIVERSE++, as a backbone to\nevaluate the effectiveness of contextual representations. We incorporate\nacoustic context embeddings extracted from the CLAP model, which capture the\nenvironmental attributes of input audio. Additionally, we propose an Acoustic\nContext (ACX) representation that refines CLAP embeddings to better handle\nvarious distortion factors and their intensity in speech signals. Unlike\ncontent-based approaches that rely on linguistic and speaker attributes, ACX\nprovides contextual information that enables the restoration model to\ndistinguish and mitigate distortions better. Experimental results indicate that\ncontext-aware conditioning improves both restoration performance and its\nstability across diverse distortion conditions, reducing variability compared\nto content-based methods."}
{"id": "2508.08790", "categories": ["eess.SP", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.08790", "abs": "https://arxiv.org/abs/2508.08790", "authors": ["Kumar Pratik", "Pouriya Sadeghi", "Gabriele Cesa", "Sanaz Barghi", "Joseph B. Soriaga", "Yuanning Yu", "Supratik Bhattacharjee", "Arash Behboodi"], "title": "ReQuestNet: A Foundational Learning model for Channel Estimation", "comment": "Accepted at IEEE Globecom 2025. Permission from IEEE must be obtained\n  for all other uses, in any current or future media, including\n  reprinting/republishing this material for advertising or promotional\n  purposes, creating new collective works, for resale or redistribution to\n  servers or lists, or reuse of any copyrighted component of this work in other\n  works", "summary": "In this paper, we present a novel neural architecture for channel estimation\n(CE) in 5G and beyond, the Recurrent Equivariant UERS Estimation Network\n(ReQuestNet). It incorporates several practical considerations in wireless\ncommunication systems, such as ability to handle variable number of resource\nblock (RB), dynamic number of transmit layers, physical resource block groups\n(PRGs) bundling size (BS), demodulation reference signal (DMRS) patterns with a\nsingle unified model, thereby, drastically simplifying the CE pipeline. Besides\nit addresses several limitations of the legacy linear MMSE solutions, for\nexample, by being independent of other reference signals and particularly by\njointly processing MIMO layers and differently precoded channels with unknown\nprecoding at the receiver. ReQuestNet comprises of two sub-units, CoarseNet\nfollowed by RefinementNet. CoarseNet performs per PRG, per transmit-receive\n(Tx-Rx) stream channel estimation, while RefinementNet refines the CoarseNet\nchannel estimate by incorporating correlations across differently precoded\nPRGs, and correlation across multiple input multiple output (MIMO) channel\nspatial dimensions (cross-MIMO). Simulation results demonstrate that ReQuestNet\nsignificantly outperforms genie minimum mean squared error (MMSE) CE across a\nwide range of channel conditions, delay-Doppler profiles, achieving up to 10dB\ngain at high SNRs. Notably, ReQuestNet generalizes effectively to unseen\nchannel profiles, efficiently exploiting inter-PRG and cross-MIMO correlations\nunder dynamic PRG BS and varying transmit layer allocations."}
{"id": "2508.08796", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08796", "abs": "https://arxiv.org/abs/2508.08796", "authors": ["Jun Dong", "Tianwai Bo", "Zhuo Wang", "Haolei Gao", "Zhongwei Tan", "Yi Dong"], "title": "Iterative Distortion Cancellation Algorithms for Single-Sideband Systems", "comment": null, "summary": "We propose an iterative distortion cancellation algorithm to digitally\nmitigate the impact of double-sideband dither signal amplitude from the\nautomatic bias control module on Kramers-Kronig receivers without modifying\nphysical layer structures. The algorithm utilizes the KK relation for initial\nsignal decisions and reconstructs the distortion caused by dither signals.\nExperimental tests in back-to-back showed it improved tolerance to dither\namplitudes up to 10% V{\\pi}. For 80-km fiber transmission, the algorithm\nincreased the receiver sensitivity by more than 1 dB, confirming the\neffectiveness of the proposed distortion cancellation method."}
{"id": "2508.08894", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08894", "abs": "https://arxiv.org/abs/2508.08894", "authors": ["Sicong Ye", "Yulan Gao", "Ming Xiao", "Peng Wang", "Marios Poulakis", "Ulrik Imberg"], "title": "Trajectory-adaptive Beam Shaping: Towards Beam-Management-Free Near-field Communications", "comment": null, "summary": "The quest for higher wireless carrier frequencies spanning the\nmillimeter-wave (mmWave) and Terahertz (THz) bands heralds substantial\nenhancements in data throughput and spectral efficiency for next-generation\nwireless networks. However, these gains come at the cost of severe path loss\nand a heightened risk of beam misalignment due to user mobility, especially\npronounced in near-field communication. Traditional solutions rely on extremely\ndirectional beamforming and frequent beam updates via beam management, but such\ntechniques impose formidable computational and signaling overhead. In response,\nwe propose a novel approach termed trajectory-adaptive beam shaping (TABS) that\neliminates the need for real-time beam management by shaping the\nelectromagnetic wavefront to follow the user's predefined trajectory. Drawing\ninspiration from self-accelerating beams in optics, TABS concentrates energy\nalong pre-defined curved paths corresponding to the user's motion without\nrequiring real-time beam reconfiguration. We further introduce a dedicated\nquantitative metric to characterize performance under the TABS framework.\nComprehensive simulations substantiate the superiority of TABS in terms of link\nperformance, overhead reduction, and implementation complexity."}
{"id": "2508.08993", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08993", "abs": "https://arxiv.org/abs/2508.08993", "authors": ["Giulia Torcolacci", "Malte Schellmann", "Davide Dardari"], "title": "Scalable RIS-Aided Beamforming Strategies for Near-Field MU-MISO via Multi-Antenna Feeder", "comment": "This work has been submitted to IEEE for possible publication", "summary": "This paper investigates a modular beamforming framework for reconfigurable\nintelligent surface (RIS)-aided multi-user (MU) communications in the\nnear-field regime, built upon a novel antenna architecture integrating an\nactive multi-antenna feeder (AMAF) array with a transmissive RIS (T-RIS),\nreferred to as AT-RIS. This decoupling enables coordinated yet independently\nconfigurable designs in the AMAF and T-RIS domains, supporting flexible\nstrategies with diverse complexity-performance trade-offs. Several\nimplementations are analyzed, including diagonal and non-diagonal T-RIS\narchitectures, paired with precoding schemes based on focusing, minimum mean\nsquare error, and eigenmode decomposition. Simulation results demonstrate that\nwhile non-diagonal schemes maximize sum rate in scenarios with a limited number\nof User Equipments (UEs) and high angular separability, they exhibit fairness\nand scalability limitations as UE density increases. Conversely, diagonal T-RIS\nconfigurations, particularly the proposed focusing-based scheme with uniform\nfeeder-side power allocation, offer robust, fair, and scalable performance with\nminimal channel state information. The findings emphasize the critical impact\nof UEs' angular separability and reveal inherent trade-offs among spectral\nefficiency, complexity, and fairness, positioning diagonal AT-RIS architectures\nas practical solutions for scalable near-field MU multiple-input single-output\nsystems."}
{"id": "2508.09020", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.09020", "abs": "https://arxiv.org/abs/2508.09020", "authors": ["Maria Cecilia Fern√°ndez Montefiore", "Gustavo Gonz√°lez", "F. Javier L√≥pez-Mart√≠nez", "Fernando Gregorio"], "title": "Improved SINR Approximation for Downlink SDMA-based Networks with Outdated Channel State Information", "comment": "5 pages, 3 figures. This work has been submitted to the IEEE for\n  publication", "summary": "Understanding the performance of multi-user multiple-input multiple-output\n(MU-MIMO) systems under imperfect channel state information at the transmitter\n(CSIT) remains a critical challenge in next-generation wireless networks. In\nthis context, accurate statistical modeling of the\nsignal-to-interference-plus-noise ratio (SINR) is essential for enabling\ntractable performance analysis of multi-user systems. This paper presents an\nimproved statistical approximation of the SINR for downlink (DL) MU-MIMO\nsystems with imperfect CSIT. The proposed model retains the analytical\nsimplicity of existing approaches (e.g., Gamma-based approximations) while\novercoming their limitations, particularly the underestimation of SINR\nvariance. We evaluate the proposed approximation in the context of\nRate-Splitting Multiple Access (RSMA)-enabled MIMO DL systems with outdated\nCSIT. The results demonstrate excellent accuracy across a wide range of system\nconfigurations, including varying numbers of users, antennas, and degrees of\nCSIT staleness."}
{"id": "2508.09055", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09055", "abs": "https://arxiv.org/abs/2508.09055", "authors": ["Lorenzo Cazzella", "Francesco Linsalata", "Mahdi Maleki", "Damiano Badini", "Matteo Matteucci", "Umberto Spagnolini"], "title": "Chartwin: a Case Study on Channel Charting-aided Localization in Dynamic Digital Network Twins", "comment": null, "summary": "Wireless communication systems can significantly benefit from the\navailability of spatially consistent representations of the wireless channel to\nefficiently perform a wide range of communication tasks. Towards this purpose,\nchannel charting has been introduced as an effective unsupervised learning\ntechnique to achieve both locally and globally consistent radio maps. In this\nletter, we propose Chartwin, a case study on the integration of\nlocalization-oriented channel charting with dynamic Digital Network Twins\n(DNTs). Numerical results showcase the significant performance of\nsemi-supervised channel charting in constructing a spatially consistent chart\nof the considered extended urban environment. The considered method results in\n$\\approx$ 4.5 m localization error for the static DNT and $\\approx$ 6 m in the\ndynamic DNT, fostering DNT-aided channel charting and localization."}
{"id": "2508.09117", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09117", "abs": "https://arxiv.org/abs/2508.09117", "authors": ["Joseph Boccuzzi"], "title": "Spectral Efficiency Considerations for 6G", "comment": "13 pages, 12 figures", "summary": "As wireless connectivity continues to evolve towards 6G, there is an\never-increasing demand to not only deliver higher throughput, lower latency,\nand improved reliability, but also do so as efficiently as possible. To this\npoint, the term efficiency has been quantified through applications to Spectral\nEfficiency (SE) and Energy Efficiency (EE). In this paper we introduce a new\nsystem metric called Radio Resource Utilization Efficiency (RUE). This metric\nquantifies the efficiency of the available radio resources (Spectrum, Access\nMethod, Time Slots, Data Symbols, etc.) used to deliver future 6G demands. We\ncompare the system performance of Typical Cellular and Cell-Free Massive MIMO\ndeployments as a vehicle to demonstrate the need for this new metric. We begin\nby providing a concise treatment of items impacting SE by introducing three\ncategories: 5G Radio Resources, Practical Limitations (such as channel matrix\nrank deficiency) and Implementation Losses (SINR degradation). For the example\nRadio Access Technology configuration analyzed, we show 5G yields an RUE of 47%\n(revealing significant room for improvement when defining 6G). Practical\nlimitation assumptions are compared to 5G Multi-User MIMO (MU-MIMO)\nmeasurements conducted in a commercialized deployment. SE losses are\ncharacterized to offer guidance to advanced algorithms employing Machine\nLearning (ML) based techniques. We present the benefits of increasing the\ntransmission Bandwidth (BW) from 100MHz to 1.6GHz. We describe a Next\nGeneration RAN architecture that can support 6G and AI-RAN."}
{"id": "2508.08399", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08399", "abs": "https://arxiv.org/abs/2508.08399", "authors": ["Ryo Aihara", "Yoshiki Masuyama", "Gordon Wichern", "Fran√ßois G. Germain", "Jonathan Le Roux"], "title": "Exploring Disentangled Neural Speech Codecs from Self-Supervised Representations", "comment": null, "summary": "Neural audio codecs (NACs), which use neural networks to generate compact\naudio representations, have garnered interest for their applicability to many\ndownstream tasks -- especially quantized codecs due to their compatibility with\nlarge language models. However, unlike text, speech conveys not only linguistic\ncontent but also rich paralinguistic features. Encoding these elements in an\nentangled fashion may be suboptimal, as it limits flexibility. For instance,\nvoice conversion (VC) aims to convert speaker characteristics while preserving\nthe original linguistic content, which requires a disentangled representation.\nInspired by VC methods utilizing $k$-means quantization with self-supervised\nfeatures to disentangle phonetic information, we develop a discrete NAC capable\nof structured disentanglement. Experimental evaluations show that our approach\nachieves reconstruction performance on par with conventional NACs that do not\nexplicitly perform disentanglement, while also matching the effectiveness of\nconventional VC techniques."}
{"id": "2508.08468", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08468", "abs": "https://arxiv.org/abs/2508.08468", "authors": ["Anis Hamadouche", "Haifeng Luo", "Mathini Sellathurai", "Tharm Ratnarajah"], "title": "Audio-Visual Speech Enhancement: Architectural Design and Deployment Strategies", "comment": null, "summary": "This paper introduces a new AI-based Audio-Visual Speech Enhancement (AVSE)\nsystem and presents a comparative performance analysis of different deployment\narchitectures. The proposed AVSE system employs convolutional neural networks\n(CNNs) for spectral feature extraction and long short-term memory (LSTM)\nnetworks for temporal modeling, enabling robust speech enhancement through\nmultimodal fusion of audio and visual cues. Multiple deployment scenarios are\ninvestigated, including cloud-based, edge-assisted, and standalone device\nimplementations. Their performance is evaluated in terms of speech quality\nimprovement, latency, and computational overhead. Real-world experiments are\nconducted across various network conditions, including Ethernet, Wi-Fi, 4G, and\n5G, to analyze the trade-offs between processing delay, communication latency,\nand perceptual speech quality. The results show that while cloud deployment\nachieves the highest enhancement quality, edge-assisted architectures offer the\nbest balance between latency and intelligibility, meeting real-time\nrequirements under 5G and Wi-Fi 6 conditions. These findings provide practical\nguidelines for selecting and optimizing AVSE deployment architectures in\ndiverse applications, including assistive hearing devices, telepresence, and\nindustrial communications."}
{"id": "2508.08715", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08715", "abs": "https://arxiv.org/abs/2508.08715", "authors": ["Xiaoxue Gao", "Huayun Zhang", "Nancy F. Chen"], "title": "MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs", "comment": "5 figures", "summary": "Generative speech models have demonstrated significant potential in\npersonalizing teacher-student interactions, offering valuable real-world\napplications for language learning in children's education. However, achieving\nhigh-quality, child-friendly speech generation remains challenging,\nparticularly for low-resource languages across diverse languages and cultural\ncontexts. In this paper, we propose MultiAiTutor, an educational multilingual\ngenerative AI tutor with child-friendly designs, leveraging LLM architecture\nfor speech generation tailored for educational purposes. We propose to\nintegrate age-appropriate multilingual speech generation using LLM\narchitectures, facilitating young children's language learning through\nculturally relevant image-description tasks in three low-resource languages:\nSingaporean-accent Mandarin, Malay, and Tamil. Experimental results from both\nobjective metrics and subjective evaluations demonstrate the superior\nperformance of the proposed MultiAiTutor compared to baseline methods."}

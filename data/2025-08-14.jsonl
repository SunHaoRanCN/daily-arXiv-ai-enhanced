{"id": "2508.09228", "categories": ["eess.AS", "cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.09228", "abs": "https://arxiv.org/abs/2508.09228", "authors": ["A F M Saif", "Lisha Chen", "Xiaodong Cui", "Songtao Lu", "Brian Kingsbury", "Tianyi Chen"], "title": "Objective Soups: Multilingual Multi-Task Modeling for Speech Processing", "comment": null, "summary": "Training a single model for multilingual, multi-task speech processing (MSP)\nis severely hampered by conflicting objectives between tasks like speech\nrecognition and translation. While multi-objective optimization (MOO) aims to\nalign gradient updates, its effectiveness diminishes as the number of tasks\ngrows, making it difficult to find a common descent direction. This raises a\nfundamental question: should highly conflicting objectives be optimized jointly\nor separated into a hierarchical structure? To address this question, this\npaper investigates three multi-objective MSP formulations, which we refer to as\n\\textbf{objective soup recipes}. These formulations apply multi-objective\noptimization at different optimization levels to mitigate potential conflicts\namong all objectives. To ensure efficiency, we introduce a lightweight\nlayer-selection mechanism that computes the conflict-avoiding gradient using\nonly the most problematic layers, minimizing computational and memory overhead.\nExtensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a\nbi-level recipe separating recognition and translation tasks consistently\noutperforms standard flat optimization. Our work demonstrates that hierarchical\nMOO is a more effective and scalable approach for building state-of-the-art MSP\nmodels. Our code has been released at\nhttps://github.com/afmsaif/Objective_Soups."}
{"id": "2508.09294", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09294", "abs": "https://arxiv.org/abs/2508.09294", "authors": ["Xi Xuan", "Zimo Zhu", "Wenxin Zhang", "Yi-Cheng Lin", "Tomi Kinnunen"], "title": "Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative", "comment": "Accepted at IEEE ASRU 2025", "summary": "Advances in speech synthesis intensify security threats, motivating real-time\ndeepfake detection research. We investigate whether bidirectional Mamba can\nserve as a competitive alternative to Self-Attention in detecting synthetic\nspeech. Our solution, Fake-Mamba, integrates an XLSR front-end with\nbidirectional Mamba to capture both local and global artifacts. Our core\ninnovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and\nPN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can\neffectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof\n21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and\n5.85% EER, respectively, representing substantial relative gains over SOTA\nmodels XLSR-Conformer and XLSR-Mamba. The framework maintains real-time\ninference across utterance lengths, demonstrating strong generalization and\npractical viability. The code is available at\nhttps://github.com/xuanxixi/Fake-Mamba."}
{"id": "2508.09389", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09389", "abs": "https://arxiv.org/abs/2508.09389", "authors": ["Eray Eren", "Qingju Liu", "Hyeongwoo Kim", "Pablo Garrido", "Abeer Alwan"], "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs", "comment": "Interspeech 2025; demo page at\n  https://promode8272.github.io/promode/index.html", "summary": "Prosody conveys rich emotional and semantic information of the speech signal\nas well as individual idiosyncrasies. We propose a stand-alone model that maps\ntext-to-prosodic features such as F0 and energy and can be used in downstream\ntasks such as TTS. The ProMode encoder takes as input acoustic features and\ntime-aligned textual content, both are partially masked, and obtains a\nfixed-length latent prosodic embedding. The decoder predicts acoustics in the\nmasked region using both the encoded prosody input and unmasked textual\ncontent. Trained on the GigaSpeech dataset, we compare our method with\nstate-of-the-art style encoders. For F0 and energy predictions, we show\nconsistent improvements for our model at different levels of granularity. We\nalso integrate these predicted prosodic features into a TTS system and conduct\nperceptual tests, which show higher prosody preference compared to the\nbaselines, demonstrating the model's potential in tasks where prosody modeling\nis important."}
{"id": "2508.09702", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09702", "abs": "https://arxiv.org/abs/2508.09702", "authors": ["Boyu Zhu", "Cheng Gong", "Muyang Wu", "Ruihao Jing", "Fan Liu", "Xiaolei Zhang", "Chi Zhang", "Xuelong Li"], "title": "$\\text{M}^3\\text{PDB}$: A Multimodal, Multi-Label, Multilingual Prompt Database for Speech Generation", "comment": null, "summary": "Recent advancements in zero-shot speech generation have enabled models to\nsynthesize speech that mimics speaker identity and speaking style from speech\nprompts. However, these models' effectiveness is significantly limited in\nreal-world scenarios where high-quality speech prompts are absent, incomplete,\nor out of domain. This issue arises primarily from a significant quality\nmismatch between the speech data utilized for model training and the input\nprompt speech during inference. To address this, we introduce\n$\\text{M}^3\\text{PDB}$, the first large-scale, multi-modal, multi-label, and\nmultilingual prompt database designed for robust prompt selection in speech\ngeneration. Our dataset construction leverages a novel multi-modal, multi-agent\nannotation framework, enabling precise and hierarchical labeling across diverse\nmodalities. Furthermore, we propose a lightweight yet effective prompt\nselection strategy tailored for real-time, resource-constrained inference\nsettings. Experimental results demonstrate that our proposed database and\nselection strategy effectively support various challenging speech generation\nscenarios. We hope our work can inspire the community to shift focus from\nimproving performance on standard benchmarks to addressing more realistic and\ndiverse application scenarios in speech generation. Code and dataset are\navailable at: https://github.com/hizening/M3PDB."}
{"id": "2508.09600", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09600", "abs": "https://arxiv.org/abs/2508.09600", "authors": ["Xuelong Geng", "Qijie Shao", "Hongfei Xue", "Shuiyuan Wang", "Hanke Xie", "Zhao Guo", "Yi Zhao", "Guojian Li", "Wenjie Tian", "Chengyou Wang", "Zhixian Zhao", "Kangxiang Xia", "Ziyu Zhang", "Zhennan Lin", "Tianlun Zuo", "Mingchen Shao", "Yuang Cao", "Guobin Ma", "Longhao Li", "Yuhang Dai", "Dehui Gao", "Dake Guo", "Lei Xie"], "title": "OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue", "comment": null, "summary": "Empathy is crucial in enabling natural interactions within spoken dialogue\nsystems, allowing machines to recognize and respond appropriately to\nparalinguistic cues such as age, gender, and emotion. Recent advancements in\nend-to-end speech language models, which unify speech understanding and\ngeneration, provide promising solutions. However, several challenges persist,\nincluding an over-reliance on large-scale dialogue datasets, insufficient\nextraction of paralinguistic cues vital for conveying empathy, and the lack of\nempathy-specific datasets and evaluation frameworks. To address these issues,\nwe introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system\ndesigned to enhance empathetic interactions, particularly in resource-limited\nsettings. OSUM-EChat introduces two key innovations: (1) a three-stage\nunderstanding-driven spoken dialogue training strategy that extends the\ncapabilities of a large speech understanding model to spoken dialogue tasks,\nand (2) a linguistic-paralinguistic dual thinking mechanism that integrates\nparalinguistic understanding through a chain of thought with dialogue\ngeneration, enabling the system to produce more empathetic responses. This\napproach reduces reliance on large-scale dialogue datasets while maintaining\nhigh-quality empathetic interactions. Additionally, we introduce the EChat-200K\ndataset, a rich corpus of empathetic speech-to-speech dialogues, and the\nEChat-eval benchmark, a comprehensive framework for evaluating the empathetic\ncapabilities of dialogue systems. Experimental results demonstrate that\nOSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic\nresponsiveness, validating its effectiveness."}
{"id": "2508.09140", "categories": ["eess.SP", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.09140", "abs": "https://arxiv.org/abs/2508.09140", "authors": ["Honggang Jia", "Nan Cheng", "Xiucheng Wang", "Conghao Zhou", "Ruijin Sun", "Xuemin", "Shen"], "title": "RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet", "comment": null, "summary": "Radio map (RM) has recently attracted much attention since it can provide\nreal-time and accurate spatial channel information for 6G services and\napplications. However, current deep learning-based methods for RM construction\nexhibit well known accuracy-efficiency trade-off. In this paper, we introduce\nRadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the\ntrade-off. Generally, accurate RM construction requires modeling long-range\nspatial dependencies, reflecting the global nature of wave propagation physics.\nRadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures\nthese global dependencies with linear complexity, while a parallel\nconvolutional branch extracts local features. This hybrid design generates\nfeature representations that capture both global context and local detail.\nExperiments show that RadioMamba achieves higher accuracy than existing\nmethods, including diffusion models, while operating nearly 20 times faster and\nusing only 2.9\\% of the model parameters. By improving both accuracy and\nefficiency, RadioMamba presents a viable approach for real-time intelligent\noptimization in next generation wireless systems."}
{"id": "2508.09803", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09803", "abs": "https://arxiv.org/abs/2508.09803", "authors": ["Carlos Franzreb", "Arnab Das", "Tim Polzehl", "Sebastian Möller"], "title": "Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning", "comment": null, "summary": "The current privacy evaluation for speaker anonymization often overestimates\nprivacy when a same-gender target selection algorithm (TSA) is used, although\nthis TSA leaks the speaker's gender and should hence be more vulnerable. We\nhypothesize that this occurs because the evaluation does not account for the\nfact that anonymized speech contains information from both the source and\ntarget speakers. To address this, we propose to add a target classifier that\nmeasures the influence of target speaker information in the evaluation, which\ncan also be removed with adversarial learning. Experiments demonstrate that\nthis approach is effective for multiple anonymizers, particularly when using a\nsame-gender TSA, leading to a more reliable assessment."}
{"id": "2508.09728", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09728", "abs": "https://arxiv.org/abs/2508.09728", "authors": ["Zhiyuan Ning", "Zheng Wang", "Zhanyong Tang"], "title": "MetaGuardian: Enhancing Voice Assistant Security through Advanced Acoustic Metamaterials", "comment": null, "summary": "We present MetaGuardian, a voice assistant (VA) protection system based on\nacoustic metamaterials. MetaGuardian can be directly integrated into the\nenclosures of various smart devices, effectively defending against inaudible,\nadversarial and laser attacks without relying on additional software support or\naltering the underlying hardware, ensuring usability. To achieve this,\nMetaGuardian leverages the mutual impedance effects between metamaterial units\nto extend the signal filtering range to 16-40 kHz to effectively block\nwide-band inaudible attacks. Additionally, it adopts a carefully designed\ncoiled space structure to precisely interfere with adversarial attacks while\nensuring the normal functioning of VAs. Furthermore, MetaGuardian offers a\nuniversal structural design, allowing itself to be flexibly adapted to various\nsmart devices, striking a balance between portability and protection\neffectiveness. In controled evaluation environments, MetaGuardian achieves a\nhigh defense success rate against various attack types, including adversarial,\ninaudible and laser attacks."}
{"id": "2508.09142", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09142", "abs": "https://arxiv.org/abs/2508.09142", "authors": ["Wenlihan Lu", "Shijian Gao", "Miaowen Wen", "Yuxuan Liang", "Chan-Byoung Chae", "H. Vincent Poor"], "title": "Bayesian-Driven Graph Reasoning for Active Radio Map Construction", "comment": null, "summary": "With the emergence of the low-altitude economy, radio maps have become\nessential for ensuring reliable wireless connectivity to aerial platforms.\nAutonomous aerial agents are commonly deployed for data collection using\nwaypoint-based navigation; however, their limited battery capacity\nsignificantly constrains coverage and efficiency. To address this, we propose\nan uncertainty-aware radio map (URAM) reconstruction framework that explicitly\nleverages graph-based reasoning tailored for waypoint navigation. Our approach\nintegrates two key deep learning components: (1) a Bayesian neural network that\nestimates spatial uncertainty in real time, and (2) an attention-based\nreinforcement learning policy that performs global reasoning over a\nprobabilistic roadmap, using uncertainty estimates to plan informative and\nenergy-efficient trajectories. This graph-based reasoning enables intelligent,\nnon-myopic trajectory planning, guiding agents toward the most informative\nregions while satisfying safety constraints. Experimental results show that\nURAM improves reconstruction accuracy by up to 34% over existing baselines."}
{"id": "2508.09788", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.09788", "abs": "https://arxiv.org/abs/2508.09788", "authors": ["Ganghui Ru", "Jieying Wang", "Jiahao Zhao", "Yulun Wu", "Yi Yu", "Nannan Jiang", "Wei Wang", "Wei Li"], "title": "HingeNet: A Harmonic-Aware Fine-Tuning Approach for Beat Tracking", "comment": "This paper has been accepted by ICME2025", "summary": "Fine-tuning pre-trained foundation models has made significant progress in\nmusic information retrieval. However, applying these models to beat tracking\ntasks remains unexplored as the limited annotated data renders conventional\nfine-tuning methods ineffective. To address this challenge, we propose\nHingeNet, a novel and general parameter-efficient fine-tuning method\nspecifically designed for beat tracking tasks. HingeNet is a lightweight and\nseparable network, visually resembling a hinge, designed to tightly interface\nwith pre-trained foundation models by using their intermediate feature\nrepresentations as input. This unique architecture grants HingeNet broad\ngeneralizability, enabling effective integration with various pre-trained\nfoundation models. Furthermore, considering the significance of harmonics in\nbeat tracking, we introduce harmonic-aware mechanism during the fine-tuning\nprocess to better capture and emphasize the harmonic structures in musical\nsignals. Experiments on benchmark datasets demonstrate that HingeNet achieves\nstate-of-the-art performance in beat and downbeat tracking"}
{"id": "2508.09348", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09348", "abs": "https://arxiv.org/abs/2508.09348", "authors": ["Chunmei Xu", "Yi Ma", "Rahim Tafazolli", "Peiying Zhu"], "title": "Generative AI-Enabled Robust 6G Uplink: Principles, Challenges, and Directions", "comment": null, "summary": "Next-generation wireless networks (6G) face a critical uplink bottleneck due\nto stringent device-side resource constraints and challenging channel\nconditions. This article introduces GenCom, a novel system-level paradigm for\nrobust 6G uplink that leverages Generative AI and exploits the inherent\nresource imbalance between transmitters and receivers. In GenCom, resource-rich\nreceivers deploy powerful offline-trained GenAI models to reconstruct high\nsemantic-fidelity content from degraded signals, while resource-constrained\ntransmitters are simplified in both source and channel coding design. We\npresent the core mechanisms and key design principles behind GenCom, which\nshifts from conventional approaches toward simple semantic-preserving\ncompression, weak error-distribution codes, and semantic-aware retransmissions.\nThrough a case study, GenCom is shown to deliver robust performance across a\nwide range of low and uncertain SNR/SINR conditions where conventional systems\nfail. Finally, we outline critical challenges and research directions toward\nmaking GenCom a practical enabler of future human-centric, intelligent, and\nsustainable wireless networks."}
{"id": "2508.09790", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09790", "abs": "https://arxiv.org/abs/2508.09790", "authors": ["Ganghui Ru", "Jieying Wang", "Jiahao Zhao", "Yulun Wu", "Yi Yu", "Nannan Jiang", "Wei Wang", "Wei Li"], "title": "BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model", "comment": "This paper has been accepted by ICME2025", "summary": "Beat tracking is a widely researched topic in music information retrieval.\nHowever, current beat tracking methods face challenges due to the scarcity of\nlabeled data, which limits their ability to generalize across diverse musical\nstyles and accurately capture complex rhythmic structures. To overcome these\nchallenges, we propose a novel beat tracking paradigm BeatFM, which introduces\na pre-trained music foundation model and leverages its rich semantic knowledge\nto improve beat tracking performance. Pre-training on diverse music datasets\nendows music foundation models with a robust understanding of music, thereby\neffectively addressing these challenges. To further adapt it for beat tracking,\nwe design a plug-and-play multi-dimensional semantic aggregation module, which\nis composed of three parallel sub-modules, each focusing on semantic\naggregation in the temporal, frequency, and channel domains, respectively.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nperformance in beat and downbeat tracking across multiple benchmark datasets."}
{"id": "2508.09374", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09374", "abs": "https://arxiv.org/abs/2508.09374", "authors": ["Rohith Reddy Vennam", "Luke Wilson", "Ish Kumar Jain", "Dinesh Bharadia"], "title": "Satellites are closer than you think: A near field MIMO approach for Ground stations", "comment": "11 pages, 11 figures", "summary": "The rapid growth of low Earth orbit (LEO) satellite constellations has\nrevolutionized broadband access, earth observation, and direct-to-device\nconnectivity. However, the expansion of ground station infrastructure has not\nkept pace, creating a critical bottleneck in satellite-to-ground backhaul\ncapacity. Traditional parabolic dish antennas, though effective for\ngeostationary (GEO) satellites, are ill-suited for dense, fastmoving LEO\nnetworks due to mechanical steering delays and their inability to track\nmultiple satellites simultaneously. Phased array antennas offer electronically\nsteerable beams and multisatellite support, but their integration into ground\nstations is limited by the high cost, hardware issues, and complexity of\nachieving sufficient antenna gain. We introduce ArrayLink, a distributed phased\narray architecture that coherently combines multiple small commercially\navailable panels to achieve high-gain beamforming and unlock line-of-sight MIMO\nspatial multiplexing with minimal additional capital expenditure. By spacing 16\n(32x32) panels across a kilometer-scale aperture, ArrayLink enters the\nradiative near-field, focusing energy in both angle and range while supporting\nup to four simultaneous spatial streams on a single feeder link. Through\nrigorous theoretical analysis, detailed 2D beam pattern simulations and\nreal-world hardware experiments, we show that ArrayLink (i) achieves dish-class\ngain with in range 1-2 dB of 1.47 m reflector, (ii) maintains four parallel\nstreams at ranges of hundreds of kilometers (falling to two beyond 2000 km),\nand (iii) exhibits tight agreement across theory, simulation, and experiment\nwith minimal variance. These findings open a practical and scalable path to\nboosting LEO backhaul capacity."}
{"id": "2508.09868", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09868", "abs": "https://arxiv.org/abs/2508.09868", "authors": ["Tina Raissi", "Nick Rossenbach", "Ralf Schlüter"], "title": "Analysis of Domain Shift across ASR Architectures via TTS-Enabled Separation of Target Domain and Acoustic Conditions", "comment": "Accepted for presentation at IEEE ASRU 2025", "summary": "We analyze automatic speech recognition (ASR) modeling choices under domain\nmismatch, comparing classic modular and novel sequence-to-sequence (seq2seq)\narchitectures. Across the different ASR architectures, we examine a spectrum of\nmodeling choices, including label units, context length, and topology. To\nisolate language domain effects from acoustic variation, we synthesize target\ndomain audio using a text-to-speech system trained on LibriSpeech. We\nincorporate target domain n-gram and neural language models for domain\nadaptation without retraining the acoustic model. To our knowledge, this is the\nfirst controlled comparison of optimized ASR systems across state-of-the-art\narchitectures under domain shift, offering insights into their generalization.\nThe results show that, under domain shift, rather than the decoder architecture\nchoice or the distinction between classic modular and novel seq2seq models, it\nis specific modeling choices that influence performance."}
{"id": "2508.09545", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09545", "abs": "https://arxiv.org/abs/2508.09545", "authors": ["Lutfi Samara", "Simon Haussmann", "Erind Tufa", "Antonio Alberto D'Amico", "Tommaso Zugno", "Ingmar Kallfass", "Thomas Kürner"], "title": "Sub-THz Power Amplifiers: Measurements, Behavioral Modeling and Predistortion Algorithms", "comment": null, "summary": "With global IMT traffic expected to grow 10-100 times from 2020 to 20301, the\nTerahertz (THz) spectrum offers a promising solution to satisfy such forecasts.\nHowever, occupying the THz spectrum comes with its own challenges, an important\none being impairments caused by broadband RF components in THz transceivers.\nNonlinearities in power amplifiers (PAs) complicate meeting link budget\nrequirements, with amplitude and phase distortions degrading the system's\nperformance, especially when adopting waveforms with high peak-to-average power\nratios (PAPRs), such as Orthogonal Frequency Division Multiplexing (OFDM). In\nthis paper, we present characterization results of a 300 GHz PA using\nsmall-signal and large-signal continuous-wave measurements. Models capturing\nAmplitude-to- Amplitude Modulation (AM-AM) and Amplitude-to-Phase Modulation\n(AMPM) behavior across 270-330 GHz are developed and verified with wideband\nmeasurements, confirming the compression behavior, while nonetheless showing\ninaccuracies for low input powers due to unaccounted frequency dependencies.\nBased on the derived models, a predistortion algorithm is designed and\nanalyzed, revealing significant error performance degradation when switching\nbetween single- and multi-carrier waveforms. We finally show that an\nappropriate selection of pre-distorter parameters can significantly improve the\nperformance."}
{"id": "2508.09880", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09880", "abs": "https://arxiv.org/abs/2508.09880", "authors": ["Noureldin Bayoumi", "Robin Schmitt", "Tina Raissi", "Albert Zeyer", "Ralf Schlüter", "Hermann Ney"], "title": "A Comparative Analysis on ASR System Combination for Attention, CTC, Factored Hybrid, and Transducer Models", "comment": "Accepted for presentation at IEEE Speech Communication; 16th ITG\n  Conference", "summary": "Combination approaches for speech recognition (ASR) systems cover structured\nsentence-level or word-based merging techniques as well as combination of model\nscores during beam search. In this work, we compare model combination across\npopular ASR architectures. Our method leverages the complementary strengths of\ndifferent models in exploring diverse portions of the search space. We rescore\na joint hypothesis list of two model candidates. We then identify the best\nhypothesis through log-linear combination of these sequence-level scores. While\nmodel combination during first-pass recognition may yield improved performance,\nit introduces variability due to differing decoding methods, making direct\ncomparison more challenging. Our two-pass method ensures consistent comparisons\nacross all system combination results presented in this study. We evaluate\nmodel pair candidates with varying architectures and label topologies and\nunits. Experimental results are provided for the Librispeech 960h task."}
{"id": "2508.09546", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.09546", "abs": "https://arxiv.org/abs/2508.09546", "authors": ["Dumitra Iancu", "Liang Liu", "Ove Edfors", "Erik Leitinger", "Xuhong Li"], "title": "Low-latency D-MIMO Localization using Distributed Scalable Message-Passing Algorithm", "comment": "This work has been submitted to the IEEE for possible publication,\n  copyright information may be affected upon publication", "summary": "Distributed MIMO and integrated sensing and communication are expected to be\nkey technologies in future wireless systems, enabling reliable, low-latency\ncommunication and accurate localization. Dedicated localization solutions must\nsupport distributed architecture, provide scalability across different system\nconfigurations and meet strict latency requirements. We present a scalable\nmessage-passing localization method and architecture co-designed for a\npanel-based distributed MIMO system and network topology, in which\ninterconnected units operate without centralized processing. This method\njointly detects line-of-sight paths to distributed units from multipath\nmeasurements in dynamic scenarios, localizes the agent, and achieves very low\nlatency. Additionally, we introduce a cycle-accurate system latency model based\non implemented FPGA operations, and show important insights into processing\nlatency and hardware utilization and system-level trade-offs. We compare our\nmethod to a multipath-based localization method and show that it can achieve\nsimilar localization performance, with wide enough distribution of array\nelements, while offering lower latency and computational complexity."}
{"id": "2508.09389", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09389", "abs": "https://arxiv.org/abs/2508.09389", "authors": ["Eray Eren", "Qingju Liu", "Hyeongwoo Kim", "Pablo Garrido", "Abeer Alwan"], "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs", "comment": "Interspeech 2025; demo page at\n  https://promode8272.github.io/promode/index.html", "summary": "Prosody conveys rich emotional and semantic information of the speech signal\nas well as individual idiosyncrasies. We propose a stand-alone model that maps\ntext-to-prosodic features such as F0 and energy and can be used in downstream\ntasks such as TTS. The ProMode encoder takes as input acoustic features and\ntime-aligned textual content, both are partially masked, and obtains a\nfixed-length latent prosodic embedding. The decoder predicts acoustics in the\nmasked region using both the encoded prosody input and unmasked textual\ncontent. Trained on the GigaSpeech dataset, we compare our method with\nstate-of-the-art style encoders. For F0 and energy predictions, we show\nconsistent improvements for our model at different levels of granularity. We\nalso integrate these predicted prosodic features into a TTS system and conduct\nperceptual tests, which show higher prosody preference compared to the\nbaselines, demonstrating the model's potential in tasks where prosody modeling\nis important."}
{"id": "2508.09574", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09574", "abs": "https://arxiv.org/abs/2508.09574", "authors": ["Zhiyuan Ren", "Yutao Liu", "Wenchi Cheng", "Kun Yang"], "title": "Profiling Multi-Level Operator Costs for Bottleneck Diagnosis in High-Speed Data Planes", "comment": null, "summary": "This paper proposes a saturation throughput delta-based methodology to\nprecisely measure operator costs in high-speed data planes without intrusive\ninstrumentation. The approach captures non-linear scaling, revealing that\ncompute-intensive operators like CRC exhibit super-linear behavior, while most\nothers are sub-linear. We introduce the Operator Performance Quadrant (OPQ)\nframework to classify operators by base and scaling costs, exposing a\ncross-architecture Quadrant Shift between Arm and x86. This method provides\naccurate, architecture-aware bottleneck diagnosis and a realistic basis for\nperformance modeling and optimization."}
{"id": "2508.09702", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09702", "abs": "https://arxiv.org/abs/2508.09702", "authors": ["Boyu Zhu", "Cheng Gong", "Muyang Wu", "Ruihao Jing", "Fan Liu", "Xiaolei Zhang", "Chi Zhang", "Xuelong Li"], "title": "$\\text{M}^3\\text{PDB}$: A Multimodal, Multi-Label, Multilingual Prompt Database for Speech Generation", "comment": null, "summary": "Recent advancements in zero-shot speech generation have enabled models to\nsynthesize speech that mimics speaker identity and speaking style from speech\nprompts. However, these models' effectiveness is significantly limited in\nreal-world scenarios where high-quality speech prompts are absent, incomplete,\nor out of domain. This issue arises primarily from a significant quality\nmismatch between the speech data utilized for model training and the input\nprompt speech during inference. To address this, we introduce\n$\\text{M}^3\\text{PDB}$, the first large-scale, multi-modal, multi-label, and\nmultilingual prompt database designed for robust prompt selection in speech\ngeneration. Our dataset construction leverages a novel multi-modal, multi-agent\nannotation framework, enabling precise and hierarchical labeling across diverse\nmodalities. Furthermore, we propose a lightweight yet effective prompt\nselection strategy tailored for real-time, resource-constrained inference\nsettings. Experimental results demonstrate that our proposed database and\nselection strategy effectively support various challenging speech generation\nscenarios. We hope our work can inspire the community to shift focus from\nimproving performance on standard benchmarks to addressing more realistic and\ndiverse application scenarios in speech generation. Code and dataset are\navailable at: https://github.com/hizening/M3PDB."}
{"id": "2508.09708", "categories": ["eess.SP", "cs.NI", "C.2.1; C.2.2; C.2.4"], "pdf": "https://arxiv.org/pdf/2508.09708", "abs": "https://arxiv.org/abs/2508.09708", "authors": ["Thomas Fehrenbach", "Luis Omar Ortiz Abrego", "Cornelius Hellge", "Thomas Schierl", "Jörg Ott"], "title": "3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator", "comment": "7 pages, 10 figures, 2 tables, V2X communication, vehicular networks,\n  platooning simulation", "summary": "Vehicle-to-everything (V2X) communication is a key technology for enabling\nintelligent transportation systems (ITS) that can improve road safety, traffic\nefficiency, and environmental sustainability. Among the various V2X\napplications, platooning is one of the most promising ones, as it allows a\ngroup of vehicles to travel closely together at high speeds, reducing fuel\nconsumption and emissions. However, it poses significant challenges for\nwireless communication, such as high reliability and low latency. In this\npaper, we evaluate the benefits of group scheduling, also referred to as Mode\n2d, which is based on a distributed and scheduled resource allocation scheme\nthat allows the group of cars to select resources from a configured pool\nwithout network assistance. We evaluated the scheme through simulations, and\nthe results show that this approach can meet the reliability, low latency, and\ndata rate requirements for platooning."}
{"id": "2508.09727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09727", "abs": "https://arxiv.org/abs/2508.09727", "authors": ["Jinhui Hu", "Haiquan Zhao", "Yi Peng"], "title": "CKFNet: Neural Network Aided Cubature Kalman filtering", "comment": null, "summary": "The cubature Kalman filter (CKF), while theoretically rigorous for nonlinear\nestimation, often suffers performance degradation due to model-environment\nmismatches in practice. To address this limitation, we propose CKFNet-a hybrid\narchitecture that synergistically integrates recurrent neural networks (RNN)\nwith the CKF framework while preserving its cubature principles. Unlike\nconventional model-driven approaches, CKFNet embeds RNN modules in the\nprediction phase to dynamically adapt to unmodeled uncertainties, effectively\nreducing cumulative error propagation through temporal noise correlation\nlearning. Crucially, the architecture maintains CKF's analytical\ninterpretability via constrained optimization of cubature point distributions.\nNumerical simulation experiments have confirmed that our proposed CKFNet\nexhibits superior accuracy and robustness compared to conventional model-based\nmethods and existing KalmanNet algorithms."}
{"id": "2508.09751", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09751", "abs": "https://arxiv.org/abs/2508.09751", "authors": ["Sungyoung Ha", "Ikbeom Lee", "Seunghyeon Jeon", "Yo-Seb Jeon"], "title": "Online Data Generation for MIMO-OFDM Channel Denoising: Transfer Learning vs. Meta Learning", "comment": null, "summary": "Channel denoising is a practical and effective technique for mitigating\nchannel estimation errors in multiple-input multiple-output orthogonal\nfrequency-division multiplexing (MIMO-OFDM) systems. However, adapting\ndenoising techniques to varying channel conditions typically requires prior\nknowledge or incurs significant training overhead. To address these challenges,\nwe propose a standard-compatible strategy for generating online training data\nthat enables online adaptive channel denoising. The key idea is to leverage\nhigh-quality channel estimates obtained via data-aided channel estimation as\npractical substitutes for unavailable ground-truth channels. Our data-aided\nmethod exploits adjacent detected data symbols within a specific time-frequency\nneighborhood as virtual reference signals, and we analytically derive the\noptimal size of this neighborhood to minimize the mean squared error of the\nresulting estimates. By leveraging the proposed strategy, we devise two channel\ndenoising approaches, one based on transfer learning, which fine-tunes a\npre-trained denoising neural network, and the other based on meta learning,\nwhich rapidly adapts to new channel environments with minimal updates.\nSimulation results demonstrate that the proposed methods effectively adapt to\ndynamic channel conditions and significantly reduce channel estimation errors\ncompared to conventional techniques."}
{"id": "2508.09882", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09882", "abs": "https://arxiv.org/abs/2508.09882", "authors": ["Umair Ali Khan", "Lester Ho", "Holger Claussen", "Chinmoy Kundu"], "title": "Location Privacy-Enabled Beamforming in ISAC Scenarios", "comment": "This paper has been submitted to IEEE Globecom Workshops 2025 and is\n  currently under review", "summary": "Integrated sensing and communication (ISAC) technology enables simultaneous\nenvironmental perception and data transmission in wireless networks; however,\nit also exposes user location to receivers. In this paper, we introduce a novel\nbeamforming framework guided by the proposed privacy metric direction of\narrival obfuscation ratio (DAOR) to protect transmitter location privacy in\nISAC scenarios. Unlike previous approaches, we do not suppress the\nline-of-sight (LOS) component while reshaping the angular power distribution so\nthat a false direction appears dominant at the receiver. We derive closed-form\nbounds on the feasible DAOR via generalized eigenvalue analysis and formulate\nan achievable rate-maximization problem under the DAOR constraint. The\nresulting problem is non-convex, which is efficiently solved using semidefinite\nrelaxation, eigenmode selection, and optimal power allocation. A suboptimal\ndesign strategy is also proposed with reduced complexity. Numerical results\ndemonstrate that the proposed DAOR-based beamformer achieves a trade-off\nbetween location privacy and communication rate without nullifying the LOS\npath. Results also show that a suboptimal design achieves a near-optimal\ncommunication rate with nearly an 85% reduction in computation time at a\nsignal-to-noise ratio (SNR) of 10 dB."}
{"id": "2508.09942", "categories": ["eess.SP", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.09942", "abs": "https://arxiv.org/abs/2508.09942", "authors": ["Vaibhav Choudhary", "Akshay Agarwal", "Vivek K Goyal"], "title": "Beam Cross Sections Create Mixtures: Improving Feature Localization in Secondary Electron Imaging", "comment": "13 pages, 8 figures", "summary": "Secondary electron (SE) imaging techniques, such as scanning electron\nmicroscopy and helium ion microscopy (HIM), use electrons emitted by a sample\nin response to a focused beam of charged particles incident at a grid of raster\nscan positions. Spot size -- the diameter of the incident beam's spatial\nprofile -- is one of the limiting factors for resolution, along with various\nsources of noise in the SE signal. The effect of the beam spatial profile is\ncommonly understood as convolutional. We show that under a simple and plausible\nphysical abstraction for the beam, though convolution describes the mean of the\nSE counts, the full distribution of SE counts is a mixture. We demonstrate that\nthis more detailed modeling can enable resolution improvements over\nconventional estimators through a stylized application in semiconductor\ninspection of localizing the edge in a two-valued sample. We derive Fisher\ninformation about edge location in conventional and time-resolved measurements\n(TRM) and also derive the maximum likelihood estimate (MLE) from the latter.\nEmpirically, the MLE computed from TRM is approximately efficient except at\nvery low beam diameter, so Fisher information comparisons are predictive of\nperformance and can be used to optimize the beam diameter relative to the\nraster scan spacing. Monte Carlo simulations show that the MLE gives a 5-fold\nreduction in root mean-squared error (RMSE) of edge localization as compared to\nconventional interpolation-based estimation. Applied to three real HIM\ndatasets, the average RMSE reduction factor is 5.4."}

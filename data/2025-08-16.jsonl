{"id": "2508.10332", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10332", "abs": "https://arxiv.org/abs/2508.10332", "authors": ["Abhijit Sinha", "Harishankar Kumar", "Mohit Joshi", "Hemant Kumar Kathania", "Shrikanth Narayanan", "Sudarsana Reddy Kadiri"], "title": "Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children's Speech", "comment": "Accepted at Workshop on Child Computer Interaction (WOCCI 2025)", "summary": "Children's speech presents challenges for age and gender classification due\nto high variability in pitch, articulation, and developmental traits. While\nself-supervised learning (SSL) models perform well on adult speech tasks, their\nability to encode speaker traits in children remains underexplored. This paper\npresents a detailed layer-wise analysis of four Wav2Vec2 variants using the\nPFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture\nspeaker-specific cues more effectively than deeper layers, which increasingly\nfocus on linguistic information. Applying PCA further improves classification,\nreducing redundancy and highlighting the most informative components. The\nWav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU\nKids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These\nresults reveal how speaker traits are structured across SSL model depth and\nsupport more targeted, adaptive strategies for child-aware speech interfaces."}
{"id": "2508.10374", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10374", "abs": "https://arxiv.org/abs/2508.10374", "authors": ["Michael Kuhlmann", "Fritz Seebauer", "Petra Wagner", "Reinhold Haeb-Umbach"], "title": "Towards Frame-level Quality Predictions of Synthetic Speech", "comment": "Accepted at Interspeech 2025", "summary": "While automatic subjective speech quality assessment has witnessed much\nprogress, an open question is whether an automatic quality assessment at frame\nresolution is possible. This would be highly desirable, as it adds\nexplainability to the assessment of speech synthesis systems. Here, we take\nfirst steps towards this goal by identifying issues of existing quality\npredictors that prevent sensible frame-level prediction. Further, we define\ncriteria that a frame-level predictor should fulfill. We also suggest a\nchunk-based processing that avoids the impact of a localized distortion on the\nscore of neighboring frames. Finally, we measure in experiments with localized\nartificial distortions the localization performance of a set of frame-level\nquality predictors and show that they can outperform detection performance of\nhuman annotations obtained from a crowd-sourced perception experiment."}
{"id": "2508.10456", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10456", "abs": "https://arxiv.org/abs/2508.10456", "authors": ["Mingyu Cui", "Mengzhe Geng", "Jiajun Deng", "Chengxi Deng", "Jiawen Kang", "Shujie Hu", "Guinan Li", "Tianzi Wang", "Zhaoqing Li", "Xie Chen", "Xunying Liu"], "title": "Exploring Cross-Utterance Speech Contexts for Conformer-Transducer Speech Recognition Systems", "comment": null, "summary": "This paper investigates four types of cross-utterance speech contexts\nmodeling approaches for streaming and non-streaming Conformer-Transformer (C-T)\nASR systems: i) input audio feature concatenation; ii) cross-utterance Encoder\nembedding concatenation; iii) cross-utterance Encoder embedding pooling\nprojection; or iv) a novel chunk-based approach applied to C-T models for the\nfirst time. An efficient batch-training scheme is proposed for contextual C-Ts\nthat uses spliced speech utterances within each minibatch to minimize the\nsynchronization overhead while preserving the sequential order of\ncross-utterance speech contexts. Experiments are conducted on four benchmark\nspeech datasets across three languages: the English GigaSpeech and Mandarin\nWenetspeech corpora used in contextual C-T models pre-training; and the English\nDementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets used in\ndomain fine-tuning. The best performing contextual C-T systems consistently\noutperform their respective baselines using no cross-utterance speech contexts\nin pre-training and fine-tuning stages with statistically significant average\nword error rate (WER) or character error rate (CER) reductions up to 0.9%,\n1.1%, 0.51%, and 0.98% absolute (6.0%, 5.4%, 2.0%, and 3.4% relative) on the\nfour tasks respectively. Their performance competitiveness against\nWav2vec2.0-Conformer, XLSR-128, and Whisper models highlights the potential\nbenefit of incorporating cross-utterance speech contexts into current speech\nfoundation models."}
{"id": "2508.09994", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.09994", "abs": "https://arxiv.org/abs/2508.09994", "authors": ["Zheng Jie Wong", "Bingquan Shen"], "title": "Whisper Smarter, not Harder: Adversarial Attack on Partial Suppression", "comment": "13 pages, 7 figures", "summary": "Currently, Automatic Speech Recognition (ASR) models are deployed in an\nextensive range of applications. However, recent studies have demonstrated the\npossibility of adversarial attack on these models which could potentially\nsuppress or disrupt model output. We investigate and verify the robustness of\nthese attacks and explore if it is possible to increase their imperceptibility.\nWe additionally find that by relaxing the optimisation objective from complete\nsuppression to partial suppression, we can further decrease the\nimperceptibility of the attack. We also explore possible defences against these\nattacks and show a low-pass filter defence could potentially serve as an\neffective defence."}
{"id": "2508.09994", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.09994", "abs": "https://arxiv.org/abs/2508.09994", "authors": ["Zheng Jie Wong", "Bingquan Shen"], "title": "Whisper Smarter, not Harder: Adversarial Attack on Partial Suppression", "comment": "13 pages, 7 figures", "summary": "Currently, Automatic Speech Recognition (ASR) models are deployed in an\nextensive range of applications. However, recent studies have demonstrated the\npossibility of adversarial attack on these models which could potentially\nsuppress or disrupt model output. We investigate and verify the robustness of\nthese attacks and explore if it is possible to increase their imperceptibility.\nWe additionally find that by relaxing the optimisation objective from complete\nsuppression to partial suppression, we can further decrease the\nimperceptibility of the attack. We also explore possible defences against these\nattacks and show a low-pass filter defence could potentially serve as an\neffective defence."}
{"id": "2508.09996", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.09996", "abs": "https://arxiv.org/abs/2508.09996", "authors": ["Ferhat Ozgur Catak", "Murat Kuzlu", "Umit Cali"], "title": "Comparative Analysis of Attention Mechanisms for Automatic Modulation Classification in Radio Frequency Signals", "comment": "6 pages", "summary": "Automatic Modulation Classification (AMC) is a critical component in\ncognitive radio systems and spectrum management applications. This study\npresents a comprehensive comparative analysis of three attention mechanisms\n(i.e., baseline multi-head attention, causal attention, and sparse attention)\nintegrated with Convolutional Neural Networks (CNNs) for radio frequency (RF)\nsignal classification. It proposes a novel CNN-Transformer hybrid architecture\nthat leverages different attention patterns to capture temporal dependencies in\nI/Q samples from the RML2016.10a dataset. The experimental results demonstrate\nthat while baseline attention achieves the highest accuracy of 85.05\\%, causal\nand sparse attention mechanisms offer significant computational advantages with\ninference times reduced by 83\\% and 75\\% respectively, while maintaining\ncompetitive classification performance above 84\\%. The analysis reveals\ndistinct attention pattern preferences across different modulation schemes,\nproviding insights for designing efficient attention mechanisms for real-time\nradio signal processing applications."}
{"id": "2508.10360", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10360", "abs": "https://arxiv.org/abs/2508.10360", "authors": ["Henry Zhong", "JÃ¶rg M. Buchholz", "Julian Maclaren", "Simon Carlile", "Richard Lyon"], "title": "A dataset and model for recognition of audiologically relevant environments for hearing aids: AHEAD-DS and YAMNet+", "comment": null, "summary": "Scene recognition of audiologically relevant environments is important for\nhearing aids; however, it is challenging, in part because of the limitations of\nexisting datasets. Datasets often lack public accessibility, completeness, or\naudiologically relevant labels, hindering systematic comparison of machine\nlearning models. Deploying these models on resource-constrained edge devices\npresents another challenge. Our solution is two-fold: we leverage several open\nsource datasets to create AHEAD-DS, a dataset designed for scene recognition of\naudiologically relevant environments, and introduce YAMNet+, a sound\nrecognition model. AHEAD-DS aims to provide a standardised, publicly available\ndataset with consistent labels relevant to hearing aids, facilitating model\ncomparison. YAMNet+ is designed for deployment on edge devices like smartphones\nconnected to hearing devices, such as hearing aids and wireless earphones with\nhearing aid functionality; serving as a baseline model for sound-based scene\nrecognition. YAMNet+ achieved a mean average precision of 0.83 and accuracy of\n0.93 on the testing set of AHEAD-DS across fourteen categories of\naudiologically relevant environments. We found that applying transfer learning\nfrom the pretrained YAMNet model was essential. We demonstrated real-time\nsound-based scene recognition capabilities on edge devices by deploying YAMNet+\nto an Android smartphone. Even with a Google Pixel 3 (a phone with modest\nspecifications, released in 2018), the model processes audio with approximately\n50ms of latency to load the model, and an approximate linear increase of 30ms\nper 1 second of audio. Our website and code\nhttps://github.com/Australian-Future-Hearing-Initiative ."}
{"id": "2508.10049", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10049", "abs": "https://arxiv.org/abs/2508.10049", "authors": ["Akika Nakamichi", "Izumi Uesaka", "Masahiro Morikawa"], "title": "Dynamic Synchronization and Resonance as a Universal Origin of 1/f Fluctuations -- Amplitude Modulation Across Music and Nature", "comment": "14 pages, 10 figures", "summary": "We propose a universal physical mechanism for the emergence of 1/f\nfluctuations, observed across a wide range of systems. In particular, we verify\nthis on acoustic cases. The mechanism is based on amplitude modulation (AM) and\ndemodulation (DM), where the 1/f spectral law arises not in the raw waveform\nbut in its demodulated amplitude envelope. Two distinct yet complementary\nprocesses generate the required AM: (i) stochastic synchronization among\noscillators, modeled via an extended Kuramoto framework that captures perpetual\nsynchronization-desynchronization cycles, and (ii) frequency-selective\nresonance, modeled by spectral accumulation of eigenmodes in acoustic or\nstructural environments. Numerical simulations demonstrate that both\nmechanisms, acting separately or in combination, robustly produce 1/f spectra\nover several decades when DM is applied, and that the classical Kuramoto\ncritical point is not necessary for their emergence. We demonstrate the\ncross-domain relevance of this AM/DM framework through analyses of musical\nperformances, seismic records, and astrophysical time series, revealing a\ncommon underlying structure. This work establishes demodulation as a general\nroute to 1/f fluctuations, providing a simple and scalable explanation for its\nubiquity in both natural and engineered systems.\n  Keywords: 1/f fluctuation, amplitude modulation, synchronization, resonance,\nKuramoto model, music, natural noise, demodulation"}
{"id": "2508.10218", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10218", "abs": "https://arxiv.org/abs/2508.10218", "authors": ["Nazanin Mirhosseini"], "title": "Information Retention in Iterative Random Projection of Convex Bodies to Lower Dimensions", "comment": null, "summary": "In this paper, we consider a bounded convex body $K_0 \\subset \\mathbb{R}^{n}$\nsubjected to two successive random orthogonal projections onto\n$\\mathbb{R}^{n-1}$ and $\\mathbb{R}^{n-2}$, respectively. First, we project\n$K_0$ orthogonally onto $U_{1}^{\\perp}$, the orthogonal complement of\n$\\mbox{\\boldmath $U$}_1$, where $\\mbox{\\boldmath $U$}_1$ is uniformly\ndistributed on the unit sphere $S^{n-1}$. This yields a random convex body $K_1\n= \\mathrm{Proj}_{{U_1}^{\\perp}}(K_0) \\subset \\mathbb{R}^{n-1}$. We then repeat\nthe process, projecting $K_1$ orthogonally onto ${U}_{2}^{\\perp}$, the\northogonal complement of $\\mbox{\\boldmath $U$}_2$ chosen uniformly from the\nunit sphere in $\\mbox{\\boldmath $U$}_{1}^{\\perp}$ or $S^{n-2}$, resulting in a\nsecond random convex body $K_2 = \\mathrm{Proj}_{{U_2}^{\\perp}}(K_1) \\subset\n\\mathbb{R}^{n-2}$. To quantify information retention through these sequential\ndimension reductions, we derive an upper bound for the conditional mutual\ninformation $I(K_1;K_2 \\mid K_0)$. Furthermore, we extend this process to $m$\niterations and generalize the upper bound on $I(K_1;K_2 \\mid K_0)$ to establish\nan analogous upper bound for $I(K_1;K_m \\mid K_0)$. Finally, we examine the\ninfluence of $K_0$'s symmetry on the achievability of this upper bound for\n$I(K_1;K_m \\mid K_0)$."}
{"id": "2508.10436", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10436", "abs": "https://arxiv.org/abs/2508.10436", "authors": ["Iksoon Jeong", "Kyung-Joong Kim", "Kang-Hun Ahn"], "title": "Alternating Approach-Putt Models for Multi-Stage Speech Enhancement", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Speech enhancement using artificial neural networks aims to remove noise from\nnoisy speech signals while preserving the speech content. However, speech\nenhancement networks often introduce distortions to the speech signal, referred\nto as artifacts, which can degrade audio quality. In this work, we propose a\npost-processing neural network designed to mitigate artifacts introduced by\nspeech enhancement models. Inspired by the analogy of making a `Putt' after an\n`Approach' in golf, we name our model PuttNet. We demonstrate that alternating\nbetween a speech enhancement model and the proposed Putt model leads to\nimproved speech quality, as measured by perceptual quality scores (PESQ),\nobjective intelligibility (STOI), and background noise intrusiveness (CBAK)\nscores. Furthermore, we illustrate with graphical analysis why this alternating\nApproach outperforms repeated application of either model alone."}
{"id": "2508.10230", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10230", "abs": "https://arxiv.org/abs/2508.10230", "authors": ["Chenggang Chen", "Zhiyu Yang"], "title": "No Free Lunch from Audio Pretraining in Bioacoustics: A Benchmark Study of Embeddings", "comment": null, "summary": "Bioacoustics, the study of animal sounds, offers a non-invasive method to\nmonitor ecosystems. Extracting embeddings from audio-pretrained deep learning\n(DL) models without fine-tuning has become popular for obtaining bioacoustic\nfeatures for tasks. However, a recent benchmark study reveals that while\nfine-tuned audio-pretrained VGG and transformer models achieve state-of-the-art\nperformance in some tasks, they fail in others. This study benchmarks 11 DL\nmodels on the same tasks by reducing their learned embeddings' dimensionality\nand evaluating them through clustering. We found that audio-pretrained DL\nmodels 1) without fine-tuning even underperform fine-tuned AlexNet, 2) both\nwith and without fine-tuning fail to separate the background from labeled\nsounds, but ResNet does, and 3) outperform other models when fewer background\nsounds are included during fine-tuning. This study underscores the necessity of\nfine-tuning audio-pretrained models and checking the embeddings after\nfine-tuning. Our codes are available:\nhttps://github.com/NeuroscienceAI/Audio\\_Embeddings"}
{"id": "2508.10263", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10263", "abs": "https://arxiv.org/abs/2508.10263", "authors": ["Yugang Ma", "Yonghong Zeng", "Sumei Sun", "Gary Lee", "Ernest Kurniawan", "Francois Chin Po Shin"], "title": "A Deep Learning based Signal Dimension Estimator with Single Snapshot Signal in Phased Array Radar Application", "comment": "6 pages; 4 figures", "summary": "Signal dimension, defined here as the number of copies with different delays\nor angular shifts, is a prerequisite for many high-resolution delay estimation\nand direction-finding algorithms in sensing and communication systems. Thus,\ncorrectly estimating signal dimension itself becomes crucial. In this paper, we\npresent a deep learning-based signal dimension estimator (DLSDE) with\nsingle-snapshot observation in the example application of phased array radar.\nUnlike traditional model-based and existing deep learning-based signal\ndimension estimators relying on eigen-decomposition and information criterion,\nto which multiple data snapshots would be needed, the proposed DLSDE uses\ntwo-dimensional convolutional neural network (2D-CNN) to automatically develop\nfeatures corresponding to the dimension of the received signal. Our study shows\nthat DLSDE significantly outperforms traditional methods in terms of the\nsuccessful detection rate and resolution. In a phased array radar with 32\nantenna elements, DLSDE improves detection Signal to Noise Ratio (SNR) by >15dB\nand resolution by >1{\\deg}. This makes the proposed method suitable for\ndistinguishing multiple signals that are spatially correlated or have small\nangular separation. More importantly, our solution operates with a single\nsnapshot signal, which is incompatible with other existing deep learning-based\nmethods."}
{"id": "2508.10830", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10830", "abs": "https://arxiv.org/abs/2508.10830", "authors": ["Kai Li", "Guo Chen", "Wendi Sang", "Yi Luo", "Zhuo Chen", "Shuai Wang", "Shulin He", "Zhong-Qiu Wang", "Andong Li", "Zhiyong Wu", "Xiaolin Hu"], "title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends", "comment": "34 pages, 10 figures", "summary": "The field of speech separation, addressing the \"cocktail party problem\", has\nseen revolutionary advances with DNNs. Speech separation enhances clarity in\ncomplex acoustic environments and serves as crucial pre-processing for speech\nrecognition and speaker recognition. However, current literature focuses\nnarrowly on specific architectures or isolated approaches, creating fragmented\nunderstanding. This survey addresses this gap by providing systematic\nexamination of DNN-based speech separation techniques. Our work differentiates\nitself through: (I) Comprehensive perspective: We systematically investigate\nlearning paradigms, separation scenarios with known/unknown speakers,\ncomparative analysis of supervised/self-supervised/unsupervised frameworks, and\narchitectural components from encoders to estimation strategies. (II)\nTimeliness: Coverage of cutting-edge developments ensures access to current\ninnovations and benchmarks. (III) Unique insights: Beyond summarization, we\nevaluate technological trajectories, identify emerging patterns, and highlight\npromising directions including domain-robust frameworks, efficient\narchitectures, multimodal integration, and novel self-supervised paradigms.\n(IV) Fair evaluation: We provide quantitative evaluations on standard datasets,\nrevealing true capabilities and limitations of different methods. This\ncomprehensive survey serves as an accessible reference for experienced\nresearchers and newcomers navigating speech separation's complex landscape."}
{"id": "2508.10360", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10360", "abs": "https://arxiv.org/abs/2508.10360", "authors": ["Henry Zhong", "JÃ¶rg M. Buchholz", "Julian Maclaren", "Simon Carlile", "Richard Lyon"], "title": "A dataset and model for recognition of audiologically relevant environments for hearing aids: AHEAD-DS and YAMNet+", "comment": null, "summary": "Scene recognition of audiologically relevant environments is important for\nhearing aids; however, it is challenging, in part because of the limitations of\nexisting datasets. Datasets often lack public accessibility, completeness, or\naudiologically relevant labels, hindering systematic comparison of machine\nlearning models. Deploying these models on resource-constrained edge devices\npresents another challenge. Our solution is two-fold: we leverage several open\nsource datasets to create AHEAD-DS, a dataset designed for scene recognition of\naudiologically relevant environments, and introduce YAMNet+, a sound\nrecognition model. AHEAD-DS aims to provide a standardised, publicly available\ndataset with consistent labels relevant to hearing aids, facilitating model\ncomparison. YAMNet+ is designed for deployment on edge devices like smartphones\nconnected to hearing devices, such as hearing aids and wireless earphones with\nhearing aid functionality; serving as a baseline model for sound-based scene\nrecognition. YAMNet+ achieved a mean average precision of 0.83 and accuracy of\n0.93 on the testing set of AHEAD-DS across fourteen categories of\naudiologically relevant environments. We found that applying transfer learning\nfrom the pretrained YAMNet model was essential. We demonstrated real-time\nsound-based scene recognition capabilities on edge devices by deploying YAMNet+\nto an Android smartphone. Even with a Google Pixel 3 (a phone with modest\nspecifications, released in 2018), the model processes audio with approximately\n50ms of latency to load the model, and an approximate linear increase of 30ms\nper 1 second of audio. Our website and code\nhttps://github.com/Australian-Future-Hearing-Initiative ."}
{"id": "2508.10372", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10372", "abs": "https://arxiv.org/abs/2508.10372", "authors": ["Zitong Fang", "Yejian Lyu", "Ziming Yu", "Chong Han"], "title": "Environment Reconstruction in Terahertz Monostatic Sensing: Joint Millimeter-level Geometry Mapping and Material Identification", "comment": null, "summary": "Terahertz (THz) integrated sensing and communication (ISAC) offers high-speed\ncommunication alongside precise environmental sensing. This paper presents a\ncomputationally efficient framework for THz-based environment reconstruction by\nintegrating connected component analysis (CCA)-assisted multipath component\n(MPC) estimation with a sliding-window refinement strategy. To start with, a\nmonostatic sensing experiment is conducted in an indoor scenario using a vector\nnetwork analyzer (VNA)-based sounder operating from 290 to 310 GHz. On one\nhand, as for geometry mapping, a CCA-based region search is employed to\naccelerate parameter extraction, significantly reducing the search space for\nspace-alternating generalized expectation-maximization (SAGE)-based estimation\nand achieving an 8.4 times acceleration, while preserving resolution. Further\nanalysis of the connected component structure enables the identification of\nindoor features such as flat walls and corners. A sliding-window refinement\napplied to the identified regions improves geometric mapping, achieving the\nmean distance error of 4.9 mm, which is one order of magnitude better than the\nliterature. On the other hand, the deterministic and stochastic components of\nthe monostatic channel are classified through reflection loss analysis. Then,\nmaterial identification is performed by looking up the reflection loss in a THz\ntime-domain spectroscopy (THz-TDS) database, which comprises over 200 materials\nacross a 0-6 THz range. Experimental results validate millimeter-level accuracy\nin geometry mapping and reliable material classification, enhancing the\nenvironmental awareness capabilities of THz ISAC systems."}
{"id": "2508.10412", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10412", "abs": "https://arxiv.org/abs/2508.10412", "authors": ["Yejin Jeon", "Solee Im", "Youngjae Kim", "Gary Geunbae Lee"], "title": "Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning", "comment": "Interspeech 2025", "summary": "Dysarthric speakers experience substantial communication challenges due to\nimpaired motor control of the speech apparatus, which leads to reduced speech\nintelligibility. This creates significant obstacles in dataset curation since\nactual recording of long, articulate sentences for the objective of training\npersonalized TTS models becomes infeasible. Thus, the limited availability of\naudio data, in addition to the articulation errors that are present within the\naudio, complicates personalized speech synthesis for target dysarthric speaker\nadaptation. To address this, we frame the issue as a domain transfer task and\nintroduce a knowledge anchoring framework that leverages a teacher-student\nmodel, enhanced by curriculum learning through audio augmentation. Experimental\nresults show that the proposed zero-shot multi-speaker TTS model effectively\ngenerates synthetic speech with markedly reduced articulation errors and high\nspeaker fidelity, while maintaining prosodic naturalness."}
{"id": "2508.10430", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10430", "abs": "https://arxiv.org/abs/2508.10430", "authors": ["Yating Chen", "Cai Wen", "Yan Huang", "Jinye Peng", "Wei Hong", "Timothy N. Davidson"], "title": "Interleaved Transceiver Design for a Continuous- Transmission MIMO-OFDM ISAC System", "comment": null, "summary": "This paper proposes an interleaved transceiver design method for a\nmultiple-input multiple-output (MIMO) integrated sensing and communication\n(ISAC) system utilizing orthogonal frequency division multiplexing (OFDM)\nwaveforms. We consider a continuous transmission system and focus on the design\nof the transmission signal and a receiving filter in the time domain for an\ninterleaved transmission architecture. For communication performance,\nconstructive interference (CI) is integrated into the optimization problem. For\nradar sensing performance, the integrated mainlobe-to-sidelobe ratio (IMSR) of\nthe beampattern is considered to ensure desirable directivity. Additionally, we\ntackle the challenges of inter-block interference and eliminate the spurious\npeaks, which are crucial for accurate target detection. Regarding the hardware\nimplementation aspect, the power of each time sample is constrained to manage\nthe peak-to-average power ratio (PAPR). The design problem is addressed using\nan alternating optimization (AO) framework, with the subproblem for transmitted\nwaveform design being solved via the successive convex approximation (SCA)\nmethod. To further enhance computational efficiency, the alternate direction\npenalty method (ADPM) is employed to solve the subproblems within the SCA\niterations. The convergence of ADPM is established, with convergence of the\ncase of more than two auxiliary variables being established for the first time.\nNumerical simulations validate the effectiveness of our transceiver design in\nachieving desirable performance in both radar sensing and communication, with\nthe fast algorithm achieving comparable performance with greater computational\nefficiency."}
{"id": "2508.10436", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10436", "abs": "https://arxiv.org/abs/2508.10436", "authors": ["Iksoon Jeong", "Kyung-Joong Kim", "Kang-Hun Ahn"], "title": "Alternating Approach-Putt Models for Multi-Stage Speech Enhancement", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Speech enhancement using artificial neural networks aims to remove noise from\nnoisy speech signals while preserving the speech content. However, speech\nenhancement networks often introduce distortions to the speech signal, referred\nto as artifacts, which can degrade audio quality. In this work, we propose a\npost-processing neural network designed to mitigate artifacts introduced by\nspeech enhancement models. Inspired by the analogy of making a `Putt' after an\n`Approach' in golf, we name our model PuttNet. We demonstrate that alternating\nbetween a speech enhancement model and the proposed Putt model leads to\nimproved speech quality, as measured by perceptual quality scores (PESQ),\nobjective intelligibility (STOI), and background noise intrusiveness (CBAK)\nscores. Furthermore, we illustrate with graphical analysis why this alternating\nApproach outperforms repeated application of either model alone."}
{"id": "2508.10476", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10476", "abs": "https://arxiv.org/abs/2508.10476", "authors": ["Amy S. Inwood", "Peter J. Smith", "Philippa A. Martin", "Graeme K. Woodward"], "title": "Second Order Channel Statistics: An Analysis for Optimal Single-user RIS Systems", "comment": null, "summary": "A key challenge facing reconfigurable intelligent surfaces (RISs) is channel\nstate information acquisition. Passive RISs cannot generate pilot signals or\nprocess data, making rapid temporal changes in the channel problematic.\nAdditionally, the impact of spatial changes in RIS channels has not been\nthoroughly investigated. Therefore, in this work, we use second order\nstatistics to investigate the spatio-temporal behaviour of a single-user (SU)\nRIS system. Assuming a line-of-sight (LoS) RIS to base station (BS) link, we\nderive an exact expression for the level crossing rate (LCR) of the RIS link\n(user equipment (UE)-RIS-BS path) and propose a numerically stable\napproximation for the LCR of the global UE-BS channel. Each LCR expression\nattained is then utilised to find the corresponding average fade duration\n(AFD). The temporal signal-to-noise ratio (SNR) correlation is also derived\nassuming an LoS RIS-BS link. Assuming a Ricean RIS-BS link, expressions for the\nspatial correlation matrix of the global channel and the mean SNR loss due to\nchannel ageing are derived. All of the analyses are verified by simulation, and\nthe impact of key system parameters is investigated. We show that the use of an\nRIS does not significantly amplify changes in the channel."}
{"id": "2508.10472", "categories": ["cs.SD", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.10472", "abs": "https://arxiv.org/abs/2508.10472", "authors": ["Danbinaerin Han", "Dasaem Jeong", "Juhan Nam"], "title": "Motive-level Analysis of Form-functions Association in Korean Folk song", "comment": null, "summary": "Computational analysis of folk song audio is challenging due to structural\nirregularities and the need for manual annotation. We propose a method for\nautomatic motive segmentation in Korean folk songs by fine-tuning a speech\ntranscription model on audio lyric with motif boundary annotation. Applying\nthis to 856 songs, we extracted motif count and duration entropy as structural\nfeatures. Statistical analysis revealed that these features vary systematically\naccording to the social function of the songs. Songs associated with collective\nlabor, for instance, showed different structural patterns from those for\nentertainment or personal settings. This work offers a scalable approach for\nquantitative structural analysis of oral music traditions."}
{"id": "2508.10485", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10485", "abs": "https://arxiv.org/abs/2508.10485", "authors": ["Amy S. Inwood", "Peter J. Smith", "Rajitha Senanayake", "Michail Matthaiou"], "title": "High SNR Probabilities of Continuous Fluid Antenna Systems in Ricean Environments", "comment": null, "summary": "We consider a single-user (SU) continuous fluid antenna system (CFAS)\nemploying matched filtering (MF) operating over a Ricean fading channel.\nFocusing on the upper tail of the received signal-to-noise ratio (SNR)\ndistribution (the high SNR probability (HSP)), we derive accurate\napproximations for the HSP in 1, 2, and 3 dimensions using the expected Euler\ncharacteristic (EEC), presenting the first analytical results for a CFAS in a\nRicean environment. In the process, we provide the first closed-form expression\nfor the Euler characteristic density of a non-central chi-squared random field.\nWe then examine the impact of the Ricean K-factor on the CFAS performance,\nemphasizing the critical role of channel variations in achieving a strong HSP."}
{"id": "2508.10559", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10559", "abs": "https://arxiv.org/abs/2508.10559", "authors": ["Yuankun Xie", "Ruibo Fu", "Xiaopeng Wang", "Zhiyong Wang", "Ya Li", "Zhengqi Wen", "Haonnan Cheng", "Long Ye"], "title": "Fake Speech Wild: Detecting Deepfake Speech on Social Media Platform", "comment": null, "summary": "The rapid advancement of speech generation technology has led to the\nwidespread proliferation of deepfake speech across social media platforms.\nWhile deepfake audio countermeasures (CMs) achieve promising results on public\ndatasets, their performance degrades significantly in cross-domain scenarios.\nTo advance CMs for real-world deepfake detection, we first propose the Fake\nSpeech Wild (FSW) dataset, which includes 254 hours of real and deepfake audio\nfrom four different media platforms, focusing on social media. As CMs, we\nestablish a benchmark using public datasets and advanced selfsupervised\nlearning (SSL)-based CMs to evaluate current CMs in real-world scenarios. We\nalso assess the effectiveness of data augmentation strategies in enhancing CM\nrobustness for detecting deepfake speech on social media. Finally, by\naugmenting public datasets and incorporating the FSW training set, we\nsignificantly advanced real-world deepfake audio detection performance,\nachieving an average equal error rate (EER) of 3.54% across all evaluation\nsets."}
{"id": "2508.10536", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10536", "abs": "https://arxiv.org/abs/2508.10536", "authors": ["Christer Larsson"], "title": "Inverse Synthetic Aperture Radar, Radar Cross Section, and Iterative Smooth Reweighting $\\ell_1$-minimization", "comment": "Presented in ISCS25", "summary": "Radar Cross Section measurement data is often analyzed using Inverse\nSynthetic Aperture Radar images. This paper compares backprojection and\niterative smooth reweighted $\\ell_1$-minimization as methods to analyze radar\ncross section measurements and extract radar cross section for parts of the\nmeasured object. The main conclusion is that using backprojection images to\nextract RCS is robust and accurate but is more limited by the resolution than\niterative smooth reweighted $\\ell_1$-minimization. The latter method can be\nused for closely spaced scatterers but is limited in accuracy."}
{"id": "2508.10830", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10830", "abs": "https://arxiv.org/abs/2508.10830", "authors": ["Kai Li", "Guo Chen", "Wendi Sang", "Yi Luo", "Zhuo Chen", "Shuai Wang", "Shulin He", "Zhong-Qiu Wang", "Andong Li", "Zhiyong Wu", "Xiaolin Hu"], "title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends", "comment": "34 pages, 10 figures", "summary": "The field of speech separation, addressing the \"cocktail party problem\", has\nseen revolutionary advances with DNNs. Speech separation enhances clarity in\ncomplex acoustic environments and serves as crucial pre-processing for speech\nrecognition and speaker recognition. However, current literature focuses\nnarrowly on specific architectures or isolated approaches, creating fragmented\nunderstanding. This survey addresses this gap by providing systematic\nexamination of DNN-based speech separation techniques. Our work differentiates\nitself through: (I) Comprehensive perspective: We systematically investigate\nlearning paradigms, separation scenarios with known/unknown speakers,\ncomparative analysis of supervised/self-supervised/unsupervised frameworks, and\narchitectural components from encoders to estimation strategies. (II)\nTimeliness: Coverage of cutting-edge developments ensures access to current\ninnovations and benchmarks. (III) Unique insights: Beyond summarization, we\nevaluate technological trajectories, identify emerging patterns, and highlight\npromising directions including domain-robust frameworks, efficient\narchitectures, multimodal integration, and novel self-supervised paradigms.\n(IV) Fair evaluation: We provide quantitative evaluations on standard datasets,\nrevealing true capabilities and limitations of different methods. This\ncomprehensive survey serves as an accessible reference for experienced\nresearchers and newcomers navigating speech separation's complex landscape."}
{"id": "2508.10546", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10546", "abs": "https://arxiv.org/abs/2508.10546", "authors": ["Haotian Tian", "Lixiang Lian"], "title": "Unsupervised Deep Equilibrium Model Learning for Large-Scale Channel Estimation with Performance Guarantees", "comment": "17 pages, 10 figures", "summary": "Supervised deep learning methods have shown promise for large-scale channel\nestimation (LCE), but their reliance on ground-truth channel labels greatly\nlimits their practicality in real-world systems. In this paper, we propose an\nunsupervised learning framework for LCE that does not require ground-truth\nchannels. The proposed approach leverages Generalized Stein's Unbiased Risk\nEstimate (GSURE) as a principled unsupervised loss function, which provides an\nunbiased estimate of the projected mean-squared error (PMSE) from compressed\nnoisy measurements. To ensure a guaranteed performance, we integrate a deep\nequilibrium (DEQ) model, which implicitly represents an infinite-depth network\nby directly learning the fixed point of a parameterized iterative process. We\ntheoretically prove that, under mild conditions, the proposed GSURE-based\nunsupervised DEQ learning can achieve oracle-level supervised performance. In\nparticular, we show that the DEQ architecture inherently enforces a\ncompressible solution. We then demonstrate that DEQ-induced compressibility\nensures that optimizing the projected error via GSURE suffices to guarantee a\ngood MSE performance, enabling a rigorous performance guarantee. Extensive\nsimulations validate the theoretical findings and demonstrate that the proposed\nframework significantly outperforms various baselines when ground-truth channel\nis unavailable."}
{"id": "2508.10332", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10332", "abs": "https://arxiv.org/abs/2508.10332", "authors": ["Abhijit Sinha", "Harishankar Kumar", "Mohit Joshi", "Hemant Kumar Kathania", "Shrikanth Narayanan", "Sudarsana Reddy Kadiri"], "title": "Layer-Wise Analysis of Self-Supervised Representations for Age and Gender Classification in Children's Speech", "comment": "Accepted at Workshop on Child Computer Interaction (WOCCI 2025)", "summary": "Children's speech presents challenges for age and gender classification due\nto high variability in pitch, articulation, and developmental traits. While\nself-supervised learning (SSL) models perform well on adult speech tasks, their\nability to encode speaker traits in children remains underexplored. This paper\npresents a detailed layer-wise analysis of four Wav2Vec2 variants using the\nPFSTAR and CMU Kids datasets. Results show that early layers (1-7) capture\nspeaker-specific cues more effectively than deeper layers, which increasingly\nfocus on linguistic information. Applying PCA further improves classification,\nreducing redundancy and highlighting the most informative components. The\nWav2Vec2-large-lv60 model achieves 97.14% (age) and 98.20% (gender) on CMU\nKids; base-100h and large-lv60 models reach 86.05% and 95.00% on PFSTAR. These\nresults reveal how speaker traits are structured across SSL model depth and\nsupport more targeted, adaptive strategies for child-aware speech interfaces."}
{"id": "2508.10569", "categories": ["eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.10569", "abs": "https://arxiv.org/abs/2508.10569", "authors": ["ClÃ©ment Thomas", "Laurent Jacques", "Marc Georges"], "title": "Compressive Spectral Imaging in View of Earth Observation Applications", "comment": "Presented in ISCS25", "summary": "Earth observation from space is an important scientific and industrial\nactivity that has applications in many sectors. The instruments employed are\noften large, complex, and expensive. In addition, they generate large amounts\nof data, which is challenging for storage and transfer purposes. Compressive\nspectral imaging would be a cheaper, more efficient, and well-adapted technique\nto perform Earth observation. An interesting architecture is compressive\nspectral imaging with diffractive lenses, which is extremely compact. This work\ninvestigates the possibility of replacing the diffractive lens in this system\nwith a classical refractive lens. Taking advantage of the chromatic aberration\nof a lens makes the use of expensive diffractive lenses unnecessary.\nSimulations are performed to test the feasibility of the method. Signal\nrecovery is a basis pursuit solved using the Douglas-Rashford algorithm."}
{"id": "2508.10699", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10699", "abs": "https://arxiv.org/abs/2508.10699", "authors": ["Robert PÃ¶hlmann", "Emanuel Staudinger", "Gonzalo Seco-Granados"], "title": "Towards Hybrid Lunar PNT: Error Models, Lower Bounds and Algorithms", "comment": "Submitted to NAVIGATION, Journal of the Institute of Navigation. See\n  navi.ion.org", "summary": "Accurate positioning, navigation and timing (PNT) are crucial for upcoming\nlunar surface missions. Lunar satellite navigation systems are being developed,\nbut lack coverage during early deployment phases. Hybrid lunar PNT combining\ncooperative navigation, satellite systems, and an optional reference station\noffers improved accuracy and availability. This study develops realistic error\nmodels that incorporate temporal correlations often ignored in existing works.\nWe derive a cooperative navigation error model considering fading and\npseudorange bias from multipath propagation, and compare three error models for\nlunar satellite pseudorange and pseudorange rate signal-in-space error. These\ntemporal error correlation models integrate easily into Kalman filters and\nprovide realistic performance predictions essential for robust navigation\nengines. We perform case studies to demonstrate that hybrid navigation\nsignificantly improves accuracy, particularly with static users present. Most\nnotably, hybrid navigation enables optimal performance when using a lunar\nreference station, achieving sub-meter accuracy with only two visible\nsatellites."}
{"id": "2508.10783", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10783", "abs": "https://arxiv.org/abs/2508.10783", "authors": ["Murat Temiz", "Christos Masouros"], "title": "Affine Frequency Division Multiplexing with Subcarrier Power-Level Index Modulation for Integrated Sensing and Communications", "comment": "6 pages, 5 figures, conference article accepted for IEEE SPAWC 2025", "summary": "This study proposes an index modulation (IM) technique for affine frequency\ndivision multiplexing (AFDM) signals and examines its communication and sensing\nperformance toward integrated sensing and communication (ISAC) systems. The\npower levels of subcarriers are utilized as modulation indices while also\ntransmitting data symbols within each subcarrier. Thus, the proposed AFDM with\nsubcarrier power-level index modulation (AFDM-PLIM) maintains all subcarriers\nactive at all times to achieve a higher spectral efficiency compared to other\nAFDM-IM techniques, where some of the subcarriers are turned off for IM. A low\ncomplexity estimator and subcarrier grouping are also proposed to reduce the\ncomputational complexity of the maximum likelihood estimator. Furthermore, this\nstudy also examines the delay and Doppler ambiguity functions of the proposed\nAFDM-PLIM and evaluates its range estimation performance. The results show that\nits sensing performance is better than AFDM-IM waveforms due to keeping all\nsubcarriers active at all times."}
{"id": "2508.10805", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10805", "abs": "https://arxiv.org/abs/2508.10805", "authors": ["Giulio Basso", "Xi Long", "Reinder Haakma", "Rik Vullings"], "title": "Reduction of motion artifacts from photoplethysmography signals using learned convolutional sparse coding", "comment": "23 pages, 6 figures", "summary": "Objective. Wearable devices with embedded photoplethysmography (PPG) enable\ncontinuous non-invasive monitoring of cardiac activity, offering a promising\nstrategy to reduce the global burden of cardiovascular diseases. However,\nmonitoring during daily life introduces motion artifacts that can compromise\nthe signals. Traditional signal decomposition techniques often fail with severe\nartifacts. Deep learning denoisers are more effective but have poorer\ninterpretability, which is critical for clinical acceptance. This study\nproposes a framework that combines the advantages of both signal decomposition\nand deep learning approaches. Approach. We leverage algorithm unfolding to\nintegrate prior knowledge about the PPG structure into a deep neural network,\nimproving its interpretability. A learned convolutional sparse coding model\nencodes the signal into a sparse representation using a learned dictionary of\nkernels that capture recurrent morphological patterns. The network is trained\nfor denoising using the PulseDB dataset and a synthetic motion artifact model\nfrom the literature. Performance is benchmarked with PPG during daily\nactivities using the PPG-DaLiA dataset and compared with two reference deep\nlearning methods. Main results. On the synthetic dataset, the proposed method,\non average, improved the signal-to-noise ratio (SNR) from -7.07 dB to 11.23 dB\nand reduced the heart rate mean absolute error (MAE) by 55%. On the PPG-DaLiA\ndataset, the MAE decreased by 23%. The proposed method obtained higher SNR and\ncomparable MAE to the reference methods. Significance. Our method effectively\nenhances the quality of PPG signals from wearable devices and enables the\nextraction of meaningful waveform features, which may inspire innovative tools\nfor monitoring cardiovascular diseases."}
{"id": "2508.10820", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10820", "abs": "https://arxiv.org/abs/2508.10820", "authors": ["He Xu", "Tuo Wu", "Ye Tian", "Kangda Zhi", "Wei Liu", "Baiyang Liu", "Hing Cheung So", "Naofal Al-Dhahir", "Kin-Fai Tong", "Chan-Byoung Chae", "Kai-Kit Wong"], "title": "Fluid Antenna Enabled Direction-of-Arrival Estimation Under Time-Constrained Mobility", "comment": "13 pages", "summary": "Fluid antenna (FA) technology has emerged as a promising approach in wireless\ncommunications due to its capability of providing increased degrees of freedom\n(DoFs) and exceptional design flexibility. This paper addresses the challenge\nof direction-of-arrival (DOA) estimation for aligned received signals (ARS) and\nnon-aligned received signals (NARS) by designing two specialized uniform FA\nstructures under time-constrained mobility. For ARS scenarios, we propose a\nfully movable antenna configuration that maximizes the virtual array aperture,\nwhereas for NARS scenarios, we design a structure incorporating a fixed\nreference antenna to reliably extract phase information from the signal\ncovariance. To overcome the limitations of large virtual arrays and limited\nsample data inherent in time-varying channels (TVC), we introduce two novel DOA\nestimation methods: TMRLS-MUSIC for ARS, combining Toeplitz matrix\nreconstruction (TMR) with linear shrinkage (LS) estimation, and TMR-MUSIC for\nNARS, utilizing sub-covariance matrices to construct virtual array responses.\nBoth methods employ Nystrom approximation to significantly reduce computational\ncomplexity while maintaining estimation accuracy. Theoretical analyses and\nextensive simulation results demonstrate that the proposed methods achieve\nunderdetermined DOA estimation using minimal FA elements, outperform\nconventional methods in estimation accuracy, and substantially reduce\ncomputational complexity."}
{"id": "2508.10826", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10826", "abs": "https://arxiv.org/abs/2508.10826", "authors": ["He Xu", "Tuo Wu", "Ye Tian", "Ming Jin", "Wei Liu", "Qinghua Guo", "Maged Elkashlan", "Matthew C. Valenti", "Chan-Byoung Chae", "Kin-Fai Tong", "Kai-Kit Wong"], "title": "The Future is Fluid: Revolutionizing DOA Estimation with Sparse Fluid Antennas", "comment": "13 pages", "summary": "This paper investigates a design framework for sparse fluid antenna systems\n(FAS) enabling high-performance direction-of-arrival (DOA) estimation,\nparticularly in challenging millimeter-wave (mmWave) environments. By\ningeniously harnessing the mobility of fluid antenna (FA) elements, the\nproposed architectures achieve an extended range of spatial degrees of freedom\n(DoF) compared to conventional fixed-position antenna (FPA) arrays. This\ninnovation not only facilitates the seamless application of super-resolution\nDOA estimators but also enables robust DOA estimation, accurately localizing\nmore sources than the number of physical antenna elements. We introduce two\nbespoke FA array structures and mobility strategies tailored to scenarios with\naligned and misaligned received signals, respectively, demonstrating a\nhardware-driven approach to overcoming complexities typically addressed by\nintricate algorithms. A key contribution is a light-of-sight (LoS)-centric,\nclosed-form DOA estimator, which first employs an eigenvalue-ratio test for\nprecise LoS path number detection, followed by a polynomial root-finding\nprocedure. This method distinctly showcases the unique advantages of FAS by\nsimplifying the estimation process while enhancing accuracy. Numerical results\ncompellingly verify that the proposed FA array designs and estimation\ntechniques yield an extended DoF range, deliver superior DOA accuracy, and\nmaintain robustness across diverse signal conditions."}
{"id": "2508.10831", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.10831", "abs": "https://arxiv.org/abs/2508.10831", "authors": ["Tuo Wu", "Ye Tian", "Jie Tang", "Kangda Zhi", "Maged Elkashlan", "Kin-Fai Tong", "Naofal Al-Dhahir", "Chan-Byoung Chae", "Matthew C. Valenti", "George K. Karagiannidis", "Kwai-Man Luk"], "title": "Scalable FAS: A New Paradigm for Array Signal Processing", "comment": "13 pages", "summary": "Most existing antenna array-based source localization methods rely on\nfixed-position arrays (FPAs) and strict assumptions about source field\nconditions (near-field or far-field), which limits their effectiveness in\ncomplex, dynamic real-world scenarios where high-precision localization is\nrequired. In contrast, this paper introduces a novel scalable fluid antenna\nsystem (SFAS) that can dynamically adjust its aperture configuration to\noptimize performance for different localization tasks. Within this framework,\nwe develop a two-stage source localization strategy based on the exact spatial\ngeometry (ESG) model: the first stage uses a compact aperture configuration for\ninitial direction-of-arrival (DOA) estimation, while the second stage employs\nan expanded aperture for enhanced DOA and range estimation. The proposed\napproach eliminates the traditional need for signal separation or isolation to\nclassify source types and enables a single SFAS array to achieve high\nlocalization accuracy without field-specific assumptions, model\nsimplifications, or approximations, representing a new paradigm in array-based\nsource localization. Extensive simulations demonstrate the superiority of the\nproposed method in terms of localization accuracy, computational efficiency,\nand robustness to different source types."}
{"id": "2508.10856", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10856", "abs": "https://arxiv.org/abs/2508.10856", "authors": ["Bastian Heinlein", "Kaikai Zhu", "SÃ¼meyye Carkit-Yilmaz", "Sebastian Lotter", "Helene M. Loos", "Andrea Buettner", "Yansha Deng", "Robert Schober", "Vahid Jamali"], "title": "Molecule Mixture Detection and Alphabet Design for Non-linear, Cross-reactive Receiver Arrays in MC", "comment": "7 pages, 3 figures. Accepted at ACM NanoCom 2025", "summary": "Air-based molecular communication (MC) has the potential to be one of the\nfirst MC systems to be deployed in real-world applications, enabled by existing\nsensor technologies such as metal-oxide semi-conductor (MOS) sensors. However,\ncommercially available sensors usually exhibit non-linear and cross-reactive\nbehavior, contrary to the idealizing assumptions about linear and perfectly\nmolecule type-specific sensing often made in the MC literature. To address this\ngap, we propose a detector for molecule mixture communication with a general\nnon-linear, cross-reactive receiver (RX) array that performs approximate\nmaximum likelihood detection on the sensor outputs. Additionally, we introduce\nan algorithm for the design of mixture alphabets that accounts for the RX\ncharacteristics. We evaluate our detector and alphabet design algorithm through\nsimulations that are based on measurements reported for two commercial MOS\nsensors. Our simulations demonstrate that the proposed detector achieves\nsimilar symbol error rates as data-driven methods without requiring large\nnumbers of training samples and that the alphabet design algorithm outperforms\nmethods that do not account for the RX characteristics. Since the proposed\ndetector and alphabet design algorithm are also applicable to other chemical\nsensors, they pave the way for reliable air-based MC."}

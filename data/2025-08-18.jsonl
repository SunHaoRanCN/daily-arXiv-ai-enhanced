{"id": "2508.10949", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10949", "abs": "https://arxiv.org/abs/2508.10949", "authors": ["Chongyang Gao", "Marco Postiglione", "Isabel Gortner", "Sarit Kraus", "V. S. Subrahmanian"], "title": "Perturbed Public Voices (P$^{2}$V): A Dataset for Robust Audio Deepfake Detection", "comment": null, "summary": "Current audio deepfake detectors cannot be trusted. While they excel on\ncontrolled benchmarks, they fail when tested in the real world. We introduce\nPerturbed Public Voices (P$^{2}$V), an IRB-approved dataset capturing three\ncritical aspects of malicious deepfakes: (1) identity-consistent transcripts\nvia LLMs, (2) environmental and adversarial noise, and (3) state-of-the-art\nvoice cloning (2020-2025). Experiments reveal alarming vulnerabilities of 22\nrecent audio deepfake detectors: models trained on current datasets lose 43%\nperformance when tested on P$^{2}$V, with performance measured as the mean of\nF1 score on deepfake audio, AUC, and 1-EER. Simple adversarial perturbations\ninduce up to 16% performance degradation, while advanced cloning techniques\nreduce detectability by 20-30%. In contrast, P$^{2}$V-trained models maintain\nrobustness against these attacks while generalizing to existing datasets,\nestablishing a new benchmark for robust audio deepfake detection. P$^{2}$V will\nbe publicly released upon acceptance by a conference/journal."}
{"id": "2508.11074", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11074", "abs": "https://arxiv.org/abs/2508.11074", "authors": ["Haomin Zhang", "Kristin Qi", "Shuxin Yang", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters", "comment": "Gen4AVC@ICCV: 1st Workshop on Generative AI for Audio-Visual Content\n  Creation", "summary": "Generating high-quality and temporally synchronized audio from video content\nis essential for video editing and post-production tasks, enabling the creation\nof semantically aligned audio for silent videos. However, most existing\napproaches focus on short-form audio generation for video segments under 10\nseconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To\naddress these limitations, we introduce LD-LAudio-V1, an extension of\nstate-of-the-art video-to-audio models and it incorporates dual lightweight\nadapters to enable long-form audio generation. In addition, we release a clean\nand human-annotated video-to-audio dataset that contains pure sound effects\nwithout noise or artifacts. Our method significantly reduces splicing artifacts\nand temporal inconsistencies while maintaining computational efficiency.\nCompared to direct fine-tuning with short training videos, LD-LAudio-V1\nachieves significant improvements across multiple metrics: $FD_{\\text{passt}}$\n450.00 $\\rightarrow$ 327.29 (+27.27%), $FD_{\\text{panns}}$ 34.88 $\\rightarrow$\n22.68 (+34.98%), $FD_{\\text{vgg}}$ 3.75 $\\rightarrow$ 1.28 (+65.87%),\n$KL_{\\text{panns}}$ 2.49 $\\rightarrow$ 2.07 (+16.87%), $KL_{\\text{passt}}$ 1.78\n$\\rightarrow$ 1.53 (+14.04%), $IS_{\\text{panns}}$ 4.17 $\\rightarrow$ 4.30\n(+3.12%), $IB_{\\text{score}}$ 0.25 $\\rightarrow$ 0.28 (+12.00%),\n$Energy\\Delta10\\text{ms}$ 0.3013 $\\rightarrow$ 0.1349 (+55.23%),\n$Energy\\Delta10\\text{ms(vs.GT)}$ 0.0531 $\\rightarrow$ 0.0288 (+45.76%), and\n$Sem.\\,Rel.$ 2.73 $\\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate\nfurther research in long-form video-to-audio generation and is available at\nhttps://github.com/deepreasonings/long-form-video2audio."}
{"id": "2508.11224", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11224", "abs": "https://arxiv.org/abs/2508.11224", "authors": ["Kentaro Onda", "Satoru Fukayama", "Daisuke Saito", "Nobuaki Minematsu"], "title": "Benchmarking Prosody Encoding in Discrete Speech Tokens", "comment": "Accepted by ASRU2025", "summary": "Recently, discrete tokens derived from self-supervised learning (SSL) models\nvia k-means clustering have been actively studied as pseudo-text in speech\nlanguage models and as efficient intermediate representations for various\ntasks. However, these discrete tokens are typically learned in advance,\nseparately from the training of language models or downstream tasks. As a\nresult, choices related to discretization, such as the SSL model used or the\nnumber of clusters, must be made heuristically. In particular, speech language\nmodels are expected to understand and generate responses that reflect not only\nthe semantic content but also prosodic features. Yet, there has been limited\nresearch on the ability of discrete tokens to capture prosodic information. To\naddress this gap, this study conducts a comprehensive analysis focusing on\nprosodic encoding based on their sensitivity to the artificially modified\nprosody, aiming to provide practical guidelines for designing discrete tokens."}
{"id": "2508.11362", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11362", "abs": "https://arxiv.org/abs/2508.11362", "authors": ["Honghong Wang", "Yankai Wang", "Dejun Zhang", "Jing Deng", "Rong Zheng"], "title": "Mitigating Category Imbalance: Fosafer System for the Multimodal Emotion and Intent Joint Understanding Challenge", "comment": "2 pages. pubilshed by ICASSP2025", "summary": "This paper presents Fosafer approach to the Track 2 Mandarin in the\nMultimodal Emotion and Intent Joint Understandingchallenge, which focuses on\nachieving joint recognition of emotion and intent in Mandarin, despite the\nissue of category imbalance. To alleviate this issue, we use a variety of data\naugmentation techniques across text, video, and audio modalities. Additionally,\nwe introduce the SampleWeighted Focal Contrastive loss, designed to address the\nchallenges of recognizing minority class samples and those that are\nsemantically similar but difficult to distinguish. Moreover, we fine-tune the\nHubert model to adapt the emotion and intent joint recognition. To mitigate\nmodal competition, we introduce a modal dropout strategy. For the final\npredictions, a plurality voting approach is used to determine the results. The\nexperimental results demonstrate the effectiveness of our method, which\nachieves the second-best performance in the Track 2 Mandarin challenge."}
{"id": "2508.10924", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10924", "abs": "https://arxiv.org/abs/2508.10924", "authors": ["Zhiyuan Zhu", "Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhou Zhao"], "title": "ASAudio: A Survey of Advanced Spatial Audio Research", "comment": null, "summary": "With the rapid development of spatial audio technologies today, applications\nin AR, VR, and other scenarios have garnered extensive attention. Unlike\ntraditional mono sound, spatial audio offers a more realistic and immersive\nauditory experience. Despite notable progress in the field, there remains a\nlack of comprehensive surveys that systematically organize and analyze these\nmethods and their underlying technologies. In this paper, we provide a\ncomprehensive overview of spatial audio and systematically review recent\nliterature in the area. To address this, we chronologically outlining existing\nwork related to spatial audio and categorize these studies based on\ninput-output representations, as well as generation and understanding tasks,\nthereby summarizing various research aspects of spatial audio. In addition, we\nreview related datasets, evaluation metrics, and benchmarks, offering insights\nfrom both training and evaluation perspectives. Related materials are available\nat https://github.com/dieKarotte/ASAudio."}
{"id": "2508.11029", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11029", "abs": "https://arxiv.org/abs/2508.11029", "authors": ["Yuchen Zhang", "Francis Soualle", "Musa Furkan Keskin", "Yuan Liu", "Linlong Wu", "Jos√© A. del Peral-Rosado", "Bhavani Shankar M. R.", "Gonzalo Seco-Granados", "Henk Wymeersch", "Tareq Y. Al-Naffouri"], "title": "Distributed Integrated Sensing, Localization, and Communications over LEO Satellite Constellations", "comment": "This paper has been submitted to IEEE for possible publication", "summary": "Low Earth orbit (LEO) satellite constellations are rapidly becoming essential\nenablers of next-generation wireless systems, offering global broadband access,\nhigh-precision localization, and reliable sensing beyond terrestrial coverage.\nHowever, the inherent limitations of individual LEO satellites, including\nrestricted power, limited antenna aperture, and constrained onboard processing,\nhinder their ability to meet the growing demands of 6G applications. To address\nthese challenges, this article introduces the concept of distributed integrated\nsensing, localization, and communication (DISLAC) over LEO constellations,\ninspired by distributed multiple input multiple output architectures. By\nenabling inter-satellite cooperation through inter-satellite links, DISLAC can\nsubstantially improve throughput, positioning accuracy, and sensing robustness.\nWe present illustrative case studies that quantify these benefits and analyze\nkey system-level considerations, including synchronization, antenna\nreconfigurability, and ISL design. The article concludes by outlining open\nresearch directions to advance the practical deployment of DISLAC in future\nnon-terrestrial networks."}
{"id": "2508.11371", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11371", "abs": "https://arxiv.org/abs/2508.11371", "authors": ["Honghong Wang", "Xupeng Jia", "Jing Deng", "Rong Zheng"], "title": "Speech Emotion Recognition Using Fine-Tuned DWFormer:A Study on Track 1 of the IERPChallenge 2024", "comment": "5 pages,1 figures", "summary": "The field of artificial intelligence has a strong interest in the topic of\nemotion recognition. The majority of extant emotion recognition models are\noriented towards enhancing the precision of discrete emotion label prediction.\nGiven the direct relationship between human personality and emotion, as well as\nthe significant inter-individual differences in subjective emotional\nexpression, the IERP Challenge 2024 incorporates personality traits into\nemotion recognition research. This paper presents the Fosafer submissions to\nthe Track 1 of the IERP Challenge 2024. This task primarily concerns the\nrecognition of emotions in audio, while also providing text and audio features.\nIn Track 1, we utilized exclusively audio-based features and fine-tuned a\npre-trained speech emotion recognition model, DWFormer, through the integration\nof data augmentation and score fusion strategies, thereby achieving the first\nplace among the participating teams."}
{"id": "2508.10928", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10928", "abs": "https://arxiv.org/abs/2508.10928", "authors": ["Sheng Wong", "Beth Albert", "Gabriel Davis Jones"], "title": "CleanCTG: A Deep Learning Model for Multi-Artefact Detection and Reconstruction in Cardiotocography", "comment": null, "summary": "Cardiotocography (CTG) is essential for fetal monitoring but is frequently\ncompromised by diverse artefacts which obscure true fetal heart rate (FHR)\npatterns and can lead to misdiagnosis or delayed intervention. Current\ndeep-learning approaches typically bypass comprehensive noise handling,\napplying minimal preprocessing or focusing solely on downstream classification,\nwhile traditional methods rely on simple interpolation or rule-based filtering\nthat addresses only missing samples and fail to correct complex artefact types.\nWe present CleanCTG, an end-to-end dual-stage model that first identifies\nmultiple artefact types via multi-scale convolution and context-aware\ncross-attention, then reconstructs corrupted segments through artefact-specific\ncorrection branches. Training utilised over 800,000 minutes of physiologically\nrealistic, synthetically corrupted CTGs derived from expert-verified \"clean\"\nrecordings. On synthetic data, CleanCTG achieved perfect artefact detection\n(AU-ROC = 1.00) and reduced mean squared error (MSE) on corrupted segments to\n2.74 x 10^-4 (clean-segment MSE = 2.40 x 10^-6), outperforming the next best\nmethod by more than 60%. External validation on 10,190 minutes of\nclinician-annotated segments yielded AU-ROC = 0.95 (sensitivity = 83.44%,\nspecificity 94.22%), surpassing six comparator classifiers. Finally, when\nintegrated with the Dawes-Redman system on 933 clinical CTG recordings,\ndenoised traces increased specificity (from 80.70% to 82.70%) and shortened\nmedian time to decision by 33%. These findings suggest that explicit artefact\nremoval and signal reconstruction can both maintain diagnostic accuracy and\nenable shorter monitoring sessions, offering a practical route to more reliable\nCTG interpretation."}
{"id": "2508.11132", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.11132", "abs": "https://arxiv.org/abs/2508.11132", "authors": ["Sangwon Jo", "Seok-Hwan Park"], "title": "Multi-Satellite Cooperative MIMO Transmission: Statistical CSI-Aware RSMA Precoding Design", "comment": "accepted for publication in IEEE Wireless Communications Letters", "summary": "We investigate inter-satellite cooperative transmission in a multiple\nlow-Earth orbit (LEO) satellite communication system to enhance spectral\nefficiency. Specifically, we design multiple-input multipleoutput (MIMO)\nprecoding at LEO satellites for cooperative rate-splitting multiple access\n(RSMA). Given the difficulty of acquiring instantaneous channel state\ninformation (iCSI) due to long delays and Doppler effects, we formulate an\nergodic max-min fairness rate (MMFR) maximization problem based on statistical\nCSI (sCSI). To address the challenge of ergodic rate evaluation, we approximate\nthe problem using closed-form upper bounds and develop a weighted minimum mean\nsquared error-based algorithm to obtain a stationary point. Simulation results\ndemonstrate that the proposed sCSI-based RSMA scheme approaches iCSI-based\nperformance and significantly outperforms conventional space-division multiple\naccess."}
{"id": "2508.11609", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11609", "abs": "https://arxiv.org/abs/2508.11609", "authors": ["Kemal Altwlkany", "Elmedin Selmanovic", "Sead Delalic"], "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval", "comment": null, "summary": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes."}
{"id": "2508.11187", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.11187", "abs": "https://arxiv.org/abs/2508.11187", "authors": ["Wonjune Kang", "Deb Roy"], "title": "Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style", "comment": "Accepted to ASRU 2025", "summary": "We introduce the task of expressive speech retrieval, where the goal is to\nretrieve speech utterances spoken in a given style based on a natural language\ndescription of that style. While prior work has primarily focused on performing\nspeech retrieval based on what was said in an utterance, we aim to do so based\non how something was said. We train speech and text encoders to embed speech\nand text descriptions of speaking styles into a joint latent space, which\nenables using free-form text prompts describing emotions or styles as queries\nto retrieve matching expressive speech segments. We perform detailed analyses\nof various aspects of our proposed framework, including encoder architectures,\ntraining criteria for effective cross-modal alignment, and prompt augmentation\nfor improved generalization to arbitrary text queries. Experiments on multiple\ndatasets encompassing 22 speaking styles demonstrate that our approach achieves\nstrong retrieval performance as measured by Recall@k."}
{"id": "2508.11178", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11178", "abs": "https://arxiv.org/abs/2508.11178", "authors": ["Yida Zhang", "Qiuyan Liu", "Qiang Wang", "Hongtao Luo", "Yuqi Xia"], "title": "Near-Field Variable-Width Beam Coverage and Codebook Design for XL-RIS", "comment": null, "summary": "To mitigate the issue of limited base station coverage caused by severe\nhigh-frequency electromagnetic wave attenuation, Extremely Large Reconfigurable\nIntelligent Surface (XL-RIS) has garnered significant attention due to its high\nbeam gain. However, XL-RIS exhibits a narrower beam width compared to\ntraditional RIS, which increases the complexity of beam alignment and\nbroadcast. To address this problem, we propose a variable-width beam generation\nalgorithm under the near-field assumption and apply it to the near-field\ncodebook design for XL-RIS. Our algorithm can achieve beam coverage for\narbitrarily shaped codeword regions and generate a joint codebook for the\nmulti-XL-RIS system. The simulation results demonstrate that our proposed\nscheme enables user equipment (UE) to achieve higher spectral efficiency and\nlower communication outage probability within the codeword region compared to\nexisting works. Furthermore, our scheme exhibits better robustness to codeword\nregion location and area variations."}
{"id": "2508.10924", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10924", "abs": "https://arxiv.org/abs/2508.10924", "authors": ["Zhiyuan Zhu", "Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhou Zhao"], "title": "ASAudio: A Survey of Advanced Spatial Audio Research", "comment": null, "summary": "With the rapid development of spatial audio technologies today, applications\nin AR, VR, and other scenarios have garnered extensive attention. Unlike\ntraditional mono sound, spatial audio offers a more realistic and immersive\nauditory experience. Despite notable progress in the field, there remains a\nlack of comprehensive surveys that systematically organize and analyze these\nmethods and their underlying technologies. In this paper, we provide a\ncomprehensive overview of spatial audio and systematically review recent\nliterature in the area. To address this, we chronologically outlining existing\nwork related to spatial audio and categorize these studies based on\ninput-output representations, as well as generation and understanding tasks,\nthereby summarizing various research aspects of spatial audio. In addition, we\nreview related datasets, evaluation metrics, and benchmarks, offering insights\nfrom both training and evaluation perspectives. Related materials are available\nat https://github.com/dieKarotte/ASAudio."}
{"id": "2508.11273", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11273", "abs": "https://arxiv.org/abs/2508.11273", "authors": ["Joonyong Park", "Kenichi Nakamura"], "title": "EmoSSLSphere: Multilingual Emotional Speech Synthesis with Spherical Vectors and Discrete Speech Tokens", "comment": "In Proceedings of the 13th ISCA Speech Synthesis Workshop", "summary": "This paper introduces EmoSSLSphere, a novel framework for multilingual\nemotional text-to-speech (TTS) synthesis that combines spherical emotion\nvectors with discrete token features derived from self-supervised learning\n(SSL). By encoding emotions in a continuous spherical coordinate space and\nleveraging SSL-based representations for semantic and acoustic modeling,\nEmoSSLSphere enables fine-grained emotional control, effective cross-lingual\nemotion transfer, and robust preservation of speaker identity. We evaluate\nEmoSSLSphere on English and Japanese corpora, demonstrating significant\nimprovements in speech intelligibility, spectral fidelity, prosodic\nconsistency, and overall synthesis quality. Subjective evaluations further\nconfirm that our method outperforms baseline models in terms of naturalness and\nemotional expressiveness, underscoring its potential as a scalable solution for\nmultilingual emotional TTS."}
{"id": "2508.11186", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11186", "abs": "https://arxiv.org/abs/2508.11186", "authors": ["Mohammad Alikhani"], "title": "KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network", "comment": null, "summary": "Human Activity Recognition (HAR) plays a critical role in numerous\napplications, including healthcare monitoring, fitness tracking, and smart\nenvironments. Traditional deep learning (DL) approaches, while effective, often\nrequire extensive parameter tuning and may lack interpretability. In this work,\nwe investigate the use of a single three-axis accelerometer and the\nKolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model\ncomplex nonlinear relationships with improved interpretability and parameter\nefficiency. The MotionSense dataset, containing smartphone-based motion sensor\nsignals across various physical activities, is employed to evaluate the\nproposed approach. Our methodology involves preprocessing and normalization of\naccelerometer and gyroscope data, followed by KAN-based feature learning and\nclassification. Experimental results demonstrate that the KAN achieves\ncompetitive or superior classification performance compared to conventional\ndeep neural networks, while maintaining a significantly reduced parameter\ncount. This highlights the potential of KAN architectures as an efficient and\ninterpretable alternative for real-world HAR systems. The open-source\nimplementation of the proposed framework is available at the Project's GitHub\nRepository."}
{"id": "2508.10928", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.10928", "abs": "https://arxiv.org/abs/2508.10928", "authors": ["Sheng Wong", "Beth Albert", "Gabriel Davis Jones"], "title": "CleanCTG: A Deep Learning Model for Multi-Artefact Detection and Reconstruction in Cardiotocography", "comment": null, "summary": "Cardiotocography (CTG) is essential for fetal monitoring but is frequently\ncompromised by diverse artefacts which obscure true fetal heart rate (FHR)\npatterns and can lead to misdiagnosis or delayed intervention. Current\ndeep-learning approaches typically bypass comprehensive noise handling,\napplying minimal preprocessing or focusing solely on downstream classification,\nwhile traditional methods rely on simple interpolation or rule-based filtering\nthat addresses only missing samples and fail to correct complex artefact types.\nWe present CleanCTG, an end-to-end dual-stage model that first identifies\nmultiple artefact types via multi-scale convolution and context-aware\ncross-attention, then reconstructs corrupted segments through artefact-specific\ncorrection branches. Training utilised over 800,000 minutes of physiologically\nrealistic, synthetically corrupted CTGs derived from expert-verified \"clean\"\nrecordings. On synthetic data, CleanCTG achieved perfect artefact detection\n(AU-ROC = 1.00) and reduced mean squared error (MSE) on corrupted segments to\n2.74 x 10^-4 (clean-segment MSE = 2.40 x 10^-6), outperforming the next best\nmethod by more than 60%. External validation on 10,190 minutes of\nclinician-annotated segments yielded AU-ROC = 0.95 (sensitivity = 83.44%,\nspecificity 94.22%), surpassing six comparator classifiers. Finally, when\nintegrated with the Dawes-Redman system on 933 clinical CTG recordings,\ndenoised traces increased specificity (from 80.70% to 82.70%) and shortened\nmedian time to decision by 33%. These findings suggest that explicit artefact\nremoval and signal reconstruction can both maintain diagnostic accuracy and\nenable shorter monitoring sessions, offering a practical route to more reliable\nCTG interpretation."}
{"id": "2508.11326", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.11326", "abs": "https://arxiv.org/abs/2508.11326", "authors": ["Heyang Xue", "Xuchen Song", "Yu Tang", "Jianyu Chen", "Yanru Chen", "Yang Li", "Yahui Zhou"], "title": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for Description-based TTS via Mixture-of-Experts", "comment": null, "summary": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/."}
{"id": "2508.11234", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11234", "abs": "https://arxiv.org/abs/2508.11234", "authors": ["Shengheng Liu", "Ningning Fu"], "title": "Enabling low-power massive MIMO with ternary ADCs for AIoT sensing", "comment": "Already published in ACM TOSN. 27 pages, 7 figures", "summary": "The proliferation of networked devices and the surging demand for ubiquitous\nintelligence have given rise to the artificial intelligence of things (AIoT).\nHowever, the utilization of high-resolution analog-to-digital converters (ADCs)\nand numerous radio frequency chains significantly raises power consumption.\nThis paper explores a cost-effective solution using ternary ADCs (T-ADCs) in\nmassive multiple-input-multiple-output (MIMO) systems for low-power AIoT and\nspecifically addresses channel sensing challenges. The channel is first\nestimated through a pilot-aided scheme and refined using a joint-pilot-and-data\n(JPD) approach. To assess the performance limits of this two-threshold ADC\nsystem, the analysis includes its hardware-ideal counterpart, the parallel\none-bit ADCs (PO-ADCs) and a realistic scenario where noise variance is unknown\nat the receiver is considered. Analytical findings indicate that the JPD scheme\neffectively mitigates performance degradation in channel estimation due to\ncoarse quantization effects under mild conditions, without necessitating\nadditional pilot overhead. For deterministic and random channels, we propose\nmodified expectation maximization (EM) and variational inference EM estimators,\nrespectively. Extensive simulations validate the theoretical results and\ndemonstrate the effectiveness of the proposed estimators in terms of mean\nsquare error and symbol error rate, which showcases the feasibility of\nimplementing T-ADCs and the associated JPD scheme for greener AIoT smart\nsensing."}
{"id": "2508.11187", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.11187", "abs": "https://arxiv.org/abs/2508.11187", "authors": ["Wonjune Kang", "Deb Roy"], "title": "Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style", "comment": "Accepted to ASRU 2025", "summary": "We introduce the task of expressive speech retrieval, where the goal is to\nretrieve speech utterances spoken in a given style based on a natural language\ndescription of that style. While prior work has primarily focused on performing\nspeech retrieval based on what was said in an utterance, we aim to do so based\non how something was said. We train speech and text encoders to embed speech\nand text descriptions of speaking styles into a joint latent space, which\nenables using free-form text prompts describing emotions or styles as queries\nto retrieve matching expressive speech segments. We perform detailed analyses\nof various aspects of our proposed framework, including encoder architectures,\ntraining criteria for effective cross-modal alignment, and prompt augmentation\nfor improved generalization to arbitrary text queries. Experiments on multiple\ndatasets encompassing 22 speaking styles demonstrate that our approach achieves\nstrong retrieval performance as measured by Recall@k."}
{"id": "2508.11535", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11535", "abs": "https://arxiv.org/abs/2508.11535", "authors": ["Navin Raj Prabhu", "Danilo de Oliveira", "Nale Lehmann-Willenbrock", "Timo Gerkmann"], "title": "Enhancing In-the-Wild Speech Emotion Conversion with Resynthesis-based Duration Modeling", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Speech Emotion Conversion aims to modify the emotion expressed in input\nspeech while preserving lexical content and speaker identity. Recently,\ngenerative modeling approaches have shown promising results in changing local\nacoustic properties such as fundamental frequency, spectral envelope and\nenergy, but often lack the ability to control the duration of sounds. To\naddress this, we propose a duration modeling framework using resynthesis-based\ndiscrete content representations, enabling modification of speech duration to\nreflect target emotions and achieve controllable speech rates without using\nparallel data. Experimental results reveal that the inclusion of the proposed\nduration modeling framework significantly enhances emotional expressiveness, in\nthe in-the-wild MSP-Podcast dataset. Analyses show that low-arousal emotions\ncorrelate with longer durations and slower speech rates, while high-arousal\nemotions produce shorter, faster speech."}
{"id": "2508.11259", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11259", "abs": "https://arxiv.org/abs/2508.11259", "authors": ["Ryosuke Isono", "Shunsuke Ono"], "title": "Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing.\n  arXiv admin note: text overlap with arXiv:2308.00500", "summary": "This paper proposes a novel spatiotemporal (ST) fusion framework for\nsatellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).\nST fusion is a promising approach to address the trade-off between the spatial\nand temporal resolution of satellite images. In real-world scenarios, observed\nsatellite images are severely degraded by noise due to measurement equipment\nand environmental conditions. Consequently, some recent studies have focused on\nenhancing the robustness of ST fusion methods against noise. However, existing\nnoise-robust ST fusion approaches often fail to capture fine spatial structure,\nleading to oversmoothing and artifacts. To address this issue, TSSTF introduces\ntwo key mechanisms: Temporally-Guided Total Variation (TGTV) and\nTemporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization\nfunction that promotes spatial piecewise smoothness while preserving structural\ndetails, guided by a reference high spatial resolution image acquired on a\nnearby date. TGEC enforces consistency in edge locations between two temporally\nadjacent images, while allowing for spectral variations. We formulate the ST\nfusion task as a constrained optimization problem incorporating TGTV and TGEC,\nand develop an efficient algorithm based on a preconditioned primal-dual\nsplitting method. Experimental results demonstrate that TSSTF performs\ncomparably to state-of-the-art methods under noise-free conditions and\noutperforms them under noisy conditions. Additionally, we provide a\ncomprehensive set of recommended parameter values that consistently yield high\nperformance across diverse target regions and noise conditions, aiming to\nenhance reproducibility and practical utility."}
{"id": "2508.11326", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.11326", "abs": "https://arxiv.org/abs/2508.11326", "authors": ["Heyang Xue", "Xuchen Song", "Yu Tang", "Jianyu Chen", "Yanru Chen", "Yang Li", "Yahui Zhou"], "title": "MoE-TTS: Enhancing Out-of-Domain Text Understanding for Description-based TTS via Mixture-of-Experts", "comment": null, "summary": "Description-based text-to-speech (TTS) models exhibit strong performance on\nin-domain text descriptions, i.e., those encountered during training. However,\nin real-world applications, the diverse range of user-generated descriptions\ninevitably introduces numerous out-of-domain inputs that challenge the text\nunderstanding capabilities of these systems. To address this issue, we propose\nMoE-TTS, a description-based TTS model designed to enhance the understanding of\nout-of-domain text descriptions. MoE-TTS employs a modality-based\nmixture-of-experts (MoE) approach to augment a pre-trained textual large\nlanguage model (LLM) with a set of specialized weights adapted to the speech\nmodality while maintaining the original LLM frozen during training. This\napproach allows MoE-TTS to effectively leverage the pre-trained knowledge and\ntext understanding abilities of textual LLMs. Our experimental results indicate\nthat: first, even the most advanced closed-source commercial products can be\nchallenged by carefully designed out-of-domain description test sets; second,\nMoE-TTS achieves superior performance in generating speech that more accurately\nreflects the descriptions. We encourage readers to listen to the demos at\nhttps://welkinyang.github.io/MoE-TTS/."}
{"id": "2508.11566", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11566", "abs": "https://arxiv.org/abs/2508.11566", "authors": ["Shaun Cassini", "Thomas Hain", "Anton Ragni"], "title": "Emphasis Sensitivity in Speech Representations", "comment": "Accepted to IEEE ASRU 2025", "summary": "This work investigates whether modern speech models are sensitive to prosodic\nemphasis - whether they encode emphasized and neutral words in systematically\ndifferent ways. Prior work typically relies on isolated acoustic correlates\n(e.g., pitch, duration) or label prediction, both of which miss the relational\nstructure of emphasis. This paper proposes a residual-based framework, defining\nemphasis as the difference between paired neutral and emphasized word\nrepresentations. Analysis on self-supervised speech models shows that these\nresiduals correlate strongly with duration changes and perform poorly at word\nidentity prediction, indicating a structured, relational encoding of prosodic\nemphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more\ncompact than in pre-trained models, further suggesting that emphasis is encoded\nas a consistent, low-dimensional transformation that becomes more structured\nwith task-specific learning."}
{"id": "2508.11292", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.11292", "abs": "https://arxiv.org/abs/2508.11292", "authors": ["Xiaoqi Zhang", "Liang Liu", "Shuowen Zhang", "Haijun Zhang"], "title": "Beyond Diagonal Reconfigurable Intelligent Surface Enabled Sensing: Cramer-Rao Bound Optimization", "comment": "to appear in IEEE Wireless Communications Letters", "summary": "Recently, beyond diagonal reconfigurable intelligent surface (BD-RIS) has\nemerged as a more flexible solution to engineer the wireless propagation\nchannels, thanks to its non-diagonal reflecting matrix. Although the gain of\nthe BD-RIS over the conventional RIS in communication has been revealed in many\nworks, its gain in 6G sensing is still unknown. This motivates us to study the\nBD-RIS assisted sensing in this letter. Specifically, we derive the Cramer-Rao\nbound (CRB) for estimating the angle-of-arrival (AOA) from the target to the\nBD-RIS under the constraint that the BD-RIS scattering matrix is unitary. To\nminimize the CRB, we develop an optimization scheme based on an adaptive\nRiemannian steepest ascent algorithm that can satisfy the non-convex unitary\nconstraint. Numerical results demonstrate that the proposed BD-RIS-assisted\ntarget localization method achieves superior sensing performance."}
{"id": "2508.10949", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10949", "abs": "https://arxiv.org/abs/2508.10949", "authors": ["Chongyang Gao", "Marco Postiglione", "Isabel Gortner", "Sarit Kraus", "V. S. Subrahmanian"], "title": "Perturbed Public Voices (P$^{2}$V): A Dataset for Robust Audio Deepfake Detection", "comment": null, "summary": "Current audio deepfake detectors cannot be trusted. While they excel on\ncontrolled benchmarks, they fail when tested in the real world. We introduce\nPerturbed Public Voices (P$^{2}$V), an IRB-approved dataset capturing three\ncritical aspects of malicious deepfakes: (1) identity-consistent transcripts\nvia LLMs, (2) environmental and adversarial noise, and (3) state-of-the-art\nvoice cloning (2020-2025). Experiments reveal alarming vulnerabilities of 22\nrecent audio deepfake detectors: models trained on current datasets lose 43%\nperformance when tested on P$^{2}$V, with performance measured as the mean of\nF1 score on deepfake audio, AUC, and 1-EER. Simple adversarial perturbations\ninduce up to 16% performance degradation, while advanced cloning techniques\nreduce detectability by 20-30%. In contrast, P$^{2}$V-trained models maintain\nrobustness against these attacks while generalizing to existing datasets,\nestablishing a new benchmark for robust audio deepfake detection. P$^{2}$V will\nbe publicly released upon acceptance by a conference/journal."}
{"id": "2508.11295", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.11295", "abs": "https://arxiv.org/abs/2508.11295", "authors": ["Xiaoqi Zhang", "Liang Liu", "Shuowen Zhang", "Weifeng Zhu", "Haijun Zhang"], "title": "Optimizing Rate-CRB Performance for Beyond Diagonal Reconfigurable Intelligent Surface Enabled ISAC", "comment": "to appear in IEEE Communications Letters", "summary": "This letter considers a beyond diagonal reconfigurable intelligent surface\n(BD-RIS) aided integrated sensing and communication (ISAC) system, where the\nBD-RIS can help a multi-antenna base station (BS) serve multiple user\nequipments (UEs) and localize a target simultaneously. We formulate an\noptimization problem that designs the BS beamforming matrix and the BD-RIS\nscattering matrix to maximize UEs' sum rate subject to a localization\nCramer-Rao bound (CRB) constraint and an additional unitary matrix constraint\nfor the scattering matrix. Because unitary matrices form a manifold, our\nproblem belongs to constrained manifold optimization. This letter proposes a\nlog-barrier based Riemannian steepest ascent method to solve this problem\neffectively. Numerical results verify the effectiveness of our algorithm and\nthe performance gain of the BD-RIS aided ISAC systems over the conventional RIS\naided ISAC systems."}
{"id": "2508.11074", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11074", "abs": "https://arxiv.org/abs/2508.11074", "authors": ["Haomin Zhang", "Kristin Qi", "Shuxin Yang", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters", "comment": "Gen4AVC@ICCV: 1st Workshop on Generative AI for Audio-Visual Content\n  Creation", "summary": "Generating high-quality and temporally synchronized audio from video content\nis essential for video editing and post-production tasks, enabling the creation\nof semantically aligned audio for silent videos. However, most existing\napproaches focus on short-form audio generation for video segments under 10\nseconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To\naddress these limitations, we introduce LD-LAudio-V1, an extension of\nstate-of-the-art video-to-audio models and it incorporates dual lightweight\nadapters to enable long-form audio generation. In addition, we release a clean\nand human-annotated video-to-audio dataset that contains pure sound effects\nwithout noise or artifacts. Our method significantly reduces splicing artifacts\nand temporal inconsistencies while maintaining computational efficiency.\nCompared to direct fine-tuning with short training videos, LD-LAudio-V1\nachieves significant improvements across multiple metrics: $FD_{\\text{passt}}$\n450.00 $\\rightarrow$ 327.29 (+27.27%), $FD_{\\text{panns}}$ 34.88 $\\rightarrow$\n22.68 (+34.98%), $FD_{\\text{vgg}}$ 3.75 $\\rightarrow$ 1.28 (+65.87%),\n$KL_{\\text{panns}}$ 2.49 $\\rightarrow$ 2.07 (+16.87%), $KL_{\\text{passt}}$ 1.78\n$\\rightarrow$ 1.53 (+14.04%), $IS_{\\text{panns}}$ 4.17 $\\rightarrow$ 4.30\n(+3.12%), $IB_{\\text{score}}$ 0.25 $\\rightarrow$ 0.28 (+12.00%),\n$Energy\\Delta10\\text{ms}$ 0.3013 $\\rightarrow$ 0.1349 (+55.23%),\n$Energy\\Delta10\\text{ms(vs.GT)}$ 0.0531 $\\rightarrow$ 0.0288 (+45.76%), and\n$Sem.\\,Rel.$ 2.73 $\\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate\nfurther research in long-form video-to-audio generation and is available at\nhttps://github.com/deepreasonings/long-form-video2audio."}
{"id": "2508.11351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11351", "abs": "https://arxiv.org/abs/2508.11351", "authors": ["Haonan Lu", "Rui Meng", "Xiaodong Xu", "Yiming Liu", "Ping Zhang", "Dusit Niyato"], "title": "Important Bit Prefix M-ary Quadrature Amplitude Modulation for Semantic Communications", "comment": null, "summary": "M-ary Quadrature Amplitude Modulation (MQAM) is a commonly used channel\nmodulation technology in wireless communication systems. To achieve dedicated\nchannel modulation for semantic communication (SemCom), we propose an\nImportant-Bit-Prefixed MQAM (IBP-MQAM) scheme and derive its approximate\nexpression of important symbol error rate (ISER) and unimportant symbol error\nrate (USER). By extracting and quantifying text semantics using Latent\nDirichlet Allocation (LDA), we verify that IBP-MQAM achieves improved\nperformance over MQAM in SemCom scenarios and further analyze the effects of\nkey system parameters."}
{"id": "2508.11224", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11224", "abs": "https://arxiv.org/abs/2508.11224", "authors": ["Kentaro Onda", "Satoru Fukayama", "Daisuke Saito", "Nobuaki Minematsu"], "title": "Benchmarking Prosody Encoding in Discrete Speech Tokens", "comment": "Accepted by ASRU2025", "summary": "Recently, discrete tokens derived from self-supervised learning (SSL) models\nvia k-means clustering have been actively studied as pseudo-text in speech\nlanguage models and as efficient intermediate representations for various\ntasks. However, these discrete tokens are typically learned in advance,\nseparately from the training of language models or downstream tasks. As a\nresult, choices related to discretization, such as the SSL model used or the\nnumber of clusters, must be made heuristically. In particular, speech language\nmodels are expected to understand and generate responses that reflect not only\nthe semantic content but also prosodic features. Yet, there has been limited\nresearch on the ability of discrete tokens to capture prosodic information. To\naddress this gap, this study conducts a comprehensive analysis focusing on\nprosodic encoding based on their sensitivity to the artificially modified\nprosody, aiming to provide practical guidelines for designing discrete tokens."}
{"id": "2508.11457", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11457", "abs": "https://arxiv.org/abs/2508.11457", "authors": ["Hui Cao", "Rui Meng", "Xiaodong Xu", "Shujun Han", "Ping Zhang"], "title": "Importance-Aware Robust Semantic Transmission for LEO Satellite-Ground Communication", "comment": null, "summary": "Satellite-ground semantic communication is anticipated to serve a critical\nrole in the forthcoming 6G era. Nonetheless, task-oriented data transmission in\nsuch systems remains a formidable challenge, primarily due to the dynamic\nnature of signal-to-noise ratio (SNR) fluctuations and the stringent bandwidth\nlimitations inherent to low Earth orbit (LEO) satellite channels. In response\nto these constraints, we propose an importance-aware robust semantic\ntransmission (IRST) framework, specifically designed for scenarios\ncharacterized by bandwidth scarcity and channel variability. The IRST scheme\nbegins by applying a segmentation model enhancement algorithm to improve the\ngranularity and accuracy of semantic segmentation. Subsequently, a task-driven\nsemantic selection method is employed to prioritize the transmission of\nsemantically vital content based on real-time channel state information.\nFurthermore, the framework incorporates a stack-based, SNR-aware channel codec\ncapable of executing adaptive channel coding in alignment with SNR variations.\nComparative evaluations across diverse operating conditions demonstrate the\nsuperior performance and resilience of the IRST model relative to existing\nbenchmarks."}
{"id": "2508.11362", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11362", "abs": "https://arxiv.org/abs/2508.11362", "authors": ["Honghong Wang", "Yankai Wang", "Dejun Zhang", "Jing Deng", "Rong Zheng"], "title": "Mitigating Category Imbalance: Fosafer System for the Multimodal Emotion and Intent Joint Understanding Challenge", "comment": "2 pages. pubilshed by ICASSP2025", "summary": "This paper presents Fosafer approach to the Track 2 Mandarin in the\nMultimodal Emotion and Intent Joint Understandingchallenge, which focuses on\nachieving joint recognition of emotion and intent in Mandarin, despite the\nissue of category imbalance. To alleviate this issue, we use a variety of data\naugmentation techniques across text, video, and audio modalities. Additionally,\nwe introduce the SampleWeighted Focal Contrastive loss, designed to address the\nchallenges of recognizing minority class samples and those that are\nsemantically similar but difficult to distinguish. Moreover, we fine-tune the\nHubert model to adapt the emotion and intent joint recognition. To mitigate\nmodal competition, we introduce a modal dropout strategy. For the final\npredictions, a plurality voting approach is used to determine the results. The\nexperimental results demonstrate the effectiveness of our method, which\nachieves the second-best performance in the Track 2 Mandarin challenge."}
{"id": "2508.11459", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11459", "abs": "https://arxiv.org/abs/2508.11459", "authors": ["Tzu-Chi Liu", "Po-Lin Chen", "Yi-Chieh Chen", "Po-Hsun Tu", "Chih-Hua Yeh", "Mun-Chun Yeap", "Chiung-Chu Chen", "Hau-Tieng Wu"], "title": "Efficient Artifacts Removal for Adaptive Deep Brain Stimulation and a Temporal Event Localization Analysis", "comment": "This manuscript is under review at Journal of Neural Engineering", "summary": "Adaptive deep brain stimulation (aDBS) leverages symptom-related biomarkers\nto deliver personalized neuromodulation therapy, with the potential to improve\ntreatment efficacy and reduce power consumption compared to conventional DBS.\nHowever, stimulation-induced signal contamination remains a major technical\nbarrier to advancing its clinical application. Existing artifact removal\nstrategies, both front-end and back-end, face trade-offs between artifact\nsuppression and algorithmic flexibility. Among back-end algorithms, Shrinkage\nand Manifold-based Artifact Removal using Template Adaptation (SMARTA) has\nshown promising performance in mitigating stimulus artifacts with minimal\ndistortion to local field potentials (LFPs), but its high computational demand\nand inability to handle transient direct current (DC) artifacts limit its use\nin real-time applications. To address this, we developed SMARTA+, a\ncomputationally efficient extension of SMARTA capable of suppressing both\nstimulus and transient DC artifacts while supporting flexible algorithmic\ndesign. We evaluated SMARTA+ using semi-real aDBS data and real data from\nParkinson's disease patients. Compared to SMARTA and other established methods,\nSMARTA+ achieved comparable or superior artifact removal while significantly\nreducing computation time. It preserved spectral and temporal structures,\nranging from beta band to high-frequency oscillations, and demonstrated\nrobustness across diverse stimulation protocols. Temporal event localization\nanalysis further showed improved accuracy in detecting beta bursts. These\nfindings support SMARTA+ as a promising tool for advancing real-time,\nclosed-loop aDBS systems."}
{"id": "2508.11371", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11371", "abs": "https://arxiv.org/abs/2508.11371", "authors": ["Honghong Wang", "Xupeng Jia", "Jing Deng", "Rong Zheng"], "title": "Speech Emotion Recognition Using Fine-Tuned DWFormer:A Study on Track 1 of the IERPChallenge 2024", "comment": "5 pages,1 figures", "summary": "The field of artificial intelligence has a strong interest in the topic of\nemotion recognition. The majority of extant emotion recognition models are\noriented towards enhancing the precision of discrete emotion label prediction.\nGiven the direct relationship between human personality and emotion, as well as\nthe significant inter-individual differences in subjective emotional\nexpression, the IERP Challenge 2024 incorporates personality traits into\nemotion recognition research. This paper presents the Fosafer submissions to\nthe Track 1 of the IERP Challenge 2024. This task primarily concerns the\nrecognition of emotions in audio, while also providing text and audio features.\nIn Track 1, we utilized exclusively audio-based features and fine-tuned a\npre-trained speech emotion recognition model, DWFormer, through the integration\nof data augmentation and score fusion strategies, thereby achieving the first\nplace among the participating teams."}
{"id": "2508.11473", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11473", "abs": "https://arxiv.org/abs/2508.11473", "authors": ["Yuqin Liu", "Mona Jaber", "Yan Liu", "Arumugam Nallanathan"], "title": "Reducing AoI and Improving Throughput for NOMA-assisted SGF Systems: A Hierarchical Learning Approach", "comment": null, "summary": "A non-orthogonal multiple access (NOMA) assisted semi-grant-free (SGF)\nframework is proposed to enable channel access for grant-free users (GFUs) by\nusing residual resources from grant-based users. Under this framework, the\nproblem of joint beamforming design and transmission scheduling is formulated\nto improve the system throughput and reduce the age-of-information of GFUs. The\naforementioned problem is transferred into a Markov Decision Process to model\nthe changing environment with the transmission/ waiting/ retransmission of\nGFUs. In an effort to solve the pertinent problem, firstly, a deep\nreinforcement learning (DRL) based transmission scheduling approach is proposed\nfor determining the optimal transmission probability based on the available\ntransmission slots and transmission status of GFUs. Secondly, a hierarchical\nlearning algorithm is proposed to analyze the channel state information of GBUs\nand the transmission status of GFUs, and to train an upper-level policy based\non this analysis for beamforming to achieve efficient grant-based transmission,\nwhile a lower-level policy adapts to maximize the utilization of transmission\nslots allocated by the upper-level agent. The two policies interact to improve\nchannel access and avoid collisions. Numerical results reveal that 1) The DRL\nbased transmission scheduling outperforms existing adaptive and state-dependent\nbaselines in AoI reduction, where an average\nthree-time-slots-earlier-transmission can be obtained compared to the\nstate-dependent choice, and five time slots earlier can be achieved when\ncomparing to the adaptive choice; 2) The hierarchical learning algorithm is\nable to achieve approximately a 31.82% gain while maintaining the average AoI\nof GFUs within 1.5 time slots. 3) The effectiveness of the hierarchical\nlearning scheme in NOMA-assisted SGF system is validated across scenarios with\nGFUs counts from 1-5 times of GBUs."}
{"id": "2508.11609", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11609", "abs": "https://arxiv.org/abs/2508.11609", "authors": ["Kemal Altwlkany", "Elmedin Selmanovic", "Sead Delalic"], "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval", "comment": null, "summary": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes."}
{"id": "2508.11489", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11489", "abs": "https://arxiv.org/abs/2508.11489", "authors": ["Bowu Wang", "Mohamadreza Delbari", "Robin Neuder", "Alejandro Jim√©nez-S√°ez", "Vahid Jamali"], "title": "Liquid Crystal-Based RIS Loss-Trade-Off Analysis", "comment": null, "summary": "Liquid crystal (LC) technology has emerged as a promising solution for large\nreconfigurable intelligent surfaces (RISs) at millimeter wave (mmWave) bands,\noffering advantages such as low power consumption, scalability, and\ncontinuously tunable phase shifts. For LC-RIS based on the delay-line\narchitecture, i.e., with dedicated phase shifters, there exists a trade-off\nbetween the maximum achievable phase-shift range and the corresponding\ninsertion loss, which has not been studied for LC-RIS-assisted wireless systems\nyet. In this paper, we investigate this trade-off where a base station (BS) and\nan RIS are configured to minimize the transmit power while satisfying a given\nquality of service (QoS) for a number of users. Simulation results reveal a\nfundamental trade-off between the total transmit power and the achievable data\nrate as a function of the LC phase-shift range."}

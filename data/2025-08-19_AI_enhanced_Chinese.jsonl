{"id": "2508.11632", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11632", "abs": "https://arxiv.org/abs/2508.11632", "authors": ["Ian Jacob Cabansag", "Paul Ntegeka"], "title": "Prediction of Spotify Chart Success Using Audio and Streaming Features", "comment": null, "summary": "Spotify's streaming charts offer a real-time lens into music popularity,\ndriving discovery, playlists, and even revenue potential. Understanding what\ninfluences a song's rise in ranks on these charts-especially early on-can guide\nmarketing efforts, investment decisions, and even artistic direction. In this\nproject, we developed a classification pipeline to predict a song's chart\nsuccess based on its musical characteristics and early engagement data. Using\nall 2024 U.S. Top 200 Spotify Daily Charts and the Spotify Web API, we built a\ndataset containing both metadata and audio features for 14,639 unique songs.\n  The project was structured in two phases. First, we benchmarked four models:\nLogistic Regression, K Nearest Neighbors, Random Forest, and XGBoost-using a\nstandard train-test split. In the second phase, we incorporated\ncross-validation, hyperparameter tuning, and detailed class-level evaluation to\nensure robustness. Tree-based models consistently outperformed the rest, with\nRandom Forest and XGBoost achieving macro F1-scores near 0.95 and accuracy\naround 97%.\n  Even when stream count and rank history were excluded, models trained solely\non audio attributes retained predictive power. These findings validate the\npotential of audio-based modeling in A&R scouting, playlist optimization, and\nhit forecasting-long before a track reaches critical mass.", "AI": {"tldr": "\u57fa\u4e8eSpotify\u6d41\u884c\u699c\u548c\u97f3\u4e50\u7279\u5f81\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u9884\u6d4b\u6b4c\u66f2\u5728\u699c\u5355\u4e0a\u7684\u6210\u529f\u6982\u7387\uff0c\u6811\u57fa\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u7cbe\u5ea6\u8fbe97%", "motivation": "\u7406\u89e3\u6b4c\u66f2\u5728Spotify\u6d41\u884c\u699c\u4e0a\u5347\u6392\u7684\u56e0\u7d20\uff0c\u4ee5\u6307\u5bfc\u8425\u9500\u7b56\u7565\u3001\u6295\u8d44\u51b3\u7b56\u548c\u827a\u672f\u521b\u4f5c", "method": "\u4f7f\u75282024\u5e74\u7f8e\u56fdTop 200 Spotify\u65e5\u699c\u548cSpotify Web API\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5bf914,639\u9996\u6b4c\u66f2\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u903b\u8f91\u56de\u5f52\u3001K\u8fd1\u90bb\u3001\u968f\u673a\u68ee\u6797\u548cXGBoost\u56db\u79cd\u6a21\u578b", "result": "\u6811\u57fa\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u968f\u673a\u68ee\u6797\u548cXGBoost\u7684\u5b8fF1\u5206\u6570\u63a5\u8fd10.95\uff0c\u51c6\u786e\u5ea6\u7ea697%\uff0c\u5373\u4f7f\u4e0d\u5305\u542b\u6d41\u91cf\u6570\u636e\uff0c\u4ec5\u9760\u97f3\u9891\u7279\u5f81\u4e5f\u4fdd\u6301\u9884\u6d4b\u80fd\u529b", "conclusion": "\u97f3\u9891\u7279\u5f81\u6a21\u578b\u5728A&R\u6311\u9009\u3001\u6b4c\u5355\u4f18\u5316\u548c\u7206\u6b4c\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u53ef\u5728\u6b4c\u66f2\u8fbe\u5230\u5173\u952e\u6d41\u91cf\u524d\u63d0\u524d\u9884\u6d4b"}}
{"id": "2508.11818", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11818", "abs": "https://arxiv.org/abs/2508.11818", "authors": ["Zhifeng Kong", "Arushi Goel", "Joao Felipe Santos", "Sreyan Ghosh", "Rafael Valle", "Wei Ping", "Bryan Catanzaro"], "title": "Audio Flamingo Sound-CoT Technical Report: Improving Chain-of-Thought Reasoning in Sound Understanding", "comment": null, "summary": "Chain-of-thought reasoning has demonstrated significant improvements in large\nlanguage models and vision language models, yet its potential for audio\nlanguage models remains largely unexplored. In this technical report, we take a\npreliminary step towards closing this gap. For better assessment of sound\nreasoning, we propose AF-Reasoning-Eval, a benchmark targeting common-sense\nreasoning and the ability to discriminate among closely related choices. To\nprepare training corpus for sound reasoning abilities, we propose automatic\npipelines that transform existing audio question answering and classification\ndata into explicit reasoning chains, yielding AF-CoT-Train with 1.24M samples.\nWe study the effect of finetuning Audio Flamingo series on AF-CoT-Train and\nobserve considerable improvements on several reasoning benchmarks, validating\nthe effectiveness of chain-of-thought finetuning on advanced sound\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86AF-Reasoning-Eval\u8bc4\u4f30\u57fa\u51c6\u548cAF-CoT-Train\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5728Audio Flamingo\u7cfb\u5217\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u4f46\u5728\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86AF-Reasoning-Eval\u8bc4\u4f30\u57fa\u51c6\u7528\u4e8e\u58f0\u97f3\u63a8\u7406\u8bc4\u4f30\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u6d41\u6c34\u7ebf\u5c06\u73b0\u6709\u97f3\u9891\u95ee\u7b54\u548c\u5206\u7c7b\u6570\u636e\u8f6c\u6362\u4e3a\u663e\u5f0f\u63a8\u7406\u94fe\uff0c\u6784\u5efa\u4e86\u5305\u542b124\u4e07\u4e2a\u6837\u672c\u7684AF-CoT-Train\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5728Audio Flamingo\u7cfb\u5217\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89c2\u5bdf\u5230\u663e\u8457\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u601d\u7ef4\u94fe\u5fae\u8c03\u5728\u9ad8\u7ea7\u58f0\u97f3\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u601d\u7ef4\u94fe\u5fae\u8c03\u5bf9\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4e3a\u97f3\u9891\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2508.11845", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11845", "abs": "https://arxiv.org/abs/2508.11845", "authors": ["Marius Miron", "David Robinson", "Milad Alizadeh", "Ellen Gilsenan-McMahon", "Gagan Narula", "Olivier Pietquin", "Matthieu Geist", "Emmanuel Chemla", "Maddie Cusimano", "Felix Effenberger", "Masato Hagiwara", "Benjamin Hoffman", "Sara Keen", "Diane Kim", "Jane Lawton", "Jen-Yu Liu", "Aza Raskin"], "title": "What Matters for Bioacoustic Encoding", "comment": null, "summary": "Bioacoustics, the study of sounds produced by living organisms, plays a vital\nrole in conservation, biodiversity monitoring, and behavioral studies. Many\ntasks in this field, such as species, individual, and behavior classification\nand detection, are well-suited to machine learning. However, they often suffer\nfrom limited annotated data, highlighting the need for a general-purpose\nbioacoustic encoder capable of extracting useful representations for diverse\ndownstream tasks. Such encoders have been proposed before, but are often\nlimited in scope due to a focus on a narrow range of species (typically birds),\nand a reliance on a single model architecture or training paradigm. Moreover,\nthey are usually evaluated on a small set of tasks and datasets. In this work,\nwe present a large-scale empirical study that covers aspects of bioacoustics\nthat are relevant to research but have previously been scarcely considered:\ntraining data diversity and scale, model architectures and training recipes,\nand the breadth of evaluation tasks and datasets. We obtain encoders that are\nstate-of-the-art on the existing and proposed benchmarks. We also identify what\nmatters for training these encoders, such that this work can be extended when\nmore data are available or better architectures are proposed. Specifically,\nacross 26 datasets with tasks including species classification, detection,\nindividual ID, and vocal repertoire discovery, we find self-supervised\npre-training followed by supervised post-training on a mixed bioacoustics +\ngeneral-audio corpus yields the strongest in- and out-of-distribution\nperformance. We show the importance of data diversity in both stages. To\nsupport ongoing research and application, we will release the model\ncheckpoints.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u751f\u7269\u58f0\u5b66\u7f16\u7801\u5668\u7814\u7a76\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5f0f\u540e\u8bad\u7ec3\u7ec4\u5408\uff0c\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u5c06\u53d1\u5e03\u6a21\u578b\u68c0\u67e5\u70b9\u3002", "motivation": "\u751f\u7269\u58f0\u5b66\u7814\u7a76\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u9700\u8981\u901a\u7528\u7684\u7f16\u7801\u5668\u6765\u63d0\u53d6\u6709\u7528\u8868\u5f81\u3002\u4ee5\u524d\u7684\u7f16\u7801\u5668\u5b58\u5728\u8303\u56f4\u7a84\u5c40\u9650\u6027\u3001\u6a21\u578b\u67b6\u6784\u5355\u4e00\u3001\u8bc4\u4f30\u6570\u636e\u96c6\u5c11\u7b49\u95ee\u9898\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u7814\u7a76\uff0c\u6db5\u76d6\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u548c\u89c4\u6a21\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u7b49\u591a\u4e2a\u65b9\u9762\u3002\u91c7\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7ecf\u9a8c\u540e\u76d1\u7763\u5f0f\u540e\u8bad\u7ec3\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df7\u5408\u751f\u7269\u58f0\u5b66+\u901a\u7528\u97f3\u9891\u8bed\u6599\u5e93\u3002", "result": "\u572826\u4e2a\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u72b6\u6001\u4e4b\u6700\u4f73\u6027\u80fd\uff0c\u5305\u62ec\u7269\u79cd\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u4e2a\u4f53\u8bc6\u522b\u3001\u53eb\u58f0\u8bed\u6599\u5e93\u53d1\u73b0\u7b49\u4efb\u52a1\u3002\u8bc1\u660e\u4e86\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u5728\u4e24\u4e2a\u8bad\u7ec3\u9636\u6bb5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u751f\u7269\u58f0\u5b66\u7f16\u7801\u5668\u8bad\u7ec3\u7684\u5173\u952e\u56e0\u7d20\u548c\u6700\u4f73\u5b9e\u8df5\uff0c\u4e3a\u672a\u6765\u66f4\u591a\u6570\u636e\u6216\u66f4\u597d\u67b6\u6784\u7684\u6269\u5c55\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5c06\u53d1\u5e03\u6a21\u578b\u68c0\u67e5\u70b9\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2508.11966", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.11966", "abs": "https://arxiv.org/abs/2508.11966", "authors": ["Yuhang Jia", "Hui Wang", "Xin Nie", "Yujie Guo", "Lianru Gao", "Yong Qin"], "title": "Towards Automatic Evaluation and High-Quality Pseudo-Parallel Dataset Construction for Audio Editing: A Human-in-the-Loop Method", "comment": null, "summary": "Audio editing aims to manipulate audio content based on textual descriptions,\nsupporting tasks such as adding, removing, or replacing audio events. Despite\nrecent progress, the lack of high-quality benchmark datasets and comprehensive\nevaluation metrics remains a major challenge for both assessing audio editing\nquality and improving the task itself. In this work, we propose a novel\napproach for audio editing task by incorporating expert knowledge into both the\nevaluation and dataset construction processes: 1) First, we establish\nAuditScore, the first comprehensive dataset for subjective evaluation of audio\nediting, consisting of over 6,300 edited samples generated from 7\nrepresentative audio editing frameworks and 23 system configurations. Each\nsample is annotated by professional raters on three key aspects of audio\nediting quality: overall Quality, Relevance to editing intent, and Faithfulness\nto original features. 2) Based on this dataset, we train AuditEval, the first\nmodel designed for automatic MOS-style scoring tailored to audio editing tasks.\nAuditEval addresses the critical lack of objective evaluation metrics and the\nprohibitive cost of subjective assessment in this field. 3) We further leverage\nAuditEval to evaluate and filter a large amount of synthetically mixed editing\npairs, constructing a high-quality pseudo-parallel dataset by selecting the\nmost plausible samples. Objective experiments validate the effectiveness of our\nexpert-informed filtering strategy in yielding higher-quality data, while also\nrevealing the limitations of relying solely on objective metrics. The dataset,\ncodes and tools can be found at: https://github.com/NKU-HLT/AuditEval.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u97f3\u9891\u7f16\u8f91\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e3b\u89c2\u8bc4\u6d4b\u6570\u636e\u96c6AuditScore\u548c\u81ea\u52a8\u8bc4\u4f30\u6a21\u578bAuditEval\uff0c\u4ee5\u89e3\u51b3\u97f3\u9891\u7f16\u8f91\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u7684\u6311\u6218\u3002", "motivation": "\u97f3\u9891\u7f16\u8f91\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u97f3\u9891\u7f16\u8f91\u8d28\u91cf\u7684\u8bc4\u4f30\u548c\u4efb\u52a1\u672c\u8eab\u7684\u6539\u8fdb\u3002", "method": "1\uff09\u6784\u5efa\u5305\u542b6,300\u4e2a\u6837\u672c\u7684\u4e3b\u89c2\u8bc4\u6d4b\u6570\u636e\u96c6AuditScore\uff0c\u7531\u4e13\u4e1a\u8bc4\u6d4b\u8005\u5728\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u51c6\u786e\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u6ce8\u91ca 2\uff09\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u81ea\u52a8MOS\u8bc4\u5206\u6a21\u578bAuditEval 3\uff09\u5229\u7528AuditEval\u7b5b\u9009\u9ad8\u8d28\u91cf\u7684\u4f2a\u5e76\u884c\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e13\u5bb6\u77e5\u8bc6\u5bfc\u5411\u7684\u7b5b\u9009\u7b56\u7565\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\uff0c\u540c\u65f6\u4e5f\u66dd\u9732\u4e86\u4ec5\u4f9d\u9760\u5bf9\u8c61\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u97f3\u9891\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7efc\u5408\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u4e3b\u89c2\u6570\u636e\u96c6\u3001\u81ea\u52a8\u8bc4\u4f30\u6a21\u578b\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u6784\u5efa\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u97f3\u9891\u7f16\u8f91\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.12001", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12001", "abs": "https://arxiv.org/abs/2508.12001", "authors": ["Qingliang Meng", "Luogeng Xiong", "Wei Liang", "Limei Yu", "Huizhi Liang", "Tian Li"], "title": "FNH-TTS: A Fast, Natural, and Human-Like Speech Synthesis System with advanced prosodic modeling based on Mixture of Experts", "comment": null, "summary": "Achieving natural and human-like speech synthesis with low inference costs\nremains a major challenge in speech synthesis research. This study focuses on\nhuman prosodic patterns and synthesized spectrum harmony, addressing the\nchallenges of prosody modeling and artifact issues in non-autoregressive\nmodels. To enhance prosody modeling and synthesis quality, we introduce a new\nDuration Predictor based on the Mixture of Experts alongside a new Vocoder with\ntwo advanced multi-scale discriminators. We integrated the these new modules\ninto the VITS system, forming our FNH-TTS system. Our experiments on LJSpeech,\nVCTK, and LibriTTS demonstrate the system's superiority in synthesis quality,\nphoneme duration prediction, Vocoder results, and synthesis speed. Our prosody\nvisualization results show that FNH-TTS produces duration predictions that more\nclosely align with natural human beings than other systems.", "AI": {"tldr": "FNH-TTS\u7cfb\u7edf\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u65f6\u957f\u9884\u6d4b\u5668\u548c\u53cc\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u58f0\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5408\u6210\u7684\u81ea\u7136\u5ea6\u3001\u65f6\u957f\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5408\u6210\u901f\u5ea6", "motivation": "\u89e3\u51b3\u8bed\u97f3\u5408\u6210\u4e2d\u81ea\u7136\u5ea6\u548c\u63a8\u7406\u6210\u672c\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u97f5\u5f8b\u5efa\u6a21\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4f2a\u5f71\u95ee\u9898", "method": "\u57fa\u4e8eVITS\u7cfb\u7edf\uff0c\u5f15\u5165\u6df7\u5408\u4e13\u5bb6\u65f6\u957f\u9884\u6d4b\u5668\u548c\u5177\u6709\u53cc\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u7684\u65b0\u578b\u58f0\u7801\u5668", "result": "\u5728LJSpeech\u3001VCTK\u548cLibriTTS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5408\u6210\u8d28\u91cf\u3001\u97f3\u7d20\u65f6\u957f\u9884\u6d4b\u51c6\u786e\u6027\u3001\u58f0\u7801\u5668\u6548\u679c\u548c\u5408\u6210\u901f\u5ea6", "conclusion": "FNH-TTS\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u66f4\u63a5\u8fd1\u4eba\u7c7b\u81ea\u7136\u8bed\u97f3\u7684\u97f5\u5f8b\u6a21\u5f0f\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf"}}
{"id": "2508.11640", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11640", "abs": "https://arxiv.org/abs/2508.11640", "authors": ["Danny Scott", "William LaForest", "Hritom Das", "Ioannis Polykretis", "Catherine D. Schuman", "Charles Rizzo", "James Plank", "Sai Swaminathan"], "title": "Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks", "comment": "International Conference on Neuromorphic Systems (ICONS) 2025 9\n  pages, 7 images", "summary": "The deployment of dense, low-cost sensors is critical for realizing\nubiquitous smart environments. However, existing sensing solutions struggle\nwith the energy, scalability, and reliability trade-offs imposed by battery\nmaintenance, wireless transmission overhead, and data processing complexity. In\nthis work, we present Vibe2Spike, a novel battery-free, wireless sensing\nframework that enables vibration-based activity recognition using visible light\ncommunication (VLC) and spiking neural networks (SNNs). Our system uses\nultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and\nan LED, which harvest vibration energy and emit sparse visible light spikes\nwithout requiring batteries or RF radios. These optical spikes are captured by\nevent cameras and classified using optimized SNN models evolved via the EONS\nframework. We evaluate Vibe2Spike across five device classes, achieving 94.9\\%\naverage classification fitness while analyzing the latency-accuracy trade-offs\nof different temporal binning strategies. Vibe2Spike demonstrates a scalable,\nand energy-efficient approach for enabling intelligent environments in a\nbatteryless manner.", "AI": {"tldr": "Vibe2Spike\u662f\u4e00\u4e2a\u65e0\u7535\u6c60\u65e0\u7ebf\u4f20\u611f\u6846\u67b6\uff0c\u5229\u7528\u53ef\u89c1\u5149\u901a\u4fe1\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u632f\u52a8\u6d3b\u52a8\u8bc6\u522b\uff0c\u901a\u8fc7\u538b\u7535\u76d8\u6536\u96c6\u632f\u52a8\u80fd\u91cf\u5e76\u53d1\u5c04\u5149\u8109\u51b2\uff0c\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u6355\u83b7\u5e76\u7531\u4f18\u5316SNN\u5206\u7c7b\uff0c\u8fbe\u523094.9%\u7684\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u5728\u7535\u6c60\u7ef4\u62a4\u3001\u65e0\u7ebf\u4f20\u8f93\u5f00\u9500\u548c\u6570\u636e\u5904\u7406\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u80fd\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8282\u80fd\u3001\u53ef\u6269\u5c55\u7684\u4f20\u611f\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4ec5\u7531\u538b\u7535\u76d8\u3001\u9f50\u7eb3\u4e8c\u6781\u7ba1\u548cLED\u7ec4\u6210\u7684\u8d85\u4f4e\u6210\u672c\u6807\u7b7e\uff0c\u91c7\u96c6\u632f\u52a8\u80fd\u91cf\u5e76\u53d1\u5c04\u7a00\u758f\u53ef\u89c1\u5149\u8109\u51b2\uff1b\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u6355\u83b7\u5149\u8109\u51b2\uff0c\u4f7f\u7528EONS\u6846\u67b6\u4f18\u5316\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u4e94\u7c7b\u8bbe\u5907\u4e0a\u8bc4\u4f30\uff0c\u8fbe\u523094.9%\u7684\u5e73\u5747\u5206\u7c7b\u9002\u5e94\u5ea6\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65f6\u95f4\u5206\u7bb1\u7b56\u7565\u7684\u5ef6\u8fdf-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "Vibe2Spike\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u65e0\u7535\u6c60\u65b9\u5f0f\u5b9e\u73b0\u667a\u80fd\u73af\u5883\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.12009", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12009", "abs": "https://arxiv.org/abs/2508.12009", "authors": ["Arnav Ramamoorthy"], "title": "Optimizing Neural Architectures for Hindi Speech Separation and Enhancement in Noisy Environments", "comment": "ICAD 2025", "summary": "This paper addresses the challenges of Hindi speech separation and\nenhancement using advanced neural network architectures, with a focus on edge\ndevices. We propose a refined approach leveraging the DEMUCS model to overcome\nlimitations of traditional methods, achieving substantial improvements in\nspeech clarity and intelligibility. The model is fine-tuned with U-Net and LSTM\nlayers, trained on a dataset of 400,000 Hindi speech clips augmented with\nESC-50 and MS-SNSD for diverse acoustic environments. Evaluation using PESQ and\nSTOI metrics shows superior performance, particularly under extreme noise\nconditions. To ensure deployment on resource-constrained devices like TWS\nearbuds, we explore quantization techniques to reduce computational\nrequirements. This research highlights the effectiveness of customized AI\nalgorithms for speech processing in Indian contexts and suggests future\ndirections for optimizing edge-based architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDEMUCS\u6a21\u578b\u7684\u4f18\u5316\u65b9\u6848\uff0c\u7528\u4e8e\u5370\u5ea6\u8bed\u8a00\u8bed\u97f3\u5206\u79bb\u548c\u589e\u5f3a\uff0c\u5728\u6781\u7aef\u566a\u97f3\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u53ef\u61c2\u6027\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u6280\u672f\u9002\u914d\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6311\u6218\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e3a\u5370\u5ea6\u8bed\u5883\u63d0\u4f9b\u9ad8\u6548\u7684\u8bed\u97f3\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528DEMUCS\u6a21\u578b\u7ec3\u4e60\u572840\u4e07\u4e2a\u5370\u5ea6\u8bed\u8bed\u97f3\u526a\u8f91\u7684\u6570\u636e\u96c6\uff0c\u7d27\u5bc6\u7ed3\u5408U-Net\u548cLSTM\u5c42\uff0c\u5e76\u4f7f\u7528ESC-50\u548cMS-SNSD\u8fdb\u884c\u591a\u6837\u5316\u97f3\u54cd\u73af\u5883\u589e\u5f3a\u3002\u901a\u8fc7\u91cf\u5316\u6280\u672f\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4f7f\u7528PESQ\u548cSTOI\u6307\u6807\u8bc4\u4f30\uff0c\u663e\u793a\u5728\u6781\u7aef\u566a\u97f3\u6761\u4ef6\u4e0b\u8fbe\u5230\u4e86\u8d85\u8d8a\u6027\u80fd\uff0c\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u53ef\u61c2\u6027\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u91cf\u5316\u6280\u672f\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5b9a\u5236\u5316AI\u7b97\u6cd5\u5728\u5370\u5ea6\u8bed\u5883\u4e0b\u8bed\u97f3\u5904\u7406\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u6307\u660e\u4e86\u8fb9\u7f18\u67b6\u6784\u4f18\u5316\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.12024", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12024", "abs": "https://arxiv.org/abs/2508.12024", "authors": ["Georg K. J. Fischer", "Thomas Schaechtle", "Moritz Schabinger", "Alexander Richter", "Ivo H\u00e4ring", "Fabian H\u00f6flinger", "Stefan J. Rupitsch"], "title": "MASSLOC: A Massive Sound Source Localization System based on Direction-of-Arrival Estimation", "comment": "IEEE Transactions on Instrumentation and Measurement", "summary": "Acoustic indoor localization offers the potential for highly accurate\nposition estimation while generally exhibiting low hardware requirements\ncompared to Radio Frequency (RF)-based solutions. Furthermore, angular-based\nlocalization significantly reduces installation effort by minimizing the number\nof required fixed anchor nodes. In this contribution, we propose the so-called\nMASSLOC system, which leverages sparse two-dimensional array geometries to\nlocalize and identify a large number of concurrently active sources.\nAdditionally, the use of complementary Zadoff-Chu sequences is introduced to\nenable efficient, beamforming-based source identification. These sequences\nprovide a trade-off between favorable correlation properties and accurate,\nunsynchronized direction-of-arrival estimation by exhibiting a spectrally\nbalanced waveform. The system is evaluated in both a controlled anechoic\nchamber and a highly reverberant lobby environment with a reverberation time of\n1.6 s. In a laboratory setting, successful direction-of-arrival estimation and\nidentification of up to 14 simultaneously emitting sources are demonstrated.\nAdopting a Perspective-n-Point (PnP) calibration approach, the system achieves\na median three-dimensional localization error of 55.7 mm and a median angular\nerror of 0.84 deg with dynamic source movement of up to 1.9 mps in the\nchallenging reverberant environment. The multi-source capability is also\ndemonstrated and evaluated in that environment with a total of three tags.\nThese results indicate the scalability and robustness of the MASSLOC system,\neven under challenging acoustic conditions.", "AI": {"tldr": "MASSLOC\u7cfb\u7edf\u5229\u7528\u7a00\u758f\u4e8c\u7ef4\u9635\u5217\u51e0\u4f55\u7ed3\u6784\u548cZadoff-Chu\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e86\u591a\u58f0\u6e90\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u8bc6\u522b\uff0c\u5728\u6df7\u54cd\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272", "motivation": "\u58f0\u5b66\u5ba4\u5185\u5b9a\u4f4d\u76f8\u6bd4RF\u65b9\u6848\u786c\u4ef6\u8981\u6c42\u4f4e\uff0c\u57fa\u4e8e\u89d2\u5ea6\u7684\u5b9a\u4f4d\u53ef\u51cf\u5c11\u951a\u8282\u70b9\u6570\u91cf\uff0c\u9700\u8981\u89e3\u51b3\u591a\u58f0\u6e90\u5e76\u53d1\u5b9a\u4f4d\u7684\u6311\u6218", "method": "\u4f7f\u7528\u7a00\u758f\u4e8c\u7ef4\u9635\u5217\u51e0\u4f55\u7ed3\u6784\uff0c\u91c7\u7528\u4e92\u8865Zadoff-Chu\u5e8f\u5217\u8fdb\u884c\u6ce2\u675f\u6210\u5f62\u6e90\u8bc6\u522b\uff0c\u7ed3\u5408Perspective-n-Point\u6821\u51c6\u65b9\u6cd5", "result": "\u5728\u6df7\u54cd\u65f6\u95f41.6s\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\uff0c\u5b9e\u73b055.7mm\u4e2d\u4f4d\u5b9a\u4f4d\u8bef\u5dee\u548c0.84\u5ea6\u89d2\u5ea6\u8bef\u5dee\uff0c\u6700\u591a\u540c\u65f6\u8bc6\u522b14\u4e2a\u58f0\u6e90", "conclusion": "MASSLOC\u7cfb\u7edf\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u6076\u52a3\u58f0\u5b66\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\u591a\u6e90\u5b9a\u4f4d"}}
{"id": "2508.11654", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11654", "abs": "https://arxiv.org/abs/2508.11654", "authors": ["Yang Zhao", "Tao Wang", "Said Elhadi"], "title": "Data-driven RF Tomography via Cross-modal Sensing and Continual Learning", "comment": "6 pages, 4 figures, to be published in IEEE AVSS Conference", "summary": "Data-driven radio frequency (RF) tomography has demonstrated significant\npotential for underground target detection, due to the penetrative nature of RF\nsignals through soil. However, it is still challenging to achieve accurate and\nrobust performance in dynamic environments. In this work, we propose a\ndata-driven radio frequency tomography (DRIFT) framework with the following key\ncomponents to reconstruct cross section images of underground root tubers, even\nwith significant changes in RF signals. First, we design a cross-modal sensing\nsystem with RF and visual sensors, and propose to train an RF tomography deep\nneural network (DNN) model following the cross-modal learning approach. Then we\npropose to apply continual learning to automatically update the DNN model, once\nenvironment changes are detected in a dynamic environment. Experimental results\nshow that our approach achieves an average equivalent diameter error of 2.29\ncm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and\ndataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u9a71\u52a8\u65e0\u7ebf\u7535\u9891\u6210\u50cf\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u6280\u672f\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5730\u4e0b\u690d\u6839\u5757\u83b7\u53d6\u56fe\u50cf\u91cd\u5efa", "motivation": "\u867d\u7136\u6570\u636e\u9a71\u52a8\u7684\u65e0\u7ebf\u7535\u9891\u6210\u50cf\u5728\u5730\u4e0b\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u5c55\u73b0\u4e86\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u51c6\u786e\u548c\u7a33\u5065\u7684\u6027\u80fd\u4ecd\u7136\u9762\u4e34\u6311\u6218", "method": "\u8bbe\u8ba1\u4e86\u65e0\u7ebf\u7535\u9891\u548c\u89c6\u89c9\u4f20\u611f\u5668\u7684\u8de8\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3RF\u6210\u50cf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u5728\u73af\u5883\u53d8\u5316\u65f6\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u81ea\u52a8\u66f4\u65b0\u6a21\u578b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u76f8\u5f53\u76f4\u5f84\u8bef\u5dee\u4e3a2.29cm\uff0c\u8f83\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534723.2%\u7684\u6027\u80fd\u6536\u76ca", "conclusion": "\u63d0\u51fa\u7684DRIFT\u6846\u67b6\u80fd\u591f\u5728\u65e0\u7ebf\u7535\u9891\u4fe1\u53f7\u53d1\u751f\u663e\u8457\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u7136\u5b9e\u73b0\u4e86\u5730\u4e0b\u690d\u6839\u5757\u6839\u622a\u9762\u56fe\u50cf\u7684\u51c6\u786e\u91cd\u5efa\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5730\u4e0b\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12230", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12230", "abs": "https://arxiv.org/abs/2508.12230", "authors": ["Bing Han", "Anbai Jiang", "Xinhu Zheng", "Wei-Qiang Zhang", "Jia Liu", "Pingyi Fan", "Yanmin Qian"], "title": "Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection", "comment": "Accepted by TASLP. 15 pages, 7 figures;", "summary": "Machine anomalous sound detection (ASD) is a valuable technique across\nvarious applications. However, its generalization performance is often limited\ndue to challenges in data collection and the complexity of acoustic\nenvironments. Inspired by the success of large pre-trained models in numerous\nfields, this paper introduces a robust ASD model that leverages self-supervised\npre-trained models trained on large-scale speech and audio datasets. Although\nthere are inconsistencies between the pre-training datasets and the ASD task,\nour findings indicate that pre-training still provides substantial benefits for\nASD. To mitigate overfitting and retain learned knowledge when fine-tuning with\nlimited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as an\nalternative to full fine-tuning. Additionally, we propose a Machine-aware Group\nAdapter module, which enables the model to capture differences between various\nmachines within a unified framework, thereby enhancing the generalization\nperformance of ASD systems. To address the challenge of missing attribute\nlabels, we design a novel objective function that dynamically clusters\nunattributed data using vector quantization and optimizes through a dual-level\ncontrastive learning loss. The proposed methods are evaluated on all benchmark\ndatasets, including the DCASE 2020-2024 five ASD challenges, and the\nexperimental results show significant improvements of our new approach and\ndemonstrate the effectiveness of our proposed strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u673a\u5668\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7LoRA\u7b80\u5316\u5fae\u8c03\u3001\u673a\u5668\u611f\u77e5\u7ec4\u9002\u914d\u5668\u548c\u53cc\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6350\u5931\u51fd\u6570\uff0c\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u673a\u5668\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u7684\u901a\u7528\u6027\u80fd\u53d7\u9650\u4e8e\u6570\u636e\u6536\u96c6\u56f0\u96be\u548c\u58f0\u5b66\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u8bed\u97f3\u548c\u97f3\u9891\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u91c7\u7528Fully-Connected LoRA\u6765\u907f\u514d\u8fc7\u62df\u5408\uff0c\u63d0\u51fa\u673a\u5668\u611f\u77e5\u7ec4\u9002\u914d\u5668\u6a21\u5757\uff0c\u4ee5\u53ca\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u548c\u53cc\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6350\u5931\u7684\u65b0\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728DCASE 2020-2024\u4e94\u4e2aASD\u6311\u6218\u8d5b\u7684\u6240\u6709\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b0\u65b9\u6cd5\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u9884\u8bad\u7ec3\u5bf9\u4e8eASD\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u5f02\u5e38\u58f0\u97f3\u68c0\u6d4b\u7cfb\u7edf\u7684\u901a\u7528\u6027\u80fd\u3002"}}
{"id": "2508.12666", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12666", "abs": "https://arxiv.org/abs/2508.12666", "authors": ["Anton Mitrofanov", "Sergei Novoselov", "Tatiana Prisyach", "Vladislav Marchevskiy", "Arseniy Karelin", "Nikita Khmelev", "Dmitry Dutov", "Stepan Malykh", "Igor Agafonov", "Aleksandr Nikitin", "Oleg Petrov"], "title": "Cryfish: On deep audio analysis with Large Language Models", "comment": null, "summary": "The recent revolutionary progress in text-based large language models (LLMs)\nhas contributed to the growth of interest in extending capabilities of such\nmodels to multimodal perception and understanding tasks. Hearing is an\nessential capability that is highly desired to be integrated into LLMs.\nHowever, effective integrating listening capabilities into LLMs is a\nsignificant challenge lying in generalizing complex auditory tasks across\nspeech and sounds. To address these issues, we introduce Cryfish, our version\nof auditory-capable LLM. The model integrates WavLM audio-encoder features into\nQwen2 model using a transformer-based connector. Cryfish is adapted to various\nauditory tasks through a specialized training strategy. We evaluate the model\non the new Dynamic SUPERB Phase-2 comprehensive multitask benchmark\nspecifically designed for auditory-capable models. The paper presents an\nin-depth analysis and detailed comparison of Cryfish with the publicly\navailable models.", "AI": {"tldr": "Cryfish\u662f\u4e00\u4e2a\u96c6\u6210\u542c\u89c9\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7WavLM\u97f3\u9891\u7f16\u7801\u5668\u548ctransformer\u8fde\u63a5\u5668\u5c06\u542c\u89c9\u529f\u80fd\u6574\u5408\u5230Qwen2\u6a21\u578b\u4e2d\uff0c\u5728Dynamic SUPERB Phase-2\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u6269\u5c55\u5176\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u542c\u89c9\u80fd\u529b\u7684\u96c6\u6210\uff0c\u4ee5\u5904\u7406\u590d\u6742\u7684\u8bed\u97f3\u548c\u58f0\u97f3\u4efb\u52a1\u3002", "method": "\u4f7f\u7528WavLM\u97f3\u9891\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u901a\u8fc7transformer\u8fde\u63a5\u5668\u5c06\u97f3\u9891\u7279\u5f81\u6574\u5408\u5230Qwen2\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u8bad\u7ec3\u7b56\u7565\u9002\u5e94\u591a\u79cd\u542c\u89c9\u4efb\u52a1\u3002", "result": "\u5728\u4e13\u95e8\u4e3a\u542c\u89c9\u6a21\u578b\u8bbe\u8ba1\u7684Dynamic SUPERB Phase-2\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u4e0e\u73b0\u6709\u516c\u5f00\u6a21\u578b\u8fdb\u884c\u4e86\u8be6\u7ec6\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "Cryfish\u6210\u529f\u5b9e\u73b0\u4e86\u542c\u89c9\u80fd\u529b\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11656", "categories": ["eess.SP", "cs.LG", "I.2.6; I.5.1; I.5.4; I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2508.11656", "abs": "https://arxiv.org/abs/2508.11656", "authors": ["Ridma Jayasundara", "Ishan Fernando", "Adeepa Fernando", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "Inductive transfer learning from regression to classification in ECG analysis", "comment": "This manuscript is 15 pages with 4 tables and 5 figures. The\n  manuscript is under review at Nature Scientific Reports", "summary": "Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,\naccounting for over 30% of global deaths according to the World Health\nOrganization (WHO). Importantly, one-third of these deaths are preventable with\ntimely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive\nmethod for recording the electrical activity of the heart, is crucial for\ndiagnosing CVDs. However, privacy concerns surrounding the use of patient ECG\ndata in research have spurred interest in synthetic data, which preserves the\nstatistical properties of real data without compromising patient\nconfidentiality. This study explores the potential of synthetic ECG data for\ntraining deep learning models from regression to classification tasks and\nevaluates the feasibility of transfer learning to enhance classification\nperformance on real ECG data. We experimented with popular deep learning models\nto predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,\nQT interval, and QRS complex-using separate regression models. Subsequently, we\nleveraged these regression models for transfer learning to perform 5-class ECG\nsignal classification. Our experiments systematically investigate whether\ntransfer learning from regression to classification is viable, enabling better\nutilization of diverse open-access and synthetic ECG datasets. Our findings\ndemonstrate that transfer learning from regression to classification improves\nclassification performance, highlighting its potential to maximize the utility\nof available data and advance deep learning applications in this domain.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4ece\u56de\u5f52\u5230\u5206\u7c7b\u7684\u8f6c\u79fb\u5b66\u4e60\u5728\u5fc3\u7535\u56fe\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u5229\u7528\u5408\u6210\u5fc3\u7535\u56fe\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u9996\u8981\u6b7b\u56e0\uff0c\u65f6\u65e9\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002\u5fc3\u7535\u56fe\u662f\u5173\u952e\u8bca\u65ad\u5de5\u5177\uff0c\u4f46\u75c5\u4eba\u6570\u636e\u9690\u79c1\u95ee\u9898\u4fc3\u4f7f\u7814\u7a76\u8005\u5bfb\u627e\u5408\u6210\u6570\u636e\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6700\u5927\u5316\u5229\u7528\u5408\u6210\u6570\u636e\u6765\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6d41\u884c\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u56db\u4e2a\u5173\u952e\u5fc3\u810f\u53c2\u6570\u7684\u56de\u5f52\u9884\u6d4b\uff08\u5fc3\u7387\u3001PR\u95f4\u671f\u3001QT\u95f4\u671f\u3001QRS\u590d\u5408\u6ce2\uff09\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u8f6c\u79fb\u5b66\u4e60\uff0c\u5b8c\u62105\u7c7b\u5fc3\u7535\u56fe\u4fe1\u53f7\u5206\u7c7b\u4efb\u52a1\u3002\u5b9e\u9a8c\u7cfb\u7edf\u6027\u5730\u9a8c\u8bc1\u4e86\u4ece\u56de\u5f52\u5230\u5206\u7c7b\u7684\u8f6c\u79fb\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4ece\u56de\u5f52\u5230\u5206\u7c7b\u7684\u8f6c\u79fb\u5b66\u4e60\u80fd\u591f\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u5229\u7528\u5404\u79cd\u5f00\u653e\u8bbf\u95ee\u548c\u5408\u6210\u5fc3\u7535\u56fe\u6570\u636e\u96c6\u3002", "conclusion": "\u8f6c\u79fb\u5b66\u4e60\u6280\u672f\u5728\u5fc3\u7535\u56fe\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u80fd\u591f\u6700\u5927\u5316\u6570\u636e\u5229\u7528\u6548\u7387\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.12292", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12292", "abs": "https://arxiv.org/abs/2508.12292", "authors": ["Hyebin Ahn", "Kangwook Jang", "Hoirin Kim"], "title": "HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization", "comment": "Accepted at Interspeech 2025", "summary": "Noise robustness in speech foundation models (SFMs) has been a critical\nchallenge, as most models are primarily trained on clean data and experience\nperformance degradation when the models are exposed to noisy speech. To address\nthis issue, we propose HuBERT-VIC, a noise-robust SFM with variance,\nin-variance, and covariance regularization (VICReg) objectives. These\nobjectives adjust the statistics of noisy speech representations, enabling the\nmodel to capture diverse acoustic characteristics and improving the\ngeneralization ability across different types of noise. When applied to HuBERT,\nour model shows relative performance improvements of 23.3% on LibriSpeech\ntest-clean and 13.2% on test-other, compared to the baseline model pre-trained\non noisy speech.", "AI": {"tldr": "HuBERT-VIC\u901a\u8fc7VICReg\u76ee\u6807\u51fd\u6570\u8c03\u6574\u566a\u58f0\u8bed\u97f3\u8868\u793a\u7edf\u8ba1\u7279\u6027\uff0c\u63d0\u5347\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5728LibriSpeech\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u5927\u591a\u6570\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u566a\u58f0\u9c81\u68d2\u6027\u95ee\u9898", "method": "\u63d0\u51faHuBERT-VIC\u6a21\u578b\uff0c\u91c7\u7528\u65b9\u5dee\u3001\u4e0d\u53d8\u6027\u548c\u534f\u65b9\u5dee\u6b63\u5219\u5316(VICReg)\u76ee\u6807\u51fd\u6570\u6765\u8c03\u6574\u566a\u58f0\u8bed\u97f3\u8868\u793a\u7edf\u8ba1\u7279\u6027\uff0c\u6355\u6349\u591a\u6837\u5316\u58f0\u5b66\u7279\u5f81", "result": "\u76f8\u6bd4\u5728\u566a\u58f0\u8bed\u97f3\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728LibriSpeech test-clean\u4e0a\u76f8\u5bf9\u6027\u80fd\u63d0\u534723.3%\uff0c\u5728test-other\u4e0a\u63d0\u534713.2%", "conclusion": "VICReg\u76ee\u6807\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u7684\u566a\u58f0\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u7c7b\u578b\u566a\u58f0\u4e0b\u90fd\u8868\u73b0\u826f\u597d"}}
{"id": "2508.12968", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12968", "abs": "https://arxiv.org/abs/2508.12968", "authors": ["Branislav Gerazov", "Marcello Politi", "S\u00e9bastien Brati\u00e8res"], "title": "Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with Transformer-Based Models", "comment": null, "summary": "We explore the performance of several state-of-the-art automatic speech\nrecognition (ASR) models on a large-scale Arabic speech dataset, the SADA\n(Saudi Audio Dataset for Arabic), which contains 668 hours of high-quality\naudio from Saudi television shows. The dataset includes multiple dialects and\nenvironments, specifically a noisy subset that makes it particularly\nchallenging for ASR. We evaluate the performance of the models on the SADA test\nset, and we explore the impact of fine-tuning, language models, as well as\nnoise and denoising on their performance. We find that the best performing\nmodel is the MMS 1B model finetuned on SADA with a 4-gram language model that\nachieves a WER of 40.9\\% and a CER of 17.6\\% on the SADA test clean set.", "AI": {"tldr": "\u7814\u7a76\u591a\u4e2a\u6700\u65b0\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u6570\u636e\u96c6SADA\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5305\u62ec\u5bf9\u7cbe\u8c03\u3001\u8bed\u8a00\u6a21\u578b\u3001\u566a\u58f0\u548c\u53bb\u566a\u6548\u679c\u7684\u5206\u6790", "motivation": "\u8bc4\u4f30\u73b0\u6709ASR\u6a21\u578b\u5728\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u591a\u79cd\u65b9\u8a00\u548c\u566a\u58f0\u73af\u5883\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d", "method": "\u4f7f\u7528SADA\u6570\u636e\u96c6\uff08668\u5c0f\u65f6\u6c99\u7279\u7535\u89c6\u8282\u76ee\u97f3\u9891\uff09\u8bc4\u6d4b\u591a\u4e2aASR\u6a21\u578b\uff0c\u6d89\u53ca\u7cbe\u8c03\u3001\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u3001\u566a\u58f0\u5904\u7406\u7b49\u5b9e\u9a8c", "result": "MMS 1B\u6a21\u578b\u7ecf\u8fc7SADA\u7cbe\u8c03\u5e76\u914d\u54084-gram\u8bed\u8a00\u6a21\u578b\u540e\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6e05\u6d01\u96c6\u4e0a\u8fbe\u5230WER 40.9%\u548cCER 17.6%", "conclusion": "\u7cbe\u8c03\u548c\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u53ef\u663e\u8457\u63d0\u5347ASR\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd"}}
{"id": "2508.11657", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11657", "abs": "https://arxiv.org/abs/2508.11657", "authors": ["Yuanhao Li", "Badong Chen", "Wenjun Bai", "Yasuharu Koike", "Okito Yamashita"], "title": "Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding", "comment": null, "summary": "Objective: Sparse Bayesian learning provides an effective scheme to solve the\nhigh-dimensional problem in brain signal decoding. However, traditional\nassumptions regarding data distributions such as Gaussian and binomial are\npotentially inadequate to characterize the noisy signals of brain activity.\nHence, this study aims to propose a robust sparse Bayesian learning framework\nto address noisy highdimensional brain activity decoding. Methods: Motivated by\nthe commendable robustness of the minimum error entropy (MEE) criterion for\nhandling complex data distributions, we proposed an MEE-based likelihood\nfunction to facilitate the accurate inference of sparse Bayesian learning in\nanalyzing noisy brain datasets. Results: Our proposed approach was evaluated\nusing two high-dimensional brain decoding tasks in regression and\nclassification contexts, respectively. The experimental results showed that,\nour approach can realize superior decoding metrics and physiological patterns\nthan the conventional and state-of-the-art methods. Conclusion: Utilizing the\nproposed MEE-based likelihood model, sparse Bayesian learning is empowered to\nsimultaneously address the challenges of noise and high dimensionality in the\nbrain decoding task. Significance: This work provides a powerful tool to\nrealize robust brain decoding, advancing biomedical engineering applications\nsuch as brain-computer interface.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5c0f\u8bef\u5dee\u71b5(MEE)\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u7ef4\u8111\u4fe1\u53f7\u89e3\u7801\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9ad8\u65af\u548c\u4e8c\u9879\u5206\u5e03\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u5047\u8bbe\u5728\u5904\u7406\u8111\u4fe1\u53f7\u566a\u58f0\u65f6\u53ef\u80fd\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5904\u7406\u590d\u6742\u6570\u636e\u5206\u5e03", "method": "\u5229\u7528\u6700\u5c0f\u8bef\u5dee\u71b5(MEE)\u51c6\u5219\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u57fa\u4e8eMEE\u7684\u4f3c\u7136\u51fd\u6570\uff0c\u6539\u8fdb\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u5728\u566a\u58f0\u8111\u6570\u636e\u96c6\u4e2d\u7684\u51c6\u786e\u63a8\u65ad", "result": "\u5728\u4e24\u4e2a\u9ad8\u7ef4\u8111\u89e3\u7801\u4efb\u52a1\uff08\u56de\u5f52\u548c\u5206\u7c7b\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u89e3\u7801\u6307\u6807\u548c\u751f\u7406\u6a21\u5f0f\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u57fa\u4e8eMEE\u7684\u4f3c\u7136\u6a21\u578b\u4f7f\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8111\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\u566a\u58f0\u548c\u9ad8\u7ef4\u6311\u6218\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u7b49\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177"}}
{"id": "2508.12334", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12334", "abs": "https://arxiv.org/abs/2508.12334", "authors": ["Qing Wang", "Ya Jiang", "Hang Chen", "Sabato Marco Siniscalchi", "Jun Du", "Jianqing Gao"], "title": "Cross-Modal Knowledge Distillation with Multi-Level Data Augmentation for Low-Resource Audio-Visual Sound Event Localization and Detection", "comment": "34 pages, 7 figures", "summary": "This work presents a cross-modal knowledge distillation (CMKD) framework\ncombined with multi-level data augmentation for low-resource audio-visual (AV)\nsound event localization and detection (SELD). An audio-only SELD model acts as\nthe teacher, transferring knowledge to an AV student model through both output\nresponses and intermediate feature representations. To enhance learning, data\naugmentation is applied by mixing features randomly selected from multiple\nnetwork layers and associated loss functions tailored to the SELD task.\nExtensive experiments on the DCASE 2023 and 2024 SELD datasets show that the\nproposed method significantly improves AV SELD performance, yielding relative\ngains of 22%~36% in the overall metric over the baseline. Notably, our approach\nachieves results comparable to or better than teacher models trained on much\nlarger datasets, surpassing state-of-the-art methods on both DCASE 2023 and\n2024 SELD tasks.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u8403\u7c89\u548c\u591a\u5c42\u6b21\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u97f3\u89c6\u9891\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7cfb\u7edf\u83b7\u5f9722%~36%\u7684\u76f8\u5bf9\u6536\u76ca", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u97f3\u89c6\u9891\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6311\u6218\uff0c\u901a\u8fc7\u77e5\u8bc6\u8403\u7c89\u6280\u672f\u5c06\u5355\u6a21\u6001\u6a21\u578b\u7684\u77e5\u8bc6\u4f20\u9012\u7ed9\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u6027\u80fd", "method": "\u91c7\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u8403\u7c89\u6846\u67b6\uff0c\u4ee5\u97f3\u9891\u5355\u6a21\u6001SELD\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u5411\u97f3\u89c6\u9891\u5b66\u751f\u6a21\u578b\u4f20\u9012\u8f93\u51fa\u54cd\u5e94\u548c\u4e2d\u95f4\u7279\u5f81\u8868\u793a\u3002\u7ed3\u5408\u591a\u5c42\u6b21\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u968f\u673a\u6df7\u5408\u591a\u4e2a\u7f51\u7edc\u5c42\u7684\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u4e13\u95e8\u4e3aSELD\u4efb\u52a1\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570", "result": "\u5728DCASE 2023\u548c2024 SELD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d89\u53ca\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86AV SELD\u6027\u80fd\uff0c\u5728\u6574\u4f53\u6307\u6807\u4e0a\u83b7\u5f9722%~36%\u7684\u76f8\u5bf9\u6536\u76ca\u3002\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u5728\u66f4\u5927\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86DCASE 2023\u548c2024 SELD\u4efb\u52a1\u4e2d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8403\u7c89\u7ed3\u5408\u591a\u5c42\u6b21\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u5728\u4f4e\u8d44\u6e90\u97f3\u89c6\u9891\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8270\u56fa\u7684\u6027\u80fd\uff0c\u4e3a\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u77e5\u8bc6\u8403\u7c89\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11658", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11658", "abs": "https://arxiv.org/abs/2508.11658", "authors": ["Honggui Li", "Zhengyang Zhang", "Dingtai Li", "Sinan Chen", "Nahid Md Lokman Hossain", "Xinfeng Xu", "Yuting Feng", "Hantao Lu", "Yinlu Qin", "Ruobing Wang", "Maria Trocan", "Dimitri Galayko", "Amara Amara", "Mohamad Sawan"], "title": "CECGSR: Circular ECG Super-Resolution", "comment": null, "summary": "The electrocardiogram (ECG) plays a crucial role in the diagnosis and\ntreatment of various cardiac diseases. ECG signals suffer from low-resolution\n(LR) due to the use of convenient acquisition devices, as well as internal and\nexternal noises and artifacts. Classical ECG super-resolution (ECGSR) methods\nadopt an open-loop architecture that converts LR ECG signals to\nsuper-resolution (SR) ones. According to the theory of automatic control, a\nclosed-loop framework exhibits superior dynamic and static performance compared\nwith its open-loop counterpart. This paper proposes a closed-loop approach,\ntermed circular ECGSR (CECGSR), which models the degradation process from SR\nECG signals to LR ones. The negative feedback mechanism of the closed-loop\nsystem is based on the differences between the LR ECG signals. A mathematical\nloop equation is constructed to characterize the closed-loop infrastructure.\nThe Taylor series expansion is employed to demonstrate the near-zero\nsteady-state error of the proposed method. A Plug-and-Play strategy is\nconsidered to establish the SR unit of the proposed architecture, leveraging\nany existing advanced open-loop ECGSR methods. Simulation experiments on both\nnoiseless and noisy subsets of the PTB-XL datasets demonstrate that the\nproposed CECGSR outperforms state-of-the-art open-loop ECGSR algorithms in the\nreconstruction performance of ECG signals.", "AI": {"tldr": "\u95ed\u73af\u5faa\u73af\u5fc3\u7535\u56fe\u8d85\u5206\u8fa8\u65b9\u6cd5CECGSR\uff0c\u901a\u8fc7\u5efa\u6a21\u9000\u5316\u8fc7\u7a0b\u548c\u8d1f\u53cd\u9988\u673a\u5236\uff0c\u5728PTB-XL\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u73af\u65b9\u6cd5\u7684\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7535\u56fe\u8d85\u5206\u8fa8\u65b9\u6cd5\u91c7\u7528\u5f00\u73af\u67b6\u6784\uff0c\u800c\u6839\u636e\u81ea\u52a8\u63a7\u5236\u7406\u8bba\uff0c\u95ed\u73af\u6846\u67b6\u5177\u6709\u66f4\u4f18\u7684\u52a8\u6001\u548c\u9759\u6001\u6027\u80fd\u3002", "method": "\u63d0\u51faCECGSR\u95ed\u73af\u65b9\u6cd5\uff0c\u5efa\u6a21\u4ece\u9ad8\u5206\u8fa8\u7387\u5230\u4f4e\u5206\u8fa8\u7387\u5fc3\u7535\u56fe\u7684\u9000\u5316\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387\u4fe1\u53f7\u5dee\u5f02\u5b9e\u73b0\u8d1f\u53cd\u9988\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u8bc1\u660e\u7a33\u6001\u8bef\u5dee\u8fd1\u96f6\u3002\u91c7\u7528Plug-and-Play\u7b56\u7565\u6784\u5efa\u8d85\u5206\u8fa8\u5355\u5143\u3002", "result": "\u5728PTB-XL\u6570\u636e\u96c6\u7684\u65e0\u566a\u58f0\u548c\u6709\u566a\u58f0\u5b50\u96c6\u4e0a\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aCECGSR\u5728\u5fc3\u7535\u56fe\u4fe1\u53f7\u91cd\u5efa\u6027\u80fd\u65b9\u9762\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u73afECGSR\u7b97\u6cd5\u3002", "conclusion": "\u95ed\u73afCECGSR\u65b9\u6cd5\u901a\u8fc7\u5faa\u73af\u67b6\u6784\u548c\u8d1f\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u7535\u56fe\u8d85\u5206\u8fa8\u7684\u6027\u80fd\uff0c\u4e3a\u5fc3\u7535\u56fe\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12626", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.12626", "abs": "https://arxiv.org/abs/2508.12626", "authors": ["Meng Yang", "Jon McCormack", "Maria Teresa Llano", "Wanchao Su"], "title": "Exploring the Feasibility of LLMs for Automated Music Emotion Annotation", "comment": "Accepted to be published at ISMIR 2025", "summary": "Current approaches to music emotion annotation remain heavily reliant on\nmanual labelling, a process that imposes significant resource and labour\nburdens, severely limiting the scale of available annotated data. This study\nexamines the feasibility and reliability of employing a large language model\n(GPT-4o) for music emotion annotation. In this study, we annotated\nGiantMIDI-Piano, a classical MIDI piano music dataset, in a four-quadrant\nvalence-arousal framework using GPT-4o, and compared against annotations\nprovided by three human experts. We conducted extensive evaluations to assess\nthe performance and reliability of GPT-generated music emotion annotations,\nincluding standard accuracy, weighted accuracy that accounts for inter-expert\nagreement, inter-annotator agreement metrics, and distributional similarity of\nthe generated labels.\n  While GPT's annotation performance fell short of human experts in overall\naccuracy and exhibited less nuance in categorizing specific emotional states,\ninter-rater reliability metrics indicate that GPT's variability remains within\nthe range of natural disagreement among experts. These findings underscore both\nthe limitations and potential of GPT-based annotation: despite its current\nshortcomings relative to human performance, its cost-effectiveness and\nefficiency render it a promising scalable alternative for music emotion\nannotation.", "AI": {"tldr": "\u8fd9\u7814\u7a76\u8bc4\u4f30\u4e86GPT-4o\u5728\u97f3\u4e50\u60c5\u611f\u6ce8\u91ca\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5bfb\u627e\u4eba\u5de5\u667a\u80fd\u6ce8\u91ca\u7684\u7ecf\u6d4e\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848", "motivation": "\u73b0\u6709\u97f3\u4e50\u60c5\u611f\u6ce8\u91ca\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u6ce8\u91ca\u6570\u636e\u7684\u89c4\u6a21", "method": "\u4f7f\u7528GPT-4o\u5bf9GiantMIDI-Piano\u6570\u636e\u96c6\u8fdb\u884c\u56db\u8c61\u9650\u6fc0\u6d3b\u5ea6-\u4ef7\u503c\u89c2\u6ce8\u91ca\uff0c\u4e0e3\u540d\u4eba\u7c7b\u4e13\u5bb6\u6ce8\u91ca\u8fdb\u884c\u5bf9\u6bd4", "result": "GPT\u6ce8\u91ca\u51c6\u786e\u7387\u6682\u8f83\u4eba\u7c7b\u4e13\u5bb6\u5dee\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5728\u4e13\u5bb6\u4e4b\u95f4\u7684\u81ea\u7136\u5206\u6b67\u8303\u56f4\u5185", "conclusion": "\u867d\u7136GPT\u76ee\u524d\u6027\u80fd\u8f83\u4eba\u7c7b\u5dee\uff0c\u4f46\u5176\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\u4f7f\u5f97\u5b83\u6210\u4e3a\u97f3\u4e50\u60c5\u611f\u6ce8\u91ca\u7684\u6709\u524d\u666f\u53ef\u6269\u5c55\u65b9\u6848"}}
{"id": "2508.11663", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11663", "abs": "https://arxiv.org/abs/2508.11663", "authors": ["Guangli Li", "Canbiao Wu", "Zhen Liang"], "title": "Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation", "comment": null, "summary": "Affective computing is a rapidly developing interdisciplinary research\ndirection in the field of brain-computer interface. In recent years, the\nintroduction of deep learning technology has greatly promoted the development\nof the field of emotion recognition. However, due to physiological differences\nbetween subjects, as well as the variations in experimental environments and\nequipment, cross-corpus emotion recognition faces serious challenges,\nespecially for samples near the decision boundary. To solve the above problems,\nwe propose an optimization method based on domain adversarial transfer learning\nto fine-grained alignment of affective features, named Maximum classifier\ndiscrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a\ndual adversarial classifier (Ada classifier and RMS classifier), and apply a\nthree-stage adversarial training to maximize classification discrepancy and\nminimize feature distribution to align controversy samples near the decision\nboundary. In the process of domain adversarial training, the two classifiers\nalso maintain an adversarial relationship, ultimately enabling precise\ncross-corpus feature alignment. In addition, the introduction of pairwise\nlearning transforms the classification problem of samples into a similarity\nproblem between samples, alleviating the influence of label noise. We conducted\nsystematic experimental evaluation of the model using publicly available SEED,\nSEED-IV and SEED-V databases. The results show that the McdPL model is superior\nto other baseline models in the cross-corpus emotion recognition task, and the\naverage accuracy improvements of 4.76\\% and 3.97\\%, respectively. Our work\nprovides a promising solution for emotion recognition cross-corpus. The source\ncode is available at https://github.com/WuCB-BCI/Mcd_PL.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9886\u57df\u5bf9\u6297\u8fc1\u79fb\u5b66\u4e60\u7684McdPL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5bf9\u6297\u5206\u7c7b\u5668\u548c\u4e09\u9636\u6bb5\u5bf9\u6297\u8bad\u7ec3\u89e3\u51b3\u8de8\u8bed\u6599\u5e93\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u51b3\u7b56\u8fb9\u754c\u6837\u672c\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u88ab\u8bd5\u751f\u7406\u5dee\u5f02\u3001\u5b9e\u9a8c\u73af\u5883\u548c\u8bbe\u5907\u53d8\u5316\uff0c\u8de8\u8bed\u6599\u5e93\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff0c\u7279\u522b\u662f\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u6837\u672c\u96be\u4ee5\u51c6\u786e\u5206\u7c7b\u3002", "method": "\u63d0\u51faMcdPL\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5bf9\u6297\u5206\u7c7b\u5668\uff08Ada\u548cRMS\u5206\u7c7b\u5668\uff09\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5bf9\u6297\u8bad\u7ec3\u6700\u5927\u5316\u5206\u7c7b\u5dee\u5f02\u5e76\u6700\u5c0f\u5316\u7279\u5f81\u5206\u5e03\u5dee\u5f02\uff0c\u540c\u65f6\u5f15\u5165\u6210\u5bf9\u5b66\u4e60\u5c06\u5206\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u6837\u672c\u76f8\u4f3c\u6027\u95ee\u9898\u3002", "result": "\u5728SEED\u3001SEED-IV\u548cSEED-V\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMcdPL\u6a21\u578b\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u63d0\u53474.76%\u548c3.97%\u3002", "conclusion": "McdPL\u6846\u67b6\u4e3a\u8de8\u8bed\u6599\u5e93\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7684\u7279\u5f81\u5bf9\u9f50\u548c\u5bf9\u6297\u8bad\u7ec3\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2508.12709", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12709", "abs": "https://arxiv.org/abs/2508.12709", "authors": ["Aurian Quelennec", "Pierre Chouteau", "Geoffroy Peeters", "Slim Essid"], "title": "MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning", "comment": "Under review", "summary": "Masked latent prediction has emerged as a leading paradigm in self-supervised\nlearning (SSL), especially for general audio and music representation learning.\nWhile recent methods have demonstrated strong performance, the role of the\npredictor module used at the output of such SSL systems remains mainly\noverlooked, despite being crucial for solving the pretext task at hand. In\nparticular, this module should be able to deal with the ambiguity inherent in\naudio content, especially when it is composed of multiple sound sources. This\nwork proposes a novel enhancement: integrating Multiple Choice Learning (MCL)\nto explicitly model prediction ambiguity and improve representation quality. We\nbuild on top of the recently proposed MATPAC system, improving its prediction\nand unsupervised classification pretext tasks with MCL. We extensively evaluate\nour method, MATPAC++, through both linear probing across multiple downstream\ntasks and fine-tuning on AudioSet, employing a unified protocol that enables\nrigorous and fair comparisons with state-of-the-art SSL approaches. Results\nshow that our proposal achieves state-of-the-art when fine-tuned on AudioSet\nand overall state-of-the-art scores on downstream tasks. Additionally, we\nexamine domain specialisation by training exclusively on music data, where our\nmodel achieves state-of-the-art performance with significantly improved\nefficiency.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u591a\u91cd\u9009\u62e9\u5b66\u4e60(MCL)\u6765\u663e\u5f0f\u6a21\u578b\u9884\u6d4b\u6a21\u7cca\u6027\uff0c\u6539\u5584MATPAC\u7cfb\u7edf\u7684\u8868\u5f81\u5b66\u4e60\u6027\u80fd\uff0c\u5728AudioSet\u548c\u591a\u4e2b\u6e38\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73", "motivation": "\u73b0\u6709\u7684\u63a9\u7801\u6f5c\u5728\u9884\u6d4bSSL\u65b9\u6cd5\u5bf9\u9884\u6d4b\u5668\u6a21\u5757\u7684\u4f5c\u7528\u5173\u6ce8\u4e0d\u591f\uff0c\u800c\u97f3\u9891\u5185\u5bb9\u672c\u8eab\u5b58\u5728\u591a\u91cd\u58f0\u97f3\u6e90\u7684\u6a21\u7cca\u6027\uff0c\u9700\u8981\u66f4\u597d\u5730\u5904\u7406\u8fd9\u79cd\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027", "method": "\u5728MATPAC\u7cfb\u7edf\u57fa\u7840\u4e0a\u96c6\u6210\u591a\u91cd\u9009\u62e9\u5b66\u4e60(MCL)\uff0c\u663e\u5f0f\u6a21\u578b\u9884\u6d4b\u6a21\u7cca\u6027\uff0c\u6539\u8fdb\u9884\u6d4b\u548c\u65e0\u76d1\u7763\u5206\u7c7b\u9884\u6587\u4efb\u52a1", "result": "\u5728AudioSet\u4e0a\u7ec6\u8c03\u65f6\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u591a\u4e2b\u6e38\u4efb\u52a1\u4e0a\u83b7\u5f97\u603b\u4f53\u6700\u4f73\u6210\u7ee9\uff0c\u5728\u97f3\u4e50\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u4e14\u6548\u7387\u663e\u8457\u63d0\u9ad8", "conclusion": "\u96c6\u6210MCL\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u97f3\u9891\u9884\u6d4b\u7684\u6a21\u7cca\u6027\uff0c\u663e\u8457\u63d0\u5347\u8868\u5f81\u5b66\u4e60\u8d28\u91cf\uff0c\u4e3a\u97f3\u9891SSL\u9884\u6587\u4efb\u52a1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2"}}
{"id": "2508.11664", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11664", "abs": "https://arxiv.org/abs/2508.11664", "authors": ["Zahra Mohammadi", "Parnian Fazel", "Siamak Mohammadi"], "title": "Energy-Efficient Real-Time 4-Stage Sleep Classification at 10-Second Resolution: A Comprehensive Study", "comment": null, "summary": "Sleep stage classification is crucial for diagnosing and managing disorders\nsuch as sleep apnea and insomnia. Conventional clinical methods like\npolysomnography are costly and impractical for long-term home use. We present\nan energy-efficient pipeline that detects four sleep stages (wake, REM, light,\nand deep) from a single-lead ECG. Two windowing strategies are introduced: (1)\na 5-minute window with 30-second steps for machine-learning models that use\nhandcrafted features, and (2) a 30-second window with 10-second steps for\ndeep-learning models, enabling near-real-time 10-second resolution. Lightweight\nnetworks such as MobileNet-v1 reach 92 percent accuracy and 91 percent F1-score\nbut still draw significant energy. We therefore design SleepLiteCNN, a custom\nmodel that achieves 89 percent accuracy and 89 percent F1-score while lowering\nenergy use to 5.48 microjoules per inference at 45 nm. Applying eight-bit\nquantization preserves accuracy and further reduces power, and FPGA deployment\nconfirms low resource usage. The proposed system offers a practical solution\nfor continuous, wearable ECG-based sleep monitoring.", "AI": {"tldr": "\u57fa\u4e8e\u5355\u5bfc\u8054\u7535\u5fc3\u56fe\u7684\u80fd\u6548\u7761\u7720\u5206\u671f\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u9ad8\u51c6\u786e\u5ea6\u548c\u4f4e\u80fd\u8017\uff0c\u9002\u5408\u53ef\u7a7f\u6234\u5f0f\u957f\u671f\u76d1\u6d4b", "motivation": "\u4f20\u7edf\u591a\u5bfc\u8054\u7761\u7720\u76d1\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e0d\u65b9\u4fbf\u5bb6\u5ead\u957f\u671f\u4f7f\u7528\uff0c\u9700\u8981\u5f00\u53d1\u4ece\u5355\u5bfc\u8054\u7535\u5fc3\u56fe\u51c6\u786e\u5206\u6790\u7761\u7720\u9636\u6bb5\u7684\u80fd\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u4e24\u79cd\u7a97\u53e3\u5207\u5206\u7b56\u7565\uff1a5\u5206\u949f\u7a97\u53e3\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c30\u79d2\u7a97\u53e3\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u81ea\u5b9a\u4e49\u6a21\u578bSleepLiteCNN\uff0c\u5e76\u5e94\u75288\u4f4d\u91cf\u5316\u6280\u672f", "result": "MobileNet-v1\u6a21\u578b\u8fbe\u523092%\u51c6\u786e\u7387\u548c91% F1\u5206\u6570\uff0cSleepLiteCNN\u6a21\u578b\u5728\u51c6\u786e\u738789%\u3001F1\u5206\u657089%\u7684\u60c5\u51b5\u4e0b\uff0c\u6bcf\u6b21\u63a8\u7406\u80fd\u8017\u964d\u81f35.48\u5fae\u7126\u8033\uff0cFPGA\u90e8\u7f72\u8bc1\u660e\u8d44\u6e90\u5360\u7528\u4f4e", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u53ef\u7a7f\u6234\u7535\u5fc3\u56fe\u57fa\u7840\u7684\u8fde\u7eed\u7761\u7720\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f4e\u80fd\u8017\u548c\u5b9e\u65f6\u6027\u80fd"}}
{"id": "2508.12918", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.12918", "abs": "https://arxiv.org/abs/2508.12918", "authors": ["Lei Zhao", "Rujin Chen", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "FoleySpace: Vision-Aligned Binaural Spatial Audio Generation", "comment": null, "summary": "Recently, with the advancement of AIGC, deep learning-based video-to-audio\n(V2A) technology has garnered significant attention. However, existing research\nmostly focuses on mono audio generation that lacks spatial perception, while\nthe exploration of binaural spatial audio generation technologies, which can\nprovide a stronger sense of immersion, remains insufficient. To solve this\nproblem, we propose FoleySpace, a framework for video-to-binaural audio\ngeneration that produces immersive and spatially consistent stereo sound guided\nby visual information. Specifically, we develop a sound source estimation\nmethod to determine the sound source 2D coordinates and depth in each video\nframe, and then employ a coordinate mapping mechanism to convert the 2D source\npositions into a 3D trajectory. This 3D trajectory, together with the monaural\naudio generated by a pre-trained V2A model, serves as a conditioning input for\na diffusion model to generate spatially consistent binaural audio. To support\nthe generation of dynamic sound fields, we constructed a training dataset based\non recorded Head-Related Impulse Responses that includes various sound source\nmovement scenarios. Experimental results demonstrate that the proposed method\noutperforms existing approaches in spatial perception consistency, effectively\nenhancing the immersive quality of the audio-visual experience.", "AI": {"tldr": "FoleySpace\u662f\u4e00\u4e2a\u89c6\u9891\u5230\u53cc\u8033\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u4fe1\u606f\u5f15\u5bfc\u751f\u6210\u6c89\u6d78\u5f0f\u7a7a\u95f4\u4e00\u81f4\u7684\u7acb\u4f53\u58f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709V2A\u6280\u672f\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u89c6\u9891\u5230\u97f3\u9891\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u5355\u58f0\u9053\u97f3\u9891\u751f\u6210\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u800c\u80fd\u591f\u63d0\u4f9b\u66f4\u5f3a\u6c89\u6d78\u611f\u7684\u53cc\u8033\u7a7a\u95f4\u97f3\u9891\u751f\u6210\u6280\u672f\u7814\u7a76\u4e0d\u8db3", "method": "\u5f00\u53d1\u58f0\u97f3\u6e90\u4f30\u8ba1\u65b9\u6cd5\u786e\u5b9a\u89c6\u9891\u5e27\u4e2d\u7684\u58f0\u6e902D\u5750\u6807\u548c\u6df1\u5ea6\uff0c\u901a\u8fc7\u5750\u6807\u6620\u5c04\u673a\u5236\u8f6c\u6362\u4e3a3D\u8f68\u8ff9\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3V2A\u6a21\u578b\u751f\u6210\u7684\u5355\u58f0\u9053\u97f3\u9891\uff0c\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\u6761\u4ef6\u6765\u751f\u6210\u7a7a\u95f4\u4e00\u81f4\u7684\u53cc\u8033\u97f3\u9891", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7a7a\u95f4\u611f\u77e5\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u97f3\u89c6\u9891\u4f53\u9a8c\u7684\u6c89\u6d78\u8d28\u91cf", "conclusion": "FoleySpace\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u89c6\u9891\u5230\u53cc\u8033\u97f3\u9891\u7684\u751f\u6210\uff0c\u4e3a\u6c89\u6d78\u5f0f\u97f3\u89c6\u9891\u4f53\u9a8c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12403", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12403", "abs": "https://arxiv.org/abs/2508.12403", "authors": ["Federico Miotello", "Davide Albertini", "Alberto Bernardini"], "title": "On the Extension of Differential Beamforming Theory to Arbitrary Planar Arrays of First-Order Elements", "comment": null, "summary": "Small-size acoustic arrays exploit spatial diversity to achieve capabilities\nbeyond those of single-element devices, with applications ranging from\nteleconferencing to immersive multimedia. A key requirement for broadband array\nprocessing is a frequency-invariant spatial response, which ensures consistent\ndirectivity across wide bandwidths and prevents spectral coloration.\nDifferential beamforming offers an inherently frequency-invariant solution by\nleveraging pressure differences between closely spaced elements of small-size\narrays. Traditional approaches, however, assume the array elements to be\nomnidirectional, whereas real transducers exhibit frequency-dependent\ndirectivity that can degrade performance if not properly modeled. To address\nthis limitation, we propose a generalized modal matching framework for\nfrequency-invariant differential beamforming, applicable to unconstrained\nplanar arrays of first-order directional elements. By representing the desired\nbeampattern as a truncated circular harmonic expansion and fitting it to the\nactual element responses, our method accommodates arbitrary planar geometries\nand element orientations. This approach enables the synthesis of beampatterns\nof any order and steering direction without imposing rigid layout requirements.\nSimulations confirm that accounting for sensor directivity at the design stage\nyields accurate and robust performance across varying frequencies, geometries,\nand noise conditions.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u6001\u5339\u914d\u6846\u67b6\u7684\u5e7f\u4e49\u5fae\u5206\u5835\u5f62\u6280\u672f\uff0c\u89e3\u51b3\u4f20\u611f\u5668\u65b9\u5411\u6027\u5bf9\u5e7f\u5e26\u5a01\u5d4c\u5854\u6570\u7ec4\u6027\u80fd\u7684\u5f71\u54cd", "motivation": "\u4f20\u7edf\u5fae\u5206\u5835\u5f62\u6280\u672f\u5047\u8bbe\u6570\u7ec4\u5143\u4ef6\u4e3a\u5168\u5411\u6027\uff0c\u800c\u5b9e\u9645\u4f20\u611f\u5668\u5177\u6709\u9891\u7387\u76f8\u5173\u7684\u65b9\u5411\u6027\uff0c\u8fd9\u4f1a\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd", "method": "\u63d0\u51fa\u4e00\u79cd\u5e7f\u4e49\u7684\u6a21\u6001\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u5706\u5468\u8c03\u548c\u5c55\u5f00\u8868\u793a\u671f\u671b\u7684\u5835\u5f62\u56fe\uff0c\u5e76\u4e0e\u5b9e\u9645\u5143\u4ef6\u54cd\u5e94\u8fdb\u884c\u62df\u5408\uff0c\u652f\u6301\u4efb\u610f\u5e73\u9762\u5e03\u5c40\u548c\u5143\u4ef6\u65b9\u5411", "result": "\u6a21\u62df\u7ed3\u679c\u8bc1\u660e\uff0c\u5728\u8bbe\u8ba1\u9636\u6bb5\u8003\u8651\u4f20\u611f\u5668\u65b9\u5411\u6027\u80fd\u591f\u5728\u4e0d\u540c\u9891\u7387\u3001\u4e0d\u540c\u5e03\u5c40\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u83b7\u5f97\u51c6\u786e\u800c\u7a33\u5065\u7684\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c0f\u578b\u5e7f\u5e26\u5a01\u58f0\u6570\u7ec4\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u6709\u6548\u7684\u65b9\u6848\uff0c\u5145\u5206\u8003\u8651\u4e86\u5b9e\u9645\u4f20\u611f\u5668\u7684\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9891\u7387\u72ec\u7acb\u7684\u7a7a\u95f4\u54cd\u5e94"}}
{"id": "2508.11666", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11666", "abs": "https://arxiv.org/abs/2508.11666", "authors": ["Timothy Oladunni", "Ehimen Aneni"], "title": "Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate vs Late Fusion", "comment": null, "summary": "The limitations of unimodal deep learning models, particularly their tendency\nto overfit and limited generalizability, have renewed interest in multimodal\nfusion strategies. Multimodal deep neural networks (MDNN) have the capability\nof integrating diverse data domains and offer a promising solution for robust\nand accurate predictions. However, the optimal fusion strategy, intermediate\nfusion (feature-level) versus late fusion (decision-level) remains\ninsufficiently examined, especially in high-stakes clinical contexts such as\nECG-based cardiovascular disease (CVD) classification. This study investigates\nthe comparative effectiveness of intermediate and late fusion strategies using\nECG signals across three domains: time, frequency, and time-frequency. A series\nof experiments were conducted to identify the highest-performing fusion\narchitecture. Results demonstrate that intermediate fusion consistently\noutperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's\nd > 0.8 relative to standalone models and d = 0.40 compared to late fusion.\nInterpretability analyses using saliency maps reveal that both models align\nwith the discretized ECG signals. Statistical dependency between the\ndiscretized ECG signals and corresponding saliency maps for each class was\nconfirmed using Mutual Information (MI). The proposed ECG domain-based\nmultimodal model offers superior predictive capability and enhanced\nexplainability, crucial attributes in medical AI applications, surpassing\nstate-of-the-art models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728ECG\u5fc3\u8840\u7ba1\u75be\u75c5\u5206\u7c7b\u4e2d\u7684\u878d\u5408\u7b56\u7565\uff0c\u53d1\u73b0\u7279\u5f81\u5c42\u878d\u5408\u6bd4\u51b3\u7b56\u5c42\u878d\u5408\u6548\u679c\u66f4\u597d\uff0c\u8fbe\u5230\u4e8697%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5355\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u8fc7\u62df\u5408\u548c\u6cbf\u7528\u6027\u5c40\u9650\u7684\u95ee\u9898\uff0c\u800c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6700\u4f18\u65b9\u6848\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728ECG\u5fc3\u8840\u7ba1\u75be\u75c5\u5206\u7c7b\u4e2d\u3002", "method": "\u91c7\u7528\u65f6\u57df\u3001\u9891\u57df\u548c\u65f6\u9891\u57df\u4e09\u79cdECG\u4fe1\u53f7\u57df\uff0c\u5bf9\u6bd4\u7814\u7a76\u4e2d\u95f4\u878d\u5408\uff08\u7279\u5f81\u5c42\uff09\u548c\u540e\u671f\u878d\u5408\uff08\u51b3\u7b56\u5c42\uff09\u4e24\u79cd\u7b56\u7565\u7684\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u663e\u8457\u6027\u5730\u56fe\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u90fd\u8868\u73b0\u66f4\u4f18\uff0c\u8fbe\u5230\u5cf0\u503c\u51c6\u786e\u738797%\uff0c\u76f8\u6bd4\u5355\u6a21\u6001\u6a21\u578b\u7684Cohen's d > 0.8\uff0c\u76f8\u6bd4\u540e\u671f\u878d\u5408\u7684Cohen's d = 0.40\u3002\u663e\u8457\u6027\u5730\u56fe\u5206\u6790\u786e\u8ba4\u4e86\u6a21\u578b\u4e0eECG\u4fe1\u53f7\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eECG\u57df\u7684\u591a\u6a21\u6001\u6a21\u578b\u5177\u6709\u66f4\u4f18\u5f02\u7684\u9884\u6d4b\u80fd\u529b\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u533b\u7597AI\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2508.11668", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11668", "abs": "https://arxiv.org/abs/2508.11668", "authors": ["Muhammad Umer", "Muhammad Ahmed Mohsin", "Ahsan Bilal", "John M. Cioffi"], "title": "Neural Gaussian Radio Fields for Channel Estimation", "comment": "This paper has been submitted to NeurIPS 2025", "summary": "Accurate channel state information (CSI) remains the most critical bottleneck\nin modern wireless networks, with pilot overhead consuming up to 11-21% of\ntransmission bandwidth, increasing latency by 20-40% in massive MIMO systems,\nand reducing potential spectral efficiency by over 53%. Traditional estimation\ntechniques fundamentally fail under mobility, with feedback delays as small as\n4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We\npresent neural Gaussian radio fields (nGRF), a novel framework that leverages\nexplicit 3D Gaussian primitives to synthesize complex channel matrices\naccurately and efficiently. Unlike NeRF-based approaches that rely on slow\nimplicit representations or existing Gaussian splatting methods that use\nnon-physical 2D projections, nGRF performs direct 3D electromagnetic field\naggregation, with each Gaussian acting as a localized radio modulator. nGRF\ndemonstrates superior performance across diverse environments: in indoor\nscenarios, it achieves a 10.9$\\times$ higher prediction SNR than state of the\nart methods while reducing inference latency from 242 ms to just 1.1 ms (a\n220$\\times$ speedup). For large-scale outdoor environments, where existing\napproaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF\nrequires only 0.011 measurements per cubic foot compared to 0.2-178.1 for\nexisting methods, thereby reducing data collection burden by 18$\\times$.\nTraining time is similarly reduced from hours to minutes (a 180$\\times$\nreduction), enabling rapid adaptation to dynamic environments. The code and\ndatasets are available at: https://github.com/anonym-auth/n-grf", "AI": {"tldr": "nGRF\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u539f\u8bed\u7684\u65e0\u7ebf\u4fe1\u9053\u4f30\u8ba1\u65b0\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u6027\u73af\u5883\u4e0b\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u6280\u672f\u5728\u79fb\u52a8\u73af\u5883\u4e0b\u5b58\u5728\u4e25\u91cd\u6027\u80fd\u9000\u5316\uff0c\u5bfc\u9891\u5f00\u9500\u5927\u3001\u5ef6\u8fdf\u9ad8\u3001\u9891\u8c31\u6548\u7387\u4f4e\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u663e\u5f0f3D\u9ad8\u65af\u539f\u8bed\u8fdb\u884c\u76f4\u63a53D\u7535\u78c1\u573a\u805a\u5408\uff0c\u6bcf\u4e2a\u9ad8\u65af\u51fd\u6570\u4f5c\u4e3a\u5c40\u90e8\u65e0\u7ebf\u7535\u8c03\u5236\u5668\uff0c\u907f\u514d\u4e86\u57fa\u4e8eNeRF\u7684\u6162\u901f\u9690\u5f0f\u8868\u793a\u6216\u975e\u7269\u74062D\u6295\u5f71\u65b9\u6cd5\u3002", "result": "\u5ba4\u5185\u573a\u666f\u9884\u6d4bSNR\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad810.9\u500d\uff0c\u63a8\u7406\u5ef6\u8fdf\u4ece242ms\u964d\u81f31.1ms\uff08220\u500d\u52a0\u901f\uff09\uff1b\u5ba4\u5916\u573a\u666f\u8fbe\u523026.2dB SNR\uff1b\u6570\u636e\u6536\u96c6\u8d1f\u62c5\u51cf\u5c1118\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u5206\u949f\u7ea7\uff08180\u500d\u51cf\u5c11\uff09\u3002", "conclusion": "nGRF\u6846\u67b6\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u79fb\u52a8\u73af\u5883\u4e0b\u7684\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11675", "categories": ["eess.SP", "53A45", "G.1"], "pdf": "https://arxiv.org/pdf/2508.11675", "abs": "https://arxiv.org/abs/2508.11675", "authors": ["Amgad A. Salama"], "title": "Direction of Arrival Estimation: A Tutorial Survey of Classical and Modern Methods", "comment": "DOA Survey, 44 pages, Not published yet", "summary": "Direction of arrival (DOA) estimation is a fundamental problem in array\nsignal processing with applications spanning radar, sonar, wireless\ncommunications, and acoustic signal processing. This tutorial survey provides a\ncomprehensive introduction to classical and modern DOA estimation methods,\nspecifically designed for students and researchers new to the field. We focus\non narrowband signal processing using uniform linear arrays, presenting\nstep-by-step mathematical derivations with geometric intuition. The survey\ncovers classical beamforming methods, subspace-based techniques (MUSIC,\nESPRIT), maximum likelihood approaches, and sparse signal processing methods.\nEach method is accompanied by Python implementations available in an\nopen-source repository, enabling reproducible research and hands-on learning.\nThrough systematic performance comparisons across various scenarios, we provide\npractical guidelines for method selection and parameter tuning. This work aims\nto bridge the gap between theoretical foundations and practical implementation,\nmaking DOA estimation accessible to beginners while serving as a comprehensive\nreference for the field. See https://github.com/AmgadSalama/DOA for detail\nimplementation of the methods.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4efd\u5173\u4e8e\u5230\u8fbe\u89d2\u5ea6\u4f30\u8ba1\u7684\u7efc\u8ff0\u6027\u6559\u7a0b\u8bba\u6587\uff0c\u4e3a\u521d\u5b66\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4ece\u7ecf\u5178\u5230\u73b0\u4ee3\u65b9\u6cd5\u7684\u5168\u9762\u4ecb\u7ecd\uff0c\u5305\u62ec\u6570\u5b66\u63a8\u5bfc\u3001Python\u5b9e\u73b0\u548c\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5230\u8fbe\u89d2\u5ea6\u4f30\u8ba1\u662f\u6570\u7ec4\u4fe1\u53f7\u5904\u7406\u57fa\u7840\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u96f7\u8fbe\u3001\u58f0\u7eb3\u3001\u65e0\u7ebf\u901a\u4fe1\u7b49\u9886\u57df\u3002\u8bba\u6587\u65e8\u5728\u4e3a\u5165\u95e8\u8005\u63d0\u4f9b\u5168\u9762\u7684\u5b66\u4e60\u8d44\u6e90\uff0c\u7f29\u5c0f\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91cd\u70b9\u5173\u6ce8\u7a84\u5e26\u4fe1\u53f7\u5904\u7406\u548c\u5747\u5300\u7ebf\u6027\u6570\u7ec4\uff0c\u5305\u542b\u7ecf\u5178\u6ce2\u675f\u5f62\u6210\u3001\u5b50\u7a7a\u95f4\u65b9\u6cd5\uff08MUSIC\u3001ESPRIT\uff09\u3001\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u548c\u7a00\u758f\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u3002\u63d0\u4f9b\u6b65\u9aa4\u6027\u6570\u5b66\u63a8\u5bfc\u548c\u51e0\u4f55\u76f4\u89c2\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5f00\u6e90Python\u5b9e\u73b0\u4ee3\u7801\u5e93\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7814\u7a76\u548c\u5b9e\u8df5\u5b66\u4e60\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u80fd\u5bf9\u6bd4\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u548c\u53c2\u6570\u8c03\u6574\u7684\u5b9e\u8df5\u6307\u5357\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u6027\u8bba\u6587\u6210\u529f\u5730\u4e3a\u5230\u8fbe\u89d2\u5ea6\u4f30\u8ba1\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5165\u95e8\u6559\u7a0b\uff0c\u65e2\u9002\u5408\u521d\u5b66\u8005\u5b66\u4e60\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u9886\u57df\u53c2\u8003\u624b\u518c\u3002\u5f00\u6e90\u4ee3\u7801\u5e93\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u8df5\u652f\u6301\u3002"}}
{"id": "2508.11682", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11682", "abs": "https://arxiv.org/abs/2508.11682", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study", "comment": null, "summary": "Non-invasive glucose monitoring remains a critical challenge in the\nmanagement of diabetes. HRV during sleep shows promise for glucose prediction\nhowever, age-related autonomic changes significantly confound traditional HRV\nanalyses. We analyzed 43 subjects with multi-modal data including sleep-stage\nspecific ECG, HRV features, and clinical measurements. A novel\nage-normalization technique was applied to the HRV features by, dividing the\nraw values by age-scaled factors. BayesianRidge regression with 5-fold\ncross-validation was employed for log-glucose prediction. Age-normalized HRV\nfeatures achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction,\nrepresenting a 25.6% improvement over non-normalized features (R2 = 0.132). The\ntop predictive features were hrv rem mean rr age normalized (r = 0.443, p =\n0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic\nblood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed\nage-normalization as the critical component, with sleep-stage specific features\nproviding additional predictive value. Age-normalized HRV features\nsignificantly enhance glucose prediction accuracy compared with traditional\napproaches. This sleep-aware methodology addresses fundamental limitations in\nautonomic function assessment and suggests a preliminary feasibility for\nnon-invasive glucose monitoring applications. However, these results require\nvalidation in larger cohorts before clinical consideration.", "AI": {"tldr": "\u901a\u8fc7\u5e74\u9f84\u6b63\u89c4\u5316\u7684\u7761\u7720\u5fc3\u7387\u53d8\u5f02\u6027\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u521b\u8840\u7cd6\u76d1\u6d4b\u7684\u51c6\u786e\u5ea6\uff0c\u4e3a\u7cd6\u5c3f\u75c5\u7ba1\u7406\u63d0\u4f9b\u65b0\u65b9\u6cd5", "motivation": "\u975e\u4fb5\u5165\u6027\u8840\u7cd6\u76d1\u6d4b\u662f\u7cd6\u5c3f\u75c5\u7ba1\u7406\u7684\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u7684\u5fc3\u7387\u53d8\u5f02\u6027\u5206\u6790\u53d7\u5230\u5e74\u9f84\u76f8\u5173\u81ea\u4e3b\u795e\u7ecf\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u9700\u8981\u627e\u5230\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u65b9\u6cd5", "method": "\u5bf943\u540d\u53d7\u8bd5\u8005\u8fdb\u884c\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\uff0c\u5305\u62ec\u7761\u7720\u9636\u6bb5\u7279\u5f02\u5fc3\u7535\u56fe\u3001\u5fc3\u7387\u53d8\u5f02\u6027\u7279\u5f81\u548c\u4e34\u5e8a\u6d4b\u91cf\u3002\u91c7\u7528\u65b0\u7684\u5e74\u9f84\u6b63\u89c4\u5316\u6280\u672f\uff0c\u5c06HRV\u539f\u59cb\u503c\u9664\u4ee5\u5e74\u9f84\u7f29\u653e\u56e0\u5b50\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u5cad\u56de\u5f52\u548c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c\u5bf9\u6570\u8840\u7cd6\u9884\u6d4b", "result": "\u5e74\u9f84\u6b63\u89c4\u5316\u7684HRV\u7279\u5f81\u5b9e\u73b0\u4e86R2 = 0.161 (MAE = 0.182)\u7684\u9884\u6d4b\u6548\u679c\uff0c\u6bd4\u975e\u6b63\u89c4\u5316\u7279\u5f81\u63d0\u9ad825.6%\u3002\u6700\u4f73\u9884\u6d4b\u7279\u5f81\u5305\u62ec\u7761\u7720\u773c\u52a8\u671f\u5e73\u5747RR\u95f4\u671f\u3001\u6df1\u7761\u671f\u5e73\u5747RR\u95f4\u671f\u548c\u8214\u5f20\u538b", "conclusion": "\u5e74\u9f84\u6b63\u89c4\u5316\u7684HRV\u7279\u5f81\u663e\u8457\u63d0\u9ad8\u4e86\u8840\u7cd6\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd9\u79cd\u8003\u8651\u7761\u7720\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u81ea\u4e3b\u795e\u7ecf\u529f\u80fd\u8bc4\u4f30\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u975e\u4fb5\u5165\u6027\u8840\u7cd6\u76d1\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u521d\u6b65\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u5728\u66f4\u5927\u7fa4\u4f53\u4e2d\u9a8c\u8bc1"}}
{"id": "2508.11684", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11684", "abs": "https://arxiv.org/abs/2508.11684", "authors": ["BG Tong"], "title": "A Graph Neural Network based on a Functional Topology Model: Unveiling the Dynamic Mechanisms of Non-Suicidal Self-Injury in Single-Channel EEG", "comment": null, "summary": "Objective: This study proposes and preliminarily validates a novel\n\"Functional-Energetic Topology Model\" to uncover neurodynamic mechanisms of\nNon-Suicidal Self-Injury (NSSI), using Graph Neural Networks (GNNs) to decode\nbrain network patterns from single-channel EEG in real-world settings.Methods:\nEEG data were collected over ~1 month from three adolescents with NSSI using a\nsmartphone app and a portable Fp1 EEG headband during impulsive and\nnon-impulsive states. A theory-driven GNN with seven functional nodes was\nbuilt. Performance was evaluated via intra-subject (80/20 split) and\nleave-one-subject-out cross-validation (LOSOCV). GNNExplainer was used for\ninterpretability.Results: The model achieved high intra-subject accuracy (>85%)\nand significantly above-chance cross-subject performance (approximately73.7%).\nExplainability analysis revealed a key finding: during NSSI states, a critical\nfeedback loop regulating somatic sensation exhibits dysfunction and directional\nreversal. Specifically, the brain loses its ability to self-correct via\nnegative bodily feedback, and the regulatory mechanism enters an \"ineffective\nidling\" state.Conclusion: This work demonstrates the feasibility of applying\ntheory-guided GNNs to sparse, single-channel EEG for decoding complex mental\nstates. The identified \"feedback loop reversal\" offers a novel, dynamic, and\ncomputable model of NSSI mechanisms, paving the way for objective biomarkers\nand next-generation Digital Therapeutics (DTx).", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9898\u7684\u529f\u80fd-\u80fd\u91cf\u62d3\u6251\u6a21\u578b\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u5355\u901a\u9053EEG\u6570\u636e\uff0c\u63ed\u793a\u4e86\u975e\u81ea\u6740\u6027\u81ea\u4f24\u884c\u4e3a\u7684\u795e\u7ecf\u52a8\u529b\u5b66\u673a\u5236\uff0c\u53d1\u73b0\u4e86\u5173\u952e\u7684\u53cd\u9988\u5faa\u73af\u5931\u8c03\u73b0\u8c61\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u975e\u81ea\u6740\u6027\u81ea\u4f24\u884c\u4e3a\uff08NSSI\uff09\u7684\u795e\u7ecf\u673a\u5236\uff0c\u5e76\u5c1d\u8bd5\u4f7f\u7528\u5355\u901a\u9053EEG\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u6280\u672f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u89e3\u7801\u590d\u6742\u7684\u5fc3\u7406\u72b6\u6001\u3002", "method": "\u4f7f\u7528\u667a\u80fd\u624b\u673a\u5e94\u7528\u548c\u4fbf\u643a\u5f0fFp1 EEG\u5934\u5e26\u6536\u96c6\u4e09\u540d\u9752\u5c11\u5e74NSSI\u60a3\u8005\u7684EEG\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u4e03\u8282\u70b9\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5185\u90e8\u5206\u5272\u548c\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\uff0c\u4f7f\u7528GNNExplainer\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u6a21\u578b\u5728\u5185\u90e8\u9a8c\u8bc1\u4e2d\u8fbe\u5230\u4e86\u8d85\u8fc785%\u7684\u51c6\u786e\u7387\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u4e5f\u663e\u793a\u663e\u8457\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\u7684\u7ea673.7%\u6027\u80fd\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53d1\u73b0\u4e86\u5173\u952e\u53cd\u9988\u5faa\u73af\u5931\u8c03\u548c\u65b9\u5411\u9006\u8f6c\u7684\u73b0\u8c61\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8bc1\u660e\u4e86\u7406\u8bba\u5bfc\u5411\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7a00\u758f\u5355\u901a\u9053EEG\u4e2d\u89e3\u7801\u590d\u6742\u5fc3\u7406\u72b6\u6001\u7684\u53ef\u884c\u6027\uff0c\u6240\u8bc6\u522b\u7684\"\u53cd\u9988\u5faa\u73af\u9006\u8f6c\"\u673a\u5236\u4e3aNSSI\u63d0\u4f9b\u4e86\u65b0\u9898\u7684\u52a8\u6001\u8ba1\u7b97\u6a21\u578b\uff0c\u4e3a\u5ba2\u89c2\u751f\u7269\u6807\u8bb0\u7269\u548c\u4e0b\u4e00\u4ee3\u6570\u5b57\u7597\u6cd5\u5f00\u62d3\u4e86\u9053\u8def\u3002"}}
{"id": "2508.11685", "categories": ["eess.SP", "cond-mat.mtrl-sci", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11685", "abs": "https://arxiv.org/abs/2508.11685", "authors": ["Farnaz Kaboudvand", "Maham Khalid", "Nydia Assaf", "Vardaan Sahgal", "Jon P. Ruffley", "Brian J. McDermott"], "title": "Enhancing Corrosion Resistance of Aluminum Alloys Through AI and ML Modeling", "comment": "Manuscript length: 11 pages, 6 figures", "summary": "Corrosion poses a significant challenge to the performance of aluminum\nalloys, particularly in marine environments. This study investigates the\napplication of machine learning (ML) algorithms to predict and optimize\ncorrosion resistance, utilizing a comprehensive open-source dataset compiled\nfrom various sources. The dataset encompasses corrosion rate data and\nenvironmental conditions, preprocessed to standardize units and formats. We\nexplored two different approaches, a direct approach, where the material's\ncomposition and environmental conditions were used as inputs to predict\ncorrosion rates; and an inverse approach, where corrosion rate served as the\ninput to identify suitable material compositions as output. We employed and\ncompared three distinct ML methodologies for forward predictions: Random Forest\nregression, optimized via grid search; a feed-forward neural network, utilizing\nReLU activation and Adam optimization; and Gaussian Process Regression (GPR),\nimplemented with GPyTorch and employing various kernel functions. The Random\nForest and neural network models provided predictive capabilities based on\nelemental compositions and environmental conditions. Notably, Gaussian Process\nRegression demonstrated superior performance, particularly with hybrid kernel\nfunctions. Log-transformed GPR further refined predictions. This study\nhighlights the efficacy of ML, particularly GPR, in predicting corrosion rates\nand material properties.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u94dd\u5408\u91d1\u5728\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u8150\u8d1f\u901f\u7387\uff0c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u8868\u73b0\u6700\u4f18", "motivation": "\u94dd\u5408\u91d1\u5728\u6d77\u6d0b\u73af\u5883\u4e2d\u9762\u4e34\u4e25\u91cd\u8150\u8d1f\u6311\u6218\uff0c\u9700\u8981\u5feb\u901f\u9884\u6d4b\u548c\u4f18\u5316\u8150\u8d1f\u6027\u80fd", "method": "\u4f7f\u7528\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u91c7\u7528\u76f4\u63a5\u548c\u9006\u5411\u4e24\u79cd\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\u3001\u795e\u7ecf\u7f51\u7edc\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5", "result": "\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u8868\u73b0\u6700\u4f18\uff0c\u5c24\u5176\u662f\u4f7f\u7528\u6df7\u5408\u6838\u51fd\u6570\u65f6\uff0c\u5bf9\u6570\u53d8\u6362\u540e\u7684GPR\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6", "conclusion": "\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662fGPR\uff09\u5728\u9884\u6d4b\u8150\u8d1f\u901f\u7387\u548c\u6750\u6599\u6027\u80fd\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027"}}
{"id": "2508.11686", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11686", "abs": "https://arxiv.org/abs/2508.11686", "authors": ["Shuai Jiao", "Jian Fang", "Tianshu Zhou", "Jinsong Li", "Yanhong Liu", "Ye Liu", "Ming Ju"], "title": "The Lost-K and Shorter-J Phenomenon in Non-Standard Ballistocardiography Data", "comment": null, "summary": "Non-standard ballistocardiogram(BCG) data generally do not have prominent J\npeaks. This paper introduces two phenomena that reduce the prominence of\nJpeaks: the shorter-J phenomenon and the lost-K phenomenon, both of which are\ncommonly observed in non-standard BCG signals . This paper also proposes three\nsignal transformation methods that effectively improve the lost-K and shorter-J\nphenomena. The methods were evaluated on a time-aligned ECG-BCG dataset with 40\nsubjects. The results show that based on the transformed signal, simple\nJ-peak-based methods using only the detection of local maxima or minima show\nbetter performance in locating J-peaks and extracting BCG cycles, especially\nfor non-standard BCG data.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u975e\u6807\u51c6\u5fc3\u51b2\u56fe(BCG)\u4fe1\u53f7\u4e2dJ\u5cf0\u4e0d\u663e\u8457\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u73b0\u8c61\u548c\u4e09\u79cd\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86J\u5cf0\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u975e\u6807\u51c6BCG\u6570\u636e\u4e2dJ\u5cf0\u4e0d\u663e\u8457\uff0c\u5f71\u54cd\u4e86\u5fc3\u51b2\u56fe\u4fe1\u53f7\u7684\u5206\u6790\u548c\u5e94\u7528\u3002\u8bc6\u522b\u548c\u89e3\u51b3J\u5cf0\u4e0d\u663e\u8457\u7684\u95ee\u9898\u5bf9\u4e8e\u63d0\u9ad8BCG\u4fe1\u53f7\u5904\u7406\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u73b0\u8c61\uff1a\u77edJ\u73b0\u8c61\u548c\u5931K\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u4fe1\u53f7\u53d8\u6362\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e9b\u73b0\u8c61\u3002\u572840\u540d\u53d7\u8bd5\u8005\u7684\u65f6\u95f4\u5bf9\u9f50ECG-BCG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u53d8\u6362\u540e\u7684\u4fe1\u53f7\uff0c\u7b80\u5355\u7684J\u5cf0\u57fa\u7840\u65b9\u6cd5\uff08\u4ec5\u4f7f\u7528\u5c40\u90e8\u6781\u5927\u503c\u6216\u6781\u5c0f\u503c\u68c0\u6d4b\uff09\u5728\u5b9a\u4f4dJ\u5cf0\u548c\u63d0\u53bcG\u5468\u671f\u65b9\u9762\u663e\u793a\u51fa\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5bf9\u975e\u6807\u51c6BCG\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4fe1\u53f7\u53d8\u6362\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u6539\u5584\u975e\u6807\u51c6BCG\u4fe1\u53f7\u4e2dJ\u5cf0\u4e0d\u663e\u8457\u7684\u95ee\u9898\uff0c\u4e3a\u63d0\u9ad8BCG\u4fe1\u53f7\u5904\u7406\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11687", "categories": ["eess.SP", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.11687", "abs": "https://arxiv.org/abs/2508.11687", "authors": ["Jingpu Yang", "Mingxuan Cui", "Hang Zhang", "Fengxian Ji", "Zhengzhao Lai", "Yufeng Wang"], "title": "Agent-Based Anti-Jamming Techniques for UAV Communications in Adversarial Environments: A Comprehensive Survey", "comment": null, "summary": "Unmanned Aerial Vehicle communications are encountering increasingly severe\nmulti-source interference challenges in dynamic adversarial environments, which\nimpose higher demands on their reliability and resilience. To address these\nchallenges, agent-based autonomous anti-jamming techniques have emerged as a\ncrucial research direction. This paper presents a comprehensive survey that\nfirst formalizes the concept of intelligent anti-jamming agents for UAV\ncommunications and establishes a closed-loop decision-making framework centered\non the \"Perception-Decision-Action\" (P-D-A) paradigm. Within this framework, we\nsystematically review key technologies at each stage, with particular emphasis\non employing game theory to model UAV-jammer interactions and integrating\nreinforcement learning-based intelligent algorithms to derive adaptive\nanti-jamming strategies. Furthermore, we discuss potential limitations of\ncurrent approaches, identify critical engineering challenges, and outline\npromising future research directions, aiming to provide valuable references for\ndeveloping more intelligent and robust anti-jamming communication systems for\nUAVs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u65e0\u4eba\u673a\u901a\u4fe1\u4e2d\u667a\u80fd\u6297\u5e72\u6270\u6280\u672f\u7684\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\"\u611f\u77e5-\u51b3\u7b56-\u884c\u52a8\"\u8303\u5f0f\u7684\u95ed\u73af\u51b3\u7b56\u6846\u67b6\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u535a\u5f08\u8bba\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u6297\u5e72\u6270\u7b56\u7565\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u65e0\u4eba\u673a\u901a\u4fe1\u5728\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u9762\u4e34\u65e5\u76ca\u4e25\u91cd\u7684\u591a\u6e90\u5e72\u6270\u6311\u6218\uff0c\u5bf9\u901a\u4fe1\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u9700\u8981\u53d1\u5c55\u667a\u80fd\u5316\u7684\u81ea\u4e3b\u6297\u5e72\u6270\u6280\u672f\u3002", "method": "\u5efa\u7acb\u4ee5\"\u611f\u77e5-\u51b3\u7b56-\u884c\u52a8\"\u8303\u5f0f\u4e3a\u6838\u5fc3\u7684\u95ed\u73af\u51b3\u7b56\u6846\u67b6\uff0c\u7cfb\u7edf\u56de\u987e\u5404\u9636\u6bb5\u5173\u952e\u6280\u672f\uff0c\u91cd\u70b9\u91c7\u7528\u535a\u5f08\u8bba\u5efa\u6a21\u65e0\u4eba\u673a\u4e0e\u5e72\u6270\u5668\u4ea4\u4e92\uff0c\u5e76\u6574\u5408\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u7b97\u6cd5\u6765\u63a8\u5bfc\u81ea\u9002\u5e94\u6297\u5e72\u6270\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u667a\u80fd\u6297\u5e72\u6270\u4ee3\u7406\u6982\u5ff5\u548c\u6846\u67b6\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u76f8\u5173\u6280\u672f\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u548c\u9c81\u68d2\u7684\u65e0\u4eba\u673a\u6297\u5e72\u6270\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u53c2\u8003\u3002", "conclusion": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u6f5c\u5728\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u5de5\u7a0b\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u6709\u524d\u666f\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u65e0\u4eba\u673a\u667a\u80fd\u6297\u5e72\u6270\u901a\u4fe1\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2508.11691", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11691", "abs": "https://arxiv.org/abs/2508.11691", "authors": ["Mathis Rezzouk", "Fabrice Gagnon", "Alyson Champagne", "Mathieu Roy", "Philippe Albouy", "Michel-Pierre Coll", "Cem Subakan"], "title": "Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception", "comment": "6 pages, 2 figures, 2 tables, MLSP IEEE conference", "summary": "EEG-based analysis of pain perception, enhanced by machine learning, reveals\nhow the brain encodes pain by identifying neural patterns evoked by noxious\nstimulation. However, a major challenge that remains is the generalization of\nmachine learning models across individuals, given the high cross-participant\nvariability inherent to EEG signals and the limited focus on direct pain\nperception identification in current research. In this study, we systematically\nevaluate the performance of cross-participant generalization of a wide range of\nmodels, including traditional classifiers and deep neural classifiers for\nidentifying the sensory modality of thermal pain and aversive auditory\nstimulation from EEG recordings. Using a novel dataset of EEG recordings from\n108 participants, we benchmark model performance under both within- and\ncross-participant evaluation settings. Our findings show that traditional\nmodels suffered the largest drop from within- to cross-participant performance,\nwhile deep learning models proved more resilient, underscoring their potential\nfor subject-invariant EEG decoding. Even though performance variability\nremained high, the strong results of the graph-based model highlight its\npotential to capture subject-invariant structure in EEG signals. On the other\nhand, we also share the preprocessed dataset used in this study, providing a\nstandardized benchmark for evaluating future algorithms under the same\ngeneralization constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u88ab\u8bd5\u8111\u7535\u4fe1\u53f7\u75bc\u75db\u611f\u77e5\u8bc6\u522b\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u5728\u8de8\u88ab\u8bd5\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5c55\u73b0\u51fa\u6355\u83b7\u88ab\u8bd5\u4e0d\u53d8\u7279\u5f81\u7684\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eEEG\u7684\u75bc\u75db\u611f\u77e5\u7814\u7a76\u9762\u4e34\u8de8\u88ab\u8bd5\u6cdb\u5316\u7684\u91cd\u5927\u6311\u6218\uff0c\u7531\u4e8eEEG\u4fe1\u53f7\u5b58\u5728\u9ad8\u5ea6\u4e2a\u4f53\u5dee\u5f02\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u76f4\u63a5\u75bc\u75db\u611f\u77e5\u7684\u8de8\u88ab\u8bd5\u8bc6\u522b\u95ee\u9898\u3002", "method": "\u4f7f\u7528108\u540d\u88ab\u8bd5\u7684EEG\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4f20\u7edf\u5206\u7c7b\u5668\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u70ed\u75db\u548c\u538c\u6076\u542c\u89c9\u523a\u6fc0\u611f\u77e5\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u88ab\u8bd5\u5185\u548c\u8de8\u88ab\u8bd5\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u3002", "result": "\u4f20\u7edf\u6a21\u578b\u5728\u8de8\u88ab\u8bd5\u573a\u666f\u4e0b\u6027\u80fd\u4e0b\u964d\u6700\u663e\u8457\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5c55\u73b0\u51fa\u6355\u83b7\u88ab\u8bd5\u4e0d\u53d8EEG\u4fe1\u53f7\u7ed3\u6784\u7684\u6f5c\u529b\uff0c\u5c3d\u7ba1\u6027\u80fd\u53d8\u5f02\u4ecd\u7136\u8f83\u9ad8\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7279\u522b\u662f\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8de8\u88ab\u8bd5EEG\u89e3\u7801\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u9884\u5904\u7406\u6570\u636e\u96c6\u4f5c\u4e3a\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u4fc3\u8fdb\u672a\u6765\u7b97\u6cd5\u5728\u76f8\u540c\u6cdb\u5316\u7ea6\u675f\u4e0b\u7684\u8bc4\u4f30\u3002"}}
{"id": "2508.11692", "categories": ["eess.SP", "cs.AI", "68T07, 68T05", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11692", "abs": "https://arxiv.org/abs/2508.11692", "authors": ["Eduardo Di Santi", "Ruixiang Ci", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Jonathan Brown", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025, Dresden,\n  Germany. Conference: https://tu-dresden.de/raildresden2025. Book of\n  abstracts: https://tu-dresden.de/raildresden2025/BoA.pdf. 8 pages, 6 figures,\n  1 table", "summary": "The Point Machine (PM) is a critical piece of railway equipment that switches\ntrain routes by diverting tracks through a switchblade. As with any critical\nsafety equipment, a failure will halt operations leading to service\ndisruptions; therefore, pre-emptive maintenance may avoid unnecessary\ninterruptions by detecting anomalies before they become failures. Previous work\nrelies on several inputs and crafting custom features by segmenting the signal.\nThis not only adds additional requirements for data collection and processing,\nbut it is also specific to the PM technology, the installed locations and\noperational conditions limiting scalability. Based on the available maintenance\nrecords, the main failure causes for PM are obstacles, friction, power source\nissues and misalignment. Those failures affect the energy consumption pattern\nof PMs, altering the usual (or healthy) shape of the power signal during the PM\nmovement. In contrast to the current state-of-the-art, our method requires only\none input. We apply a deep learning model to the power signal pattern to\nclassify if the PM is nominal or associated with any failure type, achieving\n>99.99\\% precision, <0.01\\% false positives and negligible false negatives. Our\nmethodology is generic and technology-agnostic, proven to be scalable on\nseveral electromechanical PM types deployed in both real-world and test bench\nenvironments. Finally, by using conformal prediction the maintainer gets a\nclear indication of the certainty of the system outputs, adding a confidence\nlayer to operations and making the method compliant with the ISO-17359\nstandard.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ea\u9700\u5355\u4e00\u7535\u6e90\u4fe1\u53f7\u8f93\u5165\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u8f66\u8f86\u8f6c\u5411\u673a\u7684\u6545\u969c\uff0c\u8fbe\u5230\u4e86\u6781\u9ad8\u7684\u51c6\u786e\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8f66\u8f86\u8f6c\u5411\u673a\u4f5c\u4e3a\u94c1\u8def\u5173\u952e\u8bbe\u5907\uff0c\u6545\u969c\u4f1a\u5bfc\u81f4\u670d\u52a1\u4e2d\u65ad\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u591a\u79cd\u8f93\u5165\u548c\u7279\u5f81\u5de5\u7a0b\uff0c\u7279\u5b9a\u4e8e\u6280\u672f\u7c7b\u578b\u548c\u5b89\u88c5\u73af\u5883\uff0c\u53ef\u6269\u5c55\u6027\u5f88\u5dee\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u5206\u6790\u8f6c\u5411\u673a\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u7684\u7535\u6e90\u529f\u7387\u4fe1\u53f7\u6a21\u5f0f\uff0c\u8bc6\u522b\u5065\u5eb7\u72b6\u6001\u548c\u6545\u969c\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u9075\u5faa\u9884\u6d4b\u63d0\u4f9b\u4fe1\u5fc3\u5ea6\u8ba4\u8bc1\u3002", "result": "\u65b9\u6cd5\u8fbe\u5230\u4e86>99.99%\u7684\u7cbe\u786e\u5ea6\uff0c<0.01%\u7684\u5047\u6b63\u7387\uff0c\u5047\u9636\u7387\u53ef\u5ffd\u7565\u3002\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6d4b\u8bd5\u73af\u5883\u4e2d\u591a\u79cd\u7535\u673a\u68b0\u8f6c\u5411\u673a\u7c7b\u578b\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b80\u5316\u6570\u636e\u9700\u6c42\u548c\u63d0\u4f9b\u4fe1\u5fc3\u5ea6\u8ba4\u8bc1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u7b26\u5408ISO-17359\u6807\u51c6\uff0c\u5177\u6709\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.11693", "categories": ["eess.SP", "cs.AI", "cs.LG", "68T05, 68T10", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11693", "abs": "https://arxiv.org/abs/2508.11693", "authors": ["Francisco L\u00f3pez", "Eduardo Di Santi", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025\n  (International Conference on Railway Operations Modelling and Analysis),\n  Dresden, Germany", "summary": "Track Circuits (TC) are the main signalling devices used to detect the\npresence of a train on a rail track. It has been used since the 19th century\nand nowadays there are many types depending on the technology. As a general\nclassification, Track Circuits can be divided into 2 main groups, DC (Direct\nCurrent) and AC (Alternating Current) circuits. This work is focused on a\nparticular AC track circuit, called \"Smart Train Detection System\" (STDS),\ndesigned with both high and low-frequency bands. This approach uses STDS\ncurrent data applied to an SVM (support vector machine) classifier as a type of\nfailure identifier. The main purpose of this work consists on determine\nautomatically which is the component of the track that is failing to improve\nthe maintenance action. Model was trained to classify 15 different failures\nthat belong to 3 more general categories. The method was tested with field data\nfrom 10 different track circuits and validated by the STDS track circuit expert\nand maintainers. All use cases were correctly classified by the method.", "AI": {"tldr": "\u57fa\u4e8eSVM\u5206\u7c7b\u5668\u7684\u667a\u80fd\u8f68\u9053\u7535\u8def\u6545\u969c\u81ea\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u5206\u7c7b15\u79cd\u6545\u969c\u7c7b\u578b\uff0c\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387", "motivation": "\u4f20\u7edf\u8f68\u9053\u7535\u8def\u6545\u969c\u68c0\u6d4b\u4f9d\u9760\u4eba\u5de5\u7ecf\u9a8c\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u51c6\u786e\u8bc6\u522b\u6545\u969c\u7ec4\u4ef6\u4ee5\u6539\u5584\u7ef4\u62a4\u6548\u679c", "method": "\u91c7\u7528STDS\u7535\u6d41\u6570\u636e\uff0c\u4f7f\u7528SVM\u652f\u6301\u5411\u91cf\u673a\u5206\u7c7b\u5668\u8fdb\u884c\u6545\u969c\u8bc6\u522b\uff0c\u6a21\u578b\u8bad\u7ec3\u5206\u7c7b15\u79cd\u5c5e\u4e8e3\u4e2a\u4e3b\u8981\u7c7b\u522b\u7684\u6545\u969c", "result": "\u572810\u4e2a\u4e0d\u540c\u8f68\u9053\u7535\u8def\u7684\u73b0\u573a\u6570\u636e\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6240\u6709\u7528\u4f8b\u90fd\u88ab\u6b63\u786e\u5206\u7c7b\uff0c\u7ecfSTDS\u4e13\u5bb6\u548c\u7ef4\u62a4\u4eba\u5458\u9a8c\u8bc1", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u81ea\u52a8\u8bc6\u522b\u8f68\u9053\u7535\u8def\u6545\u969c\uff0c\u4e3a\u8f68\u9053\u4ea4\u901a\u5b89\u5168\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6545\u969c\u8bc6\u522b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11700", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11700", "abs": "https://arxiv.org/abs/2508.11700", "authors": ["Mesut Ko\u00e7yi\u011fit", "Bahman Javadi", "Russell Thomson", "Sebastian Pfautsch", "Oliver Obst"], "title": "Operational machine learning for park-scale irrigation to support urban cooling", "comment": "6 pages, 3 figures", "summary": "Urban parks can mitigate local heat, yet irrigation control is usually tuned\nfor water savings rather than cooling. We report on SIMPaCT (Smart Irrigation\nManagement for Parks and Cool Towns), a park-scale deployment that links\nper-zone soil-moisture forecasts to overnight irrigation set-points in support\nof urban cooling. SIMPaCT ingests data from 202 soil-moisture sensors, 50\ntemperature-relative humidity (TRH) nodes, and 13 weather stations, and trains\na per-sensor k-nearest neighbours (kNN) predictor on short rolling windows\n(200-900h). A rule-first anomaly pipeline screens missing and stuck-at signals,\nwith model-based checks (Isolation Forest and ARIMA). When a device fails, a\nmutual-information neighbourhood selects the most informative neighbour and a\nsmall multilayer perceptron supplies a \"virtual sensor\" until restoration.\nAcross sensors the mean absolute error was 0.78%, comparable to more complex\nbaselines; the upper-quartile error (P75) was lower for kNN than SARIMA (0.71%\nvs 0.93%). SIMPaCT runs daily and writes proposed set-points to the existing\ncontroller for operator review. This short communication reports an operational\nrecipe for robust, cooling-oriented irrigation at city-park scale.", "AI": {"tldr": "SIMPaCT\u662f\u4e00\u4e2a\u667a\u80fd\u704c\u6e89\u7cfb\u7edf\uff0c\u901a\u8fc7\u571f\u58e4\u6e7f\u5ea6\u9884\u6d4b\u4f18\u5316\u516c\u56ed\u704c\u6e89\u6765\u964d\u4f4e\u57ce\u5e02\u6e29\u5ea6\uff0c\u4f7f\u7528kNN\u7b97\u6cd5\u548c\u5f02\u5e38\u68c0\u6d4b\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e860.78%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002", "motivation": "\u4f20\u7edf\u516c\u56ed\u704c\u6e89\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u8282\u6c34\u800c\u975e\u964d\u6e29\u6548\u679c\uff0c\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\u9700\u8981\u66f4\u667a\u80fd\u7684\u704c\u6e89\u7ba1\u7406\u6765\u7f13\u89e3\u5c40\u90e8\u9ad8\u6e29\u3002", "method": "\u4f7f\u7528202\u4e2a\u571f\u58e4\u6e7f\u5ea6\u4f20\u611f\u5668\u300150\u4e2a\u6e29\u6e7f\u5ea6\u8282\u70b9\u548c13\u4e2a\u6c14\u8c61\u7ad9\u6570\u636e\uff0c\u91c7\u7528kNN\u9884\u6d4b\u7b97\u6cd5\u548c\u6eda\u52a8\u7a97\u53e3\u8bad\u7ec3\uff0c\u914d\u5408\u5f02\u5e38\u68c0\u6d4b\u548c\u865a\u62df\u4f20\u611f\u5668\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.78%\uff0c\u4f18\u4e8eSARIMA\u7b49\u590d\u6742\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0a\u56db\u5206\u4f4d\u8bef\u5dee\u66f4\u4f4e(0.71% vs 0.93%)\uff0c\u5df2\u5b9e\u73b0\u65e5\u5e38\u8fd0\u884c\u3002", "conclusion": "SIMPaCT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u57ce\u5e02\u516c\u56ed\u5c3a\u5ea6\u5b9e\u73b0\u7a33\u5065\u3001\u964d\u6e29\u5bfc\u5411\u704c\u6e89\u7684\u64cd\u4f5c\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8282\u6c34\u4e0e\u964d\u6e29\u7684\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2508.11790", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11790", "abs": "https://arxiv.org/abs/2508.11790", "authors": ["Oveys Delafrooz Noroozi", "Jiyoon Han", "Wei Tang", "Zhengya Zhang", "Upamanyu Madhow"], "title": "Scaling Wideband Massive MIMO Radar via Beamspace Dimension Reduction", "comment": null, "summary": "We present an architecture for scaling digital beamforming for wideband\nmassive MIMO radar. Conventional spatial processing becomes computationally\nprohibitive as array size grows; for example, the computational complexity of\nMVDR beamforming scales as O(N^3) for an N-element array. In this paper, we\nshow that energy concentration in beamspace provides the basis for drastic\ncomplexity reduction, with array scaling governed by the O(NlogN) complexity of\nthe spatial FFT used for beamspace transformation. Specifically, we propose an\narchitecture for windowed beamspace MVDR beamforming, parallelized across\ntargets and subbands, and evaluate its efficacy for beamforming and\ninterference suppression for government-supplied wideband radar data from the\nDARPA SOAP (Scalable On-Array Processing) program. We demonstrate that our\napproach achieves detection performance comparable to full-dimensional\nbenchmarks while significantly reducing computational and training overhead,\nand provide insight into tradeoffs between beamspace window size and FFT\nresolution in balancing complexity, detection accuracy, and interference\nsuppression.", "AI": {"tldr": "\u57fa\u4e8e\u675f\u7a7a\u95f4\u53d8\u6362\u7684\u7a97\u5316MVDR\u675f\u5f62\u6210\u67b6\u6784\uff0c\u901a\u8fc7\u7a7a\u95f4FFT\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(N^3)\u964d\u81f3O(NlogN)\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u548c\u8bad\u7ec3\u5f00\u9500", "motivation": "\u4f20\u7edf\u7a7a\u95f4\u5904\u7406\u5728\u5927\u89c4\u6a21\u6570\u7ec4\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff08MVDR\u675f\u5f62\u6210\u590d\u6742\u5ea6\u4e3aO(N^3)\uff09\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u6570\u5b57\u675f\u5f62\u6210\u65b9\u6848", "method": "\u5229\u7528\u675f\u7a7a\u95f4\u80fd\u91cf\u805a\u96c6\u7279\u6027\uff0c\u91c7\u7528\u7a97\u5316\u675f\u7a7a\u95f4MVDR\u675f\u5f62\u6210\u67b6\u6784\uff0c\u901a\u8fc7\u7a7a\u95f4FFT\u8fdb\u884c\u675f\u7a7a\u95f4\u53d8\u6362\uff0c\u5e76\u5728\u76ee\u6807\u5484\u5b50\u5e26\u4e0a\u5e76\u884c\u5316\u5904\u7406", "result": "\u5728DARPA SOAP\u7a0b\u5e8f\u7684\u5bbd\u5e26\u96f7\u8fbe\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u5168\u7ef4\u5ea6\u57fa\u51c6\u76f8\u4f3c\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5484\u8bad\u7ec3\u5f00\u9500", "conclusion": "\u675f\u7a7a\u95f4\u53d8\u6362\u4e3a\u5927\u89c4\u6a21MIMO\u96f7\u8fbe\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u5b57\u675f\u5f62\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7a7a\u95f4FFT\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u5e76\u9700\u5728\u7a97\u53e3\u5927\u5c0f\u4e0eFFT\u5206\u8fa8\u7387\u4e4b\u95f4\u627e\u5230\u590d\u6742\u5ea6\u3001\u68c0\u6d4b\u51c6\u786e\u6027\u5484\u5e72\u6270\u538b\u5236\u7684\u5e73\u8861\u70b9"}}
{"id": "2508.11792", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11792", "abs": "https://arxiv.org/abs/2508.11792", "authors": ["Daniel Sch\u00e4ufele", "Jochen Fink", "Renato L. G. Cavalcante", "S\u0142awomir Sta\u0144czak"], "title": "Digital Post-Distortion Architectures for Nonlinear Power Amplifiers: Volterra and Kernel Methods", "comment": null, "summary": "In modern 5G user equipments (UEs), the power amplifier (PA) contributes\nsignificantly to power consumption during uplink transmissions, especially in\ncell-edge scenarios. While reducing power backoff can enhance PA efficiency, it\nintroduces nonlinear distortions that degrade signal quality. Existing\nsolutions, such as digital pre-distortion, require complex feedback mechanisms\nfor optimal performance, leading to increased UE complexity and power\nconsumption. Instead, in this study we explore digital post-distortion (DPoD)\ntechniques, which compensate for these distortions at the base station,\nleveraging its superior computational resources. In this study, we conduct an\ncomprehensive study concerning the challenges and advantages associated with\napplying DPoD in time-domain, frequency-domain, and DFT-s-domain. Our findings\nsuggest that implementing DPoD in the time-domain, complemented by\nfrequency-domain channel equalization, strikes a good balance between low\ncomputational complexity and efficient nonlinearity compensation. In addition,\nwe demonstrate that memory has to be taken into account regardless of the\nmemory of the PA. Subsequently, we show how to pose the complex-valued problem\nof nonlinearity compensation in a real Hilbert space, emphasizing the potential\nperformance enhancements as a result. We then discuss the traditional Volterra\nseries and show an equivalent kernel method that can reduce algorithmic\ncomplexity. Simulations validate the results of our analysis and show that our\nproposed algorithm can significantly improve performance compared to\nstate-of-the-art algorithms.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u57285G\u7ec8\u7aef\u8bbe\u5907\u4e2d\u901a\u8fc7\u6570\u5b57\u540e\u7f6e\u6269\u5f55(DPoD)\u6280\u672f\u5728\u57fa\u7ad9\u7aef\u8865\u507f\u529f\u653e\u975e\u7ebf\u6027\u5931\u771f\u7684\u65b9\u6848\uff0c\u4ee5\u63d0\u9ad8\u529f\u6548\u548c\u964d\u4f4e\u7ec8\u7aef\u590d\u6742\u5ea6\u3002", "motivation": "5G\u7ec8\u7aef\u8bbe\u5907\u4e2d\u529f\u653e(PA)\u5728\u4e0a\u884c\u4f20\u8f93\u65f6\u8010\u8017\u8f83\u5927\uff0c\u51cf\u5c11\u529f\u7387\u540e\u9000\u867d\u80fd\u63d0\u9ad8\u6548\u7387\u4f46\u4f1a\u5f15\u5165\u975e\u7ebf\u6027\u5931\u771f\u3002\u73b0\u6709\u6570\u5b57\u524d\u7f6e\u6269\u5f55\u65b9\u6848\u9700\u8981\u590d\u6742\u53cd\u9988\u673a\u5236\uff0c\u589e\u52a0\u4e86\u7ec8\u7aef\u590d\u6742\u6027\u548c\u8017\u7535\u3002", "method": "\u7814\u7a76\u4e86\u5728\u65f6\u57df\u3001\u9891\u57df\u548cDFT-s\u57df\u5e94\u7528DPoD\u7684\u6311\u6218\u548c\u4f18\u52bf\uff0c\u63d0\u51fa\u5728\u65f6\u57df\u5b9e\u65bdDPoD\u5e76\u914d\u5408\u9891\u57df\u901a\u9053\u5747\u8861\u7684\u65b9\u6848\u3002\u8fd8\u63d0\u51fa\u4e86\u5c06\u590d\u6570\u503c\u975e\u7ebf\u6027\u8865\u507f\u95ee\u9898\u8f6c\u6362\u5230\u5b9e\u5e0c\u5c14\u4f2f\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u7b49\u6548\u5185\u6838\u65b9\u6cd5\u6765\u964d\u4f4e\u7b97\u6cd5\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u6548\u679c\u8d85\u8fc7\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u3002\u65f6\u57dfDPoD\u914d\u5408\u9891\u57df\u901a\u9053\u5747\u8861\u80fd\u5728\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u9ad8\u6548\u975e\u7ebf\u6027\u8865\u507f\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u5728\u57fa\u7ad9\u7aef\u5b9e\u65bdDPoD\u6280\u672f\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u57fa\u7ad9\u4f18\u8d8a\u7684\u8ba1\u7b97\u8d44\u6e90\u6765\u8865\u507f\u7ec8\u7aef\u529f\u653e\u7684\u975e\u7ebf\u6027\u5931\u771f\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u4fe1\u53f7\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u7ec8\u7aef\u7684\u590d\u6742\u6027\u548c\u8017\u7535\u3002"}}
{"id": "2508.12012", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12012", "abs": "https://arxiv.org/abs/2508.12012", "authors": ["Yi Wang", "Yingyang Chen", "Li Wang", "Donghong Cai", "Xiaofan Li", "Pingzhi Fan"], "title": "Autonomous Driving with RSMA-Enabled Finite Blocklength Transmissions: Ergodic Performance Analysis and Optimization", "comment": "This work has been accepted by IEEE Transactions on Wireless\n  Communications", "summary": "Rate-splitting multiple access (RSMA) is a key technology for next-generation\nmultiple access systems due to its robustness against imperfect channel state\ninformation (CSI). This makes RSMA particularly suitable for high-mobility\nautonomous driving, where ultra-reliable and low-latency communication (URLLC)\nis essential. To address the stringent requirements, this study enables RSMA\nfinite blocklength (FBL) transmissions and explicitly evaluates the ergodic\nperformance. We derive the closed-form lower bound for the ergodic sum-rate of\nRSMA, considering vital factors such as the vehicle velocities, vehicle\npositions, power allocation of each stream, blocklengths, and block error rates\n(BLERs). To further enhance the ergodic sum-rate while complying with quality\nof service (QoS) rate constraints, we jointly optimize the global power\ncoefficient, private power distribution, and common rate splitting. Guided by\ngradient descent, we first adjust the global power coefficient based on its\nsum-rate solution. This parameter regulates the power state of the common\nstream, allowing for dynamic activation or deactivation: if active, we optimize\nthe private power distribution and adjust the common rate splitting to meet\nminimum transmission constraints; if inactive, we use the sequential quadratic\nprogramming for private power distribution optimization. Simulation results\nconfirm that our RSMA scheme significantly improves the ergodic performance,\nreduces blocklength and BLER, surpassing the RSMA counterpart with average\nprivate power and space division multiple access (SDMA). Furthermore, our\napproach is validated to guarantee the rates for users with the poorest channel\nconditions, thereby enhancing fairness across the network.", "AI": {"tldr": "RSMA\u5728FBL\u4f20\u8f93\u4e0b\u7684\u6027\u80fd\u5206\u6790\u4e0e\u4f18\u5316\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u901f\u7387\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u904d\u5386\u548c\u901f\u7387\u3001\u964d\u4f4e\u5757\u957f\u5ea6\u548c\u8bef\u5757\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u7f51\u7edc\u516c\u5e73\u6027", "motivation": "\u9488\u5bf9\u9ad8\u79fb\u52a8\u6027\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2dURLLC\u7684\u4e25\u683c\u9700\u6c42\uff0cRSMA\u5bf9\u4e0d\u5b8c\u7f8eCSI\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6709\u9650\u5757\u957f\u5ea6\u4f20\u8f93\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u4f18\u5316\u95ee\u9898", "method": "\u63a8\u5bfcRSMA\u904d\u5386\u548c\u901f\u7387\u7684\u95ed\u5f0f\u4e0b\u754c\uff0c\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u8054\u5408\u4f18\u5316\u5168\u5c40\u529f\u7387\u7cfb\u6570\u3001\u79c1\u6709\u529f\u7387\u5206\u914d\u548c\u516c\u5171\u901f\u7387\u5206\u5272\uff0c\u6839\u636e\u516c\u5171\u6d41\u6fc0\u6d3b\u72b6\u6001\u91c7\u7528\u4e0d\u540c\u4f18\u5316\u7b56\u7565", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5RSMA\u65b9\u6848\u663e\u8457\u63d0\u5347\u904d\u5386\u6027\u80fd\uff0c\u51cf\u5c11\u5757\u957f\u5ea6\u548cBLER\uff0c\u4f18\u4e8e\u5e73\u5747\u79c1\u6709\u529f\u7387\u7684RSMA\u548cSDMA\uff0c\u540c\u65f6\u4fdd\u8bc1\u6700\u5dee\u4fe1\u9053\u6761\u4ef6\u7528\u6237\u7684\u901f\u7387", "conclusion": "\u6240\u63d0\u51fa\u7684RSMA FBL\u4f20\u8f93\u65b9\u6848\u6709\u6548\u6ee1\u8db3URLLC\u8981\u6c42\uff0c\u5728\u4fdd\u8bc1\u516c\u5e73\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u81ea\u52a8\u9a7e\u9a76\u901a\u4fe1\u573a\u666f"}}
{"id": "2508.12099", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12099", "abs": "https://arxiv.org/abs/2508.12099", "authors": ["Guangpu Guo", "Xiang-Gen Xia"], "title": "A Generalized Multidimensional Chinese Remainder Theorem (MD-CRT) for Multiple Integer Vectors", "comment": null, "summary": "Chinese remainder theorem (CRT) is widely applied in cryptography, coding\ntheory, and signal processing. It has been extended to the multidimensional CRT\n(MD-CRT), which reconstructs an integer vector from its vector remainders\nmodulo multiple integer matrices. This paper investigates a generalized MD-CRT\nfor multiple integer vectors, where the goal is to determine multiple integer\nvectors from multiple vector residue sets modulo multiple integer\nmatrices.Comparing to the existing generalized CRT for multiple scalar\nintegers, the challenge is that the moduli in MD-CRT are matrices that do not\ncommute and the corresponding uniquely determinable range is multidimensional\nand the inclusion relationship is much more complicated. In this paper,we\naddress two fundamental questions regarding the generalized MD-CRT. The first\nquestion concerns the uniquely determinable range of multiple integer vectors\nwhen no prior information about them is available. The second question is about\nthe conditions under which the maximal possible dynamic range can be\nachieved.To answer these two questions, we first derive a uniquely determinable\nrange without prior information and accordingly propose an algorithm to achieve\nit. A special case involving only two integer vectors is investigated for the\nsecond question, leading to a new condition for achieving the maximal possible\ndynamic range. Interestingly, this newly obtained condition, when the dimension\nis reduced to $1$, is even better than the existing ones for the conventional\ngeneralized CRT for scalar integers.These results may have applications for\nfrequency detection in multidimensional signal processing.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u7ef4\u4e2d\u56fd\u5269\u4f59\u5b9a\u7406\u7684\u666e\u904d\u5316\u95ee\u9898\uff0c\u63a2\u8ba8\u5982\u4f55\u4ece\u591a\u4e2a\u77e9\u9635\u6a21\u7684\u5411\u91cf\u4f59\u6570\u6062\u590d\u591a\u4e2a\u6574\u6570\u5411\u91cf\uff0c\u89e3\u51b3\u4e86\u552f\u4e00\u53ef\u786e\u5b9a\u8303\u56f4\u548c\u6700\u5927\u52a8\u6001\u8303\u56f4\u8fbe\u5230\u6761\u4ef6\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "motivation": "\u591a\u7ef4CRT\u5728\u52a0\u5bc6\u3001\u7f16\u7801\u548c\u4fe1\u53f7\u5904\u7406\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u666e\u904d\u5316\u65b9\u6cd5\u9762\u4e34\u77e9\u9635\u4e0d\u4ea4\u6362\u548c\u591a\u7ef4\u552f\u4e00\u786e\u5b9a\u8303\u56f4\u590d\u6742\u7684\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7406\u8bba\u7814\u7a76\u3002", "method": "\u9996\u5148\u6c42\u89e3\u65e0\u5148\u9a8c\u4fe1\u606f\u60c5\u51b5\u4e0b\u7684\u552f\u4e00\u53ef\u786e\u5b9a\u8303\u56f4\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7b97\u6cd5\u3002\u91cd\u70b9\u7814\u7a76\u4e86\u4ec5\u5305\u542b\u4e24\u4e2a\u6574\u6570\u5411\u91cf\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u63a8\u5bfc\u8fbe\u5230\u6700\u5927\u52a8\u6001\u8303\u56f4\u7684\u65b0\u6761\u4ef6\u3002", "result": "\u5f97\u5230\u4e86\u65e0\u5148\u9a8c\u4fe1\u606f\u60c5\u51b5\u4e0b\u7684\u552f\u4e00\u53ef\u786e\u5b9a\u8303\u56f4\uff0c\u4ee5\u53ca\u4e24\u5411\u91cf\u7279\u6b8a\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u5927\u52a8\u6001\u8303\u56f4\u7684\u65b0\u6761\u4ef6\u3002\u5f53\u7ef4\u5ea6\u964d\u4e3a1\u65f6\uff0c\u8be5\u6761\u4ef6\u751a\u81f3\u8d85\u8fc7\u4f20\u7edf\u666e\u904d\u5316CRT\u7684\u73b0\u6709\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u7ed3\u679c\u4e3a\u591a\u7ef4\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u9891\u7387\u68c0\u6d4b\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\uff0c\u5728\u591a\u7ef4CRT\u666e\u904d\u5316\u7406\u8bba\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.12106", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12106", "abs": "https://arxiv.org/abs/2508.12106", "authors": ["Hao Chen", "Rui Jin", "Dayuan Tan"], "title": "RFSS: A Comprehensive Multi-Standard RF Signal Source Separation Dataset with Advanced Channel Modeling", "comment": null, "summary": "The rapid evolution of wireless communication systems has created complex\nelectromagnetic environments where multiple cellular standards (2G/3G/4G/5G)\ncoexist, necessitating advanced signal source separation techniques. We present\nRFSS (RF Signal Source Separation), a comprehensive open-source dataset\ncontaining 52,847 realistic multi-standard RF signal samples with complete 3GPP\nstandards compliance. Our framework generates authentic baseband signals for\nGSM, UMTS, LTE, and 5G NR with advanced channel modeling including multipath\nfading, MIMO processing up to 8 by 8 antennas, and realistic interference\nscenarios. Experimental validation demonstrates superior performance of\nCNN-LSTM architectures achieving 26.7 dB SINR improvement in source separation\ntasks, significantly outperforming traditional ICA (15.2 dB) and NMF (18.3 dB)\napproaches. The RFSS dataset enables reproducible research in RF source\nseparation, cognitive radio, and machine learning applications while\nmaintaining complete open-source accessibility", "AI": {"tldr": "RFSS\u662f\u4e00\u4e2a\u5305\u542b52,847\u4e2a\u591a\u6807\u51c6RF\u4fe1\u53f7\u6837\u672c\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u652f\u63012G/3G/4G/5G\u4fe1\u53f7\u5206\u79bb\u7814\u7a76\uff0cCNN-LSTM\u67b6\u6784\u5728\u4fe1\u53f7\u6e90\u5206\u79bb\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u590d\u6742\u7535\u78c1\u73af\u5883\u4e2d\u591a\u79cd\u8702\u7a9d\u6807\u51c6\u5171\u5b58\uff0c\u9700\u8981\u5148\u8fdb\u7684\u4fe1\u53f7\u6e90\u5206\u79bb\u6280\u672f\u6765\u5904\u7406\u591a\u6807\u51c6RF\u4fe1\u53f7\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542bGSM\u3001UMTS\u3001LTE\u548c5G NR\u771f\u5b9e\u57fa\u5e26\u4fe1\u53f7\u7684RFSS\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5148\u8fdb\u4fe1\u9053\u5efa\u6a21\u5305\u62ec\u591a\u5f84\u8870\u843d\u30018\u00d78 MIMO\u5904\u7406\u548c\u771f\u5b9e\u5e72\u6270\u573a\u666f\u3002", "result": "CNN-LSTM\u67b6\u6784\u5728\u4fe1\u53f7\u6e90\u5206\u79bb\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8626.7 dB\u7684SINR\u6539\u5584\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfICA\uff0815.2 dB\uff09\u548cNMF\uff0818.3 dB\uff09\u65b9\u6cd5\u3002", "conclusion": "RFSS\u6570\u636e\u96c6\u4e3aRF\u4fe1\u53f7\u6e90\u5206\u79bb\u3001\u8ba4\u77e5\u65e0\u7ebf\u7535\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7814\u7a76\u7684\u57fa\u7840\uff0c\u5e76\u4fdd\u6301\u5b8c\u5168\u5f00\u6e90\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2508.12114", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12114", "abs": "https://arxiv.org/abs/2508.12114", "authors": ["Mustafa Gusaibat", "Mohammed Hnaish", "Abdelhamid Salem", "Khaled Rabie", "Zubair Md Fadlullah", "Wali Ullah Khan", "Mohamad A. Alawad", "Yazeed Alkhrijah"], "title": "Effect of Phase Shift Errors on the Security of UAV-assisted STAR-RIS IoT Networks", "comment": null, "summary": "Unmanned aerial vehicles (UAV)-mounted simultaneous transmitting and\nreflecting reconfigurable intelligent surface (STAR-RIS) systems can provide\nfull-dimensional coverage and flexible deployment opportunities in future\n6G-enabled IoT networks. However, practical imperfections such as jittering and\nairflow of UAV could affect the phase shift of STAR-RIS, and consequently\ndegrade network security. In this respect, this paper investigates the impact\nof phase shift errors on the secrecy performance of UAV-mounted\nSTAR-RIS-assisted IoT systems. More specifically, we consider a UAV-mounted\nSTAR-RIS-assisted non-orthogonal multiple access (NOMA) system where IoT\ndevices are grouped into two groups: one group on each side of the STAR-RIS.\nThe nodes in each group are considered as potential Malicious nodes for the\nones on the other side. By modeling phase estimation errors using a von Mises\ndistribution, analytical closed-form expressions for the ergodic secrecy rates\nunder imperfect phase adjustment are derived. An optimization problem to\nmaximize the weighted sum secrecy rate (WSSR) by optimizing the UAV placement\nis formulated and is then solved using a linear grid-based algorithm. Monte\nCarlo simulations are provided to validate the analytical derivations. The\nimpact of phase estimation errors on system's secrecy performance is analyzed,\nproviding critical insights for the practical realisation of STAR-RIS\ndeployments for secure UAV-enabled IoT networks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86UAV\u642d\u8f7dSTAR-RIS\u7cfb\u7edf\u4e2d\u76f8\u4f4d\u504f\u79fb\u9519\u8bef\u5bf9\u7f51\u7edc\u5b89\u5168\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5efa\u6a21\u5206\u6790\u548c\u4f18\u5316\u7b97\u6cd5\u63d0\u9ad8\u79d8\u5bc6\u901f\u7387\u3002", "motivation": "\u5b9e\u9645UAV\u7cfb\u7edf\u4e2d\u7684\u632f\u52a8\u548c\u6c14\u6d41\u7b49\u7f3a\u9677\u4f1a\u5f71\u54cdSTAR-RIS\u7684\u76f8\u4f4d\u504f\u79fb\uff0c\u4ece\u800c\u5bb9\u6613\u9020\u6210\u7f51\u7edc\u5b89\u5168\u6027\u80fd\u7684\u964d\u7ea7\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u5f71\u54cd\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528von Mises\u5206\u5e03\u6a21\u578b\u5316\u76f8\u4f4d\u4f30\u8ba1\u9519\u8bef\uff0c\u63a8\u5bfc\u4e86\u4e0d\u5b8c\u7f8e\u76f8\u4f4d\u8c03\u6574\u4e0b\u7684\u9053\u5e38\u79d8\u5bc6\u901f\u7387\u5206\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u7f51\u683c\u7b97\u6cd5\u4f18\u5316UAV\u4f4d\u7f6e\u4ee5\u6700\u5927\u5316\u6743\u91cd\u79d8\u5bc6\u901f\u7387\u548c\u3002", "result": "\u901a\u8fc7Monte Carlo\u6a21\u62df\u9a8c\u8bc1\u4e86\u5206\u6790\u63a8\u5bfc\u7684\u6b63\u786e\u6027\uff0c\u5206\u6790\u4e86\u76f8\u4f4d\u4f30\u8ba1\u9519\u8bef\u5bf9\u7cfb\u7edf\u5b89\u5168\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "\u8bba\u6587\u4e3aUAV\u642d\u8f7dSTAR-RIS\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u901a\u8fc7\u5bf9\u76f8\u4f4d\u9519\u8bef\u7684\u5b8c\u6574\u5206\u6790\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u79d8\u5bc6\u6027\u80fd\u3002"}}
{"id": "2508.12204", "categories": ["eess.SP", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.12204", "abs": "https://arxiv.org/abs/2508.12204", "authors": ["Mauro Belgiovine", "Suyash Pradhan", "Johannes Lange", "Michael L\u00f6hning", "Kaushik Chowdhury"], "title": "ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search", "comment": "Accepted at IEEE PIMRC 2025", "summary": "Industry adoption of Artificial Intelligence (AI)-native wireless receivers,\nor even modular, Machine Learning (ML)-aided wireless signal processing blocks,\nhas been slow. The main concern is the lack of explainability of these trained\nML models and the significant risks posed to network functionalities in case of\nfailures, especially since (i) testing on every exhaustive case is infeasible\nand (ii) the data used for model training may not be available. This paper\nproposes ATLAS, an AI-guided approach that generates a battery of tests for\npre-trained AI-native receiver models and benchmarks the performance against a\nclassical receiver architecture. Using gradient-based optimization, it avoids\nspanning the exhaustive set of all environment and channel conditions; instead,\nit generates the next test in an online manner to further probe specific\nconfigurations that offer the highest risk of failure. We implement and\nvalidate our approach by adopting the well-known DeepRx AI-native receiver\nmodel as well as a classical receiver using differentiable tensors in NVIDIA's\nSionna environment. ATLAS uncovers specific combinations of mobility, channel\ndelay spread, and noise, where fully and partially trained variants of\nAI-native DeepRx perform suboptimally compared to the classical receivers. Our\nproposed method reduces the number of tests required per failure found by 19%\ncompared to grid search for a 3-parameters input optimization problem,\ndemonstrating greater efficiency. In contrast, the computational cost of the\ngrid-based approach scales exponentially with the number of variables, making\nit increasingly impractical for high-dimensional problems.", "AI": {"tldr": "ATLAS\u662f\u4e00\u79cdAI\u5f15\u5bfc\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u53d1\u73b0AI\u539f\u751f\u65e0\u7ebf\u63a5\u6536\u5668\u6a21\u578b\u7684\u6545\u969c\u573a\u666f\uff0c\u76f8\u6bd4\u4f20\u7edf\u7f51\u683c\u641c\u7d22\u65b9\u6cd5\u6548\u7387\u63d0\u9ad819%", "motivation": "AI\u539f\u751f\u65e0\u7ebf\u63a5\u6536\u5668\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u65e0\u6cd5\u5728\u6240\u6709\u573a\u666f\u4e0b\u8fdb\u884c\u7a77\u5c3d\u6d4b\u8bd5\uff0c\u5b58\u5728\u7f51\u7edc\u529f\u80fd\u98ce\u9669", "method": "\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u7ebf\u751f\u6210\u9ad8\u98ce\u9669\u6545\u969c\u914d\u7f6e\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u907f\u514d\u7a77\u4e3e\u6240\u6709\u73af\u5883\u6761\u4ef6", "result": "\u53d1\u73b0\u4e86AI\u539f\u751fDeepRx\u63a5\u6536\u5668\u5728\u7279\u5b9a\u79fb\u52a8\u6027\u3001\u4fe1\u9053\u5ef6\u8fdf\u6269\u5c55\u548c\u566a\u58f0\u7ec4\u5408\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u6d4b\u8bd5\u6548\u7387\u6bd4\u7f51\u683c\u641c\u7d22\u9ad819%", "conclusion": "ATLAS\u4e3aAI\u65e0\u7ebf\u63a5\u6536\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6d4b\u8bd5\u7a7a\u95f4\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898"}}
{"id": "2508.12207", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12207", "abs": "https://arxiv.org/abs/2508.12207", "authors": ["Chenxin Tu", "Xiaowei Cui", "Gang Liu", "Mingquan Lu"], "title": "Weighted Covariance Intersection for Range-based Distributed Cooperative Localization of Multi-Agent Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Precise localization of multi-agent systems (MAS) in harsh environments is a\ncritical challenge for swarm applications, and cooperative localization is\nconsidered a key solution to this issue. Among all solutions, distributed\ncooperative localization (DCL) has garnered widespread attention due to its\nrobustness and scalability. The main challenge of DCL lies in how to fuse\nrelative measurements between agents under unknown correlations. To address\nthis, covariance intersection (CI) was introduced to DCL. However, the\nclassical CI optimization criteria suffer from issues such as scale imbalance\nand correlation mismatch during the fusion process. These deficiencies are not\nas pronounced in 2D scenarios, where the state space is relatively simple and\nthe observability of each state component is well. However, in 3D scenarios,\nwhere the state space is more complex and there are significant disparities in\nthe scale and observability of state components, performance degradation\nbecomes severe. This necessitates the design of specialized mechanisms to\nimprove the data fusion process. In this paper, we identify three main\ndrawbacks of the classical CI optimization criteria in recursive filtering and\nintroduce a weighting mechanism, namely weighted covariance intersection (WCI),\nto improve its performance. We then introduce WCI into range-based distributed\ncooperative localization in 3D scenarios, developing a concurrent fusion\nstrategy for multiple distance measurements and designing a weighting matrix\nbased on the error propagation rule of the inertial navigation system (INS).\nSimulation results demonstrate that the proposed WCI significantly enhances\ncooperative localization performance compared to classical CI, while the\ndistributed approach outperforms the centralized approach in terms of\nrobustness, scalability, and is more suitable for large-scale swarms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u52a0\u6743\u534f\u65b9\u5dee\u4ea4\u53c9(WCI)\u65b9\u6cd5\u6765\u6539\u8fdb3D\u5206\u5e03\u5f0f\u534f\u540c\u5b9a\u4f4d\u4e2d\u7684\u7ecf\u5178CI\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5c3a\u5ea6\u4e0d\u5e73\u8861\u548c\u76f8\u5173\u6027\u5931\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "3D\u573a\u666f\u4e2d\u7ecf\u5178CI\u4f18\u5316\u51c6\u5219\u5b58\u5728\u5c3a\u5ea6\u4e0d\u5e73\u8861\u548c\u76f8\u5173\u6027\u5931\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u5206\u5e03\u5f0f\u534f\u540c\u5b9a\u4f4d\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u9700\u8981\u8bbe\u8ba1\u4e13\u95e8\u673a\u5236\u6765\u6539\u8fdb\u6570\u636e\u878d\u5408\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u52a0\u6743\u534f\u65b9\u5dee\u4ea4\u53c9(WCI)\u673a\u5236\uff0c\u5f00\u53d1\u4e86\u591a\u8ddd\u79bb\u6d4b\u91cf\u7684\u5e76\u53d1\u878d\u5408\u7b56\u7565\uff0c\u5e76\u57fa\u4e8e\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u8bef\u5dee\u4f20\u64ad\u89c4\u5219\u8bbe\u8ba1\u6743\u91cd\u77e9\u9635\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u7ecf\u5178CI\uff0cWCI\u663e\u8457\u63d0\u5347\u4e86\u534f\u540c\u5b9a\u4f4d\u6027\u80fd\uff0c\u5206\u5e03\u5f0f\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u65b9\u6cd5\uff0c\u66f4\u9002\u5408\u5927\u89c4\u6a21\u96c6\u7fa4\u3002", "conclusion": "WCI\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u5206\u5e03\u5f0f\u534f\u540c\u5b9a\u4f4d\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u7cbe\u786e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12213", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12213", "abs": "https://arxiv.org/abs/2508.12213", "authors": ["Yize Cai", "Baoshen Guo", "Flora Salim", "Zhiqing Hong"], "title": "Towards Generalizable Human Activity Recognition: A Survey", "comment": null, "summary": "As a critical component of Wearable AI, IMU-based Human Activity Recognition\n(HAR) has attracted increasing attention from both academia and industry in\nrecent years. Although HAR performance has improved considerably in specific\nscenarios, its generalization capability remains a key barrier to widespread\nreal-world adoption. For example, domain shifts caused by variations in users,\nsensor positions, or environments can significantly decrease the performance in\npractice. As a result, in this survey, we explore the rapidly evolving field of\nIMU-based generalizable HAR, reviewing 229 research papers alongside 25\npublicly available datasets to provide a broad and insightful overview. We\nfirst present the background and overall framework of IMU-based HAR tasks, as\nwell as the generalization-oriented training settings. Then, we categorize\nrepresentative methodologies from two perspectives: (i) model-centric\napproaches, including pre-training method, end-to-end method, and large\nlanguage model (LLM)-based learning method; and (ii) data-centric approaches,\nincluding multi-modal learning and data augmentation techniques. In addition,\nwe summarize widely used datasets in this field, as well as relevant tools and\nbenchmarks. Building on these methodological advances, the broad applicability\nof IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent\nchallenges (e.g., data scarcity, efficient training, and reliable evaluation)\nand also outline future directions for HAR, including the adoption of\nfoundation and large language models, physics-informed and context-aware\nreasoning, generative modeling, and resource-efficient training and inference.\nThe complete list of this survey is available at\nhttps://github.com/rh20624/Awesome-IMU-Sensing, which will be updated\ncontinuously.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u57fa\u4e8eIMU\u7684\u53ef\u7a7f\u6234AI\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\u7684\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u6db5\u76d6\u4e86229\u7bc7\u7814\u7a76\u8bba\u6587\u548c25\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ece\u6a21\u578b\u4e2d\u5fc3\u548c\u6570\u636e\u4e2d\u5fc3\u4e24\u4e2a\u89d2\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u6301\u7eed\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u867d\u7136IMU-based HAR\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u662f\u963b\u788d\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\u3002\u7528\u6237\u3001\u4f20\u611f\u5668\u4f4d\u7f6e\u6216\u73af\u5883\u53d8\u5316\u5f15\u8d77\u7684\u9886\u57df\u504f\u79fb\u4f1a\u663e\u8457\u964d\u4f4e\u5b9e\u9645\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u6cdb\u5316\u6027\u95ee\u9898\u3002", "method": "\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5206\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\uff1a(i)\u6a21\u578b\u4e2d\u5fc3\u65b9\u6cd5\uff1a\u5305\u62ec\u9884\u8bad\u7ec3\u65b9\u6cd5\u3001\u7aef\u5230\u7aef\u65b9\u6cd5\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u65b9\u6cd5\uff1b(ii)\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\uff1a\u5305\u62ec\u591a\u6a21\u6001\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u540c\u65f6\u603b\u7ed3\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3001\u5de5\u5177\u548c\u57fa\u51c6\u3002", "result": "\u63d0\u4f9b\u4e86IMU-based\u6cdb\u5316HAR\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5efa\u7acb\u4e86\u65b9\u6cd5\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7684\u7814\u7a76\u73b0\u72b6\u548c\u6280\u672f\u8def\u7ebf\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u53c2\u8003\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u6301\u7eed\u6311\u6218\uff08\u5982\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u6548\u8bad\u7ec3\u548c\u53ef\u9760\u8bc4\u4f30\uff09\u5e76\u5c55\u671b\u4e86\u672a\u6765\u65b9\u5411\uff0c\u5305\u62ec\u91c7\u7528\u57fa\u7840\u548c\u5927\u8bed\u8a00\u6a21\u578b\u3001\u7269\u7406\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u751f\u6210\u5efa\u6a21\u4ee5\u53ca\u8d44\u6e90\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002"}}
{"id": "2508.12215", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12215", "abs": "https://arxiv.org/abs/2508.12215", "authors": ["Shuntian Tang", "Zesong Fei", "Xinyi Wang", "Dongkai Zhou", "Zhiqiang Wei", "Christos Masouros"], "title": "A Novel Symbol Level Precoding based AFDM Transmission Framework: Offloading Equalization Burden to Transmitter Side", "comment": "13 pages, 9 figures; submitted to IEEE journals for possible\n  publication", "summary": "Affine Frequency Division Multiplexing (AFDM) has attracted considerable\nattention for its robustness to Doppler effects. However, its high\nreceiver-side computational complexity remains a major barrier to practical\ndeployment. To address this, we propose a novel symbol-level precoding\n(SLP)-based AFDM transmission framework, which shifts the signal processing\nburden in downlink communications from user side to the base station (BS),\nenabling direct symbol detection without requiring channel estimation or\nequalization at the receiver. Specifically, in the uplink phase, we propose a\nSparse Bayesian Learning (SBL) based channel estimation algorithm by exploiting\nthe inherent sparsity of affine frequency (AF) domain channels. In particular,\nthe sparse prior is modeled via a hierarchical Laplace distribution, and\nparameters are iteratively updated using the Expectation-Maximization (EM)\nalgorithm. We also derive the Bayesian Cramer-Rao Bound (BCRB) to characterize\nthe theoretical performance limit. In the downlink phase, the BS employs the\nSLP technology to design the transmitted waveform based on the estimated uplink\nchannel state information (CSI) and channel reciprocity. The resulting\noptimization problem is formulated as a second-order cone programming (SOCP)\nproblem, and its dual problem is investigated by Lagrangian function and\nKarush-Kuhn-Tucker conditions. Simulation results demonstrate that the proposed\nSBL estimator outperforms traditional orthogonal matching pursuit (OMP) in\naccuracy and robustness to off-grid effects, while the SLP-based waveform\ndesign scheme achieves performance comparable to conventional AFDM receivers\nwhile significantly reducing the computational complexity at receiver,\nvalidating the practicality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u7ea7\u9884\u7f16\u7801\u7684AFDM\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5904\u7406\u8d1f\u62c5\u4ece\u7528\u6237\u7aef\u8f6c\u79fb\u5230\u57fa\u7ad9\u7aef\uff0c\u4f4e\u5904\u7406\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4f20\u7edf\u6027\u80fd\u3002", "motivation": "AFDM\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u591a\u666e\u52d2\u7f13\u51fb\u80fd\u529b\uff0c\u4f46\u5176\u9ad8\u7684\u63a5\u6536\u7aef\u8ba1\u7b97\u590d\u6742\u5ea6\u4ecd\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "\u4e0a\u884c\u94fe\u8def\u91c7\u7528\u57fa\u4e8e\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u9891\u57df\u901a\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u4e0b\u884c\u94fe\u8def\u91c7\u7528\u7b26\u53f7\u7ea7\u9884\u7f16\u7801\u6280\u672f\u8bbe\u8ba1\u53d1\u5c04\u6ce2\u5f62\uff0c\u901a\u8fc7\u4e8c\u9636\u9525\u89c4\u5212\u95ee\u9898\u6c42\u89e3\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u7a33\u5065\u6027\u65b9\u9762\u8d85\u8fc7\u4f20\u7edfOMP\u7b97\u6cd5\uff0c\u4e14\u80fd\u5728\u663e\u8457\u964d\u4f4e\u63a5\u6536\u7aef\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4f20\u7edf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86AFDM\u9ad8\u63a5\u6536\u7aef\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2508.12298", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12298", "abs": "https://arxiv.org/abs/2508.12298", "authors": ["Seungcheol Oh", "Han Han", "Joongheon Kim", "Sean Kwon"], "title": "Polarization Reconfigurable Transmit-Receive Beam Alignment with Interpretable Transformer", "comment": null, "summary": "Recent advancement in next generation reconfigurable antenna and fluid\nantenna technology has influenced the wireless system with polarization\nreconfigurable (PR) channels to attract significant attention for promoting\nbeneficial channel condition. We exploit the benefit of PR antennas by\nintegrating such technology into massive multiple-input-multiple-output (MIMO)\nsystem. In particular, we aim to jointly design the polarization and\nbeamforming vectors on both transceivers for simultaneous channel\nreconfiguration and beam alignment, which remarkably enhance the beamforming\ngain. However, joint optimization over polarization and beamforming vectors\nwithout channel state information (CSI) is a challenging task, since\ndepolarization increases the channel dimension; whereas massive MIMO systems\ntypically have low-dimensional pilot measurement from limited radio frequency\n(RF) chain. This leads to pilot overhead because the transceivers can only\nobserve low-dimensional measurement of the high-dimension channel. This paper\npursues the reduction of the pilot overhead in such systems by proposing to\nemploy \\emph{interpretable transformer}-based deep learning framework on both\ntransceivers to actively design the polarization and beamforming vectors for\npilot stage and transmission stage based on the sequence of accumulated\nreceived pilots. Numerical experiments demonstrate the significant performance\ngain of our proposed framework over the existing non-adaptive and active\ndata-driven methods. Furthermore, we exploit the interpretability of our\nproposed framework to analyze the learning capabilities of the model.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.12320", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12320", "abs": "https://arxiv.org/abs/2508.12320", "authors": ["Pengyu Wang", "Zhaocheng Wang", "Tianqi Mao", "Weijie Yuan", "Haijun Zhang", "George K. Karagiannidis"], "title": "Jamming Identification with Differential Transformer for Low-Altitude Wireless Networks", "comment": null, "summary": "Wireless jamming identification, which detects and classifies electromagnetic\njamming from non-cooperative devices, is crucial for emerging low-altitude\nwireless networks consisting of many drone terminals that are highly\nsusceptible to electromagnetic jamming. However, jamming identification schemes\nadopting deep learning (DL) are vulnerable to attacks involving carefully\ncrafted adversarial samples, resulting in inevitable robustness degradation. To\naddress this issue, we propose a differential transformer framework for\nwireless jamming identification. Firstly, we introduce a differential\ntransformer network in order to distinguish jamming signals, which overcomes\nthe attention noise when compared with its traditional counterpart by\nperforming self-attention operations in a differential manner. Secondly, we\npropose a randomized masking training strategy to improve network robustness,\nwhich leverages the patch partitioning mechanism inherent to transformer\narchitectures in order to create parallel feature extraction branches. Each\nbranch operates on a distinct, randomly masked subset of patches, which\nfundamentally constrains the propagation of adversarial perturbations across\nthe network. Additionally, the ensemble effect generated by fusing predictions\nfrom these diverse branches demonstrates superior resilience against\nadversarial attacks. Finally, we introduce a novel consistent training\nframework that significantly enhances adversarial robustness through dualbranch\nregularization. Simulation results demonstrate that our proposed methodology is\nsuperior to existing methods in boosting robustness to adversarial samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u53d8\u6362\u5668\u7684\u65e0\u7ebf\u5e72\u6270\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5dee\u5206\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u968f\u673a\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027", "motivation": "\u65e0\u4eba\u673a\u7b49\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u6613\u53d7\u7535\u78c1\u5e72\u6270\uff0c\u800c\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u5e72\u6270\u8bc6\u522b\u65b9\u6848\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6837\u672c\u653b\u51fb\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0b\u964d", "method": "\u91c7\u7528\u5dee\u5206\u53d8\u6362\u5668\u7f51\u7edc\u8fdb\u884c\u5e72\u6270\u4fe1\u53f7\u8bc6\u522b\uff0c\u63d0\u51fa\u968f\u673a\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u521b\u5efa\u5e76\u884c\u7279\u5f81\u63d0\u53d6\u5206\u652f\uff0c\u5e76\u5f15\u5165\u53cc\u5206\u652f\u6b63\u5219\u5316\u7684\u4e00\u81f4\u6027\u8bad\u7ec3\u6846\u67b6", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u5347\u5bf9\u6297\u6837\u672c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u5dee\u5206\u53d8\u6362\u5668\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u65e0\u7ebf\u5e72\u6270\u8bc6\u522b\u7cfb\u7edf\u5bf9\u6297\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027"}}
{"id": "2508.12371", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12371", "abs": "https://arxiv.org/abs/2508.12371", "authors": ["Lin Wang", "Zhiqing Wei", "Xu Chen", "Zhiyong Feng"], "title": "Coherent Compensation-Based Sensing for Long-Range Targets in Integrated Sensing and Communication System", "comment": "15 pages, 10 figures", "summary": "Integrated sensing and communication (ISAC) is a promising candidate\ntechnology for 6G due to its improvement in spectral efficiency and energy\nefficiency. Orthogonal frequency division multiplexing (OFDM) signal is a\nmainstream candidate ISAC waveform. However, there are inter-symbol\ninterference (ISI) and inter-carrier interference (ICI) when the round-trip\ndelay exceeds the cyclic prefix (CP) duration for OFDM signals, which limits\nthe maximum sensing range of ISAC system. When detecting a long-range target,\nthe wide beam inevitably covers the close-range target, of which the echo's\npower is much larger than that of the long-range target. In order to tackle the\nabove problem, a multiple signal classification (MUSIC) and least squares\n(LS)-based spatial signal separation method is proposed to separate the echo\nsignals reflected from different targets. Moreover, a coherent\ncompensation-based sensing signal processing method at the receiver is proposed\nto enhance the signal to interference plus noise power ratio (SINR) of the OFDM\nblock for generating the range-Doppler map (RDM) with higher SINR. Simulation\nresults reveal that the proposed method greatly enhances the SINR of RDM by 10\ndB for a target at 500 m compared with two-dimensional fast Fourier transform\n(2D-FFT) method. Besides, the detection probability is also significantly\nimproved compared to the benchmarking method.", "AI": {"tldr": "\u57fa\u4e8eMUSIC\u548c\u6700\u5c0f\u4e8c\u4e58\u7684\u7a7a\u95f4\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\uff0c\u89e3\u51b3OFDM\u4fe1\u53f7\u5728\u8d85\u51facyclic prefix\u65f6\u95f4\u7684\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u5347\u8fdc\u7a0b\u76ee\u6807\u68c0\u6d4b\u6027\u80fd", "motivation": "\u89e3\u51b3ISAC\u7cfb\u7edf\u4e2dOFDM\u4fe1\u53f7\u5728\u8fdc\u7a0b\u76ee\u6807\u68c0\u6d4b\u65f6\u7684\u95ee\u9898\uff1a\u5faa\u73af\u524d\u7f00\u8d85\u65f6\u5bfc\u81f4\u7684\u7801\u95f4\u5e72\u6270\u548c\u8f7d\u6ce2\u5e72\u6270\uff0c\u4ee5\u53ca\u8fd1\u8ddd\u79bb\u76ee\u6807\u5f3a\u56de\u6ce2\u5bf9\u8fdc\u8ddd\u79bb\u76ee\u6807\u7684\u5a92\u4ecb\u5e72\u6270", "method": "\u63d0\u51fa\u57fa\u4e8eMUSIC\u548c\u6700\u5c0f\u4e8c\u4e58\u7684\u7a7a\u95f4\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\uff0c\u5206\u79bb\u4e0d\u540c\u76ee\u6807\u7684\u56de\u6ce2\u4fe1\u53f7\uff1b\u91c7\u7528\u76f8\u5e72\u8865\u507f\u57fa\u7840\u7684\u611f\u77e5\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u5347SINR\u6765\u751f\u6210\u66f4\u9ad8SINR\u7684\u8ddd\u79bb-\u591a\u666e\u52d2\u56fe", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u5728500\u7c73\u8fdc\u7a0b\u76ee\u6807\u4e0a\uff0c\u65b9\u6cd5\u6bd4\u4f20\u7edf\u4e8c\u7ef4\u5feb\u901f\u5f17\u91cc\u53f6\u53d8\u6362\u65b9\u6cd5\u63d0\u5347\u4e8610dB\u7684SINR\uff0c\u68c0\u6d4b\u6982\u7387\u4e5f\u663e\u8457\u63d0\u9ad8", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ISAC\u7cfb\u7edf\u4e2d\u8fdc\u7a0b\u76ee\u6807\u68c0\u6d4b\u7684\u5e72\u6270\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u611f\u77e5\u6027\u80fd\uff0c\u4e3a6G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12614", "categories": ["eess.SP", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12614", "abs": "https://arxiv.org/abs/2508.12614", "authors": ["Zhongqin Wang", "J. Andrew Zhang", "Kai Wu", "Min Xu", "Y. Jay Guo"], "title": "Towards SISO Bistatic Sensing for ISAC", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) is a key enabler for\nnext-generation wireless systems. However, real-world deployment is often\nlimited to low-cost, single-antenna transceivers. In such bistatic Single-Input\nSingle-Output (SISO) setup, clock asynchrony introduces random phase offsets in\nChannel State Information (CSI), which cannot be mitigated using conventional\nmulti-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic\nSISO sensing framework that enables accurate delay and Doppler estimation from\ndistorted CSI by effectively suppressing Doppler mirroring ambiguity. It\noperates with only a single antenna at both the transmitter and receiver,\nmaking it suitable for low-complexity deployments. We propose a\nself-referencing cross-correlation (SRCC) method for SISO random phase removal\nand employ delay-domain beamforming to resolve Doppler ambiguity. The resulting\nunambiguous delay-Doppler-time features enable robust sensing with compact\nneural networks. Extensive experiments show that WiDFS 3.0 achieves accurate\nparameter estimation, with performance comparable to or even surpassing that of\nprior multi-antenna methods, especially in delay estimation. Validated under\nsingle- and multi-target scenarios, the extracted ambiguity-resolved features\nshow strong sensing accuracy and generalization. For example, when deployed on\nthe embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0\nconsistently outperforms conventional features such as CSI amplitude, mirrored\nDoppler, and multi-receiver aggregated Doppler.", "AI": {"tldr": "WiDFS 3.0\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53cc\u57fa\u5730SISO\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u53c2\u8003\u4e92\u76f8\u5173\u65b9\u6cd5\u548c\u5ef6\u8fdf\u57df\u6ce2\u675f\u6210\u5f62\u6280\u672f\uff0c\u5728\u5355\u5929\u7ebf\u6536\u53d1\u5668\u8bbe\u7f6e\u4e0b\u6709\u6548\u6291\u5236\u591a\u666e\u52d2\u955c\u50cf\u6a21\u7cca\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u4f30\u8ba1\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u53d7\u9650\u4e8e\u4f4e\u6210\u672c\u5355\u5929\u7ebf\u6536\u53d1\u5668\u3002\u5728\u53cc\u57fa\u5730SISO\u8bbe\u7f6e\u4e2d\uff0c\u65f6\u949f\u4e0d\u540c\u6b65\u4f1a\u5728CSI\u4e2d\u5f15\u5165\u968f\u673a\u76f8\u4f4d\u504f\u79fb\uff0c\u4f20\u7edf\u591a\u5929\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u53c2\u8003\u4e92\u76f8\u5173(SRCC)\u65b9\u6cd5\u7528\u4e8eSISO\u968f\u673a\u76f8\u4f4d\u53bb\u9664\uff0c\u5e76\u91c7\u7528\u5ef6\u8fdf\u57df\u6ce2\u675f\u6210\u5f62\u6280\u672f\u6765\u89e3\u51b3\u591a\u666e\u52d2\u6a21\u7cca\u95ee\u9898\u3002\u4f7f\u7528\u7d27\u51d1\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u65e0\u6a21\u7cca\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2-\u65f6\u95f4\u7279\u5f81\u3002", "result": "WiDFS 3.0\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u6027\u80fd\u4e0e\u751a\u81f3\u4f18\u4e8e\u5148\u524d\u7684\u591a\u5929\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5ef6\u8fdf\u4f30\u8ba1\u65b9\u9762\u3002\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\u4e0b\u9a8c\u8bc1\uff0c\u63d0\u53d6\u7684\u7279\u5f81\u663e\u793a\u51fa\u5f3a\u5927\u7684\u611f\u77e5\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ec5\u9700\u5355\u5929\u7ebf\u6536\u53d1\u5668\uff0c\u9002\u5408\u4f4e\u590d\u6742\u5ea6\u90e8\u7f72\uff0c\u5728\u5d4c\u5165\u5f0f\u53cb\u597d\u7684MobileViT-XXS\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6301\u7eed\u4f18\u4e8eCSI\u5e45\u5ea6\u3001\u955c\u50cf\u591a\u666e\u52d2\u548c\u591a\u63a5\u6536\u5668\u805a\u5408\u591a\u666e\u52d2\u7b49\u4f20\u7edf\u7279\u5f81\u3002"}}
{"id": "2508.12660", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12660", "abs": "https://arxiv.org/abs/2508.12660", "authors": ["Yezhuo Zhang", "Zinan Zhou", "Guangyu Li", "Xuanpeng Li"], "title": "Factorized Disentangled Representation Learning for Interpretable Radio Frequency Fingerprint", "comment": "14 pages, 8 figures", "summary": "In response to the rapid growth of Internet of Things (IoT) devices and\nrising security risks, Radio Frequency Fingerprint (RFF) has become key for\ndevice identification and authentication. However, various changing factors -\nbeyond the RFF itself - can be entangled from signal transmission to reception,\nreducing the effectiveness of RFF Identification (RFFI). Existing RFFI methods\nmainly rely on domain adaptation techniques, which often lack explicit factor\nrepresentations, resulting in less robustness and limited controllability for\ndownstream tasks. To tackle this problem, we propose a novel Disentangled\nRepresentation Learning (DRL) framework that learns explicit and independent\nrepresentations of multiple factors, including the RFF. Our framework\nintroduces modules for disentanglement, guided by the principles of\nexplicitness, modularity, and compactness. We design two dedicated modules for\nfactor classification and signal reconstruction, each with tailored loss\nfunctions that encourage effective disentanglement and enhance support for\ndownstream tasks. Thus, the framework can extract a set of interpretable\nvectors that explicitly represent corresponding factors. We evaluate our\napproach on two public benchmark datasets and a self-collected dataset. Our\nmethod achieves impressive performance on multiple DRL metrics. We also analyze\nthe effectiveness of our method on downstream RFFI task and conditional signal\ngeneration task. All modules of the framework contribute to improved\nclassification accuracy, and enable precise control over conditional generated\nsignals. These results highlight the potential of our DRL framework for\ninterpretable and explicit RFFs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u7f20\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u660e\u786e\u72ec\u7acb\u7684\u591a\u56e0\u7d20\u8868\u5f81\u6765\u63d0\u9ad8\u65e0\u7ebf\u8bbe\u5907\u8bc6\u522b\u7684\u7a33\u5065\u6027\u548c\u53ef\u63a7\u6027", "motivation": "\u5e94\u5bf9IoT\u8bbe\u5907\u5feb\u901f\u589e\u957f\u548c\u5b89\u5168\u98ce\u9669\uff0c\u65e0\u7ebf\u7535\u989c\u7eb9\u8bc6\u522b\u5b58\u5728\u56e0\u7d20\u6df7\u5408\u95ee\u9898\uff0c\u73b0\u6709\u57df\u9002\u914d\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u56e0\u7d20\u8868\u5f81\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6548\u679c", "method": "\u8bbe\u8ba1\u4e86\u89e3\u7f20\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u56e0\u7d20\u5206\u7c7b\u548c\u4fe1\u53f7\u91cd\u5efa\u4e13\u95e8\u6a21\u5757\uff0c\u91c7\u7528\u7279\u5236\u635f\u5931\u51fd\u6570\u4fc3\u8fdb\u89e3\u7f20\u548c\u652f\u6301\u4e0b\u6e38\u4efb\u52a1", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4f18\u5f02\u6027\u80fd\uff0c\u6240\u6709\u6a21\u5757\u90fd\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u6761\u4ef6\u751f\u6210\u4fe1\u53f7", "conclusion": "\u8be5DRL\u6846\u67b6\u4e3a\u89e3\u91ca\u6027\u548c\u660e\u786e\u7684\u65e0\u7ebf\u7535\u989c\u7eb9\u8868\u5f81\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u901a\u8fc7\u56e0\u7d20\u89e3\u7f20\u63d0\u5347\u4e86\u8bbe\u5907\u8bc6\u522b\u7684\u7a33\u5065\u6027\u548c\u53ef\u63a7\u6027"}}
{"id": "2508.12689", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12689", "abs": "https://arxiv.org/abs/2508.12689", "authors": ["Ning Gao", "Tianrui Zeng", "Bowen Chen", "Donghong Cai", "Shi Jin", "Michail Matthaiou"], "title": "Multi-Domain Supervised Contrastive Learning for UAV Radio-Frequency Open-Set Recognition", "comment": null, "summary": "5G-Advanced (5G-A) has enabled the vibrant development of low altitude\nintegrated sensing and communication (LA-ISAC) networks. As a core component of\nthese networks, unmanned aerial vehicles (UAVs) have witnessed rapid growth in\nrecent years. However, due to the lag in traditional industry regulatory norms,\nunauthorized flight incidents occur frequently, posing a severe security threat\nto LA-ISAC networks. To surveil the non-cooperative UAVs, in this paper, we\npropose a multi-domain supervised contrastive learning (MD-SupContrast)\nframework for UAV radio frequency (RF) open-set recognition. Specifically,\nfirst, the texture features and the time-frequency position features from the\nResNet and the TransformerEncoder are fused, and then the supervised\ncontrastive learning is applied to optimize the feature representation of the\nclosed-set samples. Next, to surveil the invasive UAVs that appear in real\nlife, we propose an improved generative OpenMax (IG-OpenMax) algorithm and\nconstruct an open-set recognition model, namely Open-RFNet. According to the\nunknown samples, we first freeze the feature extraction layers and then only\nretrain the classification layer, which achieves excellent recognition\nperformance both in closed-set and open-set recognitions. We analyze the\ncomputational complexity of the proposed model. Experiments are conducted with\na large-scale UAV open dataset. The results show that the proposed Open-RFNet\noutperforms the existing benchmark methods in terms of recognition accuracy\nbetween the known and the unknown UAVs, as it achieves 95.12% in closed-set and\n96.08% in open-set under 25 UAV types, respectively.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u57df\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u6539\u8fdb\u751f\u6210\u5f0fOpenMax\u7b97\u6cd5\u7684\u65b0\u9898\u65b9\u6cd5Open-RFNet\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5f00\u653e\u96c6\u8bc6\u522b\uff0c\u5728\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u8bc6\u522b\u4e2d\u90fd\u53d6\u5f97\u4e86\u8d85\u8fc795%\u7684\u9ad8\u7cbe\u5ea6", "motivation": "\u89e3\u51b35G-A\u4f4e\u7a7a\u57df\u611f\u77e5\u901a\u4fe1\u7f51\u7edc\u4e2d\u975e\u6cd5\u65e0\u4eba\u673a\u98de\u884c\u5e26\u6765\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u76d1\u63a7\u975e\u5408\u4f5c\u65e0\u4eba\u673a", "method": "\u7ed3\u5408ResNet\u548cTransformerEncoder\u878d\u5408\u7eb9\u7406\u7279\u5f81\u548c\u65f6\u9891\u4f4d\u7f6e\u7279\u5f81\uff0c\u91c7\u7528\u76d1\u7763\u5bf0\u6bd4\u5b66\u4e60\u4f18\u5316\u7279\u5f81\u8868\u5f81\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u751f\u6210\u5f0fOpenMax\u7b97\u6cd5IG-OpenMax\u6784\u5efaOpen-RFNet\u6a21\u578b", "result": "\u572825\u79cd\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\uff0c\u5c01\u95ed\u96c6\u8bc6\u522b\u7cbe\u5ea6\u8fbe\u95f895.12%\uff0c\u5f00\u653e\u96c6\u8bc6\u522b\u7cbe\u5ea6\u8fbe\u95f896.08%\uff0c\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5", "conclusion": "Open-RFNet\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u76d1\u63a7\u975e\u5408\u4f5c\u65e0\u4eba\u673a\uff0c\u4e3a5G-A\u4f4e\u7a7a\u57df\u611f\u77e5\u901a\u4fe1\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c"}}
{"id": "2508.12728", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12728", "abs": "https://arxiv.org/abs/2508.12728", "authors": ["Yunsong Huang", "Hui-Ming Wang", "Qingli Yan", "Zhaowei Wang"], "title": "LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent Metasurface Antenna Systems", "comment": null, "summary": "The evolution of 6G networks demands ultra-massive connectivity and\nintelligent radio environments, yet existing reconfigurable intelligent surface\n(RIS) technologies face critical limitations in hardware efficiency, dynamic\ncontrol, and scalability. This paper introduces LLM-RIMSA, a transformative\nframework that integrates large language models (LLMs) with a novel\nreconfigurable intelligent metasurface antenna (RIMSA) architecture to address\nthese challenges. Unlike conventional RIS designs, RIMSA employs parallel\ncoaxial feeding and 2D metasurface integration, enabling each individual\nmetamaterial element to independently adjust both its amplitude and phase.\nWhile traditional optimization and deep learning (DL) methods struggle with\nhigh-dimensional state spaces and prohibitive training costs for RIMSA control,\nLLM-RIMSA leverages pre-trained LLMs cross-modal reasoning and few-shot\nlearning capabilities to dynamically optimize RIMSA configurations. Simulations\ndemonstrate that LLM-RIMSA achieves state-of-the-art performance, outperforming\nconventional DL-based methods in sum rate while reducing training overhead. The\nproposed framework pave the way for LLM-driven intelligent radio environments.", "AI": {"tldr": "LLM-RIMSA\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u65b0\u578b\u53ef\u91cd\u6784\u667a\u80fd\u8d85\u8868\u9762\u5929\u7ebf\u67b6\u6784\uff0c\u901a\u8fc7LLM\u7684\u8de8\u6a21\u6001\u63a8\u7406\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u52a8\u6001\u4f18\u5316\u5929\u7ebf\u914d\u7f6e\uff0c\u57286G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u8fde\u63a5\u548c\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u6280\u672f\u5728\u786c\u4ef6\u6548\u7387\u3001\u52a8\u6001\u63a7\u5236\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\uff0c\u6ee1\u8db36G\u7f51\u7edc\u5bf9\u8d85\u5927\u89c4\u6a21\u8fde\u63a5\u548c\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faLLM-RIMSA\u6846\u67b6\uff0c\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u65b0\u578bRIMSA\u67b6\u6784\uff08\u91c7\u7528\u5e76\u884c\u540c\u8f74\u9988\u7535\u548c2D\u8d85\u8868\u9762\u96c6\u6210\uff09\uff0c\u4f7f\u6bcf\u4e2a\u8d85\u6750\u6599\u5355\u5143\u80fd\u72ec\u7acb\u8c03\u6574\u5e45\u5ea6\u548c\u76f8\u4f4d\uff0c\u5229\u7528LLM\u7684\u8de8\u6a21\u6001\u63a8\u7406\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u8fdb\u884c\u52a8\u6001\u4f18\u5316\u3002", "result": "\u4eff\u771f\u663e\u793aLLM-RIMSA\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u603b\u901f\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f18\u5316\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u8bad\u7ec3\u6210\u672c\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2508.12746", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12746", "abs": "https://arxiv.org/abs/2508.12746", "authors": ["Muhammad Ammad", "Paul Schwarzbach", "Michael Schultz", "Oliver Michler"], "title": "Range-Angle Likelihood Maps for Indoor Positioning Using Deep Neural Networks", "comment": null, "summary": "Accurate and high precision of the indoor positioning is as important as\nensuring reliable navigation in outdoor environments. Using the\nstate-of-the-art deep learning models provides better reliability and accuracy\nto navigate and monitor the accurate positions in the aircraft cabin\nenvironment. We utilize the simulated aircraft cabin environment measurements\nand propose a residual neural network (ResNet) model to predict the accurate\npositions inside the cabin. The measurements include the ranges and angles\nbetween a tag and the anchors points which are then mapped onto a grid as range\nand angle residuals. These residual maps are then transformed into the\nlikelihood grid maps where each cell of the grid shows the likelihood of being\na true location. These grid maps along with the true positions are then passed\nas inputs to train the ResNet model. Since any deep learning model involve\nnumerous parameter settings, hyperparameter optimization is performed to get\nthe optimal parameters for training the model effectively with the highest\naccuracy. Once we get the best hyperparameters settings of the model, it is\nthen trained to predict the positions which provides a centimeter-level\naccuracy of the localization.", "AI": {"tldr": "\u4f7f\u7528ResNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u62df\u673a\u8231\u73af\u5883\u4e2d\u5b9e\u73b0\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5ba4\u5185\u5b9a\u4f4d\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u83b7\u5f97\u6700\u4f73\u6027\u80fd", "motivation": "\u5ba4\u5185\u7cbe\u786e\u5b9a\u4f4d\u5728\u673a\u8231\u73af\u5883\u4e2d\u4e0e\u5ba4\u5916\u5bfc\u822a\u540c\u6837\u91cd\u8981\uff0c\u9700\u8981\u9ad8\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u7684\u5b9a\u4f4d\u6280\u672f\u6765\u786e\u4fdd\u5bfc\u822a\u548c\u76d1\u63a7", "method": "\u5229\u7528\u6a21\u62df\u673a\u8231\u73af\u5883\u6d4b\u91cf\u6570\u636e\uff0c\u5c06\u6807\u7b7e\u4e0e\u951a\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\u548c\u89d2\u5ea6\u6620\u5c04\u4e3a\u6b8b\u5dee\u7f51\u683c\uff0c\u8f6c\u6362\u4e3a\u4f3c\u7136\u7f51\u683c\u56fe\uff0c\u4f7f\u7528ResNet\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u4f4d\u7f6e\u9884\u6d4b\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316", "result": "\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u83b7\u5f97\u6700\u4f73\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u51c6\u786e\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eResNet\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u673a\u8231\u73af\u5883\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u5ba4\u5185\u5b9a\u4f4d\u95ee\u9898"}}
{"id": "2508.12892", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12892", "abs": "https://arxiv.org/abs/2508.12892", "authors": ["Mahdi Abdollahpour", "Marco Bertuletti", "Yichao Zhang", "Yawei Li", "Luca Benini", "Alessandro Vanelli-Coralli"], "title": "A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN", "comment": "Accepted to IEEE GLOBECOM 2025", "summary": "Artificial intelligence approaches for base-band processing for radio\nreceivers have demonstrated significant performance gains. Most of the proposed\nmethods are characterized by high compute and memory requirements, hindering\ntheir deployment at the edge of the Radio Access Networks (RAN) and limiting\ntheir scalability to large bandwidths and many antenna 6G systems. In this\npaper, we propose a low-complexity, model-driven neural network-based receiver,\ndesigned for multi-user multiple-input multiple-output (MU-MIMO) systems and\nsuitable for implementation at the RAN edge. The proposed solution is compliant\nwith the 5G New Radio (5G NR), and supports different modulation schemes,\nbandwidths, number of users, and number of base-station antennas with a single\ntrained model without the need for further training. Numerical simulations of\nthe Physical Uplink Shared Channel (PUSCH) processing show that the proposed\nsolution outperforms the state-of-the-art methods in terms of achievable\nTransport Block Error Rate (TBLER), while reducing the Floating Point\nOperations (FLOPs) by 66$\\times$, and the learnable parameters by 396$\\times$.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u578b\u9a71\u52a8\u7684\u4f4e\u590d\u6742\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u673a\uff0c\u9002\u7528\u4e8eMU-MIMO\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u5f53\u524dAI\u57fa\u5e26\u5904\u7406\u65b9\u6cd5\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5728RAN\u8fb9\u7f18\u90e8\u7f72\u548c\u5411\u5927\u5e26\u5bbd\u591a\u5929\u7ebf\u7cfb\u7edf\u6269\u5c55\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6a21\u578b\u9a71\u52a8\u7684\u795e\u7ecf\u7f51\u7edc\u57fa\u7840\u63a5\u6536\u673a\uff0c\u652f\u6301\u591a\u79cd\u8c03\u5236\u65b9\u6848\u3001\u5e26\u5bbd\u3001\u7528\u6237\u6570\u548c\u57fa\u7ad9\u5929\u7ebf\u6570\uff0c\u4ec5\u9700\u5355\u4e2a\u8bad\u7ec3\u6a21\u578b\u5373\u53ef\u5e94\u7528\u3002", "result": "\u5728PUSCH\u5904\u7406\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6848\u5728TBLER\u6027\u80fd\u4e0a\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06FLOPs\u51cf\u5c1166\u500d\uff0c\u53ef\u5b66\u53c2\u6570\u51cf\u5c11396\u500d\u3002", "conclusion": "\u8be5\u4f4e\u590d\u6742\u5ea6\u63a5\u6536\u673a\u65b9\u6848\u4e3a6G\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8fb9\u7f18\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2508.12941", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12941", "abs": "https://arxiv.org/abs/2508.12941", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ozgur Ozdemir", "Mani Bharathi Pandian", "Ismail Guvenc"], "title": "Interference-Asymmetric UAV Remote Control Links: Measurements and Performance Evaluation", "comment": null, "summary": "Reliable and secure connectivity is crucial for remote control (RC) and\nuncrewed aerial vehicles (UAVs) links. A major problem for UAV RC links is that\ninterference sources within the coverage may degrade the link quality. Such\ninterference problems are a higher concern for the UAV than the RC unit on the\nground due to the UAV being in line of sight (LoS) with a larger number of\ninterference sources. As a result, lost hybrid automatic repeat request (HARQ)\nindicators (ACK/NACK) feedback in the uplink (UL, RC to UAV) may degrade the\ndownlink (DL, UAV to RC) throughput. To get physical evidence for our\ninterference asymmetry argument, we first conducted a measurement campaign\nusing a helikite platform at the Main Campus area of NC State University during\nthe 2024 Packapalooza festival. Subsequently, we evaluated the throughput\nimpact of the loss of HARQ indicator feedback caused by UL asymmetry using\nMATLAB long-term-evolution (LTE) and fifth-generation (5G) toolboxes. Our\nnumerical results confirm that UL interference asymmetry substantially degrades\nthe throughput performance due to the loss of HARQ indicator feedback.", "AI": {"tldr": "\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u5b58\u5728\u4e0a\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5bfc\u81f4HARQ\u53cd\u9988\u4e22\u5931\uff0c\u4e25\u91cd\u5f71\u54cd\u4e0b\u884c\u541e\u5410\u91cf\u6027\u80fd", "motivation": "\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u9700\u8981\u53ef\u9760\u5b89\u5168\u7684\u8fde\u63a5\uff0c\u4f46\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u63a7\u5236\u5355\u5143\u4e4b\u95f4\u7684\u5e72\u6270\u4e0d\u5bf9\u79f0\u95ee\u9898\uff08\u65e0\u4eba\u673a\u66b4\u9732\u5728\u66f4\u591a\u5e72\u6270\u6e90\u4e0b\uff09\u4f1a\u5bfc\u81f4\u4e0a\u884cHARQ\u53cd\u9988\u4e22\u5931\uff0c\u8fdb\u800c\u5f71\u54cd\u4e0b\u884c\u541e\u5410\u91cf", "method": "\u9996\u5148\u4f7f\u7528helikite\u5e73\u53f0\u5728NC\u5dde\u7acb\u5927\u5b66\u4e3b\u6821\u533a\u8fdb\u884c\u5b9e\u5730\u6d4b\u91cf\uff0c\u7136\u540e\u4f7f\u7528MATLAB LTE\u548c5G\u5de5\u5177\u7bb1\u8bc4\u4f30HARQ\u53cd\u9988\u4e22\u5931\u5bf9\u541e\u5410\u91cf\u7684\u5f71\u54cd", "result": "\u6570\u503c\u7ed3\u679c\u8bc1\u5b9e\u4e0a\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u786e\u5b9e\u4f1a\u56e0HARQ\u6307\u793a\u5668\u53cd\u9988\u4e22\u5931\u800c\u663e\u8457\u964d\u4f4e\u541e\u5410\u91cf\u6027\u80fd", "conclusion": "\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u7684\u4e0a\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u662f\u4e00\u4e2a\u4e25\u91cd\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u76f8\u5e94\u63aa\u65bd\u6765\u7f13\u89e3HARQ\u53cd\u9988\u4e22\u5931\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd"}}
{"id": "2508.12964", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12964", "abs": "https://arxiv.org/abs/2508.12964", "authors": ["Osman Tokluoglu", "Enver Cavus", "Ebrahim Bedeer", "Halim Yanikomeroglu"], "title": "A Novel CNN Based Standalone Detector for Faster-than-Nyquist Signaling", "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Communications (IEEE TCOM)", "summary": "This paper presents a novel convolutional neural network (CNN)-based detector\nfor faster-than-Nyquist (FTN) signaling, introducing structured fixed kernel\nlayers with domain-informed masking to effectively mitigate intersymbol\ninterference (ISI). Unlike standard CNN architectures that rely on moving\nkernels, the proposed approach employs fixed convolutional kernels at\npredefined positions to explicitly learn ISI patterns at varying distances from\nthe central symbol. To enhance feature extraction, a hierarchical filter\nallocation strategy is employed, assigning more filters to earlier layers for\nstronger ISI components and fewer to later layers for weaker components. This\nstructured design improves feature representation, eliminates redundant\ncomputations, and enhances detection accuracy while maintaining computational\nefficiency. Simulation results demonstrate that the proposed detector achieves\nnear-optimal bit error rate (BER) performance, comparable to the BCJR algorithm\nfor the compression factor $\\tau \\geq 0.7$, while offering up to $46\\%$ and\n$84\\%$ computational cost reduction over M-BCJR for BPSK and QPSK,\nrespectively. Additional evaluations confirm the method's adaptability to\nhigh-order modulations (up to 64-QAM), resilience in quasi-static multipath\nRayleigh fading channels, and effectiveness under LDPC-coded FTN transmission,\nhighlighting its robustness and practicality.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u5668\uff0c\u7528\u4e8e\u66f4\u5feb\u4e8e\u5948\u5e93\u65af\u7279\u4fe1\u53f7\u4f20\u8f93\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u56fa\u5b9a\u5185\u6838\u548c\u57df\u77e5\u8bc6\u63a9\u7801\u6765\u6709\u6548\u51cf\u5c11\u7801\u95f4\u5e72\u6270\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9519\u8bef\u7387\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684FTN\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\u5982BCJR\u7b97\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u800c\u6807\u51c6CNN\u67b6\u6784\u4f7f\u7528\u79fb\u52a8\u5185\u6838\u65f6\u5bf9\u7801\u95f4\u5e72\u6270\u7684\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u5904\u7406FTN\u4fe1\u53f7\u4e2d\u7684\u590d\u6742ISI\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u56fa\u5b9a\u5377\u79ef\u5185\u6838\u5c42\uff0c\u5728\u9884\u5b9a\u4e49\u4f4d\u7f6e\u4f7f\u7528\u56fa\u5b9a\u5185\u6838\u663e\u5f0f\u5b66\u4e60\u4e0d\u540c\u8ddd\u79bb\u5904\u7684ISI\u6a21\u5f0f\u3002\u91c7\u7528\u5c42\u6b21\u6ee4\u6ce2\u5668\u5206\u914d\u7b56\u7565\uff0c\u5728\u65e9\u671f\u5c42\u5206\u914d\u66f4\u591a\u6ee4\u6ce2\u5668\u5904\u7406\u5f3a\u5ea6\u66f4\u5927\u7684ISI\u5206\u91cf\uff0c\u540e\u671f\u5c42\u5206\u914d\u66f4\u5c11\u6ee4\u6ce2\u5668\u5904\u7406\u5f31\u5ea6ISI\u3002\u7ed3\u5408\u57df\u77e5\u8bc6\u63a9\u7801\u6765\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u6548\u679c\u3002", "result": "\u5728\u538b\u7f29\u56e0\u5b50\u03c4\u22650.7\u65f6\uff0c\u8be5\u68c0\u6d4b\u5668\u5b9e\u73b0\u4e86\u63a5\u8fd1BCJR\u7b97\u6cd5\u7684\u6700\u4f18BER\u6027\u80fd\u3002\u4e0eM-BCJR\u76f8\u6bd4\uff0c\u5728BPSK\u548cQPSK\u4e0b\u5206\u522b\u51cf\u5c1186%\u548c84%\u7684\u8ba1\u7b97\u6210\u672c\u3002\u65b9\u6cd5\u540c\u65f6\u517c\u5bb9\u9ad8\u9636\u8c03\u5236\uff08\u81f364-QAM\uff09\uff0c\u5728\u51e0\u4e4e\u9759\u6b62\u591a\u8def\u96f7\u5229\u8870\u843d\u9891\u9053\u4e2d\u4fdd\u6301\u7a33\u5065\u6027\uff0c\u5e76\u5728LDPC\u7f16\u7801FTN\u4f20\u8f93\u4e0b\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u6784\u5316CNN\u68c0\u6d4b\u5668\u4e3aFTN\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u5185\u6838\u8bbe\u8ba1\u548c\u5c42\u6b21\u6ee4\u6ce2\u5668\u5206\u914d\uff0c\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u7ef4\u6301\u4e86\u4f18\u5f02\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u4f53\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.13017", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13017", "abs": "https://arxiv.org/abs/2508.13017", "authors": ["Scott Schoen Jr", "Brian Lause", "Marko Jakovljevic", "Rimon Tadross", "Mike Washburn", "Anthony E. Samir"], "title": "Wavefield Correlation Imaging in Arbitrary Media with Inherent Aberration Correction", "comment": null, "summary": "Ultrasound (US) imaging is an indispensable tool for diagnostic imaging,\nparticularly given its cost, safety, and portability profiles compared to other\nmodalities. However, US is challenged in subjects with morphological\nheterogeneity (e.g., those with overweight or obesity), largely because\nconventional imaging algorithms do not account for such variation in the\nbeamforming process. Specific knowledge of the these spatial variations enables\nsupplemental corrections of these algorithms, but with added computational\ncomplexity. Wavefield correlation imaging (WCI) enables efficient image\nformation in the spatial frequency domain that, in its canonical formulation,\nassumes a uniform medium. In this work, we present an extension of WCI to\narbitrary known speed-of-sound distributions directly in the image formation\nprocess, and demonstrate its feasibility in silico, in vitro, and in vivo. We\nreport resolution improvements of over 30% and contrast improvements of order\n10% over conventional WCI imaging. Together our results suggest heterogeneous\nWCI (HWCI) may have high translational potential to improve the objective\nquality, and thus clinical utility, of ultrasound images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8d85\u58f0\u6210\u50cf\u4e2d\u5f62\u6001\u5f02\u8d28\u6027\u95ee\u9898\u7684\u6539\u8fdb\u65b9\u6cd5\u2014\u2014\u5f02\u8d28\u6027\u6ce2\u573a\u76f8\u5173\u6210\u50cf\uff08HWCI\uff09\uff0c\u901a\u8fc7\u5728\u6210\u50cf\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u8003\u8651\u5df2\u77e5\u7684\u58f0\u901f\u5206\u5e03\uff0c\u76f8\u6bd4\u4f20\u7edfWCI\u65b9\u6cd5\u5b9e\u73b0\u4e8630%\u4ee5\u4e0a\u7684\u5206\u8fa8\u7387\u63d0\u5347\u548c\u7ea610%\u7684\u5bf9\u6bd4\u5ea6\u6539\u5584\u3002", "motivation": "\u8d85\u58f0\u6210\u50cf\u5728\u5f62\u6001\u5f02\u8d28\u6027\u5bf9\u8c61\uff08\u5982\u8d85\u91cd\u6216\u80a5\u80d6\u60a3\u8005\uff09\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u7edf\u6210\u50cf\u7b97\u6cd5\u5728\u6ce2\u675f\u5f62\u6210\u8fc7\u7a0b\u4e2d\u672a\u8003\u8651\u8fd9\u79cd\u53d8\u5316\u3002\u867d\u7136\u4e86\u89e3\u7a7a\u95f4\u53d8\u5316\u53ef\u4ee5\u8fdb\u884c\u7b97\u6cd5\u4fee\u6b63\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u6269\u5c55\u4e86\u6ce2\u573a\u76f8\u5173\u6210\u50cf\uff08WCI\uff09\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6210\u50cf\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5904\u7406\u4efb\u610f\u5df2\u77e5\u7684\u58f0\u901f\u5206\u5e03\uff0c\u4ece\u800c\u9002\u5e94\u5f02\u8d28\u6027\u4ecb\u8d28\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u6a21\u62df\u3001\u4f53\u5916\u5b9e\u9a8c\u548c\u4f53\u5185\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u76f8\u6bd4\u4f20\u7edfWCI\u6210\u50cf\uff0c\u5206\u8fa8\u7387\u63d0\u5347\u8d85\u8fc730%\uff0c\u5bf9\u6bd4\u5ea6\u6539\u5584\u7ea610%\u3002", "conclusion": "\u5f02\u8d28\u6027\u6ce2\u573a\u76f8\u5173\u6210\u50cf\uff08HWCI\uff09\u5177\u6709\u5f88\u9ad8\u7684\u8f6c\u5316\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u8d85\u58f0\u56fe\u50cf\u7684\u5ba2\u89c2\u8d28\u91cf\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.13067", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13067", "abs": "https://arxiv.org/abs/2508.13067", "authors": ["Iv\u00e1n Alexander Morales Sandoval", "Getuar Rexhepi", "Kengo Ando", "Giuseppe Thadeu Freitas de Abreu"], "title": "Low-complexity Leakage Minimization Beamforming for Large-scale Multi-user Cell-Free Massive MIMO", "comment": "Submitted to an IEEE journal for possible publication", "summary": "We propose a low-complexity beamforming (BF) design for information leakage\nminimization in multi-user (MU) cell-free massive multiple-input\nmultiple-output (CF-mMIMO) systems. Our approach leverages fractional\nprogramming (FP) to reformulate the secrecy rate maximization problem into a\ntractable difference-of-convex form. To efficiently solve the resulting\nnon-convex problem, we employ the Concave-Convex Procedure (CCP), enabling fast\nconvergence to a local optimum. Simulation results demonstrate that the\nproposed scheme achieves secrecy rates comparable to state-of-the-art (SotA)\nmethods, while significantly reducing computational complexity and improving\nconvergence speed.", "AI": {"tldr": "\u57fa\u4e8e\u5206\u6570\u89c4\u5212\u548c\u51f9\u51f8\u51f8\u89c4\u5212\u7684\u4f4e\u590d\u6742\u5ea6\u5f62\u6210\u6ce2\u675f\u8bbe\u8ba1\uff0c\u5728\u7edd\u5bc6\u901f\u7387\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4f18\u5f02\u5e73\u8861", "motivation": "\u89e3\u51b3\u7ec6\u80de\u514d\u5927\u89c8\u591a\u7528\u6237\u7cfb\u7edf\u4e2d\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u7edd\u5bc6\u6027\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6", "method": "\u5229\u7528\u5206\u6570\u89c4\u5212(FP)\u5c06\u7edd\u5bc6\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u91cd\u6784\u4e3a\u53ef\u89e3\u7684\u51f9\u51f8\u5dee\u5f62\u5f0f\uff0c\u91c7\u7528\u51f9\u51f8\u51f8\u89c4\u5212(CCP)\u7b97\u6cd5\u9ad8\u6548\u6c42\u89e3", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6848\u80fd\u591f\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7edd\u5bc6\u901f\u7387\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u6536\u655b\u901f\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ec6\u80de\u514d\u5927\u89c8MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7edd\u5bc6\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u590d\u6742\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4f18\u5f02\u5e73\u8861"}}
{"id": "2508.13075", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13075", "abs": "https://arxiv.org/abs/2508.13075", "authors": ["Arav Sharma", "Lei Chi", "Ari Gebhardt", "Alon S. Levin", "Timothy R. Hoerning", "Sam Keene"], "title": "BeamSeek: Deep Learning-based DOA Estimation for Low-Complexity mmWave Phased Arrays", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A novel approach combining agile beam switching with deep learning to enhance\nthe speed and accuracy of Direction of Arrival (DOA) estimation for\nmillimeter-wave (mmWave) phased array systems with low-complexity hardware\nimplementations is proposed and evaluated. Traditional DOA methods requiring\ndirect access to individual antenna elements are impractical for analog or\nhybrid beamforming systems prevalent in modern mmWave implementations. Recent\nagile beam switching techniques have demonstrated rapid DOA estimation, but\ntheir accuracy and robustness can be further improved via deep learning.\nBeamSeek addresses these limitations by employing a Multi-Layer Perceptron\n(MLP) and specialized data augmentation that emulates real-world propagation\nconditions. The proposed approach was experimentally validated at 60 GHz using\nthe NSF PAWR COSMOS testbed, demonstrating significant improvements over a\ncorrelation-based method across various Signal-to-Noise Ratio (SNR) levels.\nResults show that BeamSeek achieves up to an 8 degree reduction in average\nestimation error compared to this baseline, with particular advantages in noisy\nchannels. This makes it especially suitable for practical mmWave deployments in\nenvironments characterized by multipath interference and hardware constraints.", "AI": {"tldr": "BeamSeek\u7ed3\u5408\u654f\u6377\u6ce2\u675f\u5207\u6362\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u572860GHz\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u66f4\u51c6\u786e\u7684DOA\u4f30\u8ba1\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5e73\u5747\u8bef\u5dee\u964d\u4f4e8\u5ea6\uff0c\u7279\u522b\u9002\u5408\u566a\u58f0\u73af\u5883\u3002", "motivation": "\u4f20\u7edfDOA\u65b9\u6cd5\u9700\u8981\u76f4\u63a5\u8bbf\u95ee\u5355\u4e2a\u5929\u7ebf\u5355\u5143\uff0c\u4e0d\u9002\u7528\u4e8e\u73b0\u4ee3\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u666e\u904d\u91c7\u7528\u7684\u6a21\u62df\u6216\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u7cfb\u7edf\u3002\u73b0\u6709\u654f\u6377\u6ce2\u675f\u5207\u6362\u6280\u672f\u867d\u7136\u5feb\u901f\u4f46\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u6709\u5f85\u63d0\u5347\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u673a(MLP)\u548c\u4e13\u95e8\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u6a21\u62df\u771f\u5b9e\u4f20\u64ad\u6761\u4ef6\uff0c\u7ed3\u5408\u654f\u6377\u6ce2\u675f\u5207\u6362\u6280\u672f\uff0c\u5728NSF PAWR COSMOS\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c60GHz\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u57fa\u51c6\u65b9\u6cd5\uff0cBeamSeek\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u6c34\u5e73\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u4f30\u8ba1\u8bef\u5dee\u6700\u591a\u964d\u4f4e8\u5ea6\uff0c\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u4f18\u52bf\u5c24\u5176\u660e\u663e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u5408\u5177\u6709\u591a\u5f84\u5e72\u6270\u548c\u786c\u4ef6\u7ea6\u675f\u7684\u5b9e\u9645\u6beb\u7c73\u6ce2\u90e8\u7f72\u73af\u5883\uff0c\u4e3a\u4f4e\u590d\u6742\u5ea6\u786c\u4ef6\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u7684DOA\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}

{"id": "2508.13681", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13681", "abs": "https://arxiv.org/abs/2508.13681", "authors": ["Ladan Khaloopour", "Matthias Hollick", "Vahid Jamali"], "title": "CKM-Assisted Physical-Layer Security for Resilience Against Unknown Eavesdropping Location", "comment": null, "summary": "Channel Knowledge Map (CKM) is an emerging data-driven toolbox that captures\nour awareness of the wireless channel and enables efficient communication and\nresource allocation beyond the state of the art. In this work, we consider CKM\nfor improving physical-layer security (PLS) in the presence of a passive\neavesdropper (Eve), without making any assumptions about Eve's location or\nchannel state information (CSI). We employ highly directional mmWave\ntransmissions, with the confidential message jointly encoded across multiple\nbeams. By exploiting CKM, we derive an algorithm for time and power allocation\namong the beams that maximizes the absolute secrecy rate under the worst-case\nscenario for Eve's location."}
{"id": "2508.13714", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13714", "abs": "https://arxiv.org/abs/2508.13714", "authors": ["Donatella Darsena", "Francesco Verde", "Marco Di Renzo", "Vincenzo Galdi"], "title": "Airy beams for near-field communications: Fundamentals, potentials, and limitations", "comment": "28 pages, 29 figures", "summary": "In next-generation wireless networks, the combination of electrically large\nradiating apertures and high-frequency transmission extends the radiating\nnear-field region around the transmitter. In this region, unlike in the far\nfield, the wavefront is nonplanar, which provides additional degrees of freedom\nto shape and steer the transmitted beam in a desired manner. In this paper, we\nfocus on Airy beams, which may exhibit several highly desirable properties in\nthe near-field region. Ideally, these beams follow self-accelerating (curved)\ntrajectories, demonstrate resilience to perturbations through self-healing, and\nmaintain a consistent intensity profile across all planes perpendicular to the\npropagation direction, making them effectively diffraction-free. Specifically,\nwe first present the underlying principles of self-accelerating beams radiated\nby continuous aperture field distributions. We then address several challenges\nregarding the generation of Airy beams, including their exponential decay due\nto finite energy constraints and spatial truncation of the aperture. Moreover,\nwe examine their free-space propagation characteristics. The second part of the\npaper focuses on the propagation behavior of Airy beams in non-line-of-sight\n(NLoS) scenarios. A comparison is also presented between Airy beams and\nGaussian beams. Our theoretical and numerical results show that Airy beams may\noffer a performance advantage over Gaussian beams in certain NLoS channels,\nprovided that their key properties are largely preserved, specifically,\nself-acceleration along a parabolic trajectory and diffraction-free\npropagation. In the presence of an obstacle, this requires that the portion of\nthe transmit aperture with a clear line-of-sight to the receiver is\nsufficiently large."}
{"id": "2508.13771", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13771", "abs": "https://arxiv.org/abs/2508.13771", "authors": ["Mustafa S. Abbas", "Zahra Mobini", "Hien Quoc Ngo", "Hyundong Shin", "Michail Matthaiou"], "title": "Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free Massive MIMO", "comment": null, "summary": "Joint unicast and multicast transmissions are becoming increasingly important\nin practical wireless systems, such as Internet of Things networks. This paper\ninvestigates a cell-free massive multiple-input multiple-output system that\nsimultaneously supports both transmission types, with multicast serving\nmultiple groups. Exact closed-form expressions for the achievable downlink\nspectral efficiency (SE) of both unicast and multicast users are derived for\nzero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum\nSE (SSE) maximization problem is formulated to jointly optimize the access\npoint (AP) selection and power allocation. The optimization framework accounts\nfor practical constraints, including the maximum transmit power per AP,\nfronthaul capacity limitations between APs and the central processing unit, and\nquality-of-service requirements for all users. The resulting non-convex\noptimization problem is reformulated into a tractable structure, and an\naccelerated projected gradient (APG)-based algorithm is developed to\nefficiently obtain near-optimal solutions. As a performance benchmark, a\nsuccessive convex approximation (SCA)-based algorithm is also implemented.\nSimulation results demonstrate that the proposed joint optimization approach\nsignificantly enhances the SSE across various system setups and precoding\nstrategies. In particular, the APG-based algorithm achieves substantial\ncomplexity reduction while maintaining competitive performance, making it\nwell-suited for large-scale practical deployments."}
{"id": "2508.13818", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13818", "abs": "https://arxiv.org/abs/2508.13818", "authors": ["Yue Xiu", "Yang Zhao", "Ran Yang", "Wanting Lyu", "Dusit Niyato", "Dong In Kim", "Guangyi Liu", "Ning Wei"], "title": "Robust Optimization for Movable Antenna-aided Cell-Free ISAC with Time Synchronization Errors", "comment": null, "summary": "The cell-free integrated sensing and communication (CF-ISAC) system, which\neffectively mitigates intra-cell interference and provides precise sensing\naccuracy, is a promising technology for future 6G networks. However, to fully\ncapitalize on the potential of CF-ISAC, accurate time synchronization (TS)\nbetween access points (APs) is critical. Due to the limitations of current\nsynchronization technologies, TS errors have become a significant challenge in\nthe development of the CF-ISAC system. In this paper, we propose a novel\nCF-ISAC architecture based on movable antennas (MAs), which exploits spatial\ndiversity to enhance communication rates, maintain sensing accuracy, and reduce\nthe impact of TS errors. We formulate a worst-case sensing accuracy\noptimization problem for TS errors to address this challenge, deriving the\nworst-case Cram\\'er-Rao lower bound (CRLB). Subsequently, we develop a joint\noptimization framework for AP beamforming and MA positions to satisfy\ncommunication rate constraints while improving sensing accuracy. A robust\noptimization framework is designed for the highly complex and non-convex\nproblem. Specifically, we employ manifold optimization (MO) to solve the\nworst-case sensing accuracy optimization problem. Then, we propose an\nMA-enabled meta-reinforcement learning (MA-MetaRL) to design optimization\nvariables while satisfying constraints on MA positions, communication rate, and\ntransmit power, thereby improving sensing accuracy. The simulation results\ndemonstrate that the proposed robust optimization algorithm significantly\nimproves the accuracy of the detection and is strong against TS errors.\nMoreover, compared to conventional fixed position antenna (FPA) technologies,\nthe proposed MA-aided CF-ISAC architecture achieves higher system capacity,\nthus validating its effectiveness."}
{"id": "2508.13516", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.13516", "abs": "https://arxiv.org/abs/2508.13516", "authors": ["Yueh-Po Peng", "Ting-Kang Wang", "Li Su", "Vincent K. M. Cheung"], "title": "Is Transfer Learning Necessary for Violin Transcription?", "comment": null, "summary": "Automatic music transcription (AMT) has achieved remarkable progress for\ninstruments such as the piano, largely due to the availability of large-scale,\nhigh-quality datasets. In contrast, violin AMT remains underexplored due to\nlimited annotated data. A common approach is to fine-tune pretrained models for\nother downstream tasks, but the effectiveness of such transfer remains unclear\nin the presence of timbral and articulatory differences. In this work, we\ninvestigate whether training from scratch on a medium-scale violin dataset can\nmatch the performance of fine-tuned piano-pretrained models. We adopt a piano\ntranscription architecture without modification and train it on the MOSA\ndataset, which contains about 30 hours of aligned violin recordings. Our\nexperiments on URMP and Bach10 show that models trained from scratch achieved\ncompetitive or even superior performance compared to fine-tuned counterparts.\nThese findings suggest that strong violin AMT is possible without relying on\npretrained piano representations, highlighting the importance of\ninstrument-specific data collection and augmentation strategies."}
{"id": "2508.13320", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.13320", "abs": "https://arxiv.org/abs/2508.13320", "authors": ["Ashi Garg", "Zexin Cai", "Henry Li Xinyuan", "Leibny Paola Garc√≠a-Perera", "Kevin Duh", "Sanjeev Khudanpur", "Matthew Wiesner", "Nicholas Andrews"], "title": "Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of Synthesized Speech Under Distribution Shifts", "comment": null, "summary": "We address the challenge of detecting synthesized speech under distribution\nshifts -- arising from unseen synthesis methods, speakers, languages, or audio\nconditions -- relative to the training data. Few-shot learning methods are a\npromising way to tackle distribution shifts by rapidly adapting on the basis of\na few in-distribution samples. We propose a self-attentive prototypical network\nto enable more robust few-shot adaptation. To evaluate our approach, we\nsystematically compare the performance of traditional zero-shot detectors and\nthe proposed few-shot detectors, carefully controlling training conditions to\nintroduce distribution shifts at evaluation time. In conditions where\ndistribution shifts hamper the zero-shot performance, our proposed few-shot\nadaptation technique can quickly adapt using as few as 10 in-distribution\nsamples -- achieving upto 32% relative EER reduction on deepfakes in Japanese\nlanguage and 20% relative reduction on ASVspoof 2021 Deepfake dataset."}
{"id": "2508.13839", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13839", "abs": "https://arxiv.org/abs/2508.13839", "authors": ["Yue Xiu", "Yang Zhao", "Ran Yang", "Zheng Dong", "Wanting Lyu", "Zeyuan Zhang", "Dusit Niyato", "Guangyi Liu", "Ning Wei"], "title": "Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems", "comment": null, "summary": "The cell-free integrated sensing and communication (CF-ISAC) architecture is\na promising enabler for 6G, offering spectrum efficiency and ubiquitous\ncoverage. However, real deployments suffer from hardware impairments,\nespecially nonlinear distortion from power amplifiers (PAs), which degrades\nboth communication and sensing. To address this, we propose a movable antenna\n(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.\nThe PAs nonlinearities are modeled by a third-order memoryless polynomial,\nwhere the third-order distortion coefficients (3RDCs) vary across access points\n(APs) due to hardware differences, aging, and environmental conditions. We\ndesign a distributed distortion-aware worst-case robust optimization framework\nthat explicitly incorporates uncertainty in 3RDCs. First, we analyze the\nworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)\nand communication rate. Then, to address the resulting non-convexity, we apply\nsuccessive convex approximation (SCA) for estimating the 3RDCs. With these, we\njointly optimize beamforming and MA positions under transmit power and sensing\nconstraints. To efficiently solve this highly non-convex problem, we develop an\nMA-enabled self-attention convolutional graph neural network (SACGNN)\nalgorithm. Simulations demonstrate that our method substantially enhances the\ncommunication-sensing trade-off under distortion and outperforms fixed-position\nantenna baselines in terms of robustness and capacity, thereby highlighting the\nadvantages of MA-aided CF-ISAC systems."}
{"id": "2508.13624", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.13624", "abs": "https://arxiv.org/abs/2508.13624", "authors": ["Rong Chao", "Wenze Ren", "You-Jin Li", "Kuo-Hsuan Hung", "Sung-Feng Huang", "Szu-Wei Fu", "Wen-Huang Cheng", "Yu Tsao"], "title": "Leveraging Mamba with Full-Face Vision for Audio-Visual Speech Enhancement", "comment": "Accepted to Interspeech 2025 Workshop", "summary": "Recent Mamba-based models have shown promise in speech enhancement by\nefficiently modeling long-range temporal dependencies. However, models like\nSpeech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios\nand struggle in complex multi-speaker environments such as the cocktail party\nproblem. To overcome this, we introduce AVSEMamba, an audio-visual speech\nenhancement model that integrates full-face visual cues with a Mamba-based\ntemporal backbone. By leveraging spatiotemporal visual information, AVSEMamba\nenables more accurate extraction of target speech in challenging conditions.\nEvaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba\noutperforms other monaural baselines in speech intelligibility (STOI),\nperceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves\n\\textbf{1st place} on the monaural leaderboard."}
{"id": "2508.13576", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13576", "abs": "https://arxiv.org/abs/2508.13576", "authors": ["Meng-Ping Lin", "Enoch Hsin-Ho Huang", "Shao-Yi Chien", "Yu Tsao"], "title": "End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments", "comment": "6 pages, 4 figures", "summary": "The cochlear implant (CI) is a remarkable biomedical device that successfully\nenables individuals with severe-to-profound hearing loss to perceive sound by\nconverting speech into electrical stimulation signals. Despite advancements in\nthe performance of recent CI systems, speech comprehension in noisy or\nreverberant conditions remains a challenge. Recent and ongoing developments in\ndeep learning reveal promising opportunities for enhancing CI sound coding\ncapabilities, not only through replicating traditional signal processing\nmethods with neural networks, but also through integrating visual cues as\nauxiliary data for multimodal speech processing. Therefore, this paper\nintroduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an\naudio-visual speech enhancement (AVSE) model as a pre-processing module for the\ndeep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically,\na joint training approach is applied to model AVSE-ECS, an end-to-end CI\nsystem. Experimental results indicate that the proposed method outperforms the\nprevious ECS strategy in noisy conditions, with improved objective speech\nintelligibility scores. The methods and findings in this study demonstrate the\nfeasibility and potential of using deep learning to integrate the AVSE module\ninto an end-to-end CI system"}
{"id": "2508.13937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13937", "abs": "https://arxiv.org/abs/2508.13937", "authors": ["Halim Lee", "Jongmin Park", "Kwansik Park"], "title": "Evaluating Particle Filtering for RSS-Based Target Localization under Varying Noise Levels and Sensor Geometries", "comment": null, "summary": "Target localization is a critical task in various applications, such as\nsearch and rescue, surveillance, and wireless sensor networks. When a target\nemits a radio frequency (RF) signal, spatially distributed sensors can collect\nsignal measurements to estimate the target's location. Among various\nmeasurement modalities, received signal strength (RSS) is particularly\nattractive due to its low cost, low power consumption, and ease of deployment.\nWhile particle filtering has previously been applied to RSS-based target\nlocalization, few studies have systematically analyzed its performance under\nvarying sensor geometries and RSS noise levels. This paper addresses this gap\nby designing and evaluating a particle filtering algorithm for localizing a\nstationary target. The proposed method is compared with a conventional\nRSS-based trilateration approach across different sensor configurations and\nnoise conditions. Simulation results indicate that particle filtering provides\nmore accurate target localization than trilateration, particularly in scenarios\nwith unfavorable sensor geometries and high RSS noise."}
{"id": "2508.13786", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13786", "abs": "https://arxiv.org/abs/2508.13786", "authors": ["Yisu Liu", "Chenxing Li", "Wanqian Zhang", "Wenfu Wang", "Meng Yu", "Ruibo Fu", "Zheng Lin", "Weiping Wang", "Dong Yu"], "title": "DegDiT: Controllable Audio Generation with Dynamic Event Graph Guided Diffusion Transformer", "comment": null, "summary": "Controllable text-to-audio generation aims to synthesize audio from textual\ndescriptions while satisfying user-specified constraints, including event\ntypes, temporal sequences, and onset and offset timestamps. This enables\nprecise control over both the content and temporal structure of the generated\naudio. Despite recent progress, existing methods still face inherent trade-offs\namong accurate temporal localization, open-vocabulary scalability, and\npractical efficiency. To address these challenges, we propose DegDiT, a novel\ndynamic event graph-guided diffusion transformer framework for open-vocabulary\ncontrollable audio generation. DegDiT encodes the events in the description as\nstructured dynamic graphs. The nodes in each graph are designed to represent\nthree aspects: semantic features, temporal attributes, and inter-event\nconnections. A graph transformer is employed to integrate these nodes and\nproduce contextualized event embeddings that serve as guidance for the\ndiffusion model. To ensure high-quality and diverse training data, we introduce\na quality-balanced data selection pipeline that combines hierarchical event\nannotation with multi-criteria quality scoring, resulting in a curated dataset\nwith semantic diversity. Furthermore, we present consensus preference\noptimization, facilitating audio generation through consensus among multiple\nreward signals. Extensive experiments on AudioCondition, DESED, and AudioTime\ndatasets demonstrate that DegDiT achieves state-of-the-art performances across\na variety of objective and subjective evaluation metrics."}
{"id": "2508.13992", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.13992", "abs": "https://arxiv.org/abs/2508.13992", "authors": ["Sonal Kumar", "≈†imon Sedl√°ƒçek", "Vaibhavi Lokegaonkar", "Fernando L√≥pez", "Wenyi Yu", "Nishit Anand", "Hyeonggon Ryu", "Lichang Chen", "Maxim Pliƒçka", "Miroslav Hlav√°ƒçek", "William Fineas Ellingwood", "Sathvik Udupa", "Siyuan Hou", "Allison Ferner", "Sara Barahona", "Cecilia Bola√±os", "Satish Rahi", "Laura Herrera-Alarc√≥n", "Satvik Dixit", "Siddhi Patil", "Soham Deshmukh", "Lasha Koroshinadze", "Yao Liu", "Leibny Paola Garcia Perera", "Eleni Zanou", "Themos Stafylakis", "Joon Son Chung", "David Harwath", "Chao Zhang", "Dinesh Manocha", "Alicia Lozano-Diez", "Santosh Kesiraju", "Sreyan Ghosh", "Ramani Duraiswami"], "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence", "comment": null, "summary": "Audio comprehension-including speech, non-speech sounds, and music-is\nessential for achieving human-level intelligence. Consequently, AI agents must\ndemonstrate holistic audio understanding to qualify as generally intelligent.\nHowever, evaluating auditory intelligence comprehensively remains challenging.\nTo address this gap, we introduce MMAU-Pro, the most comprehensive and\nrigorously curated benchmark for assessing audio intelligence in AI systems.\nMMAU-Pro contains 5,305 instances, where each instance has one or more audios\npaired with human expert-generated question-answer pairs, spanning speech,\nsound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro\nevaluates auditory intelligence across 49 unique skills and multiple complex\ndimensions, including long-form audio comprehension, spatial audio reasoning,\nmulti-audio understanding, among others. All questions are meticulously\ndesigned to require deliberate multi-hop reasoning, including both\nmultiple-choice and open-ended response formats. Importantly, audio data is\nsourced directly ``from the wild\" rather than from existing datasets with known\ndistributions. We evaluate 22 leading open-source and proprietary multimodal AI\nmodels, revealing significant limitations: even state-of-the-art models such as\nGemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy,\nrespectively, approaching random performance in multiple categories. Our\nextensive analysis highlights specific shortcomings and provides novel\ninsights, offering actionable perspectives for the community to enhance future\nAI systems' progression toward audio general intelligence. The benchmark and\ncode is available at https://sonalkum.github.io/mmau-pro."}
{"id": "2508.14012", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14012", "abs": "https://arxiv.org/abs/2508.14012", "authors": ["Seungmin Seo", "Oleg Aulov", "Afzal Godil", "Kevin Mangold"], "title": "Evaluating Identity Leakage in Speaker De-Identification Systems", "comment": "Submitted to ICASSP 2026", "summary": "Speaker de-identification aims to conceal a speaker's identity while\npreserving intelligibility of the underlying speech. We introduce a benchmark\nthat quantifies residual identity leakage with three complementary error rates:\nequal error rate, cumulative match characteristic hit rate, and embedding-space\nsimilarity measured via canonical correlation analysis and Procrustes analysis.\nEvaluation results reveal that all state-of-the-art speaker de-identification\nsystems leak identity information. The highest performing system in our\nevaluation performs only slightly better than random guessing, while the lowest\nperforming system achieves a 45% hit rate within the top 50 candidates based on\nCMC. These findings highlight persistent privacy risks in current speaker\nde-identification technologies."}
{"id": "2508.13516", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.13516", "abs": "https://arxiv.org/abs/2508.13516", "authors": ["Yueh-Po Peng", "Ting-Kang Wang", "Li Su", "Vincent K. M. Cheung"], "title": "Is Transfer Learning Necessary for Violin Transcription?", "comment": null, "summary": "Automatic music transcription (AMT) has achieved remarkable progress for\ninstruments such as the piano, largely due to the availability of large-scale,\nhigh-quality datasets. In contrast, violin AMT remains underexplored due to\nlimited annotated data. A common approach is to fine-tune pretrained models for\nother downstream tasks, but the effectiveness of such transfer remains unclear\nin the presence of timbral and articulatory differences. In this work, we\ninvestigate whether training from scratch on a medium-scale violin dataset can\nmatch the performance of fine-tuned piano-pretrained models. We adopt a piano\ntranscription architecture without modification and train it on the MOSA\ndataset, which contains about 30 hours of aligned violin recordings. Our\nexperiments on URMP and Bach10 show that models trained from scratch achieved\ncompetitive or even superior performance compared to fine-tuned counterparts.\nThese findings suggest that strong violin AMT is possible without relying on\npretrained piano representations, highlighting the importance of\ninstrument-specific data collection and augmentation strategies."}
{"id": "2508.13576", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13576", "abs": "https://arxiv.org/abs/2508.13576", "authors": ["Meng-Ping Lin", "Enoch Hsin-Ho Huang", "Shao-Yi Chien", "Yu Tsao"], "title": "End-to-End Audio-Visual Learning for Cochlear Implant Sound Coding in Noisy Environments", "comment": "6 pages, 4 figures", "summary": "The cochlear implant (CI) is a remarkable biomedical device that successfully\nenables individuals with severe-to-profound hearing loss to perceive sound by\nconverting speech into electrical stimulation signals. Despite advancements in\nthe performance of recent CI systems, speech comprehension in noisy or\nreverberant conditions remains a challenge. Recent and ongoing developments in\ndeep learning reveal promising opportunities for enhancing CI sound coding\ncapabilities, not only through replicating traditional signal processing\nmethods with neural networks, but also through integrating visual cues as\nauxiliary data for multimodal speech processing. Therefore, this paper\nintroduces a novel noise-suppressing CI system, AVSE-ECS, which utilizes an\naudio-visual speech enhancement (AVSE) model as a pre-processing module for the\ndeep-learning-based ElectrodeNet-CS (ECS) sound coding strategy. Specifically,\na joint training approach is applied to model AVSE-ECS, an end-to-end CI\nsystem. Experimental results indicate that the proposed method outperforms the\nprevious ECS strategy in noisy conditions, with improved objective speech\nintelligibility scores. The methods and findings in this study demonstrate the\nfeasibility and potential of using deep learning to integrate the AVSE module\ninto an end-to-end CI system"}
{"id": "2508.13624", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.13624", "abs": "https://arxiv.org/abs/2508.13624", "authors": ["Rong Chao", "Wenze Ren", "You-Jin Li", "Kuo-Hsuan Hung", "Sung-Feng Huang", "Szu-Wei Fu", "Wen-Huang Cheng", "Yu Tsao"], "title": "Leveraging Mamba with Full-Face Vision for Audio-Visual Speech Enhancement", "comment": "Accepted to Interspeech 2025 Workshop", "summary": "Recent Mamba-based models have shown promise in speech enhancement by\nefficiently modeling long-range temporal dependencies. However, models like\nSpeech Enhancement Mamba (SEMamba) remain limited to single-speaker scenarios\nand struggle in complex multi-speaker environments such as the cocktail party\nproblem. To overcome this, we introduce AVSEMamba, an audio-visual speech\nenhancement model that integrates full-face visual cues with a Mamba-based\ntemporal backbone. By leveraging spatiotemporal visual information, AVSEMamba\nenables more accurate extraction of target speech in challenging conditions.\nEvaluated on the AVSEC-4 Challenge development and blind test sets, AVSEMamba\noutperforms other monaural baselines in speech intelligibility (STOI),\nperceptual quality (PESQ), and non-intrusive quality (UTMOS), and achieves\n\\textbf{1st place} on the monaural leaderboard."}
{"id": "2508.13992", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.13992", "abs": "https://arxiv.org/abs/2508.13992", "authors": ["Sonal Kumar", "≈†imon Sedl√°ƒçek", "Vaibhavi Lokegaonkar", "Fernando L√≥pez", "Wenyi Yu", "Nishit Anand", "Hyeonggon Ryu", "Lichang Chen", "Maxim Pliƒçka", "Miroslav Hlav√°ƒçek", "William Fineas Ellingwood", "Sathvik Udupa", "Siyuan Hou", "Allison Ferner", "Sara Barahona", "Cecilia Bola√±os", "Satish Rahi", "Laura Herrera-Alarc√≥n", "Satvik Dixit", "Siddhi Patil", "Soham Deshmukh", "Lasha Koroshinadze", "Yao Liu", "Leibny Paola Garcia Perera", "Eleni Zanou", "Themos Stafylakis", "Joon Son Chung", "David Harwath", "Chao Zhang", "Dinesh Manocha", "Alicia Lozano-Diez", "Santosh Kesiraju", "Sreyan Ghosh", "Ramani Duraiswami"], "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic Evaluation of Audio General Intelligence", "comment": null, "summary": "Audio comprehension-including speech, non-speech sounds, and music-is\nessential for achieving human-level intelligence. Consequently, AI agents must\ndemonstrate holistic audio understanding to qualify as generally intelligent.\nHowever, evaluating auditory intelligence comprehensively remains challenging.\nTo address this gap, we introduce MMAU-Pro, the most comprehensive and\nrigorously curated benchmark for assessing audio intelligence in AI systems.\nMMAU-Pro contains 5,305 instances, where each instance has one or more audios\npaired with human expert-generated question-answer pairs, spanning speech,\nsound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro\nevaluates auditory intelligence across 49 unique skills and multiple complex\ndimensions, including long-form audio comprehension, spatial audio reasoning,\nmulti-audio understanding, among others. All questions are meticulously\ndesigned to require deliberate multi-hop reasoning, including both\nmultiple-choice and open-ended response formats. Importantly, audio data is\nsourced directly ``from the wild\" rather than from existing datasets with known\ndistributions. We evaluate 22 leading open-source and proprietary multimodal AI\nmodels, revealing significant limitations: even state-of-the-art models such as\nGemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy,\nrespectively, approaching random performance in multiple categories. Our\nextensive analysis highlights specific shortcomings and provides novel\ninsights, offering actionable perspectives for the community to enhance future\nAI systems' progression toward audio general intelligence. The benchmark and\ncode is available at https://sonalkum.github.io/mmau-pro."}

{"id": "2508.14048", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14048", "abs": "https://arxiv.org/abs/2508.14048", "authors": ["Pengcheng Wang", "Sheng Li", "Takahiro Shinozaki"], "title": "RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition", "comment": "accepted at Interspeech2025 MLC-SLM Challenge workshop (task I system\n  description)", "summary": "In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which\nenhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I)\nwith a retrieval-augmented generation (RAG) module on the fly. Each partial ASR\nhypothesis queries a vector store of audio-text pairs and domain terms, and the\nretrieved results are fused with the live ASR hypotheses to fix recognition\nerrors. The fused hypotheses are passed to the LLM, yielding improved\nresponses."}
{"id": "2508.14049", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14049", "abs": "https://arxiv.org/abs/2508.14049", "authors": ["Jaskaran Singh", "Amartya Roy Chowdhury", "Raghav Prabhakar", "Varshul C. W"], "title": "MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis", "comment": null, "summary": "Current Text-to-Speech models pose a multilingual challenge, where most of\nthe models traditionally focus on English and European languages, thereby\nhurting the potential to provide access to information to many more people. To\naddress this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker\nText-To-Speech (TTS) system that has excellent multilingual expressive\ncapabilities in Indic languages. The model has been trained on around 20K hours\nof data specifically focused on Indian languages. Our approach leverages\nWav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for\ntext-to-semantic modeling. Additionally, we have used a Conditional Flow Model\n(CFM) for semantics to melspectogram generation. The experimental results\nindicate the effectiveness of the proposed approach over other frameworks. Our\ncode is available at https://github.com/dubverse-ai/MahaTTSv2"}
{"id": "2508.14115", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14115", "abs": "https://arxiv.org/abs/2508.14115", "authors": ["Taous Iatariene", "Alexandre Guérin", "Romain Serizel"], "title": "Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings", "comment": null, "summary": "Speaker embeddings are promising identity-related features that can enhance\nthe identity assignment performance of a tracking system by leveraging its\nspatial predictions, i.e, by performing identity reassignment. Common speaker\nembedding extractors usually struggle with short temporal contexts and\noverlapping speech, which imposes long-term identity reassignment to exploit\nlonger temporal contexts. However, this increases the probability of tracking\nsystem errors, which in turn impacts negatively on identity reassignment. To\naddress this, we propose a Knowledge Distillation (KD) based training approach\nfor short context speaker embedding extraction from two speaker mixtures. We\nleverage the spatial information of the speaker of interest using beamforming\nto reduce overlap. We study the feasibility of performing identity reassignment\nover blocks of fixed size, i.e., blockwise identity reassignment, to go towards\na low-latency speaker embedding based tracking system. Results demonstrate that\nour distilled models are effective at short-context embedding extraction and\nmore robust to overlap. Although, blockwise reassignment results indicate that\nfurther work is needed to handle simultaneous speech more effectively."}
{"id": "2508.14130", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14130", "abs": "https://arxiv.org/abs/2508.14130", "authors": ["Hugo Thimonier", "Antony Perzo", "Renaud Seguier"], "title": "EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition", "comment": null, "summary": "Emotion recognition from speech is a challenging task that requires capturing\nboth linguistic and paralinguistic cues, with critical applications in\nhuman-computer interaction and mental health monitoring. Recent works have\nhighlighted the ability of Large Language Models (LLMs) to perform tasks\noutside of the sole natural language area. In particular, recent approaches\nhave investigated coupling LLMs with other data modalities by using pre-trained\nbackbones and different fusion mechanisms. This work proposes a novel approach\nthat fine-tunes an LLM with audio and text representations for emotion\nprediction. Our method first extracts audio features using an audio feature\nextractor, which are then mapped into the LLM's representation space via a\nlearnable interfacing module. The LLM takes as input (1) the transformed audio\nfeatures, (2) additional features in the form of natural language (e.g., the\ntranscript), and (3) a textual prompt describing the emotion prediction task.\nTo efficiently adapt the LLM to this multimodal task, we employ Low-Rank\nAdaptation (LoRA), enabling parameter-efficient fine-tuning. Experimental\nresults on standard emotion recognition benchmarks demonstrate that our model\noutperforms all but one existing Speech-Text LLMs in the literature, while\nrequiring less than half the parameters of competing approaches. This\nhighlights our approach's effectiveness in integrating multi-modal inputs for\nspeech-based emotion understanding while maintaining significant computational\nefficiency."}
{"id": "2508.14204", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14204", "abs": "https://arxiv.org/abs/2508.14204", "authors": ["Xingyu Chen", "Jianrong Ding", "Kai Zheng", "Xinmin Fang", "Xinyu Zhang", "Chris Xiaoxuan Lu", "Zhengxiong Li"], "title": "InverTwin: Solving Inverse Problems via Differentiable Radio Frequency Digital Twin", "comment": null, "summary": "Digital twins (DTs), virtual simulated replicas of physical scenes, are\ntransforming various industries. However, their potential in radio frequency\n(RF) sensing applications has been limited by the unidirectional nature of\nconventional RF simulators. In this paper, we present InverTwin, an\noptimization-driven framework that creates RF digital twins by enabling\nbidirectional interaction between virtual and physical realms. InverTwin\novercomes the fundamental differentiability challenges of RF optimization\nproblems through novel design components, including path-space differentiation\nto address discontinuity in complex simulation functions, and a radar surrogate\nmodel to mitigate local non-convexity caused by RF signal periodicity. These\ntechniques enable smooth gradient propagation and robust optimization of the DT\nmodel. Our implementation and experiments demonstrate InverTwin's versatility\nand effectiveness in augmenting both data-driven and model-driven RF sensing\nsystems for DT reconstruction."}
{"id": "2508.14089", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14089", "abs": "https://arxiv.org/abs/2508.14089", "authors": ["Ishaan Mahapatra", "Nihar R. Mahapatra"], "title": "Systematic FAIRness Assessment of Open Voice Biomarker Datasets for Mental Health and Neurodegenerative Diseases", "comment": "To appear in the Proceedings of the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025), Erlangen, Germany, August 25-28, 2025", "summary": "Voice biomarkers--human-generated acoustic signals such as speech, coughing,\nand breathing--are promising tools for scalable, non-invasive detection and\nmonitoring of mental health and neurodegenerative diseases. Yet, their clinical\nadoption remains constrained by inconsistent quality and limited usability of\npublicly available datasets. To address this gap, we present the first\nsystematic FAIR (Findable, Accessible, Interoperable, Reusable) evaluation of\n27 publicly available voice biomarker datasets focused on these disease areas.\nUsing the FAIR Data Maturity Model and a structured, priority-weighted scoring\nmethod, we assessed FAIRness at subprinciple, principle, and composite levels.\nOur analysis revealed consistently high Findability but substantial variability\nand weaknesses in Accessibility, Interoperability, and Reusability. Mental\nhealth datasets exhibited greater variability in FAIR scores, while\nneurodegenerative datasets were slightly more consistent. Repository choice\nalso significantly influenced FAIRness scores. To enhance dataset quality and\nclinical utility, we recommend adopting structured, domain-specific metadata\nstandards, prioritizing FAIR-compliant repositories, and routinely applying\nstructured FAIR evaluation frameworks. These findings provide actionable\nguidance to improve dataset interoperability and reuse, thereby accelerating\nthe clinical translation of voice biomarker technologies."}
{"id": "2508.14623", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14623", "abs": "https://arxiv.org/abs/2508.14623", "authors": ["Simon Dahl Jepsen", "Mads Græsbøll Christensen", "Jesper Rindom Jensen"], "title": "A Study of the Scale Invariant Signal to Distortion Ratio in Speech Separation with Noisy References", "comment": "Accepted for IEEE ASRU 2025, Workshop on Automatic Speech Recognition\n  and Understanding. Copyright (c) 2025 IEEE. 8 pages, 6 figures, 2 tables", "summary": "This paper examines the implications of using the Scale-Invariant\nSignal-to-Distortion Ratio (SI-SDR) as both evaluation and training objective\nin supervised speech separation, when the training references contain noise, as\nis the case with the de facto benchmark WSJ0-2Mix. A derivation of the SI-SDR\nwith noisy references reveals that noise limits the achievable SI-SDR, or leads\nto undesired noise in the separated outputs. To address this, a method is\nproposed to enhance references and augment the mixtures with WHAM!, aiming to\ntrain models that avoid learning noisy references. Two models trained on these\nenhanced datasets are evaluated with the non-intrusive NISQA.v2 metric. Results\nshow reduced noise in separated speech but suggest that processing references\nmay introduce artefacts, limiting overall quality gains. Negative correlation\nis found between SI-SDR and perceived noisiness across models on the WSJ0-2Mix\nand Libri2Mix test sets, underlining the conclusion from the derivation."}
{"id": "2508.14438", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14438", "abs": "https://arxiv.org/abs/2508.14438", "authors": ["Akash Prabakar", "Abhishek Shreekant Bhandiwad", "Abijith Jagannath Kamath", "Chandra Sekhar Seelamantula"], "title": "Weakly-Convex Regularization for Magnetic Resonance Image Denoising", "comment": "Presented in ISCS25", "summary": "Regularization for denoising in magnetic resonance imaging (MRI) is typically\nachieved using convex regularization functions. Recently, deep learning\ntechniques have been shown to provide superior denoising performance. However,\nthis comes at the price of lack of explainability, interpretability and\nstability, which are all crucial to MRI. In this work, we present a\nconstructive approach for designing weakly-convex regularization functions for\nMR image denoising. We show that our technique performs on par with\nstate-of-the-art denoisers for diffusion-weighted MR image denoising. Our\ntechnique can be applied to design weakly-convex convolutional neural networks\nwith prototype activation functions that impart interpretability and are\nprovably convergent. We also show that our technique exhibits fewer denoising\nartifacts by demonstrating its effect on brain microstructure modelling."}
{"id": "2508.14525", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14525", "abs": "https://arxiv.org/abs/2508.14525", "authors": ["Bin Wen", "Tien-Ping Tan"], "title": "EffiFusion-GAN: Efficient Fusion Generative Adversarial Network for Speech Enhancement", "comment": null, "summary": "We introduce EffiFusion-GAN (Efficient Fusion Generative Adversarial\nNetwork), a lightweight yet powerful model for speech enhancement. The model\nintegrates depthwise separable convolutions within a multi-scale block to\ncapture diverse acoustic features efficiently. An enhanced attention mechanism\nwith dual normalization and residual refinement further improves training\nstability and convergence. Additionally, dynamic pruning is applied to reduce\nmodel size while maintaining performance, making the framework suitable for\nresource-constrained environments. Experimental evaluation on the public\nVoiceBank+DEMAND dataset shows that EffiFusion-GAN achieves a PESQ score of\n3.45, outperforming existing models under the same parameter settings."}
{"id": "2508.14709", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14709", "abs": "https://arxiv.org/abs/2508.14709", "authors": ["Heitor R. Guimarães", "Ke Tan", "Juan Azcarreta", "Jesus Alvarez", "Prabhav Agrawal", "Ashutosh Pandey", "Buye Xu"], "title": "Improving Resource-Efficient Speech Enhancement via Neural Differentiable DSP Vocoder Refinement", "comment": "Accepted to the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU)", "summary": "Deploying speech enhancement (SE) systems in wearable devices, such as smart\nglasses, is challenging due to the limited computational resources on the\ndevice. Although deep learning methods have achieved high-quality results,\ntheir computational cost limits their feasibility on embedded platforms. This\nwork presents an efficient end-to-end SE framework that leverages a\nDifferentiable Digital Signal Processing (DDSP) vocoder for high-quality speech\nsynthesis. First, a compact neural network predicts enhanced acoustic features\nfrom noisy speech: spectral envelope, fundamental frequency (F0), and\nperiodicity. These features are fed into the DDSP vocoder to synthesize the\nenhanced waveform. The system is trained end-to-end with STFT and adversarial\nlosses, enabling direct optimization at the feature and waveform levels.\nExperimental results show that our method improves intelligibility and quality\nby 4% (STOI) and 19% (DNSMOS) over strong baselines without significantly\nincreasing computation, making it well-suited for real-time applications."}
{"id": "2508.14458", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14458", "abs": "https://arxiv.org/abs/2508.14458", "authors": ["Jingjing Zhao", "Haowen Song", "Xidong Mu", "Kaiquan Cai", "Yanbo Zhu", "Yuanwei Liu"], "title": "Pinching-Antenna Systems-Enabled Multi-User Communications: Transmission Structures and Beamforming Optimization", "comment": null, "summary": "Pinching-antenna systems (PASS) represent an innovative advancement in\nflexible-antenna technologies, aimed at significantly improving wireless\ncommunications by ensuring reliable line-of-sight connections and dynamic\nantenna array reconfigurations. To employ multi-waveguide PASS in multi-user\ncommunications, three practical transmission structures are proposed, namely\nwaveguide multiplexing (WM), waveguide division (WD), and waveguide switching\n(WS). Based on the proposed structures, the joint baseband signal processing\nand pinching beamforming design is studied for a general multi-group multicast\ncommunication system, with the unicast communication encompassed as a special\ncase. A max-min fairness problem is formulated for each proposed transmission\nstructure, subject to the maximum transmit power constraint. For WM, to solve\nthe highly-coupled and non-convex MMF problem with complex exponential and\nfractional expressions, a penalty dual decomposition (PDD)-based algorithm is\ninvoked for obtaining locally optimal solutions. Specifically, the augmented\nLagrangian relaxation is first applied to alleviate the stringent coupling\nconstraints, which is followed by the block decomposition over the resulting\naugmented Lagrangian function. Then, the proposed PDD-based algorithm is\nextended to solve the MMF problem for both WD and WS. Furthermore, a\nlow-complexity algorithm is proposed for the unicast case employing the WS\nstructure, by simultaneously aligning the signal phases and minimizing the\nlarge-scale path loss at each user. Finally, numerical results reveal that: 1)\nthe MMF performance is significantly improved by employing the PASS compared to\nconventional fixed-position antenna systems; 2) WS and WM are suitable for\nunicast and multicast communications, respectively; 3) the performance gap\nbetween WD and WM can be significantly alleviated when the users are\ngeographically isolated."}
{"id": "2508.14556", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14556", "abs": "https://arxiv.org/abs/2508.14556", "authors": ["Euiyeon Kim", "Yong-Hoon Choi"], "title": "Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions", "comment": null, "summary": "We introduce a new music source separation model tailored for accurate vocal\nisolation. Unlike Transformer-based approaches, which often fail to capture\nintermittently occurring vocals, our model leverages Mamba2, a recent state\nspace model, to better capture long-range temporal dependencies. To handle long\ninput sequences efficiently, we combine a band-splitting strategy with a\ndual-path architecture. Experiments show that our approach outperforms recent\nstate-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to\ndate-and delivering substantial gains in uSDR. Moreover, the model exhibits\nstable and consistent performance across varying input lengths and vocal\noccurrence patterns. These results demonstrate the effectiveness of Mamba-based\nmodels for high-resolution audio processing and open up new directions for\nbroader applications in audio research."}
{"id": "2508.14713", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14713", "abs": "https://arxiv.org/abs/2508.14713", "authors": ["Zhipeng Li", "Xiaofen Xing", "Jingyuan Xing", "Hangrui Hu", "Heng Lu", "Xiangmin Xu"], "title": "Long-Context Speech Synthesis with Context-Aware Memory", "comment": "Accepted by Interspeech25", "summary": "In long-text speech synthesis, current approaches typically convert text to\nspeech at the sentence-level and concatenate the results to form\npseudo-paragraph-level speech. These methods overlook the contextual coherence\nof paragraphs, leading to reduced naturalness and inconsistencies in style and\ntimbre across the long-form speech. To address these issues, we propose a\nContext-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The\nCAM block integrates and retrieves both long-term memory and local context\ndetails, enabling dynamic memory updates and transfers within long paragraphs\nto guide sentence-level speech synthesis. Furthermore, the prefix mask enhances\nthe in-context learning ability by enabling bidirectional attention on prefix\ntokens while maintaining unidirectional generation. Experimental results\ndemonstrate that the proposed method outperforms baseline and state-of-the-art\nlong-context methods in terms of prosody expressiveness, coherence and context\ninference cost across paragraph-level speech."}
{"id": "2508.14611", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14611", "abs": "https://arxiv.org/abs/2508.14611", "authors": ["Jinkun Yang"], "title": "FPGA Design and Implementation of Fixed-Point Fast Divider Using Goldschmidt Division Algorithm and Mitchell Multiplication Algorithm", "comment": "7 pages,9 figures", "summary": "This paper presents a variable bit-width fixed-point fast divider using\nGoldschmidt division algorithm and Mitchell multiplication algorithm. Described\nusing Verilog HDL and implemented on a Xilinx XC7Z020-2CLG400I FPGA, the\nproposed divider achieves over 99% computational accuracy with a minimum\nlatency of 99.1 ns, which is 31.7 ns faster than existing single-precision\ndividers. Compared with a Goldschmidt divider using a Vedic multiplier, the\nproposed design reduces Slice Registers by 46.68%, Slice LUTs by 4.93%, and\nSlices by 11.85%, with less than 1% accuracy loss and only 24.1 ns additional\ndelay. These results demonstrate an improved balance between computational\nspeed and resource utilization, making the divider well-suited for\nhigh-performance FPGA-based systems with strict resource constraints."}
{"id": "2508.14688", "categories": ["cs.SD", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14688", "abs": "https://arxiv.org/abs/2508.14688", "authors": ["Veronica Ruozzi", "Sasan Matinfar", "Laura Schütz", "Benedikt Wiestler", "Alberto Redaelli", "Emiliano Votta", "Nassir Navab"], "title": "BioSonix: Can Physics-Based Sonification Perceptualize Tissue Deformations From Tool Interactions?", "comment": "V. Ruozzi and S. Matinfar contributed equally to this work", "summary": "Perceptualizing tool interactions with deformable structures in surgical\nprocedures remains challenging, as unimodal visualization techniques often fail\nto capture the complexity of these interactions due to constraints such as\nocclusion and limited depth perception. This paper presents a novel approach to\naugment tool navigation in mixed reality environments by providing auditory\nrepresentations of tool-tissue dynamics, particularly for interactions with\nsoft tissue. BioSonix, a physics-informed design framework, utilizes tissue\ndisplacements in 3D space to compute excitation forces for a sound model\nencoding tissue properties such as stiffness and density. Biomechanical\nsimulations were employed to model particle displacements resulting from\ntool-tissue interactions, establishing a robust foundation for the method. An\noptimization approach was used to define configurations for capturing diverse\ninteraction scenarios with varying tool trajectories. Experiments were\nconducted to validate the accuracy of the sound-displacement mappings.\nAdditionally, two user studies were performed: the first involved two clinical\nprofessionals (a neuroradiologist and a cardiologist), who confirmed the\nmethod's impact and achieved high task accuracy; the second included 22\nbiomedical experts, who demonstrated high discrimination accuracy in tissue\ndifferentiation and targeting tasks. The results revealed a strong correlation\nbetween tool-tissue dynamics and their corresponding auditory profiles,\nhighlighting the potential of these sound representations to enhance the\nintuitive understanding of complex interactions."}
{"id": "2508.14732", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14732", "abs": "https://arxiv.org/abs/2508.14732", "authors": ["Zijun Huang", "Chengdong Liang", "Jiadi Yao", "Xiao-Lei Zhang"], "title": "PadAug: Robust Speaker Verification with Simple Waveform-Level Silence Padding", "comment": null, "summary": "The presence of non-speech segments in utterances often leads to the\nperformance degradation of speaker verification. Existing systems usually use\nvoice activation detection as a preprocessing step to cut off long silence\nsegments. However, short silence segments, particularly those between speech\nsegments, still remain a problem for speaker verification. To address this\nissue, in this paper, we propose a simple wave-level data augmentation method,\n\\textit{PadAug}, which aims to enhance the system's robustness to silence\nsegments. The core idea of \\textit{PadAug} is to concatenate silence segments\nwith speech segments at the waveform level for model training. Due to its\nsimplicity, it can be directly applied to the current state-of-the art\narchitectures. Experimental results demonstrate the effectiveness of the\nproposed \\textit{PadAug}. For example, applying \\textit{PadAug} to ResNet34\nachieves a relative equal error rate reduction of 5.0\\% on the voxceleb\ndataset. Moreover, the \\textit{PadAug} based systems are robust to different\nlengths and proportions of silence segments in the test data."}
{"id": "2508.14637", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14637", "abs": "https://arxiv.org/abs/2508.14637", "authors": ["Jinkun Yang", "Pengbin Xu"], "title": "Design of a Gm-C Dynamic Amplifier with High Linearity and High Temperature and Power Supply Voltage Stability", "comment": "5 pages, 15 figures", "summary": "This paper presents a Gm-C dynamic amplifier with high linearity and high\ntemperature and power supply voltage stability. The main part of the amplifier\nemploys two asymmetric differential pairs to enhance transconductance\nlinearity. The amplifier maintains a nearly constant gain within a differential\ninput range of -40 mV to 40 mV, and achieves a total harmonic distortion (THD)\nof 70.5 dB. The bias part of the amplifier adopts a constant-gm bias circuit,\nwhich improves the temperature and supply voltage stability of the amplifier's\ntransconductance and gain. When the differential input is 1 mV, the power\nsupply voltage fluctuates by $\\pm$10%, and the temperature varies between\n-40$\\mathrm{^\\circ C}$ and 120$\\mathrm{^\\circ C}$, the standard deviation of\nthe gain distribution is 262m, and the distribution range is from 15.1 to 16.3."}
{"id": "2508.14689", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14689", "abs": "https://arxiv.org/abs/2508.14689", "authors": ["Yucong Zhang", "Juan Liu", "Ming Li"], "title": "ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal", "comment": null, "summary": "Pre-trained foundation models have demonstrated remarkable success in vision\nand language, yet their potential for general machine signal modeling-covering\nacoustic, vibration, and other industrial sensor data-remains under-explored.\nExisting approach using sub-band-based encoders has achieved competitive\nresults but are limited by fixed input lengths, and the absence of explicit\nfrequency positional encoding. In this work, we propose a novel foundation\nmodel that integrates an advanced band-split architecture with relative\nfrequency positional embeddings, enabling precise spectral localization across\narbitrary sampling configurations. The model supports inputs of arbitrary\nlength without padding or segmentation, producing a concise embedding that\nretains both temporal and spectral fidelity. We evaluate our method on SIREN\n(https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmark\nfor machine signal encoding that unifies multiple datasets, including all DCASE\ntask 2 challenges (2020-2025) and widely-used industrial signal corpora.\nExperimental results demonstrate consistent state-of-the-art performance in\nanomaly detection and fault identification, confirming the effectiveness and\ngeneralization capability of the proposed model. We open-sourced ECHO on\nhttps://github.com/yucongzh/ECHO."}
{"id": "2508.14089", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14089", "abs": "https://arxiv.org/abs/2508.14089", "authors": ["Ishaan Mahapatra", "Nihar R. Mahapatra"], "title": "Systematic FAIRness Assessment of Open Voice Biomarker Datasets for Mental Health and Neurodegenerative Diseases", "comment": "To appear in the Proceedings of the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025), Erlangen, Germany, August 25-28, 2025", "summary": "Voice biomarkers--human-generated acoustic signals such as speech, coughing,\nand breathing--are promising tools for scalable, non-invasive detection and\nmonitoring of mental health and neurodegenerative diseases. Yet, their clinical\nadoption remains constrained by inconsistent quality and limited usability of\npublicly available datasets. To address this gap, we present the first\nsystematic FAIR (Findable, Accessible, Interoperable, Reusable) evaluation of\n27 publicly available voice biomarker datasets focused on these disease areas.\nUsing the FAIR Data Maturity Model and a structured, priority-weighted scoring\nmethod, we assessed FAIRness at subprinciple, principle, and composite levels.\nOur analysis revealed consistently high Findability but substantial variability\nand weaknesses in Accessibility, Interoperability, and Reusability. Mental\nhealth datasets exhibited greater variability in FAIR scores, while\nneurodegenerative datasets were slightly more consistent. Repository choice\nalso significantly influenced FAIRness scores. To enhance dataset quality and\nclinical utility, we recommend adopting structured, domain-specific metadata\nstandards, prioritizing FAIR-compliant repositories, and routinely applying\nstructured FAIR evaluation frameworks. These findings provide actionable\nguidance to improve dataset interoperability and reuse, thereby accelerating\nthe clinical translation of voice biomarker technologies."}
{"id": "2508.14739", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14739", "abs": "https://arxiv.org/abs/2508.14739", "authors": ["Fatih Ayten", "Mehmet C. Ilter", "Akshay Jain", "Ossi Kaltiokallio", "Jukka Talvitie", "Elena Simona Lohan", "Henk Wymeersch", "Mikko Valkama"], "title": "Failure Tolerant Phase-Only Indoor Positioning via Deep Learning", "comment": null, "summary": "High-precision localization turns into a crucial added value and asset for\nnext-generation wireless systems. Carrier phase positioning (CPP) enables\nsub-meter to centimeter-level accuracy and is gaining interest in 5G-Advanced\nstandardization. While CPP typically complements time-of-arrival (ToA)\nmeasurements, recent literature has introduced a phase-only positioning\napproach in a distributed antenna/MIMO system context with minimal bandwidth\nrequirements, using deep learning (DL) when operating under ideal hardware\nassumptions. In more practical scenarios, however, antenna failures can largely\ndegrade the performance. In this paper, we address the challenging phase-only\npositioning task, and propose a new DL-based localization approach harnessing\nthe so-called hyperbola intersection principle, clearly outperforming the\nprevious methods. Additionally, we consider and propose a processing and\nlearning mechanism that is robust to antenna element failures. Our results show\nthat the proposed DL model achieves robust and accurate positioning despite\nantenna impairments, demonstrating the viability of data-driven,\nimpairment-tolerant phase-only positioning mechanisms. Comprehensive set of\nnumerical results demonstrates large improvements in localization accuracy\nagainst the prior art methods."}
{"id": "2508.14115", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14115", "abs": "https://arxiv.org/abs/2508.14115", "authors": ["Taous Iatariene", "Alexandre Guérin", "Romain Serizel"], "title": "Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings", "comment": null, "summary": "Speaker embeddings are promising identity-related features that can enhance\nthe identity assignment performance of a tracking system by leveraging its\nspatial predictions, i.e, by performing identity reassignment. Common speaker\nembedding extractors usually struggle with short temporal contexts and\noverlapping speech, which imposes long-term identity reassignment to exploit\nlonger temporal contexts. However, this increases the probability of tracking\nsystem errors, which in turn impacts negatively on identity reassignment. To\naddress this, we propose a Knowledge Distillation (KD) based training approach\nfor short context speaker embedding extraction from two speaker mixtures. We\nleverage the spatial information of the speaker of interest using beamforming\nto reduce overlap. We study the feasibility of performing identity reassignment\nover blocks of fixed size, i.e., blockwise identity reassignment, to go towards\na low-latency speaker embedding based tracking system. Results demonstrate that\nour distilled models are effective at short-context embedding extraction and\nmore robust to overlap. Although, blockwise reassignment results indicate that\nfurther work is needed to handle simultaneous speech more effectively."}
{"id": "2508.14556", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14556", "abs": "https://arxiv.org/abs/2508.14556", "authors": ["Euiyeon Kim", "Yong-Hoon Choi"], "title": "Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions", "comment": null, "summary": "We introduce a new music source separation model tailored for accurate vocal\nisolation. Unlike Transformer-based approaches, which often fail to capture\nintermittently occurring vocals, our model leverages Mamba2, a recent state\nspace model, to better capture long-range temporal dependencies. To handle long\ninput sequences efficiently, we combine a band-splitting strategy with a\ndual-path architecture. Experiments show that our approach outperforms recent\nstate-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to\ndate-and delivering substantial gains in uSDR. Moreover, the model exhibits\nstable and consistent performance across varying input lengths and vocal\noccurrence patterns. These results demonstrate the effectiveness of Mamba-based\nmodels for high-resolution audio processing and open up new directions for\nbroader applications in audio research."}
{"id": "2508.14753", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14753", "abs": "https://arxiv.org/abs/2508.14753", "authors": ["Ahsan Nazar", "Zhambyl Shaikhanov", "Sennur Ulukus"], "title": "Full-Duplex Beamforming Optimization for Near-Field ISAC", "comment": null, "summary": "Integrated Sensing and Communications (ISAC) is a promising technology for\nfuture wireless networks, enabling simultaneous communication and sensing using\nshared resources. This paper investigates the performance of full-duplex (FD)\ncommunication in near-field ISAC systems, where spherical-wave propagation\nintroduces unique beam-focusing capabilities. We propose a joint optimization\nframework for transmit and receive beamforming at the base station to minimize\ntransmit power while satisfying rate constraints for multi-user downlink\ntransmission, multi-user uplink reception, and multi-target sensing. Our\napproach employs alternating optimization combined with semidefinite relaxation\nand Rayleigh quotient techniques to address the non-convexity of the problem.\nSimulation results demonstrate that FD-enabled near-field ISAC achieves\nsuperior power efficiency compared to half-duplex and far-field benchmarks,\neffectively detecting targets at identical angles while meeting communication\nrequirements."}
{"id": "2508.14623", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14623", "abs": "https://arxiv.org/abs/2508.14623", "authors": ["Simon Dahl Jepsen", "Mads Græsbøll Christensen", "Jesper Rindom Jensen"], "title": "A Study of the Scale Invariant Signal to Distortion Ratio in Speech Separation with Noisy References", "comment": "Accepted for IEEE ASRU 2025, Workshop on Automatic Speech Recognition\n  and Understanding. Copyright (c) 2025 IEEE. 8 pages, 6 figures, 2 tables", "summary": "This paper examines the implications of using the Scale-Invariant\nSignal-to-Distortion Ratio (SI-SDR) as both evaluation and training objective\nin supervised speech separation, when the training references contain noise, as\nis the case with the de facto benchmark WSJ0-2Mix. A derivation of the SI-SDR\nwith noisy references reveals that noise limits the achievable SI-SDR, or leads\nto undesired noise in the separated outputs. To address this, a method is\nproposed to enhance references and augment the mixtures with WHAM!, aiming to\ntrain models that avoid learning noisy references. Two models trained on these\nenhanced datasets are evaluated with the non-intrusive NISQA.v2 metric. Results\nshow reduced noise in separated speech but suggest that processing references\nmay introduce artefacts, limiting overall quality gains. Negative correlation\nis found between SI-SDR and perceived noisiness across models on the WSJ0-2Mix\nand Libri2Mix test sets, underlining the conclusion from the derivation."}
{"id": "2508.14688", "categories": ["cs.SD", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14688", "abs": "https://arxiv.org/abs/2508.14688", "authors": ["Veronica Ruozzi", "Sasan Matinfar", "Laura Schütz", "Benedikt Wiestler", "Alberto Redaelli", "Emiliano Votta", "Nassir Navab"], "title": "BioSonix: Can Physics-Based Sonification Perceptualize Tissue Deformations From Tool Interactions?", "comment": "V. Ruozzi and S. Matinfar contributed equally to this work", "summary": "Perceptualizing tool interactions with deformable structures in surgical\nprocedures remains challenging, as unimodal visualization techniques often fail\nto capture the complexity of these interactions due to constraints such as\nocclusion and limited depth perception. This paper presents a novel approach to\naugment tool navigation in mixed reality environments by providing auditory\nrepresentations of tool-tissue dynamics, particularly for interactions with\nsoft tissue. BioSonix, a physics-informed design framework, utilizes tissue\ndisplacements in 3D space to compute excitation forces for a sound model\nencoding tissue properties such as stiffness and density. Biomechanical\nsimulations were employed to model particle displacements resulting from\ntool-tissue interactions, establishing a robust foundation for the method. An\noptimization approach was used to define configurations for capturing diverse\ninteraction scenarios with varying tool trajectories. Experiments were\nconducted to validate the accuracy of the sound-displacement mappings.\nAdditionally, two user studies were performed: the first involved two clinical\nprofessionals (a neuroradiologist and a cardiologist), who confirmed the\nmethod's impact and achieved high task accuracy; the second included 22\nbiomedical experts, who demonstrated high discrimination accuracy in tissue\ndifferentiation and targeting tasks. The results revealed a strong correlation\nbetween tool-tissue dynamics and their corresponding auditory profiles,\nhighlighting the potential of these sound representations to enhance the\nintuitive understanding of complex interactions."}
{"id": "2508.14884", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14884", "abs": "https://arxiv.org/abs/2508.14884", "authors": ["Brian Kim", "Justin H. Kong", "Terrence J. Moore", "Fikadu T. Dagefu"], "title": "Deep Reinforcement Learning Based Routing for Heterogeneous Multi-Hop Wireless Networks", "comment": null, "summary": "Routing in multi-hop wireless networks is a complex problem, especially in\nheterogeneous networks where multiple wireless communication technologies\ncoexist. Reinforcement learning (RL) methods, such as Q-learning, have been\nintroduced for decentralized routing by allowing nodes to make decisions based\non local observations. However, Q-learning suffers from scalability issues and\npoor generalization due to the difficulty in managing the Q-table in large or\ndynamic network topologies, especially in heterogeneous networks (HetNets) with\ndiverse channel characteristics. Thus, in this paper, we propose a novel deep\nQ-network (DQN)-based routing framework for heterogeneous multi-hop wireless\nnetworks to maximize the end-to-end rate of the route by improving scalability\nand adaptability, where each node uses a deep neural network (DNN) to estimate\nthe Q-values and jointly select the next-hop relay and a communication\ntechnology for transmission. To achieve better performance with the DNN,\nselecting which nodes to exchange information is critical, as it not only\ndefines the state and action spaces but also determines the input to the DNN.\nTo this end, we propose neighbor node selection strategies based on channel\ngain and rate between nodes rather than a simple distance-based approach for an\nimproved set of states and actions for DQN-based routing. During training, the\nmodel experiences diverse network topologies to ensure generalization and\nrobustness, and simulation results show that the proposed neighbor node\nselection outperforms simple distance-based selection. Further, we observe that\nthe DQN-based approach outperforms various benchmark schemes and performs\ncomparably to the optimal approach."}
{"id": "2508.14709", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14709", "abs": "https://arxiv.org/abs/2508.14709", "authors": ["Heitor R. Guimarães", "Ke Tan", "Juan Azcarreta", "Jesus Alvarez", "Prabhav Agrawal", "Ashutosh Pandey", "Buye Xu"], "title": "Improving Resource-Efficient Speech Enhancement via Neural Differentiable DSP Vocoder Refinement", "comment": "Accepted to the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU)", "summary": "Deploying speech enhancement (SE) systems in wearable devices, such as smart\nglasses, is challenging due to the limited computational resources on the\ndevice. Although deep learning methods have achieved high-quality results,\ntheir computational cost limits their feasibility on embedded platforms. This\nwork presents an efficient end-to-end SE framework that leverages a\nDifferentiable Digital Signal Processing (DDSP) vocoder for high-quality speech\nsynthesis. First, a compact neural network predicts enhanced acoustic features\nfrom noisy speech: spectral envelope, fundamental frequency (F0), and\nperiodicity. These features are fed into the DDSP vocoder to synthesize the\nenhanced waveform. The system is trained end-to-end with STFT and adversarial\nlosses, enabling direct optimization at the feature and waveform levels.\nExperimental results show that our method improves intelligibility and quality\nby 4% (STOI) and 19% (DNSMOS) over strong baselines without significantly\nincreasing computation, making it well-suited for real-time applications."}
{"id": "2508.14115", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14115", "abs": "https://arxiv.org/abs/2508.14115", "authors": ["Taous Iatariene", "Alexandre Guérin", "Romain Serizel"], "title": "Towards Low-Latency Tracking of Multiple Speakers With Short-Context Speaker Embeddings", "comment": null, "summary": "Speaker embeddings are promising identity-related features that can enhance\nthe identity assignment performance of a tracking system by leveraging its\nspatial predictions, i.e, by performing identity reassignment. Common speaker\nembedding extractors usually struggle with short temporal contexts and\noverlapping speech, which imposes long-term identity reassignment to exploit\nlonger temporal contexts. However, this increases the probability of tracking\nsystem errors, which in turn impacts negatively on identity reassignment. To\naddress this, we propose a Knowledge Distillation (KD) based training approach\nfor short context speaker embedding extraction from two speaker mixtures. We\nleverage the spatial information of the speaker of interest using beamforming\nto reduce overlap. We study the feasibility of performing identity reassignment\nover blocks of fixed size, i.e., blockwise identity reassignment, to go towards\na low-latency speaker embedding based tracking system. Results demonstrate that\nour distilled models are effective at short-context embedding extraction and\nmore robust to overlap. Although, blockwise reassignment results indicate that\nfurther work is needed to handle simultaneous speech more effectively."}
{"id": "2508.14713", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14713", "abs": "https://arxiv.org/abs/2508.14713", "authors": ["Zhipeng Li", "Xiaofen Xing", "Jingyuan Xing", "Hangrui Hu", "Heng Lu", "Xiangmin Xu"], "title": "Long-Context Speech Synthesis with Context-Aware Memory", "comment": "Accepted by Interspeech25", "summary": "In long-text speech synthesis, current approaches typically convert text to\nspeech at the sentence-level and concatenate the results to form\npseudo-paragraph-level speech. These methods overlook the contextual coherence\nof paragraphs, leading to reduced naturalness and inconsistencies in style and\ntimbre across the long-form speech. To address these issues, we propose a\nContext-Aware Memory (CAM)-based long-context Text-to-Speech (TTS) model. The\nCAM block integrates and retrieves both long-term memory and local context\ndetails, enabling dynamic memory updates and transfers within long paragraphs\nto guide sentence-level speech synthesis. Furthermore, the prefix mask enhances\nthe in-context learning ability by enabling bidirectional attention on prefix\ntokens while maintaining unidirectional generation. Experimental results\ndemonstrate that the proposed method outperforms baseline and state-of-the-art\nlong-context methods in terms of prosody expressiveness, coherence and context\ninference cost across paragraph-level speech."}

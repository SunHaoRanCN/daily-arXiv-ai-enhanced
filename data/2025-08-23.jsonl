{"id": "2508.15257", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15257", "abs": "https://arxiv.org/abs/2508.15257", "authors": ["Eduard E. Bahingayi", "Shuying Lin", "Murat Uysal", "Marco Di Renzo", "Le-Nam Tran"], "title": "A Refined Alternating Optimization for Sum Rate Maximization in SIM-Aided Multiuser MISO Systems", "comment": null, "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a disruptive\ntechnology for future wireless networks. To investigate their capabilities, we\nstudy the sum rate maximization problem in an SIM-based multiuser (MU)\nmultiple-input single-output (MISO) downlink system. A vast majority of pioneer\nstudies, if not all, address this fundamental problem using the prevailing\nalternating optimization (AO) framework, where the digital beamforming (DB) and\nSIM phase shifts are optimized alternately. However, many of these approaches\nsuffer from suboptimal performance, quickly leading to performance saturation,\nwhen the number of SIM layers increases assuming the \\emph{fixed SIM\nthickness}. In this letter, we demonstrate that significant performance gains\ncan still be achieved, and such saturation does not occur with the proposed\nmethod in the considered setting. To this end, we provide practical design\nguidelines to improve AO-based optimization of digital precoders and SIM phase\nshifts. Specifically, we show that (i) optimizing the SIM phase shifts first\nyields significant performance improvements, compared to optimizing the DB\nfirst; and (ii) when applying projected gradient (PG) methods, which are\ngradually becoming more popular to optimize the phase shifts thanks to their\nscalability, we find that using an iterative PG method achieves better\nperformance than the single PG step, which is commonly used in existing\nsolutions. Based on these customizations, the proposed method achieves a higher\nachievable sum rate (ASR) of up to $\\ensuremath{115.53\\%}$, compared to\nbenchmark schemes for the scenarios under consideration."}
{"id": "2508.15375", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15375", "abs": "https://arxiv.org/abs/2508.15375", "authors": ["Hanwen Hu", "Jiancheng An", "Lu Gan", "Chau Yuen"], "title": "Performance Analysis of RIS-Aided High-Mobility Wireless Systems", "comment": "This paper has been accepted for publication in 2025 IEEE 102nd\n  Vehicular Technology Conference", "summary": "Reconfigurable intelligent surface (RIS) technology holds immense potential\nfor increasing the performance of wireless networks. Therefore, RIS is also\nregarded as one of the solutions to address communication challenges in\nhigh-mobility scenarios, such as Doppler shift and fast fading. This paper\ninvestigates a high-speed train (HST) multiple-input single-output (MISO)\ncommunication system aided by a RIS. We propose a block coordinate descent\n(BCD) algorithm to jointly optimize the RIS phase shifts and the transmit\nbeamforming vectors to maximize the channel gain. Numerical results are\nprovided to demonstrate that the proposed algorithm significantly enhances the\nsystem performance, achieving an average channel gain improvement of 15 dB\ncompared to traditional schemes. Additionally, the introduction of RIS\neliminates outage probability and improves key performance metrics such as\nachievable rate, channel capacity, and bit error rate (BER). These findings\nhighlight the critical role of RIS in enhancing HST communication systems."}
{"id": "2508.15544", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15544", "abs": "https://arxiv.org/abs/2508.15544", "authors": ["Pedro H. C. de Souza", "Luiz A. M. Pereira", "Faustino R. Gómez", "Elsa M. Materón", "Jorge Ricardo Mejía-Salazar"], "title": "Lightweight Gradient Descent Optimization for Mitigating Hardware Imperfections in RIS Systems", "comment": null, "summary": "Ongoing discussions about the future of wireless communications are reaching\na turning point as standardization activities for the sixth generation of\nmobile networks (6G) become more mature. New technologies must now face renewed\nscrutiny by the industry and academia in order to be ready for deployment in\nthe near future. Recently, reconfigurable intelligent surfaces (RISs) gained\nattention as a promising solution for improving the propagation conditions of\nsignal transmission in general. The RIS is a planar array of tunable resonant\nelements designed to dynamically and precisely manipulate the reflection of\nincident electromagnetic waves. However, the physical structure of the RIS and\nits components may be subject to practical limitations and imperfections. It is\nimperative that the hardware imperfections (HWIs) associated with the RIS be\nanalyzed, so that it remains a feasible technology from a practical standpoint.\nMoreover, solutions for mitigating the HWIs must be considered, as is discussed\nin this work. More specifically, we introduce a gradient descent optimization\nfor mitigating HWIs in RIS-aided wideband communication systems. Numerical\nresults show that the proposed optimization is able to compensate for HWIs such\nas the phase-shift noise (PSN) and RIS surface deformations."}
{"id": "2508.15581", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15581", "abs": "https://arxiv.org/abs/2508.15581", "authors": ["Pedro H. C. de Souza", "Luciano Mendes"], "title": "Frequency Selective Reflection of Wideband Signals with Reconfigurable Intelligent Surfaces", "comment": null, "summary": "Recently, the reconfigurable intelligent surface (RIS) technology has ushered\nin the prospect of control over the wireless propagation environment. By\nestablishing alternative propagation paths for the transmitted signals, and by\nreflecting them in a controllable manner, the RIS is able to improve the signal\nreception. However, an aspect often overlooked is the potential bandwidth\nrestrictions on the wideband signal reflected by the RIS. If not carefully\nconsidered, this can become an impediment for the adoption of the RIS in the\nnext generation of communications systems. Therefore, in this work we propose a\nRIS configuration method that provides frequency selective signal reflection\nfor wideband signals."}
{"id": "2508.14908", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14908", "abs": "https://arxiv.org/abs/2508.14908", "authors": ["Yue Pan", "Liwei Liu", "Changxin Li", "Xinyao Wang", "Yili Xia", "Hanyue Zhang", "Ming Chu"], "title": "A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification", "comment": null, "summary": "Speech is a cost-effective and non-intrusive data source for identifying\nacute and chronic heart failure (HF). However, there is a lack of research on\nwhether Chinese syllables contain HF-related information, as observed in other\nwell-studied languages. This study presents the first Chinese speech database\nof HF patients, featuring paired recordings taken before and after\nhospitalisation. The findings confirm the effectiveness of the Chinese language\nin HF detection using both standard 'patient-wise' and personalised 'pair-wise'\nclassification approaches, with the latter serving as an ideal\nspeaker-decoupled baseline for future research. Statistical tests and\nclassification results highlight individual differences as key contributors to\ninaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for\nfrequency importance analysis. The data and demonstrations are published at\nhttps://github.com/panyue1998/Voice_HF."}
{"id": "2508.14919", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14919", "abs": "https://arxiv.org/abs/2508.14919", "authors": ["Hadrien Pujol", "Matteo Bevillacqua", "Christophe Thirard", "Thierry Mazoyer"], "title": "Denoising by neural network for muzzle blast detection", "comment": "INTER-NOISE 2024, Aug 2024, Nantes (France), France", "summary": "Acoem develops gunshot detection systems, consisting of a microphone array\nand software that detects and locates shooters on the battlefield. The\nperformance of such systems is obviously affected by the acoustic environment\nin which they are operating: in particular, when mounted on a moving military\nvehicle, the presence of noise reduces the detection performance of the\nsoftware. To limit the influence of the acoustic environment, a neural network\nhas been developed. Instead of using a heavy convolutional neural network, a\nlightweight neural network architecture was chosen to limit the computational\nresources required to embed the algorithm on as many hardware platforms as\npossible. Thanks to the combination of a two hidden layer perceptron and\nappropriate signal processing techniques, the detection rate of impulsive\nmuzzle blast waveforms (the wave coming from the detonation and indicating the\nposition of the shooter) is significantly increased. With a rms value of noise\nof the same order as the muzzle blast peak amplitude, the detect rate is more\nthan doubled with this denoising processing."}
{"id": "2508.15599", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15599", "abs": "https://arxiv.org/abs/2508.15599", "authors": ["P. H. C. de Souza", "M. Khazaee", "L. L. Mendes"], "title": "On the Compromise Between Performance and Efficiency in RIS-aided Communication Systems", "comment": "To be published in the PIERS2025 Abu Dhabi proceedings", "summary": "The reconfigurable intelligent surface (RIS) technology for metasurfaces is\nushering in a new paradigm for wireless communication systems. It provides an\naccessible way for controlling the interaction between electromagnetic waves\nwith the propagation medium. One particularly important aspect is the\nconfiguration of the RIS elements or reflectors. Simply stated, the objective\nof the RIS configuration is to choose the optimum phase-shift combination that\nmaximizes the channel capacity. Recently, neural networks (NNs) were proposed\nfor tackling this task and results have shown that the proposed NN promotes far\nless reconfigurations of the RIS, consequently reducing the configuration\noverhead. Beyond that, the RIS can be repurposed for tackling the Doppler shift\nin high-mobility communication systems. Despite not being its usual primary\ngoal, results have also demonstrated that the RIS can compensate for the\nDoppler shift at a small cost in performance. However, the typical\nreflection-only constraint for RIS systems limits the spatial coverage and\nsignal amplification potential achieved by such systems. Therefore, the\nsimultaneously transmitting and reflecting reconfigurable intelligent surface\n(STAR-RIS) can be employed to address these limitations by its dual\nfunctionality of transmitting and reflecting signals concurrently. It can be\nshown that the STAR-RIS can augment coverage, energy efficiency, and latency\nreduction, while enhancing sum-rate and physical-layer security across several\nwireless contexts."}
{"id": "2508.14916", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14916", "abs": "https://arxiv.org/abs/2508.14916", "authors": ["Xiaoxiao Li", "An Zhu", "Youhai Jiang", "Fengjie Zhu"], "title": "Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge", "comment": null, "summary": "This paper presents the architecture and performance of a novel Multilingual\nAutomatic Speech Recognition (ASR) system developed by the Transsion Speech\nTeam for Track 1 of the MLC-SLM 2025 Challenge. The proposed system comprises\nthree key components: 1) a frozen Whisper-large-v3 based speech encoder,\nleveraging large-scale pretraining to ensure robust acoustic feature\nextraction; 2) a trainable adaptor module using Linear-ReLU-Linear\ntransformation mechanisms to effectively align speech and text representations;\nand 3) a frozen Qwen2.5-7B-Instruct large language model (LLM) integrated with\ntrainable LoRA for optimized contextual linguistic decoding. By systematically\ncombining pretrained models with task specific fine-tuning, the system achieved\na word/character error rate (WER/CER) of 9.83% across 11 languages in the\nevaluation set and ranked third place among global participants."}
{"id": "2508.14920", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14920", "abs": "https://arxiv.org/abs/2508.14920", "authors": ["Ilya Fedorov", "Dmitry Korobchenko"], "title": "Human Feedback Driven Dynamic Speech Emotion Recognition", "comment": null, "summary": "This work proposes to explore a new area of dynamic speech emotion\nrecognition. Unlike traditional methods, we assume that each audio track is\nassociated with a sequence of emotions active at different moments in time. The\nstudy particularly focuses on the animation of emotional 3D avatars. We propose\na multi-stage method that includes the training of a classical speech emotion\nrecognition model, synthetic generation of emotional sequences, and further\nmodel improvement based on human feedback. Additionally, we introduce a novel\napproach to modeling emotional mixtures based on the Dirichlet distribution.\nThe models are evaluated based on ground-truth emotions extracted from a\ndataset of 3D facial animations. We compare our models against the sliding\nwindow approach. Our experimental results show the effectiveness of\nDirichlet-based approach in modeling emotional mixtures. Incorporating human\nfeedback further improves the model quality while providing a simplified\nannotation procedure."}
{"id": "2508.15671", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.15671", "abs": "https://arxiv.org/abs/2508.15671", "authors": ["Nishant Mehrotra", "Sandesh Rao Mattu", "Saif Khan Mohammed", "Ronny Hadani", "Robert Calderbank"], "title": "Discrete Radar based on Modulo Arithmetic", "comment": "36 pages, 7 figures, submitted to EURASIP JASP", "summary": "Zak-OTFS is modulation scheme where signals are formed in the delay-Doppler\n(DD) domain, converted to the time domain (DD) for transmission and reception,\nthen returned to the DD domain for processing. We describe how to use the same\narchitecture for radar sensing. The intended delay resolution is $\\frac{1}{B}$\nwhere $B$ is the radar bandwidth, and the intended Doppler resolution is\n$\\frac{1}{T}$ where $T$ is the transmission time. We form a radar waveform in\nthe DD domain, illuminate the scattering environment, match filter the return,\nthen correlate with delay and Doppler shifts of the transmitted waveform. This\nproduces an image of the scattering environment, and the radar ambiguity\nfunction expresses the blurriness of this image. The possible delay and Doppler\nshifts generate the continuous Heisenberg-Weyl group which has been widely\nstudied in the theory of radar. We describe how to approach the problem of\nwaveform design, not from the perspective of this continuous group, but from\nthe perspective of a discrete group of delay and Doppler shifts, where the\ndiscretization is determined by the intended delay and Doppler resolution of\nthe radar. We describe how to approach the problem of shaping the ambiguity\nsurface through symplectic transformations that normalize our discrete\nHeisenberg-Weyl group. The complexity of traditional continuous radar signal\nprocessing is $\\mathcal{O}\\big(B^2T^2\\big)$. We describe how to reduce this\ncomplexity to $\\mathcal{O}\\big(BT\\log T\\big)$ by choosing the radar waveform to\nbe a common eigenvector of a maximal commutative subgroup of our discrete\nHeisenberg-Weyl group. The theory of symplectic transformations also enables\ndefining libraries of optimal radar waveforms with small peak-to-average power\nratios."}
{"id": "2508.15442", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15442", "abs": "https://arxiv.org/abs/2508.15442", "authors": ["Chenlin Liu", "Minghui Fang", "Patrick Zhang", "Wei Zhou", "Jie Gao", "Jiqing Han"], "title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets", "comment": null, "summary": "Language Model (LM)-based Text-to-Speech (TTS) systems often generate\nhallucinated speech that deviates from input text. Existing mitigation\nstrategies either demand excessive training resources or introduce significant\ninference latency. In this paper, we propose GFlOwNet-guided distribution\nAlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates\nhallucinations without relying on massive resources or inference cost.\nSpecifically, we first conduct an uncertainty analysis, revealing a strong\npositive correlation between hallucination and model uncertainty. Based on\nthis, we reformulate TTS generation as a trajectory flow optimization problem\nand introduce an enhanced Subtrajectory Balance objective together with a\nsharpened internal reward as target distribution. We further integrate reward\ntemperature decay and learning rate optimization for stability and performance\nbalance. Extensive experiments show that GOAT reduce over 50% character error\nrates on challenging test cases and lowering uncertainty by up to 58%,\ndemonstrating its strong generalization ability and effectiveness."}
{"id": "2508.14949", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14949", "abs": "https://arxiv.org/abs/2508.14949", "authors": ["Patricia Amado-Caballero", "Luis Miguel San-José-Revuelta", "María Dolores Aguilar-García", "José Ramón Garmendia-Leiza", "Carlos Alberola-López", "Pablo Casaseca-de-la-Higuera"], "title": "XAI-Driven Spectral Analysis of Cough Sounds for Respiratory Disease Characterization", "comment": null, "summary": "This paper proposes an eXplainable Artificial Intelligence (XAI)-driven\nmethodology to enhance the understanding of cough sound analysis for\nrespiratory disease management. We employ occlusion maps to highlight relevant\nspectral regions in cough spectrograms processed by a Convolutional Neural\nNetwork (CNN). Subsequently, spectral analysis of spectrograms weighted by\nthese occlusion maps reveals significant differences between disease groups,\nparticularly in patients with COPD, where cough patterns appear more variable\nin the identified spectral regions of interest. This contrasts with the lack of\nsignificant differences observed when analyzing raw spectrograms. The proposed\napproach extracts and analyzes several spectral features, demonstrating the\npotential of XAI techniques to uncover disease-specific acoustic signatures and\nimprove the diagnostic capabilities of cough sound analysis by providing more\ninterpretable results."}
{"id": "2508.15673", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15673", "abs": "https://arxiv.org/abs/2508.15673", "authors": ["Enrico Testi", "Giulia Torcolacci", "Nicolò Decarli", "Davide Dardari", "Enrico Paolini"], "title": "A Grant-free Coded Random Access Scheme for Near-field Communications", "comment": null, "summary": "The industrial Internet of things (IIoT) is revolutionizing industrial\nprocesses by facilitating massive machine-type communications among countless\ninterconnected devices. To efficiently handle the resulting large-scale and\nsporadic traffic, grant-free random access protocols-especially coded random\naccess (CRA)-have emerged as scalable and reliable solutions. At the same time,\nadvancements in wireless hardware, including extremely large-scale MIMO arrays\nand high-frequency communication (e.g., mmWave, Terahertz), are pushing network\noperations into the near-field propagation regime, allowing for dense\nconnectivity and enhanced spatial multiplexing. This paper proposes an\ninnovative approach that combines near-field spatial multiplexing with the\ninterference mitigation capabilities of CRA, utilizing an extremely large\naperture array at the access point. This integration improves reliability and\nreduces access latency, offering a robust framework for IIoT connectivity in\nnext-generation 6G networks."}
{"id": "2508.15473", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15473", "abs": "https://arxiv.org/abs/2508.15473", "authors": ["Ching-Chih Sung", "Cheng-Hung Hsin", "Yu-Anne Shiah", "Bo-Jyun Lin", "Yi-Xuan Lai", "Chia-Ying Lee", "Yu-Te Wang", "Borchin Su", "Yu Tsao"], "title": "EffortNet: A Deep Learning Framework for Objective Assessment of Speech Enhancement Technologies Using EEG-Based Alpha Oscillations", "comment": null, "summary": "This paper presents EffortNet, a novel deep learning framework for decoding\nindividual listening effort from electroencephalography (EEG) during speech\ncomprehension. Listening effort represents a significant challenge in\nspeech-hearing research, particularly for aging populations and those with\nhearing impairment. We collected 64-channel EEG data from 122 participants\nduring speech comprehension under four conditions: clean, noisy, MMSE-enhanced,\nand Transformer-enhanced speech. Statistical analyses confirmed that alpha\noscillations (8-13 Hz) exhibited significantly higher power during noisy speech\nprocessing compared to clean or enhanced conditions, confirming their validity\nas objective biomarkers of listening effort. To address the substantial\ninter-individual variability in EEG signals, EffortNet integrates three\ncomplementary learning paradigms: self-supervised learning to leverage\nunlabeled data, incremental learning for progressive adaptation to individual\ncharacteristics, and transfer learning for efficient knowledge transfer to new\nsubjects. Our experimental results demonstrate that Effort- Net achieves 80.9%\nclassification accuracy with only 40% training data from new subjects,\nsignificantly outperforming conventional CNN (62.3%) and STAnet (61.1%) models.\nThe probability-based metric derived from our model revealed that\nTransformer-enhanced speech elicited neural responses more similar to clean\nspeech than MMSEenhanced speech. This finding contrasted with subjective\nintelligibility ratings but aligned with objective metrics. The proposed\nframework provides a practical solution for personalized assessment of hearing\ntechnologies, with implications for designing cognitive-aware speech\nenhancement systems."}
{"id": "2508.15088", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15088", "abs": "https://arxiv.org/abs/2508.15088", "authors": ["Prosanta Barai", "Gondy Leroy", "Arif Ahmed"], "title": "Comparative Evaluation of Text and Audio Simplification: A Methodological Replication Study", "comment": null, "summary": "This study serves as a methodological replication of Leroy et al. (2022)\nresearch, which investigated the impact of text simplification on healthcare\ninformation comprehension in the evolving multimedia landscape. Building upon\nthe original studys insights, our replication study evaluates audio content,\nrecognizing its increasing importance in disseminating healthcare information\nin the digital age. Specifically, we explored the influence of text\nsimplification on perceived and actual difficulty when users engage with audio\ncontent automatically generated from that text. Our replication involved 44\nparticipants for whom we assessed their comprehension of healthcare information\npresented as audio created using Leroy et al. (2022) original and simplified\ntexts. The findings from our study highlight the effectiveness of text\nsimplification in enhancing perceived understandability and actual\ncomprehension, aligning with the original studys results. Additionally, we\nexamined the role of education level and language proficiency, shedding light\non their potential impact on healthcare information access and understanding.\nThis research underscores the practical value of text simplification tools in\npromoting health literacy. It suggests the need for tailored communication\nstrategies to reach diverse audiences effectively in the healthcare domain."}
{"id": "2508.15687", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.15687", "abs": "https://arxiv.org/abs/2508.15687", "authors": ["Masoud Nateghi", "Reza Sameni"], "title": "Estimation-Theoretic Bias Reduction for Oscillometric Blood Pressure Readings", "comment": null, "summary": "Oscillometry is the standard method for non-invasive, cuff-based blood\npressure (BP) measurement, but it introduces systematic errors that may impact\nclinical accuracy. This study investigates the sources of these\nerrors--primarily the limitations of oscillometry itself and\nrespiration-induced fluctuations--using BP waveform data from the MIMIC\ndatabase. Oscillometry tends to underestimate systolic BP and overestimate\ndiastolic BP, while respiration introduces cyclical variations that further\ndegrade measurement precision. To mitigate these effects, we propose an\nestimation-theoretic framework employing least squares (LS) and maximum\nlikelihood (ML) methods for correcting both single and repeated BP\nmeasurements. LS estimation supports conventional multi-measurement averaging\nprotocols, whereas the ML approach incorporates prior knowledge of measurement\nerrors, offering improved performance. Our results demonstrate that leveraging\nstatistical priors across multiple readings can enhance the accuracy of\nnon-invasive BP monitoring, with potential implications for improving\ncardiovascular diagnosis and treatment."}
{"id": "2508.14919", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14919", "abs": "https://arxiv.org/abs/2508.14919", "authors": ["Hadrien Pujol", "Matteo Bevillacqua", "Christophe Thirard", "Thierry Mazoyer"], "title": "Denoising by neural network for muzzle blast detection", "comment": "INTER-NOISE 2024, Aug 2024, Nantes (France), France", "summary": "Acoem develops gunshot detection systems, consisting of a microphone array\nand software that detects and locates shooters on the battlefield. The\nperformance of such systems is obviously affected by the acoustic environment\nin which they are operating: in particular, when mounted on a moving military\nvehicle, the presence of noise reduces the detection performance of the\nsoftware. To limit the influence of the acoustic environment, a neural network\nhas been developed. Instead of using a heavy convolutional neural network, a\nlightweight neural network architecture was chosen to limit the computational\nresources required to embed the algorithm on as many hardware platforms as\npossible. Thanks to the combination of a two hidden layer perceptron and\nappropriate signal processing techniques, the detection rate of impulsive\nmuzzle blast waveforms (the wave coming from the detonation and indicating the\nposition of the shooter) is significantly increased. With a rms value of noise\nof the same order as the muzzle blast peak amplitude, the detect rate is more\nthan doubled with this denoising processing."}
{"id": "2508.15334", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15334", "abs": "https://arxiv.org/abs/2508.15334", "authors": ["Guirui Zhong", "Qing Wang", "Jun Du", "Lei Wang", "Mingqi Cai", "Xin Fang"], "title": "An Enhanced Audio Feature Tailored for Anomalous Sound Detection Based on Pre-trained Models", "comment": "13 pages, 3 figures, accepted by ICANN2025", "summary": "Anomalous Sound Detection (ASD) aims at identifying anomalous sounds from\nmachines and has gained extensive research interests from both academia and\nindustry. However, the uncertainty of anomaly location and much redundant\ninformation such as noise in machine sounds hinder the improvement of ASD\nsystem performance. This paper proposes a novel audio feature of filter banks\nwith evenly distributed intervals, ensuring equal attention to all frequency\nranges in the audio, which enhances the detection of anomalies in machine\nsounds. Moreover, based on pre-trained models, this paper presents a\nparameter-free feature enhancement approach to remove redundant information in\nmachine audio. It is believed that this parameter-free strategy facilitates the\neffective transfer of universal knowledge from pre-trained tasks to the ASD\ntask during model fine-tuning. Evaluation results on the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2024 Challenge dataset\ndemonstrate significant improvements in ASD performance with our proposed\nmethods."}
{"id": "2508.14949", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14949", "abs": "https://arxiv.org/abs/2508.14949", "authors": ["Patricia Amado-Caballero", "Luis Miguel San-José-Revuelta", "María Dolores Aguilar-García", "José Ramón Garmendia-Leiza", "Carlos Alberola-López", "Pablo Casaseca-de-la-Higuera"], "title": "XAI-Driven Spectral Analysis of Cough Sounds for Respiratory Disease Characterization", "comment": null, "summary": "This paper proposes an eXplainable Artificial Intelligence (XAI)-driven\nmethodology to enhance the understanding of cough sound analysis for\nrespiratory disease management. We employ occlusion maps to highlight relevant\nspectral regions in cough spectrograms processed by a Convolutional Neural\nNetwork (CNN). Subsequently, spectral analysis of spectrograms weighted by\nthese occlusion maps reveals significant differences between disease groups,\nparticularly in patients with COPD, where cough patterns appear more variable\nin the identified spectral regions of interest. This contrasts with the lack of\nsignificant differences observed when analyzing raw spectrograms. The proposed\napproach extracts and analyzes several spectral features, demonstrating the\npotential of XAI techniques to uncover disease-specific acoustic signatures and\nimprove the diagnostic capabilities of cough sound analysis by providing more\ninterpretable results."}
{"id": "2508.14920", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.14920", "abs": "https://arxiv.org/abs/2508.14920", "authors": ["Ilya Fedorov", "Dmitry Korobchenko"], "title": "Human Feedback Driven Dynamic Speech Emotion Recognition", "comment": null, "summary": "This work proposes to explore a new area of dynamic speech emotion\nrecognition. Unlike traditional methods, we assume that each audio track is\nassociated with a sequence of emotions active at different moments in time. The\nstudy particularly focuses on the animation of emotional 3D avatars. We propose\na multi-stage method that includes the training of a classical speech emotion\nrecognition model, synthetic generation of emotional sequences, and further\nmodel improvement based on human feedback. Additionally, we introduce a novel\napproach to modeling emotional mixtures based on the Dirichlet distribution.\nThe models are evaluated based on ground-truth emotions extracted from a\ndataset of 3D facial animations. We compare our models against the sliding\nwindow approach. Our experimental results show the effectiveness of\nDirichlet-based approach in modeling emotional mixtures. Incorporating human\nfeedback further improves the model quality while providing a simplified\nannotation procedure."}
{"id": "2508.15429", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15429", "abs": "https://arxiv.org/abs/2508.15429", "authors": ["Yulin Sun", "Qisheng Xu", "Yi Su", "Qian Zhu", "Yong Dou", "Xinwang Liu", "Kele Xu"], "title": "AudioSet-R: A Refined AudioSet with Multi-Stage LLM Label Reannotation", "comment": "8 pages, 5 figures, accepted in ACM MM 2025 dataset track", "summary": "AudioSet is a widely used benchmark in the audio research community and has\nsignificantly advanced various audio-related tasks. However, persistent issues\nwith label accuracy and completeness remain critical bottlenecks that limit\nperformance in downstream applications.To address the aforementioned\nchallenges, we propose a three-stage reannotation framework that harnesses\ngeneral-purpose audio-language foundation models to systematically improve the\nlabel quality of AudioSet. The framework employs a cross-modal prompting\nstrategy, inspired by the concept of prompt chaining, wherein prompts are\nsequentially composed to execute subtasks (audio comprehension, label\nsynthesis, and semantic alignment). Leveraging this framework, we construct a\nhigh-quality, structured relabeled version of AudioSet-R. Extensive experiments\nconducted on representative audio classification models--including AST, PANNs,\nSSAST, and AudioMAE--consistently demonstrate substantial performance\nimprovements, thereby validating the generalizability and effectiveness of the\nproposed approach in enhancing label reliability.The code is publicly available\nat: https://github.com/colaudiolab/AudioSet-R."}
{"id": "2508.14949", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.14949", "abs": "https://arxiv.org/abs/2508.14949", "authors": ["Patricia Amado-Caballero", "Luis Miguel San-José-Revuelta", "María Dolores Aguilar-García", "José Ramón Garmendia-Leiza", "Carlos Alberola-López", "Pablo Casaseca-de-la-Higuera"], "title": "XAI-Driven Spectral Analysis of Cough Sounds for Respiratory Disease Characterization", "comment": null, "summary": "This paper proposes an eXplainable Artificial Intelligence (XAI)-driven\nmethodology to enhance the understanding of cough sound analysis for\nrespiratory disease management. We employ occlusion maps to highlight relevant\nspectral regions in cough spectrograms processed by a Convolutional Neural\nNetwork (CNN). Subsequently, spectral analysis of spectrograms weighted by\nthese occlusion maps reveals significant differences between disease groups,\nparticularly in patients with COPD, where cough patterns appear more variable\nin the identified spectral regions of interest. This contrasts with the lack of\nsignificant differences observed when analyzing raw spectrograms. The proposed\napproach extracts and analyzes several spectral features, demonstrating the\npotential of XAI techniques to uncover disease-specific acoustic signatures and\nimprove the diagnostic capabilities of cough sound analysis by providing more\ninterpretable results."}
{"id": "2508.15521", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15521", "abs": "https://arxiv.org/abs/2508.15521", "authors": ["Xuefeng Yang", "Jian Guan", "Feiyang Xiao", "Congyi Fan", "Haohe Liu", "Qiaoxi Zhu", "Dongli Xu", "Youtian Lin"], "title": "DualMark: Identifying Model and Training Data Origins in Generated Audio", "comment": "13 pages, 5 figures", "summary": "Existing watermarking methods for audio generative models only enable\nmodel-level attribution, allowing the identification of the originating\ngeneration model, but are unable to trace the underlying training dataset. This\nsignificant limitation raises critical provenance questions, particularly in\nscenarios involving copyright and accountability concerns. To bridge this\nfundamental gap, we introduce DualMark, the first dual-provenance watermarking\nframework capable of simultaneously encoding two distinct attribution\nsignatures, i.e., model identity and dataset origin, into audio generative\nmodels during training. Specifically, we propose a novel Dual Watermark\nEmbedding (DWE) module to seamlessly embed dual watermarks into Mel-spectrogram\nrepresentations, accompanied by a carefully designed Watermark Consistency Loss\n(WCL), which ensures reliable extraction of both watermarks from generated\naudio signals. Moreover, we establish the Dual Attribution Benchmark (DAB), the\nfirst robustness evaluation benchmark specifically tailored for joint\nmodel-data attribution. Extensive experiments validate that DualMark achieves\noutstanding attribution accuracy (97.01% F1-score for model attribution, and\n91.51% AUC for dataset attribution), while maintaining exceptional robustness\nagainst aggressive pruning, lossy compression, additive noise, and sampling\nattacks, conditions that severely compromise prior methods. Our work thus\nprovides a foundational step toward fully accountable audio generative models,\nsignificantly enhancing copyright protection and responsibility tracing\ncapabilities."}
{"id": "2508.15088", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15088", "abs": "https://arxiv.org/abs/2508.15088", "authors": ["Prosanta Barai", "Gondy Leroy", "Arif Ahmed"], "title": "Comparative Evaluation of Text and Audio Simplification: A Methodological Replication Study", "comment": null, "summary": "This study serves as a methodological replication of Leroy et al. (2022)\nresearch, which investigated the impact of text simplification on healthcare\ninformation comprehension in the evolving multimedia landscape. Building upon\nthe original studys insights, our replication study evaluates audio content,\nrecognizing its increasing importance in disseminating healthcare information\nin the digital age. Specifically, we explored the influence of text\nsimplification on perceived and actual difficulty when users engage with audio\ncontent automatically generated from that text. Our replication involved 44\nparticipants for whom we assessed their comprehension of healthcare information\npresented as audio created using Leroy et al. (2022) original and simplified\ntexts. The findings from our study highlight the effectiveness of text\nsimplification in enhancing perceived understandability and actual\ncomprehension, aligning with the original studys results. Additionally, we\nexamined the role of education level and language proficiency, shedding light\non their potential impact on healthcare information access and understanding.\nThis research underscores the practical value of text simplification tools in\npromoting health literacy. It suggests the need for tailored communication\nstrategies to reach diverse audiences effectively in the healthcare domain."}
{"id": "2508.15565", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15565", "abs": "https://arxiv.org/abs/2508.15565", "authors": ["Liping Chen", "Chenyang Guo", "Rui Wang", "Kong Aik Lee", "Zhenhua Ling"], "title": "Any-to-any Speaker Attribute Perturbation for Asynchronous Voice Anonymization", "comment": null, "summary": "Speaker attribute perturbation offers a feasible approach to asynchronous\nvoice anonymization by employing adversarially perturbed speech as anonymized\noutput. In order to enhance the identity unlinkability among anonymized\nutterances from the same original speaker, the targeted attack training\nstrategy is usually applied to anonymize the utterances to a common designated\nspeaker. However, this strategy may violate the privacy of the designated\nspeaker who is an actual speaker. To mitigate this risk, this paper proposes an\nany-to-any training strategy. It is accomplished by defining a batch mean loss\nto anonymize the utterances from various speakers within a training mini-batch\nto a common pseudo-speaker, which is approximated as the average speaker in the\nmini-batch. Based on this, a speaker-adversarial speech generation model is\nproposed, incorporating the supervision from both the untargeted attack and the\nany-to-any strategies. The speaker attribute perturbations are generated and\nincorporated into the original speech to produce its anonymized version. The\neffectiveness of the proposed model was justified in asynchronous voice\nanonymization through experiments conducted on the VoxCeleb datasets.\nAdditional experiments were carried out to explore the potential limitations of\nspeaker-adversarial speech in voice privacy protection. With them, we aim to\nprovide insights for future research on its protective efficacy against\nblack-box speaker extractors \\textcolor{black}{and adaptive attacks, as well\nas} generalization to out-of-domain datasets \\textcolor{black}{and stability}.\nAudio samples and open-source code are published in\nhttps://github.com/VoicePrivacy/any-to-any-speaker-attribute-perturbation."}
{"id": "2508.15334", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15334", "abs": "https://arxiv.org/abs/2508.15334", "authors": ["Guirui Zhong", "Qing Wang", "Jun Du", "Lei Wang", "Mingqi Cai", "Xin Fang"], "title": "An Enhanced Audio Feature Tailored for Anomalous Sound Detection Based on Pre-trained Models", "comment": "13 pages, 3 figures, accepted by ICANN2025", "summary": "Anomalous Sound Detection (ASD) aims at identifying anomalous sounds from\nmachines and has gained extensive research interests from both academia and\nindustry. However, the uncertainty of anomaly location and much redundant\ninformation such as noise in machine sounds hinder the improvement of ASD\nsystem performance. This paper proposes a novel audio feature of filter banks\nwith evenly distributed intervals, ensuring equal attention to all frequency\nranges in the audio, which enhances the detection of anomalies in machine\nsounds. Moreover, based on pre-trained models, this paper presents a\nparameter-free feature enhancement approach to remove redundant information in\nmachine audio. It is believed that this parameter-free strategy facilitates the\neffective transfer of universal knowledge from pre-trained tasks to the ASD\ntask during model fine-tuning. Evaluation results on the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2024 Challenge dataset\ndemonstrate significant improvements in ASD performance with our proposed\nmethods."}
{"id": "2508.15632", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15632", "abs": "https://arxiv.org/abs/2508.15632", "authors": ["Bochao Sun", "Dong Wang", "Han Yin"], "title": "ASCMamba: Multimodal Time-Frequency Mamba for Acoustic Scene Classification", "comment": null, "summary": "Acoustic Scene Classification (ASC) is a fundamental problem in computational\naudition, which seeks to classify environments based on the distinctive\nacoustic features. In the ASC task of the APSIPA ASC 2025 Grand Challenge, the\norganizers introduce a multimodal ASC task. Unlike traditional ASC systems that\nrely solely on audio inputs, this challenge provides additional textual\ninformation as inputs, including the location where the audio is recorded and\nthe time of recording. In this paper, we present our proposed system for the\nASC task in the APSIPA ASC 2025 Grand Challenge. Specifically, we propose a\nmultimodal network, \\textbf{ASCMamba}, which integrates audio and textual\ninformation for fine-grained acoustic scene understanding and effective\nmultimodal ASC. The proposed ASCMamba employs a DenseEncoder to extract\nhierarchical spectral features from spectrograms, followed by a dual-path Mamba\nblocks that capture long-range temporal and frequency dependencies using\nMamba-based state space models. In addition, we present a two-step\npseudo-labeling mechanism to generate more reliable pseudo-labels. Results show\nthat the proposed system outperforms all the participating teams and achieves a\n6.2% improvement over the baseline. Code, model and pre-trained checkpoints are\navailable at https://github.com/S-Orion/ASCMamba.git."}
{"id": "2508.14908", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.14908", "abs": "https://arxiv.org/abs/2508.14908", "authors": ["Yue Pan", "Liwei Liu", "Changxin Li", "Xinyao Wang", "Yili Xia", "Hanyue Zhang", "Ming Chu"], "title": "A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification", "comment": null, "summary": "Speech is a cost-effective and non-intrusive data source for identifying\nacute and chronic heart failure (HF). However, there is a lack of research on\nwhether Chinese syllables contain HF-related information, as observed in other\nwell-studied languages. This study presents the first Chinese speech database\nof HF patients, featuring paired recordings taken before and after\nhospitalisation. The findings confirm the effectiveness of the Chinese language\nin HF detection using both standard 'patient-wise' and personalised 'pair-wise'\nclassification approaches, with the latter serving as an ideal\nspeaker-decoupled baseline for future research. Statistical tests and\nclassification results highlight individual differences as key contributors to\ninaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for\nfrequency importance analysis. The data and demonstrations are published at\nhttps://github.com/panyue1998/Voice_HF."}
{"id": "2508.15442", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15442", "abs": "https://arxiv.org/abs/2508.15442", "authors": ["Chenlin Liu", "Minghui Fang", "Patrick Zhang", "Wei Zhou", "Jie Gao", "Jiqing Han"], "title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets", "comment": null, "summary": "Language Model (LM)-based Text-to-Speech (TTS) systems often generate\nhallucinated speech that deviates from input text. Existing mitigation\nstrategies either demand excessive training resources or introduce significant\ninference latency. In this paper, we propose GFlOwNet-guided distribution\nAlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates\nhallucinations without relying on massive resources or inference cost.\nSpecifically, we first conduct an uncertainty analysis, revealing a strong\npositive correlation between hallucination and model uncertainty. Based on\nthis, we reformulate TTS generation as a trajectory flow optimization problem\nand introduce an enhanced Subtrajectory Balance objective together with a\nsharpened internal reward as target distribution. We further integrate reward\ntemperature decay and learning rate optimization for stability and performance\nbalance. Extensive experiments show that GOAT reduce over 50% character error\nrates on challenging test cases and lowering uncertainty by up to 58%,\ndemonstrating its strong generalization ability and effectiveness."}

{"id": "2508.16055", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16055", "abs": "https://arxiv.org/abs/2508.16055", "authors": ["Mengzhen Liu", "Ming Li", "Rang Liu", "Qian Liu"], "title": "Secure ISAC Systems Empowered by Compound Reconfigurable Antenna Arrays", "comment": "13 pages, 9 figures", "summary": "In integrated sensing and communication (ISAC) systems, the use of\ndual-functional signals inherently exposes confidential communication\ninformation during radar sensing, particularly when the sensing target itself\nacts as an eavesdropper. Conventional physical-layer security solutions rely on\ndirectional beamforming or artificial noise injection implemented via signal\nprocessing in the baseband (BB) domain. However, these BB-domain approaches are\nconstrained by insufficient spatial resolution and the adverse effects of\nspatially correlated channels. To overcome these limitations, this paper\nproposes a novel secure ISAC framework empowered by compound reconfigurable\nantenna (CRA) arrays, which offer simultaneous reconfigurability of radiation\npatterns and polarization states in the electromagnetic (EM) domain.\nSpecifically, we develop a comprehensive channel model incorporating virtual\nangular domain, spatial domain, and depolarization effects, and formulate a\nmixed-integer nonlinear programming (MINLP) problem to jointly design EM-domain\nand BB-domain precoders and combiners. To efficiently solve this complex\noptimization problem, we propose an iterative decomposition-based algorithm\nleveraging fractional programming (FP), majorization-minimization (MM),\nsecond-order cone programming (SOCP), and penalty methods. Extensive simulation\nresults demonstrate that the CRA array architecture with proposed joint EM-and\nBB-domain design achieves significant performance improvements in secure ISAC\nsystems. In particular, radar sensing gains of up to 12dB are observed over\nconventional beamforming, while robust communication security is maintained.\nThese results highlight the considerable benefits attainable by jointly\nleveraging additional degrees of freedom (DoFs) in the EM domain for secure\nISAC system design."}
{"id": "2508.16232", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.16232", "abs": "https://arxiv.org/abs/2508.16232", "authors": ["Junyi Peng", "Lin Zhang", "Jiangyu Han", "Oldřich Plchot", "Johan Rohdin", "Themos Stafylakis", "Shuai Wang", "Jan Černocký"], "title": "Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for Speaker Verification and Anti-Spoofing", "comment": null, "summary": "Although large-scale self-supervised learning (SSL) models like WavLM have\nachieved state-of-the-art performance in speech processing, their significant\nsize impedes deployment on resource-constrained devices. While structured\npruning is a key technique for model compression, existing methods typically\nseparate it from task-specific fine-tuning. This multi-stage approach struggles\nto create optimal architectures tailored for diverse downstream tasks. In this\nwork, we introduce a unified framework that integrates structured pruning into\nthe downstream fine-tuning process. Our framework unifies these steps, jointly\noptimizing for task performance and model sparsity in a single stage. This\nallows the model to learn a compressed architecture specifically for the end\ntask, eliminating the need for complex multi-stage pipelines and knowledge\ndistillation. Our pruned models achieve up to a 70\\% parameter reduction with\nnegligible performance degradation on large-scale datasets, achieving equal\nerror rates of 0.7\\%, 0.8\\%, and 1.6\\% on Vox1-O, -E, and -H, respectively.\nFurthermore, our approach demonstrates improved generalization in low-resource\nscenarios, reducing overfitting and achieving a state-of-the-art 3.7\\% EER on\nASVspoof5."}
{"id": "2508.16107", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16107", "abs": "https://arxiv.org/abs/2508.16107", "authors": ["Amir Bouziane", "Huseyin Arslan"], "title": "FM OFDM Unifying High Mobility Communications and Sensing", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) is foundational to future sixth\ngeneration (6G) systems, demanding waveform co-design that supports both high\nthroughput data transmission and accurate environmental perception. While\nOrthogonal Frequency Division Multiplexing (OFDM) offers flexibility and\nbackward compatibility, its high Peak to Average Power Ratio (PAPR) poses\nsignificant challenges at higher frequency bands. To address this, we\ninvestigate Frequency Modulated Orthogonal Frequency Division Multiplexing (FM\nOFDM) a constant envelope waveform that facilitates robust joint sensing and\ncommunication under highly mobile, doubly dispersive channel conditions. We\nderive a comprehensive input output relationship for FM OFDM in time varying\nmultipath channels, including analytical expressions for Inter Carrier\nInterference (ICI), Doppler effects, and effective channel gains. Extensive\nsimulations comparing FM OFDM with conventional Cyclic Prefix Orthogonal\nFrequency Division Multiplexing (CP OFDM) and Constant Envelope Orthogonal\nFrequency Division Multiplexing (CE OFDM) demonstrate superior range and\nvelocity estimation accuracy of FM-OFDM, particularly at high Signal to Noise\nRatios (SNRs) and under high mobility, highlighting its suitability as a\nunified waveform for high frequency ISAC applications in 6G."}
{"id": "2508.15882", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15882", "abs": "https://arxiv.org/abs/2508.15882", "authors": ["Neta Glazer", "Yael Segal-Feldman", "Hilit Segev", "Aviv Shamsian", "Asaf Buchnick", "Gill Hetz", "Ethan Fetaya", "Joseph Keshet", "Aviv Navon"], "title": "Beyond Transcription: Mechanistic Interpretability in ASR", "comment": null, "summary": "Interpretability methods have recently gained significant attention,\nparticularly in the context of large language models, enabling insights into\nlinguistic representations, error detection, and model behaviors such as\nhallucinations and repetitions. However, these techniques remain underexplored\nin automatic speech recognition (ASR), despite their potential to advance both\nthe performance and interpretability of ASR systems. In this work, we adapt and\nsystematically apply established interpretability methods such as logit lens,\nlinear probing, and activation patching, to examine how acoustic and semantic\ninformation evolves across layers in ASR systems. Our experiments reveal\npreviously unknown internal dynamics, including specific encoder-decoder\ninteractions responsible for repetition hallucinations and semantic biases\nencoded deep within acoustic representations. These insights demonstrate the\nbenefits of extending and applying interpretability techniques to speech\nrecognition, opening promising directions for future research on improving\nmodel transparency and robustness."}
{"id": "2508.16169", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16169", "abs": "https://arxiv.org/abs/2508.16169", "authors": ["Lukas Herrmann", "Ángel F. García-Fernández", "Edmund F. Brekke", "Egil Eide"], "title": "A Scalable Hybrid Track-Before-Detect Tracking System: Application to Coastal Maritime Radar Surveillance", "comment": "Submitted for possible publication in IEEE Journal of Oceanic\n  Engineering (JOE)", "summary": "Despite their theoretical advantages, track-before-detect (TBD) methods\nremain largely absent from real-world multi-target tracking applications due to\ntheir computational complexity and limited scalability. This paper presents a\nscalable hybrid tracking framework that combines a TBD multi-target tracking\nalgorithm with a detection-based multi-target tracking algorithm for coastal\nradar surveillance. In particular, the approach uses an integrated existence\nPoisson histogram-probabilistic multi-hypothesis tracking (IE-PHPMHT)-based TBD\nmodule with a conventional Poisson multi-Bernoulli Mixture (PMBM) point\ntracker. The system processes raw radar data through land clutter suppression,\ncell-wise detection, and clustering-based feature extraction. High-threshold\ndetections are used to track strong targets via the point tracker, while\nlow-threshold detections are employed for adaptive birth in the TBD module,\nenabling early initiation and sustained tracking of weak or ambiguous targets.\nValidated using real X-band radar data from the Trondheim Fjord, Norway, the\napproach demonstrates robust multi-target tracking performance in a full-scale\napplication with a large observation area under resource constraints,\nhighlighting its suitability for operational deployment in complex maritime\nenvironments needed for coastal surveillance and to support autonomy."}
{"id": "2508.15931", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15931", "abs": "https://arxiv.org/abs/2508.15931", "authors": ["Zhiyu Wu", "Jingyi Fang", "Yufei Tang", "Yuanzhong Zheng", "Yaoxuan Wang", "Haojun Fei"], "title": "QvTAD: Differential Relative Attribute Learning for Voice Timbre Attribute Detection", "comment": "Accepted by National Conference on Man-Machine Speech Communication,\n  NCMMSC'2025", "summary": "Voice Timbre Attribute Detection (vTAD) plays a pivotal role in fine-grained\ntimbre modeling for speech generation tasks. However, it remains challenging\ndue to the inherently subjective nature of timbre descriptors and the severe\nlabel imbalance in existing datasets. In this work, we present QvTAD, a novel\npairwise comparison framework based on differential attention, designed to\nenhance the modeling of perceptual timbre attributes. To address the label\nimbalance in the VCTK-RVA dataset, we introduce a graph-based data augmentation\nstrategy that constructs a Directed Acyclic Graph and employs Disjoint-Set\nUnion techniques to automatically mine unobserved utterance pairs with valid\nattribute comparisons. Our framework leverages speaker embeddings from a\npretrained FACodec, and incorporates a Relative Timbre Shift-Aware Differential\nAttention module. This module explicitly models attribute-specific contrasts\nbetween paired utterances via differential denoising and contrast amplification\nmechanisms. Experimental results on the VCTK-RVA benchmark demonstrate that\nQvTAD achieves substantial improvements across multiple timbre descriptors,\nwith particularly notable gains in cross-speaker generalization scenarios."}
{"id": "2508.16218", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16218", "abs": "https://arxiv.org/abs/2508.16218", "authors": ["Mintaek Oh", "Jinseok Choi"], "title": "Hybrid Precoding Revisited: Low-Dimensional Subspace Perspective for MU-MIMO Systems", "comment": "5 pages, 2 figures", "summary": "This letter presents a low-complexity hybrid precoding framework for\nmultiuser multiple-input multiple-output (MIMO) systems by leveraging a\nlow-dimensional subspace property. Under the low-dimensional subspace\nperspective, we first identify an unconstrained optimal radio-frequency (RF)\nprecoder. We then optimize a hybrid precoder via a reduced-complexity precoding\nmethod. We further extend the proposed framework to (i) a dynamic-subarray\nantenna partitioning algorithm that adaptively allocates subsets of antennas\nassociated with RF chains, and (ii) a channel covariance-based approach to\nexploit statistical channel state information at a transmitter (CSIT), ensuring\nrobustness with partial CSIT. Simulations validate that our proposed algorithms\nachieve superior performance while significantly reducing complexity compared\nto existing methods."}
{"id": "2508.16176", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.16176", "abs": "https://arxiv.org/abs/2508.16176", "authors": ["Ryan Niu", "Shoichi Koyama", "Tomohiko Nakamura"], "title": "Head-Related Transfer Function Individualization Using Anthropometric Features and Spatially Independent Latent Representation", "comment": "Accepted to IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "A method for head-related transfer function (HRTF) individualization from the\nsubject's anthropometric parameters is proposed. Due to the high cost of\nmeasurement, the number of subjects included in many HRTF datasets is limited,\nand the number of those that include anthropometric parameters is even smaller.\nTherefore, HRTF individualization based on deep neural networks (DNNs) is a\nchallenging task. We propose a HRTF individualization method using the latent\nrepresentation of HRTF magnitude obtained through an autoencoder conditioned on\nsound source positions, which makes it possible to combine multiple HRTF\ndatasets with different measured source positions, and makes the network\ntraining tractable by reducing the number of parameters to be estimated from\nanthropometric parameters. Experimental evaluation shows that high estimation\naccuracy is achieved by the proposed method, compared to current DNN-based\nmethods."}
{"id": "2508.15882", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15882", "abs": "https://arxiv.org/abs/2508.15882", "authors": ["Neta Glazer", "Yael Segal-Feldman", "Hilit Segev", "Aviv Shamsian", "Asaf Buchnick", "Gill Hetz", "Ethan Fetaya", "Joseph Keshet", "Aviv Navon"], "title": "Beyond Transcription: Mechanistic Interpretability in ASR", "comment": null, "summary": "Interpretability methods have recently gained significant attention,\nparticularly in the context of large language models, enabling insights into\nlinguistic representations, error detection, and model behaviors such as\nhallucinations and repetitions. However, these techniques remain underexplored\nin automatic speech recognition (ASR), despite their potential to advance both\nthe performance and interpretability of ASR systems. In this work, we adapt and\nsystematically apply established interpretability methods such as logit lens,\nlinear probing, and activation patching, to examine how acoustic and semantic\ninformation evolves across layers in ASR systems. Our experiments reveal\npreviously unknown internal dynamics, including specific encoder-decoder\ninteractions responsible for repetition hallucinations and semantic biases\nencoded deep within acoustic representations. These insights demonstrate the\nbenefits of extending and applying interpretability techniques to speech\nrecognition, opening promising directions for future research on improving\nmodel transparency and robustness."}
{"id": "2508.16544", "categories": ["eess.SP", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.16544", "abs": "https://arxiv.org/abs/2508.16544", "authors": ["Stephen Ekaputra Limantoro"], "title": "Parameter-Free Logit Distillation via Sorting Mechanism", "comment": "Accepted in IEEE Signal Processing Letters 2025", "summary": "Knowledge distillation (KD) aims to distill the knowledge from the teacher\n(larger) to the student (smaller) model via soft-label for the efficient neural\nnetwork. In general, the performance of a model is determined by accuracy,\nwhich is measured with labels. However, existing KD approaches usually use the\nteacher with its original distribution, neglecting the potential of incorrect\nprediction. This may contradict the motivation of hard-label learning through\ncross-entropy loss, which may lead to sub-optimal knowledge distillation on\ncertain samples. To address this issue, we propose a novel logit processing\nscheme via a sorting mechanism. Specifically, our method has a two-fold goal:\n(1) fixing the incorrect prediction of the teacher based on the labels and (2)\nreordering the distribution in a natural way according to priority rank at\nonce. As an easy-to-use, plug-and-play pre-processing, our sort method can be\neffectively applied to existing logit-based KD methods. Extensive experiments\non the CIFAR-100 and ImageNet datasets demonstrate the effectiveness of our\nmethod."}
{"id": "2508.15931", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15931", "abs": "https://arxiv.org/abs/2508.15931", "authors": ["Zhiyu Wu", "Jingyi Fang", "Yufei Tang", "Yuanzhong Zheng", "Yaoxuan Wang", "Haojun Fei"], "title": "QvTAD: Differential Relative Attribute Learning for Voice Timbre Attribute Detection", "comment": "Accepted by National Conference on Man-Machine Speech Communication,\n  NCMMSC'2025", "summary": "Voice Timbre Attribute Detection (vTAD) plays a pivotal role in fine-grained\ntimbre modeling for speech generation tasks. However, it remains challenging\ndue to the inherently subjective nature of timbre descriptors and the severe\nlabel imbalance in existing datasets. In this work, we present QvTAD, a novel\npairwise comparison framework based on differential attention, designed to\nenhance the modeling of perceptual timbre attributes. To address the label\nimbalance in the VCTK-RVA dataset, we introduce a graph-based data augmentation\nstrategy that constructs a Directed Acyclic Graph and employs Disjoint-Set\nUnion techniques to automatically mine unobserved utterance pairs with valid\nattribute comparisons. Our framework leverages speaker embeddings from a\npretrained FACodec, and incorporates a Relative Timbre Shift-Aware Differential\nAttention module. This module explicitly models attribute-specific contrasts\nbetween paired utterances via differential denoising and contrast amplification\nmechanisms. Experimental results on the VCTK-RVA benchmark demonstrate that\nQvTAD achieves substantial improvements across multiple timbre descriptors,\nwith particularly notable gains in cross-speaker generalization scenarios."}
{"id": "2508.16176", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.16176", "abs": "https://arxiv.org/abs/2508.16176", "authors": ["Ryan Niu", "Shoichi Koyama", "Tomohiko Nakamura"], "title": "Head-Related Transfer Function Individualization Using Anthropometric Features and Spatially Independent Latent Representation", "comment": "Accepted to IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "A method for head-related transfer function (HRTF) individualization from the\nsubject's anthropometric parameters is proposed. Due to the high cost of\nmeasurement, the number of subjects included in many HRTF datasets is limited,\nand the number of those that include anthropometric parameters is even smaller.\nTherefore, HRTF individualization based on deep neural networks (DNNs) is a\nchallenging task. We propose a HRTF individualization method using the latent\nrepresentation of HRTF magnitude obtained through an autoencoder conditioned on\nsound source positions, which makes it possible to combine multiple HRTF\ndatasets with different measured source positions, and makes the network\ntraining tractable by reducing the number of parameters to be estimated from\nanthropometric parameters. Experimental evaluation shows that high estimation\naccuracy is achieved by the proposed method, compared to current DNN-based\nmethods."}
{"id": "2508.16332", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16332", "abs": "https://arxiv.org/abs/2508.16332", "authors": ["Xueyao Zhang", "Junan Zhang", "Yuancheng Wang", "Chaoren Wang", "Yuanzhe Chen", "Dongya Jia", "Zhuo Chen", "Zhizheng Wu"], "title": "Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning", "comment": "We will release code and model checkpoints at\n  https://github.com/open-mmlab/Amphion", "summary": "Controllable human voice generation, particularly for expressive domains like\nsinging, remains a significant challenge. This paper introduces Vevo2, a\nunified framework for controllable speech and singing voice generation. To\ntackle issues like the scarcity of annotated singing data and to enable\nflexible controllability, Vevo2 introduces two audio tokenizers: (1) a\nmusic-notation-free prosody tokenizer that captures prosody and melody from\nspeech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5\nHz) content-style tokenizer that encodes linguistic content, prosody, and style\nfor both speech and singing, while enabling timbre disentanglement. Vevo2\nconsists of an auto-regressive (AR) content-style modeling stage, which aims to\nenable controllability over text, prosody, and style, as well as a\nflow-matching acoustic modeling stage that allows for timbre control.\nParticularly, during pre-training of the AR model, we propose both explicit and\nimplicit prosody learning strategies to bridge speech and singing voice.\nMoreover, to further enhance the AR model's ability to follow text and prosody,\nwe design a multi-objective post-training task that integrates both\nintelligibility and prosody similarity alignment. Experimental results show\nthat the unified modeling in Vevo2 brings mutual benefits to both speech and\nsinging voice generation. Additionally, Vevo2's effectiveness across a wide\nrange of synthesis, conversion, and editing tasks for both speech and singing\nfurther demonstrates its strong generalization ability and versatility. Audio\nsamples are are available at https://versasinger.github.io/."}

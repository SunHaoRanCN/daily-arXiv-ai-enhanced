{"id": "2508.16601", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.16601", "abs": "https://arxiv.org/abs/2508.16601", "authors": ["Marco Donald Migliore"], "title": "Notes on Deterministic and Stochastic Approaches in Electromagnetic Information Theory", "comment": null, "summary": "This paper investigates the relationship between the Number of Degrees of\nFreedom ($N_{\\rm DoF}$) of the field in deterministic and stochastic source\nmodels within Electromagnetic Information Theory (EIT). Our findings\ndemonstrate a fundamental connection between these two approaches.\nSpecifically, we show that a deterministic model and a stochastic model with a\nspatially incoherent and homogeneous source yield not only the same $N_{\\rm\nDoF}$ but also identical eigenvalues and basis functions for field\nrepresentation. This key equivalence not only explains the effectiveness of\ndeterministic approaches in EIT but also corroborates the use of classical\nelectromagnetic methods within this new discipline."}
{"id": "2508.16735", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16735", "abs": "https://arxiv.org/abs/2508.16735", "authors": ["Seyed Mohammad Amin Shirinbayan", "Gholamreza Moradi"], "title": "A Practical Approach to the Design of an S-Band Image-Rejecting Dual-Conversion Super-Heterodyne RF Chain of a Receiver Considering Spur Signals", "comment": "18 pages, 31 figures, 13 tables, This preprint is being submitted\n  with the intention of publication in the Journal of Electrical Engineering &\n  Technology (Springer)", "summary": "This paper presents a typical design of the RF section of a radar receiver,\nthe chain within a superheterodyne dual-conversion architecture. A significant\nchallenge in this framework is the occurrence of spur signals, which negatively\nimpact the dynamic range of the RF chain. When addressing this issue, the paper\nintroduces an innovative approach to mitigate (or even wipe out) these\nundesired effects, utilizing two mutually verifying MATLAB codes. These codes\nhave been tested with two distinct commercial mixers and could be applied to\nany superheterodyne configuration with various mixers. The presented method\nmakes the Spurious-Free Dynamic Range (SFDR) of the chain the least different\nfrom the dynamic range of the chain. Also, the selection of other components\ngets optimized to align with spurious signals consideration, with explanations\nprovided for these choices. Moreover, two filters of the RF chain, the second\nand the third, have been designed to reduce implementation costs. Various\nMicrowave software and full-wave analyses were employed for detailed design and\nanalysis, with the results compared to evaluate their performance."}
{"id": "2508.16888", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.16888", "abs": "https://arxiv.org/abs/2508.16888", "authors": ["Jiazhe Li", "Nicol√≤ Decarli", "Francesco Guidi", "Anna Guerra", "Alessandro Bazzi", "Zhuoming Li"], "title": "Dual Orthogonal Projections-Based Multiuser Interference Cancellation for mmWave Beamforming in XL-MIMO Systems", "comment": null, "summary": "This paper investigates multiuser interference (MUI) cancellation for\nmillimeter-wave (mmWave) beamforming in extremely large-scale multiple-input\nmultiple-output (XL-MIMO) communication systems. We propose a linear algorithm,\ntermed iterative dual orthogonal projections (DOP), which alternates between\ntwo orthogonal projections: one to eliminate MUI and the other to refine\ncombiners, ensuring a monotonic increase in spectral efficiency. Theoretical\nanalysis and simulation results show that, with each iteration, the signal\npower for each user increases monotonically, the equivalent noise power after\nreceive combining decreases monotonically, and the spectral efficiency improves\naccordingly and converges rapidly, closely approaching the theoretical optimum\ndetermined by dirty paper coding (DPC), outperforming existing linear\nalgorithms in spectral efficiency."}
{"id": "2508.16946", "categories": ["eess.SP", "cs.NA", "math.NA", "60D05, 60G55, 68M10", "C.2.1; C.2.3; G.3"], "pdf": "https://arxiv.org/pdf/2508.16946", "abs": "https://arxiv.org/abs/2508.16946", "authors": ["Rashmi Kumari", "Gourab Ghatak", "Abhishek K. Gupta"], "title": "Spatially Correlated Blockage Aware Placement of RIS in IIoT Networks", "comment": "13 pages, 21 figures. A preliminary version of this work was accepted\n  in IEEE PIMRC 2025 under the title \"Blockage Aware Placement of RIS in IIoT\n  Networks\"", "summary": "We study the impact of deploying reconfigurable intelligent surfaces (RISs)\nin mitigating coverage gaps and enhancing transmission reliability in an\nindustrial internet of things (IIoT) network. First, we consider a single\nblockage scenario and characterize the correlation between blocking events of\nthe base station (BS)-user and the RIS-user links and study its impact on the\nprobability of establishing a viable reflected link. Then, by considering\nmultiple blockages, we derive the distribution of the signal to noise ratio\n(SNR) as a function of data size, blockage density, the number of RISs, and the\ndeployment area. We analyze the impact of normalized blockage radius and\nidentify the threshold beyond which the assumption of independent blockages\ndeviates from the ground truth of correlated blocking. Finally, we compare the\noutage performance of this RIS-assisted system with that operated with network-\ncontrolled relays, and demonstrate that while the relays provide a higher\nreliability beyond a certain blockage threshold, increasing the number of RISs\nmay help mitigate this effect. These insights offer valuable design guidelines\nfor deploying RIS-aided IIoT networks in dense blockage environments."}
{"id": "2508.16790", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.16790", "abs": "https://arxiv.org/abs/2508.16790", "authors": ["Yuancheng Wang", "Dekun Chen", "Xueyao Zhang", "Junan Zhang", "Jiaqi Li", "Zhizheng Wu"], "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling", "comment": null, "summary": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer."}
{"id": "2508.16908", "categories": ["eess.AS", "cs.HC", "cs.NI", "cs.SD", "eess.SP", "C.3; C.2.1; C.2.4; I.5.4; H.5.2; J.7"], "pdf": "https://arxiv.org/pdf/2508.16908", "abs": "https://arxiv.org/abs/2508.16908", "authors": ["Amod K. Agrawal"], "title": "Localization using Angle-of-Arrival Triangulation", "comment": "6 pages, 5 figures, 1 table. Accepted at the ACM International\n  Workshop on Environmental Sensing Systems for Smart Cities (EnvSys 2025). To\n  appear in the MobiSys 2025 Proceedings", "summary": "Indoor localization is a long-standing challenge in mobile computing, with\nsignificant implications for enabling location-aware and intelligent\napplications within smart environments such as homes, offices, and retail\nspaces. As AI assistants such as Amazon Alexa and Google Nest become\nincreasingly pervasive, microphone-equipped devices are emerging as key\ncomponents of everyday life and home automation. This paper introduces a\npassive, infrastructure-light system for localizing human speakers using speech\nsignals captured by two or more spatially distributed smart devices. The\nproposed approach, GCC+, extends the Generalized Cross-Correlation with Phase\nTransform (GCC-PHAT) method to estimate the Angle-of-Arrival (AoA) of audio\nsignals at each device and applies robust triangulation techniques to infer the\nspeaker's two-dimensional position. To further improve temporal resolution and\nlocalization accuracy, feature-space expansion and subsample interpolation\ntechniques are employed for precise Time Difference of Arrival (TDoA)\nestimation. The system operates without requiring hardware modifications, prior\ncalibration, explicit user cooperation, or knowledge of the speaker's signal\ncontent, thereby offering a highly practical solution for real-world\ndeployment. Experimental evaluation in a real-world home environment yields a\nmedian AoA estimation error of 2.2 degrees and a median localization error of\n1.25 m, demonstrating the feasibility and effectiveness of audio-based\nlocalization for enabling context-aware, privacy-preserving ambient\nintelligence."}
{"id": "2508.17051", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17051", "abs": "https://arxiv.org/abs/2508.17051", "authors": ["Christopher Saetia", "Daniel M. Dobkin", "Gregory Durgin"], "title": "Radio Frequency Identification: Decades at a Time", "comment": null, "summary": "In this article, we briefly review the history of the use of radio signals to\nidentify objects, and of the key Radio Frequency Identification (RFID)\nstandards for ultra-high-frequency (UHF) and near-field communications that\nenabled broad use of these technologies in daily life. We will compare the\nvision for the future presented by the Auto-ID Lab in the early 21st century\nwith the reality we see today, two decades and a little after. We will review\nsome of the applications in which UHF RFID technology has become hugely\nsuccessful, others where High Frequency Near-field Communications (HF NFC) is\npreferred, and applications where optical identification or active wireless\ncommunications are dominant.\n  We will then examine some possible future paths for RFID technology. We\nanticipate that UHF read capability will become widely available for\ncellphones, making it as universal as NFC and Bluetooth are today. We will look\nat more sophisticated radio interfaces, such as multiple-antenna phased arrays\nfor readers, and tunnel diode reflection for tags. We will discuss the\nintegration of information from Artificial Intelligence (AI)-based image\nprocessing, barcodes, NFC and UHF tags, into a digital twin of the real\nenvironment experienced by the human user. We will examine the role of RFID\nwith sensing in improving the management of perishable goods. The role that\nRFID might play in a truly circular economy, with intelligent recycling and\nreuse, will be discussed. Finally, we survey the many hazards and obstacles\nthat obstruct the path to an RF-informed future."}
{"id": "2508.16858", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16858", "abs": "https://arxiv.org/abs/2508.16858", "authors": ["Yihan Wu", "Jee-weon Jung", "Hye-jin Shim", "Xin Cheng", "Xin Wang"], "title": "WildSpoof Challenge Evaluation Plan", "comment": "ICASSP 2026 challenge", "summary": "The WildSpoof Challenge aims to advance the use of in-the-wild data in two\nintertwined speech processing tasks. It consists of two parallel tracks: (1)\nText-to-Speech (TTS) synthesis for generating spoofed speech, and (2)\nSpoofing-robust Automatic Speaker Verification (SASV) for detecting spoofed\nspeech. While the organizers coordinate both tracks and define the data\nprotocols, participants treat them as separate and independent tasks. The\nprimary objectives of the challenge are: (i) to promote the use of in-the-wild\ndata for both TTS and SASV, moving beyond conventional clean and controlled\ndatasets and considering real-world scenarios; and (ii) to encourage\ninterdisciplinary collaboration between the spoofing generation (TTS) and\nspoofing detection (SASV) communities, thereby fostering the development of\nmore integrated, robust, and realistic systems."}
{"id": "2508.16930", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.16930", "abs": "https://arxiv.org/abs/2508.16930", "authors": ["Sizhe Shan", "Qiulin Li", "Yutao Cui", "Miles Yang", "Yuehai Wang", "Qun Yang", "Jin Zhou", "Zhao Zhong"], "title": "HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation", "comment": null, "summary": "Recent advances in video generation produce visually realistic content, yet\nthe absence of synchronized audio severely compromises immersion. To address\nkey challenges in video-to-audio generation, including multimodal data\nscarcity, modality imbalance and limited audio quality in existing methods, we\npropose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that\nsynthesizes high-fidelity audio precisely aligned with visual dynamics and\nsemantic context. Our approach incorporates three core innovations: (1) a\nscalable data pipeline curating 100k-hour multimodal datasets through automated\nannotation; (2) a representation alignment strategy using self-supervised audio\nfeatures to guide latent diffusion training, efficiently improving audio\nquality and generation stability; (3) a novel multimodal diffusion transformer\nresolving modal competition, containing dual-stream audio-video fusion through\njoint attention, and textual semantic injection via cross-attention.\nComprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new\nstate-of-the-art performance across audio fidelity, visual-semantic alignment,\ntemporal alignment and distribution matching. The demo page is available at:\nhttps://szczesnys.github.io/hunyuanvideo-foley/."}
{"id": "2508.17246", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17246", "abs": "https://arxiv.org/abs/2508.17246", "authors": ["Takuma Sumi", "Georgi S. Medvedev"], "title": "Graphon Signal Processing for Spiking and Biological Neural Networks", "comment": "20 pages, 10 figures", "summary": "Graph Signal Processing (GSP) extends classical signal processing to signals\ndefined on graphs, enabling filtering, spectral analysis, and sampling of data\ngenerated by networks of various kinds. Graphon Signal Processing (GnSP)\ndevelops this framework further by employing the theory of graphons. Graphons\nare measurable functions on the unit square that represent graphs and limits of\nconvergent graph sequences. The use of graphons provides stability of GSP\nmethods to stochastic variability in network data and improves computational\nefficiency for very large networks. We use GnSP to address the stimulus\nidentification problem (SIP) in computational and biological neural networks.\nThe SIP is an inverse problem that aims to infer the unknown stimulus s from\nthe observed network output f. We first validate the approach in spiking neural\nnetwork simulations and then analyze calcium imaging recordings. Graphon-based\nspectral projections yield trial-invariant, lowdimensional embeddings that\nimprove stimulus classification over Principal Component Analysis and discrete\nGSP baselines. The embeddings remain stable under variations in network\nstochasticity, providing robustness to different network sizes and noise\nlevels. To the best of our knowledge, this is the first application of GnSP to\nbiological neural networks, opening new avenues for graphon-based analysis in\nneuroscience."}
{"id": "2508.17031", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.17031", "abs": "https://arxiv.org/abs/2508.17031", "authors": ["Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer", "comment": null, "summary": "We propose a method for the task of text-conditioned speech insertion, i.e.\ninserting a speech sample in an input speech sample, conditioned on the\ncorresponding complete text transcript. An example use case of the task would\nbe to update the speech audio when corrections are done on the corresponding\ntext transcript. The proposed method follows a transformer-based\nnon-autoregressive approach that allows speech insertions of variable lengths,\nwhich are dynamically determined during inference, based on the text transcript\nand tempo of the available partial input. It is capable of maintaining the\nspeaker's voice characteristics, prosody and other spectral properties of the\navailable speech input. Results from our experiments and user study on LibriTTS\nshow that our method outperforms baselines based on an existing adaptive text\nto speech method. We also provide numerous qualitative results to appreciate\nthe quality of the output from the proposed method."}
{"id": "2508.17134", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.17134", "abs": "https://arxiv.org/abs/2508.17134", "authors": ["Kong Aik Lee", "Zeyan Liu", "Liping Chen", "Zhenhua Ling"], "title": "Pinhole Effect on Linkability and Dispersion in Speaker Anonymization", "comment": "5 pages, 2 figures", "summary": "Speaker anonymization aims to conceal speaker-specific attributes in speech\nsignals, making the anonymized speech unlinkable to the original speaker\nidentity. Recent approaches achieve this by disentangling speech into content\nand speaker components, replacing the latter with pseudo speakers. The\nanonymized speech can be mapped either to a common pseudo speaker shared across\nutterances or to distinct pseudo speakers unique to each utterance. This paper\ninvestigates the impact of these mapping strategies on three key dimensions:\nspeaker linkability, dispersion in the anonymized speaker space, and\nde-identification from the original identity. Our findings show that using\ndistinct pseudo speakers increases speaker dispersion and reduces linkability\ncompared to common pseudo-speaker mapping, thereby enhancing privacy\npreservation. These observations are interpreted through the proposed pinhole\neffect, a conceptual framework introduced to explain the relationship between\nmapping strategies and anonymization performance. The hypothesis is validated\nthrough empirical evaluation."}
{"id": "2508.17354", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17354", "abs": "https://arxiv.org/abs/2508.17354", "authors": ["Jun Wu", "Weijie Yuan", "Xiaoqi Zhang", "Yaohuan Yu", "Yuanhao Cui", "Fan Liu", "Geng Sun", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim"], "title": "Toward Multi-Functional LAWNs with ISAC: Opportunities, Challenges, and the Road Ahead", "comment": null, "summary": "Integrated sensing and communication (ISAC) has been envisioned as a\nfoundational technology for future low-altitude wireless networks (LAWNs),\nenabling real-time environmental perception and data exchange across\naerial-ground systems. In this article, we first explore the roles of ISAC in\nLAWNs from both node-level and network-level perspectives. We highlight the\nperformance gains achieved through hierarchical integration and cooperation,\nwherein key design trade-offs are demonstrated. Apart from physical-layer\nenhancements, emerging LAWN applications demand broader functionalities. To\nthis end, we propose a multi-functional LAWN framework that extends ISAC with\ncapabilities in control, computation, wireless power transfer, and large\nlanguage model (LLM)-based intelligence. We further provide a representative\ncase study to present the benefits of ISAC-enabled LAWNs and the promising\nresearch directions are finally outlined."}
{"id": "2508.17194", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17194", "abs": "https://arxiv.org/abs/2508.17194", "authors": ["Yucong Zhang", "Juan Liu", "Ming Li"], "title": "Multi-scale Scanning Network for Machine Anomalous Sound Detection", "comment": "Accepted by ICONIP 2025", "summary": "Machine sounds exhibit consistent and repetitive patterns in both the\nfrequency and time domains, which vary significantly across scales for\ndifferent machine types. For instance, rotating machines often show periodic\nfeatures in short time intervals, while reciprocating machines exhibit broader\npatterns spanning the time domain. While prior studies have leveraged these\npatterns to improve Anomalous Sound Detection (ASD), the variation of patterns\nacross scales remains insufficiently explored. To address this gap, we\nintroduce a Multi-scale Scanning Network (MSN) designed to capture patterns at\nmultiple scales. MSN employs kernel boxes of varying sizes to scan audio\nspectrograms and integrates a lightweight convolutional network with shared\nweights for efficient and scalable feature representation. Experimental\nevaluations on the DCASE 2020 and DCASE 2023 Task 2 datasets demonstrate that\nMSN achieves state-of-the-art performance, highlighting its effectiveness in\nadvancing ASD systems."}
{"id": "2508.17840", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.17840", "abs": "https://arxiv.org/abs/2508.17840", "authors": ["Jack Webb", "Lorenzo Picinali"], "title": "Optimal Pairwise Comparison Procedures for Subjective Evaluation", "comment": "11th Convention of the European Acoustics Association, Forum\n  Acusticum 2025, M\\'alaga", "summary": "Audio signal processing algorithms are frequently assessed through subjective\nlistening tests in which participants directly score degraded signals on a\nunidimensional numerical scale. However, this approach is susceptible to\ninconsistencies in scale calibration between assessors. Pairwise comparisons\nbetween degraded signals offer a more intuitive alternative, eliciting the\nrelative scores of candidate signals with lower measurement error and reduced\nparticipant fatigue. Yet, due to the quadratic growth of the number of\nnecessary comparisons, a complete set of pairwise comparisons becomes\nunfeasible for large datasets. This paper compares pairwise comparison\nprocedures to identify the most efficient methods for approximating true\nquality scores with minimal comparisons. A novel sampling procedure is proposed\nand benchmarked against state-of-the-art methods on simulated datasets.\nBayesian sampling produces the most robust score estimates among previously\nestablished methods, while the proposed procedure consistently converges\nfastest on the underlying ranking with comparable score accuracy."}
{"id": "2508.17526", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17526", "abs": "https://arxiv.org/abs/2508.17526", "authors": ["Kangda Zhi", "Tianyu Yang", "Shuangyang Li", "Yi Song", "Amir Rezaei", "Giuseppe Caire"], "title": "Near-Field Integrated Imaging and Communication in Distributed MIMO Networks", "comment": "18 pages, 15 figures", "summary": "In this work, we propose a general framework for wireless imaging in\ndistributed MIMO wideband communication systems, considering multi-view\nnon-isotropic targets and near-field propagation effects. For indoor scenarios\nwhere the objective is to image small-scale objects with high resolution, we\npropose a range migration algorithm (RMA)-based scheme using three kinds of\narray architectures: the full array, boundary array, and distributed boundary\narray. With non-isotropic near-field channels, we establish the Fourier\ntransformation (FT)-based relationship between the imaging reflectivity and the\ndistributed spatial-domain signals and discuss the corresponding theoretical\nproperties. Next, for outdoor scenarios where the objective is to reconstruct\nthe large-scale three-dimensional (3D) environment with coarse resolution, we\npropose a sparse Bayesian learning (SBL)-based algorithm to solve the multiple\nmeasurement vector (MMV) problem, which further addresses the non-isotropic\nreflectivity across different subcarriers. Numerical results demonstrate the\neffectiveness of the proposed algorithms in acquiring high-resolution small\nobjects and accurately reconstructing large-scale environments."}
{"id": "2508.17229", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.17229", "abs": "https://arxiv.org/abs/2508.17229", "authors": ["Junan Zhang", "Xueyao Zhang", "Jing Yang", "Yuancheng Wang", "Fan Fan", "Zhizheng Wu"], "title": "Multi-Metric Preference Alignment for Generative Speech Restoration", "comment": "16 pages, 10 figures. demopage: https://gensr-pref.github.io", "summary": "Recent generative models have significantly advanced speech restoration\ntasks, yet their training objectives often misalign with human perceptual\npreferences, resulting in suboptimal quality. While post-training alignment has\nproven effective in other generative domains like text and image generation,\nits application to generative speech restoration remains largely\nunder-explored. This work investigates the challenges of applying\npreference-based post-training to this task, focusing on how to define a robust\npreference signal and curate high-quality data to avoid reward hacking. To\naddress these challenges, we propose a multi-metric preference alignment\nstrategy. We construct a new dataset, GenSR-Pref, comprising 80K preference\npairs, where each chosen sample is unanimously favored by a complementary suite\nof metrics covering perceptual quality, signal fidelity, content consistency,\nand timbre preservation. This principled approach ensures a holistic preference\nsignal. Applying Direct Preference Optimization (DPO) with our dataset, we\nobserve consistent and significant performance gains across three diverse\ngenerative paradigms: autoregressive models (AR), masked generative models\n(MGM), and flow-matching models (FM) on various restoration benchmarks, in both\nobjective and subjective evaluations. Ablation studies confirm the superiority\nof our multi-metric strategy over single-metric approaches in mitigating reward\nhacking. Furthermore, we demonstrate that our aligned models can serve as\npowerful ''data annotators'', generating high-quality pseudo-labels to serve as\na supervision signal for traditional discriminative models in data-scarce\nscenarios like singing voice restoration. Demo\nPage:https://gensr-pref.github.io"}
{"id": "2508.17980", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17980", "abs": "https://arxiv.org/abs/2508.17980", "authors": ["Dimme de Groot", "Tanvina Patel", "Devendra Kayande", "Odette Scharenborg", "Zhengjun Yue"], "title": "Objective and Subjective Evaluation of Diffusion-Based Speech Enhancement for Dysarthric Speech", "comment": "Accepted to Interspeech 2025", "summary": "Dysarthric speech poses significant challenges for automatic speech\nrecognition (ASR) systems due to its high variability and reduced\nintelligibility. In this work we explore the use of diffusion models for\ndysarthric speech enhancement, which is based on the hypothesis that using\ndiffusion-based speech enhancement moves the distribution of dysarthric speech\ncloser to that of typical speech, which could potentially improve dysarthric\nspeech recognition performance. We assess the effect of two diffusion-based and\none signal-processing-based speech enhancement algorithms on intelligibility\nand speech quality of two English dysarthric speech corpora. We applied speech\nenhancement to both typical and dysarthric speech and evaluate the ASR\nperformance using Whisper-Turbo, and the subjective and objective speech\nquality of the original and enhanced dysarthric speech. We also fine-tuned\nWhisper-Turbo on the enhanced speech to assess its impact on recognition\nperformance."}
{"id": "2508.17607", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17607", "abs": "https://arxiv.org/abs/2508.17607", "authors": ["Yankai Zhang", "Jiafeng Ding", "Jingjing Ning", "Qiaoxi Zhu"], "title": "Steerable Invariant Beamformer Using a Differential Line Array of Omnidirectional and Directional Microphones with Null Constraints", "comment": "12 pages, 15 figures", "summary": "Line differential microphone arrays have attracted attention for their\nability to achieve frequency-invariant beampatterns and high directivity.\nRecently, the Jacobi-Anger expansion-based approach has enabled the design of\nfully steerable-invariant differential beamformers for line arrays combining\nomnidirectional and directional microphones. However, this approach relies on\nthe analytical expression of the ideal beam pattern and the proper selection of\ntruncation order, which is not always practical. This paper introduces a\nnull-constraint-based method for designing frequency- and steerable-invariant\ndifferential beamformers using a line array of omnidirectional and directional\nmicrophones. The approach employs a multi-constraint optimisation framework,\nwhere the reference filter and ideal beam pattern are first determined based on\nspecified nulls and desired direction. Subsequently, the white noise gain\nconstraint is derived from the reference filter, and the beampattern constraint\nis from the ideal beam pattern. The optimal filter is then obtained by\nconsidering constraints related to the beampattern, nulls, and white noise\ngain. This method achieves a balance between white noise gain and mean square\nerror, allowing robust, frequency- and steerableinvariant differential\nbeamforming performance. It addresses limitations in beam pattern flexibility\nand truncation errors, offering greater design freedom and improved practical\napplicability. Simulations and experiments demonstrate that this method\noutperforms the Jacobi-Anger expansion-based approach in three key aspects: an\nextended effective range, improved main lobe and null alignment, and greater\nflexibility in microphone array configuration and beam pattern design,\nrequiring only steering direction and nulls instead of an analytic beam pattern\nexpression."}
{"id": "2508.17336", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17336", "abs": "https://arxiv.org/abs/2508.17336", "authors": ["Yunsik Kim", "Yoonyoung Chung"], "title": "Modality-Specific Speech Enhancement and Noise-Adaptive Fusion for Acoustic and Body-Conduction Microphone Framework", "comment": null, "summary": "Body\\-conduction microphone signals (BMS) bypass airborne sound, providing\nstrong noise resistance. However, a complementary modality is required to\ncompensate for the inherent loss of high\\-frequency information. In this study,\nwe propose a novel multi\\-modal framework that combines BMS and acoustic\nmicrophone signals (AMS) to achieve both noise suppression and high\\-frequency\nreconstruction. Unlike conventional multi\\-modal approaches that simply merge\nfeatures, our method employs two specialized networks\\: a mapping-based model\nto enhance BMS and a masking-based model to denoise AMS. These networks are\nintegrated through a dynamic fusion mechanism that adapts to local noise\nconditions, ensuring the optimal use of each modality's strengths. We performed\nevaluations on the TAPS dataset, augmented with DNS\\-2023 noise clips, using\nobjective speech quality metrics. The results clearly demonstrate that our\napproach outperforms single\\-modal solutions in a wide range of noisy\nenvironments."}
{"id": "2508.18006", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.18006", "abs": "https://arxiv.org/abs/2508.18006", "authors": ["Alessio Falai", "Ziyao Zhang", "Akos Gangoly"], "title": "Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters", "comment": "Accepted at IEEE MLSP 2025", "summary": "In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesis\nthrough the lens of adapters, in the context of lightweight TTS systems. In\nparticular, we compare the tasks of unseen speaker and language adaptation with\nthe goal of synthesising a target voice in a target language, in which the\ntarget voice has no recordings therein. Results from objective evaluations\ndemonstrate the effectiveness of adapters in learning language-specific and\nspeaker-specific information, allowing pre-trained models to learn unseen\nspeaker identities or languages, while avoiding catastrophic forgetting of the\noriginal model's speaker or language information. Additionally, to measure how\nnative the generated voices are in terms of accent, we propose and validate an\nobjective metric inspired by mispronunciation detection techniques in\nsecond-language (L2) learners. The paper also provides insights into the impact\nof adapter placement, configuration and the number of speakers used."}
{"id": "2508.17640", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17640", "abs": "https://arxiv.org/abs/2508.17640", "authors": ["Can Zheng", "Jiguang He", "Chung G. Kang", "Guofa Cai", "Henk Wymeersch"], "title": "Multimodal Radio and Vision Fusion for Robust Localization in Urban V2I Communications", "comment": "6 pages, 6 figures, submitted to conference", "summary": "Accurate localization is critical for vehicle-to-infrastructure (V2I)\ncommunication systems, especially in urban areas where GPS signals are often\nobstructed by tall buildings, leading to significant positioning errors,\nnecessitating alternative or complementary techniques for reliable and precise\npositioning in applications like autonomous driving and smart city\ninfrastructure. This paper proposes a multimodal contrastive learning\nregression based localization framework for V2I scenarios that combines channel\nstate information (CSI) with visual information to achieve improved accuracy\nand reliability. The approach leverages the complementary strengths of wireless\nand visual data to overcome the limitations of traditional localization\nmethods, offering a robust solution for V2I applications. Simulation results\ndemonstrate that the proposed CSI and vision fusion model significantly\noutperforms traditional methods and single modal models, achieving superior\nlocalization accuracy and precision in complex urban environments."}
{"id": "2508.17660", "categories": ["cs.SD", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.17660", "abs": "https://arxiv.org/abs/2508.17660", "authors": ["Yuanda Wang", "Bocheng Chen", "Hanqing Guo", "Guangjing Wang", "Weikang Ding", "Qiben Yan"], "title": "ClearMask: Noise-Free and Naturalness-Preserving Protection Against Voice Deepfake Attacks", "comment": "14 Pages, Accepted by AsiaCCS 2025", "summary": "Voice deepfake attacks, which artificially impersonate human speech for\nmalicious purposes, have emerged as a severe threat. Existing defenses\ntypically inject noise into human speech to compromise voice encoders in speech\nsynthesis models. However, these methods degrade audio quality and require\nprior knowledge of the attack approaches, limiting their effectiveness in\ndiverse scenarios. Moreover, real-time audios, such as speech in virtual\nmeetings and voice messages, are still exposed to voice deepfake threats. To\novercome these limitations, we propose ClearMask, a noise-free defense\nmechanism against voice deepfake attacks. Unlike traditional approaches,\nClearMask modifies the audio mel-spectrogram by selectively filtering certain\nfrequencies, inducing a transferable voice feature loss without injecting\nnoise. We then apply audio style transfer to further deceive voice decoders\nwhile preserving perceived sound quality. Finally, optimized reverberation is\nintroduced to disrupt the output of voice generation models without affecting\nthe naturalness of the speech. Additionally, we develop LiveMask to protect\nstreaming speech in real-time through a universal frequency filter and\nreverberation generator. Our experimental results show that ClearMask and\nLiveMask effectively prevent voice deepfake attacks from deceiving speaker\nverification models and human listeners, even for unseen voice synthesis models\nand black-box API services. Furthermore, ClearMask demonstrates resilience\nagainst adaptive attackers who attempt to recover the original audio signal\nfrom the protected speech samples."}
{"id": "2508.16790", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.16790", "abs": "https://arxiv.org/abs/2508.16790", "authors": ["Yuancheng Wang", "Dekun Chen", "Xueyao Zhang", "Junan Zhang", "Jiaqi Li", "Zhizheng Wu"], "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling", "comment": null, "summary": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer."}
{"id": "2508.17704", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17704", "abs": "https://arxiv.org/abs/2508.17704", "authors": ["Neil Irwin Bernardo"], "title": "Symbol Detection Using an Integrate-and-Fire Time Encoding Receiver", "comment": "5 pages, 2 figures. This work has been accepted for publication at\n  the 38th IEEE Workshop on Signal Processing Systems (SiPS 2025)", "summary": "Event-driven sampling is a promising alternative to uniform sampling methods,\nparticularly for systems constrained by power and hardware cost. A notable\nexample of this sampling approach is the integrate-and-fire time encoding\nmachine (IF-TEM), which encodes an analog signal into a sequence of time stamps\nby generating an event each time the integral of the input signal reaches a\nfixed threshold. In this paper, we propose a receiver architecture that\nestimates the sequence of transmitted symbols directly from the encoded time\nstamps, called time encodings, produced by the IF-TEM sampler on the received\nsignal. We show that waveform reconstruction from time encodings is not\nnecessary for symbol detection. We develop an analytical approximation for the\nsymbol error probability (SEP) of the proposed IF-TEM-based receiver and show\nthat it closely matches the SEP results obtained through Monte Carlo\nsimulations. Additionally, we demonstrate that narrowing the 3 dB bandwidth of\nthe transmit pulse shaping filter degrades the proposed IF-TEM receiver's\nperformance, highlighting a trade-off between spectral efficiency and error\nresilience."}
{"id": "2508.17868", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.17868", "abs": "https://arxiv.org/abs/2508.17868", "authors": ["Takuhiro Kaneko", "Hirokazu Kameoka", "Kou Tanaka", "Yuto Kondo"], "title": "FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation", "comment": "Accepted to Interspeech 2025. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastervoicegrad/", "summary": "A diffusion-based voice conversion (VC) model (e.g., VoiceGrad) can achieve\nhigh speech quality and speaker similarity; however, its conversion process is\nslow owing to iterative sampling. FastVoiceGrad overcomes this limitation by\ndistilling VoiceGrad into a one-step diffusion model. However, it still\nrequires a computationally intensive content encoder to disentangle the\nspeaker's identity and content, which slows conversion. Therefore, we propose\nFasterVoiceGrad, a novel one-step diffusion-based VC model obtained by\nsimultaneously distilling a diffusion model and content encoder using\nadversarial diffusion conversion distillation (ADCD), where distillation is\nperformed in the conversion process while leveraging adversarial and score\ndistillation training. Experimental evaluations of one-shot VC demonstrated\nthat FasterVoiceGrad achieves competitive VC performance compared to\nFastVoiceGrad, with 6.6-6.9 and 1.8 times faster speed on a GPU and CPU,\nrespectively."}
{"id": "2508.17229", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.17229", "abs": "https://arxiv.org/abs/2508.17229", "authors": ["Junan Zhang", "Xueyao Zhang", "Jing Yang", "Yuancheng Wang", "Fan Fan", "Zhizheng Wu"], "title": "Multi-Metric Preference Alignment for Generative Speech Restoration", "comment": "16 pages, 10 figures. demopage: https://gensr-pref.github.io", "summary": "Recent generative models have significantly advanced speech restoration\ntasks, yet their training objectives often misalign with human perceptual\npreferences, resulting in suboptimal quality. While post-training alignment has\nproven effective in other generative domains like text and image generation,\nits application to generative speech restoration remains largely\nunder-explored. This work investigates the challenges of applying\npreference-based post-training to this task, focusing on how to define a robust\npreference signal and curate high-quality data to avoid reward hacking. To\naddress these challenges, we propose a multi-metric preference alignment\nstrategy. We construct a new dataset, GenSR-Pref, comprising 80K preference\npairs, where each chosen sample is unanimously favored by a complementary suite\nof metrics covering perceptual quality, signal fidelity, content consistency,\nand timbre preservation. This principled approach ensures a holistic preference\nsignal. Applying Direct Preference Optimization (DPO) with our dataset, we\nobserve consistent and significant performance gains across three diverse\ngenerative paradigms: autoregressive models (AR), masked generative models\n(MGM), and flow-matching models (FM) on various restoration benchmarks, in both\nobjective and subjective evaluations. Ablation studies confirm the superiority\nof our multi-metric strategy over single-metric approaches in mitigating reward\nhacking. Furthermore, we demonstrate that our aligned models can serve as\npowerful ''data annotators'', generating high-quality pseudo-labels to serve as\na supervision signal for traditional discriminative models in data-scarce\nscenarios like singing voice restoration. Demo\nPage:https://gensr-pref.github.io"}
{"id": "2508.17710", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17710", "abs": "https://arxiv.org/abs/2508.17710", "authors": ["Dianhao Jia", "Wenqian Shen", "Jianping An", "Byonghyo Shim"], "title": "Blind Channel Estimation for RIS-Assisted Millimeter Wave Communication Systems", "comment": null, "summary": "In the research of RIS-assisted communication systems, channel estimation is\na problem of vital importance for further performance optimization. In order to\nreduce the pilot overhead to the greatest extent, blind channel estimation\nmethods are required, which can estimate the channel and the transmit signals\nat the same time without transmitting pilot sequence. Different from existing\nresearches in traditional MIMO systems, the RIS-assisted two-hop channel brings\nnew challenges to the blind channel estimation design. Hence, a novel blind\nchannel estimation method based on compressed sensing for RIS-assisted\nmultiuser millimeter wave communication systems is proposed for the first time\nin this paper. Specifically, for accurately estimating the RIS-assisted two-hop\nchannel without transmitting pilots, we propose a block-wise transmission\nscheme. Among different blocks of data transmission, RIS elements are\nreconfigured for better estimating the cascade channel. Inside each block, data\nfor each user are mapped to a codeword for realizing the transmit signal\nrecovery and equivalent channel estimation simultaneously. Simulation results\ndemonstrate that our method can achieve a considerable accuracy of channel\nestimation and transmit signal recovery."}
{"id": "2508.17874", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.17874", "abs": "https://arxiv.org/abs/2508.17874", "authors": ["Takuhiro Kaneko", "Hirokazu Kameoka", "Kou Tanaka", "Yuto Kondo"], "title": "Vocoder-Projected Feature Discriminator", "comment": "Accepted to Interspeech 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/vpfd/", "summary": "In text-to-speech (TTS) and voice conversion (VC), acoustic features, such as\nmel spectrograms, are typically used as synthesis or conversion targets owing\nto their compactness and ease of learning. However, because the ultimate goal\nis to generate high-quality waveforms, employing a vocoder to convert these\nfeatures into waveforms and applying adversarial training in the time domain is\nreasonable. Nevertheless, upsampling the waveform introduces significant time\nand memory overheads. To address this issue, we propose a vocoder-projected\nfeature discriminator (VPFD), which uses vocoder features for adversarial\ntraining. Experiments on diffusion-based VC distillation demonstrated that a\npretrained and frozen vocoder feature extractor with a single upsampling step\nis necessary and sufficient to achieve a VC performance comparable to that of\nwaveform discriminators while reducing the training time and memory consumption\nby 9.6 and 11.4 times, respectively."}
{"id": "2508.17868", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.17868", "abs": "https://arxiv.org/abs/2508.17868", "authors": ["Takuhiro Kaneko", "Hirokazu Kameoka", "Kou Tanaka", "Yuto Kondo"], "title": "FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation", "comment": "Accepted to Interspeech 2025. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/fastervoicegrad/", "summary": "A diffusion-based voice conversion (VC) model (e.g., VoiceGrad) can achieve\nhigh speech quality and speaker similarity; however, its conversion process is\nslow owing to iterative sampling. FastVoiceGrad overcomes this limitation by\ndistilling VoiceGrad into a one-step diffusion model. However, it still\nrequires a computationally intensive content encoder to disentangle the\nspeaker's identity and content, which slows conversion. Therefore, we propose\nFasterVoiceGrad, a novel one-step diffusion-based VC model obtained by\nsimultaneously distilling a diffusion model and content encoder using\nadversarial diffusion conversion distillation (ADCD), where distillation is\nperformed in the conversion process while leveraging adversarial and score\ndistillation training. Experimental evaluations of one-shot VC demonstrated\nthat FasterVoiceGrad achieves competitive VC performance compared to\nFastVoiceGrad, with 6.6-6.9 and 1.8 times faster speed on a GPU and CPU,\nrespectively."}
{"id": "2508.17742", "categories": ["eess.SP", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17742", "abs": "https://arxiv.org/abs/2508.17742", "authors": ["Wei Xiong", "Jiangtong Li", "Jie Li", "Kun Zhu"], "title": "EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models", "comment": "17 pages, 7 pages", "summary": "Electroencephalography (EEG) foundation models are poised to significantly\nadvance brain signal analysis by learning robust representations from\nlarge-scale, unlabeled datasets. However, their rapid proliferation has\noutpaced the development of standardized evaluation benchmarks, which\ncomplicates direct model comparisons and hinders systematic scientific\nprogress. This fragmentation fosters scientific inefficiency and obscures\ngenuine architectural advancements. To address this critical gap, we introduce\nEEG-FM-Bench, the first comprehensive benchmark for the systematic and\nstandardized evaluation of EEG foundation models (EEG-FMs). Our contributions\nare threefold: (1) we curate a diverse suite of downstream tasks and datasets\nfrom canonical EEG paradigms, implementing standardized processing and\nevaluation protocols within a unified open-source framework; (2) we benchmark\nprominent state-of-the-art foundation models to establish comprehensive\nbaseline results for a clear comparison of the current landscape; (3) we\nperform qualitative analyses of the learned representations to provide insights\ninto model behavior and inform future architectural design. Through extensive\nexperiments, we find that fine-grained spatio-temporal feature interaction,\nmultitask unified training and neuropsychological priors would contribute to\nenhancing model performance and generalization capabilities. By offering a\nunified platform for fair comparison and reproducible research, EEG-FM-Bench\nseeks to catalyze progress and guide the community toward the development of\nmore robust and generalizable EEG-FMs. Code is released at\nhttps://github.com/xw1216/EEG-FM-Bench."}
{"id": "2508.17878", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17878", "abs": "https://arxiv.org/abs/2508.17878", "authors": ["Honghong Wang", "Jing Deng", "Fanqin Meng", "Rong Zheng"], "title": "Enhancing Speech Emotion Recognition with Multi-Task Learning and Dynamic Feature Fusion", "comment": "accepted by interspeech2025", "summary": "This study investigates fine-tuning self-supervised learn ing (SSL) models\nusing multi-task learning (MTL) to enhance\n  speech emotion recognition (SER). The framework simultane ously handles four\nrelated tasks: emotion recognition, gender\n  recognition, speaker verification, and automatic speech recog nition. An\ninnovative co-attention module is introduced to dy namically capture the\ninteractions between features from the\n  primary emotion classification task and auxiliary tasks, en abling\ncontext-aware fusion. Moreover, We introduce the Sam ple Weighted Focal\nContrastive (SWFC) loss function to ad dress class imbalance and semantic\nconfusion by adjusting sam ple weights for difficult and minority samples. The\nmethod is\n  validated on the Categorical Emotion Recognition task of the\n  Speech Emotion Recognition in Naturalistic Conditions Chal lenge, showing\nsignificant performance improvements."}
{"id": "2508.17874", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.17874", "abs": "https://arxiv.org/abs/2508.17874", "authors": ["Takuhiro Kaneko", "Hirokazu Kameoka", "Kou Tanaka", "Yuto Kondo"], "title": "Vocoder-Projected Feature Discriminator", "comment": "Accepted to Interspeech 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/vpfd/", "summary": "In text-to-speech (TTS) and voice conversion (VC), acoustic features, such as\nmel spectrograms, are typically used as synthesis or conversion targets owing\nto their compactness and ease of learning. However, because the ultimate goal\nis to generate high-quality waveforms, employing a vocoder to convert these\nfeatures into waveforms and applying adversarial training in the time domain is\nreasonable. Nevertheless, upsampling the waveform introduces significant time\nand memory overheads. To address this issue, we propose a vocoder-projected\nfeature discriminator (VPFD), which uses vocoder features for adversarial\ntraining. Experiments on diffusion-based VC distillation demonstrated that a\npretrained and frozen vocoder feature extractor with a single upsampling step\nis necessary and sufficient to achieve a VC performance comparable to that of\nwaveform discriminators while reducing the training time and memory consumption\nby 9.6 and 11.4 times, respectively."}
{"id": "2508.17852", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17852", "abs": "https://arxiv.org/abs/2508.17852", "authors": ["Hossein Mohammadi Firouzjaei", "Rafaela Scaciota", "Sumudu Samarakoon", "Beatriz Lorenzo"], "title": "Cross-Domain Lifelong Reinforcement Learning for Wireless Sensor Networks", "comment": null, "summary": "Wireless sensor networks (WSNs) with energy harvesting (EH) are expected to\nplay a vital role in intelligent 6G systems, especially in industrial sensing\nand control, where continuous operation and sustainable energy use are\ncritical. Given limited energy resources, WSNs must operate efficiently to\nensure long-term performance. Their deployment, however, is challenged by\ndynamic environments where EH conditions, network scale, and traffic rates\nchange over time. In this work, we address system dynamics that yield different\nlearning tasks, where decision variables remain fixed but strategies vary, as\nwell as learning domains, where both decision space and strategies evolve. To\nhandle such scenarios, we propose a cross-domain lifelong reinforcement\nlearning (CD-L2RL) framework for energy-efficient WSN design. Our CD-L2RL\nalgorithm leverages prior experience to accelerate adaptation across tasks and\ndomains. Unlike conventional approaches based on Markov decision processes or\nLyapunov optimization, which assume relatively stable environments, our\nsolution achieves rapid policy adaptation by reusing knowledge from past tasks\nand domains to ensure continuous operations. We validate the approach through\nextensive simulations under diverse conditions. Results show that our method\nimproves adaptation speed by up to 35% over standard reinforcement learning and\nup to 70% over Lyapunov-based optimization, while also increasing total\nharvested energy. These findings highlight the strong potential of CD-L2RL for\ndeployment in dynamic 6G WSNs."}
{"id": "2508.18057", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18057", "abs": "https://arxiv.org/abs/2508.18057", "authors": ["Wenqiang Sun", "Han Yin", "Jisheng Bai", "Jianfeng Chen"], "title": "Dynamic Fusion Multimodal Network for SpeechWellness Detection", "comment": "6 pages, 5figures", "summary": "Suicide is one of the leading causes of death among adolescents. Previous\nsuicide risk prediction studies have primarily focused on either textual or\nacoustic information in isolation, the integration of multimodal signals, such\nas speech and text, offers a more comprehensive understanding of an\nindividual's mental state. Motivated by this, and in the context of the 1st\nSpeechWellness detection challenge, we explore a lightweight multi-branch\nmultimodal system based on a dynamic fusion mechanism for speechwellness\ndetection. To address the limitation of prior approaches that rely on\ntime-domain waveforms for acoustic analysis, our system incorporates both\ntime-domain and time-frequency (TF) domain acoustic features, as well as\nsemantic representations. In addition, we introduce a dynamic fusion block to\nadaptively integrate information from different modalities. Specifically, it\napplies learnable weights to each modality during the fusion process, enabling\nthe model to adjust the contribution of each modality. To enhance computational\nefficiency, we design a lightweight structure by simplifying the original\nbaseline model. Experimental results demonstrate that the proposed system\nexhibits superior performance compared to the challenge baseline, achieving a\n78% reduction in model parameters and a 5% improvement in accuracy."}
{"id": "2508.17873", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.17873", "abs": "https://arxiv.org/abs/2508.17873", "authors": ["Mehdi Abdollahpour", "Carsten Bockelmann", "Tajim Md Hasibur Rahman", "Armin Dekorsy", "Andreas Fischer"], "title": "Compressed Learning for Nanosurface Deficiency Recognition Using Angle-resolved Scatterometry Data", "comment": null, "summary": "Nanoscale manufacturing requires high-precision surface inspection to\nguarantee the quality of the produced nanostructures. For production\nenvironments, angle-resolved scatterometry offers a non- invasive and in-line\ncompatible alternative to traditional surface inspection methods, such as\nscanning electron microscopy. However, angle-resolved scatterometry currently\nsuffers from long data acquisition time. Our study addresses the issue of slow\ndata acquisition by proposing a compressed learning framework for the accurate\nrecognition of nanosurface deficiencies using angle-resolved scatterometry\ndata. The framework uses the particle swarm optimization algorithm with a\nsampling scheme customized for scattering patterns. This combination allows the\nidentification of optimal sampling points in scatterometry data that maximize\nthe detection accuracy of five different levels of deficiency in ZnO\nnanosurfaces. The proposed method significantly reduces the amount of sampled\ndata while maintaining a high accuracy in deficiency detection, even in noisy\nenvironments. Notably, by sampling only 1% of the data, the method achieves an\naccuracy of over 86%, which further improves to 94% when the sampling rate is\nincreased to 6%. These results demonstrate a favorable balance between data\nreduction and classification performance. The obtained results also show that\nthe compressed learning framework effectively identifies critical sampling\nareas."}
{"id": "2508.16908", "categories": ["eess.AS", "cs.HC", "cs.NI", "cs.SD", "eess.SP", "C.3; C.2.1; C.2.4; I.5.4; H.5.2; J.7"], "pdf": "https://arxiv.org/pdf/2508.16908", "abs": "https://arxiv.org/abs/2508.16908", "authors": ["Amod K. Agrawal"], "title": "Localization using Angle-of-Arrival Triangulation", "comment": "6 pages, 5 figures, 1 table. Accepted at the ACM International\n  Workshop on Environmental Sensing Systems for Smart Cities (EnvSys 2025). To\n  appear in the MobiSys 2025 Proceedings", "summary": "Indoor localization is a long-standing challenge in mobile computing, with\nsignificant implications for enabling location-aware and intelligent\napplications within smart environments such as homes, offices, and retail\nspaces. As AI assistants such as Amazon Alexa and Google Nest become\nincreasingly pervasive, microphone-equipped devices are emerging as key\ncomponents of everyday life and home automation. This paper introduces a\npassive, infrastructure-light system for localizing human speakers using speech\nsignals captured by two or more spatially distributed smart devices. The\nproposed approach, GCC+, extends the Generalized Cross-Correlation with Phase\nTransform (GCC-PHAT) method to estimate the Angle-of-Arrival (AoA) of audio\nsignals at each device and applies robust triangulation techniques to infer the\nspeaker's two-dimensional position. To further improve temporal resolution and\nlocalization accuracy, feature-space expansion and subsample interpolation\ntechniques are employed for precise Time Difference of Arrival (TDoA)\nestimation. The system operates without requiring hardware modifications, prior\ncalibration, explicit user cooperation, or knowledge of the speaker's signal\ncontent, thereby offering a highly practical solution for real-world\ndeployment. Experimental evaluation in a real-world home environment yields a\nmedian AoA estimation error of 2.2 degrees and a median localization error of\n1.25 m, demonstrating the feasibility and effectiveness of audio-based\nlocalization for enabling context-aware, privacy-preserving ambient\nintelligence."}
{"id": "2508.17942", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17942", "abs": "https://arxiv.org/abs/2508.17942", "authors": ["Qingtang Jiang", "Shuixin Li", "Jiecheng Chen", "Lin Li"], "title": "Synchrosqueezed X-Ray Wavelet-Chirplet Transform for Accurate Chirp Rate Estimation and Retrieval of Modes from Multicomponent Signals with Crossover Instantaneous Frequencies", "comment": null, "summary": "Recent advances in the chirplet transform and wavelet-chirplet transform\n(WCT) have enabled the estimation of instantaneous frequencies (IFs) and\nchirprates, as well as mode retrieval from multicomponent signals with\ncrossover IF curves. However, chirprate estimation via these approaches remains\nless accurate than IF estimation, primarily due to the slow decay of the\nchirplet transform or WCT along the chirprate direction. To address this, the\nsynchrosqueezed chirplet transform (SCT) and multiple SCT methods were\nproposed, achieving moderate improvements in IF and chirprate estimation\naccuracy. Nevertheless, a novel approach is still needed to enhance the\ntransform's decay along the chirprate direction.\n  This paper introduces an X-ray transform-based wavelet-chirprate transform,\ntermed the X-ray wavelet-chirplet transform (XWCT), which exhibits superior\ndecay along the chirprate direction compared to the WCT. Furthermore,\nthird-order synchrosqueezed variants of the WCT and XWCT are developed to yield\nsharp time-frequency-chirprate representations of signals. Experimental results\ndemonstrate that the XWCT achieves significantly faster decay along the\nchirprate axis, while the third-order synchrosqueezed XWCT enables accurate IF\nand chirprate estimation, as well as mode retrieval, without requiring multiple\nsynchrosqueezing operations."}
{"id": "2508.16930", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.16930", "abs": "https://arxiv.org/abs/2508.16930", "authors": ["Sizhe Shan", "Qiulin Li", "Yutao Cui", "Miles Yang", "Yuehai Wang", "Qun Yang", "Jin Zhou", "Zhao Zhong"], "title": "HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation", "comment": null, "summary": "Recent advances in video generation produce visually realistic content, yet\nthe absence of synchronized audio severely compromises immersion. To address\nkey challenges in video-to-audio generation, including multimodal data\nscarcity, modality imbalance and limited audio quality in existing methods, we\npropose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that\nsynthesizes high-fidelity audio precisely aligned with visual dynamics and\nsemantic context. Our approach incorporates three core innovations: (1) a\nscalable data pipeline curating 100k-hour multimodal datasets through automated\nannotation; (2) a representation alignment strategy using self-supervised audio\nfeatures to guide latent diffusion training, efficiently improving audio\nquality and generation stability; (3) a novel multimodal diffusion transformer\nresolving modal competition, containing dual-stream audio-video fusion through\njoint attention, and textual semantic injection via cross-attention.\nComprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new\nstate-of-the-art performance across audio fidelity, visual-semantic alignment,\ntemporal alignment and distribution matching. The demo page is available at:\nhttps://szczesnys.github.io/hunyuanvideo-foley/."}
{"id": "2508.17960", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17960", "abs": "https://arxiv.org/abs/2508.17960", "authors": ["Yuto Kawai", "Rajeev Koodli"], "title": "A Unified Transformer Architecture for Low-Latency and Scalable Wireless Signal Processing", "comment": "10 pages, 8 figures", "summary": "We propose a unified Transformer-based architecture for wireless signal\nprocessing tasks, offering a low-latency, task-adaptive alternative to\nconventional receiver pipelines. Unlike traditional modular designs, our model\nintegrates channel estimation, interpolation, and demapping into a single,\ncompact attention-driven architecture designed for real-time deployment. The\nmodel's structure allows dynamic adaptation to diverse output formats by simply\nmodifying the final projection layer, enabling consistent reuse across receiver\nsubsystems. Experimental results demonstrate strong generalization to varying\nuser counts, modulation schemes, and pilot configurations, while satisfying\nlatency constraints imposed by practical systems. The architecture is evaluated\nacross three core use cases: (1) an End-to-End Receiver, which replaces the\nentire baseband processing pipeline from pilot symbols to bit-level decisions;\n(2) Channel Frequency Interpolation, implemented and tested within a\n3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model\ninfers full-band channel responses from sparse pilot observations. In all\ncases, our approach outperforms classical baselines in terms of accuracy,\nrobustness, and computational efficiency. This work presents a deployable,\ndata-driven alternative to hand-engineered PHY-layer blocks, and lays the\nfoundation for intelligent, software-defined signal processing in\nnext-generation wireless communication systems."}
{"id": "2508.17980", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.17980", "abs": "https://arxiv.org/abs/2508.17980", "authors": ["Dimme de Groot", "Tanvina Patel", "Devendra Kayande", "Odette Scharenborg", "Zhengjun Yue"], "title": "Objective and Subjective Evaluation of Diffusion-Based Speech Enhancement for Dysarthric Speech", "comment": "Accepted to Interspeech 2025", "summary": "Dysarthric speech poses significant challenges for automatic speech\nrecognition (ASR) systems due to its high variability and reduced\nintelligibility. In this work we explore the use of diffusion models for\ndysarthric speech enhancement, which is based on the hypothesis that using\ndiffusion-based speech enhancement moves the distribution of dysarthric speech\ncloser to that of typical speech, which could potentially improve dysarthric\nspeech recognition performance. We assess the effect of two diffusion-based and\none signal-processing-based speech enhancement algorithms on intelligibility\nand speech quality of two English dysarthric speech corpora. We applied speech\nenhancement to both typical and dysarthric speech and evaluate the ASR\nperformance using Whisper-Turbo, and the subjective and objective speech\nquality of the original and enhanced dysarthric speech. We also fine-tuned\nWhisper-Turbo on the enhanced speech to assess its impact on recognition\nperformance."}
{"id": "2508.18009", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18009", "abs": "https://arxiv.org/abs/2508.18009", "authors": ["Leonardo Tercas", "Markku Juntti"], "title": "Positioning via Probabilistic Graphical Models in RIS-Aided Systems with Channel Estimation Errors", "comment": "6 pages, 4 figures, 2 tables. Presented at 2025 IEEE International\n  Conference on Communications (ICC), June 2025, Montreal, Canada", "summary": "We propose a 6D Bayesian-based localization framework to estimate the\nposition and rotation angles of a mobile station (MS) within an indoor\nreconfigurable intelligent surface (RIS)-aided system. This framework relies on\na probabilistic graphical model to represent the joint probability distribution\nof random variables through their conditional dependencies and employs the\nNo-U-Turn Sampler (NUTS) to approximate the posterior distribution based on the\nestimated channel parameters. Our framework estimates both the position and\nrotation of the mobile station (MS), in the presence of channel parameter\nestimation errors. We derive the Cramer-Rao lower bound (CRLB) for the proposed\nscenario and use it to evaluate the system's position error bound (PEB) and\nrotation error bound (REB). We compare the system performances with and without\nRIS. The results demonstrate that the RIS can enhance positioning accuracy\nsignificantly."}
{"id": "2508.18006", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.18006", "abs": "https://arxiv.org/abs/2508.18006", "authors": ["Alessio Falai", "Ziyao Zhang", "Akos Gangoly"], "title": "Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters", "comment": "Accepted at IEEE MLSP 2025", "summary": "In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesis\nthrough the lens of adapters, in the context of lightweight TTS systems. In\nparticular, we compare the tasks of unseen speaker and language adaptation with\nthe goal of synthesising a target voice in a target language, in which the\ntarget voice has no recordings therein. Results from objective evaluations\ndemonstrate the effectiveness of adapters in learning language-specific and\nspeaker-specific information, allowing pre-trained models to learn unseen\nspeaker identities or languages, while avoiding catastrophic forgetting of the\noriginal model's speaker or language information. Additionally, to measure how\nnative the generated voices are in terms of accent, we propose and validate an\nobjective metric inspired by mispronunciation detection techniques in\nsecond-language (L2) learners. The paper also provides insights into the impact\nof adapter placement, configuration and the number of speakers used."}
{"id": "2508.16908", "categories": ["eess.AS", "cs.HC", "cs.NI", "cs.SD", "eess.SP", "C.3; C.2.1; C.2.4; I.5.4; H.5.2; J.7"], "pdf": "https://arxiv.org/pdf/2508.16908", "abs": "https://arxiv.org/abs/2508.16908", "authors": ["Amod K. Agrawal"], "title": "Localization using Angle-of-Arrival Triangulation", "comment": "6 pages, 5 figures, 1 table. Accepted at the ACM International\n  Workshop on Environmental Sensing Systems for Smart Cities (EnvSys 2025). To\n  appear in the MobiSys 2025 Proceedings", "summary": "Indoor localization is a long-standing challenge in mobile computing, with\nsignificant implications for enabling location-aware and intelligent\napplications within smart environments such as homes, offices, and retail\nspaces. As AI assistants such as Amazon Alexa and Google Nest become\nincreasingly pervasive, microphone-equipped devices are emerging as key\ncomponents of everyday life and home automation. This paper introduces a\npassive, infrastructure-light system for localizing human speakers using speech\nsignals captured by two or more spatially distributed smart devices. The\nproposed approach, GCC+, extends the Generalized Cross-Correlation with Phase\nTransform (GCC-PHAT) method to estimate the Angle-of-Arrival (AoA) of audio\nsignals at each device and applies robust triangulation techniques to infer the\nspeaker's two-dimensional position. To further improve temporal resolution and\nlocalization accuracy, feature-space expansion and subsample interpolation\ntechniques are employed for precise Time Difference of Arrival (TDoA)\nestimation. The system operates without requiring hardware modifications, prior\ncalibration, explicit user cooperation, or knowledge of the speaker's signal\ncontent, thereby offering a highly practical solution for real-world\ndeployment. Experimental evaluation in a real-world home environment yields a\nmedian AoA estimation error of 2.2 degrees and a median localization error of\n1.25 m, demonstrating the feasibility and effectiveness of audio-based\nlocalization for enabling context-aware, privacy-preserving ambient\nintelligence."}

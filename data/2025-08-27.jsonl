{"id": "2508.18295", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18295", "abs": "https://arxiv.org/abs/2508.18295", "authors": ["Huangyu Dai", "Lingtao Mao", "Ben Chen", "Zihan Wang", "Zihan Liang", "Ying Han", "Chenyi Lei", "Han Li"], "title": "H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems", "comment": null, "summary": "Hotword customization is crucial in ASR to enhance the accuracy of\ndomain-specific terms. It has been primarily driven by the advancements in\ntraditional models and Audio large language models (LLMs). However, existing\nmodels often struggle with large-scale hotwords, as the recognition rate drops\ndramatically with the number of hotwords increasing. In this paper, we\nintroduce a novel hotword customization system that utilizes a hotword\npre-retrieval module (H-PRM) to identify the most relevant hotword candidate by\nmeasuring the acoustic similarity between the hotwords and the speech segment.\nThis plug-and-play solution can be easily integrated into traditional models\nsuch as SeACo-Paraformer, significantly enhancing hotwords post-recall rate\n(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a\nprompt-based approach, enabling seamless customization of hotwords. Extensive\ntesting validates that H-PRM can outperform existing methods, showing a new\ndirection for hotword customization in ASR."}
{"id": "2508.18440", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18440", "abs": "https://arxiv.org/abs/2508.18440", "authors": ["Lars Nieradzik"], "title": "SwiftF0: Fast and Accurate Monophonic Pitch Detection", "comment": null, "summary": "Accurate and real-time monophonic pitch estimation in noisy conditions,\nparticularly on resource-constrained devices, remains an open challenge in\naudio processing. We present \\emph{SwiftF0}, a novel, lightweight neural model\nthat sets a new state-of-the-art for monophonic pitch estimation. Through\ntraining on diverse speech, music, and synthetic datasets with extensive data\naugmentation, SwiftF0 achieves robust generalization across acoustic domains\nwhile maintaining computational efficiency. SwiftF0 achieves a 91.80\\% harmonic\nmean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12\npercentage points and degrading by only 2.3 points from clean audio. SwiftF0\nrequires only 95,842 parameters and runs approximately 42x faster than CREPE on\nCPU, making it ideal for efficient, real-time deployment. To address the\ncritical lack of perfectly accurate ground truth pitch in speech corpora (which\ntypically rely on algorithmic estimators or laryngograph signals), we introduce\n\\emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level\nTTS model, provides exact, on-demand ground-truth pitch curves, enabling more\nrobust model training and evaluation. Furthermore, we propose a unified metric,\ncombining six complementary performance measures for comprehensive and reliable\npitch evaluation, and release an open-source pitch benchmark suite. A live demo\nof SwiftF0 is available at https://swift-f0.github.io/, the source code at\nhttps://github.com/lars76/swift-f0, and the benchmark framework at\nhttps://github.com/lars76/pitch-benchmark."}
{"id": "2508.18732", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18732", "abs": "https://arxiv.org/abs/2508.18732", "authors": ["Qing Xiao", "Yingshan Peng", "PeiPei Zhang"], "title": "Cross-Learning Fine-Tuning Strategy for Dysarthric Speech Recognition Via CDSD database", "comment": null, "summary": "Dysarthric speech recognition faces challenges from severity variations and\ndisparities relative to normal speech. Conventional approaches individually\nfine-tune ASR models pre-trained on normal speech per patient to prevent\nfeature conflicts. Counter-intuitively, experiments reveal that multi-speaker\nfine-tuning (simultaneously on multiple dysarthric speakers) improves\nrecognition of individual speech patterns. This strategy enhances\ngeneralization via broader pathological feature learning, mitigates\nspeaker-specific overfitting, reduces per-patient data dependence, and improves\ntarget-speaker accuracy - achieving up to 13.15% lower WER versus\nsingle-speaker fine-tuning."}
{"id": "2508.18907", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18907", "abs": "https://arxiv.org/abs/2508.18907", "authors": ["Ridwan Arefeen", "Xiaoxiao Miao", "Rong Tong", "Aik Beng Ng", "Simon See"], "title": "SegReConcat: A Data Augmentation Method for Voice Anonymization Attack", "comment": "The Paper has been accepted by APCIPA ASC 2025", "summary": "Anonymization of voice seeks to conceal the identity of the speaker while\nmaintaining the utility of speech data. However, residual speaker cues often\npersist, which pose privacy risks. We propose SegReConcat, a data augmentation\nmethod for attacker-side enhancement of automatic speaker verification systems.\nSegReConcat segments anonymized speech at the word level, rearranges segments\nusing random or similarity-based strategies to disrupt long-term contextual\ncues, and concatenates them with the original utterance, allowing an attacker\nto learn source speaker traits from multiple perspectives. The proposed method\nhas been evaluated in the VoicePrivacy Attacker Challenge 2024 framework across\nseven anonymization systems, SegReConcat improves de-anonymization on five out\nof seven systems."}
{"id": "2508.18288", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.18288", "abs": "https://arxiv.org/abs/2508.18288", "authors": ["Jay L. Cunningham", "Adinawa Adjagbodjou", "Jeffrey Basoah", "Jainaba Jawara", "Kowe Kadoma", "Aaleyah Lewis"], "title": "Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology", "comment": "10 pages, 9 Pages (References and Appendices). The archival version\n  has been accepted to AAAI (AIES 2025) without the extended Appendices. This\n  extended version includes Appendices", "summary": "This scoping literature review examines how fairness, bias, and equity are\nconceptualized and operationalized in Automatic Speech Recognition (ASR) and\nadjacent speech and language technologies (SLT) for African American English\n(AAE) speakers and other linguistically diverse communities. Drawing from 44\npeer-reviewed publications across Human-Computer Interaction (HCI), Machine\nLearning/Natural Language Processing (ML/NLP), and Sociolinguistics, we\nidentify four major areas of inquiry: (1) how researchers understand\nASR-related harms; (2) inclusive data practices spanning collection, curation,\nannotation, and model training; (3) methodological and theoretical approaches\nto linguistic inclusion; and (4) emerging practices and design recommendations\nfor more equitable systems. While technical fairness interventions are growing,\nour review highlights a critical gap in governance-centered approaches that\nforeground community agency, linguistic justice, and participatory\naccountability. We propose a governance-centered ASR lifecycle as an emergent\ninterdisciplinary framework for responsible ASR development and offer\nimplications for researchers, practitioners, and policymakers seeking to\naddress language marginalization in speech AI systems."}
{"id": "2508.18712", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18712", "abs": "https://arxiv.org/abs/2508.18712", "authors": ["Samin Yaser", "Mahad Ali", "Laura J. Brattain", "Yang Jiang", "VP Nguyen", "Jing Xiang"], "title": "A Synoptic Review of High-Frequency Oscillations as a Biomarker in Neurodegenerative Disease", "comment": null, "summary": "High Frequency Oscillations (HFOs), rapid bursts of brain activity above 80\nHz, have emerged as a highly specific biomarker for epileptogenic tissue.\nRecent evidence suggests that HFOs are also present in Alzheimer's Disease\n(AD), reflecting underlying network hyperexcitability and offering a promising,\nnoninvasive tool for early diagnosis and disease tracking. This synoptic review\nprovides a comprehensive analysis of publicly available electroencephalography\n(EEG) datasets relevant to HFO research in neurodegenerative disorders. We\nconducted a bibliometric analysis of 1,222 articles, revealing a significant\nand growing research interest in HFOs, particularly within the last ten years.\nWe then systematically profile and compare key public datasets, evaluating\ntheir participant cohorts, data acquisition parameters, and accessibility, with\na specific focus on their technical suitability for HFO analysis. Our\ncomparative synthesis highlights critical methodological heterogeneity across\ndatasets, particularly in sampling frequency and recording paradigms, which\nposes challenges for cross-study validation, but also offers opportunities for\nrobustness testing. By consolidating disparate information, clarifying\nnomenclature, and providing a detailed methodological framework, this review\nserves as a guide for researchers aiming to leverage public data to advance the\nrole of HFOs as a cross-disease biomarker for AD and related conditions."}
{"id": "2508.19180", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19180", "abs": "https://arxiv.org/abs/2508.19180", "authors": ["Yibo Bai", "Sizhou Chen", "Michele Panariello", "Xiao-Lei Zhang", "Massimiliano Todisco", "Nicholas Evans"], "title": "MDD: a Mask Diffusion Detector to Protect Speaker Verification Systems from Adversarial Perturbations", "comment": "Accepted by APSIPA ASC 2025", "summary": "Speaker verification systems are increasingly deployed in security-sensitive\napplications but remain highly vulnerable to adversarial perturbations. In this\nwork, we propose the Mask Diffusion Detector (MDD), a novel adversarial\ndetection and purification framework based on a \\textit{text-conditioned masked\ndiffusion model}. During training, MDD applies partial masking to\nMel-spectrograms and progressively adds noise through a forward diffusion\nprocess, simulating the degradation of clean speech features. A reverse process\nthen reconstructs the clean representation conditioned on the input\ntranscription. Unlike prior approaches, MDD does not require adversarial\nexamples or large-scale pretraining. Experimental results show that MDD\nachieves strong adversarial detection performance and outperforms prior\nstate-of-the-art methods, including both diffusion-based and neural codec-based\napproaches. Furthermore, MDD effectively purifies adversarially-manipulated\nspeech, restoring speaker verification performance to levels close to those\nobserved under clean conditions. These findings demonstrate the potential of\ndiffusion-based masking strategies for secure and reliable speaker verification\nsystems."}
{"id": "2508.18337", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.18337", "abs": "https://arxiv.org/abs/2508.18337", "authors": ["Haijie Yang", "Zhenyu Zhang", "Hao Tang", "Jianjun Qian", "Jian Yang"], "title": "EAI-Avatar: Emotion-Aware Interactive Talking Head Generation", "comment": null, "summary": "Generative models have advanced rapidly, enabling impressive talking head\ngeneration that brings AI to life. However, most existing methods focus solely\non one-way portrait animation. Even the few that support bidirectional\nconversational interactions lack precise emotion-adaptive capabilities,\nsignificantly limiting their practical applicability. In this paper, we propose\nEAI-Avatar, a novel emotion-aware talking head generation framework for dyadic\ninteractions. Leveraging the dialogue generation capability of large language\nmodels (LLMs, e.g., GPT-4), our method produces temporally consistent virtual\navatars with rich emotional variations that seamlessly transition between\nspeaking and listening states. Specifically, we design a Transformer-based head\nmask generator that learns temporally consistent motion features in a latent\nmask space, capable of generating arbitrary-length, temporally consistent mask\nsequences to constrain head motions. Furthermore, we introduce an interactive\ntalking tree structure to represent dialogue state transitions, where each tree\nnode contains information such as child/parent/sibling nodes and the current\ncharacter's emotional state. By performing reverse-level traversal, we extract\nrich historical emotional cues from the current node to guide expression\nsynthesis. Extensive experiments demonstrate the superior performance and\neffectiveness of our method."}
{"id": "2508.18735", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18735", "abs": "https://arxiv.org/abs/2508.18735", "authors": ["Afan Ali", "Irfanullah Khan"], "title": "SkyTrust: Blockchain-Enhanced UAV Security for NTNs with Dynamic Trust and Energy-Aware Consensus", "comment": "6 pages, 7 figures", "summary": "Non-Terrestrial Networks (NTNs) based on Unmanned Aerial Vehicles (UAVs) as\nbase stations are extremely susceptible to security attacks due to their\ndistributed and dynamic nature, which makes them vulnerable to rogue nodes. In\nthis paper, a new Dynamic Trust Score Adjustment Mechanism with Energy-Aware\nConsensus (DTSAM-EAC) is proposed to enhance security in UAV-based NTNs. The\nproposed framework integrates a permissioned Hyperledger Fabric blockchain with\nFederated Learning (FL) to support privacy-preserving trust evaluation. Trust\nratings are updated continuously through weighted aggregation of past trust,\npresent behavior, and energy contribution, thus making the system adaptive to\nchanging network conditions. An energy-aware consensus mechanism prioritizes\nUAVs with greater available energy for block validation, ensuring efficient use\nof resources under resource-constrained environments. FL aggregation with\ntrust-weighting further increases the resilience of the global trust model.\nSimulation results verify the designed framework achieves 94\\% trust score\nprediction accuracy and 96\\% rogue UAV detection rate while outperforming\ncentralized and static baselines of trust-based solutions on privacy, energy\nefficiency, and reliability. It complies with 6G requirements in terms of\ndistributed intelligence and sustainability and is an energy-efficient and\nscalable solution to secure NTNs."}
{"id": "2508.18833", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18833", "abs": "https://arxiv.org/abs/2508.18833", "authors": ["Adrian Meise", "Tobias Cord-Landwehr", "Reinhold Haeb-Umbach"], "title": "On the Application of Diffusion Models for Simultaneous Denoising and Dereverberation", "comment": "Accepted at 16th ITG Conference on Speech Communication 2025", "summary": "Diffusion models have been shown to achieve natural-sounding enhancement of\nspeech degraded by noise or reverberation. However, their simultaneous\ndenoising and dereverberation capability has so far not been studied much,\nalthough this is arguably the most common scenario in a practical application.\nIn this work, we investigate different approaches to enhance noisy and/or\nreverberant speech. We examine the cascaded application of models, each trained\non only one of the distortions, and compare it with a single model, trained\neither solely on data that is both noisy and reverberated, or trained on data\ncomprising subsets of purely noisy, of purely reverberated, and of noisy\nreverberant speech. Tests are performed both on artificially generated and real\nrecordings of noisy and/or reverberant data. The results show that, when using\nthe cascade of models, satisfactory results are only achieved if they are\napplied in the order of the dominating distortion. If only a single model is\ndesired that can operate on all distortion scenarios, the best compromise\nappears to be a model trained on the aforementioned three subsets of degraded\nspeech data."}
{"id": "2508.18810", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18810", "abs": "https://arxiv.org/abs/2508.18810", "authors": ["Yonghwi Kim", "Sang-Hyun Park", "Siyun Yang", "Kai-Kit Wong", "Linglong Dai", "Chan-Byoung Chae"], "title": "Near-Field Challenges in Ultra-Wideband ISAC: Beamforming Strategies and System Insights", "comment": "7 pages, 6 figures", "summary": "The shift toward sixth-generation (6G) wireless networks places integrated\nsensing and communications (ISAC) at the core of future applications such as\nautonomous driving, extended reality, and smart manufacturing. However, the\ncombination of large antenna arrays and ultra-wide bandwidths brings near-field\npropagation effects and beam squint to the forefront, fundamentally challenging\ntraditional far-field designs. True time delay units (TTDs) offer a potential\nsolution, but their cost and hardware complexity limit scalability. In this\narticle, we present practical beamforming strategies for near-field\nultra-wideband ISAC systems. We explore codebook designs across analog and\ndigital domains that mitigate beam squint, ensure reliable user coverage, and\nenhance sensing accuracy. We further validate these approaches through\nlarge-scale system-level simulations, including 3D map-based evaluations that\nreflect real-world urban environments. Our results demonstrate how carefully\ndesigned beamforming can balance communication throughput with sensing\nperformance, achieving reliable coverage and efficient resource use even under\nsevere near-field conditions. We conclude by highlighting open challenges in\nhardware, algorithms, and system integration, pointing toward research\ndirections that will shape the deployment of 6G-ready ISAC networks."}
{"id": "2508.18913", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18913", "abs": "https://arxiv.org/abs/2508.18913", "authors": ["Adam Katav", "Yair Moshe", "Israel Cohen"], "title": "A Framework for Robust Speaker Verification in Highly Noisy Environments Leveraging Both Noisy and Enhanced Audio", "comment": "5 pages, 2 figures, 1 table. Submitted to EUSIPCO 2025. Keywords:\n  speaker verification, speaker recognition, speaker embedding, speech\n  enhancement, ECAPA-TDNN, SpeakerNet, x-vectors, noisy speech, robust\n  embeddings", "summary": "Recent advancements in speaker verification techniques show promise, but\ntheir performance often deteriorates significantly in challenging acoustic\nenvironments. Although speech enhancement methods can improve perceived audio\nquality, they may unintentionally distort speaker-specific information, which\ncan affect verification accuracy. This problem has become more noticeable with\nthe increasing use of generative deep neural networks (DNNs) for speech\nenhancement. While these networks can produce intelligible speech even in\nconditions of very low signal-to-noise ratio (SNR), they may also severely\nalter distinctive speaker characteristics. To tackle this issue, we propose a\nnovel neural network framework that effectively combines speaker embeddings\nextracted from both noisy and enhanced speech using a Siamese architecture.\nThis architecture allows us to leverage complementary information from both\nsources, enhancing the robustness of speaker verification under severe noise\nconditions. Our framework is lightweight and agnostic to specific speaker\nverification and speech enhancement techniques, enabling the use of a wide\nrange of state-of-the-art solutions without modification. Experimental results\ndemonstrate the superior performance of our proposed framework."}
{"id": "2508.18854", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18854", "abs": "https://arxiv.org/abs/2508.18854", "authors": ["Ruifeng Dong", "Ming Wang", "Ning Liu", "Tong Guo", "Jiayi Kang", "Xiaojing Shen", "Yao Mao"], "title": "DIFNet: Decentralized Information Filtering Fusion Neural Network with Unknown Correlation in Sensor Measurement Noises", "comment": null, "summary": "In recent years, decentralized sensor networks have garnered significant\nattention in the field of state estimation owing to enhanced robustness,\nscalability, and fault tolerance. Optimal fusion performance can be achieved\nunder fully connected communication and known noise correlation structures. To\nmitigate communication overhead, the global state estimation problem is\ndecomposed into local subproblems through structured observation model. This\nensures that even when the communication network is not fully connected, each\nsensor can achieve locally optimal estimates of its observable state\ncomponents. To address the degradation of fusion accuracy induced by unknown\ncorrelations in measurement noise, this paper proposes a data-driven method,\ntermed Decentralized Information Filter Neural Network (DIFNet), to learn\nunknown noise correlations in data for discrete-time nonlinear state space\nmodels with cross-correlated measurement noises. Numerical simulations\ndemonstrate that DIFNet achieves superior fusion performance compared to\nconventional filtering methods and exhibits robust characteristics in more\ncomplex scenarios, such as the presence of time-varying noise. The source code\nused in our numerical experiment can be found online at\nhttps://wisdom-estimation.github.io/DIFNet_Demonstrate/."}
{"id": "2508.18998", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.18998", "abs": "https://arxiv.org/abs/2508.18998", "authors": ["Junjie Li", "Jing Peng", "Yangui Fang", "Shuai Wang", "Kai Yu"], "title": "MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in LLM-based Multilingual ASR", "comment": null, "summary": "End-to-end multilingual ASR aims to transcribe speech from different\nlanguages into corresponding text, but is often limited by scarce multilingual\ndata. LLM-based ASR aligns speech encoder outputs with LLM input space via a\nprojector and has achieved notable success. However, prior work mainly improves\nperformance by increasing data, with little focus on cross-lingual knowledge\nsharing. Moreover, a single complex projector struggles to capture both shared\nand language-specific features effectively. In this work, we propose MOSA\n(Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to\ncombine lightweight adapters that learn shared and language-specific knowledge.\nThis enables better utilization of high-resource language data to support\nlow-resource languages, mitigating data scarcity issues. Experimental results\nshow that MOSA-Base achieves a 15.4\\% relative reduction in average WER\ncompared to the Baseline-Base and consistently outperforms it across all\nlanguages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained\nwith only 60\\% of its parameters. Similarly, MOSA-Large outperforms the\nBaseline-Large in average WER and demonstrates greater robustness to data\nimbalance. Ablation studies further indicate that MOSA is more effective at\nhandling individual languages and learning both language-specific and shared\nlinguistic knowledge. These findings support that, in LLM-based ASR, a mixture\nof simple adapters is more effective than a single, complex adapter design."}
{"id": "2508.19000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19000", "abs": "https://arxiv.org/abs/2508.19000", "authors": ["Atso Iivanainen", "Robin Rajamäki", "Visa Koivunen"], "title": "Beyond-Diagonal RIS: Adversarial Channels and Optimality of Low-Complexity Architectures", "comment": "\\copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) have recently\ngained attention as an enhancement to conventional RISs. BD-RISs allow\noptimizing not only the phase, but also the amplitude responses of their\ndiscrete surface elements by introducing adjustable inter-element couplings.\nVarious BD-RIS architectures have been proposed to optimally trade off between\naverage performance and complexity of the architecture. However, little\nattention has been paid to worst-case performance. This paper characterizes\nnovel sets of adversarial channels for which certain low-complexity BD-RIS\narchitectures have suboptimal performance in terms of received signal power at\nan intended communications user. Specifically, we consider two recent BD-RIS\nmodels: the so-called group-connected and tree-connected architecture. The\nderived adversarial channel sets reveal new surprising connections between the\ntwo architectures. We validate our analytical results numerically,\ndemonstrating that adversarial channels can cause a significant performance\nloss. Our results pave the way towards efficient BD-RIS designs that are robust\nto adversarial propagation conditions and malicious attacks."}
{"id": "2508.19098", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19098", "abs": "https://arxiv.org/abs/2508.19098", "authors": ["Chun Yat Wu", "Jiajun Deng", "Guinan Li", "Qiuqiang Kong", "Simon Lui"], "title": "CLEAR: Continuous Latent Autoregressive Modeling for High-quality and Low-latency Speech Synthesis", "comment": "Preprint", "summary": "Autoregressive (AR) language models have emerged as powerful solutions for\nzero-shot text-to-speech (TTS) synthesis, capable of generating natural speech\nfrom a few seconds of audio prompts. However, conventional AR-based TTS systems\nrelying on discrete audio tokens face the challenge of lossy compression during\ntokenization, requiring longer discrete token sequences to capture the same\ninformation as continuous ones, which adds inference latency and complicates AR\nmodeling. To address this challenge, this paper proposes the Continuous Latent\nAutoregressive model (CLEAR), a unified zero-shot TTS framework that directly\nmodels continuous audio representations. More specifically, CLEAR introduces an\nenhanced variational autoencoder with shortcut connections, which achieves a\nhigh compression ratio to map waveforms into compact continuous latents. A\nlightweight MLP-based rectified flow head that operates independently for each\nhidden state is presented to model the continuous latent probability\ndistribution, and trained jointly with the AR model within a single-stage\nframework. Experiments show that the proposed zero-shot CLEAR TTS can\nsynthesize high-quality speech with low latency. Compared to state-of-the-art\n(SOTA) TTS models, CLEAR delivers competitive performance in robustness,\nspeaker similarity and naturalness, while offering a lower real-time factor\n(RTF). In particular, CLEAR achieves SOTA results on the LibriSpeech test-clean\ndataset, with a word error rate of 1.88\\% and an RTF of 0.29. Moreover, CLEAR\nfacilitates streaming speech synthesis with a first-frame delay of 96ms, while\nmaintaining high-quality speech synthesis."}
{"id": "2508.19010", "categories": ["eess.SP", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19010", "abs": "https://arxiv.org/abs/2508.19010", "authors": ["Poorya Mollahosseini", "Yasaman Ghasempour"], "title": "mmKey: Channel-Aware Beam Shaping for Reliable Key Generation in mmWave Wireless Networks", "comment": null, "summary": "Physical-layer key generation (PLKG) has emerged as a promising technique to\nsecure next-generation wireless networks by exploiting the inherent properties\nof the wireless channel. However, PLKG faces fundamental challenges in the\nmillimeter wave (mmWave) regime due to channel sparsity, higher phase noise,\nand higher path loss, which undermine both the randomness and reciprocity\nrequired for secure key generation. In this paper, we present mmKey, a novel\nPLKG framework that capitalizes on the availability of multiple antennas at\nmmWave wireless nodes to inject randomness into an otherwise quasi-static\nwireless channel. Different from prior works that sacrifice either the secrecy\nof the key generation or the robustness, mmKey balances these two requirements.\nIn particular, mmKey leverages a genetic algorithm to gradually evolve the\ninitial weight vector population toward configurations that suppress the LOS\ncomponent while taking into account the channel conditions, specifically, the\nsparsity and the signal-to-noise ratio (SNR). Extensive simulations show that\nmmKey improves the secrecy gap by an average of 39.4% over random beamforming\nand 34.0% over null beamforming, outperforming conventional schemes."}
{"id": "2508.19180", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19180", "abs": "https://arxiv.org/abs/2508.19180", "authors": ["Yibo Bai", "Sizhou Chen", "Michele Panariello", "Xiao-Lei Zhang", "Massimiliano Todisco", "Nicholas Evans"], "title": "MDD: a Mask Diffusion Detector to Protect Speaker Verification Systems from Adversarial Perturbations", "comment": "Accepted by APSIPA ASC 2025", "summary": "Speaker verification systems are increasingly deployed in security-sensitive\napplications but remain highly vulnerable to adversarial perturbations. In this\nwork, we propose the Mask Diffusion Detector (MDD), a novel adversarial\ndetection and purification framework based on a \\textit{text-conditioned masked\ndiffusion model}. During training, MDD applies partial masking to\nMel-spectrograms and progressively adds noise through a forward diffusion\nprocess, simulating the degradation of clean speech features. A reverse process\nthen reconstructs the clean representation conditioned on the input\ntranscription. Unlike prior approaches, MDD does not require adversarial\nexamples or large-scale pretraining. Experimental results show that MDD\nachieves strong adversarial detection performance and outperforms prior\nstate-of-the-art methods, including both diffusion-based and neural codec-based\napproaches. Furthermore, MDD effectively purifies adversarially-manipulated\nspeech, restoring speaker verification performance to levels close to those\nobserved under clean conditions. These findings demonstrate the potential of\ndiffusion-based masking strategies for secure and reliable speaker verification\nsystems."}
{"id": "2508.19034", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19034", "abs": "https://arxiv.org/abs/2508.19034", "authors": ["Poorya Mollahosseini", "Yasaman Ghasempour"], "title": "Fast Vortex Beam Alignment for OAM Mode Multiplexing in LOS MIMO Networks", "comment": "13 pages, 12 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Orbital Angular Momentum (OAM)-based communication systems offer\nhigh-capacity multiplexing in line-of-sight (LOS) scenarios; yet, their\nperformance is sensitive to nodal misalignment, which disrupts modal\northogonality, hindering the data multiplexing gain. To tackle this challenge,\nwe present OrthoVortex, a novel framework that estimates the misalignment\nangles and applies the appropriate phase correction to restore orthogonality\nbetween modes. Unlike purely theoretical prior efforts that rely on impractical\nfully digital arrays or exhaustive beam scans, OrthoVortex introduces and\nleverages the cross-modal phase, as a unique signature for identifying the\nmisalignment angles. OrthoVortex is a few-shot alignment technique, making it\nfeasible for real-world implementations. Our key contributions include: (i) a\nrobust angle estimation and phase correction framework based on the physics of\nOAM propagation that estimates the misalignment and restores modal\northogonality, (ii) the first-ever experimental validation of OAM beam\nalignment with RF transceivers, and (iii) a comprehensive analysis of practical\nconstraints, including the impact of antenna count and bandwidth. Simulations\nand over-the-air measurements using low-cost, rapidly prototyped metasurfaces\noperating at 120 GHz demonstrate that OrthoVortex achieves fast and precise\nmisalignment estimation (mean absolute error of $0.69^{\\circ}$ for azimuth and\n$2.54^{\\circ}$ for elevation angle). Further, OrthoVortex can mitigate the\ninter-modal interference, yielding more than 12 dB increase in\nsignal-to-interference ratio and more than 4.5-fold improvement in link\ncapacity."}
{"id": "2508.19210", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19210", "abs": "https://arxiv.org/abs/2508.19210", "authors": ["Tianchi Liu", "Ruijie Tao", "Qiongqiong Wang", "Yidi Jiang", "Hardik B. Sailor", "Ke Zhang", "Jingru Lin", "Haizhou Li"], "title": "Interpolating Speaker Identities in Embedding Space for Data Expansion", "comment": "accepted by APSIPA ASC 2025", "summary": "The success of deep learning-based speaker verification systems is largely\nattributed to access to large-scale and diverse speaker identity data. However,\ncollecting data from more identities is expensive, challenging, and often\nlimited by privacy concerns. To address this limitation, we propose INSIDE\n(Interpolating Speaker Identities in Embedding Space), a novel data expansion\nmethod that synthesizes new speaker identities by interpolating between\nexisting speaker embeddings. Specifically, we select pairs of nearby speaker\nembeddings from a pretrained speaker embedding space and compute intermediate\nembeddings using spherical linear interpolation. These interpolated embeddings\nare then fed to a text-to-speech system to generate corresponding speech\nwaveforms. The resulting data is combined with the original dataset to train\ndownstream models. Experiments show that models trained with INSIDE-expanded\ndata outperform those trained only on real data, achieving 3.06\\% to 5.24\\%\nrelative improvements. While INSIDE is primarily designed for speaker\nverification, we also validate its effectiveness on gender classification,\nwhere it yields a 13.44\\% relative improvement. Moreover, INSIDE is compatible\nwith other augmentation techniques and can serve as a flexible, scalable\naddition to existing training pipelines."}
{"id": "2508.19129", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19129", "abs": "https://arxiv.org/abs/2508.19129", "authors": ["Tayfun Yilmaz", "Haci Ilhan", "Ibrahim Hokelek"], "title": "Space-Time Coded RIS-Assisted Wireless Systems with Practical Reflection Models: Error Rate Analysis and Negative Moment-Based Optimization with Saddle Point Approximation", "comment": "This work has been submitted for consideration in an IEEE journal", "summary": "RIS-assisted communication has recently attracted significant attention for\nenhancing wireless performance in challenging environments, making accurate\nerror analysis under practical hardware constraints crucial for future\nmulti-antenna systems. This paper presents a theoretical framework for SER\nanalysis of RIS-assisted multiple antenna systems employing OSTBC under\npractical reflection models with amplitude-dependent and quantized phase\nresponses. By exploiting the Gramian structure of the cascaded channel f, we\nderive exact MGF expressions of the nonzero eigenvalue of f'f for small RIS\nsizes. For large-scale RIS deployments, where closed-form analysis becomes\nintractable, we employ Saddle Point Approximation to approximate the eigenvalue\ndistribution. Using these results, we derive unified SER expressions using\nexact and SPA-based MGF formulations, applicable to arbitrary RIS sizes, phase\nconfiguration, and both identical and non-identical amplitude responses.\nExtensive Monte Carlo simulations confirm the accuracy of the proposed SER\nexpressions, demonstrating very close agreement for all configurations."}
{"id": "2508.19185", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19185", "abs": "https://arxiv.org/abs/2508.19185", "authors": ["Nishant Mehrotra", "Sandesh Rao Mattu", "Robert Calderbank"], "title": "Instantaneous Polarimetry with Zak-OTFS", "comment": "8 pages, 4 figures, submitted to IEEE Transactions on Radar Systems\n  (Correspondence)", "summary": "Polarimetry, which is the ability to measure the scattering response of the\nenvironment across orthogonal polarizations, is fundamental to enhancing\nwireless communication and radar system performance. In this paper, we utilize\nthe Zak-OTFS modulation to enable instantaneous polarimetry within a single\ntransmission frame. We transmit a Zak-OTFS carrier waveform and a spread\ncarrier waveform mutually unbiased to it simultaneously over orthogonal\npolarizations. The mutual unbiasedness of the two waveforms enables the\nreceiver to estimate the full polarimetric response of the scattering\nenvironment from a single received frame. Unlike existing methods for\ninstantaneous polarimetry with computational complexity quadratic in the\ntime-bandwidth product, the proposed method enables instantaneous polarimetry\nat complexity that is only sublinear in the time-bandwidth product. Via\nnumerical simulations, we show ideal polarimetric target detection and\nparameter estimation results with the proposed method, with improvements in\nperformance and computational complexity over comparable baselines."}

{"id": "2508.19251", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19251", "abs": "https://arxiv.org/abs/2508.19251", "authors": ["Qian Liang", "Menghaoran Tang", "Yi Zeng"], "title": "MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks", "comment": null, "summary": "Symbolic music generation has seen rapid progress with artificial neural\nnetworks, yet remains underexplored in the biologically plausible domain of\nspiking neural networks (SNNs), where both standardized benchmarks and\ncomprehensive evaluation methods are lacking. To address this gap, we introduce\nMuSpike, a unified benchmark and evaluation framework that systematically\nassesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,\nSNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,\nstructural, emotional, and stylistic variations. MuSpike emphasizes\ncomprehensive evaluation, combining established objective metrics with a\nlarge-scale listening study. We propose new subjective metrics, targeting\nmusical impression, autobiographical association, and personal preference, that\ncapture perceptual dimensions often overlooked in prior work. Results reveal\nthat (1) different SNN models exhibit distinct strengths across evaluation\ndimensions; (2) participants with different musical backgrounds exhibit diverse\nperceptual patterns, with experts showing greater tolerance toward AI-composed\nmusic; and (3) a noticeable misalignment exists between objective and\nsubjective evaluations, highlighting the limitations of purely statistical\nmetrics and underscoring the value of human perceptual judgment in assessing\nmusical quality. MuSpike provides the first systematic benchmark and systemic\nevaluation framework for SNN models in symbolic music generation, establishing\na solid foundation for future research into biologically plausible and\ncognitively grounded music generation."}
{"id": "2508.19262", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19262", "abs": "https://arxiv.org/abs/2508.19262", "authors": ["Maximilian Wachter", "Sebastian Murgul", "Michael Heizmann"], "title": "Beat-Based Rhythm Quantization of MIDI Performances", "comment": "Accepted to the Late Breaking Demo Papers of the 1st AES\n  International Conference on Artificial Intelligence and Machine Learning for\n  Audio (AIMLA LBDP), 2025", "summary": "We propose a transformer-based rhythm quantization model that incorporates\nbeat and downbeat information to quantize MIDI performances into\nmetrically-aligned, human-readable scores. We propose a beat-based\npreprocessing method that transfers score and performance data into a unified\ntoken representation. We optimize our model architecture and data\nrepresentation and train on piano and guitar performances. Our model exceeds\nstate-of-the-art performance based on the MUSTER metric."}
{"id": "2508.19308", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19308", "abs": "https://arxiv.org/abs/2508.19308", "authors": ["Haolin Yu", "Yanxiong Li"], "title": "Infant Cry Detection In Noisy Environment Using Blueprint Separable Convolutions and Time-Frequency Recurrent Neural Network", "comment": null, "summary": "Infant cry detection is a crucial component of baby care system. In this\npaper, we propose a lightweight and robust method for infant cry detection. The\nmethod leverages blueprint separable convolutions to reduce computational\ncomplexity, and a time-frequency recurrent neural network for adaptive\ndenoising. The overall framework of the method is structured as a multi-scale\nconvolutional recurrent neural network, which is enhanced by efficient spatial\nattention mechanism and contrast-aware channel attention module, and acquire\nlocal and global information from the input feature of log Mel-spectrogram.\nMultiple public datasets are adopted to create a diverse and representative\ndataset, and environmental corruption techniques are used to generate the noisy\nsamples encountered in real-world scenarios. Results show that our method\nexceeds many state-of-the-art methods in accuracy, F1-score, and complexity\nunder various signal-to-noise ratio conditions. The code is at\nhttps://github.com/fhfjsd1/ICD_MMSP."}
{"id": "2508.19514", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19514", "abs": "https://arxiv.org/abs/2508.19514", "authors": ["Zhihao Ouyang", "Ju-Chiang Wang", "Daiyu Zhang", "Bin Chen", "Shangjie Li", "Quan Lin"], "title": "MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models", "comment": null, "summary": "Question-answering (QA) is a natural approach for humans to understand a\npiece of music audio. However, for machines, accessing a large-scale dataset\ncovering diverse aspects of music is crucial, yet challenging, due to the\nscarcity of publicly available music data of this type. This paper introduces\nMQAD, a music QA dataset built on the Million Song Dataset (MSD), encompassing\na rich array of musical features, including beat, chord, key, structure,\ninstrument, and genre -- across 270,000 tracks, featuring nearly 3 million\ndiverse questions and captions. MQAD distinguishes itself by offering detailed\ntime-varying musical information such as chords and sections, enabling\nexploration into the inherent structure of music within a song. To compile\nMQAD, our methodology leverages specialized Music Information Retrieval (MIR)\nmodels to extract higher-level musical features and Large Language Models\n(LLMs) to generate natural language QA pairs. Then, we leverage a multimodal\nLLM that integrates the LLaMA2 and Whisper architectures, along with novel\nsubjective metrics to assess the performance of MQAD. In experiments, our model\ntrained on MQAD demonstrates advancements over conventional music audio\ncaptioning approaches. The dataset and code are available at\nhttps://github.com/oyzh888/MQAD."}
{"id": "2508.19390", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19390", "abs": "https://arxiv.org/abs/2508.19390", "authors": ["Jana Weber", "Marcel Weber", "Juan Miguel Lopez Alcaraz"], "title": "Depression diagnosis from patient interviews using multimodal machine learning", "comment": "15 pages, 4 figures, source code under\n  https://github.com/UOLMDA2025/Depression", "summary": "Background: Depression is a major public health concern, affecting an\nestimated five percent of the global population. Early and accurate diagnosis\nis essential to initiate effective treatment, yet recognition remains\nchallenging in many clinical contexts. Speech, language, and behavioral cues\ncollected during patient interviews may provide objective markers that support\nclinical assessment.\n  Methods: We developed a diagnostic approach that integrates features derived\nfrom patient interviews, including speech patterns, linguistic characteristics,\nand structured clinical information. Separate models were trained for each\nmodality and subsequently combined through multimodal fusion to reflect the\ncomplexity of real-world psychiatric assessment. Model validity was assessed\nwith established performance metrics, and further evaluated using calibration\nand decision-analytic approaches to estimate potential clinical utility.\n  Results: The multimodal model achieved superior diagnostic accuracy compared\nto single-modality models, with an AUROC of 0.88 and an F1-score of 0.75.\nImportantly, the fused model demonstrated good calibration and offered higher\nnet clinical benefit compared to baseline strategies, highlighting its\npotential to assist clinicians in identifying patients with depression more\nreliably.\n  Conclusion: Multimodal analysis of patient interviews using machine learning\nmay serve as a valuable adjunct to psychiatric evaluation. By combining speech,\nlanguage, and clinical features, this approach provides a robust framework that\ncould enhance early detection of depressive disorders and support\nevidence-based decision-making in mental healthcare."}
{"id": "2508.19483", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19483", "abs": "https://arxiv.org/abs/2508.19483", "authors": ["Nasir Saleem", "Mandar Gogate", "Kia Dashtipour", "Adeel Hussain", "Usman Anwar", "Adewale Adetomi", "Tughrul Arslan", "Amir Hussain"], "title": "Audio-Visual Feature Synchronization for Robust Speech Enhancement in Hearing Aids", "comment": "Preprint of the paper presented at Euronoise 2025 Malaga, Spain", "summary": "Audio-visual feature synchronization for real-time speech enhancement in\nhearing aids represents a progressive approach to improving speech\nintelligibility and user experience, particularly in strong noisy backgrounds.\nThis approach integrates auditory signals with visual cues, utilizing the\ncomplementary description of these modalities to improve speech\nintelligibility. Audio-visual feature synchronization for real-time SE in\nhearing aids can be further optimized using an efficient feature alignment\nmodule. In this study, a lightweight cross-attentional model learns robust\naudio-visual representations by exploiting large-scale data and simple\narchitecture. By incorporating the lightweight cross-attentional model in an\nAVSE framework, the neural system dynamically emphasizes critical features\nacross audio and visual modalities, enabling defined synchronization and\nimproved speech intelligibility. The proposed AVSE model not only ensures high\nperformance in noise suppression and feature alignment but also achieves\nreal-time processing with minimal latency (36ms) and energy consumption.\nEvaluations on the AVSEC3 dataset show the efficiency of the model, achieving\nsignificant gains over baselines in perceptual quality (PESQ:0.52),\nintelligibility (STOI:19\\%), and fidelity (SI-SDR:10.10dB)."}
{"id": "2508.19603", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19603", "abs": "https://arxiv.org/abs/2508.19603", "authors": ["Zhejing Hu", "Yan Liu", "Gong Chen", "Bruce X. B. Yu"], "title": "CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation", "comment": null, "summary": "Generative artificial intelligence in music has made significant strides, yet\nit still falls short of the substantial achievements seen in natural language\nprocessing, primarily due to the limited availability of music data.\nKnowledge-informed approaches have been shown to enhance the performance of\nmusic generation models, even when only a few pieces of musical knowledge are\nintegrated. This paper seeks to leverage comprehensive music theory in\nAI-driven music generation tasks, such as algorithmic composition and style\ntransfer, which traditionally require significant manual effort with existing\ntechniques. We introduce a novel automatic music lexicon construction model\nthat generates a lexicon, named CompLex, comprising 37,432 items derived from\njust 9 manually input category keywords and 5 sentence prompt templates. A new\nmulti-agent algorithm is proposed to automatically detect and mitigate\nhallucinations. CompLex demonstrates impressive performance improvements across\nthree state-of-the-art text-to-music generation models, encompassing both\nsymbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of\ncompleteness, accuracy, non-redundancy, and executability, confirming that it\npossesses the key characteristics of an effective lexicon."}
{"id": "2508.19408", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19408", "abs": "https://arxiv.org/abs/2508.19408", "authors": ["Vaclav Pavlicek", "Ayush Bhandari"], "title": "1-Bit Unlimited Sampling Beyond Fourier Domain: Low-Resolution Sampling of Quantization Noise", "comment": "20 pages, accepted to IEEE Journal of Selected Topics in Signal\n  Processing", "summary": "Analog-to-digital converters (ADCs) play a critical role in digital signal\nacquisition across various applications, but their performance is inherently\nconstrained by sampling rates and bit budgets. This bit budget imposes a\ntrade-off between dynamic range (DR) and digital resolution, with ADC energy\nconsumption scaling linearly with sampling rate and exponentially with bit\ndepth. To bypass this, numerous approaches, including oversampling with\nlow-resolution ADCs, have been explored. A prominent example is 1-Bit ADCs with\nSigma-Delta Quantization (SDQ), a widely used consumer-grade solution. However,\nSDQs suffer from overloading or saturation issues, limiting their ability to\nhandle inputs with arbitrary DR. The Unlimited Sensing Framework (USF)\naddresses this challenge by injecting modulo non-linearity in hardware,\nresulting in a new digital sensing technology. In this paper, we introduce a\nnovel 1-Bit sampling architecture that extends both conventional 1-Bit SDQ and\nUSF. Our contributions are twofold: (1) We generalize the concept of noise\nshaping beyond the Fourier domain, allowing the inclusion of non-bandlimited\nsignals in the Fourier domain but bandlimited in alternative transform domains.\n(2) Building on this generalization, we develop a new transform-domain recovery\nmethod for 1-Bit USF. When applied to the Fourier domain, our method\ndemonstrates superior performance compared to existing time-domain techniques,\noffering reduced oversampling requirements and improved robustness. Extensive\nnumerical experiments validate our findings, laying the groundwork for a\nbroader generalization of 1-Bit sampling systems."}
{"id": "2508.19528", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19528", "abs": "https://arxiv.org/abs/2508.19528", "authors": ["Haoxu Wang", "Yiheng Jiang", "Gang Qiao", "Pengteng Shi", "Biao Tian"], "title": "FLASepformer: Efficient Speech Separation with Gated Focused Linear Attention Transformer", "comment": "Accepted by Interspeech 2025", "summary": "Speech separation always faces the challenge of handling prolonged time\nsequences. Past methods try to reduce sequence lengths and use the Transformer\nto capture global information. However, due to the quadratic time complexity of\nthe attention module, memory usage and inference time still increase\nsignificantly with longer segments. To tackle this, we introduce Focused Linear\nAttention and build FLASepformer with linear complexity for efficient speech\nseparation. Inspired by SepReformer and TF-Locoformer, we have two variants:\nFLA-SepReformer and FLA-TFLocoformer. We also add a new Gated module to improve\nperformance further. Experimental results on various datasets show that\nFLASepformer matches state-of-the-art performance with less memory consumption\nand faster inference. FLA-SepReformer-T/B/L increases speed by 2.29x, 1.91x,\nand 1.49x, with 15.8%, 20.9%, and 31.9% GPU memory usage, proving our model's\neffectiveness."}
{"id": "2508.19876", "categories": ["cs.SD", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.19876", "abs": "https://arxiv.org/abs/2508.19876", "authors": ["Sepideh Shafiei", "Shapour Hakam"], "title": "The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music", "comment": null, "summary": "We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level,\nopen-access corpus designed for the computational study of Iranian classical\nmusic, with a particular emphasis on the radif, a structured repertoire of\nmodal-melodic units central to pedagogy and performance. The dataset combines\nsymbolic MIDI representations, phrase-level audio-MIDI alignment, musicological\ntranscriptions in PDF format, and comparative tables of theoretical information\ncurated from a range of performers and scholars. We outline the multi-phase\nconstruction process, including segment annotation, alignment methods, and a\nstructured system of identifier codes to reference individual musical units.\nThe current release includes the complete radif of Karimi; MIDI files and\nmetadata from Mirza Abdollah's radif; selected segments from the vocal radif of\nDavami, as transcribed by Payvar and Fereyduni; and a dedicated section\nfeaturing audio-MIDI examples of tahrir ornamentation performed by prominent\n20th-century vocalists. While the symbolic and analytical components are\nreleased under an open-access license (CC BY-NC 4.0), some referenced audio\nrecordings and third-party transcriptions are cited using discographic\ninformation to enable users to locate the original materials independently,\npending copyright permission. Serving both as a scholarly archive and a\nresource for computational analysis, this dataset supports applications in\nethnomusicology, pedagogy, symbolic audio research, cultural heritage\npreservation, and AI-driven tasks such as automatic transcription and music\ngeneration. We welcome collaboration and feedback to support its ongoing\nrefinement and broader integration into musicological and machine learning\nworkflows."}
{"id": "2508.19439", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19439", "abs": "https://arxiv.org/abs/2508.19439", "authors": ["Jorge L. Gonzalez-Rios", "Eva Lagunas", "Hayder Al-Hraishawi", "Luis M. Garces-Socarras", "Symeon Chatzinotas"], "title": "In-Lab Carrier Aggregation Testbed for Satellite Communication Systems", "comment": null, "summary": "Carrier Aggregation (CA) is a technique used in 5G and previous cellular\ngenerations to temporarily increase the data rate of a specific user during\npeak demand periods or to reduce carrier congestion. CA is achieved by\ncombining two or more carriers and providing a virtual, wider overall bandwidth\nto high-demand users of the system. CA was introduced in the 4G/LTE wireless\nera and has been proven effective in 5G as well, where it is said to play a\nsignificant role in efficient network capacity management. Given this success,\nthe satellite communication (SatCom) community has put its attention into CA\nand the potential benefits it can bring in terms of better spectrum utilization\nand better meeting the user traffic demand. While the theoretical evaluation of\nCA for SatCom has already been presented in several works, this article\npresents the design and results obtained with an experimentation testbed based\non Software Defined Radio (SDR) and a satellite channel emulator. We first\npresent the detailed implementation design, which includes a Gateway (GW)\nmodule responsible for PDU-scheduling across the aggregated carriers, and a\nUser Terminal (UT) module responsible for aggregating the multiple received\nstreams. The second part of the article presents the experimental evaluation,\nincluding CA over a single Geostationary (GEO) satellite, CA over a single\nMedium Earth Orbit (MEO) satellite, and CA combining carriers sent over GEO and\nMEO satellites. A key contribution of this work is the explicit consideration\nof multi-orbit scenarios in the testbed design and validation. The testing\nresults show promising benefits of CA over SatCom systems, motivating potential\nupcoming testing on over-the-air systems."}
{"id": "2508.19583", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19583", "abs": "https://arxiv.org/abs/2508.19583", "authors": ["Ziling Huang", "Junnan Wu", "Lichun Fan", "Zhenbo Luo", "Jian Luan", "Haixin Guan", "Yanhua Long"], "title": "Lightweight speech enhancement guided target speech extraction in noisy multi-speaker scenarios", "comment": "This paper has been submitted to ICASSP 2026. Copyright 2026 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses, including reprinting/republishing, creating new\n  collective works, for resale or redistribution to servers or lists, or reuse\n  of any copyrighted component of this work. DOI will be added upon IEEE Xplore\n  publication", "summary": "Target speech extraction (TSE) has achieved strong performance in relatively\nsimple conditions such as one-speaker-plus-noise and two-speaker mixtures, but\nits performance remains unsatisfactory in noisy multi-speaker scenarios. To\naddress this issue, we introduce a lightweight speech enhancement model, GTCRN,\nto better guide TSE in noisy environments. Building on our competitive previous\nspeaker embedding/encoder-free framework SEF-PNet, we propose two extensions:\nLGTSE and D-LGTSE. LGTSE incorporates noise-agnostic enrollment guidance by\ndenoising the input noisy speech before context interaction with enrollment\nspeech, thereby reducing noise interference. D-LGTSE further improves system\nrobustness against speech distortion by leveraging denoised speech as an\nadditional noisy input during training, expanding the dynamic range of noisy\nconditions and enabling the model to directly learn from distorted signals.\nFurthermore, we propose a two-stage training strategy, first with GTCRN\nenhancement-guided pre-training and then joint fine-tuning, to fully exploit\nmodel potential.Experiments on the Libri2Mix dataset demonstrate significant\nimprovements of 0.89 dB in SISDR, 0.16 in PESQ, and 1.97% in STOI, validating\nthe effectiveness of our approach. Our code is publicly available at\nhttps://github.com/isHuangZiling/D-LGTSE."}
{"id": "2508.19528", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.19528", "abs": "https://arxiv.org/abs/2508.19528", "authors": ["Haoxu Wang", "Yiheng Jiang", "Gang Qiao", "Pengteng Shi", "Biao Tian"], "title": "FLASepformer: Efficient Speech Separation with Gated Focused Linear Attention Transformer", "comment": "Accepted by Interspeech 2025", "summary": "Speech separation always faces the challenge of handling prolonged time\nsequences. Past methods try to reduce sequence lengths and use the Transformer\nto capture global information. However, due to the quadratic time complexity of\nthe attention module, memory usage and inference time still increase\nsignificantly with longer segments. To tackle this, we introduce Focused Linear\nAttention and build FLASepformer with linear complexity for efficient speech\nseparation. Inspired by SepReformer and TF-Locoformer, we have two variants:\nFLA-SepReformer and FLA-TFLocoformer. We also add a new Gated module to improve\nperformance further. Experimental results on various datasets show that\nFLASepformer matches state-of-the-art performance with less memory consumption\nand faster inference. FLA-SepReformer-T/B/L increases speed by 2.29x, 1.91x,\nand 1.49x, with 15.8%, 20.9%, and 31.9% GPU memory usage, proving our model's\neffectiveness."}
{"id": "2508.19522", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19522", "abs": "https://arxiv.org/abs/2508.19522", "authors": ["Si Wang", "Guoqiang Xiao"], "title": "Fourth-Order Hierarchical Array: A Novel Scheme for Sparse Array Design Based on Fourth-Order Difference Co-Array", "comment": "Sparse linear array, fourth-order cumulant, mutual coupling,\n  redundancy, direction of arrival estimation", "summary": "Conventional array designs based on circular fourth-order cumulant typically\nadopt a single expression form of the fourth-order difference co-array (FODCA),\nwhich limits the achievable degrees of freedom (DOFs) and neglects the impact\nof mutual coupling among physical sensors. To address above issues, this paper\nproposes a novel scheme to design arrays with increased DOFs by combining\ndifferent forms of FODCA while accounting for mutual coupling. A novel\nfourth-order hierarchical array (FOHA) based on different forms of FODCA is\nconstructed using an arbitrary generator set. The analytical expression between\nthe coupling leakage of the generator and the resulting FOHA is derived. Two\nspecific FOHA configurations are presented with closed-form sensor placements.\nThe arrays not only offer increased DOFs for resolving more sources in\ndirection of-arrival (DOA) estimation but also effectively suppress mutual\ncoupling. Moreover, the redundancy of FODCA is examined, and it is shown that\narrays based on the proposed scheme achieve lower redundancy compared to\nexisting arrays based on FODCA. Meanwhile, the necessary and sufficient\nconditions for signal reconstruction by FOHA are derived. Compared with\nexisting arrays based on FODCA, the proposed arrays provide enhanced DOFs and\nimproved robustness against mutual coupling. Numerical simulations verify that\nFOHAs achieve superior DOA estimation performance compared with other sparse\nlinear arrays."}
{"id": "2508.19671", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19671", "abs": "https://arxiv.org/abs/2508.19671", "authors": ["Yunkyu Lim", "Jihwan Park", "Hyung Yong Kim", "Hanbin Lee", "Byeong-Yeol Kim"], "title": "Hybrid Decoding: Rapid Pass and Selective Detailed Correction for Sequence Models", "comment": "Accepted to ASRU 2025", "summary": "Recently, Transformer-based encoder-decoder models have demonstrated strong\nperformance in multilingual speech recognition. However, the decoder's\nautoregressive nature and large size introduce significant bottlenecks during\ninference. Additionally, although rare, repetition can occur and negatively\naffect recognition accuracy. To tackle these challenges, we propose a novel\nHybrid Decoding approach that both accelerates inference and alleviates the\nissue of repetition. Our method extends the transformer encoder-decoder\narchitecture by attaching a lightweight, fast decoder to the pretrained\nencoder. During inference, the fast decoder rapidly generates an output, which\nis then verified and, if necessary, selectively corrected by the Transformer\ndecoder. This results in faster decoding and improved robustness against\nrepetitive errors. Experiments on the LibriSpeech and GigaSpeech test sets\nindicate that, with fine-tuning limited to the added decoder, our method\nachieves word error rates comparable to or better than the baseline, while more\nthan doubling the inference speed."}
{"id": "2508.19540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19540", "abs": "https://arxiv.org/abs/2508.19540", "authors": ["Haochen Li", "Ruikang Zhong", "Jiayi Lei", "Yuanwei Liu"], "title": "Pinching Antenna System for Integrated Sensing and Communications", "comment": "13 pages, 8 figures", "summary": "Recently, the pinching antenna system (PASS) has attracted considerable\nattention due to their advantages in flexible deployment and reduction of\nsignal propagation loss. In this work, a multiple waveguide PASS assisted\nintegrated sensing and communication (ISAC) system is proposed, where the base\nstation (BS) is equipped with transmitting pinching antennas (PAs) and\nreceiving uniform linear array (ULA) antennas. The full-duplex (FD) BS\ntransmits the communication and sensing signals through the PAs on waveguides\nand collects the echo sensing signals with the mounted ULA. Based on this\nconfiguration, a target sensing Cramer Rao Bound (CRB) minimization problem is\nformulated under communication quality-of-service (QoS) constraints, power\nbudget constraint, and PA deployment constraints. The alternating optimization\n(AO) method is employed to address the formulated non-convex optimization\nproblem. In each iteration, the overall optimization problem is decomposed into\na digital beamforming sub-problem and a pinching beamforming sub-problem. The\nsensing covariance matrix and communication beamforming matrix at the BS are\noptimized by solving the digital beamforming sub-problem with semidefinite\nrelaxation (SDR). The PA deployment is updated by solving the pinching\nbeamforming sub-problem with the successive convex approximation (SCA) method,\npenalty method, and element-wise optimization. Simulation results show that the\nproposed PASS assisted ISAC framework achieves superior performance over\nbenchmark schemes, is less affected by stringent communication constraints\ncompared to conventional MIMO-ISAC, and benefits further from increasing the\nnumber of waveguides and PAs per waveguide."}
{"id": "2508.19691", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19691", "abs": "https://arxiv.org/abs/2508.19691", "authors": ["Nikolaos Stefanakis", "Marinos Kalaitzakis", "Andreas Symiakakis", "Stefanos Papadakis", "Despoina Pavlidi"], "title": "CAVEMOVE: An Acoustic Database for the Study of Voice-enabled Technologies inside Moving Vehicles", "comment": null, "summary": "In this paper, we present an acoustic database, designed to drive and support\nresearch on voiced enabled technologies inside moving vehicles. The recording\nprocess involves (i) recordings of acoustic impulse responses, acquired under\nstatic conditions to provide the means for modeling the speech and car-audio\ncomponents (ii) recordings of acoustic noise at a wide range of static and\nin-motion conditions. Data are recorded with two different microphone\nconfigurations, particularly (i) a compact microphone array and (ii) a\ndistributed microphone setup. We briefly describe the conditions under which\nthe recordings were acquired, and we provide insight into a Python API that we\ndesigned to support the research and development of voice-enabled technologies\ninside moving vehicles. The first version of this Python API and part of the\ndescribed dataset are available for free download."}
{"id": "2508.19552", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19552", "abs": "https://arxiv.org/abs/2508.19552", "authors": ["Shuo Chang", "Rui Sun", "Jiashuo He", "Sai Huang", "Kan Yu", "Zhiyong Feng"], "title": "CSRD2025: A Large-Scale Synthetic Radio Dataset for Spectrum Sensing in Wireless Communications", "comment": null, "summary": "The development of Large AI Models (LAMs) for wireless communications,\nparticularly for complex tasks like spectrum sensing, is critically dependent\non the availability of vast, diverse, and realistic datasets. Addressing this\nneed, this paper introduces the ChangShuoRadioData (CSRD) framework, an\nopen-source, modular simulation platform designed for generating large-scale\nsynthetic radio frequency (RF) data. CSRD simulates the end-to-end transmission\nand reception process, incorporating an extensive range of modulation schemes\n(100 types, including analog, digital, OFDM, and OTFS), configurable channel\nmodels featuring both statistical fading and site-specific ray tracing using\nOpenStreetMap data, and detailed modeling of realistic RF front-end impairments\nfor various antenna configurations (SISO/MISO/MIMO). Using this framework, we\ncharacterize CSRD2025, a substantial dataset benchmark comprising over\n25,000,000 frames (approx. 200TB), which is approximately 10,000 times larger\nthan the widely used RML2018 dataset. CSRD2025 offers unprecedented signal\ndiversity and complexity, specifically engineered to bridge the Sim2Real gap.\nFurthermore, we provide processing pipelines to convert IQ data into\nspectrograms annotated in COCO format, facilitating object detection approaches\nfor time-frequency signal analysis. The dataset specification includes\nstandardized 8:1:1 training, validation, and test splits (via frame indices) to\nensure reproducible research. The CSRD framework is released at\nhttps://github.com/Singingkettle/ChangShuoRadioData to accelerate the\nadvancement of AI-driven spectrum sensing and management."}
{"id": "2508.19251", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19251", "abs": "https://arxiv.org/abs/2508.19251", "authors": ["Qian Liang", "Menghaoran Tang", "Yi Zeng"], "title": "MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks", "comment": null, "summary": "Symbolic music generation has seen rapid progress with artificial neural\nnetworks, yet remains underexplored in the biologically plausible domain of\nspiking neural networks (SNNs), where both standardized benchmarks and\ncomprehensive evaluation methods are lacking. To address this gap, we introduce\nMuSpike, a unified benchmark and evaluation framework that systematically\nassesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,\nSNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,\nstructural, emotional, and stylistic variations. MuSpike emphasizes\ncomprehensive evaluation, combining established objective metrics with a\nlarge-scale listening study. We propose new subjective metrics, targeting\nmusical impression, autobiographical association, and personal preference, that\ncapture perceptual dimensions often overlooked in prior work. Results reveal\nthat (1) different SNN models exhibit distinct strengths across evaluation\ndimensions; (2) participants with different musical backgrounds exhibit diverse\nperceptual patterns, with experts showing greater tolerance toward AI-composed\nmusic; and (3) a noticeable misalignment exists between objective and\nsubjective evaluations, highlighting the limitations of purely statistical\nmetrics and underscoring the value of human perceptual judgment in assessing\nmusical quality. MuSpike provides the first systematic benchmark and systemic\nevaluation framework for SNN models in symbolic music generation, establishing\na solid foundation for future research into biologically plausible and\ncognitively grounded music generation."}
{"id": "2508.19566", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19566", "abs": "https://arxiv.org/abs/2508.19566", "authors": ["Chen Shang", "Jiadong Yu", "Dinh Thai Hoang"], "title": "Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks", "comment": "6 pages, 4 figures, conference paper", "summary": "This work proposes an energy-efficient, learning-based beamforming scheme for\nintegrated sensing and communication (ISAC)-enabled V2X networks. Specifically,\nwe first model the dynamic and uncertain nature of V2X environments as a Markov\nDecision Process. This formulation allows the roadside unit to generate\nbeamforming decisions based solely on current sensing information, thereby\neliminating the need for frequent pilot transmissions and extensive channel\nstate information acquisition. We then develop a deep reinforcement learning\n(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring\nboth communication throughput and sensing accuracy in highly dynamic scenario.\nTo address the high energy demands of conventional learning-based schemes, we\nembed spiking neural networks (SNNs) into the DRL framework. Leveraging their\nevent-driven and sparsely activated architecture, SNNs significantly enhance\nenergy efficiency while maintaining robust performance. Simulation results\nconfirm that the proposed method achieves substantial energy savings and\nsuperior communication performance, demonstrating its potential to support\ngreen and sustainable connectivity in future V2X systems."}
{"id": "2508.19262", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19262", "abs": "https://arxiv.org/abs/2508.19262", "authors": ["Maximilian Wachter", "Sebastian Murgul", "Michael Heizmann"], "title": "Beat-Based Rhythm Quantization of MIDI Performances", "comment": "Accepted to the Late Breaking Demo Papers of the 1st AES\n  International Conference on Artificial Intelligence and Machine Learning for\n  Audio (AIMLA LBDP), 2025", "summary": "We propose a transformer-based rhythm quantization model that incorporates\nbeat and downbeat information to quantize MIDI performances into\nmetrically-aligned, human-readable scores. We propose a beat-based\npreprocessing method that transfers score and performance data into a unified\ntoken representation. We optimize our model architecture and data\nrepresentation and train on piano and guitar performances. Our model exceeds\nstate-of-the-art performance based on the MUSTER metric."}
{"id": "2508.19631", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19631", "abs": "https://arxiv.org/abs/2508.19631", "authors": ["Yubeen Jo", "Geon Choi", "Yongjune Kim", "Namyoon Lee"], "title": "Code-Weight Sphere Decoding", "comment": "5 pages, 6 figures", "summary": "Ultra-reliable low-latency communications (URLLC) demand high-performance\nerror-correcting codes and decoders in the finite blocklength regime. This\nletter introduces a novel two-stage near-maximum likelihood (near-ML) decoding\nframework applicable to any linear block code. Our approach first employs a\nlow-complexity initial decoder. If this initial stage fails a cyclic redundancy\ncheck, it triggers a second stage: the proposed code-weight sphere decoding\n(WSD). WSD iteratively refines the codeword estimate by exploring a localized\nsphere of candidates constructed from pre-computed low-weight codewords. This\nstrategy adaptively minimizes computational overhead at high signal-to-noise\nratios while achieving near-ML performance, especially for low-rate codes.\nExtensive simulations demonstrate that our two-stage decoder provides an\nexcellent trade-off between decoding reliability and complexity, establishing\nit as a promising solution for next-generation URLLC systems."}
{"id": "2508.19637", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19637", "abs": "https://arxiv.org/abs/2508.19637", "authors": ["Maha Shatta", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Georgios Panagopoulos", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge", "comment": "Accepted at 2025 International Conference on Computer-Aided Design\n  (ICCAD)", "summary": "Flexible Electronics (FE) offer a promising alternative to rigid\nsilicon-based hardware for wearable healthcare devices, enabling lightweight,\nconformable, and low-cost systems. However, their limited integration density\nand large feature sizes impose strict area and power constraints, making\nML-based healthcare systems-integrating analog frontend, feature extraction and\nclassifier-particularly challenging. Existing FE solutions often neglect\npotential system-wide solutions and focus on the classifier, overlooking the\nsubstantial hardware cost of feature extraction and Analog-to-Digital\nConverters (ADCs)-both major contributors to area and power consumption. In\nthis work, we present a holistic mixed-signal feature-to-classifier co-design\nframework for flexible smart wearable systems. To the best of our knowledge, we\ndesign the first analog feature extractors in FE, significantly reducing\nfeature extraction cost. We further propose an hardware-aware NAS-inspired\nfeature selection strategy within ML training, enabling efficient,\napplication-specific designs. Our evaluation on healthcare benchmarks shows our\napproach delivers highly accurate, ultra-area-efficient flexible systems-ideal\nfor disposable, low-power wearable monitoring."}
{"id": "2508.19657", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19657", "abs": "https://arxiv.org/abs/2508.19657", "authors": ["Jorge L. González-Rios", "Liz Martínez Marrero", "Juan Duncan", "Luis M. Garcés-Socarrás", "Raudel Cuiman Marquez", "Juan A. Vásquez Peralvo", "Jevgenij Krivochiza", "Symeon Chatzinotas", "Björn Ottersten"], "title": "Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites", "comment": null, "summary": "The use of communication satellites in medium Earth orbit (MEO) is foreseen\nto provide quasi-global broadband Internet connectivity in the coming\nnetworking ecosystems. Multi-user multiple-input single-output (MU-MISO)\ndigital signal processing techniques, such as precoding, emerge as appealing\ntechnological enablers in the forward link of multi-beam satellite systems\noperating in full frequency reuse (FFR). However, the orbit dynamics of MEO\nsatellites pose additional challenges that must be carefully evaluated and\naddressed. This work presents the design of an in-lab testbed based on\nsoftware-defined radio (SDR) platforms and the corresponding adaptations\nrequired for efficient precoding in a MEO scenario. The setup incorporates a\nprecise orbit model and the radiation pattern of a custom-designed direct\nradiating array (DRA). We analyze the main impairments affecting precoding\nperformance, including Doppler shifts and payload phase noise, and propose a\nsynchronization loop to mitigate these effects. Preliminary experimental\nresults validate the feasibility and effectiveness of the proposed solution."}
{"id": "2508.19660", "categories": ["eess.SP", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19660", "abs": "https://arxiv.org/abs/2508.19660", "authors": ["Vojtech Mrazek", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Zdenek Vasicek", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation", "comment": "Accepted at IEEE Transactions on Circuits and Systems for Artificial\n  Intelligence", "summary": "Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs."}
{"id": "2508.19739", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19739", "abs": "https://arxiv.org/abs/2508.19739", "authors": ["Sebastian Lotter", "Marco Seiter", "Maryam Pirmoradi", "Lukas Brand", "Dagmar Fischer", "Robert Schober"], "title": "MC for Gastroretentive Drug Delivery", "comment": "4 pages, 2 figures, This paper has been submitted to IEEE\n  Transactions on Molecular, Biological, and Multi-Scale Communications as\n  Transactions Letter", "summary": "Recently, bacterial nanocellulose (BNC), a biological material produced by\nnon-pathogenic bacteria that possesses excellent material properties for\nvarious medical applications, has received increased interest as a carrier\nsystem for drug delivery. However, the vast majority of existing studies on\ndrug release from BNC are feasibility studies with modeling and design aspects\nremaining largely unexplored. To narrow this research gap, this paper proposes\na novel model for the drug release from BNC. Specifically, the drug delivery\nsystem considered in this paper consists of a BNC fleece coated with a polymer.\nThe polymer coating is used as an additional diffusion barrier, enabling the\ncontrolled release of an active pharmaceutical ingredient. The proposed\nphysics-based model reflects the geometry of the BNC and incorporates the\nimpact of the polymer coating on the drug release. Hence, it can be useful for\ndesigning BNC-based drug delivery systems in the future. The accuracy of the\nmodel is validated with experimental data obtained in wet lab experiments."}
{"id": "2508.19822", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19822", "abs": "https://arxiv.org/abs/2508.19822", "authors": ["Chunxuan Shi", "Yongzhe Li", "Ran Tao"], "title": "On Minimization/Maximization of the Generalized Multi-Order Complex Quadratic Form With Constant-Modulus Constraints", "comment": "14 pages, 3 figures (16 subfigures)", "summary": "In this paper, we study the generalized problem that minimizes or maximizes a\nmulti-order complex quadratic form with constant-modulus constraints on all\nelements of its optimization variable. Such a mathematical problem is commonly\nencountered in various applications of signal processing. We term it as the\nconstant-modulus multi-order complex quadratic programming (CMCQP) in this\npaper. In general, the CMCQP is non-convex and difficult to solve. Its\nobjective function typically relates to metrics such as signal-to-noise ratio,\nCram\\'er-Rao bound, integrated sidelobe level, etc., and constraints normally\ncorrespond to requirements on similarity to desired aspects,\npeak-to-average-power ratio, or constant-modulus property in practical\nscenarios. In order to find efficient solutions to the CMCQP, we first\nreformulate it into an unconstrained optimization problem with respect to phase\nvalues of the studied variable only. Then, we devise a steepest descent/ascent\nmethod with fast determinations on its optimal step sizes. Specifically, we\nconvert the step-size searching problem into a polynomial form that leads to\nclosed-form solutions of high accuracy, wherein the third-order Taylor\nexpansion of the search function is conducted. Our major contributions also lie\nin investigating the effect of the order and specific form of matrices embedded\nin the CMCQP, for which two representative cases are identified. Examples of\nrelated applications associated with the two cases are also provided for\ncompleteness. The proposed methods are summarized into algorithms, whose\nconvergence speeds are verified to be fast by comprehensive simulations and\ncomparisons to existing methods. The accuracy of our proposed fast step-size\ndetermination is also evaluated."}
{"id": "2508.19910", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19910", "abs": "https://arxiv.org/abs/2508.19910", "authors": ["Sergio Hernandez", "Christophe Peucheret", "Francesco Da Ros", "Darko Zibar"], "title": "Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission", "comment": "10 pages, 10 figures, submitted to journal of lightwave technology", "summary": "Directly modulated lasers (DMLs) are an attractive technology for short-reach\nintensity modulation and direct detection communication systems. However, their\ncomplex nonlinear dynamics make the modeling and optimization of DML-based\nsystems challenging. In this paper, we study the end-to-end optimization of\nDML-based systems based on a data-driven surrogate model trained on\nexperimental data. The end-to-end optimization includes the pulse shaping and\nequalizer filters, the bias current and the modulation radio-frequency (RF)\npower applied to the laser. The performance of the end-to-end optimization\nscheme is tested on the experimental setup and compared to 4 different\nbenchmark schemes based on linear and nonlinear receiver-side equalization. The\nresults show that the proposed end-to-end scheme is able to deliver better\nperformance throughout the studied symbol rates and transmission distances\nwhile employing lower modulation RF power, fewer filter taps and utilizing a\nsmaller signal bandwidth."}
{"id": "2508.19931", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19931", "abs": "https://arxiv.org/abs/2508.19931", "authors": ["Isabella W. G. da Silva", "Zahra Mobini", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Cell-Free Massive MIMO-Based Physical-Layer Authentication", "comment": null, "summary": "In this paper, we exploit the cell-free massive multiple-input\nmultiple-output (CF-mMIMO) architecture to design a physical-layer\nauthentication (PLA) framework that can simultaneously authenticate multiple\ndistributed users across the coverage area. Our proposed scheme remains\neffective even in the presence of active adversaries attempting impersonation\nattacks to disrupt the authentication process. Specifically, we introduce a\ntag-based PLA CFmMIMO system, wherein the access points (APs) first estimate\ntheir channels with the legitimate users during an uplink training phase.\nSubsequently, a unique secret key is generated and securely shared between each\nuser and the APs. We then formulate a hypothesis testing problem and derive a\nclosed-form expression for the probability of detection for each user in the\nnetwork. Numerical results validate the effectiveness of the proposed approach,\ndemonstrating that it maintains a high detection probability even as the number\nof users in the system increases."}
{"id": "2508.19994", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2508.19994", "abs": "https://arxiv.org/abs/2508.19994", "authors": ["Noah Shore"], "title": "The Coherent Multiplex: Scalable Real-Time Wavelet Coherence Architecture", "comment": "Submitted to International Symposium for Signal Processing 2025", "summary": "The Coherent Multiplex is formalized and validated as a scalable, real-time\nsystem for identifying, analyzing, and visualizing coherence among multiple\ntime series. Its architecture comprises a fast spectral similarity layer based\non cosine similarity metrics of Fourier-transformed signals, and a sparse\ntime-frequency layer for wavelet coherence. The system constructs and evolves a\nmultilayer graph representing inter-signal relationships, enabling low-latency\ninference and monitoring. A simulation prototype demonstrates functionality\nacross 8 synthetic channels with a high similarity threshold for further\ncomputation, with additional opportunities for scaling the architecture up to\nsupport thousands of input signals with constrained hardware. Applications\ndiscussed include neuroscience, finance, and biomedical signal analysis."}

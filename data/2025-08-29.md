<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 7]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels](https://arxiv.org/abs/2508.20277)
*Xiaoyan Ma,Shahryar Zehtabi,Taejoon Kim,Christopher G. Brinton*

Main category: eess.SP

TL;DR: 本文研究了基于OFDM的空中联邦学习系统，分析了移动设备高速移动导致的信道估计不完美和模型参数不对齐问题，推导了单轮和多轮全局模型更新的闭式表达式和误差界限。


<details>
  <summary>Details</summary>
Motivation: 移动设备（如无人机）的高移动性导致信道估计不完美，造成模型参数传输不同步和时间变化的上传信道，这些因素共同导致OTA-FL训练过程中的失真问题，而这些问题尚未得到充分研究。

Method: 首先推导了单轮全局模型更新的闭式表达式，然后扩展到多轮全局更新的分析，得到了OTA-FL中累积误差的界限，并通过大量数值模拟验证理论结果。

Result: 通过理论推导得到了信道不完美对全局模型更新的量化影响，建立了多轮更新的累积误差界限，数值模拟结果验证了理论分析的正确性。

Conclusion: 本文成功量化了移动性引起的信道不完美对OTA-FL系统的影响，为理解和管理这类系统中的失真问题提供了理论框架和分析工具。

Abstract: This paper investigates an OFDM-based over-the-air federated learning
(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles
(UAVs), transmit local machine learning (ML) models to a central parameter
server (PS) for global model aggregation. The high mobility of local devices
results in imperfect channel estimation, leading to a misalignment problem,
i.e., the model parameters transmitted from different local devices do not
arrive at the central PS simultaneously. Moreover, the mobility introduces
time-varying uploading channels, which further complicates the aggregation
process. All these factors collectively cause distortions in the OTA-FL
training process which are underexplored. To quantify these effects, we first
derive a closed-form expression for a single-round global model update in terms
of these channel imperfections. We then extend our analysis to capture multiple
rounds of global updates, yielding a bound on the accumulated error in OTA-FL.
We validate our theoretical results via extensive numerical simulations, which
corroborate our derived analysis.

</details>


### [2] [Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design](https://arxiv.org/abs/2508.20531)
*Chaoying Huang,Wen Chen,Qingqing Wu,Xusheng Zhu,Zhendong Li,Ying Wang,Jinhong Yuan*

Main category: eess.SP

TL;DR: 这篇论文提出了一种新颖的双智能反射表面(IRS)辅助的干扰限制同时无线信息与能量传输(SWIPT)系统，采用独立功率分配(PS)技术，通过优化双IRS相位、PS比例和接收波束形成来最大化收获功率。


<details>
  <summary>Details</summary>
Motivation: 为了在干扰限制环境下实现更好的信息传输与能量收集的交换平衡，并考虑到近场和混合场渡进的实际通信场景需求。

Method: 分别建立近场和混合场通道模型，采用交替优化算法解决非凸优化问题：近场情况下使用拉格朗日对偶方法和凸差分解编程，混合场情况下利用通道增益与IRS相位无关的性质转化为凸优化问题。

Result: 数值结果验证了分析正确性，表明所提方案在收获功率方面显著优于其他对照方案，双IRS辅助的独立PS技术带来显著性能提升。

Conclusion: 论文成功开发了一种高效的双IRS辅助SWIPT系统，通过独立PS技术和优化算法，在近场和混合场情况下都能实现优异的信息与能量收集性能，为未来智能反射表面技术在实际应用中提供了有价值的解决方案。

Abstract: This paper proposes a novel dual-intelligent reflecting surface (IRS) aided
interference-limited simultaneous wireless information and power transfer
(SWIPT) system with independent power splitting (PS), where each receiving
antenna applies different PS factors to offer an advantageous trade-off between
the useful information and harvested energy. We separately establish the near-
and hybrid-field channel models for IRS-reflected links to evaluate the
performance gain more precisely and practically. Specifically, we formulate an
optimization problem of maximizing the harvested power by jointly optimizing
dual-IRS phase shifts, independent PS ratio, and receive beamforming vector in
both near- and hybrid-field cases. In the near-field case, the alternating
optimization algorithm is proposed to solve the non-convex problem by applying
the Lagrange duality method and the difference-of-convex (DC) programming. In
the hybrid-field case, we first present an interesting result that the
AP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which
allows the optimization problem to be transformed into a convex one. Then, we
derive the asymptotic performance of the combined channel gains in closed-form
and analyze the characteristics of the dual-IRS. Numerical results validate our
analysis and indicate the performance gains of the proposed scheme that
dual-IRS-aided SWIPT with independent PS over other benchmark schemes.

</details>


### [3] [Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders](https://arxiv.org/abs/2508.20535)
*Annika Stiehl,Nicolas Weeger,Christian Uhl,Dominic Bechtold,Nicole Ille,Stefan Geißelsöder*

Main category: eess.SP

TL;DR: 提出基于深度卷积自编码器(DCAE)的癫痫检测方法，通过同时考虑时域和频域损失来提取EEG信号的低维表征，解决了现有方法在敏感性和误报率之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫检测需要可靠高效的癫痫发作检测方法。现有深度学习方法在保持高敏感性的同时难以维持低误报率，且在EEG输入表征（时域或频域）方面缺乏一致性。

Method: 使用深度卷积自编码器(DCAE)提取EEG信号的低维潜在表征，通过比较基于时域和频域的重构误差来评估模型保留关键特征的能力。训练了多种具有不同时域和频域损失函数的自编码器。

Result: 同时考虑时域和频域损失的DCAE模型获得了最佳重构性能，表明单一表征的深度神经网络可能无法保留相关信号特性。

Conclusion: 该研究揭示了深度学习模型处理EEG数据的方式，并验证了在时域信号输入时频率信息是否被有效捕获，为癫痫自动检测提供了新的思路。

Abstract: Epilepsy is one of the most common neurological disorders. This disease
requires reliable and efficient seizure detection methods.
Electroencephalography (EEG) is the gold standard for seizure monitoring, but
its manual analysis is a time-consuming task that requires expert knowledge. In
addition, there are no well-defined features that allow fully automated
analysis. Existing deep learning-based approaches struggle to achieve high
sensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack
consistency in the optimal EEG input representation, whether in the time or
frequency domain. To address these issues, we propose a Deep Convolutional
Autoencoder (DCAE) to extract low-dimensional latent representations that
preserve essential EEG signal features. The ability of the model to preserve
relevant information was evaluated by comparing reconstruction errors based on
both time series and frequency-domain representations. Several autoencoders
with different loss functions based on time and frequency were trained and
evaluated to determine their effectiveness in reconstructing EEG features. Our
results show that the DCAE model taking both time series and frequency losses
into account achieved the best reconstruction performance. This indicates that
Deep Neural Networks with a single representation might not preserve the
relevant signal properties. This work provides insight into how deep learning
models process EEG data and examines whether frequency information is captured
when time series signals are used as input.

</details>


### [4] [Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis](https://arxiv.org/abs/2508.20602)
*Matthieu Correa,Nicolas Vignais,Isabelle A. Siegler,Maxime Projetti*

Main category: eess.SP

TL;DR: 本研究提出了一种基于完整集成经验模态分解的适配性筛波方法，用于动态条件下肌音动图信号的运动假影除除，效果显著优于传统带通筛波技术。


<details>
  <summary>Details</summary>
Motivation: 肌音动图(MMG)在现场肌肉活动测量中应用前景广阔，但其对运动假影的敏感性限制了实际应用。需要开发更有效的方法来在动态条件下隔离运动假影。

Method: 采用基于完整集成经验模态分解(CEEMDAN)的适配性筛波方法，结合适应性噪声和谱模糊熵来隔离MMG信号中的运动假影。

Result: 与传统带通筛波技术相比，新方法在肩胛肌和站立脊肌的运动重构方面表现更优(R² = 0.907和0.842)，能够在5-20Hz带宽内动态筛除运动假影。

Conclusion: 该创新方法能够动态筛除运动假影，但对于踏步或跑步时胸部和下肢肌肉的加速度MMG信号解释仍需静慎，因为冲击相关加速度仍然存在且需要量化。

Abstract: Mechanomyography (MMG) is a promising tool for measuring muscle activity in
the field but its sensitivity to motion artifacts limits its application. In
this study, we proposed an adaptative filtering method for MMG accelerometers
based on the complete ensemble empirical mode decomposition, with adaptative
noise and spectral fuzzy entropy, to isolate motions artefacts from the MMG
signal in dynamic conditions. We compared our method with the traditional
band-pass filtering technique, demonstrating better results concerning motion
recomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and
0.842). Thus, this innovative method allows the filtering of motion artifacts
dynamically in the 5-20 Hz bandwidth, which is not achievable with traditional
method. However, the interpretation of accelerometric MMG signals from the
trunk and lower-limb muscles during walking or running should be approached
with great caution as impact-related accelerations are still present, though
their exact quantity still needs to be quantified.

</details>


### [5] [Weighted Bayesian Cram$\acute{\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation](https://arxiv.org/abs/2508.20761)
*Yaniv Mazor,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 本文针对混合分辨率系统开发了加权贝叶斯克拉美罗界(WBCRB)，提出了基于区域划分的MSE近似方法，在存在量化误差时能更准确地预测MSE的非单调行为。


<details>
  <summary>Details</summary>
Motivation: 混合分辨率架构在通信和雷达系统中广泛应用以降低硬件成本，但粗量化数据会引入参数估计的非平凡权衡，需要开发更精确的性能界限。

Method: 开发了具有一般权重函数的WBCRB，包括经典BCRB、BFIM逆加权WBCRB和最优权重WBCRB三种特例。提出将估计问题划分为信息区和饱和区，分别应用区域特定的WBCRB近似来获得复合MSE估计。

Result: 在线性高斯正交模型中的仿真表明，WBCRB优于BCRB，BFIM逆加权版本接近最优WBCRB。基于WBCRB的MSE近似更紧致，能准确预测量化误差下MSE的非单调行为。

Conclusion: 所提出的WBCRB界限和MSE近似方法为混合分辨率系统的性能分析提供了更准确的工具，特别是在处理量化误差方面表现出色。

Abstract: Mixed-resolution architectures, combining high-resolution (analog) data with
coarsely quantized (e.g., 1-bit) data, are widely employed in emerging
communication and radar systems to reduce hardware costs and power consumption.
However, the use of coarsely quantized data introduces non-trivial tradeoffs in
parameter estimation tasks. In this paper, we investigate the derivation of
lower bounds for such systems. In particular, we develop the weighted Bayesian
Cramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight
function. We demonstrate the special cases of: (i) the classical BCRB; (ii) the
WBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse
weighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight
function. Based on the developed WBCRB, we propose a new method to approximate
the mean-squared-error (MSE) by partitioning the estimation problem into two
regions: (a) where the 1-bit quantized data is informative; and (b) where it is
saturated. We apply region-specific WBCRB approximations in these regions to
achieve an accurate composite MSE estimate. We derive the bounds and MSE
approximation for the linear Gaussian orthonormal (LGO) model, which is
commonly used in practical signal processing applications. Our simulation
results demonstrate the use of the proposed bounds and approximation method in
the LGO model with a scalar unknown parameter. It is shown that the WBCRB
outperforms the BCRB, where the BFIM-Inverse weighting version approaches the
optimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is
tighter and accurately predicts the non-monotonic behavior of the MSE in the
presence of quantization errors.

</details>


### [6] [Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar](https://arxiv.org/abs/2508.20864)
*Ehsan Sadeghi,Paul Havinga*

Main category: eess.SP

TL;DR: 本文研究毫米波FMCW雷达在多种场景下的生命体征检测，通过改进Prony和MUSIC算法显著提高了非接触式心率呼吸监测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 克服传统传感方法的局限性，通过增强信号处理技术有效捕捉细微生理变化，为非接触式生命体征监测提供更可靠的解决方案。

Method: 引入针对实时心率和呼吸率监测的Prony和MUSIC算法新变体，专门用于抑制噪声和谐波干扰。

Result: MUSIC和Prony算法在心率检测中的平均绝对误差分别为1.8和0.81，呼吸率检测中的平均绝对误差分别为1.01和0.8。

Conclusion: FMCW雷达具有作为医疗环境中可靠、非侵入式连续生命体征监测解决方案的潜力，特别是在传统接触式监测不实用的临床和急救场景中。

Abstract: This paper explores the deployment of mm-wave Frequency Modulated Continuous
Wave (FMCW) radar for vital sign detection across multiple scenarios. We focus
on overcoming the limitations of traditional sensing methods by enhancing
signal processing techniques to capture subtle physiological changes
effectively. Our study introduces novel adaptations of the Prony and MUSIC
algorithms tailored for real-time heart and respiration rate monitoring,
significantly advancing the accuracy and reliability of non-contact vital sign
monitoring using radar technologies. Notably, these algorithms demonstrate a
robust ability to suppress noise and harmonic interference. For instance, the
mean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8
and 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,
respectively. These results underscore the potential of FMCW radar as a
reliable, non-invasive solution for continuous vital sign monitoring in
healthcare settings, particularly in clinical and emergency scenarios where
traditional contact-based monitoring is impractical.

</details>


### [7] [A Correction for the Paper "Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis"](https://arxiv.org/abs/2508.20990)
*Hong-Yan Zhang,Haoting Liu,Rui-Jia Lin,Yu Zhou*

Main category: eess.SP

TL;DR: 本文指出了SGMD方法在轨迹矩阵形式扩展后未同步更新对角平均原理的局限性，并通过回拉定理修复了该bug


<details>
  <summary>Details</summary>
Motivation: SGMD方法虽然扩展了SSA中轨迹矩阵的形式，但未同步更新对角平均原理(DAP)，存在计算时间序列分量时的缺陷

Method: 使用回拉定理(pulling back theorem)来计算轨迹矩阵对应分量的时间序列分量

Result: 修复了SGMD方法中的bug，完善了时间序列分解的计算方法

Conclusion: 通过回拉定理成功解决了SGMD方法中DAP未更新的问题，提高了时间序列分解的准确性和可靠性

Abstract: The symplectic geometry mode decomposition (SGMD) is a powerful method for
decomposing time series, which is based on the diagonal averaging principle
(DAP) inherited from the singular spectrum analysis (SSA). Although the authors
of SGMD method generalized the form of the trajectory matrix in SSA, the DAP is
not updated simultaneously. In this work, we pointed out the limitations of the
SGMD method and fixed the bugs with the pulling back theorem for computing the
given component of time series from the corresponding component of trajectory
matrix.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [8] [Live Vocal Extraction from K-pop Performances](https://arxiv.org/abs/2508.20273)
*Yujin Kim,Richa Namballa,Magdalena Fuentes*

Main category: eess.AS

TL;DR: 自动从K-pop现场表演中提取现场唱音的方法


<details>
  <summary>Details</summary>
Motivation: 受K-pop粉丝文化的启发，开发自动提取现场唱音的技术，以支持现场表演分析和粉丝互动

Method: 结合來源分离、相关性分析和振幅缩放技术，自动移除预录音频和乐器配音

Result: 提出了现场唱音分离的新任务，为该领域的未来研究奠定了基础

Conclusion: 这项领先工作为现场表演音频分析开启了新的研究方向，具有广阔的应用前景

Abstract: K-pop's global success is fueled by its dynamic performances and vibrant fan
engagement. Inspired by K-pop fan culture, we propose a methodology for
automatically extracting live vocals from performances. We use a combination of
source separation, cross-correlation, and amplitude scaling to automatically
remove pre-recorded vocals and instrumentals from a live performance. Our
preliminary work introduces the task of live vocal separation and provides a
foundation for future research in this topic.

</details>


### [9] [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
*Muhammad Shakeel,Yui Sudo,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: 本文提出统一多说话人编码器(UME)，通过共享语音基础编码器联合学习说话人日志、语音分离和多说话人语音识别任务，利用残差加权求和编码有效利用不同语义层次信息，在重叠语音数据上显著提升各项任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的说话人日志、语音分离和多说话人语音识别任务通常是独立处理的，但这些任务在处理重叠语音时存在内在的相互依赖性。通过联合学习可以捕捉这些任务的相互依赖关系，提升整体性能。

Method: 提出统一多说话人编码器(UME)架构，使用共享的语音基础编码器，从多个隐藏层提取表示作为残差加权求和编码(RWSE)，实现不同语义层次信息的有效利用和任务间的自底向上对齐。

Result: 在LibriMix评估集上，UME显著优于单任务基线方法。说话人日志任务在Libri2Mix和Libri3Mix上分别达到1.37%和2.29%的错误率，超越了之前的研究成果。

Conclusion: 联合学习方法能够有效捕捉说话人日志、语音分离和多说话人语音识别任务之间的内在依赖关系，统一的编码器架构在重叠语音处理方面展现出优越性能，为多任务语音处理提供了有效解决方案。

Abstract: This paper presents a unified multi-speaker encoder (UME), a novel
architecture that jointly learns representations for speaker diarization (SD),
speech separation (SS), and multi-speaker automatic speech recognition (ASR)
tasks using a shared speech foundational encoder. We leverage the hidden
representations from multiple layers of UME as a residual weighted-sum encoding
(RWSE) to effectively use information from different semantic levels,
contributing to bottom-up alignment between tasks. This joint training approach
captures the inherent interdependencies among the tasks, enhancing overall
performance on overlapping speech data. Our evaluations demonstrate that UME
substantially improves over the single-task baselines dedicated to SD, SS, and
multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms
the previous studies, achieving diarization error rates of 1.37% and 2.29% on
Libri2Mix and Libri3Mix evaluation sets, respectively.

</details>


### [10] [CodecBench: A Comprehensive Benchmark for Acoustic and Semantic Evaluation](https://arxiv.org/abs/2508.20660)
*Ruifan Deng,Yitian Gong,Qinghui Gao,Luozhijie Jin,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Xipeng Qiu*

Main category: eess.AS

TL;DR: CodecBench是一个全面的音频编解码器评估数据集，用于从声学和语义两个角度评估音频编解码器在四个数据域中的性能，旨在识别当前局限性并推动音频编解码器的发展。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的兴起，音频编解码器在将音频编码为离散标记方面发挥着越来越重要的作用。现有编解码器的评估受限于简单指标和场景，现有基准测试不适合复杂应用场景，限制了在复杂数据集上对声学和语义能力的评估。

Method: 提出了CodecBench评估数据集，涵盖四个数据域，从声学和语义两个角度全面评估音频编解码器性能。

Result: 开发了一个全面的评估基准，能够更好地评估音频编解码器在复杂场景下的性能，代码已开源。

Conclusion: CodecBench基准测试有助于识别当前音频编解码器的局限性，突出未来研究方向，并促进音频编解码器技术的发展。

Abstract: With the rise of multimodal large language models (LLMs), audio codec plays
an increasingly vital role in encoding audio into discrete tokens, enabling
integration of audio into text-based LLMs. Current audio codec captures two
types of information: acoustic and semantic. As audio codec is applied to
diverse scenarios in speech language model , it needs to model increasingly
complex information and adapt to varied contexts, such as scenarios with
multiple speakers, background noise, or richer paralinguistic information.
However, existing codec's own evaluation has been limited by simplistic metrics
and scenarios, and existing benchmarks for audio codec are not designed for
complex application scenarios, which limits the assessment performance on
complex datasets for acoustic and semantic capabilities. We introduce
CodecBench, a comprehensive evaluation dataset to assess audio codec
performance from both acoustic and semantic perspectives across four data
domains. Through this benchmark, we aim to identify current limitations,
highlight future research directions, and foster advances in the development of
audio codec. The codes are available at https://github.com/RayYuki/CodecBench.

</details>


### [11] [Sound event detection with audio-text models and heterogeneous temporal annotations](https://arxiv.org/abs/2508.20703)
*Manu Harju,Annamaria Mesaros*

Main category: eess.AS

TL;DR: 通过使用机器生成的合成文本描述作为补充信息，提出了一种用自由文本指导声音事件检测的新方法，在强标签和弱标签训练情况下都显著提高了系统性能。


<details>
  <summary>Details</summary>
Motivation: 利用基于音频和相关元数据生成的合成文本描述中含有的自然语言信息，作为其他音频任务的输入，以改善声音事件检测系统的性能。

Method: 提出了一种用自由文本指导声音事件检测系统的新方法，使用机器生成的文本描述作为训练中强标签的补充信息，并且研究了仅部分训练数据有强标签、其余只有时间弱标签的情况。

Result: 在50个高度不平衡类别的数据集上，使用合成文本描述后PSDS-1分数从0.223提高到0.277（完全强标签训练），以及从0.166提高到0.218（一半训练数据只有弱标签），都显著超越了传统的CRNN结构。

Conclusion: 合成文本描述可以作为有效的补充信息，在不同的标签条件下都能显著提高声音事件检测系统的性能，为少标签或弱标签情况下的音频任务提供了新的解决方案。

Abstract: Recent advances in generating synthetic captions based on audio and related
metadata allow using the information contained in natural language as input for
other audio tasks. In this paper, we propose a novel method to guide a sound
event detection system with free-form text. We use machine-generated captions
as complementary information to the strong labels for training, and evaluate
the systems using different types of textual inputs. In addition, we study a
scenario where only part of the training data has strong labels, and the rest
of it only has temporally weak labels. Our findings show that synthetic
captions improve the performance in both cases compared to the CRNN
architecture typically used for sound event detection. On a dataset of 50
highly unbalanced classes, the PSDS-1 score increases from 0.223 to 0.277 when
trained with strong labels, and from 0.166 to 0.218 when half of the training
data has only weak labels.

</details>


### [12] [Online incremental learning for audio classification using a pretrained audio model](https://arxiv.org/abs/2508.20732)
*Manjunath Mulimani,Annamaria Mesaros*

Main category: eess.AS

TL;DR: 提出了一种基于预训练音频嵌入的在线增量学习方法，通过在预训练模型和分类器之间插入非线性激活层来扩展嵌入维度并捕获声音类别特征，实现了单次前向传播的在线适应，在音频分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的音频增量学习方法通常需要从头开始训练初始任务模型，并通过多次迭代来适应新任务，这会导致旧任务的遗忘问题。本文旨在利用预训练模型的通用音频嵌入来开发在线增量学习器，以最小化遗忘并实现快速适应。

Method: 在预训练模型的音频嵌入和分类器之间插入一个带有非线性激活函数的层，该层扩展了嵌入维度并有效捕获声音类别的独特特征。该方法通过单次前向传播在线适应任何任务的训练样本。

Result: 在ESC-50数据集上的类增量学习和TAU Urban Acoustic Scenes 2019数据集上的域增量学习中，所提出的方法均优于其他对比方法。

Conclusion: 该方法成功实现了在线增量音频分类，仅需单次前向传播即可适应新任务，同时最小化对旧任务的遗忘，在两种不同的增量学习设置中都表现出优越性能。

Abstract: Incremental learning aims to learn new tasks sequentially without forgetting
the previously learned ones. Most of the existing incremental learning methods
for audio focus on training the model from scratch on the initial task, and the
same model is used to learn upcoming incremental tasks. The model is trained
for several iterations to adapt to each new task, using some specific
approaches to reduce the forgetting of old tasks. In this work, we propose a
method for using generalizable audio embeddings produced by a pre-trained model
to develop an online incremental learner that solves sequential audio
classification tasks over time. Specifically, we inject a layer with a
nonlinear activation function between the pre-trained model's audio embeddings
and the classifier; this layer expands the dimensionality of the embeddings and
effectively captures the distinct characteristics of sound classes. Our method
adapts the model in a single forward pass (online) through the training samples
of any task, with minimal forgetting of old tasks. We demonstrate the
performance of the proposed method in two incremental learning setups: one
class-incremental learning using ESC-50 and one domain-incremental learning of
different cities from the TAU Urban Acoustic Scenes 2019 dataset; for both
cases, the proposed approach outperforms other methods.

</details>


### [13] [A Solution of Ultra Wideband Based High-resolution and Lossless Audio Transmission](https://arxiv.org/abs/2508.20782)
*Fengyun Zhang*

Main category: eess.AS

TL;DR: 本文提出利用超宽带(UWB)技术实现高分辨率无损音频传输方案，解决现有无线音频传输技术在带宽、压缩、延迟和设备兼容性方面的局限性


<details>
  <summary>Details</summary>
Motivation: 现有无线音频传输技术存在数据带宽不足、数据压缩损失、延迟高和设备兼容性差等问题，无法满足高质量实时音频应用的需求

Method: 采用超宽带(UWB)技术，利用其高带宽特性实现高分辨率无损音频传输，同时提供超低延迟和精确位置跟踪能力

Result: UWB技术能够提供足够的带宽支持卓越音质，实现超低延迟传输，解决音视频同步问题，并支持增强现实和虚拟现实应用中的精确定位

Conclusion: UWB技术是无线音频传输的理想解决方案，不仅能实现高分辨率无损音频传输，还能为AR/VR应用提供额外的位置跟踪功能

Abstract: This paper provides an overview of the current challenges in wireless audio
transmission and highlights the limitations of existing technologies regarding
data bandwidth, data compression, latency, and inter-device compatibility. To
address these shortcomings, it proposes a high-resolution, lossless audio
transmission scheme utilizing ultra wideband (UWB) technology. UWB emerges as a
promising solution by offering the necessary bandwidth to enable exceptional
sound quality with ultra-low latency, making it ideal for real-time audio
applications and addressing synchronization concerns in audio-visual use cases.
Additionally, UWB's unique capabilities extend beyond high-resolution audio,
allowing for precise location tracking in augmented and virtual reality
applications.

</details>


### [14] [Leveraging Discriminative Latent Representations for Conditioning GAN-Based Speech Enhancement](https://arxiv.org/abs/2508.20859)
*Shrishti Saha Shetu,Emanuël A. P. Habets,Andreas Brendel*

Main category: eess.AS

TL;DR: 提出DisCoGAN方法，利用判别式语音增强模型的潜在特征作为条件特征，改进GAN在低信噪比场景下的语音增强性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式语音增强方法（GAN和扩散模型）在极低信噪比场景下性能有限，需要解决这一挑战

Method: 使用判别式语音增强模型提取的潜在特征作为通用条件特征，改进基于GAN的语音增强方法

Result: DisCoGAN在低信噪比场景下性能优于基线模型，在高信噪比条件和真实录音中保持竞争力或更优表现

Conclusion: 提出的判别式条件方法有效提升了GAN在低信噪比下的语音增强性能，并通过消融研究验证了各组件贡献

Abstract: Generative speech enhancement methods based on generative adversarial
networks (GANs) and diffusion models have shown promising results in various
speech enhancement tasks. However, their performance in very low
signal-to-noise ratio (SNR) scenarios remains under-explored and limited, as
these conditions pose significant challenges to both discriminative and
generative state-of-the-art methods. To address this, we propose a method that
leverages latent features extracted from discriminative speech enhancement
models as generic conditioning features to improve GAN-based speech
enhancement. The proposed method, referred to as DisCoGAN, demonstrates
performance improvements over baseline models, particularly in low-SNR
scenarios, while also maintaining competitive or superior performance in
high-SNR conditions and on real-world recordings. We also conduct a
comprehensive evaluation of conventional GAN-based architectures, including
GANs trained end-to-end, GANs as a first processing stage, and post-filtering
GANs, as well as discriminative models under low-SNR conditions. We show that
DisCoGAN consistently outperforms existing methods. Finally, we present an
ablation study that investigates the contributions of individual components
within DisCoGAN and analyzes the impact of the discriminative conditioning
method on overall performance.

</details>


### [15] [Automatic Inspection Based on Switch Sounds of Electric Point Machines](https://arxiv.org/abs/2508.20870)
*Ayano Shibata,Toshiki Gunji,Mitsuaki Tsuda,Takashi Endo,Kota Dohi,Tomoya Nishida,Satoko Nomoto*

Main category: eess.AS

TL;DR: JR东日本与日立公司合作开发基于声音信息的道岔转换故障检测系统，通过摄像头和麦克风远程监控设备状态，替代人工巡检


<details>
  <summary>Details</summary>
Motivation: 解决人工设备巡检的人力需求问题，实现预防性维护，降低设备故障导致的停机时间，同时避免使用昂贵的高性能传感器

Method: 在NS电动转辙机中安装摄像头和麦克风，通过分析"转换声音"来检测道岔转换错误，实现远程监控和实时故障检测

Result: 获得了预期的测试结果，能够有效检测设备故障，为实时故障检测提供了可行方案

Conclusion: 基于声音的检测方法能够有效替代人工视觉检查，实现设备检查的自动化，降低维护成本和提高运营效率

Abstract: Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working to
replace human inspections with IoT-based monitoring. The purpose is
Labor-saving required for equipment inspections and provide appropriate
preventive maintenance. As an alternative to visual inspection, it has been
difficult to substitute electrical characteristic monitoring, and the
introduction of new high-performance sensors has been costly. In 2019, we
implemented cameras and microphones in an ``NS'' electric point machines to
reduce downtime from equipment failures, allowing for remote monitoring of
lock-piece conditions. This method for detecting turnout switching errors based
on sound information was proposed, and the expected test results were obtained.
The proposed method will make it possible to detect equipment failures in real
time, thereby reducing the need for visual inspections. This paper presents the
results of our technical studies aimed at automating the inspection of
electronic point machines using sound, specifically focusing on ``switch
sound'' beginning in 2019.

</details>


### [16] [Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System](https://arxiv.org/abs/2508.20983)
*Hashim Ali,Surya Subramani,Lekha Bollinani,Nithin Sai Adupa,Sali El-Loh,Hafiz Malik*

Main category: eess.AS

TL;DR: SAFE挑战赛评估合成语音检测，使用AASIST架构结合WavLM前端和RawBoost增强，在多语言数据集上训练，在未修改音频和清洗音频检测任务中获得第二名


<details>
  <summary>Details</summary>
Motivation: 系统评估合成语音检测在不同场景下的性能，包括未修改音频、压缩处理音频和刻意规避检测的清洗音频，探索自监督学习前端、训练数据组成和音频长度配置对深度伪造检测鲁棒性的影响

Method: 基于AASIST架构，采用WavLM大型前端和RawBoost数据增强，在多语言数据集（256,600个样本，9种语言，70多个TTS系统）上进行训练，系统研究不同SSL前端、三种训练数据版本和两种音频长度配置

Result: 在Task 1（未修改音频检测）和Task 3（清洗音频检测）中获得第二名，展示了强大的泛化能力和鲁棒性

Conclusion: 该方法在合成语音检测方面表现出色，特别是在处理复杂音频场景时具有很好的鲁棒性和泛化性能，为深度伪造检测提供了有效的解决方案

Abstract: The SAFE Challenge evaluates synthetic speech detection across three tasks:
unmodified audio, processed audio with compression artifacts, and laundered
audio designed to evade detection. We systematically explore self-supervised
learning (SSL) front-ends, training data compositions, and audio length
configurations for robust deepfake detection. Our AASIST-based approach
incorporates WavLM large frontend with RawBoost augmentation, trained on a
multilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS
systems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.
Through extensive experimentation with different SSL front-ends, three training
data versions, and two audio lengths, we achieved second place in both Task 1
(unmodified audio detection) and Task 3 (laundered audio detection),
demonstrating strong generalization and robustness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [17] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: MoTAS框架通过TTS数据增强和MoE特征选择机制，在AD语音筛查中达到85.71%的准确率，优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病早期筛查中数据有限和缺乏细粒度自适应特征选择的问题

Method: 使用ASR获取准确转录，TTS合成语音增强数据集，MoE机制动态选择最优特征进行多模态特征融合

Result: 在ADReSSo数据集上达到85.71%的准确率，超越现有基线方法

Conclusion: MoTAS在数据有限的实际AD筛查场景中具有重要实用价值

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [18] [Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement](https://arxiv.org/abs/2508.20584)
*Mattias Cross,Anton Ragni*

Main category: cs.SD

TL;DR: 本文研究概率路径的直线性对语音增强质量的影响，发现直线路径比弯曲路径在训练和泛化方面表现更好，并提出单步推理解决方案


<details>
  <summary>Details</summary>
Motivation: 当前基于流的生成式语音增强方法使用弯曲概率路径，但其影响未知。机器学习研究表明直线路径更易训练且泛化更好，需要量化路径直线性对语音增强质量的影响

Method: 使用Schrodinger桥实验不同配置，提出独立条件流匹配方法建模噪声语音与纯净语音间的直线路径，并开发单步推理解决方案

Result: 时间无关方差比梯度对样本质量影响更大，条件流匹配改善了多个语音质量指标，但需要多步推理

Conclusion: 直线时间无关概率路径比弯曲时间相关路径能更好地改善生成式语音增强性能

Abstract: Current flow-based generative speech enhancement methods learn curved
probability paths which model a mapping between clean and noisy speech. Despite
impressive performance, the implications of curved probability paths are
unknown. Methods such as Schrodinger bridges focus on curved paths, where
time-dependent gradients and variance do not promote straight paths. Findings
in machine learning research suggest that straight paths, such as conditional
flow matching, are easier to train and offer better generalisation. In this
paper we quantify the effect of path straightness on speech enhancement
quality. We report experiments with the Schrodinger bridge, where we show that
certain configurations lead to straighter paths. Conversely, we propose
independent conditional flow-matching for speech enhancement, which models
straight paths between noisy and clean speech. We demonstrate empirically that
a time-independent variance has a greater effect on sample quality than the
gradient. Although conditional flow matching improves several speech quality
metrics, it requires multiple inference steps. We rectify this with a one-step
solution by inferring the trained flow-based model as if it was directly
predictive. Our work suggests that straighter time-independent probability
paths improve generative speech enhancement over curved time-dependent paths.

</details>


### [19] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: Amadeus是一个创新的符号音乐生成框架，采用两层架构：自回归模型处理音符序列，双向离散扩散模型处理属性，突破了传统单向依赖假设，实现了性能提升和4倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐生成模型假设音符属性具有固定的单向时间依赖结构，但研究发现不同属性作为初始token时性能相当，表明音符属性本质上是并发无序的集合而非时间依赖序列。

Method: 提出Amadeus框架：音符序列使用自回归模型，属性使用双向离散扩散模型。引入MLSDES增强策略通过对比学习提高中间表示判别性，CIEM模块通过注意力机制增强音符潜在向量表示。

Result: 在无条件和文本条件生成任务中，Amadeus在多个指标上显著优于SOTA模型，同时实现至少4倍加速，并展示了无需训练即可进行细粒度音符属性控制的可行性。

Conclusion: Amadeus通过重新思考音符属性的本质特性，采用创新的两层架构和增强策略，在符号音乐生成领域取得了突破性进展，为音乐AI提供了新的技术路径。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


### [20] [Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions](https://arxiv.org/abs/2508.20717)
*Ran Piao,Yuan Lu,Hareld Kemps,Tong Xia,Aaqib Saeed*

Main category: cs.SD

TL;DR: MARVEL是一个多任务学习框架，通过语音分析同时检测9种神经、呼吸和声音疾病，使用衍生声学特征保护隐私，在多项任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音健康评估方法通常只关注单一疾病，未能充分利用语音中丰富的多维度信息，需要开发能够同时检测多种疾病的可扩展解决方案。

Method: 采用双分支架构，包含专用编码器和任务特定头部，共享共同的声学骨干网络，实现跨疾病知识迁移，仅使用衍生声学特征无需传输原始音频。

Result: 在Bridge2AI-Voice v2.0数据集上总体AUROC达到0.78，神经疾病检测表现优异（AUROC=0.89），阿尔茨海默病/轻度认知障碍检测AUROC达0.97，在9项任务中的7项超越最先进的自监督模型。

Conclusion: 该研究证明单一统一模型可有效筛查多种疾病，为资源有限和远程医疗环境中的语音诊断部署奠定了基础，学习到的表征与临床认可的声学模式一致。

Abstract: Voice-based health assessment offers unprecedented opportunities for
scalable, non-invasive disease screening, yet existing approaches typically
focus on single conditions and fail to leverage the rich, multi-faceted
information embedded in speech. We present MARVEL (Multi-task Acoustic
Representations for Voice-based Health Analysis), a privacy-conscious multitask
learning framework that simultaneously detects nine distinct neurological,
respiratory, and voice disorders using only derived acoustic features,
eliminating the need for raw audio transmission. Our dual-branch architecture
employs specialized encoders with task-specific heads sharing a common acoustic
backbone, enabling effective cross-condition knowledge transfer. Evaluated on
the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC
of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),
particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).
Our framework consistently outperforms single-modal baselines by 5-19% and
surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while
correlation analysis reveals that the learned representations exhibit
meaningful similarities with established acoustic features, indicating that the
model's internal representations are consistent with clinically recognized
acoustic patterns. By demonstrating that a single unified model can effectively
screen for diverse conditions, this work establishes a foundation for
deployable voice-based diagnostics in resource-constrained and remote
healthcare settings.

</details>


### [21] [Speech Emotion Recognition via Entropy-Aware Score Selection](https://arxiv.org/abs/2508.20796)
*ChenYi Chua,JunKai Wong,Chengxin Chen,Xiaoxiao Miao*

Main category: cs.SD

TL;DR: 提出了一种基于熵感知分数选择的多模态语音情感识别框架，结合语音和文本预测，通过后期分数融合提升传统单模态系统的性能


<details>
  <summary>Details</summary>
Motivation: 传统单模态语音情感识别系统存在置信度限制，需要克服主要管道预测的置信约束，通过多模态融合提升识别性能

Method: 集成基于wav2vec2.0的声学模型和基于RoBERTa-XLM的情感分析模型，使用Whisper-large-v3生成转录，提出基于熵和变熵阈值的后期分数融合方法，采用情感映射策略将三种情感类别转换为四种目标情感类别

Result: 在IEMOCAP和MSP-IMPROV数据集上验证，该方法相比传统单模态系统提供了实用且可靠的性能提升

Conclusion: 所提出的多模态框架通过熵感知分数选择和情感映射策略，有效提升了语音情感识别的性能，为多模态情感识别提供了实用的解决方案

Abstract: In this paper, we propose a multimodal framework for speech emotion
recognition that leverages entropy-aware score selection to combine speech and
textual predictions. The proposed method integrates a primary pipeline that
consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that
consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions
generated via Whisper-large-v3. We propose a late score fusion approach based
on entropy and varentropy thresholds to overcome the confidence constraints of
primary pipeline predictions. A sentiment mapping strategy translates three
sentiment categories into four target emotion classes, enabling coherent
integration of multimodal predictions. The results on the IEMOCAP and
MSP-IMPROV datasets show that the proposed method offers a practical and
reliable enhancement over traditional single-modality systems.

</details>


### [22] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 该研究提出了大规模语音识别数据集OLMoASR-Pool和OLMoASR-Mix，训练了从39M到1.5B参数的模型系列，在短语音和长语音识别任务上达到了与OpenAI Whisper相当的性能。


<details>
  <summary>Details</summary>
Motivation: 语音识别领域中训练数据的规模和质量影响研究不足，需要构建大规模高质量数据集来发展健壮的零样本语音识别模型。

Method: 从3M小时的OLMoASR-Pool数据集出发，设计文本含义筛选器去除低质量和错误转写数据，生成1M小时高质量的OLMoASR-Mix数据集，用其训练了从39M到1.5B参数的模型系列。

Result: OLMoASR模型在各个规模下都达到了与Whisper相当的平均性能。OLMoASR-medium.en在短语音识别任务上达到12.8%词误率，长语音任务上达到11.0%词误率，与Whisper-medium.en的12.4%和10.5%相当。

Conclusion: 通过构建大规模高质量语音数据集和相应的模型训练方法，可以开发出与商业级语音识别模型相竞争的零样本识别模型，并将数据集、模型和代码开源以促进语音处理领域的研究。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


### [23] [SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization](https://arxiv.org/abs/2508.20885)
*Chien-Chun Wang,En-Lun Yu,Jeih-Weih Hung,Shih-Chieh Huang,Berlin Chen*

Main category: cs.SD

TL;DR: SincQDR-VAD是一个紧凑且鲁棒的语音活动检测框架，结合Sinc提取器前端和新型二次差异排序损失，在噪声环境中显著提升性能同时减少参数数量


<details>
  <summary>Details</summary>
Motivation: 现有VAD方法在噪声环境中缺乏鲁棒性，且帧级分类损失与VAD评估指标耦合不紧密，需要更有效的解决方案

Method: 使用可学习带通滤波器的Sinc提取器捕获抗噪声频谱特征，结合二次差异排序损失优化语音和非语音帧的成对得分顺序

Result: 在代表性基准数据集上显著提高了AUROC和F2-Score，参数数量仅为现有技术的69%

Conclusion: SincQDR-VAD框架在效率和实用性方面表现出色，为噪声环境下的语音活动检测提供了有效的解决方案

Abstract: Voice activity detection (VAD) is essential for speech-driven applications,
but remains far from perfect in noisy and resource-limited environments.
Existing methods often lack robustness to noise, and their frame-wise
classification losses are only loosely coupled with the evaluation metric of
VAD. To address these challenges, we propose SincQDR-VAD, a compact and robust
framework that combines a Sinc-extractor front-end with a novel quadratic
disparity ranking loss. The Sinc-extractor uses learnable bandpass filters to
capture noise-resistant spectral features, while the ranking loss optimizes the
pairwise score order between speech and non-speech frames to improve the area
under the receiver operating characteristic curve (AUROC). A series of
experiments conducted on representative benchmark datasets show that our
framework considerably improves both AUROC and F2-Score, while using only 69%
of the parameters compared to prior arts, confirming its efficiency and
practical viability.

</details>


### [24] [Learning Robust Spatial Representations from Binaural Audio through Feature Distillation](https://arxiv.org/abs/2508.20914)
*Holger Severin Bovbjerg,Jan Østergaard,Jesper Jensen,Shinji Watanabe,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 该论文提出了一种基于特征蒸馏的预训练方法，用于从双声道音频中学习空间表示，无需数据标签。通过从干净的双声道语音样本计算空间特征作为预测标签，然后用神经网络从增强语音中预测这些特征。预训练后的编码器权重用于初始化DoA估计模型，在噪声和混响环境中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 深度表示学习在音频任务中表现优异，但从多声道音频学习空间表示的研究不足。需要探索无标签数据下的空间表示学习方法，以提高在噪声和混响环境中的方向估计性能。

Method: 使用特征蒸馏预训练框架：1）从干净双声道语音计算空间特征作为标签；2）用神经网络从增强语音预测这些特征；3）丢弃空间特征预测器，用学习到的编码器权重初始化DoA估计模型；4）对DoA估计进行微调。

Result: 实验表明，经过微调的预训练模型在噪声和混响环境中的方向估计性能优于完全监督模型和传统信号处理方法。

Conclusion: 基于特征蒸馏的无监督预训练方法能有效学习鲁棒的空间表示，为双声道语音的空间信息提取提供了有效解决方案，在复杂声学环境中具有优越性能。

Abstract: Recently, deep representation learning has shown strong performance in
multiple audio tasks. However, its use for learning spatial representations
from multichannel audio is underexplored. We investigate the use of a
pretraining stage based on feature distillation to learn a robust spatial
representation of binaural speech without the need for data labels. In this
framework, spatial features are computed from clean binaural speech samples to
form prediction labels. These clean features are then predicted from
corresponding augmented speech using a neural network. After pretraining, we
throw away the spatial feature predictor and use the learned encoder weights to
initialize a DoA estimation model which we fine-tune for DoA estimation. Our
experiments demonstrate that the pretrained models show improved performance in
noisy and reverberant environments after fine-tuning for direction-of-arrival
estimation, when compared to fully supervised models and classic signal
processing methods.

</details>


### [25] [WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations](https://arxiv.org/abs/2508.20976)
*Jaeyeon Kim,Heeseung Yun,Sang Hoon Woo,Chao-Han Huck Yang,Gunhee Kim*

Main category: cs.SD

TL;DR: 提出了WoW-Bench基准测试，用于评估大型音频语言模型在低层次听觉感知和认知方面的能力，特别是针对海洋哺乳动物叫声的新颖声音处理。实验显示当前模型性能远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在低层次听觉能力（如音高和时长检测）方面研究不足，而这些能力对于处理现实世界中的分布外任务至关重要，需要基于细粒度声学线索进行推理。

Method: 引入World-of-Whale基准测试（WoW-Bench），包含感知基准（分类新颖声音）和认知基准（基于Bloom分类法评估记忆、理解、应用和分析声音事件的能力），并引入干扰问题来验证模型是否真正通过听觉解决问题。

Result: 对最先进的大型音频语言模型进行实验，结果显示其性能远低于人类水平，表明这些模型在听觉基础方面存在明显不足。

Conclusion: 当前的大型音频语言模型在低层次听觉感知和认知能力方面仍有很大改进空间，需要更强的听觉基础能力来处理现实世界的听觉任务。

Abstract: Large audio language models (LALMs) extend language understanding into the
auditory domain, yet their ability to perform low-level listening, such as
pitch and duration detection, remains underexplored. However, low-level
listening is critical for real-world, out-of-distribution tasks where models
must reason about unfamiliar sounds based on fine-grained acoustic cues. To
address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to
evaluate low-level auditory perception and cognition using marine mammal
vocalizations. WoW-bench is composed of a Perception benchmark for categorizing
novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess
the abilities to remember, understand, apply, and analyze sound events. For the
Cognition benchmark, we additionally introduce distractor questions to evaluate
whether models are truly solving problems through listening rather than relying
on other heuristics. Experiments with state-of-the-art LALMs show performance
far below human levels, indicating a need for stronger auditory grounding in
LALMs.

</details>

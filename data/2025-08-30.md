<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 7]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels](https://arxiv.org/abs/2508.20277)
*Xiaoyan Ma,Shahryar Zehtabi,Taejoon Kim,Christopher G. Brinton*

Main category: eess.SP

TL;DR: 本文研究基于OFDM的空中联邦学习系统，分析了移动设备高速移动导致的信道估计不完美和模型参数不对齐问题，推导了单轮和多轮全局模型更新的闭式表达式和误差界限。


<details>
  <summary>Details</summary>
Motivation: 移动设备的高移动性导致信道估计不完美，造成模型参数传输不同步和时变上传信道，这些因素共同导致OTA-FL训练过程中的失真问题，目前对此研究不足。

Method: 首先推导了单轮全局模型更新的闭式表达式，然后扩展到多轮全局更新分析，得到了OTA-FL累积误差的界限，并通过大量数值模拟验证理论结果。

Result: 获得了考虑信道不完美性的全局模型更新闭式表达式和累积误差界限，数值模拟结果证实了理论分析的正确性。

Conclusion: 移动性引起的信道不完美性对OTA-FL系统性能有显著影响，提出的分析框架能够有效量化这些影响，为系统设计提供理论指导。

Abstract: This paper investigates an OFDM-based over-the-air federated learning
(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles
(UAVs), transmit local machine learning (ML) models to a central parameter
server (PS) for global model aggregation. The high mobility of local devices
results in imperfect channel estimation, leading to a misalignment problem,
i.e., the model parameters transmitted from different local devices do not
arrive at the central PS simultaneously. Moreover, the mobility introduces
time-varying uploading channels, which further complicates the aggregation
process. All these factors collectively cause distortions in the OTA-FL
training process which are underexplored. To quantify these effects, we first
derive a closed-form expression for a single-round global model update in terms
of these channel imperfections. We then extend our analysis to capture multiple
rounds of global updates, yielding a bound on the accumulated error in OTA-FL.
We validate our theoretical results via extensive numerical simulations, which
corroborate our derived analysis.

</details>


### [2] [Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design](https://arxiv.org/abs/2508.20531)
*Chaoying Huang,Wen Chen,Qingqing Wu,Xusheng Zhu,Zhendong Li,Ying Wang,Jinhong Yuan*

Main category: eess.SP

TL;DR: 这篇论文提出了一种新颖的双智能反射表面（IRS）助力干扰限制同时无线信息与能量传输（SWIPT）系统，采用独立功率分配策略，在近场和混合场情况下优化收益功率和传输性能。


<details>
  <summary>Details</summary>
Motivation: 为了在干扰限制环境中实现更好的信息传输与能量收集的权衡，并考虑实际通信场景中的近场和混合场模型影响。

Method: 采用双IRS结构和独立功率分配，建立近场和混合场通道模型。在近场情况下使用交替优化算法（拉格朗日对偶方法和DC规划），在混合场情况下利用通道增益与IRS相位无关的特性将问题转化为凸优化问题。

Result: 数值结果验证了分析的正确性，并显示所提方案在收益功率和系统性能方面都超过了其他对照方案。

Conclusion: 论文提出的双IRS助力SWIPT系统以及独立功率分配策略能够有效提升系统性能，特别是在近场和混合场通信场景中显示出显著的性能优势。

Abstract: This paper proposes a novel dual-intelligent reflecting surface (IRS) aided
interference-limited simultaneous wireless information and power transfer
(SWIPT) system with independent power splitting (PS), where each receiving
antenna applies different PS factors to offer an advantageous trade-off between
the useful information and harvested energy. We separately establish the near-
and hybrid-field channel models for IRS-reflected links to evaluate the
performance gain more precisely and practically. Specifically, we formulate an
optimization problem of maximizing the harvested power by jointly optimizing
dual-IRS phase shifts, independent PS ratio, and receive beamforming vector in
both near- and hybrid-field cases. In the near-field case, the alternating
optimization algorithm is proposed to solve the non-convex problem by applying
the Lagrange duality method and the difference-of-convex (DC) programming. In
the hybrid-field case, we first present an interesting result that the
AP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which
allows the optimization problem to be transformed into a convex one. Then, we
derive the asymptotic performance of the combined channel gains in closed-form
and analyze the characteristics of the dual-IRS. Numerical results validate our
analysis and indicate the performance gains of the proposed scheme that
dual-IRS-aided SWIPT with independent PS over other benchmark schemes.

</details>


### [3] [Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders](https://arxiv.org/abs/2508.20535)
*Annika Stiehl,Nicolas Weeger,Christian Uhl,Dominic Bechtold,Nicole Ille,Stefan Geißelsöder*

Main category: eess.SP

TL;DR: 提出基于深度卷积自编码器(DCAE)的癫痫检测方法，通过结合时域和频域损失函数来提取EEG信号的低维表征，解决了现有方法在敏感性和误报率之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫检测需要可靠高效的EEG分析方法，但现有深度学习模型难以同时实现高敏感性和低误报率，且在时域或频域输入表征选择上缺乏一致性。

Method: 使用深度卷积自编码器提取EEG信号的低维潜在表征，通过比较基于时域和频域的重构误差来评估模型性能，训练了多种具有不同损失函数的自编码器。

Result: 同时考虑时域和频域损失的DCAE模型获得了最佳重构性能，表明单一表征的深度神经网络可能无法保留所有相关信号特性。

Conclusion: 该研究揭示了深度学习模型处理EEG数据的方式，验证了时域信号输入时频域信息是否被有效捕获，为癫痫自动检测提供了新思路。

Abstract: Epilepsy is one of the most common neurological disorders. This disease
requires reliable and efficient seizure detection methods.
Electroencephalography (EEG) is the gold standard for seizure monitoring, but
its manual analysis is a time-consuming task that requires expert knowledge. In
addition, there are no well-defined features that allow fully automated
analysis. Existing deep learning-based approaches struggle to achieve high
sensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack
consistency in the optimal EEG input representation, whether in the time or
frequency domain. To address these issues, we propose a Deep Convolutional
Autoencoder (DCAE) to extract low-dimensional latent representations that
preserve essential EEG signal features. The ability of the model to preserve
relevant information was evaluated by comparing reconstruction errors based on
both time series and frequency-domain representations. Several autoencoders
with different loss functions based on time and frequency were trained and
evaluated to determine their effectiveness in reconstructing EEG features. Our
results show that the DCAE model taking both time series and frequency losses
into account achieved the best reconstruction performance. This indicates that
Deep Neural Networks with a single representation might not preserve the
relevant signal properties. This work provides insight into how deep learning
models process EEG data and examines whether frequency information is captured
when time series signals are used as input.

</details>


### [4] [Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis](https://arxiv.org/abs/2508.20602)
*Matthieu Correa,Nicolas Vignais,Isabelle A. Siegler,Maxime Projetti*

Main category: eess.SP

TL;DR: 提出基于CEEMDAN和谱模糊熵的自适应滤波方法，有效分离MMG信号中的运动伪影，在动态条件下优于传统带通滤波技术


<details>
  <summary>Details</summary>
Motivation: MMG在肌肉活动测量中很有前景，但对运动伪影敏感限制了其应用，需要开发更好的滤波方法来处理动态条件下的运动伪影

Method: 基于完全集成经验模态分解(CEEMDAN)和谱模糊熵的自适应滤波方法，用于分离MMG加速度计信号中的运动伪影

Result: 与传统带通滤波相比，在三角肌和竖脊肌的运动重构方面表现更好(R²=0.907和0.842)，能在5-20Hz带宽内动态过滤运动伪影

Conclusion: 该方法能有效过滤运动伪影，但处理躯干和下肢肌肉在行走/跑步时的加速度MMG信号仍需谨慎，因为冲击相关的加速度仍然存在且需要进一步量化

Abstract: Mechanomyography (MMG) is a promising tool for measuring muscle activity in
the field but its sensitivity to motion artifacts limits its application. In
this study, we proposed an adaptative filtering method for MMG accelerometers
based on the complete ensemble empirical mode decomposition, with adaptative
noise and spectral fuzzy entropy, to isolate motions artefacts from the MMG
signal in dynamic conditions. We compared our method with the traditional
band-pass filtering technique, demonstrating better results concerning motion
recomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and
0.842). Thus, this innovative method allows the filtering of motion artifacts
dynamically in the 5-20 Hz bandwidth, which is not achievable with traditional
method. However, the interpretation of accelerometric MMG signals from the
trunk and lower-limb muscles during walking or running should be approached
with great caution as impact-related accelerations are still present, though
their exact quantity still needs to be quantified.

</details>


### [5] [Weighted Bayesian Cram$\acute{\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation](https://arxiv.org/abs/2508.20761)
*Yaniv Mazor,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 本文针对混合分辨率系统开发了加权贝叶斯克拉美-罗界(WBCRB)，提出了基于区域划分的MSE近似方法，在LGO模型中验证了WBCRB优于传统BCRB，并能准确预测量化误差导致的MSE非单调行为。


<details>
  <summary>Details</summary>
Motivation: 混合分辨率架构在通信和雷达系统中广泛应用以降低硬件成本，但粗量化数据在参数估计中引入非平凡权衡，需要推导此类系统的下界。

Method: 开发了具有通用权重函数的WBCRB，包括经典BCRB、BFIM逆加权WBCRB和最优权重WBCRB。提出将估计问题划分为信息区和饱和区，分别应用区域特定的WBCRB近似来获得复合MSE估计。

Result: 在线性高斯正交(LGO)模型中，WBCRB优于BCRB，BFIM逆加权版本接近最优WBCRB。基于WBCRB的MSE近似更紧致，能准确预测量化误差导致的MSE非单调行为。

Conclusion: 所提出的WBCRB和MSE近似方法为混合分辨率系统的参数估计性能分析提供了有效的理论工具，特别适用于处理量化误差带来的挑战。

Abstract: Mixed-resolution architectures, combining high-resolution (analog) data with
coarsely quantized (e.g., 1-bit) data, are widely employed in emerging
communication and radar systems to reduce hardware costs and power consumption.
However, the use of coarsely quantized data introduces non-trivial tradeoffs in
parameter estimation tasks. In this paper, we investigate the derivation of
lower bounds for such systems. In particular, we develop the weighted Bayesian
Cramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight
function. We demonstrate the special cases of: (i) the classical BCRB; (ii) the
WBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse
weighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight
function. Based on the developed WBCRB, we propose a new method to approximate
the mean-squared-error (MSE) by partitioning the estimation problem into two
regions: (a) where the 1-bit quantized data is informative; and (b) where it is
saturated. We apply region-specific WBCRB approximations in these regions to
achieve an accurate composite MSE estimate. We derive the bounds and MSE
approximation for the linear Gaussian orthonormal (LGO) model, which is
commonly used in practical signal processing applications. Our simulation
results demonstrate the use of the proposed bounds and approximation method in
the LGO model with a scalar unknown parameter. It is shown that the WBCRB
outperforms the BCRB, where the BFIM-Inverse weighting version approaches the
optimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is
tighter and accurately predicts the non-monotonic behavior of the MSE in the
presence of quantization errors.

</details>


### [6] [Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar](https://arxiv.org/abs/2508.20864)
*Ehsan Sadeghi,Paul Havinga*

Main category: eess.SP

TL;DR: 本文提出基于毫米波FMCW雷达的非接触式生命体征监测方法，通过改进Prony和MUSIC算法显著提升心率和呼吸率检测精度，MAE分别达到0.81/1.8和0.8/1.01。


<details>
  <summary>Details</summary>
Motivation: 传统接触式生命体征监测方法在临床和急救场景中存在局限性，需要开发可靠的非侵入式监测解决方案。

Method: 采用毫米波FMCW雷达技术，针对性地改进Prony和MUSIC算法，增强信号处理能力以捕捉细微生理变化。

Result: 改进算法在抑制噪声和谐波干扰方面表现优异，心率和呼吸率检测的平均绝对误差显著降低。

Conclusion: FMCW雷达结合优化算法为医疗环境提供了可靠的非接触式连续生命体征监测方案，特别适用于传统方法不便实施的场景。

Abstract: This paper explores the deployment of mm-wave Frequency Modulated Continuous
Wave (FMCW) radar for vital sign detection across multiple scenarios. We focus
on overcoming the limitations of traditional sensing methods by enhancing
signal processing techniques to capture subtle physiological changes
effectively. Our study introduces novel adaptations of the Prony and MUSIC
algorithms tailored for real-time heart and respiration rate monitoring,
significantly advancing the accuracy and reliability of non-contact vital sign
monitoring using radar technologies. Notably, these algorithms demonstrate a
robust ability to suppress noise and harmonic interference. For instance, the
mean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8
and 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,
respectively. These results underscore the potential of FMCW radar as a
reliable, non-invasive solution for continuous vital sign monitoring in
healthcare settings, particularly in clinical and emergency scenarios where
traditional contact-based monitoring is impractical.

</details>


### [7] [A Correction for the Paper "Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis"](https://arxiv.org/abs/2508.20990)
*Hong-Yan Zhang,Haoting Liu,Rui-Jia Lin,Yu Zhou*

Main category: eess.SP

TL;DR: 本文指出了SGMD方法的局限性并修复了其缺陷，通过引入回拉定理来正确计算时间序列分量


<details>
  <summary>Details</summary>
Motivation: SGMD方法虽然推广了SSA中的轨迹矩阵形式，但未同步更新对角平均原理(DAP)，存在计算缺陷

Method: 提出回拉定理(pulling back theorem)来正确地从轨迹矩阵分量计算对应的时间序列分量

Result: 修复了SGMD方法的bug，完善了时间序列分解的计算方法

Conclusion: 通过回拉定理解决了SGMD方法的局限性，为时间序列分解提供了更准确的计算框架

Abstract: The symplectic geometry mode decomposition (SGMD) is a powerful method for
decomposing time series, which is based on the diagonal averaging principle
(DAP) inherited from the singular spectrum analysis (SSA). Although the authors
of SGMD method generalized the form of the trajectory matrix in SSA, the DAP is
not updated simultaneously. In this work, we pointed out the limitations of the
SGMD method and fixed the bugs with the pulling back theorem for computing the
given component of time series from the corresponding component of trajectory
matrix.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [8] [Live Vocal Extraction from K-pop Performances](https://arxiv.org/abs/2508.20273)
*Yujin Kim,Richa Namballa,Magdalena Fuentes*

Main category: eess.AS

TL;DR: 自动从K-pop现场表演中提取现场唱音的方法


<details>
  <summary>Details</summary>
Motivation: 受K-pop粉丝文化的启发，开发自动分离现场唱音的技术

Method: 结合了源分离、相关性分析和幅度调整技术，自动移除预录唱音和乐器音

Result: 提出了现场唱音分离的新任务，为该领域研究奠定基础

Conclusion: 这项预研工作为现场唱音分离领域的未来研究提供了重要的起点

Abstract: K-pop's global success is fueled by its dynamic performances and vibrant fan
engagement. Inspired by K-pop fan culture, we propose a methodology for
automatically extracting live vocals from performances. We use a combination of
source separation, cross-correlation, and amplitude scaling to automatically
remove pre-recorded vocals and instrumentals from a live performance. Our
preliminary work introduces the task of live vocal separation and provides a
foundation for future research in this topic.

</details>


### [9] [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
*Muhammad Shakeel,Yui Sudo,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: 本文提出了一种统一的多说话人编码器(UME)架构，通过共享语音基础编码器联合学习说话人日志、语音分离和多说话人语音识别任务，利用残差加权求和编码有效利用不同语义层级信息，显著提升了重叠语音处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的说话人日志、语音分离和多说话人语音识别任务通常独立处理，但它们在处理重叠语音时存在内在的相互依赖性。为了充分利用这些任务之间的协同效应，需要开发一个统一的架构来联合学习这些任务。

Method: 提出统一多说话人编码器(UME)架构，使用共享语音基础编码器联合训练三个任务。采用残差加权求和编码(RWSE)技术，从编码器的多个隐藏层提取表示，有效利用不同语义层级的信息，实现任务间的自底向上对齐。

Result: 在LibriMix评估集上，UME显著优于专门针对单个任务的基线方法。在说话人日志任务上，UME在Libri2Mix和Libri3Mix评估集上分别实现了1.37%和2.29%的日志错误率，超越了之前的研究成果。

Conclusion: UME通过联合学习多个相关任务，成功捕获了任务间的内在相互依赖性，在处理重叠语音数据方面表现出色，为多说话人语音处理任务提供了一个有效的统一解决方案。

Abstract: This paper presents a unified multi-speaker encoder (UME), a novel
architecture that jointly learns representations for speaker diarization (SD),
speech separation (SS), and multi-speaker automatic speech recognition (ASR)
tasks using a shared speech foundational encoder. We leverage the hidden
representations from multiple layers of UME as a residual weighted-sum encoding
(RWSE) to effectively use information from different semantic levels,
contributing to bottom-up alignment between tasks. This joint training approach
captures the inherent interdependencies among the tasks, enhancing overall
performance on overlapping speech data. Our evaluations demonstrate that UME
substantially improves over the single-task baselines dedicated to SD, SS, and
multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms
the previous studies, achieving diarization error rates of 1.37% and 2.29% on
Libri2Mix and Libri3Mix evaluation sets, respectively.

</details>


### [10] [CodecBench: A Comprehensive Benchmark for Acoustic and Semantic Evaluation](https://arxiv.org/abs/2508.20660)
*Ruifan Deng,Yitian Gong,Qinghui Gao,Luozhijie Jin,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Xipeng Qiu*

Main category: eess.AS

TL;DR: CodecBench是一个全面的音频编解码器评估数据集，用于从声学和语义两个角度评估音频编解码器在四个数据域中的性能，旨在解决现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的兴起，音频编解码器在将音频编码为离散标记方面发挥着越来越重要的作用。然而，现有的音频编解码器评估受限于简单的指标和场景，现有的基准测试也不适用于复杂的应用场景，这限制了对复杂数据集中声学和语义能力的评估性能。

Method: 研究者引入了CodecBench，这是一个全面的评估数据集，从声学和语义两个角度评估音频编解码器在四个数据域中的性能。

Result: 通过这个基准测试，研究旨在识别当前局限性，突出未来研究方向，并促进音频编解码器开发方面的进展。

Conclusion: CodecBench为音频编解码器提供了一个更全面的评估框架，能够更好地适应复杂的应用场景，如多说话人、背景噪声或更丰富的副语言信息场景，推动音频编解码器技术的发展。

Abstract: With the rise of multimodal large language models (LLMs), audio codec plays
an increasingly vital role in encoding audio into discrete tokens, enabling
integration of audio into text-based LLMs. Current audio codec captures two
types of information: acoustic and semantic. As audio codec is applied to
diverse scenarios in speech language model , it needs to model increasingly
complex information and adapt to varied contexts, such as scenarios with
multiple speakers, background noise, or richer paralinguistic information.
However, existing codec's own evaluation has been limited by simplistic metrics
and scenarios, and existing benchmarks for audio codec are not designed for
complex application scenarios, which limits the assessment performance on
complex datasets for acoustic and semantic capabilities. We introduce
CodecBench, a comprehensive evaluation dataset to assess audio codec
performance from both acoustic and semantic perspectives across four data
domains. Through this benchmark, we aim to identify current limitations,
highlight future research directions, and foster advances in the development of
audio codec. The codes are available at https://github.com/RayYuki/CodecBench.

</details>


### [11] [Sound event detection with audio-text models and heterogeneous temporal annotations](https://arxiv.org/abs/2508.20703)
*Manu Harju,Annamaria Mesaros*

Main category: eess.AS

TL;DR: 提出使用机器生成的合成字幕作为补充信息来指导声音事件检测系统，在强标签和弱标签训练场景下均能提升性能


<details>
  <summary>Details</summary>
Motivation: 利用音频和元数据生成的合成字幕包含自然语言信息，可以作为其他音频任务的输入，为声音事件检测提供额外指导

Method: 使用机器生成的合成字幕作为强标签的补充信息进行训练，评估不同类型的文本输入，并研究部分训练数据只有弱标签的场景

Result: 在50个高度不平衡类别的数据集上，PSDS-1分数从0.223提升到0.277（强标签训练），从0.166提升到0.218（一半数据只有弱标签）

Conclusion: 合成字幕在强标签和弱标签训练场景下都能显著改善声音事件检测性能，优于传统的CRNN架构

Abstract: Recent advances in generating synthetic captions based on audio and related
metadata allow using the information contained in natural language as input for
other audio tasks. In this paper, we propose a novel method to guide a sound
event detection system with free-form text. We use machine-generated captions
as complementary information to the strong labels for training, and evaluate
the systems using different types of textual inputs. In addition, we study a
scenario where only part of the training data has strong labels, and the rest
of it only has temporally weak labels. Our findings show that synthetic
captions improve the performance in both cases compared to the CRNN
architecture typically used for sound event detection. On a dataset of 50
highly unbalanced classes, the PSDS-1 score increases from 0.223 to 0.277 when
trained with strong labels, and from 0.166 to 0.218 when half of the training
data has only weak labels.

</details>


### [12] [Online incremental learning for audio classification using a pretrained audio model](https://arxiv.org/abs/2508.20732)
*Manjunath Mulimani,Annamaria Mesaros*

Main category: eess.AS

TL;DR: 提出了一种基于预训练音频嵌入的在线增量学习方法，通过在预训练模型和分类器之间插入非线性激活层来扩展嵌入维度并捕捉声音类别特征，实现单次前向传播的在线学习，有效减少旧任务遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有音频增量学习方法需要从初始任务开始训练模型，并通过多次迭代适应新任务，存在训练效率低和遗忘问题。本文旨在利用预训练模型的通用音频嵌入来开发在线增量学习器，实现高效的单次前向传播适应。

Method: 在预训练模型的音频嵌入和分类器之间插入带有非线性激活函数的层，扩展嵌入维度以更好地捕捉声音类别特征。该方法只需对任何任务的训练样本进行单次前向传播即可适应模型。

Result: 在ESC-50数据集上的类增量学习和TAU Urban Acoustic Scenes 2019数据集上的域增量学习中，所提方法均优于其他对比方法，表现出更好的性能和更少的旧任务遗忘。

Conclusion: 该方法成功利用预训练音频嵌入实现了高效的在线增量学习，通过简单的架构修改就能在单次前向传播中适应新任务，同时最小化对旧任务的遗忘，在两个不同的增量学习设置中都取得了优越性能。

Abstract: Incremental learning aims to learn new tasks sequentially without forgetting
the previously learned ones. Most of the existing incremental learning methods
for audio focus on training the model from scratch on the initial task, and the
same model is used to learn upcoming incremental tasks. The model is trained
for several iterations to adapt to each new task, using some specific
approaches to reduce the forgetting of old tasks. In this work, we propose a
method for using generalizable audio embeddings produced by a pre-trained model
to develop an online incremental learner that solves sequential audio
classification tasks over time. Specifically, we inject a layer with a
nonlinear activation function between the pre-trained model's audio embeddings
and the classifier; this layer expands the dimensionality of the embeddings and
effectively captures the distinct characteristics of sound classes. Our method
adapts the model in a single forward pass (online) through the training samples
of any task, with minimal forgetting of old tasks. We demonstrate the
performance of the proposed method in two incremental learning setups: one
class-incremental learning using ESC-50 and one domain-incremental learning of
different cities from the TAU Urban Acoustic Scenes 2019 dataset; for both
cases, the proposed approach outperforms other methods.

</details>


### [13] [A Solution of Ultra Wideband Based High-resolution and Lossless Audio Transmission](https://arxiv.org/abs/2508.20782)
*Fengyun Zhang*

Main category: eess.AS

TL;DR: 本文提出使用超宽带(UWB)技术实现高分辨率无损音频传输，解决现有无线音频技术在带宽、压缩、延迟和设备兼容性方面的局限性


<details>
  <summary>Details</summary>
Motivation: 现有无线音频传输技术在数据带宽、数据压缩、延迟和设备兼容性方面存在诸多限制，无法满足高质量实时音频应用的需求

Method: 采用超宽带(UWB)技术，利用其高带宽特性实现高分辨率无损音频传输，同时提供超低延迟和精确定位能力

Result: UWB技术能够提供足够的带宽来实现卓越的音质和超低延迟，适合实时音频应用，并能解决音视频同步问题，还支持增强现实和虚拟现实应用中的精确定位

Conclusion: 超宽带技术是解决无线音频传输挑战的有前景的解决方案，不仅能实现高分辨率无损音频传输，还能为增强现实和虚拟现实应用提供额外的定位功能

Abstract: This paper provides an overview of the current challenges in wireless audio
transmission and highlights the limitations of existing technologies regarding
data bandwidth, data compression, latency, and inter-device compatibility. To
address these shortcomings, it proposes a high-resolution, lossless audio
transmission scheme utilizing ultra wideband (UWB) technology. UWB emerges as a
promising solution by offering the necessary bandwidth to enable exceptional
sound quality with ultra-low latency, making it ideal for real-time audio
applications and addressing synchronization concerns in audio-visual use cases.
Additionally, UWB's unique capabilities extend beyond high-resolution audio,
allowing for precise location tracking in augmented and virtual reality
applications.

</details>


### [14] [Leveraging Discriminative Latent Representations for Conditioning GAN-Based Speech Enhancement](https://arxiv.org/abs/2508.20859)
*Shrishti Saha Shetu,Emanuël A. P. Habets,Andreas Brendel*

Main category: eess.AS

TL;DR: 提出DisCoGAN方法，利用判别式语音增强模型的潜在特征作为条件特征来改进GAN语音增强，在低信噪比场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有生成式语音增强方法在极低信噪比场景下性能有限，需要解决这一挑战

Method: 使用判别式语音增强模型提取的潜在特征作为通用条件特征，改进基于GAN的语音增强方法

Result: DisCoGAN在低信噪比场景下性能提升，在高信噪比条件和真实录音中保持竞争力或更优表现

Conclusion: 该方法持续优于现有方法，判别式条件方法对整体性能有重要贡献

Abstract: Generative speech enhancement methods based on generative adversarial
networks (GANs) and diffusion models have shown promising results in various
speech enhancement tasks. However, their performance in very low
signal-to-noise ratio (SNR) scenarios remains under-explored and limited, as
these conditions pose significant challenges to both discriminative and
generative state-of-the-art methods. To address this, we propose a method that
leverages latent features extracted from discriminative speech enhancement
models as generic conditioning features to improve GAN-based speech
enhancement. The proposed method, referred to as DisCoGAN, demonstrates
performance improvements over baseline models, particularly in low-SNR
scenarios, while also maintaining competitive or superior performance in
high-SNR conditions and on real-world recordings. We also conduct a
comprehensive evaluation of conventional GAN-based architectures, including
GANs trained end-to-end, GANs as a first processing stage, and post-filtering
GANs, as well as discriminative models under low-SNR conditions. We show that
DisCoGAN consistently outperforms existing methods. Finally, we present an
ablation study that investigates the contributions of individual components
within DisCoGAN and analyzes the impact of the discriminative conditioning
method on overall performance.

</details>


### [15] [Automatic Inspection Based on Switch Sounds of Electric Point Machines](https://arxiv.org/abs/2508.20870)
*Ayano Shibata,Toshiki Gunji,Mitsuaki Tsuda,Takashi Endo,Kota Dohi,Tomoya Nishida,Satoko Nomoto*

Main category: eess.AS

TL;DR: 日本JR东旅公司与日立公司合作，通过声音监测技术实现电动道叉转换器的自动化检查，以替代人工目视检查，降低设备故障停机时间。


<details>
  <summary>Details</summary>
Motivation: 解决设备检查中的人力节约需求和提供适当的预防性维护，寻找以物联网监测替代人工检查的方案。

Method: 在2019年在"NS"电动道叉转换器中安装摄像头和麦克风，通过声音信息进行道叉转换错误检测的方法，实现远程监控锁块状态。

Result: 获得了预期的测试结果，证明该方法能够实时检测设备故障，减少目视检查的需求。

Conclusion: 通过声音技术实现电子道叉转换器检查自动化的技术研究取得成功，为道路运营维护提供了有效的预防性维护方案。

Abstract: Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working to
replace human inspections with IoT-based monitoring. The purpose is
Labor-saving required for equipment inspections and provide appropriate
preventive maintenance. As an alternative to visual inspection, it has been
difficult to substitute electrical characteristic monitoring, and the
introduction of new high-performance sensors has been costly. In 2019, we
implemented cameras and microphones in an ``NS'' electric point machines to
reduce downtime from equipment failures, allowing for remote monitoring of
lock-piece conditions. This method for detecting turnout switching errors based
on sound information was proposed, and the expected test results were obtained.
The proposed method will make it possible to detect equipment failures in real
time, thereby reducing the need for visual inspections. This paper presents the
results of our technical studies aimed at automating the inspection of
electronic point machines using sound, specifically focusing on ``switch
sound'' beginning in 2019.

</details>


### [16] [Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System](https://arxiv.org/abs/2508.20983)
*Hashim Ali,Surya Subramani,Lekha Bollinani,Nithin Sai Adupa,Sali El-Loh,Hafiz Malik*

Main category: eess.AS

TL;DR: SAFE挑战赛评估合成语音检测，在三个任务中取得第二名：未修改音频、压缩处理音频和规避检测的清洗音频。采用AASIST架构结合WavLM大前端和RawBoost增强，使用多语言数据集训练。


<details>
  <summary>Details</summary>
Motivation: 系统评估合成语音检测在不同场景下的鲁棒性，包括原始音频、压缩处理音频和专门设计的规避检测音频，以应对日益复杂的深度伪造威胁。

Method: 基于AASIST架构，整合WavLM大型自监督学习前端和RawBoost数据增强技术，使用包含9种语言、70多个TTS系统的256,600个样本的多语言数据集进行训练。

Result: 在Task 1（未修改音频检测）和Task 3（清洗音频检测）中均获得第二名，展示了强大的泛化能力和鲁棒性。

Conclusion: 该方法在合成语音检测方面表现出色，特别是在处理复杂规避技术时仍能保持高检测性能，为深度伪造检测提供了有效的解决方案。

Abstract: The SAFE Challenge evaluates synthetic speech detection across three tasks:
unmodified audio, processed audio with compression artifacts, and laundered
audio designed to evade detection. We systematically explore self-supervised
learning (SSL) front-ends, training data compositions, and audio length
configurations for robust deepfake detection. Our AASIST-based approach
incorporates WavLM large frontend with RawBoost augmentation, trained on a
multilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS
systems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.
Through extensive experimentation with different SSL front-ends, three training
data versions, and two audio lengths, we achieved second place in both Task 1
(unmodified audio detection) and Task 3 (laundered audio detection),
demonstrating strong generalization and robustness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [17] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: MoTAS框架通过TTS数据增强和MoE特征选择机制，在ADReSSo数据集上实现了85.71%的阿尔茨海默病筛查准确率，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病早期筛查中数据有限和缺乏细粒度自适应特征选择的问题，提高筛查效率和模型泛化能力。

Method: 使用自动语音识别获取转录文本，通过文本到语音技术进行数据增强，采用混合专家机制动态选择最优的多模态特征进行融合分类。

Result: 在ADReSSo数据集上达到85.71%的准确率，消融实验验证了TTS增强和MoE机制各自对性能提升的贡献。

Conclusion: MoTAS在数据有限的实际场景中具有重要实用价值，为阿尔茨海默病的非侵入性语音筛查提供了有效解决方案。

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [18] [Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement](https://arxiv.org/abs/2508.20584)
*Mattias Cross,Anton Ragni*

Main category: cs.SD

TL;DR: 本文研究了概率路径的直线性对语音增强质量的影响，比较了弯曲路径（Schrodinger桥）和直线路径（条件流匹配）方法，发现直线路径能提升语音质量但需要多步推理，因此提出了一步推理解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的生成式语音增强方法使用弯曲概率路径来建模干净和噪声语音之间的映射，但弯曲路径的影响未知。机器学习研究表明直线路径更容易训练且泛化更好，因此需要量化路径直线性对语音增强质量的影响。

Method: 通过Schrodinger桥实验探索不同配置对路径直线性的影响，提出独立条件流匹配方法建模噪声和干净语音间的直线路径，并开发一步推理解决方案来解决多步推理需求。

Result: 实验表明时间独立方差比梯度对样本质量影响更大，条件流匹配改善了多个语音质量指标，但需要多步推理。一步推理方案成功解决了这个问题。

Conclusion: 直线时间独立概率路径相比弯曲时间依赖路径能改善生成式语音增强性能，时间独立方差对质量的影响大于梯度，一步推理方案有效解决了多步推理的需求。

Abstract: Current flow-based generative speech enhancement methods learn curved
probability paths which model a mapping between clean and noisy speech. Despite
impressive performance, the implications of curved probability paths are
unknown. Methods such as Schrodinger bridges focus on curved paths, where
time-dependent gradients and variance do not promote straight paths. Findings
in machine learning research suggest that straight paths, such as conditional
flow matching, are easier to train and offer better generalisation. In this
paper we quantify the effect of path straightness on speech enhancement
quality. We report experiments with the Schrodinger bridge, where we show that
certain configurations lead to straighter paths. Conversely, we propose
independent conditional flow-matching for speech enhancement, which models
straight paths between noisy and clean speech. We demonstrate empirically that
a time-independent variance has a greater effect on sample quality than the
gradient. Although conditional flow matching improves several speech quality
metrics, it requires multiple inference steps. We rectify this with a one-step
solution by inferring the trained flow-based model as if it was directly
predictive. Our work suggests that straighter time-independent probability
paths improve generative speech enhancement over curved time-dependent paths.

</details>


### [19] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: Amadeus是一个创新的符号音乐生成框架，采用自回归音符序列和双向离散扩散属性模型的两级架构，通过对比学习和注意力机制增强表示能力，在多个指标上显著超越现有SOTA模型，同时实现4倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐生成模型假设音符属性具有固定的单向时序依赖关系，但实验发现不同属性作为初始token都能获得相近性能，表明音符属性本质上是并发无序的集合而非时序依赖序列。

Method: 提出Amadeus框架：1）自回归模型处理音符序列；2）双向离散扩散模型处理属性；3）MLSDES策略通过对比学习增强音乐表示判别性；4）CIEM模块通过注意力机制强化音符潜在向量表示。

Result: 在无条件和文本条件生成任务上，Amadeus在多个指标上显著超越SOTA模型，同时实现至少4倍加速，并展示了无需训练即可进行细粒度音符属性控制的可行性。

Conclusion: Amadeus证明了音符属性作为并发无序集合的建模方式优于传统时序依赖假设，为符号音乐生成提供了新的有效框架，同时构建了最大的开源符号音乐数据集AMD来探索性能上限。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


### [20] [Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions](https://arxiv.org/abs/2508.20717)
*Ran Piao,Yuan Lu,Hareld Kemps,Tong Xia,Aaqib Saeed*

Main category: cs.SD

TL;DR: MARVEL是一个多任务学习框架，使用语音特征同时检测9种神经、呼吸和声音疾病，无需原始音频传输，在多项任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语音健康评估方法通常只关注单一疾病，未能充分利用语音中丰富的多维度信息，需要开发能够同时检测多种疾病的隐私保护型框架。

Method: 采用双分支架构，包含专用编码器和任务特定头部，共享通用声学骨干网络，实现跨条件知识转移，仅使用派生声学特征而不需要原始音频。

Result: 在Bridge2AI-Voice v2.0数据集上总体AUROC达到0.78，神经疾病检测表现优异（AUROC=0.89），阿尔茨海默病/轻度认知障碍检测AUROC达0.97，在9项任务中的7项超越最先进的自监督模型。

Conclusion: 该研究证明了单一统一模型可有效筛查多种疾病，为资源受限和远程医疗环境中的可部署语音诊断奠定了基础。

Abstract: Voice-based health assessment offers unprecedented opportunities for
scalable, non-invasive disease screening, yet existing approaches typically
focus on single conditions and fail to leverage the rich, multi-faceted
information embedded in speech. We present MARVEL (Multi-task Acoustic
Representations for Voice-based Health Analysis), a privacy-conscious multitask
learning framework that simultaneously detects nine distinct neurological,
respiratory, and voice disorders using only derived acoustic features,
eliminating the need for raw audio transmission. Our dual-branch architecture
employs specialized encoders with task-specific heads sharing a common acoustic
backbone, enabling effective cross-condition knowledge transfer. Evaluated on
the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC
of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),
particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).
Our framework consistently outperforms single-modal baselines by 5-19% and
surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while
correlation analysis reveals that the learned representations exhibit
meaningful similarities with established acoustic features, indicating that the
model's internal representations are consistent with clinically recognized
acoustic patterns. By demonstrating that a single unified model can effectively
screen for diverse conditions, this work establishes a foundation for
deployable voice-based diagnostics in resource-constrained and remote
healthcare settings.

</details>


### [21] [Speech Emotion Recognition via Entropy-Aware Score Selection](https://arxiv.org/abs/2508.20796)
*ChenYi Chua,JunKai Wong,Chengxin Chen,Xiaoxiao Miao*

Main category: cs.SD

TL;DR: 一种基于熵的多模态语音情感识别框架，通过熵感知分数选择结合语音和文本预测，在IEMOCAP和MSP-IMPROV数据集上得到了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 充分利用语音和文本信息进行情感识别，克服单一模态系统的信心限制问题，提高识别的准确性和可靠性。

Method: 使用wav2vec2.0音响模型和RoBERTa-XLM情感分析模型构建双流水线，通过Whisper-large-v3生成语音转写文本，采用基于熵和变异熵阈值的晚期分数融合策略，并将三个情感类别映射到四个情绪类别。

Result: 在IEMOCAP和MSP-IMPROV数据集上，该方法表现出了比传统单模态系统更好的性能，提供了实用而可靠的性能提升。

Conclusion: 该多模态框架通过熵感知分数选择策略，有效结合了语音和文本信息，为语音情感识别领域提供了一种有效的解决方案。

Abstract: In this paper, we propose a multimodal framework for speech emotion
recognition that leverages entropy-aware score selection to combine speech and
textual predictions. The proposed method integrates a primary pipeline that
consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that
consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions
generated via Whisper-large-v3. We propose a late score fusion approach based
on entropy and varentropy thresholds to overcome the confidence constraints of
primary pipeline predictions. A sentiment mapping strategy translates three
sentiment categories into four target emotion classes, enabling coherent
integration of multimodal predictions. The results on the IEMOCAP and
MSP-IMPROV datasets show that the proposed method offers a practical and
reliable enhancement over traditional single-modality systems.

</details>


### [22] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 该论文提出了大规模高质量语音识别数据集OLMoASR-Pool和OLMoASR-Mix，训练了从39M到1.5B参数的模型系列，在短语音和长语音识别任务上达到了与OpenAI Whisper相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 虽然训练数据的规模和质量提升带来了显著进步，但在语音识别领域其影响仍未得到充分探索。需要研究和开发健壮的零检验语音识别模型。

Method: 构建了300万小时英语音频和1700万转写文本的OLMoASR-Pool数据集，设计文本启发式筛选器移除低质量或错误转写数据，生成了100万小时高质量音频-转写对的OLMoASR-Mix数据集，用其训练了从39M到1.5B参数的模型系列。

Result: 在所有模型规模上，OLMoASR在短语音和长语音识别性能基准上达到了与OpenAI Whisper相当的平均性能。OLMoASR-medium.en在短语音和长语音识别任务上分别达到了12.8%和11.0%的词错误率，与参数数量相等的Whisper-medium.en模型的12.4%和10.5%词错误率相当。

Conclusion: 通过构建大规模高质量语音识别数据集和训练多规模模型，OLMoASR系统证明了数据质量对语音识别模型性能的重要影响，为健壮的零检验语音处理研究提供了重要资源。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


### [23] [SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization](https://arxiv.org/abs/2508.20885)
*Chien-Chun Wang,En-Lun Yu,Jeih-Weih Hung,Shih-Chieh Huang,Berlin Chen*

Main category: cs.SD

TL;DR: 基于Sinc提取器前端和二次差异排序损失的简洁粗策语音活动检测框架，在噪声环境下显著提升性能且参数量减少31%


<details>
  <summary>Details</summary>
Motivation: 现有语音活动检测方法在噪声环境下缺乏稳健性，并且帧级分类损失与评估指标耦合弱

Method: 结合Sinc-extractor前端（可学习带通滤波器提取抗噪谱特征）和二次差异排序损失（优化语音/非语音帧的对应分数排序）

Result: 在标准数据集上实验显示，AUROC和F2-Score显著提升，参数量仅为现有方法的69%

Conclusion: SincQDR-VAD框架通过结构优化和损失函数设计，实现了高效率和强抗噪性的语音活动检测

Abstract: Voice activity detection (VAD) is essential for speech-driven applications,
but remains far from perfect in noisy and resource-limited environments.
Existing methods often lack robustness to noise, and their frame-wise
classification losses are only loosely coupled with the evaluation metric of
VAD. To address these challenges, we propose SincQDR-VAD, a compact and robust
framework that combines a Sinc-extractor front-end with a novel quadratic
disparity ranking loss. The Sinc-extractor uses learnable bandpass filters to
capture noise-resistant spectral features, while the ranking loss optimizes the
pairwise score order between speech and non-speech frames to improve the area
under the receiver operating characteristic curve (AUROC). A series of
experiments conducted on representative benchmark datasets show that our
framework considerably improves both AUROC and F2-Score, while using only 69%
of the parameters compared to prior arts, confirming its efficiency and
practical viability.

</details>


### [24] [Learning Robust Spatial Representations from Binaural Audio through Feature Distillation](https://arxiv.org/abs/2508.20914)
*Holger Severin Bovbjerg,Jan Østergaard,Jesper Jensen,Shinji Watanabe,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 提出基于特征蒸馏的预训练方法，从双耳语音中学习鲁棒的空间表示，无需数据标签，用于提升噪声和混响环境中的声源定位性能


<details>
  <summary>Details</summary>
Motivation: 深度表示学习在音频任务中表现优异，但在多通道音频空间表示学习方面研究不足，需要探索无标签数据下的空间特征学习方法

Method: 使用特征蒸馏预训练框架：从干净双耳语音计算空间特征作为预测标签，通过神经网络从增强语音预测这些特征，预训练后使用编码器权重初始化DoA估计模型并进行微调

Result: 实验表明，经过微调的预训练模型在噪声和混响环境中的声源定位性能优于全监督模型和传统信号处理方法

Conclusion: 基于特征蒸馏的无监督预训练方法能有效学习空间表示，显著提升复杂声学环境中的声源定位精度

Abstract: Recently, deep representation learning has shown strong performance in
multiple audio tasks. However, its use for learning spatial representations
from multichannel audio is underexplored. We investigate the use of a
pretraining stage based on feature distillation to learn a robust spatial
representation of binaural speech without the need for data labels. In this
framework, spatial features are computed from clean binaural speech samples to
form prediction labels. These clean features are then predicted from
corresponding augmented speech using a neural network. After pretraining, we
throw away the spatial feature predictor and use the learned encoder weights to
initialize a DoA estimation model which we fine-tune for DoA estimation. Our
experiments demonstrate that the pretrained models show improved performance in
noisy and reverberant environments after fine-tuning for direction-of-arrival
estimation, when compared to fully supervised models and classic signal
processing methods.

</details>


### [25] [WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations](https://arxiv.org/abs/2508.20976)
*Jaeyeon Kim,Heeseung Yun,Sang Hoon Woo,Chao-Han Huck Yang,Gunhee Kim*

Main category: cs.SD

TL;DR: 提出了WoW-Bench基准测试，用于评估大型音频语言模型在低频听觉感知和认知方面的能力，特别是在海洋哺乳动物发声识别任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在低频听觉（如音高和时长检测）方面的能力尚未充分探索，而这对处理现实世界中分布外任务至关重要，模型需要基于细粒度声学线索来推理不熟悉的声音。

Method: 引入World-of-Whale基准测试（WoW-Bench），包含感知基准（分类新声音）和认知基准（基于Bloom分类法评估记忆、理解、应用和分析声音事件的能力），并引入干扰问题来验证模型是否真正通过听觉解决问题。

Result: 实验显示最先进的大型音频语言模型性能远低于人类水平，表明这些模型需要更强的听觉基础能力。

Conclusion: 当前的大型音频语言模型在低频听觉任务上表现不足，需要改进模型的听觉基础能力以更好地处理现实世界的音频理解任务。

Abstract: Large audio language models (LALMs) extend language understanding into the
auditory domain, yet their ability to perform low-level listening, such as
pitch and duration detection, remains underexplored. However, low-level
listening is critical for real-world, out-of-distribution tasks where models
must reason about unfamiliar sounds based on fine-grained acoustic cues. To
address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to
evaluate low-level auditory perception and cognition using marine mammal
vocalizations. WoW-bench is composed of a Perception benchmark for categorizing
novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess
the abilities to remember, understand, apply, and analyze sound events. For the
Cognition benchmark, we additionally introduce distractor questions to evaluate
whether models are truly solving problems through listening rather than relying
on other heuristics. Experiments with state-of-the-art LALMs show performance
far below human levels, indicating a need for stronger auditory grounding in
LALMs.

</details>

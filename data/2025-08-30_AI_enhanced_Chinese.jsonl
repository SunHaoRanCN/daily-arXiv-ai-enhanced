{"id": "2508.20513", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.20513", "abs": "https://arxiv.org/abs/2508.20513", "authors": ["Yongqi Shao", "Binxin Mei", "Cong Tan", "Hong Huo", "Tao Fang"], "title": "MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening", "comment": null, "summary": "Early screening for Alzheimer's Disease (AD) through speech presents a\npromising non-invasive approach. However, challenges such as limited data and\nthe lack of fine-grained, adaptive feature selection often hinder performance.\nTo address these issues, we propose MoTAS, a robust framework designed to\nenhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)\naugmentation to increase data volume and employs a Mixture of Experts (MoE)\nmechanism to improve multimodal feature selection, jointly enhancing model\ngeneralization. The process begins with automatic speech recognition (ASR) to\nobtain accurate transcriptions. TTS is then used to synthesize speech that\nenriches the dataset. After extracting acoustic and text embeddings, the MoE\nmechanism dynamically selects the most informative features, optimizing feature\nfusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS\nachieves a leading accuracy of 85.71\\%, outperforming existing baselines.\nAblation studies further validate the individual contributions of TTS\naugmentation and MoE in boosting classification performance. These findings\nhighlight the practical value of MoTAS in real-world AD screening scenarios,\nparticularly in data-limited settings.", "AI": {"tldr": "MoTAS\u6846\u67b6\u901a\u8fc7TTS\u6570\u636e\u589e\u5f3a\u548cMoE\u7279\u5f81\u9009\u62e9\u673a\u5236\uff0c\u5728ADReSSo\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8685.71%\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7b5b\u67e5\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u7b5b\u67e5\u4e2d\u6570\u636e\u6709\u9650\u548c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u7b5b\u67e5\u6548\u7387\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u83b7\u53d6\u8f6c\u5f55\u6587\u672c\uff0c\u901a\u8fc7\u6587\u672c\u5230\u8bed\u97f3\u6280\u672f\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u673a\u5236\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684\u591a\u6a21\u6001\u7279\u5f81\u8fdb\u884c\u878d\u5408\u5206\u7c7b\u3002", "result": "\u5728ADReSSo\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.71%\u7684\u51c6\u786e\u7387\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TTS\u589e\u5f3a\u548cMoE\u673a\u5236\u5404\u81ea\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u8d21\u732e\u3002", "conclusion": "MoTAS\u5728\u6570\u636e\u6709\u9650\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u975e\u4fb5\u5165\u6027\u8bed\u97f3\u7b5b\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20584", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20584", "abs": "https://arxiv.org/abs/2508.20584", "authors": ["Mattias Cross", "Anton Ragni"], "title": "Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement", "comment": "preprint, accepted", "summary": "Current flow-based generative speech enhancement methods learn curved\nprobability paths which model a mapping between clean and noisy speech. Despite\nimpressive performance, the implications of curved probability paths are\nunknown. Methods such as Schrodinger bridges focus on curved paths, where\ntime-dependent gradients and variance do not promote straight paths. Findings\nin machine learning research suggest that straight paths, such as conditional\nflow matching, are easier to train and offer better generalisation. In this\npaper we quantify the effect of path straightness on speech enhancement\nquality. We report experiments with the Schrodinger bridge, where we show that\ncertain configurations lead to straighter paths. Conversely, we propose\nindependent conditional flow-matching for speech enhancement, which models\nstraight paths between noisy and clean speech. We demonstrate empirically that\na time-independent variance has a greater effect on sample quality than the\ngradient. Although conditional flow matching improves several speech quality\nmetrics, it requires multiple inference steps. We rectify this with a one-step\nsolution by inferring the trained flow-based model as if it was directly\npredictive. Our work suggests that straighter time-independent probability\npaths improve generative speech enhancement over curved time-dependent paths.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6982\u7387\u8def\u5f84\u7684\u76f4\u7ebf\u6027\u5bf9\u8bed\u97f3\u589e\u5f3a\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u5f2f\u66f2\u8def\u5f84\uff08Schrodinger\u6865\uff09\u548c\u76f4\u7ebf\u8def\u5f84\uff08\u6761\u4ef6\u6d41\u5339\u914d\uff09\u65b9\u6cd5\uff0c\u53d1\u73b0\u76f4\u7ebf\u8def\u5f84\u80fd\u63d0\u5347\u8bed\u97f3\u8d28\u91cf\u4f46\u9700\u8981\u591a\u6b65\u63a8\u7406\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u6b65\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u4f7f\u7528\u5f2f\u66f2\u6982\u7387\u8def\u5f84\u6765\u5efa\u6a21\u5e72\u51c0\u548c\u566a\u58f0\u8bed\u97f3\u4e4b\u95f4\u7684\u6620\u5c04\uff0c\u4f46\u5f2f\u66f2\u8def\u5f84\u7684\u5f71\u54cd\u672a\u77e5\u3002\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8868\u660e\u76f4\u7ebf\u8def\u5f84\u66f4\u5bb9\u6613\u8bad\u7ec3\u4e14\u6cdb\u5316\u66f4\u597d\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5316\u8def\u5f84\u76f4\u7ebf\u6027\u5bf9\u8bed\u97f3\u589e\u5f3a\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7Schrodinger\u6865\u5b9e\u9a8c\u63a2\u7d22\u4e0d\u540c\u914d\u7f6e\u5bf9\u8def\u5f84\u76f4\u7ebf\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u72ec\u7acb\u6761\u4ef6\u6d41\u5339\u914d\u65b9\u6cd5\u5efa\u6a21\u566a\u58f0\u548c\u5e72\u51c0\u8bed\u97f3\u95f4\u7684\u76f4\u7ebf\u8def\u5f84\uff0c\u5e76\u5f00\u53d1\u4e00\u6b65\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u6765\u89e3\u51b3\u591a\u6b65\u63a8\u7406\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65f6\u95f4\u72ec\u7acb\u65b9\u5dee\u6bd4\u68af\u5ea6\u5bf9\u6837\u672c\u8d28\u91cf\u5f71\u54cd\u66f4\u5927\uff0c\u6761\u4ef6\u6d41\u5339\u914d\u6539\u5584\u4e86\u591a\u4e2a\u8bed\u97f3\u8d28\u91cf\u6307\u6807\uff0c\u4f46\u9700\u8981\u591a\u6b65\u63a8\u7406\u3002\u4e00\u6b65\u63a8\u7406\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002", "conclusion": "\u76f4\u7ebf\u65f6\u95f4\u72ec\u7acb\u6982\u7387\u8def\u5f84\u76f8\u6bd4\u5f2f\u66f2\u65f6\u95f4\u4f9d\u8d56\u8def\u5f84\u80fd\u6539\u5584\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u6027\u80fd\uff0c\u65f6\u95f4\u72ec\u7acb\u65b9\u5dee\u5bf9\u8d28\u91cf\u7684\u5f71\u54cd\u5927\u4e8e\u68af\u5ea6\uff0c\u4e00\u6b65\u63a8\u7406\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6b65\u63a8\u7406\u7684\u9700\u6c42\u3002"}}
{"id": "2508.20665", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.20665", "abs": "https://arxiv.org/abs/2508.20665", "authors": ["Hongju Su", "Ke Li", "Lan Yang", "Honggang Zhang", "Yi-Zhe Song"], "title": "Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music", "comment": "Under review", "summary": "Existing state-of-the-art symbolic music generation models predominantly\nadopt autoregressive or hierarchical autoregressive architectures, modelling\nsymbolic music as a sequence of attribute tokens with unidirectional temporal\ndependencies, under the assumption of a fixed, strict dependency structure\namong these attributes. However, we observe that using different attributes as\nthe initial token in these models leads to comparable performance. This\nsuggests that the attributes of a musical note are, in essence, a concurrent\nand unordered set, rather than a temporally dependent sequence. Based on this\ninsight, we introduce Amadeus, a novel symbolic music generation framework.\nAmadeus adopts a two-level architecture: an autoregressive model for note\nsequences and a bidirectional discrete diffusion model for attributes. To\nenhance performance, we propose Music Latent Space Discriminability Enhancement\nStrategy(MLSDES), incorporating contrastive learning constraints that amplify\ndiscriminability of intermediate music representations. The Conditional\nInformation Enhancement Module (CIEM) simultaneously strengthens note latent\nvector representation via attention mechanisms, enabling more precise note\ndecoding. We conduct extensive experiments on unconditional and\ntext-conditioned generation tasks. Amadeus significantly outperforms SOTA\nmodels across multiple metrics while achieving at least 4$\\times$ speed-up.\nFurthermore, we demonstrate training-free, fine-grained note attribute control\nfeasibility using our model. To explore the upper performance bound of the\nAmadeus architecture, we compile the largest open-source symbolic music dataset\nto date, AMD (Amadeus MIDI Dataset), supporting both pre-training and\nfine-tuning.", "AI": {"tldr": "Amadeus\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u56de\u5f52\u97f3\u7b26\u5e8f\u5217\u548c\u53cc\u5411\u79bb\u6563\u6269\u6563\u5c5e\u6027\u6a21\u578b\u7684\u4e24\u7ea7\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u8868\u793a\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709SOTA\u6a21\u578b\uff0c\u540c\u65f6\u5b9e\u73b04\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u6a21\u578b\u5047\u8bbe\u97f3\u7b26\u5c5e\u6027\u5177\u6709\u56fa\u5b9a\u7684\u5355\u5411\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u5c5e\u6027\u4f5c\u4e3a\u521d\u59cbtoken\u90fd\u80fd\u83b7\u5f97\u76f8\u8fd1\u6027\u80fd\uff0c\u8868\u660e\u97f3\u7b26\u5c5e\u6027\u672c\u8d28\u4e0a\u662f\u5e76\u53d1\u65e0\u5e8f\u7684\u96c6\u5408\u800c\u975e\u65f6\u5e8f\u4f9d\u8d56\u5e8f\u5217\u3002", "method": "\u63d0\u51faAmadeus\u6846\u67b6\uff1a1\uff09\u81ea\u56de\u5f52\u6a21\u578b\u5904\u7406\u97f3\u7b26\u5e8f\u5217\uff1b2\uff09\u53cc\u5411\u79bb\u6563\u6269\u6563\u6a21\u578b\u5904\u7406\u5c5e\u6027\uff1b3\uff09MLSDES\u7b56\u7565\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u97f3\u4e50\u8868\u793a\u5224\u522b\u6027\uff1b4\uff09CIEM\u6a21\u5757\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5f3a\u5316\u97f3\u7b26\u6f5c\u5728\u5411\u91cf\u8868\u793a\u3002", "result": "\u5728\u65e0\u6761\u4ef6\u548c\u6587\u672c\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e0a\uff0cAmadeus\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8aSOTA\u6a21\u578b\uff0c\u540c\u65f6\u5b9e\u73b0\u81f3\u5c114\u500d\u52a0\u901f\uff0c\u5e76\u5c55\u793a\u4e86\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8fdb\u884c\u7ec6\u7c92\u5ea6\u97f3\u7b26\u5c5e\u6027\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Amadeus\u8bc1\u660e\u4e86\u97f3\u7b26\u5c5e\u6027\u4f5c\u4e3a\u5e76\u53d1\u65e0\u5e8f\u96c6\u5408\u7684\u5efa\u6a21\u65b9\u5f0f\u4f18\u4e8e\u4f20\u7edf\u65f6\u5e8f\u4f9d\u8d56\u5047\u8bbe\uff0c\u4e3a\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u6846\u67b6\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u6700\u5927\u7684\u5f00\u6e90\u7b26\u53f7\u97f3\u4e50\u6570\u636e\u96c6AMD\u6765\u63a2\u7d22\u6027\u80fd\u4e0a\u9650\u3002"}}
{"id": "2508.20717", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20717", "abs": "https://arxiv.org/abs/2508.20717", "authors": ["Ran Piao", "Yuan Lu", "Hareld Kemps", "Tong Xia", "Aaqib Saeed"], "title": "Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions", "comment": null, "summary": "Voice-based health assessment offers unprecedented opportunities for\nscalable, non-invasive disease screening, yet existing approaches typically\nfocus on single conditions and fail to leverage the rich, multi-faceted\ninformation embedded in speech. We present MARVEL (Multi-task Acoustic\nRepresentations for Voice-based Health Analysis), a privacy-conscious multitask\nlearning framework that simultaneously detects nine distinct neurological,\nrespiratory, and voice disorders using only derived acoustic features,\neliminating the need for raw audio transmission. Our dual-branch architecture\nemploys specialized encoders with task-specific heads sharing a common acoustic\nbackbone, enabling effective cross-condition knowledge transfer. Evaluated on\nthe large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC\nof 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),\nparticularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).\nOur framework consistently outperforms single-modal baselines by 5-19% and\nsurpasses state-of-the-art self-supervised models on 7 of 9 tasks, while\ncorrelation analysis reveals that the learned representations exhibit\nmeaningful similarities with established acoustic features, indicating that the\nmodel's internal representations are consistent with clinically recognized\nacoustic patterns. By demonstrating that a single unified model can effectively\nscreen for diverse conditions, this work establishes a foundation for\ndeployable voice-based diagnostics in resource-constrained and remote\nhealthcare settings.", "AI": {"tldr": "MARVEL\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u8bed\u97f3\u7279\u5f81\u540c\u65f6\u68c0\u6d4b9\u79cd\u795e\u7ecf\u3001\u547c\u5438\u548c\u58f0\u97f3\u75be\u75c5\uff0c\u65e0\u9700\u539f\u59cb\u97f3\u9891\u4f20\u8f93\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5065\u5eb7\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u75be\u75c5\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8bed\u97f3\u4e2d\u4e30\u5bcc\u7684\u591a\u7ef4\u5ea6\u4fe1\u606f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u68c0\u6d4b\u591a\u79cd\u75be\u75c5\u7684\u9690\u79c1\u4fdd\u62a4\u578b\u6846\u67b6\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u5305\u542b\u4e13\u7528\u7f16\u7801\u5668\u548c\u4efb\u52a1\u7279\u5b9a\u5934\u90e8\uff0c\u5171\u4eab\u901a\u7528\u58f0\u5b66\u9aa8\u5e72\u7f51\u7edc\uff0c\u5b9e\u73b0\u8de8\u6761\u4ef6\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4ec5\u4f7f\u7528\u6d3e\u751f\u58f0\u5b66\u7279\u5f81\u800c\u4e0d\u9700\u8981\u539f\u59cb\u97f3\u9891\u3002", "result": "\u5728Bridge2AI-Voice v2.0\u6570\u636e\u96c6\u4e0a\u603b\u4f53AUROC\u8fbe\u52300.78\uff0c\u795e\u7ecf\u75be\u75c5\u68c0\u6d4b\u8868\u73b0\u4f18\u5f02\uff08AUROC=0.89\uff09\uff0c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5/\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4bAUROC\u8fbe0.97\uff0c\u57289\u9879\u4efb\u52a1\u4e2d\u76847\u9879\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u53ef\u6709\u6548\u7b5b\u67e5\u591a\u79cd\u75be\u75c5\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u548c\u8fdc\u7a0b\u533b\u7597\u73af\u5883\u4e2d\u7684\u53ef\u90e8\u7f72\u8bed\u97f3\u8bca\u65ad\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.20277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20277", "abs": "https://arxiv.org/abs/2508.20277", "authors": ["Xiaoyan Ma", "Shahryar Zehtabi", "Taejoon Kim", "Christopher G. Brinton"], "title": "Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels", "comment": null, "summary": "This paper investigates an OFDM-based over-the-air federated learning\n(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles\n(UAVs), transmit local machine learning (ML) models to a central parameter\nserver (PS) for global model aggregation. The high mobility of local devices\nresults in imperfect channel estimation, leading to a misalignment problem,\ni.e., the model parameters transmitted from different local devices do not\narrive at the central PS simultaneously. Moreover, the mobility introduces\ntime-varying uploading channels, which further complicates the aggregation\nprocess. All these factors collectively cause distortions in the OTA-FL\ntraining process which are underexplored. To quantify these effects, we first\nderive a closed-form expression for a single-round global model update in terms\nof these channel imperfections. We then extend our analysis to capture multiple\nrounds of global updates, yielding a bound on the accumulated error in OTA-FL.\nWe validate our theoretical results via extensive numerical simulations, which\ncorroborate our derived analysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u57fa\u4e8eOFDM\u7684\u7a7a\u4e2d\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u5206\u6790\u4e86\u79fb\u52a8\u8bbe\u5907\u9ad8\u901f\u79fb\u52a8\u5bfc\u81f4\u7684\u4fe1\u9053\u4f30\u8ba1\u4e0d\u5b8c\u7f8e\u548c\u6a21\u578b\u53c2\u6570\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u5355\u8f6e\u548c\u591a\u8f6e\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u548c\u8bef\u5dee\u754c\u9650\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u7684\u9ad8\u79fb\u52a8\u6027\u5bfc\u81f4\u4fe1\u9053\u4f30\u8ba1\u4e0d\u5b8c\u7f8e\uff0c\u9020\u6210\u6a21\u578b\u53c2\u6570\u4f20\u8f93\u4e0d\u540c\u6b65\u548c\u65f6\u53d8\u4e0a\u4f20\u4fe1\u9053\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5171\u540c\u5bfc\u81f4OTA-FL\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5931\u771f\u95ee\u9898\uff0c\u76ee\u524d\u5bf9\u6b64\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u5355\u8f6e\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u7136\u540e\u6269\u5c55\u5230\u591a\u8f6e\u5168\u5c40\u66f4\u65b0\u5206\u6790\uff0c\u5f97\u5230\u4e86OTA-FL\u7d2f\u79ef\u8bef\u5dee\u7684\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u83b7\u5f97\u4e86\u8003\u8651\u4fe1\u9053\u4e0d\u5b8c\u7f8e\u6027\u7684\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u95ed\u5f0f\u8868\u8fbe\u5f0f\u548c\u7d2f\u79ef\u8bef\u5dee\u754c\u9650\uff0c\u6570\u503c\u6a21\u62df\u7ed3\u679c\u8bc1\u5b9e\u4e86\u7406\u8bba\u5206\u6790\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u79fb\u52a8\u6027\u5f15\u8d77\u7684\u4fe1\u9053\u4e0d\u5b8c\u7f8e\u6027\u5bf9OTA-FL\u7cfb\u7edf\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63d0\u51fa\u7684\u5206\u6790\u6846\u67b6\u80fd\u591f\u6709\u6548\u91cf\u5316\u8fd9\u4e9b\u5f71\u54cd\uff0c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2508.20273", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20273", "abs": "https://arxiv.org/abs/2508.20273", "authors": ["Yujin Kim", "Richa Namballa", "Magdalena Fuentes"], "title": "Live Vocal Extraction from K-pop Performances", "comment": "2 pages + references, 1 figure, Extended Abstracts for the\n  Late-Breaking Demo Session of the 26th International Society for Music\n  Information Retrieval Conference", "summary": "K-pop's global success is fueled by its dynamic performances and vibrant fan\nengagement. Inspired by K-pop fan culture, we propose a methodology for\nautomatically extracting live vocals from performances. We use a combination of\nsource separation, cross-correlation, and amplitude scaling to automatically\nremove pre-recorded vocals and instrumentals from a live performance. Our\npreliminary work introduces the task of live vocal separation and provides a\nfoundation for future research in this topic.", "AI": {"tldr": "\u81ea\u52a8\u4eceK-pop\u73b0\u573a\u8868\u6f14\u4e2d\u63d0\u53d6\u73b0\u573a\u5531\u97f3\u7684\u65b9\u6cd5", "motivation": "\u53d7K-pop\u7c89\u4e1d\u6587\u5316\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u81ea\u52a8\u5206\u79bb\u73b0\u573a\u5531\u97f3\u7684\u6280\u672f", "method": "\u7ed3\u5408\u4e86\u6e90\u5206\u79bb\u3001\u76f8\u5173\u6027\u5206\u6790\u548c\u5e45\u5ea6\u8c03\u6574\u6280\u672f\uff0c\u81ea\u52a8\u79fb\u9664\u9884\u5f55\u5531\u97f3\u548c\u4e50\u5668\u97f3", "result": "\u63d0\u51fa\u4e86\u73b0\u573a\u5531\u97f3\u5206\u79bb\u7684\u65b0\u4efb\u52a1\uff0c\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u5960\u5b9a\u57fa\u7840", "conclusion": "\u8fd9\u9879\u9884\u7814\u5de5\u4f5c\u4e3a\u73b0\u573a\u5531\u97f3\u5206\u79bb\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8d77\u70b9"}}
{"id": "2508.20796", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20796", "abs": "https://arxiv.org/abs/2508.20796", "authors": ["ChenYi Chua", "JunKai Wong", "Chengxin Chen", "Xiaoxiao Miao"], "title": "Speech Emotion Recognition via Entropy-Aware Score Selection", "comment": "The paper has been accepted by APCIPA ASC 2025", "summary": "In this paper, we propose a multimodal framework for speech emotion\nrecognition that leverages entropy-aware score selection to combine speech and\ntextual predictions. The proposed method integrates a primary pipeline that\nconsists of an acoustic model based on wav2vec2.0 and a secondary pipeline that\nconsists of a sentiment analysis model using RoBERTa-XLM, with transcriptions\ngenerated via Whisper-large-v3. We propose a late score fusion approach based\non entropy and varentropy thresholds to overcome the confidence constraints of\nprimary pipeline predictions. A sentiment mapping strategy translates three\nsentiment categories into four target emotion classes, enabling coherent\nintegration of multimodal predictions. The results on the IEMOCAP and\nMSP-IMPROV datasets show that the proposed method offers a practical and\nreliable enhancement over traditional single-modality systems.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u591a\u6a21\u6001\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u611f\u77e5\u5206\u6570\u9009\u62e9\u7ed3\u5408\u8bed\u97f3\u548c\u6587\u672c\u9884\u6d4b\uff0c\u5728IEMOCAP\u548cMSP-IMPROV\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5145\u5206\u5229\u7528\u8bed\u97f3\u548c\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\uff0c\u514b\u670d\u5355\u4e00\u6a21\u6001\u7cfb\u7edf\u7684\u4fe1\u5fc3\u9650\u5236\u95ee\u9898\uff0c\u63d0\u9ad8\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528wav2vec2.0\u97f3\u54cd\u6a21\u578b\u548cRoBERTa-XLM\u60c5\u611f\u5206\u6790\u6a21\u578b\u6784\u5efa\u53cc\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7Whisper-large-v3\u751f\u6210\u8bed\u97f3\u8f6c\u5199\u6587\u672c\uff0c\u91c7\u7528\u57fa\u4e8e\u71b5\u548c\u53d8\u5f02\u71b5\u9608\u503c\u7684\u665a\u671f\u5206\u6570\u878d\u5408\u7b56\u7565\uff0c\u5e76\u5c06\u4e09\u4e2a\u60c5\u611f\u7c7b\u522b\u6620\u5c04\u5230\u56db\u4e2a\u60c5\u7eea\u7c7b\u522b\u3002", "result": "\u5728IEMOCAP\u548cMSP-IMPROV\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u4e86\u6bd4\u4f20\u7edf\u5355\u6a21\u6001\u7cfb\u7edf\u66f4\u597d\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u800c\u53ef\u9760\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u6846\u67b6\u901a\u8fc7\u71b5\u611f\u77e5\u5206\u6570\u9009\u62e9\u7b56\u7565\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u8bed\u97f3\u548c\u6587\u672c\u4fe1\u606f\uff0c\u4e3a\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20531", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20531", "abs": "https://arxiv.org/abs/2508.20531", "authors": ["Chaoying Huang", "Wen Chen", "Qingqing Wu", "Xusheng Zhu", "Zhendong Li", "Ying Wang", "Jinhong Yuan"], "title": "Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design", "comment": null, "summary": "This paper proposes a novel dual-intelligent reflecting surface (IRS) aided\ninterference-limited simultaneous wireless information and power transfer\n(SWIPT) system with independent power splitting (PS), where each receiving\nantenna applies different PS factors to offer an advantageous trade-off between\nthe useful information and harvested energy. We separately establish the near-\nand hybrid-field channel models for IRS-reflected links to evaluate the\nperformance gain more precisely and practically. Specifically, we formulate an\noptimization problem of maximizing the harvested power by jointly optimizing\ndual-IRS phase shifts, independent PS ratio, and receive beamforming vector in\nboth near- and hybrid-field cases. In the near-field case, the alternating\noptimization algorithm is proposed to solve the non-convex problem by applying\nthe Lagrange duality method and the difference-of-convex (DC) programming. In\nthe hybrid-field case, we first present an interesting result that the\nAP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which\nallows the optimization problem to be transformed into a convex one. Then, we\nderive the asymptotic performance of the combined channel gains in closed-form\nand analyze the characteristics of the dual-IRS. Numerical results validate our\nanalysis and indicate the performance gains of the proposed scheme that\ndual-IRS-aided SWIPT with independent PS over other benchmark schemes.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u667a\u80fd\u53cd\u5c04\u8868\u9762\uff08IRS\uff09\u52a9\u529b\u5e72\u6270\u9650\u5236\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\uff08SWIPT\uff09\u7cfb\u7edf\uff0c\u91c7\u7528\u72ec\u7acb\u529f\u7387\u5206\u914d\u7b56\u7565\uff0c\u5728\u8fd1\u573a\u548c\u6df7\u5408\u573a\u60c5\u51b5\u4e0b\u4f18\u5316\u6536\u76ca\u529f\u7387\u548c\u4f20\u8f93\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u5e72\u6270\u9650\u5236\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u4fe1\u606f\u4f20\u8f93\u4e0e\u80fd\u91cf\u6536\u96c6\u7684\u6743\u8861\uff0c\u5e76\u8003\u8651\u5b9e\u9645\u901a\u4fe1\u573a\u666f\u4e2d\u7684\u8fd1\u573a\u548c\u6df7\u5408\u573a\u6a21\u578b\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53ccIRS\u7ed3\u6784\u548c\u72ec\u7acb\u529f\u7387\u5206\u914d\uff0c\u5efa\u7acb\u8fd1\u573a\u548c\u6df7\u5408\u573a\u901a\u9053\u6a21\u578b\u3002\u5728\u8fd1\u573a\u60c5\u51b5\u4e0b\u4f7f\u7528\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff08\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u65b9\u6cd5\u548cDC\u89c4\u5212\uff09\uff0c\u5728\u6df7\u5408\u573a\u60c5\u51b5\u4e0b\u5229\u7528\u901a\u9053\u589e\u76ca\u4e0eIRS\u76f8\u4f4d\u65e0\u5173\u7684\u7279\u6027\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5206\u6790\u7684\u6b63\u786e\u6027\uff0c\u5e76\u663e\u793a\u6240\u63d0\u65b9\u6848\u5728\u6536\u76ca\u529f\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u90fd\u8d85\u8fc7\u4e86\u5176\u4ed6\u5bf9\u7167\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u53ccIRS\u52a9\u529bSWIPT\u7cfb\u7edf\u4ee5\u53ca\u72ec\u7acb\u529f\u7387\u5206\u914d\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8fd1\u573a\u548c\u6df7\u5408\u573a\u901a\u4fe1\u573a\u666f\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2508.20474", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20474", "abs": "https://arxiv.org/abs/2508.20474", "authors": ["Muhammad Shakeel", "Yui Sudo", "Yifan Peng", "Chyi-Jiunn Lin", "Shinji Watanabe"], "title": "Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder", "comment": "Accepted to IEEE ASRU 2025", "summary": "This paper presents a unified multi-speaker encoder (UME), a novel\narchitecture that jointly learns representations for speaker diarization (SD),\nspeech separation (SS), and multi-speaker automatic speech recognition (ASR)\ntasks using a shared speech foundational encoder. We leverage the hidden\nrepresentations from multiple layers of UME as a residual weighted-sum encoding\n(RWSE) to effectively use information from different semantic levels,\ncontributing to bottom-up alignment between tasks. This joint training approach\ncaptures the inherent interdependencies among the tasks, enhancing overall\nperformance on overlapping speech data. Our evaluations demonstrate that UME\nsubstantially improves over the single-task baselines dedicated to SD, SS, and\nmulti-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms\nthe previous studies, achieving diarization error rates of 1.37% and 2.29% on\nLibri2Mix and Libri3Mix evaluation sets, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668(UME)\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u8bed\u97f3\u57fa\u7840\u7f16\u7801\u5668\u8054\u5408\u5b66\u4e60\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u3001\u8bed\u97f3\u5206\u79bb\u548c\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\uff0c\u5229\u7528\u6b8b\u5dee\u52a0\u6743\u6c42\u548c\u7f16\u7801\u6709\u6548\u5229\u7528\u4e0d\u540c\u8bed\u4e49\u5c42\u7ea7\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u53e0\u8bed\u97f3\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u3001\u8bed\u97f3\u5206\u79bb\u548c\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u901a\u5e38\u72ec\u7acb\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u91cd\u53e0\u8bed\u97f3\u65f6\u5b58\u5728\u5185\u5728\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u8fd9\u4e9b\u4efb\u52a1\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u67b6\u6784\u6765\u8054\u5408\u5b66\u4e60\u8fd9\u4e9b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u591a\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668(UME)\u67b6\u6784\uff0c\u4f7f\u7528\u5171\u4eab\u8bed\u97f3\u57fa\u7840\u7f16\u7801\u5668\u8054\u5408\u8bad\u7ec3\u4e09\u4e2a\u4efb\u52a1\u3002\u91c7\u7528\u6b8b\u5dee\u52a0\u6743\u6c42\u548c\u7f16\u7801(RWSE)\u6280\u672f\uff0c\u4ece\u7f16\u7801\u5668\u7684\u591a\u4e2a\u9690\u85cf\u5c42\u63d0\u53d6\u8868\u793a\uff0c\u6709\u6548\u5229\u7528\u4e0d\u540c\u8bed\u4e49\u5c42\u7ea7\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u4efb\u52a1\u95f4\u7684\u81ea\u5e95\u5411\u4e0a\u5bf9\u9f50\u3002", "result": "\u5728LibriMix\u8bc4\u4f30\u96c6\u4e0a\uff0cUME\u663e\u8457\u4f18\u4e8e\u4e13\u95e8\u9488\u5bf9\u5355\u4e2a\u4efb\u52a1\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4efb\u52a1\u4e0a\uff0cUME\u5728Libri2Mix\u548cLibri3Mix\u8bc4\u4f30\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e861.37%\u548c2.29%\u7684\u65e5\u5fd7\u9519\u8bef\u7387\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u7814\u7a76\u6210\u679c\u3002", "conclusion": "UME\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u591a\u4e2a\u76f8\u5173\u4efb\u52a1\uff0c\u6210\u529f\u6355\u83b7\u4e86\u4efb\u52a1\u95f4\u7684\u5185\u5728\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5728\u5904\u7406\u91cd\u53e0\u8bed\u97f3\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20869", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.20869", "abs": "https://arxiv.org/abs/2508.20869", "authors": ["Huong Ngo", "Matt Deitke", "Martijn Bartelds", "Sarah Pratt", "Josh Gardner", "Matt Jordan", "Ludwig Schmidt"], "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models", "comment": "17 pages, 7 figures", "summary": "Improvements in training data scale and quality have led to significant\nadvances, yet its influence in speech recognition remains underexplored. In\nthis paper, we present a large-scale dataset, OLMoASR-Pool, and series of\nmodels, OLMoASR, to study and develop robust zero-shot speech recognition\nmodels. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio\nand 17M transcripts, we design text heuristic filters to remove low-quality or\nmistranscribed data. Our curation pipeline produces a new dataset containing 1M\nhours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use\nOLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M\n(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR\nachieves comparable average performance to OpenAI's Whisper on short and\nlong-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a\n12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest\nEnglish-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and\nlong-form recognition respectively (at equivalent parameter count).\nOLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will\nbe made publicly available to further research on robust speech processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6OLMoASR-Pool\u548cOLMoASR-Mix\uff0c\u8bad\u7ec3\u4e86\u4ece39M\u52301.5B\u53c2\u6570\u7684\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u77ed\u8bed\u97f3\u548c\u957f\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e0eOpenAI Whisper\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u867d\u7136\u8bad\u7ec3\u6570\u636e\u7684\u89c4\u6a21\u548c\u8d28\u91cf\u63d0\u5347\u5e26\u6765\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5728\u8bed\u97f3\u8bc6\u522b\u9886\u57df\u5176\u5f71\u54cd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u7814\u7a76\u548c\u5f00\u53d1\u5065\u58ee\u7684\u96f6\u68c0\u9a8c\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u3002", "method": "\u6784\u5efa\u4e86300\u4e07\u5c0f\u65f6\u82f1\u8bed\u97f3\u9891\u548c1700\u4e07\u8f6c\u5199\u6587\u672c\u7684OLMoASR-Pool\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u6587\u672c\u542f\u53d1\u5f0f\u7b5b\u9009\u5668\u79fb\u9664\u4f4e\u8d28\u91cf\u6216\u9519\u8bef\u8f6c\u5199\u6570\u636e\uff0c\u751f\u6210\u4e86100\u4e07\u5c0f\u65f6\u9ad8\u8d28\u91cf\u97f3\u9891-\u8f6c\u5199\u5bf9\u7684OLMoASR-Mix\u6570\u636e\u96c6\uff0c\u7528\u5176\u8bad\u7ec3\u4e86\u4ece39M\u52301.5B\u53c2\u6570\u7684\u6a21\u578b\u7cfb\u5217\u3002", "result": "\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e0a\uff0cOLMoASR\u5728\u77ed\u8bed\u97f3\u548c\u957f\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u4e0eOpenAI Whisper\u76f8\u5f53\u7684\u5e73\u5747\u6027\u80fd\u3002OLMoASR-medium.en\u5728\u77ed\u8bed\u97f3\u548c\u957f\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u5230\u4e8612.8%\u548c11.0%\u7684\u8bcd\u9519\u8bef\u7387\uff0c\u4e0e\u53c2\u6570\u6570\u91cf\u76f8\u7b49\u7684Whisper-medium.en\u6a21\u578b\u768412.4%\u548c10.5%\u8bcd\u9519\u8bef\u7387\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u591a\u89c4\u6a21\u6a21\u578b\uff0cOLMoASR\u7cfb\u7edf\u8bc1\u660e\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u4e3a\u5065\u58ee\u7684\u96f6\u68c0\u9a8c\u8bed\u97f3\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2508.20535", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20535", "abs": "https://arxiv.org/abs/2508.20535", "authors": ["Annika Stiehl", "Nicolas Weeger", "Christian Uhl", "Dominic Bechtold", "Nicole Ille", "Stefan Gei\u00dfels\u00f6der"], "title": "Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders", "comment": "\\c{opyright} 2025 IEEE. Accepted in 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  2025", "summary": "Epilepsy is one of the most common neurological disorders. This disease\nrequires reliable and efficient seizure detection methods.\nElectroencephalography (EEG) is the gold standard for seizure monitoring, but\nits manual analysis is a time-consuming task that requires expert knowledge. In\naddition, there are no well-defined features that allow fully automated\nanalysis. Existing deep learning-based approaches struggle to achieve high\nsensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack\nconsistency in the optimal EEG input representation, whether in the time or\nfrequency domain. To address these issues, we propose a Deep Convolutional\nAutoencoder (DCAE) to extract low-dimensional latent representations that\npreserve essential EEG signal features. The ability of the model to preserve\nrelevant information was evaluated by comparing reconstruction errors based on\nboth time series and frequency-domain representations. Several autoencoders\nwith different loss functions based on time and frequency were trained and\nevaluated to determine their effectiveness in reconstructing EEG features. Our\nresults show that the DCAE model taking both time series and frequency losses\ninto account achieved the best reconstruction performance. This indicates that\nDeep Neural Networks with a single representation might not preserve the\nrelevant signal properties. This work provides insight into how deep learning\nmodels process EEG data and examines whether frequency information is captured\nwhen time series signals are used as input.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5377\u79ef\u81ea\u7f16\u7801\u5668(DCAE)\u7684\u766b\u75eb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u635f\u5931\u51fd\u6570\u6765\u63d0\u53d6EEG\u4fe1\u53f7\u7684\u4f4e\u7ef4\u8868\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u654f\u611f\u6027\u548c\u8bef\u62a5\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u766b\u75eb\u68c0\u6d4b\u9700\u8981\u53ef\u9760\u9ad8\u6548\u7684EEG\u5206\u6790\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u654f\u611f\u6027\u548c\u4f4e\u8bef\u62a5\u7387\uff0c\u4e14\u5728\u65f6\u57df\u6216\u9891\u57df\u8f93\u5165\u8868\u5f81\u9009\u62e9\u4e0a\u7f3a\u4e4f\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u81ea\u7f16\u7801\u5668\u63d0\u53d6EEG\u4fe1\u53f7\u7684\u4f4e\u7ef4\u6f5c\u5728\u8868\u5f81\uff0c\u901a\u8fc7\u6bd4\u8f83\u57fa\u4e8e\u65f6\u57df\u548c\u9891\u57df\u7684\u91cd\u6784\u8bef\u5dee\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8bad\u7ec3\u4e86\u591a\u79cd\u5177\u6709\u4e0d\u540c\u635f\u5931\u51fd\u6570\u7684\u81ea\u7f16\u7801\u5668\u3002", "result": "\u540c\u65f6\u8003\u8651\u65f6\u57df\u548c\u9891\u57df\u635f\u5931\u7684DCAE\u6a21\u578b\u83b7\u5f97\u4e86\u6700\u4f73\u91cd\u6784\u6027\u80fd\uff0c\u8868\u660e\u5355\u4e00\u8868\u5f81\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u65e0\u6cd5\u4fdd\u7559\u6240\u6709\u76f8\u5173\u4fe1\u53f7\u7279\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5904\u7406EEG\u6570\u636e\u7684\u65b9\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u65f6\u57df\u4fe1\u53f7\u8f93\u5165\u65f6\u9891\u57df\u4fe1\u606f\u662f\u5426\u88ab\u6709\u6548\u6355\u83b7\uff0c\u4e3a\u766b\u75eb\u81ea\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.20660", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20660", "abs": "https://arxiv.org/abs/2508.20660", "authors": ["Ruifan Deng", "Yitian Gong", "Qinghui Gao", "Luozhijie Jin", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "title": "CodecBench: A Comprehensive Benchmark for Acoustic and Semantic Evaluation", "comment": null, "summary": "With the rise of multimodal large language models (LLMs), audio codec plays\nan increasingly vital role in encoding audio into discrete tokens, enabling\nintegration of audio into text-based LLMs. Current audio codec captures two\ntypes of information: acoustic and semantic. As audio codec is applied to\ndiverse scenarios in speech language model , it needs to model increasingly\ncomplex information and adapt to varied contexts, such as scenarios with\nmultiple speakers, background noise, or richer paralinguistic information.\nHowever, existing codec's own evaluation has been limited by simplistic metrics\nand scenarios, and existing benchmarks for audio codec are not designed for\ncomplex application scenarios, which limits the assessment performance on\ncomplex datasets for acoustic and semantic capabilities. We introduce\nCodecBench, a comprehensive evaluation dataset to assess audio codec\nperformance from both acoustic and semantic perspectives across four data\ndomains. Through this benchmark, we aim to identify current limitations,\nhighlight future research directions, and foster advances in the development of\naudio codec. The codes are available at https://github.com/RayYuki/CodecBench.", "AI": {"tldr": "CodecBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u97f3\u9891\u7f16\u89e3\u7801\u5668\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u58f0\u5b66\u548c\u8bed\u4e49\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u56db\u4e2a\u6570\u636e\u57df\u4e2d\u7684\u6027\u80fd\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u5c06\u97f3\u9891\u7f16\u7801\u4e3a\u79bb\u6563\u6807\u8bb0\u65b9\u9762\u53d1\u6325\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u97f3\u9891\u7f16\u89e3\u7801\u5668\u8bc4\u4f30\u53d7\u9650\u4e8e\u7b80\u5355\u7684\u6307\u6807\u548c\u573a\u666f\uff0c\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e5f\u4e0d\u9002\u7528\u4e8e\u590d\u6742\u7684\u5e94\u7528\u573a\u666f\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u590d\u6742\u6570\u636e\u96c6\u4e2d\u58f0\u5b66\u548c\u8bed\u4e49\u80fd\u529b\u7684\u8bc4\u4f30\u6027\u80fd\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86CodecBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u4ece\u58f0\u5b66\u548c\u8bed\u4e49\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u56db\u4e2a\u6570\u636e\u57df\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u8fd9\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u5f53\u524d\u5c40\u9650\u6027\uff0c\u7a81\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u4fc3\u8fdb\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5f00\u53d1\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "conclusion": "CodecBench\u4e3a\u97f3\u9891\u7f16\u89e3\u7801\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u591a\u8bf4\u8bdd\u4eba\u3001\u80cc\u666f\u566a\u58f0\u6216\u66f4\u4e30\u5bcc\u7684\u526f\u8bed\u8a00\u4fe1\u606f\u573a\u666f\uff0c\u63a8\u52a8\u97f3\u9891\u7f16\u89e3\u7801\u5668\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.20885", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20885", "abs": "https://arxiv.org/abs/2508.20885", "authors": ["Chien-Chun Wang", "En-Lun Yu", "Jeih-Weih Hung", "Shih-Chieh Huang", "Berlin Chen"], "title": "SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization", "comment": "Accepted to IEEE ASRU 2025", "summary": "Voice activity detection (VAD) is essential for speech-driven applications,\nbut remains far from perfect in noisy and resource-limited environments.\nExisting methods often lack robustness to noise, and their frame-wise\nclassification losses are only loosely coupled with the evaluation metric of\nVAD. To address these challenges, we propose SincQDR-VAD, a compact and robust\nframework that combines a Sinc-extractor front-end with a novel quadratic\ndisparity ranking loss. The Sinc-extractor uses learnable bandpass filters to\ncapture noise-resistant spectral features, while the ranking loss optimizes the\npairwise score order between speech and non-speech frames to improve the area\nunder the receiver operating characteristic curve (AUROC). A series of\nexperiments conducted on representative benchmark datasets show that our\nframework considerably improves both AUROC and F2-Score, while using only 69%\nof the parameters compared to prior arts, confirming its efficiency and\npractical viability.", "AI": {"tldr": "\u57fa\u4e8eSinc\u63d0\u53d6\u5668\u524d\u7aef\u548c\u4e8c\u6b21\u5dee\u5f02\u6392\u5e8f\u635f\u5931\u7684\u7b80\u6d01\u7c97\u7b56\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e14\u53c2\u6570\u91cf\u51cf\u5c1131%", "motivation": "\u73b0\u6709\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7f3a\u4e4f\u7a33\u5065\u6027\uff0c\u5e76\u4e14\u5e27\u7ea7\u5206\u7c7b\u635f\u5931\u4e0e\u8bc4\u4f30\u6307\u6807\u8026\u5408\u5f31", "method": "\u7ed3\u5408Sinc-extractor\u524d\u7aef\uff08\u53ef\u5b66\u4e60\u5e26\u901a\u6ee4\u6ce2\u5668\u63d0\u53d6\u6297\u566a\u8c31\u7279\u5f81\uff09\u548c\u4e8c\u6b21\u5dee\u5f02\u6392\u5e8f\u635f\u5931\uff08\u4f18\u5316\u8bed\u97f3/\u975e\u8bed\u97f3\u5e27\u7684\u5bf9\u5e94\u5206\u6570\u6392\u5e8f\uff09", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cAUROC\u548cF2-Score\u663e\u8457\u63d0\u5347\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a\u73b0\u6709\u65b9\u6cd5\u768469%", "conclusion": "SincQDR-VAD\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\u548c\u5f3a\u6297\u566a\u6027\u7684\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b"}}
{"id": "2508.20602", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20602", "abs": "https://arxiv.org/abs/2508.20602", "authors": ["Matthieu Correa", "Nicolas Vignais", "Isabelle A. Siegler", "Maxime Projetti"], "title": "Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis", "comment": null, "summary": "Mechanomyography (MMG) is a promising tool for measuring muscle activity in\nthe field but its sensitivity to motion artifacts limits its application. In\nthis study, we proposed an adaptative filtering method for MMG accelerometers\nbased on the complete ensemble empirical mode decomposition, with adaptative\nnoise and spectral fuzzy entropy, to isolate motions artefacts from the MMG\nsignal in dynamic conditions. We compared our method with the traditional\nband-pass filtering technique, demonstrating better results concerning motion\nrecomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and\n0.842). Thus, this innovative method allows the filtering of motion artifacts\ndynamically in the 5-20 Hz bandwidth, which is not achievable with traditional\nmethod. However, the interpretation of accelerometric MMG signals from the\ntrunk and lower-limb muscles during walking or running should be approached\nwith great caution as impact-related accelerations are still present, though\ntheir exact quantity still needs to be quantified.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCEEMDAN\u548c\u8c31\u6a21\u7cca\u71b5\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u6709\u6548\u5206\u79bbMMG\u4fe1\u53f7\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u5e26\u901a\u6ee4\u6ce2\u6280\u672f", "motivation": "MMG\u5728\u808c\u8089\u6d3b\u52a8\u6d4b\u91cf\u4e2d\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5bf9\u8fd0\u52a8\u4f2a\u5f71\u654f\u611f\u9650\u5236\u4e86\u5176\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u597d\u7684\u6ee4\u6ce2\u65b9\u6cd5\u6765\u5904\u7406\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u4f2a\u5f71", "method": "\u57fa\u4e8e\u5b8c\u5168\u96c6\u6210\u7ecf\u9a8c\u6a21\u6001\u5206\u89e3(CEEMDAN)\u548c\u8c31\u6a21\u7cca\u71b5\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u79bbMMG\u52a0\u901f\u5ea6\u8ba1\u4fe1\u53f7\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71", "result": "\u4e0e\u4f20\u7edf\u5e26\u901a\u6ee4\u6ce2\u76f8\u6bd4\uff0c\u5728\u4e09\u89d2\u808c\u548c\u7ad6\u810a\u808c\u7684\u8fd0\u52a8\u91cd\u6784\u65b9\u9762\u8868\u73b0\u66f4\u597d(R\u00b2=0.907\u548c0.842)\uff0c\u80fd\u57285-20Hz\u5e26\u5bbd\u5185\u52a8\u6001\u8fc7\u6ee4\u8fd0\u52a8\u4f2a\u5f71", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u8fd0\u52a8\u4f2a\u5f71\uff0c\u4f46\u5904\u7406\u8eaf\u5e72\u548c\u4e0b\u80a2\u808c\u8089\u5728\u884c\u8d70/\u8dd1\u6b65\u65f6\u7684\u52a0\u901f\u5ea6MMG\u4fe1\u53f7\u4ecd\u9700\u8c28\u614e\uff0c\u56e0\u4e3a\u51b2\u51fb\u76f8\u5173\u7684\u52a0\u901f\u5ea6\u4ecd\u7136\u5b58\u5728\u4e14\u9700\u8981\u8fdb\u4e00\u6b65\u91cf\u5316"}}
{"id": "2508.20703", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.20703", "abs": "https://arxiv.org/abs/2508.20703", "authors": ["Manu Harju", "Annamaria Mesaros"], "title": "Sound event detection with audio-text models and heterogeneous temporal annotations", "comment": "Accepted to IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "Recent advances in generating synthetic captions based on audio and related\nmetadata allow using the information contained in natural language as input for\nother audio tasks. In this paper, we propose a novel method to guide a sound\nevent detection system with free-form text. We use machine-generated captions\nas complementary information to the strong labels for training, and evaluate\nthe systems using different types of textual inputs. In addition, we study a\nscenario where only part of the training data has strong labels, and the rest\nof it only has temporally weak labels. Our findings show that synthetic\ncaptions improve the performance in both cases compared to the CRNN\narchitecture typically used for sound event detection. On a dataset of 50\nhighly unbalanced classes, the PSDS-1 score increases from 0.223 to 0.277 when\ntrained with strong labels, and from 0.166 to 0.218 when half of the training\ndata has only weak labels.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u673a\u5668\u751f\u6210\u7684\u5408\u6210\u5b57\u5e55\u4f5c\u4e3a\u8865\u5145\u4fe1\u606f\u6765\u6307\u5bfc\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5728\u5f3a\u6807\u7b7e\u548c\u5f31\u6807\u7b7e\u8bad\u7ec3\u573a\u666f\u4e0b\u5747\u80fd\u63d0\u5347\u6027\u80fd", "motivation": "\u5229\u7528\u97f3\u9891\u548c\u5143\u6570\u636e\u751f\u6210\u7684\u5408\u6210\u5b57\u5e55\u5305\u542b\u81ea\u7136\u8bed\u8a00\u4fe1\u606f\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u5176\u4ed6\u97f3\u9891\u4efb\u52a1\u7684\u8f93\u5165\uff0c\u4e3a\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u989d\u5916\u6307\u5bfc", "method": "\u4f7f\u7528\u673a\u5668\u751f\u6210\u7684\u5408\u6210\u5b57\u5e55\u4f5c\u4e3a\u5f3a\u6807\u7b7e\u7684\u8865\u5145\u4fe1\u606f\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u672c\u8f93\u5165\uff0c\u5e76\u7814\u7a76\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u53ea\u6709\u5f31\u6807\u7b7e\u7684\u573a\u666f", "result": "\u572850\u4e2a\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7c7b\u522b\u7684\u6570\u636e\u96c6\u4e0a\uff0cPSDS-1\u5206\u6570\u4ece0.223\u63d0\u5347\u52300.277\uff08\u5f3a\u6807\u7b7e\u8bad\u7ec3\uff09\uff0c\u4ece0.166\u63d0\u5347\u52300.218\uff08\u4e00\u534a\u6570\u636e\u53ea\u6709\u5f31\u6807\u7b7e\uff09", "conclusion": "\u5408\u6210\u5b57\u5e55\u5728\u5f3a\u6807\u7b7e\u548c\u5f31\u6807\u7b7e\u8bad\u7ec3\u573a\u666f\u4e0b\u90fd\u80fd\u663e\u8457\u6539\u5584\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684CRNN\u67b6\u6784"}}
{"id": "2508.20914", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T10", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.20914", "abs": "https://arxiv.org/abs/2508.20914", "authors": ["Holger Severin Bovbjerg", "Jan \u00d8stergaard", "Jesper Jensen", "Shinji Watanabe", "Zheng-Hua Tan"], "title": "Learning Robust Spatial Representations from Binaural Audio through Feature Distillation", "comment": "To appear in Proc. WASPAA 2025, October 12-15, 2025, Tahoe, US.\n  Copyright (c) 2025 IEEE. 5 pages, 2 figures, 2 tables", "summary": "Recently, deep representation learning has shown strong performance in\nmultiple audio tasks. However, its use for learning spatial representations\nfrom multichannel audio is underexplored. We investigate the use of a\npretraining stage based on feature distillation to learn a robust spatial\nrepresentation of binaural speech without the need for data labels. In this\nframework, spatial features are computed from clean binaural speech samples to\nform prediction labels. These clean features are then predicted from\ncorresponding augmented speech using a neural network. After pretraining, we\nthrow away the spatial feature predictor and use the learned encoder weights to\ninitialize a DoA estimation model which we fine-tune for DoA estimation. Our\nexperiments demonstrate that the pretrained models show improved performance in\nnoisy and reverberant environments after fine-tuning for direction-of-arrival\nestimation, when compared to fully supervised models and classic signal\nprocessing methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u84b8\u998f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ece\u53cc\u8033\u8bed\u97f3\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u65e0\u9700\u6570\u636e\u6807\u7b7e\uff0c\u7528\u4e8e\u63d0\u5347\u566a\u58f0\u548c\u6df7\u54cd\u73af\u5883\u4e2d\u7684\u58f0\u6e90\u5b9a\u4f4d\u6027\u80fd", "motivation": "\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u5728\u97f3\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u901a\u9053\u97f3\u9891\u7a7a\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u7684\u7a7a\u95f4\u7279\u5f81\u5b66\u4e60\u65b9\u6cd5", "method": "\u4f7f\u7528\u7279\u5f81\u84b8\u998f\u9884\u8bad\u7ec3\u6846\u67b6\uff1a\u4ece\u5e72\u51c0\u53cc\u8033\u8bed\u97f3\u8ba1\u7b97\u7a7a\u95f4\u7279\u5f81\u4f5c\u4e3a\u9884\u6d4b\u6807\u7b7e\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4ece\u589e\u5f3a\u8bed\u97f3\u9884\u6d4b\u8fd9\u4e9b\u7279\u5f81\uff0c\u9884\u8bad\u7ec3\u540e\u4f7f\u7528\u7f16\u7801\u5668\u6743\u91cd\u521d\u59cb\u5316DoA\u4f30\u8ba1\u6a21\u578b\u5e76\u8fdb\u884c\u5fae\u8c03", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u566a\u58f0\u548c\u6df7\u54cd\u73af\u5883\u4e2d\u7684\u58f0\u6e90\u5b9a\u4f4d\u6027\u80fd\u4f18\u4e8e\u5168\u76d1\u7763\u6a21\u578b\u548c\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5", "conclusion": "\u57fa\u4e8e\u7279\u5f81\u84b8\u998f\u7684\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7a7a\u95f4\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u7684\u58f0\u6e90\u5b9a\u4f4d\u7cbe\u5ea6"}}
{"id": "2508.20761", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20761", "abs": "https://arxiv.org/abs/2508.20761", "authors": ["Yaniv Mazor", "Tirza Routtenberg"], "title": "Weighted Bayesian Cram$\\acute{\\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation", "comment": null, "summary": "Mixed-resolution architectures, combining high-resolution (analog) data with\ncoarsely quantized (e.g., 1-bit) data, are widely employed in emerging\ncommunication and radar systems to reduce hardware costs and power consumption.\nHowever, the use of coarsely quantized data introduces non-trivial tradeoffs in\nparameter estimation tasks. In this paper, we investigate the derivation of\nlower bounds for such systems. In particular, we develop the weighted Bayesian\nCramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight\nfunction. We demonstrate the special cases of: (i) the classical BCRB; (ii) the\nWBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse\nweighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight\nfunction. Based on the developed WBCRB, we propose a new method to approximate\nthe mean-squared-error (MSE) by partitioning the estimation problem into two\nregions: (a) where the 1-bit quantized data is informative; and (b) where it is\nsaturated. We apply region-specific WBCRB approximations in these regions to\nachieve an accurate composite MSE estimate. We derive the bounds and MSE\napproximation for the linear Gaussian orthonormal (LGO) model, which is\ncommonly used in practical signal processing applications. Our simulation\nresults demonstrate the use of the proposed bounds and approximation method in\nthe LGO model with a scalar unknown parameter. It is shown that the WBCRB\noutperforms the BCRB, where the BFIM-Inverse weighting version approaches the\noptimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is\ntighter and accurately predicts the non-monotonic behavior of the MSE in the\npresence of quantization errors.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6df7\u5408\u5206\u8fa8\u7387\u7cfb\u7edf\u5f00\u53d1\u4e86\u52a0\u6743\u8d1d\u53f6\u65af\u514b\u62c9\u7f8e-\u7f57\u754c(WBCRB)\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u533a\u57df\u5212\u5206\u7684MSE\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5728LGO\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86WBCRB\u4f18\u4e8e\u4f20\u7edfBCRB\uff0c\u5e76\u80fd\u51c6\u786e\u9884\u6d4b\u91cf\u5316\u8bef\u5dee\u5bfc\u81f4\u7684MSE\u975e\u5355\u8c03\u884c\u4e3a\u3002", "motivation": "\u6df7\u5408\u5206\u8fa8\u7387\u67b6\u6784\u5728\u901a\u4fe1\u548c\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\u4ee5\u964d\u4f4e\u786c\u4ef6\u6210\u672c\uff0c\u4f46\u7c97\u91cf\u5316\u6570\u636e\u5728\u53c2\u6570\u4f30\u8ba1\u4e2d\u5f15\u5165\u975e\u5e73\u51e1\u6743\u8861\uff0c\u9700\u8981\u63a8\u5bfc\u6b64\u7c7b\u7cfb\u7edf\u7684\u4e0b\u754c\u3002", "method": "\u5f00\u53d1\u4e86\u5177\u6709\u901a\u7528\u6743\u91cd\u51fd\u6570\u7684WBCRB\uff0c\u5305\u62ec\u7ecf\u5178BCRB\u3001BFIM\u9006\u52a0\u6743WBCRB\u548c\u6700\u4f18\u6743\u91cdWBCRB\u3002\u63d0\u51fa\u5c06\u4f30\u8ba1\u95ee\u9898\u5212\u5206\u4e3a\u4fe1\u606f\u533a\u548c\u9971\u548c\u533a\uff0c\u5206\u522b\u5e94\u7528\u533a\u57df\u7279\u5b9a\u7684WBCRB\u8fd1\u4f3c\u6765\u83b7\u5f97\u590d\u5408MSE\u4f30\u8ba1\u3002", "result": "\u5728\u7ebf\u6027\u9ad8\u65af\u6b63\u4ea4(LGO)\u6a21\u578b\u4e2d\uff0cWBCRB\u4f18\u4e8eBCRB\uff0cBFIM\u9006\u52a0\u6743\u7248\u672c\u63a5\u8fd1\u6700\u4f18WBCRB\u3002\u57fa\u4e8eWBCRB\u7684MSE\u8fd1\u4f3c\u66f4\u7d27\u81f4\uff0c\u80fd\u51c6\u786e\u9884\u6d4b\u91cf\u5316\u8bef\u5dee\u5bfc\u81f4\u7684MSE\u975e\u5355\u8c03\u884c\u4e3a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684WBCRB\u548cMSE\u8fd1\u4f3c\u65b9\u6cd5\u4e3a\u6df7\u5408\u5206\u8fa8\u7387\u7cfb\u7edf\u7684\u53c2\u6570\u4f30\u8ba1\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u91cf\u5316\u8bef\u5dee\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2508.20732", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.20732", "abs": "https://arxiv.org/abs/2508.20732", "authors": ["Manjunath Mulimani", "Annamaria Mesaros"], "title": "Online incremental learning for audio classification using a pretrained audio model", "comment": "Accepted to IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "Incremental learning aims to learn new tasks sequentially without forgetting\nthe previously learned ones. Most of the existing incremental learning methods\nfor audio focus on training the model from scratch on the initial task, and the\nsame model is used to learn upcoming incremental tasks. The model is trained\nfor several iterations to adapt to each new task, using some specific\napproaches to reduce the forgetting of old tasks. In this work, we propose a\nmethod for using generalizable audio embeddings produced by a pre-trained model\nto develop an online incremental learner that solves sequential audio\nclassification tasks over time. Specifically, we inject a layer with a\nnonlinear activation function between the pre-trained model's audio embeddings\nand the classifier; this layer expands the dimensionality of the embeddings and\neffectively captures the distinct characteristics of sound classes. Our method\nadapts the model in a single forward pass (online) through the training samples\nof any task, with minimal forgetting of old tasks. We demonstrate the\nperformance of the proposed method in two incremental learning setups: one\nclass-incremental learning using ESC-50 and one domain-incremental learning of\ndifferent cities from the TAU Urban Acoustic Scenes 2019 dataset; for both\ncases, the proposed approach outperforms other methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u97f3\u9891\u5d4c\u5165\u7684\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5206\u7c7b\u5668\u4e4b\u95f4\u63d2\u5165\u975e\u7ebf\u6027\u6fc0\u6d3b\u5c42\u6765\u6269\u5c55\u5d4c\u5165\u7ef4\u5ea6\u5e76\u6355\u6349\u58f0\u97f3\u7c7b\u522b\u7279\u5f81\uff0c\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u4f20\u64ad\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u6709\u6548\u51cf\u5c11\u65e7\u4efb\u52a1\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4ece\u521d\u59cb\u4efb\u52a1\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u591a\u6b21\u8fed\u4ee3\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5b58\u5728\u8bad\u7ec3\u6548\u7387\u4f4e\u548c\u9057\u5fd8\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u97f3\u9891\u5d4c\u5165\u6765\u5f00\u53d1\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5355\u6b21\u524d\u5411\u4f20\u64ad\u9002\u5e94\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u97f3\u9891\u5d4c\u5165\u548c\u5206\u7c7b\u5668\u4e4b\u95f4\u63d2\u5165\u5e26\u6709\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u7684\u5c42\uff0c\u6269\u5c55\u5d4c\u5165\u7ef4\u5ea6\u4ee5\u66f4\u597d\u5730\u6355\u6349\u58f0\u97f3\u7c7b\u522b\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u5bf9\u4efb\u4f55\u4efb\u52a1\u7684\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u9002\u5e94\u6a21\u578b\u3002", "result": "\u5728ESC-50\u6570\u636e\u96c6\u4e0a\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u548cTAU Urban Acoustic Scenes 2019\u6570\u636e\u96c6\u4e0a\u7684\u57df\u589e\u91cf\u5b66\u4e60\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5747\u4f18\u4e8e\u5176\u4ed6\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u548c\u66f4\u5c11\u7684\u65e7\u4efb\u52a1\u9057\u5fd8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5229\u7528\u9884\u8bad\u7ec3\u97f3\u9891\u5d4c\u5165\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u67b6\u6784\u4fee\u6539\u5c31\u80fd\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u65e7\u4efb\u52a1\u7684\u9057\u5fd8\uff0c\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u589e\u91cf\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.20976", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.20976", "abs": "https://arxiv.org/abs/2508.20976", "authors": ["Jaeyeon Kim", "Heeseung Yun", "Sang Hoon Woo", "Chao-Han Huck Yang", "Gunhee Kim"], "title": "WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations", "comment": "Preprint. Project page: https://jaeyeonkim99.github.io/wow_bench/", "summary": "Large audio language models (LALMs) extend language understanding into the\nauditory domain, yet their ability to perform low-level listening, such as\npitch and duration detection, remains underexplored. However, low-level\nlistening is critical for real-world, out-of-distribution tasks where models\nmust reason about unfamiliar sounds based on fine-grained acoustic cues. To\naddress this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to\nevaluate low-level auditory perception and cognition using marine mammal\nvocalizations. WoW-bench is composed of a Perception benchmark for categorizing\nnovel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess\nthe abilities to remember, understand, apply, and analyze sound events. For the\nCognition benchmark, we additionally introduce distractor questions to evaluate\nwhether models are truly solving problems through listening rather than relying\non other heuristics. Experiments with state-of-the-art LALMs show performance\nfar below human levels, indicating a need for stronger auditory grounding in\nLALMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86WoW-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u9891\u542c\u89c9\u611f\u77e5\u548c\u8ba4\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6d77\u6d0b\u54fa\u4e73\u52a8\u7269\u53d1\u58f0\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u9891\u542c\u89c9\uff08\u5982\u97f3\u9ad8\u548c\u65f6\u957f\u68c0\u6d4b\uff09\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u5bf9\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5206\u5e03\u5916\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u6a21\u578b\u9700\u8981\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u58f0\u5b66\u7ebf\u7d22\u6765\u63a8\u7406\u4e0d\u719f\u6089\u7684\u58f0\u97f3\u3002", "method": "\u5f15\u5165World-of-Whale\u57fa\u51c6\u6d4b\u8bd5\uff08WoW-Bench\uff09\uff0c\u5305\u542b\u611f\u77e5\u57fa\u51c6\uff08\u5206\u7c7b\u65b0\u58f0\u97f3\uff09\u548c\u8ba4\u77e5\u57fa\u51c6\uff08\u57fa\u4e8eBloom\u5206\u7c7b\u6cd5\u8bc4\u4f30\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u5e94\u7528\u548c\u5206\u6790\u58f0\u97f3\u4e8b\u4ef6\u7684\u80fd\u529b\uff09\uff0c\u5e76\u5f15\u5165\u5e72\u6270\u95ee\u9898\u6765\u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u771f\u6b63\u901a\u8fc7\u542c\u89c9\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u9700\u8981\u66f4\u5f3a\u7684\u542c\u89c9\u57fa\u7840\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u9891\u542c\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u7684\u542c\u89c9\u57fa\u7840\u80fd\u529b\u4ee5\u66f4\u597d\u5730\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u7684\u97f3\u9891\u7406\u89e3\u4efb\u52a1\u3002"}}
{"id": "2508.20864", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20864", "abs": "https://arxiv.org/abs/2508.20864", "authors": ["Ehsan Sadeghi", "Paul Havinga"], "title": "Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar", "comment": null, "summary": "This paper explores the deployment of mm-wave Frequency Modulated Continuous\nWave (FMCW) radar for vital sign detection across multiple scenarios. We focus\non overcoming the limitations of traditional sensing methods by enhancing\nsignal processing techniques to capture subtle physiological changes\neffectively. Our study introduces novel adaptations of the Prony and MUSIC\nalgorithms tailored for real-time heart and respiration rate monitoring,\nsignificantly advancing the accuracy and reliability of non-contact vital sign\nmonitoring using radar technologies. Notably, these algorithms demonstrate a\nrobust ability to suppress noise and harmonic interference. For instance, the\nmean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8\nand 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,\nrespectively. These results underscore the potential of FMCW radar as a\nreliable, non-invasive solution for continuous vital sign monitoring in\nhealthcare settings, particularly in clinical and emergency scenarios where\ntraditional contact-based monitoring is impractical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u7684\u975e\u63a5\u89e6\u5f0f\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbProny\u548cMUSIC\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u5fc3\u7387\u548c\u547c\u5438\u7387\u68c0\u6d4b\u7cbe\u5ea6\uff0cMAE\u5206\u522b\u8fbe\u52300.81/1.8\u548c0.8/1.01\u3002", "motivation": "\u4f20\u7edf\u63a5\u89e6\u5f0f\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u65b9\u6cd5\u5728\u4e34\u5e8a\u548c\u6025\u6551\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u975e\u4fb5\u5165\u5f0f\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6beb\u7c73\u6ce2FMCW\u96f7\u8fbe\u6280\u672f\uff0c\u9488\u5bf9\u6027\u5730\u6539\u8fdbProny\u548cMUSIC\u7b97\u6cd5\uff0c\u589e\u5f3a\u4fe1\u53f7\u5904\u7406\u80fd\u529b\u4ee5\u6355\u6349\u7ec6\u5fae\u751f\u7406\u53d8\u5316\u3002", "result": "\u6539\u8fdb\u7b97\u6cd5\u5728\u6291\u5236\u566a\u58f0\u548c\u8c10\u6ce2\u5e72\u6270\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5fc3\u7387\u548c\u547c\u5438\u7387\u68c0\u6d4b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "FMCW\u96f7\u8fbe\u7ed3\u5408\u4f18\u5316\u7b97\u6cd5\u4e3a\u533b\u7597\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u975e\u63a5\u89e6\u5f0f\u8fde\u7eed\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u4fbf\u5b9e\u65bd\u7684\u573a\u666f\u3002"}}
{"id": "2508.20782", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.20782", "abs": "https://arxiv.org/abs/2508.20782", "authors": ["Fengyun Zhang"], "title": "A Solution of Ultra Wideband Based High-resolution and Lossless Audio Transmission", "comment": "5 pages", "summary": "This paper provides an overview of the current challenges in wireless audio\ntransmission and highlights the limitations of existing technologies regarding\ndata bandwidth, data compression, latency, and inter-device compatibility. To\naddress these shortcomings, it proposes a high-resolution, lossless audio\ntransmission scheme utilizing ultra wideband (UWB) technology. UWB emerges as a\npromising solution by offering the necessary bandwidth to enable exceptional\nsound quality with ultra-low latency, making it ideal for real-time audio\napplications and addressing synchronization concerns in audio-visual use cases.\nAdditionally, UWB's unique capabilities extend beyond high-resolution audio,\nallowing for precise location tracking in augmented and virtual reality\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u8d85\u5bbd\u5e26(UWB)\u6280\u672f\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u65e0\u635f\u97f3\u9891\u4f20\u8f93\uff0c\u89e3\u51b3\u73b0\u6709\u65e0\u7ebf\u97f3\u9891\u6280\u672f\u5728\u5e26\u5bbd\u3001\u538b\u7f29\u3001\u5ef6\u8fdf\u548c\u8bbe\u5907\u517c\u5bb9\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u65e0\u7ebf\u97f3\u9891\u4f20\u8f93\u6280\u672f\u5728\u6570\u636e\u5e26\u5bbd\u3001\u6570\u636e\u538b\u7f29\u3001\u5ef6\u8fdf\u548c\u8bbe\u5907\u517c\u5bb9\u6027\u65b9\u9762\u5b58\u5728\u8bf8\u591a\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u8d28\u91cf\u5b9e\u65f6\u97f3\u9891\u5e94\u7528\u7684\u9700\u6c42", "method": "\u91c7\u7528\u8d85\u5bbd\u5e26(UWB)\u6280\u672f\uff0c\u5229\u7528\u5176\u9ad8\u5e26\u5bbd\u7279\u6027\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u65e0\u635f\u97f3\u9891\u4f20\u8f93\uff0c\u540c\u65f6\u63d0\u4f9b\u8d85\u4f4e\u5ef6\u8fdf\u548c\u7cbe\u786e\u5b9a\u4f4d\u80fd\u529b", "result": "UWB\u6280\u672f\u80fd\u591f\u63d0\u4f9b\u8db3\u591f\u7684\u5e26\u5bbd\u6765\u5b9e\u73b0\u5353\u8d8a\u7684\u97f3\u8d28\u548c\u8d85\u4f4e\u5ef6\u8fdf\uff0c\u9002\u5408\u5b9e\u65f6\u97f3\u9891\u5e94\u7528\uff0c\u5e76\u80fd\u89e3\u51b3\u97f3\u89c6\u9891\u540c\u6b65\u95ee\u9898\uff0c\u8fd8\u652f\u6301\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d", "conclusion": "\u8d85\u5bbd\u5e26\u6280\u672f\u662f\u89e3\u51b3\u65e0\u7ebf\u97f3\u9891\u4f20\u8f93\u6311\u6218\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u80fd\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u65e0\u635f\u97f3\u9891\u4f20\u8f93\uff0c\u8fd8\u80fd\u4e3a\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u63d0\u4f9b\u989d\u5916\u7684\u5b9a\u4f4d\u529f\u80fd"}}
{"id": "2508.20990", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20990", "abs": "https://arxiv.org/abs/2508.20990", "authors": ["Hong-Yan Zhang", "Haoting Liu", "Rui-Jia Lin", "Yu Zhou"], "title": "A Correction for the Paper \"Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis\"", "comment": "13 pages, 4 figures, 2 tables", "summary": "The symplectic geometry mode decomposition (SGMD) is a powerful method for\ndecomposing time series, which is based on the diagonal averaging principle\n(DAP) inherited from the singular spectrum analysis (SSA). Although the authors\nof SGMD method generalized the form of the trajectory matrix in SSA, the DAP is\nnot updated simultaneously. In this work, we pointed out the limitations of the\nSGMD method and fixed the bugs with the pulling back theorem for computing the\ngiven component of time series from the corresponding component of trajectory\nmatrix.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u4e86SGMD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u4fee\u590d\u4e86\u5176\u7f3a\u9677\uff0c\u901a\u8fc7\u5f15\u5165\u56de\u62c9\u5b9a\u7406\u6765\u6b63\u786e\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u5206\u91cf", "motivation": "SGMD\u65b9\u6cd5\u867d\u7136\u63a8\u5e7f\u4e86SSA\u4e2d\u7684\u8f68\u8ff9\u77e9\u9635\u5f62\u5f0f\uff0c\u4f46\u672a\u540c\u6b65\u66f4\u65b0\u5bf9\u89d2\u5e73\u5747\u539f\u7406(DAP)\uff0c\u5b58\u5728\u8ba1\u7b97\u7f3a\u9677", "method": "\u63d0\u51fa\u56de\u62c9\u5b9a\u7406(pulling back theorem)\u6765\u6b63\u786e\u5730\u4ece\u8f68\u8ff9\u77e9\u9635\u5206\u91cf\u8ba1\u7b97\u5bf9\u5e94\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u91cf", "result": "\u4fee\u590d\u4e86SGMD\u65b9\u6cd5\u7684bug\uff0c\u5b8c\u5584\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u7684\u8ba1\u7b97\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u56de\u62c9\u5b9a\u7406\u89e3\u51b3\u4e86SGMD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8ba1\u7b97\u6846\u67b6"}}
{"id": "2508.20859", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.20859", "abs": "https://arxiv.org/abs/2508.20859", "authors": ["Shrishti Saha Shetu", "Emanu\u00ebl A. P. Habets", "Andreas Brendel"], "title": "Leveraging Discriminative Latent Representations for Conditioning GAN-Based Speech Enhancement", "comment": "This manuscript has been submitted to IEEE Transactions on Audio,\n  Speech and Language Processing", "summary": "Generative speech enhancement methods based on generative adversarial\nnetworks (GANs) and diffusion models have shown promising results in various\nspeech enhancement tasks. However, their performance in very low\nsignal-to-noise ratio (SNR) scenarios remains under-explored and limited, as\nthese conditions pose significant challenges to both discriminative and\ngenerative state-of-the-art methods. To address this, we propose a method that\nleverages latent features extracted from discriminative speech enhancement\nmodels as generic conditioning features to improve GAN-based speech\nenhancement. The proposed method, referred to as DisCoGAN, demonstrates\nperformance improvements over baseline models, particularly in low-SNR\nscenarios, while also maintaining competitive or superior performance in\nhigh-SNR conditions and on real-world recordings. We also conduct a\ncomprehensive evaluation of conventional GAN-based architectures, including\nGANs trained end-to-end, GANs as a first processing stage, and post-filtering\nGANs, as well as discriminative models under low-SNR conditions. We show that\nDisCoGAN consistently outperforms existing methods. Finally, we present an\nablation study that investigates the contributions of individual components\nwithin DisCoGAN and analyzes the impact of the discriminative conditioning\nmethod on overall performance.", "AI": {"tldr": "\u63d0\u51faDisCoGAN\u65b9\u6cd5\uff0c\u5229\u7528\u5224\u522b\u5f0f\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\u7684\u6f5c\u5728\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u7279\u5f81\u6765\u6539\u8fdbGAN\u8bed\u97f3\u589e\u5f3a\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u5728\u6781\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u6027\u80fd\u6709\u9650\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218", "method": "\u4f7f\u7528\u5224\u522b\u5f0f\u8bed\u97f3\u589e\u5f3a\u6a21\u578b\u63d0\u53d6\u7684\u6f5c\u5728\u7279\u5f81\u4f5c\u4e3a\u901a\u7528\u6761\u4ef6\u7279\u5f81\uff0c\u6539\u8fdb\u57fa\u4e8eGAN\u7684\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5", "result": "DisCoGAN\u5728\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u5347\uff0c\u5728\u9ad8\u4fe1\u566a\u6bd4\u6761\u4ef6\u548c\u771f\u5b9e\u5f55\u97f3\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u8868\u73b0", "conclusion": "\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5224\u522b\u5f0f\u6761\u4ef6\u65b9\u6cd5\u5bf9\u6574\u4f53\u6027\u80fd\u6709\u91cd\u8981\u8d21\u732e"}}
{"id": "2508.20870", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.20870", "abs": "https://arxiv.org/abs/2508.20870", "authors": ["Ayano Shibata", "Toshiki Gunji", "Mitsuaki Tsuda", "Takashi Endo", "Kota Dohi", "Tomoya Nishida", "Satoko Nomoto"], "title": "Automatic Inspection Based on Switch Sounds of Electric Point Machines", "comment": "Accepted at ASPECT 2025", "summary": "Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working to\nreplace human inspections with IoT-based monitoring. The purpose is\nLabor-saving required for equipment inspections and provide appropriate\npreventive maintenance. As an alternative to visual inspection, it has been\ndifficult to substitute electrical characteristic monitoring, and the\nintroduction of new high-performance sensors has been costly. In 2019, we\nimplemented cameras and microphones in an ``NS'' electric point machines to\nreduce downtime from equipment failures, allowing for remote monitoring of\nlock-piece conditions. This method for detecting turnout switching errors based\non sound information was proposed, and the expected test results were obtained.\nThe proposed method will make it possible to detect equipment failures in real\ntime, thereby reducing the need for visual inspections. This paper presents the\nresults of our technical studies aimed at automating the inspection of\nelectronic point machines using sound, specifically focusing on ``switch\nsound'' beginning in 2019.", "AI": {"tldr": "\u65e5\u672cJR\u4e1c\u65c5\u516c\u53f8\u4e0e\u65e5\u7acb\u516c\u53f8\u5408\u4f5c\uff0c\u901a\u8fc7\u58f0\u97f3\u76d1\u6d4b\u6280\u672f\u5b9e\u73b0\u7535\u52a8\u9053\u53c9\u8f6c\u6362\u5668\u7684\u81ea\u52a8\u5316\u68c0\u67e5\uff0c\u4ee5\u66ff\u4ee3\u4eba\u5de5\u76ee\u89c6\u68c0\u67e5\uff0c\u964d\u4f4e\u8bbe\u5907\u6545\u969c\u505c\u673a\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u8bbe\u5907\u68c0\u67e5\u4e2d\u7684\u4eba\u529b\u8282\u7ea6\u9700\u6c42\u548c\u63d0\u4f9b\u9002\u5f53\u7684\u9884\u9632\u6027\u7ef4\u62a4\uff0c\u5bfb\u627e\u4ee5\u7269\u8054\u7f51\u76d1\u6d4b\u66ff\u4ee3\u4eba\u5de5\u68c0\u67e5\u7684\u65b9\u6848\u3002", "method": "\u57282019\u5e74\u5728\"NS\"\u7535\u52a8\u9053\u53c9\u8f6c\u6362\u5668\u4e2d\u5b89\u88c5\u6444\u50cf\u5934\u548c\u9ea6\u514b\u98ce\uff0c\u901a\u8fc7\u58f0\u97f3\u4fe1\u606f\u8fdb\u884c\u9053\u53c9\u8f6c\u6362\u9519\u8bef\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8fdc\u7a0b\u76d1\u63a7\u9501\u5757\u72b6\u6001\u3002", "result": "\u83b7\u5f97\u4e86\u9884\u671f\u7684\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u8bbe\u5907\u6545\u969c\uff0c\u51cf\u5c11\u76ee\u89c6\u68c0\u67e5\u7684\u9700\u6c42\u3002", "conclusion": "\u901a\u8fc7\u58f0\u97f3\u6280\u672f\u5b9e\u73b0\u7535\u5b50\u9053\u53c9\u8f6c\u6362\u5668\u68c0\u67e5\u81ea\u52a8\u5316\u7684\u6280\u672f\u7814\u7a76\u53d6\u5f97\u6210\u529f\uff0c\u4e3a\u9053\u8def\u8fd0\u8425\u7ef4\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u9632\u6027\u7ef4\u62a4\u65b9\u6848\u3002"}}
{"id": "2508.20983", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20983", "abs": "https://arxiv.org/abs/2508.20983", "authors": ["Hashim Ali", "Surya Subramani", "Lekha Bollinani", "Nithin Sai Adupa", "Sali El-Loh", "Hafiz Malik"], "title": "Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System", "comment": null, "summary": "The SAFE Challenge evaluates synthetic speech detection across three tasks:\nunmodified audio, processed audio with compression artifacts, and laundered\naudio designed to evade detection. We systematically explore self-supervised\nlearning (SSL) front-ends, training data compositions, and audio length\nconfigurations for robust deepfake detection. Our AASIST-based approach\nincorporates WavLM large frontend with RawBoost augmentation, trained on a\nmultilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS\nsystems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.\nThrough extensive experimentation with different SSL front-ends, three training\ndata versions, and two audio lengths, we achieved second place in both Task 1\n(unmodified audio detection) and Task 3 (laundered audio detection),\ndemonstrating strong generalization and robustness.", "AI": {"tldr": "SAFE\u6311\u6218\u8d5b\u8bc4\u4f30\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u7b2c\u4e8c\u540d\uff1a\u672a\u4fee\u6539\u97f3\u9891\u3001\u538b\u7f29\u5904\u7406\u97f3\u9891\u548c\u89c4\u907f\u68c0\u6d4b\u7684\u6e05\u6d17\u97f3\u9891\u3002\u91c7\u7528AASIST\u67b6\u6784\u7ed3\u5408WavLM\u5927\u524d\u7aef\u548cRawBoost\u589e\u5f3a\uff0c\u4f7f\u7528\u591a\u8bed\u8a00\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "motivation": "\u7cfb\u7edf\u8bc4\u4f30\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u539f\u59cb\u97f3\u9891\u3001\u538b\u7f29\u5904\u7406\u97f3\u9891\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u89c4\u907f\u68c0\u6d4b\u97f3\u9891\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\u3002", "method": "\u57fa\u4e8eAASIST\u67b6\u6784\uff0c\u6574\u5408WavLM\u5927\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u524d\u7aef\u548cRawBoost\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4f7f\u7528\u5305\u542b9\u79cd\u8bed\u8a00\u300170\u591a\u4e2aTTS\u7cfb\u7edf\u7684256,600\u4e2a\u6837\u672c\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Task 1\uff08\u672a\u4fee\u6539\u97f3\u9891\u68c0\u6d4b\uff09\u548cTask 3\uff08\u6e05\u6d17\u97f3\u9891\u68c0\u6d4b\uff09\u4e2d\u5747\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u89c4\u907f\u6280\u672f\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}

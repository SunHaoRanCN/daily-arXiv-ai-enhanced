<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 7]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Error Analysis for Over-the-Air Federated Learning under Misaligned and Time-Varying Channels](https://arxiv.org/abs/2508.20277)
*Xiaoyan Ma,Shahryar Zehtabi,Taejoon Kim,Christopher G. Brinton*

Main category: eess.SP

TL;DR: 本文研究OFDM空中联邦学习系统，分析移动设备高速移动导致的信道估计不完善和模型参数不对齐问题，推导了单轮和多轮全局模型更新的闭式表达式和误差界限。


<details>
  <summary>Details</summary>
Motivation: 移动设备（如无人机）的高速移动导致信道估计不完善，造成模型参数传输不同步和时间变化的上传信道，这些因素在OTA-FL训练过程中引起失真但尚未被充分研究。

Method: 首先推导了单轮全局模型更新的闭式表达式，然后扩展到多轮全局更新分析，得出OTA-FL累积误差的界限，并通过大量数值模拟验证理论结果。

Result: 获得了单轮和多轮全局模型更新的理论表达式，确定了信道不完善对OTA-FL系统性能的影响程度，数值模拟结果与理论分析一致。

Conclusion: 移动性导致的信道不完善会显著影响OTA-FL性能，所提出的分析框架能够有效量化这些影响，为实际系统设计提供理论指导。

Abstract: This paper investigates an OFDM-based over-the-air federated learning
(OTA-FL) system, where multiple mobile devices, e.g., unmanned aerial vehicles
(UAVs), transmit local machine learning (ML) models to a central parameter
server (PS) for global model aggregation. The high mobility of local devices
results in imperfect channel estimation, leading to a misalignment problem,
i.e., the model parameters transmitted from different local devices do not
arrive at the central PS simultaneously. Moreover, the mobility introduces
time-varying uploading channels, which further complicates the aggregation
process. All these factors collectively cause distortions in the OTA-FL
training process which are underexplored. To quantify these effects, we first
derive a closed-form expression for a single-round global model update in terms
of these channel imperfections. We then extend our analysis to capture multiple
rounds of global updates, yielding a bound on the accumulated error in OTA-FL.
We validate our theoretical results via extensive numerical simulations, which
corroborate our derived analysis.

</details>


### [2] [Dual-IRS Aided Near-/Hybrid-Field SWIPT: Passive Beamforming and Independent Antenna Power Splitting Design](https://arxiv.org/abs/2508.20531)
*Chaoying Huang,Wen Chen,Qingqing Wu,Xusheng Zhu,Zhendong Li,Ying Wang,Jinhong Yuan*

Main category: eess.SP

TL;DR: 本文提出一种双智能反射表面(IRS)辅助的干扰限制同时无线信息与能量传输(SWIPT)系统，采用独立功率分割(PS)技术，通过优化双IRS相位、PS比例和接收波形形成来最大化收获功率。


<details>
  <summary>Details</summary>
Motivation: 为了在干扰限制环境下提高SWIPT系统性能，通过双IRS协同优化和独立PS技术来获得更好的信息与能量收益比例。

Method: 分别建立近场和混合场模型，采用交替优化算法，在近场情况下使用拉格朗日对偶方法和DC规划，在混合场情况下利用题设特性将问题转化为凸优化问题。

Result: 数值结果验证了分析的正确性，显示了提出的双IRS辅助SWIPT与独立PS方案相比其他基准方案具有显著的性能收益。

Conclusion: 该方案能够有效提高干扰限制SWIPT系统的性能，为无线信息与能量同时传输提供了一种高效的解决方案。

Abstract: This paper proposes a novel dual-intelligent reflecting surface (IRS) aided
interference-limited simultaneous wireless information and power transfer
(SWIPT) system with independent power splitting (PS), where each receiving
antenna applies different PS factors to offer an advantageous trade-off between
the useful information and harvested energy. We separately establish the near-
and hybrid-field channel models for IRS-reflected links to evaluate the
performance gain more precisely and practically. Specifically, we formulate an
optimization problem of maximizing the harvested power by jointly optimizing
dual-IRS phase shifts, independent PS ratio, and receive beamforming vector in
both near- and hybrid-field cases. In the near-field case, the alternating
optimization algorithm is proposed to solve the non-convex problem by applying
the Lagrange duality method and the difference-of-convex (DC) programming. In
the hybrid-field case, we first present an interesting result that the
AP-IRS-user channel gains are invariant to the phase shifts of dual-IRS, which
allows the optimization problem to be transformed into a convex one. Then, we
derive the asymptotic performance of the combined channel gains in closed-form
and analyze the characteristics of the dual-IRS. Numerical results validate our
analysis and indicate the performance gains of the proposed scheme that
dual-IRS-aided SWIPT with independent PS over other benchmark schemes.

</details>


### [3] [Towards Automated EEG-Based Detection Using Deep Convolutional Autoencoders](https://arxiv.org/abs/2508.20535)
*Annika Stiehl,Nicolas Weeger,Christian Uhl,Dominic Bechtold,Nicole Ille,Stefan Geißelsöder*

Main category: eess.SP

TL;DR: 提出基于深度卷积自编码器(DCAE)的癫痫检测方法，结合时域和频域损失函数来提取EEG信号的低维表征，解决了现有方法在敏感性和假阳性率之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫检测需要可靠高效的EEG分析方法，但现有深度学习方法在敏感性和假阳性率之间难以平衡，且缺乏统一的EEG输入表征标准。

Method: 使用深度卷积自编码器提取EEG信号的低维潜在表征，通过比较时域和频域的重构误差来评估模型性能，训练了多种基于不同损失函数的自编码器。

Result: 同时考虑时域和频域损失的DCAE模型获得了最佳重构性能，表明单一表征的深度神经网络可能无法保留所有相关信号特性。

Conclusion: 该研究揭示了深度学习模型处理EEG数据的方式，证明了结合时域和频域信息的重要性，为癫痫自动检测提供了新的思路。

Abstract: Epilepsy is one of the most common neurological disorders. This disease
requires reliable and efficient seizure detection methods.
Electroencephalography (EEG) is the gold standard for seizure monitoring, but
its manual analysis is a time-consuming task that requires expert knowledge. In
addition, there are no well-defined features that allow fully automated
analysis. Existing deep learning-based approaches struggle to achieve high
sensitivity while maintaining a low false alarm rate per hour (FAR/h) and lack
consistency in the optimal EEG input representation, whether in the time or
frequency domain. To address these issues, we propose a Deep Convolutional
Autoencoder (DCAE) to extract low-dimensional latent representations that
preserve essential EEG signal features. The ability of the model to preserve
relevant information was evaluated by comparing reconstruction errors based on
both time series and frequency-domain representations. Several autoencoders
with different loss functions based on time and frequency were trained and
evaluated to determine their effectiveness in reconstructing EEG features. Our
results show that the DCAE model taking both time series and frequency losses
into account achieved the best reconstruction performance. This indicates that
Deep Neural Networks with a single representation might not preserve the
relevant signal properties. This work provides insight into how deep learning
models process EEG data and examines whether frequency information is captured
when time series signals are used as input.

</details>


### [4] [Removing motion artifacts from mechanomyographic signals: an innovative filtering method applied to human movement analysis](https://arxiv.org/abs/2508.20602)
*Matthieu Correa,Nicolas Vignais,Isabelle A. Siegler,Maxime Projetti*

Main category: eess.SP

TL;DR: 基于完整集成经验模态分解的适应性筛波方法，通过调整性噪声和谱模糊熵燃来消除肌肉振动图中的运动伪影


<details>
  <summary>Details</summary>
Motivation: 肌肉振动图(MMG)在实际场景中应用受限，主要因为对运动伪影敏感，需要开发更有效的筛波方法

Method: 采用完整集成经验模态分解(CEEMD)技术，结合调整性噪声和谱模糊熵燃来分离运动伪影

Result: 在肩肌和脊肌应用中表现优于传统带通筛波(R² = 0.907和0.842)，能在5-20Hz带宽内动态筛除伪影

Conclusion: 该方法在动态条件下有效消除运动伪影，但对于胸部和下肢肌肉在走跑过程中的加速度计MMG信号仍需谨慎解释

Abstract: Mechanomyography (MMG) is a promising tool for measuring muscle activity in
the field but its sensitivity to motion artifacts limits its application. In
this study, we proposed an adaptative filtering method for MMG accelerometers
based on the complete ensemble empirical mode decomposition, with adaptative
noise and spectral fuzzy entropy, to isolate motions artefacts from the MMG
signal in dynamic conditions. We compared our method with the traditional
band-pass filtering technique, demonstrating better results concerning motion
recomposition for deltoid and erector spinae muscles (R${}^2$ = 0.907 and
0.842). Thus, this innovative method allows the filtering of motion artifacts
dynamically in the 5-20 Hz bandwidth, which is not achievable with traditional
method. However, the interpretation of accelerometric MMG signals from the
trunk and lower-limb muscles during walking or running should be approached
with great caution as impact-related accelerations are still present, though
their exact quantity still needs to be quantified.

</details>


### [5] [Weighted Bayesian Cram$\acute{\text{e}}$r-Rao Bound for Mixed-Resolution Parameter Estimation](https://arxiv.org/abs/2508.20761)
*Yaniv Mazor,Tirza Routtenberg*

Main category: eess.SP

TL;DR: 本文针对混合分辨率系统开发了加权贝叶斯克拉美罗下界(WBCRB)，提出了基于区域划分的MSE近似方法，在LGO模型中验证了WBCRB优于传统BCRB，并能准确预测量化误差导致的MSE非单调行为。


<details>
  <summary>Details</summary>
Motivation: 混合分辨率架构在通信和雷达系统中广泛应用以降低硬件成本和功耗，但粗量化数据会引入参数估计的非平凡权衡，需要开发更精确的性能下界。

Method: 开发了具有通用权重函数的WBCRB，包括经典BCRB、BFIM逆加权WBCRB和最优权重函数的Aharon-Tabrikian最紧WBCRB；提出将估计问题划分为信息区域和饱和区域的MSE近似方法。

Result: 在LGO模型中的仿真表明，WBCRB优于BCRB，BFIM逆加权版本接近最优WBCRB；基于WBCRB的MSE近似更紧，能准确预测量化误差导致的MSE非单调行为。

Conclusion: WBCRB为混合分辨率系统提供了更精确的性能下界，提出的MSE近似方法能有效处理量化误差影响，对实际系统设计具有重要指导意义。

Abstract: Mixed-resolution architectures, combining high-resolution (analog) data with
coarsely quantized (e.g., 1-bit) data, are widely employed in emerging
communication and radar systems to reduce hardware costs and power consumption.
However, the use of coarsely quantized data introduces non-trivial tradeoffs in
parameter estimation tasks. In this paper, we investigate the derivation of
lower bounds for such systems. In particular, we develop the weighted Bayesian
Cramer-Rao bound (WBCRB) for the mixed-resolution setting with a general weight
function. We demonstrate the special cases of: (i) the classical BCRB; (ii) the
WBCRB that is based on the Bayesian Fisher information matrix (BFIM)-Inverse
weighting; and (iii) the Aharon-Tabrikian tightest WBCRB with an optimal weight
function. Based on the developed WBCRB, we propose a new method to approximate
the mean-squared-error (MSE) by partitioning the estimation problem into two
regions: (a) where the 1-bit quantized data is informative; and (b) where it is
saturated. We apply region-specific WBCRB approximations in these regions to
achieve an accurate composite MSE estimate. We derive the bounds and MSE
approximation for the linear Gaussian orthonormal (LGO) model, which is
commonly used in practical signal processing applications. Our simulation
results demonstrate the use of the proposed bounds and approximation method in
the LGO model with a scalar unknown parameter. It is shown that the WBCRB
outperforms the BCRB, where the BFIM-Inverse weighting version approaches the
optimal WBCRB. Moreover, it is shown that the WBCRB-based MSE approximation is
tighter and accurately predicts the non-monotonic behavior of the MSE in the
presence of quantization errors.

</details>


### [6] [Breaking Barriers in Health Monitoring: Multi-Scenario Vital Sign Detection Using Mm-Wave MIMO FMCW Radar](https://arxiv.org/abs/2508.20864)
*Ehsan Sadeghi,Paul Havinga*

Main category: eess.SP

TL;DR: 使用毫米波FMCW雷达进行多场景生命体征检测，通过改进Prony和MUSIC算法实现高精度非接触式心率和呼吸率监测


<details>
  <summary>Details</summary>
Motivation: 克服传统传感方法的局限性，通过增强信号处理技术有效捕捉细微生理变化，为非接触式生命体征监测提供可靠解决方案

Method: 针对实时心率和呼吸率监测定制了Prony和MUSIC算法的改进版本，特别增强了抑制噪声和谐波干扰的能力

Result: MUSIC算法心率检测MAE为1.8，呼吸率MAE为1.01；Prony算法心率检测MAE为0.81，呼吸率MAE为0.8，表现出优异的准确性

Conclusion: FMCW雷达技术具有作为医疗环境中可靠、非侵入式连续生命体征监测解决方案的巨大潜力，特别适用于临床和急救场景

Abstract: This paper explores the deployment of mm-wave Frequency Modulated Continuous
Wave (FMCW) radar for vital sign detection across multiple scenarios. We focus
on overcoming the limitations of traditional sensing methods by enhancing
signal processing techniques to capture subtle physiological changes
effectively. Our study introduces novel adaptations of the Prony and MUSIC
algorithms tailored for real-time heart and respiration rate monitoring,
significantly advancing the accuracy and reliability of non-contact vital sign
monitoring using radar technologies. Notably, these algorithms demonstrate a
robust ability to suppress noise and harmonic interference. For instance, the
mean absolute errors (MAE) for MUSIC and Prony in heart rate detection are 1.8
and 0.81, respectively, while for respiration rate, the MAEs are 1.01 and 0.8,
respectively. These results underscore the potential of FMCW radar as a
reliable, non-invasive solution for continuous vital sign monitoring in
healthcare settings, particularly in clinical and emergency scenarios where
traditional contact-based monitoring is impractical.

</details>


### [7] [A Correction for the Paper "Symplectic geometry mode decomposition and its application to rotating machinery compound fault diagnosis"](https://arxiv.org/abs/2508.20990)
*Hong-Yan Zhang,Haoting Liu,Rui-Jia Lin,Yu Zhou*

Main category: eess.SP

TL;DR: 本文指出了SGMD方法在轨迹矩阵形式更新后未同步更新对角平均原理的局限性，并通过回拉定理修复了该bug


<details>
  <summary>Details</summary>
Motivation: SGMD方法虽然推广了SSA中轨迹矩阵的形式，但未同步更新对角平均原理(DAP)，存在计算时间序列分量的缺陷

Method: 使用回拉定理(pulling back theorem)来计算轨迹矩阵对应分量的时间序列分量

Result: 成功修复了SGMD方法中的bug，完善了时间序列分解的计算方法

Conclusion: 通过回拉定理修正了SGMD方法的局限性，使其能够正确计算时间序列分量

Abstract: The symplectic geometry mode decomposition (SGMD) is a powerful method for
decomposing time series, which is based on the diagonal averaging principle
(DAP) inherited from the singular spectrum analysis (SSA). Although the authors
of SGMD method generalized the form of the trajectory matrix in SSA, the DAP is
not updated simultaneously. In this work, we pointed out the limitations of the
SGMD method and fixed the bugs with the pulling back theorem for computing the
given component of time series from the corresponding component of trajectory
matrix.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [8] [Live Vocal Extraction from K-pop Performances](https://arxiv.org/abs/2508.20273)
*Yujin Kim,Richa Namballa,Magdalena Fuentes*

Main category: eess.AS

TL;DR: 基于K-pop演出特点的自动直播人声提取方法，通过源分离、相关性分析和幅度调整自动去除预录音和乐器音


<details>
  <summary>Details</summary>
Motivation: 受K-pop粉丝文化的启发，开发自动提取现场演出中真实人声的方法

Method: 结合源分离技术、交叉相关性分析和幅度缩放算法，自动识别和移除预充音频和乐器音

Result: 提出了直播人声分离的新任务概念，为该领域的未来研究奠定基础

Conclusion: 该方法为自动化处理现场演出音频提供了初步解决方案，并开启了直播音频分离研究的新方向

Abstract: K-pop's global success is fueled by its dynamic performances and vibrant fan
engagement. Inspired by K-pop fan culture, we propose a methodology for
automatically extracting live vocals from performances. We use a combination of
source separation, cross-correlation, and amplitude scaling to automatically
remove pre-recorded vocals and instrumentals from a live performance. Our
preliminary work introduces the task of live vocal separation and provides a
foundation for future research in this topic.

</details>


### [9] [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
*Muhammad Shakeel,Yui Sudo,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: 本文提出了一种统一的多说话人编码器（UME）架构，通过共享语音基础编码器联合学习说话人日志、语音分离和多说话人语音识别任务，利用残差加权和编码有效整合不同语义层次信息，显著提升重叠语音处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单任务方法在处理重叠语音时存在性能瓶颈，不同任务之间存在内在依赖关系但缺乏有效的联合学习框架，需要一种统一的架构来同时优化说话人日志、语音分离和多说话人语音识别任务。

Method: 提出统一多说话人编码器（UME）架构，使用共享语音基础编码器，通过残差加权和编码（RWSE）整合多层隐藏表示，实现自下而上的任务对齐，进行联合训练以捕获任务间的内在依赖关系。

Result: 在LibriMix评估集上显著优于单任务基线方法，说话人日志任务在Libri2Mix和Libri3Mix上分别达到1.37%和2.29%的错误率，优于先前研究。

Conclusion: UME架构通过联合学习和多层次表示整合，有效提升了重叠语音处理中多个相关任务的性能，证明了多任务联合学习的有效性。

Abstract: This paper presents a unified multi-speaker encoder (UME), a novel
architecture that jointly learns representations for speaker diarization (SD),
speech separation (SS), and multi-speaker automatic speech recognition (ASR)
tasks using a shared speech foundational encoder. We leverage the hidden
representations from multiple layers of UME as a residual weighted-sum encoding
(RWSE) to effectively use information from different semantic levels,
contributing to bottom-up alignment between tasks. This joint training approach
captures the inherent interdependencies among the tasks, enhancing overall
performance on overlapping speech data. Our evaluations demonstrate that UME
substantially improves over the single-task baselines dedicated to SD, SS, and
multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms
the previous studies, achieving diarization error rates of 1.37% and 2.29% on
Libri2Mix and Libri3Mix evaluation sets, respectively.

</details>


### [10] [CodecBench: A Comprehensive Benchmark for Acoustic and Semantic Evaluation](https://arxiv.org/abs/2508.20660)
*Ruifan Deng,Yitian Gong,Qinghui Gao,Luozhijie Jin,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Xipeng Qiu*

Main category: eess.AS

TL;DR: CodecBench是一个全面的音频编解码器评估数据集，用于从声学和语义两个角度评估音频编解码器在四个数据域中的性能，旨在解决现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的兴起，音频编解码器在将音频编码为离散标记方面发挥着越来越重要的作用。然而，现有的音频编解码器评估受到简单指标和场景的限制，现有基准测试无法适应复杂的应用场景，限制了在复杂数据集上对声学和语义能力的评估。

Method: 研究人员开发了CodecBench，一个全面的评估数据集，从声学和语义两个角度评估音频编解码器在四个数据域中的性能。

Result: 通过这个基准测试，能够识别当前音频编解码器的局限性，突出未来的研究方向，并促进音频编解码器开发方面的进展。

Conclusion: CodecBench为音频编解码器提供了更全面的评估框架，有助于推动该领域在复杂场景下的发展，代码已在GitHub上开源。

Abstract: With the rise of multimodal large language models (LLMs), audio codec plays
an increasingly vital role in encoding audio into discrete tokens, enabling
integration of audio into text-based LLMs. Current audio codec captures two
types of information: acoustic and semantic. As audio codec is applied to
diverse scenarios in speech language model , it needs to model increasingly
complex information and adapt to varied contexts, such as scenarios with
multiple speakers, background noise, or richer paralinguistic information.
However, existing codec's own evaluation has been limited by simplistic metrics
and scenarios, and existing benchmarks for audio codec are not designed for
complex application scenarios, which limits the assessment performance on
complex datasets for acoustic and semantic capabilities. We introduce
CodecBench, a comprehensive evaluation dataset to assess audio codec
performance from both acoustic and semantic perspectives across four data
domains. Through this benchmark, we aim to identify current limitations,
highlight future research directions, and foster advances in the development of
audio codec. The codes are available at https://github.com/RayYuki/CodecBench.

</details>


### [11] [Sound event detection with audio-text models and heterogeneous temporal annotations](https://arxiv.org/abs/2508.20703)
*Manu Harju,Annamaria Mesaros*

Main category: eess.AS

TL;DR: 使用机器生成的合成字幕作为补充信息来指导声音事件检测系统，在强标签和弱标签训练数据下都能提升性能


<details>
  <summary>Details</summary>
Motivation: 利用音频和元数据生成的合成字幕包含丰富的自然语言信息，可以作为其他音频任务的输入，探索如何用自由文本指导声音事件检测系统

Method: 提出新方法，使用机器生成的合成字幕作为强标签的补充信息进行训练，评估不同类型文本输入的效果，研究部分训练数据只有弱标签的场景

Result: 在50个高度不平衡类别的数据集上，PSDS-1分数从0.223提升到0.277（强标签训练），从0.166提升到0.218（一半训练数据只有弱标签）

Conclusion: 合成字幕在两种情况下都能改善性能，相比传统CRNN架构有明显提升

Abstract: Recent advances in generating synthetic captions based on audio and related
metadata allow using the information contained in natural language as input for
other audio tasks. In this paper, we propose a novel method to guide a sound
event detection system with free-form text. We use machine-generated captions
as complementary information to the strong labels for training, and evaluate
the systems using different types of textual inputs. In addition, we study a
scenario where only part of the training data has strong labels, and the rest
of it only has temporally weak labels. Our findings show that synthetic
captions improve the performance in both cases compared to the CRNN
architecture typically used for sound event detection. On a dataset of 50
highly unbalanced classes, the PSDS-1 score increases from 0.223 to 0.277 when
trained with strong labels, and from 0.166 to 0.218 when half of the training
data has only weak labels.

</details>


### [12] [Online incremental learning for audio classification using a pretrained audio model](https://arxiv.org/abs/2508.20732)
*Manjunath Mulimani,Annamaria Mesaros*

Main category: eess.AS

TL;DR: 通过使用预训练音频嵌入和添加非线性激活层，提出了一种在线增量学习方法，可在单次前向传播中适应新任务且最小化遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有音频增量学习方法多需从头训练模型，并需多次迭代来适应新任务，这会导致对旧任务的遗忘。需要一种更高效的在线学习方法。

Method: 在预训练模型的音频嵌入与分类器之间注入一个具有非线性激活函数的层，扩展嵌入维度并抓取声音类别的特征。模型只需单次前向传播即可适应新任务。

Result: 在ESC-50数据集的类别增量学习和TAU城市音响场景数据集的域增量学习中，该方法都超越了其他方法。

Conclusion: 通过利用预训练音频嵌入和添加特定层，可以实现高效的在线增量学习，在单次前向传播中完成新任务适应且最小化遗忘。

Abstract: Incremental learning aims to learn new tasks sequentially without forgetting
the previously learned ones. Most of the existing incremental learning methods
for audio focus on training the model from scratch on the initial task, and the
same model is used to learn upcoming incremental tasks. The model is trained
for several iterations to adapt to each new task, using some specific
approaches to reduce the forgetting of old tasks. In this work, we propose a
method for using generalizable audio embeddings produced by a pre-trained model
to develop an online incremental learner that solves sequential audio
classification tasks over time. Specifically, we inject a layer with a
nonlinear activation function between the pre-trained model's audio embeddings
and the classifier; this layer expands the dimensionality of the embeddings and
effectively captures the distinct characteristics of sound classes. Our method
adapts the model in a single forward pass (online) through the training samples
of any task, with minimal forgetting of old tasks. We demonstrate the
performance of the proposed method in two incremental learning setups: one
class-incremental learning using ESC-50 and one domain-incremental learning of
different cities from the TAU Urban Acoustic Scenes 2019 dataset; for both
cases, the proposed approach outperforms other methods.

</details>


### [13] [A Solution of Ultra Wideband Based High-resolution and Lossless Audio Transmission](https://arxiv.org/abs/2508.20782)
*Fengyun Zhang*

Main category: eess.AS

TL;DR: 本文提出基于超宽带(UWB)技术的高分辨率无损音频传输方案，解决现有无线音频传输技术在带宽、压缩、延迟和兼容性方面的局限性


<details>
  <summary>Details</summary>
Motivation: 现有无线音频传输技术存在数据带宽不足、压缩损失、延迟高和设备兼容性差等问题，无法满足高保真实时音频传输需求

Method: 采用超宽带(UWB)技术，利用其高带宽特性实现高分辨率无损音频传输，同时提供超低延迟和精确定位能力

Result: UWB技术能够提供足够的带宽支持高保真音频传输，实现超低延迟，解决音视频同步问题，并支持增强现实和虚拟现实应用的精确位置跟踪

Conclusion: UWB技术是解决无线音频传输挑战的理想方案，不仅能实现高分辨率无损音频传输，还能为AR/VR应用提供额外的定位功能

Abstract: This paper provides an overview of the current challenges in wireless audio
transmission and highlights the limitations of existing technologies regarding
data bandwidth, data compression, latency, and inter-device compatibility. To
address these shortcomings, it proposes a high-resolution, lossless audio
transmission scheme utilizing ultra wideband (UWB) technology. UWB emerges as a
promising solution by offering the necessary bandwidth to enable exceptional
sound quality with ultra-low latency, making it ideal for real-time audio
applications and addressing synchronization concerns in audio-visual use cases.
Additionally, UWB's unique capabilities extend beyond high-resolution audio,
allowing for precise location tracking in augmented and virtual reality
applications.

</details>


### [14] [Leveraging Discriminative Latent Representations for Conditioning GAN-Based Speech Enhancement](https://arxiv.org/abs/2508.20859)
*Shrishti Saha Shetu,Emanuël A. P. Habets,Andreas Brendel*

Main category: eess.AS

TL;DR: 提出DisCoGAN方法，利用判别式语音增强模型的潜在特征作为条件特征，改进基于GAN的语音增强，在低信噪比场景中表现优异


<details>
  <summary>Details</summary>
Motivation: 生成式语音增强方法在极低信噪比场景下的性能仍有局限，需要解决这一挑战

Method: 使用判别式语音增强模型提取的潜在特征作为通用条件特征，改进GAN-based语音增强，提出DisCoGAN方法

Result: DisCoGAN在低信噪比场景中表现优于基线模型，在高信噪比条件和真实录音中也保持竞争力

Conclusion: 该方法通过判别式条件特征有效提升了生成式语音增强性能，特别是在挑战性的低信噪比环境中

Abstract: Generative speech enhancement methods based on generative adversarial
networks (GANs) and diffusion models have shown promising results in various
speech enhancement tasks. However, their performance in very low
signal-to-noise ratio (SNR) scenarios remains under-explored and limited, as
these conditions pose significant challenges to both discriminative and
generative state-of-the-art methods. To address this, we propose a method that
leverages latent features extracted from discriminative speech enhancement
models as generic conditioning features to improve GAN-based speech
enhancement. The proposed method, referred to as DisCoGAN, demonstrates
performance improvements over baseline models, particularly in low-SNR
scenarios, while also maintaining competitive or superior performance in
high-SNR conditions and on real-world recordings. We also conduct a
comprehensive evaluation of conventional GAN-based architectures, including
GANs trained end-to-end, GANs as a first processing stage, and post-filtering
GANs, as well as discriminative models under low-SNR conditions. We show that
DisCoGAN consistently outperforms existing methods. Finally, we present an
ablation study that investigates the contributions of individual components
within DisCoGAN and analyzes the impact of the discriminative conditioning
method on overall performance.

</details>


### [15] [Automatic Inspection Based on Switch Sounds of Electric Point Machines](https://arxiv.org/abs/2508.20870)
*Ayano Shibata,Toshiki Gunji,Mitsuaki Tsuda,Takashi Endo,Kota Dohi,Tomoya Nishida,Satoko Nomoto*

Main category: eess.AS

TL;DR: 日本铁路公司与日立公司合作开发基于声音分析的电子转辙机故障检测系统，使用摄像头和麦克风替代人工检查，实现远程监控和实时故障检测


<details>
  <summary>Details</summary>
Motivation: 替代人工设备检查以节省劳动力，提供适当的预防性维护，降低因设备故障导致的停机时间，解决电气特性监测难以替代视觉检查的问题

Method: 在电子转辙机中安装摄像头和麦克风，基于声音信息检测转辙切换错误，特别关注"切换声音"的分析技术

Result: 获得了预期的测试结果，能够实时检测设备故障，减少对视觉检查的需求

Conclusion: 基于声音分析的自动化检查方法可以有效替代人工检查，实现设备故障的实时检测和预防性维护

Abstract: Since 2018, East Japan Railway Company and Hitachi, Ltd. have been working to
replace human inspections with IoT-based monitoring. The purpose is
Labor-saving required for equipment inspections and provide appropriate
preventive maintenance. As an alternative to visual inspection, it has been
difficult to substitute electrical characteristic monitoring, and the
introduction of new high-performance sensors has been costly. In 2019, we
implemented cameras and microphones in an ``NS'' electric point machines to
reduce downtime from equipment failures, allowing for remote monitoring of
lock-piece conditions. This method for detecting turnout switching errors based
on sound information was proposed, and the expected test results were obtained.
The proposed method will make it possible to detect equipment failures in real
time, thereby reducing the need for visual inspections. This paper presents the
results of our technical studies aimed at automating the inspection of
electronic point machines using sound, specifically focusing on ``switch
sound'' beginning in 2019.

</details>


### [16] [Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System](https://arxiv.org/abs/2508.20983)
*Hashim Ali,Surya Subramani,Lekha Bollinani,Nithin Sai Adupa,Sali El-Loh,Hafiz Malik*

Main category: eess.AS

TL;DR: SAFE挑战赛评估合成语音检测，研究团队使用AASIST架构结合WavLM前端和RawBoost增强，在多语言数据集上训练，在未修改音频和洗钱音频检测任务中获得第二名


<details>
  <summary>Details</summary>
Motivation: 系统评估合成语音检测在不同场景下的性能，包括未修改音频、压缩处理音频和刻意规避检测的洗钱音频，探索自监督学习前端、训练数据组成和音频长度配置对深度伪造检测鲁棒性的影响

Method: 采用基于AASIST的方法架构，整合WavLM大型前端和RawBoost数据增强技术，在多语言数据集（256,600个样本，9种语言，70多个TTS系统）上进行训练，系统研究不同SSL前端、三种训练数据版本和两种音频长度配置

Result: 在SAFE挑战赛的Task 1（未修改音频检测）和Task 3（洗钱音频检测）中均获得第二名，证明了方法的强泛化能力和鲁棒性

Conclusion: 该方法在合成语音检测方面表现出优秀的性能，特别是在处理复杂音频场景时具有很好的泛化能力和鲁棒性，为深度伪造检测提供了有效的解决方案

Abstract: The SAFE Challenge evaluates synthetic speech detection across three tasks:
unmodified audio, processed audio with compression artifacts, and laundered
audio designed to evade detection. We systematically explore self-supervised
learning (SSL) front-ends, training data compositions, and audio length
configurations for robust deepfake detection. Our AASIST-based approach
incorporates WavLM large frontend with RawBoost augmentation, trained on a
multilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS
systems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.
Through extensive experimentation with different SSL front-ends, three training
data versions, and two audio lengths, we achieved second place in both Task 1
(unmodified audio detection) and Task 3 (laundered audio detection),
demonstrating strong generalization and robustness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [17] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: MoTAS是一个用于阿尔茨海默病早期筛查的鲁棒框架，通过TTS数据增强和MoE特征选择机制，在ADReSSo数据集上达到85.71%的准确率


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病语音筛查中数据有限和缺乏细粒度自适应特征选择的问题

Method: 使用ASR获取转录文本，TTS进行数据增强，然后通过MoE机制动态选择最优声学和文本特征进行融合分类

Result: 在ADReSSo数据集上达到85.71%的准确率，优于现有基线方法

Conclusion: MoTAS在数据有限的实际AD筛查场景中具有重要实用价值，TTS增强和MoE机制均对性能提升有显著贡献

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [18] [Flowing Straighter with Conditional Flow Matching for Accurate Speech Enhancement](https://arxiv.org/abs/2508.20584)
*Mattias Cross,Anton Ragni*

Main category: cs.SD

TL;DR: 本文研究概率路径的直线性对语音增强质量的影响，比较了弯曲路径（Schrodinger桥）和直线路径（条件流匹配）方法，发现时间无关的方差比梯度对样本质量影响更大，直线路径能提升语音增强性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的生成式语音增强方法学习弯曲的概率路径来建模干净和噪声语音之间的映射，但弯曲路径的影响未知。机器学习研究发现直线路径（如条件流匹配）更容易训练且泛化更好，因此需要量化路径直线性对语音增强质量的影响。

Method: 使用Schrodinger桥进行实验，展示某些配置能产生更直的路径；提出独立条件流匹配方法用于语音增强，建模噪声和干净语音间的直线路径；开发一步推理解决方案，将训练好的流模型直接用于预测。

Result: 实验证明时间无关的方差比梯度对样本质量影响更大；条件流匹配改善了多个语音质量指标，但需要多步推理；一步推理解决方案有效解决了推理步骤多的问题。

Conclusion: 相比弯曲的时间相关概率路径，更直的时间无关概率路径能改善生成式语音增强的性能，直线路径在语音增强中具有优势。

Abstract: Current flow-based generative speech enhancement methods learn curved
probability paths which model a mapping between clean and noisy speech. Despite
impressive performance, the implications of curved probability paths are
unknown. Methods such as Schrodinger bridges focus on curved paths, where
time-dependent gradients and variance do not promote straight paths. Findings
in machine learning research suggest that straight paths, such as conditional
flow matching, are easier to train and offer better generalisation. In this
paper we quantify the effect of path straightness on speech enhancement
quality. We report experiments with the Schrodinger bridge, where we show that
certain configurations lead to straighter paths. Conversely, we propose
independent conditional flow-matching for speech enhancement, which models
straight paths between noisy and clean speech. We demonstrate empirically that
a time-independent variance has a greater effect on sample quality than the
gradient. Although conditional flow matching improves several speech quality
metrics, it requires multiple inference steps. We rectify this with a one-step
solution by inferring the trained flow-based model as if it was directly
predictive. Our work suggests that straighter time-independent probability
paths improve generative speech enhancement over curved time-dependent paths.

</details>


### [19] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: Amadeus是一个创新的符号音乐生成框架，采用两级架构：自回归模型处理音符序列，双向离散扩散模型处理属性，实现了性能显著提升和4倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐生成模型假设音符属性具有固定的单向时间依赖关系，但研究发现不同属性作为初始token时性能相当，表明音符属性本质上是并发无序的集合而非时序依赖序列。

Method: 提出Amadeus框架：1）自回归模型处理音符序列；2）双向离散扩散模型处理属性；3）MLSDES策略增强音乐表示判别性；4）CIEM模块通过注意力机制强化音符潜在向量表示。

Result: 在无条件和文本条件生成任务中，Amadeus在多个指标上显著优于SOTA模型，同时实现至少4倍加速，并展示了无需训练即可进行细粒度音符属性控制的可行性。

Conclusion: Amadeus框架通过重新思考音符属性的本质特性，采用创新的两级架构和增强策略，为符号音乐生成提供了新的有效解决方案，同时构建了最大的开源符号音乐数据集AMD来探索性能上限。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


### [20] [Unified Multi-task Learning for Voice-Based Detection of Diverse Clinical Conditions](https://arxiv.org/abs/2508.20717)
*Ran Piao,Yuan Lu,Hareld Kemps,Tong Xia,Aaqib Saeed*

Main category: cs.SD

TL;DR: MARVEL是一个多任务学习框架，使用语音特征同时检测9种神经、呼吸和声音疾病，无需传输原始音频，在Bridge2AI-Voice数据集上表现出色，AUROC达到0.78。


<details>
  <summary>Details</summary>
Motivation: 现有语音健康评估方法通常只关注单一疾病，未能充分利用语音中丰富的多层面信息，需要开发能够同时检测多种疾病的可扩展解决方案。

Method: 采用双分支架构，包含专用编码器和任务特定头部，共享共同的声学骨干网络，实现跨条件知识转移，仅使用派生声学特征而不需要原始音频。

Result: 整体AUROC为0.78，神经系统疾病表现优异（AUROC=0.89），阿尔茨海默病/轻度认知障碍达到0.97，比单模态基线提升5-19%，在9个任务中的7个超越最先进的自监督模型。

Conclusion: 该研究证明单一统一模型可有效筛查多种疾病，为资源受限和远程医疗环境中的可部署语音诊断奠定了基础，学习到的表征与临床认可的声学模式一致。

Abstract: Voice-based health assessment offers unprecedented opportunities for
scalable, non-invasive disease screening, yet existing approaches typically
focus on single conditions and fail to leverage the rich, multi-faceted
information embedded in speech. We present MARVEL (Multi-task Acoustic
Representations for Voice-based Health Analysis), a privacy-conscious multitask
learning framework that simultaneously detects nine distinct neurological,
respiratory, and voice disorders using only derived acoustic features,
eliminating the need for raw audio transmission. Our dual-branch architecture
employs specialized encoders with task-specific heads sharing a common acoustic
backbone, enabling effective cross-condition knowledge transfer. Evaluated on
the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC
of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89),
particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97).
Our framework consistently outperforms single-modal baselines by 5-19% and
surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while
correlation analysis reveals that the learned representations exhibit
meaningful similarities with established acoustic features, indicating that the
model's internal representations are consistent with clinically recognized
acoustic patterns. By demonstrating that a single unified model can effectively
screen for diverse conditions, this work establishes a foundation for
deployable voice-based diagnostics in resource-constrained and remote
healthcare settings.

</details>


### [21] [Speech Emotion Recognition via Entropy-Aware Score Selection](https://arxiv.org/abs/2508.20796)
*ChenYi Chua,JunKai Wong,Chengxin Chen,Xiaoxiao Miao*

Main category: cs.SD

TL;DR: 提出了一种基于熵感知分数选择的多模态语音情感识别框架，结合语音和文本预测，通过后期分数融合提升传统单模态系统性能


<details>
  <summary>Details</summary>
Motivation: 传统单模态语音情感识别系统存在置信度限制，需要利用多模态信息（语音和文本）来提升识别准确性和可靠性

Method: 使用wav2vec2.0作为声学模型和RoBERTa-XLM进行情感分析的双管道架构，通过Whisper-large-v3生成转录，采用基于熵和变熵阈值的后期分数融合方法，以及情感类别映射策略

Result: 在IEMOCAP和MSP-IMPROV数据集上验证，该方法相比传统单模态系统提供了实用且可靠的性能提升

Conclusion: 提出的多模态框架通过熵感知分数选择和情感映射策略，有效整合语音和文本信息，为语音情感识别提供了增强的解决方案

Abstract: In this paper, we propose a multimodal framework for speech emotion
recognition that leverages entropy-aware score selection to combine speech and
textual predictions. The proposed method integrates a primary pipeline that
consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that
consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions
generated via Whisper-large-v3. We propose a late score fusion approach based
on entropy and varentropy thresholds to overcome the confidence constraints of
primary pipeline predictions. A sentiment mapping strategy translates three
sentiment categories into four target emotion classes, enabling coherent
integration of multimodal predictions. The results on the IEMOCAP and
MSP-IMPROV datasets show that the proposed method offers a practical and
reliable enhancement over traditional single-modality systems.

</details>


### [22] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 该论文提出了大规模语音识别数据集OLMoASR-Pool和高质量数据集OLMoASR-Mix，训练了一系列从39M到1.5B参数的零样本语音识别模型，性能与OpenAI Whisper相当。


<details>
  <summary>Details</summary>
Motivation: 虽然训练数据的规模和质量提升带来了显著进步，但对语音识别领域的影响仍未充分探索。需要研究和开发稳健的零样本语音识别模型。

Method: 从3M小时英语音频和17M词幕的OLMoASR-Pool数据集出发，设计文本启应过滤器去除低质量或错误转写数据，生成1M小时高质量音频-词幕对的OLMoASR-Mix数据集，用乎训练39M到1.5B参数的模型套件。

Result: 所有模型规模下，OLMoASR在短语音和长语音识别性能与OpenAI Whisper相当。OLMoASR-medium.en在短语音和长语音识别上分别达到12.8%和11.0%的词误率，与相当参数规模的Whisper-medium.en(12.4%和10.5%)性能相似。

Conclusion: 研究展示了高质量训练数据对语音识别模型性能的重要性，OLMoASR模型套件为稳健语音处理研究提供了公开资源。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


### [23] [SincQDR-VAD: A Noise-Robust Voice Activity Detection Framework Leveraging Learnable Filters and Ranking-Aware Optimization](https://arxiv.org/abs/2508.20885)
*Chien-Chun Wang,En-Lun Yu,Jeih-Weih Hung,Shih-Chieh Huang,Berlin Chen*

Main category: cs.SD

TL;DR: 提出了SincQDR-VAD框架，结合Sinc提取器前端和二次差异排序损失，在噪声环境中显著提升语音活动检测性能，同时减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有语音活动检测方法在噪声环境中鲁棒性不足，且帧级分类损失与评估指标关联不紧密，需要更紧凑和鲁棒的解决方案。

Method: 使用可学习带通滤波器的Sinc提取器前端捕获抗噪声频谱特征，结合新颖的二次差异排序损失优化语音和非语音帧的成对得分顺序。

Result: 在代表性基准数据集上，框架显著提高了AUROC和F2-Score，参数使用量仅为先前方法的69%。

Conclusion: SincQDR-VAD框架在保持高效率的同时，显著提升了语音活动检测在噪声环境中的性能，具有实际应用价值。

Abstract: Voice activity detection (VAD) is essential for speech-driven applications,
but remains far from perfect in noisy and resource-limited environments.
Existing methods often lack robustness to noise, and their frame-wise
classification losses are only loosely coupled with the evaluation metric of
VAD. To address these challenges, we propose SincQDR-VAD, a compact and robust
framework that combines a Sinc-extractor front-end with a novel quadratic
disparity ranking loss. The Sinc-extractor uses learnable bandpass filters to
capture noise-resistant spectral features, while the ranking loss optimizes the
pairwise score order between speech and non-speech frames to improve the area
under the receiver operating characteristic curve (AUROC). A series of
experiments conducted on representative benchmark datasets show that our
framework considerably improves both AUROC and F2-Score, while using only 69%
of the parameters compared to prior arts, confirming its efficiency and
practical viability.

</details>


### [24] [Learning Robust Spatial Representations from Binaural Audio through Feature Distillation](https://arxiv.org/abs/2508.20914)
*Holger Severin Bovbjerg,Jan Østergaard,Jesper Jensen,Shinji Watanabe,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 该论文提出了一种基于特征蒸馏的预训练方法，用于学习双耳语音的空间表示，无需数据标签，通过预测增强语音的空间特征来预训练模型，然后在DoA估计任务上进行微调。


<details>
  <summary>Details</summary>
Motivation: 深度表示学习在音频任务中表现优异，但在多通道音频空间表示学习方面研究不足，特别是在无标签数据的情况下学习鲁棒的空间表示。

Method: 使用特征蒸馏预训练框架：从干净双耳语音样本计算空间特征作为预测标签，然后用神经网络从对应的增强语音预测这些特征。预训练后丢弃空间特征预测器，用学习到的编码器权重初始化DoA估计模型进行微调。

Result: 实验表明，经过微调的预训练模型在噪声和混响环境中的方向估计性能优于全监督模型和传统信号处理方法。

Conclusion: 基于特征蒸馏的无监督预训练方法能够有效学习空间表示，提升DoA估计在挑战性环境中的性能。

Abstract: Recently, deep representation learning has shown strong performance in
multiple audio tasks. However, its use for learning spatial representations
from multichannel audio is underexplored. We investigate the use of a
pretraining stage based on feature distillation to learn a robust spatial
representation of binaural speech without the need for data labels. In this
framework, spatial features are computed from clean binaural speech samples to
form prediction labels. These clean features are then predicted from
corresponding augmented speech using a neural network. After pretraining, we
throw away the spatial feature predictor and use the learned encoder weights to
initialize a DoA estimation model which we fine-tune for DoA estimation. Our
experiments demonstrate that the pretrained models show improved performance in
noisy and reverberant environments after fine-tuning for direction-of-arrival
estimation, when compared to fully supervised models and classic signal
processing methods.

</details>


### [25] [WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations](https://arxiv.org/abs/2508.20976)
*Jaeyeon Kim,Heeseung Yun,Sang Hoon Woo,Chao-Han Huck Yang,Gunhee Kim*

Main category: cs.SD

TL;DR: 提出了WoW-Bench基准测试，用于评估大型音频语言模型在低频听觉感知和认知方面的能力，特别是针对海洋哺乳动物叫声的新颖声音处理。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型在低频听觉（如音高和时长检测）方面的能力未被充分探索，而这些能力对于处理现实世界中分布外任务和基于细粒度声学线索进行推理至关重要。

Method: 引入World-of-Whale基准测试（WoW-Bench），包含感知基准（分类新颖声音）和认知基准（基于Bloom分类法评估记忆、理解、应用和分析声音事件的能力），并添加干扰问题以确保模型真正通过听觉解决问题。

Result: 实验显示最先进的大型音频语言模型性能远低于人类水平，表明这些模型需要更强的听觉基础能力。

Conclusion: 大型音频语言模型在低频听觉感知和认知方面存在明显不足，需要进一步改进模型的听觉基础能力以处理现实世界的音频理解任务。

Abstract: Large audio language models (LALMs) extend language understanding into the
auditory domain, yet their ability to perform low-level listening, such as
pitch and duration detection, remains underexplored. However, low-level
listening is critical for real-world, out-of-distribution tasks where models
must reason about unfamiliar sounds based on fine-grained acoustic cues. To
address this gap, we introduce the World-of-Whale benchmark (WoW-Bench) to
evaluate low-level auditory perception and cognition using marine mammal
vocalizations. WoW-bench is composed of a Perception benchmark for categorizing
novel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assess
the abilities to remember, understand, apply, and analyze sound events. For the
Cognition benchmark, we additionally introduce distractor questions to evaluate
whether models are truly solving problems through listening rather than relying
on other heuristics. Experiments with state-of-the-art LALMs show performance
far below human levels, indicating a need for stronger auditory grounding in
LALMs.

</details>

{"id": "2508.21193", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.21193", "abs": "https://arxiv.org/abs/2508.21193", "authors": ["Coralie Serrand", "Gilles Boulianne", "Amira Morsli"], "title": "Benchmarking Large Pretrained Multilingual Models on Qu\u00e9bec French Speech Recognition", "comment": "11 pages, 3 figures", "summary": "We evaluate the performance of large pretrained multilingual speech\nrecognition models on a regional variety of French spoken in Qu\\'ebec, Canada,\nin terms of speed, word error rate and semantic accuracy. To this end we build\na benchmark and evaluation pipeline based on the CommissionsQc datasets, a\ncorpus of spontaneous conversations recorded during public inquiries recently\nheld in Qu\\'ebec. Published results for these models on well-known benchmarks\nsuch as FLEURS or CommonVoice are not good predictors of the performance we\nobserve on CommissionsQC. Our results should be of interest for practitioners\ninterested in building speech applications for realistic conditions or regional\nlanguage varieties.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u9b41\u5317\u514b\u6cd5\u8bed\u65b9\u8a00\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5305\u62ec\u901f\u5ea6\u3001\u8bcd\u9519\u8bef\u7387\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u4e0d\u80fd\u51c6\u786e\u9884\u6d4b\u5728\u771f\u5b9e\u533a\u57df\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982FLEURS\u3001CommonVoice\uff09\u4e0a\u7684\u8868\u73b0\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u5176\u5728\u771f\u5b9e\u533a\u57df\u65b9\u8a00\uff08\u5982\u9b41\u5317\u514b\u6cd5\u8bed\uff09\u4e0a\u7684\u5b9e\u9645\u6027\u80fd\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u8d34\u8fd1\u73b0\u5b9e\u5e94\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u9b41\u5317\u514b\u516c\u5171\u8c03\u67e5\u5bf9\u8bdd\u8bed\u6599\u5e93CommissionsQc\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4ece\u901f\u5ea6\u3001\u8bcd\u9519\u8bef\u7387\u548c\u8bed\u4e49\u51c6\u786e\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u5927\u578b\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u3002", "result": "\u5728\u9b41\u5317\u514b\u6cd5\u8bed\u65b9\u8a00\u4e0a\uff0c\u5927\u578b\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u4e0d\u80fd\u5f88\u597d\u5730\u9884\u6d4b\u5728\u533a\u57df\u65b9\u8a00\u4e0a\u7684\u5b9e\u9645\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u9762\u5411\u771f\u5b9e\u6761\u4ef6\u548c\u533a\u57df\u8bed\u8a00\u53d8\u4f53\u7684\u8bed\u97f3\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u65b9\u8a00\u548c\u771f\u5b9e\u573a\u666f\u8fdb\u884c\u4e13\u95e8\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.21225", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21225", "abs": "https://arxiv.org/abs/2508.21225", "authors": ["Abhijit Sinha", "Hemant Kumar Kathania", "Sudarsana Reddy Kadiri", "Shrikanth Narayanan"], "title": "Can Layer-wise SSL Features Improve Zero-Shot ASR Performance for Children's Speech?", "comment": "Accepted", "summary": "Automatic Speech Recognition (ASR) systems often struggle to accurately\nprocess children's speech due to its distinct and highly variable acoustic and\nlinguistic characteristics. While recent advancements in self-supervised\nlearning (SSL) models have greatly enhanced the transcription of adult speech,\naccurately transcribing children's speech remains a significant challenge. This\nstudy investigates the effectiveness of layer-wise features extracted from\nstate-of-the-art SSL pre-trained models - specifically, Wav2Vec2, HuBERT,\nData2Vec, and WavLM in improving the performance of ASR for children's speech\nin zero-shot scenarios. A detailed analysis of features extracted from these\nmodels was conducted, integrating them into a simplified DNN-based ASR system\nusing the Kaldi toolkit. The analysis identified the most effective layers for\nenhancing ASR performance on children's speech in a zero-shot scenario, where\nWSJCAM0 adult speech was used for training and PFSTAR children speech for\ntesting. Experimental results indicated that Layer 22 of the Wav2Vec2 model\nachieved the lowest Word Error Rate (WER) of 5.15%, representing a 51.64%\nrelative improvement over the direct zero-shot decoding using Wav2Vec2 (WER of\n10.65%). Additionally, age group-wise analysis demonstrated consistent\nperformance improvements with increasing age, along with significant gains\nobserved even in younger age groups using the SSL features. Further experiments\non the CMU Kids dataset confirmed similar trends, highlighting the\ngeneralizability of the proposed approach.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u591a\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u5c42\u7ea7\u7279\u5f81\u6548\u679c\uff0cWav2Vec2\u6a21\u578b\u7b2c22\u5c42\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff0cWER\u76f8\u5bf9\u6539\u558451.64%", "motivation": "\u5b58\u5728\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u513f\u7ae5\u8bed\u97f3\u5904\u7406\u4e0a\u7684\u6027\u80fd\u7f3a\u53e3\uff0c\u5e76\u9700\u8981\u7814\u7a76\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u5c42\u7ea7\u7279\u5f81\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6548\u679c", "method": "\u63d0\u53d6Wav2Vec2\u3001HuBERT\u3001Data2Vec\u548cWavLM\u7b49SSL\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5c42\u7ea7\u7279\u5f81\uff0c\u5e76\u96c6\u6210\u5230Kaldi\u5de5\u5177\u96c6\u7684\u7b80\u5316DNN-ASR\u7cfb\u7edf\u4e2d\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8fdb\u884c\u6d4b\u8bd5", "result": "Wav2Vec2\u6a21\u578b\u7b2c22\u5c42\u83b7\u5f97\u6700\u4f4eWER 5.15%\uff0c\u76f8\u6bd4\u76f4\u63a5\u96f6\u6837\u672c\u89e3\u7801\u6539\u558451.64%\uff0c\u5e76\u5728\u4e0d\u540c\u5e74\u9f84\u6bb5\u90fd\u8868\u73b0\u4e00\u81f4\u6027\u6539\u5584\uff0c\u5728CMU Kids\u6570\u636e\u96c6\u4e0a\u4e5f\u9a8c\u8bc1\u4e86\u901a\u7528\u6027", "conclusion": "\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684SSL\u6a21\u578b\u5c42\u7ea7\u7279\u5f81\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0cWav2Vec2\u6a21\u578b\u7684\u6df1\u5c42\u7279\u5f81\u8868\u73b0\u7279\u522b\u4f18\u5f02"}}
{"id": "2508.21248", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21248", "abs": "https://arxiv.org/abs/2508.21248", "authors": ["Subham Kutum", "Abhijit Sinha", "Hemant Kumar Kathania", "Sudarsana Reddy Kadiri", "Mahesh Chandra Govil"], "title": "Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models", "comment": "Accepted", "summary": "Numerous methods have been proposed to enhance Keyword Spotting (KWS) in\nadult speech, but children's speech presents unique challenges for KWS systems\ndue to its distinct acoustic and linguistic characteristics. This paper\nintroduces a zero-shot KWS approach that leverages state-of-the-art\nself-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec.\nFeatures are extracted layer-wise from these SSL models and used to train a\nKaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for\ntraining, while the PFSTAR children's speech dataset was used for testing,\ndemonstrating the zero-shot capability of our method. Our approach achieved\nstate-of-the-art results across all keyword sets for children's speech.\nNotably, the Wav2Vec2 model, particularly layer 22, performed the best,\ndelivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of\nfalse alarm and probability of miss of 0.0164 and 0.0547 respectively, for a\nset of 30 keywords. Furthermore, age-specific performance evaluation confirmed\nthe system's effectiveness across different age groups of children. To assess\nthe system's robustness against noise, additional experiments were conducted\nusing the best-performing layer of the best-performing Wav2Vec2 model. The\nresults demonstrated a significant improvement over traditional MFCC-based\nbaseline, emphasizing the potential of SSL embeddings even in noisy conditions.\nTo further generalize the KWS framework, the experiments were repeated for an\nadditional CMU dataset. Overall the results highlight the significant\ncontribution of SSL features in enhancing Zero-Shot KWS performance for\nchildren's speech, effectively addressing the challenges associated with the\ndistinct characteristics of child speakers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u96f6\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e13\u95e8\u5904\u7406\u513f\u7ae5\u8bed\u97f3\u7684\u7279\u6b8a\u6311\u6218\uff0c\u5728\u513f\u7ae5\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u513f\u7ae5\u8bed\u97f3\u5177\u6709\u72ec\u7279\u7684\u97f3\u54cd\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u5bf9\u4f20\u7edf\u5173\u952e\u8bcd\u68c0\u6d4b\u7cfb\u7edf\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u6765\u5904\u7406\u3002", "method": "\u5229\u7528Wav2Vec2\u3001HuBERT\u548cData2Vec\u7b49\u5148\u8fdb\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5c42\u7ea7\u7279\u5f81\u63d0\u53d6\uff0c\u8bad\u7ec3Kaldi\u57fa\u4e8eDNN\u7684KWS\u7cfb\u7edf\uff0c\u5728\u6210\u4eba\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\u76f4\u63a5\u6d4b\u8bd5\u513f\u7ae5\u8bed\u97f3\u3002", "result": "\u5728PFSTAR\u513f\u7ae5\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6700\u4f73\u6027\u80fd\uff1aWav2Vec2\u6a21\u578b\u7b2c22\u5c42\u8fbe\u5230ATWV 0.691\u3001MTWV 0.7003\uff0c\u9519\u8bef\u544a\u8b66\u7387\u548c\u6f0f\u68c0\u7387\u5206\u522b\u4e3a0.0164\u548c0.0547\u3002\u5728\u566a\u58f0\u73af\u5883\u4e2d\u4e5f\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMFCC\u57fa\u7ebf\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u7279\u5f81\u80fd\u591f\u6709\u6548\u63d0\u5347\u513f\u7ae5\u8bed\u97f3\u7684\u96f6\u6837\u672c\u5173\u952e\u8bcd\u68c0\u6d4b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u513f\u7ae5\u8bed\u97f3\u7279\u6b8a\u7279\u5f81\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2508.21347", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.21347", "abs": "https://arxiv.org/abs/2508.21347", "authors": ["Sabbir Ahmed", "Nursadul Mamun", "Md Azad Hossain"], "title": "Cochleagram-based Noise Adapted Speaker Identification System for Distorted Speech", "comment": "10 pages, 10 figures, 4 tables", "summary": "Speaker Identification refers to the process of identifying a person using\none's voice from a collection of known speakers. Environmental noise,\nreverberation and distortion make the task of automatic speaker identification\nchallenging as extracted features get degraded thus affecting the performance\nof the speaker identification (SID) system. This paper proposes a robust noise\nadapted SID system under noisy, mismatched, reverberated and distorted\nenvironments. This method utilizes an auditory features called cochleagram to\nextract speaker characteristics and thus identify the speaker. A $128$ channel\ngammatone filterbank with a frequency range from $50$ to $8000$ Hz was used to\ngenerate 2-D cochleagrams. Wideband as well as narrowband noises were used\nalong with clean speech to obtain noisy cochleagrams at various levels of\nsignal to noise ratio (SNR). Both clean and noisy cochleagrams of only $-5$ dB\nSNR were then fed into a convolutional neural network (CNN) to build a speaker\nmodel in order to perform SID which is referred as noise adapted speaker model\n(NASM). The NASM was trained using a certain noise and then was evaluated using\nclean and various types of noises. Moreover, the robustness of the proposed\nsystem was tested using reverberated as well as distorted test data.\nPerformance of the proposed system showed a measurable accuracy improvement\nover existing neurogram based SID system.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8033\u87ba\u56fe\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u566a\u58f0\u9002\u914d\u8bb2\u8005\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5bf9\u566a\u58f0\u3001\u56de\u54cd\u548c\u5931\u771f\u73af\u5883\u7684\u9002\u5e94\u6765\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73af\u5883\u566a\u58f0\u3001\u56de\u54cd\u548c\u5931\u771f\u4f1a\u964d\u4f4e\u8bed\u97f3\u7279\u5f81\u63d0\u53d6\u7684\u8d28\u91cf\uff0c\u5f71\u54cd\u81ea\u52a8\u8bb2\u8005\u8bc6\u522b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u7cfb\u7edf\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u4f7f\u7528128\u901a\u9053\u7684\u4e2d\u8033\u6d1e\u6ee4\u6ce2\u5668\u751f\u6210\u8033\u87ba\u56fe\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u566a\u58f0\u9002\u914d\u8bb2\u8005\u6a21\u578b(NASM)\uff0c\u5728-5dB\u4fe1\u566a\u6bd4\u4e0b\u8bad\u7ec3\u6a21\u578b\u4ee5\u63d0\u9ad8\u5bf9\u566a\u58f0\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u7cfb\u7edf\u5728\u566a\u58f0\u3001\u56de\u54cd\u548c\u5931\u771f\u73af\u5883\u4e0b\u90fd\u663e\u793a\u51fa\u4e86\u8f83\u9ad8\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u795e\u7ecf\u56fe\u57fa\u7840\u8bb2\u8005\u8bc6\u522b\u7cfb\u7edf\u3002", "conclusion": "\u57fa\u4e8e\u8033\u87ba\u56fe\u7279\u5f81\u548c\u566a\u58f0\u9002\u914d\u6a21\u578b\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u8bb2\u8005\u8bc6\u522b\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.21079", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.21079", "abs": "https://arxiv.org/abs/2508.21079", "authors": ["Kaixuan Bao", "Wei Xu", "Xiaohu You", "Derrick Wing Kwan Ng"], "title": "A Framework of Arithmetic-Level Variable Precision Computing for In-Memory Architecture: Case Study in MIMO Signal Processing", "comment": "to appear in TMC", "summary": "Computational complexity poses a significant challenge in wireless\ncommunication. Most existing attempts aim to reduce it through\nalgorithm-specific approaches. However, the precision of computing, which\ndirectly relates to both computing performance and computational complexity, is\na dimension that is fundamental but rarely explored in the literature. With the\nemerging architecture of in-memory computing, variable precision computing\n(VPC) is enabled, allowing each arithmetic operation to be processed with a\ndistinct and specifically optimized computing precision. In this paper, we\nestablish a unified framework of arithmetic-level variable precision computing\n(AL-VPC), which aims to determine the optimized computing precision for each\narithmetic operation. We first develop an arithmetic propagation error model\nexploiting stochastic analysis, and then formulate a mathematical optimization\nproblem to strike balance between computing performance and computational\ncomplexity. Two algorithms, namely, offline VPC and online VPC, are proposed to\nsolve the problem considering various practical concerns. Particularly, in a\ncase study on zero-forcing (ZF) precoding, we reveal the Pareto boundary\nbetween computing performance and complexity, which exhibits up to a 60%\nsum-rate enhancement or equivalently up to a 30% complexity reduction compared\nto the traditional fixed-length methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b97\u672f\u7ea7\u53ef\u53d8\u7cbe\u5ea6\u8ba1\u7b97(AL-VPC)\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7b97\u672f\u8fd0\u7b97\u4f18\u5316\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u5e73\u8861\u8ba1\u7b97\u6027\u80fd\u548c\u590d\u6742\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u7cbe\u5ea6\u65b9\u6cd5\u53ef\u63d0\u534760%\u548c\u901f\u7387\u6216\u964d\u4f4e30%\u590d\u6742\u5ea6", "motivation": "\u8ba1\u7b97\u590d\u6742\u5ea6\u662f\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u91c7\u7528\u7b97\u6cd5\u7279\u5b9a\u65b9\u5f0f\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u4f46\u8ba1\u7b97\u7cbe\u5ea6\u8fd9\u4e00\u76f4\u63a5\u5f71\u54cd\u8ba1\u7b97\u6027\u80fd\u548c\u590d\u6742\u5ea6\u7684\u57fa\u672c\u7ef4\u5ea6\u5374\u5f88\u5c11\u88ab\u63a2\u7d22", "method": "\u5efa\u7acb\u7b97\u672f\u7ea7\u53ef\u53d8\u7cbe\u5ea6\u8ba1\u7b97\u7edf\u4e00\u6846\u67b6\uff0c\u5f00\u53d1\u57fa\u4e8e\u968f\u673a\u5206\u6790\u7684\u7b97\u672f\u4f20\u64ad\u8bef\u5dee\u6a21\u578b\uff0c\u63d0\u51fa\u79bb\u7ebfVPC\u548c\u5728\u7ebfVPC\u4e24\u79cd\u7b97\u6cd5\uff0c\u4ee5\u96f6\u8feb\u9884\u7f16\u7801\u4e3a\u6848\u4f8b\u8fdb\u884c\u7814\u7a76", "result": "\u63ed\u793a\u4e86\u8ba1\u7b97\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u957f\u5ea6\u65b9\u6cd5\u53ef\u5b9e\u73b0\u9ad8\u8fbe60%\u7684\u548c\u901f\u7387\u63d0\u5347\u6216\u7b49\u654830%\u7684\u590d\u6742\u5ea6\u964d\u4f4e", "conclusion": "\u53ef\u53d8\u7cbe\u5ea6\u8ba1\u7b97\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5728\u8ba1\u7b97\u6027\u80fd\u548c\u590d\u6742\u5ea6\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\u7684\u5174\u8d77\u4f7f\u5f97\u8fd9\u79cd\u7b97\u672f\u7ea7\u7cbe\u5ea6\u4f18\u5316\u6210\u4e3a\u53ef\u80fd"}}
{"id": "2508.21153", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.21153", "abs": "https://arxiv.org/abs/2508.21153", "authors": ["Kevin Putra Santoso", "Rizka Wakhidatus Sholikah", "Raden Venantius Hari Ginardi"], "title": "WaveLLDM: Design and Development of a Lightweight Latent Diffusion Model for Speech Enhancement and Restoration", "comment": null, "summary": "High-quality audio is essential in a wide range of applications, including\nonline communication, virtual assistants, and the multimedia industry. However,\ndegradation caused by noise, compression, and transmission artifacts remains a\nmajor challenge. While diffusion models have proven effective for audio\nrestoration, they typically require significant computational resources and\nstruggle to handle longer missing segments. This study introduces WaveLLDM\n(Wave Lightweight Latent Diffusion Model), an architecture that integrates an\nefficient neural audio codec with latent diffusion for audio restoration and\ndenoising. Unlike conventional approaches that operate in the time or spectral\ndomain, WaveLLDM processes audio in a compressed latent space, reducing\ncomputational complexity while preserving reconstruction quality. Empirical\nevaluations on the Voicebank+DEMAND test set demonstrate that WaveLLDM achieves\naccurate spectral reconstruction with low Log-Spectral Distance (LSD) scores\n(0.48 to 0.60) and good adaptability to unseen data. However, it still\nunderperforms compared to state-of-the-art methods in terms of perceptual\nquality and speech clarity, with WB-PESQ scores ranging from 1.62 to 1.71 and\nSTOI scores between 0.76 and 0.78. These limitations are attributed to\nsuboptimal architectural tuning, the absence of fine-tuning, and insufficient\ntraining duration. Nevertheless, the flexible architecture that combines a\nneural audio codec and latent diffusion model provides a strong foundation for\nfuture development.", "AI": {"tldr": "WaveLLDM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u76f8\u7ed3\u5408\uff0c\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5904\u7406\u97f3\u9891\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4f46\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u9ad8\u8d28\u91cf\u97f3\u9891\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u566a\u58f0\u3001\u538b\u7f29\u548c\u4f20\u8f93\u4f2a\u5f71\u9020\u6210\u7684\u9000\u5316\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002\u4f20\u7edf\u6269\u6563\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u4e14\u96be\u4ee5\u5904\u7406\u8f83\u957f\u7f3a\u5931\u7247\u6bb5\u3002", "method": "\u63d0\u51faWaveLLDM\u67b6\u6784\uff0c\u96c6\u6210\u9ad8\u6548\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u97f3\u9891\u4fee\u590d\u548c\u53bb\u566a\uff0c\u800c\u975e\u4f20\u7edf\u7684\u65f6\u95f4\u6216\u9891\u8c31\u57df\u5904\u7406\u3002", "result": "\u5728Voicebank+DEMAND\u6d4b\u8bd5\u96c6\u4e0a\uff0cWaveLLDM\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u9891\u8c31\u91cd\u5efa\uff08LSD\u5206\u65700.48-0.60\uff09\uff0c\u4f46\u5bf9\u672a\u89c1\u6570\u636e\u7684\u611f\u77e5\u8d28\u91cf\u548c\u8bed\u97f3\u6e05\u6670\u5ea6\u4ecd\u4f4e\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff08WB-PESQ 1.62-1.71\uff0cSTOI 0.76-0.78\uff09\u3002", "conclusion": "\u867d\u7136\u5b58\u5728\u67b6\u6784\u8c03\u4f18\u4e0d\u8db3\u3001\u7f3a\u4e4f\u5fae\u8c03\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0d\u8db3\u7b49\u9650\u5236\uff0c\u4f46\u7ed3\u5408\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u7075\u6d3b\u67b6\u6784\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.21470", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.21470", "abs": "https://arxiv.org/abs/2508.21470", "authors": ["Chao Pan"], "title": "Fundamentals of Data-Driven Approaches to Acoustic Signal Detection, Filtering, and Transformation", "comment": null, "summary": "In recent decades, the field of signal processing has rapidly evolved due to\ndiverse application demands, leading to a rich array of scientific questions\nand research areas. The forms of signals, their formation mechanisms, and the\ninformation extraction methods vary by application, resulting in diverse signal\nprocessing techniques. Common techniques can be categorized into three types:\ntransformation, detection, and filtering. Signal transformation converts\nsignals from their original domain to a more suitable target domain for\nanalysis; signal detection aims to identify the existence of relevant\ninformation within a signal and its specific time and location; and signal\nfiltering focuses on extracting or separating source signals of interest from\nobserved signals. In acoustic signal processing, techniques include sound\nsource localization, sound event detection, voiceprint extraction and\nrecognition, noise reduction, and source separation, with applications in\nspeech communication, voice interaction, smart healthcare, and industrial\ndiagnostics. Recently, the advancement of deep learning technologies has\nshifted methodologies in acoustic signal processing from knowledge-driven to\ndata-driven approaches, leading to significant research outcomes. This paper\naims to systematically summarize the principles and methods of data-driven\nacoustic signal processing, providing a comprehensive understanding framework\nfor academic exploration and practical applications.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u603b\u7ed3\u4e86\u6570\u636e\u9a71\u52a8\u7684\u58f0\u5b66\u4fe1\u53f7\u5904\u7406\u539f\u7406\u4e0e\u65b9\u6cd5\uff0c\u6db5\u76d6\u4fe1\u53f7\u53d8\u6362\u3001\u68c0\u6d4b\u548c\u6ee4\u6ce2\u4e09\u5927\u6280\u672f\u7c7b\u522b\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u5168\u9762\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7684\u53d1\u5c55\uff0c\u58f0\u5b66\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u4ece\u77e5\u8bc6\u9a71\u52a8\u8f6c\u5411\u6570\u636e\u9a71\u52a8\uff0c\u4ea7\u751f\u4e86\u5927\u91cf\u7814\u7a76\u6210\u679c\uff0c\u9700\u8981\u7cfb\u7edf\u603b\u7ed3\u8fd9\u4e9b\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u539f\u7406\u548c\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u603b\u7ed3\u58f0\u5b66\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u4e09\u5927\u6280\u672f\u7c7b\u522b\uff1a\u4fe1\u53f7\u53d8\u6362\uff08\u57df\u8f6c\u6362\uff09\u3001\u4fe1\u53f7\u68c0\u6d4b\uff08\u4fe1\u606f\u8bc6\u522b\uff09\u548c\u4fe1\u53f7\u6ee4\u6ce2\uff08\u6e90\u5206\u79bb\uff09\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u8fd9\u4e9b\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u6570\u636e\u9a71\u52a8\u58f0\u5b66\u4fe1\u53f7\u5904\u7406\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u58f0\u97f3\u6e90\u5b9a\u4f4d\u3001\u58f0\u97f3\u4e8b\u4ef6\u68c0\u6d4b\u3001\u58f0\u7eb9\u63d0\u53d6\u4e0e\u8bc6\u522b\u3001\u964d\u566a\u548c\u6e90\u5206\u79bb\u7b49\u5177\u4f53\u6280\u672f\u5e94\u7528\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u663e\u8457\u63a8\u52a8\u4e86\u58f0\u5b66\u4fe1\u53f7\u5904\u7406\u9886\u57df\u7684\u53d1\u5c55\uff0c\u672c\u6587\u63d0\u4f9b\u7684\u7cfb\u7edf\u603b\u7ed3\u6846\u67b6\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7684\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2508.21351", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21351", "abs": "https://arxiv.org/abs/2508.21351", "authors": ["Alireza Fadakar", "Yuchen Zhang", "Hui Chen", "Musa Furkan Keskin", "Henk Wymeersch", "Andreas F. Molisch"], "title": "Hybrid Codebook Design for Localization Using Electromagnetically Reconfigurable Fluid Antenna System", "comment": null, "summary": "Electromagnetically reconfigurable fluid antenna systems (ER-FAS) introduce\nadditional degrees of freedom in the electromagnetic (EM) domain by dynamically\nsteering per-antenna radiation patterns, thereby enhancing power efficiency in\nwireless links. Unlike prior works on spatially reconfigurable FAS, which\nadjust element positions, ER-FAS provides direct control over each element's EM\ncharacteristics to realize on-demand beam-pattern shaping. While existing\nstudies have exploited ER-FAS to boost spectral efficiency, this paper explores\nits application for downlink localization. We consider a multiple-input\nsingle-output (MISO) system in which a multi-antenna ER-FAS at the base station\nserves a single-antenna user equipment (UE). We consider two reconfigurability\nparadigms: (i) a synthesis model where each antenna generates desired\nbeampatterns from a finite set of EM basis functions, and (ii) a finite-state\nselection model in which each antenna selects a pattern from a predefined set\nof patterns. For both paradigms, we formulate the joint baseband (BB) and EM\nprecoder design to minimize the UE position error bound. In the synthesis case\nwe derive low-dimensional closed-form expressions for both the BB and EM\nprecoders. For the finite-state model we obtain closed-form BB structures and\npropose a low-complexity block-coordinate-descent algorithm for EM pattern\nselection. Analytical bounds and extensive simulations show that the proposed\nhybrid designs for ER-FAS substantially improve UE positioning accuracy over\ntraditional non-reconfigurable arrays.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u7535\u78c1\u53ef\u91cd\u6784\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf(ER-FAS)\u5728\u4e0b\u884c\u94fe\u8def\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u53ef\u91cd\u6784\u6027\u8303\u5f0f\u7684\u6df7\u5408\u9884\u7f16\u7801\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u8bbe\u5907\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5229\u7528ER-FAS\u63d0\u5347\u9891\u8c31\u6548\u7387\uff0c\u672c\u6587\u63a2\u7d22\u5176\u5728\u5b9a\u4f4d\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u5929\u7ebf\u5355\u5143\u7684\u7535\u78c1\u7279\u6027\u6765\u5b9e\u73b0\u6309\u9700\u6ce2\u675f\u6210\u5f62\uff0c\u4ece\u800c\u589e\u5f3a\u65e0\u7ebf\u94fe\u8def\u7684\u529f\u7387\u6548\u7387\u3002", "method": "\u8003\u8651MISO\u7cfb\u7edf\uff0c\u57fa\u7ad9\u914d\u5907\u591a\u5929\u7ebfER-FAS\u670d\u52a1\u5355\u5929\u7ebfUE\u3002\u63d0\u51fa\u4e24\u79cd\u53ef\u91cd\u6784\u6027\u8303\u5f0f\uff1a\u5408\u6210\u6a21\u578b\uff08\u4ece\u6709\u9650\u7535\u78c1\u57fa\u51fd\u6570\u751f\u6210\u671f\u671b\u6ce2\u675f\u6a21\u5f0f\uff09\u548c\u6709\u9650\u72b6\u6001\u9009\u62e9\u6a21\u578b\uff08\u4ece\u9884\u5b9a\u4e49\u6a21\u5f0f\u96c6\u4e2d\u9009\u62e9\uff09\u3002\u9488\u5bf9\u4e24\u79cd\u8303\u5f0f\u5206\u522b\u8bbe\u8ba1\u4e86\u8054\u5408\u57fa\u5e26\u548c\u7535\u78c1\u9884\u7f16\u7801\u5668\u6765\u6700\u5c0f\u5316UE\u4f4d\u7f6e\u8bef\u5dee\u754c\u9650\u3002", "result": "\u5728\u5408\u6210\u6a21\u578b\u4e2d\u63a8\u5bfc\u51fa\u4f4e\u7ef4\u95ed\u5f0f\u8868\u8fbe\u7684\u57fa\u5e26\u548c\u7535\u78c1\u9884\u7f16\u7801\u5668\uff1b\u5728\u6709\u9650\u72b6\u6001\u6a21\u578b\u4e2d\u83b7\u5f97\u4e86\u95ed\u5f0f\u57fa\u5e26\u7ed3\u6784\u5e76\u63d0\u51fa\u4e86\u4f4e\u590d\u6742\u5ea6\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u8fdb\u884c\u7535\u78c1\u6a21\u5f0f\u9009\u62e9\u3002\u5206\u6790\u548c\u4eff\u771f\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4e0d\u53ef\u91cd\u6784\u9635\u5217\u3002", "conclusion": "\u63d0\u51fa\u7684ER-FAS\u6df7\u5408\u8bbe\u8ba1\u80fd\u591f\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u5929\u7ebf\u7535\u78c1\u7279\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e0b\u884c\u94fe\u8def\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u65e0\u7ebf\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2508.21167", "categories": ["cs.SD", "cs.LG", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.21167", "abs": "https://arxiv.org/abs/2508.21167", "authors": ["Dong Yoon Lee", "Alyssa Weakley", "Hui Wei", "Blake Brown", "Keyana Carrion", "Shijia Pan"], "title": "RARR : Robust Real-World Activity Recognition with Vibration by Scavenging Near-Surface Audio Online", "comment": null, "summary": "One in four people dementia live alone, leading family members to take on\ncaregiving roles from a distance. Many researchers have developed remote\nmonitoring solutions to lessen caregiving needs; however, limitations remain\nincluding privacy preserving solutions, activity recognition, and model\ngeneralizability to new users and environments. Structural vibration sensor\nsystems are unobtrusive solutions that have been proven to accurately monitor\nhuman information, such as identification and activity recognition, in\ncontrolled settings by sensing surface vibrations generated by activities.\nHowever, when deploying in an end user's home, current solutions require a\nsubstantial amount of labeled data for accurate activity recognition. Our\nscalable solution adapts synthesized data from near-surface acoustic audio to\npretrain a model and allows fine tuning with very limited data in order to\ncreate a robust framework for daily routine tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u632f\u52a8\u4f20\u611f\u5668\u7684\u8fdc\u7a0b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u5408\u6210\u97f3\u9891\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u65e5\u5e38\u6d3b\u52a8\u8bc6\u522b", "motivation": "\u4e3a\u72ec\u5c45\u75f4\u5446\u60a3\u8005\u63d0\u4f9b\u8fdc\u7a0b\u7167\u62a4\u652f\u6301\uff0c\u89e3\u51b3\u73b0\u6709\u76d1\u6d4b\u65b9\u6848\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u6d3b\u52a8\u8bc6\u522b\u548c\u6a21\u578b\u6cdb\u5316\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027", "method": "\u4f7f\u7528\u7ed3\u6784\u632f\u52a8\u4f20\u611f\u5668\u91c7\u96c6\u8868\u9762\u632f\u52a8\u6570\u636e\uff0c\u5c06\u8fd1\u8868\u9762\u58f0\u5b66\u97f3\u9891\u6570\u636e\u5408\u6210\u7528\u4e8e\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u5fae\u8c03", "result": "\u5f00\u53d1\u51fa\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7528\u6237\u5bb6\u4e2d\u90e8\u7f72\u65f6\u4ec5\u9700\u6709\u9650\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u7a33\u5065\u7684\u65e5\u5e38\u6d3b\u52a8\u8ddf\u8e2a", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8fdc\u7a0b\u7167\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6570\u636e\u6807\u6ce8\u96be\u9898\uff0c\u5177\u6709\u8f83\u597d\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u4ef7\u503c"}}
{"id": "2508.21631", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2508.21631", "abs": "https://arxiv.org/abs/2508.21631", "authors": ["Yanis Perrin", "Gilles Boulianne"], "title": "Towards Improved Speech Recognition through Optimized Synthetic Data Generation", "comment": "12 pages, 3 figures", "summary": "Supervised training of speech recognition models requires access to\ntranscribed audio data, which often is not possible due to confidentiality\nissues. Our approach to this problem is to generate synthetic audio from a\ntext-only corpus using a state-of-the-art text-to-speech model with voice\ncloning capabilities. Our goal is to achieve automatic speech recognition (ASR)\nperformance comparable to models trained on real data. We explore ways to\noptimize synthetic data generation through finetuning, filtering and\nevaluation, and its use for training an end-to-end encoder-decoder ASR model.\nExperiments were conducted using two datasets of spontaneous, conversational\nspeech in Qu\\'ebec French. We show that improving data generation leads to\nlarge improvements in the final ASR system trained on synthetic data.", "AI": {"tldr": "\u901a\u8fc7\u6587\u672c\u5230\u8bed\u97f3\u6a21\u578b\u751f\u6210\u5408\u6210\u97f3\u9891\u6570\u636e\uff0c\u8bad\u7ec3\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5b9e\u9645\u97f3\u9891\u6570\u636e\u4fdd\u5bc6\u6027\u95ee\u9898", "motivation": "\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u8bc6\u522b\u8fc7\u7684\u97f3\u9891\u6570\u636e\uff0c\u4f46\u5b9e\u9645\u97f3\u9891\u5f80\u5f80\u56e0\u4fdd\u5bc6\u6027\u95ee\u9898\u65e0\u6cd5\u83b7\u53d6", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u6587\u672c\u5230\u8bed\u97f3\u6a21\u578b\u751f\u6210\u5408\u6210\u97f3\u9891\uff0c\u901a\u8fc7\u5fae\u8c03\u3001\u7b5b\u9009\u548c\u8bc4\u4f30\u4f18\u5316\u6570\u636e\u751f\u6210\uff0c\u7528\u4e8e\u8bad\u7ec3\u7f16\u7801\u5668-\u89e3\u7801\u5668ASR\u6a21\u578b", "result": "\u5728\u52a0\u62ff\u5927\u6cd5\u8bed\u81ea\u53d1\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6539\u5584\u6570\u636e\u751f\u6210\u8d77\u5230\u4e86\u663e\u8457\u63d0\u5347\u6700\u7ec8ASR\u7cfb\u7edf\u6027\u80fd\u7684\u4f5c\u7528", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u8bad\u7ec3\u51fa\u6027\u80fd\u53ef\u4e0e\u5b9e\u9645\u97f3\u9891\u8bad\u7ec3\u76f8\u6bd4\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b"}}
{"id": "2508.21373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21373", "abs": "https://arxiv.org/abs/2508.21373", "authors": ["Niladri Halder", "Chandra R. Murthy"], "title": "Channel Estimation and Data Detection in DS-Spread Channels: A Unified Framework, Novel Algorithms, and Waveform Comparison", "comment": "The paper is submitted for publication in IEEE Transactions on Signal\n  Processing and is under review now. It has 15 pages with 9 figures. This work\n  was presented in parts at IEEE ICASSP 2023 [1] and IEEE SPAWC 2023 [2]", "summary": "We present a unified receiver processing framework for communication over\ndelay-scale (DS)-spread channels that arise in underwater acoustic (UWA)\ncommunications that addresses both channel estimation (CE) and data detection\nfor different modulation waveforms, namely OFDM, OTFS, OCDM, and ODSS, through\na common input--output relation. Using this framework, we conduct a fair and\ncomprehensive comparative study of these waveforms under DS-spread UWA channels\nand similar receiver complexities.\n  We also develop a novel iterative variational Bayesian (VB) off-grid CE\nalgorithm to estimate the delay and scale parameters of the channel paths, via\ntwo approaches: a first-order approximation scheme (FVB) and a second-order\napproximation scheme (SVB). We propose a low-complexity variational soft symbol\ndetection (VSSD) algorithm that outputs soft symbols and log-likelihood ratios\nfor the data bits, and a data-aided iterative CE and data detection (ICED)\nscheme that utilizes detected data symbols as \\emph{virtual} pilots to further\nimprove the CE and data detection accuracy.\n  Our numerical results reveal the efficacy of the proposed algorithms for CE\nand data detection. In terms of relative performance of different waveforms, in\nuncoded communications, (a) with a low-complexity subcarrier-by-subcarrier\nequalizer, ODSS offers the best performance, followed by OCDM and OTFS, while\nOFDM performs the worst, and (b) with the VSSD algorithm, OTFS, OCDM, and ODSS\nperform similarly, and they outperform OFDM. With coded communications,\ninterestingly, all waveforms offer nearly the same BER when the VSSD receiver\nis employed. Hence, we conclude that when the receiver complexity is\nconstrained, waveform choice matters, especially under harsh channel\nconditions, whereas with more sophisticated receiver algorithms, these\ndifferences disappear.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u63a5\u6536\u673a\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u4e2d\u7684\u5ef6\u8fdf-\u7f29\u653e\u901a\u9053\uff0c\u5305\u542b\u901a\u7528\u7684\u8f93\u5165-\u8f93\u51fa\u5173\u7cfb\u3001\u65b0\u9898\u8fed\u4ee3\u53d8\u5206\u8d1f\u7f51\u683c\u989d\u4f30\u7b97\u7b97\u6cd5\u4ee5\u53ca\u4f4e\u590d\u6742\u5ea6\u8f6f\u7b26\u53f7\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5e76\u5bf9OFDM\u3001OTFS\u3001OCDM\u548cODSS\u56db\u79cd\u8c03\u5236\u6ce2\u5f62\u8fdb\u884c\u4e86\u516c\u5e73\u7efc\u5408\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u4e2d\u5b58\u5728\u5ef6\u8fdf-\u7f29\u653e\u6269\u6563\u901a\u9053\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u63a5\u6536\u673a\u5904\u7406\u6846\u67b6\u6765\u5904\u7406\u4e0d\u540c\u8c03\u5236\u6ce2\u5f62\u7684\u9891\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\uff0c\u5e76\u8fdb\u884c\u516c\u5e73\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u8f93\u5165-\u8f93\u51fa\u5173\u7cfb\u6846\u67b6\uff1b\u53d1\u5c55\u4e86\u65b0\u9898\u8fed\u4ee3\u53d8\u5206\u8d1f\u7f51\u683c\u989d\u4f30\u7b97\u7b97\u6cd5\uff08FVB\u548cSVB\uff09\uff1b\u8bbe\u8ba1\u4e86\u4f4e\u590d\u6742\u5ea6\u53d8\u5206\u8f6f\u7b26\u53f7\u68c0\u6d4b\u7b97\u6cd5\uff08VSSD\uff09\uff1b\u5efa\u7acb\u4e86\u6570\u636e\u8f85\u52a9\u8fed\u4ee3\u989d\u4f30\u548c\u68c0\u6d4b\u65b9\u6848\uff08ICED\uff09\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u63d0\u51fa\u7b97\u6cd5\u5728\u9891\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u65b9\u9762\u6709\u6548\u3002\u5728\u65e0\u7f16\u7801\u901a\u4fe1\u4e2d\uff0cODSS\u6027\u80fd\u6700\u4f73\uff0cOCDM\u548cOTFS\u6b21\u4e4b\uff0cOFDM\u6700\u5dee\uff1b\u4f7f\u7528VSSD\u7b97\u6cd5\u65f6\uff0cOTFS\u3001OCDM\u548cODSS\u6027\u80fd\u76f8\u4f3c\u4e14\u90fd\u8d85\u8fc7OFDM\u3002\u5728\u7f16\u7801\u901a\u4fe1\u4e2d\uff0c\u6240\u6709\u6ce2\u5f62\u90fd\u63d0\u4f9b\u51e0\u4e4e\u76f8\u540c\u7684BER\u3002", "conclusion": "\u5f53\u63a5\u6536\u673a\u590d\u6742\u5ea6\u53d7\u9650\u65f6\uff0c\u6ce2\u5f62\u9009\u62e9\u5f88\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u4e25\u5cfb\u7684\u901a\u9053\u6761\u4ef6\u4e0b\uff1b\u800c\u4f7f\u7528\u66f4\u590d\u6742\u7684\u63a5\u6536\u673a\u7b97\u6cd5\u65f6\uff0c\u4e0d\u540c\u6ce2\u5f62\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u4f1a\u6d88\u5931\u3002"}}
{"id": "2508.21243", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21243", "abs": "https://arxiv.org/abs/2508.21243", "authors": ["Aditya Makineni", "Baocheng Geng", "Qing Tian"], "title": "Full-Frequency Temporal Patching and Structured Masking for Enhanced Audio Classification", "comment": null, "summary": "Transformers and State-Space Models (SSMs) have advanced audio classification\nby modeling spectrograms as sequences of patches. However, existing models such\nas the Audio Spectrogram Transformer (AST) and Audio Mamba (AuM) adopt square\npatching from computer vision, which disrupts continuous frequency patterns and\nproduces an excessive number of patches, slowing training, and increasing\ncomputation. We propose Full-Frequency Temporal Patching (FFTP), a patching\nstrategy that better matches the time-frequency asymmetry of spectrograms by\nspanning full frequency bands with localized temporal context, preserving\nharmonic structure, and significantly reducing patch count and computation. We\nalso introduce SpecMask, a patch-aligned spectrogram augmentation that combines\nfull-frequency and localized time-frequency masks under a fixed masking budget,\nenhancing temporal robustness while preserving spectral continuity. When\napplied on both AST and AuM, our patching method with SpecMask improves mAP by\nup to +6.76 on AudioSet-18k and accuracy by up to +8.46 on SpeechCommandsV2,\nwhile reducing computation by up to 83.26%, demonstrating both performance and\nefficiency gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86FFTP\u5168\u9891\u65f6\u57df\u5206\u5757\u7b56\u7565\u548cSpecMask\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u97f3\u9891\u5206\u7c7b\u6027\u80fd\u5e76\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u91cf", "motivation": "\u73b0\u6709\u97f3\u9891\u5206\u7c7b\u6a21\u578b\u91c7\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u65b9\u5f62\u5206\u5757\u65b9\u6cd5\uff0c\u7834\u574f\u4e86\u9891\u8c31\u56fe\u7684\u8fde\u7eed\u9891\u7387\u6a21\u5f0f\uff0c\u4ea7\u751f\u8fc7\u591a\u5206\u5757\u5bfc\u81f4\u8bad\u7ec3\u7f13\u6162\u548c\u8ba1\u7b97\u91cf\u589e\u52a0", "method": "FFTP\u5206\u5757\u7b56\u7565\uff1a\u8de8\u5168\u9891\u5e26\u4f46\u5c40\u90e8\u65f6\u57df\u4e0a\u4e0b\u6587\u7684\u5206\u5757\u65b9\u5f0f\uff0c\u5339\u914d\u9891\u8c31\u56fe\u7684\u65f6\u9891\u4e0d\u5bf9\u79f0\u6027\uff1bSpecMask\u589e\u5f3a\uff1a\u5728\u56fa\u5b9a\u63a9\u7801\u9884\u7b97\u4e0b\u7ed3\u5408\u5168\u9891\u548c\u5c40\u90e8\u65f6\u9891\u63a9\u7801", "result": "\u5728AST\u548cAuM\u6a21\u578b\u4e0a\u5e94\u7528\uff0cAudioSet-18k mAP\u63d0\u5347\u6700\u9ad8+6.76\uff0cSpeechCommandsV2\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8+8.46\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u6700\u9ad883.26%", "conclusion": "FFTP\u548cSpecMask\u65b9\u6cd5\u5728\u4fdd\u6301\u9891\u8c31\u8fde\u7eed\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u5206\u7c7b\u6027\u80fd\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c"}}
{"id": "2508.21412", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21412", "abs": "https://arxiv.org/abs/2508.21412", "authors": ["Hang Sheng", "Hui Feng", "Junhao Yu", "Feng Ji", "Bo Hu"], "title": "Sampling Theory of Jointly Bandlimited Time-vertex Graph Signals", "comment": "This paper was published in Signal Processing, Elsevier", "summary": "Time-vertex graph signal (TVGS) models describe time-varying data with\nirregular structures. The bandlimitedness in the joint time-vertex Fourier\nspectral domain reflects smoothness in both temporal and graph topology. In\nthis paper, we study the critical sampling of three types of TVGS including\ncontinuous-time signals, infinite-length sequences, and finite-length sequences\nin the time domain for each vertex on the graph. For a jointly bandlimited\nTVGS, we prove a lower bound on sampling density or sampling ratio, which\ndepends on the measure of the spectral support in the joint time-vertex Fourier\nspectral domain. We also provide a lower bound on the sampling density or\nsampling ratio of each vertex on sampling sets for perfect recovery. To\ndemonstrate that critical sampling is achievable, we propose the sampling and\nreconstruction procedures for the different types of TVGS. Finally, we show how\nthe proposed sampling schemes can be applied to numerical as well as real\ndatasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u65f6-\u9876\u70b9\u56fe\u4fe1\u53f7(TVGS)\u7684\u4e34\u754c\u91c7\u6837\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7\u3001\u65e0\u9650\u957f\u5e8f\u5217\u548c\u6709\u9650\u957f\u5e8f\u5217\u4e09\u79cdTVGS\u7c7b\u578b\u7684\u91c7\u6837\u5bc6\u5ea6\u4e0b\u754c\uff0c\u5e76\u7ed9\u51fa\u4e86\u53ef\u5b9e\u73b0\u5b8c\u7f8e\u91cd\u5efa\u7684\u91c7\u6837\u4e0e\u91cd\u6784\u65b9\u6848\u3002", "motivation": "\u65f6-\u9876\u70b9\u56fe\u4fe1\u53f7\u6a21\u578b\u7528\u4e8e\u63cf\u8ff0\u5177\u6709\u4e0d\u89c4\u5219\u7ed3\u6784\u7684\u65f6\u53d8\u6570\u636e\uff0c\u5176\u8054\u5408\u65f6-\u9876\u70b9\u5085\u91cc\u53f6\u8c31\u57df\u7684\u5e26\u9650\u7279\u6027\u53cd\u6620\u4e86\u65f6\u95f4\u548c\u56fe\u62d3\u6251\u4e0a\u7684\u5e73\u6ed1\u6027\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u5bf9\u8fd9\u4e9b\u4fe1\u53f7\u8fdb\u884c\u4e34\u754c\u91c7\u6837\u4ee5\u5b9e\u73b0\u5b8c\u7f8e\u91cd\u5efa\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8054\u5408\u65f6-\u9876\u70b9\u5085\u91cc\u53f6\u8c31\u57df\u7684\u9891\u8c31\u652f\u6491\u6d4b\u5ea6\uff0c\u63a8\u5bfc\u4e86TVGS\u91c7\u6837\u5bc6\u5ea6\u6216\u91c7\u6837\u7387\u7684\u4e0b\u754c\uff0c\u5e76\u9488\u5bf9\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684TVGS\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u91c7\u6837\u548c\u91cd\u6784\u7a0b\u5e8f\u3002", "result": "\u8bc1\u660e\u4e86\u8054\u5408\u5e26\u9650TVGS\u7684\u91c7\u6837\u5bc6\u5ea6\u4e0b\u754c\uff0c\u8be5\u4e0b\u754c\u4f9d\u8d56\u4e8e\u8054\u5408\u8c31\u57df\u7684\u9891\u8c31\u652f\u6491\u6d4b\u5ea6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6bcf\u4e2a\u9876\u70b9\u91c7\u6837\u96c6\u4e0a\u53ef\u5b9e\u73b0\u5b8c\u7f8e\u91cd\u5efa\u7684\u91c7\u6837\u5bc6\u5ea6\u4e0b\u754c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u91c7\u6837\u65b9\u6848\u80fd\u591f\u5b9e\u73b0\u4e34\u754c\u91c7\u6837\uff0c\u5e76\u5728\u6570\u503c\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u65f6-\u9876\u70b9\u56fe\u4fe1\u53f7\u7684\u91c7\u6837\u7406\u8bba\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2508.21407", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21407", "abs": "https://arxiv.org/abs/2508.21407", "authors": ["Cheng-Yeh Yang", "Kuan-Tang Huang", "Chien-Chun Wang", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen"], "title": "DRASP: A Dual-Resolution Attentive Statistics Pooling Framework for Automatic MOS Prediction", "comment": "Accepted to APSIPA ASC 2025", "summary": "A pooling mechanism is essential for mean opinion score (MOS) prediction,\nfacilitating the transformation of variable-length audio features into a\nconcise fixed-size representation that effectively encodes speech quality.\nExisting pooling methods typically operate at a singular granularity,\nconcentrating either on a comprehensive global perspective or a detailed\nframe-level analysis, which may overlook complementary perceptual insights. To\naddress this limitation, we introduce the Dual-Resolution Attentive Statistics\nPooling (DRASP) framework. DRASP integrates both coarse-grained, global\nstatistical summaries and fine-grained, attentive analyses of perceptually\nsignificant segments. This dual-view architecture empowers our model to\nformulate a more thorough and robust representation, capturing both the\noverarching structural context and salient local details concurrently.\nExtensive experiments validate the effectiveness and strong generalization\nability of the proposed framework. It consistently outperforms various baseline\nmethods across diverse datasets (MusicEval and AES-Natural), MOS prediction\nbackbones (including a CLAP-based model and AudioBox-Aesthetics), and different\naudio generation systems, achieving a relative improvement of 10.39% in\nsystem-level Spearman's rank correlation coefficient (SRCC) over the\nwidely-used average pooling approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53cc\u5206\u8fa8\u7387\u6ce8\u610f\u529b\u7edf\u8ba1\u6c60\u5316\u6846\u67b6DRASP\uff0c\u901a\u8fc7\u7ed3\u5408\u7c97\u7c92\u5ea6\u5168\u5c40\u7edf\u8ba1\u548c\u7ec6\u7c92\u5ea6\u6ce8\u610f\u529b\u5206\u6790\uff0c\u5728MOS\u9884\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6c60\u5316\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6c60\u5316\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u5355\u4e00\u7c92\u5ea6\u4e0a\u64cd\u4f5c\uff0c\u8981\u4e48\u5173\u6ce8\u5168\u5c40\u89c6\u89d2\uff0c\u8981\u4e48\u5173\u6ce8\u5e27\u7ea7\u5206\u6790\uff0c\u53ef\u80fd\u5ffd\u7565\u4e86\u4e92\u8865\u7684\u611f\u77e5\u4fe1\u606f", "method": "DRASP\u6846\u67b6\u6574\u5408\u4e86\u7c97\u7c92\u5ea6\u7684\u5168\u5c40\u7edf\u8ba1\u6458\u8981\u548c\u7ec6\u7c92\u5ea6\u7684\u611f\u77e5\u663e\u8457\u7247\u6bb5\u6ce8\u610f\u529b\u5206\u6790\uff0c\u91c7\u7528\u53cc\u89c6\u56fe\u67b6\u6784\u540c\u65f6\u6355\u83b7\u6574\u4f53\u7ed3\u6784\u4e0a\u4e0b\u6587\u548c\u663e\u8457\u5c40\u90e8\u7ec6\u8282", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7ea7Spearman\u76f8\u5173\u7cfb\u6570\u76f8\u5bf9\u5e73\u5747\u6c60\u5316\u65b9\u6cd5\u63d0\u534710.39%", "conclusion": "\u53cc\u5206\u8fa8\u7387\u6c60\u5316\u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u5168\u9762\u9c81\u68d2\u7684\u97f3\u9891\u8868\u793a\uff0c\u5728MOS\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2508.21415", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21415", "abs": "https://arxiv.org/abs/2508.21415", "authors": ["Hang Sheng", "Qinji Shu", "Hui Feng", "Bo Hu"], "title": "Subset Random Sampling and Reconstruction of Finite Time-Vertex Graph Signals", "comment": "This paper was published in IEEE Transactions on Signal and\n  Information Processing over Networks (2025)", "summary": "Finite time-vertex graph signals (FTVGS) provide an efficient representation\nfor capturing spatio-temporal correlations across multiple data sources on\nirregular structures. Although sampling and reconstruction of FTVGS with known\nspectral support have been extensively studied, the case of unknown spectral\nsupport requires further investigation. Existing random sampling methods may\nextract samples from any vertex at any time, but such strategies are not\nfriendly in practice, where sampling is typically limited to a subset of\nvertices and moments. To address this requirement, we propose a subset random\nsampling scheme for FTVGS. Specifically, we first randomly select a subset of\nrows and columns to form a submatrix, followed by random sampling within that\nsubmatrix. In theory, we provide sufficient conditions for reconstructing the\noriginal FTVGS with high probability. Additionally, we introduce a\nreconstruction framework incorporating low-rank, sparsity, and smoothness\npriors (LSSP), and verify the feasibility of the reconstruction and the\neffectiveness of the framework through experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6709\u9650\u65f6\u95f4-\u9876\u70b9\u56fe\u4fe1\u53f7\u7684\u5b50\u96c6\u968f\u673a\u91c7\u6837\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u9009\u5b9a\u7684\u884c\u548c\u5217\u5b50\u77e9\u9635\u5185\u8fdb\u884c\u968f\u673a\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u91c7\u6837\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6982\u7387\u91cd\u6784\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u5141\u8bb8\u5728\u4efb\u4f55\u9876\u70b9\u548c\u4efb\u4f55\u65f6\u95f4\u63d0\u53d6\u6837\u672c\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u91c7\u6837\u901a\u5e38\u4ec5\u9650\u4e8e\u9876\u70b9\u7684\u5b50\u96c6\u548c\u7279\u5b9a\u65f6\u523b\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u91c7\u6837\u7b56\u7565\u3002", "method": "\u9996\u5148\u968f\u673a\u9009\u62e9\u884c\u548c\u5217\u7684\u5b50\u96c6\u5f62\u6210\u5b50\u77e9\u9635\uff0c\u7136\u540e\u5728\u5b50\u77e9\u9635\u5185\u8fdb\u884c\u968f\u673a\u91c7\u6837\uff1b\u63d0\u51fa\u4e86\u7ed3\u5408\u4f4e\u79e9\u3001\u7a00\u758f\u6027\u548c\u5e73\u6ed1\u6027\u5148\u9a8c\uff08LSSP\uff09\u7684\u91cd\u6784\u6846\u67b6\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6982\u7387\u91cd\u6784\u539f\u59cb\u4fe1\u53f7\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cd\u6784\u7684\u53ef\u884c\u6027\u548c\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b50\u96c6\u968f\u673a\u91c7\u6837\u65b9\u6848\u80fd\u591f\u6709\u6548\u5904\u7406\u5b9e\u9645\u91c7\u6837\u7ea6\u675f\u4e0b\u7684\u6709\u9650\u65f6\u95f4-\u9876\u70b9\u56fe\u4fe1\u53f7\u91cd\u6784\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2508.21563", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21563", "abs": "https://arxiv.org/abs/2508.21563", "authors": ["Pierluigi Poggiolini", "Yanchao Jiang", "Yifeng Gao", "Fabrizio Forghieri"], "title": "Polynomial Closed Form Model for Ultra-Wideband Transmission Systems", "comment": "The paper is identical to a manuscript submitted to JLT in August\n  2025", "summary": "Ultrafast and accurate physical layer models are essential for designing,\noptimizing and managing ultra-wideband optical transmission systems. We present\na closed-form GN/EGN model, named Polynomial Closed-Form Model (PCFM),\nimproving reliability, accuracy, and generality. The key to deriving PCFM is\nexpressing the spatial power profile of each channel along a span as a\npolynomial. Then, under reasonable approximations, the integral calculation can\nbe carried out analytically, for any chosen degree of the polynomial. We\npresent a full detailed derivation of the model. We then validate it vs. the\nnumerically integrated GN-model in a challenging multiband (C+L+S) scenario,\nincluding Raman amplification and inter-channel Raman scattering. We then show\nthat the approach works well also in the special case of the presence of\nmultiple lumped loss along the fiber. Overall, the approach shows very good\naccuracy and broad applicability. A software implementing the model, fully\nreconfigurable to any type of system layout, is available for download under\nthe Creative Commons 4.0 License.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ebPCFM\u7684\u95ed\u5f0fGN/EGN\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u63cf\u8ff0\u901a\u9053\u529f\u7387\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u3001\u9ad8\u7cbe\u5ea6\u7684\u5149\u4f20\u8f93\u7cfb\u7edf\u6a21\u62df", "motivation": "\u9700\u8981\u5feb\u901f\u800c\u51c6\u786e\u7684\u7269\u7406\u5c42\u6a21\u578b\u6765\u8bbe\u8ba1\u3001\u4f18\u5316\u548c\u7ba1\u7406\u8d85\u5bbd\u5e26\u5149\u4f20\u8f93\u7cfb\u7edf", "method": "\u5c06\u6bcf\u4e2a\u901a\u9053\u5728\u5149\u7ea4\u6bb5\u4e2d\u7684\u7a7a\u95f4\u529f\u7387\u5206\u5e03\u8868\u8fbe\u4e3a\u591a\u9879\u5f0f\uff0c\u901a\u8fc7\u5206\u6790\u79ef\u5206\u8ba1\u7b97\u5b9e\u73b0\u95ed\u5f0f\u6a21\u578b\u6c42\u89e3", "result": "\u5728\u5305\u62ec\u62c9\u66fc\u589e\u76ca\u548c\u901a\u9053\u95f4\u62c9\u66fc\u6563\u5c04\u7684\u591a\u5e26(C+L+S)\u573a\u666f\u4e2d\uff0c\u4e0e\u6570\u503c\u79ef\u5206GN\u6a21\u578b\u5bf9\u6bd4\u663e\u793a\u51fa\u5f88\u597d\u7684\u51c6\u786e\u6027", "conclusion": "PCFM\u6a21\u578b\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u5ea6\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u8f6f\u4ef6\u5b9e\u73b0"}}
{"id": "2508.21614", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21614", "abs": "https://arxiv.org/abs/2508.21614", "authors": ["He Huang", "Zeping Sui", "Zilong Liu", "Wei Huang", "Md. Noor-A-Rahim", "Haishi Wang", "Zhiheng Hu"], "title": "Energy Detection over Composite $\u03ba-\u03bc$ Shadowed Fading Channels with Inverse Gaussian Distribution in Ultra mMTC Networks", "comment": "5 pages, 5 figures, submitted to IEEE TVT", "summary": "This paper investigates the characteristics of energy detection (ED) over\ncomposite $\\kappa$-$\\mu$ shadowed fading channels in ultra machine-type\ncommunication (mMTC) networks. We have derived the closed-form expressions of\nthe probability density function (PDF) of signal-to-noise ratio (SNR) based on\nthe Inverse Gaussian (\\emph{IG}) distribution. By adopting novel integration\nand mathematical transformation techniques, we derive a truncation-based\nclosed-form expression for the average detection probability for the first\ntime. It can be observed from our simulations that the number of propagation\npaths has a more pronounced effect on average detection probability compared to\naverage SNR, which is in contrast to earlier studies that focus on\ndevice-to-device networks. It suggests that for 6G mMTC network design, we\nshould consider enhancing transmitter-receiver placement and antenna alignment\nstrategies, rather than relying solely on increasing the device-to-device\naverage SNR.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e866G mMTC\u7f51\u7edc\u4e2d\u03ba-\u03bc\u9634\u5f71\u590d\u5408\u8870\u843d\u4fe1\u9053\u4e0b\u80fd\u91cf\u68c0\u6d4b\u7684\u7279\u6027\uff0c\u9996\u6b21\u63a8\u5bfc\u4e86\u57fa\u4e8e\u622a\u65ad\u65b9\u6cd5\u7684\u5e73\u5747\u68c0\u6d4b\u6982\u7387\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u53d1\u73b0\u4f20\u64ad\u8def\u5f84\u6570\u6bd4\u5e73\u5747SNR\u5bf9\u68c0\u6d4b\u6982\u7387\u5f71\u54cd\u66f4\u5927", "motivation": "\u7814\u7a766G\u8d85\u5927\u89c4\u6a21\u673a\u5668\u7c7b\u578b\u901a\u4fe1(mMTC)\u7f51\u7edc\u4e2d\u80fd\u91cf\u68c0\u6d4b\u5728\u590d\u5408\u03ba-\u03bc\u9634\u5f71\u8870\u843d\u4fe1\u9053\u4e0b\u7684\u6027\u80fd\u7279\u5f81\uff0c\u4e3a\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc", "method": "\u57fa\u4e8e\u9006\u9ad8\u65af\u5206\u5e03\u63a8\u5bfcSNR\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u91c7\u7528\u65b0\u9896\u7684\u79ef\u5206\u548c\u6570\u5b66\u53d8\u6362\u6280\u672f\uff0c\u9996\u6b21\u63a8\u5bfc\u51fa\u622a\u65ad\u578b\u5e73\u5747\u68c0\u6d4b\u6982\u7387\u95ed\u5f0f\u8868\u8fbe\u5f0f", "result": "\u4eff\u771f\u663e\u793a\u4f20\u64ad\u8def\u5f84\u6570\u6bd4\u5e73\u5747SNR\u5bf9\u5e73\u5747\u68c0\u6d4b\u6982\u7387\u7684\u5f71\u54cd\u66f4\u663e\u8457\uff0c\u8fd9\u4e0e\u4e4b\u524d\u9488\u5bf9\u8bbe\u5907\u5230\u8bbe\u5907\u7f51\u7edc\u7684\u7814\u7a76\u7ed3\u8bba\u76f8\u53cd", "conclusion": "6G mMTC\u7f51\u7edc\u8bbe\u8ba1\u5e94\u91cd\u70b9\u8003\u8651\u589e\u5f3a\u53d1\u5c04\u673a-\u63a5\u6536\u673a\u5e03\u5c40\u548c\u5929\u7ebf\u5bf9\u51c6\u7b56\u7565\uff0c\u800c\u975e\u5355\u7eaf\u63d0\u9ad8\u8bbe\u5907\u95f4\u5e73\u5747SNR"}}
{"id": "2508.21640", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.21640", "abs": "https://arxiv.org/abs/2508.21640", "authors": ["Amirhossein Azarbahram", "Onel L. A. L\u00f3pez", "Petar Popovski", "Matti Latva-aho"], "title": "On the Deployment of Multiple Radio Stripes for Large-Scale Near-Field RF Wireless Power Transfer", "comment": null, "summary": "This paper investigates the deployment of radio stripe systems for indoor\nradio-frequency (RF) wireless power transfer (WPT) in line-of-sight near-field\nscenarios. The focus is on environments where energy demand is concentrated in\nspecific areas, referred to as 'hotspots', spatial zones with higher user\ndensity or consistent energy requirements. We formulate a joint clustering and\nradio stripe deployment problem that aims to maximize the minimum received\npower across all hotspots. To address the complexity, we decouple the problem\ninto two stages: i) clustering for assigning radio stripes to hotspots based on\ntheir spatial positions and near-field propagation characteristics, and ii)\nantenna element placement optimization. In particular, we propose four radio\nstripe deployment algorithms. Two are based on general successive convex\napproximation (SCA) and signomial programming (SGP) methods. The other two are\nshape-constrained solutions where antenna elements are arranged along either\nstraight lines or regular polygons, enabling simpler deployment. Numerical\nresults show that the proposed clustering method converges effectively, with\nChebyshev initialization significantly outperforming random initialization. The\noptimized deployments consistently outperform baseline benchmarks across a wide\nrange of frequencies and radio stripe lengths, while the polygon-shaped\ndeployment achieves better performance compared to other approaches. Meanwhile,\nthe line-shaped deployment demonstrates an advantage under high boresight gain\nsettings, benefiting from increased spatial diversity and broader angular\ncoverage.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5ba4\u5185\u8fd1\u573a\u65e0\u7ebf\u7535\u80fd\u91cf\u4f20\u8f93\u573a\u666f\u4e0b\u7684\u65e0\u7ebf\u7535\u7cfb\u7edf\u90e8\u7f72\uff0c\u901a\u8fc7\u805a\u7c7b\u7b97\u6cd5\u548c\u5929\u7ebf\u5143\u4ef6\u4f18\u5316\u63d0\u9ad8\u70ed\u70b9\u533a\u57df\u7684\u6700\u5c0f\u6536\u5230\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u65e0\u7ebf\u7535\u80fd\u91cf\u4f20\u8f93\u4e2d\u80fd\u6e90\u9700\u6c42\u96c6\u4e2d\u7684\u70ed\u70b9\u533a\u57df\u7684\u6548\u7387\u95ee\u9898\uff0c\u6700\u5927\u5316\u6240\u6709\u70ed\u70b9\u7684\u6700\u5c0f\u6536\u5230\u529f\u7387\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u6839\u636e\u7a7a\u95f4\u4f4d\u7f6e\u548c\u8fd1\u573a\u4f20\u64ad\u7279\u6027\u8fdb\u884c\u805a\u7c7b\u5206\u914d\u65e0\u7ebf\u7535\u7cfb\u7edf\uff0c\u7136\u540e\u8fdb\u884c\u5929\u7ebf\u5143\u4ef6\u5e03\u7f6e\u4f18\u5316\u3002\u63d0\u51fa\u56db\u79cd\u90e8\u7f72\u7b97\u6cd5\uff08SCA\u3001SGP\u3001\u76f4\u7ebf\u5f62\u3001\u6b63\u591a\u8fb9\u5f62\uff09\u3002", "result": "\u805a\u7c7b\u65b9\u6cd5\u6709\u6548\u6536\u655b\uff0cChebyshev\u521d\u59cb\u5316\u663e\u8457\u4f18\u4e8e\u968f\u673a\u521d\u59cb\u5316\u3002\u4f18\u5316\u90e8\u7f72\u65b9\u6848\u5728\u5e7f\u6cdb\u9891\u7387\u548c\u7cfb\u7edf\u957f\u5ea6\u8303\u56f4\u5185\u90fd\u8d85\u8fc7\u57fa\u51c6\u65b9\u6848\uff0c\u591a\u8fb9\u5f62\u5e03\u7f6e\u8868\u73b0\u6700\u4f73\uff0c\u76f4\u7ebf\u5f62\u5728\u9ad8\u5355\u5143\u6536\u76ca\u8bbe\u7f6e\u4e0b\u4f53\u73b0\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5ba4\u5185\u65e0\u7ebf\u7535\u80fd\u91cf\u4f20\u8f93\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7cfb\u7edf\u90e8\u7f72\u65b9\u6848\uff0c\u7279\u522b\u662f\u591a\u8fb9\u5f62\u5e03\u7f6e\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u800c\u76f4\u7ebf\u5f62\u5e03\u7f6e\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u6709\u5176\u4f18\u52bf\u3002"}}
{"id": "2508.21652", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21652", "abs": "https://arxiv.org/abs/2508.21652", "authors": ["Haozhe Tian", "Qiyu Rao", "Nina Moutonnet", "Pietro Ferraro", "Danilo Mandic"], "title": "Machine Intelligence on the Edge: Interpretable Cardiac Pattern Localisation Using Reinforcement Learning", "comment": null, "summary": "Matched filters are widely used to localise signal patterns due to their high\nefficiency and interpretability. However, their effectiveness deteriorates for\nlow signal-to-noise ratio (SNR) signals, such as those recorded on edge\ndevices, where prominent noise patterns can closely resemble the target within\nthe limited length of the filter. One example is the ear-electrocardiogram\n(ear-ECG), where the cardiac signal is attenuated and heavily corrupted by\nartefacts. To address this, we propose the Sequential Matched Filter (SMF), a\nparadigm that replaces the conventional single matched filter with a sequence\nof filters designed by a Reinforcement Learning agent. By formulating filter\ndesign as a sequential decision-making process, SMF adaptively design\nsignal-specific filter sequences that remain fully interpretable by revealing\nkey patterns driving the decision-making. The proposed SMF framework has strong\npotential for reliable and interpretable clinical decision support, as\ndemonstrated by its state-of-the-art R-peak detection and physiological state\nclassification performance on two challenging real-world ECG datasets. The\nproposed formulation can also be extended to a broad range of applications that\nrequire accurate pattern localisation from noise-corrupted signals.", "AI": {"tldr": "\u63d0\u51fa\u5e8f\u5217\u5339\u914d\u6ee4\u6ce2\u5668(SMF)\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u7684\u6ee4\u6ce2\u5668\u5e8f\u5217\u66ff\u4ee3\u4f20\u7edf\u5355\u5339\u914d\u6ee4\u6ce2\u5668\uff0c\u89e3\u51b3\u4f4e\u4fe1\u566a\u6bd4\u4fe1\u53f7\u4e2d\u7684\u6a21\u5f0f\u5b9a\u4f4d\u95ee\u9898\uff0c\u5728ECG\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5339\u914d\u6ee4\u6ce2\u5668\u5728\u4f4e\u4fe1\u566a\u6bd4\u4fe1\u53f7(\u5982\u8fb9\u7f18\u8bbe\u5907\u8bb0\u5f55\u7684\u4fe1\u53f7)\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u566a\u58f0\u6a21\u5f0f\u5bb9\u6613\u4e0e\u76ee\u6807\u4fe1\u53f7\u6df7\u6dc6\uff0c\u7279\u522b\u662f\u5728\u8033\u90e8\u5fc3\u7535\u56fe\u7b49\u4fe1\u53f7\u8870\u51cf\u4e25\u91cd\u7684\u5e94\u7528\u4e2d\u3002", "method": "\u5c06\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u8bbe\u8ba1\u4fe1\u53f7\u7279\u5b9a\u7684\u6ee4\u6ce2\u5668\u5e8f\u5217\uff0c\u4fdd\u6301\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9eECG\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684R\u5cf0\u68c0\u6d4b\u548c\u751f\u7406\u72b6\u6001\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "SMF\u6846\u67b6\u5177\u6709\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u6f5c\u529b\uff0c\u53ef\u6269\u5c55\u5230\u5404\u79cd\u9700\u8981\u4ece\u566a\u58f0\u6c61\u67d3\u4fe1\u53f7\u4e2d\u51c6\u786e\u5b9a\u4f4d\u6a21\u5f0f\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.21724", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.21724", "abs": "https://arxiv.org/abs/2508.21724", "authors": ["Dario Sanalitro", "Marco Finocchiaro", "Pasquale Memmolo", "Emanuela Cutuli", "Maide Bucolo"], "title": "A Single Subject Machine Learning Based Classification of Motor Imagery EEGs", "comment": "Conference Paper", "summary": "Motor Imagery-Based Brain-Computer Interfaces (MI-BCIs) are systems that\ndetect and interpret brain activity patterns linked to the mental visualization\nof movement, and then translate these into instructions for controlling\nexternal robotic or domotic devices. Such devices have the potential to be\nuseful in a broad variety of applications. While implementing a system that\nwould help individuals restore some freedom levels, the interpretation of\n(Electroencephalography) EEG data remains a complex and unsolved problem. In\nthe literature, the classification of left and right imagined movements has\nbeen extensively studied. This study introduces a novel pipeline that makes use\nof machine learning techniques for classifying MI EEG data. The entire\nframework is capable of accurately categorizing left and imagined motions, as\nwell as rest phases, for a set of 52 subjects who performed a MI task. We\ntrained a within subject model on each individual subject. The methodology has\nbeen offline evaluated and compared to four studies that are currently the\nstate-of-the-art regarding the specified dataset. The results show that our\nproposed framework could be used with MI-BCI systems in light of its failsafe\nclassification performances, i.e. 99.5% in accuracy", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u578b\u8111\u673a\u63a5\u53e3\u7ba1\u9053\uff0c\u80fd\u591f\u51c6\u786e\u5206\u7c7b\u5de6\u53f3\u624b\u8fd0\u52a8\u60f3\u8c61\u548c\u4f11\u606f\u72b6\u6001\u7684\u8111\u7535\u56fe\u6570\u636e\uff0c\u572852\u540d\u53d7\u8bd5\u8005\u4e0a\u5b9e\u73b0\u4e8699.5%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u8fd0\u52a8\u60f3\u8c61\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\u5728\u5e2e\u52a9\u4e2a\u4f53\u6062\u590d\u81ea\u7531\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8111\u7535\u56fe\u6570\u636e\u7684\u89e3\u91ca\u4ecd\u7136\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5de6\u53f3\u624b\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u65b9\u9762\u9700\u8981\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u673a\u5668\u5b66\u4e60\u7ba1\u9053\uff0c\u4e3a\u6bcf\u4e2a\u53d7\u8bd5\u8005\u8bad\u7ec3\u4e2a\u4f53\u5316\u6a21\u578b\uff0c\u4f7f\u7528\u8111\u7535\u56fe\u6570\u636e\u6765\u5206\u7c7b\u5de6\u53f3\u624b\u8fd0\u52a8\u60f3\u8c61\u548c\u4f11\u606f\u72b6\u6001\uff0c\u5e76\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7814\u7a76\u8fdb\u884c\u4e86\u79bb\u7ebf\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u572852\u540d\u53d7\u8bd5\u8005\u7684\u8fd0\u52a8\u60f3\u8c61\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8fbe\u5230\u4e8699.5%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u56db\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u6240\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u8fd0\u52a8\u60f3\u8c61\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u6280\u672f\u652f\u6491\u3002"}}

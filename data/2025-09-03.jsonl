{"id": "2509.00025", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00025", "abs": "https://arxiv.org/abs/2509.00025", "authors": ["Tai Vu"], "title": "DeepEmoNet: Building Machine Learning Models for Automatic Emotion Recognition in Human Speeches", "comment": null, "summary": "Speech emotion recognition (SER) has been a challenging problem in spoken\nlanguage processing research, because it is unclear how human emotions are\nconnected to various components of sounds such as pitch, loudness, and energy.\nThis paper aims to tackle this problem using machine learning. Particularly, we\nbuilt several machine learning models using SVMs, LTSMs, and CNNs to classify\nemotions in human speeches. In addition, by leveraging transfer learning and\ndata augmentation, we efficiently trained our models to attain decent\nperformances on a relatively small dataset. Our best model was a ResNet34\nnetwork, which achieved an accuracy of $66.7\\%$ and an F1 score of $0.631$."}
{"id": "2509.00077", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00077", "abs": "https://arxiv.org/abs/2509.00077", "authors": ["Tai Vu"], "title": "Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition", "comment": null, "summary": "Speech Emotion Recognition (SER) presents a significant yet persistent\nchallenge in human-computer interaction. While deep learning has advanced\nspoken language processing, achieving high performance on limited datasets\nremains a critical hurdle. This paper confronts this issue by developing and\nevaluating a suite of machine learning models, including Support Vector\nMachines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional\nNeural Networks (CNNs), for automated emotion classification in human speech.\nWe demonstrate that by strategically employing transfer learning and innovative\ndata augmentation techniques, our models can achieve impressive performance\ndespite the constraints of a relatively small dataset. Our most effective\nmodel, a ResNet34 architecture, establishes a new performance benchmark on the\ncombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1\nscore of 0.631. These results underscore the substantial benefits of leveraging\npre-trained models and data augmentation to overcome data scarcity, thereby\npaving the way for more robust and generalizable SER systems."}
{"id": "2509.00078", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00078", "abs": "https://arxiv.org/abs/2509.00078", "authors": ["Tatiana Likhomanenko", "Luke Carlson", "Richard He Bai", "Zijin Gu", "Han Tran", "Zakaria Aldeneh", "Yizhe Zhang", "Ruixiang Zhang", "Huangjie Zheng", "Navdeep Jaitly"], "title": "ChipChat: Low-Latency Cascaded Conversational Agent in MLX", "comment": "ASRU 2025", "summary": "The emergence of large language models (LLMs) has transformed spoken dialog\nsystems, yet the optimal architecture for real-time on-device voice agents\nremains an open question. While end-to-end approaches promise theoretical\nadvantages, cascaded systems (CSs) continue to outperform them in language\nunderstanding tasks, despite being constrained by sequential processing\nlatency. In this work, we introduce ChipChat, a novel low-latency CS that\novercomes traditional bottlenecks through architectural innovations and\nstreaming optimizations. Our system integrates streaming (a) conversational\nspeech recognition with mixture-of-experts, (b) state-action augmented LLM, (c)\ntext-to-speech synthesis, (d) neural vocoder, and (e) speaker modeling.\nImplemented using MLX, ChipChat achieves sub-second response latency on a Mac\nStudio without dedicated GPUs, while preserving user privacy through complete\non-device processing. Our work shows that strategically redesigned CSs can\novercome their historical latency limitations, offering a promising path\nforward for practical voice-based AI agents."}
{"id": "2509.00094", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00094", "abs": "https://arxiv.org/abs/2509.00094", "authors": ["Abdullah Abdelfattah", "Mahmoud I. Khalil", "Hazem Abbas"], "title": "Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning", "comment": null, "summary": "Assessing spoken language is challenging, and quantifying pronunciation\nmetrics for machine learning models is even harder. However, for the Holy\nQuran, this task is simplified by the rigorous recitation rules (tajweed)\nestablished by Muslim scholars, enabling highly effective assessment. Despite\nthis advantage, the scarcity of high-quality annotated data remains a\nsignificant barrier.\n  In this work, we bridge these gaps by introducing: (1) A 98% automated\npipeline to produce high-quality Quranic datasets -- encompassing: Collection\nof recitations from expert reciters, Segmentation at pause points (waqf) using\nour fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript\nverification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K\nannotated utterances); (3) A novel ASR-based approach for pronunciation error\ndetection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed\nrules (unlike the IPA standard for Modern Standard Arabic). QPS uses a\ntwo-level script: (Phoneme level): Encodes Arabic letters with short/long\nvowels. (Sifa level): Encodes articulation characteristics of every phoneme. We\nfurther include comprehensive modeling with our novel multi-level CTC Model\nwhich achieved 0.16% average Phoneme Error Rate (PER) on the testset. We\nrelease all code, data, and models as open-source:\nhttps://obadx.github.io/prepare-quran-dataset/"}
{"id": "2509.00012", "categories": ["eess.SP", "cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.00012", "abs": "https://arxiv.org/abs/2509.00012", "authors": ["Chun Hin Siu", "Hossein Miri"], "title": "Exploring the Efficacy of Convolutional Neural Networks in Sleep Apnea Detection from Single Channel EEG", "comment": "5 pages, 6 figures, 1 table", "summary": "Sleep apnea, a prevalent sleep disorder, involves repeated episodes of\nbreathing interruptions during sleep, leading to various health complications,\nincluding cognitive impairments, high blood pressure, heart disease, stroke,\nand even death. One of the main challenges in diagnosing and treating sleep\napnea is identifying individuals at risk. The current gold standard for\ndiagnosis, Polysomnography (PSG), is costly, labor intensive, and inconvenient,\noften resulting in poor quality sleep data. This paper presents a novel\napproach to the detection of sleep apnea using a Convolutional Neural Network\n(CNN) trained on single channel EEG data. The proposed CNN achieved an accuracy\nof 85.1% and a Matthews Correlation Coefficient (MCC) of 0.22, demonstrating a\nsignificant potential for home based applications by addressing the limitations\nof PSG in automated sleep apnea detection. Key contributions of this work also\ninclude the development of a comprehensive preprocessing pipeline with an\nInfinite Impulse Response (IIR) Butterworth filter, a dataset construction\nmethod providing broader temporal context, and the application of SMOTETomek to\naddress class imbalance. This research underscores the feasibility of\ntransitioning from traditional laboratory based diagnostics to more accessible,\nautomated home based solutions, improving patient outcomes and broadening the\naccessibility of sleep disorder diagnostics."}
{"id": "2509.00029", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00029", "abs": "https://arxiv.org/abs/2509.00029", "authors": ["Leo Vitasovic", "Stella Graßhof", "Agnes Mercedes Kloft", "Ville V. Lehtola", "Martin Cunneen", "Justyna Starostka", "Glenn McGarry", "Kun Li", "Sami S. Brandt"], "title": "From Sound to Sight: Towards AI-authored Music Videos", "comment": "1st Workshop on Generative AI for Storytelling (AISTORY), 2025", "summary": "Conventional music visualisation systems rely on handcrafted ad hoc\ntransformations of shapes and colours that offer only limited expressiveness.\nWe propose two novel pipelines for automatically generating music videos from\nany user-specified, vocal or instrumental song using off-the-shelf deep\nlearning models. Inspired by the manual workflows of music video producers, we\nexperiment on how well latent feature-based techniques can analyse audio to\ndetect musical qualities, such as emotional cues and instrumental patterns, and\ndistil them into textual scene descriptions using a language model. Next, we\nemploy a generative model to produce the corresponding video clips. To assess\nthe generated videos, we identify several critical aspects and design and\nconduct a preliminary user evaluation that demonstrates storytelling potential,\nvisual coherency and emotional alignment with the music. Our findings\nunderscore the potential of latent feature techniques and deep generative\nmodels to expand music visualisation beyond traditional approaches."}
{"id": "2509.00106", "categories": ["eess.AS", "cs.SD", "H.5.5; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.00106", "abs": "https://arxiv.org/abs/2509.00106", "authors": ["Rohan Agarwal"], "title": "Quantum-Enhanced Analysis and Grading of Vocal Performance", "comment": "4 pages, 5 figures. Hybrid quantum - classical feasibility study;\n  simulator - only results", "summary": "We present QuantumMelody, a hybrid quantum-classical method for objective\nsinging assessment. Grouped vocal features (pitch stability, dynamics, timbre)\nare encoded into a small simulated quantum circuit; all nine qubits are\ninitialized with a Hadamard on each qubit and then receive Rx, Ry, and Rz\nrotations, with intra- and cross-group entanglement. The circuit measurement\nprobabilities are fused with spectrogram transformer embeddings to estimate a\ngrade on labels 2-5 and to surface technique-level feedback. On 168 labeled 20\nsecond excerpts, the hybrid reaches 74.29% agreement with expert graders, a\n+12.86 point gain over a classical-features baseline. Processing is sub-minute\nper recording on a laptop-class Qiskit simulator; we do not claim hardware\nspeedups. This is a feasibility step toward interpretable, objective singing\nassessment in applied audio signal processing."}
{"id": "2509.00016", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.00016", "abs": "https://arxiv.org/abs/2509.00016", "authors": ["Marcin Kolakowski"], "title": "Conditional Generative Adversarial Networks Based Inertial Signal Translation", "comment": "Originally presented at: 2025 Signal Processing Symposium (SPSympo)\n  Warsaw, Poland; Associated data available at: M. Kolakowski, \"Wrist and\n  Tibia/Shoe Mounted IMU Measurement Results for Gait Analysis.\" Zenodo, Dec.\n  27, 2023. doi: https://doi.org/10.5281/ZENODO.10436579", "summary": "The paper presents an approach in which inertial signals measured with a\nwrist-worn sensor (e.g., a smartwatch) are translated into those that would be\nrecorded using a shoe-mounted sensor, enabling the use of state-of-the-art gait\nanalysis methods. In the study, the signals are translated using Conditional\nGenerative Adversarial Networks (GANs). Two different GAN versions are used for\nexperimental verification: traditional ones trained using binary cross-entropy\nloss and Wasserstein GANs (WGANs). For the generator, two architectures, a\nconvolutional autoencoder, and a convolutional U-Net, are tested. The\nexperiment results have shown that the proposed approach allows for an accurate\ntranslation, enabling the use of wrist sensor inertial signals for efficient,\nevery-day gait analysis."}
{"id": "2509.00051", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00051", "abs": "https://arxiv.org/abs/2509.00051", "authors": ["Faria Binte Kader", "Santu Karmaker"], "title": "A Survey on Evaluation Metrics for Music Generation", "comment": "19 pages, 2 figures", "summary": "Despite significant advancements in music generation systems, the\nmethodologies for evaluating generated music have not progressed as expected\ndue to the complex nature of music, with aspects such as structure, coherence,\ncreativity, and emotional expressiveness. In this paper, we shed light on this\nresearch gap, introducing a detailed taxonomy for evaluation metrics for both\naudio and symbolic music representations. We include a critical review\nidentifying major limitations in current evaluation methodologies which\nincludes poor correlation between objective metrics and human perception,\ncross-cultural bias, and lack of standardization that hinders cross-model\ncomparisons. Addressing these gaps, we further propose future research\ndirections towards building a comprehensive evaluation framework for music\ngeneration evaluation."}
{"id": "2509.00400", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00400", "abs": "https://arxiv.org/abs/2509.00400", "authors": ["Xikun Lu", "Yunda Chen", "Zehua Chen", "Jie Wang", "Mingxing Liu", "Hongmei Hu", "Chengshi Zheng", "Stefan Bleeck", "Jinqiu Sang"], "title": "Deep Learning for Personalized Binaural Audio Reproduction", "comment": null, "summary": "Personalized binaural audio reproduction is the basis of realistic spatial\nlocalization, sound externalization, and immersive listening, directly shaping\nuser experience and listening effort. This survey reviews recent advances in\ndeep learning for this task and organizes them by generation mechanism into two\nparadigms: explicit personalized filtering and end-to-end rendering. Explicit\nmethods predict personalized head-related transfer functions (HRTFs) from\nsparse measurements, morphological features, or environmental cues, and then\nuse them in the conventional rendering pipeline. End-to-end methods map source\nsignals directly to binaural signals, aided by other inputs such as visual,\ntextual, or parametric guidance, and they learn personalization within the\nmodel. We also summarize the field's main datasets and evaluation metrics to\nsupport fair and repeatable comparison. Finally, we conclude with a discussion\nof key applications enabled by these technologies, current technical\nlimitations, and potential research directions for deep learning-based spatial\naudio systems."}
{"id": "2509.00018", "categories": ["eess.SP", "cs.AI", "cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.00018", "abs": "https://arxiv.org/abs/2509.00018", "authors": ["Jiacheng Guo", "Ning Gao", "Yiping Zuo", "Hao Xu", "Shi Jin", "Kai Kit Wong"], "title": "A Fluid Antenna Enabled Physical Layer Key Generation for Next-G Wireless Networks", "comment": null, "summary": "As a promising physical layer security technique, physical layer key\ngeneration (PLKG) enables legitimate users to obtain secret keys from wireless\nchannel without security infrastructures. However, in harsh propagation\nenvironments, the channel characteristic becomes unsatisfactory, the key\ngeneration rate (KGR) is significantly deteriorated. In this paper, we propose\na novel fluid antenna (FA) enabled PLKG system to address this challenge.\nSpecifically, we first derive the closed-form expression of the KGR for FA\narray, and then jointly optimize the precoding matrix and the antenna positions\nvia a particle swarm optimization (PSO) algorithm. Next, to further reduce the\ncomputational complexity of the optimization procedure, we develop an\nalternating optimization (AO) algorithm, which combines the projected gradient\ndescent (PGD) and the PSO. Simulation results demonstrate that by exploiting\nthe additional spatial degree of freedom (DoF), our FA enabled PLKG system is\nsuperior to the benchmarks, such as the conventional fixed-position antenna\n(FPA) array and the reconfigurable intelligent surface (RIS). It is worth\nhighlighting that compared to the conventional uniform planar antenna (UPA),\nthe FA enabled PLKG achieves a 35.42\\% KGR performance improvement under PSO\nalgorithm and a 67.73\\% KGR performance improvement under AO algorithm,\nrespectively."}
{"id": "2509.00120", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00120", "abs": "https://arxiv.org/abs/2509.00120", "authors": ["Eyal Briman", "Eyal Leizerovich", "Nimrod Talmon"], "title": "Algorithms for Collaborative Harmonization", "comment": "Presented at the 15th Multidisciplinary Workshop on Advances in\n  Preference Handling M-PREF 2024, Santiago de Compostela, Oct 20, 2024", "summary": "We consider a specific scenario of text aggregation, in the realm of musical\nharmonization. Musical harmonization shares similarities with text aggregation,\nhowever the language of harmony is more structured than general text.\nConcretely, given a set of harmonization suggestions for a given musical\nmelody, our interest lies in devising aggregation algorithms that yield an\nharmonization sequence that satisfies the following two key criteria: (1) an\neffective representation of the collective suggestions; and (2) an\nharmonization that is musically coherent. We present different algorithms for\nthe aggregation of harmonies given by a group of agents and analyze their\ncomplexities. The results indicate that the Kemeny and plurality-based\nalgorithms are most effective in assessing representation and maintaining\nmusical coherence."}
{"id": "2509.00675", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00675", "abs": "https://arxiv.org/abs/2509.00675", "authors": ["Dong Yang", "Yuki Saito", "Takaaki Saeki", "Tomoki Koriyama", "Wataru Nakata", "Detai Xin", "Hiroshi Saruwatari"], "title": "Speaker-Conditioned Phrase Break Prediction for Text-to-Speech with Phoneme-Level Pre-trained Language Model", "comment": "Under Review", "summary": "This paper advances phrase break prediction (also known as phrasing) in\nmulti-speaker text-to-speech (TTS) systems. We integrate speaker-specific\nfeatures by leveraging speaker embeddings to enhance the performance of the\nphrasing model. We further demonstrate that these speaker embeddings can\ncapture speaker-related characteristics solely from the phrasing task. Besides,\nwe explore the potential of pre-trained speaker embeddings for unseen speakers\nthrough a few-shot adaptation method. Furthermore, we pioneer the application\nof phoneme-level pre-trained language models to this TTS front-end task, which\nsignificantly boosts the accuracy of the phrasing model. Our methods are\nrigorously assessed through both objective and subjective evaluations,\ndemonstrating their effectiveness."}
{"id": "2509.00260", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00260", "abs": "https://arxiv.org/abs/2509.00260", "authors": ["Bastian Latsch", "Felix Herbst", "Mark Suppelt", "Julian Seiler", "Stephan Schaumann", "Sven Suppelt", "Alexander A. Altmann", "Martin Grimmer", "and Mario Kupnik"], "title": "A Review of Sensor Insoles", "comment": "18 pages, 8 figures", "summary": "Plantar pressure measurement, or pedobarography, is an essential tool for\nanalyzing human motion in healthy individuals and patients. Across the reviewed\nliterature, sensor insoles are motivated as wearable, mobile solutions for\nassessing pressure distribution in applications including diabetic foot\nmonitoring, rehabilitation guidance, assistive device control, and sports\nperformance analysis. This review evaluates the current state of the art with\nparticular attention to sensor technologies, sensor quantity and placement,\nparticipant cohorts, and reference standards. The focus lies on original works\nwith innovative designs, preferably supported by ambulation experiments. The\nmodalities covered include resistive, capacitive, inductive, piezoelectric,\ntriboelectric, and optical sensing approaches. We identify a lack of proper\nsensor calibration, gait-based verification, and human study validation, and\npropose a gold standard based on testing machines and instrumented treadmills\nto ensure comparability across studies. The bidirectional interaction between\ninsole insertion and foot-sole mechanics is examined, with tissue stiffness\nidentified as a key source of uncertainty in sensor signals. Guidelines are\nprovided for sensor dimensions and unobtrusive insole designs to foster natural\ngait. Finally, future directions include the development of multimodal sensors\nto compensate for limitations of individual modalities and the emerging trend\nof multiaxial sensing for capturing shear components in pressure distributions."}
{"id": "2509.00132", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00132", "abs": "https://arxiv.org/abs/2509.00132", "authors": ["Peiwen Xing", "Aske Plaat", "Niki van Stein"], "title": "CoComposer: LLM Multi-agent Collaborative Music Composition", "comment": null, "summary": "Existing AI Music composition tools are limited in generation duration,\nmusical quality, and controllability. We introduce CoComposer, a multi-agent\nsystem that consists of five collaborating agents, each with a task based on\nthe traditional music composition workflow. Using the AudioBox-Aesthetics\nsystem, we experimentally evaluate CoComposer on four compositional criteria.\nWe test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find\n(1) that CoComposer outperforms existing multi-agent LLM-based systems in music\nquality, and (2) compared to a single-agent system, in production complexity.\nCompared to non- LLM MusicLM, CoComposer has better interpretability and\neditability, although MusicLM still produces better music."}
{"id": "2509.00685", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00685", "abs": "https://arxiv.org/abs/2509.00685", "authors": ["Kangxiang Xia", "Xinfa Zhu", "Jixun Yao", "Lei Xie"], "title": "MPO: Multidimensional Preference Optimization for Language Model-based Text-to-Speech", "comment": "Accepted by NCMMSC2025", "summary": "In recent years, text-to-speech (TTS) has seen impressive advancements\nthrough large-scale language models, achieving human-level speech quality.\nIntegrating human feedback has proven effective for enhancing robustness in\nthese systems. However, current approaches face challenges in optimizing TTS\nwith preference data across multiple dimensions and often suffer from\nperformance degradation due to overconfidence in rewards. We propose\nMultidimensional Preference Optimization (MPO) to better align TTS systems with\nhuman preferences. MPO introduces a preference set that streamlines the\nconstruction of data for multidimensional preference optimization, enabling\nalignment with multiple dimensions. Additionally, we incorporate regularization\nduring training to address the typical degradation issues in DPO-based\napproaches. Our experiments demonstrate MPO's effectiveness, showing\nsignificant improvements in intelligibility, speaker similarity, and prosody\ncompared to baseline systems."}
{"id": "2509.00314", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00314", "abs": "https://arxiv.org/abs/2509.00314", "authors": ["Ang Li", "Zikai Wang", "Liuyin Yang", "Zhenyu Wang", "Tianheng Xu", "Honglin Hu", "Marc M. Van Hulle"], "title": "CoMET: A Contrastive-Masked Brain Foundation Model for Universal EEG Representation", "comment": null, "summary": "Electroencephalography (EEG) is a non-invasive technique for recording brain\nactivity, widely used in brain-computer interfaces, clinic, and healthcare.\nTraditional EEG deep models typically focus on specific dataset and task,\nlimiting model size and generalization. Recently, self-supervised brain\nfoundation models have emerged and been applied to various downstream tasks.\nNevertheless, these models still have limitations: current SOTA models\ntypically rely on masked reconstruction strategy; however, EEG features of\nadjacent channels are highly correlated, which causes the pre-training to\noverly focus on low-dimensional signal-similarity features in local regions and\nneglect the global discriminative patterns vital for downstream tasks. To\naddress these limitations, we propose a brain foundation model called CoMET.\nSpecifically, we employ the masked autoencoder with redesigned patching and\nembedding for EEG as backbone and devise a novel contrastive learning framework\nwith mirror-scale augmentation to strengthen the global discrimination ability.\nCoMET is pre-trained on mixed EEG datasets over 3000 subjects with over one\nmillion samples. It is evaluated on ten different downstream datasets, and the\nSOTA results demonstrate CoMET's superior ability in extracting universal EEG\nrepresentations and strong clinical potential."}
{"id": "2509.00186", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00186", "abs": "https://arxiv.org/abs/2509.00186", "authors": ["Arnab Das", "Yassine El Kheir", "Carlos Franzreb", "Tim Herzig", "Tim Polzehl", "Sebastian Möller"], "title": "Generalizable Audio Spoofing Detection using Non-Semantic Representations", "comment": null, "summary": "Rapid advancements in generative modeling have made synthetic audio\ngeneration easy, making speech-based services vulnerable to spoofing attacks.\nConsequently, there is a dire need for robust countermeasures more than ever.\nExisting solutions for deepfake detection are often criticized for lacking\ngeneralizability and fail drastically when applied to real-world data. This\nstudy proposes a novel method for generalizable spoofing detection leveraging\nnon-semantic universal audio representations. Extensive experiments have been\nperformed to find suitable non-semantic features using TRILL and TRILLsson\nmodels. The results indicate that the proposed method achieves comparable\nperformance on the in-domain test set while significantly outperforming\nstate-of-the-art approaches on out-of-domain test sets. Notably, it\ndemonstrates superior generalization on public-domain data, surpassing methods\nbased on hand-crafted features, semantic embeddings, and end-to-end\narchitectures."}
{"id": "2509.01087", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.01087", "abs": "https://arxiv.org/abs/2509.01087", "authors": ["Shuangyuan Chen", "Shuang Wei", "Dongxing Xu", "Yanhua Long"], "title": "Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech Recognition", "comment": "11 pages,4 figures", "summary": "To enhance the performance of end-to-end (E2E) speech recognition systems in\nnoisy or low signal-to-noise ratio (SNR) conditions, this paper introduces\nNoisyD-CT, a novel tri-stage training framework built on the\nConformer-Transducer architecture. The core of NoisyD-CT is a especially\ndesigned compact noisy disentanglement (NoisyD) module (adding only 1.71M\nparameters), integrated between the Conformer blocks and Transducer Decoder to\nperform deep noise suppression and improve ASR robustness in challenging\nacoustic noise environments. To fully exploit the noise suppression capability\nof the NoisyD-CT, we further propose a clean representation consistency loss to\nalign high-level representations derived from noisy speech with those obtained\nfrom corresponding clean speech. Together with a noisy reconstruction loss,\nthis consistency alignment enables the NoisyD module to effectively suppress\nnoise while preserving essential acoustic and linguistic features consistent\nacross both clean and noisy conditions, thereby producing cleaner internal\nrepresentations that enhance ASR performance. Moreover, our tri-stage training\nstrategy is designed to fully leverage the functionalities of both the noisy\ndisentanglement and speech recognition modules throughout the model training\nprocess, ultimately maximizing performance gains under noisy conditions. Our\nexperiments are performed on the LibriSpeech and CHiME-4 datasets, extensive\nresults demonstrate that our proposed NoisyD-CT significantly outperforms the\ncompetitive Conformer-Transducer baseline, achieving up to 25.7% and 10.6%\nrelative word error rate reductions on simulated and real-world noisy test\nsets, respectively, while maintaining or even improving performance on clean\nspeech test sets. The source code, model checkpoint and data simulation scripts\nwill be available at https://github.com/litchimo/NoisyD-CT."}
{"id": "2509.00323", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00323", "abs": "https://arxiv.org/abs/2509.00323", "authors": ["R. Abhishek Shankar", "Hyungjun Ha", "Byunghoo Jung"], "title": "Gait Analysis using 6DoF Magnetic Tracking", "comment": "10 pages, 5 figures, submitted to IEEE Sensors Journal", "summary": "Gait analysis using wearable devices has advantages over non-wearable devices\nwhen it comes to portability and accessibility. However, non-wearable devices\nhave consistently shown superior performance in terms of the gait information\nthey can provide. This calls for the need to improve the performance of\nwearable device based gait analysis. To that end, we developed a 6\nDegrees-of-Freedom (6DoF) magnetic tracking based gait analysis system as a\nstep in this direction. The system is portable, minimally intrusive, wireless\nand power efficient. As a proof-of-concept, the system was used for the task of\nHuman Activity Recognition (HAR) to classify four tasks - walking (W), walking\nwith weight (WW), jogging (J) and marching on the spot (M). Gait data of 12\nparticipants was collected. The classification performance of two deep learning\n(DL) classifiers - Convolutional Neural Networks (CNN) and Long Short Term\nMemory (LSTM) - was compared. The performance of the magnetic tracking based\ngait analysis system was also compared with an Inertial Measurement Unit (IMU)\n+ magnetometer based system. The magnetic tracking based system showed an\noverall classification accuracy of 92\\% compared to 86.69\\% for the IMU +\nmagnetometer system. Moreover, the magnetic tracking system showed an\nimprovement of about 8\\% in being able to differentiate between W and WW. This\nhighlights the insufficiency in the information content in the data from IMU +\nmagnetometer, warranting the need for a complete 6DoF tracking. Our work, thus,\nproves the feasibility of using magnetic tracking systems for the purpose of\ngait analysis."}
{"id": "2509.00230", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00230", "abs": "https://arxiv.org/abs/2509.00230", "authors": ["Linus Stuhlmann", "Michael Alexander Saxer"], "title": "Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks", "comment": null, "summary": "This study evaluates the performance of three advanced speech encoder models,\nWav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By\nfine-tuning these models and analyzing their layer-wise representations using\nSVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0\nand XLS-R capture speaker-specific features effectively in their early layers,\nwith fine-tuning improving stability and performance. Whisper showed better\nperformance in deeper layers. Additionally, we determined the optimal number of\ntransformer layers for each model when fine-tuned for speaker identification\ntasks."}
{"id": "2509.01391", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.01391", "abs": "https://arxiv.org/abs/2509.01391", "authors": ["Joonyong Park", "Daisuke Saito", "Nobuaki Minematsu"], "title": "MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model", "comment": "In Proceedings of the 17th Asia Pacific Signal and Information\n  Processing Association Annual Summit and Conference (APSIPA ASC 2025)", "summary": "This study presents a novel approach to voice synthesis that can substitute\nthe traditional grapheme-to-phoneme (G2P) conversion by using a deep\nlearning-based model that generates discrete tokens directly from speech.\nUtilizing a pre-trained voice SSL model, we train a T5 encoder to produce\npseudo-language labels from mixed-script texts (e.g., containing Kanji and\nKana). This method eliminates the need for manual phonetic transcription,\nreducing costs and enhancing scalability, especially for large non-transcribed\naudio datasets. Our model matches the performance of conventional G2P-based\ntext-to-speech systems and is capable of synthesizing speech that retains\nnatural linguistic and paralinguistic features, such as accents and\nintonations."}
{"id": "2509.00331", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00331", "abs": "https://arxiv.org/abs/2509.00331", "authors": ["Yaqian Yi", "Guangchi Zhang", "Miao Cui", "Changsheng You", "Qingqing Wu"], "title": "AN-Aided Secure Beamforming for ELAA-SWIPT in Mixed Near- and Far-Field", "comment": null, "summary": "This letter investigates secure hybrid beamforming (HB) design for an\nextremely large-scale antenna array-aided simultaneous wireless information and\npower transfer (SWIPT) system operating in a mixed near-field (NF)/far-field\n(FF) environment. A base station (BS) employs HB to transmit information and\nartificial noise (AN) signals simultaneously to multiple FF information\nreceivers (IRs) and NF energy receivers (ERs). The objective is to maximize the\nweighted sum secrecy rate for the IRs, considering both Type-I (unable to\ncancel AN) and Type-II (capable of canceling AN) IRs, subject to minimum energy\nharvesting requirements at the ERs and a BS transmit power constraint. We\nformulate optimization problems for both IR types and develop an efficient\niterative algorithm based on successive convex approximation. Simulation\nresults validate the proposed scheme and provide crucial insights into the\nsecurity performance of mixed-field SWIPT systems, highlighting the influence\nof visibility regions and angular user separation."}
{"id": "2509.00318", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00318", "abs": "https://arxiv.org/abs/2509.00318", "authors": ["Tianyu Song", "Ton Viet Ta"], "title": "Towards High-Fidelity and Controllable Bioacoustic Generation via Enhanced Diffusion Learning", "comment": null, "summary": "Generative modeling offers new opportunities for bioacoustics, enabling the\nsynthesis of realistic animal vocalizations that could support biomonitoring\nefforts and supplement scarce data for endangered species. However, directly\ngenerating bird call waveforms from noisy field recordings remains a major\nchallenge.\n  We propose BirdDiff, a generative framework designed to synthesize bird calls\nfrom a noisy dataset of 12 wild bird species. The model incorporates a \"zeroth\nlayer\" stage for multi-scale adaptive bird-call enhancement, followed by a\ndiffusion-based generator conditioned on three modalities: Mel-frequency\ncepstral coefficients, species labels, and textual descriptions. The\nenhancement stage improves signal-to-noise ratio (SNR) while minimizing\nspectral distortion, achieving the highest SNR gain (+10.45 dB) and lowest\nItakura-Saito Distance (0.54) compared to three widely used non-training\nenhancement methods.\n  We evaluate BirdDiff against a baseline generative model, DiffWave. Our\nmethod yields substantial improvements in generative quality metrics: Fr\\'echet\nAudio Distance (0.590 to 0.213), Jensen-Shannon Divergence (0.259 to 0.226),\nand Number of Statistically-Different Bins (7.33 to 5.58). To assess\nspecies-specific detail preservation, we use a ResNet50 classifier trained on\nthe original dataset to identify generated samples. Classification accuracy\nimproves from 35.9% (DiffWave) to 70.1% (BirdDiff), with 8 of 12 species\nexceeding 70% accuracy.\n  These results demonstrate that BirdDiff enables high-fidelity, controllable\nbird call generation directly from noisy field recordings."}
{"id": "2509.01419", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01419", "abs": "https://arxiv.org/abs/2509.01419", "authors": ["Ting Dang", "Trini Manoj Jeyaseelan", "Eliathamby Ambikairajah", "Vidhyasaharan Sethu"], "title": "Characterization of Speech Similarity Between Australian Aboriginal and High-Resource Languages: A Case Study on Dharawal", "comment": "Accepted at APSIPA ASC 2025", "summary": "Australian Aboriginal languages are of significant cultural and linguistic\nvalue but remain severely underrepresented in modern speech AI systems. While\nstate-of-the-art speech foundation models and automatic speech recognition\nexcel in high-resource settings, they often struggle to generalize to\nlow-resource languages, especially those lacking clean, annotated speech data.\nIn this work, we collect and clean a speech dataset for Dharawal, a\nlow-resource Australian Aboriginal language, by carefully sourcing and\nprocessing publicly available recordings. Using this dataset, we analyze the\nspeech similarity between Dharawal and 107 high-resource languages using a\npre-trained multilingual speech encoder. Our approach combines (1)\nmisclassification rate analysis to assess language confusability, and (2)\nfine-grained similarity measurements using cosine similarity and Fr\\'echet\nInception Distance (FID) in the embedding space. Experimental results reveal\nthat Dharawal shares strong speech similarity with languages such as Latin,\nM\\=aori, Korean, Thai, and Welsh. These findings offer practical guidance for\nfuture transfer learning and model adaptation efforts, and underscore the\nimportance of data collection and embedding-based analysis in supporting speech\ntechnologies for endangered language communities."}
{"id": "2509.00478", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00478", "abs": "https://arxiv.org/abs/2509.00478", "authors": ["Getuar Rexhepi", "Kuranage Roche Rayan Ranasinghe", "Kengo Ando", "Giuseppe Thadeu Freitas de Abreu", "David Gonzalez G"], "title": "Pilot Allocation and Receiver Design for Cell-Free Massive MIMO ISAC Systems", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "This paper tackles two key challenges in cell-freemassive multiple input\nmultiple output (CF-mMIMO) systems:efficient pilot allocation and practical\nreceiver design. To thisend, we introduce a novel pilot allocation framework\nleveragingmanifold optimization to maximize the system sum rate, wherepilot\nsequences are designed as nearly orthogonal sequences. Theproposed pilot design\nenforces unimodularity constraints in thefrequency domain, ensuring pilots are\nsuitable for both communi-cation and sensing tasks. Additionally, a gaussian\nbelief propaga-tion (GaBP)-based receiver is introduced, providing\nnear-optimaldetection performance with substantially reduced\ncomputationalcomplexity. Simulation results demonstrate that the proposedpilot\nallocation method achieves communication performancecomparable to\nstate-of-the-art (SotA) algorithms, while deliveringsuperior sensing\ncapabilities due to its unimodular pilot design.The GaBP-based receiver\nachieves robust performance andlower complexity compared to conventional\napproaches. Thesecontributions advance the practical deployment of CF-mMIMOfor\nintegrated sensing and communications (ISAC)."}
{"id": "2509.00405", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00405", "abs": "https://arxiv.org/abs/2509.00405", "authors": ["Xihao Yuan", "Siqi Liu", "Yan Chen", "Hang Zhou", "Chang Liu", "Hanting Chen", "Jie Hu"], "title": "SaD: A Scenario-Aware Discriminator for Speech Enhancement", "comment": "5 pages, 2 figures.Accepted by InterSpeech2025", "summary": "Generative adversarial network-based models have shown remarkable performance\nin the field of speech enhancement. However, the current optimization\nstrategies for these models predominantly focus on refining the architecture of\nthe generator or enhancing the quality evaluation metrics of the discriminator.\nThis approach often overlooks the rich contextual information inherent in\ndiverse scenarios. In this paper, we propose a scenario-aware discriminator\nthat captures scene-specific features and performs frequency-domain division,\nthereby enabling a more accurate quality assessment of the enhanced speech\ngenerated by the generator. We conducted comprehensive experiments on three\nrepresentative models using two publicly available datasets. The results\ndemonstrate that our method can effectively adapt to various generator\narchitectures without altering their structure, thereby unlocking further\nperformance gains in speech enhancement across different scenarios."}
{"id": "2509.01787", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.01787", "abs": "https://arxiv.org/abs/2509.01787", "authors": ["Yiwei Guo", "Bohan Li", "Hankun Wang", "Zhihan Li", "Shuai Wang", "Xie Chen", "Kai Yu"], "title": "AHAMask: Reliable Task Specification for Large Audio Language Models without Instructions", "comment": "15 pages, 7 tables, 6 figures", "summary": "Although current large audio language models (LALMs) extend text large\nlanguage models (LLMs) with generic acoustic understanding abilities, they\nusually suffer from instruction sensitivity, where different instructions of\nthe same intention can yield drastically different outcomes. In this work, we\npropose AHAMask, where we simply mask some of the attention heads in the\ndecoder-only LLM backbone of LALMs, to trigger specific acoustic task\nfunctionalities without instructions. These masks are efficiently obtained by\ntraining on an LALM, with the number of trainable parameters equal to the\nattention head count in its LLM backbone. We show by experiments that applying\nsuch selective attention head masks achieves comparable or even better\nperformance than using instructions, either on single or composite tasks.\nBesides achieving reliable acoustic task specification for LALMs, this also\nreveals that LALMs exhibit certain \"functional pathways\" in their attention\nheads."}
{"id": "2509.00492", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00492", "abs": "https://arxiv.org/abs/2509.00492", "authors": ["Liesbet Van der Perre", "Gilles Callebaut", "Thomas Eriksson", "Muris Sarajlic", "Christian Fager", "Fredrik Tufvesson", "Buon Kiong Lau", "Erik G. Larsson"], "title": "Distributed Deployment and Dual-Frequency Concepts to Strengthen Sub-THz Wireless Systems", "comment": null, "summary": "The vast bandwidth available at sub-THz frequencies holds great promise for\nhigh-speed wireless access, precise localization, and advanced sensing\napplications. However, fundamental physical constraints and technological\nlimitations make the deployment of reliable sub-THz networks challenging. We\npropose a new paradigm for sub-THz coverage by transmitting the RF signals over\npolymer microwave fibers (PMFs) that interconnect low-complexity radio units\n(RUs) in a daisy-chain configuration. The distributed architecture ensures that\nuser equipments (UEs) connect to RUs in their proximity, reducing path loss and\nmitigating blocking. The RUs leverage low-complexity, compact integrated\nantenna modules. Additionally, dual-frequency tandem operation is proposed,\nintegrating the sub-THz system with a sub-10 GHz system that provides control\nsignalling and a robust fallback solution for the sub-THz system. This proposed\ntandem architecture can open up the full potential of sub-THz technology and\npaves the way to cost- and energy-efficient, high-performance, real-time\nconnectivity in dynamic environments."}
{"id": "2509.00654", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00654", "abs": "https://arxiv.org/abs/2509.00654", "authors": ["Ashwin Nagarajan", "Hao-Wen Dong"], "title": "The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation", "comment": "10 pages, 2 figures", "summary": "Text-to-music models capture broad attributes such as instrumentation or\nmood, but fine-grained stylistic control remains an open challenge. Existing\nstylization methods typically require retraining or specialized conditioning,\nwhich complicates reproducibility and limits policy compliance when artist\nnames are restricted. We study whether lightweight, human-readable modifiers\nsampled from a large language model can provide a policy-robust alternative for\nstylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish\n(vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use\nfifteen reference excerpts and evaluate matched seeds under three conditions:\nbaseline prompts, artist-name prompts, and five descriptor sets. All prompts\nare generated using a large language model. Evaluation uses both VGGish and\nCLAP embeddings with distributional and per-clip similarity measures, including\na new min-distance attribution metric. Results show that artist names are the\nstrongest control signal across both artists, while name-free descriptors\nrecover much of this effect. This highlights that existing safeguards such as\nthe restriction of artist names in music generation prompts may not fully\nprevent style imitation. Cross-artist transfers reduce alignment, showing that\ndescriptors encode targeted stylistic cues. We also present a descriptor table\nacross ten contemporary artists to illustrate the breadth of the tokens.\nTogether these findings define the name-free gap, the controllability\ndifference between artist-name prompts and policy-compliant descriptors, shown\nthrough a reproducible evaluation protocol for prompt-level controllability."}
{"id": "2509.01889", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01889", "abs": "https://arxiv.org/abs/2509.01889", "authors": ["Yu Tsao"], "title": "From Evaluation to Optimization: Neural Speech Assessment for Downstream Applications", "comment": "5 pages, 1 figure", "summary": "The evaluation of synthetic and processed speech has long been a cornerstone\nof audio engineering and speech science. Although subjective listening tests\nremain the gold standard for assessing perceptual quality and intelligibility,\ntheir high cost, time requirements, and limited scalability present significant\nchallenges in the rapid development cycles of modern speech technologies.\nTraditional objective metrics, while computationally efficient, often exhibit\nweak correlation with human perception, creating a perceptual gap between\nsystem optimization and actual user experience. Bridging this gap requires\nspeech assessment models that are more closely aligned with human perception.\nIn recent years, numerous neural network-based speech assessment models have\nbeen developed to predict quality and intelligibility, achieving promising\nresults. Beyond their role in evaluation, these models are increasingly\nintegrated into downstream speech processing tasks. This review focuses on\ntheir role in two main areas: (1) serving as differentiable perceptual proxies\nthat not only assess but also guide the optimization of speech enhancement and\nsynthesis models; and (2) enabling the detection of salient speech\ncharacteristics to support more precise and efficient downstream processing.\nFinally, we discuss current limitations and outline future research directions\nto further advance the integration of speech assessment into speech processing\npipelines."}
{"id": "2509.00568", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00568", "abs": "https://arxiv.org/abs/2509.00568", "authors": ["Zahra Rostamikafaki", "Francois Chan", "Claude D'Amours"], "title": "Robust Resource Allocation for LEO Satellite-Assisted Secure SWIPT via STAR-RIS under CSI Uncertainty", "comment": null, "summary": "This paper proposes a robust resource allocation framework for a low Earth\norbit (LEO) satellite-enabled simultaneous wireless information and power\ntransfer (SWIPT) system, assisted by a ground-deployed simultaneously\ntransmitting and reflecting reconfigurable intelligent surface (STAR-RIS). We\nconsider a scenario where direct satellite-to-ground links are obstructed, and\nthe satellite serves multiple single-antenna energy receivers, information\nreceivers, and eavesdroppers exclusively via the STAR-RIS. A robust\noptimization problem is formulated to maximize the total harvested power,\nsubject to secrecy rate requirements, transmit power limits, and STAR-RIS\ncoefficient constraints, under a practical bounded channel state information\n(CSI) error model. To achieve optimal robust resource allocation, we address\nthe challenges posed by coupled optimization variables and bounded channel\nestimation errors by first applying the S-procedure to handle robustness\nagainst channel uncertainty. An alternating optimization (AO) framework is\nsubsequently proposed, where the active beamforming at the LEO satellite and\nthe passive beamforming at the STAR-RIS are jointly optimized, and a\npenalty-based strategy is incorporated to enforce the STAR-RIS beamforming\ndesign. Simulation results validate the effectiveness of the proposed algorithm\nand demonstrate that the STAR-RIS architecture achieves substantial performance\ngains in total harvested power over conventional RIS and other baseline\nschemes."}
{"id": "2509.00683", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.00683", "abs": "https://arxiv.org/abs/2509.00683", "authors": ["Zihao Zheng", "Zeyu Xie", "Xuenan Xu", "Wen Wu", "Chao Zhang", "Mengyue Wu"], "title": "PicoAudio2: Temporal Controllable Text-to-Audio Generation with Natural Language Description", "comment": "Demo page: https://HiRookie9.github.io/PicoAudio2-Page", "summary": "Controllable text-to-audio generation (TTA) has attracted much attention\nrecently. Although existing works can achieve fine-grained controllability\nbased on timestamp information, sound event categories are limited to a fixed\nset. Moreover, since only simulated data is used for training, the generated\naudio quality and generalization performance on real data are limited. To\ntackle this issue, we propose PicoAudio2, improving temporal-controllable TTA\nvia a new data processing pipeline and model architecture. Specifically, we use\na grounding model to annotate event timestamps of real audio-text datasets to\ncurate temporally-strong real data, in addition to simulation data from\nexisting works. The model is trained on the combination of real and simulation\ndata. Moreover, following PicoAudio, we encode timestamp information into a\ntimestamp matrix to provide extra fine-grained time-aligned information to the\nmodel, on top of the coarse-grained textual description. Experiments show that\nPicoAudio2 exhibits superior performance in terms of temporal controllability\nand audio quality."}
{"id": "2509.01900", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01900", "abs": "https://arxiv.org/abs/2509.01900", "authors": ["Zehan Li", "Yan Yang", "Xueqing Li", "Jian Kang", "Xiao-Lei Zhang", "Jie Li"], "title": "Multilingual Speech Recognition Using Discrete Tokens with a Two-step Training Strategy", "comment": "Accepted by NCMMSC 2024", "summary": "Pre-trained models, especially self-supervised learning (SSL) models, have\ndemonstrated impressive results in automatic speech recognition (ASR) task.\nWhile most applications of SSL models focus on leveraging continuous\nrepresentations as features for training downstream tasks, the utilization of\ndiscrete units has gained increasing attention in recent years owing to its\nlower storage requirements and broader range of applications. In multilingual\nASR tasks, representations at different layers of the model contribute\ndifferently to various languages, complicating the unification of discrete unit\nmodeling. In this paper, we propose a two-stage training strategy to improve\nthe discrete token performance of pre-trained models and narrow the gap with\ncontinuous representation performance. We validate our method on the XLS-R\nmodel following the settings of Interspeech2024 Speech Processing Using\nDiscrete Speech Unit Challenge. Our method demonstrates a significant\nimprovement on the ML-SUPERB dataset, achieving a 44% relative reduction on CER\nfor the XLS-R model. This surpasses the previous baseline set by the WavLM\nmodel, which achieves a 26% relative reduction on CER. Furthermore, our method\nachieves the first place among all the single-system results on the\nleaderboard."}
{"id": "2509.00670", "categories": ["eess.SP", "cs.HC", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.00670", "abs": "https://arxiv.org/abs/2509.00670", "authors": ["Gursimran Singh", "Aviral Chharia", "Rahul Upadhyay", "Vinay Kumar", "Luca Longo"], "title": "PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces", "comment": "PLoS One 2025. Project Website: https://neurodiag.github.io/PyNoetic", "summary": "Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have\nemerged as a transformative technology with applications spanning robotics,\nvirtual reality, medicine, and rehabilitation. However, existing BCI frameworks\nface several limitations, including a lack of stage-wise flexibility essential\nfor experimental research, steep learning curves for researchers without\nprogramming expertise, elevated costs due to reliance on proprietary software,\nand a lack of all-inclusive features leading to the use of multiple external\ntools affecting research outcomes. To address these challenges, we present\nPyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI\nresearch. PyNoetic is one of the very few frameworks in Python that encompasses\nthe entire BCI design pipeline, from stimulus presentation and data acquisition\nto channel selection, filtering, feature extraction, artifact removal, and\nfinally simulation and visualization. Notably, PyNoetic introduces an intuitive\nand end-to-end GUI coupled with a unique pick-and-place configurable flowchart\nfor no-code BCI design, making it accessible to researchers with minimal\nprogramming experience. For advanced users, it facilitates the seamless\nintegration of custom functionalities and novel algorithms with minimal coding,\nensuring adaptability at each design stage. PyNoetic also includes a rich array\nof analytical tools such as machine learning models, brain-connectivity\nindices, systematic testing functionalities via simulation, and evaluation\nmethods of novel paradigms. PyNoetic's strengths lie in its versatility for\nboth offline and real-time BCI development, which streamlines the design\nprocess, allowing researchers to focus on more intricate aspects of BCI\ndevelopment and thus accelerate their research endeavors. Project Website:\nhttps://neurodiag.github.io/PyNoetic"}
{"id": "2509.00813", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00813", "abs": "https://arxiv.org/abs/2509.00813", "authors": ["Gyehun Go", "Satbyul Han", "Ahyeon Choi", "Eunjin Choi", "Juhan Nam", "Jeong Mi Park"], "title": "AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation", "comment": "to be published in HCMIR25: 3rd Workshop on Human-Centric Music\n  Information Research", "summary": "Recent advances in text-to-music (TTM) generation have enabled controllable\nand expressive music creation using natural language prompts. However, the\nemotional fidelity of TTM systems remains largely underexplored compared to\nhuman preference or text alignment. In this study, we introduce AImoclips, a\nbenchmark for evaluating how well TTM systems convey intended emotions to human\nlisteners, covering both open-source and commercial models. We selected 12\nemotion intents spanning four quadrants of the valence-arousal space, and used\nsix state-of-the-art TTM systems to generate over 1,000 music clips. A total of\n111 participants rated the perceived valence and arousal of each clip on a\n9-point Likert scale. Our results show that commercial systems tend to produce\nmusic perceived as more pleasant than intended, while open-source systems tend\nto perform the opposite. Emotions are more accurately conveyed under\nhigh-arousal conditions across all models. Additionally, all systems exhibit a\nbias toward emotional neutrality, highlighting a key limitation in affective\ncontrollability. This benchmark offers valuable insights into model-specific\nemotion rendering characteristics and supports future development of\nemotionally aligned TTM systems."}
{"id": "2509.01929", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01929", "abs": "https://arxiv.org/abs/2509.01929", "authors": ["Rina Kotani", "Chiaki Miyazaki", "Shiro Suzuki"], "title": "Binaural Unmasking in Practical Use: Perceived Level of Phase-inverted Speech in Environmental Noise", "comment": null, "summary": "We aim to develop a technology that makes the sound from earphones and\nheadphones easier to hear without increasing the sound pressure or eliminating\nambient noise. To this end, we focus on harnessing the phenomenon of binaural\nunmasking through phase reversal in one ear. Specifically, we conduct\nexperiments to evaluate the improvement of audibility caused by the phenomenon,\nusing conditions that approximate practical scenarios. We use speech sounds by\nvarious speakers, including women, and noises that can be encountered in daily\nlife (urban environmental sounds, cheers) to verify the effects of binaural\nunmasking under conditions close to practical situations. The results of\nexperiments using the Japanese language showed that (i) speech in a noisy\nenvironment is perceived to be up to about 6 dB louder with phase reversal in\none ear, and (ii) a certain effect (improvement of audibility by 5 dB or more)\nis obtained for all speakers and noises targeted in this study. These findings\ndemonstrate the effectiveness of binaural unmasking attributed to interaural\nphase differences in practical scenarios."}
{"id": "2509.00727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00727", "abs": "https://arxiv.org/abs/2509.00727", "authors": ["Lin Guo", "Tiejun Lv", "Yashuai Cao", "Mugen Peng"], "title": "Uninformed-to-Informed Estimation: A Ping-Pong Positioning Method for Multi-user Wideband mmWave Systems", "comment": "16 pages, 13 figures, Accepted by IEEE Transactions on Communications", "summary": "To enhance the positioning and tracking performance of dynamic user equipment\n(UE) in wideband millimeter-wave (mmWave) systems, we propose a novel\npositioning error lower bound (PELB)-driven ping-pong positioning framework,\nwhere the base station (BS) and UE alternately transmit and receive adaptive\nbeamforming signals for positioning. All beam-formers are scheduled based on\nthe locally evaluated PELB. In this framework, we exploit multi-dimensional\ninformation fusion to assist in positioning. Firstly, a multi-subcarrier\ncollaborative positioning error lower bound (MSCPEB) is proposed to evaluate\nthe positioning error limits of wideband mmWave systems, which quantifies the\ncontribution of all subcarriers to positioning accuracy. Moreover, we prove\nthat the MSCPEB does not exceed the arithmetic mean of the PELBs of the\nindividual subcarriers. Subsequently, we develop an alternating optimization\n(AO) algorithm to optimize the hybrid beamformers targeted for MSCPEB\nminimization. By convexifying this problem, closed-form solutions of\nbeamformers are derived. Finally, we develop a multipath collaborative\npositioning method that quantifies the impact of path reliability on\npositioning accuracy, with a closed-form solution for user position derived.\nThe proposed method does not rely on path resolution and traditional triangular\nrelationships. Numerical results validate that the proposed method improves\nestimation accuracy by at least 16% compared to potential schemes without\noptimized beam configurations, while requiring only approximately one-quarter\nof the slot resources."}
{"id": "2509.00839", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00839", "abs": "https://arxiv.org/abs/2509.00839", "authors": ["Yuli Zhang", "Pengfei Fan", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Xinheng Wang"], "title": "Adaptive Vehicle Speed Classification via BMCNN with Reinforcement Learning-Enhanced Acoustic Processing", "comment": null, "summary": "Traffic congestion remains a pressing urban challenge, requiring intelligent\ntransportation systems for real-time management. We present a hybrid framework\nthat combines deep learning and reinforcement learning for acoustic vehicle\nspeed classification. A dual-branch BMCNN processes MFCC and wavelet features\nto capture complementary frequency patterns. An attention-enhanced DQN\nadaptively selects the minimal number of audio frames and triggers early\ndecisions once confidence thresholds are reached. Evaluations on IDMT-Traffic\nand our SZUR-Acoustic (Suzhou) datasets show 95.99% and 92.3% accuracy, with up\nto 1.63x faster average processing via early termination. Compared with A3C,\nDDDQN, SA2C, PPO, and TD3, the method provides a superior accuracy-efficiency\ntrade-off and is suitable for real-time ITS deployment in heterogeneous urban\nenvironments."}
{"id": "2509.01939", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01939", "abs": "https://arxiv.org/abs/2509.01939", "authors": ["Prashanth Gurunath Shivakumar", "Yile Gu", "Ankur Gandhe", "Ivan Bulyko"], "title": "Group Relative Policy Optimization for Speech Recognition", "comment": "Accepted for ASRU 2025", "summary": "Speech Recognition has seen a dramatic shift towards adopting Large Language\nModels (LLMs). This shift is partly driven by good scalability properties\ndemonstrated by LLMs, ability to leverage large amounts of labelled, unlabelled\nspeech and text data, streaming capabilities with auto-regressive framework and\nmulti-tasking with instruction following characteristics of LLMs. However,\nsimple next-token prediction objective, typically employed with LLMs, have\ncertain limitations in performance and challenges with hallucinations. In this\npaper, we propose application of Group Relative Policy Optimization (GRPO) to\nenable reinforcement learning from human feedback for automatic speech\nrecognition (ASR). We design simple rule based reward functions to guide the\npolicy updates. We demonstrate significant improvements in word error rate\n(upto 18.4% relative), reduction in hallucinations, increased robustness on\nout-of-domain datasets and effectiveness in domain adaptation."}
{"id": "2509.00766", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00766", "abs": "https://arxiv.org/abs/2509.00766", "authors": ["G. Maiolini Capez", "M. A. Caceres", "C. P. Bridges", "S. Frey", "R. Armellin", "R. Garello", "P. Bargellini"], "title": "Characterization of Mega-Constellation Links for LEO Missions With Applications to EO and ISS Use Cases", "comment": null, "summary": "Satellite missions demand ever greater connectivity, especially in the LEO\nregime. In this paper, we introduce the new mega-constellation services in\nspace paradigm: we show that megaconstellations, deployed to offer innovative\nservices to Earth's users, can provide excellent connectivity to LEO spacecraft\nas well. First, we characterise the communication link between space users and\nthe actual OneWeb and Starlink constellations. A full set of results in terms\nof availability, access duration, Doppler, and path losses as a function of\nuser orbital parameters, identifying optimal user orbits, is provided. The\nresults achieved by a multi-system user able to communicate with both fleets\nare also presented. The potential improvements available if geostationary\nconstellations are used to complement LEO megaconstellations in a multi-orbit\nsystem are discussed as well. Finally, we focus on two LEO use cases: the\nInternational Space Station and an Earth Observation Sun Synchronous satellite.\nAll the results demonstrate the numerous advantages of the mega-constellation\nconnectivity solution, which can transform LEO spacecraft into highly\nresponsive nodes of a space-to-space network."}
{"id": "2509.00862", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00862", "abs": "https://arxiv.org/abs/2509.00862", "authors": ["Yuriy Izotov", "Andrei Velichko"], "title": "Speech Command Recognition Using LogNNet Reservoir Computing for Embedded Systems", "comment": "20 pages, 6 figures", "summary": "This paper presents a low-resource speech-command recognizer combining\nenergy-based voice activity detection (VAD), an optimized Mel-Frequency\nCepstral Coefficients (MFCC) pipeline, and the LogNNet reservoir-computing\nclassifier. Using four commands from the Speech Commands da-taset downsampled\nto 8 kHz, we evaluate four MFCC aggregation schemes and find that adaptive\nbinning (64-dimensional feature vector) offers the best accuracy-to-compactness\ntrade-off. The LogNNet classifier with architecture 64:33:9:4 reaches 92.04%\naccuracy under speaker-independent evaluation, while requiring significantly\nfewer parameters than conventional deep learn-ing models. Hardware\nimplementation on Arduino Nano 33 IoT (ARM Cor-tex-M0+, 48 MHz, 32 KB RAM)\nvalidates the practical feasibility, achieving ~90% real-time recognition\naccuracy while consuming only 18 KB RAM (55% utilization). The complete\npipeline (VAD -> MFCC -> LogNNet) thus enables reliable on-device\nspeech-command recognition under strict memory and compute limits, making it\nsuitable for battery-powered IoT nodes, wire-less sensor networks, and\nhands-free control interfaces."}
{"id": "2509.00029", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00029", "abs": "https://arxiv.org/abs/2509.00029", "authors": ["Leo Vitasovic", "Stella Graßhof", "Agnes Mercedes Kloft", "Ville V. Lehtola", "Martin Cunneen", "Justyna Starostka", "Glenn McGarry", "Kun Li", "Sami S. Brandt"], "title": "From Sound to Sight: Towards AI-authored Music Videos", "comment": "1st Workshop on Generative AI for Storytelling (AISTORY), 2025", "summary": "Conventional music visualisation systems rely on handcrafted ad hoc\ntransformations of shapes and colours that offer only limited expressiveness.\nWe propose two novel pipelines for automatically generating music videos from\nany user-specified, vocal or instrumental song using off-the-shelf deep\nlearning models. Inspired by the manual workflows of music video producers, we\nexperiment on how well latent feature-based techniques can analyse audio to\ndetect musical qualities, such as emotional cues and instrumental patterns, and\ndistil them into textual scene descriptions using a language model. Next, we\nemploy a generative model to produce the corresponding video clips. To assess\nthe generated videos, we identify several critical aspects and design and\nconduct a preliminary user evaluation that demonstrates storytelling potential,\nvisual coherency and emotional alignment with the music. Our findings\nunderscore the potential of latent feature techniques and deep generative\nmodels to expand music visualisation beyond traditional approaches."}
{"id": "2509.00774", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00774", "abs": "https://arxiv.org/abs/2509.00774", "authors": ["Okyanus Oral"], "title": "Fast Regularized 3D Near-Field MIMO Imaging Using Stochastic Proximal Gradient Method", "comment": "Presented in ISCS25", "summary": "Near-field multiple-input multiple-output (MIMO) radar imaging suffers from\nhigh computational load inherently due to irregular spatial sampling with\ndistributed antennas. Existing acceleration methods for near-field MIMO imaging\ntypically rely on interpolation or compensation of measurements and are\nprimarily developed for direct reconstruction. This hinders their ease of\nadoption for different MIMO geometries and requires further modification for\nregularized inversion. In this study, we address these challenges by developing\na fast regularized reconstruction approach for three-dimensional near-field\nMIMO imaging based on the Stochastic Proximal Gradient Method. We demonstrate\nthe performance of the developed approach through experimental measurements.\nThe results show a significant improvement in runtime without any notable\ncompromise in reconstruction quality."}
{"id": "2509.00914", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00914", "abs": "https://arxiv.org/abs/2509.00914", "authors": ["Hainan Wang", "Mehdi Hosseinzadeh", "Reza Rawassizadeh"], "title": "TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization", "comment": "12 pages for main context, 5 figures", "summary": "The success of the generative model has gained unprecedented attention in the\nmusic generation area. Transformer-based architectures have set new benchmarks\nfor model performance. However, their practical adoption is hindered by some\ncritical challenges: the demand for massive computational resources and\ninference time, due to their large number of parameters. These obstacles make\nthem infeasible to deploy on edge devices, such as smartphones and wearables,\nwith limited computational resources. In this work, we present TinyMusician, a\nlightweight music generation model distilled from MusicGen (a State-of-the-art\nmusic generation model). TinyMusician integrates two innovations: (i)\nStage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive\nMixed-Precision Quantization. The experimental results demonstrate that\nTinyMusician retains 93% of the MusicGen-Small performance with 55% less model\nsize. TinyMusician is the first mobile-deployable music generation model that\neliminates cloud dependency while maintaining high audio fidelity and efficient\nresource usage"}
{"id": "2509.00051", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00051", "abs": "https://arxiv.org/abs/2509.00051", "authors": ["Faria Binte Kader", "Santu Karmaker"], "title": "A Survey on Evaluation Metrics for Music Generation", "comment": "19 pages, 2 figures", "summary": "Despite significant advancements in music generation systems, the\nmethodologies for evaluating generated music have not progressed as expected\ndue to the complex nature of music, with aspects such as structure, coherence,\ncreativity, and emotional expressiveness. In this paper, we shed light on this\nresearch gap, introducing a detailed taxonomy for evaluation metrics for both\naudio and symbolic music representations. We include a critical review\nidentifying major limitations in current evaluation methodologies which\nincludes poor correlation between objective metrics and human perception,\ncross-cultural bias, and lack of standardization that hinders cross-model\ncomparisons. Addressing these gaps, we further propose future research\ndirections towards building a comprehensive evaluation framework for music\ngeneration evaluation."}
{"id": "2509.00782", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00782", "abs": "https://arxiv.org/abs/2509.00782", "authors": ["Dvir Avrahami", "Amit Milstein", "Caroline Chaux", "Tirza Routtenberg", "Nir Shlezinger"], "title": "Deep Unfolding with Approximated Computations for Rapid Optimization", "comment": "Under review for publication in the IEEE", "summary": "Optimization-based solvers play a central role in a wide range of signal\nprocessing and communication tasks. However, their applicability in\nlatency-sensitive systems is limited by the sequential nature of iterative\nmethods and the high computational cost per iteration. While deep unfolding has\nemerged as a powerful paradigm for converting iterative algorithms into learned\nmodels that operate with a fixed number of iterations, it does not inherently\naddress the cost of each iteration. In this paper, we introduce a learned\noptimization framework that jointly tackles iteration count and per-iteration\ncomplexity. Our approach is based on unfolding a fixed number of optimization\nsteps, replacing selected iterations with low-complexity approximated\ncomputations, and learning extended hyperparameters from data to compensate for\nthe introduced approximations. We demonstrate the effectiveness of our method\non two representative problems: (i) hybrid beamforming; and (ii) robust\nprincipal component analysis. These fundamental case studies show that our\nlearned approximated optimizers can achieve state-of-the-art performance while\nreducing computational complexity by over three orders of magnitude. Our\nresults highlight the potential of our approach to enable rapid, interpretable,\nand efficient decision-making in real-time systems."}
{"id": "2509.00988", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00988", "abs": "https://arxiv.org/abs/2509.00988", "authors": ["Swadhin Biswas", "Imran", "Tuhin Sheikh"], "title": "A Unified Denoising and Adaptation Framework for Self-Supervised Bengali Dialectal ASR", "comment": null, "summary": "Automatic Speech Recognition (ASR) for Bengali, the world's fifth most spoken\nlanguage, remains a significant challenge, critically hindering technological\naccessibility for its over 270 million speakers. This challenge is compounded\nby two persistent and intertwined factors: the language's vast dialectal\ndiversity and the prevalence of acoustic noise in real-world environments.\nWhile state-of-the-art self-supervised learning (SSL) models have advanced ASR\nfor low-resource languages, they often lack explicit mechanisms to handle\nenvironmental noise during pre-training or specialized adaptation strategies\nfor the complex phonetic and lexical variations across Bengali dialects. This\npaper introduces a novel, unified framework designed to address these dual\nchallenges simultaneously. Our approach is founded on the WavLM model, which is\nuniquely pre-trained with a masked speech denoising objective, making it\ninherently robust to acoustic distortions. We propose a specialized multi-stage\nfine-tuning strategy that first adapts the model to general-domain standard\nBengali to establish a strong linguistic foundation and subsequently\nspecializes it for noise-robust dialectal recognition through targeted data\naugmentation. The framework is rigorously evaluated on a comprehensive\nbenchmark comprising multiple Bengali dialects under a wide range of simulated\nnoisy conditions, from clean audio to low Signal-to-Noise Ratio (SNR) levels.\n  Experimental results demonstrate that the proposed framework significantly\noutperforms strong baselines, including standard fine-tuned wav2vec 2.0 and the\nlarge-scale multilingual Whisper model. This work establishes a new\nstate-of-the-art for this task and provides a scalable, effective blueprint for\ndeveloping practical ASR systems for other low-resource, high-variation\nlanguages globally."}
{"id": "2509.00120", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00120", "abs": "https://arxiv.org/abs/2509.00120", "authors": ["Eyal Briman", "Eyal Leizerovich", "Nimrod Talmon"], "title": "Algorithms for Collaborative Harmonization", "comment": "Presented at the 15th Multidisciplinary Workshop on Advances in\n  Preference Handling M-PREF 2024, Santiago de Compostela, Oct 20, 2024", "summary": "We consider a specific scenario of text aggregation, in the realm of musical\nharmonization. Musical harmonization shares similarities with text aggregation,\nhowever the language of harmony is more structured than general text.\nConcretely, given a set of harmonization suggestions for a given musical\nmelody, our interest lies in devising aggregation algorithms that yield an\nharmonization sequence that satisfies the following two key criteria: (1) an\neffective representation of the collective suggestions; and (2) an\nharmonization that is musically coherent. We present different algorithms for\nthe aggregation of harmonies given by a group of agents and analyze their\ncomplexities. The results indicate that the Kemeny and plurality-based\nalgorithms are most effective in assessing representation and maintaining\nmusical coherence."}
{"id": "2509.00851", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00851", "abs": "https://arxiv.org/abs/2509.00851", "authors": ["Hao Zhang", "Fuhui Zhou", "Qihui Wuand Chau Yuen"], "title": "Spectrum Cognition: Semantic Situation for Next-Generation Spectrum Management", "comment": "accpeted by IEEE Network", "summary": "In response to the growing complexity and demands of future wireless\ncommunication networks, spectrum cognition has emerged as an essential\ntechnique for optimizing spectrum utilization in next-generation wireless\nnetworks. This article presents a comprehensive overview of spectrum cognition,\nunderscoring its critical role in enhancing the efficiency and security of\nfuture wireless systems through the innovative perspective of \"data processing\nto signal analysis to semantic situation\". Semantic situation, as the highest\nlevel of spectrum cognition, enables the extraction of meaningful information\nfrom raw spectrum data to provide intelligent support for network decisions. We\nformally define spectrum cognition, clearly distinguishing it from traditional\nspectrum sensing, and delve into the latest advancements in both traditional\nand intelligent spectrum cognition frameworks, addressing key challenges in\nspectrum cognition. Furthermore, we propose concrete technical solutions to\naddress these challenges, highlighting the transformative potential of semantic\nsituation in shaping next-generation wireless systems. Our findings not only\ncontribute to the theoretical understanding of spectrum cognition but also\noffer practical insights for its implementation in real-world scenarios."}
{"id": "2509.01153", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01153", "abs": "https://arxiv.org/abs/2509.01153", "authors": ["Yun Chu", "Qiuhao Wang", "Enze Zhou", "Qian Liu", "Gang Zheng"], "title": "EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection", "comment": null, "summary": "Auscultation is a key method for early diagnosis of respiratory and pulmonary\ndiseases, relying on skilled healthcare professionals. However, the process is\noften subjective, with variability between experts. As a result, numerous deep\nlearning-based automatic classification methods have emerged, most of which\nfocus on respiratory sound classification. In contrast, research on respiratory\nsound event detection remains limited. Existing sound event detection methods\ntypically rely on frame-level predictions followed by post-processing to\ngenerate event-level outputs, making interval boundaries challenging to learn\ndirectly. Furthermore, many approaches can only handle fixed-length audio, lim-\niting their applicability to variable-length respiratory sounds. Additionally,\nthe impact of respiratory sound location information on detection performance\nhas not been extensively explored. To address these issues, we propose a graph\nneural network-based framework with anchor intervals, capable of handling\nvariable-length audio and providing more precise temporal localization for\nabnormal respi- ratory sound events. Our method improves both the flexibility\nand applicability of respiratory sound detection. Experiments on the SPRSound\n2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposed\napproach, and incorporating respiratory position information enhances the\ndiscrimination between abnormal sounds."}
{"id": "2509.00132", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00132", "abs": "https://arxiv.org/abs/2509.00132", "authors": ["Peiwen Xing", "Aske Plaat", "Niki van Stein"], "title": "CoComposer: LLM Multi-agent Collaborative Music Composition", "comment": null, "summary": "Existing AI Music composition tools are limited in generation duration,\nmusical quality, and controllability. We introduce CoComposer, a multi-agent\nsystem that consists of five collaborating agents, each with a task based on\nthe traditional music composition workflow. Using the AudioBox-Aesthetics\nsystem, we experimentally evaluate CoComposer on four compositional criteria.\nWe test with three LLMs (GPT-4o, DeepSeek-V3-0324, Gemini-2.5-Flash), and find\n(1) that CoComposer outperforms existing multi-agent LLM-based systems in music\nquality, and (2) compared to a single-agent system, in production complexity.\nCompared to non- LLM MusicLM, CoComposer has better interpretability and\neditability, although MusicLM still produces better music."}
{"id": "2509.00962", "categories": ["eess.SP", "94C10, 81V80, 94A24, 94B05, 68M15", "B.6.1; B.7.1; C.3; E.4"], "pdf": "https://arxiv.org/pdf/2509.00962", "abs": "https://arxiv.org/abs/2509.00962", "authors": ["Yerzhan Mustafa", "Berker Peköz", "Selçuk Köse"], "title": "Lightweight Error-Correction Code Encoders in Superconducting Electronic Systems", "comment": "5 pages, will be presented at IEEE SOCC 2025 Session 5: Emerging and\n  Disruptive Technologies from Mon, September 29, 2025 15:40 +04 until 17:20\n  (4th paper) in Luxor 2 (20 min.)", "summary": "Data transmission from superconducting electronic circuits, such as single\nflux quantum (SFQ) logic, to room-temperature electronics is susceptible to bit\nerrors, which may result from flux trapping, fabrication defects, and process\nparameter variations (PPV). Due to the cooling power budget at 4.2 K and\nconstraints on the chip area, the size of the error-correction code encoders is\nlimited. In this work, three lightweight error-correction code encoders are\nproposed that are based on Hamming(7,4), Hamming(8,4), and Reed-Muller(1,3)\ncodes and implemented with SFQ logic. The performance of these encoders is\nanalyzed in the presence of PPV. The trade-offs between the theoretical\ncomplexity and physical size of error-correction code encoders are identified."}
{"id": "2509.01336", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01336", "abs": "https://arxiv.org/abs/2509.01336", "authors": ["Wen-Chin Huang", "Hui Wang", "Cheng Liu", "Yi-Chiao Wu", "Andros Tjandra", "Wei-Ning Hsu", "Erica Cooper", "Yong Qin", "Tomoki Toda"], "title": "The AudioMOS Challenge 2025", "comment": "IEEE ASRU 2025", "summary": "This is the summary paper for the AudioMOS Challenge 2025, the very first\nchallenge for automatic subjective quality prediction for synthetic audio. The\nchallenge consists of three tracks. The first track aims to assess\ntext-to-music samples in terms of overall quality and textual alignment. The\nsecond track is based on the four evaluation dimensions of Meta Audiobox\nAesthetics, and the test set consists of text-to-speech, text-to-audio, and\ntext-to-music samples. The third track focuses on synthetic speech quality\nassessment in different sampling rates. The challenge attracted 24 unique teams\nfrom both academia and industry, and improvements over the baselines were\nconfirmed. The outcome of this challenge is expected to facilitate development\nand progress in the field of automatic evaluation for audio generation systems."}
{"id": "2509.00186", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00186", "abs": "https://arxiv.org/abs/2509.00186", "authors": ["Arnab Das", "Yassine El Kheir", "Carlos Franzreb", "Tim Herzig", "Tim Polzehl", "Sebastian Möller"], "title": "Generalizable Audio Spoofing Detection using Non-Semantic Representations", "comment": null, "summary": "Rapid advancements in generative modeling have made synthetic audio\ngeneration easy, making speech-based services vulnerable to spoofing attacks.\nConsequently, there is a dire need for robust countermeasures more than ever.\nExisting solutions for deepfake detection are often criticized for lacking\ngeneralizability and fail drastically when applied to real-world data. This\nstudy proposes a novel method for generalizable spoofing detection leveraging\nnon-semantic universal audio representations. Extensive experiments have been\nperformed to find suitable non-semantic features using TRILL and TRILLsson\nmodels. The results indicate that the proposed method achieves comparable\nperformance on the in-domain test set while significantly outperforming\nstate-of-the-art approaches on out-of-domain test sets. Notably, it\ndemonstrates superior generalization on public-domain data, surpassing methods\nbased on hand-crafted features, semantic embeddings, and end-to-end\narchitectures."}
{"id": "2509.00964", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00964", "abs": "https://arxiv.org/abs/2509.00964", "authors": ["Kuranage Roche Rayan Ranasinghe", "Zhaolin Wang", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "Emil Björnson"], "title": "Doubly-Dispersive Continuous MIMO Systems: Channel Modeling and Beamforming Design", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "We address the modeling and optimal beamforming (BF) design for\nmultiple-input multiple-output (MIMO) continuous aperture array (CAPA) systems\noperating over doubly-dispersive (DD) channels. First, a comprehensive DD\ncontinuous MIMO (DDC MIMO) channel model that incorporates CAPAs at both the\ntransmitter (TX) and receiver (RX) is derived, which is used to obtain explicit\ninput-output (I/O) relations for various waveforms well suited to integrated\nsensing and communications (ISAC) and robust to DD channels, namely orthogonal\nfrequency division multiplexing (OFDM), orthogonal time frequency space (OTFS),\nand affine frequency division multiplexing (AFDM). Then, functional\noptimization problems are formulated for the design of TX and RX BF matrices\nthat maximize received power, in which novel low-complexity, closed-form\nsolutions are obtained via the calculus of variations (CoV) method, yielding\nexpressions closely related to the classical matched filter commonly used in\nconventional MIMO systems. Simulation results confirm that the proposed TX/RX\nBF designs with CAPAs provide significant performance and computational\ncomplexity gains over conventional MIMO systems in DD channels."}
{"id": "2509.01399", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01399", "abs": "https://arxiv.org/abs/2509.01399", "authors": ["Runduo Han", "Yanxin Hu", "Yihui Fu", "Zihan Zhang", "Yukai Jv", "Li Chen", "Lei Xie"], "title": "CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays", "comment": "Accepted by Interspeech 2025", "summary": "Separating overlapping speech from multiple speakers is crucial for effective\nhuman-vehicle interaction. This paper proposes CabinSep, a lightweight neural\nmask-based minimum variance distortionless response (MVDR) speech separation\napproach, to reduce speech recognition errors in back-end automatic speech\nrecognition (ASR) models. Our contributions are threefold: First, we utilize\nchannel information to extract spatial features, which improves the estimation\nof speech and noise masks. Second, we employ MVDR during inference, reducing\nspeech distortion to make it more ASR-friendly. Third, we introduce a data\naugmentation method combining simulated and real-recorded impulse responses\n(IRs), improving speaker localization at zone boundaries and further reducing\nspeech recognition errors. With a computational complexity of only 0.4 GMACs,\nCabinSep achieves a 17.5% relative reduction in speech recognition error rate\nin a real-recorded dataset compared to the state-of-the-art DualSep model.\nDemos are available at: https://cabinsep.github.io/cabinsep/."}
{"id": "2509.00230", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00230", "abs": "https://arxiv.org/abs/2509.00230", "authors": ["Linus Stuhlmann", "Michael Alexander Saxer"], "title": "Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks", "comment": null, "summary": "This study evaluates the performance of three advanced speech encoder models,\nWav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By\nfine-tuning these models and analyzing their layer-wise representations using\nSVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0\nand XLS-R capture speaker-specific features effectively in their early layers,\nwith fine-tuning improving stability and performance. Whisper showed better\nperformance in deeper layers. Additionally, we determined the optimal number of\ntransformer layers for each model when fine-tuned for speaker identification\ntasks."}
{"id": "2509.00968", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.00968", "abs": "https://arxiv.org/abs/2509.00968", "authors": ["Vinith Kishore", "Valentin Debarnot", "AmirEhsan Khorashadizadeh", "Ivan Dokmanić"], "title": "Localized Supervised Learning for Cryo-ET Reconstruction", "comment": "Presented in ISCS25", "summary": "Cryo-electron tomography (Cryo-ET) is a powerful tool in structural biology\nfor 3D visualization of cells and biological systems at resolutions sufficient\nto identify individual proteins in situ. The measurements are collected by\ntilting the frozen specimen and exposing it to an electron beam of known\ndosage. As the biological samples are prone to electron damage, the samples can\nbe exposed to only a limited dosage of electrons, leading to noisy and\nincomplete measurements. Thus, the reconstructions are noisy and incomplete,\nleading to the missing wedge problem. Currently, self-supervised learning is\nused to compensate for this issue. This typically involves, for each volume to\nrecover, training a large 3D UNet on the initial noisy reconstruction, leading\nto large training time and memory requirements. In this work, we exploit the\nlocal nature of the forward model to train a lightweight network using only\nlocalized data from the measurements. This design provides flexibility in\nbalancing computational and time requirements while reconstructing the volumes\nwith high accuracy. We observe experimentally that this network can work well\non unseen datasets, despite using a network trained on a few measurements."}
{"id": "2509.01401", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01401", "abs": "https://arxiv.org/abs/2509.01401", "authors": ["Ali Abouzeid", "Bilal Elbouardi", "Mohamed Maged", "Shady Shehata"], "title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition", "comment": "Accepted (The Third Arabic Natural Language Processing Conference)", "summary": "Speech emotion recognition is vital for human-computer interaction,\nparticularly for low-resource languages like Arabic, which face challenges due\nto limited data and research. We introduce ArabEmoNet, a lightweight\narchitecture designed to overcome these limitations and deliver\nstate-of-the-art performance. Unlike previous systems relying on discrete MFCC\nfeatures and 1D convolutions, which miss nuanced spectro-temporal patterns,\nArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving\ncritical emotional cues often lost in traditional methods.\n  While recent models favor large-scale architectures with millions of\nparameters, ArabEmoNet achieves superior results with just 1 million\nparameters, 90 times smaller than HuBERT base and 74 times smaller than\nWhisper. This efficiency makes it ideal for resource-constrained environments.\nArabEmoNet advances Arabic speech emotion recognition, offering exceptional\nperformance and accessibility for real-world applications."}
{"id": "2509.00318", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00318", "abs": "https://arxiv.org/abs/2509.00318", "authors": ["Tianyu Song", "Ton Viet Ta"], "title": "Towards High-Fidelity and Controllable Bioacoustic Generation via Enhanced Diffusion Learning", "comment": null, "summary": "Generative modeling offers new opportunities for bioacoustics, enabling the\nsynthesis of realistic animal vocalizations that could support biomonitoring\nefforts and supplement scarce data for endangered species. However, directly\ngenerating bird call waveforms from noisy field recordings remains a major\nchallenge.\n  We propose BirdDiff, a generative framework designed to synthesize bird calls\nfrom a noisy dataset of 12 wild bird species. The model incorporates a \"zeroth\nlayer\" stage for multi-scale adaptive bird-call enhancement, followed by a\ndiffusion-based generator conditioned on three modalities: Mel-frequency\ncepstral coefficients, species labels, and textual descriptions. The\nenhancement stage improves signal-to-noise ratio (SNR) while minimizing\nspectral distortion, achieving the highest SNR gain (+10.45 dB) and lowest\nItakura-Saito Distance (0.54) compared to three widely used non-training\nenhancement methods.\n  We evaluate BirdDiff against a baseline generative model, DiffWave. Our\nmethod yields substantial improvements in generative quality metrics: Fr\\'echet\nAudio Distance (0.590 to 0.213), Jensen-Shannon Divergence (0.259 to 0.226),\nand Number of Statistically-Different Bins (7.33 to 5.58). To assess\nspecies-specific detail preservation, we use a ResNet50 classifier trained on\nthe original dataset to identify generated samples. Classification accuracy\nimproves from 35.9% (DiffWave) to 70.1% (BirdDiff), with 8 of 12 species\nexceeding 70% accuracy.\n  These results demonstrate that BirdDiff enables high-fidelity, controllable\nbird call generation directly from noisy field recordings."}
{"id": "2509.01070", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01070", "abs": "https://arxiv.org/abs/2509.01070", "authors": ["Erqi Huang", "John Restrepo", "Xun Cao", "Ivo Ihrke"], "title": "BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot Multispectral Light-field Imaging", "comment": "Presented in ISCS25", "summary": "Snapshot Multispectral Light-field Imaging (SMLI) is an emerging\ncomputational imaging technique that captures high-dimensional data (x, y, z,\n$\\theta$, $\\phi$, $\\lambda$) in a single shot using a low-dimensional sensor.\nThe accuracy of high-dimensional data reconstruction depends on representing\nthe spectrum using neural radiance field models, which requires consideration\nof broadband spectral decoupling during optimization. Currently, some SMLI\napproaches avoid the challenge of model decoupling by either reducing\nlight-throughput or prolonging imaging time. In this work, we propose a\nbroadband spectral neural radiance field (BSNeRF) for SMLI systems. Experiments\nshow that our model successfully decouples a broadband multiplexed spectrum.\nConsequently, this approach enhances multispectral light-field image\nreconstruction and further advances plenoptic imaging."}
{"id": "2509.01588", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01588", "abs": "https://arxiv.org/abs/2509.01588", "authors": ["Andrea Poltronieri", "Xavier Serra", "Martín Rocamora"], "title": "From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Audio Chord Estimation (ACE) holds a pivotal role in music information\nresearch, having garnered attention for over two decades due to its relevance\nfor music transcription and analysis. Despite notable advancements, challenges\npersist in the task, particularly concerning unique characteristics of harmonic\ncontent, which have resulted in existing systems' performances reaching a glass\nceiling. These challenges include annotator subjectivity, where varying\ninterpretations among annotators lead to inconsistencies, and class imbalance\nwithin chord datasets, where certain chord classes are over-represented\ncompared to others, posing difficulties in model training and evaluation. As a\nfirst contribution, this paper presents an evaluation of inter-annotator\nagreement in chord annotations, using metrics that extend beyond traditional\nbinary measures. In addition, we propose a consonance-informed distance metric\nthat reflects the perceptual similarity between harmonic annotations. Our\nanalysis suggests that consonance-based distance metrics more effectively\ncapture musically meaningful agreement between annotations. Expanding on these\nfindings, we introduce a novel ACE conformer-based model that integrates\nconsonance concepts into the model through consonance-based label smoothing.\nThe proposed model also addresses class imbalance by separately estimating\nroot, bass, and all note activations, enabling the reconstruction of chord\nlabels from decomposed outputs."}
{"id": "2509.00405", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00405", "abs": "https://arxiv.org/abs/2509.00405", "authors": ["Xihao Yuan", "Siqi Liu", "Yan Chen", "Hang Zhou", "Chang Liu", "Hanting Chen", "Jie Hu"], "title": "SaD: A Scenario-Aware Discriminator for Speech Enhancement", "comment": "5 pages, 2 figures.Accepted by InterSpeech2025", "summary": "Generative adversarial network-based models have shown remarkable performance\nin the field of speech enhancement. However, the current optimization\nstrategies for these models predominantly focus on refining the architecture of\nthe generator or enhancing the quality evaluation metrics of the discriminator.\nThis approach often overlooks the rich contextual information inherent in\ndiverse scenarios. In this paper, we propose a scenario-aware discriminator\nthat captures scene-specific features and performs frequency-domain division,\nthereby enabling a more accurate quality assessment of the enhanced speech\ngenerated by the generator. We conducted comprehensive experiments on three\nrepresentative models using two publicly available datasets. The results\ndemonstrate that our method can effectively adapt to various generator\narchitectures without altering their structure, thereby unlocking further\nperformance gains in speech enhancement across different scenarios."}
{"id": "2509.01117", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01117", "abs": "https://arxiv.org/abs/2509.01117", "authors": ["Gyoseung Lee", "Junil Choi"], "title": "A Bayesian Framework For Cascaded Channel Estimation in RIS-Aided mmWave Systems", "comment": "Accepted to IEEE Wireless Communications Letters", "summary": "In this paper, we investigate cascaded channel estimation for reconfigurable\nintelligent surface (RIS)-aided millimeter-wave multi-user communication\nsystems. Since the complex channel gains of the cascaded RIS channel are\ngenerally non-Gaussian, the use of the linear minimum mean squared error\n(LMMSE) estimator leads to inevitable performance degradation. To tackle this\nissue, we propose a variational inference-based framework that approximates the\ncomplex channel gains using a complex adaptive Laplace prior, which effectively\ncaptures their probability distributions in a tractable way. Numerical results\ndemonstrate that the proposed estimator outperforms conventional estimators\nincluding least squares and LMMSE in terms of cascaded channel estimation\nerror."}
{"id": "2509.01762", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01762", "abs": "https://arxiv.org/abs/2509.01762", "authors": ["Alokit Mishra", "Ryyan Akhtar"], "title": "Music Genre Classification Using Machine Learning Techniques", "comment": "10 pages, 20 figures. Submitted in partial fulfillment of the\n  requirements for the Bachelor of Technology (B.Tech) degree in Artificial\n  Intelligence and Data Science", "summary": "This paper presents a comparative analysis of machine learning methodologies\nfor automatic music genre classification. We evaluate the performance of\nclassical classifiers, including Support Vector Machines (SVM) and ensemble\nmethods, trained on a comprehensive set of hand-crafted audio features, against\na Convolutional Neural Network (CNN) operating on Mel spectrograms. The study\nis conducted on the widely-used GTZAN dataset. Our findings demonstrate a\nnoteworthy result: the SVM, leveraging domain-specific feature engineering,\nachieves superior classification accuracy compared to the end-to-end CNN model.\nWe attribute this outcome to the data-constrained nature of the benchmark\ndataset, where the strong inductive bias of engineered features provides a\nregularization effect that mitigates the risk of overfitting inherent in\nhigh-capacity deep learning models. This work underscores the enduring\nrelevance of traditional feature extraction in practical audio processing tasks\nand provides a critical perspective on the universal applicability of deep\nlearning, especially for moderately sized datasets."}
{"id": "2509.00654", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00654", "abs": "https://arxiv.org/abs/2509.00654", "authors": ["Ashwin Nagarajan", "Hao-Wen Dong"], "title": "The Name-Free Gap: Policy-Aware Stylistic Control in Music Generation", "comment": "10 pages, 2 figures", "summary": "Text-to-music models capture broad attributes such as instrumentation or\nmood, but fine-grained stylistic control remains an open challenge. Existing\nstylization methods typically require retraining or specialized conditioning,\nwhich complicates reproducibility and limits policy compliance when artist\nnames are restricted. We study whether lightweight, human-readable modifiers\nsampled from a large language model can provide a policy-robust alternative for\nstylistic control. Using MusicGen-small, we evaluate two artists: Billie Eilish\n(vocal pop) and Ludovico Einaudi (instrumental piano). For each artist, we use\nfifteen reference excerpts and evaluate matched seeds under three conditions:\nbaseline prompts, artist-name prompts, and five descriptor sets. All prompts\nare generated using a large language model. Evaluation uses both VGGish and\nCLAP embeddings with distributional and per-clip similarity measures, including\na new min-distance attribution metric. Results show that artist names are the\nstrongest control signal across both artists, while name-free descriptors\nrecover much of this effect. This highlights that existing safeguards such as\nthe restriction of artist names in music generation prompts may not fully\nprevent style imitation. Cross-artist transfers reduce alignment, showing that\ndescriptors encode targeted stylistic cues. We also present a descriptor table\nacross ten contemporary artists to illustrate the breadth of the tokens.\nTogether these findings define the name-free gap, the controllability\ndifference between artist-name prompts and policy-compliant descriptors, shown\nthrough a reproducible evaluation protocol for prompt-level controllability."}
{"id": "2509.01121", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01121", "abs": "https://arxiv.org/abs/2509.01121", "authors": ["Yali Zhang", "Haifan Yin", "Weidong Li", "Emil Bjornson", "Merouane Debbah"], "title": "Fluid Antenna Port Prediction based on Large Language Models", "comment": "6 pages, 4 figures, 1 table, To appear in IEEE Globecom 2025 SAC -\n  MLCN", "summary": "This study seeks to utilize large language models (LLMs) to forecast the\nmoving ports of fluid antenna (FA). By repositioning the antenna to the\nlocations identified by our proposed model, we intend to address the mobility\nchallenges faced by user equipment (UE). To the best of our knowledge, this\npaper introduces, for the first time, the application of LLMs in the prediction\nof FA ports, presenting a novel model termed Port-LLM. The architecture of our\nmodel is based on the pre-trained GPT-2 framework. We designed specialized data\npreprocessing, input embedding, and output projection modules to effectively\nbridge the disparities between the wireless communication data and the data\nformat utilized by the pre-trained LLM. Simulation results demonstrate that our\nmodel exhibits superior predictive performance under different numbers of base\nstation (BS) antennas and varying UE speeds, indicating strong generalization\nand robustness ability. Furthermore, the spectral efficiency (SE) attained by\nour model surpasses that achieved by traditional methods in both medium and\nhigh-speed mobile environments."}
{"id": "2509.02020", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02020", "abs": "https://arxiv.org/abs/2509.02020", "authors": ["Kun Xie", "Feiyu Shen", "Junjie Li", "Fenglong Xie", "Xu Tang", "Yao Hu"], "title": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot", "comment": null, "summary": "Current dialogue generation approaches typically require the complete\ndialogue text before synthesis and produce a single, inseparable speech\ncontaining all voices, making them unsuitable for interactive chat; moreover,\nthey suffer from unstable synthesis, inaccurate speaker transitions, and\nincoherent prosody. In this work, we present FireRedTTS-2, a long-form\nstreaming TTS system for multi-speaker dialogue generation, delivering stable,\nnatural speech with reliable speaker switching and context-aware prosody. A new\n12.5Hz streaming speech tokenizer accelerates training and inference, extends\nmaximum dialogue length, encodes richer semantics to stabilize text-to-token\nmodeling and supports high-fidelity streaming generation for real-time\napplications. We adopt a text-speech interleaved format, concatenating\nspeaker-labeled text with aligned speech tokens in chronological order, and\nmodel it with a dual-transformer: a large decoder-only transformer predicts\ntokens at the first layer, and a smaller one completes subsequent layers.\nExperimental results show that FireRedTTS-2 integrates seamlessly with chat\nframeworks and, with minimal fine-tuning, produces emotionally expressive\nspeech guided by implicit contextual cues. In podcast generation, it surpasses\nexisting systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in\nobjective intelligibility, speaker-turn reliability, and perceived naturalness\nwith context-consistent prosody. Our demos are available at\nhttps://fireredteam.github.io/demos/firered_tts_2."}
{"id": "2509.00683", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.00683", "abs": "https://arxiv.org/abs/2509.00683", "authors": ["Zihao Zheng", "Zeyu Xie", "Xuenan Xu", "Wen Wu", "Chao Zhang", "Mengyue Wu"], "title": "PicoAudio2: Temporal Controllable Text-to-Audio Generation with Natural Language Description", "comment": "Demo page: https://HiRookie9.github.io/PicoAudio2-Page", "summary": "Controllable text-to-audio generation (TTA) has attracted much attention\nrecently. Although existing works can achieve fine-grained controllability\nbased on timestamp information, sound event categories are limited to a fixed\nset. Moreover, since only simulated data is used for training, the generated\naudio quality and generalization performance on real data are limited. To\ntackle this issue, we propose PicoAudio2, improving temporal-controllable TTA\nvia a new data processing pipeline and model architecture. Specifically, we use\na grounding model to annotate event timestamps of real audio-text datasets to\ncurate temporally-strong real data, in addition to simulation data from\nexisting works. The model is trained on the combination of real and simulation\ndata. Moreover, following PicoAudio, we encode timestamp information into a\ntimestamp matrix to provide extra fine-grained time-aligned information to the\nmodel, on top of the coarse-grained textual description. Experiments show that\nPicoAudio2 exhibits superior performance in terms of temporal controllability\nand audio quality."}
{"id": "2509.01125", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01125", "abs": "https://arxiv.org/abs/2509.01125", "authors": ["Yuan Gao", "Zichen Lu", "Yifan Wu", "Yanliang Jin", "Shunqing Zhang", "Xiaoli Chu", "Shugong Xu", "Cheng-Xiang Wang"], "title": "Enabling 6G Through Multi-Domain Channel Extrapolation: Opportunities and Challenges of Generative Artificial Intelligence", "comment": null, "summary": "Channel extrapolation has attracted wide attention due to its potential to\nacquire channel state information (CSI) with high accuracy and minimal\noverhead. This is becoming increasingly crucial as the sixth-generation (6G)\nmobile networks aim to support complex scenarios, for example, high-mobility\ncommunications utilizing ultra-massive multiple-input multiple-output (MIMO)\ntechnologies and broad spectrum bands, necessitating multi-domain channel\nextrapolation. Current research predominantly addresses channel extrapolation\nwithin a single domain, lacking a comprehensive approach to multi-domain\nchannel extrapolation. To bridge the gap, we propose the concept of\nmulti-domain channel extrapolation, detailing the essential performance\nrequirements for 6G networks. These include precise channel extrapolation,\nadaptability to varying scenarios, and manageable computational complexity\nduring both training and inference stages. In light of these requirements, we\nelaborate the potential and challenges of incorporating generative artificial\nintelligence (GAI)-based models for effective multi-domain channel\nextrapolation. Given the ability of the Transformer to capture long-range\ndependencies and hidden patterns, we propose a novel Transformer encoder-like\nmodel by eliminating the positional encoding module and replacing the original\nmulti-head attention with a multilayer perceptron (MLP) for multi-domain\nchannel extrapolation. Simulation results indicate that this model surpasses\nexisting baseline models in terms of extrapolation accuracy and inference\nspeed. Ablation studies further demonstrate the effectiveness of the module\ndesign of the proposed design. Finally, we pose several open questions for the\ndevelopment of practical GAI-based multi-domain channel extrapolation models,\nincluding the issues of explainability, generalization, and dataset collection."}
{"id": "2509.02167", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.02167", "abs": "https://arxiv.org/abs/2509.02167", "authors": ["Jiayu Xiong", "Jun Xue", "Jianlong Kwan", "Jing Wang"], "title": "AudioRWKV: Efficient and Stable Bidirectional RWKV for Audio Pattern Recognition", "comment": "6 pages, 3 figures", "summary": "Recently, Transformers (e.g., Audio Spectrogram Transformers, AST) and\nstate-space models (e.g., Audio Mamba, AuM) have achieved remarkable progress\nin audio modeling. However, the O(L^2) computational complexity of the\nTransformer architecture hinders efficient long-sequence processing, while the\nMamba architecture tends to become unstable when scaling parameters and data.\nTo address these challenges, this paper proposes AudioRWKV (A-RWKV), a highly\nefficient and stable architecture for audio modeling. Specifically, we inherit\nthe stable and efficient recurrent formulation of RWKV7 and replace its 1D\ntoken-shift operation with a 2D depthwise separable convolution to better\ncapture local spectro-temporal patterns. Furthermore, we adapt the original\ncausal WKV kernel into a bidirectional WKV kernel (Bi-WKV), enabling global\ncontext modeling over the entire audio sequence while maintaining linear\ncomputational complexity. Benefiting from the inherent stability of the RWKV7\nfoundation, A-RWKV scales seamlessly to larger model sizes. Experimental\nresults demonstrate that, under the same linear-model regime, A-RWKV-S (22M)\nachieves performance parity with AuM-B (92M) while exhibiting more stable\nthroughput than AST; for long-form audio (~5 minutes 28 seconds), WKV7 achieves\nup to a 13.3X speedup in processing."}
{"id": "2509.00813", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00813", "abs": "https://arxiv.org/abs/2509.00813", "authors": ["Gyehun Go", "Satbyul Han", "Ahyeon Choi", "Eunjin Choi", "Juhan Nam", "Jeong Mi Park"], "title": "AImoclips: A Benchmark for Evaluating Emotion Conveyance in Text-to-Music Generation", "comment": "to be published in HCMIR25: 3rd Workshop on Human-Centric Music\n  Information Research", "summary": "Recent advances in text-to-music (TTM) generation have enabled controllable\nand expressive music creation using natural language prompts. However, the\nemotional fidelity of TTM systems remains largely underexplored compared to\nhuman preference or text alignment. In this study, we introduce AImoclips, a\nbenchmark for evaluating how well TTM systems convey intended emotions to human\nlisteners, covering both open-source and commercial models. We selected 12\nemotion intents spanning four quadrants of the valence-arousal space, and used\nsix state-of-the-art TTM systems to generate over 1,000 music clips. A total of\n111 participants rated the perceived valence and arousal of each clip on a\n9-point Likert scale. Our results show that commercial systems tend to produce\nmusic perceived as more pleasant than intended, while open-source systems tend\nto perform the opposite. Emotions are more accurately conveyed under\nhigh-arousal conditions across all models. Additionally, all systems exhibit a\nbias toward emotional neutrality, highlighting a key limitation in affective\ncontrollability. This benchmark offers valuable insights into model-specific\nemotion rendering characteristics and supports future development of\nemotionally aligned TTM systems."}
{"id": "2509.01127", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01127", "abs": "https://arxiv.org/abs/2509.01127", "authors": ["Asli Alpman", "Mustafa Utkur", "Emine Ulku Saritas"], "title": "A Model-Based Dictionary Approach for Magnetic Nanoparticle Signal Prediction", "comment": null, "summary": "Magnetic particle imaging (MPI) is a tracer-based medical imaging modality\nthat enables quantification and spatial mapping of magnetic nanoparticle (MNP)\ndistribution. The magnetization response of MNPs depends on experimental\nconditions such as drive field (DF) settings and medium viscosity, as well as\non magnetic parameters of MNPs such as magnetic core diameter, hydrodynamic\ndiameter, and magnetic anisotropy constant. A comprehensive understanding of\nthe magnetization response of MNPs can facilitate the optimization of DF and\nMNP type for a given MPI application, without the need for extensive\nexperimentation. In this work, we propose a calibration-free iterative\nalgorithm using model-based dictionaries for MNP signal prediction at untested\nsettings. Dictionaries were constructed with the MNP signals simulated using\nthe coupled Brown-N\\'eel rotation model. Based on the available measurements,\nthe proposed algorithm jointly estimates the dictionary weights and the\ntransfer functions due to non-model-based dynamics. These dynamics include the\nsystem response of the measurement setup as well as magnetization dynamics not\naccounted for by the employed coupled Brown-N\\'eel rotation model. The\nalgorithm was first validated on synthetic signals at SNR levels of 1 and 10,\nand then tested on an in-house MPS setup across six viscosity levels\n(0.89-15.33 mPa.s) and DF frequencies of 0.25-2 kHz using two commercial MNPs.\nValidation on synthetic signals showed accurate weight and transfer function\nestimation even at SNR 1. MPS experiments demonstrated successful prediction of\nMNP signals at untested viscosities, with NRMSE below 1.51% and 3.5% for the\ntwo tested MNPs across all DF settings. Predicted signals captured viscosity\ndependent trends, and NWD values remained low (<0.10 and <0.07 for the two\ntested MNPs), confirming robust weight estimation."}
{"id": "2509.02244", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02244", "abs": "https://arxiv.org/abs/2509.02244", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding", "comment": null, "summary": "We present a neural speech codec that challenges the need for complex\nresidual vector quantization (RVQ) stacks by introducing a simpler,\nsingle-stage quantization approach. Our method operates directly on the\nmel-spectrogram, treating it as a 2D data and quantizing non-overlapping 4x4\npatches into a single, shared codebook. This patchwise design simplifies the\narchitecture, enables low-latency streaming, and yields a discrete latent grid.\nTo ensure high-fidelity synthesis, we employ a late-stage adversarial\nfine-tuning for the VQ-VAE and train a HiFi-GAN vocoder from scratch on the\ncodec's reconstructed spectrograms. Operating at approximately 7.5 kbits/s for\n16 kHz speech, our system was evaluated against several state-of-the-art neural\ncodecs using objective metrics such as STOI, PESQ, MCD, and ViSQOL. The results\ndemonstrate that our simplified, non-residual architecture achieves competitive\nperceptual quality and intelligibility, validating it as an effective and open\nfoundation for future low-latency codec designs."}
{"id": "2509.00839", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00839", "abs": "https://arxiv.org/abs/2509.00839", "authors": ["Yuli Zhang", "Pengfei Fan", "Ruiyuan Jiang", "Hankang Gu", "Dongyao Jia", "Xinheng Wang"], "title": "Adaptive Vehicle Speed Classification via BMCNN with Reinforcement Learning-Enhanced Acoustic Processing", "comment": null, "summary": "Traffic congestion remains a pressing urban challenge, requiring intelligent\ntransportation systems for real-time management. We present a hybrid framework\nthat combines deep learning and reinforcement learning for acoustic vehicle\nspeed classification. A dual-branch BMCNN processes MFCC and wavelet features\nto capture complementary frequency patterns. An attention-enhanced DQN\nadaptively selects the minimal number of audio frames and triggers early\ndecisions once confidence thresholds are reached. Evaluations on IDMT-Traffic\nand our SZUR-Acoustic (Suzhou) datasets show 95.99% and 92.3% accuracy, with up\nto 1.63x faster average processing via early termination. Compared with A3C,\nDDDQN, SA2C, PPO, and TD3, the method provides a superior accuracy-efficiency\ntrade-off and is suitable for real-time ITS deployment in heterogeneous urban\nenvironments."}
{"id": "2509.01163", "categories": ["eess.SP", "53-04", "I.6.3"], "pdf": "https://arxiv.org/pdf/2509.01163", "abs": "https://arxiv.org/abs/2509.01163", "authors": ["Duc Viet Nguyen", "Haiquan Zhao", "Jinhui Hu"], "title": "Dynamic State Estimation of Power System Utilizing Cauchy Kernel-Based Maximum Mixture Correntropy UKF over Beluga Whale-Bat Optimization", "comment": "11 pages, 10 figures", "summary": "Non-Gaussian noise, outliers, sudden load changes, and bad measurement data\nare key factors that diminish the accuracy of dynamic state estimation in power\nsystems. Additionally, unscented Kalman filters (UKF) based on correntropy\ncriteria utilize bandwidth-sensitive Gaussian kernels, which may lead to\nsingular matrices in the Cholesky decomposition. To overcome all the above\nproblems, in this paper, a robust UKF based on Cauchy kernel maximum mixture\ncorrentropy (CKMMC) criteria over hybrid Beluga Whale-Bat (BWB) optimization\n(BWB-CKMMC-UKF) is proposed, in which the kernel is merged of two Cauchy\nfunctions. Specifically, the measurement error and state error are unified in\nthe cost function by the statistical linearization technique, and the optimal\nvalue of state estimation is obtained by fixed-point iteration. Because of its\ninsensitive feature to kernel bandwidth and notable thick-tailed feature, the\nCauchy kernel function is utilized instead of the Gaussian kernel in the\noptimization criteria. Additionally, to fit the power system model, the shape\ncoefficients of the kernel in the CKMMC criterion and scale coefficients that\ninfluence the selection of sigma points in the unscented transform are\ndetermined based on the BWB algorithm. Simulation results on IEEE 14, 30, and\n57-bus test systems validated the performance of the proposed algorithm."}
{"id": "2509.02259", "categories": ["cs.SD", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.02259", "abs": "https://arxiv.org/abs/2509.02259", "authors": ["Guillem Bonafos", "Jéremy Rouch", "Lény Lego", "David Reby", "Hugues Patural", "Nicolas Mathevon", "Rémy Emonet"], "title": "Speech transformer models for extracting information from baby cries", "comment": "Accepted to WOCCI2025 (interspeech2025 workshop)", "summary": "Transfer learning using latent representations from pre-trained speech models\nachieves outstanding performance in tasks where labeled data is scarce.\nHowever, their applicability to non-speech data and the specific acoustic\nproperties encoded in these representations remain largely unexplored. In this\nstudy, we investigate both aspects. We evaluate five pre-trained speech models\non eight baby cries datasets, encompassing 115 hours of audio from 960 babies.\nFor each dataset, we assess the latent representations of each model across all\navailable classification tasks. Our results demonstrate that the latent\nrepresentations of these models can effectively classify human baby cries and\nencode key information related to vocal source instability and identity of the\ncrying baby. In addition, a comparison of the architectures and training\nstrategies of these models offers valuable insights for the design of future\nmodels tailored to similar tasks, such as emotion detection."}
{"id": "2509.00862", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00862", "abs": "https://arxiv.org/abs/2509.00862", "authors": ["Yuriy Izotov", "Andrei Velichko"], "title": "Speech Command Recognition Using LogNNet Reservoir Computing for Embedded Systems", "comment": "20 pages, 6 figures", "summary": "This paper presents a low-resource speech-command recognizer combining\nenergy-based voice activity detection (VAD), an optimized Mel-Frequency\nCepstral Coefficients (MFCC) pipeline, and the LogNNet reservoir-computing\nclassifier. Using four commands from the Speech Commands da-taset downsampled\nto 8 kHz, we evaluate four MFCC aggregation schemes and find that adaptive\nbinning (64-dimensional feature vector) offers the best accuracy-to-compactness\ntrade-off. The LogNNet classifier with architecture 64:33:9:4 reaches 92.04%\naccuracy under speaker-independent evaluation, while requiring significantly\nfewer parameters than conventional deep learn-ing models. Hardware\nimplementation on Arduino Nano 33 IoT (ARM Cor-tex-M0+, 48 MHz, 32 KB RAM)\nvalidates the practical feasibility, achieving ~90% real-time recognition\naccuracy while consuming only 18 KB RAM (55% utilization). The complete\npipeline (VAD -> MFCC -> LogNNet) thus enables reliable on-device\nspeech-command recognition under strict memory and compute limits, making it\nsuitable for battery-powered IoT nodes, wire-less sensor networks, and\nhands-free control interfaces."}
{"id": "2509.01180", "categories": ["eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.01180", "abs": "https://arxiv.org/abs/2509.01180", "authors": ["Fabian Kruse", "Vinith Kishore", "Valentin Debarnot", "Ivan Dokmanić"], "title": "Beyond Exhaustive Sampling: Efficient Rotational Matching via Ball Harmonics", "comment": "Presented in ISCS25", "summary": "Cryo-ET allows to generate tomograms of biological samples in situ, capturing\ncomplex structures in their native context. Despite low signal-to-noise ratio\nin reconstructed volumes, the large number of copies of the same macromolecules\nmakes it possible to retrieve high-resolution maps by averaging many aligned\nsubtomograms. To keep up with technical advances in the imaging process and the\nresulting huge amounts of data available, there is a need for scalable, fast\nand robust procedures to align subtomograms. We propose a subtomogram alignment\nframework based on the ball harmonics expansion that combines frequency- and\ngradient-based optimization strategies to avoid exhaustive rotation sampling,\nenabling a speed-up of an order of magnitude compared to current approaches."}
{"id": "2509.02349", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02349", "abs": "https://arxiv.org/abs/2509.02349", "authors": ["Lu Wang", "Hao Chen", "Siyu Wu", "Zhiyue Wu", "Hao Zhou", "Chengfeng Zhang", "Ting Wang", "Haodi Zhang"], "title": "AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have been widely applied in speech\nand music. This tendency has led to a focus on audio tokenization for Large\nModels (LMs). Unlike semantic-only text tokens, audio tokens must both capture\nglobal semantic content and preserve fine-grained acoustic details. Moreover,\nthey provide a discrete method for speech and music that can be effectively\nintegrated into MLLMs. However, existing research is unsuitable in the\ndefinitions of semantic tokens and acoustic tokens. In addition, the evaluation\nof different codecs typically concentrates on specific domains or tasks, such\nas reconstruction or Automatic Speech Recognition (ASR) task, which prevents\nfair and comprehensive comparisons. To address these problems, this paper\nprovides suitable definitions for semantic and acoustic tokens and introduces a\nsystematic evaluation framework. This framework allows for a comprehensive\nassessment of codecs' capabilities which evaluate across four dimensions: audio\nreconstruction metric, codebook index (ID) stability, decoder-only transformer\nperplexity, and performance on downstream probe tasks. Our results show the\ncorrectness of the provided suitable definitions and the correlation among\nreconstruction metrics, codebook ID stability, downstream probe tasks and\nperplexity."}
{"id": "2509.00914", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00914", "abs": "https://arxiv.org/abs/2509.00914", "authors": ["Hainan Wang", "Mehdi Hosseinzadeh", "Reza Rawassizadeh"], "title": "TinyMusician: On-Device Music Generation with Knowledge Distillation and Mixed Precision Quantization", "comment": "12 pages for main context, 5 figures", "summary": "The success of the generative model has gained unprecedented attention in the\nmusic generation area. Transformer-based architectures have set new benchmarks\nfor model performance. However, their practical adoption is hindered by some\ncritical challenges: the demand for massive computational resources and\ninference time, due to their large number of parameters. These obstacles make\nthem infeasible to deploy on edge devices, such as smartphones and wearables,\nwith limited computational resources. In this work, we present TinyMusician, a\nlightweight music generation model distilled from MusicGen (a State-of-the-art\nmusic generation model). TinyMusician integrates two innovations: (i)\nStage-mixed Bidirectional and Skewed KL-Divergence and (ii) Adaptive\nMixed-Precision Quantization. The experimental results demonstrate that\nTinyMusician retains 93% of the MusicGen-Small performance with 55% less model\nsize. TinyMusician is the first mobile-deployable music generation model that\neliminates cloud dependency while maintaining high audio fidelity and efficient\nresource usage"}
{"id": "2509.01197", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01197", "abs": "https://arxiv.org/abs/2509.01197", "authors": ["Shugong Xu", "Jun Jiang", "Wenjun Yu", "Yilin Gao", "Guangjin Pan", "Shiyi Mu", "Zhiqi Ai", "Yuan Gao", "Peigang Jiang", "Cheng-Xiang Wang"], "title": "Enhanced Fingerprint-based Positioning With Practical Imperfections: Deep learning-based approaches", "comment": "accepted by IEEE Wireless Communications Magazine", "summary": "High-precision positioning is vital for cellular networks to support\ninnovative applications such as extended reality, unmanned aerial vehicles\n(UAVs), and industrial Internet of Things (IoT) systems. Existing positioning\nalgorithms using deep learning techniques require vast amounts of labeled data,\nwhich are difficult to obtain in real-world cellular environments, and these\nmodels often struggle to generalize effectively. To advance cellular\npositioning techniques, the 2024 Wireless Communication Algorithm Elite\nCompetition as conducted, which provided a dataset from a three-sector outdoor\ncellular system, incorporating practical challenges such as limited\nlabeled-dataset, dynamic wireless environments within the target and\nunevenly-spaced anchors, Our team developed three innovative positioning\nframeworks that swept the top three awards of this competition, namely the\nsemi-supervised framework with consistency, ensemble learning-based algorithm\nand decoupled mapping heads-based algorithm. Specifically, the semi-supervised\nframework with consistency effectively generates high-quality pseudo-labels,\nenlarging the labeled-dataset for model training. The ensemble learning-based\nalgorithm amalgamates the positioning coordinates from models trained under\ndifferent strategies, effectively combating the dynamic positioning\nenvironments. The decoupled mapping heads-based algorithm utilized sector\nrotation scheme to resolve the uneven-spaced anchor issue. Simulation results\ndemonstrate the superior performance of our proposed positioning algorithms\ncompared to existing benchmarks in terms of the {90%, 80%, 67%, 50%} percentile\nand mean distance error."}
{"id": "2509.02398", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02398", "abs": "https://arxiv.org/abs/2509.02398", "authors": ["Hui Wang", "Cheng Liu", "Junyang Chen", "Haoze Liu", "Yuhang Jia", "Shiwan Zhao", "Jiaming Zhou", "Haoqin Sun", "Hui Bu", "Yong Qin"], "title": "TTA-Bench: A Comprehensive Benchmark for Evaluating Text-to-Audio Models", "comment": null, "summary": "Text-to-Audio (TTA) generation has made rapid progress, but current\nevaluation methods remain narrow, focusing mainly on perceptual quality while\noverlooking robustness, generalization, and ethical concerns. We present\nTTA-Bench, a comprehensive benchmark for evaluating TTA models across\nfunctional performance, reliability, and social responsibility. It covers seven\ndimensions including accuracy, robustness, fairness, and toxicity, and includes\n2,999 diverse prompts generated through automated and manual methods. We\nintroduce a unified evaluation protocol that combines objective metrics with\nover 118,000 human annotations from both experts and general users. Ten\nstate-of-the-art models are benchmarked under this framework, offering detailed\ninsights into their strengths and limitations. TTA-Bench establishes a new\nstandard for holistic and responsible evaluation of TTA systems. The dataset\nand evaluation tools are open-sourced at https://nku-hlt.github.io/tta-bench/."}
{"id": "2509.00988", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.00988", "abs": "https://arxiv.org/abs/2509.00988", "authors": ["Swadhin Biswas", "Imran", "Tuhin Sheikh"], "title": "A Unified Denoising and Adaptation Framework for Self-Supervised Bengali Dialectal ASR", "comment": null, "summary": "Automatic Speech Recognition (ASR) for Bengali, the world's fifth most spoken\nlanguage, remains a significant challenge, critically hindering technological\naccessibility for its over 270 million speakers. This challenge is compounded\nby two persistent and intertwined factors: the language's vast dialectal\ndiversity and the prevalence of acoustic noise in real-world environments.\nWhile state-of-the-art self-supervised learning (SSL) models have advanced ASR\nfor low-resource languages, they often lack explicit mechanisms to handle\nenvironmental noise during pre-training or specialized adaptation strategies\nfor the complex phonetic and lexical variations across Bengali dialects. This\npaper introduces a novel, unified framework designed to address these dual\nchallenges simultaneously. Our approach is founded on the WavLM model, which is\nuniquely pre-trained with a masked speech denoising objective, making it\ninherently robust to acoustic distortions. We propose a specialized multi-stage\nfine-tuning strategy that first adapts the model to general-domain standard\nBengali to establish a strong linguistic foundation and subsequently\nspecializes it for noise-robust dialectal recognition through targeted data\naugmentation. The framework is rigorously evaluated on a comprehensive\nbenchmark comprising multiple Bengali dialects under a wide range of simulated\nnoisy conditions, from clean audio to low Signal-to-Noise Ratio (SNR) levels.\n  Experimental results demonstrate that the proposed framework significantly\noutperforms strong baselines, including standard fine-tuned wav2vec 2.0 and the\nlarge-scale multilingual Whisper model. This work establishes a new\nstate-of-the-art for this task and provides a scalable, effective blueprint for\ndeveloping practical ASR systems for other low-resource, high-variation\nlanguages globally."}
{"id": "2509.01208", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01208", "abs": "https://arxiv.org/abs/2509.01208", "authors": ["Niclas Führling", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "David González G.", "Gonzalo Seco-Granados", "Osvaldo Gonsa"], "title": "Rigid Body Localization and Tracking for 6G V2X: Algorithms, Applications, and Road to Adoption", "comment": null, "summary": "Vehicle-to-everything (V2X) perception refers to a suite of technologies that\nempower vehicles to sense their environment and communicate with other\nentities, including surrounding vehicles, infrastructure, and cloud/edge\nnetworks. With the growing demands of autonomous driving, V2X perception has\ngained significant attention, particularly through the emergence of integrated\nsensing and communication (ISAC) frameworks. Within this landscape, rigid body\nlocalization (RBL) has emerged as a promising paradigm, enabling the estimation\nof not only the position and velocity of the targets, but also its\nthree-dimensional (3D) geometric structure and orientation. This article\nintroduces the concept of RBL, highlights its unique advantages and\napplications, identifies key technical challenges, and finally outlines future\nresearch directions. In addition, the potential of RBL in next-generation -\ne.g. beyond fifth generation (B5G) and sixth-generation (6G) - wireless systems\napplied to V2X perception is also discussed, with a focus on its role in\nstandardization efforts and its relevance across automotive and industrial\ndomains."}
{"id": "2509.02471", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02471", "abs": "https://arxiv.org/abs/2509.02471", "authors": ["Chengyuan Ma", "Peng Jia", "Hongyue Guo", "Wenming Yang"], "title": "ESTM: An Enhanced Dual-Branch Spectral-Temporal Mamba for Anomalous Sound Detection", "comment": "Accepted in IEEE Signal Processing Letters 2025", "summary": "The core challenge in industrial equipment anoma lous sound detection (ASD)\nlies in modeling the time-frequency coupling characteristics of acoustic\nfeatures. Existing modeling methods are limited by local receptive fields,\nmaking it difficult to capture long-range temporal patterns and cross-band\ndynamic coupling effects in machine acoustic features. In this paper, we\npropose a novel framework, ESTM, which is based on a dual-path Mamba\narchitecture with time-frequency decoupled modeling and utilizes Selective\nState-Space Models (SSM) for long-range sequence modeling. ESTM extracts rich\nfeature representations from different time segments and frequency bands by\nfusing enhanced Mel spectrograms and raw audio features, while further\nimproving sensitivity to anomalous patterns through the TriStat-Gating (TSG)\nmodule. Our experiments demonstrate that ESTM improves anomalous detection\nperformance on the DCASE 2020 Task 2 dataset, further validating the\neffectiveness of the proposed method."}
{"id": "2509.01153", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01153", "abs": "https://arxiv.org/abs/2509.01153", "authors": ["Yun Chu", "Qiuhao Wang", "Enze Zhou", "Qian Liu", "Gang Zheng"], "title": "EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection", "comment": null, "summary": "Auscultation is a key method for early diagnosis of respiratory and pulmonary\ndiseases, relying on skilled healthcare professionals. However, the process is\noften subjective, with variability between experts. As a result, numerous deep\nlearning-based automatic classification methods have emerged, most of which\nfocus on respiratory sound classification. In contrast, research on respiratory\nsound event detection remains limited. Existing sound event detection methods\ntypically rely on frame-level predictions followed by post-processing to\ngenerate event-level outputs, making interval boundaries challenging to learn\ndirectly. Furthermore, many approaches can only handle fixed-length audio, lim-\niting their applicability to variable-length respiratory sounds. Additionally,\nthe impact of respiratory sound location information on detection performance\nhas not been extensively explored. To address these issues, we propose a graph\nneural network-based framework with anchor intervals, capable of handling\nvariable-length audio and providing more precise temporal localization for\nabnormal respi- ratory sound events. Our method improves both the flexibility\nand applicability of respiratory sound detection. Experiments on the SPRSound\n2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposed\napproach, and incorporating respiratory position information enhances the\ndiscrimination between abnormal sounds."}
{"id": "2509.01210", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01210", "abs": "https://arxiv.org/abs/2509.01210", "authors": ["Rens Baeyens", "Dennis Laurijssen", "Jan Steckel", "Walter Daems"], "title": "High-Density MIMO Localization Using a 32x64 Ultrasonic Transducer-Microphone Array with Real-Time Data Streaming", "comment": "Accepted for publication at IEEE IUS 2025", "summary": "In this work, we present a novel ultrasonic array system designed for\nhigh-precision localization using a large-scale MIMO (Multiple-Input\nMultiple-Output) architecture. The system combines 32 transmitters with 62\nmicrophones, creating an extended virtual aperture that improves channel\nseparability and spatial resolution. Each transmitter is excited by a\nrandom-phase multisine within the ultrasonic band, which reduces inter-channel\ncorrelation and increases robustness against multipath. The feasibility of the\napproach is demonstrated through simulations of reflector imaging and analysis\nof channel separation under realistic transducer bandwidth constraints. Results\nshow that MIMO processing enables improved separation of reflectors compared to\nsingle-emitter configurations, although practical limitations such as\ntransducer bandwidth reduce the achievable channel isolation."}
{"id": "2509.02521", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.02521", "abs": "https://arxiv.org/abs/2509.02521", "authors": ["Yiqun Yao", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Naitong Yu", "Wenjia Ma", "Aixin Sun", "Yequan Wang"], "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training", "comment": null, "summary": "Full-duplex dialog models are designed to listen and speak simultaneously\nwith rapid responses to fast-changing user input. Among existing approaches,\nnative full-duplex models merges different channels (e.g. listen and speak) in\na single time step, overcoming the high response latency inherent to\ntime-division multiplexing time-division multiplexing (TDM) alternatives. Yet,\na key challenge remains: aligning textual monologues with audio streams that\noperate at different bitrates. The prevailing solution relies on word-level\nalignment, but this can degrade the language ability of large pre-trained\nmodels. Moreover, it requires highly accurate timestamps for every token, which\nintroduces cascading errors and increases pre-processing costs. In this paper,\nwe propose textual monologues in continuous tokens sequence, namely \"natural\"\nmonologues, which mimics humanoid cognitive behavior in dialogs. For temporal\nalignment, we alternate the position of the natural monologue - leading or\ntrailing the audio - across different training stages. This \"dual\" training\nparadigm proves highly effective in building FLM-Audio, our 7B spoken dialog\nmodel that demonstrates superior responsiveness, duplexity, and chatting\nexperiences, as confirmed by experimental results."}
{"id": "2509.01210", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01210", "abs": "https://arxiv.org/abs/2509.01210", "authors": ["Rens Baeyens", "Dennis Laurijssen", "Jan Steckel", "Walter Daems"], "title": "High-Density MIMO Localization Using a 32x64 Ultrasonic Transducer-Microphone Array with Real-Time Data Streaming", "comment": "Accepted for publication at IEEE IUS 2025", "summary": "In this work, we present a novel ultrasonic array system designed for\nhigh-precision localization using a large-scale MIMO (Multiple-Input\nMultiple-Output) architecture. The system combines 32 transmitters with 62\nmicrophones, creating an extended virtual aperture that improves channel\nseparability and spatial resolution. Each transmitter is excited by a\nrandom-phase multisine within the ultrasonic band, which reduces inter-channel\ncorrelation and increases robustness against multipath. The feasibility of the\napproach is demonstrated through simulations of reflector imaging and analysis\nof channel separation under realistic transducer bandwidth constraints. Results\nshow that MIMO processing enables improved separation of reflectors compared to\nsingle-emitter configurations, although practical limitations such as\ntransducer bandwidth reduce the achievable channel isolation."}
{"id": "2509.01212", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.01212", "abs": "https://arxiv.org/abs/2509.01212", "authors": ["Rens Baeyens", "Dennis Laurijssen", "Jan Steckel", "Walter Daems"], "title": "nRTIS: Low-Cost Real-Time 3D Sonar Imaging Circular Array Supporting Beamforming for Industrial Applications", "comment": "Accepted for publication at IEEE IUS 2025", "summary": "Conventional ultrasonic inspection systems rely on phased arrays and\nhigh-performance computing hardware, making them costly, bulky, and unsuitable\nfor portable or embedded use. In this work, we present nRTIS (nano Real-Time 3D\nImaging Sonar), a compact ultrasonic sensing platform built around a circular\narray of MEMS microphones and a central ultrasonic transducer. The device\nachieves real-time acquisition through an RP2350 microcontroller and high-speed\nUSB transfer. We validate the system using both simulations and controlled\nexperiments: point spread function (PSF) simulations demonstrate beamforming\nresolution and sidelobe suppression, while reflector measurements confirm\nrobust data acquisition. These results highlight the potential of nRTIS for\nscalable industrial applications such as weld inspection, pipe mapping, and\nrobotic navigation."}
{"id": "2509.00077", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00077", "abs": "https://arxiv.org/abs/2509.00077", "authors": ["Tai Vu"], "title": "Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition", "comment": null, "summary": "Speech Emotion Recognition (SER) presents a significant yet persistent\nchallenge in human-computer interaction. While deep learning has advanced\nspoken language processing, achieving high performance on limited datasets\nremains a critical hurdle. This paper confronts this issue by developing and\nevaluating a suite of machine learning models, including Support Vector\nMachines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional\nNeural Networks (CNNs), for automated emotion classification in human speech.\nWe demonstrate that by strategically employing transfer learning and innovative\ndata augmentation techniques, our models can achieve impressive performance\ndespite the constraints of a relatively small dataset. Our most effective\nmodel, a ResNet34 architecture, establishes a new performance benchmark on the\ncombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1\nscore of 0.631. These results underscore the substantial benefits of leveraging\npre-trained models and data augmentation to overcome data scarcity, thereby\npaving the way for more robust and generalizable SER systems."}
{"id": "2509.01336", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01336", "abs": "https://arxiv.org/abs/2509.01336", "authors": ["Wen-Chin Huang", "Hui Wang", "Cheng Liu", "Yi-Chiao Wu", "Andros Tjandra", "Wei-Ning Hsu", "Erica Cooper", "Yong Qin", "Tomoki Toda"], "title": "The AudioMOS Challenge 2025", "comment": "IEEE ASRU 2025", "summary": "This is the summary paper for the AudioMOS Challenge 2025, the very first\nchallenge for automatic subjective quality prediction for synthetic audio. The\nchallenge consists of three tracks. The first track aims to assess\ntext-to-music samples in terms of overall quality and textual alignment. The\nsecond track is based on the four evaluation dimensions of Meta Audiobox\nAesthetics, and the test set consists of text-to-speech, text-to-audio, and\ntext-to-music samples. The third track focuses on synthetic speech quality\nassessment in different sampling rates. The challenge attracted 24 unique teams\nfrom both academia and industry, and improvements over the baselines were\nconfirmed. The outcome of this challenge is expected to facilitate development\nand progress in the field of automatic evaluation for audio generation systems."}
{"id": "2509.01222", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01222", "abs": "https://arxiv.org/abs/2509.01222", "authors": ["Tong Lin", "Jianyue Zhu", "Wei Huang", "Meng Hua", "Zhizhong Zhang"], "title": "Rate Optimization for Downlink URLLC via Pinching Antenna Arrays", "comment": null, "summary": "This work studies an ultra-reliable and low-latency communications (uRLLC)\ndownlink system using pinching antennas which are realized by activating small\ndielectric particles along a dielectric waveguide. Our goal is to maximize the\ndata rate by optimizing the positions of the pinching antennas. By proposing a\ncompact and cost-efficient antenna architecture and formulating a finite\nblocklength-based optimization model, we derive a closed-form solution for the\noptimal antenna placement under quality-of-service (QoS) and antenna spacing\nconstraints. Meanwhile, a phase-alignment strategy is integrated into the\ndesign, enabling coherent signal superposition across the array. Simulation\nresults confirm significant rate improvements over conventional antenna systems\nwhile satisfying uRLLC requirements, making the proposed design well-suited for\ncompact and latency-critical future applications."}
{"id": "2509.00078", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00078", "abs": "https://arxiv.org/abs/2509.00078", "authors": ["Tatiana Likhomanenko", "Luke Carlson", "Richard He Bai", "Zijin Gu", "Han Tran", "Zakaria Aldeneh", "Yizhe Zhang", "Ruixiang Zhang", "Huangjie Zheng", "Navdeep Jaitly"], "title": "ChipChat: Low-Latency Cascaded Conversational Agent in MLX", "comment": "ASRU 2025", "summary": "The emergence of large language models (LLMs) has transformed spoken dialog\nsystems, yet the optimal architecture for real-time on-device voice agents\nremains an open question. While end-to-end approaches promise theoretical\nadvantages, cascaded systems (CSs) continue to outperform them in language\nunderstanding tasks, despite being constrained by sequential processing\nlatency. In this work, we introduce ChipChat, a novel low-latency CS that\novercomes traditional bottlenecks through architectural innovations and\nstreaming optimizations. Our system integrates streaming (a) conversational\nspeech recognition with mixture-of-experts, (b) state-action augmented LLM, (c)\ntext-to-speech synthesis, (d) neural vocoder, and (e) speaker modeling.\nImplemented using MLX, ChipChat achieves sub-second response latency on a Mac\nStudio without dedicated GPUs, while preserving user privacy through complete\non-device processing. Our work shows that strategically redesigned CSs can\novercome their historical latency limitations, offering a promising path\nforward for practical voice-based AI agents."}
{"id": "2509.01399", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01399", "abs": "https://arxiv.org/abs/2509.01399", "authors": ["Runduo Han", "Yanxin Hu", "Yihui Fu", "Zihan Zhang", "Yukai Jv", "Li Chen", "Lei Xie"], "title": "CabinSep: IR-Augmented Mask-Based MVDR for Real-Time In-Car Speech Separation with Distributed Heterogeneous Arrays", "comment": "Accepted by Interspeech 2025", "summary": "Separating overlapping speech from multiple speakers is crucial for effective\nhuman-vehicle interaction. This paper proposes CabinSep, a lightweight neural\nmask-based minimum variance distortionless response (MVDR) speech separation\napproach, to reduce speech recognition errors in back-end automatic speech\nrecognition (ASR) models. Our contributions are threefold: First, we utilize\nchannel information to extract spatial features, which improves the estimation\nof speech and noise masks. Second, we employ MVDR during inference, reducing\nspeech distortion to make it more ASR-friendly. Third, we introduce a data\naugmentation method combining simulated and real-recorded impulse responses\n(IRs), improving speaker localization at zone boundaries and further reducing\nspeech recognition errors. With a computational complexity of only 0.4 GMACs,\nCabinSep achieves a 17.5% relative reduction in speech recognition error rate\nin a real-recorded dataset compared to the state-of-the-art DualSep model.\nDemos are available at: https://cabinsep.github.io/cabinsep/."}
{"id": "2509.01223", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01223", "abs": "https://arxiv.org/abs/2509.01223", "authors": ["Niclas Führling", "Giuseppe Abreu", "David González G.", "Osvaldo Gonsa"], "title": "SMDS-based Rigid Body Localization", "comment": null, "summary": "We consider a novel rigid body localization (RBL) method, based only on a set\nof measurements of the distances, as well as the angles between sensors of the\nvehicle to the anchor landmark points. A key point of the proposed method is to\nuse a variation of the super multidimensional scaling (SMDS) algorithm, where\nonly a minor part of the complex edge kernel is used, based on the available\ninformation, which in the case of RBL is anchor-to-anchor and target-to-target\ninformation. Simulation results illustrate the good performance of the proposed\ntechnique in terms of mean square error (MSE) of the estimates, compared also\nto the corresponding Cram\\'er-Rao Lower Bound (CRLB)."}
{"id": "2509.00094", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00094", "abs": "https://arxiv.org/abs/2509.00094", "authors": ["Abdullah Abdelfattah", "Mahmoud I. Khalil", "Hazem Abbas"], "title": "Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning", "comment": null, "summary": "Assessing spoken language is challenging, and quantifying pronunciation\nmetrics for machine learning models is even harder. However, for the Holy\nQuran, this task is simplified by the rigorous recitation rules (tajweed)\nestablished by Muslim scholars, enabling highly effective assessment. Despite\nthis advantage, the scarcity of high-quality annotated data remains a\nsignificant barrier.\n  In this work, we bridge these gaps by introducing: (1) A 98% automated\npipeline to produce high-quality Quranic datasets -- encompassing: Collection\nof recitations from expert reciters, Segmentation at pause points (waqf) using\nour fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript\nverification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K\nannotated utterances); (3) A novel ASR-based approach for pronunciation error\ndetection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed\nrules (unlike the IPA standard for Modern Standard Arabic). QPS uses a\ntwo-level script: (Phoneme level): Encodes Arabic letters with short/long\nvowels. (Sifa level): Encodes articulation characteristics of every phoneme. We\nfurther include comprehensive modeling with our novel multi-level CTC Model\nwhich achieved 0.16% average Phoneme Error Rate (PER) on the testset. We\nrelease all code, data, and models as open-source:\nhttps://obadx.github.io/prepare-quran-dataset/"}
{"id": "2509.01401", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01401", "abs": "https://arxiv.org/abs/2509.01401", "authors": ["Ali Abouzeid", "Bilal Elbouardi", "Mohamed Maged", "Shady Shehata"], "title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition", "comment": "Accepted (The Third Arabic Natural Language Processing Conference)", "summary": "Speech emotion recognition is vital for human-computer interaction,\nparticularly for low-resource languages like Arabic, which face challenges due\nto limited data and research. We introduce ArabEmoNet, a lightweight\narchitecture designed to overcome these limitations and deliver\nstate-of-the-art performance. Unlike previous systems relying on discrete MFCC\nfeatures and 1D convolutions, which miss nuanced spectro-temporal patterns,\nArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving\ncritical emotional cues often lost in traditional methods.\n  While recent models favor large-scale architectures with millions of\nparameters, ArabEmoNet achieves superior results with just 1 million\nparameters, 90 times smaller than HuBERT base and 74 times smaller than\nWhisper. This efficiency makes it ideal for resource-constrained environments.\nArabEmoNet advances Arabic speech emotion recognition, offering exceptional\nperformance and accessibility for real-world applications."}
{"id": "2509.01331", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01331", "abs": "https://arxiv.org/abs/2509.01331", "authors": ["Koshi Nagahisa", "Ryo Hayakawa", "Youji Iiguni"], "title": "Comparison between Supervised and Unsupervised Learning in Deep Unfolded Sparse Signal Recovery", "comment": "This work will be submitted to the IEEE for possible publication", "summary": "This paper investigates the impact of loss function selection in deep\nunfolding techniques for sparse signal recovery algorithms. Deep unfolding\ntransforms iterative optimization algorithms into trainable lightweight neural\nnetworks by unfolding their iterations as network layers, with various loss\nfunctions employed for parameter learning depending on application contexts. We\nfocus on deep unfolded versions of the fundamental iterative shrinkage\nthresholding algorithm (ISTA) and the iterative hard thresholding algorithm\n(IHT), comparing supervised learning using mean squared error with unsupervised\nlearning using the objective function of the original optimization problem. Our\nsimulation results reveal that the effect of the choice of loss function\nsignificantly depends on the convexity of the optimization problem. For convex\n$\\ell_1$-regularized problems, supervised-ISTA achieves better final recovery\naccuracy but fails to minimize the original objective function, whereas we\nempirically observe that unsupervised-ISTA converges to a nearly identical\nsolution as conventional ISTA but with accelerated convergence. Conversely, for\nnonconvex $\\ell_0$-regularized problems, both supervised-IHT and\nunsupervised-IHT converge to better local minima than the original IHT, showing\nsimilar performance regardless of the loss function employed. These findings\nprovide valuable insights into the design of effective deep unfolded networks\nfor sparse signal recovery applications."}
{"id": "2509.00106", "categories": ["eess.AS", "cs.SD", "H.5.5; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.00106", "abs": "https://arxiv.org/abs/2509.00106", "authors": ["Rohan Agarwal"], "title": "Quantum-Enhanced Analysis and Grading of Vocal Performance", "comment": "4 pages, 5 figures. Hybrid quantum - classical feasibility study;\n  simulator - only results", "summary": "We present QuantumMelody, a hybrid quantum-classical method for objective\nsinging assessment. Grouped vocal features (pitch stability, dynamics, timbre)\nare encoded into a small simulated quantum circuit; all nine qubits are\ninitialized with a Hadamard on each qubit and then receive Rx, Ry, and Rz\nrotations, with intra- and cross-group entanglement. The circuit measurement\nprobabilities are fused with spectrogram transformer embeddings to estimate a\ngrade on labels 2-5 and to surface technique-level feedback. On 168 labeled 20\nsecond excerpts, the hybrid reaches 74.29% agreement with expert graders, a\n+12.86 point gain over a classical-features baseline. Processing is sub-minute\nper recording on a laptop-class Qiskit simulator; we do not claim hardware\nspeedups. This is a feasibility step toward interpretable, objective singing\nassessment in applied audio signal processing."}
{"id": "2509.01588", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.01588", "abs": "https://arxiv.org/abs/2509.01588", "authors": ["Andrea Poltronieri", "Xavier Serra", "Martín Rocamora"], "title": "From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Audio Chord Estimation (ACE) holds a pivotal role in music information\nresearch, having garnered attention for over two decades due to its relevance\nfor music transcription and analysis. Despite notable advancements, challenges\npersist in the task, particularly concerning unique characteristics of harmonic\ncontent, which have resulted in existing systems' performances reaching a glass\nceiling. These challenges include annotator subjectivity, where varying\ninterpretations among annotators lead to inconsistencies, and class imbalance\nwithin chord datasets, where certain chord classes are over-represented\ncompared to others, posing difficulties in model training and evaluation. As a\nfirst contribution, this paper presents an evaluation of inter-annotator\nagreement in chord annotations, using metrics that extend beyond traditional\nbinary measures. In addition, we propose a consonance-informed distance metric\nthat reflects the perceptual similarity between harmonic annotations. Our\nanalysis suggests that consonance-based distance metrics more effectively\ncapture musically meaningful agreement between annotations. Expanding on these\nfindings, we introduce a novel ACE conformer-based model that integrates\nconsonance concepts into the model through consonance-based label smoothing.\nThe proposed model also addresses class imbalance by separately estimating\nroot, bass, and all note activations, enabling the reconstruction of chord\nlabels from decomposed outputs."}
{"id": "2509.01410", "categories": ["eess.SP", "math.ST", "stat.TH", "94A12, 41A45, 94A20"], "pdf": "https://arxiv.org/pdf/2509.01410", "abs": "https://arxiv.org/abs/2509.01410", "authors": ["Debraj Banerjee", "Amitava Chatterjee"], "title": "A James-Stein Estimator based Generalized OMP Algorithm for Robust Signal Recovery using Sparse Representation", "comment": "5 pages, 3 figures, conference paper", "summary": "In this paper, we introduce a novel algorithm named JS-gOMP, which enhances\nthe generalized Orthogonal Matching Pursuit (gOMP) algorithm for improved noise\nrobustness in sparse signal processing. The JS-gOMP algorithm uniquely\nincorporates the James-Stein estimator, optimizing the trade-off between signal\nrecovery and noise suppression. This modification addresses the challenges\nposed by noise in the dictionary, a common issue in sparse representation\nscenarios. Comparative analyses demonstrate that JS-gOMP outperforms\ntraditional gOMP, especially in noisy environments, offering a more effective\nsolution for signal and image processing applications where noise presence is\nsignificant."}
{"id": "2509.00400", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00400", "abs": "https://arxiv.org/abs/2509.00400", "authors": ["Xikun Lu", "Yunda Chen", "Zehua Chen", "Jie Wang", "Mingxing Liu", "Hongmei Hu", "Chengshi Zheng", "Stefan Bleeck", "Jinqiu Sang"], "title": "Deep Learning for Personalized Binaural Audio Reproduction", "comment": null, "summary": "Personalized binaural audio reproduction is the basis of realistic spatial\nlocalization, sound externalization, and immersive listening, directly shaping\nuser experience and listening effort. This survey reviews recent advances in\ndeep learning for this task and organizes them by generation mechanism into two\nparadigms: explicit personalized filtering and end-to-end rendering. Explicit\nmethods predict personalized head-related transfer functions (HRTFs) from\nsparse measurements, morphological features, or environmental cues, and then\nuse them in the conventional rendering pipeline. End-to-end methods map source\nsignals directly to binaural signals, aided by other inputs such as visual,\ntextual, or parametric guidance, and they learn personalization within the\nmodel. We also summarize the field's main datasets and evaluation metrics to\nsupport fair and repeatable comparison. Finally, we conclude with a discussion\nof key applications enabled by these technologies, current technical\nlimitations, and potential research directions for deep learning-based spatial\naudio systems."}
{"id": "2509.02020", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02020", "abs": "https://arxiv.org/abs/2509.02020", "authors": ["Kun Xie", "Feiyu Shen", "Junjie Li", "Fenglong Xie", "Xu Tang", "Yao Hu"], "title": "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot", "comment": null, "summary": "Current dialogue generation approaches typically require the complete\ndialogue text before synthesis and produce a single, inseparable speech\ncontaining all voices, making them unsuitable for interactive chat; moreover,\nthey suffer from unstable synthesis, inaccurate speaker transitions, and\nincoherent prosody. In this work, we present FireRedTTS-2, a long-form\nstreaming TTS system for multi-speaker dialogue generation, delivering stable,\nnatural speech with reliable speaker switching and context-aware prosody. A new\n12.5Hz streaming speech tokenizer accelerates training and inference, extends\nmaximum dialogue length, encodes richer semantics to stabilize text-to-token\nmodeling and supports high-fidelity streaming generation for real-time\napplications. We adopt a text-speech interleaved format, concatenating\nspeaker-labeled text with aligned speech tokens in chronological order, and\nmodel it with a dual-transformer: a large decoder-only transformer predicts\ntokens at the first layer, and a smaller one completes subsequent layers.\nExperimental results show that FireRedTTS-2 integrates seamlessly with chat\nframeworks and, with minimal fine-tuning, produces emotionally expressive\nspeech guided by implicit contextual cues. In podcast generation, it surpasses\nexisting systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in\nobjective intelligibility, speaker-turn reliability, and perceived naturalness\nwith context-consistent prosody. Our demos are available at\nhttps://fireredteam.github.io/demos/firered_tts_2."}
{"id": "2509.01506", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01506", "abs": "https://arxiv.org/abs/2509.01506", "authors": ["Marcel Grec", "Federico Clazzer", "Israel Leyva-Mayorga", "Andrea Munari", "Gianluigi Liva", "Petar Popovski"], "title": "To Share, or Not to Share: A Study on GEO-LEO Systems for IoT Services with Random Access", "comment": "6 pages, 7 figures; accepted to be presented at the 2025 IEEE Global\n  Communications Conference", "summary": "The increasing number of satellite deployments, both in the low and\ngeostationary Earth orbit exacerbates the already ongoing scarcity of wireless\nresources when targeting ubiquitous connectivity. For the aim of supporting a\nmassive number of IoT devices characterized by bursty traffic and modern\nvariants of random access, we pose the following question: Should competing\nsatellite operators share spectrum resources or is an exclusive allocation\npreferable? This question is addressed by devising a communication model for\ntwo operators which serve overlapping coverage areas with independent IoT\nservices. Analytical approximations, validated by Monte Carlo simulations,\nreveal that spectrum sharing can yield significant throughput gains for both\noperators under certain conditions tied to the relative serviced user\npopulations and coding rates in use. These gains are sensitive also to the\nsystem parameters and may not always render the spectral coexistence mutually\nadvantageous. Our model captures basic trade-offs in uplink spectrum sharing\nand provides novel actionable insights for the design and regulation of future\n6G non-terrestrial networks."}
{"id": "2509.00685", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.00685", "abs": "https://arxiv.org/abs/2509.00685", "authors": ["Kangxiang Xia", "Xinfa Zhu", "Jixun Yao", "Lei Xie"], "title": "MPO: Multidimensional Preference Optimization for Language Model-based Text-to-Speech", "comment": "Accepted by NCMMSC2025", "summary": "In recent years, text-to-speech (TTS) has seen impressive advancements\nthrough large-scale language models, achieving human-level speech quality.\nIntegrating human feedback has proven effective for enhancing robustness in\nthese systems. However, current approaches face challenges in optimizing TTS\nwith preference data across multiple dimensions and often suffer from\nperformance degradation due to overconfidence in rewards. We propose\nMultidimensional Preference Optimization (MPO) to better align TTS systems with\nhuman preferences. MPO introduces a preference set that streamlines the\nconstruction of data for multidimensional preference optimization, enabling\nalignment with multiple dimensions. Additionally, we incorporate regularization\nduring training to address the typical degradation issues in DPO-based\napproaches. Our experiments demonstrate MPO's effectiveness, showing\nsignificant improvements in intelligibility, speaker similarity, and prosody\ncompared to baseline systems."}
{"id": "2509.02244", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02244", "abs": "https://arxiv.org/abs/2509.02244", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding", "comment": null, "summary": "We present a neural speech codec that challenges the need for complex\nresidual vector quantization (RVQ) stacks by introducing a simpler,\nsingle-stage quantization approach. Our method operates directly on the\nmel-spectrogram, treating it as a 2D data and quantizing non-overlapping 4x4\npatches into a single, shared codebook. This patchwise design simplifies the\narchitecture, enables low-latency streaming, and yields a discrete latent grid.\nTo ensure high-fidelity synthesis, we employ a late-stage adversarial\nfine-tuning for the VQ-VAE and train a HiFi-GAN vocoder from scratch on the\ncodec's reconstructed spectrograms. Operating at approximately 7.5 kbits/s for\n16 kHz speech, our system was evaluated against several state-of-the-art neural\ncodecs using objective metrics such as STOI, PESQ, MCD, and ViSQOL. The results\ndemonstrate that our simplified, non-residual architecture achieves competitive\nperceptual quality and intelligibility, validating it as an effective and open\nfoundation for future low-latency codec designs."}
{"id": "2509.01641", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.01641", "abs": "https://arxiv.org/abs/2509.01641", "authors": ["Yuzhi Yang", "Omar Alhussein", "Mérouane Debbah"], "title": "Non-Identical Diffusion Models in MIMO-OFDM Channel Generation", "comment": null, "summary": "We propose a novel diffusion model, termed the non-identical diffusion model,\nand investigate its application to wireless orthogonal frequency division\nmultiplexing (OFDM) channel generation. Unlike the standard diffusion model\nthat uses a scalar-valued time index to represent the global noise level, we\nextend this notion to an element-wise time indicator to capture local error\nvariations more accurately. Non-identical diffusion enables us to characterize\nthe reliability of each element (e.g., subcarriers in OFDM) within the noisy\ninput, leading to improved generation results when the initialization is\nbiased. Specifically, we focus on the recovery of wireless multi-input\nmulti-output (MIMO) OFDM channel matrices, where the initial channel estimates\nexhibit highly uneven reliability across elements due to the pilot scheme.\nConventional time embeddings, which assume uniform noise progression, fail to\ncapture such variability across pilot schemes and noise levels. We introduce a\nmatrix that matches the input size to control element-wise noise progression.\nFollowing a similar diffusion procedure to existing methods, we show the\ncorrectness and effectiveness of the proposed non-identical diffusion scheme\nboth theoretically and numerically. For MIMO-OFDM channel generation, we\npropose a dimension-wise time embedding strategy. We also develop and evaluate\nmultiple training and generation methods and compare them through numerical\nexperiments."}
{"id": "2509.01087", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.01087", "abs": "https://arxiv.org/abs/2509.01087", "authors": ["Shuangyuan Chen", "Shuang Wei", "Dongxing Xu", "Yanhua Long"], "title": "Noisy Disentanglement with Tri-stage Training for Noise-Robust Speech Recognition", "comment": "11 pages,4 figures", "summary": "To enhance the performance of end-to-end (E2E) speech recognition systems in\nnoisy or low signal-to-noise ratio (SNR) conditions, this paper introduces\nNoisyD-CT, a novel tri-stage training framework built on the\nConformer-Transducer architecture. The core of NoisyD-CT is a especially\ndesigned compact noisy disentanglement (NoisyD) module (adding only 1.71M\nparameters), integrated between the Conformer blocks and Transducer Decoder to\nperform deep noise suppression and improve ASR robustness in challenging\nacoustic noise environments. To fully exploit the noise suppression capability\nof the NoisyD-CT, we further propose a clean representation consistency loss to\nalign high-level representations derived from noisy speech with those obtained\nfrom corresponding clean speech. Together with a noisy reconstruction loss,\nthis consistency alignment enables the NoisyD module to effectively suppress\nnoise while preserving essential acoustic and linguistic features consistent\nacross both clean and noisy conditions, thereby producing cleaner internal\nrepresentations that enhance ASR performance. Moreover, our tri-stage training\nstrategy is designed to fully leverage the functionalities of both the noisy\ndisentanglement and speech recognition modules throughout the model training\nprocess, ultimately maximizing performance gains under noisy conditions. Our\nexperiments are performed on the LibriSpeech and CHiME-4 datasets, extensive\nresults demonstrate that our proposed NoisyD-CT significantly outperforms the\ncompetitive Conformer-Transducer baseline, achieving up to 25.7% and 10.6%\nrelative word error rate reductions on simulated and real-world noisy test\nsets, respectively, while maintaining or even improving performance on clean\nspeech test sets. The source code, model checkpoint and data simulation scripts\nwill be available at https://github.com/litchimo/NoisyD-CT."}
{"id": "2509.02398", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02398", "abs": "https://arxiv.org/abs/2509.02398", "authors": ["Hui Wang", "Cheng Liu", "Junyang Chen", "Haoze Liu", "Yuhang Jia", "Shiwan Zhao", "Jiaming Zhou", "Haoqin Sun", "Hui Bu", "Yong Qin"], "title": "TTA-Bench: A Comprehensive Benchmark for Evaluating Text-to-Audio Models", "comment": null, "summary": "Text-to-Audio (TTA) generation has made rapid progress, but current\nevaluation methods remain narrow, focusing mainly on perceptual quality while\noverlooking robustness, generalization, and ethical concerns. We present\nTTA-Bench, a comprehensive benchmark for evaluating TTA models across\nfunctional performance, reliability, and social responsibility. It covers seven\ndimensions including accuracy, robustness, fairness, and toxicity, and includes\n2,999 diverse prompts generated through automated and manual methods. We\nintroduce a unified evaluation protocol that combines objective metrics with\nover 118,000 human annotations from both experts and general users. Ten\nstate-of-the-art models are benchmarked under this framework, offering detailed\ninsights into their strengths and limitations. TTA-Bench establishes a new\nstandard for holistic and responsible evaluation of TTA systems. The dataset\nand evaluation tools are open-sourced at https://nku-hlt.github.io/tta-bench/."}
{"id": "2509.01705", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01705", "abs": "https://arxiv.org/abs/2509.01705", "authors": ["Junting Chen", "Bowen Li", "Hao Sun", "Shuguang Cui", "Nikolaos Pappas"], "title": "Predictive Communications for Low-Altitude Networks", "comment": null, "summary": "The emergence of dense, mission-driven aerial networks supporting the\nlow-altitude economy presents unique communication challenges, including\nextreme channel dynamics and severe cross-tier interference. Traditional\nreactive communication paradigms are ill-suited to these environments, as they\nfail to leverage the network's inherent predictability. This paper introduces\npredictive communication, a novel paradigm transforming network management from\nreactive adaptation to proactive optimization. The approach is enabled by\nfusing predictable mission trajectories with stable, large-scale radio\nenvironment models (e.g., radio maps). Specifically, we present a hierarchical\nframework that decomposes the predictive cross-layer resource allocation\nproblem into three layers: strategic (routing), tactical (timing), and\noperational (power). This structure aligns decision-making timescales with the\naccuracy levels and ranges of available predictive information. We demonstrate\nthat this foresight-driven framework achieves an order-of-magnitude reduction\nin cross-tier interference, laying the groundwork for robust and scalable\nlow-altitude communication systems."}
{"id": "2509.01787", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.01787", "abs": "https://arxiv.org/abs/2509.01787", "authors": ["Yiwei Guo", "Bohan Li", "Hankun Wang", "Zhihan Li", "Shuai Wang", "Xie Chen", "Kai Yu"], "title": "AHAMask: Reliable Task Specification for Large Audio Language Models without Instructions", "comment": "15 pages, 7 tables, 6 figures", "summary": "Although current large audio language models (LALMs) extend text large\nlanguage models (LLMs) with generic acoustic understanding abilities, they\nusually suffer from instruction sensitivity, where different instructions of\nthe same intention can yield drastically different outcomes. In this work, we\npropose AHAMask, where we simply mask some of the attention heads in the\ndecoder-only LLM backbone of LALMs, to trigger specific acoustic task\nfunctionalities without instructions. These masks are efficiently obtained by\ntraining on an LALM, with the number of trainable parameters equal to the\nattention head count in its LLM backbone. We show by experiments that applying\nsuch selective attention head masks achieves comparable or even better\nperformance than using instructions, either on single or composite tasks.\nBesides achieving reliable acoustic task specification for LALMs, this also\nreveals that LALMs exhibit certain \"functional pathways\" in their attention\nheads."}
{"id": "2509.01802", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01802", "abs": "https://arxiv.org/abs/2509.01802", "authors": ["Anouar Boumeftah", "Gunes Karabulut Kurt"], "title": "Leveraging Orbital Dynamics with RF Signal Features for Satellite Multi-Orbit Proximity Threat Detection", "comment": null, "summary": "Proximity-based interference is a growing threat to satellite communications,\ndriven by dense multi-orbit constellations and increasingly agile adversarial\nmaneuvers. We propose a hybrid simulation framework that integrates orbital\nmaneuver modeling with RF signal degradation analysis to detect and classify\nsuspicious proximity operations. Using the open-source Maneuver Detection Data\nGeneration (MaDDG) library from MIT Lincoln Laboratory, we generate labeled\ndatasets combining impulsive maneuver profiles with radio-frequency (RF)\nimpacts across a range of behavioral intents: routine station-keeping, covert\nshadowing, and overt jamming. Our approach fuses kinematic features such as\nrange, velocity, acceleration, and Time of Closest Approach (TCA), with RF\nmetrics including Received Signal Strength Indicator (RSSI), throughput, and\nJammer-to-Signal Ratio (JSR). These features are further enhanced with temporal\nderivatives and rolling-window statistics to capture subtle or transient\ninterference patterns. A Random Forest classifier trained on this fused feature\nset achieves 94.67% accuracy and a macro F1 score of 0.9471, outperforming\nmodels using only kinematic or RF inputs. The system is particularly effective\nin detecting covert threats, such as surveillance or intermittent jamming, that\nevade RF-only methods."}
{"id": "2509.01905", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01905", "abs": "https://arxiv.org/abs/2509.01905", "authors": ["Khawaja Fahad Masood", "Kai Wu", "Zhongqin Wang", "J. Andrew Zhang", "Shu-Lin Chen", "Y. Jay Guo"], "title": "Efficient River Water Level Sensing Using Cellular CSI and Joint Space-Time Processing", "comment": "12 pages, 13 figures, submitted to an ieee journal for possible\n  publication", "summary": "Accurate and timely water level monitoring is critical for flood prevention,\nenvironmental management, and emerging smart infrastructure systems.\nTraditional water sensing methods often rely on dedicated sensors, which can be\ncostly to deploy and difficult to maintain and are vulnerable to damage during\nfloods.In this work, we propose a novel cellular signalbased sensing scheme\nthat passively estimates water level changes using downlink mobile signals from\nexisting communication infrastructure. By capturing subtle variations in\nchannel state information (CSI), the proposed method estimates the length\nchanges of the water-reflected signal path, which correspond to water level\nvariations. A space-time processing framework is developed to jointly estimate\nthe angle of arrival and Doppler shift, enabling isolation and enhancement of\nthe water-reflected path via beamforming, while effectively suppressing\nenvironmental noise. The phase evolution of the beamformed signal is then\nextracted to infer water level changes. To address clock asynchronism between\nthe transmitter and receiver inherent in bistatic systems, we introduce a\nbeamforming-based compensation technique for removing time-varying random phase\noffsets in CSI. Field experiments conducted across a river demonstrate that the\nproposed method enables accurate and reliable water level estimation, achieving\na mean accuracy ranging from 1.5 cm to 3.05 cm across different receiver\nconfigurations and deployments."}
{"id": "2509.01923", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01923", "abs": "https://arxiv.org/abs/2509.01923", "authors": ["Md. Mohibbul Haque Chowdhury", "Nafisa Anjum", "Md. Rokonuzzaman Mim"], "title": "ECG-Based Stress Prediction with Power Spectral Density Features and Classification Models", "comment": "6 pages, 4 figures, 2 tables", "summary": "Stress has emerged as a critical global health issue, contributing to\ncardiovascular disorders, depression, and several other long-term illnesses.\nConsequently, accurate and reliable stress monitoring systems are of growing\nimportance. In this work, we propose a stress prediction framework based on\nelectrocardiogram (ECG) signals recorded during multiple daily activities such\nas sitting, walking, and jogging. Frequency-domain indicators of autonomic\nnervous system activity were obtained through Power Spectral Density (PSD)\nanalysis and utilized as input for machine learning models including Decision\nTree, Random Forest, XGBoost, LightGBM, and CatBoost. In addition, deep\nlearning approaches, namely Convolutional Neural Networks (CNN) and Long\nShort-Term Memory (LSTM) networks, were directly applied to the raw ECG\nsignals. Our experiments highlight the effectiveness of ensemble-based\nclassifiers, with CatBoost achieving 90% accuracy. Moreover, the LSTM model\nprovided superior results, attaining 94% accuracy with balanced precision,\nrecall, and F1-score, reflecting its strength in modeling temporal dependencies\nin ECG data. Overall, the findings suggest that integrating frequency-domain\nfeature extraction with advanced learning algorithms enhances stress prediction\nand paves the way for real-time healthcare monitoring solutions."}
{"id": "2509.01935", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01935", "abs": "https://arxiv.org/abs/2509.01935", "authors": ["Thai-Hoc Vu", "Anh-Tu Le", "Ngo Hoang Tu", "Tan N. Nguyen", "Miroslav Voznak"], "title": "On Performance of IoT Networks with Coordinated NOMA Transmission: Covert Monitoring and Information Decoding", "comment": null, "summary": "This work investigates the covertness and security performance of\nInternet-of-Things (IoTs) networks under Rayleigh fading environments.\nSpecifically, a cellular source transmits covert information to cell-edge users\nwith the assistance of an IoT master node, employing a coordinated direct and\nrelay transmission strategy combined with non-orthogonal multiple access\n(NOMA). This approach not only enhances spectrum utilization but also generates\nfriendly interference to complicate a warden's surveillance or an\neavesdropper's decoding efforts. From a covertness perspective, we derive exact\nclosed-form expressions for the detection error probability (DEP) under\narbitrary judgment thresholds. We then identify the optimal judgment threshold\nfor the worst-case scenario, at which the warden minimizes its DEP performance.\nAccordingly, we determine the effective region for user power allocation (PA)\nin NOMA transmission that satisfies the DEP constraint. From a security\nperspective, we derive analytical expressions for the secrecy outage\nprobability under two eavesdropping strategies using selection combining and\nmaximal ratio combining. Based on this analysis, we propose an adaptive PA\nscheme that maximizes covert rate while ensuring the quality-of-service (QoS)\nrequirements of legitimate users, the system's minimum covertness requirements,\nand supporting successive interference cancellation (SIC) procedures.\nFurthermore, we design an adaptive PA scheme that maximizes the secrecy rate\nwhile ensuring the QoS requirements of legitimate users and SIC conditions.\nNumerical results demonstrate the accuracy of the analytical framework, while\nthe proposed optimization strategies effectively adjust PA coefficients to\nmaximize either the covert rate or the secrecy rate."}
{"id": "2509.01958", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.01958", "abs": "https://arxiv.org/abs/2509.01958", "authors": ["Jongmin Park", "Junwoo Song", "Taewon Kang", "Jaewon Yu", "Pyo-Woong Son"], "title": "Correlation Analysis Between MF R-Mode Temporal ASF and Meteorological Factors", "comment": "Submitted to ICCAS 2025", "summary": "As the vulnerabilities of global navigation satellite systems (GNSS) have\nbecome more widely recognized, the need for complementary navigation systems\nhas grown. Medium frequency ranging mode (MF R-Mode) has gained attention as an\neffective backup system during GNSS outages, owing to its strong signal\nstrength and cost-effective scalability. However, to achieve accurate\npositioning, MF R-Mode requires correction for the additional secondary factor\n(ASF), a propagation delay affected by terrain. The temporal variation of ASF,\nknown as temporal ASF, is typically corrected using reference stations;\nhowever, the effectiveness of this method decreases with distance from the\nreference station. In this study, we analyzed the correlation between temporal\nASF and meteorological factors to evaluate the feasibility of predicting\ntemporal ASF based on meteorological factors. Among these factors, temperature\nand humidity showed significant correlations with temporal ASF, suggesting\ntheir potential utility in ASF correction."}
{"id": "2509.02030", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.02030", "abs": "https://arxiv.org/abs/2509.02030", "authors": ["Zehra Yigit", "Sefa Kayraklik", "Ertugrul Basar", "Ali Gorcin"], "title": "Dual Target-Mounted RISs-Assisted ISAC Against Eavesdropping and Malicious Interference", "comment": "9 pages, 8 figures", "summary": "The synergy between integrated sensing and communication (ISAC) and\nreconfigurable intelligent surfaces (RISs) unlocks novel applications and\nadvanced services for next-generation wireless networks, yet also introduces\nnew security challenges. In this study, a novel dual target-mounted\nRISs-assisted ISAC scheme is proposed, where a base station with ISAC\ncapability performs sensing of two unmanned aerial vehicle (UAV) targets, one\nof which is legitimate and the other is eavesdropper, while communicating with\nthe users through an RIS mounted on the legitimate UAV target. The proposed\nscheme addresses dual security threats posed by a hostile UAV target:\neavesdropping on legitimate user communications and random interference attacks\nlaunched by a malicious RIS mounted on this eavesdropper UAV target, aiming to\ndisrupt secure transmissions. A non-convex optimization problem maximizing the\nsecrecy rate of the users is formulated, and a semi-definite relaxation\n(SDR)-based two-stage solution is developed to optimize the transmit\nbeamforming matrix of the base station and the phase shift coefficients of the\nlegitimate RIS. Extensive computer simulations are conducted to evaluate the\nrobustness of the proposed solution under various system configurations. The\nproposed system's communication performance is assessed using the secrecy rate\nmetric, while the sensing performance is evaluated through the\nsignal-to-interference-plus-noise ratio and the Cramer-Rao bound (CRB) for\nangle-of-departure (AoD) estimation of the eavesdropper UAV target."}
{"id": "2509.02031", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02031", "abs": "https://arxiv.org/abs/2509.02031", "authors": ["Sijiang Li", "Rongqing Zhang", "Xiang Cheng", "Jian Tang"], "title": "Synesthesia of Machines (SoM)-Based Task-Driven MIMO System for Image Transmission", "comment": null, "summary": "To support cooperative perception (CP) of networked mobile agents in dynamic\nscenarios, the efficient and robust transmission of sensory data is a critical\nchallenge. Deep learning-based joint source-channel coding (JSCC) has\ndemonstrated promising results for image transmission under adverse channel\nconditions, outperforming traditional rule-based codecs. While recent works\nhave explored to combine JSCC with the widely adopted multiple-input\nmultiple-output (MIMO) technology, these approaches are still limited to the\ndiscrete-time analog transmission (DTAT) model and simple tasks. Given the\nlimited performance of existing MIMO JSCC schemes in supporting complex CP\ntasks for networked mobile agents with digital MIMO communication systems, this\npaper presents a Synesthesia of Machines (SoM)-based task-driven MIMO system\nfor image transmission, referred to as SoM-MIMO. By leveraging the structural\nproperties of the feature pyramid for perceptual tasks and the channel\nproperties of the closed-loop MIMO communication system, SoM-MIMO enables\nefficient and robust digital MIMO transmission of images. Experimental results\nhave shown that compared with two JSCC baseline schemes, our approach achieves\naverage mAP improvements of 6.30 and 10.48 across all SNR levels, while\nmaintaining identical communication overhead."}
{"id": "2509.02088", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02088", "abs": "https://arxiv.org/abs/2509.02088", "authors": ["Yejian Lyu", "Zhiqiang Yuan", "Henk Wymeersch", "Chong Han"], "title": "Environment-Aware Channel Measurement and Modeling for Terahertz Monostatic Sensing", "comment": null, "summary": "Integrated sensing and communication (ISAC) at terahertz (THz) frequencies\nholds significant promise for unifying ultra-high-speed wireless connectivity\nwith fine-grained environmental awareness. Realistic and interpretable channel\nmodeling is essential to fully realize the potential of such systems. This work\npresents a comprehensive investigation of monostatic sensing channels at\n300~GHz, based on an extensive measurement campaign conducted at 57 co-located\ntransceiver (TRx) positions across three representative indoor scenarios.\nMultipath component (MPC) parameters, including amplitude, delay, and angle,\nare extracted using a high-resolution space-alternating generalized\nexpectation-maximization (SAGE) algorithm. To cluster the extracted MPCs, an\nimage-processing-based clustering method, i.e., connected component labeling\n(CCL), is applied to group MPCs based on delay-angle consistency. Based on the\nmeasurement data, an environment-aware channel modeling framework is proposed\nto establish mappings between physical scenario attributes (e.g., reflector\ngeometry, surface materials, and roughness) and their corresponding\nchannel-domain manifestations. The framework incorporates both specular and\ndiffuse reflections and leverages several channel parameters, e.g., reflection\nloss, Lambertian scattering, and intra-cluster dispersion models, to\ncharacterize reflection behavior. Experimental results demonstrate that the\nproposed approach can reliably extract physical characteristics, e.g.,\nstructural and material information, from the observed channel characteristics,\noffering a promising foundation for advanced THz ISAC channel modeling."}
{"id": "2509.02116", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02116", "abs": "https://arxiv.org/abs/2509.02116", "authors": ["Yuanfang Ma", "Zulin Wang", "Peng Yuan", "Qin Huang", "Yuanhan Ni"], "title": "Affine-Doppler Division Multiplexing for High-Mobility Wireless Communications Systems", "comment": "6 pages, 4 figures, 1 table", "summary": "Affine Frequency Division Multiplexing (AFDM) has been regarded as a\ncandidate integrated sensing and communications (ISAC) waveform owing to its\nsuperior communication performance, outperforming the Orthogonal Time-Frequency\nSpace (OTFS) that has been researched for a longer time. However, since the\nabove two waveforms are incompatible with each other, the state-of-the-art\nmethods well-designed for OTFS may not be directly applicable to AFDM. This\npaper introduces a new orthogonal multicarrier waveform, namely Affine-Doppler\nDivision Multiplexing (ADDM), which can provide a generic framework and subsume\nthe existing OTFS and AFDM as a particular case. ADDM modulating information\nsymbols in the Affine-Doppler (A-D) domain based on a two-dimensional (2D)\ntransform can enjoy both excellent unambiguous Doppler and Doppler resolution,\nwhich is the same as AFDM but outperforms OTFS. Moreover, benefiting from the\n2D transform, the symbols block of ADDM in the A-D domain undergoes a 2D cyclic\nshift produced by the delay and the Doppler of the channel, similar to the 2D\ncyclic shift in the delay-Doppler domain of cyclic prefix (CP)-OTFS. This\noffers a potential to directly apply the state-of-the-art methods well-designed\nfor OTFS and AFDM to ADDM. Numerical results show that ADDM achieves comparable\nBER performance with AFDM but outperforms OTFS in high-mobility scenarios."}
{"id": "2509.02137", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02137", "abs": "https://arxiv.org/abs/2509.02137", "authors": ["Salmane Naoumi", "Ahmad Bazzi", "Roberto Bomfin", "Marwa Chafii"], "title": "High-Resolution Sensing in Communication-Centric ISAC: Deep Learning and Parametric Methods", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "This paper introduces two novel algorithms designed to address the challenge\nof super-resolution sensing parameter estimation in bistatic configurations\nwithin communication-centric integrated sensing and communication (ISAC)\nsystems. Our approach leverages the estimated channel state information derived\nfrom reference symbols originally intended for communication to achieve\nsuper-resolution sensing parameter estimation. The first algorithm, IFFT-C2VNN,\nemploys complex-valued convolutional neural networks to estimate the parameters\nof different targets, achieving significant reductions in computational\ncomplexity compared to traditional methods. The second algorithm, PARAMING,\nutilizes a parametric method that capitalizes on the knowledge of the system\nmodel, including the transmit and receive array geometries, to extract the\nsensing parameters accurately. Through a comprehensive performance analysis, we\ndemonstrate the effectiveness and robustness of both algorithms across a range\nof signal-to-noise ratios, underscoring their applicability in realistic ISAC\nscenarios."}
{"id": "2509.02166", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.02166", "abs": "https://arxiv.org/abs/2509.02166", "authors": ["Enzhi Zhou", "Yue Xiao", "Ziyue Liu", "Sotiris A. Tegos", "Panagiotis D. Diamantoulakis", "George K. Karagiannidis"], "title": "Beamforming Design for Pinching Antenna Systems with Multiple Receive Antennas", "comment": null, "summary": "Next-generation networks require intelligent and robust channel conditions to\nsupport ultra-high data rates, seamless connectivity, and large-scale device\ndeployments in dynamic environments. While flexible antenna technologies such\nas fluid and movable antennas offer some degree of adaptability, their limited\nreconfiguration range and structural rigidity reduce their effectiveness in\nrestoring line-of-sight (LoS) links. As a complementary solution, pinching\nantenna systems (PASs) enable fine-grained, hardware-free control of radiation\nlocations along a waveguide, offering enhanced flexibility in challenging\npropagation environments, especially under non-LoS (NLoS) conditions. This\npaper introduces a general and novel modeling framework for downlink PASs\ntargeting users equipped with multiple receive antennas, addressing a practical\nyet underexplored scenario in the existing literature. Specifically, we first\nderive an analytical relationship between the received signal-to-noise ratio\nand the pinching antenna (PA) positions, and based on this, we propose a\ntwo-layer placement strategy. First, we optimize the central radiation point\nusing large-scale channel characteristics, and then we use a heuristic\ncompressed placement algorithm to approximate phase alignment across multiple\nreceive antennas and select a spatially compact set of active elements.\nSimulation results demonstrate notable performance gains over conventional\nsingle-antenna schemes, particularly in short-range scenarios with dense PAs\nand widely spaced user antennas."}
{"id": "2509.02260", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02260", "abs": "https://arxiv.org/abs/2509.02260", "authors": ["Yifan Guo", "Junshan Luo", "Fanggang Wang", "Haiyang Ding", "Shilian Wang", "Zhenhai Xu"], "title": "Dual-end Fluid Antennas For Robust Anti-jamming in Low-altitude Air-ground Communications", "comment": "14 pages, 8 figures, submitted to IEEE journal for possible\n  publications", "summary": "This paper addresses the challenge of co-channel interference and intentional\njamming in low-altitude air-ground communications. Since conventional\nfixed-position antenna (FPA) systems lack spatial adaptability to dynamically\nbalance signal enhancement against interference suppression, we propose a\ntransformative fluid antenna system (FAS)-assisted heterogeneous dual-layer\ntransmission architecture. Specifically, a terrestrial base station with FPA\nserves ground users, while a low altitude-serving base station equipped with\nFAS communicates with the aerial user, also equipped with FAS, under the attack\nof a malicious jammer. We formulate a worst-case achievable rate maximization\nproblem for aerial user subject to constraints including quality-of-service for\nterrestrial users, imperfect jamming directions, minimum antenna separation,\netc. To address the non-convex problem, we propose a fractional\nprogramming-block coordinate descent algorithm that alternately optimizes the\ntransmit precoders, receive combiner, and antenna positions at both transceiver\nsides. Convex hull-based approach and geometric boundary method are used to\nhandle the jamming uncertainty and antenna placement constraints in confined\nspatial regions, respectively. Extensive simulations validate significant\nperformance gains. The FAS achieves up to 56\\% higher data rates than FPA under\nequivalent power constraints. Strategic antenna repositioning demonstrably\nenhances signal quality while suppressing interference, maintaining robustness\nacross diverse jammer channel uncertainties."}
{"id": "2509.02352", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02352", "abs": "https://arxiv.org/abs/2509.02352", "authors": ["Kexin Chen", "Yijie Mao", "Wonjae Shin", "Bruno Clerckx", "Christos Masouros"], "title": "Interference Management for Integrated Sensing and Communications: A Multiple Access Perspective", "comment": null, "summary": "The integrated sensing and communication (ISAC) technique has been considered\na key enabler for 6G radio access networks. ISAC fulfills a brand new paradigm\nshift in wireless networks via the seamless interplay between communication and\nsensing within a unified network. However, the tight integration of these\nfunctionalities inevitably gives rise to various types of interference, posing\nsignificant challenges to existing ISAC waveform designs and rendering\ninterference management a critical concern. Inspired by the development\ntrajectory of wireless communications, different multiple access (MA)\ntechniques, such as orthogonal multiple access (OMA), space-division multiple\naccess (SDMA), and more recently, non-orthogonal multiple access (NOMA) and\nrate-splitting multiple access (RSMA), have been demonstrated to play a pivotal\nrole in efficiently utilizing limited spectrum resources, designing ISAC\nwaveforms, as well as managing inter-user interference and inter-functionality\ninterference in ISAC. Notably, the interplay between MA and ISAC presents\nmutually beneficial integration. On the one hand, ISAC helps MA techniques\nbetter exploit their interference management capability beyond the\ncommunication-only networks. On the other hand, different MA techniques serve\nas promising solutions for inter-functionality and inter-user interference\nmanagement in ISAC. In this paper, we deliver the first comprehensive tutorial\nof MA techniques in ISAC networks. Specifically, we illustrate the fundamental\nprinciples of ISAC, classify the diverse types of interference in different\nISAC systems, and compare MA-assisted ISAC designs, highlighting their\nrespective advantages and limitations. Moreover, we provide an outlook on the\nemerging applications and future research directions of different MA-assisted\nISAC."}
{"id": "2509.02442", "categories": ["eess.SP", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.02442", "abs": "https://arxiv.org/abs/2509.02442", "authors": ["Chen Sun", "Wenqi Zhang", "Bizhu Wang", "Xiaodong Xu", "Chau Yuen", "Yan Zhang", "Ping Zhang"], "title": "Know What, Know Why: Semantic Hazard Communication for Intelligent V2X Systems", "comment": null, "summary": "In current vehicle-to-everything (V2X) communication systems, roadside units\n(RSUs) broadcast brief warning messages that alert nearby vehicles to avoid\npotential hazards. However, these messages lack contextual information on why a\nwarning is issued, leading to excessive caution or inefficient driving\nbehaviors. To avoid such a situation, we propose a semantic-enhanced and\nexplainable V2X (SEE-V2X) system. In the proposed system, RSUs equipped with\nsmart cameras detect obstructions and transmit context-aware messages to\nvehicles. By understanding both what the hazard is and why it occurs, drivers\ncan make more intelligent decisions based on their specific driving situation.\nFurthermore, through a real-field demonstration, we show the new \"see-through\"\nfeature in the proposed system, which enables drivers to visualize hidden\npedestrians behind obstacles. We also perform simulations to compare\ntraditional V2X with SEE-V2X under different traffic conditions. The results\nshow that SEE-V2X significantly improves traffic efficiency and reduces\nunnecessary deceleration."}
{"id": "2509.02540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02540", "abs": "https://arxiv.org/abs/2509.02540", "authors": ["Halvin Yang", "Sangarapillai Lambotharan", "Mahsa Derakhshani", "Lajos Hanzo"], "title": "LLM-Enhanced Space-Air-Ground-Sea Integrated Networks", "comment": "6 figures, 7 pages, magazine", "summary": "The space-air-ground-sea integrated networking (SAGSIN) concept promises\nseamless global multimedia connectivity, yet two obstacles still limit its\npractical deployment. Firstly, high-velocity satellites, aerial relays and\nsea-surface platforms suffer from obsolete channel state information (CSI),\nundermining feedback-based adaptation. Secondly, data-rate disparity across the\nprotocol stack is extreme: terabit optical links in space coexist with kilobit\nacoustic under-water links. This article shows that a single large language\nmodel (LLM) backbone, trained jointly on radio, optical and acoustic traces,\ncan provide a unified, data-driven adaptation layer that addresses both rapid\nCSI ageing and severe bandwidth disparity across the SAGSIN protocol stack.\nExplicitly, an LLM-based long-range channel predictor forecasts the strongest\ndelay-Doppler components several coherence intervals ahead, facilitating\nnear-capacity reception despite violent channel fluctuations. Furthermore, our\nLLM-based semantic encoder turns raw sensor payloads into task-oriented tokens.\nThis substantially reduces the SNR required for high-fidelity image delivery in\na coastal underwater link, circumventing the data rate limitation by semantic\ncommunications. Inclusion of these tools creates a medium-agnostic adaptation\nlayer that spans radio, optical and acoustic channels. We conclude with\npromising open research directions in on-device model compression, multimodal\nfidelity control, cross-layer resource orchestration and trustworthy operation,\ncharting a path from laboratory prototypes to field deployment."}

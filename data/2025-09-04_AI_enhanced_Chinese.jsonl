{"id": "2509.02771", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02771", "abs": "https://arxiv.org/abs/2509.02771", "authors": ["Nirmalya Mallick Thakur", "Jia Qi Yip", "Eng Siong Chng"], "title": "Analysis of Speaker Verification Performance Trade-offs with Neural Audio Codec Transmission", "comment": "Accepted by APSIPA ASC 2025", "summary": "Neural audio codecs (NACs) have made significant advancements in recent years\nand are rapidly being adopted in many audio processing pipelines. However, they\ncan introduce audio distortions which degrade speaker verification (SV)\nperformance. This study investigates the impact of both traditional and neural\naudio codecs at varying bitrates on three state of-the-art SV models evaluated\non the VoxCeleb1 dataset. Our findings reveal a consistent degradation in SV\nperformance across all models and codecs as bitrates decrease. Notably, NACs do\nnot fundamentally break SV performance when compared to traditional codecs.\nThey outperform Opus by 6-8% at low-bitrates (< 12 kbps) and remain marginally\nbehind at higher bitrates ($\\approx$ 24 kbps), with an EER increase of only\n0.4-0.7%. The disparity at higher bitrates is likely due to the primary\noptimization of NACs for perceptual quality, which can inadvertently discard\ncritical speaker-discriminative features, unlike Opus which was designed to\npreserve vocal characteristics. Our investigation suggests that NACs are a\nfeasible alternative to traditional codecs, especially under bandwidth\nlimitations. To bridge the gap at higher bitrates, future work should focus on\ndeveloping speaker-aware NACs or retraining and adapting SV models.", "AI": {"tldr": "\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u4f4e\u6bd4\u7279\u7387\uff08<12kbps\uff09\u4e0b\u6bd4\u4f20\u7edf\u7f16\u89e3\u7801\u5668Opus\u6027\u80fd\u66f4\u597d\uff0c\u4f46\u5728\u9ad8\u6bd4\u7279\u7387\uff08\u224824kbps\uff09\u4e0b\u7565\u5dee\uff0c\u4e3b\u8981\u56e0\u4e3a\u5176\u4f18\u5316\u76ee\u6807\u4e3a\u611f\u77e5\u8d28\u91cf\u800c\u975e\u8bf4\u8bdd\u4eba\u7279\u5f81\u4fdd\u7559\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u4e0e\u4f20\u7edf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u4e0d\u540c\u6bd4\u7279\u7387\u4e0b\u5bf9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u5176\u5728\u97f3\u9891\u5904\u7406\u7ba1\u9053\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5728VoxCeleb1\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u6a21\u578b\uff0c\u6bd4\u8f83\u4f20\u7edf\u548c\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u4e0d\u540c\u6bd4\u7279\u7387\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u6240\u6709\u7f16\u89e3\u7801\u5668\u548c\u6a21\u578b\u5728\u6bd4\u7279\u7387\u964d\u4f4e\u65f6\u90fd\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\u3002\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u6bd4Opus\u6027\u80fd\u597d6-8%\uff0c\u5728\u9ad8\u6bd4\u7279\u7387\u4e0bEER\u4ec5\u589e\u52a00.4-0.7%\u3002", "conclusion": "\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u662f\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5e26\u5bbd\u53d7\u9650\u60c5\u51b5\u4e0b\u3002\u672a\u6765\u9700\u8981\u5f00\u53d1\u8bf4\u8bdd\u4eba\u611f\u77e5\u7684\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u6216\u91cd\u65b0\u8bad\u7ec3\u9002\u5e94\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u6a21\u578b\u3002"}}
{"id": "2509.02859", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.02859", "abs": "https://arxiv.org/abs/2509.02859", "authors": ["Sandipana Dowerah", "Atharva Kulkarni", "Ajinkya Kulkarni", "Hoan My Tran", "Joonas Kalda", "Artem Fedorchenko", "Benoit Fauve", "Damien Lolive", "Tanel Alum\u00e4e", "Matthew Magimai Doss"], "title": "Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models", "comment": null, "summary": "Parallel to the development of advanced deepfake audio generation, audio\ndeepfake detection has also seen significant progress. However, a standardized\nand comprehensive benchmark is still missing. To address this, we introduce\nSpeech DeepFake (DF) Arena, the first comprehensive benchmark for audio\ndeepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate\ndetection systems, currently across 14 diverse datasets and attack scenarios,\nstandardized evaluation metrics and protocols for reproducibility and\ntransparency. It also includes a leaderboard to compare and rank the systems to\nhelp researchers and developers enhance their reliability and robustness. We\ninclude 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary\ndetection systems. Our study presents many systems exhibiting high EER in\nout-of-domain scenarios, highlighting the need for extensive cross-domain\nevaluation. The leaderboard is hosted on Huggingface1 and a toolkit for\nreproducing results across the listed datasets is available on GitHub.", "AI": {"tldr": "Speech DF Arena\u662f\u9996\u4e2a\u5168\u9762\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b14\u4e2a\u6570\u636e\u96c6\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u548c\u534f\u8bae\uff0c\u4ee5\u53ca\u7528\u4e8e\u6bd4\u8f83\u68c0\u6d4b\u7cfb\u7edf\u7684\u6392\u884c\u699c\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e5f\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b14\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u653b\u51fb\u573a\u666f\u7684\u5de5\u5177\u5305\uff0c\u91c7\u7528\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u548c\u534f\u8bae\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u5f00\u6e90\u548c3\u4e2a\u4e13\u6709\u68c0\u6d4b\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u7cfb\u7edf\u5728\u8de8\u57df\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8EER\uff08\u7b49\u9519\u8bef\u7387\uff09\uff0c\u51f8\u663e\u4e86\u5e7f\u6cdb\u8de8\u57df\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "Speech DF Arena\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u8de8\u57df\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5de5\u5177\u5305\u548c\u6392\u884c\u699c\u3002"}}
{"id": "2509.03409", "categories": ["cs.SD", "cs.AI", "cs.MM", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.03409", "abs": "https://arxiv.org/abs/2509.03409", "authors": ["Hoan My Tran", "Damien Lolive", "Aghilas Sini", "Arnaud Delhay", "Pierre-Fran\u00e7ois Marteau", "David Guennec"], "title": "Multi-level SSL Feature Gating for Audio Deepfake Detection", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Recent advancements in generative AI, particularly in speech synthesis, have\nenabled the generation of highly natural-sounding synthetic speech that closely\nmimics human voices. While these innovations hold promise for applications like\nassistive technologies, they also pose significant risks, including misuse for\nfraudulent activities, identity theft, and security threats. Current research\non spoofing detection countermeasures remains limited by generalization to\nunseen deepfake attacks and languages. To address this, we propose a gating\nmechanism extracting relevant feature from the speech foundation XLS-R model as\na front-end feature extractor. For downstream back-end classifier, we employ\nMulti-kernel gated Convolution (MultiConv) to capture both local and global\nspeech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as\na similarity metric to enforce diversity in learned features across different\nMultiConv layers. By integrating CKA with our gating mechanism, we hypothesize\nthat each component helps improving the learning of distinct synthetic speech\npatterns. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on in-domain benchmarks while generalizing\nrobustly to out-of-domain datasets, including multilingual speech samples. This\nunderscores its potential as a versatile solution for detecting evolving speech\ndeepfake threats.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408XLS-R\u57fa\u7840\u6a21\u578b\u7684\u95e8\u63a7\u673a\u5236\u548c\u591a\u5185\u6838\u5377\u79ef\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u68c0\u6d4b\u591a\u8bed\u8a00\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8bed\u97f3\u53ef\u80fd\u88ab\u6b3a\u8bc8\u6d3b\u52a8\u3001\u8eab\u4efd\u76d7\u7528\u7b49\u6076\u610f\u5229\u7528\uff0c\u5f53\u524d\u7684\u9632\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u5728\u5e94\u5bf9\u672a\u89c1\u653b\u51fb\u548c\u591a\u8bed\u8a00\u573a\u666f\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528XLS-R\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u524d\u7aef\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff1b\u4e0b\u6e38\u4f7f\u7528\u591a\u5185\u6838\u95e8\u63a7\u5377\u79ef(MultiConv)\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u97f3\u7279\u5f81\uff1b\u5f15\u5165\u4e2d\u5fc3\u5185\u6838\u5bf9\u9f50(CKA)\u6307\u6807\u786e\u4fdd\u4e0d\u540c\u5c42\u5b66\u4e60\u5230\u591a\u6837\u5316\u7684\u7279\u5f81\u3002", "result": "\u5728\u5185\u90e8\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5916\u90e8\u6570\u636e\u96c6(\u5305\u62ec\u591a\u8bed\u8a00\u8bed\u97f3)\u4e0a\u4e5f\u663e\u793a\u51fa\u826f\u597d\u7684\u6f14\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68c0\u6d4b\u65e5\u76ca\u53d1\u5c55\u7684\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u62a4\u8bed\u97f3\u5b89\u5168\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.02571", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02571", "abs": "https://arxiv.org/abs/2509.02571", "authors": ["Diego Di Carlo", "Koyama Shoichi", "Nugraha Aditya Arie", "Fontaine Mathieu", "Bando Yoshiaki", "Yoshii Kazuyoshi"], "title": "Gaussian Process Regression of Steering Vectors With Physics-Aware Deep Composite Kernels for Augmented Listening", "comment": null, "summary": "This paper investigates continuous representations of steering vectors over\nfrequency and position of microphone and source for augmented listening (e.g.,\nspatial filtering and binaural rendering) with precise control of the sound\nfield perceived by the user. Steering vectors have typically been used for\nrepresenting the spatial characteristics of the sound field as a function of\nthe listening position. The basic algebraic representation of steering vectors\nassuming an idealized environment cannot deal with the scattering effect of the\nsound field. One may thus collect a discrete set of real steering vectors\nmeasured in dedicated facilities and super-resolve (i.e., upsample) them.\nRecently, physics-aware deep learning methods have been effectively used for\nthis purpose. Such deterministic super-resolution, however, suffers from the\noverfitting problem due to the non-uniform uncertainty over the measurement\nspace. To solve this problem, we integrate an expressive representation based\non the neural field (NF) into the principled probabilistic framework based on\nthe Gaussian process (GP). Specifically, we propose a physics-aware composite\nkernel that model the directional incoming waves and the subsequent scattering\neffect. Our comprehensive comparative experiment showed the effectiveness of\nthe proposed method under data insufficiency conditions. In downstream tasks\nsuch as speech enhancement and binaural rendering using the simulated data of\nthe SPEAR challenge, the oracle performances were attained with less than ten\ntimes fewer measurements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u548c\u9ad8\u65af\u8fc7\u7a0b\u7684\u7269\u7406\u611f\u77e5\u590d\u5408\u6838\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u8868\u793a\u8f6c\u5411\u5411\u91cf\uff0c\u89e3\u51b3\u4f20\u7edf\u786e\u5b9a\u6027\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u6570\u636e\u4e0d\u8db3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8f6c\u5411\u5411\u91cf\u7684\u4ee3\u6570\u8868\u793a\u65e0\u6cd5\u5904\u7406\u58f0\u573a\u6563\u5c04\u6548\u5e94\uff0c\u800c\u73b0\u6709\u7684\u786e\u5b9a\u6027\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5b58\u5728\u975e\u5747\u5300\u6d4b\u91cf\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u5c06\u795e\u7ecf\u573a\uff08NF\uff09\u7684\u8868\u793a\u80fd\u529b\u4e0e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u6982\u7387\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u7269\u7406\u611f\u77e5\u590d\u5408\u6838\u6765\u5efa\u6a21\u65b9\u5411\u6027\u5165\u5c04\u6ce2\u548c\u540e\u7eed\u6563\u5c04\u6548\u5e94\u3002", "result": "\u5728SPEAR\u6311\u6218\u8d5b\u7684\u6a21\u62df\u6570\u636e\u4e0a\u8fdb\u884c\u8bed\u97f3\u589e\u5f3a\u548c\u53cc\u8033\u6e32\u67d3\u7b49\u4e0b\u6e38\u4efb\u52a1\uff0c\u4ec5\u7528\u4e0d\u5230\u5341\u5206\u4e4b\u4e00\u7684\u6d4b\u91cf\u6570\u636e\u5c31\u8fbe\u5230\u4e86oracle\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6570\u636e\u4e0d\u8db3\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u58f0\u573a\u611f\u77e5\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8fde\u7eed\u8f6c\u5411\u5411\u91cf\u8868\u793a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02568", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.02568", "abs": "https://arxiv.org/abs/2509.02568", "authors": ["Mohammad Mehedi Hasan", "Pedro G. Lind", "Hernando Ombao", "Anis Yazidi", "Rabindra Khadka"], "title": "EEG-MSAF: An Interpretable Microstate Framework uncovers Default-Mode Decoherence in Early Neurodegeneration", "comment": "Dementia, EEG, Microstates, Explainable, SHAP", "summary": "Dementia (DEM) is a growing global health challenge, underscoring the need\nfor early and accurate diagnosis. Electroencephalography (EEG) provides a\nnon-invasive window into brain activity, but conventional methods struggle to\ncapture its transient complexity. We present the \\textbf{EEG Microstate\nAnalysis Framework (EEG-MSAF)}, an end-to-end pipeline that leverages EEG\nmicrostates discrete, quasi-stable topographies to identify DEM-related\nbiomarkers and distinguish DEM, mild cognitive impairment (MCI), and normal\ncognition (NC). EEG-MSAF comprises three stages: (1) automated microstate\nfeature extraction, (2) classification with machine learning (ML), and (3)\nfeature ranking using Shapley Additive Explanations (SHAP) to highlight key\nbiomarkers. We evaluate on two EEG datasets: the public Chung-Ang University\nEEG (CAUEEG) dataset and a clinical cohort from Thessaloniki Hospital. Our\nframework demonstrates strong performance and generalizability. On CAUEEG,\nEEG-MSAF-SVM achieves \\textbf{89\\% $\\pm$ 0.01 accuracy}, surpassing the deep\nlearning baseline CEEDNET by \\textbf{19.3\\%}. On the Thessaloniki dataset, it\nreaches \\textbf{95\\% $\\pm$ 0.01 accuracy}, comparable to EEGConvNeXt. SHAP\nanalysis identifies mean correlation and occurrence as the most informative\nmetrics: disruption of microstate C (salience/attention network) dominates DEM\nprediction, while microstate F, a novel default-mode pattern, emerges as a key\nearly biomarker for both MCI and DEM. By combining accuracy, generalizability,\nand interpretability, EEG-MSAF advances EEG-based dementia diagnosis and sheds\nlight on brain dynamics across the cognitive spectrum.", "AI": {"tldr": "\u4e00\u4e2a\u57fa\u4e8eEEG\u5fae\u89c2\u6001\u5206\u6790\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548cSHAP\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5728\u7591\u60d1\u75c7\u8bca\u65ad\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u826f\u597d\u7684\u53ef\u63a8\u5e7f\u6027", "motivation": "\u7591\u60d1\u75c7\u662f\u5168\u7403\u6025\u9700\u89e3\u51b3\u7684\u5065\u5eb7\u6311\u6218\uff0c\u9700\u8981\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u3002\u4f20\u7edfEEG\u65b9\u6cd5\u65e0\u6cd5\u6293\u53d6\u8111\u7535\u6d3b\u52a8\u7684\u77ac\u6001\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5206\u6790\u65b9\u6cd5", "method": "\u5f00\u53d1\u4e86EEG\u5fae\u89c2\u6001\u5206\u6790\u6846\u67b6(EEG-MSAF)\uff0c\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u81ea\u52a8\u5316\u5fae\u89c2\u6001\u7279\u5f81\u63d0\u53d6\u3001\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u3001SHAP\u7279\u5f81\u6392\u540d\u89e3\u91ca", "result": "\u5728CAUEEG\u6570\u636e\u96c6\u4e0a\u8fbe\u523089%\u51c6\u786e\u7387(\u8d85\u8fc7\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u6a2119.3%)\uff0c\u5728Thessaloniki\u6570\u636e\u96c6\u4e0a\u8fbe\u523095%\u51c6\u786e\u7387\u3002SHAP\u5206\u6790\u53d1\u73b0\u5fae\u89c2\u6001C\u548cF\u662f\u5173\u952e\u751f\u7269\u6807\u8bb0", "conclusion": "EEG-MSAF\u6846\u67b6\u7ed3\u5408\u4e86\u9ad8\u51c6\u786e\u7387\u3001\u826f\u597d\u53ef\u63a8\u5e7f\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u8fdb\u4e86EEG\u57fa\u4e8e\u7591\u60d1\u75c7\u8bca\u65ad\u7684\u53d1\u5c55\uff0c\u4e3a\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7684\u8111\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2509.02622", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02622", "abs": "https://arxiv.org/abs/2509.02622", "authors": ["Berger Cl\u00e9mentine", "Stamadiatis Paraskevas", "Badeau Roland", "Essid Slim"], "title": "IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering", "comment": null, "summary": "We are interested in audio systems capable of performing a differentiated\nprocessing of stationary backgrounds and isolated acoustic events within an\nacoustic scene, whether for applying specific processing methods to each part\nor for focusing solely on one while ignoring the other. Such systems have\napplications in real-world scenarios, including robust adaptive audio rendering\nsystems (e.g., EQ or compression), plosive attenuation in voice mixing, noise\nsuppression or reduction, robust acoustic event classification or even\nbioacoustics. To this end, we introduce IS${}^3$, a neural network designed for\nImpulsive--Stationary Sound Separation, that isolates impulsive acoustic events\nfrom the stationary background using a deep filtering approach, that can act as\na pre-processing stage for the above-mentioned tasks. To ensure optimal\ntraining, we propose a sophisticated data generation pipeline that curates and\nadapts existing datasets for this task. We demonstrate that a learning-based\napproach, build on a relatively lightweight neural architecture and trained\nwith well-designed and varied data, is successful in this previously\nunaddressed task, outperforming the Harmonic--Percussive Sound Separation\nmasking method, adapted from music signal processing research, and wavelet\nfiltering on objective separation metrics.", "AI": {"tldr": "IS\u00b3\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5206\u79bb\u97f3\u9891\u4e2d\u7684\u8109\u51b2\u58f0\u548c\u7a33\u6001\u80cc\u666f\u58f0\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u97f3\u9891\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u5f00\u53d1\u80fd\u591f\u533a\u5206\u5904\u7406\u7a33\u6001\u80cc\u666f\u548c\u5b64\u7acb\u58f0\u5b66\u4e8b\u4ef6\u7684\u97f3\u9891\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u81ea\u9002\u5e94\u97f3\u9891\u6e32\u67d3\u3001\u8bed\u97f3\u5904\u7406\u3001\u566a\u58f0\u6291\u5236\u3001\u58f0\u5b66\u4e8b\u4ef6\u5206\u7c7b\u548c\u751f\u7269\u58f0\u5b66\u7b49\u9886\u57df", "method": "\u4f7f\u7528\u57fa\u4e8e\u6df1\u5ea6\u8fc7\u6ee4\u65b9\u6cd5\u7684IS\u00b3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u914d\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u6765\u8bad\u7ec3\u6a21\u578b", "result": "\u8be5\u65b9\u6cd5\u5728\u5ba2\u89c2\u5206\u79bb\u6307\u6807\u4e0a\u4f18\u4e8e\u4ece\u97f3\u4e50\u4fe1\u53f7\u5904\u7406\u6539\u7f16\u7684\u8c10\u6ce2-\u6253\u51fb\u4e50\u5206\u79bb\u63a9\u853d\u65b9\u6cd5\u548c\u5c0f\u6ce2\u6ee4\u6ce2\u65b9\u6cd5", "conclusion": "\u57fa\u4e8e\u76f8\u5bf9\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u8bad\u7ec3\u6570\u636e\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u8fd9\u4e2a\u5148\u524d\u672a\u89e3\u51b3\u7684\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u6210\u529f"}}
{"id": "2509.02724", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.02724", "abs": "https://arxiv.org/abs/2509.02724", "authors": ["Xiang-Gen Xia"], "title": "Recall Gabor Communication Theory and Joint Time-Frequency Analysis", "comment": null, "summary": "In this article, we first briefly recall Gabor's communication theory and\nthen Gabor transform and expansion, and also its connection with joint time\nfrequency analysis.", "AI": {"tldr": "\u56de\u987eGabor\u7684\u901a\u4fe1\u7406\u8bba\u3001Gabor\u53d8\u6362\u548c\u5c55\u5f00\uff0c\u53ca\u5176\u4e0e\u8054\u5408\u65f6\u9891\u5206\u6790\u7684\u8054\u7cfb", "motivation": "\u7cfb\u7edf\u68b3\u7406Gabor\u901a\u4fe1\u7406\u8bba\u53ca\u5176\u5728\u65f6\u9891\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\u548c\u7406\u8bba\u5206\u6790\uff0c\u9610\u8ff0Gabor\u53d8\u6362\u7684\u6570\u5b66\u57fa\u7840\u548c\u4e0e\u65f6\u9891\u5206\u6790\u7684\u5173\u7cfb", "result": "\u660e\u786e\u4e86Gabor\u7406\u8bba\u5728\u901a\u4fe1\u548c\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u57fa\u7840\u5730\u4f4d\uff0c\u5efa\u7acb\u4e86Gabor\u53d8\u6362\u4e0e\u65f6\u9891\u5206\u6790\u7684\u8fde\u63a5", "conclusion": "Gabor\u7406\u8bba\u4e3a\u73b0\u4ee3\u65f6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u5b66\u5de5\u5177\uff0c\u5728\u4fe1\u53f7\u5904\u7406\u9886\u57df\u5177\u6709\u57fa\u7840\u6027\u610f\u4e49"}}
{"id": "2509.03013", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03013", "abs": "https://arxiv.org/abs/2509.03013", "authors": ["Ryandhimas E. Zezario", "Dyah A. M. G. Wisnu", "Hsin-Min Wang", "Yu Tsao"], "title": "Speech Intelligibility Assessment with Uncertainty-Aware Whisper Embeddings and sLSTM", "comment": "Accepted to APSIPA ASC 2025", "summary": "Non-intrusive speech intelligibility prediction remains challenging due to\nvariability in speakers, noise conditions, and subjective perception. We\npropose an uncertainty-aware approach that leverages Whisper embeddings in\ncombination with statistical features, specifically the mean, standard\ndeviation, and entropy computed across the embedding dimensions. The entropy,\ncomputed via a softmax over the feature dimension, serves as a proxy for\nuncertainty, complementing global information captured by the mean and standard\ndeviation. To model the sequential structure of speech, we adopt a scalar long\nshort-term memory (sLSTM) network, which efficiently captures long-range\ndependencies. Building on this foundation, we propose iMTI-Net, an improved\nmulti-target intelligibility prediction network that integrates convolutional\nneural network (CNN) and sLSTM components within a multitask learning\nframework. It jointly predicts human intelligibility scores and machine-based\nword error rates (WER) from Google ASR and Whisper. Experimental results show\nthat iMTI-Net outperforms the original MTI-Net across multiple evaluation\nmetrics, demonstrating the effectiveness of incorporating uncertainty-aware\nfeatures and the CNN-sLSTM architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86iMTI-Net\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u76ee\u6807\u8bed\u97f3\u6e05\u6670\u5ea6\u9884\u6d4b\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408Whisper\u5d4c\u5165\u548c\u7edf\u8ba1\u7279\u5f81\uff08\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u71b5\uff09\uff0c\u5e76\u91c7\u7528CNN-sLSTM\u67b6\u6784\uff0c\u5728\u591a\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u539fMTI-Net\u3002", "motivation": "\u975e\u4fb5\u5165\u5f0f\u8bed\u97f3\u6e05\u6670\u5ea6\u9884\u6d4b\u9762\u4e34\u8bf4\u8bdd\u4eba\u3001\u566a\u58f0\u6761\u4ef6\u548c\u4e3b\u89c2\u611f\u77e5\u7684\u53d8\u5f02\u6027\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7279\u5f81\u63d0\u53d6\u548c\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Whisper\u5d4c\u5165\u7ed3\u5408\u7edf\u8ba1\u7279\u5f81\uff08\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u71b5\uff09\uff0c\u71b5\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4ee3\u7406\uff1b\u91c7\u7528\u6807\u91cfLSTM\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff1b\u63d0\u51faiMTI-Net\u7ed3\u5408CNN\u548csLSTM\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u9884\u6d4b\u4eba\u7c7b\u6e05\u6670\u5ea6\u5206\u6570\u548c\u673a\u5668WER\u3002", "result": "iMTI-Net\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u539f\u59cbMTI-Net\uff0c\u8bc1\u660e\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7279\u5f81\u548cCNN-sLSTM\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7279\u5f81\u548cCNN-sLSTM\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u6e05\u6670\u5ea6\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u9c81\u68d2\u7684\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02797", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.02797", "abs": "https://arxiv.org/abs/2509.02797", "authors": ["Sagnik Bhattacharya", "Abhiram Rao Gorle", "John M. Cioffi"], "title": "minPIC: Towards Optimal Power Allocation in Multi-User Interference Channels", "comment": "To appear in IEEE GLOBECOM 2025", "summary": "6G envisions massive cell-free networks with spatially nested multiple access\n(MAC) and broadcast (BC) channels without centralized coordination. This makes\noptimal resource allocation across power, subcarriers, and decoding orders\ncrucial for interference channels (ICs), where neither transmitters nor\nreceivers can cooperate. Current orthogonal multiple access (OMA) methods, as\nwell as non-orthogonal (NOMA) and rate-splitting (RSMA) schemes, rely on fixed\nheuristics for interference management, leading to suboptimal rates, power\ninefficiency, and scalability issues. This paper proposes a novel minPIC\nframework for optimal power, subcarrier, and decoding order allocation in\ngeneral multi-user ICs. Unlike existing methods, minPIC eliminates heuristic\nSIC order assumptions. Despite the convexity of the IC capacity region, fixing\nan SIC order induces non-convexity in resource allocation, traditionally\nrequiring heuristic approximations. We instead introduce a dual-variable-guided\nsorting criterion to identify globally optimal SIC orders, followed by convex\noptimization with auxiliary log-det constraints, efficiently solved via binary\nsearch. We also demonstrate that minPIC could potentially meet the stringent\nhigh-rate, low-power targets of immersive XR and other 6G applications. To the\nbest of our knowledge, minPIC is the first algorithmic realisation of the\nPareto boundary of the SIC-achievable rate region for Gaussian ICs, opening the\ndoor to scalable interference management in cell-free networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86minPIC\u6846\u67b6\uff0c\u7528\u4e8e6G\u65e0\u8702\u7a9d\u7f51\u7edc\u4e2d\u591a\u7528\u6237\u5e72\u6270\u4fe1\u9053\u7684\u6700\u4f18\u529f\u7387\u3001\u5b50\u8f7d\u6ce2\u548c\u89e3\u7801\u987a\u5e8f\u5206\u914d\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u542f\u53d1\u5f0fSIC\u987a\u5e8f\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u9ad8\u65af\u5e72\u6270\u4fe1\u9053SIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\u3002", "motivation": "6G\u5927\u89c4\u6a21\u65e0\u8702\u7a9d\u7f51\u7edc\u9700\u8981\u5728\u4e0d\u8fdb\u884c\u96c6\u4e2d\u534f\u8c03\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u7a7a\u95f4\u5d4c\u5957\u7684\u591a\u5740\u63a5\u5165\u548c\u5e7f\u64ad\u4fe1\u9053\u5e72\u6270\uff0c\u73b0\u6709OMA\u3001NOMA\u548cRSMA\u65b9\u6848\u4f9d\u8d56\u56fa\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u901f\u7387\u6b21\u4f18\u3001\u529f\u7387\u6548\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53cc\u53d8\u91cf\u5f15\u5bfc\u7684\u6392\u5e8f\u51c6\u5219\u6765\u8bc6\u522b\u5168\u5c40\u6700\u4f18SIC\u987a\u5e8f\uff0c\u7136\u540e\u901a\u8fc7\u5e26\u6709\u8f85\u52a9\u5bf9\u6570\u884c\u5217\u5f0f\u7ea6\u675f\u7684\u51f8\u4f18\u5316\u8fdb\u884c\u6c42\u89e3\uff0c\u91c7\u7528\u4e8c\u5206\u641c\u7d22\u9ad8\u6548\u89e3\u51b3\u3002", "result": "minPIC\u80fd\u591f\u5b9e\u73b0\u9ad8\u65af\u5e72\u6270\u4fe1\u9053SIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u6709\u671b\u6ee1\u8db3\u6c89\u6d78\u5f0fXR\u7b496G\u5e94\u7528\u7684\u9ad8\u901f\u7387\u3001\u4f4e\u529f\u8017\u8981\u6c42\u3002", "conclusion": "minPIC\u662f\u9996\u4e2a\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u65af\u5e72\u6270\u4fe1\u9053SIC\u53ef\u8fbe\u901f\u7387\u533a\u57df\u5e15\u7d2f\u6258\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u4e3a\u65e0\u8702\u7a9d\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u5e72\u6270\u7ba1\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.03017", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03017", "abs": "https://arxiv.org/abs/2509.03017", "authors": ["Ryandhimas E. Zezario"], "title": "Non-Intrusive Intelligibility Prediction for Hearing Aids: Recent Advances, Trends, and Challenges", "comment": "APSIPA ASC 2025 perspective paper", "summary": "This paper provides an overview of recent progress in non-intrusive speech\nintelligibility prediction for hearing aids (HA). We summarize developments in\nrobust acoustic feature extraction, hearing loss modeling, and the use of\nemerging architectures for long-sequence processing. Listener-specific\nadaptation strategies and domain generalization approaches that aim to improve\nrobustness in unseen acoustic environments are also discussed. Remaining\nchallenges, such as the need for large-scale, diverse datasets and reliable\ncross-profile generalization, are acknowledged. Our goal is to offer a\nperspective on current trends, ongoing challenges, and possible future\ndirections toward practical and reliable HA-oriented intelligibility prediction\nsystems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7ed3\u5408\u4e86\u52a9\u542c\u5668\u65b9\u9762\u975e\u4fb5\u5165\u5f0f\u8bed\u97f3\u53ef\u6167\u6027\u9884\u6d4b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u7279\u5f81\u63d0\u53d6\u3001\u542c\u529b\u635f\u5931\u6a21\u578b\u548c\u957f\u5e8f\u5217\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u9002\u914d\u7b56\u7565\u548c\u9886\u57df\u6cdb\u5316\u6311\u6218\u3002", "motivation": "\u63d0\u4f9b\u52a9\u542c\u5668\u65b9\u5411\u7684\u53ef\u9760\u8bed\u97f3\u53ef\u6167\u6027\u9884\u6d4b\u7cfb\u7edf\u7684\u5f53\u524d\u8d8b\u52bf\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u89c6\u89d2\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u7efc\u8ff0\u4e86\u7a33\u5065\u7684\u97f3\u54cd\u7279\u5f81\u63d0\u53d6\u3001\u542c\u529b\u635f\u5931\u5efa\u6a21\u3001\u957f\u5e8f\u5217\u5904\u7406\u65b0\u5174\u67b6\u6784\u3001\u542c\u4f17\u7279\u5b9a\u9002\u914d\u7b56\u7565\u548c\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u5f53\u524d\u9886\u57df\u7684\u8fdb\u5c55\u548c\u6210\u679c\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u5982\u5927\u89c4\u6a21\u591a\u6837\u6027\u6570\u636e\u96c6\u548c\u53ef\u9760\u8de8\u6d41\u6d3e\u6cdb\u5316\u7b49\u6301\u7eed\u6311\u6218\u3002", "conclusion": "\u8be5\u9886\u57df\u5728\u5411\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u65b9\u5411\u53d1\u5c55\uff0c\u9700\u8981\u7ee7\u7eed\u5173\u6ce8\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u6a21\u578b\u9002\u914d\u6027\u548c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u6311\u6218\u3002"}}
{"id": "2509.02819", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.02819", "abs": "https://arxiv.org/abs/2509.02819", "authors": ["Sameer Mathad", "Taejoon Kim", "David J. Love"], "title": "Protecting Legacy Wireless Systems Against Interference: Precoding and Codebook Approaches Using Massive MIMO and Region Constraints", "comment": null, "summary": "The ever-increasing demand for high-speed wireless communication has\ngenerated significant interest in utilizing frequency bands that are adjacent\nto those occupied by legacy wireless systems. Since the legacy wireless systems\nwere designed based on often decades-old assumptions about wireless\ninterference, utilizing these new bands will result in interference with the\nexisting legacy users. Many of these legacy wireless devices are used by\ncritical infrastructure networks upon which society depends. There is an urgent\nneed to develop schemes that can protect legacy users from such interference.\nFor many applications, legacy users are located within\ngeographically-constrained regions. Several studies have proposed mitigating\ninterference through the implementation of exclusion zones near these\ngeographically-constrained regions. In contrast to solutions based on\ngeographic exclusion zones, this paper presents a communication theory-based\nsolution. By leveraging knowledge of these geographically-constrained regions,\nwe aim to reduce the interference impact on legacy users. We achieve this by\nincorporating received power constraints, termed as region constraints, in our\nmassive multiple-input multiple-output (MIMO) system design. We perform a\ncapacity analysis of single-user massive MIMO and a sum-rate analysis of the\nmulti-user massive MIMO system with transmit power and region constraints. We\npresent a precoding design method that allows for the utilization of new\nfrequency bands while protecting legacy users.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u4fe1\u7406\u8bba\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728MIMO\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u52a0\u5165\u533a\u57df\u529f\u7387\u7ea6\u675f\u6765\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u514d\u53d7\u65b0\u9891\u6bb5\u4f7f\u7528\u7684\u5e72\u6270\uff0c\u800c\u4e0d\u662f\u91c7\u7528\u5730\u7406\u9694\u79bb\u533a\u7684\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5bf9\u9ad8\u901f\u65e0\u7ebf\u901a\u4fe1\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f7f\u7528\u4e0e\u4f20\u7edf\u65e0\u7ebf\u7cfb\u7edf\u76f8\u90bb\u7684\u65b0\u9891\u6bb5\u4f1a\u5bfc\u81f4\u5bf9\u4f20\u7edf\u7528\u6237\u7684\u5e72\u6270\u3002\u8bb8\u591a\u4f20\u7edf\u65e0\u7ebf\u8bbe\u5907\u7528\u4e8e\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u514d\u53d7\u5e72\u6270\u7684\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5728Massive MIMO\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u52a0\u5165\u63a5\u6536\u529f\u7387\u7ea6\u675f\uff08\u533a\u57df\u7ea6\u675f\uff09\uff0c\u8fdb\u884c\u5355\u7528\u6237Massive MIMO\u5bb9\u91cf\u5206\u6790\u548c\u591a\u7528\u6237Massive MIMO\u7cfb\u7edf\u548c\u901f\u7387\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u7f16\u7801\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5141\u8bb8\u5728\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u7684\u540c\u65f6\u5229\u7528\u65b0\u9891\u6bb5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u9884\u7f16\u7801\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5e72\u6270\u7684\u6709\u6548\u63a7\u5236\u3002", "conclusion": "\u57fa\u4e8e\u901a\u4fe1\u7406\u8bba\u7684\u533a\u57df\u7ea6\u675f\u65b9\u6cd5\u6bd4\u5730\u7406\u9694\u79bb\u533a\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4fdd\u62a4\u4f20\u7edf\u7528\u6237\u514d\u53d7\u65b0\u9891\u6bb5\u4f7f\u7528\u7684\u5e72\u6270\uff0c\u4e3a\u9ad8\u901f\u65e0\u7ebf\u901a\u4fe1\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03021", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03021", "abs": "https://arxiv.org/abs/2509.03021", "authors": ["Ryandhimas E. Zezario", "Dyah A. M. G. Wisnu", "Hsin-Min Wang", "Yu Tsao"], "title": "A Study on Zero-Shot Non-Intrusive Speech Intelligibility for Hearing Aids Using Large Language Models", "comment": "Accepted to IEEE ICCE-TW 2025", "summary": "This work focuses on zero-shot non-intrusive speech assessment for hearing\naids (HA) using large language models (LLMs). Specifically, we introduce\nGPT-Whisper-HA, an extension of GPT-Whisper, a zero-shot non-intrusive speech\nassessment model based on LLMs. GPT-Whisper-HA is designed for speech\nassessment for HA, incorporating MSBG hearing loss and NAL-R simulations to\nprocess audio input based on each individual's audiogram, two automatic speech\nrecognition (ASR) modules for audio-to-text representation, and GPT-4o to\npredict two corresponding scores, followed by score averaging for the final\nestimated score. Experimental results indicate that GPT-Whisper-HA achieves a\n2.59% relative root mean square error (RMSE) improvement over GPT-Whisper,\nconfirming the potential of LLMs for zero-shot speech assessment in predicting\nsubjective intelligibility for HA users.", "AI": {"tldr": "\u57fa\u4e8eLLM\u7684\u96f6\u6837\u672c\u975e\u4fb5\u5165\u5f0f\u8bed\u97f3\u8bc4\u4f30\u6a21\u578bGPT-Whisper-HA\uff0c\u901a\u8fc7\u7ed3\u5408\u8033\u673a\u6a21\u62df\u548cASR\u6a21\u5757\uff0c\u5728\u9884\u6d4b\u8033\u673a\u7528\u6237\u8bed\u97f3\u53ef\u61c2\u6027\u65b9\u9762\u8f83GPT-Whisper\u63d0\u53472.59%\u7684RMSE\u653b\u8fc7\u6539\u5584", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u96f6\u6837\u672c\u975e\u4fb5\u5165\u5f0f\u7684\u8bed\u97f3\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u9884\u6d4b\u8033\u673a\u7528\u6237\u7684\u4e3b\u89c2\u8bed\u97f3\u53ef\u61c2\u6027\uff0c\u5145\u5206\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b", "method": "\u6269\u5c55GPT-Whisper\u6a21\u578b\uff0c\u7ed3\u5408MSBG\u804c\u542c\u635f\u5931\u548cNAL-R\u6a21\u62df\u5904\u7406\u97f3\u9891\u8f93\u5165\uff0c\u4f7f\u7528\u4e24\u4e2a\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u6a21\u5757\u8fdb\u884c\u97f3\u9891\u5230\u6587\u672c\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528GPT-4o\u9884\u6d4b\u4e24\u4e2a\u5bf9\u5e94\u5206\u6570\uff0c\u6700\u7ec8\u901a\u8fc7\u5206\u6570\u5e73\u5747\u83b7\u5f97\u4f30\u8ba1\u5206\u6570", "result": "GPT-Whisper-HA\u5728\u76f8\u5bf9\u6839\u5747\u65b9\u8bef(RMSE)\u4e0a\u6bd4GPT-Whisper\u63d0\u5347\u4e862.59%\uff0c\u9a8c\u8bc1\u4e86LLM\u5728\u8033\u673a\u96f6\u6837\u672c\u8bed\u97f3\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u975e\u4fb5\u5165\u5f0f\u8033\u673a\u8bed\u97f3\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\uff0cGPT-Whisper-HA\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8033\u673a\u7528\u6237\u7684\u4e3b\u89c2\u8bed\u97f3\u53ef\u61c2\u6027"}}
{"id": "2509.03038", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03038", "abs": "https://arxiv.org/abs/2509.03038", "authors": ["Ruihong Jiang", "Ruichen Zhang", "Yanqing Xu", "Huimin Hu", "Yang Lu", "Dusit Niyato"], "title": "Spatially Adaptive SWIPT with Pinching Antenna under Probabilistic LoS Blockage", "comment": "5 pages, 4 figures", "summary": "This paper considers a power-splitting (PS)-based simultaneous wireless\ninformation and power transfer (SWIPT) system employing a reconfigurable\npinching antenna (PA) under probabilistic line-of-sight (LoS) blockage. We\nformulate a joint optimization of the PA position and the PS ratio to maximize\nthe average signal-to-noise ratio (SNR) at a user, subject to its average\nenergy harvesting (EH) and PA placement limits. We derive a closed-form optimal\nsolution. Results demonstrate that the EH requirement has a deterministic\nimpact on the optimal PA position as well as its feasible region, requiring\ndeployment of the PA as close to the user as possible to maximize average\nchannel gain. This spatial adaptation, combined with dynamic PS, enables robust\nSWIPT performance in the presence of probabilistic LoS blockage, revealing that\nmechanical reconfigurability primarily enhances sustainability by ensuring\nenergy feasibility in dynamic environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6982\u7387\u6027\u89c6\u7ebf\u963b\u585e\u73af\u5883\u4e0b\uff0c\u4f7f\u7528\u53ef\u91cd\u914d\u5236\u62c9\u62c9\u5929\u7ebf\u7684\u529b\u91cf\u5206\u5272\u57fa\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\u7cfb\u7edf\uff0c\u901a\u8fc7\u5173\u952e\u53c2\u6570\u4f18\u5316\u63d0\u5347\u5e73\u5747\u4fe1\u566a\u6bd4\u548c\u80fd\u91cf\u6536\u96c6\u6027\u80fd\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u89c6\u7ebf\u963b\u585e\u7684\u6982\u7387\u6027\u8d28\u5f71\u54cd\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u9700\u8981\u901a\u8fc7\u5929\u7ebf\u4f4d\u7f6e\u548c\u529b\u91cf\u5206\u5272\u6bd4\u7684\u52a8\u6001\u8c03\u6574\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u5173\u4e8e\u53ef\u91cd\u914d\u5236\u62c9\u62c9\u5929\u7ebf\u4f4d\u7f6e\u548c\u529b\u91cf\u5206\u5272\u6bd4\u7684\u805a\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u6700\u5927\u5316\u7528\u6237\u7684\u5e73\u5747\u4fe1\u566a\u6bd4\uff0c\u5e76\u6ee1\u8db3\u5e73\u5747\u80fd\u91cf\u6536\u96c6\u548c\u5929\u7ebf\u90e8\u7f72\u9650\u5236\u3002\u63a8\u5bfc\u51fa\u4e86\u95ed\u5f0f\u7684\u6700\u4f18\u89e3\u3002", "result": "\u7ed3\u679c\u663e\u793a\u80fd\u91cf\u6536\u96c6\u8981\u6c42\u5bf9\u6700\u4f18\u5929\u7ebf\u4f4d\u7f6e\u53ca\u5176\u53ef\u884c\u57df\u6709\u786e\u5b9a\u6027\u5f71\u54cd\uff0c\u9700\u8981\u5c06\u5929\u7ebf\u5c3d\u53ef\u80fd\u9760\u8fd1\u7528\u6237\u4ee5\u6700\u5927\u5316\u5e73\u5747\u901a\u9053\u589e\u76ca\u3002", "conclusion": "\u7a7a\u95f4\u9002\u914d\u4e0e\u52a8\u6001\u529b\u91cf\u5206\u5272\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u6982\u7387\u6027\u89c6\u7ebf\u963b\u585e\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u540c\u65f6\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\u6027\u80fd\u3002\u673a\u68b0\u53ef\u91cd\u914d\u6027\u4e3b\u8981\u901a\u8fc7\u786e\u4fdd\u52a8\u6001\u73af\u5883\u4e2d\u7684\u80fd\u91cf\u53ef\u884c\u6027\u6765\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2509.03292", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03292", "abs": "https://arxiv.org/abs/2509.03292", "authors": ["Dyah A. M. G. Wisnu", "Ryandhimas E. Zezario", "Stefano Rini", "Hsin-Min Wang", "Yu Tsao"], "title": "Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings", "comment": "Accepted by IEEE Automatic Speech Recognition and Understanding\n  Workshop(ASRU), 2025", "summary": "We present a system for automatic multi-axis perceptual quality prediction of\ngenerative audio, developed for Track 2 of the AudioMOS Challenge 2025. The\ntask is to predict four Audio Aesthetic Scores--Production Quality, Production\nComplexity, Content Enjoyment, and Content Usefulness--for audio generated by\ntext-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. A\nmain challenge is the domain shift between natural training data and synthetic\nevaluation data. To address this, we combine BEATs, a pretrained\ntransformer-based audio representation model, with a multi-branch long\nshort-term memory (LSTM) predictor and use a triplet loss with buffer-based\nsampling to structure the embedding space by perceptual similarity. Our results\nshow that this improves embedding discriminability and generalization, enabling\ndomain-robust audio quality assessment without synthetic training data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBEATs\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u591a\u5206\u652fLSTM\u7684\u97f3\u9891\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u548c\u7f13\u51b2\u533a\u91c7\u6837\u6765\u89e3\u51b3\u81ea\u7136\u8bad\u7ec3\u6570\u636e\u4e0e\u5408\u6210\u8bc4\u4f30\u6570\u636e\u4e4b\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3AudioMOS Challenge 2025\u4e2d\u591a\u8f74\u611f\u77e5\u8d28\u91cf\u9884\u6d4b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u81ea\u7136\u8bad\u7ec3\u6570\u636e\u4e0e\u5408\u6210\u8bc4\u4f30\u6570\u636e\u4e4b\u95f4\u7684\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u7ed3\u5408BEATs\u9884\u8bad\u7ec3transformer\u97f3\u9891\u8868\u793a\u6a21\u578b\u548c\u591a\u5206\u652fLSTM\u9884\u6d4b\u5668\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u548c\u7f13\u51b2\u533a\u91c7\u6837\u6765\u6784\u5efa\u611f\u77e5\u76f8\u4f3c\u6027\u7684\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5d4c\u5165\u53ef\u533a\u5206\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5408\u6210\u8bad\u7ec3\u6570\u636e\u7684\u9886\u57df\u9c81\u68d2\u97f3\u9891\u8d28\u91cf\u8bc4\u4f30\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728\u751f\u6210\u97f3\u9891\u7684\u591a\u8f74\u8d28\u91cf\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03066", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03066", "abs": "https://arxiv.org/abs/2509.03066", "authors": ["Huaicheng Zhang", "Ruoxin Wang", "Chenlian Zhou", "Jiguang Shi", "Yue Ge", "Zhoutong Li", "Sheng Chang", "Hao Wang", "Jin He", "Qijun Huang"], "title": "S2M2ECG: Spatio-temporal bi-directional State Space Model Enabled Multi-branch Mamba for ECG", "comment": null, "summary": "As one of the most effective methods for cardiovascular disease (CVD)\ndiagnosis, multi-lead Electrocardiogram (ECG) signals present a characteristic\nmulti-sensor information fusion challenge that has been continuously researched\nin deep learning domains. Despite the numerous algorithms proposed with\ndifferent DL architectures, maintaining a balance among performance,\ncomputational complexity, and multi-source ECG feature fusion remains\nchallenging. Recently, state space models (SSMs), particularly Mamba, have\ndemonstrated remarkable effectiveness across various fields. Their inherent\ndesign for high-efficiency computation and linear complexity makes them\nparticularly suitable for low-dimensional data like ECGs. This work proposes\nS2M2ECG, an SSM architecture featuring three-level fusion mechanisms: (1)\nSpatio-temporal bi-directional SSMs with segment tokenization for low-level\nsignal fusion, (2) Intra-lead temporal information fusion with bi-directional\nscanning to enhance recognition accuracy in both forward and backward\ndirections, (3) Cross-lead feature interaction modules for spatial information\nfusion. To fully leverage the ECG-specific multi-lead mechanisms inherent in\nECG signals, a multi-branch design and lead fusion modules are incorporated,\nenabling individual analysis of each lead while ensuring seamless integration\nwith others. Experimental results reveal that S2M2ECG achieves superior\nperformance in the rhythmic, morphological, and clinical scenarios. Moreover,\nits lightweight architecture ensures it has nearly the fewest parameters among\nexisting models, making it highly suitable for efficient inference and\nconvenient deployment. Collectively, S2M2ECG offers a promising alternative\nthat strikes an excellent balance among performance, computational complexity,\nand ECG-specific characteristics, paving the way for high-performance,\nlightweight computations in CVD diagnosis.", "AI": {"tldr": "S2M2ECG\u662f\u4e00\u4e2a\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u7684\u65b0\u578b\u591a\u5bfc\u8054\u5fc3\u7535\u56fe\u5206\u6790\u67b6\u6784\uff0c\u901a\u8fc7\u4e09\u7ea7\u878d\u5408\u673a\u5236\u5b9e\u73b0\u9ad8\u6027\u80fd\u3001\u4f4e\u590d\u6742\u5ea6\u7684CVD\u8bca\u65ad", "motivation": "\u89e3\u51b3\u591a\u5bfc\u8054ECG\u4fe1\u53f7\u5206\u6790\u4e2d\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u591a\u6e90\u7279\u5f81\u878d\u5408\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5229\u7528SSM\u6a21\u578b\u7684\u9ad8\u6548\u8ba1\u7b97\u7279\u6027\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u4f18\u52bf", "method": "\u63d0\u51fa\u4e09\u7ea7\u878d\u5408\u673a\u5236\uff1a1)\u65f6\u7a7a\u53cc\u5411SSM\u4e0e\u5206\u6bb5\u6807\u8bb0\u5316\u7684\u4f4e\u5c42\u4fe1\u53f7\u878d\u5408\uff1b2)\u5bfc\u8054\u5185\u53cc\u5411\u626b\u63cf\u7684\u65f6\u95f4\u4fe1\u606f\u878d\u5408\uff1b3)\u8de8\u5bfc\u8054\u7279\u5f81\u4ea4\u4e92\u7684\u7a7a\u95f4\u4fe1\u606f\u878d\u5408\u3002\u91c7\u7528\u591a\u5206\u652f\u8bbe\u8ba1\u548c\u5bfc\u8054\u878d\u5408\u6a21\u5757", "result": "S2M2ECG\u5728\u8282\u5f8b\u3001\u5f62\u6001\u548c\u4e34\u5e8a\u573a\u666f\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u53c2\u6570\u91cf\u6700\u5c11\uff0c\u9002\u5408\u9ad8\u6548\u63a8\u7406\u548c\u4fbf\u6377\u90e8\u7f72", "conclusion": "S2M2ECG\u5728\u6027\u80fd\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548cECG\u7279\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u4e3a\u9ad8\u6027\u80fd\u8f7b\u91cf\u5316CVD\u8bca\u65ad\u8ba1\u7b97\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2509.03372", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03372", "abs": "https://arxiv.org/abs/2509.03372", "authors": ["Tien-Hong Lo", "Szu-Yu Chen", "Yao-Ting Sung", "Berlin Chen"], "title": "An Effective Strategy for Modeling Score Ordinality and Non-uniform Intervals in Automated Speaking Assessment", "comment": "Accepted at ASRU 2025", "summary": "A recent line of research on automated speaking assessment (ASA) has\nbenefited from self-supervised learning (SSL) representations, which capture\nrich acoustic and linguistic patterns in non-native speech without underlying\nassumptions of feature curation. However, speech-based SSL models capture\nacoustic-related traits but overlook linguistic content, while text-based SSL\nmodels rely on ASR output and fail to encode prosodic nuances. Moreover, most\nprior arts treat proficiency levels as nominal classes, ignoring their ordinal\nstructure and non-uniform intervals between proficiency labels. To address\nthese limitations, we propose an effective ASA approach combining SSL with\nhandcrafted indicator features via a novel modeling paradigm. We further\nintroduce a multi-margin ordinal loss that jointly models both the score\nordinality and non-uniform intervals of proficiency labels. Extensive\nexperiments on the TEEMI corpus show that our method consistently outperforms\nstrong baselines and generalizes well to unseen prompts.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u76db\u76db\u76db\u5b66\u4e60\u8868\u5f81\u548c\u624b\u5de5\u7279\u5f81\u7684\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8fb9\u964d\u5e8f\u5217\u635f\u5931\u6765\u6a21\u578b\u8bed\u8a00\u80fd\u529b\u7ea7\u522b\u7684\u987a\u5e8f\u6027\u548c\u975e\u5747\u5300\u95f4\u9694\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709ASA\u65b9\u6cd5\u7684\u4e24\u5927\u95ee\u9898\uff1a\u8bed\u97f3SSL\u6a21\u578b\u5ffd\u89c6\u8bed\u8a00\u5185\u5bb9\uff0c\u6587\u672cSSL\u6a21\u578b\u65e0\u6cd5\u7f16\u7801\u8bed\u8c03\u7ec6\u817b\uff0c\u4ee5\u53ca\u5c06\u80fd\u529b\u7ea7\u522b\u7b80\u5355\u5904\u7406\u4e3a\u540d\u4e49\u7c7b\u522b\u800c\u5ffd\u89c6\u5176\u987a\u5e8f\u7ed3\u6784\u3002", "method": "\u7ed3\u5408\u81ea\u76db\u76db\u76db\u5b66\u4e60\u8868\u5f81\u548c\u624b\u5de5\u6307\u6807\u7279\u5f81\uff0c\u91c7\u7528\u65b0\u7684\u6a21\u578b\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u591a\u8fb9\u964d\u5e8f\u5217\u635f\u5931\u6765\u8054\u5408\u6a21\u578b\u80fd\u529b\u7ea7\u522b\u7684\u987a\u5e8f\u6027\u548c\u975e\u5747\u5300\u95f4\u9694\u3002", "result": "\u5728TEEMI\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4e00\u8d2f\u5730\u8d85\u8fc7\u4e86\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u591f\u827e\u597d\u5730\u6f14\u7eed\u5230\u672a\u89c1\u8fc7\u7684\u8bdd\u9898\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u8bf4\u8bdd\u8bc4\u4f30\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u6e90\u7684\u7279\u5f81\u548c\u8003\u8651\u80fd\u529b\u7ea7\u522b\u7684\u987a\u5e8f\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6548\u679c\u3002"}}
{"id": "2509.03070", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03070", "abs": "https://arxiv.org/abs/2509.03070", "authors": ["Po-Heng Chou", "Wei-Lung Mao", "Ru-Ping Lin"], "title": "YOLO-based Bearing Fault Diagnosis With Continuous Wavelet Transform", "comment": "5 pages, 2 figures, 2 tables", "summary": "This letter proposes a YOLO-based framework for spatial bearing fault\ndiagnosis using time-frequency spectrograms derived from continuous wavelet\ntransform (CWT). One-dimensional vibration signals are first transformed into\ntime-frequency spectrograms using Morlet wavelets to capture transient fault\nsignatures. These spectrograms are then processed by YOLOv9, v10, and v11\nmodels to classify fault types. Evaluated on three benchmark datasets,\nincluding Case Western Reserve University (CWRU), Paderborn University (PU),\nand Intelligent Maintenance System (IMS), the proposed CWT--YOLO pipeline\nachieves significantly higher accuracy and generalizability than the baseline\nMCNN--LSTM model. Notably, YOLOv11 reaches mAP scores of 99.4% (CWRU), 97.8%\n(PU), and 99.5% (IMS). In addition, its region-aware detection mechanism\nenables direct visualization of fault locations in spectrograms, offering a\npractical solution for condition monitoring in rotating machinery.", "AI": {"tldr": "\u57fa\u4e8eYOLO\u7684\u65f6\u9891\u8c31\u56fe\u8f74\u627f\u6545\u969c\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7CWT\u8f6c\u6362\u632f\u52a8\u4fe1\u53f7\u4e3a\u65f6\u9891\u8c31\u56fe\uff0c\u5229\u7528YOLOv9-v11\u6a21\u578b\u8fdb\u884c\u6545\u969c\u5206\u7c7b\u548c\u4f4d\u7f6e\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89c6\u5316\u6545\u969c\u4f4d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u7684\u8f74\u627f\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u901a\u7528\u6027\u548c\u53ef\u89c6\u5316\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u6545\u969c\u5206\u7c7b\u548c\u4f4d\u7f6e\u68c0\u6d4b\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Morlet\u5c0f\u6ce2\u5c06\u4e00\u7ef4\u632f\u52a8\u4fe1\u53f7\u8f6c\u6362\u4e3a\u65f6\u9891\u8c31\u56fe\uff0c\u7136\u540e\u91c7\u7528YOLOv9\u3001v10\u3001v11\u6a21\u578b\u8fdb\u884c\u6545\u969c\u5206\u7c7b\u68c0\u6d4b\uff0c\u5229\u7528\u5176\u533a\u57df\u611f\u77e5\u673a\u5236\u5b9e\u73b0\u6545\u969c\u4f4d\u7f6e\u53ef\u89c6\u5316\u3002", "result": "\u5728CWRU\u3001PU\u3001IMS\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6781\u9ad8\u7684mAP\u5206\u6570\uff1aYOLOv11\u5206\u522b\u83b7\u5f9799.4%\u300197.8%\u300199.5%\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebfMCNN-LSTM\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5CWT-YOLO\u6d41\u6c34\u7ebf\u4e3a\u65cb\u8f6c\u673a\u68b0\u72b6\u6001\u76d1\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89c6\u5316\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u6545\u969c\u7c7b\u578b\u8bc6\u522b\u548c\u4f4d\u7f6e\u786e\u5b9a\u3002"}}
{"id": "2509.03077", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03077", "abs": "https://arxiv.org/abs/2509.03077", "authors": ["Ogechukwu Kanu", "Ashkan Eshaghbeigi", "Hatem Abou-Zeid"], "title": "Self-supervised Radio Representation Learning: Can we Learn Multiple Tasks?", "comment": "7 pages, 7 figures, IEEE international conference on communication\n  2025", "summary": "Artificial intelligence (AI) is anticipated to play a pivotal role in 6G.\nHowever, a key challenge in developing AI-powered solutions is the extensive\ndata collection and labeling efforts required to train supervised deep learning\nmodels. To overcome this, self-supervised learning (SSL) approaches have\nrecently demonstrated remarkable success across various domains by leveraging\nlarge volumes of unlabeled data to achieve near-supervised performance. In this\npaper, we propose an effective SSL scheme for radio signal representation\nlearning using momentum contrast. By applying contrastive learning, our method\nextracts robust, transferable representations from a large real-world dataset.\nWe assess the generalizability of these learned representations across two\nwireless communications tasks: angle of arrival (AoA) estimation and automatic\nmodulation classification (AMC). Our results show that carefully designed\naugmentations and diverse data enable contrastive learning to produce\nhigh-quality, invariant latent representations. These representations are\neffective even with frozen encoder weights, and fine-tuning further enhances\nperformance, surpassing supervised baselines. To the best of our knowledge,\nthis is the first work to propose and demonstrate the effectiveness of\nself-supervised learning for radio signals across multiple tasks. Our findings\nhighlight the potential of self-supervised learning to transform AI for\nwireless communications by reducing dependence on labeled data and improving\nmodel generalization - paving the way for scalable foundational 6G AI models\nand solutions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u91cf\u5bf9\u6bd4\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6848\u7528\u4e8e\u65e0\u7ebf\u7535\u4fe1\u53f7\u8868\u793a\u5b66\u4e60\uff0c\u5728AoA\u4f30\u8ba1\u548cAMC\u5206\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf", "motivation": "\u89e3\u51b36G AI\u5f00\u53d1\u4e2d\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56", "method": "\u4f7f\u7528\u52a8\u91cf\u5bf9\u6bd4\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\u548c\u591a\u6837\u5316\u6570\u636e\u63d0\u53d6\u9c81\u68d2\u3001\u53ef\u8fc1\u79fb\u7684\u4fe1\u53f7\u8868\u793a", "result": "\u5b66\u4e60\u5230\u7684\u8868\u793a\u5728\u51bb\u7ed3\u7f16\u7801\u5668\u6743\u91cd\u65f6\u4ecd\u6709\u6548\uff0c\u5fae\u8c03\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u8d85\u8d8a\u76d1\u7763\u57fa\u7ebf\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6f5c\u529b\u901a\u8fc7\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u548c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6765\u53d8\u9769\u65e0\u7ebf\u901a\u4fe1AI\uff0c\u4e3a\u53ef\u6269\u5c55\u76846G\u57fa\u7840AI\u6a21\u578b\u94fa\u5e73\u9053\u8def"}}
{"id": "2509.03111", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03111", "abs": "https://arxiv.org/abs/2509.03111", "authors": ["Hao Yang", "Guang Ouyang"], "title": "Handwriting Imagery EEG Classification based on Convolutional Neural Networks", "comment": null, "summary": "Handwriting imagery has emerged as a promising paradigm for brain-computer\ninterfaces (BCIs) aimed at translating brain activity into text output.\nCompared with invasively recorded electroencephalography (EEG), non-invasive\nrecording offers a more practical and feasible approach to capturing brain\nsignals for BCI. This study explores the limit of decoding non-invasive EEG\nassociated with handwriting imagery into English letters using deep neural\nnetworks. To this end, five participants were instructed to imagine writing the\n26 English letters with their EEG being recorded from the scalp. A measurement\nof EEG similarity across letters was conducted to investigate letter-specific\npatterns in the dataset. Subsequently, four convolutional neural network (CNN)\nmodels were trained for EEG classification. Descriptively, the EEG data clearly\nexhibited letter-specific patterns serving as a proof-of-concept for\nEEG-to-text translation. Under the chance level of accuracy at 3.85%, the CNN\nclassifiers trained on each participant reached the highest limit of around\n20%. This study marks the first attempt to decode non-invasive EEG associated\nwith handwriting imagery. Although the achieved accuracy is not sufficient for\na usable brain-to-text BCI, the model's performance is noteworthy in revealing\nthe potential for translating non-invasively recorded brain signals into text\noutputs and establishing a baseline for future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89e3\u7801\u4e0e\u975e\u4fb5\u5165\u5f0f\u8111\u7535\u56fe\u76f8\u5173\u7684\u624b\u5199\u60f3\u8c61\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u82f1\u6587\u5b57\u6bcd\u7684\u6781\u9650\uff0c\u9996\u6b21\u5c1d\u8bd5\u89e3\u7801\u4e0e\u975e\u4fb5\u5165\u5f0f\u8111\u7535\u56fe\u76f8\u5173\u7684\u624b\u5199\u60f3\u8c61", "motivation": "\u624b\u5199\u60f3\u8c61\u5df2\u6210\u4e3a\u8111\u673a\u63a5\u53e3\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u975e\u4fb5\u5165\u5f0f\u8bb0\u5f55\u6bd4\u4fb5\u5165\u5f0f\u8111\u7535\u56fe\u8bb0\u5f55\u66f4\u5b9e\u7528\u53ef\u884c\uff0c\u9700\u8981\u63a2\u7d22\u5176\u89e3\u7801\u6781\u9650", "method": "5\u540d\u53c2\u4e0e\u8005\u60f3\u8c61\u4e66\u519926\u4e2a\u82f1\u6587\u5b57\u6bcd\u65f6\u8bb0\u5f55\u5934\u76ae\u8111\u7535\u56fe\uff0c\u6d4b\u91cf\u8111\u7535\u56fe\u76f8\u4f3c\u6027\u5206\u6790\u5b57\u6bcd\u7279\u5b9a\u6a21\u5f0f\uff0c\u8bad\u7ec34\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u8111\u7535\u56fe\u5206\u7c7b", "result": "\u8111\u7535\u56fe\u6570\u636e\u660e\u663e\u8868\u73b0\u51fa\u5b57\u6bcd\u7279\u5b9a\u6a21\u5f0f\uff0cCNN\u5206\u7c7b\u5668\u57283.85%\u7684\u968f\u673a\u6c34\u5e73\u4e0b\u8fbe\u5230\u7ea620%\u7684\u6700\u9ad8\u51c6\u786e\u7387", "conclusion": "\u867d\u7136\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u7528\u4e8e\u5b9e\u7528\u7684\u8111\u5230\u6587\u672c\u8111\u673a\u63a5\u53e3\uff0c\u4f46\u6a21\u578b\u6027\u80fd\u663e\u8457\uff0c\u63ed\u793a\u4e86\u5c06\u975e\u4fb5\u5165\u5f0f\u8bb0\u5f55\u7684\u8111\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6587\u672c\u8f93\u51fa\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u7ebf"}}
{"id": "2509.03193", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03193", "abs": "https://arxiv.org/abs/2509.03193", "authors": ["Maximilian Neidhardt", "Sarah Latus", "Tim Eixmann", "Gereon H\u00fcttmann", "Alexander Schlaefer"], "title": "Deep Learning for High Speed Optical Coherence Elastography with a Fiber Scanning Endoscope", "comment": null, "summary": "Tissue stiffness is related to soft tissue pathologies and can be assessed\nthrough palpation or via clinical imaging systems, e.g., ultrasound or magnetic\nresonance imaging. Typically, the image based approaches are not suitable\nduring interventions, particularly for minimally invasive surgery. To this end,\nwe present a miniaturized fiber scanning endoscope for fast and localized\nelastography. Moreover, we propose a deep learning based signal processing\npipeline to account for the intricate data and the need for real-time\nestimates. Our elasticity estimation approach is based on imaging complex and\ndiffuse wave fields that encompass multiple wave frequencies and propagate in\nvarious directions. We optimize the probe design to enable different scan\npatterns. To maximize temporal sampling while maintaining three-dimensional\ninformation we define a scan pattern in a conical shape with a temporal\nfrequency of 5.05 kHz. To efficiently process the image sequences of complex\nwave fields we consider a spatio-temporal deep learning network. We train the\nnetwork in an end-to-end fashion on measurements from phantoms representing\nmultiple elasticities. The network is used to obtain localized and robust\nelasticity estimates, allowing to create elasticity maps in real-time. For 2D\nscanning, our approach results in a mean absolute error of 6.31+-5.76 kPa\ncompared to 11.33+-12.78 kPa for conventional phase tracking. For scanning\nwithout estimating the wave direction, the novel 3D method reduces the error to\n4.48+-3.63 kPa compared to 19.75+-21.82 kPa for the conventional 2D method.\nFinally, we demonstrate feasibility of elasticity estimates in ex-vivo porcine\ntissue.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5c0f\u578b\u5149\u7ea4\u626b\u63cf\u5185\u955c\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u5f39\u6027\u6210\u50cf\uff0c\u5728\u5e7b\u5f71\u548c\u732a\u7ec7\u7ec7\u4e0a\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6d4b\u91cf\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u57fa\u4e8e\u8f6f\u7ec7\u7ec7\u5f39\u6027\u8bc4\u4f30\u65b9\u6cd5\u5728\u5fae\u4f24\u624b\u672f\u4e2d\u4e0d\u9002\u7528\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4ecb\u5165\u6027\u64cd\u4f5c\u4e2d\u8fdb\u884c\u5feb\u901f\u3001\u5c40\u90e8\u5316\u5f39\u6027\u6d4b\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u5c0f\u578b\u5316\u5149\u7ea4\u626b\u63cf\u5185\u955c\uff0c\u91c7\u7528\u9525\u5f62\u626b\u63cf\u6a21\u5f0f\uff085.05 kHz\uff09\uff0c\u5e76\u4f7f\u7528\u7a7a\u95f4-\u65f6\u95f4\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u5904\u7406\u590d\u6742\u6ce2\u573a\u56fe\u50cf\u5e8f\u5217\uff0c\u5728\u591a\u79cd\u5f39\u6027\u5e7b\u5f71\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u57282D\u626b\u63cf\u4e2d\uff0c\u65b9\u6cd5\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a6.31\u00b15.76 kPa\uff0c\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u76f8\u4f4d\u8ddf\u8e2a\u65b9\u6cd5\u768411.33\u00b112.78 kPa\uff1b\u57283D\u626b\u63cf\u4e2d\uff0c\u8bef\u5dee\u964d\u4f4e\u52304.48\u00b13.63 kPa\uff0c\u8fdc\u4f4e\u4e8e\u4f20\u7edf2D\u65b9\u6cd5\u768419.75\u00b121.82 kPa\uff0c\u5e76\u5728\u79bb\u4f53\u732a\u7ec7\u7ec7\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5c0f\u578b\u5316\u5149\u7ea4\u626b\u63cf\u5185\u955c\u7cfb\u7edf\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u80fd\u591f\u5728\u5fae\u4f24\u624b\u672f\u4e2d\u5b9e\u73b0\u5feb\u901f\u3001\u51c6\u786e\u7684\u5c40\u90e8\u5f39\u6027\u6210\u50cf\uff0c\u4e3a\u8f6f\u7ec7\u7ec7\u75c5\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u624b\u6bb5\u3002"}}
{"id": "2509.03273", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03273", "abs": "https://arxiv.org/abs/2509.03273", "authors": ["Zeyuan Zhang", "Yue Xiu", "Zheng Dong", "Jiacheng Yin", "Maurice J. Khabbaz", "Chadi Assi", "Ning Wei"], "title": "Crosstalk-Resilient Beamforming for Movable Antenna Enabled Integrated Sensing and Communication", "comment": null, "summary": "This paper investigates a movable antenna (MA) enabled integrated sensing and\ncommunication (ISAC) system under the influence of antenna crosstalk. First, it\ngeneralizes the antenna crosstalk model from the conventional fixed-position\nantenna (FPA) system to the MA scenario. Then, a Cramer-Rao bound (CRB)\nminimization problem driven by joint beamforming and antenna position design is\npresented. Specifically, to address this highly non-convex flexible beamforming\nproblem, we deploy a deep reinforcement learning (DRL) approach to train a\nflexible beamforming agent. To ensure stability during training, a Twin Delayed\nDeep Deterministic Policy Gradient (TD3) algorithm is adopted to balance\nexploration with reward maximization for efficient and reliable learning.\nNumerical results demonstrate that the proposed crosstalk-resilient (CR)\nalgorithm enhances the overall ISAC performance compared to other benchmark\nschemes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5b58\u5728\u5929\u7ebf\u4e32\u6270\u7684\u53ef\u79fb\u52a8\u5929\u7ebf(MA)\u96c6\u6210\u611f\u77e5\u901a\u4fe1(ISAC)\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6297\u4e32\u6270\u6ce2\u675f\u6210\u5f62\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf(FPA)\u7cfb\u7edf\u7684\u5929\u7ebf\u4e32\u6270\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\u573a\u666f\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6297\u4e32\u6270\u7b97\u6cd5\u6765\u63d0\u5347ISAC\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5c06\u5929\u7ebf\u4e32\u6270\u6a21\u578b\u63a8\u5e7f\u5230MA\u573a\u666f\uff0c\u91c7\u7528Twin Delayed Deep Deterministic Policy Gradient (TD3)\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u8bad\u7ec3\u7075\u6d3b\u7684\u6ce2\u675f\u6210\u5f62\u4ee3\u7406\uff0c\u89e3\u51b3\u9ad8\u5ea6\u975e\u51f8\u7684\u8054\u5408\u6ce2\u675f\u6210\u5f62\u548c\u5929\u7ebf\u4f4d\u7f6e\u8bbe\u8ba1\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6297\u4e32\u6270(CR)\u7b97\u6cd5\u76f8\u6bd4\u5176\u4ed6\u57fa\u51c6\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53ISAC\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6297\u4e32\u6270\u6ce2\u675f\u6210\u5f62\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406MA-enabled ISAC\u7cfb\u7edf\u4e2d\u7684\u5929\u7ebf\u4e32\u6270\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u79fb\u52a8\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03311", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03311", "abs": "https://arxiv.org/abs/2509.03311", "authors": ["Penggao Yan", "Li-Ta Hsu"], "title": "Credible Uncertainty Quantification under Noise and System Model Mismatch", "comment": "This manuscript has been submitted to IEEE Signal Processing Letters", "summary": "State estimators often provide self-assessed uncertainty metrics, such as\ncovariance matrices, whose reliability is critical for downstream tasks.\nHowever, these self-assessments can be misleading due to underlying modeling\nviolations like noise or system model mismatch. This letter addresses the\nproblem of estimator credibility by introducing a unified, multi-metric\nevaluation framework. We construct a compact credibility portfolio that\nsynergistically combines traditional metrics like the Normalized Estimation\nError Squared (NEES) and the Noncredibility Index (NCI) with proper scoring\nrules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our\nkey contributions are a novel energy distance-based location test to robustly\ndetect system model misspecification and a method that leverages the asymmetric\nsensitivities of NLL and ES to distinguish optimism covariance scaling from\nsystem bias. Monte Carlo simulations across six distinct credibility scenarios\ndemonstrate that our proposed method achieves high classification accuracy\n(80-100%), drastically outperforming single-metric baselines which consistently\nfail to provide a complete and correct diagnosis. This framework provides a\npractical tool for turning patterns of credibility indicators into actionable\ndiagnoses of model deficiencies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u6307\u6807\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u72b6\u6001\u4f30\u8ba1\u5668\u7684\u53ef\u4fe1\u5ea6\uff0c\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u6307\u6807\u548c\u9002\u5f53\u8bc4\u5206\u89c4\u5219\uff0c\u80fd\u591f\u9ad8\u7cbe\u5ea6\u8bc6\u522b\u6a21\u578b\u7f3a\u9677\u7c7b\u578b\u3002", "motivation": "\u72b6\u6001\u4f30\u8ba1\u5668\u63d0\u4f9b\u7684\u81ea\u6211\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\uff08\u5982\u534f\u65b9\u5dee\u77e9\u9635\uff09\u53ef\u80fd\u56e0\u6a21\u578b\u8fdd\u53cd\u800c\u5bfc\u81f4\u8bef\u5bfc\uff0c\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u7ec4\u5408\uff0c\u7ed3\u5408\u4f20\u7edf\u6307\u6807\uff08NEES\u3001NCI\uff09\u548c\u9002\u5f53\u8bc4\u5206\u89c4\u5219\uff08NLL\u3001ES\uff09\uff0c\u5305\u62ec\u4e00\u79cd\u65b0\u9898\u7684\u80fd\u91cf\u8ddd\u79bb\u57fa\u4e8e\u4f4d\u7f6e\u6d4b\u8bd5\u548c\u5229\u7528NLL\u4e0eES\u4e0d\u5bf9\u79f0\u654f\u611f\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u53ef\u4fe1\u5ea6\u573a\u666f\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8680-100%\u7684\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u5355\u6307\u6807\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06\u53ef\u4fe1\u5ea6\u6307\u6807\u6a21\u5f0f\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6a21\u578b\u7f3a\u9677\u8bca\u65ad\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.03333", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.03333", "abs": "https://arxiv.org/abs/2509.03333", "authors": ["Tianfu Qi", "Jun Wang"], "title": "Baseband Model, Cutoff Rate Bounds and Constellation Shaping for Mixed Gaussian-Impulsive Noise", "comment": null, "summary": "Mixed noise, composed of white Gaussian noise (WGN) and impulsive noise (IN),\nappears in numerous communication scenarios and can severely degrade system\nperformance. In this paper, we address this issue by optimizing the transmitted\nconstellation under mixed noise based on a theoretical analysis of the cutoff\nrate (CR). First, starting from the passband model of the mixed noise, we\nderive its corresponding baseband representation. Due to the complexity of the\nCR, an exact analytic expression is generally intractable. Therefore, the\nbaseband noise model is employed to obtain closed-form lower and upper bounds\nof the CR. A piecewise linear approximation is applied to derive efficient\nbounds by exploiting the algebraic properties of the integral terms. These\nbounds are then used as criteria to optimize the transmitted constellation\npoints in both geometric and probabilistic distributions. The projected\ngradient method is employed to solve the optimization problem, and the\nconvergence and properties of the solutions are analyzed. Numerical results\ndemonstrate that the proposed CR bounds are tight and exhibit the expected\nasymptotic behavior. Furthermore, the optimized constellation scheme achieves a\nsignificant rate improvement compared to baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u6df7\u5408\u566a\u58f0\uff08\u9ad8\u65af\u767d\u566a\u58f0+\u8109\u51b2\u566a\u58f0\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u622a\u6b62\u7387\u6765\u4f18\u5316\u4f20\u8f93\u661f\u5ea7\u56fe\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u7d27\u81f4\u7684CR\u4e0a\u4e0b\u754c\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u6cd5\u4f18\u5316\u661f\u5ea7\u70b9\u7684\u51e0\u4f55\u548c\u6982\u7387\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u7387\u63d0\u5347\u3002", "motivation": "\u6df7\u5408\u566a\u58f0\uff08\u9ad8\u65af\u767d\u566a\u58f0\u548c\u8109\u51b2\u566a\u58f0\uff09\u5728\u4f17\u591a\u901a\u4fe1\u573a\u666f\u4e2d\u51fa\u73b0\uff0c\u4f1a\u4e25\u91cd\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u627e\u5230\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u79cd\u566a\u58f0\u73af\u5883\u3002", "method": "1. \u4ece\u6df7\u5408\u566a\u58f0\u7684\u5e26\u901a\u6a21\u578b\u63a8\u5bfc\u57fa\u5e26\u8868\u793a 2. \u5229\u7528\u57fa\u5e26\u566a\u58f0\u6a21\u578b\u83b7\u5f97\u622a\u6b62\u7387\u7684\u95ed\u5f0f\u4e0a\u4e0b\u754c 3. \u91c7\u7528\u5206\u6bb5\u7ebf\u6027\u8fd1\u4f3c\u5904\u7406\u79ef\u5206\u9879 4. \u4f7f\u7528\u6295\u5f71\u68af\u5ea6\u6cd5\u4f18\u5316\u661f\u5ea7\u70b9\u7684\u51e0\u4f55\u548c\u6982\u7387\u5206\u5e03", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684CR\u754c\u9650\u7d27\u81f4\u4e14\u5177\u6709\u9884\u671f\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u4f18\u5316\u540e\u7684\u661f\u5ea7\u65b9\u6848\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u7387\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u622a\u6b62\u7387\u5e76\u4f18\u5316\u661f\u5ea7\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u6df7\u5408\u566a\u58f0\u73af\u5883\uff0c\u63d0\u5347\u901a\u4fe1\u7cfb\u7edf\u6027\u80fd\uff0c\u6240\u63d0\u51fa\u7684\u754c\u9650\u548c\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.03488", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03488", "abs": "https://arxiv.org/abs/2509.03488", "authors": ["Miguel Rivas-Costa", "Carlos Mosquera"], "title": "Efficient DoA Estimation with Hybrid Linear and Rectangular Arrays Using Compact DFT Codebook", "comment": null, "summary": "Hybrid Analog and Digital (HAD) architectures provide a cost-effective\nalternative for large-scale antenna arrays, but accurate Direction-of-Arrival\n(DoA) estimation remains challenging due to limited digital dimensionality and\nconstrained beamforming design. In this work, we propose a HAD architecture\nthat employs Butler matrices to synthesize DFT beams over a uniform linear\narray. By exploiting the Cauchy-like displacement structure of the beamformed\nsignal, we introduce a second-order statistics estimation algorithm that\nachieves near-optimal accuracy, approaching the Cram\\'er-Rao Lower Bound (CRLB)\nand outperforming state-of-the-art methods in simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528Butler\u77e9\u9635\u7684\u6df7\u5408\u6a21\u62df\u6570\u5b57\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528\u6ce2\u675f\u6210\u5f62\u4fe1\u53f7\u7684\u67ef\u897f\u7c7b\u4f4d\u79fb\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1CRLB\u4e0b\u754c\u7684\u8fd1\u6700\u4f18DoA\u4f30\u8ba1\u7cbe\u5ea6", "motivation": "\u6df7\u5408\u6a21\u62df\u6570\u5b57\u67b6\u6784\u5728\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u4e2d\u5177\u6709\u6210\u672c\u6548\u76ca\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u6570\u5b57\u7ef4\u5ea6\u6709\u9650\u548c\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u53d7\u9650\uff0c\u51c6\u786e\u7684\u6ce2\u8fbe\u65b9\u5411\u4f30\u8ba1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u4f7f\u7528Butler\u77e9\u9635\u5728\u5747\u5300\u7ebf\u6027\u9635\u5217\u4e0a\u5408\u6210DFT\u6ce2\u675f\uff0c\u5e76\u5229\u7528\u6ce2\u675f\u6210\u5f62\u4fe1\u53f7\u7684\u67ef\u897f\u7c7b\u4f4d\u79fb\u7ed3\u6784\u5f00\u53d1\u4e8c\u9636\u7edf\u8ba1\u91cf\u4f30\u8ba1\u7b97\u6cd5", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1Cram\u00e9r-Rao\u4e0b\u754c\u7684\u8fd1\u6700\u4f18\u7cbe\u5ea6\uff0c\u5728\u4eff\u771f\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df7\u5408\u6a21\u62df\u6570\u5b57\u67b6\u6784\u63d0\u4f9b\u4e86\u6709\u6548\u7684DoA\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6210\u672c\u6548\u76ca\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u4f30\u8ba1\u6027\u80fd"}}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Multi-Sensor Fusion for Extended Object Tracking Exploiting Active and Passive Radio Signals](https://arxiv.org/abs/2509.03686)
*Hong Zhu,Alexander Venus,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: 提出了一种贝叶斯定位方法，通过融合主动测量和被动雷达式测量来解决用户身体遮挡LOS链路时的精确定位问题，使用多传感器概率数据关联算法处理测量来源不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决无线电设备定位中由于用户身体遮挡LOS链路导致的定位不可靠问题，传统方法忽略用户身体作为扩展物体对信号的散射、衰减和阻挡效应。

Method: 提出贝叶斯方法融合设备与锚点间的主动测量和锚点间经扩展物体反射的被动多基地雷达测量，开发多传感器多测量概率数据关联算法，建立针对人体用户的扩展物体模型。

Result: 在合成和真实无线电测量上的评估表明，该算法在遮挡LOS条件下优于基于点目标假设的传统PDA方法。

Conclusion: 所提出的方法能够有效处理用户身体遮挡导致的定位挑战，通过融合多类型测量和精确的扩展物体建模，显著提升了遮挡条件下的定位性能。

Abstract: Reliable and robust positioning of radio devices remains a challenging task
due to multipath propagation, hardware impairments, and interference from other
radio transmitters. A frequently overlooked but critical factor is the agent
itself, e.g., the user carrying the device, which potentially obstructs
line-of-sight (LOS) links to the base stations (anchors). This paper addresses
the problem of accurate positioning in scenarios where LOS links are partially
blocked by the agent. The agent is modeled as an extended object (EO) that
scatters, attenuates, and blocks radio signals. We propose a Bayesian method
that fuses ``active'' measurements (between device and anchors) with
``passive'' multistatic radar-type measurements (between anchors, reflected by
the EO). To handle measurement origin uncertainty, we introduce an multi-sensor
and multiple-measurement probabilistic data association (PDA) algorithm that
jointly fuses all EO-related measurements. Furthermore, we develop an EO model
tailored to agents such as human users, accounting for multiple reflections
scattered off the body surface, and propose a simplified variant for
low-complexity implementation. Evaluation on both synthetic and real radio
measurements demonstrates that the proposed algorithm outperforms conventional
PDA methods based on point target assumptions, particularly during and after
obstructed line-of-sight (OLOS) conditions.

</details>


### [2] [Sensor placement for sparse force reconstruction](https://arxiv.org/abs/2509.03825)
*Jeunghoon Lee*

Main category: eess.SP

TL;DR: 提出基于Gram矩阵的传感器布置策略，用于频域稀疏力重构，通过最小化Gram矩阵非对角能量来优化传感器位置选择


<details>
  <summary>Details</summary>
Motivation: 传统启发式传感器布置方法在力重构精度方面存在不足，需要基于物理洞察开发更有效的传感器优化布置框架

Method: 利用Gram矩阵的模态分解分析空间相关性，提出贪婪算法选择传感器位置以最小化Gram矩阵的非对角能量

Result: 数值模拟和实验验证表明，该方法相比启发式传感器布局能提供更鲁棒和准确的力估计

Conclusion: 基于Gram矩阵模态特性的传感器布置策略能有效提高频域稀疏力重构的精度和鲁棒性

Abstract: The present study proposes a Gram-matrix-based sensor placement strategy for
sparse force reconstruction in the frequency domain. A modal decomposition of
the Gram matrix reveals that its structure is dominated by a few modes near the
target frequency, and that each modal contribution reflects the spatial
correlation of the corresponding mode shape. This suggests that placing sensors
near nodal regions where spatial correlation is low can reduce coherence in the
frequency response function (FRF) matrix and improve force reconstruction
accuracy. To translate the physical insight into a practical design framework,
a greedy algorithm is proposed to select sensor locations that minimize the
off-diagonal energy of the Gram matrix. Numerical simulations and experimental
validations demonstrate that the proposed method yields robust and accurate
force estimation, outperforming heuristic sensor layouts.

</details>


### [3] [A Low-Cost Open-Source BLE-Based Asian Hornet Tracking System](https://arxiv.org/abs/2509.03979)
*Gilles Callebaut,Jan Van Moer*

Main category: eess.SP

TL;DR: 开发基于BLE的低成本开源跟踪系统，用于定位亚洲大黄蜂巢穴，通过自定义PN序列和相关检测实现50米角度分辨和360米通信范围


<details>
  <summary>Details</summary>
Motivation: 亚洲大黄蜂对生态系统和养蜂业构成严重威胁，传统人工三角定位方法耗时且效率低，需要开发更有效的跟踪系统

Method: 使用轻量级BLE标签和GNU Radio实现的SDR接收器，绕过BLE协议栈，在未编码PHY中嵌入自定义PN序列进行相关检测，采用Yagi天线和PlutoSDR进行数字波束扫描确定方向

Result: 现场测试显示系统在50米距离具有可靠的角度分辨率，通信范围可达360米

Conclusion: 虽然调制方式增加了接收器复杂度，但为多信道扩展和标签识别等未来改进提供了可能，系统完全开源，为黄蜂跟踪和环境监测相关应用提供了可扩展框架

Abstract: The Asian hornet (Vespa velutina) poses a serious threat to ecosystems and
beekeeping. Locating nests is essential, but usually involves time-consuming
manual triangulation. We present a low-cost, open-source tracking system based
on Bluetooth Low Energy (BLE). The system consists of a lightweight BLE tag and
a software-defined radio (SDR) receiver implemented in GNU Radio. By bypassing
the BLE stack, we embed a custom pseudo-noise (PN) sequence in the uncoded PHY
for correlation-based detection. Using a Yagi antenna and PlutoSDR, the
receiver performs digital beam sweeping to determine the tag's direction. Field
tests show reliable angular resolution at 50m and a communication range up to
360m. While our modulation increases receiver complexity, it enables future
improvements such as multichannel spreading and tag identification. The design
is fully open-source and provides a scalable framework for hornet tracking and
related applications in environmental monitoring.

</details>


### [4] [Approximate Message Passing for Multi-Preamble Detection in OTFS Random Access](https://arxiv.org/abs/2509.03980)
*Alessandro Mirri,Vishnu Teja Kunde,Enrico Paolini,Jean-Francois Chamberland*

Main category: eess.SP

TL;DR: 这篇论文提出了一种新的AMP算法，用于解决OTFS随机访问系统中的多重前导码检测问题，通过强化双重稀疏性来提高检测性能


<details>
  <summary>Details</summary>
Motivation: 解决OTFS随机访问系统中的多重前导码检测问题，该问题被形式化为复数域的结构化稀疏恢复问题

Method: 提出一种新的近似消息传递(AMP)算法，强化双重稀疏性：前导码的稀疏选择和OTFS信号在延迟-多普勒域的本质稀疏性，设计了新的AMP去噪器

Result: 模拟结果显示所提方法实现了稳健的检测性能，与现有最先进技术相比获得了显著的性能提升

Conclusion: 该方法通过创新的AMP算法设计，有效解决了OTFS系统中的多重前导码检测挑战，为随机访问系统提供了更优异的解决方案

Abstract: This article addresses the problem of multiple preamble detection in random
access systems based on orthogonal time frequency space (OTFS) signaling. This
challenge is formulated as a structured sparse recovery problem in the complex
domain. To tackle it, the authors propose a new approximate message passing
(AMP) algorithm that enforces double sparsity: the sparse selection of
preambles and the inherent sparsity of OTFS signals in the delay-Doppler
domain. From an algorithmic standpoint, the non-separable complex sparsity
constraint necessitates a careful derivation and leads to the design of a novel
AMP denoiser. Simulation results demonstrate that the proposed method achieves
robust detection performance and delivers significant gains over
state-of-the-art techniques.

</details>


### [5] [Joint Frequency-Space Sparse Reconstruction for DOA Estimation under Coherent Sources and Amplitude-Phase Errors](https://arxiv.org/abs/2509.03983)
*Yutong Chen,Cong Zhou,Changsheng You,Shuo Shi*

Main category: eess.SP

TL;DR: 通过额外源构建真实指向向量，结合频域和空间稀疏性，提出一种低复杂度的相关源DOA估计方法，有效对抗数组滤波错误


<details>
  <summary>Details</summary>
Motivation: 解决相关源情况下的DOA估计问题，并消除数组滤波器幅相错误的影响

Method: 利用辅助源构建真实指向向量，结合频域稀疏性和空间稀疏性进行稀疏重建，无需迭代优化

Result: 数值实验表明方法在相关源情况下获得更高的估计精度

Conclusion: 该方法有效解决了相关源和数组错误问题，具有低计算复杂度和高估计精度的优势

Abstract: In this letter, we propose a joint frequency-space sparse reconstruction
method for direction-of-arrival (DOA) estimation, which effectively addresses
the issues arising from the existence of coherent sources and array
amplitude-phase errors. Specifically, by using an auxiliary source with known
angles, we first construct the real steering vectors (RSVs) based on the
spectral peaks of received signals in the frequency domain, which serve as a
complete basis matrix for compensation for amplitude-phase errors. Then, we
leverage the spectral sparsity of snapshot data in the frequency domain and the
spatial sparsity of incident directions to perform the DOA estimation according
to the sparse reconstruction method. The proposed method does not require
iterative optimization, hence exhibiting low computational complexity.
Numerical results demonstrate that the proposed DOA estimation method achieves
higher estimation accuracy for coherent sources as compared to various
benchmark schemes.

</details>


### [6] [Robust MIMO Semantic Communication with Imperfect CSI via Knowledge Distillation](https://arxiv.org/abs/2509.04005)
*Mingze Gong,Shuoyao Wang,Shijian Gao,Jia Yan,Suzhi Bi*

Main category: eess.SP

TL;DR: HANA-JSCC是一个针对MIMO语义通信系统的图像传输方案，通过信道矩阵和噪声自适应机制解决实际信道估计误差问题


<details>
  <summary>Details</summary>
Motivation: 现有MIMO语义通信系统假设完美信道矩阵估计，这在硬件和导频开销限制下不切实际，需要解决信道估计误差问题

Method: 提出信道矩阵适配器与信道编解码器协作适应不准确的信道状态信息，并采用两阶段训练策略和知识蒸馏解决病态问题

Result: 在各种噪声和估计误差水平下，HANA-JSCC在多个数据集上比最先进基准平均性能高出0.40~0.54dB

Conclusion: 该方案有效缓解了MIMO语义通信系统中的信道估计误差影响，提升了系统在实际场景中的性能

Abstract: Semantic communication (SemComm) has emerged as a new communication paradigm.
To enhance efficiency, multiple-input-multiple-output (MIMO) technology has
been further integrated into SemComm systems. However, existing MIMO SemComm
systems assume perfect channel matrix estimation for channel-adaptive joint
source-channel coding, which is impractical due to hardware and pilot overhead
constraints. In this paper, we propose a semantic image transmission system
with channel matrix and channel noise adaptation, named HANA-JSCC, to cope with
channel estimation errors in MIMO systems. We propose a channel matrix adaptor
that collaborates with the channel codec to adapt to misaligned channel state
information, thereby mitigating the impact of estimation errors. Since the
relationship between the estimated channel matrix and true channel matrix is
ill-posed (one-to-many), we further introduce a two-stage training strategy
with knowledge distillation to overcome the convergence difficulties caused by
the ill-posed problem. Comparing with the state-of-the-art benchmarks,
HANA-JSCC achieves $0.40\sim0.54$dB higher average performance across various
noise and estimation error levels in various datasets.

</details>


### [7] [Constellation Shaping for OFDM-ISAC Systems: From Theoretical Bounds to Practical Implementation](https://arxiv.org/abs/2509.04055)
*Benedikt Geiger,Fan Liu,Shihang Lu,Andrej Rode,Daniel Gil Gaviria,Charlotte Muth,Laurent Schmalen*

Main category: eess.SP

TL;DR: 本文研究通过星座整形技术同时提升正交频分复用ISAC系统的通信和感知性能，提出了理论上下界、联合优化方法以及实用的概率幅度整形改进方案。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)系统需要复用通信信号进行雷达式感知，但感知和通信对调制格式有冲突要求，存在性能权衡问题，需要寻找同时提升两者性能的方法。

Method: 采用自编码器优化方法研究几何、概率和联合星座整形，推导了理论上下界，提出了适用于ISAC的广义概率幅度整形(PAS)方法及低复杂度对数似然比计算方案。

Result: 星座整形能够实现感知与通信性能的灵活权衡，接近理论上限，显著优于传统调制格式。广义PAS结合传统方法可实现低复杂度且接近联合整形性能的解决方案。

Conclusion: 星座整形是提升ISAC系统性能的有效方法，提出的广义PAS方案具有实际部署可行性，能够在低复杂度下实现接近最优的性能权衡。

Abstract: Integrated sensing and communications (ISAC) promises new use cases for
mobile communication systems by reusing the communication signal for radar-like
sensing. However, sensing and communications (S&C) impose conflicting
requirements on the modulation format, resulting in a tradeoff between their
corresponding performance. This paper investigates constellation shaping as a
means to simultaneously improve S&C performance in orthogonal frequency
division multiplexing (OFDM)-based ISAC systems. We begin by deriving how the
transmit symbols affect detection performance and derive theoretical lower and
upper bounds on the maximum achievable information rate under a given sensing
constraint. Using an autoencoder-based optimization, we investigate geometric,
probabilistic, and joint constellation shaping, where joint shaping combines
both approaches, employing both optimal maximum a-posteriori decoding and
practical bit-metric decoding. Our results show that constellation shaping
enables a flexible trade-off between S&C, can approach the derived upper bound,
and significantly outperforms conventional modulation formats. Motivated by its
practical implementation feasibility, we review probabilistic amplitude shaping
(PAS) and propose a generalization tailored to ISAC. For this generalization,
we propose a low-complexity log-likelihood ratio computation with negligible
rate loss. We demonstrate that combining conventional and generalized PAS
enables a flexible and low-complexity tradeoff between S&C, closely approaching
the performance of joint constellation shaping.

</details>


### [8] [Reliable Clutter Suppression for Slow-Moving Weak Target Radar Detection](https://arxiv.org/abs/2509.04309)
*R. Zhang,J. Xue,T. Zhang*

Main category: eess.SP

TL;DR: 基于Go分解(Godec)框架的新型杂波压制方案，解决复杂环境下慢速弱目标检测问题，比传统MTI方案更可靠但耗时更长


<details>
  <summary>Details</summary>
Motivation: 复杂环境中强反射体对慢速弱目标的掩蔽效应，传统MTI方法可能会压制到目标回波，影响检测性能

Method: 利用距离-速度图在不同雷达扫描中的低秩和稀疏特性，基于Go分解(Godec)框架构建杂波压制方案

Result: 模拟结果显示，在存在掩蔽效应时，Godec方案能可靠检测慢速弱目标，但耗时更长，并揭示了虚警像素数、检测概率和迭代次数之间的权衡关系

Conclusion: 该方案以提高时间复杂度为代价，换取了更高的检测可靠性，实验验证了方案的有效性和适用场景

Abstract: Reliable slow-moving weak target detection in complicated environments is
challenging due to the masking effects from the surrounding strong reflectors.
The traditional Moving Target Indication (MTI) may suppress the echoes from not
only the static interference objects (IOs), but also the desired slow-moving
weak target. According to the low-rank and sparse properties of the
range-velocity maps across different radar scans, a novel clutter suppression
scheme based on the Go decomposition (Godec) framework is proposed in this
paper. The simulation results show that with the existence of masking effects,
the target detection scheme based on Godec clutter suppression can reliably
detect the slow-moving weak target, compared to the traditional MTI-based
scheme. Besides, the time consumption comparison is conducted, demonstrating
that the proposed solution is one that sacrifices time complexity in exchange
for enhanced reliability. Additionally, the tradeoffs among the number of false
alarm cells, the detection probability and the iteration times for convergence
have been revealed, guiding parameter settings of the proposed solution in
practical applications. Experiment validation is also conducted to verify the
proposed solution, providing further insight into the scenarios where the
solution is most applicable.

</details>


### [9] [Relative Localization of UAV Swarms in GNSS-Denied Conditions](https://arxiv.org/abs/2509.04412)
*Guangyu Lei,Yuqi Ping,Tianhao Liang,Huahao Ding,Tingting Zhang*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于聚类的无人机群相对定位框架，通过通信信号同时进行通信和测距，解决了GNSS被废止环境下大规模无人机群的定位问题。


<details>
  <summary>Details</summary>
Motivation: 在GNSS被废止环境下，现有无人机群相对定位方法存在严重的定位误差和高计算复杂度问题，特别是在大规模群中因包丢失和计算复杂性导致的问题。

Method: 采用谱聚类技术将无人机群划分为不同子群，通过矩阵补全和多维尺度规划获得高精度相对坐标，然后通过集群间锚点融合构建全局地图。以OTFS技术为例实现通信与感知一体化系统。

Result: 实验结果显示，该方法能够有效减少大规模群中的定位误差和距离信息丢失问题，同时探索了信号参数对通信和定位性能的影响。

Conclusion: 该研究提供了一种高效的无人机群相对定位方案，显著提升了在复杂环境下的定位精度，并深入分析了通信与定位性能之间的相互关系。

Abstract: Relative localization of unmanned aerial vehicle (UAV) swarms in global
navigation satellite system (GNSS) denied environments is essential for
emergency rescue and battlefield reconnaissance. Existing methods suffer from
significant localization errors among UAVs due to packet loss and high
computational complexity in large swarms. This paper proposes a
clustering-based framework where the UAVs simultaneously use communication
signals for channel estimation and ranging. Firstly, the spectral clustering is
utilized to divide the UAV swarm into different sub-clusters, where matrix
completion and multidimensional scaling yield high-precision relative
coordinates. Subsequently, a global map is created by the inter-cluster anchor
fusion. A case study of UAV integrated communication and sensing (ISAC) system
is presented, where the Orthogonal Time Frequency Space (OTFS) is adopted for
ranging and communication. Experimental results show that the proposed method
reduces localization errors in large swarms and loss of range information. It
also explores the impact of signal parameters on communication and
localization, highlighting the interplay between communication and localization
performance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Hierarchical Sparse Sound Field Reconstruction with Spherical and Linear Microphone Arrays](https://arxiv.org/abs/2509.03902)
*Shunxi Xu,Craig T. Jin*

Main category: eess.AS

TL;DR: 这篇论文提出了一种两阶段稀疏恢复框架，结合球面麦克风数组和线性麦克风数组，在混响环境中显著提高声场分析的空间分辨率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 球面麦克风数组的空间分辨率受到球象和阶数的基本限制，且在混响环境中性能会降低，需要找到更有效的方法来提高声场分析的精度和耐受性。

Method: 提出两阶段稀疏恢复框架，将中央球面麦克风数组作为主要估计器，四周四个线性麦克风数组作为空间补充精炼器，通过残差精炼技术完善估计结果。

Result: 模拟结果显示，在不同混响条件下，该SMA-LMA方法在空间能量图重建方面显著优于单纯SMA方法和直接一步联合处理方法。

Conclusion: 该框架能够有效提高复杂声学环境中的空间保真度和稳健性，为高分辨率声场分析提供了有效的解决方案。

Abstract: Spherical microphone arrays (SMAs) are widely used for sound field analysis,
and sparse recovery (SR) techniques can significantly enhance their spatial
resolution by modeling the sound field as a sparse superposition of dominant
plane waves. However, the spatial resolution of SMAs is fundamentally limited
by their spherical harmonic order, and their performance often degrades in
reverberant environments. This paper proposes a two-stage SR framework with
residue refinement that integrates observations from a central SMA and four
surrounding linear microphone arrays (LMAs). The core idea is to exploit
complementary spatial characteristics by treating the SMA as a primary
estimator and the LMAs as a spatially complementary refiner. Simulation results
demonstrate that the proposed SMA-LMA method significantly enhances spatial
energy map reconstruction under varying reverberation conditions, compared to
both SMA-only and direct one-step joint processing. These results demonstrate
the effectiveness of the proposed framework in enhancing spatial fidelity and
robustness in complex acoustic environments.

</details>


### [11] [LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis](https://arxiv.org/abs/2509.04072)
*Gaspard Michel,Elena V. Epure,Christophe Cerisara*

Main category: eess.AS

TL;DR: LibriQuote是一个从有声读物中提取的英语语料库，包含12.7K小时非表达性语音和5.3K小时表达性语音，用于表达性零样本TTS系统的微调和基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语音数据集中表达性语音比例不明确的问题，现有表达性语音语料库规模较小且主要用于基准测试。

Method: 从有声读物中提取语音数据，表达性子集包含上下文信息和伪标签（如说话动词和副词），提供7.5小时测试集用于评估TTS系统在保持音色的同时合成表达性语音的能力。

Result: 定性验证显示测试集覆盖广泛的情感和口音，主观和客观评估表明在LibriQuote上微调TTS系统显著提高合成语音的清晰度，现有系统无法合成与真实语音一样表达性和自然的语音。

Conclusion: LibriQuote数据集为表达性TTS系统的开发和评估提供了有价值的资源，现有系统在表达性语音合成方面仍有改进空间。

Abstract: Text-to-speech (TTS) systems have recently achieved more expressive and
natural speech synthesis by scaling to large speech datasets. However, the
proportion of expressive speech in such large-scale corpora is often unclear.
Besides, existing expressive speech corpora are typically smaller in scale and
primarily used for benchmarking TTS systems. In this paper, we introduce the
LibriQuote dataset, an English corpus derived from read audiobooks, designed
for both fine-tuning and benchmarking expressive zero-shot TTS system. The
training dataset includes 12.7K hours of read, non-expressive speech and 5.3K
hours of mostly expressive speech drawn from character quotations. Each
utterance in the expressive subset is supplemented with the context in which it
was written, along with pseudo-labels of speech verbs and adverbs used to
describe the quotation (\textit{e.g. ``he whispered softly''}). Additionally,
we provide a challenging 7.5 hour test set intended for benchmarking TTS
systems: given a neutral reference speech as input, we evaluate system's
ability to synthesize an expressive utterance while preserving reference
timbre. We validate qualitatively the test set by showing that it covers a wide
range of emotions compared to non-expressive speech, along with various
accents. Extensive subjective and objective evaluations show that fine-tuning a
baseline TTS system on LibriQuote significantly improves its synthesized speech
intelligibility, and that recent systems fail to synthesize speech as
expressive and natural as the ground-truth utterances. The dataset and
evaluation code are freely available. Audio samples can be found at
https://libriquote.github.io/.

</details>


### [12] [Test-Time Adaptation for Speech Enhancement via Domain Invariant Embedding Transformation](https://arxiv.org/abs/2509.04280)
*Tobias Raichle,Niels Edinger,Bin Yang*

Main category: eess.AS

TL;DR: LaDen是首个专门为语音增强设计的测试时自适应方法，利用预训练语音表示进行潜在空间去噪，通过线性变换近似干净语音表示，无需目标域标注数据即可实现跨域伪标注和有效适配。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习语音增强模型在训练分布匹配时表现优异，但在真实世界不可预测环境中遇到域偏移时性能下降的问题。

Method: 利用预训练语音表示进行潜在去噪，通过线性变换将带噪嵌入转换为干净语音表示近似，实现跨域泛化和伪标注，支持测试时自适应。

Result: 在包含噪声类型、说话人特征和语言变化的多数据集综合基准测试中，LaDen在感知指标上持续优于基线方法，特别在说话人和语言域偏移方面表现突出。

Conclusion: LaDen通过潜在去噪和伪标注机制，有效解决了语音增强模型在真实环境中的域适应问题，为跨域语音增强提供了有效的测试时自适应解决方案。

Abstract: Deep learning-based speech enhancement models achieve remarkable performance
when test distributions match training conditions, but often degrade when
deployed in unpredictable real-world environments with domain shifts. To
address this challenge, we present LaDen (latent denoising), the first
test-time adaptation method specifically designed for speech enhancement. Our
approach leverages powerful pre-trained speech representations to perform
latent denoising, approximating clean speech representations through a linear
transformation of noisy embeddings. We show that this transformation
generalizes well across domains, enabling effective pseudo-labeling for target
domains without labeled target data. The resulting pseudo-labels enable
effective test-time adaptation of speech enhancement models across diverse
acoustic environments. We propose a comprehensive benchmark spanning multiple
datasets with various domain shifts, including changes in noise types, speaker
characteristics, and languages. Our extensive experiments demonstrate that
LaDen consistently outperforms baseline methods across perceptual metrics,
particularly for speaker and language domain shifts.

</details>


### [13] [Accelerated Interactive Auralization of Highly Reverberant Spaces using Graphics Hardware](https://arxiv.org/abs/2509.04390)
*Hannes Rosseel,Toon van Waterschoot*

Main category: eess.AS

TL;DR: 这篇论文提出了一种基于GPU加速的实时多通道扬声器音响渗染系统，用于高渗演空间的交互式音响模拟，解决传统CPU卷积计算负荷大、延迟高的问题。


<details>
  <summary>Details</summary>
Motivation: 交互式音响渗染技术可以重现虚拟音乐厅和历史宗教空间的音响效果，但高渗演空间需要长过渗激测试滤波器，导致传统CPU卷积计算计算负荷过大，影响实时性能。

Method: 开发了基于GPU加速的实时多通道扬声器音响渗染系统，对比了CPU和GPU卷积性能，并在GPU上集成了音响合成与反馈消除功能。

Result: GPU加速卷积方案能够实现实时性能，并且延迟显著降低，构建了一体化的扬声器基音响渗染框架。

Conclusion: GPU加速技术有效解决了高渗演空间音响模拟的计算性能瓶颈，提高了交互式音响渗染系统的实时性能。

Abstract: Interactive acoustic auralization allows users to explore virtual acoustic
environments in real-time, enabling the acoustic recreation of concert hall or
Historical Worship Spaces (HWS) that are either no longer accessible,
acoustically altered, or impractical to visit. Interactive acoustic synthesis
requires real-time convolution of input signals with a set of synthesis filters
that model the space-time acoustic response of the space. The acoustics in
concert halls and HWS are both characterized by a long reverberation time,
resulting in synthesis filters containing many filter taps. As a result, the
convolution process can be computationally demanding, introducing significant
latency that limits the real-time interactivity of the auralization system. In
this paper, the implementation of a real-time multichannel loudspeaker-based
auralization system is presented. This system is capable of synthesizing the
acoustics of highly reverberant spaces in real-time using GPU-acceleration. A
comparison between traditional CPU-based convolution and GPU-accelerated
convolution is presented, showing that the latter can achieve real-time
performance with significantly lower latency. Additionally, the system
integrates acoustic synthesis with acoustic feedback cancellation on the GPU,
creating a unified loudspeaker-based auralization framework that minimizes
processing latency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [14] [SwinSRGAN: Swin Transformer-based Generative Adversarial Network for High-Fidelity Speech Super-Resolution](https://arxiv.org/abs/2509.03913)
*Jiajun Yuan,Xiaochen Wang,Yuhang Xiao,Yulin Wu,Chenhao Hu,Xueyang Lv*

Main category: cs.SD

TL;DR: SwinSRGAN是一个端到端的语音超分辨率框架，使用Swin Transformer U-Net结构，通过混合对抗训练方案在MDCT域实现实时48kHz上采样，在标准基准测试和零样本测试中均表现出色


<details>
  <summary>Details</summary>
Motivation: 现有语音超分辨率系统存在表示不匹配、过度平滑、计算成本高和跨域鲁棒性有限等问题，需要开发更高效、鲁棒的端到端解决方案

Method: 基于Swin Transformer的U-Net架构，在MDCT幅度域操作，采用混合对抗训练方案（时域MPD/MSD判别器+多频带MDCT判别器），使用稀疏感知正则化器处理arcsinh压缩的MDCT

Result: 在标准基准测试中降低了客观误差并提高了ABX偏好分数，在HiFi-TTS零样本测试中无需微调即优于NVSR和mdctGAN，展现出强大的跨数据集泛化能力

Conclusion: SwinSRGAN提供了一个实时、高效的语音超分辨率解决方案，能够处理不同采样率输入并实现48kHz单次上采样，在性能和泛化能力方面均优于现有方法

Abstract: Speech super-resolution (SR) reconstructs high-frequency content from
low-resolution speech signals. Existing systems often suffer from
representation mismatch in two-stage mel-vocoder pipelines and from
over-smoothing of hallucinated high-band content by CNN-only generators.
Diffusion and flow models are computationally expensive, and their robustness
across domains and sampling rates remains limited. We propose SwinSRGAN, an
end-to-end framework operating on Modified Discrete Cosine Transform (MDCT)
magnitudes. It is a Swin Transformer-based U-Net that captures long-range
spectro-temporal dependencies with a hybrid adversarial scheme combines
time-domain MPD/MSD discriminators with a multi-band MDCT discriminator
specialized for the high-frequency band. We employs a sparse-aware regularizer
on arcsinh-compressed MDCT to better preserve transient components. The system
upsamples inputs at various sampling rates to 48 kHz in a single pass and
operates in real time. On standard benchmarks, SwinSRGAN reduces objective
error and improves ABX preference scores. In zero-shot tests on HiFi-TTS
without fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong
generalization across datasets

</details>


### [15] [WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with Multi-dimensional Annotation](https://arxiv.org/abs/2509.03959)
*Longhao Li,Zhao Guo,Hongjie Chen,Yuhang Dai,Ziyu Zhang,Hongfei Xue,Tianlun Zuo,Chengyou Wang,Shuiyuan Wang,Jie Li,Xin Xu,Hui Bu,Binbin Zhang,Ruibin Yuan,Ziya Zhou,Wei Xue,Lei Xie*

Main category: cs.SD

TL;DR: 提出了WenetSpeech-Pipe管道和WenetSpeech-Yue粤语语音语料库，解决粤语语音数据稀缺问题，包含21,800小时多维度标注数据，在ASR和TTS任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 粤语作为拥有8490万母语者的重要语言，由于标注资源有限，导致ASR和TTS性能不佳，需要构建大规模高质量标注语料库

Method: 开发了包含音频收集、说话人属性标注、语音质量标注、自动语音识别、文本后处理和识别器输出投票六个模块的集成管道，构建多维度标注的大规模语音语料库

Result: 发布的WenetSpeech-Yue语料库覆盖10个领域21,800小时数据，包含ASR转录、文本置信度、说话人身份、年龄、性别、语音质量评分等多维度标注。实验表明基于该语料库训练的模型在粤语ASR和TTS任务上达到竞争性结果

Conclusion: WenetSpeech-Pipe管道和WenetSpeech-Yue语料库有效解决了粤语语音数据稀缺问题，为粤语语音理解和生成研究提供了重要资源，推动了该领域的发展

Abstract: The development of speech understanding and generation has been significantly
accelerated by the availability of large-scale, high-quality speech datasets.
Among these, ASR and TTS are regarded as the most established and fundamental
tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9
million native speakers worldwide, limited annotated resources have hindered
progress and resulted in suboptimal ASR and TTS performance. To address this
challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building
large-scale speech corpus with multi-dimensional annotation tailored for speech
understanding and generation. It comprises six modules: Audio Collection,
Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech
Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich
and high-quality annotations. Based on this pipeline, we release
WenetSpeech-Yue, the first large-scale Cantonese speech corpus with
multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10
domains with annotations including ASR transcription, text confidence, speaker
identity, age, gender, speech quality scores, among other annotations. We also
release WSYue-eval, a comprehensive Cantonese benchmark with two components:
WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long
utterances, code-switching, and diverse acoustic conditions, and
WSYue-TTS-eval, with base and coverage subsets for standard and generalization
testing. Experimental results show that models trained on WenetSpeech-Yue
achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and
TTS systems, including commercial and LLM-based models, highlighting the value
of our dataset and pipeline.

</details>


### [16] [Open-Source Full-Duplex Conversational Datasets for Natural and Interactive Speech Synthesis](https://arxiv.org/abs/2509.04093)
*Zhitong Zhou,Qingqing Zhang,Lei Luo,Jiechen Liu,Ruohua Zhou*

Main category: cs.SD

TL;DR: 提供中英双语对话数据集，包含15小时自然交谈，用于提升对话TTS系统的自然性和交互性


<details>
  <summary>Details</summary>
Motivation: 完整双工对话数据对提升对话语音合成的自然性和交互性至关重要，需要更现实的对话数据支持

Method: 收集中英双语对话数据集，包含隔离室录音、语音转写和注释，精细记录交谈模式和非语言声音，并用于微调TTS模型

Result: 微调后的TTS模型在主观和客观评估指标上都获得更高分数，显示出更好的自然性和对话现实性

Conclusion: 该数据集有效提升了对话语音合成的质量，所有数据、注释和代码已开源提供，促进进一步研究

Abstract: Full-duplex, spontaneous conversational data are essential for enhancing the
naturalness and interactivity of synthesized speech in conversational TTS
systems. We present two open-source dual-track conversational speech datasets,
one in Chinese and one in English, designed to enhance the naturalness of
synthesized speech by providing more realistic conversational data. The two
datasets contain a total of 15 hours of natural, spontaneous conversations
recorded in isolated rooms, which produces separate high-quality audio tracks
for each speaker. The conversations cover diverse daily topics and domains,
capturing realistic interaction patterns including frequent overlaps,
backchannel responses, laughter, and other non-verbal vocalizations. We
introduce the data collection procedure, transcription and annotation methods.
We demonstrate the utility of these corpora by fine-tuning a baseline TTS model
with the proposed datasets. The fine-tuned TTS model achieves higher subjective
and objective evaluation metrics compared to the baseline, indicating improved
naturalness and conversational realism in synthetic speech. All data,
annotations, and supporting code for fine-tuning and evaluation are made
available to facilitate further research in conversational speech synthesis.

</details>


### [17] [Enhancing Self-Supervised Speaker Verification Using Similarity-Connected Graphs and GCN](https://arxiv.org/abs/2509.04147)
*Zhaorui Sun,Yihao Chen,Jialong Wang,Minqiang Xu,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 本文提出了一种基于相似性连接图和图卷积网络的改进聚类框架，用于改善自监督语音识别中的伪标签质量，提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习方法DINO通过聚类生成伪标签时存在的噪声问题，这些噪声伪标签会降低识别性能。

Method: 使用相似性连接图和图卷积网络(GCN)优化聚类过程，利用GCN对结构化数据的建模能力和节点间关系信息来提高伪标签的准确性。

Result: 实验结果显示该方法显著提高了系统性能，增强了自监督语音识别系统的稳健性。

Conclusion: 该方法为自监督语音识别提供了一种新的解决方案，通过改善聚类质量有效提升了系统的识别准确性。

Abstract: With the continuous development of speech recognition technology, speaker
verification (SV) has become an important method for identity authentication.
Traditional SV methods rely on handcrafted feature extraction, while deep
learning has significantly improved system performance. However, the scarcity
of labeled data still limits the widespread application of deep learning in SV.
Self-supervised learning, by mining latent information in large unlabeled
datasets, enhances model generalization and is a key technology to address this
issue.
  DINO is an efficient self-supervised learning method that generates
pseudo-labels from unlabeled speech data through clustering, supporting
subsequent training. However, clustering may produce noisy pseudo-labels, which
can reduce overall recognition performance.
  To address this issue, this paper proposes an improved clustering framework
based on similarity connection graphs and Graph Convolutional Networks. By
leveraging GCNs' ability to model structured data and incorporating relational
information between nodes in the similarity connection graph, the clustering
process is optimized, improving pseudo-label accuracy and enhancing the
robustness and performance of the self-supervised speaker verification system.
Experimental results show that this method significantly improves system
performance and provides a new approach for self-supervised speaker
verification.
  Index Terms: Speaker Verification, Self-Supervised Learning, DINO, Clustering
Algorithm, Graph Convolutional Network, Similarity Connection Graph

</details>


### [18] [Wav2DF-TSL: Two-stage Learning with Efficient Pre-training and Hierarchical Experts Fusion for Robust Audio Deepfake Detection](https://arxiv.org/abs/2509.04161)
*Yunqi Hao,Yihao Chen,Minqiang Xu,Jianbo Zhan,Liang He,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 提出Wav2DF-TSL两阶段学习策略，通过预训练和分层专家融合实现鲁棒的音频深度伪造检测，在跨域数据集上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有SSL模型主要依赖大规模真实语音预训练，缺乏伪造样本学习，导致在音频深度伪造检测任务微调时容易受到域偏差影响

Method: 两阶段策略：预训练阶段使用适配器从3000小时无标签伪造语音中高效学习伪影；微调阶段提出分层自适应专家混合方法动态融合多级伪造线索

Result: 在四个基准数据集上显著超越基线系统，特别是在跨域In-the-wild数据集上相对等错误率提升27.5%，优于现有最先进系统

Conclusion: 该方法通过有效学习伪造样本特征和动态融合多级线索，显著提高了音频深度伪造检测的鲁棒性和跨域性能

Abstract: In recent years, self-supervised learning (SSL) models have made significant
progress in audio deepfake detection (ADD) tasks. However, existing SSL models
mainly rely on large-scale real speech for pre-training and lack the learning
of spoofed samples, which leads to susceptibility to domain bias during the
fine-tuning process of the ADD task. To this end, we propose a two-stage
learning strategy (Wav2DF-TSL) based on pre-training and hierarchical expert
fusion for robust audio deepfake detection. In the pre-training stage, we use
adapters to efficiently learn artifacts from 3000 hours of unlabelled spoofed
speech, improving the adaptability of front-end features while mitigating
catastrophic forgetting. In the fine-tuning stage, we propose the hierarchical
adaptive mixture of experts (HA-MoE) method to dynamically fuse multi-level
spoofing cues through multi-expert collaboration with gated routing.
Experimental results show that the proposed method significantly outperforms
the baseline system on all four benchmark datasets, especially on the
cross-domain In-the-wild dataset, achieving a 27.5% relative improvement in
equal error rate (EER), outperforming the existing state-of-the-art systems.
Index Terms: audio deepfake detection, self-supervised learning,
parameter-efficient fine-tuning, mixture of experts

</details>


### [19] [PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music](https://arxiv.org/abs/2509.04215)
*Hayeon Bang,Eunjin Choi,Seungheon Doh,Juhan Nam*

Main category: cs.SD

TL;DR: PianoBind是一个专门针对钢琴音乐的多模态联合嵌入模型，能够有效捕捉独奏钢琴音乐的细微语义差异，在文本到音乐检索任务上优于通用音乐表示模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用音乐表示模型难以捕捉独奏钢琴音乐中的细微语义差异，且现有钢琴专用模型多为单模态，无法处理钢琴音乐固有的多模态特性（音频、符号、文本）。

Method: 提出PianoBind钢琴专用多模态联合嵌入模型，系统研究多源训练策略和模态利用方法，在小规模和同质钢琴数据集上优化联合嵌入框架。

Result: PianoBind学习到的多模态表示能有效捕捉钢琴音乐的细微差别，在领域内和领域外钢琴数据集上的文本到音乐检索性能优于通用音乐联合嵌入模型。

Conclusion: 该模型不仅解决了钢琴音乐表示的特殊挑战，其设计选择也为其他同质数据集的多模态表示学习提供了可复用的见解。

Abstract: Solo piano music, despite being a single-instrument medium, possesses
significant expressive capabilities, conveying rich semantic information across
genres, moods, and styles. However, current general-purpose music
representation models, predominantly trained on large-scale datasets, often
struggle to captures subtle semantic distinctions within homogeneous solo piano
music. Furthermore, existing piano-specific representation models are typically
unimodal, failing to capture the inherently multimodal nature of piano music,
expressed through audio, symbolic, and textual modalities. To address these
limitations, we propose PianoBind, a piano-specific multimodal joint embedding
model. We systematically investigate strategies for multi-source training and
modality utilization within a joint embedding framework optimized for capturing
fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano
datasets. Our experimental results demonstrate that PianoBind learns multimodal
representations that effectively capture subtle nuances of piano music,
achieving superior text-to-music retrieval performance on in-domain and
out-of-domain piano datasets compared to general-purpose music joint embedding
models. Moreover, our design choices offer reusable insights for multimodal
representation learning with homogeneous datasets beyond piano music.

</details>


### [20] [AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds](https://arxiv.org/abs/2509.04345)
*Qizhou Wang,Hanxun Huang,Guansong Pang,Sarah Erfani,Christopher Leckie*

Main category: cs.SD

TL;DR: AUDETER是一个大规模、多样化的深度伪造音频数据集，包含4500+小时合成音频，用于训练和评估深度伪造音频检测模型，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音生成系统能产生难以区分真伪的语音，但现有深度伪造检测方法在真实环境中效果不佳，主要因为训练和测试样本之间存在领域偏移，且现有数据集缺乏真实世界应用的挑战性。

Method: 构建AUDETER数据集，包含4500小时合成音频，由11个最新TTS模型和10个声码器生成，总计300万音频片段，是目前规模最大的深度伪造音频数据集。

Result: 实验表明：1）现有SOTA方法在新样本上泛化能力差，假阳性率高；2）使用AUDETER训练的方法检测错误率降低44.1%-51.6%，在In-the-Wild数据集上错误率仅4.17%。

Conclusion: AUDETER数据集能有效训练通用型深度伪造音频检测器，显著提升检测性能，为解决深度伪造音频检测的领域偏移问题提供了重要资源。

Abstract: Speech generation systems can produce remarkably realistic vocalisations that
are often indistinguishable from human speech, posing significant authenticity
challenges. Although numerous deepfake detection methods have been developed,
their effectiveness in real-world environments remains unrealiable due to the
domain shift between training and test samples arising from diverse human
speech and fast evolving speech synthesis systems. This is not adequately
addressed by current datasets, which lack real-world application challenges
with diverse and up-to-date audios in both real and deep-fake categories. To
fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,
highly diverse deepfake audio dataset for comprehensive evaluation and robust
development of generalised models for deepfake audio detection. It consists of
over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10
vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio
clips, making it the largest deepfake audio dataset by scale. Through extensive
experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods
trained on existing datasets struggle to generalise to novel deepfake audio
samples and suffer from high false positive rates on unseen human voice,
underscoring the need for a comprehensive dataset; and ii) these methods
trained on AUDETER achieve highly generalised detection performance and
significantly reduce detection error rate by 44.1% to 51.6%, achieving an error
rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild
dataset, paving the way for training generalist deepfake audio detectors.
AUDETER is available on GitHub.

</details>


### [21] [Denoising GER: A Noise-Robust Generative Error Correction with LLM for Speech Recognition](https://arxiv.org/abs/2509.04392)
*Yanyan Liu,Minqiang Xu,Yihao Chen,Liang He,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 提出Denoising GER框架，通过噪声自适应声学编码器和异构特征补偿动态融合机制，提升LLM在复杂噪声环境下的语音识别后处理能力，结合强化学习训练策略显著改善噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂噪声环境中的语音识别后处理任务存在适应性差、信息利用率低的问题，导致生成错误校正效果有限。

Method: 提出噪声鲁棒多模态GER框架：1）噪声自适应声学编码器增强模型对不同噪声场景的适应性；2）异构特征补偿动态融合机制优化多模态信息整合；3）引入强化学习训练策略提升模型预测能力。

Result: 实验结果表明，Denoising GER在噪声环境中显著提高了准确性和鲁棒性，并在未见噪声场景中展现出良好的泛化能力。

Conclusion: 该框架有效解决了LLM在噪声环境中的适应性问题和信息利用率问题，为复杂环境下的语音识别后处理提供了有效的解决方案。

Abstract: In recent years, large language models (LLM) have made significant progress
in the task of generation error correction (GER) for automatic speech
recognition (ASR) post-processing. However, in complex noisy environments, they
still face challenges such as poor adaptability and low information
utilization, resulting in limited effectiveness of GER. To address these
issues, this paper proposes a noise-robust multi-modal GER framework (Denoising
GER). The framework enhances the model's adaptability to different noisy
scenarios through a noise-adaptive acoustic encoder and optimizes the
integration of multi-modal information via a heterogeneous feature compensation
dynamic fusion (HFCDF) mechanism, improving the LLM's utilization of
multi-modal information. Additionally, reinforcement learning (RL) training
strategies are introduced to enhance the model's predictive capabilities.
Experimental results demonstrate that Denoising GER significantly improves
accuracy and robustness in noisy environments and exhibits good generalization
abilities in unseen noise scenarios.

</details>


### [22] [Contextualized Token Discrimination for Speech Search Query Correction](https://arxiv.org/abs/2509.04393)
*Junyu Lu,Di Jiang,Mengze Hong,Victor Junqiu Wei,Qintian Guo,Zhiyang Su*

Main category: cs.SD

TL;DR: 这篇论文提出了一种名为上下文化分识判别(CTD)的新方法，用于改善语音搜索查询的拼写编正效果。方法利用BERT生成上下文化表征，通过比较原始和上下文化表征来编正错误的查询词汇。


<details>
  <summary>Details</summary>
Motivation: 随着语音搜索和自动语音识别(ASR)系统的普及，需要有效的方法来编正ASR转译中的错误查询，以更好地理解用户意图。

Method: 使用BERT生成词汇级别的上下文化表征，构建组合层来增强语义信息，然后通过比较原始词汇表征和上下文化表征来生成正确的查询。

Result: 大量实验表明该方法在所有指标上都表现优异，同时还提供了包含错误ASR转译的新标准数据集。

Conclusion: CTD方法能够有效地编正语音查询中的拼写错误，为音频查询编正提供了全面的评估基准。

Abstract: Query spelling correction is an important function of modern search engines
since it effectively helps users express their intentions clearly. With the
growing popularity of speech search driven by Automated Speech Recognition
(ASR) systems, this paper introduces a novel method named Contextualized Token
Discrimination (CTD) to conduct effective speech query correction. In CTD, we
first employ BERT to generate token-level contextualized representations and
then construct a composition layer to enhance semantic information. Finally, we
produce the correct query according to the aggregated token representation,
correcting the incorrect tokens by comparing the original token representations
and the contextualized representations. Extensive experiments demonstrate the
superior performance of our proposed method across all metrics, and we further
present a new benchmark dataset with erroneous ASR transcriptions to offer
comprehensive evaluations for audio query correction.

</details>

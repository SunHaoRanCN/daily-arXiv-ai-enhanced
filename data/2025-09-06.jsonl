{"id": "2509.03913", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03913", "abs": "https://arxiv.org/abs/2509.03913", "authors": ["Jiajun Yuan", "Xiaochen Wang", "Yuhang Xiao", "Yulin Wu", "Chenhao Hu", "Xueyang Lv"], "title": "SwinSRGAN: Swin Transformer-based Generative Adversarial Network for High-Fidelity Speech Super-Resolution", "comment": "5 pages", "summary": "Speech super-resolution (SR) reconstructs high-frequency content from\nlow-resolution speech signals. Existing systems often suffer from\nrepresentation mismatch in two-stage mel-vocoder pipelines and from\nover-smoothing of hallucinated high-band content by CNN-only generators.\nDiffusion and flow models are computationally expensive, and their robustness\nacross domains and sampling rates remains limited. We propose SwinSRGAN, an\nend-to-end framework operating on Modified Discrete Cosine Transform (MDCT)\nmagnitudes. It is a Swin Transformer-based U-Net that captures long-range\nspectro-temporal dependencies with a hybrid adversarial scheme combines\ntime-domain MPD/MSD discriminators with a multi-band MDCT discriminator\nspecialized for the high-frequency band. We employs a sparse-aware regularizer\non arcsinh-compressed MDCT to better preserve transient components. The system\nupsamples inputs at various sampling rates to 48 kHz in a single pass and\noperates in real time. On standard benchmarks, SwinSRGAN reduces objective\nerror and improves ABX preference scores. In zero-shot tests on HiFi-TTS\nwithout fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong\ngeneralization across datasets"}
{"id": "2509.03959", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03959", "abs": "https://arxiv.org/abs/2509.03959", "authors": ["Longhao Li", "Zhao Guo", "Hongjie Chen", "Yuhang Dai", "Ziyu Zhang", "Hongfei Xue", "Tianlun Zuo", "Chengyou Wang", "Shuiyuan Wang", "Jie Li", "Xin Xu", "Hui Bu", "Binbin Zhang", "Ruibin Yuan", "Ziya Zhou", "Wei Xue", "Lei Xie"], "title": "WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with Multi-dimensional Annotation", "comment": null, "summary": "The development of speech understanding and generation has been significantly\naccelerated by the availability of large-scale, high-quality speech datasets.\nAmong these, ASR and TTS are regarded as the most established and fundamental\ntasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9\nmillion native speakers worldwide, limited annotated resources have hindered\nprogress and resulted in suboptimal ASR and TTS performance. To address this\nchallenge, we propose WenetSpeech-Pipe, an integrated pipeline for building\nlarge-scale speech corpus with multi-dimensional annotation tailored for speech\nunderstanding and generation. It comprises six modules: Audio Collection,\nSpeaker Attributes Annotation, Speech Quality Annotation, Automatic Speech\nRecognition, Text Postprocessing and Recognizer Output Voting, enabling rich\nand high-quality annotations. Based on this pipeline, we release\nWenetSpeech-Yue, the first large-scale Cantonese speech corpus with\nmulti-dimensional annotation for ASR and TTS, covering 21,800 hours across 10\ndomains with annotations including ASR transcription, text confidence, speaker\nidentity, age, gender, speech quality scores, among other annotations. We also\nrelease WSYue-eval, a comprehensive Cantonese benchmark with two components:\nWSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long\nutterances, code-switching, and diverse acoustic conditions, and\nWSYue-TTS-eval, with base and coverage subsets for standard and generalization\ntesting. Experimental results show that models trained on WenetSpeech-Yue\nachieve competitive results against state-of-the-art (SOTA) Cantonese ASR and\nTTS systems, including commercial and LLM-based models, highlighting the value\nof our dataset and pipeline."}
{"id": "2509.04093", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04093", "abs": "https://arxiv.org/abs/2509.04093", "authors": ["Zhitong Zhou", "Qingqing Zhang", "Lei Luo", "Jiechen Liu", "Ruohua Zhou"], "title": "Open-Source Full-Duplex Conversational Datasets for Natural and Interactive Speech Synthesis", "comment": null, "summary": "Full-duplex, spontaneous conversational data are essential for enhancing the\nnaturalness and interactivity of synthesized speech in conversational TTS\nsystems. We present two open-source dual-track conversational speech datasets,\none in Chinese and one in English, designed to enhance the naturalness of\nsynthesized speech by providing more realistic conversational data. The two\ndatasets contain a total of 15 hours of natural, spontaneous conversations\nrecorded in isolated rooms, which produces separate high-quality audio tracks\nfor each speaker. The conversations cover diverse daily topics and domains,\ncapturing realistic interaction patterns including frequent overlaps,\nbackchannel responses, laughter, and other non-verbal vocalizations. We\nintroduce the data collection procedure, transcription and annotation methods.\nWe demonstrate the utility of these corpora by fine-tuning a baseline TTS model\nwith the proposed datasets. The fine-tuned TTS model achieves higher subjective\nand objective evaluation metrics compared to the baseline, indicating improved\nnaturalness and conversational realism in synthetic speech. All data,\nannotations, and supporting code for fine-tuning and evaluation are made\navailable to facilitate further research in conversational speech synthesis."}
{"id": "2509.04147", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04147", "abs": "https://arxiv.org/abs/2509.04147", "authors": ["Zhaorui Sun", "Yihao Chen", "Jialong Wang", "Minqiang Xu", "Lei Fang", "Sian Fang", "Lin Liu"], "title": "Enhancing Self-Supervised Speaker Verification Using Similarity-Connected Graphs and GCN", "comment": null, "summary": "With the continuous development of speech recognition technology, speaker\nverification (SV) has become an important method for identity authentication.\nTraditional SV methods rely on handcrafted feature extraction, while deep\nlearning has significantly improved system performance. However, the scarcity\nof labeled data still limits the widespread application of deep learning in SV.\nSelf-supervised learning, by mining latent information in large unlabeled\ndatasets, enhances model generalization and is a key technology to address this\nissue.\n  DINO is an efficient self-supervised learning method that generates\npseudo-labels from unlabeled speech data through clustering, supporting\nsubsequent training. However, clustering may produce noisy pseudo-labels, which\ncan reduce overall recognition performance.\n  To address this issue, this paper proposes an improved clustering framework\nbased on similarity connection graphs and Graph Convolutional Networks. By\nleveraging GCNs' ability to model structured data and incorporating relational\ninformation between nodes in the similarity connection graph, the clustering\nprocess is optimized, improving pseudo-label accuracy and enhancing the\nrobustness and performance of the self-supervised speaker verification system.\nExperimental results show that this method significantly improves system\nperformance and provides a new approach for self-supervised speaker\nverification.\n  Index Terms: Speaker Verification, Self-Supervised Learning, DINO, Clustering\nAlgorithm, Graph Convolutional Network, Similarity Connection Graph"}
{"id": "2509.03686", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03686", "abs": "https://arxiv.org/abs/2509.03686", "authors": ["Hong Zhu", "Alexander Venus", "Erik Leitinger", "Klaus Witrisal"], "title": "Multi-Sensor Fusion for Extended Object Tracking Exploiting Active and Passive Radio Signals", "comment": null, "summary": "Reliable and robust positioning of radio devices remains a challenging task\ndue to multipath propagation, hardware impairments, and interference from other\nradio transmitters. A frequently overlooked but critical factor is the agent\nitself, e.g., the user carrying the device, which potentially obstructs\nline-of-sight (LOS) links to the base stations (anchors). This paper addresses\nthe problem of accurate positioning in scenarios where LOS links are partially\nblocked by the agent. The agent is modeled as an extended object (EO) that\nscatters, attenuates, and blocks radio signals. We propose a Bayesian method\nthat fuses ``active'' measurements (between device and anchors) with\n``passive'' multistatic radar-type measurements (between anchors, reflected by\nthe EO). To handle measurement origin uncertainty, we introduce an multi-sensor\nand multiple-measurement probabilistic data association (PDA) algorithm that\njointly fuses all EO-related measurements. Furthermore, we develop an EO model\ntailored to agents such as human users, accounting for multiple reflections\nscattered off the body surface, and propose a simplified variant for\nlow-complexity implementation. Evaluation on both synthetic and real radio\nmeasurements demonstrates that the proposed algorithm outperforms conventional\nPDA methods based on point target assumptions, particularly during and after\nobstructed line-of-sight (OLOS) conditions."}
{"id": "2509.03902", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03902", "abs": "https://arxiv.org/abs/2509.03902", "authors": ["Shunxi Xu", "Craig T. Jin"], "title": "Hierarchical Sparse Sound Field Reconstruction with Spherical and Linear Microphone Arrays", "comment": "Accepted by APSIPA ASC 2025", "summary": "Spherical microphone arrays (SMAs) are widely used for sound field analysis,\nand sparse recovery (SR) techniques can significantly enhance their spatial\nresolution by modeling the sound field as a sparse superposition of dominant\nplane waves. However, the spatial resolution of SMAs is fundamentally limited\nby their spherical harmonic order, and their performance often degrades in\nreverberant environments. This paper proposes a two-stage SR framework with\nresidue refinement that integrates observations from a central SMA and four\nsurrounding linear microphone arrays (LMAs). The core idea is to exploit\ncomplementary spatial characteristics by treating the SMA as a primary\nestimator and the LMAs as a spatially complementary refiner. Simulation results\ndemonstrate that the proposed SMA-LMA method significantly enhances spatial\nenergy map reconstruction under varying reverberation conditions, compared to\nboth SMA-only and direct one-step joint processing. These results demonstrate\nthe effectiveness of the proposed framework in enhancing spatial fidelity and\nrobustness in complex acoustic environments."}
{"id": "2509.04161", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04161", "abs": "https://arxiv.org/abs/2509.04161", "authors": ["Yunqi Hao", "Yihao Chen", "Minqiang Xu", "Jianbo Zhan", "Liang He", "Lei Fang", "Sian Fang", "Lin Liu"], "title": "Wav2DF-TSL: Two-stage Learning with Efficient Pre-training and Hierarchical Experts Fusion for Robust Audio Deepfake Detection", "comment": null, "summary": "In recent years, self-supervised learning (SSL) models have made significant\nprogress in audio deepfake detection (ADD) tasks. However, existing SSL models\nmainly rely on large-scale real speech for pre-training and lack the learning\nof spoofed samples, which leads to susceptibility to domain bias during the\nfine-tuning process of the ADD task. To this end, we propose a two-stage\nlearning strategy (Wav2DF-TSL) based on pre-training and hierarchical expert\nfusion for robust audio deepfake detection. In the pre-training stage, we use\nadapters to efficiently learn artifacts from 3000 hours of unlabelled spoofed\nspeech, improving the adaptability of front-end features while mitigating\ncatastrophic forgetting. In the fine-tuning stage, we propose the hierarchical\nadaptive mixture of experts (HA-MoE) method to dynamically fuse multi-level\nspoofing cues through multi-expert collaboration with gated routing.\nExperimental results show that the proposed method significantly outperforms\nthe baseline system on all four benchmark datasets, especially on the\ncross-domain In-the-wild dataset, achieving a 27.5% relative improvement in\nequal error rate (EER), outperforming the existing state-of-the-art systems.\nIndex Terms: audio deepfake detection, self-supervised learning,\nparameter-efficient fine-tuning, mixture of experts"}
{"id": "2509.03825", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03825", "abs": "https://arxiv.org/abs/2509.03825", "authors": ["Jeunghoon Lee"], "title": "Sensor placement for sparse force reconstruction", "comment": null, "summary": "The present study proposes a Gram-matrix-based sensor placement strategy for\nsparse force reconstruction in the frequency domain. A modal decomposition of\nthe Gram matrix reveals that its structure is dominated by a few modes near the\ntarget frequency, and that each modal contribution reflects the spatial\ncorrelation of the corresponding mode shape. This suggests that placing sensors\nnear nodal regions where spatial correlation is low can reduce coherence in the\nfrequency response function (FRF) matrix and improve force reconstruction\naccuracy. To translate the physical insight into a practical design framework,\na greedy algorithm is proposed to select sensor locations that minimize the\noff-diagonal energy of the Gram matrix. Numerical simulations and experimental\nvalidations demonstrate that the proposed method yields robust and accurate\nforce estimation, outperforming heuristic sensor layouts."}
{"id": "2509.04072", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04072", "abs": "https://arxiv.org/abs/2509.04072", "authors": ["Gaspard Michel", "Elena V. Epure", "Christophe Cerisara"], "title": "LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis", "comment": null, "summary": "Text-to-speech (TTS) systems have recently achieved more expressive and\nnatural speech synthesis by scaling to large speech datasets. However, the\nproportion of expressive speech in such large-scale corpora is often unclear.\nBesides, existing expressive speech corpora are typically smaller in scale and\nprimarily used for benchmarking TTS systems. In this paper, we introduce the\nLibriQuote dataset, an English corpus derived from read audiobooks, designed\nfor both fine-tuning and benchmarking expressive zero-shot TTS system. The\ntraining dataset includes 12.7K hours of read, non-expressive speech and 5.3K\nhours of mostly expressive speech drawn from character quotations. Each\nutterance in the expressive subset is supplemented with the context in which it\nwas written, along with pseudo-labels of speech verbs and adverbs used to\ndescribe the quotation (\\textit{e.g. ``he whispered softly''}). Additionally,\nwe provide a challenging 7.5 hour test set intended for benchmarking TTS\nsystems: given a neutral reference speech as input, we evaluate system's\nability to synthesize an expressive utterance while preserving reference\ntimbre. We validate qualitatively the test set by showing that it covers a wide\nrange of emotions compared to non-expressive speech, along with various\naccents. Extensive subjective and objective evaluations show that fine-tuning a\nbaseline TTS system on LibriQuote significantly improves its synthesized speech\nintelligibility, and that recent systems fail to synthesize speech as\nexpressive and natural as the ground-truth utterances. The dataset and\nevaluation code are freely available. Audio samples can be found at\nhttps://libriquote.github.io/."}
{"id": "2509.04215", "categories": ["cs.SD", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.04215", "abs": "https://arxiv.org/abs/2509.04215", "authors": ["Hayeon Bang", "Eunjin Choi", "Seungheon Doh", "Juhan Nam"], "title": "PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music", "comment": "Accepted for publication at the 26th International Society for Music\n  Information Retrieval Conference (ISMIR 2025)", "summary": "Solo piano music, despite being a single-instrument medium, possesses\nsignificant expressive capabilities, conveying rich semantic information across\ngenres, moods, and styles. However, current general-purpose music\nrepresentation models, predominantly trained on large-scale datasets, often\nstruggle to captures subtle semantic distinctions within homogeneous solo piano\nmusic. Furthermore, existing piano-specific representation models are typically\nunimodal, failing to capture the inherently multimodal nature of piano music,\nexpressed through audio, symbolic, and textual modalities. To address these\nlimitations, we propose PianoBind, a piano-specific multimodal joint embedding\nmodel. We systematically investigate strategies for multi-source training and\nmodality utilization within a joint embedding framework optimized for capturing\nfine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano\ndatasets. Our experimental results demonstrate that PianoBind learns multimodal\nrepresentations that effectively capture subtle nuances of piano music,\nachieving superior text-to-music retrieval performance on in-domain and\nout-of-domain piano datasets compared to general-purpose music joint embedding\nmodels. Moreover, our design choices offer reusable insights for multimodal\nrepresentation learning with homogeneous datasets beyond piano music."}
{"id": "2509.03979", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03979", "abs": "https://arxiv.org/abs/2509.03979", "authors": ["Gilles Callebaut", "Jan Van Moer"], "title": "A Low-Cost Open-Source BLE-Based Asian Hornet Tracking System", "comment": null, "summary": "The Asian hornet (Vespa velutina) poses a serious threat to ecosystems and\nbeekeeping. Locating nests is essential, but usually involves time-consuming\nmanual triangulation. We present a low-cost, open-source tracking system based\non Bluetooth Low Energy (BLE). The system consists of a lightweight BLE tag and\na software-defined radio (SDR) receiver implemented in GNU Radio. By bypassing\nthe BLE stack, we embed a custom pseudo-noise (PN) sequence in the uncoded PHY\nfor correlation-based detection. Using a Yagi antenna and PlutoSDR, the\nreceiver performs digital beam sweeping to determine the tag's direction. Field\ntests show reliable angular resolution at 50m and a communication range up to\n360m. While our modulation increases receiver complexity, it enables future\nimprovements such as multichannel spreading and tag identification. The design\nis fully open-source and provides a scalable framework for hornet tracking and\nrelated applications in environmental monitoring."}
{"id": "2509.04280", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04280", "abs": "https://arxiv.org/abs/2509.04280", "authors": ["Tobias Raichle", "Niels Edinger", "Bin Yang"], "title": "Test-Time Adaptation for Speech Enhancement via Domain Invariant Embedding Transformation", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Deep learning-based speech enhancement models achieve remarkable performance\nwhen test distributions match training conditions, but often degrade when\ndeployed in unpredictable real-world environments with domain shifts. To\naddress this challenge, we present LaDen (latent denoising), the first\ntest-time adaptation method specifically designed for speech enhancement. Our\napproach leverages powerful pre-trained speech representations to perform\nlatent denoising, approximating clean speech representations through a linear\ntransformation of noisy embeddings. We show that this transformation\ngeneralizes well across domains, enabling effective pseudo-labeling for target\ndomains without labeled target data. The resulting pseudo-labels enable\neffective test-time adaptation of speech enhancement models across diverse\nacoustic environments. We propose a comprehensive benchmark spanning multiple\ndatasets with various domain shifts, including changes in noise types, speaker\ncharacteristics, and languages. Our extensive experiments demonstrate that\nLaDen consistently outperforms baseline methods across perceptual metrics,\nparticularly for speaker and language domain shifts."}
{"id": "2509.04345", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04345", "abs": "https://arxiv.org/abs/2509.04345", "authors": ["Qizhou Wang", "Hanxun Huang", "Guansong Pang", "Sarah Erfani", "Christopher Leckie"], "title": "AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds", "comment": null, "summary": "Speech generation systems can produce remarkably realistic vocalisations that\nare often indistinguishable from human speech, posing significant authenticity\nchallenges. Although numerous deepfake detection methods have been developed,\ntheir effectiveness in real-world environments remains unrealiable due to the\ndomain shift between training and test samples arising from diverse human\nspeech and fast evolving speech synthesis systems. This is not adequately\naddressed by current datasets, which lack real-world application challenges\nwith diverse and up-to-date audios in both real and deep-fake categories. To\nfill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,\nhighly diverse deepfake audio dataset for comprehensive evaluation and robust\ndevelopment of generalised models for deepfake audio detection. It consists of\nover 4,500 hours of synthetic audio generated by 11 recent TTS models and 10\nvocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio\nclips, making it the largest deepfake audio dataset by scale. Through extensive\nexperiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods\ntrained on existing datasets struggle to generalise to novel deepfake audio\nsamples and suffer from high false positive rates on unseen human voice,\nunderscoring the need for a comprehensive dataset; and ii) these methods\ntrained on AUDETER achieve highly generalised detection performance and\nsignificantly reduce detection error rate by 44.1% to 51.6%, achieving an error\nrate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild\ndataset, paving the way for training generalist deepfake audio detectors.\nAUDETER is available on GitHub."}
{"id": "2509.03980", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03980", "abs": "https://arxiv.org/abs/2509.03980", "authors": ["Alessandro Mirri", "Vishnu Teja Kunde", "Enrico Paolini", "Jean-Francois Chamberland"], "title": "Approximate Message Passing for Multi-Preamble Detection in OTFS Random Access", "comment": null, "summary": "This article addresses the problem of multiple preamble detection in random\naccess systems based on orthogonal time frequency space (OTFS) signaling. This\nchallenge is formulated as a structured sparse recovery problem in the complex\ndomain. To tackle it, the authors propose a new approximate message passing\n(AMP) algorithm that enforces double sparsity: the sparse selection of\npreambles and the inherent sparsity of OTFS signals in the delay-Doppler\ndomain. From an algorithmic standpoint, the non-separable complex sparsity\nconstraint necessitates a careful derivation and leads to the design of a novel\nAMP denoiser. Simulation results demonstrate that the proposed method achieves\nrobust detection performance and delivers significant gains over\nstate-of-the-art techniques."}
{"id": "2509.04390", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04390", "abs": "https://arxiv.org/abs/2509.04390", "authors": ["Hannes Rosseel", "Toon van Waterschoot"], "title": "Accelerated Interactive Auralization of Highly Reverberant Spaces using Graphics Hardware", "comment": "8 pages, 6 figures, submitted to Journal of the Audio Engineering\n  Society", "summary": "Interactive acoustic auralization allows users to explore virtual acoustic\nenvironments in real-time, enabling the acoustic recreation of concert hall or\nHistorical Worship Spaces (HWS) that are either no longer accessible,\nacoustically altered, or impractical to visit. Interactive acoustic synthesis\nrequires real-time convolution of input signals with a set of synthesis filters\nthat model the space-time acoustic response of the space. The acoustics in\nconcert halls and HWS are both characterized by a long reverberation time,\nresulting in synthesis filters containing many filter taps. As a result, the\nconvolution process can be computationally demanding, introducing significant\nlatency that limits the real-time interactivity of the auralization system. In\nthis paper, the implementation of a real-time multichannel loudspeaker-based\nauralization system is presented. This system is capable of synthesizing the\nacoustics of highly reverberant spaces in real-time using GPU-acceleration. A\ncomparison between traditional CPU-based convolution and GPU-accelerated\nconvolution is presented, showing that the latter can achieve real-time\nperformance with significantly lower latency. Additionally, the system\nintegrates acoustic synthesis with acoustic feedback cancellation on the GPU,\ncreating a unified loudspeaker-based auralization framework that minimizes\nprocessing latency."}
{"id": "2509.04392", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04392", "abs": "https://arxiv.org/abs/2509.04392", "authors": ["Yanyan Liu", "Minqiang Xu", "Yihao Chen", "Liang He", "Lei Fang", "Sian Fang", "Lin Liu"], "title": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for Speech Recognition", "comment": null, "summary": "In recent years, large language models (LLM) have made significant progress\nin the task of generation error correction (GER) for automatic speech\nrecognition (ASR) post-processing. However, in complex noisy environments, they\nstill face challenges such as poor adaptability and low information\nutilization, resulting in limited effectiveness of GER. To address these\nissues, this paper proposes a noise-robust multi-modal GER framework (Denoising\nGER). The framework enhances the model's adaptability to different noisy\nscenarios through a noise-adaptive acoustic encoder and optimizes the\nintegration of multi-modal information via a heterogeneous feature compensation\ndynamic fusion (HFCDF) mechanism, improving the LLM's utilization of\nmulti-modal information. Additionally, reinforcement learning (RL) training\nstrategies are introduced to enhance the model's predictive capabilities.\nExperimental results demonstrate that Denoising GER significantly improves\naccuracy and robustness in noisy environments and exhibits good generalization\nabilities in unseen noise scenarios."}
{"id": "2509.03983", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.03983", "abs": "https://arxiv.org/abs/2509.03983", "authors": ["Yutong Chen", "Cong Zhou", "Changsheng You", "Shuo Shi"], "title": "Joint Frequency-Space Sparse Reconstruction for DOA Estimation under Coherent Sources and Amplitude-Phase Errors", "comment": null, "summary": "In this letter, we propose a joint frequency-space sparse reconstruction\nmethod for direction-of-arrival (DOA) estimation, which effectively addresses\nthe issues arising from the existence of coherent sources and array\namplitude-phase errors. Specifically, by using an auxiliary source with known\nangles, we first construct the real steering vectors (RSVs) based on the\nspectral peaks of received signals in the frequency domain, which serve as a\ncomplete basis matrix for compensation for amplitude-phase errors. Then, we\nleverage the spectral sparsity of snapshot data in the frequency domain and the\nspatial sparsity of incident directions to perform the DOA estimation according\nto the sparse reconstruction method. The proposed method does not require\niterative optimization, hence exhibiting low computational complexity.\nNumerical results demonstrate that the proposed DOA estimation method achieves\nhigher estimation accuracy for coherent sources as compared to various\nbenchmark schemes."}
{"id": "2509.03913", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03913", "abs": "https://arxiv.org/abs/2509.03913", "authors": ["Jiajun Yuan", "Xiaochen Wang", "Yuhang Xiao", "Yulin Wu", "Chenhao Hu", "Xueyang Lv"], "title": "SwinSRGAN: Swin Transformer-based Generative Adversarial Network for High-Fidelity Speech Super-Resolution", "comment": "5 pages", "summary": "Speech super-resolution (SR) reconstructs high-frequency content from\nlow-resolution speech signals. Existing systems often suffer from\nrepresentation mismatch in two-stage mel-vocoder pipelines and from\nover-smoothing of hallucinated high-band content by CNN-only generators.\nDiffusion and flow models are computationally expensive, and their robustness\nacross domains and sampling rates remains limited. We propose SwinSRGAN, an\nend-to-end framework operating on Modified Discrete Cosine Transform (MDCT)\nmagnitudes. It is a Swin Transformer-based U-Net that captures long-range\nspectro-temporal dependencies with a hybrid adversarial scheme combines\ntime-domain MPD/MSD discriminators with a multi-band MDCT discriminator\nspecialized for the high-frequency band. We employs a sparse-aware regularizer\non arcsinh-compressed MDCT to better preserve transient components. The system\nupsamples inputs at various sampling rates to 48 kHz in a single pass and\noperates in real time. On standard benchmarks, SwinSRGAN reduces objective\nerror and improves ABX preference scores. In zero-shot tests on HiFi-TTS\nwithout fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong\ngeneralization across datasets"}
{"id": "2509.04393", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04393", "abs": "https://arxiv.org/abs/2509.04393", "authors": ["Junyu Lu", "Di Jiang", "Mengze Hong", "Victor Junqiu Wei", "Qintian Guo", "Zhiyang Su"], "title": "Contextualized Token Discrimination for Speech Search Query Correction", "comment": null, "summary": "Query spelling correction is an important function of modern search engines\nsince it effectively helps users express their intentions clearly. With the\ngrowing popularity of speech search driven by Automated Speech Recognition\n(ASR) systems, this paper introduces a novel method named Contextualized Token\nDiscrimination (CTD) to conduct effective speech query correction. In CTD, we\nfirst employ BERT to generate token-level contextualized representations and\nthen construct a composition layer to enhance semantic information. Finally, we\nproduce the correct query according to the aggregated token representation,\ncorrecting the incorrect tokens by comparing the original token representations\nand the contextualized representations. Extensive experiments demonstrate the\nsuperior performance of our proposed method across all metrics, and we further\npresent a new benchmark dataset with erroneous ASR transcriptions to offer\ncomprehensive evaluations for audio query correction."}
{"id": "2509.04005", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04005", "abs": "https://arxiv.org/abs/2509.04005", "authors": ["Mingze Gong", "Shuoyao Wang", "Shijian Gao", "Jia Yan", "Suzhi Bi"], "title": "Robust MIMO Semantic Communication with Imperfect CSI via Knowledge Distillation", "comment": null, "summary": "Semantic communication (SemComm) has emerged as a new communication paradigm.\nTo enhance efficiency, multiple-input-multiple-output (MIMO) technology has\nbeen further integrated into SemComm systems. However, existing MIMO SemComm\nsystems assume perfect channel matrix estimation for channel-adaptive joint\nsource-channel coding, which is impractical due to hardware and pilot overhead\nconstraints. In this paper, we propose a semantic image transmission system\nwith channel matrix and channel noise adaptation, named HANA-JSCC, to cope with\nchannel estimation errors in MIMO systems. We propose a channel matrix adaptor\nthat collaborates with the channel codec to adapt to misaligned channel state\ninformation, thereby mitigating the impact of estimation errors. Since the\nrelationship between the estimated channel matrix and true channel matrix is\nill-posed (one-to-many), we further introduce a two-stage training strategy\nwith knowledge distillation to overcome the convergence difficulties caused by\nthe ill-posed problem. Comparing with the state-of-the-art benchmarks,\nHANA-JSCC achieves $0.40\\sim0.54$dB higher average performance across various\nnoise and estimation error levels in various datasets."}
{"id": "2509.04072", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04072", "abs": "https://arxiv.org/abs/2509.04072", "authors": ["Gaspard Michel", "Elena V. Epure", "Christophe Cerisara"], "title": "LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis", "comment": null, "summary": "Text-to-speech (TTS) systems have recently achieved more expressive and\nnatural speech synthesis by scaling to large speech datasets. However, the\nproportion of expressive speech in such large-scale corpora is often unclear.\nBesides, existing expressive speech corpora are typically smaller in scale and\nprimarily used for benchmarking TTS systems. In this paper, we introduce the\nLibriQuote dataset, an English corpus derived from read audiobooks, designed\nfor both fine-tuning and benchmarking expressive zero-shot TTS system. The\ntraining dataset includes 12.7K hours of read, non-expressive speech and 5.3K\nhours of mostly expressive speech drawn from character quotations. Each\nutterance in the expressive subset is supplemented with the context in which it\nwas written, along with pseudo-labels of speech verbs and adverbs used to\ndescribe the quotation (\\textit{e.g. ``he whispered softly''}). Additionally,\nwe provide a challenging 7.5 hour test set intended for benchmarking TTS\nsystems: given a neutral reference speech as input, we evaluate system's\nability to synthesize an expressive utterance while preserving reference\ntimbre. We validate qualitatively the test set by showing that it covers a wide\nrange of emotions compared to non-expressive speech, along with various\naccents. Extensive subjective and objective evaluations show that fine-tuning a\nbaseline TTS system on LibriQuote significantly improves its synthesized speech\nintelligibility, and that recent systems fail to synthesize speech as\nexpressive and natural as the ground-truth utterances. The dataset and\nevaluation code are freely available. Audio samples can be found at\nhttps://libriquote.github.io/."}
{"id": "2509.04055", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04055", "abs": "https://arxiv.org/abs/2509.04055", "authors": ["Benedikt Geiger", "Fan Liu", "Shihang Lu", "Andrej Rode", "Daniel Gil Gaviria", "Charlotte Muth", "Laurent Schmalen"], "title": "Constellation Shaping for OFDM-ISAC Systems: From Theoretical Bounds to Practical Implementation", "comment": "13 pages, 14 figures, Submitted to IEEE Transactions on\n  Communications (TCOM) for peer review", "summary": "Integrated sensing and communications (ISAC) promises new use cases for\nmobile communication systems by reusing the communication signal for radar-like\nsensing. However, sensing and communications (S&C) impose conflicting\nrequirements on the modulation format, resulting in a tradeoff between their\ncorresponding performance. This paper investigates constellation shaping as a\nmeans to simultaneously improve S&C performance in orthogonal frequency\ndivision multiplexing (OFDM)-based ISAC systems. We begin by deriving how the\ntransmit symbols affect detection performance and derive theoretical lower and\nupper bounds on the maximum achievable information rate under a given sensing\nconstraint. Using an autoencoder-based optimization, we investigate geometric,\nprobabilistic, and joint constellation shaping, where joint shaping combines\nboth approaches, employing both optimal maximum a-posteriori decoding and\npractical bit-metric decoding. Our results show that constellation shaping\nenables a flexible trade-off between S&C, can approach the derived upper bound,\nand significantly outperforms conventional modulation formats. Motivated by its\npractical implementation feasibility, we review probabilistic amplitude shaping\n(PAS) and propose a generalization tailored to ISAC. For this generalization,\nwe propose a low-complexity log-likelihood ratio computation with negligible\nrate loss. We demonstrate that combining conventional and generalized PAS\nenables a flexible and low-complexity tradeoff between S&C, closely approaching\nthe performance of joint constellation shaping."}
{"id": "2509.04390", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04390", "abs": "https://arxiv.org/abs/2509.04390", "authors": ["Hannes Rosseel", "Toon van Waterschoot"], "title": "Accelerated Interactive Auralization of Highly Reverberant Spaces using Graphics Hardware", "comment": "8 pages, 6 figures, submitted to Journal of the Audio Engineering\n  Society", "summary": "Interactive acoustic auralization allows users to explore virtual acoustic\nenvironments in real-time, enabling the acoustic recreation of concert hall or\nHistorical Worship Spaces (HWS) that are either no longer accessible,\nacoustically altered, or impractical to visit. Interactive acoustic synthesis\nrequires real-time convolution of input signals with a set of synthesis filters\nthat model the space-time acoustic response of the space. The acoustics in\nconcert halls and HWS are both characterized by a long reverberation time,\nresulting in synthesis filters containing many filter taps. As a result, the\nconvolution process can be computationally demanding, introducing significant\nlatency that limits the real-time interactivity of the auralization system. In\nthis paper, the implementation of a real-time multichannel loudspeaker-based\nauralization system is presented. This system is capable of synthesizing the\nacoustics of highly reverberant spaces in real-time using GPU-acceleration. A\ncomparison between traditional CPU-based convolution and GPU-accelerated\nconvolution is presented, showing that the latter can achieve real-time\nperformance with significantly lower latency. Additionally, the system\nintegrates acoustic synthesis with acoustic feedback cancellation on the GPU,\ncreating a unified loudspeaker-based auralization framework that minimizes\nprocessing latency."}
{"id": "2509.04309", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04309", "abs": "https://arxiv.org/abs/2509.04309", "authors": ["R. Zhang", "J. Xue", "T. Zhang"], "title": "Reliable Clutter Suppression for Slow-Moving Weak Target Radar Detection", "comment": "25 pages, 20 figures, journal extended by an IEEE ICC conference\n  article", "summary": "Reliable slow-moving weak target detection in complicated environments is\nchallenging due to the masking effects from the surrounding strong reflectors.\nThe traditional Moving Target Indication (MTI) may suppress the echoes from not\nonly the static interference objects (IOs), but also the desired slow-moving\nweak target. According to the low-rank and sparse properties of the\nrange-velocity maps across different radar scans, a novel clutter suppression\nscheme based on the Go decomposition (Godec) framework is proposed in this\npaper. The simulation results show that with the existence of masking effects,\nthe target detection scheme based on Godec clutter suppression can reliably\ndetect the slow-moving weak target, compared to the traditional MTI-based\nscheme. Besides, the time consumption comparison is conducted, demonstrating\nthat the proposed solution is one that sacrifices time complexity in exchange\nfor enhanced reliability. Additionally, the tradeoffs among the number of false\nalarm cells, the detection probability and the iteration times for convergence\nhave been revealed, guiding parameter settings of the proposed solution in\npractical applications. Experiment validation is also conducted to verify the\nproposed solution, providing further insight into the scenarios where the\nsolution is most applicable."}
{"id": "2509.04412", "categories": ["eess.SP", "cs.SY", "eess.SY", "Primary 93C85, Secondary 68T42, 94A12, 90C90", "H.4.3"], "pdf": "https://arxiv.org/pdf/2509.04412", "abs": "https://arxiv.org/abs/2509.04412", "authors": ["Guangyu Lei", "Yuqi Ping", "Tianhao Liang", "Huahao Ding", "Tingting Zhang"], "title": "Relative Localization of UAV Swarms in GNSS-Denied Conditions", "comment": "Manuscript submitted to IEEE Globecom 2025", "summary": "Relative localization of unmanned aerial vehicle (UAV) swarms in global\nnavigation satellite system (GNSS) denied environments is essential for\nemergency rescue and battlefield reconnaissance. Existing methods suffer from\nsignificant localization errors among UAVs due to packet loss and high\ncomputational complexity in large swarms. This paper proposes a\nclustering-based framework where the UAVs simultaneously use communication\nsignals for channel estimation and ranging. Firstly, the spectral clustering is\nutilized to divide the UAV swarm into different sub-clusters, where matrix\ncompletion and multidimensional scaling yield high-precision relative\ncoordinates. Subsequently, a global map is created by the inter-cluster anchor\nfusion. A case study of UAV integrated communication and sensing (ISAC) system\nis presented, where the Orthogonal Time Frequency Space (OTFS) is adopted for\nranging and communication. Experimental results show that the proposed method\nreduces localization errors in large swarms and loss of range information. It\nalso explores the impact of signal parameters on communication and\nlocalization, highlighting the interplay between communication and localization\nperformance."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Multi-Sensor Fusion for Extended Object Tracking Exploiting Active and Passive Radio Signals](https://arxiv.org/abs/2509.03686)
*Hong Zhu,Alexander Venus,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: 提出一种贝叶斯定位方法，融合主动测量和被动雷达式测量，通过多传感器概率数据关联处理测量源不确定性，特别针对用户遮挡LOS的情况进行优化


<details>
  <summary>Details</summary>
Motivation: 解决无线电设备定位中由于用户自身遮挡LOS链路导致的定位不可靠问题，传统方法忽略用户作为扩展物体的影响

Method: 提出贝叶斯方法融合主动测量（设备与锚点间）和被动多基地雷达测量（锚点间经扩展物体反射），开发多传感器多测量概率数据关联算法，建立针对人体用户的扩展物体模型

Result: 在合成和真实无线电测量评估中，该方法在遮挡LOS条件下优于基于点目标假设的传统PDA方法

Conclusion: 考虑用户作为扩展物体的建模和相应的多传感器融合方法能显著提高遮挡LOS条件下的定位精度

Abstract: Reliable and robust positioning of radio devices remains a challenging task
due to multipath propagation, hardware impairments, and interference from other
radio transmitters. A frequently overlooked but critical factor is the agent
itself, e.g., the user carrying the device, which potentially obstructs
line-of-sight (LOS) links to the base stations (anchors). This paper addresses
the problem of accurate positioning in scenarios where LOS links are partially
blocked by the agent. The agent is modeled as an extended object (EO) that
scatters, attenuates, and blocks radio signals. We propose a Bayesian method
that fuses ``active'' measurements (between device and anchors) with
``passive'' multistatic radar-type measurements (between anchors, reflected by
the EO). To handle measurement origin uncertainty, we introduce an multi-sensor
and multiple-measurement probabilistic data association (PDA) algorithm that
jointly fuses all EO-related measurements. Furthermore, we develop an EO model
tailored to agents such as human users, accounting for multiple reflections
scattered off the body surface, and propose a simplified variant for
low-complexity implementation. Evaluation on both synthetic and real radio
measurements demonstrates that the proposed algorithm outperforms conventional
PDA methods based on point target assumptions, particularly during and after
obstructed line-of-sight (OLOS) conditions.

</details>


### [2] [Sensor placement for sparse force reconstruction](https://arxiv.org/abs/2509.03825)
*Jeunghoon Lee*

Main category: eess.SP

TL;DR: 基于Gram矩阵的传感器布局策略，通过最小化Gram矩阵非对角能量来优化稀疏力重构精度


<details>
  <summary>Details</summary>
Motivation: 传统启发式传感器布局方法在频域力重构中效果有限，需要基于物理洞察的系统性传感器布置策略来提高重构精度

Method: 提出Gram矩阵模态分解方法，发现目标频率附近少数模态主导结构；开发贪心算法选择传感器位置以最小化Gram矩阵非对角能量

Result: 数值模拟和实验验证表明，该方法相比启发式布局能提供更鲁棒和准确的力估计

Conclusion: Gram矩阵模态分解为传感器布局提供了物理洞察，贪心算法能有效实现优化布局，显著提升频域稀疏力重构性能

Abstract: The present study proposes a Gram-matrix-based sensor placement strategy for
sparse force reconstruction in the frequency domain. A modal decomposition of
the Gram matrix reveals that its structure is dominated by a few modes near the
target frequency, and that each modal contribution reflects the spatial
correlation of the corresponding mode shape. This suggests that placing sensors
near nodal regions where spatial correlation is low can reduce coherence in the
frequency response function (FRF) matrix and improve force reconstruction
accuracy. To translate the physical insight into a practical design framework,
a greedy algorithm is proposed to select sensor locations that minimize the
off-diagonal energy of the Gram matrix. Numerical simulations and experimental
validations demonstrate that the proposed method yields robust and accurate
force estimation, outperforming heuristic sensor layouts.

</details>


### [3] [A Low-Cost Open-Source BLE-Based Asian Hornet Tracking System](https://arxiv.org/abs/2509.03979)
*Gilles Callebaut,Jan Van Moer*

Main category: eess.SP

TL;DR: 基于蓝牙BLE和软件定义无线电的低成本开源跟踪系统，用于定位亚洲大黄蜂巢穴，实现了360米通信距离和50米角度分辨率。


<details>
  <summary>Details</summary>
Motivation: 亚洲大黄蜂对生态系统和养蜂业构成严重威胁，但传统的巢穴定位方法需要耗时的手动三角测量。需要一种低成本、高效的解决方案。

Method: 设计轻量化BLE标签和GNU Radio实现的SDR接收机。穿越BLE栈在未编码物理层嵌入自定义伪噪序列，通过相关检测实现检测。使用八木轴天线和PlutoSDR进行数字波束扫描来确定标签方向。

Result: 现场测试显示系统在50米距离上可靠地实现角度分辨，通信距离达到360米。虽然调制方式增加了接收机复杂度，但为未来多通道扩频和标签识别等改进提供了可能。

Conclusion: 该开源设计为黄蜂跟踪和环境监测等相关应用提供了一个可扩展的框架，具有重要的应用价值。

Abstract: The Asian hornet (Vespa velutina) poses a serious threat to ecosystems and
beekeeping. Locating nests is essential, but usually involves time-consuming
manual triangulation. We present a low-cost, open-source tracking system based
on Bluetooth Low Energy (BLE). The system consists of a lightweight BLE tag and
a software-defined radio (SDR) receiver implemented in GNU Radio. By bypassing
the BLE stack, we embed a custom pseudo-noise (PN) sequence in the uncoded PHY
for correlation-based detection. Using a Yagi antenna and PlutoSDR, the
receiver performs digital beam sweeping to determine the tag's direction. Field
tests show reliable angular resolution at 50m and a communication range up to
360m. While our modulation increases receiver complexity, it enables future
improvements such as multichannel spreading and tag identification. The design
is fully open-source and provides a scalable framework for hornet tracking and
related applications in environmental monitoring.

</details>


### [4] [Approximate Message Passing for Multi-Preamble Detection in OTFS Random Access](https://arxiv.org/abs/2509.03980)
*Alessandro Mirri,Vishnu Teja Kunde,Enrico Paolini,Jean-Francois Chamberland*

Main category: eess.SP

TL;DR: 基于OTFS信号的多前导碰检测问题，通过推出新的AMP算法实现双重稀疏性，在复数域解决结构化稀疏恢复问题


<details>
  <summary>Details</summary>
Motivation: 解决基于正交时频空间(OTFS)信号的随机访问系统中多前导碰检测的挑战，该问题在复数域中被形式化为结构化稀疏恢复问题

Method: 推出一种新的近似消息传递(AMP)算法，强化双重稀疏性：前导碰的稀疏选择和OTFS信号在延迟-多普勒域的本质稀疏性，设计了新的AMP去噪器

Result: 模拟结果显示，所提方法实现了稳健的检测性能，与最先进技术相比获得了显著的性能提升

Conclusion: 通过提出的新型AMP算法，成功解决了OTFS系统中的多前导碰检测问题，为结构化稀疏恢复提供了有效的解决方案

Abstract: This article addresses the problem of multiple preamble detection in random
access systems based on orthogonal time frequency space (OTFS) signaling. This
challenge is formulated as a structured sparse recovery problem in the complex
domain. To tackle it, the authors propose a new approximate message passing
(AMP) algorithm that enforces double sparsity: the sparse selection of
preambles and the inherent sparsity of OTFS signals in the delay-Doppler
domain. From an algorithmic standpoint, the non-separable complex sparsity
constraint necessitates a careful derivation and leads to the design of a novel
AMP denoiser. Simulation results demonstrate that the proposed method achieves
robust detection performance and delivers significant gains over
state-of-the-art techniques.

</details>


### [5] [Joint Frequency-Space Sparse Reconstruction for DOA Estimation under Coherent Sources and Amplitude-Phase Errors](https://arxiv.org/abs/2509.03983)
*Yutong Chen,Cong Zhou,Changsheng You,Shuo Shi*

Main category: eess.SP

TL;DR: 基于频域和空间稀疏性的聚合频率-空间稀疏重构方法，用于处理相关源和阵列振幅相位错误的DOA估计问题


<details>
  <summary>Details</summary>
Motivation: 解决相关源存在时的DOA估计困难，以及阵列振幅相位错误对估计精度的影响

Method: 使用辅助源构建真实指向向量(RSVs)作为基矩阵进行错误补偿，利用频域稀疏性和空间稀疏性通过稀疏重构方法进行DOA估计

Result: 数值实验表明方法在相关源情况下较各种基准方案具有更高的估计精度

Conclusion: 该方法无需迭代优化，计算复杂度低，能够有效处理相关源和阵列错误问题

Abstract: In this letter, we propose a joint frequency-space sparse reconstruction
method for direction-of-arrival (DOA) estimation, which effectively addresses
the issues arising from the existence of coherent sources and array
amplitude-phase errors. Specifically, by using an auxiliary source with known
angles, we first construct the real steering vectors (RSVs) based on the
spectral peaks of received signals in the frequency domain, which serve as a
complete basis matrix for compensation for amplitude-phase errors. Then, we
leverage the spectral sparsity of snapshot data in the frequency domain and the
spatial sparsity of incident directions to perform the DOA estimation according
to the sparse reconstruction method. The proposed method does not require
iterative optimization, hence exhibiting low computational complexity.
Numerical results demonstrate that the proposed DOA estimation method achieves
higher estimation accuracy for coherent sources as compared to various
benchmark schemes.

</details>


### [6] [Robust MIMO Semantic Communication with Imperfect CSI via Knowledge Distillation](https://arxiv.org/abs/2509.04005)
*Mingze Gong,Shuoyao Wang,Shijian Gao,Jia Yan,Suzhi Bi*

Main category: eess.SP

TL;DR: HANA-JSCC是一种针对MIMO语义通信系统的信道矩阵和噪声自适应方法，解决了现有系统假设完美信道估计的不切实际问题。


<details>
  <summary>Details</summary>
Motivation: 现有MIMO语义通信系统假设完美信道矩阵估计，但在实际中由于硬件和导频开销限制，这种假设不切实际，需要解决信道估计误差问题。

Method: 提出信道矩阵适配器与信道编解码器协作适应不准确的信道状态信息；引入两阶段训练策略和知识蒸馏解决估计信道矩阵与真实信道矩阵之间的一对多不适定问题。

Result: 在各种噪声和估计误差水平下，HANA-JSCC比最先进基准方法平均性能提升0.40~0.54dB。

Conclusion: HANA-JSCC有效解决了MIMO语义通信系统中的信道估计误差问题，通过自适应机制和两阶段训练策略显著提升了系统性能。

Abstract: Semantic communication (SemComm) has emerged as a new communication paradigm.
To enhance efficiency, multiple-input-multiple-output (MIMO) technology has
been further integrated into SemComm systems. However, existing MIMO SemComm
systems assume perfect channel matrix estimation for channel-adaptive joint
source-channel coding, which is impractical due to hardware and pilot overhead
constraints. In this paper, we propose a semantic image transmission system
with channel matrix and channel noise adaptation, named HANA-JSCC, to cope with
channel estimation errors in MIMO systems. We propose a channel matrix adaptor
that collaborates with the channel codec to adapt to misaligned channel state
information, thereby mitigating the impact of estimation errors. Since the
relationship between the estimated channel matrix and true channel matrix is
ill-posed (one-to-many), we further introduce a two-stage training strategy
with knowledge distillation to overcome the convergence difficulties caused by
the ill-posed problem. Comparing with the state-of-the-art benchmarks,
HANA-JSCC achieves $0.40\sim0.54$dB higher average performance across various
noise and estimation error levels in various datasets.

</details>


### [7] [Constellation Shaping for OFDM-ISAC Systems: From Theoretical Bounds to Practical Implementation](https://arxiv.org/abs/2509.04055)
*Benedikt Geiger,Fan Liu,Shihang Lu,Andrej Rode,Daniel Gil Gaviria,Charlotte Muth,Laurent Schmalen*

Main category: eess.SP

TL;DR: 该论文研究通过星座整形技术来提升正交频分复用(OFDM)集成感知与通信(ISAC)系统的性能，解决了感知与通信之间的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)系统在复用通信信号进行雷达式感知时，感知和通信对调制格式存在冲突要求，导致性能权衡问题。

Method: 采用自编码器优化方法，研究几何、概率和联合星座整形技术，并提出适用于ISAC的概率幅度整形(PAS)通用化方法及低复杂度对数似然比计算。

Result: 星座整形技术能够实现感知与通信性能的灵活权衡，接近理论上限，显著优于传统调制格式。通用化PAS结合传统方法可实现低复杂度且接近联合整形性能的权衡方案。

Conclusion: 星座整形是提升ISAC系统性能的有效方法，特别是提出的通用化PAS方案在保持低复杂度的同时能够接近最优性能，具有实际应用可行性。

Abstract: Integrated sensing and communications (ISAC) promises new use cases for
mobile communication systems by reusing the communication signal for radar-like
sensing. However, sensing and communications (S&C) impose conflicting
requirements on the modulation format, resulting in a tradeoff between their
corresponding performance. This paper investigates constellation shaping as a
means to simultaneously improve S&C performance in orthogonal frequency
division multiplexing (OFDM)-based ISAC systems. We begin by deriving how the
transmit symbols affect detection performance and derive theoretical lower and
upper bounds on the maximum achievable information rate under a given sensing
constraint. Using an autoencoder-based optimization, we investigate geometric,
probabilistic, and joint constellation shaping, where joint shaping combines
both approaches, employing both optimal maximum a-posteriori decoding and
practical bit-metric decoding. Our results show that constellation shaping
enables a flexible trade-off between S&C, can approach the derived upper bound,
and significantly outperforms conventional modulation formats. Motivated by its
practical implementation feasibility, we review probabilistic amplitude shaping
(PAS) and propose a generalization tailored to ISAC. For this generalization,
we propose a low-complexity log-likelihood ratio computation with negligible
rate loss. We demonstrate that combining conventional and generalized PAS
enables a flexible and low-complexity tradeoff between S&C, closely approaching
the performance of joint constellation shaping.

</details>


### [8] [Reliable Clutter Suppression for Slow-Moving Weak Target Radar Detection](https://arxiv.org/abs/2509.04309)
*R. Zhang,J. Xue,T. Zhang*

Main category: eess.SP

TL;DR: 基于Go分解(Godec)框架的新型杂波压制方案，能够在复杂环境中可靠检测慢速弱目标，充分利用了范围-速度图的低秩和稀疏特性。


<details>
  <summary>Details</summary>
Motivation: 传统的移动目标指示(MTI)技术在复杂环境中可能会压制慢速弱目标的回波，导致检测困难。需要一种能够在强反射体遮蔓效应下保持目标检测能力的新方案。

Method: 提出了一种基于Go分解(Godec)框架的杂波压制方案。该方法利用了不同雷达扫描中范围-速度图的低秩和稀疏特性来分离杂波和目标。

Result: 模拟结果显示，在存在遮蔓效应的情况下，基于Godec的方案能够比传统MTI方案更可靠地检测慢速弱目标。虽然时间消耗更高，但以提高可靠性为代价。实验验证了方案的有效性。

Conclusion: 该研究提出了一种有效的慢速弱目标检测方案，通过Godec框架实现了更好的杂波压制效果。虽然需要投入更多计算资源，但在需要高可靠性检测的应用场景中具有重要价值。研究还揭示了误报率、检测概率和迭代次数之间的权衡关系，为实际应用中的参数设置提供了指导。

Abstract: Reliable slow-moving weak target detection in complicated environments is
challenging due to the masking effects from the surrounding strong reflectors.
The traditional Moving Target Indication (MTI) may suppress the echoes from not
only the static interference objects (IOs), but also the desired slow-moving
weak target. According to the low-rank and sparse properties of the
range-velocity maps across different radar scans, a novel clutter suppression
scheme based on the Go decomposition (Godec) framework is proposed in this
paper. The simulation results show that with the existence of masking effects,
the target detection scheme based on Godec clutter suppression can reliably
detect the slow-moving weak target, compared to the traditional MTI-based
scheme. Besides, the time consumption comparison is conducted, demonstrating
that the proposed solution is one that sacrifices time complexity in exchange
for enhanced reliability. Additionally, the tradeoffs among the number of false
alarm cells, the detection probability and the iteration times for convergence
have been revealed, guiding parameter settings of the proposed solution in
practical applications. Experiment validation is also conducted to verify the
proposed solution, providing further insight into the scenarios where the
solution is most applicable.

</details>


### [9] [Relative Localization of UAV Swarms in GNSS-Denied Conditions](https://arxiv.org/abs/2509.04412)
*Guangyu Lei,Yuqi Ping,Tianhao Liang,Huahao Ding,Tingting Zhang*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于聚类的方案，通过通信信号同时进行通信和测距，解决GNSS被废止环境下无人机群相对定位问题，降低定位误差和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在GNSS被废止环境下，无人机群相对定位对于急救救援和战场侵查至关重要。现有方法存在包损失和计算复杂度高的问题，导致定位误差较大。

Method: 采用谱聚类将无人机群划分为子聚类，通过矩阵补全和多维尺度规划获得高精度相对坐标，然后通过聚类间锚点融合构建全局地图。以OTFS技术为例进行通信与感知集成系统的研究。

Result: 实验结果显示，该方法能够有效降位大型群体中的定位误差和距离信息损失，并探索了信号参数对通信与定位性能的影响。

Conclusion: 该研究提出的聚类基础框架能够有效解决GNSS被废止环境下的相对定位问题，显示了通信与定位性能之间的相互作用，为无人机群应用提供了有效解决方案。

Abstract: Relative localization of unmanned aerial vehicle (UAV) swarms in global
navigation satellite system (GNSS) denied environments is essential for
emergency rescue and battlefield reconnaissance. Existing methods suffer from
significant localization errors among UAVs due to packet loss and high
computational complexity in large swarms. This paper proposes a
clustering-based framework where the UAVs simultaneously use communication
signals for channel estimation and ranging. Firstly, the spectral clustering is
utilized to divide the UAV swarm into different sub-clusters, where matrix
completion and multidimensional scaling yield high-precision relative
coordinates. Subsequently, a global map is created by the inter-cluster anchor
fusion. A case study of UAV integrated communication and sensing (ISAC) system
is presented, where the Orthogonal Time Frequency Space (OTFS) is adopted for
ranging and communication. Experimental results show that the proposed method
reduces localization errors in large swarms and loss of range information. It
also explores the impact of signal parameters on communication and
localization, highlighting the interplay between communication and localization
performance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Hierarchical Sparse Sound Field Reconstruction with Spherical and Linear Microphone Arrays](https://arxiv.org/abs/2509.03902)
*Shunxi Xu,Craig T. Jin*

Main category: eess.AS

TL;DR: 这篇论文提出了一种两阶段稀疏恢复框架，通过结合球形麦克风数组和四个线性麦克风数组，显著提高了深度污浸环境下的空间分辨率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 球形麦克风数组的空间分辨率受到球象和质的基本限制，且在深度污浸环境下性能会昂化。需要一种能够利用不同麦克风数组空间特性互补的方法来提高性能。

Method: 提出两阶段稀疏恢复框架与残差精炼，将球形麦克风数组作为主要估计器，并使用四个周围线性麦克风数组作为空间互补的精炼器。

Result: 模拟结果显示，该SMA-LMA方法在不同污浸条件下显著提高了空间能量地图重建的性能，超过了单独使用SMA和直接一步联合处理的方法。

Conclusion: 该框架在复杂声学环境中有效地提高了空间保真度和稳健性，为高性能声场分析提供了一种有效的解决方案。

Abstract: Spherical microphone arrays (SMAs) are widely used for sound field analysis,
and sparse recovery (SR) techniques can significantly enhance their spatial
resolution by modeling the sound field as a sparse superposition of dominant
plane waves. However, the spatial resolution of SMAs is fundamentally limited
by their spherical harmonic order, and their performance often degrades in
reverberant environments. This paper proposes a two-stage SR framework with
residue refinement that integrates observations from a central SMA and four
surrounding linear microphone arrays (LMAs). The core idea is to exploit
complementary spatial characteristics by treating the SMA as a primary
estimator and the LMAs as a spatially complementary refiner. Simulation results
demonstrate that the proposed SMA-LMA method significantly enhances spatial
energy map reconstruction under varying reverberation conditions, compared to
both SMA-only and direct one-step joint processing. These results demonstrate
the effectiveness of the proposed framework in enhancing spatial fidelity and
robustness in complex acoustic environments.

</details>


### [11] [LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis](https://arxiv.org/abs/2509.04072)
*Gaspard Michel,Elena V. Epure,Christophe Cerisara*

Main category: eess.AS

TL;DR: LibriQuote是一个从有声读物中提取的英语语料库，包含12.7K小时非表达性语音和5.3K小时表达性语音，用于微调和基准测试表达性零样本TTS系统，并提供带有上下文和伪标签的丰富标注。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语音语料库中表达性语音的比例不明确，而现有的表达性语音语料库规模较小且主要用于TTS系统基准测试，需要更大规模且专门用于表达性TTS训练和评估的数据集。

Method: 从朗读的有声读物中构建LibriQuote数据集，包含非表达性语音和表达性语音子集，为表达性语音提供上下文语境和描述性动词/副词的伪标签，并设计专门的测试集来评估TTS系统在保持音色同时合成表达性语音的能力。

Result: 测试集涵盖了广泛的情感和口音范围，主观和客观评估表明，在LibriQuote上微调的TTS系统显著提高了语音可懂度，但现有系统仍无法合成与真实语音同样表达性和自然的语音。

Conclusion: LibriQuote为表达性零样本TTS系统的训练和评估提供了有价值的资源，证明了大规模表达性语音数据对提升TTS性能的重要性，但当前系统在表达性合成方面仍有改进空间。

Abstract: Text-to-speech (TTS) systems have recently achieved more expressive and
natural speech synthesis by scaling to large speech datasets. However, the
proportion of expressive speech in such large-scale corpora is often unclear.
Besides, existing expressive speech corpora are typically smaller in scale and
primarily used for benchmarking TTS systems. In this paper, we introduce the
LibriQuote dataset, an English corpus derived from read audiobooks, designed
for both fine-tuning and benchmarking expressive zero-shot TTS system. The
training dataset includes 12.7K hours of read, non-expressive speech and 5.3K
hours of mostly expressive speech drawn from character quotations. Each
utterance in the expressive subset is supplemented with the context in which it
was written, along with pseudo-labels of speech verbs and adverbs used to
describe the quotation (\textit{e.g. ``he whispered softly''}). Additionally,
we provide a challenging 7.5 hour test set intended for benchmarking TTS
systems: given a neutral reference speech as input, we evaluate system's
ability to synthesize an expressive utterance while preserving reference
timbre. We validate qualitatively the test set by showing that it covers a wide
range of emotions compared to non-expressive speech, along with various
accents. Extensive subjective and objective evaluations show that fine-tuning a
baseline TTS system on LibriQuote significantly improves its synthesized speech
intelligibility, and that recent systems fail to synthesize speech as
expressive and natural as the ground-truth utterances. The dataset and
evaluation code are freely available. Audio samples can be found at
https://libriquote.github.io/.

</details>


### [12] [Test-Time Adaptation for Speech Enhancement via Domain Invariant Embedding Transformation](https://arxiv.org/abs/2509.04280)
*Tobias Raichle,Niels Edinger,Bin Yang*

Main category: eess.AS

TL;DR: LaDen是首个专门为语音增强设计的测试时自适应方法，利用预训练语音表示进行潜在空间降噪，通过线性变换近似干净语音表示，无需目标域标注数据即可实现跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习语音增强模型在训练分布匹配时表现优异，但在真实世界环境域偏移时性能下降的问题。

Method: 利用预训练语音表示进行潜在降噪，通过线性变换从噪声嵌入中近似干净语音表示，实现跨域泛化的伪标注。

Result: 在包含噪声类型、说话人特征和语言变化的多数据集基准测试中，LaDen在感知指标上持续优于基线方法，特别在说话人和语言域偏移方面表现突出。

Conclusion: LaDen通过潜在降噪和伪标注机制，有效实现了语音增强模型的测试时自适应，在多样声学环境中展现出强大的跨域泛化能力。

Abstract: Deep learning-based speech enhancement models achieve remarkable performance
when test distributions match training conditions, but often degrade when
deployed in unpredictable real-world environments with domain shifts. To
address this challenge, we present LaDen (latent denoising), the first
test-time adaptation method specifically designed for speech enhancement. Our
approach leverages powerful pre-trained speech representations to perform
latent denoising, approximating clean speech representations through a linear
transformation of noisy embeddings. We show that this transformation
generalizes well across domains, enabling effective pseudo-labeling for target
domains without labeled target data. The resulting pseudo-labels enable
effective test-time adaptation of speech enhancement models across diverse
acoustic environments. We propose a comprehensive benchmark spanning multiple
datasets with various domain shifts, including changes in noise types, speaker
characteristics, and languages. Our extensive experiments demonstrate that
LaDen consistently outperforms baseline methods across perceptual metrics,
particularly for speaker and language domain shifts.

</details>


### [13] [Accelerated Interactive Auralization of Highly Reverberant Spaces using Graphics Hardware](https://arxiv.org/abs/2509.04390)
*Hannes Rosseel,Toon van Waterschoot*

Main category: eess.AS

TL;DR: 这篇论文提出了一种基于GPU加速的实时多速道扬声器音响模拟系统，用于模拟高湯演空间的音响效果，解决传统CPU卷积计算费时带来的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 交互式音响模拟需要实时卷积处理长湯演空间的音响响应，传统CPU方式计算费时带来高延迟，限制了系统的实时交互性。

Method: 开发了一种GPU加速的多速道扬声器音响模拟系统，通过GPU并行计算实现高效卷积，同时集成了音响反馈消除功能。

Result: 对比实验显示，GPU加速卷积方案能够实现实时性能，并且延迟显著低于传统CPU方式。

Conclusion: 该GPU加速系统为高湯演空间的实时音响模拟提供了高效解决方案，通过降低处理延迟提升了系统的交互性能力。

Abstract: Interactive acoustic auralization allows users to explore virtual acoustic
environments in real-time, enabling the acoustic recreation of concert hall or
Historical Worship Spaces (HWS) that are either no longer accessible,
acoustically altered, or impractical to visit. Interactive acoustic synthesis
requires real-time convolution of input signals with a set of synthesis filters
that model the space-time acoustic response of the space. The acoustics in
concert halls and HWS are both characterized by a long reverberation time,
resulting in synthesis filters containing many filter taps. As a result, the
convolution process can be computationally demanding, introducing significant
latency that limits the real-time interactivity of the auralization system. In
this paper, the implementation of a real-time multichannel loudspeaker-based
auralization system is presented. This system is capable of synthesizing the
acoustics of highly reverberant spaces in real-time using GPU-acceleration. A
comparison between traditional CPU-based convolution and GPU-accelerated
convolution is presented, showing that the latter can achieve real-time
performance with significantly lower latency. Additionally, the system
integrates acoustic synthesis with acoustic feedback cancellation on the GPU,
creating a unified loudspeaker-based auralization framework that minimizes
processing latency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [14] [SwinSRGAN: Swin Transformer-based Generative Adversarial Network for High-Fidelity Speech Super-Resolution](https://arxiv.org/abs/2509.03913)
*Jiajun Yuan,Xiaochen Wang,Yuhang Xiao,Yulin Wu,Chenhao Hu,Xueyang Lv*

Main category: cs.SD

TL;DR: SwinSRGAN是一个端到端的语音超分辨率框架，使用Swin Transformer U-Net处理MDCT幅度，通过混合对抗训练和多频段判别器，在48kHz采样率下实时运行，在标准基准和零样本测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决现有语音超分辨率系统在两阶段mel-vocoder管道中的表示不匹配问题，CNN生成器对高频内容过度平滑，以及扩散和流模型计算成本高、跨域鲁棒性有限的问题

Method: 基于Swin Transformer的U-Net架构，使用MDCT幅度作为输入，结合时域MPD/MSD判别器和专门针对高频段的多频段MDCT判别器的混合对抗方案，在arcsinh压缩的MDCT上采用稀疏感知正则化器

Result: 在标准基准测试中降低了客观误差并提高了ABX偏好分数，在HiFi-TTS零样本测试中无需微调即优于NVSR和mdctGAN，展示了强大的跨数据集泛化能力

Conclusion: SwinSRGAN提供了一个高效、实时的端到端语音超分辨率解决方案，能够处理不同采样率输入并统一上采样到48kHz，在性能和泛化能力方面均优于现有方法

Abstract: Speech super-resolution (SR) reconstructs high-frequency content from
low-resolution speech signals. Existing systems often suffer from
representation mismatch in two-stage mel-vocoder pipelines and from
over-smoothing of hallucinated high-band content by CNN-only generators.
Diffusion and flow models are computationally expensive, and their robustness
across domains and sampling rates remains limited. We propose SwinSRGAN, an
end-to-end framework operating on Modified Discrete Cosine Transform (MDCT)
magnitudes. It is a Swin Transformer-based U-Net that captures long-range
spectro-temporal dependencies with a hybrid adversarial scheme combines
time-domain MPD/MSD discriminators with a multi-band MDCT discriminator
specialized for the high-frequency band. We employs a sparse-aware regularizer
on arcsinh-compressed MDCT to better preserve transient components. The system
upsamples inputs at various sampling rates to 48 kHz in a single pass and
operates in real time. On standard benchmarks, SwinSRGAN reduces objective
error and improves ABX preference scores. In zero-shot tests on HiFi-TTS
without fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong
generalization across datasets

</details>


### [15] [WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with Multi-dimensional Annotation](https://arxiv.org/abs/2509.03959)
*Longhao Li,Zhao Guo,Hongjie Chen,Yuhang Dai,Ziyu Zhang,Hongfei Xue,Tianlun Zuo,Chengyou Wang,Shuiyuan Wang,Jie Li,Xin Xu,Hui Bu,Binbin Zhang,Ruibin Yuan,Ziya Zhou,Wei Xue,Lei Xie*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The development of speech understanding and generation has been significantly
accelerated by the availability of large-scale, high-quality speech datasets.
Among these, ASR and TTS are regarded as the most established and fundamental
tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9
million native speakers worldwide, limited annotated resources have hindered
progress and resulted in suboptimal ASR and TTS performance. To address this
challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building
large-scale speech corpus with multi-dimensional annotation tailored for speech
understanding and generation. It comprises six modules: Audio Collection,
Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech
Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich
and high-quality annotations. Based on this pipeline, we release
WenetSpeech-Yue, the first large-scale Cantonese speech corpus with
multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10
domains with annotations including ASR transcription, text confidence, speaker
identity, age, gender, speech quality scores, among other annotations. We also
release WSYue-eval, a comprehensive Cantonese benchmark with two components:
WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long
utterances, code-switching, and diverse acoustic conditions, and
WSYue-TTS-eval, with base and coverage subsets for standard and generalization
testing. Experimental results show that models trained on WenetSpeech-Yue
achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and
TTS systems, including commercial and LLM-based models, highlighting the value
of our dataset and pipeline.

</details>


### [16] [Open-Source Full-Duplex Conversational Datasets for Natural and Interactive Speech Synthesis](https://arxiv.org/abs/2509.04093)
*Zhitong Zhou,Qingqing Zhang,Lei Luo,Jiechen Liu,Ruohua Zhou*

Main category: cs.SD

TL;DR: 提供中英双语对话语料集，包含15小时自然对话，用于提升对话TTS系统的自然性和交互性


<details>
  <summary>Details</summary>
Motivation: 完整双工自然对话数据对于提高合成语音的自然性和交互性至关重要，需要更现实的对话数据来改善TTS系统

Method: 收集中英双语对话数据集，包含15小时隔离房间录制的自然对话，每个讲者独立音频轨道，涵盖多样话题和交互模式，包括重叠、回应、笑声等非语言声音

Result: 使用该数据集微调TTS模型后，主观和客观评估指标都有显著提升，合成语音的自然性和对话现实性获得改善

Conclusion: 该对话语料集能够有效提升对话语音合成的性能，所有数据、注释和代码已开源发布，以便进一步研究

Abstract: Full-duplex, spontaneous conversational data are essential for enhancing the
naturalness and interactivity of synthesized speech in conversational TTS
systems. We present two open-source dual-track conversational speech datasets,
one in Chinese and one in English, designed to enhance the naturalness of
synthesized speech by providing more realistic conversational data. The two
datasets contain a total of 15 hours of natural, spontaneous conversations
recorded in isolated rooms, which produces separate high-quality audio tracks
for each speaker. The conversations cover diverse daily topics and domains,
capturing realistic interaction patterns including frequent overlaps,
backchannel responses, laughter, and other non-verbal vocalizations. We
introduce the data collection procedure, transcription and annotation methods.
We demonstrate the utility of these corpora by fine-tuning a baseline TTS model
with the proposed datasets. The fine-tuned TTS model achieves higher subjective
and objective evaluation metrics compared to the baseline, indicating improved
naturalness and conversational realism in synthetic speech. All data,
annotations, and supporting code for fine-tuning and evaluation are made
available to facilitate further research in conversational speech synthesis.

</details>


### [17] [Enhancing Self-Supervised Speaker Verification Using Similarity-Connected Graphs and GCN](https://arxiv.org/abs/2509.04147)
*Zhaorui Sun,Yihao Chen,Jialong Wang,Minqiang Xu,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 本文提出了一种基于相似性连接图和图卷积神经网络的改进聚类框架，用于改善自监督语音识别中的伪标签质量，提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习方法DINO在语音验证中通过聚类生成伪标签时存在皂声问题，这会降低识别性能。需要一种方法来优化聚类过程，提高伪标签的准确性。

Method: 使用相似性连接图和图卷积神经网络(GCN)来改进聚类框架。GCN能够建模结构化数据并利用节点间的关系信息，从而优化聚类过程。

Result: 实验结果显示，该方法显著提高了系统性能，改善了伪标签的准确性和自监督语音验证系统的稳健性。

Conclusion: 该研究为自监督语音验证提供了一种新的接近，通过结合相似性连接图和GCN技术，有效解决了聚类产生皂声伪标签的问题，提高了系统的性能和可靠性。

Abstract: With the continuous development of speech recognition technology, speaker
verification (SV) has become an important method for identity authentication.
Traditional SV methods rely on handcrafted feature extraction, while deep
learning has significantly improved system performance. However, the scarcity
of labeled data still limits the widespread application of deep learning in SV.
Self-supervised learning, by mining latent information in large unlabeled
datasets, enhances model generalization and is a key technology to address this
issue.
  DINO is an efficient self-supervised learning method that generates
pseudo-labels from unlabeled speech data through clustering, supporting
subsequent training. However, clustering may produce noisy pseudo-labels, which
can reduce overall recognition performance.
  To address this issue, this paper proposes an improved clustering framework
based on similarity connection graphs and Graph Convolutional Networks. By
leveraging GCNs' ability to model structured data and incorporating relational
information between nodes in the similarity connection graph, the clustering
process is optimized, improving pseudo-label accuracy and enhancing the
robustness and performance of the self-supervised speaker verification system.
Experimental results show that this method significantly improves system
performance and provides a new approach for self-supervised speaker
verification.
  Index Terms: Speaker Verification, Self-Supervised Learning, DINO, Clustering
Algorithm, Graph Convolutional Network, Similarity Connection Graph

</details>


### [18] [Wav2DF-TSL: Two-stage Learning with Efficient Pre-training and Hierarchical Experts Fusion for Robust Audio Deepfake Detection](https://arxiv.org/abs/2509.04161)
*Yunqi Hao,Yihao Chen,Minqiang Xu,Jianbo Zhan,Liang He,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 提出Wav2DF-TSL两阶段学习策略，通过预训练和分层专家融合实现鲁棒的音频深度伪造检测，在跨域数据集上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有SSL模型主要依赖大规模真实语音预训练，缺乏伪造样本学习，导致在音频深度伪造检测任务微调时容易受到域偏差影响

Method: 两阶段策略：预训练阶段使用适配器从3000小时无标签伪造语音中高效学习伪影；微调阶段提出分层自适应专家混合(HA-MoE)方法，通过门控路由动态融合多级伪造线索

Result: 在四个基准数据集上显著超越基线系统，特别是在跨域In-the-wild数据集上，等错误率相对提升27.5%，优于现有最先进系统

Conclusion: 该方法通过有效学习伪造样本特征和动态融合多级线索，显著提升了音频深度伪造检测的鲁棒性和跨域性能

Abstract: In recent years, self-supervised learning (SSL) models have made significant
progress in audio deepfake detection (ADD) tasks. However, existing SSL models
mainly rely on large-scale real speech for pre-training and lack the learning
of spoofed samples, which leads to susceptibility to domain bias during the
fine-tuning process of the ADD task. To this end, we propose a two-stage
learning strategy (Wav2DF-TSL) based on pre-training and hierarchical expert
fusion for robust audio deepfake detection. In the pre-training stage, we use
adapters to efficiently learn artifacts from 3000 hours of unlabelled spoofed
speech, improving the adaptability of front-end features while mitigating
catastrophic forgetting. In the fine-tuning stage, we propose the hierarchical
adaptive mixture of experts (HA-MoE) method to dynamically fuse multi-level
spoofing cues through multi-expert collaboration with gated routing.
Experimental results show that the proposed method significantly outperforms
the baseline system on all four benchmark datasets, especially on the
cross-domain In-the-wild dataset, achieving a 27.5% relative improvement in
equal error rate (EER), outperforming the existing state-of-the-art systems.
Index Terms: audio deepfake detection, self-supervised learning,
parameter-efficient fine-tuning, mixture of experts

</details>


### [19] [PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music](https://arxiv.org/abs/2509.04215)
*Hayeon Bang,Eunjin Choi,Seungheon Doh,Juhan Nam*

Main category: cs.SD

TL;DR: PianoBind是一个专门针对钢琴音乐的多模态联合嵌入模型，能够有效捕捉独奏钢琴音乐的细微语义差异，在文本到音乐检索任务上优于通用音乐表示模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用音乐表示模型难以捕捉同质独奏钢琴音乐的细微语义差异，且现有钢琴专用模型多为单模态，无法充分利用钢琴音乐的多模态特性（音频、符号、文本）。

Method: 提出PianoBind钢琴专用多模态联合嵌入模型，系统研究多源训练策略和模态利用方法，针对小规模和同质钢琴数据集优化联合嵌入框架。

Result: 实验表明PianoBind能够学习到有效捕捉钢琴音乐细微差别的多模态表示，在领域内和领域外钢琴数据集上的文本到音乐检索性能优于通用音乐联合嵌入模型。

Conclusion: 该模型不仅解决了钢琴音乐表示的特殊挑战，其设计选择也为其他同质数据集的多模态表示学习提供了可复用的见解。

Abstract: Solo piano music, despite being a single-instrument medium, possesses
significant expressive capabilities, conveying rich semantic information across
genres, moods, and styles. However, current general-purpose music
representation models, predominantly trained on large-scale datasets, often
struggle to captures subtle semantic distinctions within homogeneous solo piano
music. Furthermore, existing piano-specific representation models are typically
unimodal, failing to capture the inherently multimodal nature of piano music,
expressed through audio, symbolic, and textual modalities. To address these
limitations, we propose PianoBind, a piano-specific multimodal joint embedding
model. We systematically investigate strategies for multi-source training and
modality utilization within a joint embedding framework optimized for capturing
fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano
datasets. Our experimental results demonstrate that PianoBind learns multimodal
representations that effectively capture subtle nuances of piano music,
achieving superior text-to-music retrieval performance on in-domain and
out-of-domain piano datasets compared to general-purpose music joint embedding
models. Moreover, our design choices offer reusable insights for multimodal
representation learning with homogeneous datasets beyond piano music.

</details>


### [20] [AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds](https://arxiv.org/abs/2509.04345)
*Qizhou Wang,Hanxun Huang,Guansong Pang,Sarah Erfani,Christopher Leckie*

Main category: cs.SD

TL;DR: AUDETER是一个大规模深度伪造音频数据集，包含4500+小时合成音频，用于训练和评估深度伪造音频检测模型，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造音频检测方法在真实环境中效果不佳，主要原因是训练和测试样本之间的域偏移问题，现有数据集缺乏真实世界应用的多样性和时效性。

Method: 构建AUDETER数据集，包含4500小时合成音频，由11个TTS模型和10个声码器生成，总计300万音频片段，是目前最大的深度伪造音频数据集。

Result: 实验表明：1）现有SOTA方法在新样本上泛化能力差，假阳性率高；2）使用AUDETER训练的方法检测错误率降低44.1%-51.6%，在跨域样本上错误率仅4.17%。

Conclusion: AUDETER数据集能有效提升深度伪造音频检测的泛化性能，为训练通用检测器铺平道路，数据集已在GitHub上开源。

Abstract: Speech generation systems can produce remarkably realistic vocalisations that
are often indistinguishable from human speech, posing significant authenticity
challenges. Although numerous deepfake detection methods have been developed,
their effectiveness in real-world environments remains unrealiable due to the
domain shift between training and test samples arising from diverse human
speech and fast evolving speech synthesis systems. This is not adequately
addressed by current datasets, which lack real-world application challenges
with diverse and up-to-date audios in both real and deep-fake categories. To
fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,
highly diverse deepfake audio dataset for comprehensive evaluation and robust
development of generalised models for deepfake audio detection. It consists of
over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10
vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio
clips, making it the largest deepfake audio dataset by scale. Through extensive
experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods
trained on existing datasets struggle to generalise to novel deepfake audio
samples and suffer from high false positive rates on unseen human voice,
underscoring the need for a comprehensive dataset; and ii) these methods
trained on AUDETER achieve highly generalised detection performance and
significantly reduce detection error rate by 44.1% to 51.6%, achieving an error
rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild
dataset, paving the way for training generalist deepfake audio detectors.
AUDETER is available on GitHub.

</details>


### [21] [Denoising GER: A Noise-Robust Generative Error Correction with LLM for Speech Recognition](https://arxiv.org/abs/2509.04392)
*Yanyan Liu,Minqiang Xu,Yihao Chen,Liang He,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 提出Denoising GER框架，通过噪声自适应声学编码器和异质特征补偿动态融合机制，提升LLM在复杂噪声环境中的语音识别后处理能力


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂噪声环境中进行生成式错误校正时适应性差、信息利用率低的问题

Method: 使用噪声自适应声学编码器增强模型对不同噪声场景的适应性，采用异质特征补偿动态融合(HFCDF)机制优化多模态信息整合，引入强化学习训练策略

Result: 在噪声环境中显著提高了准确性和鲁棒性，在未见噪声场景中表现出良好的泛化能力

Conclusion: Denoising GER框架有效解决了LLM在噪声环境中的GER问题，提升了模型的多模态信息利用能力和环境适应性

Abstract: In recent years, large language models (LLM) have made significant progress
in the task of generation error correction (GER) for automatic speech
recognition (ASR) post-processing. However, in complex noisy environments, they
still face challenges such as poor adaptability and low information
utilization, resulting in limited effectiveness of GER. To address these
issues, this paper proposes a noise-robust multi-modal GER framework (Denoising
GER). The framework enhances the model's adaptability to different noisy
scenarios through a noise-adaptive acoustic encoder and optimizes the
integration of multi-modal information via a heterogeneous feature compensation
dynamic fusion (HFCDF) mechanism, improving the LLM's utilization of
multi-modal information. Additionally, reinforcement learning (RL) training
strategies are introduced to enhance the model's predictive capabilities.
Experimental results demonstrate that Denoising GER significantly improves
accuracy and robustness in noisy environments and exhibits good generalization
abilities in unseen noise scenarios.

</details>


### [22] [Contextualized Token Discrimination for Speech Search Query Correction](https://arxiv.org/abs/2509.04393)
*Junyu Lu,Di Jiang,Mengze Hong,Victor Junqiu Wei,Qintian Guo,Zhiyang Su*

Main category: cs.SD

TL;DR: 这篇论文提出了一种名为上下文化分识判别(CTD)的新方法，用于语音搜索查询的拼写纠正，通过BERT生成上下文化表征并构建组合层来提高纠正效果。


<details>
  <summary>Details</summary>
Motivation: 随着语音搜索的普及，自动语音识别(ASR)系统产生的误识别文本需要有效的纠正方法来帮助用户更准确地表达搜索意图。

Method: 使用BERT生成词元级别的上下文化表征，构建组合层来增强语义信息，通过比较原始词元表征和上下文化表征来纠正错误词元，最终生成正确查询。

Result: 大量实验证明该方法在所有指标上都表现出优异性能，同时还提供了一个包含错误ASR转写的新基准数据集用于音频查询纠正的全面评估。

Conclusion: CTD方法为语音搜索查询的拼写纠正提供了一种高效的解决方案，通过上下文化表征和语义增强技术显著提高了纠正准确性。

Abstract: Query spelling correction is an important function of modern search engines
since it effectively helps users express their intentions clearly. With the
growing popularity of speech search driven by Automated Speech Recognition
(ASR) systems, this paper introduces a novel method named Contextualized Token
Discrimination (CTD) to conduct effective speech query correction. In CTD, we
first employ BERT to generate token-level contextualized representations and
then construct a composition layer to enhance semantic information. Finally, we
produce the correct query according to the aggregated token representation,
correcting the incorrect tokens by comparing the original token representations
and the contextualized representations. Extensive experiments demonstrate the
superior performance of our proposed method across all metrics, and we further
present a new benchmark dataset with erroneous ASR transcriptions to offer
comprehensive evaluations for audio query correction.

</details>

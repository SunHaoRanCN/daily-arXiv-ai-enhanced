<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Multi-Sensor Fusion for Extended Object Tracking Exploiting Active and Passive Radio Signals](https://arxiv.org/abs/2509.03686)
*Hong Zhu,Alexander Venus,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: 提出一种贝叶斯定位方法，融合主动测量和被动雷达测量，处理用户遮挡LOS链路时的定位问题，通过多传感器概率数据关联算法提升OLOS条件下的定位性能


<details>
  <summary>Details</summary>
Motivation: 解决无线电设备定位中由于用户自身遮挡LOS链路导致的定位不可靠问题，传统方法忽略用户作为扩展物体的散射、衰减和阻挡效应

Method: 提出贝叶斯方法融合主动测量（设备与基站）和被动多基地雷达测量（基站间经用户反射），引入多传感器多测量概率数据关联算法，开发针对人体用户的扩展物体模型

Result: 在合成和真实无线电测量评估中，该算法优于基于点目标假设的传统PDA方法，特别是在OLOS条件下表现更佳

Conclusion: 通过建模用户作为扩展物体并融合多类型测量，能够显著提升遮挡条件下的定位可靠性，为实际应用提供了有效的解决方案

Abstract: Reliable and robust positioning of radio devices remains a challenging task
due to multipath propagation, hardware impairments, and interference from other
radio transmitters. A frequently overlooked but critical factor is the agent
itself, e.g., the user carrying the device, which potentially obstructs
line-of-sight (LOS) links to the base stations (anchors). This paper addresses
the problem of accurate positioning in scenarios where LOS links are partially
blocked by the agent. The agent is modeled as an extended object (EO) that
scatters, attenuates, and blocks radio signals. We propose a Bayesian method
that fuses ``active'' measurements (between device and anchors) with
``passive'' multistatic radar-type measurements (between anchors, reflected by
the EO). To handle measurement origin uncertainty, we introduce an multi-sensor
and multiple-measurement probabilistic data association (PDA) algorithm that
jointly fuses all EO-related measurements. Furthermore, we develop an EO model
tailored to agents such as human users, accounting for multiple reflections
scattered off the body surface, and propose a simplified variant for
low-complexity implementation. Evaluation on both synthetic and real radio
measurements demonstrates that the proposed algorithm outperforms conventional
PDA methods based on point target assumptions, particularly during and after
obstructed line-of-sight (OLOS) conditions.

</details>


### [2] [Sensor placement for sparse force reconstruction](https://arxiv.org/abs/2509.03825)
*Jeunghoon Lee*

Main category: eess.SP

TL;DR: 基于Gram矩阵的传感器布置策略，用于频域稀疏力重构，通过最小化Gram矩阵非对角能量来优化传感器位置


<details>
  <summary>Details</summary>
Motivation: 传统启发式传感器布局在力重构精度方面存在不足，需要基于物理洞察的数学框架来优化传感器布置

Method: 提出Gram矩阵模态分解方法，发现目标频率附近少数模态主导结构，提出贪心算法选择传感器位置以最小化Gram矩阵非对角能量

Result: 数值模拟和实验验证表明，该方法相比启发式布局能提供更鲁棒和准确的力估计

Conclusion: Gram矩阵模态分解为传感器布置提供了物理洞察，贪心算法能有效实现优化布局，提升力重构精度

Abstract: The present study proposes a Gram-matrix-based sensor placement strategy for
sparse force reconstruction in the frequency domain. A modal decomposition of
the Gram matrix reveals that its structure is dominated by a few modes near the
target frequency, and that each modal contribution reflects the spatial
correlation of the corresponding mode shape. This suggests that placing sensors
near nodal regions where spatial correlation is low can reduce coherence in the
frequency response function (FRF) matrix and improve force reconstruction
accuracy. To translate the physical insight into a practical design framework,
a greedy algorithm is proposed to select sensor locations that minimize the
off-diagonal energy of the Gram matrix. Numerical simulations and experimental
validations demonstrate that the proposed method yields robust and accurate
force estimation, outperforming heuristic sensor layouts.

</details>


### [3] [A Low-Cost Open-Source BLE-Based Asian Hornet Tracking System](https://arxiv.org/abs/2509.03979)
*Gilles Callebaut,Jan Van Moer*

Main category: eess.SP

TL;DR: 开发基于BLE的低成本开源追踪系统，用于定位亚洲大黄蜂巢穴，通过自定义PN序列和数字波束扫描实现50米角度分辨和360米通信范围


<details>
  <summary>Details</summary>
Motivation: 亚洲大黄蜂对生态系统和养蜂业构成严重威胁，传统人工三角定位方法耗时且效率低，需要开发自动化追踪解决方案

Method: 使用BLE轻量级标签和GNU Radio实现的SDR接收器，绕过BLE协议栈，在未编码物理层嵌入自定义PN序列进行相关检测，采用Yagi天线和PlutoSDR进行数字波束扫描确定方向

Result: 现场测试显示在50米距离具有可靠的角度分辨率，通信范围可达360米

Conclusion: 虽然调制方式增加了接收器复杂度，但为多信道扩展和标签识别等未来改进提供了可能，该系统为黄蜂追踪和环境监测相关应用提供了可扩展的开源框架

Abstract: The Asian hornet (Vespa velutina) poses a serious threat to ecosystems and
beekeeping. Locating nests is essential, but usually involves time-consuming
manual triangulation. We present a low-cost, open-source tracking system based
on Bluetooth Low Energy (BLE). The system consists of a lightweight BLE tag and
a software-defined radio (SDR) receiver implemented in GNU Radio. By bypassing
the BLE stack, we embed a custom pseudo-noise (PN) sequence in the uncoded PHY
for correlation-based detection. Using a Yagi antenna and PlutoSDR, the
receiver performs digital beam sweeping to determine the tag's direction. Field
tests show reliable angular resolution at 50m and a communication range up to
360m. While our modulation increases receiver complexity, it enables future
improvements such as multichannel spreading and tag identification. The design
is fully open-source and provides a scalable framework for hornet tracking and
related applications in environmental monitoring.

</details>


### [4] [Approximate Message Passing for Multi-Preamble Detection in OTFS Random Access](https://arxiv.org/abs/2509.03980)
*Alessandro Mirri,Vishnu Teja Kunde,Enrico Paolini,Jean-Francois Chamberland*

Main category: eess.SP

TL;DR: 提出一种新的近似消息传递算法，用于OTFS系统中的多前导检测，通过强制双重稀疏性实现鲁棒性能


<details>
  <summary>Details</summary>
Motivation: 解决基于OTFS信号随机接入系统中的多前导检测问题，该问题在复数域中可表述为结构化稀疏恢复问题

Method: 设计新的近似消息传递(AMP)算法，强制双重稀疏性：前导的稀疏选择和OTFS信号在时延-多普勒域中的固有稀疏性，并推导了新颖的AMP去噪器来处理复数域的非可分离稀疏约束

Result: 仿真结果表明，所提方法实现了鲁棒的检测性能，相比最先进技术获得了显著增益

Conclusion: 该AMP算法有效解决了OTFS系统中的多前导检测问题，通过双重稀疏性约束实现了优越的性能

Abstract: This article addresses the problem of multiple preamble detection in random
access systems based on orthogonal time frequency space (OTFS) signaling. This
challenge is formulated as a structured sparse recovery problem in the complex
domain. To tackle it, the authors propose a new approximate message passing
(AMP) algorithm that enforces double sparsity: the sparse selection of
preambles and the inherent sparsity of OTFS signals in the delay-Doppler
domain. From an algorithmic standpoint, the non-separable complex sparsity
constraint necessitates a careful derivation and leads to the design of a novel
AMP denoiser. Simulation results demonstrate that the proposed method achieves
robust detection performance and delivers significant gains over
state-of-the-art techniques.

</details>


### [5] [Joint Frequency-Space Sparse Reconstruction for DOA Estimation under Coherent Sources and Amplitude-Phase Errors](https://arxiv.org/abs/2509.03983)
*Yutong Chen,Cong Zhou,Changsheng You,Shuo Shi*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this letter, we propose a joint frequency-space sparse reconstruction
method for direction-of-arrival (DOA) estimation, which effectively addresses
the issues arising from the existence of coherent sources and array
amplitude-phase errors. Specifically, by using an auxiliary source with known
angles, we first construct the real steering vectors (RSVs) based on the
spectral peaks of received signals in the frequency domain, which serve as a
complete basis matrix for compensation for amplitude-phase errors. Then, we
leverage the spectral sparsity of snapshot data in the frequency domain and the
spatial sparsity of incident directions to perform the DOA estimation according
to the sparse reconstruction method. The proposed method does not require
iterative optimization, hence exhibiting low computational complexity.
Numerical results demonstrate that the proposed DOA estimation method achieves
higher estimation accuracy for coherent sources as compared to various
benchmark schemes.

</details>


### [6] [Robust MIMO Semantic Communication with Imperfect CSI via Knowledge Distillation](https://arxiv.org/abs/2509.04005)
*Mingze Gong,Shuoyao Wang,Shijian Gao,Jia Yan,Suzhi Bi*

Main category: eess.SP

TL;DR: 提出HANA-JSCC系统，解决MIMO语义通信中信道矩阵估计误差问题，通过信道矩阵适配器和两阶段训练策略提升性能


<details>
  <summary>Details</summary>
Motivation: 现有MIMO语义通信系统假设完美信道矩阵估计，但实际中由于硬件和导频开销限制，这种假设不切实际，需要解决信道估计误差问题

Method: 提出信道矩阵适配器与信道编解码器协作适应不准确的信道状态信息；采用两阶段训练策略和知识蒸馏解决估计信道矩阵与真实信道矩阵之间的一对多不适定问题

Result: 在各种噪声和估计误差水平下，HANA-JSCC比最先进基准平均性能提升0.40~0.54dB

Conclusion: HANA-JSCC系统有效解决了MIMO语义通信中的信道估计误差问题，通过创新的适配器和训练策略显著提升了系统性能

Abstract: Semantic communication (SemComm) has emerged as a new communication paradigm.
To enhance efficiency, multiple-input-multiple-output (MIMO) technology has
been further integrated into SemComm systems. However, existing MIMO SemComm
systems assume perfect channel matrix estimation for channel-adaptive joint
source-channel coding, which is impractical due to hardware and pilot overhead
constraints. In this paper, we propose a semantic image transmission system
with channel matrix and channel noise adaptation, named HANA-JSCC, to cope with
channel estimation errors in MIMO systems. We propose a channel matrix adaptor
that collaborates with the channel codec to adapt to misaligned channel state
information, thereby mitigating the impact of estimation errors. Since the
relationship between the estimated channel matrix and true channel matrix is
ill-posed (one-to-many), we further introduce a two-stage training strategy
with knowledge distillation to overcome the convergence difficulties caused by
the ill-posed problem. Comparing with the state-of-the-art benchmarks,
HANA-JSCC achieves $0.40\sim0.54$dB higher average performance across various
noise and estimation error levels in various datasets.

</details>


### [7] [Constellation Shaping for OFDM-ISAC Systems: From Theoretical Bounds to Practical Implementation](https://arxiv.org/abs/2509.04055)
*Benedikt Geiger,Fan Liu,Shihang Lu,Andrej Rode,Daniel Gil Gaviria,Charlotte Muth,Laurent Schmalen*

Main category: eess.SP

TL;DR: 本文研究了在OFDM-based ISAC系统中通过星座整形同时提升感知与通信性能的方法，提出了几何、概率和联合星座整形方案，并证明了其能够灵活权衡S&C性能，显著优于传统调制格式。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)通过重用通信信号实现雷达式感知，但感知与通信对调制格式存在冲突要求，需要在性能之间进行权衡。

Method: 使用自编码器优化方法，研究了几何、概率和联合星座整形方案，提出了针对ISAC的广义概率幅度整形(PAS)方法，并设计了低复杂度的对数似然比计算算法。

Result: 星座整形能够实现S&C性能的灵活权衡，接近理论上限，显著优于传统调制格式。广义PAS结合传统PAS能够以低复杂度接近联合星座整形的性能。

Conclusion: 星座整形是提升ISAC系统性能的有效方法，提出的广义PAS方案具有实际部署可行性，能够在低复杂度下实现接近最优的S&C性能权衡。

Abstract: Integrated sensing and communications (ISAC) promises new use cases for
mobile communication systems by reusing the communication signal for radar-like
sensing. However, sensing and communications (S&C) impose conflicting
requirements on the modulation format, resulting in a tradeoff between their
corresponding performance. This paper investigates constellation shaping as a
means to simultaneously improve S&C performance in orthogonal frequency
division multiplexing (OFDM)-based ISAC systems. We begin by deriving how the
transmit symbols affect detection performance and derive theoretical lower and
upper bounds on the maximum achievable information rate under a given sensing
constraint. Using an autoencoder-based optimization, we investigate geometric,
probabilistic, and joint constellation shaping, where joint shaping combines
both approaches, employing both optimal maximum a-posteriori decoding and
practical bit-metric decoding. Our results show that constellation shaping
enables a flexible trade-off between S&C, can approach the derived upper bound,
and significantly outperforms conventional modulation formats. Motivated by its
practical implementation feasibility, we review probabilistic amplitude shaping
(PAS) and propose a generalization tailored to ISAC. For this generalization,
we propose a low-complexity log-likelihood ratio computation with negligible
rate loss. We demonstrate that combining conventional and generalized PAS
enables a flexible and low-complexity tradeoff between S&C, closely approaching
the performance of joint constellation shaping.

</details>


### [8] [Reliable Clutter Suppression for Slow-Moving Weak Target Radar Detection](https://arxiv.org/abs/2509.04309)
*R. Zhang,J. Xue,T. Zhang*

Main category: eess.SP

TL;DR: 基于Go分解(Godec)框架的新型杂丝压制方案，解决复杂环境下慢速弱目标检测困难，比传统MTI方法更可靠但耗时更长


<details>
  <summary>Details</summary>
Motivation: 复杂环境中强反射体的掩蔽效应使得传统移动目标指示(MTI)方法在压制静态干扰物的同时也压制了慢速弱目标回波，导致检测困难

Method: 利用不同雷达扫描中距离-速度图的低秩和稀疏特性，提出基于Go分解(Godec)框架的杂丝压制方案

Result: 模拟结果显示，在存在掩蔽效应时，基于Godec的目标检测方案比传统MTI方案更可靠地检测慢速弱目标，但以更长的计算时间为代价

Conclusion: 该方案以提高时间复杂度为代价换取了更高的检测可靠性，并揭示了虚拟报警单元数、检测概率和迭代次数之间的批执关系，为实际应用中参数设置提供指导

Abstract: Reliable slow-moving weak target detection in complicated environments is
challenging due to the masking effects from the surrounding strong reflectors.
The traditional Moving Target Indication (MTI) may suppress the echoes from not
only the static interference objects (IOs), but also the desired slow-moving
weak target. According to the low-rank and sparse properties of the
range-velocity maps across different radar scans, a novel clutter suppression
scheme based on the Go decomposition (Godec) framework is proposed in this
paper. The simulation results show that with the existence of masking effects,
the target detection scheme based on Godec clutter suppression can reliably
detect the slow-moving weak target, compared to the traditional MTI-based
scheme. Besides, the time consumption comparison is conducted, demonstrating
that the proposed solution is one that sacrifices time complexity in exchange
for enhanced reliability. Additionally, the tradeoffs among the number of false
alarm cells, the detection probability and the iteration times for convergence
have been revealed, guiding parameter settings of the proposed solution in
practical applications. Experiment validation is also conducted to verify the
proposed solution, providing further insight into the scenarios where the
solution is most applicable.

</details>


### [9] [Relative Localization of UAV Swarms in GNSS-Denied Conditions](https://arxiv.org/abs/2509.04412)
*Guangyu Lei,Yuqi Ping,Tianhao Liang,Huahao Ding,Tingting Zhang*

Main category: eess.SP

TL;DR: 提出基于聚类的无人机群相对定位框架，在GNSS拒止环境下使用通信信号进行信道估计和测距，通过谱聚类和矩阵补全技术降低定位误差


<details>
  <summary>Details</summary>
Motivation: 解决GNSS拒止环境下无人机群相对定位问题，现有方法存在包丢失导致的大定位误差和高计算复杂度问题

Method: 使用谱聚类将无人机群划分为子簇，通过矩阵补全和多维标度获得高精度相对坐标，再通过簇间锚点融合创建全局地图，采用OTFS技术进行测距和通信

Result: 实验结果显示该方法能减少大规模群组中的定位误差和距离信息丢失，并探索了信号参数对通信和定位性能的影响

Conclusion: 该方法有效解决了GNSS拒止环境下的无人机群相对定位问题，展示了通信与定位性能之间的相互影响关系

Abstract: Relative localization of unmanned aerial vehicle (UAV) swarms in global
navigation satellite system (GNSS) denied environments is essential for
emergency rescue and battlefield reconnaissance. Existing methods suffer from
significant localization errors among UAVs due to packet loss and high
computational complexity in large swarms. This paper proposes a
clustering-based framework where the UAVs simultaneously use communication
signals for channel estimation and ranging. Firstly, the spectral clustering is
utilized to divide the UAV swarm into different sub-clusters, where matrix
completion and multidimensional scaling yield high-precision relative
coordinates. Subsequently, a global map is created by the inter-cluster anchor
fusion. A case study of UAV integrated communication and sensing (ISAC) system
is presented, where the Orthogonal Time Frequency Space (OTFS) is adopted for
ranging and communication. Experimental results show that the proposed method
reduces localization errors in large swarms and loss of range information. It
also explores the impact of signal parameters on communication and
localization, highlighting the interplay between communication and localization
performance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [Hierarchical Sparse Sound Field Reconstruction with Spherical and Linear Microphone Arrays](https://arxiv.org/abs/2509.03902)
*Shunxi Xu,Craig T. Jin*

Main category: eess.AS

TL;DR: 这篇论文提出了一种两阶段稀疏恢复框架，通过结合球面麦克风数组和四个线性麦克风数组，显著提高了复杂音响环境下的空间分辨率和稳健性。


<details>
  <summary>Details</summary>
Motivation: 球面麦克风数组的空间分辨率受到球象和计的基本限制，且在混响环境下性能会降低。需要一种方法来充分利用不同类型麦克风数组的互补特性提高性能。

Method: 提出两阶段稀疏恢复框架，将球面麦克风数组作为主要估计器，四个周围线性麦克风数组作为空间互补精炼器，通过殊毛精炼进行分析。

Result: 模拟结果显示，该SMA-LMA方法在不同混响条件下都显著提高了空间能量图重建的性能，超过了单独使用SMA和直接联合处理的方法。

Conclusion: 该框架通过整合不同类型麦克风数组的互补特性，有效提高了复杂音响环境中的空间保真度和稳健性。

Abstract: Spherical microphone arrays (SMAs) are widely used for sound field analysis,
and sparse recovery (SR) techniques can significantly enhance their spatial
resolution by modeling the sound field as a sparse superposition of dominant
plane waves. However, the spatial resolution of SMAs is fundamentally limited
by their spherical harmonic order, and their performance often degrades in
reverberant environments. This paper proposes a two-stage SR framework with
residue refinement that integrates observations from a central SMA and four
surrounding linear microphone arrays (LMAs). The core idea is to exploit
complementary spatial characteristics by treating the SMA as a primary
estimator and the LMAs as a spatially complementary refiner. Simulation results
demonstrate that the proposed SMA-LMA method significantly enhances spatial
energy map reconstruction under varying reverberation conditions, compared to
both SMA-only and direct one-step joint processing. These results demonstrate
the effectiveness of the proposed framework in enhancing spatial fidelity and
robustness in complex acoustic environments.

</details>


### [11] [LibriQuote: A Speech Dataset of Fictional Character Utterances for Expressive Zero-Shot Speech Synthesis](https://arxiv.org/abs/2509.04072)
*Gaspard Michel,Elena V. Epure,Christophe Cerisara*

Main category: eess.AS

TL;DR: 提出LibriQuote数据集，包含大量表达性语音和非表达性语音，用于精调和测试表达性零样本TTS系统


<details>
  <summary>Details</summary>
Motivation: 大规模语音数据集中表达性语音比例不明，现有表达性语音数据集规模较小，需要专门的数据集来支持表达性TTS系统的开发和评测

Method: 从阅读有声书中提取数据，包含12.7K小时非表达性语音和5.3K小时以人物引言为主的表达性语音，提供上下文信息和假标签，并构建7.5小时测试集

Result: 定性验证显示测试集覆盖广泛情感和口音范围，主观和客观评估显示在LibriQuote上精调TTS系统显著提高语音可懂性，现有系统无法生成与真实语音相比的表达性和自然性

Conclusion: LibriQuote数据集为表达性零样本TTS系统提供了丰富的训练和挑战性测试资源，显著提升了TTS系统的表达力和自然度

Abstract: Text-to-speech (TTS) systems have recently achieved more expressive and
natural speech synthesis by scaling to large speech datasets. However, the
proportion of expressive speech in such large-scale corpora is often unclear.
Besides, existing expressive speech corpora are typically smaller in scale and
primarily used for benchmarking TTS systems. In this paper, we introduce the
LibriQuote dataset, an English corpus derived from read audiobooks, designed
for both fine-tuning and benchmarking expressive zero-shot TTS system. The
training dataset includes 12.7K hours of read, non-expressive speech and 5.3K
hours of mostly expressive speech drawn from character quotations. Each
utterance in the expressive subset is supplemented with the context in which it
was written, along with pseudo-labels of speech verbs and adverbs used to
describe the quotation (\textit{e.g. ``he whispered softly''}). Additionally,
we provide a challenging 7.5 hour test set intended for benchmarking TTS
systems: given a neutral reference speech as input, we evaluate system's
ability to synthesize an expressive utterance while preserving reference
timbre. We validate qualitatively the test set by showing that it covers a wide
range of emotions compared to non-expressive speech, along with various
accents. Extensive subjective and objective evaluations show that fine-tuning a
baseline TTS system on LibriQuote significantly improves its synthesized speech
intelligibility, and that recent systems fail to synthesize speech as
expressive and natural as the ground-truth utterances. The dataset and
evaluation code are freely available. Audio samples can be found at
https://libriquote.github.io/.

</details>


### [12] [Test-Time Adaptation for Speech Enhancement via Domain Invariant Embedding Transformation](https://arxiv.org/abs/2509.04280)
*Tobias Raichle,Niels Edinger,Bin Yang*

Main category: eess.AS

TL;DR: LaDen是首个针对语音增强的测试时适应方法，利用预训练语音表示进行潜在空间去噪，通过线性变换近似干净语音表示，无需目标域标注数据即可实现跨域伪标注和有效适应。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习语音增强模型在训练分布匹配时表现优异，但在真实世界不可预测环境中遇到域偏移时性能下降的问题。

Method: 利用预训练语音表示进行潜在去噪，通过线性变换将噪声嵌入映射到干净语音表示，实现跨域泛化和伪标注，支持测试时适应。

Result: 在包含噪声类型、说话人特征和语言变化的多数据集基准测试中，LaDen在感知指标上持续优于基线方法，特别是在说话人和语言域偏移方面表现突出。

Conclusion: LaDen通过潜在空间去噪和线性变换方法，有效解决了语音增强模型在域偏移环境下的适应问题，为真实世界部署提供了实用的测试时适应解决方案。

Abstract: Deep learning-based speech enhancement models achieve remarkable performance
when test distributions match training conditions, but often degrade when
deployed in unpredictable real-world environments with domain shifts. To
address this challenge, we present LaDen (latent denoising), the first
test-time adaptation method specifically designed for speech enhancement. Our
approach leverages powerful pre-trained speech representations to perform
latent denoising, approximating clean speech representations through a linear
transformation of noisy embeddings. We show that this transformation
generalizes well across domains, enabling effective pseudo-labeling for target
domains without labeled target data. The resulting pseudo-labels enable
effective test-time adaptation of speech enhancement models across diverse
acoustic environments. We propose a comprehensive benchmark spanning multiple
datasets with various domain shifts, including changes in noise types, speaker
characteristics, and languages. Our extensive experiments demonstrate that
LaDen consistently outperforms baseline methods across perceptual metrics,
particularly for speaker and language domain shifts.

</details>


### [13] [Accelerated Interactive Auralization of Highly Reverberant Spaces using Graphics Hardware](https://arxiv.org/abs/2509.04390)
*Hannes Rosseel,Toon van Waterschoot*

Main category: eess.AS

TL;DR: 这篇论文提出了一种基于GPU加速的实时多通道扬声器音响模拟系统，用于模拟高深深响空间的音响效果，解决传统CPU卷积计算复杂度高导致的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 交互式音响模拟需要实时卷积计算，而音乐厅和历史宗教空间的长深深响时间导致滤波器润数极多，使得CPU卷积计算成为性能瓶颈，影响实时交互性。

Method: 实现了基于GPU加速的多通道扬声器音响模拟系统，通过GPU加速卷积计算来处理长深深响空间的音响模拟，并在GPU上集成了音响反馈消除功能。

Result: 与传统CPU卷积相比，GPU加速卷积能够实现实时性能且延迟显著降低，极大提升了系统的交互性。

Conclusion: 该GPU加速方案为高深深响空间的实时音响模拟提供了高效的解决方案，通过低延迟的统一框架实现了更好的交互体验。

Abstract: Interactive acoustic auralization allows users to explore virtual acoustic
environments in real-time, enabling the acoustic recreation of concert hall or
Historical Worship Spaces (HWS) that are either no longer accessible,
acoustically altered, or impractical to visit. Interactive acoustic synthesis
requires real-time convolution of input signals with a set of synthesis filters
that model the space-time acoustic response of the space. The acoustics in
concert halls and HWS are both characterized by a long reverberation time,
resulting in synthesis filters containing many filter taps. As a result, the
convolution process can be computationally demanding, introducing significant
latency that limits the real-time interactivity of the auralization system. In
this paper, the implementation of a real-time multichannel loudspeaker-based
auralization system is presented. This system is capable of synthesizing the
acoustics of highly reverberant spaces in real-time using GPU-acceleration. A
comparison between traditional CPU-based convolution and GPU-accelerated
convolution is presented, showing that the latter can achieve real-time
performance with significantly lower latency. Additionally, the system
integrates acoustic synthesis with acoustic feedback cancellation on the GPU,
creating a unified loudspeaker-based auralization framework that minimizes
processing latency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [14] [SwinSRGAN: Swin Transformer-based Generative Adversarial Network for High-Fidelity Speech Super-Resolution](https://arxiv.org/abs/2509.03913)
*Jiajun Yuan,Xiaochen Wang,Yuhang Xiao,Yulin Wu,Chenhao Hu,Xueyang Lv*

Main category: cs.SD

TL;DR: SwinSRGAN是一个端到端的语音超分辨率框架，使用Swin Transformer U-Net处理MDCT幅度，通过混合对抗训练方案实现实时48kHz上采样，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有语音超分辨率系统存在表示不匹配、CNN生成器过度平滑高频内容、扩散模型计算成本高且跨域鲁棒性有限等问题。

Method: 基于Swin Transformer的U-Net架构，使用MDCT幅度作为输入，结合时域MPD/MSD判别器和多频带MDCT判别器的混合对抗方案，采用稀疏感知正则化处理arcsinh压缩的MDCT。

Result: 在标准基准测试中降低了客观误差并提高了ABX偏好分数，在HiFi-TTS零样本测试中优于NVSR和mdctGAN，展现了强大的跨数据集泛化能力。

Conclusion: SwinSRGAN提供了一个高效、实时的语音超分辨率解决方案，能够处理不同采样率输入并生成高质量的48kHz输出，具有良好的泛化性能。

Abstract: Speech super-resolution (SR) reconstructs high-frequency content from
low-resolution speech signals. Existing systems often suffer from
representation mismatch in two-stage mel-vocoder pipelines and from
over-smoothing of hallucinated high-band content by CNN-only generators.
Diffusion and flow models are computationally expensive, and their robustness
across domains and sampling rates remains limited. We propose SwinSRGAN, an
end-to-end framework operating on Modified Discrete Cosine Transform (MDCT)
magnitudes. It is a Swin Transformer-based U-Net that captures long-range
spectro-temporal dependencies with a hybrid adversarial scheme combines
time-domain MPD/MSD discriminators with a multi-band MDCT discriminator
specialized for the high-frequency band. We employs a sparse-aware regularizer
on arcsinh-compressed MDCT to better preserve transient components. The system
upsamples inputs at various sampling rates to 48 kHz in a single pass and
operates in real time. On standard benchmarks, SwinSRGAN reduces objective
error and improves ABX preference scores. In zero-shot tests on HiFi-TTS
without fine-tuning, it outperforms NVSR and mdctGAN, demonstrating strong
generalization across datasets

</details>


### [15] [WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with Multi-dimensional Annotation](https://arxiv.org/abs/2509.03959)
*Longhao Li,Zhao Guo,Hongjie Chen,Yuhang Dai,Ziyu Zhang,Hongfei Xue,Tianlun Zuo,Chengyou Wang,Shuiyuan Wang,Jie Li,Xin Xu,Hui Bu,Binbin Zhang,Ruibin Yuan,Ziya Zhou,Wei Xue,Lei Xie*

Main category: cs.SD

TL;DR: 提出了WenetSpeech-Pipe流水线来构建大规模粤语语音语料库WenetSpeech-Yue，包含21,800小时多维度标注数据，并在ASR和TTS任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 粤语作为拥有8490万母语者的重要语言，缺乏高质量标注语料库，导致ASR和TTS性能不佳，需要构建大规模多维度标注的语音数据集

Method: 开发了包含音频收集、说话人属性标注、语音质量标注、自动语音识别、文本后处理和识别器输出投票六个模块的集成流水线

Result: 构建了首个大规模粤语语音语料库WenetSpeech-Yue，在ASR和TTS任务上取得了与最先进商业和LLM模型竞争的结果

Conclusion: WenetSpeech-Pipe流水线和WenetSpeech-Yue语料库有效解决了粤语语音资源匮乏问题，为语音理解和生成任务提供了高质量数据支持

Abstract: The development of speech understanding and generation has been significantly
accelerated by the availability of large-scale, high-quality speech datasets.
Among these, ASR and TTS are regarded as the most established and fundamental
tasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9
million native speakers worldwide, limited annotated resources have hindered
progress and resulted in suboptimal ASR and TTS performance. To address this
challenge, we propose WenetSpeech-Pipe, an integrated pipeline for building
large-scale speech corpus with multi-dimensional annotation tailored for speech
understanding and generation. It comprises six modules: Audio Collection,
Speaker Attributes Annotation, Speech Quality Annotation, Automatic Speech
Recognition, Text Postprocessing and Recognizer Output Voting, enabling rich
and high-quality annotations. Based on this pipeline, we release
WenetSpeech-Yue, the first large-scale Cantonese speech corpus with
multi-dimensional annotation for ASR and TTS, covering 21,800 hours across 10
domains with annotations including ASR transcription, text confidence, speaker
identity, age, gender, speech quality scores, among other annotations. We also
release WSYue-eval, a comprehensive Cantonese benchmark with two components:
WSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long
utterances, code-switching, and diverse acoustic conditions, and
WSYue-TTS-eval, with base and coverage subsets for standard and generalization
testing. Experimental results show that models trained on WenetSpeech-Yue
achieve competitive results against state-of-the-art (SOTA) Cantonese ASR and
TTS systems, including commercial and LLM-based models, highlighting the value
of our dataset and pipeline.

</details>


### [16] [Open-Source Full-Duplex Conversational Datasets for Natural and Interactive Speech Synthesis](https://arxiv.org/abs/2509.04093)
*Zhitong Zhou,Qingqing Zhang,Lei Luo,Jiechen Liu,Ruohua Zhou*

Main category: cs.SD

TL;DR: 这篇论文提供了中英双语对话语料集，用于提升对话TTS系统的自然性和交互性。通过导入实际对话特征如重叠和反馈，细调后的TTS模型在主观和客观指标上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的对话TTS系统缺乏自然的双工对话数据，导致合成语音的自然性和交互性不足。需要更现实的对话语料来改善语音合成质量。

Method: 收集15小时自然对话数据（中英各一个），包含分离的高质量音频轨道。语料涵盖多样日常话题和交互模式（重叠、反馈、笑声等）。通过细调基准TTS模型来验证语料效果。

Result: 细调后的TTS模型在主观评价和客观指标上都超过基准模型，显示出更高的自然性和对话现实性。

Conclusion: 该双语对话语料集能够有效提升对话语音合成的自然性。所有数据、注释和代码已开源，以便进一步研究。

Abstract: Full-duplex, spontaneous conversational data are essential for enhancing the
naturalness and interactivity of synthesized speech in conversational TTS
systems. We present two open-source dual-track conversational speech datasets,
one in Chinese and one in English, designed to enhance the naturalness of
synthesized speech by providing more realistic conversational data. The two
datasets contain a total of 15 hours of natural, spontaneous conversations
recorded in isolated rooms, which produces separate high-quality audio tracks
for each speaker. The conversations cover diverse daily topics and domains,
capturing realistic interaction patterns including frequent overlaps,
backchannel responses, laughter, and other non-verbal vocalizations. We
introduce the data collection procedure, transcription and annotation methods.
We demonstrate the utility of these corpora by fine-tuning a baseline TTS model
with the proposed datasets. The fine-tuned TTS model achieves higher subjective
and objective evaluation metrics compared to the baseline, indicating improved
naturalness and conversational realism in synthetic speech. All data,
annotations, and supporting code for fine-tuning and evaluation are made
available to facilitate further research in conversational speech synthesis.

</details>


### [17] [Enhancing Self-Supervised Speaker Verification Using Similarity-Connected Graphs and GCN](https://arxiv.org/abs/2509.04147)
*Zhaorui Sun,Yihao Chen,Jialong Wang,Minqiang Xu,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 本文提出了一种基于相似性连接图和图卷积网络的改进聚类框架，用于改善自监督语音识别中的伪标签质量，提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习方法DINO在语音识别中通过聚类生成伪标签时可能产生噪声标签的问题，这会降低识别性能。

Method: 利用图卷积网络对相似性连接图中节点间关系信息进行建模，优化聚类过程，提高伪标签的准确性。

Result: 实验结果显示该方法显著提升了系统性能，为自监督语音识别提供了新的解决方案。

Conclusion: 通过图卷积网络优化聚类过程可以有效提高伪标签质量，从而提升自监督语音识别系统的稳健性和性能。

Abstract: With the continuous development of speech recognition technology, speaker
verification (SV) has become an important method for identity authentication.
Traditional SV methods rely on handcrafted feature extraction, while deep
learning has significantly improved system performance. However, the scarcity
of labeled data still limits the widespread application of deep learning in SV.
Self-supervised learning, by mining latent information in large unlabeled
datasets, enhances model generalization and is a key technology to address this
issue.
  DINO is an efficient self-supervised learning method that generates
pseudo-labels from unlabeled speech data through clustering, supporting
subsequent training. However, clustering may produce noisy pseudo-labels, which
can reduce overall recognition performance.
  To address this issue, this paper proposes an improved clustering framework
based on similarity connection graphs and Graph Convolutional Networks. By
leveraging GCNs' ability to model structured data and incorporating relational
information between nodes in the similarity connection graph, the clustering
process is optimized, improving pseudo-label accuracy and enhancing the
robustness and performance of the self-supervised speaker verification system.
Experimental results show that this method significantly improves system
performance and provides a new approach for self-supervised speaker
verification.
  Index Terms: Speaker Verification, Self-Supervised Learning, DINO, Clustering
Algorithm, Graph Convolutional Network, Similarity Connection Graph

</details>


### [18] [Wav2DF-TSL: Two-stage Learning with Efficient Pre-training and Hierarchical Experts Fusion for Robust Audio Deepfake Detection](https://arxiv.org/abs/2509.04161)
*Yunqi Hao,Yihao Chen,Minqiang Xu,Jianbo Zhan,Liang He,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 提出基于预训练和分层专家融合的两阶段学习策略Wav2DF-TSL，用于鲁棒的音频深度伪造检测，通过适配器学习伪造语音特征并使用HA-MoE方法动态融合多级伪造线索


<details>
  <summary>Details</summary>
Motivation: 现有SSL模型主要依赖大规模真实语音预训练，缺乏伪造样本学习，导致在音频深度伪造检测任务微调过程中容易受到域偏差影响

Method: 两阶段学习策略：预训练阶段使用适配器从3000小时无标签伪造语音中高效学习伪影特征；微调阶段提出分层自适应专家混合(HA-MoE)方法，通过门控路由的多专家协作动态融合多级伪造线索

Result: 在四个基准数据集上显著优于基线系统，特别是在跨域In-the-wild数据集上，等错误率相对提升27.5%，优于现有最先进系统

Conclusion: 该方法通过有效学习伪造特征和动态融合多级线索，显著提升了音频深度伪造检测的鲁棒性和跨域性能

Abstract: In recent years, self-supervised learning (SSL) models have made significant
progress in audio deepfake detection (ADD) tasks. However, existing SSL models
mainly rely on large-scale real speech for pre-training and lack the learning
of spoofed samples, which leads to susceptibility to domain bias during the
fine-tuning process of the ADD task. To this end, we propose a two-stage
learning strategy (Wav2DF-TSL) based on pre-training and hierarchical expert
fusion for robust audio deepfake detection. In the pre-training stage, we use
adapters to efficiently learn artifacts from 3000 hours of unlabelled spoofed
speech, improving the adaptability of front-end features while mitigating
catastrophic forgetting. In the fine-tuning stage, we propose the hierarchical
adaptive mixture of experts (HA-MoE) method to dynamically fuse multi-level
spoofing cues through multi-expert collaboration with gated routing.
Experimental results show that the proposed method significantly outperforms
the baseline system on all four benchmark datasets, especially on the
cross-domain In-the-wild dataset, achieving a 27.5% relative improvement in
equal error rate (EER), outperforming the existing state-of-the-art systems.
Index Terms: audio deepfake detection, self-supervised learning,
parameter-efficient fine-tuning, mixture of experts

</details>


### [19] [PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music](https://arxiv.org/abs/2509.04215)
*Hayeon Bang,Eunjin Choi,Seungheon Doh,Juhan Nam*

Main category: cs.SD

TL;DR: PianoBind是一个专门针对钢琴音乐的多模态联合嵌入模型，能够有效捕捉钢琴音乐的细微语义差异，在文本到音乐检索任务上优于通用音乐表示模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用音乐表示模型难以捕捉独奏钢琴音乐中的细微语义差异，且现有钢琴专用模型多为单模态，无法充分利用钢琴音乐固有的多模态特性（音频、符号、文本）。

Method: 提出PianoBind钢琴专用多模态联合嵌入模型，系统研究多源训练策略和模态利用方法，在联合嵌入框架中优化处理小规模和同质钢琴数据集。

Result: 实验表明PianoBind能够学习捕捉钢琴音乐细微差别的多模态表示，在领域内和领域外钢琴数据集上的文本到音乐检索性能优于通用音乐联合嵌入模型。

Conclusion: 该模型不仅解决了钢琴音乐表示的特殊挑战，其设计选择也为其他同质数据集的多模态表示学习提供了可复用的见解。

Abstract: Solo piano music, despite being a single-instrument medium, possesses
significant expressive capabilities, conveying rich semantic information across
genres, moods, and styles. However, current general-purpose music
representation models, predominantly trained on large-scale datasets, often
struggle to captures subtle semantic distinctions within homogeneous solo piano
music. Furthermore, existing piano-specific representation models are typically
unimodal, failing to capture the inherently multimodal nature of piano music,
expressed through audio, symbolic, and textual modalities. To address these
limitations, we propose PianoBind, a piano-specific multimodal joint embedding
model. We systematically investigate strategies for multi-source training and
modality utilization within a joint embedding framework optimized for capturing
fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano
datasets. Our experimental results demonstrate that PianoBind learns multimodal
representations that effectively capture subtle nuances of piano music,
achieving superior text-to-music retrieval performance on in-domain and
out-of-domain piano datasets compared to general-purpose music joint embedding
models. Moreover, our design choices offer reusable insights for multimodal
representation learning with homogeneous datasets beyond piano music.

</details>


### [20] [AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open Worlds](https://arxiv.org/abs/2509.04345)
*Qizhou Wang,Hanxun Huang,Guansong Pang,Sarah Erfani,Christopher Leckie*

Main category: cs.SD

TL;DR: AUDETER是一个大规模深度伪造音频数据集，包含4500+小时合成音频，用于训练和评估深度伪造音频检测模型，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音生成系统能产生难以区分真伪的音频，现有检测方法在真实环境中效果不佳，主要原因是训练和测试样本之间的领域偏移，以及缺乏包含多样化、最新音频的真实数据集。

Method: 构建AUDETER数据集，包含4500多小时合成音频，由11个最新TTS模型和10个声码器生成，总计300万音频片段，是目前规模最大的深度伪造音频数据集。

Result: 实验表明：1）现有SOTA方法在新样本上泛化能力差，误报率高；2）使用AUDETER训练的方法检测错误率降低44.1%-51.6%，在跨域样本上错误率仅为4.17%。

Conclusion: AUDETER数据集能有效训练通用深度伪造音频检测器，显著提升检测性能，为解决真实环境中的音频深度伪造检测问题提供了重要资源。

Abstract: Speech generation systems can produce remarkably realistic vocalisations that
are often indistinguishable from human speech, posing significant authenticity
challenges. Although numerous deepfake detection methods have been developed,
their effectiveness in real-world environments remains unrealiable due to the
domain shift between training and test samples arising from diverse human
speech and fast evolving speech synthesis systems. This is not adequately
addressed by current datasets, which lack real-world application challenges
with diverse and up-to-date audios in both real and deep-fake categories. To
fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,
highly diverse deepfake audio dataset for comprehensive evaluation and robust
development of generalised models for deepfake audio detection. It consists of
over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10
vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio
clips, making it the largest deepfake audio dataset by scale. Through extensive
experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods
trained on existing datasets struggle to generalise to novel deepfake audio
samples and suffer from high false positive rates on unseen human voice,
underscoring the need for a comprehensive dataset; and ii) these methods
trained on AUDETER achieve highly generalised detection performance and
significantly reduce detection error rate by 44.1% to 51.6%, achieving an error
rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild
dataset, paving the way for training generalist deepfake audio detectors.
AUDETER is available on GitHub.

</details>


### [21] [Denoising GER: A Noise-Robust Generative Error Correction with LLM for Speech Recognition](https://arxiv.org/abs/2509.04392)
*Yanyan Liu,Minqiang Xu,Yihao Chen,Liang He,Lei Fang,Sian Fang,Lin Liu*

Main category: cs.SD

TL;DR: 提出Denoising GER框架，通过噪声自适应声学编码器和异质特征补偿动态融合机制，提升LLM在复杂噪声环境下语音识别后处理的生成错误纠正能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在复杂噪声环境中适应性差、信息利用率低的问题，提升语音识别后处理的错误纠正效果

Method: 使用噪声自适应声学编码器增强模型对不同噪声场景的适应性，采用异质特征补偿动态融合机制优化多模态信息整合，引入强化学习训练策略

Result: 在噪声环境中显著提高了准确性和鲁棒性，在未见过的噪声场景中表现出良好的泛化能力

Conclusion: Denoising GER框架有效解决了复杂噪声环境下语音识别后处理的生成错误纠正问题，具有实际应用价值

Abstract: In recent years, large language models (LLM) have made significant progress
in the task of generation error correction (GER) for automatic speech
recognition (ASR) post-processing. However, in complex noisy environments, they
still face challenges such as poor adaptability and low information
utilization, resulting in limited effectiveness of GER. To address these
issues, this paper proposes a noise-robust multi-modal GER framework (Denoising
GER). The framework enhances the model's adaptability to different noisy
scenarios through a noise-adaptive acoustic encoder and optimizes the
integration of multi-modal information via a heterogeneous feature compensation
dynamic fusion (HFCDF) mechanism, improving the LLM's utilization of
multi-modal information. Additionally, reinforcement learning (RL) training
strategies are introduced to enhance the model's predictive capabilities.
Experimental results demonstrate that Denoising GER significantly improves
accuracy and robustness in noisy environments and exhibits good generalization
abilities in unseen noise scenarios.

</details>


### [22] [Contextualized Token Discrimination for Speech Search Query Correction](https://arxiv.org/abs/2509.04393)
*Junyu Lu,Di Jiang,Mengze Hong,Victor Junqiu Wei,Qintian Guo,Zhiyang Su*

Main category: cs.SD

TL;DR: 这篇论文提出了一种名为上下文化分识判别(CTD)的新方法，用于改善语音搜索查询的拼写纠正效果。


<details>
  <summary>Details</summary>
Motivation: 随着语音搜索和自动语音识别(ASR)系统的普及，需要有效的方法来纠正ASR转录中的错误，帮助用户更清晰地表达搜索意图。

Method: 首先使用BERT生成令牌级别的上下文表征，然后构建组合层来增强语义信息，最后通过比较原始令牌表征和上下文表征来纠正错误令牌并生成正确查询。

Result: 大量实验证明该方法在所有指标上都表现优异，还提供了包含错误ASR转录的新标准数据集以供全面评估音频查询纠正。

Conclusion: CTD方法在语音查询拼写纠正任务中表现突出，为语音搜索引擎提供了有效的纠正能力。

Abstract: Query spelling correction is an important function of modern search engines
since it effectively helps users express their intentions clearly. With the
growing popularity of speech search driven by Automated Speech Recognition
(ASR) systems, this paper introduces a novel method named Contextualized Token
Discrimination (CTD) to conduct effective speech query correction. In CTD, we
first employ BERT to generate token-level contextualized representations and
then construct a composition layer to enhance semantic information. Finally, we
produce the correct query according to the aggregated token representation,
correcting the incorrect tokens by comparing the original token representations
and the contextualized representations. Extensive experiments demonstrate the
superior performance of our proposed method across all metrics, and we further
present a new benchmark dataset with erroneous ASR transcriptions to offer
comprehensive evaluations for audio query correction.

</details>

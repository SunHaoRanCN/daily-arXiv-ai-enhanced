{"id": "2509.04629", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04629", "abs": "https://arxiv.org/abs/2509.04629", "authors": ["Hannes Rosseel", "Toon van Waterschoot"], "title": "On Time Delay Interpolation for Improved Acoustic Reflector Localization", "comment": "20 pages, 13 figures, 2 tables, submitted to J. Acoust. Soc. Am", "summary": "The localization of acoustic reflectors is a fundamental component in various\napplications, including room acoustics analysis, sound source localization, and\nacoustic scene analysis. Time Delay Estimation (TDE) is essential for\ndetermining the position of reflectors relative to a sensor array. Traditional\nTDE algorithms generally yield time delays that are integer multiples of the\noperating sampling period, potentially lacking sufficient time resolution. To\nachieve subsample TDE accuracy, various interpolation methods, including\nparabolic, Gaussian, frequency, and sinc interpolation, have been proposed.\nThis paper presents a comprehensive study on time delay interpolation to\nachieve subsample accuracy for acoustic reflector localization in reverberant\nconditions. We derive the Whittaker-Shannon interpolation formula from the\npreviously proposed sinc interpolation in the context of short-time windowed\nTDE for acoustic reflector localization. Simulations show that sinc and\nWhittaker-Shannon interpolation outperform existing methods in terms of time\ndelay error and positional error for critically sampled and band-limited\nreflections. Performance is evaluated on real-world measurements from the\nMYRiAD dataset, showing that sinc and Whittaker-Shannon interpolation\nconsistently provide reliable performance across different sensor-source pairs\nand loudspeaker positions. These results can enhance the precision of acoustic\nreflector localization systems, vital for applications such as room acoustics\nanalysis, sound source localization, and acoustic scene analysis.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u901a\u8fc7sinc\u548cWhittaker-Shannon\u63d2\u503c\u65b9\u6cd5\u5b9e\u73b0\u6b21\u6837\u672c\u7cbe\u5ea6\u7684\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u6c27\u58f0\u53cd\u5c04\u4f53\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5ef6\u8fdf\u4f30\u8ba1\u7b97\u6cd5\u53ea\u80fd\u83b7\u5f97\u6574\u6570\u500d\u91d1\u77f3\u6837\u672c\u5468\u671f\u7684\u5ef6\u8fdf\uff0c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u901a\u8fc7\u63d2\u503c\u65b9\u6cd5\u5b9e\u73b0\u6b21\u6837\u672c\u7cbe\u5ea6\u7684\u4f30\u8ba1\u3002", "method": "\u4ece\u73b0\u6709sinc\u63d2\u503c\u65b9\u6cd5\u63a8\u5bfc\u51faWhittaker-Shannon\u63d2\u503c\u516c\u5f0f\uff0c\u5e94\u7528\u4e8e\u77ed\u65f6\u7a97TDE\u65b9\u6cd5\u8fdb\u884c\u6c27\u58f0\u53cd\u5c04\u4f53\u5b9a\u4f4d\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793asinc\u548cWhittaker-Shannon\u63d2\u503c\u5728\u5173\u952e\u91c7\u6837\u548c\u5e26\u9650\u53cd\u5c04\u60c5\u51b5\u4e0b\u5728\u65f6\u95f4\u5ef6\u8fdf\u8bef\u5dee\u548c\u4f4d\u7f6e\u8bef\u5dee\u65b9\u9762\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff1b\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u540c\u4f20\u611f\u5668-\u58f0\u6e90\u5bf9\u548c\u626c\u58f0\u5668\u4f4d\u7f6e\u4e0b\u7684\u7a33\u5065\u6027\u3002", "conclusion": "sinc\u548cWhittaker-Shannon\u63d2\u503c\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6c27\u58f0\u53cd\u5c04\u4f53\u5b9a\u4f4d\u7cfb\u7edf\u7684\u7cbe\u5ea6\uff0c\u5bf9\u623f\u95f4\u58f0\u5b66\u5206\u6790\u3001\u58f0\u6e90\u5b9a\u4f4d\u548c\u58f0\u573a\u5206\u6790\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.04667", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04667", "abs": "https://arxiv.org/abs/2509.04667", "authors": ["Waris Quamer", "Ricardo Gutierrez-Osuna"], "title": "DarkStream: real-time speech anonymization with low latency", "comment": "Accepted for presentation at ASRU 2025", "summary": "We propose DarkStream, a streaming speech synthesis model for real-time\nspeaker anonymization. To improve content encoding under strict latency\nconstraints, DarkStream combines a causal waveform encoder, a short lookahead\nbuffer, and transformer-based contextual layers. To further reduce inference\ntime, the model generates waveforms directly via a neural vocoder, thus\nremoving intermediate mel-spectrogram conversions. Finally, DarkStream\nanonymizes speaker identity by injecting a GAN-generated pseudo-speaker\nembedding into linguistic features from the content encoder. Evaluations show\nour model achieves strong anonymization, yielding close to 50% speaker\nverification EER (near-chance performance) on the lazy-informed attack\nscenario, while maintaining acceptable linguistic intelligibility (WER within\n9%). By balancing low-latency, robust privacy, and minimal intelligibility\ndegradation, DarkStream provides a practical solution for privacy-preserving\nreal-time speech communication.", "AI": {"tldr": "DarkStream\u662f\u4e00\u4e2a\u5b9e\u65f6\u8bed\u97f3\u5408\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u8bf4\u8bdd\u4eba\u533f\u540d\u5316\uff0c\u901a\u8fc7\u56e0\u679c\u6ce2\u5f62\u7f16\u7801\u5668\u3001\u77ed\u524d\u77bb\u7f13\u51b2\u533a\u548cTransformer\u5c42\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u5185\u5bb9\u7f16\u7801\uff0c\u76f4\u63a5\u751f\u6210\u6ce2\u5f62\u907f\u514d\u9891\u8c31\u8f6c\u6362\uff0c\u4f7f\u7528GAN\u751f\u6210\u7684\u4f2a\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5b9e\u73b0\u8eab\u4efd\u533f\u540d\u5316", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5b9e\u65f6\u8bed\u97f3\u901a\u4fe1\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u9700\u8981\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u8bf4\u8bdd\u4eba\u533f\u540d\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u97f3\u5185\u5bb9\u53ef\u61c2\u5ea6", "method": "\u7ed3\u5408\u56e0\u679c\u6ce2\u5f62\u7f16\u7801\u5668\u3001\u77ed\u524d\u77bb\u7f13\u51b2\u533a\u548c\u57fa\u4e8eTransformer\u7684\u4e0a\u4e0b\u6587\u5c42\u8fdb\u884c\u5185\u5bb9\u7f16\u7801\uff1b\u76f4\u63a5\u901a\u8fc7\u795e\u7ecf\u58f0\u7801\u5668\u751f\u6210\u6ce2\u5f62\uff1b\u6ce8\u5165GAN\u751f\u6210\u7684\u4f2a\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5230\u8bed\u8a00\u7279\u5f81\u4e2d\u5b9e\u73b0\u533f\u540d\u5316", "result": "\u5728lazy-informed\u653b\u51fb\u573a\u666f\u4e0b\u8fbe\u5230\u63a5\u8fd150%\u7684\u8bf4\u8bdd\u4eba\u9a8c\u8bc1EER\uff08\u63a5\u8fd1\u968f\u673a\u6027\u80fd\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u8bed\u8a00\u53ef\u61c2\u5ea6\uff08WER\u57289%\u4ee5\u5185\uff09", "conclusion": "DarkStream\u901a\u8fc7\u5e73\u8861\u4f4e\u5ef6\u8fdf\u3001\u5f3a\u9690\u79c1\u4fdd\u62a4\u548c\u6700\u5c0f\u53ef\u61c2\u5ea6\u635f\u5931\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5b9e\u65f6\u8bed\u97f3\u901a\u4fe1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.04685", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04685", "abs": "https://arxiv.org/abs/2509.04685", "authors": ["Rui-Chen Zheng", "Wenrui Liu", "Hui-Peng Du", "Qinglin Zhang", "Chong Deng", "Qian Chen", "Wen Wang", "Yang Ai", "Zhen-Hua Ling"], "title": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding", "comment": null, "summary": "Existing speech tokenizers typically assign a fixed number of tokens per\nsecond, regardless of the varying information density or temporal fluctuations\nin the speech signal. This uniform token allocation mismatches the intrinsic\nstructure of speech, where information is distributed unevenly over time. To\naddress this, we propose VARSTok, a VAriable-frame-Rate Speech Tokenizer that\nadapts token allocation based on local feature similarity. VARSTok introduces\ntwo key innovations: (1) a temporal-aware density peak clustering algorithm\nthat adaptively segments speech into variable-length units, and (2) a novel\nimplicit duration coding scheme that embeds both content and temporal span into\na single token index, eliminating the need for auxiliary duration predictors.\nExtensive experiments show that VARSTok significantly outperforms strong\nfixed-rate baselines. Notably, it achieves superior reconstruction naturalness\nwhile using up to 23% fewer tokens than a 40 Hz fixed-frame-rate baseline.\nVARSTok further yields lower word error rates and improved naturalness in\nzero-shot text-to-speech synthesis. To the best of our knowledge, this is the\nfirst work to demonstrate that a fully dynamic, variable-frame-rate acoustic\nspeech tokenizer can be seamlessly integrated into downstream speech language\nmodels. Speech samples are available at https://zhengrachel.github.io/VARSTok.", "AI": {"tldr": "VARSTok\u662f\u4e00\u79cd\u53ef\u53d8\u5e27\u7387\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u6bb5\u548c\u9690\u5f0f\u65f6\u957f\u7f16\u7801\uff0c\u5728\u51cf\u5c1123%token\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u8bed\u97f3\u91cd\u5efa\u8d28\u91cf", "motivation": "\u73b0\u6709\u8bed\u97f3\u5206\u8bcd\u5668\u91c7\u7528\u56fa\u5b9a\u5e27\u7387\uff0c\u65e0\u6cd5\u5339\u914d\u8bed\u97f3\u4fe1\u53f7\u4e2d\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5747\u5300\u7684\u7279\u6027\uff0c\u5bfc\u81f4token\u5206\u914d\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51fa\u65f6\u95f4\u611f\u77e5\u5bc6\u5ea6\u5cf0\u503c\u805a\u7c7b\u7b97\u6cd5\u8fdb\u884c\u81ea\u9002\u5e94\u5206\u6bb5\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u9690\u5f0f\u65f6\u957f\u7f16\u7801\u65b9\u6848\uff0c\u5c06\u5185\u5bb9\u548c\u65f6\u95f4\u8de8\u5ea6\u5d4c\u5165\u5355\u4e2atoken\u7d22\u5f15", "result": "\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u5e27\u7387\u57fa\u7ebf\uff0c\u91cd\u5efa\u81ea\u7136\u5ea6\u66f4\u597d\uff0c\u96f6\u6837\u672cTTS\u5408\u6210\u4e2d\u8bcd\u9519\u8bef\u7387\u66f4\u4f4e\u3001\u81ea\u7136\u5ea6\u66f4\u9ad8", "conclusion": "\u9996\u6b21\u8bc1\u660e\u5b8c\u5168\u52a8\u6001\u7684\u53ef\u53d8\u5e27\u7387\u8bed\u97f3\u5206\u8bcd\u5668\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u4e0b\u6e38\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4e3a\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684token\u5316\u65b9\u6848"}}
{"id": "2509.04830", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04830", "abs": "https://arxiv.org/abs/2509.04830", "authors": ["Erica Cooper", "Takuma Okamoto", "Yamato Ohtani", "Tomoki Toda", "Hisashi Kawai"], "title": "Layer-wise Analysis for Quality of Multilingual Synthesized Speech", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "While supervised quality predictors for synthesized speech have demonstrated\nstrong correlations with human ratings, their requirement for in-domain labeled\ntraining data hinders their generalization ability to new domains. Unsupervised\napproaches based on pretrained self-supervised learning (SSL) based models and\nautomatic speech recognition (ASR) models are a promising alternative; however,\nlittle is known about how these models encode information about speech quality.\nTowards the goal of better understanding how different aspects of speech\nquality are encoded in a multilingual setting, we present a layer-wise analysis\nof multilingual pretrained speech models based on reference modeling. We find\nthat features extracted from early SSL layers show correlations with human\nratings of synthesized speech, and later layers of ASR models can predict\nquality of non-neural systems as well as intelligibility. We also demonstrate\nthe importance of using well-matched reference data.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u5c42\u5206\u6790\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\uff0c\u53d1\u73b0\u65e9\u671fSSL\u5c42\u7279\u5f81\u4e0e\u5408\u6210\u8bed\u97f3\u8d28\u91cf\u76f8\u5173\uff0c\u540e\u671fASR\u5c42\u80fd\u9884\u6d4b\u975e\u795e\u7ecf\u7cfb\u7edf\u7684\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\uff0c\u5e76\u5f3a\u8c03\u4e86\u5339\u914d\u53c2\u8003\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u76d1\u7763\u5f0f\u8bed\u97f3\u8d28\u91cf\u9884\u6d4b\u5668\u9700\u8981\u9886\u57df\u5185\u6807\u6ce8\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u65e0\u76d1\u7763\u65b9\u6cd5\u867d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5bf9\u5176\u5982\u4f55\u7f16\u7801\u8bed\u97f3\u8d28\u91cf\u4fe1\u606f\u4e86\u89e3\u751a\u5c11\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53c2\u8003\u5efa\u6a21\u7684\u5206\u5c42\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\uff08SSL\u548cASR\u6a21\u578b\uff09\u8fdb\u884c\u9010\u5c42\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u5c42\u5982\u4f55\u7f16\u7801\u8bed\u97f3\u8d28\u91cf\u4fe1\u606f\u3002", "result": "\u53d1\u73b0\u65e9\u671fSSL\u5c42\u7279\u5f81\u4e0e\u4eba\u7c7b\u5bf9\u5408\u6210\u8bed\u97f3\u7684\u8bc4\u5206\u76f8\u5173\uff1b\u540e\u671fASR\u5c42\u80fd\u9884\u6d4b\u975e\u795e\u7ecf\u7cfb\u7edf\u7684\u8d28\u91cf\u548c\u53ef\u61c2\u5ea6\uff1b\u4f7f\u7528\u826f\u597d\u5339\u914d\u7684\u53c2\u8003\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u7f16\u7801\u4e86\u8bed\u97f3\u8d28\u91cf\u7684\u4e0d\u540c\u65b9\u9762\uff0c\u4e3a\u65e0\u76d1\u7763\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u53c2\u8003\u6570\u636e\u5339\u914d\u662f\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2509.04682", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.IR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04682", "abs": "https://arxiv.org/abs/2509.04682", "authors": ["Nicholas R. Rasmussen", "Rodrigue Rizk", "Longwei Wang", "KC Santosh"], "title": "Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring", "comment": "Under review as an anonymous submission to IEEETAI - We are allowed\n  an archive submission. Final formatting is yet to be determined", "summary": "Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal\ndata for long-term ecological analysis, but intrinsic noise and complex signal\ndependencies hinder model stability and generalization. Multilayered windowing\nhas improved target sound localization, yet variability from shifting ambient\nnoise, diverse propagation effects, and mixed biological and anthropogenic\nsources demands robust architectures and rigorous evaluation. We introduce\nGetNetUPAM, a hierarchical nested cross-validation framework designed to\nquantify model stability under ecologically realistic variability. Data are\npartitioned into distinct site-year segments, preserving recording\nheterogeneity and ensuring each validation fold reflects a unique environmental\nsubset, reducing overfitting to localized noise and sensor artifacts. Site-year\nblocking enforces evaluation against genuine environmental diversity, while\nstandard cross-validation on random subsets measures generalization across\nUPAM's full signal distribution, a dimension absent from current benchmarks.\nUsing GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution\nPooling and Attention Network (ARPA-N), a neural architecture for irregular\nspectrogram dimensions. Adaptive pooling with spatial attention extends the\nreceptive field, capturing global context without excessive parameters. Under\nGetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet\nbaselines and a log2-scale order-of-magnitude drop in variability across all\nmetrics, enabling consistent detection across site-year folds and advancing\nscalable, accurate bioacoustic monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86GetNetUPAM\u8bc4\u4f30\u6846\u67b6\u548cARPA-N\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u5347\u6c34\u4e0b\u88ab\u52a8\u58f0\u5b66\u76d1\u6d4b\u7684\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u751f\u6001\u591a\u6837\u6027\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u6c34\u4e0b\u88ab\u52a8\u58f0\u5b66\u76d1\u6d4b\u5b58\u5728\u56fa\u6709\u566a\u58f0\u548c\u590d\u6742\u4fe1\u53f7\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u73af\u5883\u591a\u6837\u6027\u65f6\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u67b6\u6784\u548c\u8bc4\u4f30\u6846\u67b6", "method": "GetNetUPAM\u5206\u5c42\u5d4c\u5957\u4ea4\u53c9\u9a8c\u8bc1\u6846\u67b6\uff0c\u6309\u7ad9\u70b9-\u5e74\u4efd\u5206\u6bb5\u6570\u636e\uff1bARPA-N\u795e\u7ecf\u7f51\u7edc\u91c7\u7528\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u6c60\u5316\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u4e0d\u89c4\u5219\u9891\u8c31\u56fe", "result": "ARPA-N\u5728\u5e73\u5747\u7cbe\u5ea6\u4e0a\u6bd4DenseNet\u57fa\u7ebf\u63d0\u534714.4%\uff0c\u6240\u6709\u6307\u6807\u7684\u53d8\u5f02\u6027\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff08log2\u5c3a\u5ea6\uff09\uff0c\u5728\u4e0d\u540c\u7ad9\u70b9-\u5e74\u4efd\u6298\u53e0\u4e0a\u5b9e\u73b0\u4e00\u81f4\u68c0\u6d4b", "conclusion": "GetNetUPAM\u6846\u67b6\u548cARPA-N\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u751f\u7269\u58f0\u5b66\u76d1\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73af\u5883\u591a\u6837\u6027\u6311\u6218"}}
{"id": "2509.04576", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04576", "abs": "https://arxiv.org/abs/2509.04576", "authors": ["Ce Zheng", "Tingting Yang"], "title": "Communication-Efficient Collaborative LLM Inference via Distributed Speculative Decoding", "comment": "Accepted in the Seventeenth International Conference on Wireless\n  Communications and Signal Processing Oct. 23-25, 2025", "summary": "Speculative decoding is an emerging technique that accelerates large language\nmodel (LLM) inference by allowing a smaller draft model to predict multiple\ntokens in advance, which are then verified or corrected by a larger target\nmodel. In AI-native radio access networks (AI-RAN), this paradigm is\nwell-suited for collaborative inference between resource-constrained end\ndevices and more capable edge servers or base stations (BSs). However, existing\ndistributed speculative decoding requires transmitting the full vocabulary\nprobability distribution from the draft model on the device to the target model\nat the BS, which leads to prohibitive uplink communication overhead. To address\nthis issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,\nwhere the draft model transmits only the top-K token raw probabilities and the\ncorresponding token indices instead of the entire distribution. This approach\nsignificantly reduces bandwidth consumption while maintaining inference\nperformance. We further derive an analytical expression for the optimal draft\nlength that maximizes inference throughput, and provide a theoretical analysis\nof the achievable speedup ratio under TK-SLT. Experimental results validate\nboth the efficiency and effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51faTK-SLT\u65b9\u6848\uff0c\u901a\u8fc7\u4ec5\u4f20\u8f93top-K\u6982\u7387\u5206\u5e03\u800c\u975e\u5b8c\u6574\u8bcd\u6c47\u8868\u6765\u51cf\u5c11\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5728AI-RAN\u4e2d\uff0c\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u9700\u8981\u4ece\u8bbe\u5907\u5411\u57fa\u7ad9\u4f20\u8f93\u5b8c\u6574\u8bcd\u6c47\u6982\u7387\u5206\u5e03\uff0c\u5bfc\u81f4\u4e0a\u884c\u901a\u4fe1\u5f00\u9500\u8fc7\u5927\u3002", "method": "Top-K\u7a00\u758f\u5bf9\u6570\u4f20\u8f93\u65b9\u6848\uff0c\u53ea\u4f20\u8f93top-K token\u7684\u539f\u59cb\u6982\u7387\u548c\u5bf9\u5e94\u7d22\u5f15\uff0c\u5e76\u63a8\u5bfc\u6700\u4f18\u8349\u7a3f\u957f\u5ea6\u6700\u5927\u5316\u63a8\u7406\u541e\u5410\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u5e26\u5bbd\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "TK-SLT\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u4e3aAI-RAN\u4e2d\u7684\u534f\u4f5c\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05079", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05079", "abs": "https://arxiv.org/abs/2509.05079", "authors": ["Konstantinos Drossos", "Mikko Heikkinen", "Paschalis Tsiaflakis"], "title": "Lightweight DNN for Full-Band Speech Denoising on Mobile Devices: Exploiting Long and Short Temporal Patterns", "comment": "Accepted for publication in Proceedings of the 2025 IEEE 27th\n  International Workshop on Multimedia Signal Processing (MMSP)", "summary": "Speech denoising (SD) is an important task of many, if not all, modern signal\nprocessing chains used in devices and for everyday-life applications. While\nthere are many published and powerful deep neural network (DNN)-based methods\nfor SD, few are optimized for resource-constrained platforms such as mobile\ndevices. Additionally, most DNN-based methods for SD are not focusing on\nfull-band (FB) signals, i.e. having 48 kHz sampling rate, and/or low latency\ncases. In this paper we present a causal, low latency, and lightweight\nDNN-based method for full-band SD, leveraging both short and long temporal\npatterns. The method is based on a modified UNet architecture employing\nlook-back frames, temporal spanning of convolutional kernels, and recurrent\nneural networks for exploiting short and long temporal patterns in the signal\nand estimated denoising mask. The DNN operates on a causal frame-by-frame basis\ntaking as an input the STFT magnitude, utilizes inverted bottlenecks inspired\nby MobileNet, employs causal instance normalization for channel-wise\nnormalization, and achieves a real-time factor below 0.02 when deployed on a\nmodern mobile phone. The proposed method is evaluated using established speech\ndenoising metrics and publicly available datasets, demonstrating its\neffectiveness in achieving an (SI-)SDR value that outperforms existing FB and\nlow latency SD methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u8d28\u91cf\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5168\u5e26\u5bbd\u8bed\u97f3\u53bb\u566a\uff0c\u5177\u6709\u56e0\u679c\u6027\u3001\u4f4e\u5ef6\u8fdf\u7279\u6027\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bed\u97f3\u53bb\u566a\u65b9\u6cd5\u6ca1\u6709\u4f18\u5316\u8d28\u91cf\u5e76\u9002\u5408\u79fb\u52a8\u8bbe\u5907\uff0c\u4e14\u5c11\u6709\u65b9\u6cd5\u5173\u6ce8\u5168\u5e26\u5bbd\u4fe1\u53f7\u548c\u4f4e\u5ef6\u8fdf\u573a\u666f\u3002", "method": "\u57fa\u4e8e\u4fee\u6539\u7684UNet\u67b6\u6784\uff0c\u91c7\u7528\u56de\u987e\u5e27\u3001\u5377\u79ef\u5185\u6838\u65f6\u95f4\u8303\u56f4\u6269\u5c55\u548c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u6765\u5229\u7528\u77ed\u671f\u548c\u957f\u671f\u65f6\u95f4\u6a21\u5f0f\uff0c\u4f7f\u7528STFT\u5e45\u503c\u4f5c\u4e3a\u8f93\u5165\uff0c\u91c7\u7528MobileNet\u53cd\u5411\u74f6\u9888\u7ed3\u6784\u548c\u56e0\u679c\u5b9e\u4f8b\u5f52\u4e00\u5316\u3002", "result": "\u5728\u73b0\u4ee3\u624b\u673a\u4e0a\u5b9e\u65f6\u56e0\u5b50\u4f4e\u4e8e0.02\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684(SI-)SDR\u6307\u6807\u8d85\u8fc7\u73b0\u6709\u5168\u5e26\u5bbd\u548c\u4f4e\u5ef6\u8fdf\u8bed\u97f3\u53bb\u566a\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79fb\u52a8\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u5168\u5e26\u5bbd\u8bed\u97f3\u53bb\u566a\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8f83\u5f3a\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.04715", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04715", "abs": "https://arxiv.org/abs/2509.04715", "authors": ["Mia Y. Wang", "Mackenzie Linn", "Andrew P. Berg", "Qian Zhang"], "title": "A Multiclass Acoustic Dataset and Interactive Tool for Analyzing Drone Signatures in Real-World Environments", "comment": "This article extends our previous work presented in the 2024\n  Artificial Intelligence x Humanities, Education, and Art (2024 AIxHeart)\n  Conference", "summary": "The rapid proliferation of drones across various industries has introduced\nsignificant challenges related to privacy, security, and noise pollution.\nCurrent drone detection systems, primarily based on visual and radar\ntechnologies, face limitations under certain conditions, highlighting the need\nfor effective acoustic-based detection methods. This paper presents a unique\nand comprehensive dataset of drone acoustic signatures, encompassing 32\ndifferent categories differentiated by brand and model. The dataset includes\nraw audio recordings, spectrogram plots, and Mel-frequency cepstral coefficient\n(MFCC) plots for each drone. Additionally, we introduce an interactive web\napplication that allows users to explore this dataset by selecting specific\ndrone categories, listening to the associated audio, and viewing the\ncorresponding spectrogram and MFCC plots. This tool aims to facilitate research\nin drone detection, classification, and acoustic analysis, supporting both\ntechnological advancements and educational initiatives. The paper details the\ndataset creation process, the design and implementation of the web application,\nand provides experimental results and user feedback. Finally, we discuss\npotential applications and future work to expand and enhance the project.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b32\u79cd\u4e0d\u540c\u54c1\u724c\u548c\u6a21\u578b\u65e0\u4eba\u673a\u7684\u97f3\u54cd\u7b7e\u540d\u6570\u636e\u96c6\uff0c\u5305\u62ec\u539f\u59cb\u97f3\u9891\u3001\u8c31\u56fe\u548cMFCC\u56fe\u8c31\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7f51\u9875\u5e94\u7528\u6765\u4fbf\u4e8e\u6570\u636e\u96c6\u63a2\u7d22\u548c\u5206\u6790\u3002", "motivation": "\u65e0\u4eba\u673a\u7684\u5feb\u901f\u666e\u53ca\u5e26\u6765\u4e86\u9690\u79c1\u3001\u5b89\u5168\u548c\u566a\u58f0\u6c61\u67d3\u7b49\u6311\u6218\uff0c\u73b0\u6709\u7684\u89c6\u89c9\u548c\u96f7\u8fbe\u68c0\u6d4b\u7cfb\u7edf\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u6709\u6548\u7684\u57fa\u4e8e\u58f0\u97f3\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u6536\u96c6\u4e8632\u79cd\u4e0d\u540c\u54c1\u724c\u548c\u6a21\u578b\u65e0\u4eba\u673a\u7684\u97f3\u54cd\u6570\u636e\uff0c\u5305\u62ec\u539f\u59cb\u97f3\u9891\u5f55\u97f3\u3001\u8c31\u56fe\u548cMel\u9891\u7387\u5019\u5076\u7cfb\u6570(MFCC)\u56fe\u8c31\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7f51\u9875\u5e94\u7528\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65e0\u4eba\u673a\u97f3\u54cd\u7b7e\u540d\u6570\u636e\u96c6\uff0c\u5e76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u7f51\u9875\u5e94\u7528\uff0c\u4f7f\u5f97\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u7279\u5b9a\u65e0\u4eba\u673a\u7c7b\u578b\u3001\u542c\u53d6\u97f3\u9891\u5e76\u67e5\u770b\u5bf9\u5e94\u7684\u8c31\u56fe\u548cMFCC\u56fe\u8c31\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u5de5\u5177\u6709\u52a9\u4e8e\u63a8\u52a8\u65e0\u4eba\u673a\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u97f3\u54cd\u5206\u6790\u7684\u7814\u7a76\uff0c\u652f\u6301\u6280\u672f\u8fdb\u6b65\u548c\u6559\u80b2\u521b\u65b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u6269\u5c55\u548c\u589e\u5f3a\u8be5\u9879\u76ee\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2509.04692", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04692", "abs": "https://arxiv.org/abs/2509.04692", "authors": ["Michael Shifrin", "Joseph Tabrikian", "Igal Bilik"], "title": "Tangential Velocity Estimation Using Near-Field Automotive Radar Model", "comment": null, "summary": "This work investigates the problem of tangential velocity estimation in\nautomotive radar systems, addressing the limitations of conventionally\nconsidered models. Conventional automotive radars are usually based on\nfar-field models and estimate the target's range, radial velocity, and\ndirection-of-arrival (DOA) but are not able to estimate the tangential\ncomponent of the target 2-D velocity, which is a critical parameter for\nreliable perception of dynamic environments. To address this challenge, we\nintroduce the near-field radar model, which considers various migration\nelements in range, radial velocity, and Doppler along time and space.\nConventionally, these migration effects result in smearing of the likelihood\nfunction for estimating the target parameters. However, if the model is\ncorrectly specified, these migration effects are informative for tangential\nvelocity estimation. We conduct an identifiability analysis for tangential\nvelocity estimation using the Cram\\'er-Rao bound and ambiguity function. The\ninsights from this study motivate the use of a separated array configuration\nand the development of a computationally efficient maximum likelihood based\nalgorithm designed to utilize target migrations for tangential velocity\nestimation, while maintaining practical computational complexity. In addition\nto tangential velocity estimation, the proposed algorithm mitigates likelihood\nsmearing in range, radial velocity, and Doppler. Simulations validate the\ntheoretical feasibility study, and evaluate the algorithms' performance in both\nsingle- and multi-target scenarios. The proposed approach improves the accuracy\nand reliability of automotive radars, enhancing situational awareness for\nadvanced driver assistance systems and autonomous vehicles.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u4e2d\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u8fd1\u573a\u96f7\u8fbe\u6a21\u578b\u548c\u5229\u7528\u76ee\u6807\u8fc1\u79fb\u6548\u5e94\u6765\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u6c7d\u8f66\u96f7\u8fbe\u57fa\u4e8e\u8fdc\u573a\u6a21\u578b\uff0c\u53ea\u80fd\u4f30\u8ba1\u76ee\u6807\u7684\u8ddd\u79bb\u3001\u7eb5\u5411\u901f\u5ea6\u548c\u5230\u8fbe\u65b9\u5411\uff0c\u65e0\u6cd5\u4f30\u8ba1\u5173\u952e\u7684\u5207\u5411\u901f\u5ea6\u5206\u91cf\uff0c\u800c\u8fd9\u5bf9\u52a8\u6001\u73af\u5883\u7684\u53ef\u9760\u611f\u77e5\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u8fd1\u573a\u96f7\u8fbe\u6a21\u578b\uff0c\u8003\u8651\u8303\u56f4\u3001\u7eb5\u5411\u901f\u5ea6\u548c\u591a\u666e\u52d2\u6cbf\u65f6\u95f4\u548c\u7a7a\u95f4\u7684\u8fc1\u79fb\u5143\u7d20\u3002\u901a\u8fc7Cram\u00e9r-Rao\u754c\u548c\u6a21\u7cca\u51fd\u6570\u8fdb\u884c\u53ef\u8bc6\u522b\u6027\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u5206\u79bb\u6570\u7ec4\u914d\u7f6e\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u9ad8\u6548\u7b97\u6cd5\u6765\u5229\u7528\u76ee\u6807\u8fc1\u79fb\u8fdb\u884c\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\u3002", "result": "\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u7814\u7a76\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u7b97\u6cd5\u6027\u80fd\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u51cf\u8f7b\u4e86\u8303\u56f4\u3001\u7eb5\u5411\u901f\u5ea6\u548c\u591a\u666e\u52d2\u4e2d\u7684\u4f3c\u7136\u51fd\u6570\u6c61\u67d3\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6c7d\u8f66\u96f7\u8fbe\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5207\u5411\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u548c\u81ea\u4e3b\u9a7e\u9a76\u8f66\u8f7b\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.05175", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05175", "abs": "https://arxiv.org/abs/2509.05175", "authors": ["Georg G\u00f6tz", "Daniel Gert Nielsen", "Steinar Gu\u00f0j\u00f3nsson", "Finnur Pind"], "title": "Room-acoustic simulations as an alternative to measurements for audio-algorithm evaluation", "comment": null, "summary": "Audio-signal-processing and audio-machine-learning (ASP/AML) algorithms are\nubiquitous in modern technology like smart devices, wearables, and\nentertainment systems. Development of such algorithms and models typically\ninvolves a formal evaluation to demonstrate their effectiveness and progress\nbeyond the state-of-the-art. Ideally, a thorough evaluation should cover many\ndiverse application scenarios and room-acoustic conditions. However, in\npractice, evaluation datasets are often limited in size and diversity because\nthey rely on costly and time-consuming measurements. This paper explores how\nroom-acoustic simulations can be used for evaluating ASP/AML algorithms. To\nthis end, we evaluate three ASP/AML algorithms with room-acoustic measurements\nand data from different simulation engines, and assess the match between the\nevaluation results obtained from measurements and simulations. The presented\ninvestigation compares a numerical wave-based solver with two geometrical\nacoustics simulators. While numerical wave-based simulations yielded similar\nevaluation results as measurements for all three evaluated ASP/AML algorithms,\ngeometrical acoustic simulations could not replicate the measured evaluation\nresults as reliably.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u623f\u95f4\u58f0\u5b66\u6a21\u62df\u6765\u8bc4\u4f30\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u548c\u97f3\u9891\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5bf9\u6bd4\u4e86\u6ce2\u52a8\u6570\u503c\u6a21\u62df\u548c\u51e0\u4f55\u58f0\u5b66\u6a21\u62df\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u97f3\u9891\u7b97\u6cd5\u8bc4\u4f30\u9762\u4e34\u6570\u636e\u96c6\u9650\u5236\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u6210\u672c\u4f4e\u3001\u6548\u7387\u9ad8\u7684\u65b9\u6cd5\u6765\u6a21\u62df\u591a\u6837\u5316\u7684\u5e94\u7528\u573a\u666f\u548c\u623f\u95f4\u58f0\u5b66\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u6570\u503c\u6ce2\u52a8\u6cd5\u6a21\u62df\u5668\u548c\u4e24\u79cd\u51e0\u4f55\u58f0\u5b66\u6a21\u62df\u5668\u751f\u6210\u6a21\u62df\u6570\u636e\uff0c\u5bf9\u4e09\u79cdASP/AML\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u5b9e\u9645\u6d4b\u91cf\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6570\u503c\u6ce2\u52a8\u6cd5\u6a21\u62df\u5728\u6240\u6709\u4e09\u79cd\u7b97\u6cd5\u8bc4\u4f30\u4e2d\u90fd\u4ea7\u751f\u4e86\u4e0e\u5b9e\u9645\u6d4b\u91cf\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u800c\u51e0\u4f55\u58f0\u5b66\u6a21\u62df\u65e0\u6cd5\u50cf\u6d4b\u91cf\u7ed3\u679c\u90a3\u6837\u53ef\u9760\u5730\u590d\u73b0\u3002", "conclusion": "\u623f\u95f4\u58f0\u5b66\u6a21\u62df\u53ef\u4ee5\u4f5c\u4e3a\u97f3\u9891\u7b97\u6cd5\u8bc4\u4f30\u7684\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u662f\u6570\u503c\u6ce2\u52a8\u6cd5\u6a21\u62df\u80fd\u591f\u63d0\u4f9b\u4e0e\u5b9e\u9645\u6d4b\u91cf\u76f8\u4f3c\u7684\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2509.04744", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04744", "abs": "https://arxiv.org/abs/2509.04744", "authors": ["Gagan Mundada", "Yash Vishe", "Amit Namburi", "Xin Xu", "Zachary Novack", "Julian McAuley", "Junda Wu"], "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities across various vision-language tasks. However, their\nreasoning abilities in the multimodal symbolic music domain remain largely\nunexplored. We introduce WildScore, the first in-the-wild multimodal symbolic\nmusic reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to\ninterpret real-world music scores and answer complex musicological queries.\nEach instance in WildScore is sourced from genuine musical compositions and\naccompanied by authentic user-generated questions and discussions, capturing\nthe intricacies of practical music analysis. To facilitate systematic\nevaluation, we propose a systematic taxonomy, comprising both high-level and\nfine-grained musicological ontologies. Furthermore, we frame complex music\nreasoning as multiple-choice question answering, enabling controlled and\nscalable assessment of MLLMs' symbolic music understanding. Empirical\nbenchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns\nin their visual-symbolic reasoning, uncovering both promising directions and\npersistent challenges for MLLMs in symbolic music reasoning and analysis. We\nrelease the dataset and code.", "AI": {"tldr": "WildScore\u662f\u9996\u4e2a\u591a\u6a21\u6001\u7b26\u53f7\u97f3\u4e50\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u5728\u771f\u5b9e\u4e50\u8c31\u5206\u6790\u548c\u590d\u6742\u97f3\u4e50\u5b66\u67e5\u8be2\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4f18\u52bf\u548c\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7b26\u53f7\u97f3\u4e50\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u771f\u5b9e\u97f3\u4e50\u4f5c\u54c1\u548c\u7528\u6237\u751f\u6210\u95ee\u9898\u7684WildScore\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u97f3\u4e50\u5b66\u672c\u4f53\u5206\u7c7b\uff0c\u5e76\u5c06\u590d\u6742\u97f3\u4e50\u63a8\u7406\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u9879\u9009\u62e9\u9898\u5f62\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5bf9\u6700\u5148\u8fdbMLLMs\u7684\u5b9e\u8bc1\u8bc4\u4f30\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u89c6\u89c9-\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u7684\u6709\u8da3\u6a21\u5f0f\uff0c\u65e2\u663e\u793a\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u4e5f\u66b4\u9732\u4e86\u5728\u7b26\u53f7\u97f3\u4e50\u63a8\u7406\u548c\u5206\u6790\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "WildScore\u57fa\u51c6\u4e3a\u8bc4\u4f30MLLMs\u5728\u7b26\u53f7\u97f3\u4e50\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2509.04768", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04768", "abs": "https://arxiv.org/abs/2509.04768", "authors": ["Yilong Chen", "Zixiang Ren", "Jie Xu", "Rui Zhang"], "title": "Environment-Aware IRS Deployment via Channel Knowledge Map: Joint Sensing-Communications Coverage Optimization", "comment": "13 pages, 11 figures", "summary": "This paper studies the intelligent reflecting surface (IRS) deployment\noptimization problem for IRS-enabled integrated sensing and communications\n(ISAC) systems, in which multiple IRSs are strategically deployed at candidate\nlocations to assist a base station (BS) to enhance the coverage of both sensing\nand communications. We present an environment-aware IRS deployment design via\nexploiting the channel knowledge map (CKM), which provides the channel state\ninformation (CSI) between each candidate IRS location and BS or targeted\nsensing/communication points. Based on the obtained CSI from CKM, we optimize\nthe deployment of IRSs, jointly with the BS's transmit beamforming and IRSs'\nreflective beamforming during operation, with the objective of minimizing the\nsystem cost, while guaranteeing the minimum illumination power requirements at\nsensing areas and the minimum signal-to-noise ratio (SNR) requirements at\ncommunication areas. In particular, we consider two cases when the IRSs'\nreflective beamforming optimization can be implemented dynamically in real time\nand quasi-stationarily over the whole operation period, respectively. For both\ncases, the joint IRS deployment and transmit/reflective beamforming designs are\nformulated as mixed-integer non-convex optimization problems, which are solved\nvia the successive convex approximation (SCA)-based relax-and-bound method.\nSpecifically, we first relax the binary IRS deployment indicators into\ncontinuous variables, then find converged solutions via SCA, and finally round\nrelaxed indicators back to binary values. Numerical results demonstrate the\neffectiveness of our proposed algorithms in reducing the system cost while\nmeeting the sensing and communication requirements.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u57fa\u4e8e\u9891\u9053\u77e5\u8bc6\u5730\u56fe(CKM)\u7684\u667a\u80fd\u53cd\u5c04\u8868\u9762(IRS)\u90e8\u7f72\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u5173\u8054\u4f18\u5316IRS\u90e8\u7f72\u3001\u57fa\u7ad9\u53d1\u5c04\u689d\u5f62\u6210\u548cIRS\u53cd\u5c04\u689d\u5f62\u6210\uff0c\u5728\u6ee1\u8db3\u611f\u77e5\u548c\u901a\u4fe1\u8981\u6c42\u7684\u524d\u63d0\u4e0b\u6700\u5c0f\u5316\u7cfb\u7edf\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u5728IRS\u542f\u7528\u7684\u611f\u901a\u4e00\u5316(ISAC)\u7cfb\u7edf\u4e2d\u63d0\u9ad8\u611f\u77e5\u548c\u901a\u4fe1\u7684\u8986\u76d6\u80fd\u529b\uff0c\u9700\u8981\u5728\u5019\u9009\u4f4d\u7f6e\u6210\u7565\u90e8\u7f72\u591a\u4e2aIRS\u3002CKM\u63d0\u4f9b\u4e86\u9891\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u4e3a\u73af\u5883\u611f\u77e5\u7684IRS\u90e8\u7f72\u8bbe\u8ba1\u57fa\u7840\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u975e\u51f8\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\uff0c\u901a\u8fc7\u8fde\u7eed\u51f8\u8fd1\u4f3c(SCA)\u7684\u677e\u7f1d\u8fb9\u754c\u6cd5\u6c42\u89e3\u3002\u5148\u5c06\u4e8c\u8fdb\u5236\u90e8\u7f72\u6307\u6807\u677e\u7f1d\u4e3a\u8fde\u7eed\u53d8\u91cf\uff0c\u901a\u8fc7SCA\u627e\u5230\u6536\u655b\u89e3\uff0c\u6700\u540e\u5c06\u677e\u7f1d\u6307\u6807\u56db\u820d\u4e94\u5165\u4e3a\u4e8c\u8fdb\u5236\u503c\u3002\u8003\u8651\u4e86\u5b9e\u65f6\u52a8\u6001\u548c\u51e0\u4f55\u9759\u6b62\u4e24\u79cd\u53cd\u5c04\u689d\u5f62\u6210\u60c5\u51b5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u6ee1\u8db3\u611f\u77e5\u548c\u901a\u4fe1\u8981\u6c42\u7684\u540c\u65f6\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\u3002", "conclusion": "\u57fa\u4e8eCKM\u7684\u73af\u5883\u611f\u77e5IRS\u90e8\u7f72\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u4f18\u5316ISAC\u7cfb\u7edf\u6027\u80fd\uff0cSCA\u57fa\u4e8e\u7684\u677e\u7f1d\u8fb9\u754c\u6cd5\u4e3a\u8be5\u6df7\u5408\u6574\u6570\u975e\u51f8\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05205", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.05205", "abs": "https://arxiv.org/abs/2509.05205", "authors": ["Jiajian Chen", "Jiakang Chen", "Hang Chen", "Qing Wang", "Yu Gao", "Jun Du"], "title": "MEAN-RIR: Multi-Modal Environment-Aware Network for Robust Room Impulse Response Estimation", "comment": "Accepted by ASRU 2025", "summary": "This paper presents a Multi-Modal Environment-Aware Network (MEAN-RIR), which\nuses an encoder-decoder framework to predict room impulse response (RIR) based\non multi-level environmental information from audio, visual, and textual\nsources. Specifically, reverberant speech capturing room acoustic properties\nserves as the primary input, which is combined with panoramic images and text\ndescriptions as supplementary inputs. Each input is processed by its respective\nencoder, and the outputs are fed into cross-attention modules to enable\neffective interaction between different modalities. The MEAN-RIR decoder\ngenerates two distinct components: the first component captures the direct\nsound and early reflections, while the second produces masks that modulate\nlearnable filtered noise to synthesize the late reverberation. These two\ncomponents are mixed to reconstruct the final RIR. The results show that\nMEAN-RIR significantly improves RIR estimation, with notable gains in acoustic\nparameters.", "AI": {"tldr": "MEAN-RIR\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u73af\u5883\u611f\u77e5\u7f51\u7edc\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u6765\u9884\u6d4b\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86RIR\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u4fe1\u606f\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73af\u5883\u7684\u591a\u5c42\u6b21\u4fe1\u606f\u3002\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u6355\u6349\u623f\u95f4\u58f0\u5b66\u7279\u6027\uff0c\u9700\u8981\u6574\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u7b49\u591a\u79cd\u6a21\u6001\u7684\u73af\u5883\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff0c\u5206\u522b\u5904\u7406\u6df7\u54cd\u8bed\u97f3\uff08\u97f3\u9891\uff09\u3001\u5168\u666f\u56fe\u50cf\uff08\u89c6\u89c9\uff09\u548c\u6587\u672c\u63cf\u8ff0\u3002\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u591a\u6a21\u6001\u95f4\u7684\u6709\u6548\u4ea4\u4e92\u3002\u89e3\u7801\u5668\u751f\u6210\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u76f4\u63a5\u58f0\u548c\u65e9\u671f\u53cd\u5c04\u90e8\u5206\uff0c\u4ee5\u53ca\u7528\u4e8e\u8c03\u5236\u53ef\u5b66\u4e60\u6ee4\u6ce2\u566a\u58f0\u7684\u63a9\u7801\u6765\u5408\u6210\u665a\u671f\u6df7\u54cd\u3002", "result": "MEAN-RIR\u663e\u8457\u6539\u5584\u4e86\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u7684\u4f30\u8ba1\u6548\u679c\uff0c\u5728\u58f0\u5b66\u53c2\u6570\u65b9\u9762\u53d6\u5f97\u4e86\u660e\u663e\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u73af\u5883\u4fe1\u606f\u7684\u6574\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0cMEAN-RIR\u6846\u67b6\u4e3a\u58f0\u5b66\u73af\u5883\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.04851", "categories": ["cs.SD", "cs.ET", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04851", "abs": "https://arxiv.org/abs/2509.04851", "authors": ["Rajeshwar Tripathi", "Sahil Tomar", "Sandeep Kumar", "Monika Aggarwal"], "title": "Quantum Fourier Transform Based Denoising: Unitary Filtering for Enhanced Speech Clarity", "comment": "8 pages", "summary": "This paper introduces a quantum-inspired denoising framework that integrates\nthe Quantum Fourier Transform (QFT) into classical audio enhancement pipelines.\nUnlike conventional Fast Fourier Transform (FFT) based methods, QFT provides a\nunitary transformation with global phase coherence and energy preservation,\nenabling improved discrimination between speech and noise. The proposed\napproach replaces FFT in Wiener and spectral subtraction filters with a QFT\noperator, ensuring consistent hyperparameter settings for fair comparison.\nExperiments on clean speech, synthetic tones, and noisy mixtures across diverse\nsignal to noise ratio (SNR) conditions, demonstrate statistically significant\ngains in SNR, with up to 15 dB improvement and reduced artifact generation.\nResults confirm that QFT based denoising offers robustness under low SNR and\nnonstationary noise scenarios without additional computational overhead,\nhighlighting its potential as a scalable pathway toward quantum-enhanced speech\nprocessing.", "AI": {"tldr": "\u91c7\u7528\u91cf\u5b50\u53d7\u7075\u7684\u53bb\u566a\u6846\u67b6\uff0c\u7528\u91cf\u5b50\u7ea6\u5316\u53d8\u6362(QFT)\u66ff\u4ee3\u4f20\u7edfFFT\uff0c\u5728\u5404\u79cdSNR\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u8f83\u4f18\u7684\u53bb\u566a\u6548\u679c\u548c\u51cf\u5c11\u4eba\u5de5\u6548\u5e94", "motivation": "\u4f20\u7edf\u5feb\u901f\u5bcc\u91cc\u53d6\u53d8\u6362(FFT)\u57fa\u4e8e\u7684\u97f3\u9891\u589e\u5f3a\u65b9\u6cd5\u5728\u8bc6\u522b\u8bed\u97f3\u548c\u566a\u58f0\u65b9\u9762\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u53d8\u6362\u65b9\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd", "method": "\u5c06\u97f3\u9891\u589e\u5f3a\u7ba1\u9053\u4e2d\u7684FFT\u66ff\u6362\u4e3a\u91cf\u5b50\u5bcc\u91cc\u53d6\u53d8\u6362(QFT)\uff0c\u4fdd\u6301\u76f8\u540c\u7684\u8d85\u53c2\u6570\u8bbe\u7f6e\u4ee5\u4fbf\u516c\u5e73\u6bd4\u8f83", "result": "\u5728\u5404\u79cdSNR\u6761\u4ef6\u4e0b\u5b9e\u9a8c\u663e\u793a\uff0cQFT\u57fa\u4e8e\u53bb\u566a\u5728SNR\u65b9\u9762\u83b7\u5f97\u4e86\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u7684\u6536\u76ca\uff0c\u6700\u9ad8\u63d0\u9ad815dB\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4eba\u5de5\u6548\u5e94\u751f\u6210", "conclusion": "QFT\u57fa\u4e8e\u53bb\u566a\u5728\u4f4eSNR\u548c\u975e\u7a33\u6001\u566a\u58f0\u573a\u666f\u4e0b\u5177\u6709\u7a33\u5065\u6027\uff0c\u800c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u91cf\u5b50\u589e\u5f3a\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84"}}
{"id": "2509.04787", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04787", "abs": "https://arxiv.org/abs/2509.04787", "authors": ["Zhidi Zhang", "Rui Meng", "Song Gao", "Haixiao Gao", "Xiaodong Xu"], "title": "SREC: Encrypted Semantic Super-Resolution Enhanced Communication", "comment": "7 pages, 6 figures, conference", "summary": "Semantic communication (SemCom), as a typical paradigm of deep integration\nbetween artificial intelligence (AI) and communication technology,\nsignificantly improves communication efficiency and resource utilization\nefficiency. However, the security issues of SemCom are becoming increasingly\nprominent. Semantic features transmitted in plaintext over physical channels\nare easily intercepted by eavesdroppers. To address this issue, this paper\nproposes Encrypted Semantic Super-Resolution Enhanced Communication (SREC) to\nsecure SemCom. SREC uses the modulo-256 encryption method to encrypt semantic\nfeatures, and employs super-resolution reconstruction method to improve the\nreconstruction quality of images. The simulation results show that in the\nadditive Gaussian white noise (AWGN) channel, when different modulation methods\nare used, SREC can not only stably guarantee security, but also achieve better\ntransmission performance under low signal-to-noise ratio (SNR) conditions.", "AI": {"tldr": "\u63d0\u51fa\u52a0\u5bc6\u8bed\u4e49\u8d85\u5206\u8fa8\u7387\u589e\u5f3a\u901a\u4fe1(SREC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21256\u52a0\u5bc6\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6280\u672f\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u56fe\u50cf\u4f20\u8f93\u3002", "motivation": "\u8bed\u4e49\u901a\u4fe1(SemCom)\u4f5c\u4e3aAI\u4e0e\u901a\u4fe1\u6280\u672f\u6df1\u5ea6\u878d\u5408\u7684\u5178\u578b\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u5176\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u660e\u6587\u4f20\u8f93\u7684\u8bed\u4e49\u7279\u5f81\u5bb9\u6613\u88ab\u7a83\u542c\u8005\u62e6\u622a\uff0c\u9700\u8981\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21256\u52a0\u5bc6\u65b9\u6cd5\u5bf9\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u52a0\u5bc6\uff0c\u5e76\u5229\u7528\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5\u6765\u63d0\u9ad8\u56fe\u50cf\u7684\u91cd\u5efa\u8d28\u91cf\u3002\u5728\u52a0\u6027\u9ad8\u65af\u767d\u566a\u58f0(AWGN)\u4fe1\u9053\u4e2d\u6d4b\u8bd5\u4e0d\u540c\u8c03\u5236\u65b9\u6cd5\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4(SNR)\u6761\u4ef6\u4e0b\uff0cSREC\u4e0d\u4ec5\u80fd\u7a33\u5b9a\u4fdd\u8bc1\u5b89\u5168\u6027\uff0c\u8fd8\u80fd\u5b9e\u73b0\u66f4\u597d\u7684\u4f20\u8f93\u6027\u80fd\u3002", "conclusion": "SREC\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u8bed\u4e49\u901a\u4fe1\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.04899", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04899", "abs": "https://arxiv.org/abs/2509.04899", "authors": ["Mutsumi Kobayashi", "Hiroshi Watanabe"], "title": "Learning and composing of classical music using restricted Boltzmann machines", "comment": "19 pages, 10 figures", "summary": "Recently, software has been developed that uses machine learning to mimic the\nstyle of a particular composer, such as J. S. Bach. However, since such\nsoftware often adopts machine learning models with complex structures, it is\ndifficult to analyze how the software understands the characteristics of the\ncomposer's music. In this study, we adopted J. S. Bach's music for training of\na restricted Boltzmann machine (RBM). Since the structure of RBMs is simple, it\nallows us to investigate the internal states after learning. We found that the\nlearned RBM is able to compose music.", "AI": {"tldr": "\u4f7f\u7528\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a(RBM)\u5206\u6790\u5df4\u8d6b\u97f3\u4e50\u98ce\u683c\u7279\u5f81\uff0c\u76f8\u6bd4\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u66f4\u6613\u89e3\u91ca\u5185\u90e8\u72b6\u6001", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4f5c\u66f2\u8f6f\u4ef6\u6a21\u578b\u7ed3\u6784\u590d\u6742\uff0c\u96be\u4ee5\u5206\u6790\u5176\u5982\u4f55\u7406\u89e3\u4f5c\u66f2\u5bb6\u97f3\u4e50\u7279\u5f81\uff0c\u9700\u8981\u66f4\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u6a21\u578b", "method": "\u91c7\u7528\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a(RBM)\u8bad\u7ec3J.S.\u5df4\u8d6b\u7684\u97f3\u4e50\u6570\u636e\uff0c\u5229\u7528RBM\u7b80\u5355\u7ed3\u6784\u4fbf\u4e8e\u5206\u6790\u5b66\u4e60\u540e\u7684\u5185\u90e8\u72b6\u6001", "result": "\u5b66\u4e60\u540e\u7684RBM\u80fd\u591f\u6210\u529f\u521b\u4f5c\u97f3\u4e50\u4f5c\u54c1", "conclusion": "RBM\u4f5c\u4e3a\u4e00\u79cd\u7ed3\u6784\u7b80\u5355\u7684\u6a21\u578b\uff0c\u65e2\u80fd\u6a21\u4eff\u5df4\u8d6b\u97f3\u4e50\u98ce\u683c\u8fdb\u884c\u521b\u4f5c\uff0c\u53c8\u4fbf\u4e8e\u5206\u6790\u5176\u5185\u90e8\u5b66\u4e60\u673a\u5236"}}
{"id": "2509.04801", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04801", "abs": "https://arxiv.org/abs/2509.04801", "authors": ["Dayu Fan", "Rui Meng", "Song Gao", "Xiaodong Xu"], "title": "KGRAG-SC: Knowledge Graph RAG-Assisted Semantic Communication", "comment": "7 pages,4 figures,conference", "summary": "The state-of-the-art semantic communication (SC) schemes typically rely on\nend-to-end deep learning frameworks that lack interpretability and struggle\nwith robust semantic selection and reconstruction under noisy conditions. To\naddress this issue, this paper presents KGRAG-SC, a knowledge graph-assisted SC\nframework that leverages retrieval-augmented generation principles. KGRAG-SC\nemploys a multi-dimensional knowledge graph, enabling efficient semantic\nextraction through community-guided entity linking and GraphRAG-assisted\nprocessing. The transmitter constructs minimal connected subgraphs that capture\nessential semantic relationships and transmits only compact entity indices\nrather than full text or semantic triples. An importance-aware adaptive\ntransmission strategy provides unequal error protection based on structural\ncentrality metrics, prioritizing critical semantic elements under adverse\nchannel conditions. At the receiver, large language models perform\nknowledge-driven text reconstruction using the shared knowledge graph as\nstructured context, ensuring robust semantic recovery even with partial\ninformation loss. Experimental results demonstrate that KGRAG-SC achieves\nsuperior semantic fidelity in low Signal-to-Noise Ratio (SNR) conditions while\nsignificantly reducing transmission overhead compared to traditional\ncommunication methods, highlighting the effectiveness of integrating structured\nknowledge representation with generative language models for SC systems.", "AI": {"tldr": "KGRAG-SC\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u89e3\u51b3\u4f20\u7edf\u8bed\u4e49\u901a\u4fe1\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u7684\u95ee\u9898\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u66f4\u4f4e\u7684\u4f20\u8f93\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u65b9\u6848\u4e3b\u8981\u4f9d\u8d56\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u8bed\u4e49\u9009\u62e9\u548c\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u8f85\u52a9\u7684\u591a\u7ef4\u6846\u67b6\uff0c\u5305\u62ec\u793e\u533a\u5f15\u5bfc\u7684\u5b9e\u4f53\u94fe\u63a5\u3001GraphRAG\u5904\u7406\u3001\u6700\u5c0f\u8fde\u901a\u5b50\u56fe\u6784\u5efa\u3001\u7d27\u51d1\u5b9e\u4f53\u7d22\u5f15\u4f20\u8f93\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7ed3\u6784\u4e2d\u5fc3\u5ea6\u7684\u91cd\u8981\u6027\u611f\u77e5\u81ea\u9002\u5e94\u4f20\u8f93\u7b56\u7565\u3002\u63a5\u6536\u7aef\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u9a71\u52a8\u7684\u6587\u672c\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKGRAG-SC\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u901a\u4fe1\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u5f00\u9500\u3002", "conclusion": "\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u4e0e\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.04980", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04980", "abs": "https://arxiv.org/abs/2509.04980", "authors": ["Yuxuan Liu", "Peihong Zhang", "Rui Sang", "Zhixin Li", "Shengchen Li"], "title": "MAIA: An Inpainting-Based Approach for Music Adversarial Attacks", "comment": "Accepted at ISMIR2025", "summary": "Music adversarial attacks have garnered significant interest in the field of\nMusic Information Retrieval (MIR). In this paper, we present Music Adversarial\nInpainting Attack (MAIA), a novel adversarial attack framework that supports\nboth white-box and black-box attack scenarios. MAIA begins with an importance\nanalysis to identify critical audio segments, which are then targeted for\nmodification. Utilizing generative inpainting models, these segments are\nreconstructed with guidance from the output of the attacked model, ensuring\nsubtle and effective adversarial perturbations. We evaluate MAIA on multiple\nMIR tasks, demonstrating high attack success rates in both white-box and\nblack-box settings while maintaining minimal perceptual distortion.\nAdditionally, subjective listening tests confirm the high audio fidelity of the\nadversarial samples. Our findings highlight vulnerabilities in current MIR\nsystems and emphasize the need for more robust and secure models.", "AI": {"tldr": "MAIA\u662f\u4e00\u4e2a\u65b0\u7684\u97f3\u4e50\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u652f\u6301\u767d\u76d2\u548c\u9ed1\u76d2\u653b\u51fb\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u5206\u6790\u548c\u751f\u6210\u5f0f\u4fee\u590d\u6a21\u578b\u521b\u5efa\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u6837\u672c\uff0c\u5728\u591a\u4e2aMIR\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u653b\u51fb\u6210\u529f\u7387\u548c\u826f\u597d\u97f3\u9891\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u7814\u7a76\u5bf9\u6297\u653b\u51fb\u6765\u63ed\u793a\u8fd9\u4e9b\u8106\u5f31\u6027\uff0c\u4fc3\u8fdb\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u7684\u6a21\u578b\u53d1\u5c55\u3002", "method": "\u9996\u5148\u8fdb\u884c\u91cd\u8981\u6027\u5206\u6790\u8bc6\u522b\u5173\u952e\u97f3\u9891\u6bb5\uff0c\u7136\u540e\u4f7f\u7528\u751f\u6210\u5f0f\u4fee\u590d\u6a21\u578b\u5728\u6a21\u578b\u8f93\u51fa\u6307\u5bfc\u4e0b\u91cd\u6784\u8fd9\u4e9b\u6bb5\uff0c\u751f\u6210\u7ec6\u5fae\u6709\u6548\u7684\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5728\u591a\u4e2aMIR\u4efb\u52a1\u4e2d\uff0cMAIA\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u90fd\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5c0f\u611f\u77e5\u5931\u771f\uff0c\u4e3b\u89c2\u542c\u529b\u6d4b\u8bd5\u786e\u8ba4\u4e86\u9ad8\u97f3\u9891\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dMIR\u7cfb\u7edf\u7684\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0cMAIA\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbMIR\u7cfb\u7edf\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.04803", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04803", "abs": "https://arxiv.org/abs/2509.04803", "authors": ["Song Gao", "Rui Meng", "Xiaodong Xu", "Haixiao Gao", "Yiming Liu", "Chenyuan Feng", "Ping Zhang", "Tony Q. S. Quek", "Dusit Niyato"], "title": "SemSteDiff: Generative Diffusion Model-based Coverless Semantic Steganography Communication", "comment": "13 pages, 11 figures", "summary": "Semantic communication (SemCom), as a novel paradigm for future communication\nsystems, has recently attracted much attention due to its superiority in\ncommunication efficiency. However, similar to traditional communication, it\nalso suffers from eavesdropping threats. Intelligent eavesdroppers could launch\nadvanced semantic analysis techniques to infer secret semantic information.\nTherefore, some researchers have designed Semantic Steganography Communication\n(SemSteCom) scheme to confuse semantic eavesdroppers. However, the\nstate-of-the-art SemSteCom schemes for image transmission rely on the\npre-selected cover image, which limits the universality. To address this issue,\nwe propose a Generative Diffusion Model-based Coverless Semantic Steganography\nCommunication (SemSteDiff) scheme to hide secret images into generated stego\nimages. The semantic related private and public keys enable legitimate receiver\nto decode secret images correctly while the eavesdropper without completely\ntrue key-pairs fail to obtain them. Simulation results demonstrate the\neffectiveness of the plug-and-play design in different Joint Source-Channel\nCoding (JSCC) frameworks. The comparison results under different eavesdroppers'\nthreats show that, when Signal-to-Noise Ratio (SNR) = 0 dB, the peak\nsignal-to-noise ratio (PSNR) of the legitimate receiver is 4.14 dB higher than\nthat of the eavesdropper.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u6db5\u6563\u6a21\u578b\u7684\u65e0\u5e03\u5c4f\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u65b9\u6848SemSteDiff\uff0c\u901a\u8fc7\u751f\u6210\u9690\u5bc6\u56fe\u50cf\u6765\u62b5\u5fa1\u8bed\u4e49\u622a\u6536\u653b\u51fb\uff0c\u4f7f\u5408\u6cd5\u63a5\u6536\u65b9\u80fd\u6b63\u786e\u89e3\u7801\u800c\u622a\u6536\u8005\u5931\u8d25\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u65b9\u6848\u4f9d\u8d56\u9884\u9009\u62e9\u7684\u5e03\u5c4f\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\u6765\u63d0\u9ad8\u62b5\u5fa1\u8bed\u4e49\u622a\u6536\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5f0f\u6db5\u6563\u6a21\u578b\u5c06\u79d8\u5bc6\u56fe\u50cf\u9690\u85cf\u5230\u751f\u6210\u7684\u9690\u5bc6\u56fe\u50cf\u4e2d\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u5173\u7684\u79c1\u94a5\u548c\u516c\u94a5\u5b9e\u73b0\u5408\u6cd5\u63a5\u6536\u65b9\u7684\u6b63\u786e\u89e3\u7801\u3002", "result": "\u5728SNR=0dB\u65f6\uff0c\u5408\u6cd5\u63a5\u6536\u65b9\u7684PSNR\u6bd4\u622a\u6536\u8005\u9ad84.14dB\uff0c\u8bc1\u660e\u4e86\u65b9\u6848\u5728\u4e0d\u540cJSCC\u6846\u67b6\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "SemSteDiff\u65b9\u6848\u901a\u8fc7\u751f\u6210\u5f0f\u65b9\u6cd5\u89e3\u51b3\u4e86\u9884\u9009\u5e03\u5c4f\u56fe\u50cf\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u8bed\u4e49\u9690\u5199\u901a\u4fe1\u7684\u901a\u7528\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2509.04985", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04985", "abs": "https://arxiv.org/abs/2509.04985", "authors": ["Yuxuan Liu", "Rui Sang", "Peihong Zhang", "Zhixin Li", "Shengchen Li"], "title": "Training a Perceptual Model for Evaluating Auditory Similarity in Music Adversarial Attack", "comment": null, "summary": "Music Information Retrieval (MIR) systems are highly vulnerable to\nadversarial attacks that are often imperceptible to humans, primarily due to a\nmisalignment between model feature spaces and human auditory perception.\nExisting defenses and perceptual metrics frequently fail to adequately capture\nthese auditory nuances, a limitation supported by our initial listening tests\nshowing low correlation between common metrics and human judgments. To bridge\nthis gap, we introduce Perceptually-Aligned MERT Transformer (PAMT), a novel\nframework for learning robust, perceptually-aligned music representations. Our\ncore innovation lies in the psychoacoustically-conditioned sequential\ncontrastive transformer, a lightweight projection head built atop a frozen MERT\nencoder. PAMT achieves a Spearman correlation coefficient of 0.65 with\nsubjective scores, outperforming existing perceptual metrics. Our approach also\nachieves an average of 9.15\\% improvement in robust accuracy on challenging MIR\ntasks, including Cover Song Identification and Music Genre Classification,\nunder diverse perceptual adversarial attacks. This work pioneers\narchitecturally-integrated psychoacoustic conditioning, yielding\nrepresentations significantly more aligned with human perception and robust\nagainst music adversarial attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86PAMT\u6846\u67b6\uff0c\u901a\u8fc7\u5fc3\u7406\u58f0\u5b66\u6761\u4ef6\u5316\u7684\u5bf9\u6bd4\u53d8\u6362\u5668\u5b66\u4e60\u4e0e\u4eba\u7c7b\u542c\u89c9\u611f\u77e5\u5bf9\u9f50\u7684\u97f3\u4e50\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5bf9\u6297\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u4eba\u7c7b\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u7279\u5f81\u7a7a\u95f4\u4e0e\u4eba\u7c7b\u542c\u89c9\u611f\u77e5\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u548c\u611f\u77e5\u6307\u6807\u65e0\u6cd5\u5145\u5206\u6355\u6349\u542c\u89c9\u7ec6\u8282\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u4f4e\u3002", "method": "\u57fa\u4e8e\u51bb\u7ed3\u7684MERT\u7f16\u7801\u5668\u6784\u5efa\u5fc3\u7406\u58f0\u5b66\u6761\u4ef6\u5316\u7684\u5e8f\u5217\u5bf9\u6bd4\u53d8\u6362\u5668\u6295\u5f71\u5934\uff0c\u5b9e\u73b0\u67b6\u6784\u96c6\u6210\u7684\u5fc3\u7406\u58f0\u5b66\u6761\u4ef6\u5316\uff0c\u5b66\u4e60\u611f\u77e5\u5bf9\u9f50\u7684\u97f3\u4e50\u8868\u793a\u3002", "result": "Spearman\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.65\uff0c\u4f18\u4e8e\u73b0\u6709\u611f\u77e5\u6307\u6807\uff1b\u5728Cover Song Identification\u548cMusic Genre Classification\u7b49\u4efb\u52a1\u4e0a\uff0c\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad89.15%\u3002", "conclusion": "PAMT\u6846\u67b6\u901a\u8fc7\u5fc3\u7406\u58f0\u5b66\u6761\u4ef6\u5316\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u611f\u77e5\u66f4\u597d\u5bf9\u9f50\u7684\u97f3\u4e50\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u97f3\u4e50\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u611f\u77e5\u5bf9\u9f50\u7684\u97f3\u4e50\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.04805", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04805", "abs": "https://arxiv.org/abs/2509.04805", "authors": ["Keqin Zhang"], "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design", "comment": null, "summary": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86AI\u9a71\u52a8\u7684\u65e0\u7ebf\u7cfb\u7edffronthaul\u538b\u7f29\u6280\u672f\uff0c\u91cd\u70b9\u5206\u6790\u4e86CSI\u53cd\u9988\u538b\u7f29\u548c\u9884\u7f16\u7801\u4f18\u5316\u7b49\u9ad8\u538b\u7f29\u6bd4\u65b9\u6848\uff0c\u5e76\u4e3a\u7ec4\u7ec7\u5143\u67b6\u6784\u63d0\u51fa\u4e86\u5177\u6709\u9ad8\u538b\u7f29\u6bd4\u3001\u53ef\u63a7\u6027\u80fd\u635f\u5931\u3001RB\u7ea7\u522b\u901f\u7387\u9002\u914d\u548c\u4f4e\u5ef6\u8fdf\u7279\u6027\u7684\u538b\u7f29\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u7cfb\u7edffronthaul\u94fe\u8def\u9700\u8981\u5728\u4e25\u683c\u7684\u5e26\u5bbd\u548c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4f20\u8f93\u9ad8\u7ef4\u4fe1\u53f7\uff0c\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u9650\u5236\u6027\u5148\u9a8c\u77e5\u8bc6\u3001\u9ad8\u538b\u7f29\u6bd4\u4e0b\u6027\u80fd\u6076\u5316\u4ee5\u53ca\u8c03\u9002\u56f0\u96be\u7b49\u95ee\u9898\u3002", "method": "\u91cd\u70b9\u5206\u6790\u4e86\u4e24\u79cd\u4ee3\u8868\u6027\u9ad8\u538b\u7f29\u8def\u5f84\uff1a1)\u57fa\u4e8e\u7ec8\u7aef\u7ec4\u5b66\u4e60\u7684CSI\u53cd\u9988\u538b\u7f29\uff1b2)\u8d44\u6e90\u5757(RB)\u7c92\u5ea6\u9884\u7f16\u7801\u4f18\u5316\u4e0e\u538b\u7f29\u7684\u7ed3\u5408\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u4e3a\u7ec4\u7ec7\u5143\u67b6\u6784\u8bbe\u8ba1\u7684fronthaul\u538b\u7f29\u7b56\u7565\u3002", "result": "\u8bbe\u8ba1\u76ee\u6807\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u4e0b\u7684\u53ef\u63a7\u6027\u80fd\u635f\u5931\uff0c\u652f\u6301RB\u7ea7\u522b\u7684\u901f\u7387\u9002\u914d\u80fd\u529b\uff0c\u5e76\u652f\u6301\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u592e\u5316\u534f\u540c\u4f20\u8f93\u6240\u9700\u7684\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u538b\u7f29\u6280\u672f\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u4fe1\u9053\u72b6\u6001\u4fe1\u606f(CSI)\u3001\u9884\u7f16\u7801\u77e9\u9635\u3001I/Q\u6837\u672c\u548cLLR\u7b49\u4fe1\u53f7\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u4e3a\u73b0\u4ee3\u65e0\u7ebf\u7cfb\u7edffronthaul\u94fe\u8def\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u9ad8\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u7ec4\u7ec7\u5143\u67b6\u6784\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.05256", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05256", "abs": "https://arxiv.org/abs/2509.05256", "authors": ["Daniel P. W. Ellis", "Eduardo Fonseca", "Ron J. Weiss", "Kevin Wilson", "Scott Wisdom", "Hakan Erdogan", "John R. Hershey", "Aren Jansen", "R. Channing Moore", "Manoj Plakal"], "title": "Recomposer: Event-roll-guided generative audio editing", "comment": "5 pages, 5 figures", "summary": "Editing complex real-world sound scenes is difficult because individual sound\nsources overlap in time. Generative models can fill-in missing or corrupted\ndetails based on their strong prior understanding of the data domain. We\npresent a system for editing individual sound events within complex scenes able\nto delete, insert, and enhance individual sound events based on textual edit\ndescriptions (e.g., ``enhance Door'') and a graphical representation of the\nevent timing derived from an ``event roll'' transcription. We present an\nencoder-decoder transformer working on SoundStream representations, trained on\nsynthetic (input, desired output) audio example pairs formed by adding isolated\nsound events to dense, real-world backgrounds. Evaluation reveals the\nimportance of each part of the edit descriptions -- action, class, timing. Our\nwork demonstrates ``recomposition'' is an important and practical application.", "AI": {"tldr": "\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u548c\u4e8b\u4ef6\u65f6\u95f4\u56fe\u7f16\u8f91\u590d\u6742\u97f3\u9891\u573a\u666f\u4e2d\u7684\u5355\u4e2a\u97f3\u54cd\u4e8b\u4ef6\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u5220\u9664\u3001\u63d2\u5165\u548c\u589e\u5f3a\u529f\u80fd", "motivation": "\u590d\u6742\u5b9e\u9645\u97f3\u9891\u573a\u666f\u4e2d\u591a\u4e2a\u97f3\u6e90\u65f6\u95f4\u91cd\u53e0\uff0c\u7f16\u8f91\u5355\u4e2a\u97f3\u54cd\u4e8b\u4ef6\u56f0\u96be\uff0c\u9700\u8981\u751f\u6210\u6a21\u578b\u6765\u586b\u5145\u7f3a\u5931\u6216\u635f\u574f\u7684\u7ec6\u8282", "method": "\u4f7f\u7528\u57fa\u4e8eSoundStream\u8868\u5f81\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668Transformer\u6a21\u578b\uff0c\u5728\u5408\u6210\u7684\uff08\u8f93\u5165\uff0c\u671f\u671b\u8f93\u51fa\uff09\u97f3\u9891\u5bf9\u4e0a\u8bad\u7ec3\uff0c\u8fd9\u4e9b\u5bf9\u662f\u901a\u8fc7\u5411\u5bc6\u96c6\u5b9e\u9645\u80cc\u666f\u6dfb\u52a0\u5b64\u7acb\u97f3\u54cd\u4e8b\u4ef6\u5f62\u6210\u7684", "result": "\u8bc4\u4f30\u663e\u793a\u7f16\u8f91\u63cf\u8ff0\u7684\u5404\u4e2a\u90e8\u5206\uff08\u52a8\u4f5c\u3001\u7c7b\u522b\u3001\u65f6\u95f4\uff09\u90fd\u5f88\u91cd\u8981\uff0c\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u97f3\u9891\u573a\u666f\u7684\u7f16\u8f91", "conclusion": "\u91cd\u65b0\u7ec4\u5408\uff08recomposition\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u4e14\u5b9e\u7528\u7684\u5e94\u7528\u9886\u57df\uff0c\u8be5\u7cfb\u7edf\u4e3a\u97f3\u9891\u573a\u666f\u7cbe\u7ec6\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u65b9\u6848"}}
{"id": "2509.04860", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04860", "abs": "https://arxiv.org/abs/2509.04860", "authors": ["Rui Guo", "Yi Zhang", "Yhonatan Kvich", "Tianyao Huang", "Maokun Li", "Yonina C. Eldar"], "title": "Plug-and-Play Latent Diffusion for Electromagnetic Inverse Scattering with Application to Brain Imaging", "comment": null, "summary": "Electromagnetic (EM) imaging is an important tool for non-invasive sensing\nwith low-cost and portable devices. One emerging application is EM stroke\nimaging, which enables early diagnosis and continuous monitoring of brain\nstrokes. Quantitative imaging is achieved by solving an inverse scattering\nproblem (ISP) that reconstructs permittivity and conductivity maps from\nmeasurements. In general, the reconstruction accuracy is limited by its\ninherent nonlinearity and ill-posedness. Existing methods, including\nlearning-free and learning-based approaches, fail to either incorporate\ncomplicated prior distributions or provide theoretical guarantees, posing\ndifficulties in balancing interpretability, distortion error, and reliability.\nTo overcome these limitations, we propose a posterior sampling method based on\nlatent diffusion for quantitative EM brain imaging, adapted from a generative\nplug-and-play (PnP) posterior sampling framework. Our approach allows to\nflexibly integrate prior knowledge into physics-based inversion without\nrequiring paired measurement-label datasets. We first learn the prior\ndistribution of targets from an unlabeled dataset, and then incorporate the\nlearned prior into posterior sampling. In particular, we train a latent\ndiffusion model on permittivity and conductivity maps to capture their prior\ndistribution. Then, given measurements and the forward model describing EM wave\nphysics, we perform posterior sampling by alternating between two samplers that\nrespectively enforce the likelihood and prior distributions. Finally, reliable\nreconstruction is obtained through minimum mean squared error (MMSE) estimation\nbased on the samples. Experimental results on brain imaging demonstrate that\nour approach achieves state-of-the-art performance in reconstruction accuracy\nand structural similarity while maintaining high measurement fidelity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u7528\u4e8e\u7535\u78c1\u8111\u6210\u50cf\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u524d\u5411\u6a21\u578b\u548c\u5b66\u4e60\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u91cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7535\u78c1\u6210\u50cf\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u89e3\u91ca\u6027\u3001\u5931\u771f\u8bef\u5dee\u548c\u53ef\u9760\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u590d\u6742\u5148\u9a8c\u5206\u5e03\u6216\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u9650\u5236\u4e86\u8111\u5352\u4e2d\u6210\u50cf\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ecb\u7535\u5e38\u6570\u548c\u7535\u5bfc\u7387\u56fe\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u4ea4\u66ff\u91c7\u6837\u5668\u6267\u884c\u540e\u9a8c\u91c7\u6837\uff0c\u5206\u522b\u5f3a\u5236\u6267\u884c\u4f3c\u7136\u548c\u5148\u9a8c\u5206\u5e03\uff0c\u6700\u540e\u57fa\u4e8e\u6837\u672c\u8fdb\u884c\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1\u3002", "result": "\u5728\u8111\u6210\u50cf\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6d4b\u91cf\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u6574\u5408\u5148\u9a8c\u77e5\u8bc6\u5230\u57fa\u4e8e\u7269\u7406\u7684\u53cd\u6f14\u4e2d\uff0c\u65e0\u9700\u914d\u5bf9\u6d4b\u91cf-\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u4e3a\u7535\u78c1\u8111\u6210\u50cf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u540e\u9a8c\u91c7\u6837\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04865", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.04865", "abs": "https://arxiv.org/abs/2509.04865", "authors": ["Yunpu Zhang", "Changsheng You", "Hing Cheung So", "Dusit Niyato"], "title": "Rotatable Antenna Aided Mixed Near-Field and Far-Field Communications in the Upper Mid-Band: Interference Analysis and Joint Optimization", "comment": "13 pages, 12 figures", "summary": "In this paper, we propose to leverage rotatable antennas (RAs) for improving\nthe communication performance in mixed near-field and far-field communication\nsystems by exploiting a new spatial degree-of-freedom (DoF) offered by antenna\nrotation to mitigate complex near-field interference and mixed-field\ninterference. Specifically, we investigate a modular RA-enabled mixed-field\ndownlink communication system, where a base station (BS) consisting of multiple\nRA subarrays communicates with multiple near-field users in the presence of\nseveral legacy far-field users. We formulate an optimization problem to\nmaximize the sum-rate of the near-field users by jointly optimizing the power\nallocation and rotation angles of all subarrays at the BS. To gain useful\ninsights into the effect of RAs on mixed-field communications, we first analyze\na special case where all subarrays share the same rotation angle and obtain\nclosed-form expressions for the rotation-aware normalized near-field\ninterference and the rotation-aware normalized mixed-field interference using\nthe Fresnel integrals. We then analytically reveal that array rotation\neffectively suppresses both interference types, thereby significantly enhancing\nmixed-field communication performance. For the general case involving\nsubarray-wise rotation, we propose an efficient double-layer algorithm to\nobtain a high-quality solution, where the inner layer optimizes power\nallocation using the successive convex approximation (SCA) technique, while the\nouter layer determines the rotation angles of all subarrays via particle swarm\noptimization (PSO). Finally, numerical results highlight the significant\nperformance gains achieved by RAs over conventional fixed-antenna systems and\ndemonstrate the effectiveness of our developed joint design compared to\nbenchmark schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u53ef\u65cb\u8f6c\u5929\u7ebf(RAs)\u901a\u8fc7\u5929\u7ebf\u65cb\u8f6c\u63d0\u4f9b\u7684\u65b0\u7a7a\u95f4\u81ea\u7531\u5ea6\u6765\u6291\u5236\u590d\u6742\u7684\u8fd1\u573a\u5e72\u6270\u548c\u6df7\u5408\u573a\u5e72\u6270\uff0c\u4ece\u800c\u63d0\u5347\u6df7\u5408\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\u5728\u6df7\u5408\u8fd1\u573a\u548c\u8fdc\u573a\u901a\u4fe1\u573a\u666f\u4e2d\u9762\u4e34\u590d\u6742\u7684\u8fd1\u573a\u5e72\u6270\u548c\u6df7\u5408\u573a\u5e72\u6270\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u624b\u6bb5\u6765\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316RA\u4f7f\u80fd\u7684\u6df7\u5408\u573a\u4e0b\u884c\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u5b50\u9635\u5217\u65cb\u8f6c\u89d2\u5ea6\u6765\u6700\u5927\u5316\u8fd1\u573a\u7528\u6237\u7684\u548c\u901f\u7387\u3002\u91c7\u7528\u53cc\u5c42\u7b97\u6cd5\uff1a\u5185\u5c42\u4f7f\u7528SCA\u6280\u672f\u4f18\u5316\u529f\u7387\u5206\u914d\uff0c\u5916\u5c42\u4f7f\u7528PSO\u786e\u5b9a\u65cb\u8f6c\u89d2\u5ea6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cRA\u7cfb\u7edf\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u589e\u76ca\uff0c\u6240\u63d0\u51fa\u7684\u8054\u5408\u8bbe\u8ba1\u65b9\u6848\u76f8\u6bd4\u57fa\u51c6\u65b9\u6848\u5177\u6709\u66f4\u597d\u7684\u6548\u679c\u3002", "conclusion": "\u5929\u7ebf\u65cb\u8f6c\u80fd\u6709\u6548\u6291\u5236\u8fd1\u573a\u5e72\u6270\u548c\u6df7\u5408\u573a\u5e72\u6270\uff0c\u663e\u8457\u63d0\u5347\u6df7\u5408\u573a\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2509.04873", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04873", "abs": "https://arxiv.org/abs/2509.04873", "authors": ["Yue Geng", "Tee Hiang Cheng", "Kai Zhong", "Kah Chan Teh", "Qingqing Wu"], "title": "Movable IRS-Aided ISAC Systems: Joint Beamforming and Position Optimization", "comment": "13 pages, 8 figures", "summary": "Driven by intelligent reflecting surface (IRS) and movable antenna (MA)\ntechnologies, movable IRS (MIRS) has been proposed to improve the adaptability\nand performance of conventional IRS, enabling flexible adjustment of the IRS\nreflecting element positions. This paper investigates MIRS-aided integrated\nsensing and communication (ISAC) systems. The objective is to minimize the\npower required for satisfying the quality-of-service (QoS) of sensing and\ncommunication by jointly optimizing the MIRS element positions, IRS reflection\ncoefficients, transmit beamforming, and receive filters. To balance the\nperformance-cost trade-off, we proposed two MIRS schemes: element-wise control\nand array-wise control, where the positions of individual reflecting elements\nand arrays consisting of multiple elements are controllable, respectively. To\naddress the joint beamforming and position optimization, a product Riemannian\nmanifold optimization (PRMO) method is proposed, where the variables are\nupdated over a constructed product Riemannian manifold space (PRMS) in parallel\nvia penalty-based transformation and Riemannian\nBroyden-Fletcher-Goldfarb-Shanno (RBFGS) algorithm. Simulation results\ndemonstrate that the proposed MIRS outperforms conventional IRS in power\nminimization with both element-wise control and array-wise control.\nSpecifically, with different system parameters, the minimum power is achieved\nby the MIRS with the element-wise control scheme, while suboptimal solution and\nhigher computational efficiency are achieved by the MIRS with array-wise\ncontrol scheme.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u79fb\u52a8\u667a\u80fd\u53cd\u5c04\u9762(MIRS)\u8f85\u52a9\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316MIRS\u5143\u7d20\u4f4d\u7f6e\u3001\u53cd\u5c04\u7cfb\u6570\u3001\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u548c\u63a5\u6536\u6ee4\u6ce2\u5668\uff0c\u6700\u5c0f\u5316\u6ee1\u8db3\u611f\u77e5\u548c\u901a\u4fe1\u670d\u52a1\u8d28\u91cf\u6240\u9700\u7684\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u53cd\u5c04\u9762(IRS)\u4f4d\u7f6e\u56fa\u5b9a\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u53ef\u79fb\u52a8\u667a\u80fd\u53cd\u5c04\u9762(MIRS)\u7ed3\u5408IRS\u548c\u53ef\u79fb\u52a8\u5929\u7ebf\u6280\u672f\uff0c\u80fd\u591f\u7075\u6d3b\u8c03\u6574\u53cd\u5c04\u5143\u7d20\u4f4d\u7f6e\uff0c\u63d0\u9ad8\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u79cdMIRS\u63a7\u5236\u65b9\u6848\uff1a\u5143\u7d20\u7ea7\u63a7\u5236\u548c\u9635\u5217\u7ea7\u63a7\u5236\uff1b\u5f00\u53d1\u57fa\u4e8e\u4e58\u79ef\u9ece\u66fc\u6d41\u5f62\u4f18\u5316(PRMO)\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u53d8\u6362\u548c\u9ece\u66fcBFGS\u7b97\u6cd5\u5728\u6784\u9020\u7684\u4e58\u79ef\u9ece\u66fc\u6d41\u5f62\u7a7a\u95f4\u5e76\u884c\u66f4\u65b0\u53d8\u91cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cMIRS\u5728\u529f\u7387\u6700\u5c0f\u5316\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfIRS\u3002\u5143\u7d20\u7ea7\u63a7\u5236\u65b9\u6848\u5b9e\u73b0\u6700\u5c0f\u529f\u7387\uff0c\u9635\u5217\u7ea7\u63a7\u5236\u65b9\u6848\u83b7\u5f97\u6b21\u4f18\u89e3\u4f46\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "MIRS\u6280\u672f\u80fd\u591f\u663e\u8457\u63d0\u5347ISAC\u7cfb\u7edf\u6027\u80fd\uff0c\u5143\u7d20\u7ea7\u63a7\u5236\u63d0\u4f9b\u6700\u4f18\u6027\u80fd\uff0c\u9635\u5217\u7ea7\u63a7\u5236\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2509.04930", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04930", "abs": "https://arxiv.org/abs/2509.04930", "authors": ["Philippe Flores", "Konstantin Usevich", "David Brie"], "title": "Coupled tensor models for probability mass function estimation: Part I, Principles and algorithms", "comment": null, "summary": "In this article, a Probability Mass Function (PMF) estimation method which\ntames the curse of dimensionality is proposed. This method, called Partial\nCoupled Tensor Factorization of 3D marginals or PCTF3D, has for principle to\npartially couple order-3 data projections -- seen as order-3 tensors -- to\nobtain a tensor decomposition of the probability mass tensor. The novelty of\nPCTF3D relies on partial coupling which consists in choosing a subset of 3D\nmarginals. The choice of marginals is then formulated with hypergraphs. After\npresenting possible coupling strategies, some numerical experiments and an\napplication of the method are proposed. This article is the first of a two-part\narticle. While this first article focuses on a new algorithmic framework for\nPMF estimation, the second studies uniqueness properties of the model\nintroduced in this article.", "AI": {"tldr": "\u63d0\u51faPCTF3D\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u8026\u54083D\u8fb9\u9645\u5f20\u91cf\u6765\u4f30\u8ba1\u9ad8\u7ef4\u6982\u7387\u8d28\u91cf\u51fd\u6570\uff0c\u907f\u514d\u7ef4\u5ea6\u707e\u96be\u95ee\u9898", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u6982\u7387\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u8ba1\u7b97\u590d\u6742\u5ea6\u6025\u5267\u589e\u52a0", "method": "\u4f7f\u7528\u90e8\u5206\u8026\u5408\u76843\u9636\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5(PCTF3D)\uff0c\u901a\u8fc7\u8d85\u56fe\u9009\u62e93D\u8fb9\u9645\u5b50\u96c6\u8fdb\u884c\u90e8\u5206\u8026\u5408\uff0c\u6784\u5efa\u6982\u7387\u8d28\u91cf\u5f20\u91cf\u5206\u89e3", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u7ef4PMF\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "PCTF3D\u4e3a\u9ad8\u7ef4\u6982\u7387\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u662f\u4e24\u7bc7\u7cfb\u5217\u6587\u7ae0\u7684\u7b2c\u4e00\u7bc7\uff0c\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u6846\u67b6"}}
{"id": "2509.04931", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04931", "abs": "https://arxiv.org/abs/2509.04931", "authors": ["Philippe Flores", "Konstantin Usevich", "David Brie"], "title": "Coupled tensor models for probability mass function estimation: Part II, Uniqueness of the model", "comment": null, "summary": "In this paper, uniqueness properties of a coupled tensor model are studied.\nThis new coupled tensor model is used in a new method called Partial Coupled\nTensor Factorization of 3D marginals or PCTF3D. This method performs estimation\nof probability mass functions by coupling 3D marginals, seen as order-3\ntensors. The core novelty of PCTF3D's approach (detailed in the part I article)\nrelies on the partial coupling which consists on the choice of 3D marginals to\nbe coupled. Tensor methods are ubiquitous in many applications of statistical\nlearning, with their biggest advantage of having strong uniqueness properties.\nIn this paper, the uniqueness properties of PCTF3D's constrained coupled\nlow-rank model is assessed. While probabilistic constraints of the coupled\nmodel are handled properly, it is shown that uniqueness highly depends on the\ncoupling used in PCTF3D. After proposing a Jacobian algorithm providing maximum\nrecoverable rank, different coupling strategies presented in the Part I article\nare examined with respect to their uniqueness properties. Finally, an\nidentifiability bound is given for a so-called Cartesian coupling which permits\nenhancing sufficient bounds of the literature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86PCTF3D\u65b9\u6cd5\u4e2d\u8026\u5408\u5f20\u91cf\u6a21\u578b\u7684\u552f\u4e00\u6027\u7279\u6027\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8026\u5408\u7b56\u7565\u5bf9\u552f\u4e00\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u7ed9\u51fa\u4e86\u7b1b\u5361\u5c14\u8026\u5408\u7684\u53ef\u8bc6\u522b\u6027\u8fb9\u754c\u3002", "motivation": "\u5f20\u91cf\u65b9\u6cd5\u5728\u7edf\u8ba1\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6700\u5927\u4f18\u52bf\u662f\u5177\u6709\u5f3a\u552f\u4e00\u6027\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30PCTF3D\u65b9\u6cd5\u4e2d\u7ea6\u675f\u8026\u5408\u4f4e\u79e9\u6a21\u578b\u7684\u552f\u4e00\u6027\u7279\u6027\uff0c\u7279\u522b\u662f\u8026\u5408\u7b56\u7565\u5bf9\u552f\u4e00\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u96c5\u53ef\u6bd4\u7b97\u6cd5\u63d0\u4f9b\u6700\u5927\u53ef\u6062\u590d\u79e9\uff0c\u5206\u6790Part I\u6587\u7ae0\u4e2d\u63d0\u51fa\u7684\u4e0d\u540c\u8026\u5408\u7b56\u7565\u7684\u552f\u4e00\u6027\u7279\u6027\uff0c\u5e76\u9488\u5bf9\u7b1b\u5361\u5c14\u8026\u5408\u7ed9\u51fa\u53ef\u8bc6\u522b\u6027\u8fb9\u754c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u552f\u4e00\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8ePCTF3D\u4e2d\u4f7f\u7528\u7684\u8026\u5408\u65b9\u5f0f\uff0c\u5728\u6b63\u786e\u5904\u7406\u8026\u5408\u6a21\u578b\u7684\u6982\u7387\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u4e0d\u540c\u8026\u5408\u7b56\u7565\u5177\u6709\u4e0d\u540c\u7684\u552f\u4e00\u6027\u8868\u73b0\u3002", "conclusion": "PCTF3D\u65b9\u6cd5\u7684\u552f\u4e00\u6027\u7279\u6027\u4e0e\u8026\u5408\u7b56\u7565\u5bc6\u5207\u76f8\u5173\uff0c\u7b1b\u5361\u5c14\u8026\u5408\u80fd\u591f\u589e\u5f3a\u6587\u732e\u4e2d\u5145\u5206\u8fb9\u754c\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u4e3a\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2509.04962", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04962", "abs": "https://arxiv.org/abs/2509.04962", "authors": ["Antonio Spallone", "Marco Coraggio", "Francesco De Lellis", "Mario di Bernardo"], "title": "ROPE: A Novel Method for Real-Time Phase Estimation of Complex Biological Rhythms", "comment": null, "summary": "Accurate phase estimation -- the process of assigning phase values between\n$0$ and $2\\pi$ to repetitive or periodic signals -- is a cornerstone in the\nanalysis of oscillatory signals across diverse fields, from neuroscience to\nrobotics, where it is fundamental, e.g., to understanding coordination in\nneural networks, cardiorespiratory coupling, and human-robot interaction.\nHowever, existing methods are often limited to offline processing and/or\nconstrained to one-dimensional signals. In this paper, we introduce ROPE,\nwhich, to the best of our knowledge, is the first phase-estimation algorithm\ncapable of (i) handling signals of arbitrary dimension and (ii) operating in\nreal-time, with minimal error. ROPE identifies repetitions within the signal to\nsegment it into (pseudo-)periods and assigns phase values by performing\nefficient, tractable searches over previous signal segments. We extensively\nvalidate the algorithm on a variety of signal types, including trajectories\nfrom chaotic dynamical systems, human motion-capture data, and\nelectrocardiographic recordings. Our results demonstrate that ROPE is robust\nagainst noise and signal drift, and achieves significantly superior performance\ncompared to state-of-the-art phase estimation methods. This advancement enables\nreal-time analysis of complex biological rhythms, opening new pathways, for\nexample, for early diagnosis of pathological rhythm disruptions and developing\nrhythm-based therapeutic interventions in neurological and cardiovascular\ndisorders.", "AI": {"tldr": "ROPE\u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u4efb\u610f\u7ef4\u5ea6\u4fe1\u53f7\u5e76\u5b9e\u65f6\u8fd0\u884c\u7684\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u91cd\u590d\u6a21\u5f0f\u8fdb\u884c\u5206\u6bb5\uff0c\u5e76\u5728\u5148\u524d\u4fe1\u53f7\u6bb5\u4e0a\u8fdb\u884c\u9ad8\u6548\u641c\u7d22\u6765\u5206\u914d\u76f8\u4f4d\u503c\u3002", "motivation": "\u73b0\u6709\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u79bb\u7ebf\u5904\u7406\u548c/\u6216\u4e00\u7ef4\u4fe1\u53f7\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5206\u6790\u590d\u6742\u751f\u7269\u8282\u5f8b\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u795e\u7ecf\u79d1\u5b66\u548c\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\u9700\u8981\u5b9e\u65f6\u76f8\u4f4d\u4f30\u8ba1\u7684\u5e94\u7528\u3002", "method": "ROPE\u7b97\u6cd5\u901a\u8fc7\u8bc6\u522b\u4fe1\u53f7\u4e2d\u7684\u91cd\u590d\u6a21\u5f0f\u5c06\u4fe1\u53f7\u5206\u5272\u4e3a\uff08\u4f2a\uff09\u5468\u671f\uff0c\u5e76\u5728\u5148\u524d\u4fe1\u53f7\u6bb5\u4e0a\u6267\u884c\u9ad8\u6548\u3001\u53ef\u5904\u7406\u7684\u641c\u7d22\u6765\u5206\u914d\u76f8\u4f4d\u503c\u3002", "result": "\u5728\u591a\u79cd\u4fe1\u53f7\u7c7b\u578b\uff08\u5305\u62ec\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u8f68\u8ff9\u3001\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u6570\u636e\u548c\u5fc3\u7535\u56fe\u8bb0\u5f55\uff09\u4e0a\u7684\u5e7f\u6cdb\u9a8c\u8bc1\u8868\u660e\uff0cROPE\u5bf9\u566a\u58f0\u548c\u4fe1\u53f7\u6f02\u79fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u76f8\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "ROPE\u7b97\u6cd5\u5b9e\u73b0\u4e86\u590d\u6742\u751f\u7269\u8282\u5f8b\u7684\u5b9e\u65f6\u5206\u6790\uff0c\u4e3a\u75c5\u7406\u8282\u5f8b\u7d0a\u4e71\u7684\u65e9\u671f\u8bca\u65ad\u548c\u57fa\u4e8e\u8282\u5f8b\u7684\u6cbb\u7597\u5e72\u9884\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}

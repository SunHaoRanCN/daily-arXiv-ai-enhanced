{"id": "2509.05399", "categories": ["eess.AS", "cs.AI", "68T10, 68T07", "I.2.7; I.5.4; H.5.1"], "pdf": "https://arxiv.org/pdf/2509.05399", "abs": "https://arxiv.org/abs/2509.05399", "authors": ["Henry Grafé", "Hugo Van hamme"], "title": "Graph Connectionist Temporal Classification for Phoneme Recognition", "comment": "Accepted to the IEEE Automatic Speech Recognition and Understanding\n  Workshop (ASRU 2025)", "summary": "Automatic Phoneme Recognition (APR) systems are often trained using pseudo\nphoneme-level annotations generated from text through Grapheme-to-Phoneme (G2P)\nsystems. These G2P systems frequently output multiple possible pronunciations\nper word, but the standard Connectionist Temporal Classification (CTC) loss\ncannot account for such ambiguity during training. In this work, we adapt Graph\nTemporal Classification (GTC) to the APR setting. GTC enables training from a\ngraph of alternative phoneme sequences, allowing the model to consider multiple\npronunciations per word as valid supervision. Our experiments on English and\nDutch data sets show that incorporating multiple pronunciations per word into\nthe training loss consistently improves phoneme error rates compared to a\nbaseline trained with CTC. These results suggest that integrating pronunciation\nvariation into the loss function is a promising strategy for training APR\nsystems from noisy G2P-based supervision."}
{"id": "2509.05634", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.05634", "abs": "https://arxiv.org/abs/2509.05634", "authors": ["David Combei"], "title": "On the Contribution of Lexical Features to Speech Emotion Recognition", "comment": "Accepted to 13th Conference on Speech Technology and Human-Computer\n  Dialogue", "summary": "Although paralinguistic cues are often considered the primary drivers of\nspeech emotion recognition (SER), we investigate the role of lexical content\nextracted from speech and show that it can achieve competitive and in some\ncases higher performance compared to acoustic models. On the MELD dataset, our\nlexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to\n49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,\nwe analyze different self-supervised (SSL) speech and text representations,\nconduct a layer-wise study of transformer-based encoders, and evaluate the\neffect of audio denoising."}
{"id": "2509.05720", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05720", "abs": "https://arxiv.org/abs/2509.05720", "authors": ["Jesper Brunnström", "Martin Bo Møller", "Jan Østergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Time-domain sound field estimation using kernel ridge regression", "comment": null, "summary": "Sound field estimation methods based on kernel ridge regression have proven\neffective, allowing for strict enforcement of physical properties, in addition\nto the inclusion of prior knowledge such as directionality of the sound field.\nThese methods have been formulated for single-frequency sound fields,\nrestricting the types of data and prior knowledge that can be used. In this\npaper, the kernel ridge regression approach is generalized to consider\ndiscrete-time sound fields. The proposed method provides time-domain sound\nfield estimates that can be computed in closed form, are guaranteed to be\nphysically realizable, and for which time-domain properties of the sound fields\ncan be exploited to improve estimation performance. Exploiting prior\ninformation on the time-domain behaviour of room impulse responses, the\nestimation performance of the proposed method is shown to be improved using a\ntime-domain data weighting, demonstrating the usefulness of the proposed\napproach. It is further shown using both simulated and real data that the\ntime-domain data weighting can be combined with a directional weighting,\nexploiting prior knowledge of both spatial and temporal properties of the room\nimpulse responses. The theoretical framework of the proposed method enables\nsolving a broader class of sound field estimation problems using kernel ridge\nregression where it would be required to consider the time-domain response\nrather than the frequency-domain response of each frequency separately."}
{"id": "2509.05849", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05849", "abs": "https://arxiv.org/abs/2509.05849", "authors": ["Marvin Lavechin", "Thomas Hueber"], "title": "From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model", "comment": "Accepted at EMNLP 2025 (Main Conference)", "summary": "Human infants face a formidable challenge in speech acquisition: mapping\nextremely variable acoustic inputs into appropriate articulatory movements\nwithout explicit instruction. We present a computational model that addresses\nthe acoustic-to-articulatory mapping problem through self-supervised learning.\nOur model comprises a feature extractor that transforms speech into latent\nrepresentations, an inverse model that maps these representations to\narticulatory parameters, and a synthesizer that generates speech outputs.\nExperiments conducted in both single- and multi-speaker settings reveal that\nintermediate layers of a pre-trained wav2vec 2.0 model provide optimal\nrepresentations for articulatory learning, significantly outperforming MFCC\nfeatures. These representations enable our model to learn articulatory\ntrajectories that correlate with human patterns, discriminate between places of\narticulation, and produce intelligible speech. Critical to successful\narticulatory learning are representations that balance phonetic\ndiscriminability with speaker invariance -- precisely the characteristics of\nself-supervised representation learning models. Our findings provide\ncomputational evidence consistent with developmental theories proposing that\nperceptual learning of phonetic categories guides articulatory development,\noffering insights into how infants might acquire speech production capabilities\ndespite the complex mapping problem they face."}
{"id": "2509.05464", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05464", "abs": "https://arxiv.org/abs/2509.05464", "authors": ["Qiang Fu", "Changhui Li"], "title": "Developing a Framework to Simulate Quantitative Ultrasound Flow and Tissue Motion for Ultrafast Doppler Ultrasound", "comment": null, "summary": "Ultrafast power Doppler imaging (uPDI) has made significant progress and\nbecome an important imaging method for both research and clinical\nimplementations. While, it lacks simulation tools that can perform\nthree-dimensional (3D) quantitative flow with tissue motion close to realistic\nconditions. In this study, we explore to construct an open-source framework,\nnamed 3D-Fully Quantitative Flow (3D-FQFlow), to provide quantitative modeling\nof 3D vascular flow with tissue motion and uPDI imaging. The framework\nintegrates a L-system-based vascular generator with SimVascular CFD for\nhemodynamics, a tissue motion simulator supporting user-defined or\nclinical-data-driven condition, an optimized PFILED ultrasound simulator, a\nprecomputed-matrix-based reconstructor, and a quantitative analyzer\n(MSE/PSNR/SSIM). Results demonstrate distinct influences of four motion\npatterns on SVD decomposition; successful 3D imaging of rabbit kidney (SSIM =\n0.951), generated vasculature (SSIM = 0.902), and clinical pulmonary arteries\n(SSIM = 0.850); and GPU acceleration permitting 1-million-scatterer simulation\nin 4,117 seconds with 18.8* speedup for 100-frame 3D-uPDI generation. 3D-FQFlow\nestablishes the first open-source framework for quantitative validation of uPDI\nunder realistic vascular and motion conditions, creating a reproducible\nstandard for microvascular imaging research\n(https://github.com/FortuneOU/3D-FQFlow)."}
{"id": "2509.05983", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05983", "abs": "https://arxiv.org/abs/2509.05983", "authors": ["Minh N. H. Nguyen", "Anh Nguyen Tran", "Dung Truong Dinh", "Nam Van Vo"], "title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition", "comment": null, "summary": "Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios."}
{"id": "2509.06221", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06221", "abs": "https://arxiv.org/abs/2509.06221", "authors": ["Vishal Choudhari"], "title": "Beamforming-LLM: What, Where and When Did I Miss?", "comment": null, "summary": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing."}
{"id": "2509.05565", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05565", "abs": "https://arxiv.org/abs/2509.05565", "authors": ["Zhihao Tao", "Athina Petropulu", "H. Vincent Poor"], "title": "Time-Modulated Intelligent Reflecting Surfaces for Integrated Sensing, Communication and Security: A Generative AI Design Framework", "comment": "submitted to Nature Portfolio Journal, npj Wireless Technology, the\n  special issue on ISAC", "summary": "We propose a novel approach to achieve physical layer security for integrated\nsensing and communication (ISAC) systems operating in the presence of targets\nthat may be eavesdroppers. The system is aided by a time-modulated intelligent\nreflecting surface (TM-IRS), which is configured to preserve the integrity of\nthe transmitted data at one or more legitimate communication users (CUs) while\nmaking them appear scrambled in all other directions. The TM-IRS design\nleverages a generative flow network (GFlowNet) framework to learn a stochastic\npolicy that samples high-performing TM-IRS configurations from a vast discrete\nparameter space. Specifically, we begin by formulating the achievable sum rate\nfor the legitimate CUs and the beampattern gain toward the target direction,\nbased on which we construct reward functions for GFlowNets that jointly capture\nboth communication and sensing performance. The TM-IRS design is modeled as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a complete configuration of TM-IRS parameters. GFlowNets,\nparametrized by deep neural networks are employed to learn a stochastic policy\nthat samples TM-IRS parameter sets with probability proportional to their\nassociated reward. Experimental results demonstrate the effectiveness of the\nproposed GFlowNet-based method in integrating sensing, communication and\nsecurity simultaneously, and also exhibit significant sampling efficiency as\ncompared to the exhaustive combinatorial search and enhanced robustness against\nthe rule-based TM-IRS design method."}
{"id": "2509.05993", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05993", "abs": "https://arxiv.org/abs/2509.05993", "authors": ["Junjie Li", "Kong Aik Lee", "Duc-Tuan Truong", "Tianchi Liu", "Man-Wai Mak"], "title": "Xi+: Uncertainty Supervision for Robust Speaker Embedding", "comment": null, "summary": "There are various factors that can influence the performance of speaker\nrecognition systems, such as emotion, language and other speaker-related or\ncontext-related variations. Since individual speech frames do not contribute\nequally to the utterance-level representation, it is essential to estimate the\nimportance or reliability of each frame. The xi-vector model addresses this by\nassigning different weights to frames based on uncertainty estimation. However,\nits uncertainty estimation model is implicitly trained through classification\nloss alone and does not consider the temporal relationships between frames,\nwhich may lead to suboptimal supervision. In this paper, we propose an improved\narchitecture, xi+. Compared to xi-vector, xi+ incorporates a temporal attention\nmodule to capture frame-level uncertainty in a context-aware manner. In\naddition, we introduce a novel loss function, Stochastic Variance Loss, which\nexplicitly supervises the learning of uncertainty. Results demonstrate\nconsistent performance improvements of about 10\\% on the VoxCeleb1-O set and\n11\\% on the NIST SRE 2024 evaluation set."}
{"id": "2509.06361", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06361", "abs": "https://arxiv.org/abs/2509.06361", "authors": ["Liping Chen", "Kong Aik Lee", "Zhen-Hua Ling", "Xin Wang", "Rohan Kumar Das", "Tomoki Toda", "Haizhou Li"], "title": "Speaker Privacy and Security in the Big Data Era: Protection and Defense against Deepfake", "comment": null, "summary": "In the era of big data, remarkable advancements have been achieved in\npersonalized speech generation techniques that utilize speaker attributes,\nincluding voice and speaking style, to generate deepfake speech. This has also\namplified global security risks from deepfake speech misuse, resulting in\nconsiderable societal costs worldwide. To address the security threats posed by\ndeepfake speech, techniques have been developed focusing on both the protection\nof voice attributes and the defense against deepfake speech. Among them, the\nvoice anonymization technique has been developed to protect voice attributes\nfrom extraction for deepfake generation, while deepfake detection and\nwatermarking have been utilized to defend against the misuse of deepfake\nspeech. This paper provides a short and concise overview of the three\ntechniques, describing the methodologies, advancements, and challenges. A\ncomprehensive version, offering additional discussions, will be published in\nthe near future."}
{"id": "2509.05639", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05639", "abs": "https://arxiv.org/abs/2509.05639", "authors": ["Yijie Liu", "Weidong Mei", "He Sun", "Dong Wang", "Peilan Wang"], "title": "Power-Measurement-Based Channel Estimation for Beyond Diagonal RIS", "comment": null, "summary": "Beyond diagonal reconfigurable intelligent surface (BD-RIS), with its\nenhanced degrees of freedom compared to conventional RIS, has demonstrated\nnotable potential for enhancing wireless communication performance. However, a\nkey challenge in employing BD-RIS lies in accurately acquiring its channel\nstate information (CSI) with both the base station (BS) and users. Existing\nBD-RIS channel estimation methods rely mainly on dedicated pilot signals, which\nincrease system overhead and may be incompatible with current communication\nprotocols. To overcome these limitations, this letter proposes a new\nsingle-layer neural network (NN)-enabled channel estimation method utilizing\nonly the easily accessible received power measurements at user terminals. In\nparticular, we show that the received signal power can be expressed in a form\nsimilar to a single-layer NN, where the weights represent the BD-RIS's CSI.\nThis structure enables the recovery of CSI using the backward propagation,\nbased on power measurements collected under varying training reflection\ncoefficients. Numerical results show that our proposed method can achieve a\nsmall normalized mean square error (NMSE), particularly when the number of\ntraining reflections is large."}
{"id": "2509.06027", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06027", "abs": "https://arxiv.org/abs/2509.06027", "authors": ["Yi Yuan", "Xubo Liu", "Haohe Liu", "Xiyuan Kang", "Zhuo Chen", "Yuxuan Wang", "Mark D. Plumbley", "Wenwu Wang"], "title": "DreamAudio: Customized Text-to-Audio Generation with Diffusion Models", "comment": "Demos are available at\n  https://yyua8222.github.io/DreamAudio_demopage/", "summary": "With the development of large-scale diffusion-based and\nlanguage-modeling-based generative models, impressive progress has been\nachieved in text-to-audio generation. Despite producing high-quality outputs,\nexisting text-to-audio models mainly aim to generate semantically aligned sound\nand fall short on precisely controlling fine-grained acoustic characteristics\nof specific sounds. As a result, users that need specific sound content may\nfind it challenging to generate the desired audio clips. In this paper, we\npresent DreamAudio for customized text-to-audio generation (CTTA).\nSpecifically, we introduce a new framework that is designed to enable the model\nto identify auditory information from user-provided reference concepts for\naudio generation. Given a few reference audio samples containing personalized\naudio events, our system can generate new audio samples that include these\nspecific events. In addition, two types of datasets are developed for training\nand testing the customized systems. The experiments show that the proposed\nmodel, DreamAudio, generates audio samples that are highly consistent with the\ncustomized audio features and aligned well with the input text prompts.\nFurthermore, DreamAudio offers comparable performance in general text-to-audio\ntasks. We also provide a human-involved dataset containing audio events from\nreal-world CTTA cases as the benchmark for customized generation tasks."}
{"id": "2509.06598", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06598", "abs": "https://arxiv.org/abs/2509.06598", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.04845", "summary": "In this study, we address the multimodal task of stereo sound event\nlocalization and detection with source distance estimation (3D SELD) in regular\nvideo content. 3D SELD is a complex task that combines temporal event\nclassification with spatial localization, requiring reasoning across spatial,\ntemporal, and semantic dimensions. The last is arguably the most challenging to\nmodel. Traditional SELD approaches typically rely on multichannel input,\nlimiting their capacity to benefit from large-scale pre-training due to data\nconstraints. To overcome this, we enhance a standard SELD architecture with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. We perform an ablation study on\nthe development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the\nindividual contributions of the language-aligned models and benchmark against\nthe DCASE Task 3 baseline systems. Additionally, we detail the curation process\nof large synthetic audio and audio-visual datasets used for model pre-training.\nThese datasets were further expanded through left-right channel swapping\naugmentation. Our approach, combining extensive pre-training, model ensembling,\nand visual post-processing, achieved second rank in the DCASE 2025 Challenge\nTask 3 (Track B), underscoring the effectiveness of our method. Future work\nwill explore the modality-specific contributions and architectural refinements."}
{"id": "2509.05677", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05677", "abs": "https://arxiv.org/abs/2509.05677", "authors": ["Xuancheng Zhu", "Zhiwen Zhou", "Yong Zeng"], "title": "Full-Angle Ray Antenna Array and Omnicell Wireless Communication System", "comment": null, "summary": "Ray antenna array (RAA) was recently proposed as a novel multi-antenna\narchitecture that arranges multiple massive cheap antenna elements into simple\nuniform linear arrays (sULAs) with different orientations. Compared with\ntraditional architectures like hybrid analog/digital beamforming with uniform\nlinear array (ULA) and uniform circular array (UCA), RAA has several promising\nadvantages such as significantly reduced hardware cost, higher beamforming\ngains and the ability of providing uniform angular resolution for all\ndirections. In this paper, we propose a full-angle RAA architecture and an\ninnovative omnicell wireless communication paradigm enabled by full-angle RAA.\nThe proposed full-angle RAA expands RAA's orientation angle to the full angle\ndomain, such that the RAA's advantages can be exploited to all directions. This\nfurther enables the new concept of omnicell wireless communication system, with\nthe base station equipped by full-angle RAA and deployed at the center of each\ncell. Compared to the conventional cell sectoring wireless communication\nsystem, the proposed omnicell system is expected to not only significantly\nreduce the inter-user interference, but also improve the cost efficiency.\nExtensive analytical and numerical results are provided to compare those key\nperformance indicators such as the spatial resolution and the communication\nrate of the proposed full-angle RAA based omnicell wireless communication\nsystem against the conventional ULA/UCA-based cell sectoring systems."}
{"id": "2509.06389", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06389", "abs": "https://arxiv.org/abs/2509.06389", "authors": ["Xiaoran Yang", "Jianxuan Yang", "Xinyue Guo", "Haoyu Wang", "Ningning Pan", "Gongping Huang"], "title": "MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation", "comment": null, "summary": "A key challenge in synthesizing audios from silent videos is the inherent\ntrade-off between synthesis quality and inference efficiency in existing\nmethods. For instance, flow matching based models rely on modeling\ninstantaneous velocity, inherently require an iterative sampling process,\nleading to slow inference speeds. To address this efficiency bottleneck, we\nintroduce a MeanFlow-accelerated model that characterizes flow fields using\naverage velocity, enabling one-step generation and thereby significantly\naccelerating multimodal video-to-audio (VTA) synthesis while preserving audio\nquality, semantic alignment, and temporal synchronization. Furthermore, a\nscalar rescaling mechanism is employed to balance conditional and unconditional\npredictions when classifier-free guidance (CFG) is applied, effectively\nmitigating CFG-induced distortions in one step generation. Since the audio\nsynthesis network is jointly trained with multimodal conditions, we further\nevaluate it on text-to-audio (TTA) synthesis task. Experimental results\ndemonstrate that incorporating MeanFlow into the network significantly improves\ninference speed without compromising perceptual quality on both VTA and TTA\nsynthesis tasks."}
{"id": "2509.05983", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05983", "abs": "https://arxiv.org/abs/2509.05983", "authors": ["Minh N. H. Nguyen", "Anh Nguyen Tran", "Dung Truong Dinh", "Nam Van Vo"], "title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition", "comment": null, "summary": "Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios."}
{"id": "2509.05683", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05683", "abs": "https://arxiv.org/abs/2509.05683", "authors": ["Kuranage Roche Rayan Ranasinghe", "Henrique L. Senger", "Gustavo P. Gonçalves", "Hyeon Seok Rou", "Bruno S. Chang", "Giuseppe Thadeu Freitas de Abreu", "Didier Le Ruyet"], "title": "Affine Filter Bank Modulation (AFBM): A Novel 6G ISAC Waveform with Low PAPR and OOBE", "comment": "Submitted to an IEEE journal", "summary": "We propose the affine filter bank modulation (AFBM) waveform for enhanced\nintegrated sensing and communications (ISAC) in sixth generation (6G), designed\nby drawing on concepts from classical filter bank multicarrier modulation\n(FBMC) theory and recent advances in chirp-domain waveforms, particularly\naffine frequency division multiplexing (AFDM). Specifically, AFBM exhibits\nseveral desirable properties, with emphasis on its remarkably low\npeak-to-average power ratio (PAPR) and reduced out-of-band emission (OOBE) when\nbenchmarked against the conventional AFDM waveform under doubly-dispersive (DD)\nchannel conditions. In the communications setting, reliable symbol detection is\nachieved using a tailored low-complexity Gaussian belief propagation\n(GaBP)-based algorithm, while in the sensing setting, a range and velocity\nestimation approach is developed that integrates an expectation maximization\n(EM)-assisted probabilistic data association (PDA) framework to accurately\nidentify surrounding targets. The highlighted performance and benefits of AFBM\nare validated through analytical and numerical evaluations, including\nconventional metrics such as ambiguity function (AF), bit error rate (BER), and\nroot mean square error (RMSE), consolidating its position as a promising\nwaveform for next-generation wireless systems."}
{"id": "2509.06502", "categories": ["cs.SD", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06502", "abs": "https://arxiv.org/abs/2509.06502", "authors": ["Junjie Chen", "Yao Hu", "Junjie Li", "Kangyue Li", "Kun Liu", "Wenpeng Li", "Xu Li", "Ziyuan Li", "Feiyu Shen", "Xu Tang", "Manzhen Wei", "Yichen Wu", "Fenglong Xie", "Kaituo Xu", "Kun Xie"], "title": "FireRedChat: A Pluggable, Full-Duplex Voice Interaction System with Cascaded and Semi-Cascaded Implementations", "comment": "12 pages, 2 figures", "summary": "Full-duplex voice interaction allows users and agents to speak simultaneously\nwith controllable barge-in, enabling lifelike assistants and customer service.\nExisting solutions are either end-to-end, difficult to design and hard to\ncontrol, or modular pipelines governed by turn-taking controllers that ease\nupgrades and per-module optimization; however, prior modular frameworks depend\non non-open components and external providers, limiting holistic optimization.\nIn this work, we present a complete, practical full-duplex voice interaction\nsystem comprising a turn-taking controller, an interaction module, and a\ndialogue manager. The controller integrates streaming personalized VAD (pVAD)\nto suppress false barge-ins from noise and non-primary speakers, precisely\ntimestamp primary-speaker segments, and explicitly enable primary-speaker\nbarge-ins; a semantic end-of-turn detector improves stop decisions. It upgrades\nheterogeneous half-duplex pipelines, cascaded, semi-cascaded, and\nspeech-to-speech, to full duplex. Using internal models, we implement cascaded\nand semi-cascaded variants; the semi-cascaded one captures emotional and\nparalinguistic cues, yields more coherent responses, lowers latency and error\npropagation, and improves robustness. A dialogue manager extends capabilities\nvia tool invocation and context management. We also propose three system-level\nmetrics, barge-in, end-of-turn detection accuracy, and end-to-end latency, to\nassess naturalness, control accuracy, and efficiency. Experiments show fewer\nfalse interruptions, more accurate semantic ends, and lower latency approaching\nindustrial systems, enabling robust, natural, real-time full-duplex\ninteraction. Demos: https://fireredteam.github.io/demos/firered_chat."}
{"id": "2509.05993", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05993", "abs": "https://arxiv.org/abs/2509.05993", "authors": ["Junjie Li", "Kong Aik Lee", "Duc-Tuan Truong", "Tianchi Liu", "Man-Wai Mak"], "title": "Xi+: Uncertainty Supervision for Robust Speaker Embedding", "comment": null, "summary": "There are various factors that can influence the performance of speaker\nrecognition systems, such as emotion, language and other speaker-related or\ncontext-related variations. Since individual speech frames do not contribute\nequally to the utterance-level representation, it is essential to estimate the\nimportance or reliability of each frame. The xi-vector model addresses this by\nassigning different weights to frames based on uncertainty estimation. However,\nits uncertainty estimation model is implicitly trained through classification\nloss alone and does not consider the temporal relationships between frames,\nwhich may lead to suboptimal supervision. In this paper, we propose an improved\narchitecture, xi+. Compared to xi-vector, xi+ incorporates a temporal attention\nmodule to capture frame-level uncertainty in a context-aware manner. In\naddition, we introduce a novel loss function, Stochastic Variance Loss, which\nexplicitly supervises the learning of uncertainty. Results demonstrate\nconsistent performance improvements of about 10\\% on the VoxCeleb1-O set and\n11\\% on the NIST SRE 2024 evaluation set."}
{"id": "2509.05692", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05692", "abs": "https://arxiv.org/abs/2509.05692", "authors": ["Armin Farhadi", "Maryam Cheraghy", "Qingqing Wu", "Eduard Jorswieck"], "title": "Resource Allocation and Beamforming in FIM-Assisted BS and STAR-BD-RIS-Aided NOMA: A Meta-Learning Approach", "comment": null, "summary": "This study explores a flexible intelligent metasurface (FIM)-based wireless\ncommunication system that integrates simultaneously transmitting and reflecting\nbeyond diagonal reconfigurable intelligent surfaces (STAR-BD-RIS) with\nnon-orthogonal multiple access (NOMA). The system features a multi-antenna\nFIM-assisted base station (BS) aided by dual-sector BD-RIS. The FIM consists of\ncost-effective radiating elements that can independently emit signals and\ndynamically adjust their vertical positions (\"morphing\"). The goal is to\nmaximize energy efficiency by jointly optimizing BS beamforming, the\nSTAR-BD-RIS matrix, NOMA constraints, and the FIM surface shape under power\nlimits. Due to the problem's non-convexity, a meta-soft actor-critic (Meta-SAC)\nalgorithm is proposed for adaptive optimization. Simulation results show that\nMeta-SAC outperforms the Meta-DDPG algorithm, and FIM-assisted designs yield\nsubstantial energy efficiency gains over benchmark schemes."}
{"id": "2509.06635", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06635", "abs": "https://arxiv.org/abs/2509.06635", "authors": ["Liping Chen", "Jinghao He", "Zhengyan Sheng", "Kong Aik Lee", "Zhen-Hua Ling"], "title": "The First Voice Timbre Attribute Detection Challenge", "comment": null, "summary": "The first voice timbre attribute detection challenge is featured in a special\nsession at NCMMSC 2025. It focuses on the explainability of voice timbre and\ncompares the intensity of two speech utterances in a specified timbre\ndescriptor dimension. The evaluation was conducted on the VCTK-RVA dataset.\nParticipants developed their systems and submitted their outputs to the\norganizer, who evaluated the performance and sent feedback to them. Six teams\nsubmitted their outputs, with five providing descriptions of their\nmethodologies."}
{"id": "2509.06027", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06027", "abs": "https://arxiv.org/abs/2509.06027", "authors": ["Yi Yuan", "Xubo Liu", "Haohe Liu", "Xiyuan Kang", "Zhuo Chen", "Yuxuan Wang", "Mark D. Plumbley", "Wenwu Wang"], "title": "DreamAudio: Customized Text-to-Audio Generation with Diffusion Models", "comment": "Demos are available at\n  https://yyua8222.github.io/DreamAudio_demopage/", "summary": "With the development of large-scale diffusion-based and\nlanguage-modeling-based generative models, impressive progress has been\nachieved in text-to-audio generation. Despite producing high-quality outputs,\nexisting text-to-audio models mainly aim to generate semantically aligned sound\nand fall short on precisely controlling fine-grained acoustic characteristics\nof specific sounds. As a result, users that need specific sound content may\nfind it challenging to generate the desired audio clips. In this paper, we\npresent DreamAudio for customized text-to-audio generation (CTTA).\nSpecifically, we introduce a new framework that is designed to enable the model\nto identify auditory information from user-provided reference concepts for\naudio generation. Given a few reference audio samples containing personalized\naudio events, our system can generate new audio samples that include these\nspecific events. In addition, two types of datasets are developed for training\nand testing the customized systems. The experiments show that the proposed\nmodel, DreamAudio, generates audio samples that are highly consistent with the\ncustomized audio features and aligned well with the input text prompts.\nFurthermore, DreamAudio offers comparable performance in general text-to-audio\ntasks. We also provide a human-involved dataset containing audio events from\nreal-world CTTA cases as the benchmark for customized generation tasks."}
{"id": "2509.05903", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05903", "abs": "https://arxiv.org/abs/2509.05903", "authors": ["Wei Huang", "Junpeng Lu", "Tianhe Xu", "Jianxu Shu", "Hao Zhang", "Kaitao Meng", "Yanan Wu"], "title": "Optimal Anchor Deployment and Topology Design for Large-Scale AUV Navigation", "comment": null, "summary": "Seafloor acoustic anchors are an important component of AUV navigation,\nproviding absolute updates that correct inertial dead-reckoning. Unlike\nterrestrial positioning systems, the deployment of underwater anchor nodes is\nusually sparse due to the uneven distribution of underwater users, as well as\nthe high economic cost and difficult maintenance of underwater equipment. These\nanchor nodes lack satellite coverage and cannot form ubiquitous backhaul as\nterrestrial nodes do. In this paper, we investigate the optimal anchor\ndeployment topology to provide high-quality AUV navigation and positioning\nservices. We first analyze the possible deployment mode in large-scale\nunderwater navigation system, and formulate a topology optimization for\nunderwater anchor node deployment. Then, we derive a scaling law about the\ninfluence of anchors in each cluster on the navigation performance within a\ngiven area and demonstrate a service area coverage condition with a high\nprobability of reaching the destination. Finally, the optimization performance\nis evaluated through experimental results."}
{"id": "2509.06654", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06654", "abs": "https://arxiv.org/abs/2509.06654", "authors": ["Emmanouil Karystinaios", "Johannes Hentschel", "Markus Neuwirth", "Gerhard Widmer"], "title": "AnalysisGNN: Unified Music Analysis with Graph Neural Networks", "comment": "Accepted at the 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR) 2025", "summary": "Recent years have seen a boom in computational approaches to music analysis,\nyet each one is typically tailored to a specific analytical domain. In this\nwork, we introduce AnalysisGNN, a novel graph neural network framework that\nleverages a data-shuffling strategy with a custom weighted multi-task loss and\nlogit fusion between task-specific classifiers to integrate heterogeneously\nannotated symbolic datasets for comprehensive score analysis. We further\nintegrate a Non-Chord-Tone prediction module, which identifies and excludes\npassing and non-functional notes from all tasks, thereby improving the\nconsistency of label signals. Experimental evaluations demonstrate that\nAnalysisGNN achieves performance comparable to traditional static-dataset\napproaches, while showing increased resilience to domain shifts and annotation\ninconsistencies across multiple heterogeneous corpora."}
{"id": "2509.06926", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06926", "abs": "https://arxiv.org/abs/2509.06926", "authors": ["Rouard Simon", "Orsini Manu", "Roebel Axel", "Zeghidour Neil", "Défossez Alexandre"], "title": "Continuous Audio Language Models", "comment": "17 pages, 3 figures", "summary": "Audio Language Models (ALM) have emerged as the dominant paradigm for speech\nand music generation by representing audio as sequences of discrete tokens.\nYet, unlike text tokens, which are invertible, audio tokens are extracted from\nlossy codecs with a limited bitrate. As a consequence, increasing audio quality\nrequires generating more tokens, which imposes a trade-off between fidelity and\ncomputational cost. We address this issue by studying Continuous Audio Language\nModels (CALM). These models instantiate a large Transformer backbone that\nproduces a contextual embedding at every timestep. This sequential information\nthen conditions an MLP that generates the next continuous frame of an audio VAE\nthrough consistency modeling. By avoiding lossy compression, CALM achieves\nhigher quality at lower computational cost than their discrete counterpart.\nExperiments on speech and music demonstrate improved efficiency and fidelity\nover state-of-the-art discrete audio language models, facilitating lightweight,\nhigh-quality audio generation. Samples are available at\nhttps://continuous-audio-language-models.github.io"}
{"id": "2509.05955", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05955", "abs": "https://arxiv.org/abs/2509.05955", "authors": ["Jiali He", "Sheng Shen", "Jiamin Wu", "Xiaohan Kong", "Yamei Dai", "Liang Tan", "Zheng Xu"], "title": "Active noise cancellation in ultra-low field MRI: distinct strategies for different channels", "comment": null, "summary": "Ultra-low field magnetic resonance imaging(ULF-MRI) systems operating in open\nenvironments are highly susceptible to composite electromagnetic\ninterference(EMI). Different imaging channels respond non-uniformly to EMI\nowing to their distinct coupling characteristics. Here, we investigate\nchannel-specific interference pathways in a permanent-magnet-based low-field\nMRI system and show that saddle coils are intrinsically more vulnerable to\ntransverse EMI components than solenoidal coils. To mitigate these\nheterogeneous coupling effects, we propose a dual-stage suppression strategy\nthat combines front-end spatial-domain inverse field reconstruction with\nback-end channel-adaptive active noise cancellation. Experiments demonstrate\nthat this approach suppresses EMI by more than 80%, substantially improves\ninter-channel signal-to-noise ratio(SNR) consistency, and enhances the\nfused-image SNR by 24%. These findings elucidate the channel-dependent nature\nof EMI coupling and establish targeted mitigation strategies, providing both a\ntheoretical basis and practical guidance for noise suppression in future\narray-coil ULF-MRI systems."}
{"id": "2509.06926", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06926", "abs": "https://arxiv.org/abs/2509.06926", "authors": ["Rouard Simon", "Orsini Manu", "Roebel Axel", "Zeghidour Neil", "Défossez Alexandre"], "title": "Continuous Audio Language Models", "comment": "17 pages, 3 figures", "summary": "Audio Language Models (ALM) have emerged as the dominant paradigm for speech\nand music generation by representing audio as sequences of discrete tokens.\nYet, unlike text tokens, which are invertible, audio tokens are extracted from\nlossy codecs with a limited bitrate. As a consequence, increasing audio quality\nrequires generating more tokens, which imposes a trade-off between fidelity and\ncomputational cost. We address this issue by studying Continuous Audio Language\nModels (CALM). These models instantiate a large Transformer backbone that\nproduces a contextual embedding at every timestep. This sequential information\nthen conditions an MLP that generates the next continuous frame of an audio VAE\nthrough consistency modeling. By avoiding lossy compression, CALM achieves\nhigher quality at lower computational cost than their discrete counterpart.\nExperiments on speech and music demonstrate improved efficiency and fidelity\nover state-of-the-art discrete audio language models, facilitating lightweight,\nhigh-quality audio generation. Samples are available at\nhttps://continuous-audio-language-models.github.io"}
{"id": "2509.06936", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06936", "abs": "https://arxiv.org/abs/2509.06936", "authors": ["Pedro Ramoneda", "Pablo Alonso-Jiménez", "Sergio Oramas", "Xavier Serra", "Dmitry Bogdanov"], "title": "Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets", "comment": null, "summary": "Music autotagging aims to automatically assign descriptive tags, such as\ngenre, mood, or instrumentation, to audio recordings. Due to its challenges,\ndiversity of semantic descriptions, and practical value in various\napplications, it has become a common downstream task for evaluating the\nperformance of general-purpose music representations learned from audio data.\nWe introduce a new benchmarking dataset based on the recently published MGPHot\ndataset, which includes expert musicological annotations, allowing for\nadditional insights and comparisons with results obtained on common generic tag\ndatasets. While MGPHot annotations have been shown to be useful for\ncomputational musicology, the original dataset neither includes audio nor\nprovides evaluation setups for its use as a standardized autotagging benchmark.\nTo address this, we provide a curated set of YouTube URLs with retrievable\naudio, and propose a train/val/test split for standardized evaluation, and\nprecomputed representations for seven state-of-the-art models. Using these\nresources, we evaluated these models in MGPHot and standard reference tag\ndatasets, highlighting key differences between expert and generic tag\nannotations. Altogether, our contributions provide a more advanced benchmarking\nframework for future research in music understanding."}
{"id": "2509.05959", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05959", "abs": "https://arxiv.org/abs/2509.05959", "authors": ["Pierluigi Poggiolini", "Francesco Poletti"], "title": "The Case for a DNANF 1Pb/s Trans-Atlantic Submarine Cable", "comment": "preprint of a European Conference on Optical Communications (ECOC)\n  2025 paper", "summary": "The recent progress in low-loss hollow-core fibers allows to speculate on the\npossibility of building a transatlantic submarine cable that can achieve the\ngoal of 1 Pb/s per direction, leveraging bidirectional transmission, and at the\nsame time drastically increase span length, theoretically to 200km."}
{"id": "2509.06936", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.06936", "abs": "https://arxiv.org/abs/2509.06936", "authors": ["Pedro Ramoneda", "Pablo Alonso-Jiménez", "Sergio Oramas", "Xavier Serra", "Dmitry Bogdanov"], "title": "Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets", "comment": null, "summary": "Music autotagging aims to automatically assign descriptive tags, such as\ngenre, mood, or instrumentation, to audio recordings. Due to its challenges,\ndiversity of semantic descriptions, and practical value in various\napplications, it has become a common downstream task for evaluating the\nperformance of general-purpose music representations learned from audio data.\nWe introduce a new benchmarking dataset based on the recently published MGPHot\ndataset, which includes expert musicological annotations, allowing for\nadditional insights and comparisons with results obtained on common generic tag\ndatasets. While MGPHot annotations have been shown to be useful for\ncomputational musicology, the original dataset neither includes audio nor\nprovides evaluation setups for its use as a standardized autotagging benchmark.\nTo address this, we provide a curated set of YouTube URLs with retrievable\naudio, and propose a train/val/test split for standardized evaluation, and\nprecomputed representations for seven state-of-the-art models. Using these\nresources, we evaluated these models in MGPHot and standard reference tag\ndatasets, highlighting key differences between expert and generic tag\nannotations. Altogether, our contributions provide a more advanced benchmarking\nframework for future research in music understanding."}
{"id": "2509.05971", "categories": ["eess.SP", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.05971", "abs": "https://arxiv.org/abs/2509.05971", "authors": ["Kaiyi Chi", "Yinghui He", "Qianqian Yang", "Zhiping Jiang", "Yuanchao Shu", "Zhiqin Wang", "Jun Luo", "Jiming Chen"], "title": "DeepStream: Prototyping Deep Joint Source-Channel Coding for Real-Time Multimedia Transmissions", "comment": "13 pages, 43 figures", "summary": "Deep learning-based joint source-channel coding (DeepJSCC) has emerged as a\npromising technique in 6G for enhancing the efficiency and reliability of data\ntransmission across diverse modalities, particularly in low signal-to-noise\nratio (SNR) environments. This advantage is realized by leveraging powerful\nneural networks to learn an optimal end-to-end mapping from the source data\ndirectly to the transmit symbol sequence, eliminating the need for separate\nsource coding, channel coding, and modulation. Although numerous efforts have\nbeen made towards efficient DeepJSCC, they have largely stayed at numerical\nsimulations that can be far from practice, leaving the real-world viability of\nDeepJSCC largely unverified. To this end, we prototype DeepStream upon\northogonal frequency division multiplexing (OFDM) technology to offer efficient\nand robust DeepJSCC for multimedia transmission. In conforming to OFDM, we\ndevelop both a feature-to-symbol mapping method and a cross-subcarrier\nprecoding method to improve the subcarrier independence and reduce\npeak-to-average power ratio. To reduce system complexity and enable flexibility\nin accommodating varying quality of service requirements, we further propose a\nprogressive coding strategy that adjusts the compression ratio based on latency\nwith minimal performance loss. We implement DeepStream for real-time image\ntransmission and video streaming using software-defined radio. Extensive\nevaluations verify that DeepStream outperforms both the standard scheme and the\ndirect deployment scheme. Particularly, at an SNR of 10 dB, DeepStream achieves\na PSNR of 35 dB for image transmission and an MS-SSIM of 20 dB for video\nstreaming, whereas the standard scheme fails to recover meaningful information."}
{"id": "2509.05634", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.05634", "abs": "https://arxiv.org/abs/2509.05634", "authors": ["David Combei"], "title": "On the Contribution of Lexical Features to Speech Emotion Recognition", "comment": "Accepted to 13th Conference on Speech Technology and Human-Computer\n  Dialogue", "summary": "Although paralinguistic cues are often considered the primary drivers of\nspeech emotion recognition (SER), we investigate the role of lexical content\nextracted from speech and show that it can achieve competitive and in some\ncases higher performance compared to acoustic models. On the MELD dataset, our\nlexical-based approach obtains a weighted F1-score (WF1) of 51.5%, compared to\n49.3% for an acoustic-only pipeline with a larger parameter count. Furthermore,\nwe analyze different self-supervised (SSL) speech and text representations,\nconduct a layer-wise study of transformer-based encoders, and evaluate the\neffect of audio denoising."}
{"id": "2509.05977", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05977", "abs": "https://arxiv.org/abs/2509.05977", "authors": ["Ayush Jha", "Dhanireddy Chandrika", "Chandra Sekhar Seelamantula", "Chetan Singh Thakur"], "title": "3D-Image Reconstruction using MIMO-SAR FMCW Radar", "comment": "Presented In ISCS25", "summary": "With the advancement of millimeter-wave radar technology, Synthetic Aperture\nRadar (SAR) imaging at millimeter-wave frequencies has gained significant\nattention in both academic research and industrial applications. However,\ntraditional SAR imaging algorithms primarily focus on extracting\ntwo-dimensional information from detected targets, which limits their potential\nfor 3D scene reconstruction. In this work, we demonstrated a fast time-domain\nreconstruction algorithm for achieving high-resolution 3D radar imaging at\nmillimeter-wave (mmWave) frequencies. This approach leverages a combination of\nvirtual Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous\nWave (FMCW) radar with the precision of Synthetic Aperture Radar (SAR)\ntechnique, setting the stage for a new era of advanced radar imaging\napplications."}
{"id": "2509.05720", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05720", "abs": "https://arxiv.org/abs/2509.05720", "authors": ["Jesper Brunnström", "Martin Bo Møller", "Jan Østergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Time-domain sound field estimation using kernel ridge regression", "comment": null, "summary": "Sound field estimation methods based on kernel ridge regression have proven\neffective, allowing for strict enforcement of physical properties, in addition\nto the inclusion of prior knowledge such as directionality of the sound field.\nThese methods have been formulated for single-frequency sound fields,\nrestricting the types of data and prior knowledge that can be used. In this\npaper, the kernel ridge regression approach is generalized to consider\ndiscrete-time sound fields. The proposed method provides time-domain sound\nfield estimates that can be computed in closed form, are guaranteed to be\nphysically realizable, and for which time-domain properties of the sound fields\ncan be exploited to improve estimation performance. Exploiting prior\ninformation on the time-domain behaviour of room impulse responses, the\nestimation performance of the proposed method is shown to be improved using a\ntime-domain data weighting, demonstrating the usefulness of the proposed\napproach. It is further shown using both simulated and real data that the\ntime-domain data weighting can be combined with a directional weighting,\nexploiting prior knowledge of both spatial and temporal properties of the room\nimpulse responses. The theoretical framework of the proposed method enables\nsolving a broader class of sound field estimation problems using kernel ridge\nregression where it would be required to consider the time-domain response\nrather than the frequency-domain response of each frequency separately."}
{"id": "2509.06070", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06070", "abs": "https://arxiv.org/abs/2509.06070", "authors": ["Abdulmohsen Alsaui", "Neel Kanth Kundu", "Hyundong Shin", "Octavia A. Dobre"], "title": "Quantum Radar for ISAC: Sum-Rate Optimization", "comment": null, "summary": "Integrated sensing and communication (ISAC) is emerging as a key enabler for\nspectrum-efficient and hardware-converged wireless networks. However, classical\nradar systems within ISAC architectures face fundamental limitations under low\nsignal power and high-noise conditions. This paper proposes a novel framework\nthat embeds quantum illumination radar into a base station to simultaneously\nsupport full-duplex classical communication and quantum-enhanced target\ndetection. The resulting integrated quantum sensing and classical communication\n(IQSCC) system is optimized via a sum-rate maximization formulation subject to\nradar sensing constraints. The non-convex joint optimization of transmit power\nand beamforming vectors is tackled using the successive convex approximation\ntechnique. Furthermore, we derive performance bounds for classical and quantum\nradar protocols under the statistical detection theory, highlighting the\nquantum advantage in low signal-to-interference-plus-noise ratio regimes.\nSimulation results demonstrate that the proposed IQSCC system achieves a higher\ncommunication throughput than the conventional ISAC baseline while satisfying\nthe sensing requirement."}
{"id": "2509.06170", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06170", "abs": "https://arxiv.org/abs/2509.06170", "authors": ["Hao Jiang", "Zhaolin Wang", "Yuanwei Liu", "Arumugam Nallanathan", "Zhiguo Ding"], "title": "Pinching Antenna System (PASS) Enhanced Covert Communications: Against Warden via Sensing", "comment": "Submit to possible IEEE journal", "summary": "A sensing-aided covert communication network empowered by pinching antenna\nsystems (PASS) is proposed in this work. Unlike conventional fixed-position\nMIMO arrays, PASS dynamically reconfigures its pinching antennas (PAs) closer\nto the legitimate user, substantially enhancing covertness. To further secure\nthe adversary's channel state information (CSI), a sensing function is\nleveraged to track the malicious warden's movements. In particular, this paper\nfirst proposes an extended Kalman filter (EKF) based approach to fulfilling the\ntracking function. Building on this, a covert communication problem is\nformulated with a joint design of beamforming, artificial noise (AN) signals,\nand the position of PAs. Then, the beamforming and AN design subproblems are\nresolved jointly with a subspace approach, while the PA position optimization\nsubproblem is handled by a deep reinforcement learning (DRL) approach by\ntreating the evolution of the warden's mobility status as a temporally\ncorrected process. Numerical results are presented and demonstrate that: i) the\nEKF approach can accurately track the warden's CSI with low complexity, ii) the\neffectiveness of the proposed solution is verified by its outperformance over\nthe greedy and searching-based benchmarks, and iii) with new design degrees of\nfreedom (DoFs), the performance of PASS is superior to the conventional\nfully-digital MIMO systems."}
{"id": "2509.06257", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06257", "abs": "https://arxiv.org/abs/2509.06257", "authors": ["Yuyan Wu", "Jiale Zhang", "Moon Lee", "Cherrelle Smith", "Xinyi Li", "Ankur Senapati", "Pei Zhang", "Hae Young Noh"], "title": "Human Body Weight Estimation Through Music-Induced Bed Vibrations", "comment": "Submitted to Mobicom 2026", "summary": "Rapid and accurate body weight estimation is critical in emergency medical\ncare, as it directly influences treatment decisions, such as drug dosing,\ndefibrillation energy selection, and fluid resuscitation. Traditional methods\nsuch as stand-on scales, length-based tapes, or transfer-based weighing scales\nare often impractical for immobilized patients, inaccurate, or labor-intensive\nand time-consuming. This paper introduces MelodyBedScale, a non-intrusive and\nrapid on-bed weight estimation system that leverages bed vibration induced by\nmusic. The core insight is that body weight affects the vibration transfer\nfunction of the bed-body system, which is captured using vibration sensors\nplaced on opposite sides of the bed. First, we identify weight-sensitive\nfrequency bands and compose clinically acceptable soft, natural music with high\nsignal energy in these frequency bands. This music is then played through a\nspeaker mounted on the bed to induce bed vibrations. Additionally, to\nefficiently capture the complex weight-vibration relationship with limited data\nand enhance generalizability to unseen individuals and weights, we\ntheoretically analyze the weight-vibration relationship and integrate the\nresults into the activation functions of the neural network for\nphysics-informed weight regression. We evaluated MelodyBedScale on both wooden\nand steel beds across 11 participants, achieving a mean absolute error of up to\n1.55 kg."}
{"id": "2509.06491", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06491", "abs": "https://arxiv.org/abs/2509.06491", "authors": ["Siddarth Marwaha", "Pawel Kryszkiewicz", "Eduard Jorswieck"], "title": "Optimal Distortion-Aware Multi-User Power Allocation for Massive MIMO Networks", "comment": null, "summary": "Real-world wireless transmitter front-ends exhibit certain nonlinear\nbehavior, e.g., signal clipping by a Power Amplifier (PA). Although many\nresource allocation solutions do not consider this for simplicity, it leads to\ninaccurate results or a reduced number of degrees of freedom, not achieving the\nglobal performance. In this work, we propose an optimal PA distortion-aware\npower allocation strategy in a downlink orthogonal frequency division multiplex\n(OFDM) based massive multiple-input multiple-output (M-MIMO) system. Assuming a\nsoft-limiter PA model, where the transmission occurs under small-scale\nindependent and identically distributed (i.i.d) Rayleigh fading channel, we\nderive the wideband signal-to-noise-and-distortion ratio (SNDR) and formulate\nthe power allocation problem. Most interestingly, the distortion introduced by\nthe PA leads to an SNDR-efficient operating point without explicit transmit\npower constraints. While the optimization problem is non-convex, we decouple it\ninto a non-convex total power allocation problem and a convex power\ndistribution problem among the users (UEs). We propose an alternating\noptimization algorithm to find the optimum solution. Our simulation results\nshow significant sum-rate gains over existing distortion-neglecting solutions,\ne.g., a median 4 times increase and a median 50\\% increase for a 64-antenna and\n512-antenna base station serving 60 users, respectively."}
{"id": "2509.06506", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06506", "abs": "https://arxiv.org/abs/2509.06506", "authors": ["Ensong Liu", "Rongqing Zhang", "Xiang Cheng", "Jian Tang"], "title": "Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception", "comment": null, "summary": "Collaborative perception enables more accurate and comprehensive scene\nunderstanding by learning how to share information between agents, with LiDAR\npoint clouds providing essential precise spatial data. Due to the substantial\ndata volume generated by LiDAR sensors, efficient point cloud transmission is\nessential for low-latency multi-agent collaboration. In this work, we propose\nan efficient, robust and applicable LiDAR point cloud transmission system via\nthe Synesthesia of Machines (SoM), termed LiDAR Point Cloud Feature\nTransmission (LPC-FT), to support collaborative perception among multiple\nagents. Specifically, we employ a density-preserving deep point cloud\ncompression method that encodes the complete point cloud into a downsampled\nefficient representation. To mitigate the effects of the wireless channel, we\ndesign a channel encoder module based on self-attention to enhance LiDAR point\ncloud features and a feature fusion module based on cross-attention to\nintegrate features from transceivers. Furthermore, we utilize the nonlinear\nactivation layer and transfer learning to improve the training of deep neural\nnetworks in the presence the digital channel noise. Experimental results\ndemonstrate that the proposed LPC-FT is more robust and effective than\ntraditional octree-based compression followed by channel coding, and\noutperforms state-of-the-art deep learning-based compression techniques and\nexisting semantic communication methods, reducing the Chamfer Distance by 30%\nand improving the PSNR by 1.9 dB on average. Owing to its superior\nreconstruction performance and robustness against channel variations, LPC-FT is\nexpected to support collaborative perception tasks."}
{"id": "2509.06569", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06569", "abs": "https://arxiv.org/abs/2509.06569", "authors": ["Chenyu Zhang", "Yuanhang Wu", "Xiaoxi Ma", "Wei Yi"], "title": "Integrated Detection and Tracking Based on Radar Range-Doppler Feature", "comment": null, "summary": "Detection and tracking are the basic tasks of radar systems. Current joint\ndetection tracking methods, which focus on dynamically adjusting detection\nthresholds from tracking results, still present challenges in fully utilizing\nthe potential of radar signals. These are mainly reflected in the limited\ncapacity of the constant false-alarm rate model to accurately represent\ninformation, the insufficient depiction of complex scenes, and the limited\ninformation acquired by the tracker. We introduce the Integrated Detection and\nTracking based on radar feature (InDT) method, which comprises a network\narchitecture for radar signal detection and a tracker that leverages detection\nassistance. The InDT detector extracts feature information from each\nRange-Doppler (RD) matrix and then returns the target position through the\nfeature enhancement module and the detection head. The InDT tracker adaptively\nupdates the measurement noise covariance of the Kalman filter based on\ndetection confidence. The similarity of target RD features is measured by\ncosine distance, which enhances the data association process by combining\nlocation and feature information. Finally, the efficacy of the proposed method\nwas validated through testing on both simulated data and publicly available\ndatasets."}
{"id": "2509.06615", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06615", "abs": "https://arxiv.org/abs/2509.06615", "authors": ["Wouter Jansen", "Jan Steckel"], "title": "Towards In-Air Ultrasonic QR Codes: Deep Learning for Classification of Passive Reflector Constellations", "comment": "Accepted for publication at IEEE IUS 2025", "summary": "In environments where visual sensors falter, in-air sonar provides a reliable\nalternative for autonomous systems. While previous research has successfully\nclassified individual acoustic landmarks, this paper takes a step towards\nincreasing information capacity by introducing reflector constellations as\nencoded tags. Our primary contribution is a multi-label Convolutional Neural\nNetwork (CNN) designed to simultaneously identify multiple, closely spaced\nreflectors from a single in-air 3D sonar measurement. Our initial findings on a\nsmall dataset confirm the feasibility of this approach, validating the ability\nto decode these complex acoustic patterns. Secondly, we investigated using\nadaptive beamforming with null-steering to isolate individual reflectors for\nsingle-label classification. Finally, we discuss the experimental results and\nlimitations, offering key insights and future directions for developing\nacoustic landmark systems with significantly increased information entropy and\ntheir accurate and robust detection and classification."}
{"id": "2509.06651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06651", "abs": "https://arxiv.org/abs/2509.06651", "authors": ["Mikael Rinkinen", "Mehdi Safarpour", "Shahriar Shahabuddin", "Olli Silven", "Lauri Koskinen"], "title": "Near-Threshold Voltage Massive MIMO Computing", "comment": null, "summary": "Massive MIMO systems have the potential to significantly enhance spectral\nefficiency, yet their widespread integration is hindered by the high power\nconsumption of the underlying computations. This paper explores the\napplicability and effectiveness of Algorithm-Based Fault Tolerance (ABFT) for\nmassive MIMO signal processing to tackle the reliability challenge of Near\nThreshold Computing (NTC). We propose modifying matrix arithmetic Newton\niteration MIMO algorithm to seamlessly integrate ABFT to detect any\ncomputational errors by inspecting the final result. The overhead from ABFT\ndepends largely on the matrix dimensions, which in this context are dictated by\nthe number of user equipments involved in the computation. NTC is a promising\nstrategy for reducing the energy consumption in digital circuits by operating\ntransistors at extremely reduced voltages. However, NTC is highly susceptible\nto variations in Process, Voltage, and Temperature (PVT) which can lead to\nincreased error rates in computations. Traditional techniques for enabling NTC,\nsuch as dynamic voltage and frequency scaling guided by circuit level timing\nerror detection methods, introduce considerable hardware complexity and are\ndifficult to implement at high clock frequencies. In this context ABFT has\nemerged as a lightweight error detection method tailored for matrix operations\nwithout requiring any modifications on circuit-level and can be implemented\npurely in software.A MIMO accelerator was implemented on a reconfigurable\nhardware platform. Experimental results demonstrate that for sufficiently large\nproblem sizes, the proposed method achieves a 36% power saving compared to\nbaseline, with only an average of 3% computational overhead, at default clock\nfrequency. These results indicate that combining ABFT with near-threshold\noperation provides a viable path toward energy-efficient and robust massive\nMIMO processors."}
{"id": "2509.06662", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06662", "abs": "https://arxiv.org/abs/2509.06662", "authors": ["Ao Huang", "Xidong Mu", "Li Guo", "Guangyu Zhu"], "title": "SE and EE Tradeoff in Active STAR-RIS Assisted Systems With Hardware Impairments", "comment": null, "summary": "This paper investigates the problem of resource efficiency maximization in an\nactive simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS) assisted communication system under practical transceiver\nhardware impairments (HWIs). We aim to obtain an optimal tradeoff between\nsystem spectral efficiency (SE) and energy efficiency (EE), by jointly\noptimizing the base station (BS) transmit beamforming and the active STAR-RIS\nbeamforming. To tackle the challenges in the fractional objective function, we\nbegin by applying the quadratic transformation method to simplify it into a\nmanageable form. An alternating optimization-based algorithm is then developed\nto iteratively update the BS and STAR-RIS beamforming coefficients. Simulation\nresults demonstrate that the proposed scheme performs better than other\nbaseline schemes in the presence of HWIs. Moreover, the variation of the\nachievable SE-EE region with different transmit power budgets is analyzed."}
{"id": "2509.06672", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06672", "abs": "https://arxiv.org/abs/2509.06672", "authors": ["Ahmad Bazzi", "Mingjun Ying", "Ojas Kanhere", "Theodore S. Rappaport", "Marwa Chafii"], "title": "ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G", "comment": null, "summary": "Integrated sensing and communications (ISAC) is emerging as a cornerstone\ntechnology for sixth generation (6G) wireless systems, unifying connectivity\nand environmental mapping through shared hardware, spectrum, and waveforms. The\nfollowing paper presents an ISAC imaging framework utilizing channel state\ninformation (CSI) per-path components, transmitter (TX) positions, and receiver\n(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz in\nthe upper mid-band. Our work shows how each resolvable multipath component can\nbe extracted from CSI estimation and cast into an equivalent three-dimensional\nreflection point by fusing its angle and delay information, which is useful and\nchallenging for multi-bounce reflections. The primary contribution of the paper\nis the two-segment reflection point optimization algorithm, which independently\nestimates the path lengths from the TX position and RX position to an\nequivalent reflection point (ERP) on the object surface, thus enabling precise\ngeometric reconstruction. Subsequently, we aggregate the ERPs derived from\nmultiple pairs of TX and RX positions, generating dense three dimensional point\nclouds representing the objects in the channel. Experimental results validate\nthat the proposed ISAC imaging framework accurately reconstructs object\nsurfaces, edges, and curved features. To the best of our knowledge, this paper\nprovides the first demonstration of multi bounce ISAC imaging using wireless\nray tracing at 6.75 GHz."}
{"id": "2509.06751", "categories": ["eess.SP", "68T45", "I.5.4"], "pdf": "https://arxiv.org/pdf/2509.06751", "abs": "https://arxiv.org/abs/2509.06751", "authors": ["Weicheng Gao"], "title": "RadHARSimulator V1: Model-Based FMCW Radar Human Activity Recognition Simulator", "comment": "17 pages, 12 figures, 5 tables", "summary": "Radar-based human activity recognition (HAR) is a pivotal research area for\napplications requiring non-invasive monitoring. However, the acquisition of\ndiverse and high-fidelity radar datasets for robust algorithm development\nremains a significant challenge. To overcome this bottleneck, a model-based\nfrequency-modulated continuous wave (FMCW) radar HAR simulator is developed.\nThe simulator integrates an anthropometrically scaled $13$-scatterer kinematic\nmodel to simulate $12$ distinct activities. The FMCW radar echo model is\nemployed, which incorporates dynamic radar cross-section (RCS), free-space or\nthrough-the-wall propagation, and a calibrated noise floor to ensure signal\nfidelity. The simulated raw data is then processed through a complete pipeline,\nincluding moving target indication (MTI), bulk Doppler compensation, and\nSavitzky-Golay denoising, culminating in the generation of high-resolution\nrange-time map (RTM) and Doppler-time maps (DTMs) via both short-time Fourier\ntransform (STFT) and Fourier synchrosqueezed transform (FSST). Finally, a novel\nneural network method is proposed to validate the effectiveness of the radar\nHAR. Numerical experiments demonstrate that the simulator successfully\ngenerates high-fidelity and distinct micro-Doppler signature, which provides a\nvaluable tool for radar HAR algorithm design and validation. The installer of\nthis simulator is released at:\n\\href{https://github.com/JoeyBGOfficial/RadHARSimulatorV1-Model-Based-FMCW-Radar-Human-Activity-Recognition-Simulator}{Github/JoeyBGOfficial/RadHARSimulatorV1}."}
{"id": "2509.06820", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06820", "abs": "https://arxiv.org/abs/2509.06820", "authors": ["Yu-Hsiang Huang", "Po-Heng Chou", "Wan-Jen Huang", "Walid Saad", "C. -C. Jay Kuo"], "title": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI", "comment": "6 pages, 4 figures, 2 tables, accepted by 2025 IEEE Globecom", "summary": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios."}
{"id": "2509.05720", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.05720", "abs": "https://arxiv.org/abs/2509.05720", "authors": ["Jesper Brunnström", "Martin Bo Møller", "Jan Østergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Time-domain sound field estimation using kernel ridge regression", "comment": null, "summary": "Sound field estimation methods based on kernel ridge regression have proven\neffective, allowing for strict enforcement of physical properties, in addition\nto the inclusion of prior knowledge such as directionality of the sound field.\nThese methods have been formulated for single-frequency sound fields,\nrestricting the types of data and prior knowledge that can be used. In this\npaper, the kernel ridge regression approach is generalized to consider\ndiscrete-time sound fields. The proposed method provides time-domain sound\nfield estimates that can be computed in closed form, are guaranteed to be\nphysically realizable, and for which time-domain properties of the sound fields\ncan be exploited to improve estimation performance. Exploiting prior\ninformation on the time-domain behaviour of room impulse responses, the\nestimation performance of the proposed method is shown to be improved using a\ntime-domain data weighting, demonstrating the usefulness of the proposed\napproach. It is further shown using both simulated and real data that the\ntime-domain data weighting can be combined with a directional weighting,\nexploiting prior knowledge of both spatial and temporal properties of the room\nimpulse responses. The theoretical framework of the proposed method enables\nsolving a broader class of sound field estimation problems using kernel ridge\nregression where it would be required to consider the time-domain response\nrather than the frequency-domain response of each frequency separately."}
{"id": "2509.06598", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.06598", "abs": "https://arxiv.org/abs/2509.06598", "authors": ["Davide Berghi", "Philip J. B. Jackson"], "title": "Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.04845", "summary": "In this study, we address the multimodal task of stereo sound event\nlocalization and detection with source distance estimation (3D SELD) in regular\nvideo content. 3D SELD is a complex task that combines temporal event\nclassification with spatial localization, requiring reasoning across spatial,\ntemporal, and semantic dimensions. The last is arguably the most challenging to\nmodel. Traditional SELD approaches typically rely on multichannel input,\nlimiting their capacity to benefit from large-scale pre-training due to data\nconstraints. To overcome this, we enhance a standard SELD architecture with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. We perform an ablation study on\nthe development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the\nindividual contributions of the language-aligned models and benchmark against\nthe DCASE Task 3 baseline systems. Additionally, we detail the curation process\nof large synthetic audio and audio-visual datasets used for model pre-training.\nThese datasets were further expanded through left-right channel swapping\naugmentation. Our approach, combining extensive pre-training, model ensembling,\nand visual post-processing, achieved second rank in the DCASE 2025 Challenge\nTask 3 (Track B), underscoring the effectiveness of our method. Future work\nwill explore the modality-specific contributions and architectural refinements."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.SD](#cs.SD) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction](https://arxiv.org/abs/2509.07990)
*Charan Gajjala Chenchu,Kinam Kim,Gao Lu,Zia Ud Din*

Main category: eess.SP

TL;DR: 这篇论文比较了体表电图（sEMG）信号和视频两种数据模态在建筑行业人机协作中识别人类运动意图的性能，sEMG模型准确率87%速度0.04秒，视频模型准确率94%但需0.15秒。


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业人机协作中缺乏不同数据模态（信号与视频）在运动意图识别方面的系统性比较研究空白。

Method: 采用深度学习模型：CNN-LSTM模型处理sEMG数据，Video Swin Transformer模型处理视频序列，通过迁移学习识别干墙安装任务中的时期运动意图。

Result: sEMG模型准确率87%、预测时间0.04秒；视频模型准确率94%、预测时间0.15秒。两种模态各有优势：sEMG速度更快，视频准确性更高。

Conclusion: 不同数据模态在人机协作中具有互补性优势，需要根据实际应用场景系统化部署以提升建筑行业的人机协作效果。

Abstract: Human-robot collaboration (HRC) in the construction industry depends on
precise and prompt recognition of human motion intentions and actions by robots
to maximize safety and workflow efficiency. There is a research gap in
comparing data modalities, specifically signals and videos, for motion
intention recognition. To address this, the study leverages deep learning to
assess two different modalities in recognizing workers' motion intention at the
early stage of movement in drywall installation tasks. The Convolutional Neural
Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface
electromyography (sEMG) data achieved an accuracy of around 87% with an average
time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the
pre-trained Video Swin Transformer combined with transfer learning harnessed
video sequences as input to recognize motion intention and attained an accuracy
of 94% but with a longer average time of 0.15 seconds for a similar prediction.
This study emphasizes the unique strengths and trade-offs of both data formats,
directing their systematic deployments to enhance HRC in real-world
construction projects.

</details>


### [2] [Privacy Preserving Semantic Communications Using Vision Language Models: A Segmentation and Generation Approach](https://arxiv.org/abs/2509.08142)
*Haoran Chang,Mingzhe Chen,Huaxia Wang,Qianqian Zhang*

Main category: eess.SP

TL;DR: 基于视觉-语言模型的隐私保护语义通信框架，通过识别和移除图像中的敏感区域，使得正式接收方重构质量提升10%以上，同时减少偶发者识别泄漏50%以上


<details>
  <summary>Details</summary>
Motivation: 解决单模态表征在差通道条件下重构质量下降的问题，并应对语义信息攻击的隐私风险

Method: 利用视觉-语言模型识别和移除输入图像中的私密内容区域，通过共享隐私数据库确保发送方和接收方的语义对齐，在接收端使用生成模块根据接收到的文本嵌入和学习的语义先验重构被掩盖区域

Result: 框架在未见过的图像处理任务上表现良好，使得正式接收方重构质量提升10%以上，同时将偶发者识别泄漏减少50%以上

Conclusion: 该隐私保护语义通信框架有效地保护了图像数据的敏感内容，同时提高了通信效率和重构质量，为下一代无线系统提供了一种可靠的隐私保护方案

Abstract: Semantic communication has emerged as a promising paradigm for
next-generation wireless systems, improving the communication efficiency by
transmitting high-level semantic features. However, reliance on unimodal
representations can degrade reconstruction under poor channel conditions, and
privacy concerns of the semantic information attack also gain increasing
attention. In this work, a privacy-preserving semantic communication framework
is proposed to protect sensitive content of the image data. Leveraging a
vision-language model (VLM), the proposed framework identifies and removes
private content regions from input images prior to transmission. A shared
privacy database enables semantic alignment between the transmitter and
receiver to ensure consistent identification of sensitive entities. At the
receiver, a generative module reconstructs the masked regions using learned
semantic priors and conditioned on the received text embedding. Simulation
results show that generalizes well to unseen image processing tasks, improves
reconstruction quality at the authorized receiver by over 10% using text
embedding, and reduces identity leakage to the eavesdropper by more than 50%.

</details>


### [3] [RTR: A Transformer-Based Lossless Crossover with Perfect Phase Alignment](https://arxiv.org/abs/2509.08272)
*Xiangying Li,Jiankuan Li,Yong Tang*

Main category: eess.SP

TL;DR: 提出基于变压器的无损分频方法RTR，实现频率分离同时保证低频和高频通道在分频点完美相位对齐，具有线性互补特性，能完美重构原始信号。


<details>
  <summary>Details</summary>
Motivation: 传统LC分频器和数字滤波器存在能量损耗、相位不一致和元件容差敏感等问题，需要开发低损耗、低延迟的硬件辅助滤波解决方案。

Method: 使用谐振变压器路由器(RTR)技术，其频率响应满足线性互补关系HLF(f)+HHF(f)=1，通过理论推导和电路仿真验证。

Result: RTR在能量效率、相位一致性和元件容差鲁棒性方面表现优异，相比传统方法提供更好的性能。

Conclusion: RTR是一种适用于高保真音频和通信前端的低损耗、低延迟硬件滤波解决方案，基于中国专利技术并进行了扩展验证。

Abstract: This paper proposes a transformer-based lossless crossover method, termed
Resonant Transformer Router (RTR), which achieves frequency separation while
ensuring perfect phase alignment between low-frequency (LF) and high-frequency
(HF) channels at the crossover frequency. The core property of RTR is that its
frequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so
that the original signal can be perfectly reconstructed by linear summation of
the two channels. Theoretical derivation and circuit simulations demonstrate
that RTR provides superior energy efficiency, phase consistency, and robustness
against component tolerances. Compared with conventional LC crossovers and
digital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted
filtering solution suitable for high-fidelity audio and communication
front-ends.
  The core theory behind this paper's work, lossless crossover, is based on a
Chinese patent [CN116318117A] developed from the previous research of one of
the authors, Jianluan Li. We provide a comprehensive experimental validation of
this theory and propose a new extension.

</details>


### [4] [Fundamental Trade-off in Wideband Stacked Intelligent Metasurface Assisted OFDMA Systems](https://arxiv.org/abs/2509.08294)
*Zheao Li,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于叠加智能超表面(SIM)的全模拟放大系统，通过灵活子载波分配策略和迭代优化算法，在宽带OFDMA系统中实现了互干干扰和高资源利用率的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统的宽带多用户OFDM数字放大需要大量耐耗组件，硬件成本高。SIM虽然能进行光速波基预编码，但在宽带多用户传输中实现全模拟放大仍面临互干干扰的挑战。

Method: 提出灵活的子载波分配策略，每个子载波可选择性服务一个或多个用户，并使用迭代算法聚合优化子载波分配矩阵和SIM传输系数，以近似无互干干扰的通道。

Result: 结果显示该系统具有低拟合误差，同时允许每个用户利用更多子载波，实现了近零互干干扰和稳健的数据可靠性。

Conclusion: 该系统在不增加数字预编码硬件负担的前提下，实现了互干干扰减少和资源利用率提升的基本平衡，为宽带多用户全模拟放大提供了有效解决方案。

Abstract: Conventional digital beamforming for wideband multiuser orthogonal
frequency-division multiplexing (OFDM) demands numerous power-hungry
components, increasing hardware costs and complexity. By contrast, the stacked
intelligent metasurfaces (SIM) can perform wave-based precoding at near-light
speed, drastically reducing baseband overhead. However, realizing SIM-enhanced
fully-analog beamforming for wideband multiuser transmissions remains
challenging, as the SIM configuration has to handle interference across all
subcarriers. To address this, this paper proposes a flexible subcarrier
allocation strategy to fully reap the SIM-assisted fully-analog beamforming
capability in an orthogonal frequency-division multiple access (OFDMA) system,
where each subcarrier selectively serves one or more users to balance
interference mitigation and resource utilization of SIM. We propose an
iterative algorithm to jointly optimize the subcarrier assignment matrix and
SIM transmission coefficients, approximating an interference-free channel for
those selected subcarriers. Results show that the proposed system has low
fitting errors yet allows each user to exploit more subcarriers. Further
comparisons highlight a fundamental trade-off: our system achieves near-zero
interference and robust data reliability without incurring the hardware burdens
of digital precoding.

</details>


### [5] [Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain Location](https://arxiv.org/abs/2509.08432)
*Yingjie Wu,Junshan Luo,Weiyu Chen,Shilian Wang,Fanggang Wang,Haiyang Ding*

Main category: eess.SP

TL;DR: 提出了一种结合流体天线和人工噪声技术的无人机安全通信框架，通过联合优化部署、预编码和天线位置来最大化最差情况下的保密率，有效应对窃听威胁。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线缺乏足够的空间自由度，使得视距主导的无人机通信链路容易受到窃听攻击，需要新的安全通信方案。

Method: 采用流体天线和人工噪声技术，通过凸优化处理任意形状的窃听区域不确定性，考虑自由移动和区域移动两种天线模式，提出高效的鲁棒算法。

Result: 数值结果表明，流体天线方案通过利用额外空间自由度而非发射功率来提升安全性，人工噪声在高发射功率下提供显著增益，两者协同作用超过各自贡献之和。

Conclusion: 该框架在有限资源下实现了安全性和可靠性的平衡，流体天线与人工噪声的协同作用为无人机安全通信提供了有效解决方案。

Abstract: For autonomous aerial vehicle (AAV) secure communications, traditional
designs based on fixed position antenna (FPA) lack sufficient spatial degrees
of freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable
to eavesdropping. To overcome this problem, this paper proposes a framework
that effectively incorporates the fluid antenna (FA) and the artificial noise
(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple
eavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN
precoders, and FA positions. In particular, the worst-case MSR is considered by
taking the channel uncertainties due to the uncertainty about eavesdropping
locations into account. To tackle the highly coupled optimization variables and
the channel uncertainties in the formulated problem, an efficient and robust
algorithm is proposed. Particularly, the uncertain regions of eavesdroppers,
whose shapes can be arbitrary, are disposed by constructing convex hull. In
addition, two movement modes of FAs are considered, namely, free movement mode
and zonal movement mode, for which different optimization techniques are
applied, respectively. Numerical results show that, the proposed FA schemes
boost security by exploiting additional spatial DoF rather than transmit power,
while AN provides remarkable gains under high transmit power. Furthermore, the
synergy between FA and AN results in a secure advantage that exceeds the sum of
their individual contributions, achieving a balance between security and
reliability under limited resources.

</details>


### [6] [Information and Communication Theoretical Foundations of the Internet of Plants, Principles, Challenges, and Future Directions](https://arxiv.org/abs/2509.08434)
*Ahmet B. Kilic,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 本文从信息通信技术角度系统分析植物多模态信号交换，提出植物互联网（IoP）概念，为精准农业和生态监测提供基础。


<details>
  <summary>Details</summary>
Motivation: 将植物化学、电气、菌根和声学信号交换系统地重新解释为信息通信技术框架，补充现有生物学研究与ICT领域的跨学科沟差。

Method: 通过教程式调查方式，先介绍基础生物学知识，然后将各模态的发射、传播和接收模型重构为发射机、信道和接收机块，并综述实验研究和先进感知技术。

Result: 建立了植物信息交换的统一ICT理论框架，提出了植物互联网（IoP）的新兴视野，为跨学科研究和应用提供了理论基础。

Conclusion: 本文通过整合生物学见解与ICT框架，为通信研究社群提供了植物通信的全面教程，并为跨学科发展和植物互联网应用奠定了基础。

Abstract: Plants exchange information through multiple modalities, including chemical,
electrical, mycorrhizal, and acoustic signaling, which collectively support
survival, defense, and adaptation. While these processes are well documented in
biology, their systematic analysis from an Information and Communication
Technology (ICT) perspective remains limited. To address this gap, this article
is presented as a tutorial with survey elements. It provides the necessary
biological background, reformulates inter-plant signaling within ICT
frameworks, and surveys empirical studies to guide future research and
applications. First, the paper introduces the fundamental biological processes
to establish a foundation for readers in communications and networking.
Building on this foundation, existing models of emission, propagation, and
reception are synthesized for each modality and reformulated in terms of
transmitter, channel, and receiver blocks. To complement theory, empirical
studies and state-of-the-art sensing approaches are critically examined.
Looking forward, the paper identifies open challenges and outlines future
research directions, with particular emphasis on the emerging vision of the
Internet of Plants (IoP). This paradigm frames plants as interconnected nodes
within ecological and technological networks, offering new opportunities for
applications in precision agriculture, ecosystem monitoring, climate
resilience, and bio-inspired communication systems. By integrating biological
insights with ICT frameworks and projecting toward the IoP, this article
provides a comprehensive tutorial on plant communication for the communications
research community and establishes a foundation for interdisciplinary advances.

</details>


### [7] [On the Performance of ISAC over the D-Band in a Phase-Noise Aware OFDM Systems](https://arxiv.org/abs/2509.08504)
*Didem Aydogan,Mohaned Chraiti,Korkut Kaan Tokgöz*

Main category: eess.SP

TL;DR: 本文评估了在130GHz频段使用3GPP相位噪声模型下，OFDM ISAC系统的感知性能，发现相位噪声导致距离精度受限（0.04-0.05m）和速度估计受限（0.12-0.18m/s），多普勒旁瓣指标也出现饱和。


<details>
  <summary>Details</summary>
Motivation: D波段（110-170GHz）是未来5G/6G ISAC系统的有前景频段，但相位噪声是该频段的关键损伤因素，需要评估其对感知性能的实际影响。

Method: 采用硬件调谐的3GPP相位噪声模型，在130GHz频段使用480kHz numerology的OFDM信号，通过FFT雷达处理评估ISAC感知性能。

Result: 相位噪声导致距离RMSE平台在0.04-0.05m，速度RMSE平台在0.12-0.18m/s，多普勒旁瓣指标PSLR约-6dB，ISLR约-4dB。距离精度受带宽限制，而速度估计和旁瓣抑制对相位噪声敏感。

Conclusion: 研究强调了相位噪声感知的波形和numerology设计对于亚太赫兹ISAC系统的重要性，为未来多频段收发器提供了设计见解。

Abstract: Phase noise (PN) is a critical impairment at D-band frequencies (110 to 170
GHz), which are widely investigated as promising candidates for beyond 5G/6G
ISAC systems. This paper evaluates OFDM based ISAC sensing performance under
realistic oscillator impairments using a hardware-tuned 3GPP PN model at 130
GHz and FFT based radar processing. With a numerology of 480 kHz, results show
that PN introduces range RMSE floors of 0.04 to 0.05 m and velocity RMSE floors
of 0.12 to 0.18 m/s. Doppler sidelobe metrics also saturate, with PSLR around
minus 6 dB and ISLR around minus 4 dB. These findings confirm that range
accuracy remains bandwidth limited, while velocity estimation and sidelobe
suppression are strongly PN-sensitive. The study highlights the importance of
PN-aware waveform and numerology design for sub-THz ISAC and provides insights
for future multi-band transceivers. Communication metrics and PN mitigation
strategies such as PTRS and CPE tracking are left for future work.

</details>


### [8] [Modular PE-Structured Learning for Cross-Task Wireless Communications](https://arxiv.org/abs/2509.08614)
*Yuxuan Duan,Chenyang Yang*

Main category: eess.SP

TL;DR: 通过派生对称性知识设计模块化深度网络，提出PE-MoFormer框架，在无线通信多任务学习中实现更高效率和更快推理速度


<details>
  <summary>Details</summary>
Motivation: 现有无线通信多任务学习方法依赖大型模型，在边缘设备上预训练和微调困难，需要更简洁高效的解决方案

Method: 利用派生对称性(PE)知识设计三种PE模块（包含两种Transformer风格子层），通过模块组装构建简洁DNN，提出PE-MoFormer组合式模型

Result: 模拟实验表明，该模块化PE框架在学习效率和推理速度方面都超过相关大型模型，具有强的通用性和低样本/空间复杂度

Conclusion: 基于结构化PE知识的模块化设计为无线通信跨任务学习提供了新方向，能够高效处理预编码、协同收据材、功率分配、频道估计等多种无线政策

Abstract: Recent trends in learning wireless policies attempt to develop deep neural
networks (DNNs) for handling multiple tasks with a single model. Existing
approaches often rely on large models, which are hard to pre-train and
fine-tune at the wireless edge. In this work, we challenge this paradigm by
leveraging the structured knowledge of wireless problems -- specifically,
permutation equivariant (PE) properties. We design three types of PE-aware
modules, two of which are Transformer-style sub-layers. These modules can serve
as building blocks to assemble compact DNNs applicable to the wireless policies
with various PE properties. To guide the design, we analyze the hypothesis
space associated with each PE property, and show that the PE-structured module
assembly can boost the learning efficiency. Inspired by the reusability of the
modules, we propose PE-MoFormer, a compositional DNN capable of learning a wide
range of wireless policies -- including but not limited to precoding,
coordinated beamforming, power allocation, and channel estimation -- with
strong generalizability, low sample and space complexity. Simulations
demonstrate that the proposed modular PE-based framework outperforms relevant
large model in both learning efficiency and inference time, offering a new
direction for structured cross-task learning for wireless communications.

</details>


### [9] [RIS-Assisted Near-Field ISAC for Multi-Target Indication in NLoS Scenarios](https://arxiv.org/abs/2509.08642)
*Hang Ruan,Homa Nikbakht,Ruizhi Zhang,Honglei Chen,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 这篇论文提出了一种利用可重配智能表面(RIS)的边缓写框架，解决近场集成感知通信(ISAC)系统中多目标感知的挑战，特别是在直视路径被阻塞的情况下。


<details>
  <summary>Details</summary>
Motivation: 在近场ISAC系统中实现多目标感知存在重大挑战，特别是当直视路径被阻塞时。需要一种能够利用角度和距离信息来区分多个用户和目标的方案。

Method: 将经典的波束模型增益和目标间相关性指标扩展到近场领域，通过聚焦基站和RIS相位移的联合设计来最大化最差感知性能。采用交替优化(AO)算法和半定见松弛(SDR)解决非凸问题。

Result: 模拟实验表明，该RIS辅助框架能够在被阻塞场景下实现高分辨率的同角度目标感知。

Conclusion: 该方框架有效解决了近场ISAC系统中的多目标感知问题，特别是在直视路径缺失的情况下，通过利用近场角度和距离信息实现了高分辨率的目标区分。

Abstract: Enabling multi-target sensing in near-field integrated sensing and
communication (ISAC) systems is a key challenge, particularly when
line-of-sight paths are blocked. This paper proposes a beamforming framework
that leverages a reconfigurable intelligent surface (RIS) to achieve
multi-target indication. Our contribution is the extension of classic
beampattern gain and inter-target cross-correlation metrics to the near-field,
leveraging both angle and distance information to discriminate between multiple
users and targets. We formulate a problem to maximize the worst-case sensing
performance by jointly designing the beamforming at the base station and the
phase shifts at the RIS, while guaranteeing communication rates. The non-convex
problem is solved via an efficient alternating optimization (AO) algorithm that
utilizes semidefinite relaxation (SDR). Simulations demonstrate that our
RIS-assisted framework enables high-resolution sensing of co-angle targets in
blocked scenarios.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [10] [A Bottom-up Framework with Language-universal Speech Attribute Modeling for Syllable-based ASR](https://arxiv.org/abs/2509.08173)
*Hao Yen,Pin-Jui Ku,Sabato Marco Siniscalchi,Chin-Hui Lee*

Main category: eess.AS

TL;DR: 一种基于语言通用叙述属性模型的底层向上语音识别框架，通过结构化知识集成将叙述属性转换为音节，在中文和日语上都取得了较好效果


<details>
  <summary>Details</summary>
Motivation: 解决音节基语言中语音识别的挑战，通过语言通用的叙述属性模型来提高模型的可解释性和健壁性，并支持跨语言的零样本转移

Method: 首先识别叙述属性序列成瞹阵，然后通过结构化知识集成将其转换为音节，并且引入了两个新的评估指标PrER和SHER

Result: 在AISHELL-1普通话语料库上达到竞争性能能，低资源条件下更健壁，在日语零样本转移中比基线锐减40%错误率

Conclusion: 该底层向上框架通过统一叙述属性模型和音节预测，在音节基语言中显示出优称性能和跨语言转移能力

Abstract: We propose a bottom-up framework for automatic speech recognition (ASR) in
syllable-based languages by unifying language-universal articulatory attribute
modeling with syllable-level prediction. The system first recognizes sequences
or lattices of articulatory attributes that serve as a language-universal,
interpretable representation of pronunciation, and then transforms them into
syllables through a structured knowledge integration process. We introduce two
evaluation metrics, namely Pronunciation Error Rate (PrER) and Syllable Homonym
Error Rate (SHER), to evaluate the model's ability to capture pronunciation and
handle syllable ambiguities. Experimental results on the AISHELL-1 Mandarin
corpus demonstrate that the proposed bottom-up framework achieves competitive
performance and exhibits better robustness under low-resource conditions
compared to the direct syllable prediction model. Furthermore, we investigate
the zero-shot cross-lingual transferability on Japanese and demonstrate
significant improvements over character- and phoneme-based baselines by 40%
error rate reduction.

</details>


### [11] [Context-Aware Query Refinement for Target Sound Extraction: Handling Partially Matched Queries](https://arxiv.org/abs/2509.08292)
*Ryo Sato,Chiho Haruta,Nobuhiko Hiruma,Keisuke Imoto*

Main category: eess.AS

TL;DR: 本文提出了上下文感知查询精细化方法，以解决部分匹配查询条件下目标音频提取的性能氛涡问题。


<details>
  <summary>Details</summary>
Motivation: 传统目标音频提取研究主要关注完全匹配查询条件，而实际应用中查询可能包含不活跃的音频类别，导致部分匹配查询条件下性能显著下降。

Method: 提出上下文感知查询精细化方法，在推理过程中根据估计的音频类别活跃情况消除查询中的非活跃类别。

Result: 实验结果显示，传统方法在部分匹配查询条件下性能显著下降，而本文方法能够有效减轻这种性能氛涡，在多种查询条件下实现高稳健性。

Conclusion: 上下文感知查询精细化方法能够有效提升目标音频提取系统在部分匹配查询条件下的高稳健性和性能表现。

Abstract: Target sound extraction (TSE) is the task of extracting a target sound
specified by a query from an audio mixture. Much prior research has focused on
the problem setting under the Fully Matched Query (FMQ) condition, where the
query specifies only active sounds present in the mixture. However, in
real-world scenarios, queries may include inactive sounds that are not present
in the mixture. This leads to scenarios such as the Fully Unmatched Query (FUQ)
condition, where only inactive sounds are specified in the query, and the
Partially Matched Query (PMQ) condition, where both active and inactive sounds
are specified. Among these conditions, the performance degradation under the
PMQ condition has been largely overlooked. To achieve robust TSE under the PMQ
condition, we propose context-aware query refinement. This method eliminates
inactive classes from the query during inference based on the estimated sound
class activity. Experimental results demonstrate that while conventional
methods suffer from performance degradation under the PMQ condition, the
proposed method effectively mitigates this degradation and achieves high
robustness under diverse query conditions.

</details>


### [12] [Few-shot Personalization via In-Context Learning for Speech Emotion Recognition based on Speech-Language Model](https://arxiv.org/abs/2509.08344)
*Mana Ihori,Taiga Yamane,Naotaka Kawata,Naoki Makishima,Tomohiro Tanaka,Satoshi Suzuki,Shota Orihashi,Ryo Masumura*

Main category: eess.AS

TL;DR: 通过上下文学习实现语音情感识别的个性化方法，利用少量目标讲话者情感语音进行适配，提升识别性能


<details>
  <summary>Details</summary>
Motivation: 由于不同讲话者的情感表达方式存在差异，传统语音情感识别方法需要预先准备所有情感标签的语音样本，这在实际应用中很困难

Method: 扩展大语言模型构建语音-语言模型，通过元训练学习如何利用少量目标讲话者情感语音进行上下文学习，实现个性化语音情感识别

Result: 在新收集的SER数据集上进行实验，证明所提方法在识别性能上超过了传统方法

Conclusion: 通过上下文学习实现的个性化语音情感识别方法有效解决了传统方法需要预先准备所有情感标签样本的问题，显著提升了识别性能

Abstract: This paper proposes a personalization method for speech emotion recognition
(SER) through in-context learning (ICL). Since the expression of emotions
varies from person to person, speaker-specific adaptation is crucial for
improving the SER performance. Conventional SER methods have been personalized
using emotional utterances of a target speaker, but it is often difficult to
prepare utterances corresponding to all emotion labels in advance. Our idea to
overcome this difficulty is to obtain speaker characteristics by conditioning a
few emotional utterances of the target speaker in ICL-based inference. ICL is a
method to perform unseen tasks by conditioning a few input-output examples
through inference in large language models (LLMs). We meta-train a
speech-language model extended from the LLM to learn how to perform
personalized SER via ICL. Experimental results using our newly collected SER
dataset demonstrate that the proposed method outperforms conventional methods.

</details>


### [13] [Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition](https://arxiv.org/abs/2509.08470)
*Jing-Tong Tzeng,Carlos Busso,Chi-Chun Lee*

Main category: eess.AS

TL;DR: 提出Sparse MERIT框架，通过帧级专家路由机制解决多任务学习中语音增强和情感识别的梯度冲突问题，在噪声环境下显著提升两个任务的性能


<details>
  <summary>Details</summary>
Motivation: 语音情感识别在噪声环境下性能显著下降，传统语音增强方法会引入伪影掩盖情感线索且增加计算开销，多任务学习存在梯度干扰和表征冲突问题

Method: Sparse MERIT框架，使用自监督语音表征，通过任务特定的门控网络动态选择共享专家池中的专家进行帧级路由，实现参数高效和任务自适应的表征学习

Result: 在MSP-Podcast语料库上，-5dB SNR条件下：SER F1-macro比SE预处理基线平均提升12.0%，比朴素MTL基线提升3.4%；SE任务的SSNR比SE预处理基线提升28.2%，比朴素MTL基线提升20.0%

Conclusion: Sparse MERIT在噪声环境下为情感识别和增强任务提供了鲁棒且可泛化的性能，有效解决了多任务学习中的表征冲突问题

Abstract: Speech emotion recognition (SER) plays a critical role in building
emotion-aware speech systems, but its performance degrades significantly under
noisy conditions. Although speech enhancement (SE) can improve robustness, it
often introduces artifacts that obscure emotional cues and adds computational
overhead to the pipeline. Multi-task learning (MTL) offers an alternative by
jointly optimizing SE and SER tasks. However, conventional shared-backbone
models frequently suffer from gradient interference and representational
conflicts between tasks. To address these challenges, we propose the Sparse
Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a
flexible MTL framework that applies frame-wise expert routing over
self-supervised speech representations. Sparse MERIT incorporates task-specific
gating networks that dynamically select from a shared pool of experts for each
frame, enabling parameter-efficient and task-adaptive representation learning.
Experiments on the MSP-Podcast corpus show that Sparse MERIT consistently
outperforms baseline models on both SER and SE tasks. Under the most
challenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT
improves SER F1-macro by an average of 12.0% over a baseline relying on a SE
pre-processing strategy, and by 3.4% over a naive MTL baseline, with
statistical significance on unseen noise conditions. For SE, Sparse MERIT
improves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and
by 20.0% over the naive MTL baseline. These results demonstrate that Sparse
MERIT provides robust and generalizable performance for both emotion
recognition and enhancement tasks in noisy environments.

</details>


### [14] [Audio Deepfake Verification](https://arxiv.org/abs/2509.08476)
*Li Wang,Junyi Ao,Linyong Gan,Yuancheng Wang,Xueyao Zhang,Zhizheng Wu*

Main category: eess.AS

TL;DR: 本文提出了音频深度伪造验证(ADV)任务和Audity双分支架构，通过音频结构和生成伪影两个维度提取特征，在开放集深度伪造源追踪方面取得优异性能


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展，仅对音频进行真假二元判断已不能满足实际需求，准确确定具体的深度伪造方法变得至关重要

Method: 提出Audity双分支架构，从音频结构和生成伪影两个维度提取深度伪造特征，解决现有方法在闭集场景下的局限性

Result: 实验结果表明双分支Audity架构优于任何单分支配置，同时在深度伪造检测和验证任务中都能取得优异性能

Conclusion: 该研究有效解决了开放集深度伪造源追踪问题，双分支特征提取方法为音频深度伪造验证提供了有效解决方案

Abstract: With the rapid development of deepfake technology, simply making a binary
judgment of true or false on audio is no longer sufficient to meet practical
needs. Accurately determining the specific deepfake method has become crucial.
This paper introduces the Audio Deepfake Verification (ADV) task, effectively
addressing the limitations of existing deepfake source tracing methods in
closed-set scenarios, aiming to achieve open-set deepfake source tracing.
Meanwhile, the Audity dual-branch architecture is proposed, extracting deepfake
features from two dimensions: audio structure and generation artifacts.
Experimental results show that the dual-branch Audity architecture outperforms
any single-branch configuration, and it can simultaneously achieve excellent
performance in both deepfake detection and verification tasks.

</details>


### [15] [Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer Layer Caching](https://arxiv.org/abs/2509.08696)
*Siratish Sakpiboonchit*

Main category: eess.AS

TL;DR: 通过在变换器层中应用SmoothCache选择性缓存机制，加速源于扩散变换器的文本转语音模型推理过程，在保持语音质量的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决扩散变换器TTS模型在推理过程中存在的计算重复性问题，通过缓存机制减少冗余计算，提高推理速度而不需改变模型结构或重新训练。

Method: 在F5-TTS架构中集成SmoothCache机制，缓存自注意力和前传网络层的输出，通过标定阶段分析时间步间的L1相对误差来选择最优缓存策略，并采用统一的缓存时间表处理层间依赖性。

Result: 实验结果显示，在高去噪步数时缓存可以减少推理时间且不影响输出质量，而在低步数缓存则会像减少总去噪步数一样对语音质量产生负面影响。对比实验证明选择性缓存在高步数配置下具有显著优势。

Conclusion: 这项工作证明了变换器层缓存是一种有效的方法，可以在不需要改变模型结构或重新训练的情况下，优化基于扩散变换器的TTS模型的计算效率。

Abstract: This paper presents a method to accelerate the inference process of diffusion
transformer (DiT)-based text-to-speech (TTS) models by applying a selective
caching mechanism to transformer layers. Specifically, I integrate SmoothCache
into the F5-TTS architecture, focusing on caching outputs of self-attention and
feed-forward network layers to reduce redundant computations during the
denoising process. A calibration phase is introduced to analyze L1 relative
errors between timesteps, guiding the selection of cache schedules that
minimize quality degradation. To address the problem of inter-layer dependency,
a unified caching schedule is adopted, applying the cache pattern derived from
self-attention layers to both layer types. Experiments on LibriSpeech-PC and
Seed-TTS datasets evaluate various cache thresholds and denoising step
configurations. Results show that caching at higher denoising steps reduces
inference time without compromising output quality, whereas caching at lower
steps can negatively impact synthesis quality similarly to reducing the total
number of denoising steps. Objective and subjective metrics confirm the
effectiveness of SmoothCache in maintaining performance while improving
computational efficiency. Comparisons between cached inference and reduced-step
inference further highlight the benefits of selective caching, especially under
high-step configurations. This work demonstrates that transformer layer caching
is a practical solution for optimizing diffusion transformer-based TTS models
without requiring architectural changes or retraining. Example inference
results can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [16] [LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models](https://arxiv.org/abs/2509.08031)
*Sidharth Surapaneni,Hoang Nguyen,Jash Mehta,Aman Tiwari,Oluwanifemi Bamgbose,Akshay Kalkunte,Sai Rajeswar,Sathwik Tejaswi Madhusudhan*

Main category: cs.SD

TL;DR: LALM-Eval是一个高效的大音频语言模型评估框架，解决了现有工具包处理慢、提示不一致和任务覆盖窄的问题，实现了127%的速度提升，并引入了新的评估类别。


<details>
  <summary>Details</summary>
Motivation: 当前大音频语言模型评估工具存在处理效率低、提示不一致和任务覆盖范围有限的问题，限制了公平比较和系统评估。

Method: 开发了LALM-Eval框架，通过优化的批处理和并行执行实现高效评估，提供标准化提示协议，并引入LLM-自适应对话分析和口语推理两个新评估类别。

Result: 在380多个任务上的评估显示，现有LALM在时间理解和复杂口语推理任务上存在显著差距，指令模态缺乏标准化导致性能差异高达9.5个百分点。

Conclusion: LALM-Eval提供了实用的评估工具和模型局限性洞察，推动了大音频语言模型的系统化发展。

Abstract: Large Audio Language Models (LALMs) are rapidly advancing, but evaluating
them remains challenging due to inefficient toolkits that limit fair comparison
and systematic assessment. Current frameworks suffer from three critical
issues: slow processing that bottlenecks large-scale studies, inconsistent
prompting that hurts reproducibility, and narrow task coverage that misses
important audio reasoning capabilities. We introduce LALM-Eval, an efficient
and comprehensive evaluation framework for LALMs. Our system achieves a speedup
of up to 127% over existing toolkits through optimized batch processing and
parallel execution, enabling large-scale evaluations previously impractical. We
provide standardized prompting protocols and flexible configurations for fair
model comparison across diverse scenarios. Additionally, we introduce two new
evaluation categories: LLM-Adaptive Diarization for temporal audio
understanding and Spoken Language Reasoning for complex audio-based cognitive
tasks. Through evaluation across 380+ tasks, we reveal significant gaps in
current LALMs, particularly in temporal understanding and complex spoken
language reasoning tasks. Our findings also highlight a lack of standardization
in instruction modality existent across audio benchmarks, which can lead up
performance differences up to 9.5 absolute points on the challenging complex
instruction following downstream tasks. LALM-Eval provides both practical
evaluation tools and insights into model limitations, advancing systematic LALM
development.

</details>


### [17] [Segment Transformer: AI-Generated Music Detection via Music Structural Analysis](https://arxiv.org/abs/2509.08283)
*Yumin Kim,Seonghyeon Go*

Main category: cs.SD

TL;DR: 提出基于音乐片段结构分析的AI生成音乐检测方法，通过集成预训练模型提取音乐特征，并开发段变换器分析长音频的段间关系，在短音频和完整音频检测中均取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 随着音频和音乐生成技术的发展，AI生成音乐(AIGM)的版权归属和作者身份不明确，难以区分AI生成与人类创作的音乐，需要提高AIGM检测的准确性。

Method: 集成多种预训练模型（包括自监督学习模型和音频效果编码器）提取短音频片段的音乐特征，使用基于变换器的框架；针对长音频开发段变换器，将音乐分段并学习段间关系。

Result: 在FakeMusicCaps和SONICS数据集上，短音频和完整音频检测实验均获得高准确率。

Conclusion: 将片段级音乐特征整合到长时域分析中，能有效提升AIGM检测系统的性能和鲁棒性。

Abstract: Audio and music generation systems have been remarkably developed in the
music information retrieval (MIR) research field. The advancement of these
technologies raises copyright concerns, as ownership and authorship of
AI-generated music (AIGM) remain unclear. Also, it can be difficult to
determine whether a piece was generated by AI or composed by humans clearly. To
address these challenges, we aim to improve the accuracy of AIGM detection by
analyzing the structural patterns of music segments. Specifically, to extract
musical features from short audio clips, we integrated various pre-trained
models, including self-supervised learning (SSL) models or an audio effect
encoder, each within our suggested transformer-based framework. Furthermore,
for long audio, we developed a segment transformer that divides music into
segments and learns inter-segment relationships. We used the FakeMusicCaps and
SONICS datasets, achieving high accuracy in both the short-audio and full-audio
detection experiments. These findings suggest that integrating segment-level
musical features into long-range temporal analysis can effectively enhance both
the performance and robustness of AIGM detection systems.

</details>


### [18] [LatentVoiceGrad: Nonparallel Voice Conversion with Latent Diffusion/Flow-Matching Models](https://arxiv.org/abs/2509.08379)
*Hirokazu Kameoka,Takuhiro Kaneko,Kou Tanaka,Yuto Kondo*

Main category: cs.SD

TL;DR: VoiceGrad的改进版，通过潜空间反向滴渏模型和流匹配模型提升语音转换质量和速度


<details>
  <summary>Details</summary>
Motivation: 解决原有VoiceGrad在音频质量和转换速度方面的不足，需要提升语音质量并加快转换过程

Method: 引入潜空反向滴渏模型在自动编码器瓶颈层进行反向滴渏，并使用流匹配模型作为滴渏模型的替代方案以加速转换

Result: 实验结果显示语音质量得到显著提升，转换速度也大大加快

Conclusion: 改进的VoiceGrad方案在保持转换质量的同时有效提高了转换速度，为非平行语音转换提供了更好的解决方案

Abstract: Previously, we introduced VoiceGrad, a nonparallel voice conversion (VC)
technique enabling mel-spectrogram conversion from source to target speakers
using a score-based diffusion model. The concept involves training a score
network to predict the gradient of the log density of mel-spectrograms from
various speakers. VC is executed by iteratively adjusting an input
mel-spectrogram until resembling the target speaker's. However, challenges
persist: audio quality needs improvement, and conversion is slower compared to
modern VC methods designed to operate at very high speeds. To address these, we
introduce latent diffusion models into VoiceGrad, proposing an improved version
with reverse diffusion in the autoencoder bottleneck. Additionally, we propose
using a flow matching model as an alternative to the diffusion model to further
speed up the conversion process without compromising the conversion quality.
Experimental results show enhanced speech quality and accelerated conversion
compared to the original.

</details>


### [19] [Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition](https://arxiv.org/abs/2509.08454)
*Yujian Ma,Jinqiu Sang,Ruizhe Li*

Main category: cs.SD

TL;DR: 对Whisper语音模型进行LoRA微调机制的可解释性研究，揭示了延迟专业化和前向对齐、后向分化的动态机制


<details>
  <summary>Details</summary>
Motivation: 大型预训练语音模型如Whisper具有强大泛化能力，但参数高效微调方法LoRA在语音任务中的工作机制尚不明确，需要进行系统性机制解释研究

Method: 使用层贡献探测、logit-lens检查、SVD和CKA表示相似性分析等工具，在Whisper编码器上进行语音情感识别任务的LoRA机制分析

Result: 发现了两个关键机制：延迟专业化过程（早期层保留通用特征，后期层整合任务特定信息）和前向对齐、后向分化的矩阵动态

Conclusion: 阐明了LoRA如何重塑编码器层次结构，为设计高效可解释的大型语音模型适配策略提供了实证见解和机制理解

Abstract: Large pre-trained speech models such as Whisper offer strong generalization
but pose significant challenges for resource-efficient adaptation. Low-Rank
Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method,
yet its underlying mechanisms in speech tasks remain poorly understood. In this
work, we conduct the first systematic mechanistic interpretability study of
LoRA within the Whisper encoder for speech emotion recognition (SER). Using a
suite of analytical tools, including layer contribution probing, logit-lens
inspection, and representational similarity via singular value decomposition
(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a
delayed specialization process that preserves general features in early layers
before consolidating task-specific information, and a forward alignment,
backward differentiation dynamic between LoRA's matrices. Our findings clarify
how LoRA reshapes encoder hierarchies, providing both empirical insights and a
deeper mechanistic understanding for designing efficient and interpretable
adaptation strategies in large speech models.

</details>


### [20] [Explainability of CNN Based Classification Models for Acoustic Signal](https://arxiv.org/abs/2509.08717)
*Zubair Faruqui,Mackenzie S. McIntire,Rahul Dubey,Jay McEntee*

Main category: cs.SD

TL;DR: 该论文研究了可解释人工智能(XAI)在生物声学中的应用，通过结合多种XAI技术来解释鸟类声音分类CNN模型的决策过程，取得了94.8%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 虽然XAI在声学领域应用日益广泛，但在涉及生物音频信号分析的生物声学领域仍相对未被充分探索，需要研究如何解释深度学习模型在生物声学分类任务中的决策。

Method: 将鸟类音频录音转换为频谱图图像，使用深度卷积神经网络进行分类，并应用模型无关(LIME、SHAP)和模型特定(DeepLIFT、Grad-CAM)的XAI技术来解释模型预测。

Result: CNN模型达到94.8%的分类准确率，不同XAI技术产生互补的解释，结合使用时能提供更完整和可解释的模型决策洞察。

Conclusion: 研究表明结合多种XAI技术可以提高模型的可信度和互操作性，不仅在声学信号分析中，在其他领域特定任务中也具有广泛适用性。

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a critical tool for
interpreting the predictions of complex deep learning models. While XAI has
been increasingly applied in various domains within acoustics, its use in
bioacoustics, which involves analyzing audio signals from living organisms,
remains relatively underexplored. In this paper, we investigate the
vocalizations of a bird species with strong geographic variation throughout its
range in North America. Audio recordings were converted into spectrogram images
and used to train a deep Convolutional Neural Network (CNN) for classification,
achieving an accuracy of 94.8\%. To interpret the model's predictions, we
applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,
Grad-CAM) XAI techniques. These techniques produced different but complementary
explanations, and when their explanations were considered together, they
provided more complete and interpretable insights into the model's
decision-making. This work highlights the importance of using a combination of
XAI techniques to improve trust and interoperability, not only in broader
acoustics signal analysis but also argues for broader applicability in
different domain specific tasks.

</details>


### [21] [PianoVAM: A Multimodal Piano Performance Dataset](https://arxiv.org/abs/2509.08800)
*Yonghyun Kim,Junhyung Park,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: PianoVAM是一个包含视频、音频、MIDI、手部关键点、指法标注和丰富元数据的钢琴演奏数据集，通过Disklavier钢琴采集业余钢琴家的日常练习数据，支持多模态音乐信息检索研究。


<details>
  <summary>Details</summary>
Motivation: 音乐表演的多模态特性促使MIR社区对音频以外数据的需求增加，需要包含视频、音频、MIDI等多种模态的钢琴演奏数据集来支持相关研究。

Method: 使用Disklavier钢琴采集业余钢琴家的日常练习数据，包括同步的音频、MIDI和俯视视角视频；通过预训练的手部姿态估计模型提取手部关键点，采用半自动化指法标注算法进行指法标注。

Result: 成功构建了PianoVAM数据集，包含多模态钢琴演奏数据；提出了基于视频手部关键点的指法标注方法；在音频转录和音视频转录任务上提供了基准测试结果。

Conclusion: PianoVAM数据集为多模态音乐信息检索研究提供了有价值的资源，支持钢琴转录、指法分析等多种应用，并展示了跨模态对齐的挑战和解决方案。

Abstract: The multimodal nature of music performance has driven increasing interest in
data beyond the audio domain within the music information retrieval (MIR)
community. This paper introduces PianoVAM, a comprehensive piano performance
dataset that includes videos, audio, MIDI, hand landmarks, fingering labels,
and rich metadata. The dataset was recorded using a Disklavier piano, capturing
audio and MIDI from amateur pianists during their daily practice sessions,
alongside synchronized top-view videos in realistic and varied performance
conditions. Hand landmarks and fingering labels were extracted using a
pretrained hand pose estimation model and a semi-automated fingering annotation
algorithm. We discuss the challenges encountered during data collection and the
alignment process across different modalities. Additionally, we describe our
fingering annotation method based on hand landmarks extracted from videos.
Finally, we present benchmarking results for both audio-only and audio-visual
piano transcription using the PianoVAM dataset and discuss additional potential
applications.

</details>

{"id": "2509.08173", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08173", "abs": "https://arxiv.org/abs/2509.08173", "authors": ["Hao Yen", "Pin-Jui Ku", "Sabato Marco Siniscalchi", "Chin-Hui Lee"], "title": "A Bottom-up Framework with Language-universal Speech Attribute Modeling for Syllable-based ASR", "comment": null, "summary": "We propose a bottom-up framework for automatic speech recognition (ASR) in\nsyllable-based languages by unifying language-universal articulatory attribute\nmodeling with syllable-level prediction. The system first recognizes sequences\nor lattices of articulatory attributes that serve as a language-universal,\ninterpretable representation of pronunciation, and then transforms them into\nsyllables through a structured knowledge integration process. We introduce two\nevaluation metrics, namely Pronunciation Error Rate (PrER) and Syllable Homonym\nError Rate (SHER), to evaluate the model's ability to capture pronunciation and\nhandle syllable ambiguities. Experimental results on the AISHELL-1 Mandarin\ncorpus demonstrate that the proposed bottom-up framework achieves competitive\nperformance and exhibits better robustness under low-resource conditions\ncompared to the direct syllable prediction model. Furthermore, we investigate\nthe zero-shot cross-lingual transferability on Japanese and demonstrate\nsignificant improvements over character- and phoneme-based baselines by 40%\nerror rate reduction.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u901a\u7528\u53d9\u8ff0\u5c5e\u6027\u6a21\u578b\u7684\u5e95\u5c42\u5411\u4e0a\u8bed\u97f3\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u96c6\u6210\u5c06\u53d9\u8ff0\u5c5e\u6027\u8f6c\u6362\u4e3a\u97f3\u8282\uff0c\u5728\u4e2d\u6587\u548c\u65e5\u8bed\u4e0a\u90fd\u53d6\u5f97\u4e86\u8f83\u597d\u6548\u679c", "motivation": "\u89e3\u51b3\u97f3\u8282\u57fa\u8bed\u8a00\u4e2d\u8bed\u97f3\u8bc6\u522b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u8bed\u8a00\u901a\u7528\u7684\u53d9\u8ff0\u5c5e\u6027\u6a21\u578b\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5065\u58c1\u6027\uff0c\u5e76\u652f\u6301\u8de8\u8bed\u8a00\u7684\u96f6\u6837\u672c\u8f6c\u79fb", "method": "\u9996\u5148\u8bc6\u522b\u53d9\u8ff0\u5c5e\u6027\u5e8f\u5217\u6210\u77b9\u9635\uff0c\u7136\u540e\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u96c6\u6210\u5c06\u5176\u8f6c\u6362\u4e3a\u97f3\u8282\uff0c\u5e76\u4e14\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u8bc4\u4f30\u6307\u6807PrER\u548cSHER", "result": "\u5728AISHELL-1\u666e\u901a\u8bdd\u8bed\u6599\u5e93\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\u80fd\uff0c\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u66f4\u5065\u58c1\uff0c\u5728\u65e5\u8bed\u96f6\u6837\u672c\u8f6c\u79fb\u4e2d\u6bd4\u57fa\u7ebf\u9510\u51cf40%\u9519\u8bef\u7387", "conclusion": "\u8be5\u5e95\u5c42\u5411\u4e0a\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u53d9\u8ff0\u5c5e\u6027\u6a21\u578b\u548c\u97f3\u8282\u9884\u6d4b\uff0c\u5728\u97f3\u8282\u57fa\u8bed\u8a00\u4e2d\u663e\u793a\u51fa\u4f18\u79f0\u6027\u80fd\u548c\u8de8\u8bed\u8a00\u8f6c\u79fb\u80fd\u529b"}}
{"id": "2509.08292", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.08292", "abs": "https://arxiv.org/abs/2509.08292", "authors": ["Ryo Sato", "Chiho Haruta", "Nobuhiko Hiruma", "Keisuke Imoto"], "title": "Context-Aware Query Refinement for Target Sound Extraction: Handling Partially Matched Queries", "comment": "Accepted to IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "summary": "Target sound extraction (TSE) is the task of extracting a target sound\nspecified by a query from an audio mixture. Much prior research has focused on\nthe problem setting under the Fully Matched Query (FMQ) condition, where the\nquery specifies only active sounds present in the mixture. However, in\nreal-world scenarios, queries may include inactive sounds that are not present\nin the mixture. This leads to scenarios such as the Fully Unmatched Query (FUQ)\ncondition, where only inactive sounds are specified in the query, and the\nPartially Matched Query (PMQ) condition, where both active and inactive sounds\nare specified. Among these conditions, the performance degradation under the\nPMQ condition has been largely overlooked. To achieve robust TSE under the PMQ\ncondition, we propose context-aware query refinement. This method eliminates\ninactive classes from the query during inference based on the estimated sound\nclass activity. Experimental results demonstrate that while conventional\nmethods suffer from performance degradation under the PMQ condition, the\nproposed method effectively mitigates this degradation and achieves high\nrobustness under diverse query conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7cbe\u7ec6\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u90e8\u5206\u5339\u914d\u67e5\u8be2\u6761\u4ef6\u4e0b\u76ee\u6807\u97f3\u9891\u63d0\u53d6\u7684\u6027\u80fd\u6c1b\u6da1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u97f3\u9891\u63d0\u53d6\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b8c\u5168\u5339\u914d\u67e5\u8be2\u6761\u4ef6\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u67e5\u8be2\u53ef\u80fd\u5305\u542b\u4e0d\u6d3b\u8dc3\u7684\u97f3\u9891\u7c7b\u522b\uff0c\u5bfc\u81f4\u90e8\u5206\u5339\u914d\u67e5\u8be2\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7cbe\u7ec6\u5316\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6839\u636e\u4f30\u8ba1\u7684\u97f3\u9891\u7c7b\u522b\u6d3b\u8dc3\u60c5\u51b5\u6d88\u9664\u67e5\u8be2\u4e2d\u7684\u975e\u6d3b\u8dc3\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u90e8\u5206\u5339\u914d\u67e5\u8be2\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u672c\u6587\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u8fd9\u79cd\u6027\u80fd\u6c1b\u6da1\uff0c\u5728\u591a\u79cd\u67e5\u8be2\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u7a33\u5065\u6027\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u67e5\u8be2\u7cbe\u7ec6\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u76ee\u6807\u97f3\u9891\u63d0\u53d6\u7cfb\u7edf\u5728\u90e8\u5206\u5339\u914d\u67e5\u8be2\u6761\u4ef6\u4e0b\u7684\u9ad8\u7a33\u5065\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2509.08344", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08344", "abs": "https://arxiv.org/abs/2509.08344", "authors": ["Mana Ihori", "Taiga Yamane", "Naotaka Kawata", "Naoki Makishima", "Tomohiro Tanaka", "Satoshi Suzuki", "Shota Orihashi", "Ryo Masumura"], "title": "Few-shot Personalization via In-Context Learning for Speech Emotion Recognition based on Speech-Language Model", "comment": "Accepted by ASRU 2025", "summary": "This paper proposes a personalization method for speech emotion recognition\n(SER) through in-context learning (ICL). Since the expression of emotions\nvaries from person to person, speaker-specific adaptation is crucial for\nimproving the SER performance. Conventional SER methods have been personalized\nusing emotional utterances of a target speaker, but it is often difficult to\nprepare utterances corresponding to all emotion labels in advance. Our idea to\novercome this difficulty is to obtain speaker characteristics by conditioning a\nfew emotional utterances of the target speaker in ICL-based inference. ICL is a\nmethod to perform unseen tasks by conditioning a few input-output examples\nthrough inference in large language models (LLMs). We meta-train a\nspeech-language model extended from the LLM to learn how to perform\npersonalized SER via ICL. Experimental results using our newly collected SER\ndataset demonstrate that the proposed method outperforms conventional methods.", "AI": {"tldr": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u76ee\u6807\u8bb2\u8bdd\u8005\u60c5\u611f\u8bed\u97f3\u8fdb\u884c\u9002\u914d\uff0c\u63d0\u5347\u8bc6\u522b\u6027\u80fd", "motivation": "\u7531\u4e8e\u4e0d\u540c\u8bb2\u8bdd\u8005\u7684\u60c5\u611f\u8868\u8fbe\u65b9\u5f0f\u5b58\u5728\u5dee\u5f02\uff0c\u4f20\u7edf\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u9700\u8981\u9884\u5148\u51c6\u5907\u6240\u6709\u60c5\u611f\u6807\u7b7e\u7684\u8bed\u97f3\u6837\u672c\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u56f0\u96be", "method": "\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u8bed\u97f3-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5143\u8bad\u7ec3\u5b66\u4e60\u5982\u4f55\u5229\u7528\u5c11\u91cf\u76ee\u6807\u8bb2\u8bdd\u8005\u60c5\u611f\u8bed\u97f3\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u8bed\u97f3\u60c5\u611f\u8bc6\u522b", "result": "\u5728\u65b0\u6536\u96c6\u7684SER\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u8bc6\u522b\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u7684\u4e2a\u6027\u5316\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u9884\u5148\u51c6\u5907\u6240\u6709\u60c5\u611f\u6807\u7b7e\u6837\u672c\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u6027\u80fd"}}
{"id": "2509.08470", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08470", "abs": "https://arxiv.org/abs/2509.08470", "authors": ["Jing-Tong Tzeng", "Carlos Busso", "Chi-Chun Lee"], "title": "Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition", "comment": null, "summary": "Speech emotion recognition (SER) plays a critical role in building\nemotion-aware speech systems, but its performance degrades significantly under\nnoisy conditions. Although speech enhancement (SE) can improve robustness, it\noften introduces artifacts that obscure emotional cues and adds computational\noverhead to the pipeline. Multi-task learning (MTL) offers an alternative by\njointly optimizing SE and SER tasks. However, conventional shared-backbone\nmodels frequently suffer from gradient interference and representational\nconflicts between tasks. To address these challenges, we propose the Sparse\nMixture-of-Experts Representation Integration Technique (Sparse MERIT), a\nflexible MTL framework that applies frame-wise expert routing over\nself-supervised speech representations. Sparse MERIT incorporates task-specific\ngating networks that dynamically select from a shared pool of experts for each\nframe, enabling parameter-efficient and task-adaptive representation learning.\nExperiments on the MSP-Podcast corpus show that Sparse MERIT consistently\noutperforms baseline models on both SER and SE tasks. Under the most\nchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT\nimproves SER F1-macro by an average of 12.0% over a baseline relying on a SE\npre-processing strategy, and by 3.4% over a naive MTL baseline, with\nstatistical significance on unseen noise conditions. For SE, Sparse MERIT\nimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and\nby 20.0% over the naive MTL baseline. These results demonstrate that Sparse\nMERIT provides robust and generalizable performance for both emotion\nrecognition and enhancement tasks in noisy environments.", "AI": {"tldr": "\u63d0\u51faSparse MERIT\u6846\u67b6\uff0c\u901a\u8fc7\u5e27\u7ea7\u4e13\u5bb6\u8def\u7531\u673a\u5236\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8bed\u97f3\u589e\u5f3a\u548c\u60c5\u611f\u8bc6\u522b\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e24\u4e2a\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f20\u7edf\u8bed\u97f3\u589e\u5f3a\u65b9\u6cd5\u4f1a\u5f15\u5165\u4f2a\u5f71\u63a9\u76d6\u60c5\u611f\u7ebf\u7d22\u4e14\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u5b58\u5728\u68af\u5ea6\u5e72\u6270\u548c\u8868\u5f81\u51b2\u7a81\u95ee\u9898", "method": "Sparse MERIT\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u95e8\u63a7\u7f51\u7edc\u52a8\u6001\u9009\u62e9\u5171\u4eab\u4e13\u5bb6\u6c60\u4e2d\u7684\u4e13\u5bb6\u8fdb\u884c\u5e27\u7ea7\u8def\u7531\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u548c\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u8868\u5f81\u5b66\u4e60", "result": "\u5728MSP-Podcast\u8bed\u6599\u5e93\u4e0a\uff0c-5dB SNR\u6761\u4ef6\u4e0b\uff1aSER F1-macro\u6bd4SE\u9884\u5904\u7406\u57fa\u7ebf\u5e73\u5747\u63d0\u534712.0%\uff0c\u6bd4\u6734\u7d20MTL\u57fa\u7ebf\u63d0\u53473.4%\uff1bSE\u4efb\u52a1\u7684SSNR\u6bd4SE\u9884\u5904\u7406\u57fa\u7ebf\u63d0\u534728.2%\uff0c\u6bd4\u6734\u7d20MTL\u57fa\u7ebf\u63d0\u534720.0%", "conclusion": "Sparse MERIT\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4e3a\u60c5\u611f\u8bc6\u522b\u548c\u589e\u5f3a\u4efb\u52a1\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u8868\u5f81\u51b2\u7a81\u95ee\u9898"}}
{"id": "2509.08031", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08031", "abs": "https://arxiv.org/abs/2509.08031", "authors": ["Sidharth Surapaneni", "Hoang Nguyen", "Jash Mehta", "Aman Tiwari", "Oluwanifemi Bamgbose", "Akshay Kalkunte", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan"], "title": "LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models", "comment": null, "summary": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating\nthem remains challenging due to inefficient toolkits that limit fair comparison\nand systematic assessment. Current frameworks suffer from three critical\nissues: slow processing that bottlenecks large-scale studies, inconsistent\nprompting that hurts reproducibility, and narrow task coverage that misses\nimportant audio reasoning capabilities. We introduce LALM-Eval, an efficient\nand comprehensive evaluation framework for LALMs. Our system achieves a speedup\nof up to 127% over existing toolkits through optimized batch processing and\nparallel execution, enabling large-scale evaluations previously impractical. We\nprovide standardized prompting protocols and flexible configurations for fair\nmodel comparison across diverse scenarios. Additionally, we introduce two new\nevaluation categories: LLM-Adaptive Diarization for temporal audio\nunderstanding and Spoken Language Reasoning for complex audio-based cognitive\ntasks. Through evaluation across 380+ tasks, we reveal significant gaps in\ncurrent LALMs, particularly in temporal understanding and complex spoken\nlanguage reasoning tasks. Our findings also highlight a lack of standardization\nin instruction modality existent across audio benchmarks, which can lead up\nperformance differences up to 9.5 absolute points on the challenging complex\ninstruction following downstream tasks. LALM-Eval provides both practical\nevaluation tools and insights into model limitations, advancing systematic LALM\ndevelopment.", "AI": {"tldr": "LALM-Eval\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u5305\u5904\u7406\u6162\u3001\u63d0\u793a\u4e0d\u4e00\u81f4\u548c\u4efb\u52a1\u8986\u76d6\u7a84\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86127%\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u7c7b\u522b\u3002", "motivation": "\u5f53\u524d\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5de5\u5177\u5b58\u5728\u5904\u7406\u6548\u7387\u4f4e\u3001\u63d0\u793a\u4e0d\u4e00\u81f4\u548c\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86LALM-Eval\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u6279\u5904\u7406\u548c\u5e76\u884c\u6267\u884c\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63d0\u793a\u534f\u8bae\uff0c\u5e76\u5f15\u5165LLM-\u81ea\u9002\u5e94\u5bf9\u8bdd\u5206\u6790\u548c\u53e3\u8bed\u63a8\u7406\u4e24\u4e2a\u65b0\u8bc4\u4f30\u7c7b\u522b\u3002", "result": "\u5728380\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709LALM\u5728\u65f6\u95f4\u7406\u89e3\u548c\u590d\u6742\u53e3\u8bed\u63a8\u7406\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u6307\u4ee4\u6a21\u6001\u7f3a\u4e4f\u6807\u51c6\u5316\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u9ad8\u8fbe9.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LALM-Eval\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u6a21\u578b\u5c40\u9650\u6027\u6d1e\u5bdf\uff0c\u63a8\u52a8\u4e86\u5927\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u5316\u53d1\u5c55\u3002"}}
{"id": "2509.07990", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07990", "abs": "https://arxiv.org/abs/2509.07990", "authors": ["Charan Gajjala Chenchu", "Kinam Kim", "Gao Lu", "Zia Ud Din"], "title": "Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction", "comment": null, "summary": "Human-robot collaboration (HRC) in the construction industry depends on\nprecise and prompt recognition of human motion intentions and actions by robots\nto maximize safety and workflow efficiency. There is a research gap in\ncomparing data modalities, specifically signals and videos, for motion\nintention recognition. To address this, the study leverages deep learning to\nassess two different modalities in recognizing workers' motion intention at the\nearly stage of movement in drywall installation tasks. The Convolutional Neural\nNetwork - Long Short-Term Memory (CNN-LSTM) model utilizing surface\nelectromyography (sEMG) data achieved an accuracy of around 87% with an average\ntime of 0.04 seconds to perform prediction on a sample input. Meanwhile, the\npre-trained Video Swin Transformer combined with transfer learning harnessed\nvideo sequences as input to recognize motion intention and attained an accuracy\nof 94% but with a longer average time of 0.15 seconds for a similar prediction.\nThis study emphasizes the unique strengths and trade-offs of both data formats,\ndirecting their systematic deployments to enhance HRC in real-world\nconstruction projects.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6bd4\u8f83\u4e86\u4f53\u8868\u7535\u56fe\uff08sEMG\uff09\u4fe1\u53f7\u548c\u89c6\u9891\u4e24\u79cd\u6570\u636e\u6a21\u6001\u5728\u5efa\u7b51\u884c\u4e1a\u4eba\u673a\u534f\u4f5c\u4e2d\u8bc6\u522b\u4eba\u7c7b\u8fd0\u52a8\u610f\u56fe\u7684\u6027\u80fd\uff0csEMG\u6a21\u578b\u51c6\u786e\u738787%\u901f\u5ea60.04\u79d2\uff0c\u89c6\u9891\u6a21\u578b\u51c6\u786e\u738794%\u4f46\u97000.15\u79d2\u3002", "motivation": "\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u4eba\u673a\u534f\u4f5c\u4e2d\u7f3a\u4e4f\u4e0d\u540c\u6570\u636e\u6a21\u6001\uff08\u4fe1\u53f7\u4e0e\u89c6\u9891\uff09\u5728\u8fd0\u52a8\u610f\u56fe\u8bc6\u522b\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aCNN-LSTM\u6a21\u578b\u5904\u7406sEMG\u6570\u636e\uff0cVideo Swin Transformer\u6a21\u578b\u5904\u7406\u89c6\u9891\u5e8f\u5217\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u8bc6\u522b\u5e72\u5899\u5b89\u88c5\u4efb\u52a1\u4e2d\u7684\u65f6\u671f\u8fd0\u52a8\u610f\u56fe\u3002", "result": "sEMG\u6a21\u578b\u51c6\u786e\u738787%\u3001\u9884\u6d4b\u65f6\u95f40.04\u79d2\uff1b\u89c6\u9891\u6a21\u578b\u51c6\u786e\u738794%\u3001\u9884\u6d4b\u65f6\u95f40.15\u79d2\u3002\u4e24\u79cd\u6a21\u6001\u5404\u6709\u4f18\u52bf\uff1asEMG\u901f\u5ea6\u66f4\u5feb\uff0c\u89c6\u9891\u51c6\u786e\u6027\u66f4\u9ad8\u3002", "conclusion": "\u4e0d\u540c\u6570\u636e\u6a21\u6001\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u5177\u6709\u4e92\u8865\u6027\u4f18\u52bf\uff0c\u9700\u8981\u6839\u636e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7cfb\u7edf\u5316\u90e8\u7f72\u4ee5\u63d0\u5347\u5efa\u7b51\u884c\u4e1a\u7684\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002"}}
{"id": "2509.08476", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08476", "abs": "https://arxiv.org/abs/2509.08476", "authors": ["Li Wang", "Junyi Ao", "Linyong Gan", "Yuancheng Wang", "Xueyao Zhang", "Zhizheng Wu"], "title": "Audio Deepfake Verification", "comment": null, "summary": "With the rapid development of deepfake technology, simply making a binary\njudgment of true or false on audio is no longer sufficient to meet practical\nneeds. Accurately determining the specific deepfake method has become crucial.\nThis paper introduces the Audio Deepfake Verification (ADV) task, effectively\naddressing the limitations of existing deepfake source tracing methods in\nclosed-set scenarios, aiming to achieve open-set deepfake source tracing.\nMeanwhile, the Audity dual-branch architecture is proposed, extracting deepfake\nfeatures from two dimensions: audio structure and generation artifacts.\nExperimental results show that the dual-branch Audity architecture outperforms\nany single-branch configuration, and it can simultaneously achieve excellent\nperformance in both deepfake detection and verification tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u9a8c\u8bc1(ADV)\u4efb\u52a1\u548cAudity\u53cc\u5206\u652f\u67b6\u6784\uff0c\u901a\u8fc7\u97f3\u9891\u7ed3\u6784\u548c\u751f\u6210\u4f2a\u5f71\u4e24\u4e2a\u7ef4\u5ea6\u63d0\u53d6\u7279\u5f81\uff0c\u5728\u5f00\u653e\u96c6\u6df1\u5ea6\u4f2a\u9020\u6e90\u8ffd\u8e2a\u65b9\u9762\u53d6\u5f97\u4f18\u5f02\u6027\u80fd", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4ec5\u5bf9\u97f3\u9891\u8fdb\u884c\u771f\u5047\u4e8c\u5143\u5224\u65ad\u5df2\u4e0d\u80fd\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff0c\u51c6\u786e\u786e\u5b9a\u5177\u4f53\u7684\u6df1\u5ea6\u4f2a\u9020\u65b9\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51faAudity\u53cc\u5206\u652f\u67b6\u6784\uff0c\u4ece\u97f3\u9891\u7ed3\u6784\u548c\u751f\u6210\u4f2a\u5f71\u4e24\u4e2a\u7ef4\u5ea6\u63d0\u53d6\u6df1\u5ea6\u4f2a\u9020\u7279\u5f81\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u95ed\u96c6\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u53cc\u5206\u652fAudity\u67b6\u6784\u4f18\u4e8e\u4efb\u4f55\u5355\u5206\u652f\u914d\u7f6e\uff0c\u540c\u65f6\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u90fd\u80fd\u53d6\u5f97\u4f18\u5f02\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u96c6\u6df1\u5ea6\u4f2a\u9020\u6e90\u8ffd\u8e2a\u95ee\u9898\uff0c\u53cc\u5206\u652f\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.08283", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08283", "abs": "https://arxiv.org/abs/2509.08283", "authors": ["Yumin Kim", "Seonghyeon Go"], "title": "Segment Transformer: AI-Generated Music Detection via Music Structural Analysis", "comment": null, "summary": "Audio and music generation systems have been remarkably developed in the\nmusic information retrieval (MIR) research field. The advancement of these\ntechnologies raises copyright concerns, as ownership and authorship of\nAI-generated music (AIGM) remain unclear. Also, it can be difficult to\ndetermine whether a piece was generated by AI or composed by humans clearly. To\naddress these challenges, we aim to improve the accuracy of AIGM detection by\nanalyzing the structural patterns of music segments. Specifically, to extract\nmusical features from short audio clips, we integrated various pre-trained\nmodels, including self-supervised learning (SSL) models or an audio effect\nencoder, each within our suggested transformer-based framework. Furthermore,\nfor long audio, we developed a segment transformer that divides music into\nsegments and learns inter-segment relationships. We used the FakeMusicCaps and\nSONICS datasets, achieving high accuracy in both the short-audio and full-audio\ndetection experiments. These findings suggest that integrating segment-level\nmusical features into long-range temporal analysis can effectively enhance both\nthe performance and robustness of AIGM detection systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u97f3\u4e50\u7247\u6bb5\u7ed3\u6784\u5206\u6790\u7684AI\u751f\u6210\u97f3\u4e50\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u97f3\u4e50\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u6bb5\u53d8\u6362\u5668\u5206\u6790\u957f\u97f3\u9891\u7684\u6bb5\u95f4\u5173\u7cfb\uff0c\u5728\u77ed\u97f3\u9891\u548c\u5b8c\u6574\u97f3\u9891\u68c0\u6d4b\u4e2d\u5747\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u97f3\u9891\u548c\u97f3\u4e50\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0cAI\u751f\u6210\u97f3\u4e50(AIGM)\u7684\u7248\u6743\u5f52\u5c5e\u548c\u4f5c\u8005\u8eab\u4efd\u4e0d\u660e\u786e\uff0c\u96be\u4ee5\u533a\u5206AI\u751f\u6210\u4e0e\u4eba\u7c7b\u521b\u4f5c\u7684\u97f3\u4e50\uff0c\u9700\u8981\u63d0\u9ad8AIGM\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u96c6\u6210\u591a\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5305\u62ec\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u548c\u97f3\u9891\u6548\u679c\u7f16\u7801\u5668\uff09\u63d0\u53d6\u77ed\u97f3\u9891\u7247\u6bb5\u7684\u97f3\u4e50\u7279\u5f81\uff0c\u4f7f\u7528\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u6846\u67b6\uff1b\u9488\u5bf9\u957f\u97f3\u9891\u5f00\u53d1\u6bb5\u53d8\u6362\u5668\uff0c\u5c06\u97f3\u4e50\u5206\u6bb5\u5e76\u5b66\u4e60\u6bb5\u95f4\u5173\u7cfb\u3002", "result": "\u5728FakeMusicCaps\u548cSONICS\u6570\u636e\u96c6\u4e0a\uff0c\u77ed\u97f3\u9891\u548c\u5b8c\u6574\u97f3\u9891\u68c0\u6d4b\u5b9e\u9a8c\u5747\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u5c06\u7247\u6bb5\u7ea7\u97f3\u4e50\u7279\u5f81\u6574\u5408\u5230\u957f\u65f6\u57df\u5206\u6790\u4e2d\uff0c\u80fd\u6709\u6548\u63d0\u5347AIGM\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.08142", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08142", "abs": "https://arxiv.org/abs/2509.08142", "authors": ["Haoran Chang", "Mingzhe Chen", "Huaxia Wang", "Qianqian Zhang"], "title": "Privacy Preserving Semantic Communications Using Vision Language Models: A Segmentation and Generation Approach", "comment": "6 pages, 6 figures, Accepted at IEEE MILCOM 2025", "summary": "Semantic communication has emerged as a promising paradigm for\nnext-generation wireless systems, improving the communication efficiency by\ntransmitting high-level semantic features. However, reliance on unimodal\nrepresentations can degrade reconstruction under poor channel conditions, and\nprivacy concerns of the semantic information attack also gain increasing\nattention. In this work, a privacy-preserving semantic communication framework\nis proposed to protect sensitive content of the image data. Leveraging a\nvision-language model (VLM), the proposed framework identifies and removes\nprivate content regions from input images prior to transmission. A shared\nprivacy database enables semantic alignment between the transmitter and\nreceiver to ensure consistent identification of sensitive entities. At the\nreceiver, a generative module reconstructs the masked regions using learned\nsemantic priors and conditioned on the received text embedding. Simulation\nresults show that generalizes well to unseen image processing tasks, improves\nreconstruction quality at the authorized receiver by over 10% using text\nembedding, and reduces identity leakage to the eavesdropper by more than 50%.", "AI": {"tldr": "\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u79fb\u9664\u56fe\u50cf\u4e2d\u7684\u654f\u611f\u533a\u57df\uff0c\u4f7f\u5f97\u6b63\u5f0f\u63a5\u6536\u65b9\u91cd\u6784\u8d28\u91cf\u63d0\u534710%\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c11\u5076\u53d1\u8005\u8bc6\u522b\u6cc4\u6f0f50%\u4ee5\u4e0a", "motivation": "\u89e3\u51b3\u5355\u6a21\u6001\u8868\u5f81\u5728\u5dee\u901a\u9053\u6761\u4ef6\u4e0b\u91cd\u6784\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u5e94\u5bf9\u8bed\u4e49\u4fe1\u606f\u653b\u51fb\u7684\u9690\u79c1\u98ce\u9669", "method": "\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u548c\u79fb\u9664\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u79c1\u5bc6\u5185\u5bb9\u533a\u57df\uff0c\u901a\u8fc7\u5171\u4eab\u9690\u79c1\u6570\u636e\u5e93\u786e\u4fdd\u53d1\u9001\u65b9\u548c\u63a5\u6536\u65b9\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5728\u63a5\u6536\u7aef\u4f7f\u7528\u751f\u6210\u6a21\u5757\u6839\u636e\u63a5\u6536\u5230\u7684\u6587\u672c\u5d4c\u5165\u548c\u5b66\u4e60\u7684\u8bed\u4e49\u5148\u9a8c\u91cd\u6784\u88ab\u63a9\u76d6\u533a\u57df", "result": "\u6846\u67b6\u5728\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f7f\u5f97\u6b63\u5f0f\u63a5\u6536\u65b9\u91cd\u6784\u8d28\u91cf\u63d0\u534710%\u4ee5\u4e0a\uff0c\u540c\u65f6\u5c06\u5076\u53d1\u8005\u8bc6\u522b\u6cc4\u6f0f\u51cf\u5c1150%\u4ee5\u4e0a", "conclusion": "\u8be5\u9690\u79c1\u4fdd\u62a4\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u6709\u6548\u5730\u4fdd\u62a4\u4e86\u56fe\u50cf\u6570\u636e\u7684\u654f\u611f\u5185\u5bb9\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u901a\u4fe1\u6548\u7387\u548c\u91cd\u6784\u8d28\u91cf\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848"}}
{"id": "2509.08696", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.08696", "abs": "https://arxiv.org/abs/2509.08696", "authors": ["Siratish Sakpiboonchit"], "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer Layer Caching", "comment": "9 pages, 2 tables, 5 figures", "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .", "AI": {"tldr": "\u901a\u8fc7\u5728\u53d8\u6362\u5668\u5c42\u4e2d\u5e94\u7528SmoothCache\u9009\u62e9\u6027\u7f13\u5b58\u673a\u5236\uff0c\u52a0\u901f\u6e90\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u8bed\u97f3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u53d8\u6362\u5668TTS\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u8ba1\u7b97\u91cd\u590d\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7f13\u5b58\u673a\u5236\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u800c\u4e0d\u9700\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u5728F5-TTS\u67b6\u6784\u4e2d\u96c6\u6210SmoothCache\u673a\u5236\uff0c\u7f13\u5b58\u81ea\u6ce8\u610f\u529b\u548c\u524d\u4f20\u7f51\u7edc\u5c42\u7684\u8f93\u51fa\uff0c\u901a\u8fc7\u6807\u5b9a\u9636\u6bb5\u5206\u6790\u65f6\u95f4\u6b65\u95f4\u7684L1\u76f8\u5bf9\u8bef\u5dee\u6765\u9009\u62e9\u6700\u4f18\u7f13\u5b58\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u7edf\u4e00\u7684\u7f13\u5b58\u65f6\u95f4\u8868\u5904\u7406\u5c42\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u9ad8\u53bb\u566a\u6b65\u6570\u65f6\u7f13\u5b58\u53ef\u4ee5\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u4e14\u4e0d\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\uff0c\u800c\u5728\u4f4e\u6b65\u6570\u7f13\u5b58\u5219\u4f1a\u50cf\u51cf\u5c11\u603b\u53bb\u566a\u6b65\u6570\u4e00\u6837\u5bf9\u8bed\u97f3\u8d28\u91cf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u5bf9\u6bd4\u5b9e\u9a8c\u8bc1\u660e\u9009\u62e9\u6027\u7f13\u5b58\u5728\u9ad8\u6b65\u6570\u914d\u7f6e\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u53d8\u6362\u5668\u5c42\u7f13\u5b58\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u6539\u53d8\u6a21\u578b\u7ed3\u6784\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684TTS\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.08379", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08379", "abs": "https://arxiv.org/abs/2509.08379", "authors": ["Hirokazu Kameoka", "Takuhiro Kaneko", "Kou Tanaka", "Yuto Kondo"], "title": "LatentVoiceGrad: Nonparallel Voice Conversion with Latent Diffusion/Flow-Matching Models", "comment": "Submitted to IEEE-TASLP", "summary": "Previously, we introduced VoiceGrad, a nonparallel voice conversion (VC)\ntechnique enabling mel-spectrogram conversion from source to target speakers\nusing a score-based diffusion model. The concept involves training a score\nnetwork to predict the gradient of the log density of mel-spectrograms from\nvarious speakers. VC is executed by iteratively adjusting an input\nmel-spectrogram until resembling the target speaker's. However, challenges\npersist: audio quality needs improvement, and conversion is slower compared to\nmodern VC methods designed to operate at very high speeds. To address these, we\nintroduce latent diffusion models into VoiceGrad, proposing an improved version\nwith reverse diffusion in the autoencoder bottleneck. Additionally, we propose\nusing a flow matching model as an alternative to the diffusion model to further\nspeed up the conversion process without compromising the conversion quality.\nExperimental results show enhanced speech quality and accelerated conversion\ncompared to the original.", "AI": {"tldr": "VoiceGrad\u7684\u6539\u8fdb\u7248\uff0c\u901a\u8fc7\u6f5c\u7a7a\u95f4\u53cd\u5411\u6ef4\u6e0f\u6a21\u578b\u548c\u6d41\u5339\u914d\u6a21\u578b\u63d0\u5347\u8bed\u97f3\u8f6c\u6362\u8d28\u91cf\u548c\u901f\u5ea6", "motivation": "\u89e3\u51b3\u539f\u6709VoiceGrad\u5728\u97f3\u9891\u8d28\u91cf\u548c\u8f6c\u6362\u901f\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u63d0\u5347\u8bed\u97f3\u8d28\u91cf\u5e76\u52a0\u5feb\u8f6c\u6362\u8fc7\u7a0b", "method": "\u5f15\u5165\u6f5c\u7a7a\u53cd\u5411\u6ef4\u6e0f\u6a21\u578b\u5728\u81ea\u52a8\u7f16\u7801\u5668\u74f6\u9888\u5c42\u8fdb\u884c\u53cd\u5411\u6ef4\u6e0f\uff0c\u5e76\u4f7f\u7528\u6d41\u5339\u914d\u6a21\u578b\u4f5c\u4e3a\u6ef4\u6e0f\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u4ee5\u52a0\u901f\u8f6c\u6362", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8bed\u97f3\u8d28\u91cf\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u8f6c\u6362\u901f\u5ea6\u4e5f\u5927\u5927\u52a0\u5feb", "conclusion": "\u6539\u8fdb\u7684VoiceGrad\u65b9\u6848\u5728\u4fdd\u6301\u8f6c\u6362\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u63d0\u9ad8\u4e86\u8f6c\u6362\u901f\u5ea6\uff0c\u4e3a\u975e\u5e73\u884c\u8bed\u97f3\u8f6c\u6362\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.08272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08272", "abs": "https://arxiv.org/abs/2509.08272", "authors": ["Xiangying Li", "Jiankuan Li", "Yong Tang"], "title": "RTR: A Transformer-Based Lossless Crossover with Perfect Phase Alignment", "comment": "ICASSP2025", "summary": "This paper proposes a transformer-based lossless crossover method, termed\nResonant Transformer Router (RTR), which achieves frequency separation while\nensuring perfect phase alignment between low-frequency (LF) and high-frequency\n(HF) channels at the crossover frequency. The core property of RTR is that its\nfrequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so\nthat the original signal can be perfectly reconstructed by linear summation of\nthe two channels. Theoretical derivation and circuit simulations demonstrate\nthat RTR provides superior energy efficiency, phase consistency, and robustness\nagainst component tolerances. Compared with conventional LC crossovers and\ndigital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted\nfiltering solution suitable for high-fidelity audio and communication\nfront-ends.\n  The core theory behind this paper's work, lossless crossover, is based on a\nChinese patent [CN116318117A] developed from the previous research of one of\nthe authors, Jianluan Li. We provide a comprehensive experimental validation of\nthis theory and propose a new extension.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u65e0\u635f\u5206\u9891\u65b9\u6cd5RTR\uff0c\u5b9e\u73b0\u9891\u7387\u5206\u79bb\u540c\u65f6\u4fdd\u8bc1\u4f4e\u9891\u548c\u9ad8\u9891\u901a\u9053\u5728\u5206\u9891\u70b9\u5b8c\u7f8e\u76f8\u4f4d\u5bf9\u9f50\uff0c\u5177\u6709\u7ebf\u6027\u4e92\u8865\u7279\u6027\uff0c\u80fd\u5b8c\u7f8e\u91cd\u6784\u539f\u59cb\u4fe1\u53f7\u3002", "motivation": "\u4f20\u7edfLC\u5206\u9891\u5668\u548c\u6570\u5b57\u6ee4\u6ce2\u5668\u5b58\u5728\u80fd\u91cf\u635f\u8017\u3001\u76f8\u4f4d\u4e0d\u4e00\u81f4\u548c\u5143\u4ef6\u5bb9\u5dee\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4f4e\u635f\u8017\u3001\u4f4e\u5ef6\u8fdf\u7684\u786c\u4ef6\u8f85\u52a9\u6ee4\u6ce2\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8c10\u632f\u53d8\u538b\u5668\u8def\u7531\u5668(RTR)\u6280\u672f\uff0c\u5176\u9891\u7387\u54cd\u5e94\u6ee1\u8db3\u7ebf\u6027\u4e92\u8865\u5173\u7cfbHLF(f)+HHF(f)=1\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u7535\u8def\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "RTR\u5728\u80fd\u91cf\u6548\u7387\u3001\u76f8\u4f4d\u4e00\u81f4\u6027\u548c\u5143\u4ef6\u5bb9\u5dee\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "RTR\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u9ad8\u4fdd\u771f\u97f3\u9891\u548c\u901a\u4fe1\u524d\u7aef\u7684\u4f4e\u635f\u8017\u3001\u4f4e\u5ef6\u8fdf\u786c\u4ef6\u6ee4\u6ce2\u89e3\u51b3\u65b9\u6848\uff0c\u57fa\u4e8e\u4e2d\u56fd\u4e13\u5229\u6280\u672f\u5e76\u8fdb\u884c\u4e86\u6269\u5c55\u9a8c\u8bc1\u3002"}}
{"id": "2509.08454", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08454", "abs": "https://arxiv.org/abs/2509.08454", "authors": ["Yujian Ma", "Jinqiu Sang", "Ruizhe Li"], "title": "Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition", "comment": "Work in process", "summary": "Large pre-trained speech models such as Whisper offer strong generalization\nbut pose significant challenges for resource-efficient adaptation. Low-Rank\nAdaptation (LoRA) has become a popular parameter-efficient fine-tuning method,\nyet its underlying mechanisms in speech tasks remain poorly understood. In this\nwork, we conduct the first systematic mechanistic interpretability study of\nLoRA within the Whisper encoder for speech emotion recognition (SER). Using a\nsuite of analytical tools, including layer contribution probing, logit-lens\ninspection, and representational similarity via singular value decomposition\n(SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a\ndelayed specialization process that preserves general features in early layers\nbefore consolidating task-specific information, and a forward alignment,\nbackward differentiation dynamic between LoRA's matrices. Our findings clarify\nhow LoRA reshapes encoder hierarchies, providing both empirical insights and a\ndeeper mechanistic understanding for designing efficient and interpretable\nadaptation strategies in large speech models.", "AI": {"tldr": "\u5bf9Whisper\u8bed\u97f3\u6a21\u578b\u8fdb\u884cLoRA\u5fae\u8c03\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5ef6\u8fdf\u4e13\u4e1a\u5316\u548c\u524d\u5411\u5bf9\u9f50\u3001\u540e\u5411\u5206\u5316\u7684\u52a8\u6001\u673a\u5236", "motivation": "\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5982Whisper\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5LoRA\u5728\u8bed\u97f3\u4efb\u52a1\u4e2d\u7684\u5de5\u4f5c\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u673a\u5236\u89e3\u91ca\u7814\u7a76", "method": "\u4f7f\u7528\u5c42\u8d21\u732e\u63a2\u6d4b\u3001logit-lens\u68c0\u67e5\u3001SVD\u548cCKA\u8868\u793a\u76f8\u4f3c\u6027\u5206\u6790\u7b49\u5de5\u5177\uff0c\u5728Whisper\u7f16\u7801\u5668\u4e0a\u8fdb\u884c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u7684LoRA\u673a\u5236\u5206\u6790", "result": "\u53d1\u73b0\u4e86\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a\u5ef6\u8fdf\u4e13\u4e1a\u5316\u8fc7\u7a0b\uff08\u65e9\u671f\u5c42\u4fdd\u7559\u901a\u7528\u7279\u5f81\uff0c\u540e\u671f\u5c42\u6574\u5408\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\uff09\u548c\u524d\u5411\u5bf9\u9f50\u3001\u540e\u5411\u5206\u5316\u7684\u77e9\u9635\u52a8\u6001", "conclusion": "\u9610\u660e\u4e86LoRA\u5982\u4f55\u91cd\u5851\u7f16\u7801\u5668\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u5927\u578b\u8bed\u97f3\u6a21\u578b\u9002\u914d\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u89c1\u89e3\u548c\u673a\u5236\u7406\u89e3"}}
{"id": "2509.08294", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08294", "abs": "https://arxiv.org/abs/2509.08294", "authors": ["Zheao Li", "Jiancheng An", "Chau Yuen"], "title": "Fundamental Trade-off in Wideband Stacked Intelligent Metasurface Assisted OFDMA Systems", "comment": null, "summary": "Conventional digital beamforming for wideband multiuser orthogonal\nfrequency-division multiplexing (OFDM) demands numerous power-hungry\ncomponents, increasing hardware costs and complexity. By contrast, the stacked\nintelligent metasurfaces (SIM) can perform wave-based precoding at near-light\nspeed, drastically reducing baseband overhead. However, realizing SIM-enhanced\nfully-analog beamforming for wideband multiuser transmissions remains\nchallenging, as the SIM configuration has to handle interference across all\nsubcarriers. To address this, this paper proposes a flexible subcarrier\nallocation strategy to fully reap the SIM-assisted fully-analog beamforming\ncapability in an orthogonal frequency-division multiple access (OFDMA) system,\nwhere each subcarrier selectively serves one or more users to balance\ninterference mitigation and resource utilization of SIM. We propose an\niterative algorithm to jointly optimize the subcarrier assignment matrix and\nSIM transmission coefficients, approximating an interference-free channel for\nthose selected subcarriers. Results show that the proposed system has low\nfitting errors yet allows each user to exploit more subcarriers. Further\ncomparisons highlight a fundamental trade-off: our system achieves near-zero\ninterference and robust data reliability without incurring the hardware burdens\nof digital precoding.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53e0\u52a0\u667a\u80fd\u8d85\u8868\u9762(SIM)\u7684\u5168\u6a21\u62df\u653e\u5927\u7cfb\u7edf\uff0c\u901a\u8fc7\u7075\u6d3b\u5b50\u8f7d\u6ce2\u5206\u914d\u7b56\u7565\u548c\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u5bbd\u5e26OFDMA\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u4e92\u5e72\u5e72\u6270\u548c\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf\u7684\u5bbd\u5e26\u591a\u7528\u6237OFDM\u6570\u5b57\u653e\u5927\u9700\u8981\u5927\u91cf\u8010\u8017\u7ec4\u4ef6\uff0c\u786c\u4ef6\u6210\u672c\u9ad8\u3002SIM\u867d\u7136\u80fd\u8fdb\u884c\u5149\u901f\u6ce2\u57fa\u9884\u7f16\u7801\uff0c\u4f46\u5728\u5bbd\u5e26\u591a\u7528\u6237\u4f20\u8f93\u4e2d\u5b9e\u73b0\u5168\u6a21\u62df\u653e\u5927\u4ecd\u9762\u4e34\u4e92\u5e72\u5e72\u6270\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u7684\u5b50\u8f7d\u6ce2\u5206\u914d\u7b56\u7565\uff0c\u6bcf\u4e2a\u5b50\u8f7d\u6ce2\u53ef\u9009\u62e9\u6027\u670d\u52a1\u4e00\u4e2a\u6216\u591a\u4e2a\u7528\u6237\uff0c\u5e76\u4f7f\u7528\u8fed\u4ee3\u7b97\u6cd5\u805a\u5408\u4f18\u5316\u5b50\u8f7d\u6ce2\u5206\u914d\u77e9\u9635\u548cSIM\u4f20\u8f93\u7cfb\u6570\uff0c\u4ee5\u8fd1\u4f3c\u65e0\u4e92\u5e72\u5e72\u6270\u7684\u901a\u9053\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8be5\u7cfb\u7edf\u5177\u6709\u4f4e\u62df\u5408\u8bef\u5dee\uff0c\u540c\u65f6\u5141\u8bb8\u6bcf\u4e2a\u7528\u6237\u5229\u7528\u66f4\u591a\u5b50\u8f7d\u6ce2\uff0c\u5b9e\u73b0\u4e86\u8fd1\u96f6\u4e92\u5e72\u5e72\u6270\u548c\u7a33\u5065\u7684\u6570\u636e\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u4e0d\u589e\u52a0\u6570\u5b57\u9884\u7f16\u7801\u786c\u4ef6\u8d1f\u62c5\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e92\u5e72\u5e72\u6270\u51cf\u5c11\u548c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u7684\u57fa\u672c\u5e73\u8861\uff0c\u4e3a\u5bbd\u5e26\u591a\u7528\u6237\u5168\u6a21\u62df\u653e\u5927\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08717", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08717", "abs": "https://arxiv.org/abs/2509.08717", "authors": ["Zubair Faruqui", "Mackenzie S. McIntire", "Rahul Dubey", "Jay McEntee"], "title": "Explainability of CNN Based Classification Models for Acoustic Signal", "comment": "Accepted in IEEE ICTAI 2025", "summary": "Explainable Artificial Intelligence (XAI) has emerged as a critical tool for\ninterpreting the predictions of complex deep learning models. While XAI has\nbeen increasingly applied in various domains within acoustics, its use in\nbioacoustics, which involves analyzing audio signals from living organisms,\nremains relatively underexplored. In this paper, we investigate the\nvocalizations of a bird species with strong geographic variation throughout its\nrange in North America. Audio recordings were converted into spectrogram images\nand used to train a deep Convolutional Neural Network (CNN) for classification,\nachieving an accuracy of 94.8\\%. To interpret the model's predictions, we\napplied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT,\nGrad-CAM) XAI techniques. These techniques produced different but complementary\nexplanations, and when their explanations were considered together, they\nprovided more complete and interpretable insights into the model's\ndecision-making. This work highlights the importance of using a combination of\nXAI techniques to improve trust and interoperability, not only in broader\nacoustics signal analysis but also argues for broader applicability in\ndifferent domain specific tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd(XAI)\u5728\u751f\u7269\u58f0\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cdXAI\u6280\u672f\u6765\u89e3\u91ca\u9e1f\u7c7b\u58f0\u97f3\u5206\u7c7bCNN\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u53d6\u5f97\u4e8694.8%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u867d\u7136XAI\u5728\u58f0\u5b66\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5728\u6d89\u53ca\u751f\u7269\u97f3\u9891\u4fe1\u53f7\u5206\u6790\u7684\u751f\u7269\u58f0\u5b66\u9886\u57df\u4ecd\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u751f\u7269\u58f0\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u3002", "method": "\u5c06\u9e1f\u7c7b\u97f3\u9891\u5f55\u97f3\u8f6c\u6362\u4e3a\u9891\u8c31\u56fe\u56fe\u50cf\uff0c\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5e94\u7528\u6a21\u578b\u65e0\u5173(LIME\u3001SHAP)\u548c\u6a21\u578b\u7279\u5b9a(DeepLIFT\u3001Grad-CAM)\u7684XAI\u6280\u672f\u6765\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u3002", "result": "CNN\u6a21\u578b\u8fbe\u523094.8%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4e0d\u540cXAI\u6280\u672f\u4ea7\u751f\u4e92\u8865\u7684\u89e3\u91ca\uff0c\u7ed3\u5408\u4f7f\u7528\u65f6\u80fd\u63d0\u4f9b\u66f4\u5b8c\u6574\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u51b3\u7b56\u6d1e\u5bdf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7ed3\u5408\u591a\u79cdXAI\u6280\u672f\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u4e0d\u4ec5\u5728\u58f0\u5b66\u4fe1\u53f7\u5206\u6790\u4e2d\uff0c\u5728\u5176\u4ed6\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u4e5f\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2509.08432", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08432", "abs": "https://arxiv.org/abs/2509.08432", "authors": ["Yingjie Wu", "Junshan Luo", "Weiyu Chen", "Shilian Wang", "Fanggang Wang", "Haiyang Ding"], "title": "Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain Location", "comment": null, "summary": "For autonomous aerial vehicle (AAV) secure communications, traditional\ndesigns based on fixed position antenna (FPA) lack sufficient spatial degrees\nof freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable\nto eavesdropping. To overcome this problem, this paper proposes a framework\nthat effectively incorporates the fluid antenna (FA) and the artificial noise\n(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple\neavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN\nprecoders, and FA positions. In particular, the worst-case MSR is considered by\ntaking the channel uncertainties due to the uncertainty about eavesdropping\nlocations into account. To tackle the highly coupled optimization variables and\nthe channel uncertainties in the formulated problem, an efficient and robust\nalgorithm is proposed. Particularly, the uncertain regions of eavesdroppers,\nwhose shapes can be arbitrary, are disposed by constructing convex hull. In\naddition, two movement modes of FAs are considered, namely, free movement mode\nand zonal movement mode, for which different optimization techniques are\napplied, respectively. Numerical results show that, the proposed FA schemes\nboost security by exploiting additional spatial DoF rather than transmit power,\nwhile AN provides remarkable gains under high transmit power. Furthermore, the\nsynergy between FA and AN results in a secure advantage that exceeds the sum of\ntheir individual contributions, achieving a balance between security and\nreliability under limited resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6d41\u4f53\u5929\u7ebf\u548c\u4eba\u5de5\u566a\u58f0\u6280\u672f\u7684\u65e0\u4eba\u673a\u5b89\u5168\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u90e8\u7f72\u3001\u9884\u7f16\u7801\u548c\u5929\u7ebf\u4f4d\u7f6e\u6765\u6700\u5927\u5316\u6700\u5dee\u60c5\u51b5\u4e0b\u7684\u4fdd\u5bc6\u7387\uff0c\u6709\u6548\u5e94\u5bf9\u7a83\u542c\u5a01\u80c1\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u4f4d\u7f6e\u5929\u7ebf\u7f3a\u4e4f\u8db3\u591f\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\uff0c\u4f7f\u5f97\u89c6\u8ddd\u4e3b\u5bfc\u7684\u65e0\u4eba\u673a\u901a\u4fe1\u94fe\u8def\u5bb9\u6613\u53d7\u5230\u7a83\u542c\u653b\u51fb\uff0c\u9700\u8981\u65b0\u7684\u5b89\u5168\u901a\u4fe1\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6d41\u4f53\u5929\u7ebf\u548c\u4eba\u5de5\u566a\u58f0\u6280\u672f\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684\u7a83\u542c\u533a\u57df\u4e0d\u786e\u5b9a\u6027\uff0c\u8003\u8651\u81ea\u7531\u79fb\u52a8\u548c\u533a\u57df\u79fb\u52a8\u4e24\u79cd\u5929\u7ebf\u6a21\u5f0f\uff0c\u63d0\u51fa\u9ad8\u6548\u7684\u9c81\u68d2\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6d41\u4f53\u5929\u7ebf\u65b9\u6848\u901a\u8fc7\u5229\u7528\u989d\u5916\u7a7a\u95f4\u81ea\u7531\u5ea6\u800c\u975e\u53d1\u5c04\u529f\u7387\u6765\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4eba\u5de5\u566a\u58f0\u5728\u9ad8\u53d1\u5c04\u529f\u7387\u4e0b\u63d0\u4f9b\u663e\u8457\u589e\u76ca\uff0c\u4e24\u8005\u534f\u540c\u4f5c\u7528\u8d85\u8fc7\u5404\u81ea\u8d21\u732e\u4e4b\u548c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7684\u5e73\u8861\uff0c\u6d41\u4f53\u5929\u7ebf\u4e0e\u4eba\u5de5\u566a\u58f0\u7684\u534f\u540c\u4f5c\u7528\u4e3a\u65e0\u4eba\u673a\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08800", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.08800", "abs": "https://arxiv.org/abs/2509.08800", "authors": ["Yonghyun Kim", "Junhyung Park", "Joonhyung Bae", "Kirak Kim", "Taegyun Kwon", "Alexander Lerch", "Juhan Nam"], "title": "PianoVAM: A Multimodal Piano Performance Dataset", "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval (ISMIR) Conference, 2025", "summary": "The multimodal nature of music performance has driven increasing interest in\ndata beyond the audio domain within the music information retrieval (MIR)\ncommunity. This paper introduces PianoVAM, a comprehensive piano performance\ndataset that includes videos, audio, MIDI, hand landmarks, fingering labels,\nand rich metadata. The dataset was recorded using a Disklavier piano, capturing\naudio and MIDI from amateur pianists during their daily practice sessions,\nalongside synchronized top-view videos in realistic and varied performance\nconditions. Hand landmarks and fingering labels were extracted using a\npretrained hand pose estimation model and a semi-automated fingering annotation\nalgorithm. We discuss the challenges encountered during data collection and the\nalignment process across different modalities. Additionally, we describe our\nfingering annotation method based on hand landmarks extracted from videos.\nFinally, we present benchmarking results for both audio-only and audio-visual\npiano transcription using the PianoVAM dataset and discuss additional potential\napplications.", "AI": {"tldr": "PianoVAM\u662f\u4e00\u4e2a\u5305\u542b\u89c6\u9891\u3001\u97f3\u9891\u3001MIDI\u3001\u624b\u90e8\u5173\u952e\u70b9\u3001\u6307\u6cd5\u6807\u6ce8\u548c\u4e30\u5bcc\u5143\u6570\u636e\u7684\u94a2\u7434\u6f14\u594f\u6570\u636e\u96c6\uff0c\u901a\u8fc7Disklavier\u94a2\u7434\u91c7\u96c6\u4e1a\u4f59\u94a2\u7434\u5bb6\u7684\u65e5\u5e38\u7ec3\u4e60\u6570\u636e\uff0c\u652f\u6301\u591a\u6a21\u6001\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u3002", "motivation": "\u97f3\u4e50\u8868\u6f14\u7684\u591a\u6a21\u6001\u7279\u6027\u4fc3\u4f7fMIR\u793e\u533a\u5bf9\u97f3\u9891\u4ee5\u5916\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u5305\u542b\u89c6\u9891\u3001\u97f3\u9891\u3001MIDI\u7b49\u591a\u79cd\u6a21\u6001\u7684\u94a2\u7434\u6f14\u594f\u6570\u636e\u96c6\u6765\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002", "method": "\u4f7f\u7528Disklavier\u94a2\u7434\u91c7\u96c6\u4e1a\u4f59\u94a2\u7434\u5bb6\u7684\u65e5\u5e38\u7ec3\u4e60\u6570\u636e\uff0c\u5305\u62ec\u540c\u6b65\u7684\u97f3\u9891\u3001MIDI\u548c\u4fef\u89c6\u89c6\u89d2\u89c6\u9891\uff1b\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u63d0\u53d6\u624b\u90e8\u5173\u952e\u70b9\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6307\u6cd5\u6807\u6ce8\u7b97\u6cd5\u8fdb\u884c\u6307\u6cd5\u6807\u6ce8\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86PianoVAM\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u94a2\u7434\u6f14\u594f\u6570\u636e\uff1b\u63d0\u51fa\u4e86\u57fa\u4e8e\u89c6\u9891\u624b\u90e8\u5173\u952e\u70b9\u7684\u6307\u6cd5\u6807\u6ce8\u65b9\u6cd5\uff1b\u5728\u97f3\u9891\u8f6c\u5f55\u548c\u97f3\u89c6\u9891\u8f6c\u5f55\u4efb\u52a1\u4e0a\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u3002", "conclusion": "PianoVAM\u6570\u636e\u96c6\u4e3a\u591a\u6a21\u6001\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u652f\u6301\u94a2\u7434\u8f6c\u5f55\u3001\u6307\u6cd5\u5206\u6790\u7b49\u591a\u79cd\u5e94\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08434", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08434", "abs": "https://arxiv.org/abs/2509.08434", "authors": ["Ahmet B. Kilic", "Ozgur B. Akan"], "title": "Information and Communication Theoretical Foundations of the Internet of Plants, Principles, Challenges, and Future Directions", "comment": null, "summary": "Plants exchange information through multiple modalities, including chemical,\nelectrical, mycorrhizal, and acoustic signaling, which collectively support\nsurvival, defense, and adaptation. While these processes are well documented in\nbiology, their systematic analysis from an Information and Communication\nTechnology (ICT) perspective remains limited. To address this gap, this article\nis presented as a tutorial with survey elements. It provides the necessary\nbiological background, reformulates inter-plant signaling within ICT\nframeworks, and surveys empirical studies to guide future research and\napplications. First, the paper introduces the fundamental biological processes\nto establish a foundation for readers in communications and networking.\nBuilding on this foundation, existing models of emission, propagation, and\nreception are synthesized for each modality and reformulated in terms of\ntransmitter, channel, and receiver blocks. To complement theory, empirical\nstudies and state-of-the-art sensing approaches are critically examined.\nLooking forward, the paper identifies open challenges and outlines future\nresearch directions, with particular emphasis on the emerging vision of the\nInternet of Plants (IoP). This paradigm frames plants as interconnected nodes\nwithin ecological and technological networks, offering new opportunities for\napplications in precision agriculture, ecosystem monitoring, climate\nresilience, and bio-inspired communication systems. By integrating biological\ninsights with ICT frameworks and projecting toward the IoP, this article\nprovides a comprehensive tutorial on plant communication for the communications\nresearch community and establishes a foundation for interdisciplinary advances.", "AI": {"tldr": "\u672c\u6587\u4ece\u4fe1\u606f\u901a\u4fe1\u6280\u672f\u89d2\u5ea6\u7cfb\u7edf\u5206\u6790\u690d\u7269\u591a\u6a21\u6001\u4fe1\u53f7\u4ea4\u6362\uff0c\u63d0\u51fa\u690d\u7269\u4e92\u8054\u7f51\uff08IoP\uff09\u6982\u5ff5\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u548c\u751f\u6001\u76d1\u6d4b\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u5c06\u690d\u7269\u5316\u5b66\u3001\u7535\u6c14\u3001\u83cc\u6839\u548c\u58f0\u5b66\u4fe1\u53f7\u4ea4\u6362\u7cfb\u7edf\u5730\u91cd\u65b0\u89e3\u91ca\u4e3a\u4fe1\u606f\u901a\u4fe1\u6280\u672f\u6846\u67b6\uff0c\u8865\u5145\u73b0\u6709\u751f\u7269\u5b66\u7814\u7a76\u4e0eICT\u9886\u57df\u7684\u8de8\u5b66\u79d1\u6c9f\u5dee\u3002", "method": "\u901a\u8fc7\u6559\u7a0b\u5f0f\u8c03\u67e5\u65b9\u5f0f\uff0c\u5148\u4ecb\u7ecd\u57fa\u7840\u751f\u7269\u5b66\u77e5\u8bc6\uff0c\u7136\u540e\u5c06\u5404\u6a21\u6001\u7684\u53d1\u5c04\u3001\u4f20\u64ad\u548c\u63a5\u6536\u6a21\u578b\u91cd\u6784\u4e3a\u53d1\u5c04\u673a\u3001\u4fe1\u9053\u548c\u63a5\u6536\u673a\u5757\uff0c\u5e76\u7efc\u8ff0\u5b9e\u9a8c\u7814\u7a76\u548c\u5148\u8fdb\u611f\u77e5\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86\u690d\u7269\u4fe1\u606f\u4ea4\u6362\u7684\u7edf\u4e00ICT\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u690d\u7269\u4e92\u8054\u7f51\uff08IoP\uff09\u7684\u65b0\u5174\u89c6\u91ce\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6574\u5408\u751f\u7269\u5b66\u89c1\u89e3\u4e0eICT\u6846\u67b6\uff0c\u4e3a\u901a\u4fe1\u7814\u7a76\u793e\u7fa4\u63d0\u4f9b\u4e86\u690d\u7269\u901a\u4fe1\u7684\u5168\u9762\u6559\u7a0b\uff0c\u5e76\u4e3a\u8de8\u5b66\u79d1\u53d1\u5c55\u548c\u690d\u7269\u4e92\u8054\u7f51\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.08504", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08504", "abs": "https://arxiv.org/abs/2509.08504", "authors": ["Didem Aydogan", "Mohaned Chraiti", "Korkut Kaan Tokg\u00f6z"], "title": "On the Performance of ISAC over the D-Band in a Phase-Noise Aware OFDM Systems", "comment": null, "summary": "Phase noise (PN) is a critical impairment at D-band frequencies (110 to 170\nGHz), which are widely investigated as promising candidates for beyond 5G/6G\nISAC systems. This paper evaluates OFDM based ISAC sensing performance under\nrealistic oscillator impairments using a hardware-tuned 3GPP PN model at 130\nGHz and FFT based radar processing. With a numerology of 480 kHz, results show\nthat PN introduces range RMSE floors of 0.04 to 0.05 m and velocity RMSE floors\nof 0.12 to 0.18 m/s. Doppler sidelobe metrics also saturate, with PSLR around\nminus 6 dB and ISLR around minus 4 dB. These findings confirm that range\naccuracy remains bandwidth limited, while velocity estimation and sidelobe\nsuppression are strongly PN-sensitive. The study highlights the importance of\nPN-aware waveform and numerology design for sub-THz ISAC and provides insights\nfor future multi-band transceivers. Communication metrics and PN mitigation\nstrategies such as PTRS and CPE tracking are left for future work.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5728130GHz\u9891\u6bb5\u4f7f\u75283GPP\u76f8\u4f4d\u566a\u58f0\u6a21\u578b\u4e0b\uff0cOFDM ISAC\u7cfb\u7edf\u7684\u611f\u77e5\u6027\u80fd\uff0c\u53d1\u73b0\u76f8\u4f4d\u566a\u58f0\u5bfc\u81f4\u8ddd\u79bb\u7cbe\u5ea6\u53d7\u9650\uff080.04-0.05m\uff09\u548c\u901f\u5ea6\u4f30\u8ba1\u53d7\u9650\uff080.12-0.18m/s\uff09\uff0c\u591a\u666e\u52d2\u65c1\u74e3\u6307\u6807\u4e5f\u51fa\u73b0\u9971\u548c\u3002", "motivation": "D\u6ce2\u6bb5\uff08110-170GHz\uff09\u662f\u672a\u67655G/6G ISAC\u7cfb\u7edf\u7684\u6709\u524d\u666f\u9891\u6bb5\uff0c\u4f46\u76f8\u4f4d\u566a\u58f0\u662f\u8be5\u9891\u6bb5\u7684\u5173\u952e\u635f\u4f24\u56e0\u7d20\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u8c03\u8c10\u76843GPP\u76f8\u4f4d\u566a\u58f0\u6a21\u578b\uff0c\u5728130GHz\u9891\u6bb5\u4f7f\u7528480kHz numerology\u7684OFDM\u4fe1\u53f7\uff0c\u901a\u8fc7FFT\u96f7\u8fbe\u5904\u7406\u8bc4\u4f30ISAC\u611f\u77e5\u6027\u80fd\u3002", "result": "\u76f8\u4f4d\u566a\u58f0\u5bfc\u81f4\u8ddd\u79bbRMSE\u5e73\u53f0\u57280.04-0.05m\uff0c\u901f\u5ea6RMSE\u5e73\u53f0\u57280.12-0.18m/s\uff0c\u591a\u666e\u52d2\u65c1\u74e3\u6307\u6807PSLR\u7ea6-6dB\uff0cISLR\u7ea6-4dB\u3002\u8ddd\u79bb\u7cbe\u5ea6\u53d7\u5e26\u5bbd\u9650\u5236\uff0c\u800c\u901f\u5ea6\u4f30\u8ba1\u548c\u65c1\u74e3\u6291\u5236\u5bf9\u76f8\u4f4d\u566a\u58f0\u654f\u611f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u76f8\u4f4d\u566a\u58f0\u611f\u77e5\u7684\u6ce2\u5f62\u548cnumerology\u8bbe\u8ba1\u5bf9\u4e8e\u4e9a\u592a\u8d6b\u5179ISAC\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u591a\u9891\u6bb5\u6536\u53d1\u5668\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c1\u89e3\u3002"}}
{"id": "2509.08614", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08614", "abs": "https://arxiv.org/abs/2509.08614", "authors": ["Yuxuan Duan", "Chenyang Yang"], "title": "Modular PE-Structured Learning for Cross-Task Wireless Communications", "comment": "14 pages,7 figures", "summary": "Recent trends in learning wireless policies attempt to develop deep neural\nnetworks (DNNs) for handling multiple tasks with a single model. Existing\napproaches often rely on large models, which are hard to pre-train and\nfine-tune at the wireless edge. In this work, we challenge this paradigm by\nleveraging the structured knowledge of wireless problems -- specifically,\npermutation equivariant (PE) properties. We design three types of PE-aware\nmodules, two of which are Transformer-style sub-layers. These modules can serve\nas building blocks to assemble compact DNNs applicable to the wireless policies\nwith various PE properties. To guide the design, we analyze the hypothesis\nspace associated with each PE property, and show that the PE-structured module\nassembly can boost the learning efficiency. Inspired by the reusability of the\nmodules, we propose PE-MoFormer, a compositional DNN capable of learning a wide\nrange of wireless policies -- including but not limited to precoding,\ncoordinated beamforming, power allocation, and channel estimation -- with\nstrong generalizability, low sample and space complexity. Simulations\ndemonstrate that the proposed modular PE-based framework outperforms relevant\nlarge model in both learning efficiency and inference time, offering a new\ndirection for structured cross-task learning for wireless communications.", "AI": {"tldr": "\u901a\u8fc7\u6d3e\u751f\u5bf9\u79f0\u6027\u77e5\u8bc6\u8bbe\u8ba1\u6a21\u5757\u5316\u6df1\u5ea6\u7f51\u7edc\uff0c\u63d0\u51faPE-MoFormer\u6846\u67b6\uff0c\u5728\u65e0\u7ebf\u901a\u4fe1\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7387\u548c\u66f4\u5feb\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u6709\u65e0\u7ebf\u901a\u4fe1\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u6a21\u578b\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u56f0\u96be\uff0c\u9700\u8981\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5229\u7528\u6d3e\u751f\u5bf9\u79f0\u6027(PE)\u77e5\u8bc6\u8bbe\u8ba1\u4e09\u79cdPE\u6a21\u5757\uff08\u5305\u542b\u4e24\u79cdTransformer\u98ce\u683c\u5b50\u5c42\uff09\uff0c\u901a\u8fc7\u6a21\u5757\u7ec4\u88c5\u6784\u5efa\u7b80\u6d01DNN\uff0c\u63d0\u51faPE-MoFormer\u7ec4\u5408\u5f0f\u6a21\u578b", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u5757\u5316PE\u6846\u67b6\u5728\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u90fd\u8d85\u8fc7\u76f8\u5173\u5927\u578b\u6a21\u578b\uff0c\u5177\u6709\u5f3a\u7684\u901a\u7528\u6027\u548c\u4f4e\u6837\u672c/\u7a7a\u95f4\u590d\u6742\u5ea6", "conclusion": "\u57fa\u4e8e\u7ed3\u6784\u5316PE\u77e5\u8bc6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u4e3a\u65e0\u7ebf\u901a\u4fe1\u8de8\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u9884\u7f16\u7801\u3001\u534f\u540c\u6536\u636e\u6750\u3001\u529f\u7387\u5206\u914d\u3001\u9891\u9053\u4f30\u8ba1\u7b49\u591a\u79cd\u65e0\u7ebf\u653f\u7b56"}}
{"id": "2509.08642", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.08642", "abs": "https://arxiv.org/abs/2509.08642", "authors": ["Hang Ruan", "Homa Nikbakht", "Ruizhi Zhang", "Honglei Chen", "Yonina C. Eldar"], "title": "RIS-Assisted Near-Field ISAC for Multi-Target Indication in NLoS Scenarios", "comment": "5 pages, 3 figures; To be submitted to ICASSP 2026", "summary": "Enabling multi-target sensing in near-field integrated sensing and\ncommunication (ISAC) systems is a key challenge, particularly when\nline-of-sight paths are blocked. This paper proposes a beamforming framework\nthat leverages a reconfigurable intelligent surface (RIS) to achieve\nmulti-target indication. Our contribution is the extension of classic\nbeampattern gain and inter-target cross-correlation metrics to the near-field,\nleveraging both angle and distance information to discriminate between multiple\nusers and targets. We formulate a problem to maximize the worst-case sensing\nperformance by jointly designing the beamforming at the base station and the\nphase shifts at the RIS, while guaranteeing communication rates. The non-convex\nproblem is solved via an efficient alternating optimization (AO) algorithm that\nutilizes semidefinite relaxation (SDR). Simulations demonstrate that our\nRIS-assisted framework enables high-resolution sensing of co-angle targets in\nblocked scenarios.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53ef\u91cd\u914d\u667a\u80fd\u8868\u9762(RIS)\u7684\u8fb9\u7f13\u5199\u6846\u67b6\uff0c\u89e3\u51b3\u8fd1\u573a\u96c6\u6210\u611f\u77e5\u901a\u4fe1(ISAC)\u7cfb\u7edf\u4e2d\u591a\u76ee\u6807\u611f\u77e5\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u76f4\u89c6\u8def\u5f84\u88ab\u963b\u585e\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u5728\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u591a\u76ee\u6807\u611f\u77e5\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u76f4\u89c6\u8def\u5f84\u88ab\u963b\u585e\u65f6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u89d2\u5ea6\u548c\u8ddd\u79bb\u4fe1\u606f\u6765\u533a\u5206\u591a\u4e2a\u7528\u6237\u548c\u76ee\u6807\u7684\u65b9\u6848\u3002", "method": "\u5c06\u7ecf\u5178\u7684\u6ce2\u675f\u6a21\u578b\u589e\u76ca\u548c\u76ee\u6807\u95f4\u76f8\u5173\u6027\u6307\u6807\u6269\u5c55\u5230\u8fd1\u573a\u9886\u57df\uff0c\u901a\u8fc7\u805a\u7126\u57fa\u7ad9\u548cRIS\u76f8\u4f4d\u79fb\u7684\u8054\u5408\u8bbe\u8ba1\u6765\u6700\u5927\u5316\u6700\u5dee\u611f\u77e5\u6027\u80fd\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316(AO)\u7b97\u6cd5\u548c\u534a\u5b9a\u89c1\u677e\u5f1b(SDR)\u89e3\u51b3\u975e\u51f8\u95ee\u9898\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5RIS\u8f85\u52a9\u6846\u67b6\u80fd\u591f\u5728\u88ab\u963b\u585e\u573a\u666f\u4e0b\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u540c\u89d2\u5ea6\u76ee\u6807\u611f\u77e5\u3002", "conclusion": "\u8be5\u65b9\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fd1\u573aISAC\u7cfb\u7edf\u4e2d\u7684\u591a\u76ee\u6807\u611f\u77e5\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u76f4\u89c6\u8def\u5f84\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5229\u7528\u8fd1\u573a\u89d2\u5ea6\u548c\u8ddd\u79bb\u4fe1\u606f\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u7684\u76ee\u6807\u533a\u5206\u3002"}}

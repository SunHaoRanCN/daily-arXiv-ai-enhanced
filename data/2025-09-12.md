<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [eess.AS](#eess.AS) [Total: 7]
- [cs.SD](#cs.SD) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: SNUPHY-M模型通过自监督学习同时分析ECG、PPG和ABP三种生理信号，提取心脏周期的电学、压力和流体特征，在血流动力学监测任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 临床需要综合分析多种生理信号来监测血流动力学，但现有研究多针对单一信号，缺乏适用于真实临床场景的复杂信号分析方法

Method: 基于自监督学习的掩码表示学习模型，通过恢复三种掩码生理信号（ECG、PPG、ABP）来提取反映心脏周期多种物理特征的生理特征

Result: 在低血压、心搏量、收缩压、舒张压和年龄预测等临床下游任务中显著优于监督学习或自监督学习模型，特别是在使用无创信号的预测任务中

Conclusion: SNUPHY-M是首个将多模态自监督学习应用于心血管分析（涉及ECG、PPG和ABP信号）的模型，能有效支持临床决策和精确诊断，为无创血流动力学早期诊断和管理做出重要贡献

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [2] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: 本文探讨AI在教育领域的应用，特别是信号处理教育，关注技术局限性和实际应用，旨在促进公平、负责任的AI使用以改善教育体验


<details>
  <summary>Details</summary>
Motivation: AI技术虽取得重大突破，但仍面临公平和负责任使用的挑战，特别是在改善全球人类状况方面。文章旨在探索AI在教育领域的应用潜力

Method: 提供AI在教育环境中使用的核心技术问题入门指南，包括公平性、包容性、处理幻觉输出和资源高效利用，并通过开发沉浸式"智能教科书"来说明这些考虑因素

Result: 提出了将AI工具应用于信号处理教育的具体方法，包括解决技术限制和实际应用策略，为研究人员和教育工作者提供了实用资源

Conclusion: AI在工程教育中具有重要潜力，但需要关注透明度、可解释性和可信赖性等关键因素，智能教科书等应用展示了AI改善教育体验的可能性

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


### [3] [Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography](https://arxiv.org/abs/2509.08973)
*Harshit Agrawal,Ari Hietanen,Simo Särkkä*

Main category: eess.SP

TL;DR: 该研究通过在不同分辨率下应用网络并基于速度和准确性选择最优分辨率，解决了CBCT散射伪影深度学习模型在移动设备和边缘设备部署时内存占用大的问题。


<details>
  <summary>Details</summary>
Motivation: 散射伪影严重降低锥束CT图像质量，虽然深度学习方法在散射估计方面有潜力，但由于网络内存占用大，在移动CBCT系统或边缘设备上的部署仍然受限。

Method: 首先在六个分辨率下比较四种插值方法的CBCT散射信号重建误差，然后在五个图像分辨率上训练最新SOTA方法，评估FLOPs、推理时间和GPU内存需求的减少。

Result: 减少输入尺寸和网络参数实现了78倍的FLOPs减少，同时保持可比性能（MAPE从4.42%降至3.85%，MSE从2.01×10^{-2}降至1.34×10^{-2}），推理时间和GPU内存使用分别减少16倍和12倍。

Conclusion: 研究强调了降采样在深度学习散射估计中被低估的作用，通过大幅减少FLOPs和GPU内存需求，使得在资源受限环境（如移动CBCT和边缘设备）中进行散射校正成为可能。

Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam
computed tomography (CBCT) scans. Although deep learning-based methods show
promise in estimating scatter from CBCT measurements, their deployment in
mobile CBCT systems or edge devices is still limited due to the large memory
footprint of the networks. This study addresses the issue by applying networks
at varying resolutions and suggesting an optimal one, based on speed and
accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter
signal was examined at six resolutions by comparing four interpolation methods.
Next, a recent state-of-the-art method was trained across five image
resolutions and evaluated for the reductions in floating-point operations
(FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold
reduction in FLOPs compared to the baseline method, while maintaining comarable
performance in terms of mean-absolute-percentage-error (MAPE) and
mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to
4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times
10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and
12, respectively. Further experiments comparing scatter-corrected
reconstructions on a large, simulated dataset and real CBCT scans from water
and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling
in deep learning-based scatter estimation. The substantial reduction in FLOPs
and GPU memory requirements achieved by our method enables scatter correction
in resource-constrained environments, such as mobile CBCT and edge devices.

</details>


### [4] [6G Resilience -- White Paper](https://arxiv.org/abs/2509.09005)
*Hirley Alves,Nurul H. Mahmood,Onel L. A. López,Sumudu Samarakoon,Seppo Yrjölä,Matti Latva-Aho,Markku Juntti,Ari Pouttu,Armin Dekorsy,Arthur Sousa de Sena,Aydin Sezgin,Bho Matthiesen,Chafika Benzaid,Chathuranga Weeraddana,David Hutchison,Dileepa Marasinghe,Doganalp Ergenc,Eduard Jorswieck,Erkki Harjula,Falko Dressler,Harri Saarnisaari,Italo Atzeni,Jaap Van De Beek,Jacek Rak,Konstantin Mikhaylov,Lauri Loven,Madhusanka Liyanage,Marcos Katz,Marja Matinmikko-Blue,Mehdi Rasti,Mika Ylianttila Nhan Nguyen,Pawani Porambage,Petar Popovski,Petri Ahokangas,Premanandana Rajatheva,Robert-Jeron Reifert,Tharaka Hewa,Tommy Svensson*

Main category: eess.SP

TL;DR: 6G应将维续性设计为核心目标，通过3R框架（可靠性、稳健性、恢复力）和边缘本地化架构来应对长期中断和系统故障。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要在长期复杂中断中维持运行，并从效果优先转向维持性认知，将维持性作为与可持续性和效率并列的核心设计目标。

Method: 提出3R框架（可靠性、稳健性、恢复力），重点关注边缘本地化设计、开放接口、程序化能力，以及AI本地控制循环、零信任安全和优先级网络技术。

Result: 形成了可测量的维持性能力：温和降级、情况感知、快速重配置、学习驱动的改进和恢复，以及与维持性目标对齐的九个商业模式组。

Conclusion: 本白皮书为6G维持性建立了基础框架，并将作为吸引研究人员、专业人士、政府官员和公众参与6G维持性开发的启动点和催化剂。

Abstract: 6G must be designed to withstand, adapt to, and evolve amid prolonged,
complex disruptions. Mobile networks' shift from efficiency-first to
sustainability-aware has motivated this white paper to assert that resilience
is a primary design goal, alongside sustainability and efficiency, encompassing
technology, architecture, and economics. We promote resilience by analysing
dependencies between mobile networks and other critical systems, such as
energy, transport, and emergency services, and illustrate how cascading
failures spread through infrastructures. We formalise resilience using the 3R
framework: reliability, robustness, resilience. Subsequently, we translate this
into measurable capabilities: graceful degradation, situational awareness,
rapid reconfiguration, and learning-driven improvement and recovery.
  Architecturally, we promote edge-native and locality-aware designs, open
interfaces, and programmability to enable islanded operations, fallback modes,
and multi-layer diversity (radio, compute, energy, timing). Key enablers
include AI-native control loops with verifiable behaviour, zero-trust security
rooted in hardware and supply-chain integrity, and networking techniques that
prioritise critical traffic, time-sensitive flows, and inter-domain
coordination.
  Resilience also has a techno-economic aspect: open platforms and high-quality
complementors generate ecosystem externalities that enhance resilience while
opening new markets. We identify nine business-model groups and several
patterns aligned with the 3R objectives, and we outline governance and
standardisation. This white paper serves as an initial step and catalyst for 6G
resilience. It aims to inspire researchers, professionals, government
officials, and the public, providing them with the essential components to
understand and shape the development of 6G resilience.

</details>


### [5] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: 这篇论文提出了一种适配性空间时间模型(AdaST-Sleep)，用于预测睡眠评分，通过卷积神经网络和循环神经网络结合的方法，在多种时间窗口设置下都超过了基线模型，并通过域分类器实现了跨主体的通用性。


<details>
  <summary>Details</summary>
Motivation: 睡眠预测能够让个人和医疗服务提供者预先预见并主动处理影响睡眠的因素，最终改善心理和身体健康。

Method: 提出适配性空间时间模型(AdaST-Sleep)，结合卷积层捕捉多种特征间的空间特征交互，使用循环神经网络层处理长期健康数据，并集成域分类器实现跨主体的通用性。

Result: 在5种输入窗口大小(3,5,7,9,11天)和5种预测窗口大小(1,3,5,7,9天)的实验中，该方法均超过四个基线模型，最低RMSE为0.282(7天输入窗口和1天预测窗口)。方法在多天预测中仍保持强劲性能，可准确跟踪睡眠评分水平和日常波动。

Conclusion: 该框架为使用商业可穿戴设备的稀疏数据和域适应技术进行个性化睡眠预测提供了稳健而适配性强的解决方案。

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [6] [Improving the Elevational Focusing of Fast Orthogonal Row-Column Electronic Scanning (FORCES) Ultrasound Imaging using Retrospective Transmit Beamforming (RTB)](https://arxiv.org/abs/2509.09056)
*Michael Caulfield,Randy Palamar,Darren Dahunsi,Mohammad Rahim Sobhani,Negar Majidi,Roger Zemp*

Main category: eess.SP

TL;DR: 本文提出了一种改进的FORCES成像方案，通过在仰角方向应用回顾性发射波束成形(RTB)技术，解决了传统FORCES方法在焦点外仰角聚焦效果差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统FORCES成像方案由于固定的仰角焦点和大的发射孔径，在焦点外的仰角方向聚焦效果较差，限制了其成像质量。

Method: 在FORCES和uFORCES方法中应用回顾性发射波束成形(RTB)技术，实现整个成像平面内的仰角发射聚焦。

Result: 实验显示，应用RTB后，FORCES和uFORCES在焦点外的仰角聚焦能力得到显著改善，在焦点处的性能保持相当或更好。通过线模体和囊肿模体成像验证了改进效果。

Conclusion: RTB技术的应用有效提升了FORCES成像方案的仰角聚焦性能，使其能够在整个成像平面实现更好的三维成像质量。

Abstract: Recent developments in Row Column Arrays (RCAs) have presented promising
options for volumetric imaging without the need for the excessive channel
counts of fully wired 2D-arrays. Bias programmable RCAs, also known as Top
Orthogonal to Bottom Electrode (TOBE) Arrays, show further promise in that
imaging schemes, such as Fast Orthogonal Row-Column Electronic Scanning
(FORCES) allow for full transmit and receive focusing everywhere in the image
plane. However, due to its fixed elevational focus and large transmit aperture,
FORCES experiences poor elevational focusing away from the focal point. In this
study we present a modification to the FORCES imaging scheme by applying
Retrospective Transmit Beamforming (RTB) in the elevational direction to allow
for elevational transmit focusing everywhere in the imaging plane. We evaluate
FORCES and uFORCES methods, with and without RTB applied, when imaging both a
cyst and wire phantom. With experiment we show improved elevational focusing
capabilities away from the focal point when RTB is applied to both FORCES and
uFORCES. At the focal point, performance with RTB remains comparable or
improved relative to standard FORCES. This is quantified by the measurement of
Full Width Half Max when imaging the wire phantom, and by the generalized
Contrast to Noise Ratio when imaging the tubular cyst phantom. We also
demonstrate the volumetric imaging capabilities of FORCES RTB with the wire
phantom.

</details>


### [7] [Signed Graph Learning with Hidden Nodes](https://arxiv.org/abs/2509.09120)
*Rong Ye,Xue-Qin Jiang,Hui Feng,Jian Wang,Runhe Qiu*

Main category: eess.SP

TL;DR: 提出了一种针对带隐藏节点的符号图学习方法SGL-HNCS，通过列稀疏正则化约束来重构符号图拉普拉斯矩阵，解决了现有方法假设所有节点可见的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有符号图学习方法通常假设所有图节点都可见，但在实际应用中经常存在部分节点隐藏的情况，需要开发能够处理隐藏节点的符号图学习方法。

Method: 基于图信号在符号图上的平滑性假设，将拓扑推断问题建模为带列稀疏正则化的约束优化问题，使用定制的块坐标下降(BCD)算法求解。

Result: 在合成数据和真实数据上的实验结果表明，所提出的SGL-HNCS方法具有高效性。

Conclusion: 该方法成功解决了符号图学习中存在隐藏节点的问题，通过列稀疏正则化和优化算法有效重构了符号图结构。

Abstract: Signed graphs, which are characterized by both positive and negative edge
weights, have recently attracted significant attention in the field of graph
signal processing (GSP). Existing works on signed graph learning typically
assume that all graph nodes are available. However, in some specific
applications, only a subset of nodes can be observed while the remaining nodes
stay hidden. To address this challenge, we propose a novel method for
identifying signed graph that accounts for hidden nodes, termed \textit{signed
graph learning with hidden nodes under column-sparsity regularization}
(SGL-HNCS). Our method is based on the assumption that graph signals are smooth
over signed graphs, i.e., signal values of two nodes connected by positive
(negative) edges are similar (dissimilar). Rooted in this prior assumption, the
topology inference of a signed graph is formulated as a constrained
optimization problem with column-sparsity regularization, where the goal is to
reconstruct the signed graph Laplacian matrix without disregarding the
influence of hidden nodes. We solve the constrained optimization problem using
a tailored block coordinate descent (BCD) approach. Experimental results using
synthetic data and real-world data demonstrate the efficiency of the proposed
SGL-HNCS method.

</details>


### [8] [Sequential Spectral Clustering of Data Sequences](https://arxiv.org/abs/2509.09144)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: eess.SP

TL;DR: 提出了两种序列谱聚类算法SEQ-SPEC和IA-SEQ-SPEC，用于数据序列的非参数聚类，在有限样本下实现指数一致性，性能优于传统固定样本方法和序列K-Medoids、Single Linkage算法。


<details>
  <summary>Details</summary>
Motivation: 解决数据序列的非参数聚类问题，其中每个序列包含从未知分布生成的i.i.d.样本。传统方法需要固定样本量，而本文旨在通过观察最小样本量来估计聚类，同时控制错误概率。

Method: 提出SEQ-SPEC算法和计算效率更高的IA-SEQ-SPEC算法。SEQ-SPEC基于序列框架，在有限时间内停止且具有指数一致性。IA-SEQ-SPEC是其增量近似版本，计算更高效。

Result: 仿真实验表明，两种算法性能均优于固定样本SPEC、序列K-Medoids和序列Single Linkage算法。IA-SEQ-SPEC在保持计算效率的同时，性能接近SEQ-SPEC。

Conclusion: 这是首个在序列框架下进行数据序列谱聚类的工作，提出的算法在合成和真实数据集上均表现出色，为序列数据聚类提供了有效解决方案。

Abstract: We study the problem of nonparametric clustering of data sequences, where
each data sequence comprises i.i.d. samples generated from an unknown
distribution. The true clusters are the clusters obtained using the Spectral
clustering algorithm (SPEC) on the pairwise distance between the true
distributions corresponding to the data sequences. Since the true distributions
are unknown, the objective is to estimate the clusters by observing the minimum
number of samples from the data sequences for a given error probability. To
solve this problem, we propose the Sequential Spectral clustering algorithm
(SEQ-SPEC), and show that it stops in finite time almost surely and is
exponentially consistent. We also propose a computationally more efficient
algorithm called the Incremental Approximate Sequential Spectral clustering
algorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed
algorithms perform better than the fixed sample size SPEC, the Sequential
$K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage
clustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally
efficient, performs close to SEQ-SPEC on both synthetic and real-world
datasets. To the best of our knowledge, this is the first work on spectral
clustering of data sequences under a sequential framework.

</details>


### [9] [JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information](https://arxiv.org/abs/2509.09147)
*Ziqi Yan,Zhichao Zhang*

Main category: eess.SP

TL;DR: 提出了一种数据-模型协同驱动的去噪方法JFRFFNet，将联合时-顶点分数傅里叶变换域的维纳滤波器嵌入神经网络，通过数据驱动方式更新变换阶数对和滤波器系数，仅需部分先验信息即可有效去噪。


<details>
  <summary>Details</summary>
Motivation: 传统滤波方法需要完整的图信号先验信息，要么使用网格搜索确定变换阶数对和计算滤波器系数，要么使用梯度下降策略优化，这限制了其应用。

Method: 将JFRFT域的维纳滤波器模型嵌入神经网络架构，通过数据驱动方法联合优化变换阶数对和滤波器系数，仅需部分先验信息。

Result: 实验表明JFRFFNet在输出信噪比方面相比现有最先进方法有显著提升。

Conclusion: JFRFFNet成功克服了传统方法需要完整先验信息的限制，实现了在部分先验信息条件下的有效时变图信号去噪。

Abstract: Wiener filtering in the joint time-vertex fractional Fourier transform
(JFRFT) domain has shown high effectiveness in denoising time-varying graph
signals. Traditional filtering models use grid search to determine the
transform-order pair and compute filter coefficients, while learnable ones
employ gradient-descent strategies to optimize them; both require complete
prior information of graph signals. To overcome this shortcoming, this letter
proposes a data-model co-driven denoising approach, termed neural-network-aided
joint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the
JFRFT-domain Wiener filter model into a neural network and updates the
transform-order pair and filter coefficients through a data-driven approach.
This design enables effective denoising using only partial prior information.
Experiments demonstrate that JFRFFNet achieves significant improvements in
output signal-to-noise ratio compared with some state-of-the-art methods.

</details>


### [10] [On Sampling of Multiple Correlated Stochastic Signals](https://arxiv.org/abs/2509.09225)
*Lin Jin,Hang Sheng,Hui Feng,Bo Hu*

Main category: eess.SP

TL;DR: 提出了一种基于相关信号统计特性的高效采样方法，通过建模相关信道为少量不相关潜在源的线性组合，实现了理论最小采样密度下的近无损重建。


<details>
  <summary>Details</summary>
Motivation: 传统独立信道采样方法存在数据冗余，多随机信号具有内在统计相关性，需要利用这种相关性来提高采样效率。

Method: 将相关信道建模为少量不相关宽平稳潜在源的线性组合，通过谱分割、时空采样和插值构建多频带采样方案。

Result: 实验证明该方法在理论采样密度下实现了近无损重建，验证了其效率。

Conclusion: 提出的采样方案达到了理论最小采样密度，有效利用了信号间的统计相关性，减少了数据冗余。

Abstract: Multiple stochastic signals possess inherent statistical correlations, yet
conventional sampling methods that process each channel independently result in
data redundancy. To leverage this correlation for efficient sampling, we model
correlated channels as a linear combination of a smaller set of uncorrelated,
wide-sense stationary latent sources. We establish a theoretical lower bound on
the total sampling density for zero mean-square error reconstruction, proving
it equals the ratio of the joint spectral bandwidth of latent sources to the
number of correlated signal channels. We then develop a constructive multi-band
sampling scheme that attains this bound. The proposed method operates via
spectral partitioning of the latent sources, followed by spatio-temporal
sampling and interpolation. Experiments on synthetic and real datasets confirm
that our scheme achieves near-lossless reconstruction precisely at the
theoretical sampling density, validating its efficiency.

</details>


### [11] [Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG](https://arxiv.org/abs/2509.09264)
*Davoud Hajhassani,Quentin Barthélemy,Jérémie Mattout,Marco Congedo*

Main category: eess.SP

TL;DR: iRPF是一种快速全自动EEG伪影去除方法，在多个指标上显著优于现有方法，处理速度快至每epoch 8毫秒


<details>
  <summary>Details</summary>
Motivation: EEG信号清洁是研究领域的关键挑战，现有方法依赖人工超参数调优、对异常值敏感且计算成本高，需要自动化解决方案

Method: 改进的黎曼土豆场(iRPF)方法，基于黎曼几何的自动化伪影拒绝技术

Result: 在226个EEG记录上测试，iRPF相比其他方法在召回率提升22%，特异性提升102%，精确度提升54%，F1分数提升24%，统计显著(p<0.001)

Conclusion: iRPF为脑机接口和临床神经影像应用提供了强大、数据驱动的EEG预处理解决方案，适用于大规模数据处理和实时应用

Abstract: Electroencephalography (EEG) signal cleaning has long been a critical
challenge in the research community. The presence of artifacts can
significantly degrade EEG data quality, complicating analysis and potentially
leading to erroneous interpretations. While various artifact rejection methods
have been proposed, the gold standard remains manual visual inspection by human
experts-a process that is time-consuming, subjective, and impractical for
large-scale EEG studies. Existing techniques are often hindered by a strong
reliance on manual hyperparameter tuning, sensitivity to outliers, and high
computational costs. In this paper, we introduce the improved Riemannian Potato
Field (iRPF), a fast and fully automated method for EEG artifact rejection that
addresses key limitations of current approaches. We evaluate iRPF against
several state-of-the-art artifact rejection methods, using two publicly
available EEG databases, labeled for various artifact types, comprising 226 EEG
recordings. Our results demonstrate that iRPF outperforms all competitors
across multiple metrics, with gains of up to 22% in recall, 102% in
specificity, 54% in precision, and 24% in F1-score, compared to Isolation
Forest, Autoreject, Riemannian Potato, and Riemannian Potato Field,
respectively. Statistical analysis confirmed the significance of these
improvements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most
comparisons. Additionally, on a typical EEG recording iRPF performs artifact
cleaning in under 8 milliseconds per epoch using a standard laptop,
highlighting its efficiency for large-scale EEG data processing and real-time
applications. iRPF offers a robust and data-driven artifact rejection solution
for high-quality EEG pre-processing in brain-computer interfaces and clinical
neuroimaging applications.

</details>


### [12] [On the Relation of Characteristic Modes of Different Conducting Structures](https://arxiv.org/abs/2509.09282)
*Leonardo Mörlein,Dirk Manteuffel*

Main category: eess.SP

TL;DR: 基于特征模态的形式化方法，用一个结构的特征模态分析另一个包含其中的小结构的散射特性，并定义模态变换矩阵来进行基准转换。


<details>
  <summary>Details</summary>
Motivation: 为了建立一种方法论，能够基于一个基础结构的特征模态来分析和比较不同结构的散射特性，提高设计效率。

Method: 推导形式化方法，利用特征模态作为共同基础，定义模态变换矩阵来描述不同结构间的映射关系，并进行基准转换。

Result: 证明在这种情况下散射矩阵和微氧矩阵不再是对角矩阵，通过两个实例验证了理论的有效性和实际应用价值。

Conclusion: 该形式化方法为天线设计提供了一种有效的工具，能够基于基础结构的特征模态来分析和优化渐进修改的结构。

Abstract: A formalism is derived to analyze the scattering of a conducting structure
based on the characteristic modes of another structure whose surface is a
superset of the first structure. This enables the analysis and comparison of
different structures using a common basis of characteristic modes.
Additionally, it is shown that the scattering matrices and perturbation
matrices are no longer diagonal in these cases. Based on this, a modal
transformation matrix is defined to describe the mapping between the
characteristic fields and the weighting coefficients of the two structures.
This matrix enables the conversion of the perturbation matrices in different
bases. Finally, two examples are provided along with a discussion of some
aspects of the theory. The first example aims to validate and illustrate the
formalism. The second example shows how the formalism can be applied in the
design process of an antenna element that is gradually modified, starting from
a base structure.

</details>


### [13] [Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems](https://arxiv.org/abs/2509.09373)
*Huayan Guo,Jichen Zhang,Junhui Rao,Ross Murch,Vincent K. N. Lau*

Main category: eess.SP

TL;DR: 流体天线系统的频道估计和模拟预编码方案，通过稀疏频道恢复和变分质理论算法，显著提升多用户MIMO-OFDM系统性能


<details>
  <summary>Details</summary>
Motivation: 解决像素基流体天线引入的状态非可分离频道响应问题，提高频道估计和模拟预编码的效率和准确性

Method: 使用近似可分离频道响应模型结合DNN天线辐射函数，提出两种低复杂度算法：正交匹配追踪和变分质理论推断

Result: 在高信噪比环境下特别是多用户场景中，方案显著超过多个基准方法

Conclusion: 该研究为流体天线系统提供了高效的频道估计和预编码解决方案，对推进6G通信技术发展具有重要意义

Abstract: Pixel-based fluid antennas provide enhanced multiplexing gains and quicker
radiation pattern switching than traditional designs. However, this innovation
introduces challenges for channel estimation and analog precoding due to the
state-non-separable channel response problem. This paper explores a multiuser
MIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements
from a real-world prototype. We present a sparse channel recovery framework for
uplink channel sounding, employing an approximate separable channel response
model with DNN-based antenna radiation functions. We then propose two
low-complexity channel estimation algorithms that leverage orthogonal matching
pursuit and variational Bayesian inference to accurately recover channel
responses across various scattering cluster angles. These estimations enable
the prediction of composite channels for all fluid antenna states, leading to
an analog precoding scheme that optimally selects switching states for
different antennas. Our simulation results indicate that the proposed approach
significantly outperforms several baseline methods, especially in high
signal-to-noise ratio environments with numerous users.

</details>


### [14] [A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks](https://arxiv.org/abs/2509.09606)
*Sajjad Hussain*

Main category: eess.SP

TL;DR: 提出基于UNet的深度学习架构，结合多尺度特征提取和ASPP瓶颈，用于无人机毫米波网络路径损耗预测，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在路径损耗预测中泛化能力不足、对噪声输入鲁棒性差以及对无人机高度敏感等问题，提升无人机毫米波网络的设计和优化效果。

Method: 采用UNet架构，结合多尺度特征提取、卷积特征融合和ASPP瓶颈进行上下文聚合，输入包括对数距离、LOS掩码和建筑掩码，并开发了向量化LOS掩码计算算法加速预处理。

Result: 在内部射线追踪数据和RadioMapSeer基准测试中，模型在准确性和效率上均优于多个最先进的基线方法。

Conclusion: 所提出的模型在路径损耗预测方面表现出色，代码已公开以支持可重复性和未来研究。

Abstract: Accurate pathloss prediction is essential for the design and optimization of
UAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches
have shown strong potential, their generalization across diverse environments,
robustness to noisy inputs, and sensitivity to UAV altitude remain
underexplored. To address these challenges, we propose a UNet-based deep
learning architecture that combines multi-scale feature extraction,
convolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)
bottleneck for efficient context aggregation. The model predicts pathloss maps
from log-distance, line-of-sight (LOS) mask, and building mask inputs. In
addition, we develop a fully vectorized LOS mask computation algorithm that
significantly accelerates pre-processing and enables large-scale dataset
generation. Extensive evaluations on both in-house ray-tracing data and the
RadioMapSeer benchmark demonstrate that the proposed model outperforms several
state-of-the-art baselines in accuracy and efficiency. All source code is
publicly released to support reproducibility and future research.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [15] [Automotive sound field reproduction using deep optimization with spatial domain constraint](https://arxiv.org/abs/2509.09149)
*Yufan Qian,Tianshu Qu,Xihong Wu*

Main category: eess.AS

TL;DR: SPMnet是一种基于学习的声场重现方法，通过空间功率图约束和神经网络优化，在汽车座舱复杂声学环境中同时提升音质和空间定位精度。


<details>
  <summary>Details</summary>
Motivation: 汽车座舱声学环境复杂，传统方法需要在音质和空间定位精度之间做出权衡，需要一种能够同时改善两者的解决方案。

Method: 提出空间功率图(SPM)约束来表征重现声场的角度能量分布，通过波束成形引导能量朝向目标方向；采用多通道均衡框架改善混响条件下的音质；使用神经网络解决由此产生的非凸优化问题进行滤波器设计。

Result: 通过现场客观和主观评估证实，该方法在汽车座舱内增强了音质并改善了空间定位。还分析了不同音频材料和虚拟声源到达角度对重现声场的影响。

Conclusion: SPMnet方法成功解决了汽车音频系统中音质与空间定位精度之间的权衡问题，为复杂声学环境下的声场重现提供了有效解决方案。

Abstract: Sound field reproduction with undistorted sound quality and precise spatial
localization is desirable for automotive audio systems. However, the complexity
of automotive cabin acoustic environment often necessitates a trade-off between
sound quality and spatial accuracy. To overcome this limitation, we propose
Spatial Power Map Net (SPMnet), a learning-based sound field reproduction
method that improves both sound quality and spatial localization in complex
environments. We introduce a spatial power map (SPM) constraint, which
characterizes the angular energy distribution of the reproduced field using
beamforming. This constraint guides energy toward the intended direction to
enhance spatial localization, and is integrated into a multi-channel
equalization framework to also improve sound quality under reverberant
conditions. To address the resulting non-convexity, deep optimization that use
neural networks to solve optimization problems is employed for filter design.
Both in situ objective and subjective evaluations confirm that our method
enhances sound quality and improves spatial localization within the automotive
cabin. Furthermore, we analyze the influence of different audio materials and
the arrival angles of the virtual sound source in the reproduced sound field,
investigating the potential underlying factors affecting these results.

</details>


### [16] [MAPSS: Manifold-based Assessment of Perceptual Source Separation](https://arxiv.org/abs/2509.09212)
*Amir Ivry,Samuele Cornell,Shinji Watanabe*

Main category: eess.AS

TL;DR: 这篇论文提出了Perceptual Separation (PS)和Perceptual Match (PM)评估方法，通过流形多层面映射和马哈拉比斯距离，准确地分离漏泰和自我失真两个因素，在语音和音乐混合信号中实现了与主观人类感知最高的相关系数（86.36%-87.21%）。


<details>
  <summary>Details</summary>
Motivation: 当前的源分离系统客观评估方法与主观人类感知存在误差，特别是当漏泰和自我失真互相作用时。需要一种能够功能性地分离这两个因素的评测方法。

Method: 首先为每个参考波形信号生成基础失真库，然后使用预训练的自监督学习模型独立编码失真、参考和系统输出。通过流形多层面映射将表征聚合并投射到多层面上，使欧几里得距离与编码波形的相似性对齐。在这个多层面上，PM测量输出与其归属聚类（包含参考和失真嵌入）的马哈拉比斯距离（捕获自我失真），PS计算输出与归属聚类和最近非归属聚类的马哈拉比斯距离（量化漏泰）。

Result: 在英语、西班牙语和音乐混合信右上的实验显示，PS和PM在14个竞争方法中几乎总是获得最高的与人类平均意见分数的线性相关系数，语音达到86.36%，音乐达到87.21%。最坏情况下的误差半径为1.39%，概率95%置信区间为12.21%。

Conclusion: PS和PM是首个能够功能性地分离漏泰和自我失真的评估方法，具有微分性和高粒度特性，在语音和音乐源分离系统评估中表现優异，为可靠和知情的系统评估提供了改进。通过互信息分析发现，两个测量在系统性能下降时互补性最强，表明它们在系统性能退化时共同提供更多信息。

Abstract: Objective assessment of source-separation systems still mismatches subjective
human perception, especially when leakage and self-distortion interact. We
introduce the Perceptual Separation (PS) and Perceptual Match (PM), the first
pair of measures that functionally isolate these two factors. Our intrusive
method begins with generating a bank of fundamental distortions for each
reference waveform signal in the mixture. Distortions, references, and their
respective system outputs from all sources are then independently encoded by a
pre-trained self-supervised learning model. These representations are
aggregated and projected onto a manifold via diffusion maps, which aligns
Euclidean distances on the manifold with dissimilarities of the encoded
waveforms. On this manifold, the PM measures the Mahalanobis distance from each
output to its attributed cluster that consists of its reference and distortions
embeddings, capturing self-distortion. The PS accounts for the Mahalanobis
distance of the output to the attributed and to the closest non-attributed
clusters, quantifying leakage. Both measures are differentiable and granular,
operating at a resolution as low as 50 frames per second. We further derive,
for both measures, deterministic error radius and non-asymptotic,
high-probability confidence intervals (CIs). Experiments on English, Spanish,
and music mixtures show that the PS and PM nearly always achieve the highest
linear correlation coefficients with human mean-opinion scores than 14
competitors, reaching as high as 86.36% for speech and 87.21% for music. We
observe, at worst, an error radius of 1.39% and a probabilistic 95% CI of
12.21% for these coefficients, which improves reliable and informed evaluation.
Using mutual information, the measures complement each other most as their
values decrease, suggesting they are jointly more informative as system
performance degrades.

</details>


### [17] [Over-the-Air Adversarial Attack Detection: from Datasets to Defenses](https://arxiv.org/abs/2509.09296)
*Li Wang,Xiaoyan Lei,Haorui He,Lei Wang,Jie Shi,Zhizheng Wu*

Main category: eess.AS

TL;DR: AdvSV 2.0数据集包含62.8万样本、800小时音频，涵盖OTL和OTA对抗攻击场景，并提出基于神经重放模拟器的新攻击方法和CODA-OCC防御方法


<details>
  <summary>Details</summary>
Motivation: 现有ASV系统易受对抗攻击，但缺乏全面数据集来测试防御方法，需要开发更强大的数据集和防御机制

Method: 开发AdvSV 2.0数据集，包含古典对抗攻击算法；提出基于神经重放模拟器(NRS)的新攻击方法；设计CODA-OCC对比学习防御框架

Result: CODA-OCC在AdvSV 2.0数据集上达到11.2%的EER和0.95的AUC，优于现有检测方法

Conclusion: AdvSV 2.0数据集填补了研究空白，NRS攻击方法增强了OTA攻击威胁，CODA-OCC防御方法有效提升了ASV系统的安全性

Abstract: Automatic Speaker Verification (ASV) systems can be used for voice-enabled
applications for identity verification. However, recent studies have exposed
these systems' vulnerabilities to both over-the-line (OTL) and over-the-air
(OTA) adversarial attacks. Although various detection methods have been
proposed to counter these threats, they have not been thoroughly tested due to
the lack of a comprehensive data set. To address this gap, we developed the
AdvSV 2.0 dataset, which contains 628k samples with a total duration of 800
hours. This dataset incorporates classical adversarial attack algorithms, ASV
systems, and encompasses both OTL and OTA scenarios. Furthermore, we introduce
a novel adversarial attack method based on a Neural Replay Simulator (NRS),
which enhances the potency of adversarial OTA attacks, thereby presenting a
greater threat to ASV systems. To defend against these attacks, we propose
CODA-OCC, a contrastive learning approach within the one-class classification
framework. Experimental results show that CODA-OCC achieves an EER of 11.2% and
an AUC of 0.95 on the AdvSV 2.0 dataset, outperforming several state-of-the-art
detection methods.

</details>


### [18] [Listening for "You": Enhancing Speech Image Retrieval via Target Speaker Extraction](https://arxiv.org/abs/2509.09306)
*Wenhao Yang,Jianguo Wei,Wenhuan Lu,Xinyue Song,Xianghu Yue*

Main category: eess.AS

TL;DR: 这篇论文提出了一种新的目标讲话人语音-图像检索框架，通过目标讲话人感知对比学习，在多讲话人场景中实现了语音指令与对应图像的准确匹配，在2人和3人场景不分别获得36.3%和29.9%的Recall@1绩效。


<details>
  <summary>Details</summary>
Motivation: 虽然使用语音索引图像在多模态感知中带来了机遇，但在多讲话人场景中利用语音仍面临挑战。需要一种能够在存在多个讲话人的情况下学习图像与语音信号关系的方法。

Method: 提出了一种新的目标讲话人语音-图像检索任务和框架，通过目标讲话人感知对比学习方法，将预训练的自监督音频编码器与视觉模型集成。该方法基于目标讲话人提取和检索模块进行条件化处理，能够从目标讲话人提取语音命令并将其与对应图像对齐。

Result: 在SpokenCOCO2Mix和SpokenCOCO3Mix数据集上的实验显示，TSRE方法显著超过现有方法，在2人和3人场景下分别达到36.3%和29.9%的Recall@1指标，这是对单讲话人基线和最新模型的显著改进。

Conclusion: 该方法在多讲话人语音-图像检索任务中表现出艰强的性能，显示了在辅助机器人和多模态交互系统中实际部署的潜力。

Abstract: Image retrieval using spoken language cues has emerged as a promising
direction in multimodal perception, yet leveraging speech in multi-speaker
scenarios remains challenging. We propose a novel Target Speaker Speech-Image
Retrieval task and a framework that learns the relationship between images and
multi-speaker speech signals in the presence of a target speaker. Our method
integrates pre-trained self-supervised audio encoders with vision models via
target speaker-aware contrastive learning, conditioned on a Target Speaker
Extraction and Retrieval module. This enables the system to extract spoken
commands from the target speaker and align them with corresponding images.
Experiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantly
outperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3
speaker scenarios, respectively - substantial improvements over single speaker
baselines and state-of-the-art models. Our approach demonstrates potential for
real-world deployment in assistive robotics and multimodal interaction systems.

</details>


### [19] [Short-term cognitive fatigue of spatial selective attention after face-to-face conversations in virtual noisy environments](https://arxiv.org/abs/2509.09479)
*Ľuboš Hládek,Piotr Majdak,Robert Baumgartner*

Main category: eess.AS

TL;DR: 这篇论文研究了在噪声环境中进行英劳对话是否会影响听觉空间选择性注意力。结果发现与预期相反，噪声中对话后反应时间减少，准确性未受影响，而且显示出强烈的训练效应。


<details>
  <summary>Details</summary>
Motivation: 研究短期认知疲劳是否会影响听觉空间选择性注意力，特别是在高度实际的酒会环境中进行英劳对话后的效果。

Method: 年轻正常听力参与者在虚拟游移房间中进行听觉空间选择性注意力任务，对比三种条件：(1)在72dB噪声中进行30分钟对话，(2)被动听噪声，(3)在安静环境中对话。

Result: 噪声中对话和被动听噪声后自我报告的英劳感增加，但任务表现不降反升：反应时间减少，准确性无系统性变化，而且显示出显著的训练效应。

Conclusion: 英劳对话在噪声环境中并不影听觉空间选择性注意力功能，反而可能通过训练效应提高表现，这与预期相反。

Abstract: Spatial selective attention is an important asset for communication in
cocktail party situations but may be compromised by short-term cognitive
fatigue. Here we tested whether an effortful conversation in a highly
ecological setting depletes task performance in an auditory spatial selective
attention task. Young participants with normal hearing performed the task
before and after (1) having a real dyadic face-to-face conversation on a free
topic in a virtual reverberant room with simulated interfering conversations
and background babble noise at 72 dB SPL for 30 minutes, (2) passively
listening to the interfering conversations and babble noise, or (3) having the
conversation in quiet. Self-reported perceived effort and fatigue increased
after conversations in noise and passive listening relative to the reports
after conversations in quiet. In contrast to our expectations, response times
in the attention task decreased, rather than increased, after conversation in
noise and accuracy did not change systematically in any of the conditions on
the group level. Unexpectedly, we observed strong training effects between the
individual sessions in our within-subject design even after one hour of
training on a different day.

</details>


### [20] [Acoustic to Articulatory Speech Inversion for Children with Velopharyngeal Insufficiency](https://arxiv.org/abs/2509.09489)
*Saba Tabatabaee,Suzanne Boyce,Liran Oren,Mark Tiede,Carol Espy-Wilson*

Main category: eess.AS

TL;DR: 通过语音反向技术结合唱音源信息，提升了成人和VPI儿童鼻音评估的准确性，提供了无创伤的临床评估方案


<details>
  <summary>Details</summary>
Motivation: 传统鼻咽镜和鼻测计方法对儿童不舒适且存在问题，需要一种无创伤的语音反向技术来估计叙汇运动

Method: 在健康成人鼻音数据训练的SI系统中，增强了来自电唱图的源信息和音频引导的F0、周期性和非周期性能量估计，然后用VPI儿童数据微调模型

Result: 与之前的SI系统相比，鼻音估计的相关系数相对提升16.92%；细调后对VPI儿童的表现相对提升7.90%

Conclusion: 语音反向技术结合唱音源信息能够有效提升鼻音估计准确性，通过成人数据训练后细调适配儿童患者，为无创伤临床评估提供了可行方案

Abstract: Traditional clinical approaches for assessing nasality, such as
nasopharyngoscopy and nasometry, involve unpleasant experiences and are
problematic for children. Speech Inversion (SI), a noninvasive technique,
offers a promising alternative for estimating articulatory movement without the
need for physical instrumentation. In this study, an SI system trained on
nasalance data from healthy adults is augmented with source information from
electroglottography and acoustically derived F0, periodic and aperiodic energy
estimates as proxies for glottal control. This model achieves 16.92% relative
improvement in Pearson Product-Moment Correlation (PPMC) compared to a previous
SI system for nasalance estimation. To adapt the SI system for nasalance
estimation in children with Velopharyngeal Insufficiency (VPI), the model
initially trained on adult speech was fine-tuned using children with VPI data,
yielding an 7.90% relative improvement in PPMC compared to its performance
before fine-tuning.

</details>


### [21] [Region-Specific Audio Tagging for Spatial Sound](https://arxiv.org/abs/2509.09526)
*Jinzheng Zhao,Yong Xu,Haohe Liu,Davide Berghi,Xinyuan Qian,Qiuqiang Kong,Junqi Zhao,Mark D. Plumbley,Wenwu Wang*

Main category: eess.AS

TL;DR: 本文提出区域特定音频标签新任务，通过结合谱、空间咄位置特征，扩展PANNs咄AST模型，实现了在指定角度或距离区域内的音响事件标签，实验结果证明了方法的有效性咄方向特征的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统音频标签任务只能标记整个音频中的音响事件，无法精确定位到特定空间区域，需要一种能够在空间音频中指定区域识别音响事件的新方法。

Method: 首先研究了不同谱、空间咄位置特征的组合效果，然后扩展了预训练音频神经网络(PANNs)咄音频谱变换器(AST)等领先音频标签系统，以适应新的区域特定标签任务。

Result: 在模拟咄实际数据集上的实验结果显示，新任务是可行的，且提出的方法有效。进一步实验还表明，融入方向特征对全向标签任务也有益。

Conclusion: 本文成功提出并实现了区域特定音频标签新任务，通过结合多种特征咄扩展现有模型，能够在指定空间区域内识别音响事件，为空间音频处理提供了新的解决方案。

Abstract: Audio tagging aims to label sound events appearing in an audio recording. In
this paper, we propose region-specific audio tagging, a new task which labels
sound events in a given region for spatial audio recorded by a microphone
array. The region can be specified as an angular space or a distance from the
microphone. We first study the performance of different combinations of
spectral, spatial, and position features. Then we extend state-of-the-art audio
tagging systems such as pre-trained audio neural networks (PANNs) and audio
spectrogram transformer (AST) to the proposed region-specific audio tagging
task. Experimental results on both the simulated and the real datasets show the
feasibility of the proposed task and the effectiveness of the proposed method.
Further experiments show that incorporating the directional features is
beneficial for omnidirectional tagging.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [22] [In situ estimation of the acoustic surface impedance using simulation-based inference](https://arxiv.org/abs/2509.08873)
*Jonas M. Schmid,Johannes D. Schmid,Martin Eser,Steffen Marburg*

Main category: cs.SD

TL;DR: 提出基于贝叶斯框架和神经网络的方法，从稀疏声压测量中现场估计频率相关的声学表面阻抗，克服传统测量方法的局限性


<details>
  <summary>Details</summary>
Motivation: 传统声学边界条件测量方法依赖简化假设，限制了其在真实场景中的有效性，需要开发更精确的现场阻抗估计方法

Method: 采用基于模拟推理的贝叶斯框架，利用神经网络架构直接将模拟数据映射到模型参数的后验分布，使用带分数阶微积分项的阻尼振荡器模型建模阻抗行为

Result: 在立方体房间有限元模型上验证，通过阻抗管测量作为参考，成功估计所有六个独立阻抗，在汽车舱室模型中展示了可靠的 uncertainty 量化和高预测精度

Conclusion: 该方法具有良好校准的推理能力，为真实室内环境中的声学边界条件提供了可推广、高效且物理一致的表征方法

Abstract: Accurate acoustic simulations of enclosed spaces require precise boundary
conditions, typically expressed through surface impedances for wave-based
methods. Conventional measurement techniques often rely on simplifying
assumptions about the sound field and mounting conditions, limiting their
validity for real-world scenarios. To overcome these limitations, this study
introduces a Bayesian framework for the in situ estimation of
frequency-dependent acoustic surface impedances from sparse interior sound
pressure measurements. The approach employs simulation-based inference, which
leverages the expressiveness of modern neural network architectures to directly
map simulated data to posterior distributions of model parameters, bypassing
conventional sampling-based Bayesian approaches and offering advantages for
high-dimensional inference problems. Impedance behavior is modeled using a
damped oscillator model extended with a fractional calculus term. The framework
is verified on a finite element model of a cuboid room and further tested with
impedance tube measurements used as reference, achieving robust and accurate
estimation of all six individual impedances. Application to a numerical car
cabin model further demonstrates reliable uncertainty quantification and high
predictive accuracy even for complex-shaped geometries. Posterior predictive
checks and coverage diagnostics confirm well-calibrated inference, highlighting
the method's potential for generalizable, efficient, and physically consistent
characterization of acoustic boundary conditions in real-world interior
environments.

</details>


### [23] [MoLEx: Mixture of LoRA Experts in Speech Self-Supervised Models for Audio Deepfake Detection](https://arxiv.org/abs/2509.09175)
*Zihan Pan,Sailor Hardik Bhupendra,Jinyang Wu*

Main category: cs.SD

TL;DR: 提出MoLEx框架，结合LoRA和混合专家路由，实现参数高效的音频深度伪造检测，在ASVSpoof 5数据集上达到5.56%的SOTA错误率


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习模型完全微调计算成本高的问题，同时保持预训练知识

Method: 结合低秩适应(LoRA)和混合专家路由，只微调选定的专家模块，降低训练成本

Result: 在ASVSpoof 5评估集上达到5.56%的等错误率(EER)，无需数据增强

Conclusion: MoLEx框架在保持性能的同时显著降低计算成本，具有领域感知适应性和灵活性

Abstract: While self-supervised learning (SSL)-based models have boosted audio deepfake
detection accuracy, fully finetuning them is computationally expensive. To
address this, we propose a parameter-efficient framework that combines Low-Rank
Adaptation with a Mixture-of-Experts router, called Mixture of LoRA Experts
(MoLEx). It preserves pre-trained knowledge of SSL models while efficiently
finetuning only selected experts, reducing training costs while maintaining
robust performance. The observed utility of experts during inference shows the
router reactivates the same experts for similar attacks but switches to other
experts for novel spoofs, confirming MoLEx's domain-aware adaptability. MoLEx
additionally offers flexibility for domain adaptation by allowing extra experts
to be trained without modifying the entire model. We mainly evaluate our
approach on the ASVSpoof 5 dataset and achieve the state-of-the-art (SOTA)
equal error rate (EER) of 5.56% on the evaluation set without augmentation.

</details>


### [24] [DeCodec: Rethinking Audio Codecs as Universal Disentangled Representation Learners](https://arxiv.org/abs/2509.09201)
*Xiaoxue Luo,Jinwei Huang,Runyan Yang,Yingying Gao,Junlan Feng,Chao Deng,Shilei Zhang*

Main category: cs.SD

TL;DR: DeCodec是一个新颖的神经音频编解码器，通过正交子空间投影和表示交换训练，将音频表示解耦为语音和背景声音的正交子空间，并在语音内部进一步分解为语义和副语言成分，实现分层解耦表示学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界音频通常包含混合的语音和背景声音，下游任务需要选择性访问这些组件。现有通用音频编解码器学习纠缠表示，而特定编解码器虽然提供解耦表示但仅限于语音，因此需要开发能够解耦表示的通用音频编解码器。

Method: 基于编解码器框架，DeCodec包含两个关键创新：1）子空间正交投影模块将输入分解为两个解耦的正交子空间；2）表示交换训练程序确保这两个子空间分别与语音和背景声音相关。使用并行RVQ独立量化语音和背景声音组件，并通过语义引导实现语音内部的语义和副语言分解。

Result: 实验结果显示DeCodec在保持先进信号重建的同时实现了新能力：通过表示重组在嘈杂语音上实现优越的语音增强和有效的一次性语音转换，通过干净的语义特征提高ASR鲁棒性，以及在TTS中实现可控的背景声音保留/抑制。

Conclusion: DeCodec作为一个通用的解耦表示学习器，通过分层解耦表示实现了灵活的特征选择，使其成为多个音频应用的通用前端，为音频处理任务提供了新的可控能力。

Abstract: Universal audio codecs learn entangled representations across audio types,
whereas some specific codecs offer decoupled representations but are limited to
speech. Real-world audio, however, often contains mixed speech and background
sounds, and downstream tasks require selective access to these components.
Therefore, we rethink the audio codec as a universal disentangled
representation learner to enable controllable feature selection across
different audio tasks. To this end, we introduce DeCodec, a novel neural codec
that learns to decouple audio representations into orthogonal subspaces
dedicated to speech and background sound, and within speech, representations
are further decomposed into semantic and paralinguistic components. This
hierarchical disentanglement allows flexible feature selection, making DeCodec
a universal front-end for multiple audio applications. Technically, built upon
a codec framework, DeCodec incorporates two key innovations: a subspace
orthogonal projection module that factorizes the input into two decoupled
orthogonal subspaces, and a representation swap training procedure that ensures
these two subspaces are correlate to the speech and background sound,
respectively. These allows parallel RVQs to quantize speech and background
sound components independently. Furthermore, we employ semantic guidance to the
speech RVQ to achieve semantic and paralinguistic decomposition. Experimental
results show that DeCodec maintains advanced signal reconstruction while
enabling new capabilities: superior speech enhancement and effective one-shot
voice conversion on noisy speech via representation recombination, improved ASR
robustness through clean semantic features, and controllable background sound
preservation/suppression in TTS. Demo Page: https://luo404.github.io/DeCodecV2/

</details>


### [25] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: 提出了bona fide cross-testing评估框架，通过整合多样化的真实语音数据集和聚合EER来解决音频深度伪造检测中传统评估方法的局限性，提高了检测模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统音频深度伪造检测评估方法存在两个主要问题：1）使用单一EER指标会因合成器样本数量不均而降低评估可靠性；2）大多数数据集缺乏真实语音的多样性，通常只包含单一环境和语音风格，无法模拟真实世界条件。

Method: 提出bona fide cross-testing评估框架，整合多个多样化的真实语音数据集，采用聚合EER的方式进行更平衡的评估。在9种真实语音类型上对150多个合成器进行了基准测试，并发布了新的数据集。

Result: 新框架相比传统评估方法提高了检测模型的鲁棒性和可解释性，为音频深度伪造检测提供了更可靠的评估标准。

Conclusion: bona fide cross-testing框架有效解决了传统评估方法的局限性，通过引入真实语音多样性和平衡的EER聚合方法，为音频深度伪造检测领域提供了更准确和实用的评估方案。

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [26] [Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification](https://arxiv.org/abs/2509.09262)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: 基于知识蒸馏框架的低复杂度设备鲁棒声学场景分类系统，使用双教师集成（标准PaSST和DAFA专家）训练CP-MobileNet学生模型，并通过设备特定微调实现57.93%的准确率


<details>
  <summary>Details</summary>
Motivation: 解决DCASE 2025挑战中严格复杂度约束和设备鲁棒性双重挑战，同时利用测试时设备标签的新规则

Method: 知识蒸馏框架：CP-MobileNet学生从双教师集成（标准交叉熵训练的PaSST教师+设备感知特征对齐损失训练的专家教师）学习，最后进行设备特定微调

Result: 在开发集上达到57.93%的最终准确率，相比官方基线有显著提升，特别是在未见设备上表现优异

Conclusion: 提出的知识蒸馏框架结合设备感知特征对齐和设备特定微调，有效解决了低复杂度设备鲁棒声学场景分类问题

Abstract: In this technical report, we describe our submission for Task 1,
Low-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025
Challenge. Our work tackles the dual challenges of strict complexity
constraints and robust generalization to both seen and unseen devices, while
also leveraging the new rule allowing the use of device labels at test time.
Our proposed system is based on a knowledge distillation framework where an
efficient CP-MobileNet student learns from a compact, specialized two-teacher
ensemble. This ensemble combines a baseline PaSST teacher, trained with
standard cross-entropy, and a 'generalization expert' teacher. This expert is
trained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted
from prior work, which explicitly structures the feature space for device
robustness. To capitalize on the availability of test-time device labels, the
distilled student model then undergoes a final device-specific fine-tuning
stage. Our proposed system achieves a final accuracy of 57.93\% on the
development set, demonstrating a significant improvement over the official
baseline, particularly on unseen devices.

</details>


### [27] [Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms](https://arxiv.org/abs/2509.09318)
*Weixing Wei,Kazuyoshi Yoshii*

Main category: cs.SD

TL;DR: 通过稀疏注意力机制改进变奏器模型，在保持钢琴转载性能的同时大幅降低计算成本和内存使用，加快推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统变奏器模型的自注意力机制带来二次方复杂度，无法处理整个音乐作品，需要分段处理，影响性能。

Method: 提出使用滑动窗口自注意力机制（编码器和解码器）、混合全局-局部交叉注意力机制（根据MIDI标记类型调整关注范围）以及层次池化策略来降低计算复杂度。

Result: 在MAESTRO数据集上实验显示，模型在保持与全注意力基线相似转载性能的同时，实现了计算成本和内存使用的显著降低，推理速度加快，能够在同样硬件上处理更长的音频上下文。

Conclusion: 稀疏注意力机制可以用于构建高效且高性能的钢琴转载系统，解决了传统变奏器模型在长度依赖性捕捉方面的限制。

Abstract: This paper investigates automatic piano transcription based on
computationally-efficient yet high-performant variants of the Transformer that
can capture longer-term dependency over the whole musical piece. Recently,
transformer-based sequence-to-sequence models have demonstrated excellent
performance in piano transcription. These models, however, fail to deal with
the whole piece at once due to the quadratic complexity of the self-attention
mechanism, and music signals are thus typically processed in a sliding-window
manner in practice. To overcome this limitation, we propose an efficient
architecture with sparse attention mechanisms. Specifically, we introduce
sliding-window self-attention mechanisms for both the encoder and decoder, and
a hybrid global-local cross-attention mechanism that attends to various spans
according to the MIDI token types. We also use a hierarchical pooling strategy
between the encoder and decoder to further reduce computational load. Our
experiments on the MAESTRO dataset showed that the proposed model achieved a
significant reduction in computational cost and memory usage, accelerating
inference speed, while maintaining transcription performance comparable to the
full-attention baseline. This allows for training with longer audio contexts on
the same hardware, demonstrating the viability of sparse attention for building
efficient and high-performance piano transcription systems. The code is
available at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.

</details>


### [28] [Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates](https://arxiv.org/abs/2509.09550)
*Harry Julia,Rachel Beeson,Lohith Konathala,Johanna Ulin,Jiameng Gao*

Main category: cs.SD

TL;DR: NeuCodec是一种基于有限标量量化(FSQ)的神经音频编解码器，相比传统的残差向量量化(RVQ)，FSQ具有训练简单、天然支持单码本的优势，并且在噪声信道传输中表现出更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经音频编解码器大多依赖残差向量量化(RVQ)，而有限标量量化(FSQ)作为一种新兴的替代方案，能够简化训练过程并天然支持单码本。研究旨在探索FSQ在音频编码中的优势，特别是在噪声信道传输环境下的鲁棒性表现。

Method: 提出了基于FSQ的NeuCodec编解码器。通过编码器蒸馏实验，证明不同编码器可以将相同音频编码为差异巨大的码序列，同时保持相同的重建质量。通过模拟噪声信道传输，比较RVQ和FSQ在比特级扰动下的鲁棒性表现。

Result: 实验表明FSQ编码具有内置冗余特性，使其在噪声信道传输中更加鲁棒。两个不同的编码器能够产生截然不同的码序列但保持相近的重建质量。FSQ在比特级扰动鲁棒性方面显著优于RVQ。

Conclusion: FSQ为基础的神经音频编解码器在保持高质量音频重建的同时，提供了更好的噪声信道传输鲁棒性，这使其成为实际应用中更具吸引力的选择，特别是在需要可靠音频传输的场景中。

Abstract: Neural Audio Codecs (NACs) have become increasingly adopted in speech
processing tasks due to their excellent rate-distortion performance and
compatibility with Large Language Models (LLMs) as discrete feature
representations for audio generation. While most existing codecs rely on
Residual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has
recently emerged as a compelling alternative that simplifies training and
natively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,
and show that FSQ encodes baked-in redundancy which produces an encoding which
is robust when transmitted through noisy channels. First, through an encoder
distillation experiment, we show that two different encoders can learn to
encode identical audio into vastly different code sequences whilst maintaining
comparable reconstruction quality with the same quantizer and decoder. Second,
we demonstrate that FSQ has vastly superior bit-level perturbation robustness
by comparing the performance of RVQ and FSQ codecs when simulating the
transmission of code sequences through a noisy channel.

</details>


### [29] [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
*Ngoc-Son Nguyen,Hieu-Nghia Huynh-Nguyen,Thanh V. T. Tran,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TL;DR: DiFlow-TTS是首个探索纯离散流匹配的语音合成模型，通过显式建模分解的语音属性，在零样本设置下实现高质量语音合成，推理速度比现有基线快25.8倍


<details>
  <summary>Details</summary>
Motivation: 解决现有零样本文本转语音方法推理速度慢、存在重复伪影的问题，充分利用离散表示的优势，避免将离散标记嵌入连续空间带来的潜在问题

Method: 采用纯离散流匹配方法，在紧凑统一的架构中显式建模分解的语音属性。利用上下文学习，基于文本内容和参考语音提取的韵律、声学属性进行条件生成，使用分解的流预测机制分别处理韵律和声学细节

Result: 在自然度、韵律、说话人风格保持和能量控制等关键指标上表现优异，模型紧凑且实现低延迟推理，生成速度比最新基线快25.8倍

Conclusion: DiFlow-TTS证明了纯离散流匹配在语音合成中的有效性，为高质量、高效率的零样本TTS提供了新的解决方案

Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

</details>

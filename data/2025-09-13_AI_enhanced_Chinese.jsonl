{"id": "2509.08873", "categories": ["cs.SD", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2509.08873", "abs": "https://arxiv.org/abs/2509.08873", "authors": ["Jonas M. Schmid", "Johannes D. Schmid", "Martin Eser", "Steffen Marburg"], "title": "In situ estimation of the acoustic surface impedance using simulation-based inference", "comment": null, "summary": "Accurate acoustic simulations of enclosed spaces require precise boundary\nconditions, typically expressed through surface impedances for wave-based\nmethods. Conventional measurement techniques often rely on simplifying\nassumptions about the sound field and mounting conditions, limiting their\nvalidity for real-world scenarios. To overcome these limitations, this study\nintroduces a Bayesian framework for the in situ estimation of\nfrequency-dependent acoustic surface impedances from sparse interior sound\npressure measurements. The approach employs simulation-based inference, which\nleverages the expressiveness of modern neural network architectures to directly\nmap simulated data to posterior distributions of model parameters, bypassing\nconventional sampling-based Bayesian approaches and offering advantages for\nhigh-dimensional inference problems. Impedance behavior is modeled using a\ndamped oscillator model extended with a fractional calculus term. The framework\nis verified on a finite element model of a cuboid room and further tested with\nimpedance tube measurements used as reference, achieving robust and accurate\nestimation of all six individual impedances. Application to a numerical car\ncabin model further demonstrates reliable uncertainty quantification and high\npredictive accuracy even for complex-shaped geometries. Posterior predictive\nchecks and coverage diagnostics confirm well-calibrated inference, highlighting\nthe method's potential for generalizable, efficient, and physically consistent\ncharacterization of acoustic boundary conditions in real-world interior\nenvironments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u548c\u6a21\u62df\u63a8\u7406\u6765\u7cbe\u786e\u4f30\u8ba1\u5ba4\u5185\u58f0\u5b66\u8868\u9762\u963b\u6297\uff0c\u514d\u9664\u4e86\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u58f0\u5b66\u8fb9\u754c\u6761\u4ef6\u6d4b\u91cf\u6280\u672f\u5b58\u5728\u504f\u5dee\uff0c\u65b9\u6cd5\u6709\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u53ef\u9760\u7684\u539f\u5730\u963b\u6297\u4f30\u8ba1\u65b9\u6cd5\u6765\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\u548c\u6a21\u62df\u57fa\u4e8e\u63a8\u7406\uff0c\u5229\u7528\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u76f4\u63a5\u5c06\u6a21\u62df\u6570\u636e\u6620\u5c04\u5230\u53c2\u6570\u540e\u9a8c\u5206\u5e03\uff0c\u4f7f\u7528\u5e26\u5206\u6570\u5fae\u79ef\u5206\u9879\u7684\u963b\u5c3c\u6a21\u578b\u6765\u5efa\u6a21\u963b\u6297\u884c\u4e3a\u3002", "result": "\u5728\u7acb\u65b9\u4f53\u623f\u95f4\u6709\u9650\u5143\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u963b\u6297\u7ba1\u6d4b\u91cf\u9a8c\u8bc1\uff0c\u6210\u529f\u4f30\u8ba1\u6240\u67096\u4e2a\u5355\u72ec\u963b\u6297\uff0c\u5728\u6570\u503c\u6c7d\u8f66\u8f66\u5eca\u6a21\u578b\u4e2d\u4e5f\u5c55\u73b0\u4e86\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u901a\u7528\u3001\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u58f0\u5b66\u8fb9\u754c\u6761\u4ef6\u7279\u5f81\u5316\uff0c\u5177\u6709\u826f\u597d\u7684\u63a8\u7406\u68c0\u9a8c\u548c\u8986\u76d6\u8bca\u65ad\u786e\u8ba4\u3002"}}
{"id": "2509.09175", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09175", "abs": "https://arxiv.org/abs/2509.09175", "authors": ["Zihan Pan", "Sailor Hardik Bhupendra", "Jinyang Wu"], "title": "MoLEx: Mixture of LoRA Experts in Speech Self-Supervised Models for Audio Deepfake Detection", "comment": null, "summary": "While self-supervised learning (SSL)-based models have boosted audio deepfake\ndetection accuracy, fully finetuning them is computationally expensive. To\naddress this, we propose a parameter-efficient framework that combines Low-Rank\nAdaptation with a Mixture-of-Experts router, called Mixture of LoRA Experts\n(MoLEx). It preserves pre-trained knowledge of SSL models while efficiently\nfinetuning only selected experts, reducing training costs while maintaining\nrobust performance. The observed utility of experts during inference shows the\nrouter reactivates the same experts for similar attacks but switches to other\nexperts for novel spoofs, confirming MoLEx's domain-aware adaptability. MoLEx\nadditionally offers flexibility for domain adaptation by allowing extra experts\nto be trained without modifying the entire model. We mainly evaluate our\napproach on the ASVSpoof 5 dataset and achieve the state-of-the-art (SOTA)\nequal error rate (EER) of 5.56% on the evaluation set without augmentation.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408Low-Rank Adaptation\u548cMixture-of-Experts\u8def\u7531\u7684MoLEx\u6846\u67b6\uff0c\u5728\u4fdd\u6301SSL\u6a21\u578b\u9886\u5148\u77e5\u8bc6\u7684\u540c\u65f6\u9ad8\u6548\u5c0f\u89d2\u5ea6\u5b8c\u6210\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u89e3\u51b3SSL\u6a21\u578b\u5168\u91cf\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u53c2\u6570\u6548\u7387", "method": "\u63d0\u51faMoLEx\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u548cMoE\u8def\u7531\u5668\uff0c\u53ea\u5fae\u8c03\u9009\u62e9\u7684\u4e13\u5bb6\uff0c\u652f\u6301\u57df\u9002\u5e94\u6027\u6269\u5c55", "result": "\u5728ASVSpoof 5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0cEER\u4e3a5.56%\uff08\u65e0\u589e\u5f3a\uff09\uff0c\u8def\u7531\u5668\u80fd\u6839\u636e\u653b\u51fb\u7c7b\u578b\u6fc0\u6d3b\u76f8\u5e94\u4e13\u5bb6", "conclusion": "MoLEx\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u53c2\u6570\u6548\u7387\u5fae\u8c03\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2509.09201", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09201", "abs": "https://arxiv.org/abs/2509.09201", "authors": ["Xiaoxue Luo", "Jinwei Huang", "Runyan Yang", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "DeCodec: Rethinking Audio Codecs as Universal Disentangled Representation Learners", "comment": null, "summary": "Universal audio codecs learn entangled representations across audio types,\nwhereas some specific codecs offer decoupled representations but are limited to\nspeech. Real-world audio, however, often contains mixed speech and background\nsounds, and downstream tasks require selective access to these components.\nTherefore, we rethink the audio codec as a universal disentangled\nrepresentation learner to enable controllable feature selection across\ndifferent audio tasks. To this end, we introduce DeCodec, a novel neural codec\nthat learns to decouple audio representations into orthogonal subspaces\ndedicated to speech and background sound, and within speech, representations\nare further decomposed into semantic and paralinguistic components. This\nhierarchical disentanglement allows flexible feature selection, making DeCodec\na universal front-end for multiple audio applications. Technically, built upon\na codec framework, DeCodec incorporates two key innovations: a subspace\northogonal projection module that factorizes the input into two decoupled\northogonal subspaces, and a representation swap training procedure that ensures\nthese two subspaces are correlate to the speech and background sound,\nrespectively. These allows parallel RVQs to quantize speech and background\nsound components independently. Furthermore, we employ semantic guidance to the\nspeech RVQ to achieve semantic and paralinguistic decomposition. Experimental\nresults show that DeCodec maintains advanced signal reconstruction while\nenabling new capabilities: superior speech enhancement and effective one-shot\nvoice conversion on noisy speech via representation recombination, improved ASR\nrobustness through clean semantic features, and controllable background sound\npreservation/suppression in TTS. Demo Page: https://luo404.github.io/DeCodecV2/", "AI": {"tldr": "DeCodec\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u5b9e\u73b0\u97f3\u9891\u7684\u5c42\u6b21\u5316\u89e3\u8026\u8868\u793a\uff0c\u5c06\u97f3\u9891\u5206\u79bb\u4e3a\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\uff0c\u5e76\u5728\u8bed\u97f3\u5185\u90e8\u8fdb\u4e00\u6b65\u5206\u89e3\u4e3a\u8bed\u4e49\u548c\u526f\u8bed\u8a00\u6210\u5206\uff0c\u4e3a\u591a\u79cd\u97f3\u9891\u5e94\u7528\u63d0\u4f9b\u53ef\u63a7\u7684\u7279\u5f81\u9009\u62e9\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u97f3\u9891\u901a\u5e38\u5305\u542b\u6df7\u5408\u7684\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\uff0c\u800c\u4e0b\u6e38\u4efb\u52a1\u9700\u8981\u9009\u62e9\u6027\u8bbf\u95ee\u8fd9\u4e9b\u7ec4\u4ef6\u3002\u73b0\u6709\u901a\u7528\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5b66\u4e60\u7ea0\u7f20\u8868\u793a\uff0c\u800c\u7279\u5b9a\u7f16\u89e3\u7801\u5668\u867d\u7136\u63d0\u4f9b\u89e3\u8026\u8868\u793a\u4f46\u4ec5\u9650\u4e8e\u8bed\u97f3\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u666e\u904d\u89e3\u8026\u97f3\u9891\u8868\u793a\u7684\u7f16\u89e3\u7801\u5668\u3002", "method": "\u57fa\u4e8e\u7f16\u89e3\u7801\u5668\u6846\u67b6\uff0cDeCodec\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a\u5b50\u7a7a\u95f4\u6b63\u4ea4\u6295\u5f71\u6a21\u5757\u5c06\u8f93\u5165\u5206\u89e3\u4e3a\u4e24\u4e2a\u89e3\u8026\u7684\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff1b\u8868\u793a\u4ea4\u6362\u8bad\u7ec3\u8fc7\u7a0b\u786e\u4fdd\u8fd9\u4e24\u4e2a\u5b50\u7a7a\u95f4\u5206\u522b\u5bf9\u5e94\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\u3002\u4f7f\u7528\u5e76\u884cRVQ\u72ec\u7acb\u91cf\u5316\u8bed\u97f3\u548c\u80cc\u666f\u58f0\u97f3\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u5b9e\u73b0\u8bed\u97f3\u5185\u90e8\u7684\u8bed\u4e49\u548c\u526f\u8bed\u8a00\u5206\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDeCodec\u5728\u4fdd\u6301\u5148\u8fdb\u4fe1\u53f7\u91cd\u5efa\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u65b0\u80fd\u529b\uff1a\u901a\u8fc7\u8868\u793a\u91cd\u7ec4\u5728\u566a\u58f0\u8bed\u97f3\u4e0a\u5b9e\u73b0\u4f18\u5f02\u7684\u8bed\u97f3\u589e\u5f3a\u548c\u6709\u6548\u7684\u4e00\u952e\u8bed\u97f3\u8f6c\u6362\uff1b\u901a\u8fc7\u5e72\u51c0\u7684\u8bed\u4e49\u7279\u5f81\u63d0\u9ad8ASR\u9c81\u68d2\u6027\uff1b\u5728TTS\u4e2d\u5b9e\u73b0\u53ef\u63a7\u7684\u80cc\u666f\u58f0\u97f3\u4fdd\u7559/\u6291\u5236\u3002", "conclusion": "DeCodec\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u89e3\u8026\u8868\u793a\u5b66\u4e60\u5668\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u89e3\u8026\u8868\u793a\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u7279\u5f81\u9009\u62e9\uff0c\u4f7f\u5176\u6210\u4e3a\u591a\u4e2a\u97f3\u9891\u5e94\u7528\u7684\u901a\u7528\u524d\u7aef\uff0c\u4e3a\u97f3\u9891\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u63a7\u80fd\u529b\u3002"}}
{"id": "2509.09204", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09204", "abs": "https://arxiv.org/abs/2509.09204", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Zhen Qiu", "Chi Hung Chi", "Kwok Yan Lam"], "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems", "comment": "Published in Interspeech 2025", "summary": "Audio deepfake detection (ADD) models are commonly evaluated using datasets\nthat combine multiple synthesizers, with performance reported as a single Equal\nError Rate (EER). However, this approach disproportionately weights\nsynthesizers with more samples, underrepresenting others and reducing the\noverall reliability of EER. Additionally, most ADD datasets lack diversity in\nbona fide speech, often featuring a single environment and speech style (e.g.,\nclean read speech), limiting their ability to simulate real-world conditions.\nTo address these challenges, we propose bona fide cross-testing, a novel\nevaluation framework that incorporates diverse bona fide datasets and\naggregates EERs for more balanced assessments. Our approach improves robustness\nand interpretability compared to traditional evaluation methods. We benchmark\nover 150 synthesizers across nine bona fide speech types and release a new\ndataset to facilitate further research at\nhttps://github.com/cyaaronk/audio_deepfake_eval.", "AI": {"tldr": "\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u6539\u8fdb\uff0c\u63d0\u51fa\u771f\u5b9e\u8bed\u97f3\u4ea4\u53c9\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u504f\u5dee\u548c\u8bc4\u4f30\u4e0d\u5747\u8861\u95ee\u9898", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a1)\u591a\u5408\u6210\u5668\u6570\u636e\u96c6\u4e2d\u6837\u672c\u6570\u91cf\u4e0d\u5747\u8861\u5bfc\u81f4EER\u504f\u5dee\uff1b2)\u771f\u5b9e\u8bed\u97f3\u7c7b\u578b\u5355\u4e00\uff0c\u65e0\u6cd5\u6a21\u62df\u5b9e\u9645\u573a\u666f", "method": "\u63d0\u51fa\u771f\u5b9e\u8bed\u97f3\u4ea4\u53c9\u6d4b\u8bd5\u6846\u67b6\uff0c\u6574\u5408\u591a\u6837\u5316\u771f\u5b9e\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u805a\u5408EER\u8fdb\u884c\u66f4\u5747\u8861\u7684\u8bc4\u4f30", "result": "\u57289\u79cd\u4e0d\u540c\u771f\u5b9e\u8bed\u97f3\u7c7b\u578b\u4e0a\u5bf9\u8d85100\u591a\u4e2a\u5408\u6210\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6539\u5584\u4e86\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u91ca\u653e\u4e86\u65b0\u6570\u636e\u96c6\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2509.08830", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08830", "abs": "https://arxiv.org/abs/2509.08830", "authors": ["Seong-A Park", "Jong-Eui Chae", "Sungdong Kim", "Hyung-Chul Lee", "Hyun-Lim Yang"], "title": "A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals", "comment": "16 pages, 5 figures", "summary": "In clinical settings, monitoring hemodynamics is crucial for managing patient\nprognosis, necessitating the integrated analysis of multiple physiological\nsignals. While recent research has analyzed single signals such as\nelectrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a\nproposal for an approach that encompasses the complex signal analysis required\nin actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul\nNational University hospital PHYsiological signal Masked representation\nlearning) model extracts physiological features reflecting the electrical,\npressure, and fluid characteristics of the cardiac cycle in the process of\nrestoring three masked physiological signals based on self-supervised learning\n(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing\nmultiple physical characteristics, the model can extract more enriched features\nonly using non-invasive signals. We evaluated the model's performance in\nclinical downstream tasks such as hypotension, stroke volume, systolic blood\npressure, diastolic blood pressure, and age prediction. Our results showed that\nthe SNUPHY-M significantly outperformed supervised or SSL models, especially in\nprediction tasks using non-invasive signals. To the best of our knowledge,\nSNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis\ninvolving ECG, PPG, and ABP signals. This approach effectively supports\nclinical decision-making and enables precise diagnostics, contributing\nsignificantly to the early diagnosis and management of hemodynamics without\ninvasiveness.", "AI": {"tldr": "SNUPHY-M\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u5206\u6790\u6a21\u578b\uff0c\u901a\u8fc7\u540c\u65f6\u5904\u7406ECG\u3001PPG\u548cABP\u4e09\u79cd\u4fe1\u53f7\u6765\u63d0\u53d6\u4e30\u5bcc\u7684\u751f\u7406\u7279\u5f81\uff0c\u5728\u8840\u6d41\u52a8\u529b\u5b66\u76d1\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u73af\u5883\u4e2d\u9700\u8981\u7efc\u5408\u5206\u6790\u591a\u79cd\u751f\u7406\u4fe1\u53f7\u6765\u76d1\u6d4b\u8840\u6d41\u52a8\u529b\u5b66\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u4fe1\u53f7\u5206\u6790\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u7684\u590d\u6742\u4fe1\u53f7\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6062\u590d\u4e09\u79cd\u88ab\u906e\u853d\u7684\u751f\u7406\u4fe1\u53f7\uff08ECG\u3001PPG\u3001ABP\uff09\u6765\u63d0\u53d6\u53cd\u6620\u5fc3\u810f\u5468\u671f\u7535\u5b66\u3001\u538b\u529b\u548c\u6d41\u4f53\u7279\u6027\u7684\u751f\u7406\u7279\u5f81\uff0c\u5229\u7528\u591a\u79cd\u7269\u7406\u7279\u6027\u4ec5\u4f7f\u7528\u65e0\u521b\u4fe1\u53f7\u63d0\u53d6\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u3002", "result": "\u5728\u4f4e\u8840\u538b\u3001\u6bcf\u640f\u8f93\u51fa\u91cf\u3001\u6536\u7f29\u538b\u3001\u8212\u5f20\u538b\u548c\u5e74\u9f84\u9884\u6d4b\u7b49\u4e34\u5e8a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cSNUPHY-M\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u6216\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u65e0\u521b\u4fe1\u53f7\u7684\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SNUPHY-M\u662f\u9996\u4e2a\u5c06\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u4e8e\u6d89\u53caECG\u3001PPG\u548cABP\u4fe1\u53f7\u7684\u5fc3\u8840\u7ba1\u5206\u6790\u7684\u6a21\u578b\uff0c\u6709\u6548\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\uff0c\u5b9e\u73b0\u7cbe\u786e\u8bca\u65ad\uff0c\u4e3a\u65e0\u521b\u8840\u6d41\u52a8\u529b\u5b66\u7684\u65e9\u671f\u8bca\u65ad\u548c\u7ba1\u7406\u505a\u51fa\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.09149", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09149", "abs": "https://arxiv.org/abs/2509.09149", "authors": ["Yufan Qian", "Tianshu Qu", "Xihong Wu"], "title": "Automotive sound field reproduction using deep optimization with spatial domain constraint", "comment": "41 pages, 9 figures, Revised and submitted to The Journal of the\n  Acoustical Society of America (JASA)", "summary": "Sound field reproduction with undistorted sound quality and precise spatial\nlocalization is desirable for automotive audio systems. However, the complexity\nof automotive cabin acoustic environment often necessitates a trade-off between\nsound quality and spatial accuracy. To overcome this limitation, we propose\nSpatial Power Map Net (SPMnet), a learning-based sound field reproduction\nmethod that improves both sound quality and spatial localization in complex\nenvironments. We introduce a spatial power map (SPM) constraint, which\ncharacterizes the angular energy distribution of the reproduced field using\nbeamforming. This constraint guides energy toward the intended direction to\nenhance spatial localization, and is integrated into a multi-channel\nequalization framework to also improve sound quality under reverberant\nconditions. To address the resulting non-convexity, deep optimization that use\nneural networks to solve optimization problems is employed for filter design.\nBoth in situ objective and subjective evaluations confirm that our method\nenhances sound quality and improves spatial localization within the automotive\ncabin. Furthermore, we analyze the influence of different audio materials and\nthe arrival angles of the virtual sound source in the reproduced sound field,\ninvestigating the potential underlying factors affecting these results.", "AI": {"tldr": "SPMnet\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u58f0\u573a\u91cd\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u529f\u7387\u56fe\u7ea6\u675f\u548c\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u6c7d\u8f66\u5ea7\u8231\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u540c\u65f6\u63d0\u5347\u97f3\u8d28\u548c\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u6c7d\u8f66\u5ea7\u8231\u58f0\u5b66\u73af\u5883\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5728\u97f3\u8d28\u548c\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u4e4b\u95f4\u505a\u51fa\u6743\u8861\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6539\u5584\u4e24\u8005\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u529f\u7387\u56fe(SPM)\u7ea6\u675f\u6765\u8868\u5f81\u91cd\u73b0\u58f0\u573a\u7684\u89d2\u5ea6\u80fd\u91cf\u5206\u5e03\uff0c\u901a\u8fc7\u6ce2\u675f\u6210\u5f62\u5c06\u80fd\u91cf\u5f15\u5bfc\u81f3\u76ee\u6807\u65b9\u5411\uff1b\u7ed3\u5408\u591a\u901a\u9053\u5747\u8861\u6846\u67b6\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u6765\u89e3\u51b3\u975e\u51f8\u6027\u95ee\u9898\u3002", "result": "\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u5747\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u6c7d\u8f66\u5ea7\u8231\u5185\u63d0\u5347\u4e86\u97f3\u8d28\u548c\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u97f3\u9891\u6750\u6599\u548c\u865a\u62df\u58f0\u6e90\u5230\u8fbe\u89d2\u5ea6\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "SPMnet\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u97f3\u8d28\u4e0e\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6c7d\u8f66\u97f3\u9891\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u58f0\u573a\u91cd\u73b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09262", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09262", "abs": "https://arxiv.org/abs/2509.09262", "authors": ["Seung Gyu Jeong", "Seong Eun Kim"], "title": "Adaptive Knowledge Distillation using a Device-Aware Teacher for Low-Complexity Acoustic Scene Classification", "comment": null, "summary": "In this technical report, we describe our submission for Task 1,\nLow-Complexity Device-Robust Acoustic Scene Classification, of the DCASE 2025\nChallenge. Our work tackles the dual challenges of strict complexity\nconstraints and robust generalization to both seen and unseen devices, while\nalso leveraging the new rule allowing the use of device labels at test time.\nOur proposed system is based on a knowledge distillation framework where an\nefficient CP-MobileNet student learns from a compact, specialized two-teacher\nensemble. This ensemble combines a baseline PaSST teacher, trained with\nstandard cross-entropy, and a 'generalization expert' teacher. This expert is\ntrained using our novel Device-Aware Feature Alignment (DAFA) loss, adapted\nfrom prior work, which explicitly structures the feature space for device\nrobustness. To capitalize on the availability of test-time device labels, the\ndistilled student model then undergoes a final device-specific fine-tuning\nstage. Our proposed system achieves a final accuracy of 57.93\\% on the\ndevelopment set, demonstrating a significant improvement over the official\nbaseline, particularly on unseen devices.", "AI": {"tldr": "\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8f7b\u91cf\u7ea7\u97f3\u9891\u573a\u666f\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u6559\u5e08\u96c6\u6210\u548c\u8bbe\u5907\u611f\u77e5\u7279\u5f81\u5bf9\u9f50\u635f\u5931\uff0c\u5728DCASE 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e8657.93%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u97f3\u9891\u573a\u666f\u5206\u7c7b\u4e2d\u7684\u53cc\u91cd\u6311\u6218\uff1a\u4e25\u683c\u7684\u590d\u6742\u5ea6\u7ea6\u675f\u548c\u5bf9\u5df2\u77e5\u53ca\u672a\u77e5\u8bbe\u5907\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528\u6d4b\u8bd5\u65f6\u8bbe\u5907\u6807\u7b7e\u7684\u65b0\u89c4\u5219\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u9ad8\u6548\u7684CP-MobileNet\u5b66\u751f\u6a21\u578b\u4ece\u7d27\u51d1\u7684\u53cc\u6559\u5e08\u96c6\u6210\u4e2d\u5b66\u4e60\u3002\u6559\u5e08\u96c6\u6210\u5305\u62ec\u6807\u51c6\u4ea4\u53c9\u71b5\u8bad\u7ec3\u7684PaSST\u57fa\u51c6\u6559\u5e08\u548c\u4f7f\u7528\u65b0\u9896\u8bbe\u5907\u611f\u77e5\u7279\u5f81\u5bf9\u9f50(DAFA)\u635f\u5931\u7684\u6cdb\u5316\u4e13\u5bb6\u6559\u5e08\u3002\u6700\u540e\u5229\u7528\u6d4b\u8bd5\u65f6\u8bbe\u5907\u6807\u7b7e\u8fdb\u884c\u8bbe\u5907\u7279\u5b9a\u7684\u5fae\u8c03\u3002", "result": "\u5728\u5f00\u53d1\u96c6\u4e0a\u8fbe\u523057.93%\u7684\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5b98\u65b9\u57fa\u51c6\u6709\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u672a\u77e5\u8bbe\u5907\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u7ed3\u5408\u8bbe\u5907\u611f\u77e5\u7279\u5f81\u5bf9\u9f50\u548c\u6d4b\u8bd5\u65f6\u8bbe\u5907\u7279\u5b9a\u5fae\u8c03\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bbe\u5907\u9c81\u68d2\u6027\u548c\u590d\u6742\u5ea6\u7ea6\u675f\u7684\u53cc\u91cd\u6311\u6218\uff0c\u4e3a\u4f4e\u590d\u6742\u5ea6\u8bbe\u5907\u9c81\u68d2\u97f3\u9891\u573a\u666f\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08950", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.08950", "abs": "https://arxiv.org/abs/2509.08950", "authors": ["Jarvis Haupt", "Qin Lu", "Yanning Shen", "Jia Chen", "Yue Dong", "Dan McCreary", "Mehmet Ak\u00e7akaya", "Georgios B. Giannakis"], "title": "Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities", "comment": "Accepted to the IEEE Signal Processing Magazine Special Issue on\n  Artificial Intelligence for Education: A Signal Processing Perspective", "summary": "Powerful artificial intelligence (AI) tools that have emerged in recent years\n-- including large language models, automated coding assistants, and advanced\nimage and speech generation technologies -- are the result of monumental human\nachievements. These breakthroughs reflect mastery across multiple technical\ndisciplines and the resolution of significant technological challenges.\nHowever, some of the most profound challenges may still lie ahead. These\nchallenges are not purely technical but pertain to the fair and responsible use\nof AI in ways that genuinely improve the global human condition. This article\nexplores one promising application aligned with that vision: the use of AI\ntools to facilitate and enhance education, with a specific focus on signal\nprocessing (SP). It presents two interrelated perspectives: identifying and\naddressing technical limitations, and applying AI tools in practice to improve\neducational experiences. Primers are provided on several core technical issues\nthat arise when using AI in educational settings, including how to ensure\nfairness and inclusivity, handle hallucinated outputs, and achieve efficient\nuse of resources. These and other considerations -- such as transparency,\nexplainability, and trustworthiness -- are illustrated through the development\nof an immersive, structured, and reliable \"smart textbook.\" The article serves\nas a resource for researchers and educators seeking to advance AI's role in\nengineering education.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u4fe1\u53f7\u5904\u7406\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u6280\u672f\u9650\u5236\u548c\u5b9e\u8df5\u5e94\u7528\uff0c\u4ee5\u53ca\u516c\u5e73\u3001\u53ef\u9760\u6027\u7b49\u8d23\u4efb\u4f7f\u7528\u95ee\u9898\u3002", "motivation": "\u867d\u7136AI\u6280\u672f\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u8fd8\u9762\u4e34\u7740\u5982\u4f55\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528AI\u6765\u771f\u6b63\u6539\u5584\u5168\u7403\u4eba\u7c7b\u751f\u6d3b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6559\u80b2\u9886\u57df\u3002", "method": "\u63d0\u4f9b\u4e86\u5728\u6559\u80b2\u73af\u5883\u4e2d\u4f7f\u7528AI\u7684\u6838\u5fc3\u6280\u672f\u95ee\u9898\u5165\u95e8\uff0c\u5305\u62ec\u786e\u4fdd\u516c\u5e73\u6027\u3001\u5904\u7406\u5e7b\u89c9\u8f93\u51fa\u3001\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u7b49\uff0c\u5e76\u901a\u8fc7\u5f00\u53d1\u6c89\u6d78\u5f0f\u667a\u80fd\u6559\u79d1\u4e66\u6765\u5c55\u793a\u8fd9\u4e9b\u8003\u8651\u3002", "result": "\u8bba\u6587\u4f5c\u4e3a\u4e00\u4efd\u8d44\u6e90\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u63a8\u8fdbAI\u5e94\u7528\u7684\u5b9e\u8df5\u6307\u5357\u548c\u6280\u672f\u652f\u6301\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u9886\u57df\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\uff0c\u7279\u522b\u662f\u5728\u4fe1\u53f7\u5904\u7406\u6559\u80b2\u4e2d\uff0c\u4f46\u9700\u8981\u7ef4\u62a4\u516c\u5e73\u3001\u5305\u5bb9\u3001\u900f\u660e\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u7b49\u6838\u5fc3\u4ef7\u503c\u89c2\u3002"}}
{"id": "2509.09212", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09212", "abs": "https://arxiv.org/abs/2509.09212", "authors": ["Amir Ivry", "Samuele Cornell", "Shinji Watanabe"], "title": "MAPSS: Manifold-based Assessment of Perceptual Source Separation", "comment": "Submitted to ICLR", "summary": "Objective assessment of source-separation systems still mismatches subjective\nhuman perception, especially when leakage and self-distortion interact. We\nintroduce the Perceptual Separation (PS) and Perceptual Match (PM), the first\npair of measures that functionally isolate these two factors. Our intrusive\nmethod begins with generating a bank of fundamental distortions for each\nreference waveform signal in the mixture. Distortions, references, and their\nrespective system outputs from all sources are then independently encoded by a\npre-trained self-supervised learning model. These representations are\naggregated and projected onto a manifold via diffusion maps, which aligns\nEuclidean distances on the manifold with dissimilarities of the encoded\nwaveforms. On this manifold, the PM measures the Mahalanobis distance from each\noutput to its attributed cluster that consists of its reference and distortions\nembeddings, capturing self-distortion. The PS accounts for the Mahalanobis\ndistance of the output to the attributed and to the closest non-attributed\nclusters, quantifying leakage. Both measures are differentiable and granular,\noperating at a resolution as low as 50 frames per second. We further derive,\nfor both measures, deterministic error radius and non-asymptotic,\nhigh-probability confidence intervals (CIs). Experiments on English, Spanish,\nand music mixtures show that the PS and PM nearly always achieve the highest\nlinear correlation coefficients with human mean-opinion scores than 14\ncompetitors, reaching as high as 86.36% for speech and 87.21% for music. We\nobserve, at worst, an error radius of 1.39% and a probabilistic 95% CI of\n12.21% for these coefficients, which improves reliable and informed evaluation.\nUsing mutual information, the measures complement each other most as their\nvalues decrease, suggesting they are jointly more informative as system\nperformance degrades.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Perceptual Separation (PS)\u548cPerceptual Match (PM)\u8fd9\u5bf9\u65b0\u7684\u5ea6\u91cf\u6307\u6807\uff0c\u901a\u8fc7\u6d41\u5f62\u591a\u5c42\u9762\u5b66\u4e60\u548c\u9a6c\u6d0b\u8ddd\u79bb\u8ba1\u7b97\uff0c\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u6e90\u5206\u79bb\u7cfb\u7edf\u7684\u81ea\u6211\u5931\u771f\u548c\u6f0f\u6cf0\u95ee\u9898\uff0c\u4e0e\u4e3b\u89c2\u4eba\u7c7b\u611f\u77e5\u76f8\u5173\u6027\u8fbe\u523086-87%\u3002", "motivation": "\u76ee\u524d\u7684\u6e90\u5206\u79bb\u7cfb\u7edf\u5ba2\u89c2\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4e3b\u89c2\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u8bef\u5dee\uff0c\u5c24\u5176\u662f\u5728\u6f0f\u6cf0\u548c\u81ea\u6211\u5931\u771f\u76f8\u4e92\u4f5c\u7528\u65f6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u529f\u80fd\u6027\u5730\u9694\u79bb\u8fd9\u4e24\u4e2a\u56e0\u7d20\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7f16\u7801\u6ce2\u5f62\u4fe1\u53f7\uff0c\u901a\u8fc7\u6d41\u5f62\u591a\u5c42\u9762\u6280\u672f\u5c06\u8868\u5f81\u6295\u5f71\u5230\u4e00\u4e2a\u6d4e\u8ddd\u79bb\u4e0e\u4fe1\u53f7\u76f8\u4f3c\u6027\u5bf9\u9f50\u7684\u6d41\u5f62\u591a\u5c42\u9762\u4e0a\u3002PS\u901a\u8fc7\u9a6c\u6d0b\u8ddd\u79bb\u6d4b\u91cf\u8f93\u51fa\u4e0e\u5c5e\u4e8e\u5176\u53c2\u8003\u96c6\u7fa4\u7684\u8ddd\u79bb\uff0c\u6355\u83b7\u81ea\u6211\u5931\u771f\uff1bPM\u901a\u8fc7\u6d4b\u91cf\u8f93\u51fa\u4e0e\u5c5e\u4e8e\u548c\u6700\u8fd1\u975e\u5c5e\u4e8e\u96c6\u7fa4\u7684\u8ddd\u79bb\uff0c\u91cf\u5316\u6f0f\u6cf0\u3002", "result": "\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u97f3\u4e50\u6df7\u5408\u4fe1\u53f7\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPS\u548cPM\u572814\u79cd\u7ade\u4e89\u65b9\u6cd5\u4e2d\u51e0\u4e4e\u603b\u662f\u83b7\u5f97\u6700\u9ad8\u7684\u4e0e\u4eba\u7c7b\u5e73\u5747\u610f\u89c1\u5206\u6570\u7684\u7ebf\u6027\u76f8\u5173\u7cfb\u6570\uff0c\u8bed\u97f3\u8fbe\u523086.36%\uff0c\u97f3\u4e50\u8fbe\u523087.21%\u3002\u6700\u5dee\u60c5\u51b5\u4e0b\u8bef\u5dee\u534a\u5f84\u4e3a1.39%\uff0c\u6982\u738795%\u4fe1\u5fc3\u533a\u95f4\u4e3a12.21%\u3002", "conclusion": "PS\u548cPM\u662f\u4e00\u5bf9\u4e0d\u53ef\u5fae\u5206\u3001\u7c92\u5ea6\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u5730\u9694\u79bb\u548c\u91cf\u5316\u6e90\u5206\u79bb\u7cfb\u7edf\u4e2d\u7684\u81ea\u6211\u5931\u771f\u548c\u6f0f\u6cf0\u95ee\u9898\uff0c\u4e0e\u4eba\u7c7b\u611f\u77e5\u6709\u5f3a\u76f8\u5173\u6027\uff0c\u4e3a\u6e90\u5206\u79bb\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u4fe1\u606f\u4e30\u5bcc\u7684\u8bc4\u4f30\u3002"}}
{"id": "2509.09318", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09318", "abs": "https://arxiv.org/abs/2509.09318", "authors": ["Weixing Wei", "Kazuyoshi Yoshii"], "title": "Efficient Transformer-Based Piano Transcription With Sparse Attention Mechanisms", "comment": "Accepted by APSIPA 2025", "summary": "This paper investigates automatic piano transcription based on\ncomputationally-efficient yet high-performant variants of the Transformer that\ncan capture longer-term dependency over the whole musical piece. Recently,\ntransformer-based sequence-to-sequence models have demonstrated excellent\nperformance in piano transcription. These models, however, fail to deal with\nthe whole piece at once due to the quadratic complexity of the self-attention\nmechanism, and music signals are thus typically processed in a sliding-window\nmanner in practice. To overcome this limitation, we propose an efficient\narchitecture with sparse attention mechanisms. Specifically, we introduce\nsliding-window self-attention mechanisms for both the encoder and decoder, and\na hybrid global-local cross-attention mechanism that attends to various spans\naccording to the MIDI token types. We also use a hierarchical pooling strategy\nbetween the encoder and decoder to further reduce computational load. Our\nexperiments on the MAESTRO dataset showed that the proposed model achieved a\nsignificant reduction in computational cost and memory usage, accelerating\ninference speed, while maintaining transcription performance comparable to the\nfull-attention baseline. This allows for training with longer audio contexts on\nthe same hardware, demonstrating the viability of sparse attention for building\nefficient and high-performance piano transcription systems. The code is\navailable at https://github.com/WX-Wei/efficient-seq2seq-piano-trans.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548Transformer\u67b6\u6784\uff0c\u7528\u4e8e\u94a2\u7434\u8f6c\u5f55\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u65e0\u6cd5\u4e00\u6b21\u6027\u5904\u7406\u5b8c\u6574\u97f3\u4e50\u7247\u6bb5\uff0c\u9700\u8981\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u5904\u7406\uff0c\u9650\u5236\u4e86\u957f\u5e8f\u5217\u4f9d\u8d56\u7684\u6355\u6349", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff09\u3001\u6df7\u5408\u5168\u5c40-\u5c40\u90e8\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff08\u6839\u636eMIDI\u6807\u8bb0\u7c7b\u578b\u5173\u6ce8\u4e0d\u540c\u8de8\u5ea6\uff09\u3001\u5206\u5c42\u6c60\u5316\u7b56\u7565", "result": "\u5728MAESTRO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u663e\u8457\u964d\u4f4e\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u5feb\uff0c\u540c\u65f6\u8f6c\u5f55\u6027\u80fd\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53", "conclusion": "\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u53ef\u7528\u4e8e\u6784\u5efa\u9ad8\u6548\u9ad8\u6027\u80fd\u7684\u94a2\u7434\u8f6c\u5f55\u7cfb\u7edf\uff0c\u5141\u8bb8\u5728\u76f8\u540c\u786c\u4ef6\u4e0a\u4f7f\u7528\u66f4\u957f\u97f3\u9891\u4e0a\u4e0b\u6587\u8fdb\u884c\u8bad\u7ec3"}}
{"id": "2509.08973", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08973", "abs": "https://arxiv.org/abs/2509.08973", "authors": ["Harshit Agrawal", "Ari Hietanen", "Simo S\u00e4rkk\u00e4"], "title": "Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography", "comment": null, "summary": "Purpose: Scatter artifacts drastically degrade the image quality of cone-beam\ncomputed tomography (CBCT) scans. Although deep learning-based methods show\npromise in estimating scatter from CBCT measurements, their deployment in\nmobile CBCT systems or edge devices is still limited due to the large memory\nfootprint of the networks. This study addresses the issue by applying networks\nat varying resolutions and suggesting an optimal one, based on speed and\naccuracy.\n  Methods: First, the reconstruction error in down-up sampling of CBCT scatter\nsignal was examined at six resolutions by comparing four interpolation methods.\nNext, a recent state-of-the-art method was trained across five image\nresolutions and evaluated for the reductions in floating-point operations\n(FLOPs), inference times, and GPU memory requirements.\n  Results: Reducing the input size and network parameters achieved a 78-fold\nreduction in FLOPs compared to the baseline method, while maintaining comarable\nperformance in terms of mean-absolute-percentage-error (MAPE) and\nmean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to\n4.42%, and the MSE decreased to 1.34 \\times 10^{-2} compared to 2.01 \\times\n10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and\n12, respectively. Further experiments comparing scatter-corrected\nreconstructions on a large, simulated dataset and real CBCT scans from water\nand Sedentex CT phantoms clearly demonstrated the robustness of our method.\n  Conclusion: This study highlights the underappreciated role of downsampling\nin deep learning-based scatter estimation. The substantial reduction in FLOPs\nand GPU memory requirements achieved by our method enables scatter correction\nin resource-constrained environments, such as mobile CBCT and edge devices.", "AI": {"tldr": "\u901a\u8fc7\u4e0d\u540c\u5206\u8fa8\u7387\u5c42\u9762\u8fdb\u884c\u6563\u5c04\u4f30\u8ba1\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e8678\u500dFLOPs\u51cf\u5c11\u548c16\u500d\u63a8\u7406\u901f\u5ea6\u63d0\u5347", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6563\u5c04\u4f30\u8ba1\u7b97\u6cd5\u5728\u79fb\u52a8CBCT\u7cfb\u7edf\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u5927\u5185\u5b58\u5360\u7528\u95ee\u9898", "method": "\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u8bad\u7ec3\u72b6\u6001\u524d\u6cbf\u7f51\u7edc\uff0c\u6bd4\u8f83\u56db\u79cd\u63d2\u503c\u65b9\u6cd5\u7684\u91cd\u5efa\u9519\u8bef\uff0c\u5e76\u8bc4\u4f30FLOPs\u3001\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u9700\u6c42", "result": "\u8f93\u5165\u5927\u5c0f\u548c\u7f51\u7edc\u53c2\u6570\u51cf\u5c11\u5e26\u676578\u500dFLOPs\u51cf\u5c11\uff0cMAPE\u4ece4.42%\u964d\u81f33.85%\uff0cMSE\u4ece2.01\u00d710^{-2}\u964d\u81f31.34\u00d710^{-2}\uff0c\u63a8\u7406\u65f6\u95f4\u548cGPU\u5185\u5b58\u4f7f\u7528\u5206\u522b\u51cf\u5c1116\u500d\u548c12\u500d", "conclusion": "\u4e0b\u91c7\u6837\u5728\u6df1\u5ea6\u5b66\u4e60\u6563\u5c04\u4f30\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u901a\u8fc7\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u6563\u5c04\u6821\u6b63\u53d8\u5f97\u53ef\u884c"}}
{"id": "2509.09296", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09296", "abs": "https://arxiv.org/abs/2509.09296", "authors": ["Li Wang", "Xiaoyan Lei", "Haorui He", "Lei Wang", "Jie Shi", "Zhizheng Wu"], "title": "Over-the-Air Adversarial Attack Detection: from Datasets to Defenses", "comment": null, "summary": "Automatic Speaker Verification (ASV) systems can be used for voice-enabled\napplications for identity verification. However, recent studies have exposed\nthese systems' vulnerabilities to both over-the-line (OTL) and over-the-air\n(OTA) adversarial attacks. Although various detection methods have been\nproposed to counter these threats, they have not been thoroughly tested due to\nthe lack of a comprehensive data set. To address this gap, we developed the\nAdvSV 2.0 dataset, which contains 628k samples with a total duration of 800\nhours. This dataset incorporates classical adversarial attack algorithms, ASV\nsystems, and encompasses both OTL and OTA scenarios. Furthermore, we introduce\na novel adversarial attack method based on a Neural Replay Simulator (NRS),\nwhich enhances the potency of adversarial OTA attacks, thereby presenting a\ngreater threat to ASV systems. To defend against these attacks, we propose\nCODA-OCC, a contrastive learning approach within the one-class classification\nframework. Experimental results show that CODA-OCC achieves an EER of 11.2% and\nan AUC of 0.95 on the AdvSV 2.0 dataset, outperforming several state-of-the-art\ndetection methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdvSV 2.0\u6570\u636e\u96c6\uff08628k\u6837\u672c\uff0c800\u5c0f\u65f6\uff09\u7528\u4e8e\u6d4b\u8bd5ASV\u7cfb\u7edf\u5bf9\u6297\u653b\u51fb\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5305\u542bOTL\u548cOTA\u653b\u51fb\u573a\u666f\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u795e\u7ecf\u91cd\u653e\u6a21\u62df\u5668\u7684\u65b0\u578bOTA\u653b\u51fb\u65b9\u6cd5\u548c\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684CODA-OCC\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u96c6\u4e0a\u8fbe\u523011.2% EER\u548c0.95 AUC\u3002", "motivation": "\u73b0\u6709\u7684ASV\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u7ebf\u4e0a\u548c\u65e0\u7ebf\u5bf9\u6297\u653b\u51fb\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u6d4b\u8bd5\u5404\u79cd\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u6570\u636e\u96c6\u548c\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "1) \u6784\u5efaAdvSV 2.0\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u91cd\u653e\u6a21\u62df\u5668\u7684\u65b0\u578bOTA\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff1b3) \u63d0\u51faCODA-OCC\u5bf9\u6bd4\u5b66\u4e60\u9632\u5fa1\u6846\u67b6\uff08\u5355\u7c7b\u5206\u7c7b\uff09\u3002", "result": "CODA-OCC\u5728AdvSV 2.0\u6570\u636e\u96c6\u4e0a\u8fbe\u523011.2%\u7684\u7b49\u9519\u8bef\u7387\u548c0.95\u7684AUC\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "AdvSV 2.0\u6570\u636e\u96c6\u4e3aASV\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63d0\u51fa\u7684NRS\u653b\u51fb\u65b9\u6cd5\u589e\u5f3a\u4e86OTA\u653b\u51fb\u5a01\u80c1\uff0cCODA-OCC\u9632\u5fa1\u65b9\u6cd5\u5728\u68c0\u6d4b\u5bf9\u6297\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.09550", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09550", "abs": "https://arxiv.org/abs/2509.09550", "authors": ["Harry Julia", "Rachel Beeson", "Lohith Konathala", "Johanna Ulin", "Jiameng Gao"], "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates", "comment": null, "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.", "AI": {"tldr": "NeuCodec\u662f\u57fa\u4e8eFSQ\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u76f8\u6bd4\u4f20\u7edfRVQ\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6297\u566a\u9c81\u68d2\u6027\u548c\u7f16\u7801\u5197\u4f59\u7279\u6027", "motivation": "\u73b0\u6709\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u4e3b\u8981\u4f9d\u8d56RVQ\uff0c\u800cFSQ\u4f5c\u4e3a\u65b0\u5174\u66ff\u4ee3\u65b9\u6848\u80fd\u7b80\u5316\u8bad\u7ec3\u5e76\u652f\u6301\u5355\u7801\u672c\uff0c\u4f46\u9700\u8981\u9a8c\u8bc1\u5176\u5728\u566a\u58f0\u4fe1\u9053\u4f20\u8f93\u4e2d\u7684\u9c81\u68d2\u6027", "method": "\u63d0\u51fa\u57fa\u4e8eFSQ\u7684NeuCodec\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u7f16\u7801\u5668\u84b8\u998f\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u7f16\u7801\u5668\u80fd\u4ea7\u751f\u4e0d\u540c\u4f46\u7b49\u4ef7\u7684\u7f16\u7801\u5e8f\u5217\uff0c\u5e76\u6bd4\u8f83RVQ\u548cFSQ\u5728\u6a21\u62df\u566a\u58f0\u4fe1\u9053\u4e2d\u7684\u6bd4\u7279\u7ea7\u6270\u52a8\u9c81\u68d2\u6027", "result": "FSQ\u7f16\u7801\u5177\u6709\u5185\u7f6e\u5197\u4f59\u7279\u6027\uff0c\u4e0d\u540c\u7f16\u7801\u5668\u80fd\u5b66\u4e60\u5230\u4e0d\u540c\u4f46\u91cd\u5efa\u8d28\u91cf\u76f8\u5f53\u7684\u7f16\u7801\u5e8f\u5217\uff1bFSQ\u5728\u566a\u58f0\u4fe1\u9053\u4f20\u8f93\u4e2d\u8868\u73b0\u51fa\u8fdc\u4f18\u4e8eRVQ\u7684\u6bd4\u7279\u7ea7\u6270\u52a8\u9c81\u68d2\u6027", "conclusion": "FSQ-based NeuCodec\u5728\u4fdd\u6301\u97f3\u9891\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u566a\u58f0\u4fe1\u9053\u73af\u5883\u4e0b\u7684\u4f20\u8f93\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u97f3\u9891\u7f16\u7801\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09005", "categories": ["eess.SP", "cs.ET", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.09005", "abs": "https://arxiv.org/abs/2509.09005", "authors": ["Hirley Alves", "Nurul H. Mahmood", "Onel L. A. L\u00f3pez", "Sumudu Samarakoon", "Seppo Yrj\u00f6l\u00e4", "Matti Latva-Aho", "Markku Juntti", "Ari Pouttu", "Armin Dekorsy", "Arthur Sousa de Sena", "Aydin Sezgin", "Bho Matthiesen", "Chafika Benzaid", "Chathuranga Weeraddana", "David Hutchison", "Dileepa Marasinghe", "Doganalp Ergenc", "Eduard Jorswieck", "Erkki Harjula", "Falko Dressler", "Harri Saarnisaari", "Italo Atzeni", "Jaap Van De Beek", "Jacek Rak", "Konstantin Mikhaylov", "Lauri Loven", "Madhusanka Liyanage", "Marcos Katz", "Marja Matinmikko-Blue", "Mehdi Rasti", "Mika Ylianttila Nhan Nguyen", "Pawani Porambage", "Petar Popovski", "Petri Ahokangas", "Premanandana Rajatheva", "Robert-Jeron Reifert", "Tharaka Hewa", "Tommy Svensson"], "title": "6G Resilience -- White Paper", "comment": null, "summary": "6G must be designed to withstand, adapt to, and evolve amid prolonged,\ncomplex disruptions. Mobile networks' shift from efficiency-first to\nsustainability-aware has motivated this white paper to assert that resilience\nis a primary design goal, alongside sustainability and efficiency, encompassing\ntechnology, architecture, and economics. We promote resilience by analysing\ndependencies between mobile networks and other critical systems, such as\nenergy, transport, and emergency services, and illustrate how cascading\nfailures spread through infrastructures. We formalise resilience using the 3R\nframework: reliability, robustness, resilience. Subsequently, we translate this\ninto measurable capabilities: graceful degradation, situational awareness,\nrapid reconfiguration, and learning-driven improvement and recovery.\n  Architecturally, we promote edge-native and locality-aware designs, open\ninterfaces, and programmability to enable islanded operations, fallback modes,\nand multi-layer diversity (radio, compute, energy, timing). Key enablers\ninclude AI-native control loops with verifiable behaviour, zero-trust security\nrooted in hardware and supply-chain integrity, and networking techniques that\nprioritise critical traffic, time-sensitive flows, and inter-domain\ncoordination.\n  Resilience also has a techno-economic aspect: open platforms and high-quality\ncomplementors generate ecosystem externalities that enhance resilience while\nopening new markets. We identify nine business-model groups and several\npatterns aligned with the 3R objectives, and we outline governance and\nstandardisation. This white paper serves as an initial step and catalyst for 6G\nresilience. It aims to inspire researchers, professionals, government\nofficials, and the public, providing them with the essential components to\nunderstand and shape the development of 6G resilience.", "AI": {"tldr": "6G\u7f51\u7edc\u9700\u8981\u5c06\u97e7\u6027\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u76ee\u6807\uff0c\u901a\u8fc73R\u6846\u67b6\uff08\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u3001\u97e7\u6027\uff09\u548c\u53ef\u8861\u91cf\u7684\u80fd\u529b\u6765\u5b9e\u73b0\u5bf9\u590d\u6742\u4e2d\u65ad\u7684\u9002\u5e94\u548c\u6062\u590d\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u4ece\u6548\u7387\u4f18\u5148\u8f6c\u5411\u53ef\u6301\u7eed\u6027\u5bfc\u5411\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u627f\u53d7\u3001\u9002\u5e94\u548c\u6f14\u8fdb\u4e8e\u957f\u671f\u590d\u6742\u4e2d\u65ad\u76846G\u7f51\u7edc\uff0c\u5c06\u97e7\u6027\u4f5c\u4e3a\u4e0e\u53ef\u6301\u7eed\u6027\u548c\u6548\u7387\u5e76\u5217\u7684\u4e3b\u8981\u8bbe\u8ba1\u76ee\u6807\u3002", "method": "\u91c7\u75283R\u6846\u67b6\uff08\u53ef\u9760\u6027\u3001\u9c81\u68d2\u6027\u3001\u97e7\u6027\uff09\u5f62\u5f0f\u5316\u97e7\u6027\u6982\u5ff5\uff0c\u8f6c\u5316\u4e3a\u53ef\u8861\u91cf\u7684\u80fd\u529b\uff1a\u4f18\u96c5\u964d\u7ea7\u3001\u6001\u52bf\u611f\u77e5\u3001\u5feb\u901f\u91cd\u914d\u7f6e\u3001\u5b66\u4e60\u9a71\u52a8\u7684\u6539\u8fdb\u548c\u6062\u590d\u3002\u67b6\u6784\u4e0a\u91c7\u7528\u8fb9\u7f18\u539f\u751f\u548c\u4f4d\u7f6e\u611f\u77e5\u8bbe\u8ba1\u3001\u5f00\u653e\u63a5\u53e3\u3001\u53ef\u7f16\u7a0b\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u62ecAI\u539f\u751f\u63a7\u5236\u73af\u3001\u786c\u4ef6\u6839\u690d\u7684\u96f6\u4fe1\u4efb\u5b89\u5168\u3001\u5173\u952e\u6d41\u91cf\u4f18\u5148\u7684\u7f51\u7edc\u6280\u672f\u7b49\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff0c\u4ee5\u53ca9\u4e2a\u5546\u4e1a\u6a21\u5f0f\u7ec4\u548c\u6cbb\u7406\u6807\u51c6\u5316\u6846\u67b6\u3002", "conclusion": "\u672c\u767d\u76ae\u4e66\u4f5c\u4e3a6G\u97e7\u6027\u7814\u7a76\u7684\u8d77\u70b9\u548c\u50ac\u5316\u5242\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u4e13\u4e1a\u4eba\u58eb\u3001\u653f\u5e9c\u5b98\u5458\u548c\u516c\u4f17\u63d0\u4f9b\u4e86\u7406\u89e3\u548c\u5851\u90206G\u97e7\u6027\u53d1\u5c55\u7684\u57fa\u672c\u7ec4\u4ef6\u3002"}}
{"id": "2509.09306", "categories": ["eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09306", "abs": "https://arxiv.org/abs/2509.09306", "authors": ["Wenhao Yang", "Jianguo Wei", "Wenhuan Lu", "Xinyue Song", "Xianghu Yue"], "title": "Listening for \"You\": Enhancing Speech Image Retrieval via Target Speaker Extraction", "comment": "5 pages, 2 figures", "summary": "Image retrieval using spoken language cues has emerged as a promising\ndirection in multimodal perception, yet leveraging speech in multi-speaker\nscenarios remains challenging. We propose a novel Target Speaker Speech-Image\nRetrieval task and a framework that learns the relationship between images and\nmulti-speaker speech signals in the presence of a target speaker. Our method\nintegrates pre-trained self-supervised audio encoders with vision models via\ntarget speaker-aware contrastive learning, conditioned on a Target Speaker\nExtraction and Retrieval module. This enables the system to extract spoken\ncommands from the target speaker and align them with corresponding images.\nExperiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantly\noutperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3\nspeaker scenarios, respectively - substantial improvements over single speaker\nbaselines and state-of-the-art models. Our approach demonstrates potential for\nreal-world deployment in assistive robotics and multimodal interaction systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u8bb2\u8bdd\u4eba\u8bed\u97f3-\u56fe\u50cf\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u8bb2\u8bdd\u4eba\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u5728\u591a\u8bb2\u8bdd\u4eba\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u8bed\u97f3\u4e0e\u56fe\u50cf\u7684\u51c6\u786e\u5bf9\u9f50\uff0c\u57282\u4eba\u548c3\u4eba\u573a\u666f\u4e0d\u5206\u522b\u83b7\u5f9736.3%\u548c29.9%\u7684Recall@1\u6307\u6807\uff0c\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u4f7f\u7528\u8bed\u8a00\u7ebf\u7d22\u8fdb\u884c\u56fe\u50cf\u68c0\u7d22\u5728\u591a\u6a21\u6001\u611f\u77e5\u4e2d\u5c55\u73b0\u51fa\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u591a\u8bb2\u8bdd\u4eba\u573a\u666f\u4e2d\u5229\u7528\u8bed\u97f3\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u89e3\u51b3\u5982\u4f55\u5728\u5b58\u5728\u591a\u4e2a\u8bb2\u8bdd\u4eba\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u68c0\u7d22\u76ee\u6807\u8bb2\u8bdd\u4eba\u7684\u8bed\u97f3\u6307\u4ee4\u5e76\u4e0e\u56fe\u50cf\u5bf9\u5e94\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u8bb2\u8bdd\u4eba\u8bed\u97f3-\u56fe\u50cf\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u8bb2\u8bdd\u4eba\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u97f3\u9891\u7f16\u7801\u5668\u4e0e\u89c6\u89c9\u6a21\u578b\u96c6\u6210\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u76ee\u6807\u8bb2\u8bdd\u4eba\u63d0\u53d6\u548c\u68c0\u7d22\u6a21\u5757\u8fdb\u884c\u6761\u4ef6\u5316\u5904\u7406\uff0c\u80fd\u591f\u4ece\u76ee\u6807\u8bb2\u8bdd\u4eba\u63d0\u53d6\u8bed\u97f3\u547d\u4ee4\u5e76\u4e0e\u5bf9\u5e94\u56fe\u50cf\u5bf9\u9f50\u3002", "result": "\u5728SpokenCOCO2Mix\u548cSpokenCOCO3Mix\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTSRE\u65b9\u6cd5\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\uff0c\u57282\u4eba\u548c3\u4eba\u573a\u666f\u4e0d\u5206\u522b\u8fbe\u523036.3%\u548c29.9%\u7684Recall@1\u6307\u6807\u3002\u8fd9\u662f\u5bf9\u5355\u8bb2\u8bdd\u4eba\u57fa\u7ebf\u6a21\u578b\u548c\u6700\u65b0\u6a21\u578b\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u8bb2\u8bdd\u4eba\u8bed\u97f3-\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8270\u5f3a\u7684\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5728\u8f85\u52a9\u673a\u5668\u4eba\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002\u8be5\u7814\u7a76\u4e3a\u591a\u8bb2\u8bdd\u4eba\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u9a71\u52a8\u56fe\u50cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09631", "categories": ["cs.SD", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09631", "abs": "https://arxiv.org/abs/2509.09631", "authors": ["Ngoc-Son Nguyen", "Hieu-Nghia Huynh-Nguyen", "Thanh V. T. Tran", "Truong-Son Hy", "Van Nguyen"], "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech", "comment": null, "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines.", "AI": {"tldr": "DiFlow-TTS\u662f\u9996\u4e2a\u63a2\u7d22\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u7684\u8bed\u97f3\u5408\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5206\u89e3\u7684\u8bed\u97f3\u5c5e\u6027\uff0c\u5728\u96f6\u6837\u672cTTS\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u8bed\u97f3\u5408\u6210\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u57fa\u7ebf\u5feb25.8\u500d", "motivation": "\u89e3\u51b3\u73b0\u6709\u96f6\u6837\u672cTTS\u65b9\u6cd5\u63a8\u7406\u901f\u5ea6\u6162\u3001\u5b58\u5728\u91cd\u590d\u4f2a\u5f71\u7684\u95ee\u9898\uff0c\u5145\u5206\u5229\u7528\u79bb\u6563\u8868\u793a\u7684\u4f18\u52bf\uff0c\u907f\u514d\u5c06\u79bb\u6563token\u5d4c\u5165\u8fde\u7eed\u7a7a\u95f4\u5e26\u6765\u7684\u6548\u7387\u635f\u5931", "method": "\u91c7\u7528\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u5728\u7d27\u51d1\u7edf\u4e00\u67b6\u6784\u4e2d\u663e\u5f0f\u5efa\u6a21\u5206\u89e3\u7684\u8bed\u97f3\u5c5e\u6027\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u6761\u4ef6\u5316\u6587\u672c\u5185\u5bb9\u548c\u53c2\u8003\u8bed\u97f3\u7684\u97f5\u5f8b\u58f0\u5b66\u5c5e\u6027\uff0c\u4f7f\u7528\u5206\u89e3\u6d41\u9884\u6d4b\u673a\u5236\u5206\u522b\u5904\u7406\u97f5\u5f8b\u548c\u58f0\u5b66\u7ec6\u8282", "result": "\u5728\u81ea\u7136\u5ea6\u3001\u97f5\u5f8b\u3001\u8bf4\u8bdd\u4eba\u98ce\u683c\u4fdd\u6301\u548c\u80fd\u91cf\u63a7\u5236\u7b49\u5173\u952e\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u7d27\u51d1\u4e14\u63a8\u7406\u5ef6\u8fdf\u4f4e\uff0c\u751f\u6210\u901f\u5ea6\u6bd4\u6700\u65b0\u57fa\u7ebf\u5feb25.8\u500d", "conclusion": "DiFlow-TTS\u8bc1\u660e\u4e86\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u5728\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u96f6\u6837\u672cTTS\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09018", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09018", "abs": "https://arxiv.org/abs/2509.09018", "authors": ["Xueyi Wang", "C. J. C.", "Lamoth", "Elisabeth Wilhelm"], "title": "Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data", "comment": "The paper has been acceptted and presented in the 47th Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society", "summary": "A sleep forecast allows individuals and healthcare providers to anticipate\nand proactively address factors influencing restful rest, ultimately improving\nmental and physical well-being. This work presents an adaptive spatial and\ntemporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model\ncombines convolutional layers to capture spatial feature interactions between\nmultiple features and recurrent neural network layers to handle longer-term\ntemporal health-related data. A domain classifier is further integrated to\ngeneralize across different subjects. We conducted several experiments using\nfive input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes\n(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline\nmodels, achieving its lowest RMSE (0.282) with a seven-day input window and a\none-day predicting window. Moreover, the method maintained strong performance\neven when forecasting multiple days into the future, demonstrating its\nversatility for real-world applications. Visual comparisons reveal that the\nmodel accurately tracks both the overall sleep score level and daily\nfluctuations. These findings prove that the proposed framework provides a\nrobust and adaptable solution for personalized sleep forecasting using sparse\ndata from commercial wearable devices and domain adaptation techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdaST-Sleep\u6a21\u578b\uff0c\u7ed3\u5408\u5377\u79ef\u5c42\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u6765\u9884\u6d4b\u7761\u7720\u8bc4\u5206\uff0c\u901a\u8fc7\u57df\u5206\u7c7b\u5668\u5b9e\u73b0\u8de8\u5bf9\u8c61\u6cdb\u5316\uff0c\u5728\u591a\u4e2a\u65f6\u95f4\u7a97\u53e3\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u7761\u7720\u9884\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u4e2a\u4eba\u548c\u533b\u7597\u63d0\u4f9b\u8005\u9884\u89c1\u5e76\u4e3b\u52a8\u89e3\u51b3\u5f71\u54cd\u826f\u597d\u4f11\u606f\u7684\u56e0\u7d20\uff0c\u4ece\u800c\u6539\u5584\u8eab\u5fc3\u5065\u5eb7\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u5c42\u6355\u6349\u591a\u7279\u5f81\u95f4\u7684\u7a7a\u95f4\u4ea4\u4e92\uff0c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5c42\u5904\u7406\u957f\u671f\u65f6\u95f4\u5065\u5eb7\u6570\u636e\uff0c\u96c6\u6210\u57df\u5206\u7c7b\u5668\u5b9e\u73b0\u8de8\u5bf9\u8c61\u6cdb\u5316\u3002", "result": "\u57285\u79cd\u8f93\u5165\u7a97\u53e3\u548c5\u79cd\u9884\u6d4b\u7a97\u53e3\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e4\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u4f4eRMSE\u4e3a0.282\uff087\u5929\u8f93\u5165\u7a97\u53e3+1\u5929\u9884\u6d4b\u7a97\u53e3\uff09\uff0c\u591a\u5929\u9884\u6d4b\u4ecd\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f7f\u7528\u5546\u4e1a\u53ef\u7a7f\u6234\u8bbe\u5907\u7a00\u758f\u6570\u636e\u548c\u57df\u9002\u5e94\u6280\u672f\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u9002\u5e94\u7684\u4e2a\u6027\u5316\u7761\u7720\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09479", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09479", "abs": "https://arxiv.org/abs/2509.09479", "authors": ["\u013dubo\u0161 Hl\u00e1dek", "Piotr Majdak", "Robert Baumgartner"], "title": "Short-term cognitive fatigue of spatial selective attention after face-to-face conversations in virtual noisy environments", "comment": null, "summary": "Spatial selective attention is an important asset for communication in\ncocktail party situations but may be compromised by short-term cognitive\nfatigue. Here we tested whether an effortful conversation in a highly\necological setting depletes task performance in an auditory spatial selective\nattention task. Young participants with normal hearing performed the task\nbefore and after (1) having a real dyadic face-to-face conversation on a free\ntopic in a virtual reverberant room with simulated interfering conversations\nand background babble noise at 72 dB SPL for 30 minutes, (2) passively\nlistening to the interfering conversations and babble noise, or (3) having the\nconversation in quiet. Self-reported perceived effort and fatigue increased\nafter conversations in noise and passive listening relative to the reports\nafter conversations in quiet. In contrast to our expectations, response times\nin the attention task decreased, rather than increased, after conversation in\nnoise and accuracy did not change systematically in any of the conditions on\nthe group level. Unexpectedly, we observed strong training effects between the\nindividual sessions in our within-subject design even after one hour of\ntraining on a different day.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5608\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u5bf9\u8bdd\u540e\uff0c\u542c\u89c9\u7a7a\u95f4\u9009\u62e9\u6027\u6ce8\u610f\u4efb\u52a1\u7684\u53cd\u5e94\u65f6\u95f4\u53cd\u800c\u7f29\u77ed\uff0c\u51c6\u786e\u7387\u65e0\u663e\u8457\u53d8\u5316\uff0c\u4e0e\u9884\u671f\u76f8\u53cd\u3002", "motivation": "\u7814\u7a76\u7a7a\u95f4\u9009\u62e9\u6027\u6ce8\u610f\u5728\u9e21\u5c3e\u9152\u4f1a\u573a\u666f\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u77ed\u671f\u8ba4\u77e5\u75b2\u52b3\u662f\u5426\u4f1a\u5f71\u54cd\u8fd9\u79cd\u6ce8\u610f\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u865a\u62df\u6df7\u54cd\u623f\u95f4\u73af\u5883\uff0c\u8ba9\u6b63\u5e38\u542c\u529b\u5e74\u8f7b\u53c2\u4e0e\u8005\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\uff08\u5608\u6742\u73af\u5883\u5bf9\u8bdd\u3001\u88ab\u52a8\u8046\u542c\u566a\u58f0\u3001\u5b89\u9759\u73af\u5883\u5bf9\u8bdd\uff09\u524d\u540e\u6267\u884c\u542c\u89c9\u7a7a\u95f4\u9009\u62e9\u6027\u6ce8\u610f\u4efb\u52a1\u3002", "result": "\u5608\u6742\u73af\u5883\u5bf9\u8bdd\u548c\u88ab\u52a8\u8046\u542c\u540e\uff0c\u81ea\u6211\u62a5\u544a\u7684\u52aa\u529b\u548c\u75b2\u52b3\u611f\u589e\u52a0\uff0c\u4f46\u6ce8\u610f\u4efb\u52a1\u53cd\u5e94\u65f6\u95f4\u53cd\u800c\u7f29\u77ed\uff0c\u51c6\u786e\u7387\u65e0\u7cfb\u7edf\u6027\u53d8\u5316\uff0c\u4e14\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u8bad\u7ec3\u6548\u5e94\u3002", "conclusion": "\u77ed\u671f\u8ba4\u77e5\u75b2\u52b3\u53ef\u80fd\u4e0d\u4f1a\u635f\u5bb3\u542c\u89c9\u7a7a\u95f4\u9009\u62e9\u6027\u6ce8\u610f\uff0c\u53cd\u800c\u53ef\u80fd\u901a\u8fc7\u67d0\u79cd\u673a\u5236\u6539\u5584\u53cd\u5e94\u65f6\u95f4\uff0c\u540c\u65f6\u8bad\u7ec3\u6548\u5e94\u5728\u6ce8\u610f\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u3002"}}
{"id": "2509.09056", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09056", "abs": "https://arxiv.org/abs/2509.09056", "authors": ["Michael Caulfield", "Randy Palamar", "Darren Dahunsi", "Mohammad Rahim Sobhani", "Negar Majidi", "Roger Zemp"], "title": "Improving the Elevational Focusing of Fast Orthogonal Row-Column Electronic Scanning (FORCES) Ultrasound Imaging using Retrospective Transmit Beamforming (RTB)", "comment": "6 pages, 8 figures", "summary": "Recent developments in Row Column Arrays (RCAs) have presented promising\noptions for volumetric imaging without the need for the excessive channel\ncounts of fully wired 2D-arrays. Bias programmable RCAs, also known as Top\nOrthogonal to Bottom Electrode (TOBE) Arrays, show further promise in that\nimaging schemes, such as Fast Orthogonal Row-Column Electronic Scanning\n(FORCES) allow for full transmit and receive focusing everywhere in the image\nplane. However, due to its fixed elevational focus and large transmit aperture,\nFORCES experiences poor elevational focusing away from the focal point. In this\nstudy we present a modification to the FORCES imaging scheme by applying\nRetrospective Transmit Beamforming (RTB) in the elevational direction to allow\nfor elevational transmit focusing everywhere in the imaging plane. We evaluate\nFORCES and uFORCES methods, with and without RTB applied, when imaging both a\ncyst and wire phantom. With experiment we show improved elevational focusing\ncapabilities away from the focal point when RTB is applied to both FORCES and\nuFORCES. At the focal point, performance with RTB remains comparable or\nimproved relative to standard FORCES. This is quantified by the measurement of\nFull Width Half Max when imaging the wire phantom, and by the generalized\nContrast to Noise Ratio when imaging the tubular cyst phantom. We also\ndemonstrate the volumetric imaging capabilities of FORCES RTB with the wire\nphantom.", "AI": {"tldr": "\u63d0\u51fa\u4e86FORCES\u6210\u50cf\u65b9\u6848\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e94\u7528\u56de\u987e\u6027\u53d1\u5c04\u6ce2\u675f\u6210\u5f62(RTB)\u6765\u6539\u5584Row Column Arrays\u5728\u7126\u5e73\u9762\u5916\u7684\u4ef0\u89d2\u805a\u7126\u6027\u80fd", "motivation": "\u4f20\u7edfFORCES\u6210\u50cf\u65b9\u6848\u7531\u4e8e\u56fa\u5b9a\u7684\u4ef0\u89d2\u805a\u7126\u548c\u5927\u53d1\u5c04\u5b54\u5f84\uff0c\u5728\u7126\u70b9\u5916\u7684\u4ef0\u89d2\u805a\u7126\u6027\u80fd\u8f83\u5dee\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u5347\u6210\u50cf\u8d28\u91cf", "method": "\u5728FORCES\u548cuFORCES\u65b9\u6cd5\u4e2d\u5e94\u7528\u56de\u987e\u6027\u53d1\u5c04\u6ce2\u675f\u6210\u5f62(RTB)\u6280\u672f\uff0c\u5728\u4ef0\u89d2\u65b9\u5411\u8fdb\u884c\u53d1\u5c04\u805a\u7126", "result": "\u5b9e\u9a8c\u663e\u793a\u5e94\u7528RTB\u540e\uff0c\u5728\u7126\u70b9\u5916\u7684\u4ef0\u89d2\u805a\u7126\u80fd\u529b\u5f97\u5230\u663e\u8457\u6539\u5584\uff0c\u5728\u7126\u70b9\u5904\u6027\u80fd\u4fdd\u6301\u76f8\u5f53\u6216\u6709\u6240\u63d0\u5347\uff0c\u901a\u8fc7FWHM\u548cgCNR\u91cf\u5316\u9a8c\u8bc1", "conclusion": "RTB\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86FORCES\u6210\u50cf\u65b9\u6848\u7684\u4ef0\u89d2\u805a\u7126\u6027\u80fd\uff0c\u4e3aRow Column Arrays\u7684\u5bb9\u79ef\u6210\u50cf\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09489", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09489", "abs": "https://arxiv.org/abs/2509.09489", "authors": ["Saba Tabatabaee", "Suzanne Boyce", "Liran Oren", "Mark Tiede", "Carol Espy-Wilson"], "title": "Acoustic to Articulatory Speech Inversion for Children with Velopharyngeal Insufficiency", "comment": "Accepted to be presented at ASRU workshop 2025", "summary": "Traditional clinical approaches for assessing nasality, such as\nnasopharyngoscopy and nasometry, involve unpleasant experiences and are\nproblematic for children. Speech Inversion (SI), a noninvasive technique,\noffers a promising alternative for estimating articulatory movement without the\nneed for physical instrumentation. In this study, an SI system trained on\nnasalance data from healthy adults is augmented with source information from\nelectroglottography and acoustically derived F0, periodic and aperiodic energy\nestimates as proxies for glottal control. This model achieves 16.92% relative\nimprovement in Pearson Product-Moment Correlation (PPMC) compared to a previous\nSI system for nasalance estimation. To adapt the SI system for nasalance\nestimation in children with Velopharyngeal Insufficiency (VPI), the model\ninitially trained on adult speech was fine-tuned using children with VPI data,\nyielding an 7.90% relative improvement in PPMC compared to its performance\nbefore fine-tuning.", "AI": {"tldr": "\u901a\u8fc7\u8bed\u97f3\u53cd\u5411\u6280\u672f\u7ed3\u5408\u5531\u97f3\u6e90\u4fe1\u606f\uff0c\u5b8c\u5584\u4e86\u9f3b\u97f3\u6027\u4f30\u8ba1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u8fc7\u6e21\u5b9e\u73b0\u4e86\u6210\u4eba\u6a21\u578b\u5411\u513f\u7ae5VPI\u75c7\u60a3\u8005\u7684\u8c03\u9002", "motivation": "\u4f20\u7edf\u9f3b\u97f3\u6027\u8bc4\u4f30\u65b9\u6cd5\u5982\u9f3b\u5185\u955c\u68c0\u67e5\u548c\u9f3b\u91cf\u6d4b\u5bfc\u81f4\u4e0d\u9002\u611f\uff0c\u7279\u522b\u662f\u5bf9\u513f\u7ae5\u800c\u8a00\uff0c\u9700\u8981\u65e0\u4fb5\u5165\u6027\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u5728\u57fa\u4e8e\u5065\u5eb7\u6210\u4eba\u9f3b\u97f3\u6027\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u97f3\u53cd\u5411\u7cfb\u7edf\u4e2d\uff0c\u589e\u5f3a\u4e86\u6765\u81ea\u7535\u5531\u56fe\u7684\u6e90\u4fe1\u606f\u548c\u97f3\u9891\u63a8\u5bfc\u7684F0\u3001\u5468\u671f\u6027\u548c\u975e\u5468\u671f\u6027\u80fd\u91cf\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u5b66\u4e60\u8fc7\u6e21\u6280\u672f\u5c06\u6a21\u578b\u8c03\u6574\u5230\u513f\u7ae5VPI\u75c7\u60a3\u8005", "result": "\u6a21\u578b\u5b9e\u73b0\u4e8616.92%\u76f8\u5bf9\u6536\u76ca\u7684Pearson\u76f8\u5173\u7cfb\u6570\u63d0\u5347\uff0c\u7ec6\u8c03\u540e\u5728\u513f\u7ae5VPI\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u83b7\u5f977.90%\u7684\u76f8\u5bf9\u6536\u76ca", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u9f3b\u97f3\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u513f\u7ae5\u53e3\u8154\u53d9\u76f8\u5173\u75be\u75c5\u7684\u65e0\u4fb5\u5165\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09120", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09120", "abs": "https://arxiv.org/abs/2509.09120", "authors": ["Rong Ye", "Xue-Qin Jiang", "Hui Feng", "Jian Wang", "Runhe Qiu"], "title": "Signed Graph Learning with Hidden Nodes", "comment": "25 pages, 7 figures, published to Signal Processing", "summary": "Signed graphs, which are characterized by both positive and negative edge\nweights, have recently attracted significant attention in the field of graph\nsignal processing (GSP). Existing works on signed graph learning typically\nassume that all graph nodes are available. However, in some specific\napplications, only a subset of nodes can be observed while the remaining nodes\nstay hidden. To address this challenge, we propose a novel method for\nidentifying signed graph that accounts for hidden nodes, termed \\textit{signed\ngraph learning with hidden nodes under column-sparsity regularization}\n(SGL-HNCS). Our method is based on the assumption that graph signals are smooth\nover signed graphs, i.e., signal values of two nodes connected by positive\n(negative) edges are similar (dissimilar). Rooted in this prior assumption, the\ntopology inference of a signed graph is formulated as a constrained\noptimization problem with column-sparsity regularization, where the goal is to\nreconstruct the signed graph Laplacian matrix without disregarding the\ninfluence of hidden nodes. We solve the constrained optimization problem using\na tailored block coordinate descent (BCD) approach. Experimental results using\nsynthetic data and real-world data demonstrate the efficiency of the proposed\nSGL-HNCS method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5e26\u9690\u85cf\u8282\u70b9\u7684\u7b26\u53f7\u56fe\u5b66\u4e60\u65b9\u6cd5SGL-HNCS\uff0c\u901a\u8fc7\u5217\u7a00\u758f\u6b63\u5219\u5316\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6765\u91cd\u6784\u7b26\u53f7\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635", "motivation": "\u73b0\u6709\u7b26\u53f7\u56fe\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6240\u6709\u8282\u70b9\u53ef\u89c1\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u53ea\u6709\u90e8\u5206\u8282\u70b9\u53ef\u89c2\u6d4b\uff0c\u5176\u4f59\u8282\u70b9\u4fdd\u6301\u9690\u85cf\u72b6\u6001\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6311\u6218", "method": "\u57fa\u4e8e\u7b26\u53f7\u56fe\u4e0a\u4fe1\u53f7\u5e73\u6ed1\u7684\u5047\u8bbe\uff0c\u5c06\u7b26\u53f7\u56fe\u62d3\u6251\u63a8\u65ad\u6784\u5efa\u4e3a\u5e26\u5217\u7a00\u758f\u6b63\u5219\u5316\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u5b9a\u5236\u5316\u7684\u5757\u5750\u6807\u4e0b\u964d(BCD)\u65b9\u6cd5\u6c42\u89e3", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u6240\u63d0SGL-HNCS\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5b58\u5728\u9690\u85cf\u8282\u70b9\u7684\u7b26\u53f7\u56fe\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09526", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09526", "abs": "https://arxiv.org/abs/2509.09526", "authors": ["Jinzheng Zhao", "Yong Xu", "Haohe Liu", "Davide Berghi", "Xinyuan Qian", "Qiuqiang Kong", "Junqi Zhao", "Mark D. Plumbley", "Wenwu Wang"], "title": "Region-Specific Audio Tagging for Spatial Sound", "comment": "DCASE2025 Workshop", "summary": "Audio tagging aims to label sound events appearing in an audio recording. In\nthis paper, we propose region-specific audio tagging, a new task which labels\nsound events in a given region for spatial audio recorded by a microphone\narray. The region can be specified as an angular space or a distance from the\nmicrophone. We first study the performance of different combinations of\nspectral, spatial, and position features. Then we extend state-of-the-art audio\ntagging systems such as pre-trained audio neural networks (PANNs) and audio\nspectrogram transformer (AST) to the proposed region-specific audio tagging\ntask. Experimental results on both the simulated and the real datasets show the\nfeasibility of the proposed task and the effectiveness of the proposed method.\nFurther experiments show that incorporating the directional features is\nbeneficial for omnidirectional tagging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u533a\u57df\u7279\u5b9a\u97f3\u9891\u6807\u8bb0\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u9ea6\u514b\u98ce\u9635\u5217\u6807\u8bb0\u7a7a\u95f4\u97f3\u9891\u4e2d\u7279\u5b9a\u533a\u57df\u7684\u58f0\u97f3\u4e8b\u4ef6\uff0c\u5e76\u6269\u5c55\u4e86PANNs\u548cAST\u7b49\u5148\u8fdb\u97f3\u9891\u6807\u8bb0\u7cfb\u7edf\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u97f3\u9891\u6807\u8bb0\u53ea\u80fd\u8bc6\u522b\u58f0\u97f3\u4e8b\u4ef6\u7684\u5b58\u5728\uff0c\u65e0\u6cd5\u5b9a\u4f4d\u5176\u5728\u7a7a\u95f4\u4e2d\u7684\u5177\u4f53\u4f4d\u7f6e\u3002\u4e3a\u4e86\u5b9e\u73b0\u5bf9\u7a7a\u95f4\u97f3\u9891\u4e2d\u7279\u5b9a\u533a\u57df\u58f0\u97f3\u4e8b\u4ef6\u7684\u7cbe\u786e\u6807\u8bb0\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u533a\u57df\u7279\u5b9a\u97f3\u9891\u6807\u8bb0\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u9891\u8c31\u3001\u7a7a\u95f4\u548c\u4f4d\u7f6e\u7279\u5f81\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u5e76\u6269\u5c55\u4e86\u9884\u8bad\u7ec3\u97f3\u9891\u795e\u7ecf\u7f51\u7edc(PANNs)\u548c\u97f3\u9891\u9891\u8c31\u53d8\u6362\u5668(AST)\u6765\u5904\u7406\u533a\u57df\u7279\u5b9a\u97f3\u9891\u6807\u8bb0\u4efb\u52a1\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4efb\u52a1\u662f\u53ef\u884c\u7684\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u65b9\u5411\u7279\u5f81\u5bf9\u5168\u5411\u6807\u8bb0\u6709\u76ca\u3002", "conclusion": "\u533a\u57df\u7279\u5b9a\u97f3\u9891\u6807\u8bb0\u662f\u4e00\u4e2a\u53ef\u884c\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u65b9\u5411\u7279\u5f81\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7a7a\u95f4\u97f3\u9891\u4e2d\u58f0\u97f3\u4e8b\u4ef6\u7684\u5b9a\u4f4d\u548c\u6807\u8bb0\u6027\u80fd\u3002"}}
{"id": "2509.09144", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09144", "abs": "https://arxiv.org/abs/2509.09144", "authors": ["G Dhinesh Chandran", "Kota Srinivas Reddy", "Srikrishna Bhashyam"], "title": "Sequential Spectral Clustering of Data Sequences", "comment": null, "summary": "We study the problem of nonparametric clustering of data sequences, where\neach data sequence comprises i.i.d. samples generated from an unknown\ndistribution. The true clusters are the clusters obtained using the Spectral\nclustering algorithm (SPEC) on the pairwise distance between the true\ndistributions corresponding to the data sequences. Since the true distributions\nare unknown, the objective is to estimate the clusters by observing the minimum\nnumber of samples from the data sequences for a given error probability. To\nsolve this problem, we propose the Sequential Spectral clustering algorithm\n(SEQ-SPEC), and show that it stops in finite time almost surely and is\nexponentially consistent. We also propose a computationally more efficient\nalgorithm called the Incremental Approximate Sequential Spectral clustering\nalgorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed\nalgorithms perform better than the fixed sample size SPEC, the Sequential\n$K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage\nclustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally\nefficient, performs close to SEQ-SPEC on both synthetic and real-world\ndatasets. To the best of our knowledge, this is the first work on spectral\nclustering of data sequences under a sequential framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5e8f\u5217\u8c31\u805a\u7c7b\u7b97\u6cd5(SEQ-SPEC\u548cIA-SEQ-SPEC)\uff0c\u7528\u4e8e\u6570\u636e\u5e8f\u5217\u7684\u975e\u53c2\u6570\u805a\u7c7b\uff0c\u5728\u6709\u9650\u6837\u672c\u4e0b\u5b9e\u73b0\u6307\u6570\u4e00\u81f4\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u6837\u672c\u65b9\u6cd5\u548c\u4f20\u7edf\u5e8f\u5217\u805a\u7c7b\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u5e8f\u5217\u7684\u975e\u53c2\u6570\u805a\u7c7b\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u5305\u542b\u4ece\u672a\u77e5\u5206\u5e03\u751f\u6210\u7684i.i.d.\u6837\u672c\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u56fa\u5b9a\u6837\u672c\u91cf\uff0c\u800c\u5e8f\u5217\u6846\u67b6\u53ef\u4ee5\u5728\u7ed9\u5b9a\u9519\u8bef\u6982\u7387\u4e0b\u901a\u8fc7\u89c2\u5bdf\u6700\u5c11\u6837\u672c\u5b9e\u73b0\u805a\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86SEQ-SPEC\u7b97\u6cd5\u548c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684IA-SEQ-SPEC\u7b97\u6cd5\u3002SEQ-SPEC\u57fa\u4e8e\u8c31\u805a\u7c7b\u7b97\u6cd5\uff0c\u5728\u5e8f\u5217\u6846\u67b6\u4e0b\u5de5\u4f5c\uff1bIA-SEQ-SPEC\u662f\u5176\u589e\u91cf\u8fd1\u4f3c\u7248\u672c\uff0c\u8ba1\u7b97\u66f4\u9ad8\u6548\u3002", "result": "\u7b97\u6cd5\u5728\u6709\u9650\u65f6\u95f4\u5185\u51e0\u4e4e\u5fc5\u7136\u505c\u6b62\u4e14\u5177\u6709\u6307\u6570\u4e00\u81f4\u6027\u3002\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u7b97\u6cd5\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u6837\u672cSPEC\u3001SEQ-KMED\u548cSEQ-SLINK\u7b97\u6cd5\u3002IA-SEQ-SPEC\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6027\u80fd\u63a5\u8fd1SEQ-SPEC\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u5e8f\u5217\u6846\u67b6\u4e0b\u7814\u7a76\u6570\u636e\u5e8f\u5217\u8c31\u805a\u7c7b\u7684\u5de5\u4f5c\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5e8f\u5217\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09147", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09147", "abs": "https://arxiv.org/abs/2509.09147", "authors": ["Ziqi Yan", "Zhichao Zhang"], "title": "JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information", "comment": null, "summary": "Wiener filtering in the joint time-vertex fractional Fourier transform\n(JFRFT) domain has shown high effectiveness in denoising time-varying graph\nsignals. Traditional filtering models use grid search to determine the\ntransform-order pair and compute filter coefficients, while learnable ones\nemploy gradient-descent strategies to optimize them; both require complete\nprior information of graph signals. To overcome this shortcoming, this letter\nproposes a data-model co-driven denoising approach, termed neural-network-aided\njoint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the\nJFRFT-domain Wiener filter model into a neural network and updates the\ntransform-order pair and filter coefficients through a data-driven approach.\nThis design enables effective denoising using only partial prior information.\nExperiments demonstrate that JFRFFNet achieves significant improvements in\noutput signal-to-noise ratio compared with some state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faJFRFFNet\u65b9\u6cd5\uff0c\u5c06\u8054\u5408\u65f6-\u9876\u70b9\u5206\u6570\u5085\u91cc\u53f6\u53d8\u6362\u57df\u7684\u7ef4\u7eb3\u6ee4\u6ce2\u6a21\u578b\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u5f0f\u66f4\u65b0\u53d8\u6362\u9636\u6570\u5bf9\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\uff0c\u4ec5\u9700\u90e8\u5206\u5148\u9a8c\u4fe1\u606f\u5373\u53ef\u6709\u6548\u53bb\u566a\u3002", "motivation": "\u4f20\u7edf\u6ee4\u6ce2\u65b9\u6cd5\u9700\u8981\u5b8c\u6574\u7684\u56fe\u4fe1\u53f7\u5148\u9a8c\u4fe1\u606f\uff0c\u8981\u4e48\u901a\u8fc7\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u53d8\u6362\u9636\u6570\u5bf9\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\uff0c\u8981\u4e48\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b56\u7565\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5c06JFRFT\u57df\u7ef4\u7eb3\u6ee4\u6ce2\u6a21\u578b\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u91c7\u7528\u6570\u636e-\u6a21\u578b\u534f\u540c\u9a71\u52a8\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u66f4\u65b0\u53d8\u6362\u9636\u6570\u5bf9\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eJFRFFNet\u5728\u8f93\u51fa\u4fe1\u566a\u6bd4\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "JFRFFNet\u65b9\u6cd5\u80fd\u591f\u4ec5\u4f7f\u7528\u90e8\u5206\u5148\u9a8c\u4fe1\u606f\u5b9e\u73b0\u6709\u6548\u7684\u65f6\u53d8\u56fe\u4fe1\u53f7\u53bb\u566a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5b8c\u6574\u5148\u9a8c\u4fe1\u606f\u7684\u4f9d\u8d56\u95ee\u9898\u3002"}}
{"id": "2509.09225", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09225", "abs": "https://arxiv.org/abs/2509.09225", "authors": ["Lin Jin", "Hang Sheng", "Hui Feng", "Bo Hu"], "title": "On Sampling of Multiple Correlated Stochastic Signals", "comment": null, "summary": "Multiple stochastic signals possess inherent statistical correlations, yet\nconventional sampling methods that process each channel independently result in\ndata redundancy. To leverage this correlation for efficient sampling, we model\ncorrelated channels as a linear combination of a smaller set of uncorrelated,\nwide-sense stationary latent sources. We establish a theoretical lower bound on\nthe total sampling density for zero mean-square error reconstruction, proving\nit equals the ratio of the joint spectral bandwidth of latent sources to the\nnumber of correlated signal channels. We then develop a constructive multi-band\nsampling scheme that attains this bound. The proposed method operates via\nspectral partitioning of the latent sources, followed by spatio-temporal\nsampling and interpolation. Experiments on synthetic and real datasets confirm\nthat our scheme achieves near-lossless reconstruction precisely at the\ntheoretical sampling density, validating its efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6e90\u5206\u89e3\u7684\u591a\u901a\u9053\u76f8\u5173\u4fe1\u53f7\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u76f8\u5173\u901a\u9053\u4e3a\u5c11\u91cf\u4e0d\u76f8\u5173\u6f5c\u5728\u6e90\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u6700\u5c0f\u91c7\u6837\u5bc6\u5ea6\u4e0b\u7684\u8fd1\u65e0\u635f\u91cd\u5efa\u3002", "motivation": "\u591a\u901a\u9053\u968f\u673a\u4fe1\u53f7\u5b58\u5728\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u4f46\u4f20\u7edf\u72ec\u7acb\u91c7\u6837\u65b9\u6cd5\u5bfc\u81f4\u6570\u636e\u5197\u4f59\u3002\u4e3a\u4e86\u5229\u7528\u76f8\u5173\u6027\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fbe\u5230\u7406\u8bba\u6700\u5c0f\u91c7\u6837\u5bc6\u5ea6\u7684\u91c7\u6837\u65b9\u6848\u3002", "method": "\u5c06\u76f8\u5173\u901a\u9053\u5efa\u6a21\u4e3a\u5c11\u91cf\u4e0d\u76f8\u5173\u5bbd\u5e73\u7a33\u6f5c\u5728\u6e90\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u901a\u8fc7\u6f5c\u5728\u6e90\u7684\u9891\u8c31\u5206\u5272\uff0c\u7ed3\u5408\u7a7a\u65f6\u91c7\u6837\u548c\u63d2\u503c\u6280\u672f\uff0c\u6784\u5efa\u591a\u6ce2\u6bb5\u91c7\u6837\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u90fd\u80fd\u5728\u7406\u8bba\u91c7\u6837\u5bc6\u5ea6\u4e0b\u5b9e\u73b0\u8fd1\u65e0\u635f\u91cd\u5efa\uff0c\u9a8c\u8bc1\u4e86\u5176\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6ce2\u6bb5\u91c7\u6837\u65b9\u6848\u8fbe\u5230\u4e86\u7406\u8bba\u6700\u5c0f\u91c7\u6837\u5bc6\u5ea6\uff0c\u4e3a\u591a\u901a\u9053\u76f8\u5173\u4fe1\u53f7\u7684\u9ad8\u6548\u91c7\u6837\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09264", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09264", "abs": "https://arxiv.org/abs/2509.09264", "authors": ["Davoud Hajhassani", "Quentin Barth\u00e9lemy", "J\u00e9r\u00e9mie Mattout", "Marco Congedo"], "title": "Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG", "comment": null, "summary": "Electroencephalography (EEG) signal cleaning has long been a critical\nchallenge in the research community. The presence of artifacts can\nsignificantly degrade EEG data quality, complicating analysis and potentially\nleading to erroneous interpretations. While various artifact rejection methods\nhave been proposed, the gold standard remains manual visual inspection by human\nexperts-a process that is time-consuming, subjective, and impractical for\nlarge-scale EEG studies. Existing techniques are often hindered by a strong\nreliance on manual hyperparameter tuning, sensitivity to outliers, and high\ncomputational costs. In this paper, we introduce the improved Riemannian Potato\nField (iRPF), a fast and fully automated method for EEG artifact rejection that\naddresses key limitations of current approaches. We evaluate iRPF against\nseveral state-of-the-art artifact rejection methods, using two publicly\navailable EEG databases, labeled for various artifact types, comprising 226 EEG\nrecordings. Our results demonstrate that iRPF outperforms all competitors\nacross multiple metrics, with gains of up to 22% in recall, 102% in\nspecificity, 54% in precision, and 24% in F1-score, compared to Isolation\nForest, Autoreject, Riemannian Potato, and Riemannian Potato Field,\nrespectively. Statistical analysis confirmed the significance of these\nimprovements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most\ncomparisons. Additionally, on a typical EEG recording iRPF performs artifact\ncleaning in under 8 milliseconds per epoch using a standard laptop,\nhighlighting its efficiency for large-scale EEG data processing and real-time\napplications. iRPF offers a robust and data-driven artifact rejection solution\nfor high-quality EEG pre-processing in brain-computer interfaces and clinical\nneuroimaging applications.", "AI": {"tldr": "iRPF\u662f\u4e00\u79cd\u5feb\u901f\u5168\u81ea\u52a8\u7684EEG\u4f2a\u5f71\u53bb\u9664\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5904\u7406\u901f\u5ea6\u5feb\u81f3\u6bcfepoch 8\u6beb\u79d2", "motivation": "EEG\u4fe1\u53f7\u6e05\u6d01\u662f\u7814\u7a76\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u53c2\u6570\u8c03\u4f18\u3001\u5bf9\u5f02\u5e38\u503c\u654f\u611f\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u6539\u8fdb\u7684\u9ece\u66fc\u571f\u8c46\u573a(iRPF)\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9ece\u66fc\u51e0\u4f55\u7684\u81ea\u52a8\u4f2a\u5f71\u62d2\u7edd\u6280\u672f", "result": "\u5728226\u4e2aEEG\u8bb0\u5f55\u4e0a\u6d4b\u8bd5\uff0ciRPF\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5728\u53ec\u56de\u7387\u63d0\u534722%\uff0c\u7279\u5f02\u6027\u63d0\u5347102%\uff0c\u7cbe\u786e\u5ea6\u63d0\u534754%\uff0cF1\u5206\u6570\u63d0\u534724%\uff0c\u7edf\u8ba1\u663e\u8457(p<0.001)", "conclusion": "iRPF\u4e3a\u8111\u673a\u63a5\u53e3\u548c\u4e34\u5e8a\u795e\u7ecf\u5f71\u50cf\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u6570\u636e\u9a71\u52a8\u7684\u4f2a\u5f71\u62d2\u7edd\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21EEG\u6570\u636e\u5904\u7406\u548c\u5b9e\u65f6\u5e94\u7528"}}
{"id": "2509.09282", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09282", "abs": "https://arxiv.org/abs/2509.09282", "authors": ["Leonardo M\u00f6rlein", "Dirk Manteuffel"], "title": "On the Relation of Characteristic Modes of Different Conducting Structures", "comment": null, "summary": "A formalism is derived to analyze the scattering of a conducting structure\nbased on the characteristic modes of another structure whose surface is a\nsuperset of the first structure. This enables the analysis and comparison of\ndifferent structures using a common basis of characteristic modes.\nAdditionally, it is shown that the scattering matrices and perturbation\nmatrices are no longer diagonal in these cases. Based on this, a modal\ntransformation matrix is defined to describe the mapping between the\ncharacteristic fields and the weighting coefficients of the two structures.\nThis matrix enables the conversion of the perturbation matrices in different\nbases. Finally, two examples are provided along with a discussion of some\naspects of the theory. The first example aims to validate and illustrate the\nformalism. The second example shows how the formalism can be applied in the\ndesign process of an antenna element that is gradually modified, starting from\na base structure.", "AI": {"tldr": "\u57fa\u4e8e\u53e6\u4e00\u7ed3\u6784\u7684\u7279\u5f81\u6a21\u5f0f\u5206\u6790\u5bfc\u4f53\u7ed3\u6784\u6563\u5c04\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5b9a\u4e49\u6a21\u6001\u53d8\u6362\u77e9\u9635\u5e76\u63d0\u4f9b\u5e94\u7528\u793a\u4f8b", "motivation": "\u4e3a\u4e86\u4f7f\u7528\u5171\u540c\u7684\u7279\u5f81\u6a21\u5f0f\u57fa\u7840\u6765\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u5bfc\u4f53\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5f53\u4e00\u4e2a\u7ed3\u6784\u7684\u8868\u9762\u662f\u53e6\u4e00\u7ed3\u6784\u7684\u8d85\u96c6\u65f6", "method": "\u63a8\u5bfc\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5b9a\u4e49\u6a21\u6001\u53d8\u6362\u77e9\u9635\u6765\u63cf\u8ff0\u4e24\u4e2a\u7ed3\u6784\u4e4b\u95f4\u7279\u5f81\u573a\u548c\u6743\u91cd\u7cfb\u6570\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u8fdb\u884c\u77e9\u9635\u57fa\u51c6\u8f6c\u6362", "result": "\u8bc1\u660e\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u6563\u5c04\u77e9\u9635\u548c\u5fae\u6c27\u77e9\u9635\u4e0d\u518d\u662f\u5bf9\u89d2\u77e9\u9635\uff0c\u901a\u8fc7\u4e24\u4e2a\u793a\u4f8b\u9a8c\u8bc1\u548c\u5c55\u793a\u4e86\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e3a\u4ece\u57fa\u7840\u7ed3\u6784\u9010\u6b65\u4fee\u6539\u5929\u7ebf\u5143\u4ef6\u7684\u8bbe\u8ba1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u6a21\u6001\u53d8\u6362\u77e9\u9635\u5b9e\u73b0\u4e86\u4e0d\u540c\u57fa\u51c6\u4e0b\u5fae\u6c27\u77e9\u9635\u7684\u8f6c\u6362"}}
{"id": "2509.09373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09373", "abs": "https://arxiv.org/abs/2509.09373", "authors": ["Huayan Guo", "Jichen Zhang", "Junhui Rao", "Ross Murch", "Vincent K. N. Lau"], "title": "Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems", "comment": "13 pages, 12 figures", "summary": "Pixel-based fluid antennas provide enhanced multiplexing gains and quicker\nradiation pattern switching than traditional designs. However, this innovation\nintroduces challenges for channel estimation and analog precoding due to the\nstate-non-separable channel response problem. This paper explores a multiuser\nMIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements\nfrom a real-world prototype. We present a sparse channel recovery framework for\nuplink channel sounding, employing an approximate separable channel response\nmodel with DNN-based antenna radiation functions. We then propose two\nlow-complexity channel estimation algorithms that leverage orthogonal matching\npursuit and variational Bayesian inference to accurately recover channel\nresponses across various scattering cluster angles. These estimations enable\nthe prediction of composite channels for all fluid antenna states, leading to\nan analog precoding scheme that optimally selects switching states for\ndifferent antennas. Our simulation results indicate that the proposed approach\nsignificantly outperforms several baseline methods, especially in high\nsignal-to-noise ratio environments with numerous users.", "AI": {"tldr": "\u57fa\u4e8e\u6d53\u7a00\u901a\u9053\u6062\u590d\u6846\u67b6\u7684\u50cf\u7d20\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u901a\u9053\u4f30\u8ba1\u7b97\u6cd5\u548c\u6a21\u62df\u9884\u7f16\u7801\u65b9\u6848\uff0c\u5728\u9ad8\u4fe1\u5668\u6bd4\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u50cf\u7d20\u6d41\u4f53\u5929\u7ebf\u867d\u7136\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u591a\u5de5\u589e\u76ca\u548c\u66f4\u5feb\u7684\u8f90\u5c04\u56fe\u5207\u6362\uff0c\u4f46\u5f15\u5165\u4e86\u72b6\u6001\u975e\u53ef\u5206\u79bb\u901a\u9053\u54cd\u5e94\u95ee\u9898\uff0c\u5bf9\u901a\u9053\u4f30\u8ba1\u548c\u6a21\u62df\u9884\u7f16\u7801\u6784\u6210\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d53\u7a00\u901a\u9053\u6062\u590d\u6846\u67b6\uff0c\u4f7f\u7528\u8fd1\u4f3c\u53ef\u5206\u79bb\u901a\u9053\u54cd\u5e94\u6a21\u578b\u548c\u57fa\u4e8eDNN\u7684\u5929\u7ebf\u8f90\u5c04\u51fd\u6570\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff1a\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u6cd5\u548c\u53d8\u5206\u8d1d\u53f6\u65af\u63a8\u65ad\u6cd5\uff0c\u7528\u4e8e\u6062\u590d\u4e0d\u540c\u6563\u5c04\u96c6\u7fa4\u89d2\u5ea6\u7684\u901a\u9053\u54cd\u5e94\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u9ad8\u4fe1\u5668\u6bd4\u73af\u5883\u4e0b\u7279\u522b\u662f\u5728\u7528\u6237\u6570\u91cf\u8f83\u591a\u65f6\uff0c\u6027\u80fd\u663e\u8457\u8d85\u8fc7\u591a\u4e2a\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u50cf\u7d20\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u4e2d\u7684\u901a\u9053\u4f30\u8ba1\u548c\u9884\u7f16\u7801\u6311\u6218\uff0c\u901a\u8fc7\u6d53\u7a00\u6062\u590d\u548c\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.09606", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09606", "abs": "https://arxiv.org/abs/2509.09606", "authors": ["Sajjad Hussain"], "title": "A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Accurate pathloss prediction is essential for the design and optimization of\nUAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches\nhave shown strong potential, their generalization across diverse environments,\nrobustness to noisy inputs, and sensitivity to UAV altitude remain\nunderexplored. To address these challenges, we propose a UNet-based deep\nlearning architecture that combines multi-scale feature extraction,\nconvolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)\nbottleneck for efficient context aggregation. The model predicts pathloss maps\nfrom log-distance, line-of-sight (LOS) mask, and building mask inputs. In\naddition, we develop a fully vectorized LOS mask computation algorithm that\nsignificantly accelerates pre-processing and enables large-scale dataset\ngeneration. Extensive evaluations on both in-house ray-tracing data and the\nRadioMapSeer benchmark demonstrate that the proposed model outperforms several\nstate-of-the-art baselines in accuracy and efficiency. All source code is\npublicly released to support reproducibility and future research.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eUNet\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u6beb\u7c73\u6ce2\u7f51\u7edc\u8def\u5f84\u635f\u8017\u9884\u6d4b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548cASPP\u74f6\u9888\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u8def\u5f84\u635f\u8017\u9884\u6d4b\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u5bf9\u566a\u58f0\u8f93\u5165\u9c81\u68d2\u6027\u5dee\u4ee5\u53ca\u5bf9\u65e0\u4eba\u673a\u9ad8\u5ea6\u654f\u611f\u7b49\u95ee\u9898", "method": "\u4f7f\u7528UNet\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u5377\u79ef\u7279\u5f81\u878d\u5408\u548cASPP\u74f6\u9888\u8fdb\u884c\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u8f93\u5165\u5305\u62ec\u5bf9\u6570\u8ddd\u79bb\u3001LOS\u63a9\u7801\u548c\u5efa\u7b51\u63a9\u7801\uff0c\u5e76\u5f00\u53d1\u4e86\u5411\u91cf\u5316LOS\u63a9\u7801\u8ba1\u7b97\u7b97\u6cd5", "result": "\u5728\u5185\u90e8\u5c04\u7ebf\u8ffd\u8e2a\u6570\u636e\u548cRadioMapSeer\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u80fd\u591f\u6709\u6548\u9884\u6d4b\u8def\u5f84\u635f\u8017\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6240\u6709\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u672a\u6765\u7814\u7a76"}}

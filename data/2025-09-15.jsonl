{"id": "2509.09695", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09695", "abs": "https://arxiv.org/abs/2509.09695", "authors": ["Fabio Magarelli", "Geraldine B. Boylan", "Saeed Montazeri", "Feargal O'Sullivan", "Dominic Lightbody", "Minoo Ashoori", "Tamara Skoric Ceranic", "John M. O'Toole"], "title": "Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy", "comment": "29 pages, supplementary materials: \"supplementary materials ML\n  Comp.docx\"", "summary": "Machine learning (ML) has the potential to support and improve expert\nperformance in monitoring the brain function of at-risk newborns. Developing\naccurate and reliable ML models depends on access to high-quality, annotated\ndata, a resource in short supply. ML competitions address this need by\nproviding researchers access to expertly annotated datasets, fostering shared\nlearning through direct model comparisons, and leveraging the benefits of\ncrowdsourcing diverse expertise. We compiled a retrospective dataset containing\n353 hours of EEG from 102 individual newborns from a multi-centre study. The\ndata was fully anonymised and divided into training, testing, and held-out\nvalidation datasets. EEGs were graded for the severity of abnormal background\npatterns. Next, we created a web-based competition platform and hosted a\nmachine learning competition to develop ML models for classifying the severity\nof EEG background patterns in newborns. After the competition closed, the top 4\nperforming models were evaluated offline on a separate held-out validation\ndataset. Although a feature-based model ranked first on the testing dataset,\ndeep learning models generalised better on the validation sets. All methods had\na significant decline in validation performance compared to the testing\nperformance. This highlights the challenges for model generalisation on unseen\ndata, emphasising the need for held-out validation datasets in ML studies with\nneonatal EEG. The study underscores the importance of training ML models on\nlarge and diverse datasets to ensure robust generalisation. The competition's\noutcome demonstrates the potential for open-access data and collaborative ML\ndevelopment to foster a collaborative research environment and expedite the\ndevelopment of clinical decision-support tools for neonatal neuromonitoring."}
{"id": "2509.09820", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.09820", "abs": "https://arxiv.org/abs/2509.09820", "authors": ["Ahmed Ali Abbasi", "Namrata Vaswani"], "title": "Locally Permuted Low Rank Column-wise Sensing", "comment": null, "summary": "We precisely formulate, and provide a solution for, the Low Rank Columnwise\nSensing (LRCS) problem when some of the observed data is\nscrambled/permuted/unlabeled. This problem, which we refer to as permuted LRCS,\nlies at the intersection of two distinct topics of recent research: unlabeled\nsensing and low rank column-wise (matrix) sensing. We introduce a novel\ngeneralization of the recently developed Alternating Gradient Descent and\nMinimization (AltGDMin) algorithm to solve this problem. We also develop an\nalternating minimization (AltMin) solution. We show, using simulation\nexperiments, that both converge but PermutedAltGDmin is much faster than\nPermuted-AltMin."}
{"id": "2509.09837", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09837", "abs": "https://arxiv.org/abs/2509.09837", "authors": ["Jiapei Tian", "Abolfazl Zakeri", "Marian Codreanu", "David Gundleg책rd"], "title": "Real-Time Remote Tracking with State-Dependent Detection Probability: A POMDP Framework", "comment": null, "summary": "We consider a real-time tracking system where a binary Markov source is\nmonitored by two heterogeneous sensors. Upon command, sensors send their\nobservations to a remote sink over error-prone channels. We assume each sensor\nexhibits state-dependent detection accuracy and may occasionally fail to detect\nthe source state. At most one sensor is scheduled for sampling at each time\nslot. We assess the effectiveness of data communication using a generic\ndistortion function that captures the end application's objective. We derive\noptimal sink-side command policies to minimize the weighted sum of distortion\nand transmission costs. To model the uncertainty introduced by sensing failures\n(of the sensors) and packet loss, we formulate the problem as a partially\nobservable Markov decision process (POMDP), which we then cast into a\nbelief-MDP. Since the belief evolves continuously, the belief space is\ndiscretized into a finite grid and the belief value is quantized to the nearest\ngrid point after each update. This formulation leads to a finite-state MDP\nproblem, which is solved using the relative value iteration algorithm (RVIA).\nSimulation results demonstrate that the proposed policy significantly\noutperforms benchmark strategies and highlights the importance of accounting\nfor state-dependent sensing reliability in sensor scheduling."}
{"id": "2509.09842", "categories": ["eess.SP", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.09842", "abs": "https://arxiv.org/abs/2509.09842", "authors": ["Anu Tripathi", "Yang Wan", "Zhiren Zhu", "Furkan Camci", "Sheila Turcsanyi", "Jeneel Pravin Kachhadiya", "Mauricio Araiza Canizales", "Alison Brooks", "Haneesh Kesari", "Joseph Andrews", "Traci Snedden", "Peter Ferrazzano", "Christian Franck", "Rika Wright Carlsen"], "title": "Field evaluation of a wearable instrumented headband designed for measuring head kinematics", "comment": null, "summary": "Purpose: To study the relationship between soccer heading and the risk of\nmild traumatic brain injury (mTBI), we previously developed an instrumented\nheadband and data processing scheme to measure the angular head kinematics of\nsoccer headers. Laboratory evaluation of the headband on an anthropomorphic\ntest device showed good agreement with a reference sensor for soccer ball\nimpacts to the front of the head. In this study, we evaluate the headband in\nmeasuring the full head kinematics of soccer headers in the field. Methods: The\nheadband was evaluated under typical soccer heading scenarios (throw-ins,\ngoal-kicks, and corner-kicks) on a human subject. The measured time history and\npeak kinematics from the headband were compared with those from an instrumented\nmouthpiece, which is a widely accepted method for measuring head kinematics in\nthe field. Results: The time history agreement (CORA scores) between the\nheadband and the mouthpiece ranged from 'fair' to 'excellent', with the highest\nagreement for angular velocities (0.79 \\pm 0.08) and translational\naccelerations (0.73 \\pm 0.05) and lowest for angular accelerations (0.67 \\pm\n0.06). A Bland-Altman analysis of the peak kinematics from the headband and\nmouthpiece found the mean bias to be 40.9% (of the maximum mouthpiece reading)\nfor the angular velocity, 16.6% for the translational acceleration, and-14.1%\nfor the angular acceleration. Conclusion: The field evaluation of the\ninstrumented headband showed reasonable agreement with the mouthpiece for some\nkinematic measures and impact conditions. Future work should focus on improving\nthe headband performance across all kinematic measures."}
{"id": "2509.09719", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09719", "abs": "https://arxiv.org/abs/2509.09719", "authors": ["Hemanth Chandravamsi", "Dhanush V. Shenoy", "Itay Zinn", "Shimon Pisnoy", "Steven H. Frankel"], "title": "Spectral Bottleneck in Deep Neural Networks: Noise is All You Need", "comment": null, "summary": "Deep neural networks are known to exhibit a spectral learning bias, wherein\nlow-frequency components are learned early in training, while high-frequency\nmodes emerge more gradually in later epochs. However, when the target signal\nlacks low-frequency components and is dominated by broadband high frequencies,\ntraining suffers from a 'spectral bottleneck', and the model fails to\nreconstruct the entire signal, including the frequency components that lie\nwithin the network's representational capacity. We examine such a scenario in\nthe context of implicit neural representations (INRs) with sinusoidal\nrepresentation networks (SIRENs), focusing on the challenge of fitting\nhigh-frequency-dominant signals that are susceptible to spectral bottleneck. To\neffectively fit any target signal irrespective of it's frequency content, we\npropose a generalized target-aware 'weight perturbation scheme' (WINNER -\nweight initialization with noise for neural representations) for network\ninitialization. The scheme perturbs uniformly initialized weights with Gaussian\nnoise, where the noise scales are adaptively determined by the spectral\ncentroid of the target signal. We show that the noise scales can provide\ncontrol over the spectra of network activations and the eigenbasis of the\nempirical neural tangent kernel. This method not only addresses the spectral\nbottleneck but also yields faster convergence and with improved representation\naccuracy, outperforming state-of-the-art approaches in audio fitting and\nachieving notable gains in image fitting and denoising tasks. Beyond signal\nreconstruction, our approach opens new directions for adaptive weight\ninitialization strategies in computer vision and scientific machine learning."}
{"id": "2509.09716", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09716", "abs": "https://arxiv.org/abs/2509.09716", "authors": ["Jun Zhan", "Mingyang Han", "Yuxuan Xie", "Chen Wang", "Dong Zhang", "Kexin Huang", "Haoxiang Shi", "DongXiao Wang", "Tengtao Song", "Qinyuan Cheng", "Shimin Li", "Jun Song", "Xipeng Qiu", "Bo Zheng"], "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions", "comment": null, "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\n\\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}."}
{"id": "2509.10009", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10009", "abs": "https://arxiv.org/abs/2509.10009", "authors": ["Zhiwei Liang", "Bin Chen", "Jiwei Xu", "Yi Lei", "Qingqing Hu", "Fan Zhang", "Gabriele Liga"], "title": "A General Nonlinear Model for Arbitrary Modulation Formats in the Presence of Inter-Channel Simulated Raman Scattering", "comment": "4 Pages, 2 figures", "summary": "The four-dimensional nonlinear model is extended to include the inter-channel\nstimulated Raman scattering, enabling accurate prediction of dual-polarization\nfour-dimensional modulation formats and probabilistically shaped constellations\nin high-dispersion regimes. The proposed model is validated via comparisons\nwith the split-step Fourier method and enhanced Gaussian noise model."}
{"id": "2509.09791", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09791", "abs": "https://arxiv.org/abs/2509.09791", "authors": ["Carlos Busso", "Reza Lotfian", "Kusha Sridhar", "Ali N. Salman", "Wei-Cheng Lin", "Lucas Goncalves", "Srinivas Parthasarathy", "Abinay Reddy Naini", "Seong-Gyun Leem", "Luz Martinez-Lucas", "Huang-Cheng Chou", "Pravin Mote"], "title": "The MSP-Podcast Corpus", "comment": "IEEE Transactions on Affective Computing submission", "summary": "The availability of large, high-quality emotional speech databases is\nessential for advancing speech emotion recognition (SER) in real-world\nscenarios. However, many existing databases face limitations in size, emotional\nbalance, and speaker diversity. This study describes the MSP-Podcast corpus,\nsummarizing our ten-year effort. The corpus consists of over 400 hours of\ndiverse audio samples from various audio-sharing websites, all of which have\nCommon Licenses that permit the distribution of the corpus. We annotate the\ncorpus with rich emotional labels, including primary (single dominant emotion)\nand secondary (multiple emotions perceived in the audio) emotional categories,\nas well as emotional attributes for valence, arousal, and dominance. At least\nfive raters annotate these emotional labels. The corpus also has speaker\nidentification for most samples, and human transcriptions of the lexical\ncontent of the sentences for the entire corpus. The data collection protocol\nincludes a machine learning-driven pipeline for selecting emotionally diverse\nrecordings, ensuring a balanced and varied representation of emotions across\nspeakers and environments. The resulting database provides a comprehensive,\nhigh-quality resource, better suited for advancing SER systems in practical,\nreal-world scenarios."}
{"id": "2509.09717", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09717", "abs": "https://arxiv.org/abs/2509.09717", "authors": ["Jorge E. Le처n", "Miguel Carrasco"], "title": "Testing chatbots on the creation of encoders for audio conditioned image generation", "comment": null, "summary": "On one hand, recent advances in chatbots has led to a rising popularity in\nusing these models for coding tasks. On the other hand, modern generative image\nmodels primarily rely on text encoders to translate semantic concepts into\nvisual representations, even when there is clear evidence that audio can be\nemployed as input as well. Given the previous, in this work, we explore whether\nstate-of-the-art conversational agents can design effective audio encoders to\nreplace the CLIP text encoder from Stable Diffusion 1.5, enabling image\nsynthesis directly from sound. We prompted five publicly available chatbots to\npropose neural architectures to work as these audio encoders, with a set of\nwell-explained shared conditions. Each valid suggested encoder was trained on\nover two million context related audio-image-text observations, and evaluated\non held-out validation and test sets using various metrics, together with a\nqualitative analysis of their generated images. Although almost all chatbots\ngenerated valid model designs, none achieved satisfactory results, indicating\nthat their audio embeddings failed to align reliably with those of the original\ntext encoder. Among the proposals, the Gemini audio encoder showed the best\nquantitative metrics, while the Grok audio encoder produced more coherent\nimages (particularly, when paired with the text encoder). Our findings reveal a\nshared architectural bias across chatbots and underscore the remaining coding\ngap that needs to be bridged in future versions of these models. We also\ncreated a public demo so everyone could study and try out these audio encoders.\nFinally, we propose research questions that should be tackled in the future,\nand encourage other researchers to perform more focused and highly specialized\ntasks like this one, so the respective chatbots cannot make use of well-known\nsolutions and their creativity/reasoning is fully tested."}
{"id": "2509.10076", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10076", "abs": "https://arxiv.org/abs/2509.10076", "authors": ["Apostolos A. Tegos", "Yue Xiao", "Sotiris A. Tegos", "George K. Karagiannidis", "Panagiotis D. Diamantoulakis"], "title": "Uplink RSMA for Pinching-Antenna Systems", "comment": null, "summary": "One of the key goals of next-generation wireless networks is to adapt to\nchanging conditions and meet the growing demand for reliable, high-capacity\ncommunications from emerging applications. Overcoming the limitations of\nconventional technologies, such as fixed antenna positions, is essential to\nachieving this objective because it mitigates the impact of path loss on the\nreceived signal and creates strong line-of-sight links, enhancing system\nperformance. With this in mind, the newly proposed pinching antenna systems\n(PASs) are a promising solution for indoor applications because they can\nactivate antennas across a waveguide deployed in a room, thus reducing the\ndistance between the transmitter and receiver. In this paper, we investigate a\ntwo-user, two-pinching-antenna uplink PAS, in which the transmitters use rate\nsplitting to create a more resilient framework than non-orthogonal multiple\naccess (NOMA). For this network, we derive novel closed-form expressions for\nthe outage probability. Numerical results validate these expressions, proving\nthat the proposed rate-splitting multiple access (RSMA) scheme outperforms NOMA\nPAS."}
{"id": "2509.09931", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09931", "abs": "https://arxiv.org/abs/2509.09931", "authors": ["Ee-Leng Tan", "Jun Wei Yeow", "Santi Peksi", "Haowen Li", "Ziyi Yang", "Woon-Seng Gan"], "title": "Acoustic Scene Classification Using CNN-GRU Model Without Knowledge Distillation", "comment": "3 pages, 2 figures, 2 tables", "summary": "In this technical report, we present the SNTL-NTU team's Task 1 submission\nfor the Low-Complexity Acoustic Scenes and Events (DCASE) 2025 challenge. This\nsubmission departs from the typical application of knowledge distillation from\na teacher to a student model, aiming to achieve high performance with limited\ncomplexity. The proposed model is based on a CNN-GRU model and is trained\nsolely using the TAU Urban Acoustic Scene 2022 Mobile development dataset,\nwithout utilizing any external datasets, except for MicIRP, which is used for\ndevice impulse response (DIR) augmentation. The proposed model has a memory\nusage of 114.2KB and requires 10.9M muliply-and-accumulate (MAC) operations.\nUsing the development dataset, the proposed model achieved an accuracy of\n60.25%."}
{"id": "2509.09746", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09746", "abs": "https://arxiv.org/abs/2509.09746", "authors": ["Ning Ma", "Bahman Mirheidari", "Guy J. Brown", "Minyoi M. Maimbolwa", "Nsala Sanjase", "Solomon Chifwamba", "Seke Muzazu", "Monde Muyoyeta", "Mary Kagujje"], "title": "AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models", "comment": "submitted to The Lancet Digital Health", "summary": "Background\n  Artificial intelligence (AI) can detect disease-related acoustic patterns in\ncough sounds, offering a scalable approach to tuberculosis (TB) screening in\nhigh-burden, low-resource settings. Previous studies have been limited by small\ndatasets, under-representation of symptomatic non-TB patients, reliance on\nsimple models, and recordings collected under idealised conditions.\n  Methods\n  We enrolled 512 participants at two hospitals in Zambia, grouped as\nbacteriologically confirmed TB (TB+), symptomatic patients with other\nrespiratory diseases (OR), and healthy controls (HC). Usable cough recordings\nplus demographic and clinical data were obtained from 500 participants. Deep\nlearning classifiers based on speech foundation models were trained on cough\nrecordings. The best-performing model, trained on 3-second segments, was\nfurther evaluated with demographic and clinical features.\n  Findings\n  The best audio-only classifier achieved an AUROC of 85.2% for distinguishing\nTB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographic\nand clinical features improved performance to 92.1% (TB+/Rest) and 84.2%\n(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%\nsensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.\n  Interpretation\n  Cough analysis using speech foundation models, especially when combined with\ndemographic and clinical data, showed strong potential as a TB triage tool,\nmeeting WHO target product profile benchmarks. The model was robust to\nconfounding factors including background noise, recording time, and device\nvariability, indicating detection of genuine disease-related acoustic patterns.\nFurther validation across diverse regions and case definitions, including\nsubclinical TB, is required before clinical use."}
{"id": "2509.10082", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10082", "abs": "https://arxiv.org/abs/2509.10082", "authors": ["Weitao Tang", "Johann Vargas-Calixto", "Nasim Katebi", "Nhi Tran", "Sharmony B. Kelly", "Gari D. Clifford", "Robert Galinsky", "Faezeh Marzbanrad"], "title": "FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification", "comment": "13 pages, 4 tables, 5 figures, submitted to IEEE Journal of\n  Biomedical and Health Informatics", "summary": "Introduction: This study presents FetalSleepNet, the first published deep\nlearning approach to classifying sleep states from the ovine\nelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and\nlaborious to interpret consistently. However, accurate sleep stage\nclassification may aid in the early detection of abnormal brain maturation\nassociated with pregnancy complications (e.g. hypoxia or intrauterine growth\nrestriction).\n  Methods: EEG electrodes were secured onto the ovine dura over the parietal\ncortices of 24 late gestation fetal sheep. A lightweight deep neural network\noriginally developed for adult EEG sleep staging was trained on the ovine EEG\nusing transfer learning from adult EEG. A spectral equalisation-based domain\nadaptation strategy was used to reduce cross-domain mismatch.\n  Results: We demonstrated that while direct transfer performed poorly, full\nfine tuning combined with spectral equalisation achieved the best overall\nperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming\nbaseline models.\n  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep\nlearning framework specifically developed for automated sleep staging from the\nfetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier\nfunctions as a label engine, enabling large scale weak/semi supervised labeling\nand distillation to facilitate training on less invasive signals that can be\nacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.\nFetalSleepNet's lightweight design makes it well suited for deployment in low\npower, real time, and wearable fetal monitoring systems."}
{"id": "2509.09932", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09932", "abs": "https://arxiv.org/abs/2509.09932", "authors": ["Shilong Weng", "Liu Yang", "Ji Mao"], "title": "Effective Modeling of Critical Contextual Information for TDNN-based Speaker Verification", "comment": "5 pages, 3 figures", "summary": "Today, Time Delay Neural Network (TDNN) has become the mainstream\narchitecture for speaker verification task, in which the ECAPA-TDNN is one of\nthe state-of-the-art models. The current works that focus on improving TDNN\nprimarily address the limitations of TDNN in modeling global information and\nbridge the gap between TDNN and 2-Dimensional convolutions. However, the\nhierarchical convolutional structure in the SE-Res2Block proposed by ECAPA-TDNN\ncannot make full use of the contextual information, resulting in the weak\nability of ECAPA-TDNN to model effective context dependencies. To this end,\nthree improved architectures based on ECAPA-TDNN are proposed to fully and\neffectively extract multi-scale features with context dependence and then\naggregate these features. The experimental results on VoxCeleb and CN-Celeb\nverify the effectiveness of the three proposed architectures. One of these\narchitectures achieves nearly a 23% lower Equal Error Rate compared to that of\nECAPA-TDNN on VoxCeleb1-O dataset, demonstrating the competitive performance\nachievable among the current TDNN architectures under the comparable parameter\ncount."}
{"id": "2509.09748", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09748", "abs": "https://arxiv.org/abs/2509.09748", "authors": ["Yanru Huo", "Ziyue Jiang", "Zuoli Tang", "Qingyang Hong", "Zhou Zhao"], "title": "DiTReducio: A Training-Free Acceleration for DiT-Based TTS via Progressive Calibration", "comment": null, "summary": "While Diffusion Transformers (DiT) have advanced non-autoregressive (NAR)\nspeech synthesis, their high computational demands remain an limitation.\nExisting DiT-based text-to-speech (TTS) model acceleration approaches mainly\nfocus on reducing sampling steps through distillation techniques, yet they\nremain constrained by training costs. We introduce DiTReducio, a training-free\nacceleration framework that compresses computations in DiT-based TTS models via\nprogressive calibration. We propose two compression methods, Temporal Skipping\nand Branch Skipping, to eliminate redundant computations during inference.\nMoreover, based on two characteristic attention patterns identified within DiT\nlayers, we devise a pattern-guided strategy to selectively apply the\ncompression methods. Our method allows flexible modulation between generation\nquality and computational efficiency through adjustable compression thresholds.\nExperimental evaluations conducted on F5-TTS and MegaTTS 3 demonstrate that\nDiTReducio achieves a 75.4% reduction in FLOPs and improves the Real-Time\nFactor (RTF) by 37.1%, while preserving generation quality."}
{"id": "2509.10088", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10088", "abs": "https://arxiv.org/abs/2509.10088", "authors": ["Christian Eckrich", "Abdelhak M. Zoubir", "Vahid Jamali"], "title": "Resilient Vital Sign Monitoring Using RIS-Assisted Radar", "comment": null, "summary": "Vital sign monitoring plays a critical role in healthcare and well-being, as\nparameters such as respiration and heart rate offer valuable insights into an\nindividual's physiological state. While wearable devices allow for continuous\nmeasurement, their use in settings like in-home elderly care is often hindered\nby discomfort or user noncompliance. As a result, contactless solutions based\non radar sensing have garnered increasing attention. This is due to their\nunobtrusive design and preservation of privacy advantages compared to\ncamera-based systems. However, a single radar perspective can fail to capture\nbreathing-induced chest movements reliably, particularly when the subject's\norientation is unfavorable. To address this limitation, we integrate a\nreconfigurable intelligent surface (RIS) that provides an additional sensing\npath, thereby enhancing the robustness of respiratory monitoring. We present a\nnovel model for multi-path vital sign sensing that leverages both the direct\nradar path and an RIS-reflected path. We further discuss the potential benefits\nand improved performance our approach offers in continuous, privacy-preserving\nvital sign monitoring."}
{"id": "2509.09987", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09987", "abs": "https://arxiv.org/abs/2509.09987", "authors": ["Sung-Lin Yeh", "Yen Meng", "Hao Tang"], "title": "Whisper Has an Internal Word Aligner", "comment": "ASRU 2025", "summary": "There is an increasing interest in obtaining accurate word-level timestamps\nfrom strong automatic speech recognizers, in particular Whisper. Existing\napproaches either require additional training or are simply not competitive.\nThe evaluation in prior work is also relatively loose, typically using a\ntolerance of more than 200 ms. In this work, we discover attention heads in\nWhisper that capture accurate word alignments and are distinctively different\nfrom those that do not. Moreover, we find that using characters produces finer\nand more accurate alignments than using wordpieces. Based on these findings, we\npropose an unsupervised approach to extracting word alignments by filtering\nattention heads while teacher forcing Whisper with characters. Our approach not\nonly does not require training but also produces word alignments that are more\naccurate than prior work under a stricter tolerance between 20 ms and 100 ms."}
{"id": "2509.09752", "categories": ["cs.SD", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09752", "abs": "https://arxiv.org/abs/2509.09752", "authors": ["Abdullah All Tanvir", "Chenyu Huang", "Moe Alahmad", "Chuyang Yang", "Xin Zhong"], "title": "Combining Textual and Spectral Features for Robust Classification of Pilot Communications", "comment": null, "summary": "Accurate estimation of aircraft operations, such as takeoffs and landings, is\ncritical for effective airport management, yet remains challenging, especially\nat non-towered facilities lacking dedicated surveillance infrastructure. This\npaper presents a novel dual pipeline machine learning framework that classifies\npilot radio communications using both textual and spectral features. Audio data\ncollected from a non-towered U.S. airport was annotated by certified pilots\nwith operational intent labels and preprocessed through automatic speech\nrecognition and Mel-spectrogram extraction. We evaluate a wide range of\ntraditional classifiers and deep learning models, including ensemble methods,\nLSTM, and CNN across both pipelines. To our knowledge, this is the first system\nto classify operational aircraft intent using a dual-pipeline ML framework on\nreal-world air traffic audio. Our results demonstrate that spectral features\ncombined with deep architectures consistently yield superior classification\nperformance, with F1-scores exceeding 91%. Data augmentation further improves\nrobustness to real-world audio variability. The proposed approach is scalable,\ncost-effective, and deployable without additional infrastructure, offering a\npractical solution for air traffic monitoring at general aviation airports."}
{"id": "2509.10281", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10281", "abs": "https://arxiv.org/abs/2509.10281", "authors": ["Sudeepini Darapu", "Subrata Ghosh", "Dibakar Ghosh", "Chittaranjan Hens", "Santosh Nannuru"], "title": "Real-time identification and control of influential pandemic regions using graph signal variation", "comment": "12 pages, 13 figures", "summary": "The global spread of pandemics is facilitated by the mobility of populations,\ntransforming localized infections into widespread phenomena. To contain it,\ntimely identification of influential regions that accelerate this process is\nnecessary. In this work, we model infection as a temporally evolving graph\nsignal and propose graph signal variation-based metrics to capture\nspatio-temporal changes. Both graph domain and time domain locality are\nmodeled. Based on this metric, we propose an online algorithm to identify\ninfluential regions. Simulations demonstrate that the proposed method\neffectively identifies geographical regions with a higher capacity to spread\nthe infection. Isolating these regions leads to a significant reduction in\ncumulative infection. Simulations, along with analyses of hybrid H1N1 data and\nreal-world Indian COVID-19 data, underscore the utility of proposed metric in\nenhancing our understanding and control of infection spread"}
{"id": "2509.10031", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10031", "abs": "https://arxiv.org/abs/2509.10031", "authors": ["Peter Vieting", "Benedikt Hilmes", "Ralf Schl체ter", "Hermann Ney"], "title": "Unified Learnable 2D Convolutional Feature Extraction for ASR", "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Neural front-ends represent a promising approach to feature extraction for\nautomatic speech recognition (ASR) systems as they enable to learn specifically\ntailored features for different tasks. Yet, many of the existing techniques\nremain heavily influenced by classical methods. While this inductive bias may\nease the system design, our work aims to develop a more generic front-end for\nfeature extraction. Furthermore, we seek to unify the front-end architecture\ncontrasting with existing approaches that apply a composition of several layer\ntopologies originating from different sources. The experiments systematically\nshow how to reduce the influence of existing techniques to achieve a generic\nfront-end. The resulting 2D convolutional front-end is parameter-efficient and\nsuitable for a scenario with limited computational resources unlike large\nmodels pre-trained on unlabeled audio. The results demonstrate that this\ngeneric unified approach is not only feasible but also matches the performance\nof existing supervised learnable feature extractors."}
{"id": "2509.09823", "categories": ["cs.SD", "cs.AI", "cs.ET", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09823", "abs": "https://arxiv.org/abs/2509.09823", "authors": ["Yixuan Gao", "Tanvir Ahmed", "Shuang He", "Zhongqi Cheng", "Rajalakshmi Nandakumar"], "title": "SoilSound: Smartphone-based Soil Moisture Estimation", "comment": "12 pages, 8 figures", "summary": "Soil moisture monitoring is essential for agriculture and environmental\nmanagement, yet existing methods require either invasive probes disturbing the\nsoil or specialized equipment, limiting access to the public. We present\nSoilSound, an ubiquitous accessible smartphone-based acoustic sensing system\nthat can measure soil moisture without disturbing the soil. We leverage the\nbuilt-in speaker and microphone to perform a vertical scan mechanism to\naccurately measure moisture without any calibration. Unlike existing work that\nuse transmissive properties, we propose an alternate model for acoustic\nreflections in soil based on the surface roughness effect to enable moisture\nsensing without disturbing the soil. The system works by sending acoustic\nchirps towards the soil and recording the reflections during a vertical scan,\nwhich are then processed and fed to a convolutional neural network for\non-device soil moisture estimation with negligible computational, memory, or\npower overhead. We evaluated the system by training with curated soils in boxes\nin the lab and testing in the outdoor fields and show that SoilSound achieves a\nmean absolute error (MAE) of 2.39% across 10 different locations. Overall, the\nevaluation shows that SoilSound can accurately track soil moisture levels\nranging from 15.9% to 34.0% across multiple soil types, environments, and\nusers; without requiring any calibration or disturbing the soil, enabling\nwidespread moisture monitoring for home gardeners, urban farmers, citizen\nscientists, and agricultural communities in resource-limited settings."}
{"id": "2509.10296", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10296", "abs": "https://arxiv.org/abs/2509.10296", "authors": ["Cheng Luo", "Jie Hu", "Luping Xiang", "Kun Yang", "Zhiqin Wang"], "title": "Low-Complexity Null-Space-Based Simultaneous Wireless Information and Power Transfer Scheme", "comment": null, "summary": "Simultaneous wireless information and power transfer (SWIPT) has attracted\nsustained interest. We propose a null-space-based transmission scheme for\nmultiuser SWIPT serving both energy users (EUs) and information users (IUs).\nUnder a practical nonlinear energy-harvesting (EH) model and multiple waveform\noptions, we revisit the role of dedicated energy beams (EBs). We show that, in\ngeneral, dedicated EBs are unnecessary because information beams (IBs) with\nGaussian signaling can simultaneously support wireless energy transfer (WET)\nand wireless information transfer (WIT), unless special energy-centric\nwaveforms (e.g., deterministic sinusoidal waveforms) are employed and provide\nsufficient gains. Guided by these insights, we formulate an optimization\nproblem for EB design to enable dedicated waveform transmission for WET, and we\ndevelop a low-complexity algorithm that reduces computation by ignoring the WET\ncontribution of IBs during optimization. Numerical results corroborate that\ndeterministic sinusoidal waveforms outperform Gaussian signaling when the\nreceived RF power lies in the EH high-efficiency region, making dedicated EBs\nbeneficial. The proposed scheme achieves computational complexity reductions of\n91.43\\% and 98.54\\% for the cases $M=8,,K^I=K^E=2$ and $M=16,,K^I=K^E=4$,\nrespectively, with negligible performance loss, thereby validating the\nefficiency of the low-complexity algorithm."}
{"id": "2509.10086", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10086", "abs": "https://arxiv.org/abs/2509.10086", "authors": ["Xin Wang", "Wanying Ge", "Junichi Yamagishi"], "title": "Towards Data Drift Monitoring for Speech Deepfake Detection in the context of MLOps", "comment": "code to be pushed to https://github.com/nii-yamagishilab/AntiDeepfake", "summary": "When being delivered in applications or services on the cloud, static speech\ndeepfake detectors that are not updated will become vulnerable to newly created\nspeech deepfake attacks. From the perspective of machine learning operations\n(MLOps), this paper tries to answer whether we can monitor new and unseen\nspeech deepfake data that drifts away from a seen reference data set. We\nfurther ask, if drift is detected, whether we can fine-tune the detector using\nsimilarly drifted data, reduce the drift, and improve the detection\nperformance. On a toy dataset and the large-scale MLAAD dataset, we show that\nthe drift caused by new text-to-speech (TTS) attacks can be monitored using\ndistances between the distributions of the new data and reference data.\nFurthermore, we demonstrate that fine-tuning the detector using data generated\nby the new TTS deepfakes can reduce the drift and the detection error rates."}
{"id": "2509.09836", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09836", "abs": "https://arxiv.org/abs/2509.09836", "authors": ["Marco Pasini", "Stefan Lattner", "George Fazekas"], "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio", "comment": "Accepted to ISMIR 2025", "summary": "Efficiently representing audio signals in a compressed latent space is\ncritical for latent generative modelling. However, existing autoencoders often\nforce a choice between continuous embeddings and discrete tokens. Furthermore,\nachieving high compression ratios while maintaining audio fidelity remains a\nchallenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes\nthese limitations by both efficiently encoding global features via summary\nembeddings, and by producing both compressed continuous embeddings at ~ 11 Hz\nand discrete tokens at a rate of 2.38 kbps from the same trained model,\noffering unprecedented flexibility for different downstream generative tasks.\nThis is achieved through Finite Scalar Quantization (FSQ) and a novel\nFSQ-dropout technique, and does not require additional loss terms beyond the\nsingle consistency loss used for end-to-end training. CoDiCodec supports both\nautoregressive decoding and a novel parallel decoding strategy, with the latter\nachieving superior audio quality and faster decoding. CoDiCodec outperforms\nexisting continuous and discrete autoencoders at similar bitrates in terms of\nreconstruction audio quality. Our work enables a unified approach to audio\ncompression, bridging the gap between continuous and discrete generative\nmodelling paradigms."}
{"id": "2509.10357", "categories": ["eess.SP", "cs.NI", "94A05, 78M31", "C.2.1; I.6.5"], "pdf": "https://arxiv.org/pdf/2509.10357", "abs": "https://arxiv.org/abs/2509.10357", "authors": ["Simon Svendsen", "Dimitri Gold", "Christian Rom", "Volker Pauli", "Vuokko Nurmela"], "title": "Realistic UE Antennas for 6G in the 3GPP Channel Model", "comment": "This is a tutorial paper with the limit of 4500 words, 6\n  Fgiures/Tables and 15 refernces", "summary": "The transition to 6G has driven significant updates to the 3GPP channel\nmodel, particularly in modeling UE antennas and user-induced blockage for\nhandheld devices. The 3GPP Rel.19 revision of TR 38.901 introduces a more\nrealistic framework that captures directive antenna patterns, practical antenna\nplacements, polarization effects, and element-specific blockage. These updates\nare based on high-fidelity simulations and measurements of a reference\nsmartphone across multiple frequency ranges. By aligning link- and system-level\nsimulations with real-world device behavior, the new model enables more\naccurate evaluation of 6G technologies and supports consistent performance\nassessment across industry and research."}
{"id": "2509.10143", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10143", "abs": "https://arxiv.org/abs/2509.10143", "authors": ["Peter Vieting", "Simon Berger", "Thilo von Neumann", "Christoph Boeddeker", "Ralf Schl체ter", "Reinhold Haeb-Umbach"], "title": "Error Analysis in a Modular Meeting Transcription System", "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Meeting transcription is a field of high relevance and remarkable progress in\nrecent years. Still, challenges remain that limit its performance. In this\nwork, we extend a previously proposed framework for analyzing leakage in speech\nseparation with proper sensitivity to temporal locality. We show that there is\nsignificant leakage to the cross channel in areas where only the primary\nspeaker is active. At the same time, the results demonstrate that this does not\naffect the final performance much as these leaked parts are largely ignored by\nthe voice activity detection (VAD). Furthermore, different segmentations are\ncompared showing that advanced diarization approaches are able to reduce the\ngap to oracle segmentation by a third compared to a simple energy-based VAD. We\nadditionally reveal what factors contribute to the remaining difference. The\nresults represent state-of-the-art performance on LibriCSS among systems that\ntrain the recognition module on LibriSpeech data only."}
{"id": "2509.10074", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10074", "abs": "https://arxiv.org/abs/2509.10074", "authors": ["Christos Sgouropoulos", "Christos Nikou", "Stefanos Vlachos", "Vasileios Theiou", "Christos Foukanelis", "Theodoros Giannakopoulos"], "title": "Prototypical Contrastive Learning For Improved Few-Shot Audio Classification", "comment": "Accepted and Presented at IEEE International Workshop on Machine\n  Learning for Signal Processing, Aug.\\ 31-- Sep.\\ 3, 2025, Istanbul, Turkey ,\n  6 pages, 2 figures, 1 table", "summary": "Few-shot learning has emerged as a powerful paradigm for training models with\nlimited labeled data, addressing challenges in scenarios where large-scale\nannotation is impractical. While extensive research has been conducted in the\nimage domain, few-shot learning in audio classification remains relatively\nunderexplored. In this work, we investigate the effect of integrating\nsupervised contrastive loss into prototypical few shot training for audio\nclassification. In detail, we demonstrate that angular loss further improves\nthe performance compared to the standard contrastive loss. Our method leverages\nSpecAugment followed by a self-attention mechanism to encapsulate diverse\ninformation of augmented input versions into one unified embedding. We evaluate\nour approach on MetaAudio, a benchmark including five datasets with predefined\nsplits, standardized preprocessing, and a comprehensive set of few-shot\nlearning models for comparison. The proposed approach achieves state-of-the-art\nperformance in a 5-way, 5-shot setting."}
{"id": "2509.10433", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10433", "abs": "https://arxiv.org/abs/2509.10433", "authors": ["Junshi Chen", "Xuhong Li", "Russ Whiton", "Erik Leitinger", "Fredrik Tufvesson"], "title": "Robust Localization in Modern Cellular Networks using Global Map Features", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Radio frequency (RF) signal-based localization using modern cellular networks\nhas emerged as a promising solution to accurately locate objects in challenging\nenvironments. One of the most promising solutions for situations involving\nobstructed-line-of-sight (OLoS) and multipath propagation is multipathbased\nsimultaneous localization and mapping (MP-SLAM) that employs map features\n(MFs), such as virtual anchors. This paper presents an extended MP-SLAM method\nthat is augmented with a global map feature (GMF) repository. This repository\nstores consistent MFs of high quality that are collected during prior\ntraversals. We integrate these GMFs back into the MP-SLAM framework via a\nprobability hypothesis density (PHD) filter, which propagates GMF intensity\nfunctions over time. Extensive simulations, together with a challenging\nreal-world experiment using LTE RF signals in a dense urban scenario with\nsevere multipath propagation and inter-cell interference, demonstrate that our\nframework achieves robust and accurate localization, thereby showcasing its\neffectiveness in realistic modern cellular networks such as 5G or future 6G\nnetworks. It outperforms conventional proprioceptive sensor-based localization\nand conventional MP-SLAM methods, and achieves reliable localization even under\nadverse signal conditions."}
{"id": "2509.10202", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10202", "abs": "https://arxiv.org/abs/2509.10202", "authors": ["Alexander Popescu", "Rosie Frost", "Milos Cernak"], "title": "Low-latency Assistive Audio Enhancement for Neurodivergent People", "comment": null, "summary": "Neurodivergent people frequently experience decreased sound tolerance, with\nestimates suggesting it affects 50-70% of this population. This heightened\nsensitivity can provoke reactions ranging from mild discomfort to severe\ndistress, highlighting the critical need for assistive audio enhancement\ntechnologies In this paper, we propose several assistive audio enhancement\nalgorithms designed to selectively filter distressing sounds. To address this,\nwe curated a list of potential trigger sounds by analyzing\nneurodivergent-focused communities on platforms such as Reddit. Using this\nlist, a dataset of trigger sound samples was compiled from publicly available\nsources, including FSD50K and ESC50. These samples were then used to train and\nevaluate various Digital Signal Processing (DSP) and Machine Learning (ML)\naudio enhancement algorithms. Among the approaches explored, Dynamic Range\nCompression (DRC) proved the most effective, successfully attenuating trigger\nsounds and reducing auditory distress for neurodivergent listeners."}
{"id": "2509.10234", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10234", "abs": "https://arxiv.org/abs/2509.10234", "authors": ["Can Cui", "Paul Magron", "Mostafa Sadeghi", "Emmanuel Vincent"], "title": "Data-independent Beamforming for End-to-end Multichannel Multi-speaker ASR", "comment": "Published in the IEEE 26th International Workshop on Multimedia\n  Signal Processing (MMSP 2025)", "summary": "Automatic speech recognition (ASR) in multichannel, multi-speaker scenarios\nremains challenging due to ambient noise, reverberation and overlapping\nspeakers. In this paper, we propose a beamforming approach that processes\nspecific angular sectors based on their spherical polar coordinates before\napplying an end-to-end multichannel, multi-speaker ASR system. This method is\ndata-independent and training-free. We demonstrate that using a group of\nbeamformed signals improves ASR performance compared to using the same number\nof raw microphone signals. Moreover, increasing the number of signals used for\nbeamforming further enhances recognition accuracy, leading to a more efficient\nuse of multichannel signals while reducing the overall input load for the ASR\nsystem. We conduct experiments on the AMI meeting corpus, where the proposed\nmethod reduces word error rate by up to 11% and improves speaker counting\naccuracy by up to 27% relative compared to a multichannel ASR baseline system\nthat does not exploit beamforming."}
{"id": "2509.09823", "categories": ["cs.SD", "cs.AI", "cs.ET", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.09823", "abs": "https://arxiv.org/abs/2509.09823", "authors": ["Yixuan Gao", "Tanvir Ahmed", "Shuang He", "Zhongqi Cheng", "Rajalakshmi Nandakumar"], "title": "SoilSound: Smartphone-based Soil Moisture Estimation", "comment": "12 pages, 8 figures", "summary": "Soil moisture monitoring is essential for agriculture and environmental\nmanagement, yet existing methods require either invasive probes disturbing the\nsoil or specialized equipment, limiting access to the public. We present\nSoilSound, an ubiquitous accessible smartphone-based acoustic sensing system\nthat can measure soil moisture without disturbing the soil. We leverage the\nbuilt-in speaker and microphone to perform a vertical scan mechanism to\naccurately measure moisture without any calibration. Unlike existing work that\nuse transmissive properties, we propose an alternate model for acoustic\nreflections in soil based on the surface roughness effect to enable moisture\nsensing without disturbing the soil. The system works by sending acoustic\nchirps towards the soil and recording the reflections during a vertical scan,\nwhich are then processed and fed to a convolutional neural network for\non-device soil moisture estimation with negligible computational, memory, or\npower overhead. We evaluated the system by training with curated soils in boxes\nin the lab and testing in the outdoor fields and show that SoilSound achieves a\nmean absolute error (MAE) of 2.39% across 10 different locations. Overall, the\nevaluation shows that SoilSound can accurately track soil moisture levels\nranging from 15.9% to 34.0% across multiple soil types, environments, and\nusers; without requiring any calibration or disturbing the soil, enabling\nwidespread moisture monitoring for home gardeners, urban farmers, citizen\nscientists, and agricultural communities in resource-limited settings."}
{"id": "2509.09716", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09716", "abs": "https://arxiv.org/abs/2509.09716", "authors": ["Jun Zhan", "Mingyang Han", "Yuxuan Xie", "Chen Wang", "Dong Zhang", "Kexin Huang", "Haoxiang Shi", "DongXiao Wang", "Tengtao Song", "Qinyuan Cheng", "Shimin Li", "Jun Song", "Xipeng Qiu", "Bo Zheng"], "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions", "comment": null, "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\n\\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}."}
{"id": "2509.10391", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10391", "abs": "https://arxiv.org/abs/2509.10391", "authors": ["Shanmuka Sadhu", "Weiran Wang"], "title": "Improving Audio Event Recognition with Consistency Regularization", "comment": "Under Review", "summary": "Consistency regularization (CR), which enforces agreement between model\npredictions on augmented views, has found recent benefits in automatic speech\nrecognition [1]. In this paper, we propose the use of consistency\nregularization for audio event recognition, and demonstrate its effectiveness\non AudioSet. With extensive ablation studies for both small ($\\sim$20k) and\nlarge ($\\sim$1.8M) supervised training sets, we show that CR brings consistent\nimprovement over supervised baselines which already heavily utilize data\naugmentation, and CR using stronger augmentation and multiple augmentations\nleads to additional gain for the small training set. Furthermore, we extend the\nuse of CR into the semi-supervised setup with 20K labeled samples and 1.8M\nunlabeled samples, and obtain performance improvement over our best model\ntrained on the small set."}
{"id": "2509.09717", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09717", "abs": "https://arxiv.org/abs/2509.09717", "authors": ["Jorge E. Le처n", "Miguel Carrasco"], "title": "Testing chatbots on the creation of encoders for audio conditioned image generation", "comment": null, "summary": "On one hand, recent advances in chatbots has led to a rising popularity in\nusing these models for coding tasks. On the other hand, modern generative image\nmodels primarily rely on text encoders to translate semantic concepts into\nvisual representations, even when there is clear evidence that audio can be\nemployed as input as well. Given the previous, in this work, we explore whether\nstate-of-the-art conversational agents can design effective audio encoders to\nreplace the CLIP text encoder from Stable Diffusion 1.5, enabling image\nsynthesis directly from sound. We prompted five publicly available chatbots to\npropose neural architectures to work as these audio encoders, with a set of\nwell-explained shared conditions. Each valid suggested encoder was trained on\nover two million context related audio-image-text observations, and evaluated\non held-out validation and test sets using various metrics, together with a\nqualitative analysis of their generated images. Although almost all chatbots\ngenerated valid model designs, none achieved satisfactory results, indicating\nthat their audio embeddings failed to align reliably with those of the original\ntext encoder. Among the proposals, the Gemini audio encoder showed the best\nquantitative metrics, while the Grok audio encoder produced more coherent\nimages (particularly, when paired with the text encoder). Our findings reveal a\nshared architectural bias across chatbots and underscore the remaining coding\ngap that needs to be bridged in future versions of these models. We also\ncreated a public demo so everyone could study and try out these audio encoders.\nFinally, we propose research questions that should be tackled in the future,\nand encourage other researchers to perform more focused and highly specialized\ntasks like this one, so the respective chatbots cannot make use of well-known\nsolutions and their creativity/reasoning is fully tested."}
{"id": "2509.09719", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09719", "abs": "https://arxiv.org/abs/2509.09719", "authors": ["Hemanth Chandravamsi", "Dhanush V. Shenoy", "Itay Zinn", "Shimon Pisnoy", "Steven H. Frankel"], "title": "Spectral Bottleneck in Deep Neural Networks: Noise is All You Need", "comment": null, "summary": "Deep neural networks are known to exhibit a spectral learning bias, wherein\nlow-frequency components are learned early in training, while high-frequency\nmodes emerge more gradually in later epochs. However, when the target signal\nlacks low-frequency components and is dominated by broadband high frequencies,\ntraining suffers from a 'spectral bottleneck', and the model fails to\nreconstruct the entire signal, including the frequency components that lie\nwithin the network's representational capacity. We examine such a scenario in\nthe context of implicit neural representations (INRs) with sinusoidal\nrepresentation networks (SIRENs), focusing on the challenge of fitting\nhigh-frequency-dominant signals that are susceptible to spectral bottleneck. To\neffectively fit any target signal irrespective of it's frequency content, we\npropose a generalized target-aware 'weight perturbation scheme' (WINNER -\nweight initialization with noise for neural representations) for network\ninitialization. The scheme perturbs uniformly initialized weights with Gaussian\nnoise, where the noise scales are adaptively determined by the spectral\ncentroid of the target signal. We show that the noise scales can provide\ncontrol over the spectra of network activations and the eigenbasis of the\nempirical neural tangent kernel. This method not only addresses the spectral\nbottleneck but also yields faster convergence and with improved representation\naccuracy, outperforming state-of-the-art approaches in audio fitting and\nachieving notable gains in image fitting and denoising tasks. Beyond signal\nreconstruction, our approach opens new directions for adaptive weight\ninitialization strategies in computer vision and scientific machine learning."}
{"id": "2509.09746", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09746", "abs": "https://arxiv.org/abs/2509.09746", "authors": ["Ning Ma", "Bahman Mirheidari", "Guy J. Brown", "Minyoi M. Maimbolwa", "Nsala Sanjase", "Solomon Chifwamba", "Seke Muzazu", "Monde Muyoyeta", "Mary Kagujje"], "title": "AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models", "comment": "submitted to The Lancet Digital Health", "summary": "Background\n  Artificial intelligence (AI) can detect disease-related acoustic patterns in\ncough sounds, offering a scalable approach to tuberculosis (TB) screening in\nhigh-burden, low-resource settings. Previous studies have been limited by small\ndatasets, under-representation of symptomatic non-TB patients, reliance on\nsimple models, and recordings collected under idealised conditions.\n  Methods\n  We enrolled 512 participants at two hospitals in Zambia, grouped as\nbacteriologically confirmed TB (TB+), symptomatic patients with other\nrespiratory diseases (OR), and healthy controls (HC). Usable cough recordings\nplus demographic and clinical data were obtained from 500 participants. Deep\nlearning classifiers based on speech foundation models were trained on cough\nrecordings. The best-performing model, trained on 3-second segments, was\nfurther evaluated with demographic and clinical features.\n  Findings\n  The best audio-only classifier achieved an AUROC of 85.2% for distinguishing\nTB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographic\nand clinical features improved performance to 92.1% (TB+/Rest) and 84.2%\n(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%\nsensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.\n  Interpretation\n  Cough analysis using speech foundation models, especially when combined with\ndemographic and clinical data, showed strong potential as a TB triage tool,\nmeeting WHO target product profile benchmarks. The model was robust to\nconfounding factors including background noise, recording time, and device\nvariability, indicating detection of genuine disease-related acoustic patterns.\nFurther validation across diverse regions and case definitions, including\nsubclinical TB, is required before clinical use."}
{"id": "2509.09791", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09791", "abs": "https://arxiv.org/abs/2509.09791", "authors": ["Carlos Busso", "Reza Lotfian", "Kusha Sridhar", "Ali N. Salman", "Wei-Cheng Lin", "Lucas Goncalves", "Srinivas Parthasarathy", "Abinay Reddy Naini", "Seong-Gyun Leem", "Luz Martinez-Lucas", "Huang-Cheng Chou", "Pravin Mote"], "title": "The MSP-Podcast Corpus", "comment": "IEEE Transactions on Affective Computing submission", "summary": "The availability of large, high-quality emotional speech databases is\nessential for advancing speech emotion recognition (SER) in real-world\nscenarios. However, many existing databases face limitations in size, emotional\nbalance, and speaker diversity. This study describes the MSP-Podcast corpus,\nsummarizing our ten-year effort. The corpus consists of over 400 hours of\ndiverse audio samples from various audio-sharing websites, all of which have\nCommon Licenses that permit the distribution of the corpus. We annotate the\ncorpus with rich emotional labels, including primary (single dominant emotion)\nand secondary (multiple emotions perceived in the audio) emotional categories,\nas well as emotional attributes for valence, arousal, and dominance. At least\nfive raters annotate these emotional labels. The corpus also has speaker\nidentification for most samples, and human transcriptions of the lexical\ncontent of the sentences for the entire corpus. The data collection protocol\nincludes a machine learning-driven pipeline for selecting emotionally diverse\nrecordings, ensuring a balanced and varied representation of emotions across\nspeakers and environments. The resulting database provides a comprehensive,\nhigh-quality resource, better suited for advancing SER systems in practical,\nreal-world scenarios."}
{"id": "2509.09748", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09748", "abs": "https://arxiv.org/abs/2509.09748", "authors": ["Yanru Huo", "Ziyue Jiang", "Zuoli Tang", "Qingyang Hong", "Zhou Zhao"], "title": "DiTReducio: A Training-Free Acceleration for DiT-Based TTS via Progressive Calibration", "comment": null, "summary": "While Diffusion Transformers (DiT) have advanced non-autoregressive (NAR)\nspeech synthesis, their high computational demands remain an limitation.\nExisting DiT-based text-to-speech (TTS) model acceleration approaches mainly\nfocus on reducing sampling steps through distillation techniques, yet they\nremain constrained by training costs. We introduce DiTReducio, a training-free\nacceleration framework that compresses computations in DiT-based TTS models via\nprogressive calibration. We propose two compression methods, Temporal Skipping\nand Branch Skipping, to eliminate redundant computations during inference.\nMoreover, based on two characteristic attention patterns identified within DiT\nlayers, we devise a pattern-guided strategy to selectively apply the\ncompression methods. Our method allows flexible modulation between generation\nquality and computational efficiency through adjustable compression thresholds.\nExperimental evaluations conducted on F5-TTS and MegaTTS 3 demonstrate that\nDiTReducio achieves a 75.4% reduction in FLOPs and improves the Real-Time\nFactor (RTF) by 37.1%, while preserving generation quality."}
{"id": "2509.10031", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10031", "abs": "https://arxiv.org/abs/2509.10031", "authors": ["Peter Vieting", "Benedikt Hilmes", "Ralf Schl체ter", "Hermann Ney"], "title": "Unified Learnable 2D Convolutional Feature Extraction for ASR", "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Neural front-ends represent a promising approach to feature extraction for\nautomatic speech recognition (ASR) systems as they enable to learn specifically\ntailored features for different tasks. Yet, many of the existing techniques\nremain heavily influenced by classical methods. While this inductive bias may\nease the system design, our work aims to develop a more generic front-end for\nfeature extraction. Furthermore, we seek to unify the front-end architecture\ncontrasting with existing approaches that apply a composition of several layer\ntopologies originating from different sources. The experiments systematically\nshow how to reduce the influence of existing techniques to achieve a generic\nfront-end. The resulting 2D convolutional front-end is parameter-efficient and\nsuitable for a scenario with limited computational resources unlike large\nmodels pre-trained on unlabeled audio. The results demonstrate that this\ngeneric unified approach is not only feasible but also matches the performance\nof existing supervised learnable feature extractors."}
{"id": "2509.09752", "categories": ["cs.SD", "cs.CY", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09752", "abs": "https://arxiv.org/abs/2509.09752", "authors": ["Abdullah All Tanvir", "Chenyu Huang", "Moe Alahmad", "Chuyang Yang", "Xin Zhong"], "title": "Combining Textual and Spectral Features for Robust Classification of Pilot Communications", "comment": null, "summary": "Accurate estimation of aircraft operations, such as takeoffs and landings, is\ncritical for effective airport management, yet remains challenging, especially\nat non-towered facilities lacking dedicated surveillance infrastructure. This\npaper presents a novel dual pipeline machine learning framework that classifies\npilot radio communications using both textual and spectral features. Audio data\ncollected from a non-towered U.S. airport was annotated by certified pilots\nwith operational intent labels and preprocessed through automatic speech\nrecognition and Mel-spectrogram extraction. We evaluate a wide range of\ntraditional classifiers and deep learning models, including ensemble methods,\nLSTM, and CNN across both pipelines. To our knowledge, this is the first system\nto classify operational aircraft intent using a dual-pipeline ML framework on\nreal-world air traffic audio. Our results demonstrate that spectral features\ncombined with deep architectures consistently yield superior classification\nperformance, with F1-scores exceeding 91%. Data augmentation further improves\nrobustness to real-world audio variability. The proposed approach is scalable,\ncost-effective, and deployable without additional infrastructure, offering a\npractical solution for air traffic monitoring at general aviation airports."}
{"id": "2509.10143", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10143", "abs": "https://arxiv.org/abs/2509.10143", "authors": ["Peter Vieting", "Simon Berger", "Thilo von Neumann", "Christoph Boeddeker", "Ralf Schl체ter", "Reinhold Haeb-Umbach"], "title": "Error Analysis in a Modular Meeting Transcription System", "comment": "Accepted at ITG Conference on Speech Communication 2025", "summary": "Meeting transcription is a field of high relevance and remarkable progress in\nrecent years. Still, challenges remain that limit its performance. In this\nwork, we extend a previously proposed framework for analyzing leakage in speech\nseparation with proper sensitivity to temporal locality. We show that there is\nsignificant leakage to the cross channel in areas where only the primary\nspeaker is active. At the same time, the results demonstrate that this does not\naffect the final performance much as these leaked parts are largely ignored by\nthe voice activity detection (VAD). Furthermore, different segmentations are\ncompared showing that advanced diarization approaches are able to reduce the\ngap to oracle segmentation by a third compared to a simple energy-based VAD. We\nadditionally reveal what factors contribute to the remaining difference. The\nresults represent state-of-the-art performance on LibriCSS among systems that\ntrain the recognition module on LibriSpeech data only."}
{"id": "2509.09836", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.09836", "abs": "https://arxiv.org/abs/2509.09836", "authors": ["Marco Pasini", "Stefan Lattner", "George Fazekas"], "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio", "comment": "Accepted to ISMIR 2025", "summary": "Efficiently representing audio signals in a compressed latent space is\ncritical for latent generative modelling. However, existing autoencoders often\nforce a choice between continuous embeddings and discrete tokens. Furthermore,\nachieving high compression ratios while maintaining audio fidelity remains a\nchallenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes\nthese limitations by both efficiently encoding global features via summary\nembeddings, and by producing both compressed continuous embeddings at ~ 11 Hz\nand discrete tokens at a rate of 2.38 kbps from the same trained model,\noffering unprecedented flexibility for different downstream generative tasks.\nThis is achieved through Finite Scalar Quantization (FSQ) and a novel\nFSQ-dropout technique, and does not require additional loss terms beyond the\nsingle consistency loss used for end-to-end training. CoDiCodec supports both\nautoregressive decoding and a novel parallel decoding strategy, with the latter\nachieving superior audio quality and faster decoding. CoDiCodec outperforms\nexisting continuous and discrete autoencoders at similar bitrates in terms of\nreconstruction audio quality. Our work enables a unified approach to audio\ncompression, bridging the gap between continuous and discrete generative\nmodelling paradigms."}

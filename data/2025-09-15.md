<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy](https://arxiv.org/abs/2509.09695)
*Fabio Magarelli,Geraldine B. Boylan,Saeed Montazeri,Feargal O'Sullivan,Dominic Lightbody,Minoo Ashoori,Tamara Skoric Ceranic,John M. O'Toole*

Main category: eess.SP

TL;DR: 通过机器学习竞赛形式，使用新生儿EEG数据开发分类脑电背景模式严重程度的模型，深度学习模型在验证集上表现更好但所有模型都遇到性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 解决新生儿脑功能监测领域缺乏高质量注释数据的问题，通过ML竞赛促进协作学习和数据共享。

Method: 编译了102名新生儿的353小时EEG数据，分为训练、测试和验证集，并为EEG背景模式评定严重程度。创建网页竞赛平台开发分类模型。

Result: 虽然特征基模型在测试集上排名第一，但深度学习模型在验证集上普遍性更好。所有模型在验证集上都出现显著性能下降。

Conclusion: 突出了模型在未见数据上的普遍化挑战，强调了使用截止验证集的重要性。大型多样化数据集对于建立稳健模型至关重要，开放访问数据和协作ML开发有助于加快临床决策支持工具的发展。

Abstract: Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.

</details>


### [2] [Locally Permuted Low Rank Column-wise Sensing](https://arxiv.org/abs/2509.09820)
*Ahmed Ali Abbasi,Namrata Vaswani*

Main category: eess.SP

TL;DR: 本文解决了带有排列扰动的低秩列感知问题，提出了PermutedAltGDmin和Permuted-AltMin两种算法，实验表明前者收敛速度更快


<details>
  <summary>Details</summary>
Motivation: 解决观测数据被排列/打乱/无标签情况下的低秩列感知问题，这是无标签感知和低秩矩阵感知两个研究领域的交叉问题

Method: 提出了PermutedAltGDmin算法（交替梯度下降和最小化的推广）和Permuted-AltMin算法（交替最小化）

Result: 通过仿真实验证明两种算法都能收敛，但PermutedAltGDmin比Permuted-AltMin收敛速度快得多

Conclusion: 成功解决了排列扰动的低秩列感知问题，提出的PermutedAltGDmin算法在收敛速度方面表现优异

Abstract: We precisely formulate, and provide a solution for, the Low Rank Columnwise
Sensing (LRCS) problem when some of the observed data is
scrambled/permuted/unlabeled. This problem, which we refer to as permuted LRCS,
lies at the intersection of two distinct topics of recent research: unlabeled
sensing and low rank column-wise (matrix) sensing. We introduce a novel
generalization of the recently developed Alternating Gradient Descent and
Minimization (AltGDMin) algorithm to solve this problem. We also develop an
alternating minimization (AltMin) solution. We show, using simulation
experiments, that both converge but PermutedAltGDmin is much faster than
Permuted-AltMin.

</details>


### [3] [Real-Time Remote Tracking with State-Dependent Detection Probability: A POMDP Framework](https://arxiv.org/abs/2509.09837)
*Jiapei Tian,Abolfazl Zakeri,Marian Codreanu,David Gundlegård*

Main category: eess.SP

TL;DR: 提出了一种针对异构传感器系统的实时跟踪优化调度策略，通过POMDP建模和信念状态离散化来解决传感器故障和信道错误问题


<details>
  <summary>Details</summary>
Motivation: 解决异构传感器在状态相关检测精度和信道错误条件下的最优调度问题，以最小化失真和传输成本

Method: 将问题建模为部分可观测马尔可夫决策过程(POMDP)，转化为信念MDP，通过离散化信念空间并使用相对值迭代算法求解

Result: 仿真结果表明所提策略显著优于基准策略，证明了考虑状态相关传感可靠性的重要性

Conclusion: 该方法有效解决了传感器调度中的不确定性问题，为实际应用提供了实用的优化方案

Abstract: We consider a real-time tracking system where a binary Markov source is
monitored by two heterogeneous sensors. Upon command, sensors send their
observations to a remote sink over error-prone channels. We assume each sensor
exhibits state-dependent detection accuracy and may occasionally fail to detect
the source state. At most one sensor is scheduled for sampling at each time
slot. We assess the effectiveness of data communication using a generic
distortion function that captures the end application's objective. We derive
optimal sink-side command policies to minimize the weighted sum of distortion
and transmission costs. To model the uncertainty introduced by sensing failures
(of the sensors) and packet loss, we formulate the problem as a partially
observable Markov decision process (POMDP), which we then cast into a
belief-MDP. Since the belief evolves continuously, the belief space is
discretized into a finite grid and the belief value is quantized to the nearest
grid point after each update. This formulation leads to a finite-state MDP
problem, which is solved using the relative value iteration algorithm (RVIA).
Simulation results demonstrate that the proposed policy significantly
outperforms benchmark strategies and highlights the importance of accounting
for state-dependent sensing reliability in sensor scheduling.

</details>


### [4] [Field evaluation of a wearable instrumented headband designed for measuring head kinematics](https://arxiv.org/abs/2509.09842)
*Anu Tripathi,Yang Wan,Zhiren Zhu,Furkan Camci,Sheila Turcsanyi,Jeneel Pravin Kachhadiya,Mauricio Araiza Canizales,Alison Brooks,Haneesh Kesari,Joseph Andrews,Traci Snedden,Peter Ferrazzano,Christian Franck,Rika Wright Carlsen*

Main category: eess.SP

TL;DR: 研究评估了一种带仪器的头带在玩家进行足球顶球时测量头部动力学的性能，与口腐仪器进行对比验证


<details>
  <summary>Details</summary>
Motivation: 研究足球顶球与轻度脑继续性伤害的关系，需要在玩家进行顶球时准确测量头部动力学参数

Method: 在玩家进行典型顶球场景下（边线球、门球、角球），使用仪器头带测量头部动力学，与标准的仪器口腐进行对比验证

Result: 头带与口腐在时间历史数据上展现从"一般"到"优秀"的一致性，角速度和平移加速度一致性最高，角加速度最低；峰值动力学参数存在40.9%、16.6%和-14.1%的系统偏差

Conclusion: 仪器头带在某些动力学测量和冲击条件下与口腐有合理的一致性，未来需要改进以提高在所有动力学参数上的性能

Abstract: Purpose: To study the relationship between soccer heading and the risk of
mild traumatic brain injury (mTBI), we previously developed an instrumented
headband and data processing scheme to measure the angular head kinematics of
soccer headers. Laboratory evaluation of the headband on an anthropomorphic
test device showed good agreement with a reference sensor for soccer ball
impacts to the front of the head. In this study, we evaluate the headband in
measuring the full head kinematics of soccer headers in the field. Methods: The
headband was evaluated under typical soccer heading scenarios (throw-ins,
goal-kicks, and corner-kicks) on a human subject. The measured time history and
peak kinematics from the headband were compared with those from an instrumented
mouthpiece, which is a widely accepted method for measuring head kinematics in
the field. Results: The time history agreement (CORA scores) between the
headband and the mouthpiece ranged from 'fair' to 'excellent', with the highest
agreement for angular velocities (0.79 \pm 0.08) and translational
accelerations (0.73 \pm 0.05) and lowest for angular accelerations (0.67 \pm
0.06). A Bland-Altman analysis of the peak kinematics from the headband and
mouthpiece found the mean bias to be 40.9% (of the maximum mouthpiece reading)
for the angular velocity, 16.6% for the translational acceleration, and-14.1%
for the angular acceleration. Conclusion: The field evaluation of the
instrumented headband showed reasonable agreement with the mouthpiece for some
kinematic measures and impact conditions. Future work should focus on improving
the headband performance across all kinematic measures.

</details>


### [5] [A General Nonlinear Model for Arbitrary Modulation Formats in the Presence of Inter-Channel Simulated Raman Scattering](https://arxiv.org/abs/2509.10009)
*Zhiwei Liang,Bin Chen,Jiwei Xu,Yi Lei,Qingqing Hu,Fan Zhang,Gabriele Liga*

Main category: eess.SP

TL;DR: 四维非线性模型扩展包含跨信道受激拉曼散射，能够准确预测高色散区域的双偏振四维调制格式和概率整形星座


<details>
  <summary>Details</summary>
Motivation: 现有模型在高色散区域对双偏振四维调制格式和概率整形星座的预测精度不足，需要包含跨信道受激拉曼散射效应

Method: 扩展四维非线性模型以包含跨信道受激拉曼散射，并与分步傅里叶方法和增强高斯噪声模型进行比较验证

Result: 提出的扩展模型能够准确预测高色散区域的双偏振四维调制格式和概率整形星座性能

Conclusion: 包含跨信道受激拉曼散射的四维非线性模型为高色散区域的光通信系统设计提供了更准确的预测工具

Abstract: The four-dimensional nonlinear model is extended to include the inter-channel
stimulated Raman scattering, enabling accurate prediction of dual-polarization
four-dimensional modulation formats and probabilistically shaped constellations
in high-dispersion regimes. The proposed model is validated via comparisons
with the split-step Fourier method and enhanced Gaussian noise model.

</details>


### [6] [Uplink RSMA for Pinching-Antenna Systems](https://arxiv.org/abs/2509.10076)
*Apostolos A. Tegos,Yue Xiao,Sotiris A. Tegos,George K. Karagiannidis,Panagiotis D. Diamantoulakis*

Main category: eess.SP

TL;DR: 本文研究了一种采用速率分割多址(RSMA)的双用户双夹取天线系统(PAS)，推导了中断概率的闭式表达式，证明RSMA-PAS性能优于NOMA-PAS。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要适应变化条件并满足新兴应用对可靠、高容量通信的需求。克服传统技术限制（如固定天线位置）至关重要，新提出的夹取天线系统(PAS)通过激活波导上的天线来减少收发距离，是室内应用的有前景解决方案。

Method: 研究双用户双夹取天线上行链路PAS系统，采用速率分割多址(RSMA)技术构建比非正交多址(NOMA)更具弹性的框架，推导了中断概率的闭式表达式。

Result: 数值结果验证了推导的闭式表达式，证明提出的RSMA方案在PAS系统中性能优于NOMA方案。

Conclusion: RSMA-PAS系统通过速率分割技术有效提升了系统性能，为室内无线通信提供了更优的解决方案。

Abstract: One of the key goals of next-generation wireless networks is to adapt to
changing conditions and meet the growing demand for reliable, high-capacity
communications from emerging applications. Overcoming the limitations of
conventional technologies, such as fixed antenna positions, is essential to
achieving this objective because it mitigates the impact of path loss on the
received signal and creates strong line-of-sight links, enhancing system
performance. With this in mind, the newly proposed pinching antenna systems
(PASs) are a promising solution for indoor applications because they can
activate antennas across a waveguide deployed in a room, thus reducing the
distance between the transmitter and receiver. In this paper, we investigate a
two-user, two-pinching-antenna uplink PAS, in which the transmitters use rate
splitting to create a more resilient framework than non-orthogonal multiple
access (NOMA). For this network, we derive novel closed-form expressions for
the outage probability. Numerical results validate these expressions, proving
that the proposed rate-splitting multiple access (RSMA) scheme outperforms NOMA
PAS.

</details>


### [7] [FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification](https://arxiv.org/abs/2509.10082)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Nhi Tran,Sharmony B. Kelly,Gari D. Clifford,Robert Galinsky,Faezeh Marzbanrad*

Main category: eess.SP

TL;DR: FetalSleepNet是首个用于胎儿羊脑电图睡眠分期分类的深度学习框架，通过迁移学习和频谱均衡技术，在胎儿EEG数据上实现了86.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑电图获取复杂且人工解释困难，准确的睡眠分期分类有助于早期检测与妊娠并发症相关的异常脑发育。

Method: 使用轻量级深度神经网络，通过从成人EEG的迁移学习训练胎儿羊EEG数据，采用频谱均衡域适应策略减少跨域不匹配。

Result: 完全微调结合频谱均衡获得最佳性能（准确率86.6%，宏观F1分数62.5），优于基线模型。

Conclusion: FetalSleepNet是首个专门为胎儿EEG自动睡眠分期开发的深度学习框架，适合部署在低功耗实时可穿戴胎儿监测系统中，可作为标签引擎支持大规模弱/半监督学习。

Abstract: Introduction: This study presents FetalSleepNet, the first published deep
learning approach to classifying sleep states from the ovine
electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and
laborious to interpret consistently. However, accurate sleep stage
classification may aid in the early detection of abnormal brain maturation
associated with pregnancy complications (e.g. hypoxia or intrauterine growth
restriction).
  Methods: EEG electrodes were secured onto the ovine dura over the parietal
cortices of 24 late gestation fetal sheep. A lightweight deep neural network
originally developed for adult EEG sleep staging was trained on the ovine EEG
using transfer learning from adult EEG. A spectral equalisation-based domain
adaptation strategy was used to reduce cross-domain mismatch.
  Results: We demonstrated that while direct transfer performed poorly, full
fine tuning combined with spectral equalisation achieved the best overall
performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming
baseline models.
  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep
learning framework specifically developed for automated sleep staging from the
fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier
functions as a label engine, enabling large scale weak/semi supervised labeling
and distillation to facilitate training on less invasive signals that can be
acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.
FetalSleepNet's lightweight design makes it well suited for deployment in low
power, real time, and wearable fetal monitoring systems.

</details>


### [8] [Resilient Vital Sign Monitoring Using RIS-Assisted Radar](https://arxiv.org/abs/2509.10088)
*Christian Eckrich,Abdelhak M. Zoubir,Vahid Jamali*

Main category: eess.SP

TL;DR: 通过集成可重构智能表面(RIS)提供额外感知路径，改善单一雷达视角下的呼吸监测稳健性


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴设备的不适感和用户不配合问题，提供无接触、隐私保护的生命体征监测方案

Method: 建立多路径生命体征感知模型，结合直接雷达路径和RIS反射路径

Result: 提高了呼吸监测的稳健性，尤其在主体方位不利情况下

Conclusion: 集成RIS的多路径感知方案能够实现更可靠、连续且隐私保护的生命体征监测

Abstract: Vital sign monitoring plays a critical role in healthcare and well-being, as
parameters such as respiration and heart rate offer valuable insights into an
individual's physiological state. While wearable devices allow for continuous
measurement, their use in settings like in-home elderly care is often hindered
by discomfort or user noncompliance. As a result, contactless solutions based
on radar sensing have garnered increasing attention. This is due to their
unobtrusive design and preservation of privacy advantages compared to
camera-based systems. However, a single radar perspective can fail to capture
breathing-induced chest movements reliably, particularly when the subject's
orientation is unfavorable. To address this limitation, we integrate a
reconfigurable intelligent surface (RIS) that provides an additional sensing
path, thereby enhancing the robustness of respiratory monitoring. We present a
novel model for multi-path vital sign sensing that leverages both the direct
radar path and an RIS-reflected path. We further discuss the potential benefits
and improved performance our approach offers in continuous, privacy-preserving
vital sign monitoring.

</details>


### [9] [Real-time identification and control of influential pandemic regions using graph signal variation](https://arxiv.org/abs/2509.10281)
*Sudeepini Darapu,Subrata Ghosh,Dibakar Ghosh,Chittaranjan Hens,Santosh Nannuru*

Main category: eess.SP

TL;DR: 基于图信号变异的空间时间模型，提出在线算法识别传染传播关键区域，通过隔离这些区域显著降低总体感染数量


<details>
  <summary>Details</summary>
Motivation: 全球大流行病的传播受人口流动推动，及时识别关键传播区域对控制病毒传播至关重要

Method: 将感染模型化为时间演化的图信号，提出图信号变异指标来捕捉空间时间变化，并建立在线算法

Result: 模拟实验表明方法能有效识别传播能力强的地理区域，隔离这些区域可显著降低总体感染数量，H1N1和印度COVID-19数据分析验证了方法的实用性

Conclusion: 该研究提供的图信号变异指标和算法能够提升我们对传染病传播机制的理解和控制能力，为大流行病防控提供有效工具

Abstract: The global spread of pandemics is facilitated by the mobility of populations,
transforming localized infections into widespread phenomena. To contain it,
timely identification of influential regions that accelerate this process is
necessary. In this work, we model infection as a temporally evolving graph
signal and propose graph signal variation-based metrics to capture
spatio-temporal changes. Both graph domain and time domain locality are
modeled. Based on this metric, we propose an online algorithm to identify
influential regions. Simulations demonstrate that the proposed method
effectively identifies geographical regions with a higher capacity to spread
the infection. Isolating these regions leads to a significant reduction in
cumulative infection. Simulations, along with analyses of hybrid H1N1 data and
real-world Indian COVID-19 data, underscore the utility of proposed metric in
enhancing our understanding and control of infection spread

</details>


### [10] [Low-Complexity Null-Space-Based Simultaneous Wireless Information and Power Transfer Scheme](https://arxiv.org/abs/2509.10296)
*Cheng Luo,Jie Hu,Luping Xiang,Kun Yang,Zhiqin Wang*

Main category: eess.SP

TL;DR: 提出了一种基于零空间的多用户SWIPT传输方案，在非线性能量收集模型下重新评估了专用能量波束的作用，发现高斯信号的信息波束通常足以同时支持能量和信息传输，除非使用特殊的能量中心波形。


<details>
  <summary>Details</summary>
Motivation: 同时无线信息和能量传输(SWIPT)持续受到关注，但传统方法中专用能量波束的必要性需要重新评估，特别是在实际非线性能量收集模型和多波形选择下。

Method: 提出基于零空间的多用户SWIPT传输方案，考虑非线性能量收集模型，分析不同波形选项，制定能量波束优化问题，并开发低复杂度算法。

Result: 数值结果表明确定性正弦波形在高效率能量收集区域优于高斯信号，使专用能量波束有益。提出的低复杂度算法计算复杂度降低91.43%-98.54%，性能损失可忽略。

Conclusion: 专用能量波束通常不必要，除非使用特殊的能量中心波形。提出的低复杂度算法能有效减少计算负担，为多用户SWIPT系统提供了实用的解决方案。

Abstract: Simultaneous wireless information and power transfer (SWIPT) has attracted
sustained interest. We propose a null-space-based transmission scheme for
multiuser SWIPT serving both energy users (EUs) and information users (IUs).
Under a practical nonlinear energy-harvesting (EH) model and multiple waveform
options, we revisit the role of dedicated energy beams (EBs). We show that, in
general, dedicated EBs are unnecessary because information beams (IBs) with
Gaussian signaling can simultaneously support wireless energy transfer (WET)
and wireless information transfer (WIT), unless special energy-centric
waveforms (e.g., deterministic sinusoidal waveforms) are employed and provide
sufficient gains. Guided by these insights, we formulate an optimization
problem for EB design to enable dedicated waveform transmission for WET, and we
develop a low-complexity algorithm that reduces computation by ignoring the WET
contribution of IBs during optimization. Numerical results corroborate that
deterministic sinusoidal waveforms outperform Gaussian signaling when the
received RF power lies in the EH high-efficiency region, making dedicated EBs
beneficial. The proposed scheme achieves computational complexity reductions of
91.43\% and 98.54\% for the cases $M=8,,K^I=K^E=2$ and $M=16,,K^I=K^E=4$,
respectively, with negligible performance loss, thereby validating the
efficiency of the low-complexity algorithm.

</details>


### [11] [Realistic UE Antennas for 6G in the 3GPP Channel Model](https://arxiv.org/abs/2509.10357)
*Simon Svendsen,Dimitri Gold,Christian Rom,Volker Pauli,Vuokko Nurmela*

Main category: eess.SP

TL;DR: 3GPP Rel.19 TR 38.901更新了6G信道模型，改进了手持设备的UE天线建模和用户遮挡效应，通过高精度仿真和测量实现了更真实的性能评估。


<details>
  <summary>Details</summary>
Motivation: 6G过渡需要对3GPP信道模型进行重大更新，特别是在手持设备的UE天线建模和用户引起的遮挡方面，以更准确地评估6G技术性能。

Method: 基于参考智能手机在多个频段的高保真仿真和测量，引入更真实的框架，包括定向天线模式、实际天线布局、极化效应和元件特定遮挡。

Result: 新模型使链路级和系统级仿真与真实设备行为保持一致，支持更准确的6G技术评估。

Conclusion: 更新后的3GPP信道模型为行业和研究提供了更一致的性能评估框架，有助于6G技术的准确评估和发展。

Abstract: The transition to 6G has driven significant updates to the 3GPP channel
model, particularly in modeling UE antennas and user-induced blockage for
handheld devices. The 3GPP Rel.19 revision of TR 38.901 introduces a more
realistic framework that captures directive antenna patterns, practical antenna
placements, polarization effects, and element-specific blockage. These updates
are based on high-fidelity simulations and measurements of a reference
smartphone across multiple frequency ranges. By aligning link- and system-level
simulations with real-world device behavior, the new model enables more
accurate evaluation of 6G technologies and supports consistent performance
assessment across industry and research.

</details>


### [12] [Robust Localization in Modern Cellular Networks using Global Map Features](https://arxiv.org/abs/2509.10433)
*Junshi Chen,Xuhong Li,Russ Whiton,Erik Leitinger,Fredrik Tufvesson*

Main category: eess.SP

TL;DR: 基于客户端5G/6G网络RF信号的多路径同时定位与地图构建方法，通过全局地图特征仓库提升在复杂城市环境下的定位精度和稳健性


<details>
  <summary>Details</summary>
Motivation: 解决现代细胞网络中因障碍物挡藏(OLoS)和多路传播导致的定位困难，提高在复杂环境下的弹性定位能力

Method: 提出扩展的MP-SLAM方法，集成全局地图特征(GMF)仓库，通过概率密度(PHD)滤波器传播GMF强度函数

Result: 在密集城市场景中使用LTE RF信号进行实验，证明方法在严重多路传播和小区干扰条件下仍能实现稳健准确定位

Conclusion: 该框架在5G/6G网络中表现出艰困信号条件下的可靠定位能力，性能超过传统自感传感器定位和传统MP-SLAM方法

Abstract: Radio frequency (RF) signal-based localization using modern cellular networks
has emerged as a promising solution to accurately locate objects in challenging
environments. One of the most promising solutions for situations involving
obstructed-line-of-sight (OLoS) and multipath propagation is multipathbased
simultaneous localization and mapping (MP-SLAM) that employs map features
(MFs), such as virtual anchors. This paper presents an extended MP-SLAM method
that is augmented with a global map feature (GMF) repository. This repository
stores consistent MFs of high quality that are collected during prior
traversals. We integrate these GMFs back into the MP-SLAM framework via a
probability hypothesis density (PHD) filter, which propagates GMF intensity
functions over time. Extensive simulations, together with a challenging
real-world experiment using LTE RF signals in a dense urban scenario with
severe multipath propagation and inter-cell interference, demonstrate that our
framework achieves robust and accurate localization, thereby showcasing its
effectiveness in realistic modern cellular networks such as 5G or future 6G
networks. It outperforms conventional proprioceptive sensor-based localization
and conventional MP-SLAM methods, and achieves reliable localization even under
adverse signal conditions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [13] [Spectral Bottleneck in Deep Neural Networks: Noise is All You Need](https://arxiv.org/abs/2509.09719)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Itay Zinn,Shimon Pisnoy,Steven H. Frankel*

Main category: eess.AS

TL;DR: 提出WINNER权重扰动方案，通过自适应高斯噪声扰动均匀初始化权重来解决频谱瓶颈问题，提高高频主导信号的拟合效果


<details>
  <summary>Details</summary>
Motivation: 深度神经网络存在频谱学习偏差，低频成分先学习而高频成分后学习。当目标信号缺乏低频成分且以高频为主时，会出现频谱瓶颈，导致模型无法有效重构信号

Method: 提出WINNER权重初始化方案：对均匀初始化的权重添加高斯噪声，噪声尺度根据目标信号的频谱质心自适应确定，从而控制网络激活谱和神经正切核特征基

Result: 该方法不仅解决了频谱瓶颈问题，还实现了更快的收敛速度和更高的表示精度，在音频拟合任务中超越现有方法，在图像拟合和去噪任务中也有显著提升

Conclusion: WINNER方案为计算机视觉和科学机器学习中的自适应权重初始化策略开辟了新方向，能够有效处理各种频率内容的目标信号

Abstract: Deep neural networks are known to exhibit a spectral learning bias, wherein
low-frequency components are learned early in training, while high-frequency
modes emerge more gradually in later epochs. However, when the target signal
lacks low-frequency components and is dominated by broadband high frequencies,
training suffers from a 'spectral bottleneck', and the model fails to
reconstruct the entire signal, including the frequency components that lie
within the network's representational capacity. We examine such a scenario in
the context of implicit neural representations (INRs) with sinusoidal
representation networks (SIRENs), focusing on the challenge of fitting
high-frequency-dominant signals that are susceptible to spectral bottleneck. To
effectively fit any target signal irrespective of it's frequency content, we
propose a generalized target-aware 'weight perturbation scheme' (WINNER -
weight initialization with noise for neural representations) for network
initialization. The scheme perturbs uniformly initialized weights with Gaussian
noise, where the noise scales are adaptively determined by the spectral
centroid of the target signal. We show that the noise scales can provide
control over the spectra of network activations and the eigenbasis of the
empirical neural tangent kernel. This method not only addresses the spectral
bottleneck but also yields faster convergence and with improved representation
accuracy, outperforming state-of-the-art approaches in audio fitting and
achieving notable gains in image fitting and denoising tasks. Beyond signal
reconstruction, our approach opens new directions for adaptive weight
initialization strategies in computer vision and scientific machine learning.

</details>


### [14] [The MSP-Podcast Corpus](https://arxiv.org/abs/2509.09791)
*Carlos Busso,Reza Lotfian,Kusha Sridhar,Ali N. Salman,Wei-Cheng Lin,Lucas Goncalves,Srinivas Parthasarathy,Abinay Reddy Naini,Seong-Gyun Leem,Luz Martinez-Lucas,Huang-Cheng Chou,Pravin Mote*

Main category: eess.AS

TL;DR: MSP-Podcast语料库是一个超过400小时的情感语音数据库，具有丰富的情感标注和多样化的音频样本，旨在推动现实场景中的语音情感识别研究。


<details>
  <summary>Details</summary>
Motivation: 现有情感语音数据库在规模、情感平衡和说话人多样性方面存在局限，需要高质量的大规模数据库来推进现实世界中的语音情感识别研究。

Method: 从多个音频分享网站收集具有通用许可的音频样本，采用机器学习驱动的流程筛选情感多样化的录音，并由至少5名标注者对每个样本进行主要情感、次要情感以及效价、唤醒度、支配度等情感属性的标注。

Result: 构建了包含400多小时音频的MSP-Podcast语料库，具有丰富的情感标注、说话人识别和文本转录，确保了情感在说话人和环境间的平衡多样性。

Conclusion: 该语料库为推进实际应用中的语音情感识别系统提供了全面、高质量的资源，更适合现实场景的研究需求。

Abstract: The availability of large, high-quality emotional speech databases is
essential for advancing speech emotion recognition (SER) in real-world
scenarios. However, many existing databases face limitations in size, emotional
balance, and speaker diversity. This study describes the MSP-Podcast corpus,
summarizing our ten-year effort. The corpus consists of over 400 hours of
diverse audio samples from various audio-sharing websites, all of which have
Common Licenses that permit the distribution of the corpus. We annotate the
corpus with rich emotional labels, including primary (single dominant emotion)
and secondary (multiple emotions perceived in the audio) emotional categories,
as well as emotional attributes for valence, arousal, and dominance. At least
five raters annotate these emotional labels. The corpus also has speaker
identification for most samples, and human transcriptions of the lexical
content of the sentences for the entire corpus. The data collection protocol
includes a machine learning-driven pipeline for selecting emotionally diverse
recordings, ensuring a balanced and varied representation of emotions across
speakers and environments. The resulting database provides a comprehensive,
high-quality resource, better suited for advancing SER systems in practical,
real-world scenarios.

</details>


### [15] [Acoustic Scene Classification Using CNN-GRU Model Without Knowledge Distillation](https://arxiv.org/abs/2509.09931)
*Ee-Leng Tan,Jun Wei Yeow,Santi Peksi,Haowen Li,Ziyi Yang,Woon-Seng Gan*

Main category: eess.AS

TL;DR: SNTL-NTU团队针对DCASE 2025挑战赛Task 1提出的低复杂度声学场景分类模型，基于CNN-GRU架构，仅使用TAU数据集训练，内存占用114.2KB，计算量10.9M MAC，准确率60.25%


<details>
  <summary>Details</summary>
Motivation: 在有限复杂度约束下实现高性能声学场景分类，不同于传统的师生知识蒸馏方法

Method: 基于CNN-GRU模型，仅使用TAU Urban Acoustic Scene 2022 Mobile开发数据集训练，采用MicIRP进行设备脉冲响应增强

Result: 模型内存占用114.2KB，计算量10.9M MAC操作，在开发数据集上达到60.25%的准确率

Conclusion: 提出的轻量级模型在低复杂度条件下实现了良好的声学场景分类性能，为资源受限环境下的音频处理应用提供了可行方案

Abstract: In this technical report, we present the SNTL-NTU team's Task 1 submission
for the Low-Complexity Acoustic Scenes and Events (DCASE) 2025 challenge. This
submission departs from the typical application of knowledge distillation from
a teacher to a student model, aiming to achieve high performance with limited
complexity. The proposed model is based on a CNN-GRU model and is trained
solely using the TAU Urban Acoustic Scene 2022 Mobile development dataset,
without utilizing any external datasets, except for MicIRP, which is used for
device impulse response (DIR) augmentation. The proposed model has a memory
usage of 114.2KB and requires 10.9M muliply-and-accumulate (MAC) operations.
Using the development dataset, the proposed model achieved an accuracy of
60.25%.

</details>


### [16] [Effective Modeling of Critical Contextual Information for TDNN-based Speaker Verification](https://arxiv.org/abs/2509.09932)
*Shilong Weng,Liu Yang,Ji Mao*

Main category: eess.AS

TL;DR: 基于ECAPA-TDNN的三种改进网络结构，通过充分利用上下文信息提取多尺度特征，在语者验证任务上实现了显著性能提升


<details>
  <summary>Details</summary>
Motivation: ECAPA-TDNN中的SE-Res2Block层次卷积结构无法充分利用上下文信息，导致对上下文依赖关系的建模能力较弱

Method: 提出三种改进的ECAPA-TDNN网络结构，以更充分有效地提取具有上下文依赖的多尺度特征，并进行特征聚合

Result: 在VoxCeleb和CN-Celeb数据集上验证了有效性，其中一种结构在VoxCeleb1-O数据集上相比ECAPA-TDNN实现了近23%的等误率降低

Conclusion: 该方法在参数量可比的条件下，达到了当前TDNN网络结构中具有竞争力的性能

Abstract: Today, Time Delay Neural Network (TDNN) has become the mainstream
architecture for speaker verification task, in which the ECAPA-TDNN is one of
the state-of-the-art models. The current works that focus on improving TDNN
primarily address the limitations of TDNN in modeling global information and
bridge the gap between TDNN and 2-Dimensional convolutions. However, the
hierarchical convolutional structure in the SE-Res2Block proposed by ECAPA-TDNN
cannot make full use of the contextual information, resulting in the weak
ability of ECAPA-TDNN to model effective context dependencies. To this end,
three improved architectures based on ECAPA-TDNN are proposed to fully and
effectively extract multi-scale features with context dependence and then
aggregate these features. The experimental results on VoxCeleb and CN-Celeb
verify the effectiveness of the three proposed architectures. One of these
architectures achieves nearly a 23% lower Equal Error Rate compared to that of
ECAPA-TDNN on VoxCeleb1-O dataset, demonstrating the competitive performance
achievable among the current TDNN architectures under the comparable parameter
count.

</details>


### [17] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: 通过分析Whisper模型的注意力头，发现特定的注意力头能够捐准地抓取单词对齐信息，使用字符级输入比词段更准确，提出了一种无监督的单词对齐提取方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单词对齐方法或需要额外训练或性能不佳，评估标准过于松弛（通常>200ms），需要在更严格的对齐实现更高准确度。

Method: 分析Whisper模型的注意力头，识别出能够捐准捕捉单词对齐的特定注意头，使用字符级输入代替词段，通过过滤注意力头来提取单词对齐信息。

Result: 该方法不需要训练，在20-100ms的严格容忍范围内，产生的单词对齐比之前的方法更加准确。

Conclusion: 通过注意力头分析和字符级输入，可以实现高准确的无监督单词对齐提取，在严格的对齐要求下显示优势。

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [18] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 本文提出一种统一的二维卷积神经网络前端结构，用于语音识别的特征提取，该方法参数效率高且等效于现有监督学习特征提取器。


<details>
  <summary>Details</summary>
Motivation: 虽然神经前端在语音识别中展示了潇望潜力，但现有技术仍受到经典方法的强烈影响。研究目标是开发更通用的特征提取前端，并统一前端架构，以对比现有采用来自不同源头的多种层拓扑结构组合的方法。

Method: 通过系统实验渐进地减少现有技术的影响，开发了一种统一的2D卷积神经网络前端结构。该方法重点关注参数效率和计算资源有限的场景，不依赖大型预训练模型。

Result: 实验结果表明，这种通用的统一前端方法不仅可行，而且在性能上可以比拟现有的监督学习特征提取器。

Conclusion: 该研究成功开发了一种统一、通用的2D卷积神经网络前端结构，该方法具有高参数效率、适合计算资源有限环境的优点，并在性能上达到了与现有监督学习特征提取器相当的水平。

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [19] [Towards Data Drift Monitoring for Speech Deepfake Detection in the context of MLOps](https://arxiv.org/abs/2509.10086)
*Xin Wang,Wanying Ge,Junichi Yamagishi*

Main category: eess.AS

TL;DR: 这篇论文研究了语音深度伪造检测器在云端部署时面对新出现攻击的漏洞，通过监测数据偏移和微调策略来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 静态语音深度伪造检测器在云端应用中面临新出现攻击的威胁，需要从MLOps角度解决数据偏移问题和模型更新挑战。

Method: 使用分布距离监测新数据与参考数据集之间的偏移，并通过使用新TTS攻击生成的数据对检测器进行微调。

Result: 在玩具数据集和大规模MLAAD数据集上验证，新TTS攻击导致的偏移可被监测，通过微调可以降低偏移和检测错误率。

Conclusion: 通过监测数据偏移和针对性微调，可以有效提升静态语音深度伪造检测器在云端环境中的适应性和检测性能。

Abstract: When being delivered in applications or services on the cloud, static speech
deepfake detectors that are not updated will become vulnerable to newly created
speech deepfake attacks. From the perspective of machine learning operations
(MLOps), this paper tries to answer whether we can monitor new and unseen
speech deepfake data that drifts away from a seen reference data set. We
further ask, if drift is detected, whether we can fine-tune the detector using
similarly drifted data, reduce the drift, and improve the detection
performance. On a toy dataset and the large-scale MLAAD dataset, we show that
the drift caused by new text-to-speech (TTS) attacks can be monitored using
distances between the distributions of the new data and reference data.
Furthermore, we demonstrate that fine-tuning the detector using data generated
by the new TTS deepfakes can reduce the drift and the detection error rates.

</details>


### [20] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 本文分析了会议转录中语音分离的泄漏问题，发现交叉通道存在显著泄漏但VAD会忽略这些部分，同时比较了不同分割方法的效果。


<details>
  <summary>Details</summary>
Motivation: 会议转录领域虽然近年来取得显著进展，但仍存在性能限制的挑战，需要分析语音分离中的泄漏问题。

Method: 扩展了先前提出的语音分离泄漏分析框架，增加了对时间局部性的敏感性分析，比较了不同分割方法（包括基于能量的VAD和先进的说话人分离方法）。

Result: 发现在只有主要说话人活动的区域存在显著的交叉通道泄漏，但这些泄漏部分被VAD大部分忽略；先进的说话人分离方法能将与oracle分割的差距减少三分之一；在仅使用LibriSpeech数据训练的系统中达到了LibriCSS上的最先进性能。

Conclusion: 语音分离中的泄漏问题虽然存在，但对最终性能影响有限；先进的说话人分离方法能显著改善分割质量；研究揭示了影响剩余性能差距的因素。

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


### [21] [Low-latency Assistive Audio Enhancement for Neurodivergent People](https://arxiv.org/abs/2509.10202)
*Alexander Popescu,Rosie Frost,Milos Cernak*

Main category: eess.AS

TL;DR: 这篇论文研究了为神经分异人群设计的辅助音频增强算法，通过动态范围压缩技术有效过滤引发声音，减轻这一群体的听觉困扰。


<details>
  <summary>Details</summary>
Motivation: 神经分异人群50-70%遇到声音容忍问题，从轻微不适到严重困扰，需要辅助技术来筛选急急声音。

Method: 分析Reddit等平台的神经分异社区确定引发声音清单，从FSD50K和ESC50数据集编译样本，训练和评估多种DSP和ML音频增强算法。

Result: 动态范围压缩(DRC)算法表现最佳，能有效衰减引发声音并减轻神经分异听众的听觉困扰。

Conclusion: 该研究为神经分异人群提供了有效的音频过滤解决方案，DRC技术在减轻声音敏感性方面显示出良好效果。

Abstract: Neurodivergent people frequently experience decreased sound tolerance, with
estimates suggesting it affects 50-70% of this population. This heightened
sensitivity can provoke reactions ranging from mild discomfort to severe
distress, highlighting the critical need for assistive audio enhancement
technologies In this paper, we propose several assistive audio enhancement
algorithms designed to selectively filter distressing sounds. To address this,
we curated a list of potential trigger sounds by analyzing
neurodivergent-focused communities on platforms such as Reddit. Using this
list, a dataset of trigger sound samples was compiled from publicly available
sources, including FSD50K and ESC50. These samples were then used to train and
evaluate various Digital Signal Processing (DSP) and Machine Learning (ML)
audio enhancement algorithms. Among the approaches explored, Dynamic Range
Compression (DRC) proved the most effective, successfully attenuating trigger
sounds and reducing auditory distress for neurodivergent listeners.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [22] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: 语音语言模型在根据语音指令调整说话风格方面存在显著局限性，研究提出了语音风格适应任务和相应评测框架


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型主要关注语义准确性和指令遵循能力，而根据语音指令调整说话风格的能力受到忽视

Method: 提出语音风格适应(VSA)任务，建立VStyle双语评测集，包含声学属性、自然语言指令、角色扮演和隐式共情四类语音生成，并提出LALM as a Judge评估框架

Result: 对商业系统和开源SLMs的实验显示，当前模型在可控风格适应方面面临明显的局限性

Conclusion: 这个任务具有新颖性和挑战性，通过释放VStyle数据集和评估工具包，为推进人本中心语音交互提供了基础

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


### [23] [Testing chatbots on the creation of encoders for audio conditioned image generation](https://arxiv.org/abs/2509.09717)
*Jorge E. León,Miguel Carrasco*

Main category: cs.SD

TL;DR: 研究者使用5种公开聊天机器人设计音频编码器以替换Stable Diffusion中的CLIP文本编码器，但所有建议都未能达到满意效果，显示了聊天机器计算机编码能力的差距。


<details>
  <summary>Details</summary>
Motivation: 旨在探索现代聊天机器人是否能够设计有效的音频编码器来替换文本编码器，实现从声音直接生成图像的功能。

Method: 使用5种公开聊天机器人提出神经网络架构设计，将每个有效建议的编码器在超过200万个音频-图像-文本观测数据上训练，并使用多种指标进行评估。

Result: 虽然几乎所有聊天机器人都生成了有效的模型设计，但无一达到满意结果。Gemini音频编码器在数量指标上表现最好，Grok音频编码器生成的图像更一致（特别是与文本编码器组合时）。

Conclusion: 聊天机器人存在共享的架构偏见，且在编码能力上仍有差距。需要更专业的任务来测试它们的创造力和推理能力。

Abstract: On one hand, recent advances in chatbots has led to a rising popularity in
using these models for coding tasks. On the other hand, modern generative image
models primarily rely on text encoders to translate semantic concepts into
visual representations, even when there is clear evidence that audio can be
employed as input as well. Given the previous, in this work, we explore whether
state-of-the-art conversational agents can design effective audio encoders to
replace the CLIP text encoder from Stable Diffusion 1.5, enabling image
synthesis directly from sound. We prompted five publicly available chatbots to
propose neural architectures to work as these audio encoders, with a set of
well-explained shared conditions. Each valid suggested encoder was trained on
over two million context related audio-image-text observations, and evaluated
on held-out validation and test sets using various metrics, together with a
qualitative analysis of their generated images. Although almost all chatbots
generated valid model designs, none achieved satisfactory results, indicating
that their audio embeddings failed to align reliably with those of the original
text encoder. Among the proposals, the Gemini audio encoder showed the best
quantitative metrics, while the Grok audio encoder produced more coherent
images (particularly, when paired with the text encoder). Our findings reveal a
shared architectural bias across chatbots and underscore the remaining coding
gap that needs to be bridged in future versions of these models. We also
created a public demo so everyone could study and try out these audio encoders.
Finally, we propose research questions that should be tackled in the future,
and encourage other researchers to perform more focused and highly specialized
tasks like this one, so the respective chatbots cannot make use of well-known
solutions and their creativity/reasoning is fully tested.

</details>


### [24] [AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models](https://arxiv.org/abs/2509.09746)
*Ning Ma,Bahman Mirheidari,Guy J. Brown,Minyoi M. Maimbolwa,Nsala Sanjase,Solomon Chifwamba,Seke Muzazu,Monde Muyoyeta,Mary Kagujje*

Main category: cs.SD

TL;DR: 基于语音基础模型的咳嗽声音分析结合人口统计和临床数据，在结核病筛查中表现出色，达到WHO目标产品标准，可作为有效的结核病分诊工具。


<details>
  <summary>Details</summary>
Motivation: 人工智能可以通过分析咳嗽声音检测疾病相关的声学模式，为高负担、低资源地区的结核病筛查提供可扩展的解决方案。以往研究受限于小数据集、症状性非结核患者代表性不足、简单模型依赖以及理想化录音条件。

Method: 在赞比亚两家医院招募512名参与者，分为细菌学确诊结核病组(TB+)、其他呼吸道疾病症状患者组(OR)和健康对照组(HC)。使用基于语音基础模型的深度学习分类器对咳嗽录音进行训练，最佳模型进一步结合人口统计和临床特征进行评估。

Result: 最佳纯音频分类器在区分TB+与其他人群时AUROC达到85.2%，TB+与OR比较时为80.1%。加入人口统计和临床特征后性能提升至92.1%和84.2%。多模态模型在TB+/Rest分类中达到90.3%敏感性和73.1%特异性。

Conclusion: 基于语音基础模型的咳嗽分析，特别是结合人口统计和临床数据，显示出作为结核病分诊工具的强大潜力，满足WHO目标产品标准。模型对背景噪声、录音时间和设备变化等混杂因素具有鲁棒性，表明能够检测真正的疾病相关声学模式。

Abstract: Background
  Artificial intelligence (AI) can detect disease-related acoustic patterns in
cough sounds, offering a scalable approach to tuberculosis (TB) screening in
high-burden, low-resource settings. Previous studies have been limited by small
datasets, under-representation of symptomatic non-TB patients, reliance on
simple models, and recordings collected under idealised conditions.
  Methods
  We enrolled 512 participants at two hospitals in Zambia, grouped as
bacteriologically confirmed TB (TB+), symptomatic patients with other
respiratory diseases (OR), and healthy controls (HC). Usable cough recordings
plus demographic and clinical data were obtained from 500 participants. Deep
learning classifiers based on speech foundation models were trained on cough
recordings. The best-performing model, trained on 3-second segments, was
further evaluated with demographic and clinical features.
  Findings
  The best audio-only classifier achieved an AUROC of 85.2% for distinguishing
TB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographic
and clinical features improved performance to 92.1% (TB+/Rest) and 84.2%
(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%
sensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.
  Interpretation
  Cough analysis using speech foundation models, especially when combined with
demographic and clinical data, showed strong potential as a TB triage tool,
meeting WHO target product profile benchmarks. The model was robust to
confounding factors including background noise, recording time, and device
variability, indicating detection of genuine disease-related acoustic patterns.
Further validation across diverse regions and case definitions, including
subclinical TB, is required before clinical use.

</details>


### [25] [DiTReducio: A Training-Free Acceleration for DiT-Based TTS via Progressive Calibration](https://arxiv.org/abs/2509.09748)
*Yanru Huo,Ziyue Jiang,Zuoli Tang,Qingyang Hong,Zhou Zhao*

Main category: cs.SD

TL;DR: DiTReducio是一个无需训练的训练框架，通过渐进校准压缩基于DiT的TTS模型计算，提出时间跳跃和分支跳跃两种压缩方法，在保持生成质量的同时显著降低计算量。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)在非自回归语音合成中取得进展，但计算需求高。现有加速方法主要依赖蒸馏技术减少采样步骤，但仍受训练成本限制。

Method: 提出两种压缩方法：时间跳跃和分支跳跃，在推理时消除冗余计算。基于DiT层中识别的两种注意力模式，设计模式引导策略选择性应用压缩方法。通过可调节压缩阈值灵活调制生成质量和计算效率。

Result: 在F5-TTS和MegaTTS 3上的实验评估显示，DiTReducio实现了75.4%的FLOPs减少，实时因子(RTF)提升37.1%，同时保持生成质量。

Conclusion: DiTReducio提供了一种有效的训练免费加速方案，显著提升了DiT-based TTS模型的推理效率。

Abstract: While Diffusion Transformers (DiT) have advanced non-autoregressive (NAR)
speech synthesis, their high computational demands remain an limitation.
Existing DiT-based text-to-speech (TTS) model acceleration approaches mainly
focus on reducing sampling steps through distillation techniques, yet they
remain constrained by training costs. We introduce DiTReducio, a training-free
acceleration framework that compresses computations in DiT-based TTS models via
progressive calibration. We propose two compression methods, Temporal Skipping
and Branch Skipping, to eliminate redundant computations during inference.
Moreover, based on two characteristic attention patterns identified within DiT
layers, we devise a pattern-guided strategy to selectively apply the
compression methods. Our method allows flexible modulation between generation
quality and computational efficiency through adjustable compression thresholds.
Experimental evaluations conducted on F5-TTS and MegaTTS 3 demonstrate that
DiTReducio achieves a 75.4% reduction in FLOPs and improves the Real-Time
Factor (RTF) by 37.1%, while preserving generation quality.

</details>


### [26] [Combining Textual and Spectral Features for Robust Classification of Pilot Communications](https://arxiv.org/abs/2509.09752)
*Abdullah All Tanvir,Chenyu Huang,Moe Alahmad,Chuyang Yang,Xin Zhong*

Main category: cs.SD

TL;DR: 提出基于双管道机器学习框架的飞机操作意图分类系统，结合文本和频谱特征分析飞行员无线电通信，在非塔台机场实现91%以上F1分数的准确分类


<details>
  <summary>Details</summary>
Motivation: 非塔台机场缺乏专用监控基础设施，飞机起降操作估计困难，需要一种成本效益高且可扩展的监控解决方案

Method: 使用双管道机器学习框架：文本特征管道（自动语音识别）和频谱特征管道（梅尔频谱图提取），评估了传统分类器、集成方法、LSTM和CNN等多种模型

Result: 频谱特征结合深度学习架构表现最佳，F1分数超过91%，数据增强进一步提高了对真实音频变化的鲁棒性

Conclusion: 该方法无需额外基础设施即可部署，为通用航空机场空中交通监控提供了实用、可扩展且成本效益高的解决方案

Abstract: Accurate estimation of aircraft operations, such as takeoffs and landings, is
critical for effective airport management, yet remains challenging, especially
at non-towered facilities lacking dedicated surveillance infrastructure. This
paper presents a novel dual pipeline machine learning framework that classifies
pilot radio communications using both textual and spectral features. Audio data
collected from a non-towered U.S. airport was annotated by certified pilots
with operational intent labels and preprocessed through automatic speech
recognition and Mel-spectrogram extraction. We evaluate a wide range of
traditional classifiers and deep learning models, including ensemble methods,
LSTM, and CNN across both pipelines. To our knowledge, this is the first system
to classify operational aircraft intent using a dual-pipeline ML framework on
real-world air traffic audio. Our results demonstrate that spectral features
combined with deep architectures consistently yield superior classification
performance, with F1-scores exceeding 91%. Data augmentation further improves
robustness to real-world audio variability. The proposed approach is scalable,
cost-effective, and deployable without additional infrastructure, offering a
practical solution for air traffic monitoring at general aviation airports.

</details>


### [27] [SoilSound: Smartphone-based Soil Moisture Estimation](https://arxiv.org/abs/2509.09823)
*Yixuan Gao,Tanvir Ahmed,Shuang He,Zhongqi Cheng,Rajalakshmi Nandakumar*

Main category: cs.SD

TL;DR: SoilSound是一个基于智能手机的声学传感系统，利用内置扬声器和麦克风通过垂直扫描机制测量土壤湿度，无需校准且不干扰土壤。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度监测方法需要侵入式探头或专用设备，限制了公众的可及性。需要一种无需干扰土壤、易于获取的监测方案。

Method: 利用声学反射的表面粗糙度效应模型，发送声学啁啾信号并记录反射，通过卷积神经网络进行设备端湿度估计。

Result: 在10个不同地点测试中，平均绝对误差为2.39%，能够准确追踪15.9%到34.0%的湿度范围。

Conclusion: SoilSound无需校准或干扰土壤即可实现准确湿度监测，为家庭园丁、城市农民等提供了广泛可用的监测方案。

Abstract: Soil moisture monitoring is essential for agriculture and environmental
management, yet existing methods require either invasive probes disturbing the
soil or specialized equipment, limiting access to the public. We present
SoilSound, an ubiquitous accessible smartphone-based acoustic sensing system
that can measure soil moisture without disturbing the soil. We leverage the
built-in speaker and microphone to perform a vertical scan mechanism to
accurately measure moisture without any calibration. Unlike existing work that
use transmissive properties, we propose an alternate model for acoustic
reflections in soil based on the surface roughness effect to enable moisture
sensing without disturbing the soil. The system works by sending acoustic
chirps towards the soil and recording the reflections during a vertical scan,
which are then processed and fed to a convolutional neural network for
on-device soil moisture estimation with negligible computational, memory, or
power overhead. We evaluated the system by training with curated soils in boxes
in the lab and testing in the outdoor fields and show that SoilSound achieves a
mean absolute error (MAE) of 2.39% across 10 different locations. Overall, the
evaluation shows that SoilSound can accurately track soil moisture levels
ranging from 15.9% to 34.0% across multiple soil types, environments, and
users; without requiring any calibration or disturbing the soil, enabling
widespread moisture monitoring for home gardeners, urban farmers, citizen
scientists, and agricultural communities in resource-limited settings.

</details>


### [28] [CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio](https://arxiv.org/abs/2509.09836)
*Marco Pasini,Stefan Lattner,George Fazekas*

Main category: cs.SD

TL;DR: CoDiCodec是一种新颖的音频自编码器，通过有限标量量化和FSQ-dropout技术，在同一个训练模型中同时生成压缩的连续嵌入和离散标记，提供了前所未有的灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的音频自编码器往往需要在连续嵌入和离散标记之间做出选择，同时在保持高压缩比的同时维持音频保真度仍然是一个挑战。

Method: 使用有限标量量化(FSQ)和新型FSQ-dropout技术，通过单一一致性损失进行端到端训练，支持自回归解码和新型并行解码策略。

Result: CoDiCodec在相似比特率下优于现有的连续和离散自编码器，在重建音频质量方面表现优异，并行解码策略实现了更好的音频质量和更快的解码速度。

Conclusion: 这项工作为音频压缩提供了统一的方法，弥合了连续和离散生成建模范式之间的差距。

Abstract: Efficiently representing audio signals in a compressed latent space is
critical for latent generative modelling. However, existing autoencoders often
force a choice between continuous embeddings and discrete tokens. Furthermore,
achieving high compression ratios while maintaining audio fidelity remains a
challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes
these limitations by both efficiently encoding global features via summary
embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz
and discrete tokens at a rate of 2.38 kbps from the same trained model,
offering unprecedented flexibility for different downstream generative tasks.
This is achieved through Finite Scalar Quantization (FSQ) and a novel
FSQ-dropout technique, and does not require additional loss terms beyond the
single consistency loss used for end-to-end training. CoDiCodec supports both
autoregressive decoding and a novel parallel decoding strategy, with the latter
achieving superior audio quality and faster decoding. CoDiCodec outperforms
existing continuous and discrete autoencoders at similar bitrates in terms of
reconstruction audio quality. Our work enables a unified approach to audio
compression, bridging the gap between continuous and discrete generative
modelling paradigms.

</details>


### [29] [Prototypical Contrastive Learning For Improved Few-Shot Audio Classification](https://arxiv.org/abs/2509.10074)
*Christos Sgouropoulos,Christos Nikou,Stefanos Vlachos,Vasileios Theiou,Christos Foukanelis,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: 本文提出了一种将监督对比损失集成到原型少样本音频分类训练中的方法，使用角损失替代标准对比损失，结合SpecAugment和自注意力机制，在MetaAudio基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然少样本学习在图像领域得到广泛研究，但在音频分类中相对较少探索。本文旨在解决音频分类中少样本学习的挑战，特别是在标注数据有限的情况下。

Method: 提出将监督对比损失集成到原型少样本训练中，使用角损失替代标准对比损失。采用SpecAugment数据增强和自注意力机制，将增强输入版本的多样化信息封装到统一的嵌入表示中。

Result: 在MetaAudio基准测试（包含五个数据集）的5-way 5-shot设置中，所提出的方法达到了最先进的性能。

Conclusion: 集成监督对比损失，特别是角损失，能够有效提升少样本音频分类的性能，结合SpecAugment和自注意力机制可以更好地捕捉音频特征的多样性。

Abstract: Few-shot learning has emerged as a powerful paradigm for training models with
limited labeled data, addressing challenges in scenarios where large-scale
annotation is impractical. While extensive research has been conducted in the
image domain, few-shot learning in audio classification remains relatively
underexplored. In this work, we investigate the effect of integrating
supervised contrastive loss into prototypical few shot training for audio
classification. In detail, we demonstrate that angular loss further improves
the performance compared to the standard contrastive loss. Our method leverages
SpecAugment followed by a self-attention mechanism to encapsulate diverse
information of augmented input versions into one unified embedding. We evaluate
our approach on MetaAudio, a benchmark including five datasets with predefined
splits, standardized preprocessing, and a comprehensive set of few-shot
learning models for comparison. The proposed approach achieves state-of-the-art
performance in a 5-way, 5-shot setting.

</details>


### [30] [Data-independent Beamforming for End-to-end Multichannel Multi-speaker ASR](https://arxiv.org/abs/2509.10234)
*Can Cui,Paul Magron,Mostafa Sadeghi,Emmanuel Vincent*

Main category: cs.SD

TL;DR: 提出一种基于球面极坐标的波束形成方法，用于多通道多说话人语音识别，无需训练即可提升识别性能


<details>
  <summary>Details</summary>
Motivation: 多通道多说话人场景中的自动语音识别面临环境噪声、混响和说话人重叠等挑战，需要更有效的方法来处理多通道信号

Method: 基于球面极坐标处理特定角度扇区的波束形成方法，数据独立且无需训练，生成一组波束形成信号作为ASR系统输入

Result: 在AMI会议语料库上实验表明，词错误率相对降低11%，说话人计数准确率相对提高27%

Conclusion: 该方法能更高效地利用多通道信号，减少ASR系统输入负载，同时显著提升多说话人语音识别性能

Abstract: Automatic speech recognition (ASR) in multichannel, multi-speaker scenarios
remains challenging due to ambient noise, reverberation and overlapping
speakers. In this paper, we propose a beamforming approach that processes
specific angular sectors based on their spherical polar coordinates before
applying an end-to-end multichannel, multi-speaker ASR system. This method is
data-independent and training-free. We demonstrate that using a group of
beamformed signals improves ASR performance compared to using the same number
of raw microphone signals. Moreover, increasing the number of signals used for
beamforming further enhances recognition accuracy, leading to a more efficient
use of multichannel signals while reducing the overall input load for the ASR
system. We conduct experiments on the AMI meeting corpus, where the proposed
method reduces word error rate by up to 11% and improves speaker counting
accuracy by up to 27% relative compared to a multichannel ASR baseline system
that does not exploit beamforming.

</details>


### [31] [Improving Audio Event Recognition with Consistency Regularization](https://arxiv.org/abs/2509.10391)
*Shanmuka Sadhu,Weiran Wang*

Main category: cs.SD

TL;DR: 本文提出将一致性正则化(CR)应用于音频事件识别，在AudioSet数据集上验证了其有效性，并在小规模和大规模监督训练集以及半监督设置下均取得了性能提升


<details>
  <summary>Details</summary>
Motivation: 一致性正则化在自动语音识别中已显示出优势，但尚未在音频事件识别中得到充分探索。本文旨在验证CR在音频事件识别任务中的有效性，特别是在数据增强已广泛使用的情况下

Method: 使用一致性正则化技术，通过强制模型在增强视图上的预测一致性来提升性能。进行了广泛的消融研究，包括使用强增强和多重增强策略，并扩展到半监督设置（2万标注样本+180万未标注样本）

Result: CR在已大量使用数据增强的监督基线上带来了持续改进；对于小训练集，使用更强增强和多重增强的CR带来了额外增益；在半监督设置下，性能超过了在小数据集上训练的最佳模型

Conclusion: 一致性正则化在音频事件识别中具有显著效果，能够有效提升模型性能，特别是在数据有限的情况下，且在半监督学习中也表现出良好潜力

Abstract: Consistency regularization (CR), which enforces agreement between model
predictions on augmented views, has found recent benefits in automatic speech
recognition [1]. In this paper, we propose the use of consistency
regularization for audio event recognition, and demonstrate its effectiveness
on AudioSet. With extensive ablation studies for both small ($\sim$20k) and
large ($\sim$1.8M) supervised training sets, we show that CR brings consistent
improvement over supervised baselines which already heavily utilize data
augmentation, and CR using stronger augmentation and multiple augmentations
leads to additional gain for the small training set. Furthermore, we extend the
use of CR into the semi-supervised setup with 20K labeled samples and 1.8M
unlabeled samples, and obtain performance improvement over our best model
trained on the small set.

</details>

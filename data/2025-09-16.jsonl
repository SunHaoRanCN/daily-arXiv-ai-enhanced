{"id": "2509.10566", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10566", "abs": "https://arxiv.org/abs/2509.10566", "authors": ["Sergio Poo Hernandez", "Vadim Bulitko", "Erin Bayne"], "title": "Combining Audio and Non-Audio Inputs in Evolved Neural Networks for Ovenbird", "comment": null, "summary": "In the last several years the use of neural networks as tools to automate\nspecies classification from digital data has increased. This has been due in\npart to the high classification accuracy of image classification through\nConvolutional Neural Networks (CNN). In the case of audio data CNN based\nrecognizers are used to automate the classification of species in audio\nrecordings by using information from sound visualization (i.e., spectrograms).\nIt is common for these recognizers to use the spectrogram as their sole input.\nHowever, researchers have other non-audio data, such as habitat preferences of\na species, phenology, and range information, available that could improve\nspecies classification. In this paper we present how a single-species\nrecognizer neural network's accuracy can be improved by using non-audio data as\ninputs in addition to spectrogram information. We also analyze if the\nimprovements are merely a result of having a neural network with a higher\nnumber of parameters instead of combining the two inputs. We find that networks\nthat use the two different inputs have a higher classification accuracy than\nnetworks of similar size that use only one of the inputs."}
{"id": "2509.10781", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10781", "abs": "https://arxiv.org/abs/2509.10781", "authors": ["Xiaokang Li", "Yicheng Gong", "Dinghao Zou", "Xin Cao", "Sunbowen Lee"], "title": "Emoanti: audio anti-deepfake with refined emotion-guided representations", "comment": null, "summary": "Audio deepfake is so sophisticated that the lack of effective detection\nmethods is fatal. While most detection systems primarily rely on low-level\nacoustic features or pretrained speech representations, they frequently neglect\nhigh-level emotional cues, which can offer complementary and potentially\nanti-deepfake information to enhance generalization. In this work, we propose a\nnovel audio anti-deepfake system that utilizes emotional features (EmoAnti) by\nexploiting a pretrained Wav2Vec2 (W2V2) model fine-tuned on emotion recognition\ntasks, which derives emotion-guided representations, then designing a dedicated\nfeature extractor based on convolutional layers with residual connections to\neffectively capture and refine emotional characteristics from the transformer\nlayers outputs. Experimental results show that our proposed architecture\nachieves state-of-the-art performance on both the ASVspoof2019LA and\nASVspoof2021LA benchmarks, and demonstrates strong generalization on the\nASVspoof2021DF dataset. Our proposed approach's code is available at Anonymous\nGitHub1."}
{"id": "2509.11124", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11124", "abs": "https://arxiv.org/abs/2509.11124", "authors": ["Tutti Chi", "Letian Gao", "Yixiao Zhang"], "title": "STASE: A spatialized text-to-audio synthesis engine for music generation", "comment": "Accepted to LLM4Music @ ISMIR 2025", "summary": "While many text-to-audio systems produce monophonic or fixed-stereo outputs,\ngenerating audio with user-defined spatial properties remains a challenge.\nExisting deep learning-based spatialization methods often rely on latent-space\nmanipulations, which can limit direct control over psychoacoustic parameters\ncritical to spatial perception. To address this, we introduce STASE, a system\nthat leverages a Large Language Model (LLM) as an agent to interpret spatial\ncues from text. A key feature of STASE is the decoupling of semantic\ninterpretation from a separate, physics-based spatial rendering engine, which\nfacilitates interpretable and user-controllable spatial reasoning. The LLM\nprocesses prompts through two main pathways: (i) Description Prompts, for\ndirect mapping of explicit spatial information (e.g., \"place the lead guitar at\n45{\\deg} azimuth, 10 m distance\"), and (ii) Abstract Prompts, where a\nRetrieval-Augmented Generation (RAG) module retrieves relevant spatial\ntemplates to inform the rendering. This paper details the STASE workflow,\ndiscusses implementation considerations, and highlights current challenges in\nevaluating generative spatial audio."}
{"id": "2509.11128", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11128", "abs": "https://arxiv.org/abs/2509.11128", "authors": ["Yibo Zhang", "Liang Lin"], "title": "ENJ: Optimizing Noise with Genetic Algorithms to Jailbreak LSMs", "comment": null, "summary": "The widespread application of Large Speech Models (LSMs) has made their\nsecurity risks increasingly prominent. Traditional speech adversarial attack\nmethods face challenges in balancing effectiveness and stealth. This paper\nproposes Evolutionary Noise Jailbreak (ENJ), which utilizes a genetic algorithm\nto transform environmental noise from a passive interference into an actively\noptimizable attack carrier for jailbreaking LSMs. Through operations such as\npopulation initialization, crossover fusion, and probabilistic mutation, this\nmethod iteratively evolves a series of audio samples that fuse malicious\ninstructions with background noise. These samples sound like harmless noise to\nhumans but can induce the model to parse and execute harmful commands.\nExtensive experiments on multiple mainstream speech models show that ENJ's\nattack effectiveness is significantly superior to existing baseline methods.\nThis research reveals the dual role of noise in speech security and provides\nnew critical insights for model security defense in complex acoustic\nenvironments."}
{"id": "2509.10706", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10706", "abs": "https://arxiv.org/abs/2509.10706", "authors": ["Chin-Yun Yu", "György Fazekas"], "title": "Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method", "comment": "Published at 2025 AES International Conference on Artificial\n  Intelligence and Machine Learning for Audio\n  (https://aes2.org/publications/elibrary-page/?id=22991)", "summary": "Automatic differentiation through digital signal processing algorithms for\nvirtual analogue modelling has recently gained popularity. These algorithms are\ntypically more computationally efficient than black-box neural networks that\nrely on dense matrix multiplications. Due to their differentiable nature, they\ncan be integrated with neural networks and jointly trained using gradient\ndescent algorithms, resulting in more efficient systems. Furthermore, signal\nprocessing algorithms have significantly fewer parameters than neural networks,\nallowing the application of the Newton-Raphson method. This method offers\nfaster and more robust convergence than gradient descent at the cost of\nquadratic storage. This paper presents a method to emulate analogue levelling\namplifiers using a feed-forward digital compressor with parameters optimised\nvia the Newton-Raphson method. We demonstrate that a digital compressor can\nsuccessfully approximate the behaviour of our target unit, the Teletronix\nLA-2A. Different strategies for computing the Hessian matrix are benchmarked.\nWe leverage parallel algorithms for recursive filters to achieve efficient\ntraining on modern GPUs. The resulting model is made into a VST plugin and is\nopen-sourced at https://github.com/aim-qmul/4a2a."}
{"id": "2509.10489", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10489", "abs": "https://arxiv.org/abs/2509.10489", "authors": ["Saptarshi Purkayastha", "Hrishikesh Bhagwat", "Keerthika Sunchu", "Orlando Hoilett", "Eddy Odari", "Reuben Thuo", "Martin Wafula", "Celia Kariuki", "Sherri Bucher"], "title": "Development of AI-integrated infrastructure with biomedical device and mobile app for neonatal vital monitoring during and in between kangaroo care sessions", "comment": "Presented at EMBC 2025, July 14-17, 2025", "summary": "Premature infant mortality remains a critical challenge in low- and\nmiddle-income countries (LMICs), with continuous vital sign monitoring being\nessential for early detection of life-threatening conditions. This paper\npresents an integrated system combining NeoWarm, a novel biomedical device,\nwith NeoRoo, a mobile application, and NeoSmartML, a machine learning\ninfrastructure, to enable comprehensive vital sign monitoring during Kangaroo\nMother Care (KMC). Our power-optimized device achieves 6-6.5 days of continuous\noperation on a single charge, while the mobile application implements an\noffline-first architecture with efficient data synchronization. The optical\ncharacter recognition pipeline demonstrates promising accuracy (F1 scores\n0.78-0.875) for automated vital sign extraction from existing NICU monitors.\nExperimental validation shows the system's feasibility for deployment in\nresource-constrained settings, though further optimization of heart rate and\ntemperature detection, along with the risk classification foundation model is\nneeded."}
{"id": "2509.11168", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11168", "abs": "https://arxiv.org/abs/2509.11168", "authors": ["Peihong Zhang", "Yuxuan Liu", "Zhixin Li", "Rui Sang", "Yiqiang Cai", "Yizhou Tan", "Shengchen Li"], "title": "An Entropy-Guided Curriculum Learning Strategy for Data-Efficient Acoustic Scene Classification under Domain Shift", "comment": "Accepted at the Detection and Classification of Acoustic Scenes and\n  Events (DCASE) Workshop 2025", "summary": "Acoustic Scene Classification (ASC) faces challenges in generalizing across\nrecording devices, particularly when labeled data is limited. The DCASE 2024\nChallenge Task 1 highlights this issue by requiring models to learn from small\nlabeled subsets recorded on a few devices. These models need to then generalize\nto recordings from previously unseen devices under strict complexity\nconstraints. While techniques such as data augmentation and the use of\npre-trained models are well-established for improving model generalization,\noptimizing the training strategy represents a complementary yet less-explored\npath that introduces no additional architectural complexity or inference\noverhead. Among various training strategies, curriculum learning offers a\npromising paradigm by structuring the learning process from easier to harder\nexamples. In this work, we propose an entropy-guided curriculum learning\nstrategy to address the domain shift problem in data-efficient ASC.\nSpecifically, we quantify the uncertainty of device domain predictions for each\ntraining sample by computing the Shannon entropy of the device posterior\nprobabilities estimated by an auxiliary domain classifier. Using entropy as a\nproxy for domain invariance, the curriculum begins with high-entropy samples\nand gradually incorporates low-entropy, domain-specific ones to facilitate the\nlearning of generalizable representations. Experimental results on multiple\nDCASE 2024 ASC baselines demonstrate that our strategy effectively mitigates\ndomain shift, particularly under limited labeled data conditions. Our strategy\nis architecture-agnostic and introduces no additional inference cost, making it\neasily integrable into existing ASC baselines and offering a practical solution\nto domain shift."}
{"id": "2509.10951", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10951", "abs": "https://arxiv.org/abs/2509.10951", "authors": ["Kevin Wilkinghoff", "Haici Yang", "Janek Ebbers", "François G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "Local Density-Based Anomaly Score Normalization for Domain Generalization", "comment": null, "summary": "State-of-the-art anomalous sound detection (ASD) systems in domain-shifted\nconditions rely on projecting audio signals into an embedding space and using\ndistance-based outlier detection to compute anomaly scores. One of the major\ndifficulties to overcome is the so-called domain mismatch between the anomaly\nscore distributions of a source domain and a target domain that differ\nacoustically and in terms of the amount of training data provided. A decision\nthreshold that is optimal for one domain may be highly sub-optimal for the\nother domain and vice versa. This significantly degrades the performance when\nonly using a single decision threshold, as is required when generalizing to\nmultiple data domains that are possibly unseen during training while still\nusing the same trained ASD system as in the source domain. To reduce this\nmismatch between the domains, we propose a simple local-density-based anomaly\nscore normalization scheme. In experiments conducted on several ASD datasets,\nwe show that the proposed normalization scheme consistently improves\nperformance for various types of embedding-based ASD systems and yields better\nresults than existing anomaly score normalization approaches."}
{"id": "2509.10490", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.10490", "abs": "https://arxiv.org/abs/2509.10490", "authors": ["Yuwen Cao", "Guijun Liu", "Tomoaki Ohtsuki", "Howard H. Yang", "Tony Q. S. Quek"], "title": "Distributed Gossip-GAN for Low-overhead CSI Feedback Training in FDD mMIMO-OFDM Systems", "comment": null, "summary": "The deep autoencoder (DAE) framework has turned out to be efficient in\nreducing the channel state information (CSI) feedback overhead in massive\nmultiple-input multipleoutput (mMIMO) systems. However, these DAE approaches\npresented in prior works rely heavily on large-scale data collected through the\nbase station (BS) for model training, thus rendering excessive bandwidth usage\nand data privacy issues, particularly for mMIMO systems. When considering\nusers' mobility and encountering new channel environments, the existing CSI\nfeedback models may often need to be retrained. Returning back to previous\nenvironments, however, will make these models perform poorly and face the risk\nof catastrophic forgetting. To solve the above challenging problems, we propose\na novel gossiping generative adversarial network (Gossip-GAN)-aided CSI\nfeedback training framework. Notably, Gossip-GAN enables the CSI feedback\ntraining with low-overhead while preserving users' privacy. Specially, each\nuser collects a small amount of data to train a GAN model. Meanwhile, a fully\ndistributed gossip-learning strategy is exploited to avoid model overfitting,\nand to accelerate the model training as well. Simulation results demonstrate\nthat Gossip-GAN can i) achieve a similar CSI feedback accuracy as centralized\ntraining with real-world datasets, ii) address catastrophic forgetting\nchallenges in mobile scenarios, and iii) greatly reduce the uplink bandwidth\nusage. Besides, our results show that the proposed approach possesses an\ninherent robustness."}
{"id": "2509.11183", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11183", "abs": "https://arxiv.org/abs/2509.11183", "authors": ["Emmanouil Karystinaios"], "title": "WeaveMuse: An Open Agentic System for Multimodal Music Understanding and Generation", "comment": "Accepted at Large Language Models for Music & Audio Workshop (LLM4MA)\n  2025", "summary": "Agentic AI has been standardized in industry as a practical paradigm for\ncoordinating specialized models and tools to solve complex multimodal tasks. In\nthis work, we present WeaveMuse, a multi-agent system for music understanding,\nsymbolic composition, and audio synthesis. Each specialist agent interprets\nuser requests, derives machine-actionable requirements (modalities, formats,\nconstraints), and validates its own outputs, while a manager agent selects and\nsequences tools, mediates user interaction, and maintains state across turns.\nThe system is extendable and deployable either locally, using quantization and\ninference strategies to fit diverse hardware budgets, or via the HFApi to\npreserve free community access to open models. Beyond out-of-the-box use, the\nsystem emphasizes controllability and adaptation through constraint schemas,\nstructured decoding, policy-based inference, and parameter-efficient adapters\nor distilled variants that tailor models to MIR tasks. A central design goal is\nto facilitate intermodal interaction across text, symbolic notation and\nvisualization, and audio, enabling analysis-synthesis-render loops and\naddressing cross-format constraints. The framework aims to democratize,\nimplement, and make accessible MIR tools by supporting interchangeable\nopen-source models of various sizes, flexible memory management, and\nreproducible deployment paths."}
{"id": "2509.11084", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.11084", "abs": "https://arxiv.org/abs/2509.11084", "authors": ["Hyeongju Kim", "Juheon Lee", "Jinhyeok Yang", "Jacob Morton"], "title": "Length-Aware Rotary Position Embedding for Text-Speech Alignment", "comment": "5 pages, 3 figures, preprint", "summary": "Many recent text-to-speech (TTS) systems are built on transformer\narchitectures and employ cross-attention mechanisms for text-speech alignment.\nWithin these systems, rotary position embedding (RoPE) is commonly used to\nencode positional information in text and speech representations. In this work,\nwe introduce length-aware RoPE (LARoPE), a simple yet effective extension of\nRoPE that improves text-speech alignment. Unlike RoPE, which relies on absolute\nindices, LARoPE computes relative distances between query and key positions\nusing length-normalized indices. Experimental results show that LARoPE\nconsistently outperforms RoPE, offering faster loss convergence, more accurate\ntext-speech alignment, and higher overall TTS quality. Furthermore, LARoPE\ndemonstrates greater resilience to variations in utterance duration and\nmaintains stable performance in extended speech generation up to 30 seconds,\nwhereas RoPE suffers from notable degradation. Notably, our method achieves a\nstate-of-the-art word error rate on a standard zero-shot TTS benchmark."}
{"id": "2509.10491", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10491", "abs": "https://arxiv.org/abs/2509.10491", "authors": ["Vitalii Bondar", "Serhii Semenov", "Vira Babenko", "Dmytro Holovniak"], "title": "FlowECG: Using Flow Matching to Create a More Efficient ECG Signal Generator", "comment": "8 pages, 2 figures, 1 table, reviewed version will be published in\n  \"Sensors, Devices and Systems 2025 Proceedings\" (Springer's Lecture Notes in\n  Electrical Engineering)", "summary": "Synthetic electrocardiogram generation serves medical AI applications\nrequiring privacy-preserving data sharing and training dataset augmentation.\nCurrent diffusion-based methods achieve high generation quality but require\nhundreds of neural network evaluations during sampling, creating computational\nbottlenecks for clinical deployment. We propose FlowECG, a flow matching\napproach that adapts the SSSD-ECG architecture by replacing the iterative\ndiffusion process with continuous flow dynamics. Flow matching learns direct\ntransport paths from noise to data distributions through ordinary differential\nequation solving. We evaluate our method on the PTB-XL dataset using Dynamic\nTime Warping, Wasserstein distance, Maximum Mean Discrepancy, and spectral\nsimilarity metrics. FlowECG matches SSSD-ECG performance at 200 neural function\nevaluations, outperforming the baseline on three metrics. The key finding shows\nthat FlowECG maintains generation quality with substantially fewer sampling\nsteps, achieving comparable results with 10-25 evaluations compared to 200 for\ndiffusion methods. This efficiency improvement reduces computational\nrequirements by an order of magnitude while preserving physiologically\nrealistic 12-lead ECG characteristics. The approach enables practical\ndeployment in resource-limited clinical settings where real-time generation or\nlarge-scale synthetic data creation is needed."}
{"id": "2509.11241", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11241", "abs": "https://arxiv.org/abs/2509.11241", "authors": ["Satyajeet Prabhu"], "title": "Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches", "comment": null, "summary": "Beat and downbeat tracking, jointly referred to as Meter Tracking, is a\nfundamental task in Music Information Retrieval (MIR). Deep learning models\nhave far surpassed traditional signal processing and classical machine learning\napproaches in this domain, particularly for Western (Eurogenetic) genres, where\nlarge annotated datasets are widely available. These systems, however, perform\nless reliably on underrepresented musical traditions. Carnatic music, a rich\ntradition from the Indian subcontinent, is renowned for its rhythmic intricacy\nand unique metrical structures (t\\=alas). The most notable prior work on meter\ntracking in this context employed probabilistic Dynamic Bayesian Networks\n(DBNs). The performance of state-of-the-art (SOTA) deep learning models on\nCarnatic music, however, remains largely unexplored.\n  In this study, we evaluate two models for meter tracking in Carnatic music:\nthe Temporal Convolutional Network (TCN), a lightweight architecture that has\nbeen successfully adapted for Latin rhythms, and Beat This!, a\ntransformer-based model designed for broad stylistic coverage without the need\nfor post-processing. Replicating the experimental setup of the DBN baseline on\nthe Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess the\nperformance of these models in a directly comparable setting. We further\ninvestigate adaptation strategies, including fine-tuning the models on Carnatic\ndata and the use of musically informed parameters. Results show that while\noff-the-shelf models do not always outperform the DBN, their performance\nimproves substantially with transfer learning, matching or surpassing the\nbaseline. These findings indicate that SOTA deep learning models can be\neffectively adapted to underrepresented traditions, paving the way for more\ninclusive and broadly applicable meter tracking systems."}
{"id": "2509.11957", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11957", "abs": "https://arxiv.org/abs/2509.11957", "authors": ["Wen-Yung Wu", "Pei-Chin Hsieh", "Tai-Shih Chi"], "title": "EEND-SAA: Enrollment-Less Main Speaker Voice Activity Detection Using Self-Attention Attractors", "comment": null, "summary": "Voice activity detection (VAD) is essential in speech-based systems, but\ntraditional methods detect only speech presence without identifying speakers.\nTarget-speaker VAD (TS-VAD) extends this by detecting the speech of a known\nspeaker using a short enrollment utterance, but this assumption fails in\nopen-domain scenarios such as meetings or customer service calls, where the\nmain speaker is unknown. We propose EEND-SAA, an enrollment-less,\nstreaming-compatible framework for main-speaker VAD, which identifies the\nprimary speaker without prior knowledge. Unlike TS-VAD, our method determines\nthe main speaker as the one who talks more steadily and clearly, based on\nspeech continuity and volume. We build our model on EEND using two\nself-attention attractors in a Transformer and apply causal masking for\nreal-time use. Experiments on multi-speaker LibriSpeech mixtures show that\nEEND-SAA reduces main-speaker DER from 6.63% to 3.61% and improves F1 from\n0.9667 to 0.9818 over the SA-EEND baseline, achieving state-of-the-art\nperformance under conditions involving speaker overlap and noise."}
{"id": "2509.10666", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10666", "abs": "https://arxiv.org/abs/2509.10666", "authors": ["Chongjun Ouyang", "Hao Jiang", "Zhaolin Wang", "Yuanwei Liu", "Zhiguo Ding"], "title": "Uplink and Downlink Communications in Segmented Waveguide-Enabled Pinching-Antenna Systems (SWANs)", "comment": "Submitted to IEEE journal", "summary": "A segmented waveguide-enabled pinching-antenna system (SWAN) is proposed, in\nwhich a segmented waveguide composed of multiple short dielectric waveguide\nsegments is employed to radiate or receive signals through the pinching\nantennas (PAs) deployed on each segment. Based on this architecture, three\npractical operating protocols are proposed: segment selection (SS), segment\naggregation (SA), and segment multiplexing (SM). For uplink SWAN\ncommunications, where one PA is activated per segment, the segmented structure\neliminates the inter-antenna radiation effect, i.e., signals captured by one PA\nmay re-radiate through other PAs along the same waveguide. This yields a\ntractable and physically consistent uplink signal model for a multi-PA\npinching-antenna system (PASS), which has not been established for conventional\nPASS using a single long waveguide. Building on this model, PA placement\nalgorithms are proposed to maximize the uplink signal-to-noise ratio (SNR).\nClosed-form expressions for the received SNR under the three protocols are\nderived, and the corresponding scaling laws with respect to the number of\nsegments are analyzed. It is proven that the segmented architecture reduces\nboth the average PA-to-user distance and the PA-to-feed distance, thereby\nmitigating both large-scale path loss and in-waveguide propagation loss. These\nresults are extended to downlink SWAN communications, where multiple PAs are\nactivated per segment, and PA placement methods are proposed to maximize the\ndownlink received SNR under the three protocols. Numerical results demonstrate\nthat: \\romannumeral1) among the three protocols, SM achieves the best\nperformance, followed by SA and then SS; and \\romannumeral2) for all protocols,\nthe proposed SWAN achieves a higher SNR than conventional PASS with a single\nlong waveguide in both uplink and downlink scenarios."}
{"id": "2509.11425", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11425", "abs": "https://arxiv.org/abs/2509.11425", "authors": ["Md Mubtasim Ahasan", "Rafat Hasan Khan", "Tasnim Mohiuddin", "Aman Chadha", "Tariq Iqbal", "M Ashraful Amin", "Amin Ahsan Ali", "Md Mofijul Islam", "A K M Mahbubur Rahman"], "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs", "comment": null, "summary": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec."}
{"id": "2509.10566", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10566", "abs": "https://arxiv.org/abs/2509.10566", "authors": ["Sergio Poo Hernandez", "Vadim Bulitko", "Erin Bayne"], "title": "Combining Audio and Non-Audio Inputs in Evolved Neural Networks for Ovenbird", "comment": null, "summary": "In the last several years the use of neural networks as tools to automate\nspecies classification from digital data has increased. This has been due in\npart to the high classification accuracy of image classification through\nConvolutional Neural Networks (CNN). In the case of audio data CNN based\nrecognizers are used to automate the classification of species in audio\nrecordings by using information from sound visualization (i.e., spectrograms).\nIt is common for these recognizers to use the spectrogram as their sole input.\nHowever, researchers have other non-audio data, such as habitat preferences of\na species, phenology, and range information, available that could improve\nspecies classification. In this paper we present how a single-species\nrecognizer neural network's accuracy can be improved by using non-audio data as\ninputs in addition to spectrogram information. We also analyze if the\nimprovements are merely a result of having a neural network with a higher\nnumber of parameters instead of combining the two inputs. We find that networks\nthat use the two different inputs have a higher classification accuracy than\nnetworks of similar size that use only one of the inputs."}
{"id": "2509.10752", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10752", "abs": "https://arxiv.org/abs/2509.10752", "authors": ["Minseok Kim", "Masato Yomoda", "Minghe Mao", "Nobuaki Kuno", "Koshiro Kitao", "Satoshi Suyama"], "title": "Quasi-Deterministic Modeling of Sub-THz Band Access Channels in Street Canyon Environments", "comment": null, "summary": "Sub-terahertz (sub-THz) frequencies (100--300 GHz) are expected to play a key\nrole in beyond-5G and 6G mobile networks. However, their quasi-optical\npropagation characteristics require new channel models beyond sub-100 GHz\nextrapolations. This paper presents an extensive double-directional (D-D)\nchannel measurement campaign conducted in an outdoor street-canyon environment\nat 154 GHz and 300 GHz under both line-of-sight (LoS) and non-line-of-sight\n(NLoS) conditions using an in-house-developed channel sounder. Based on these\nmeasurements, clustering with merged datasets across the two frequencies\nenables comparative analyses that identify both common and distinct multipath\nclusters, as well as the frequency dependence of cluster-level characteristics.\nA quasi-deterministic (QD) channel model is then proposed, combining\ndeterministic components, such as LoS and single-bounce reflections from side\nwalls, with random components. Large-scale parameters (path loss, delay spread,\nangular spread, and Rician K-factor) are also evaluated. These results provide\nvaluable insights into sub-THz propagation in urban street canyons and\ncontribute toward the development of accurate, channel models for future 6G\nsystems."}
{"id": "2509.11474", "categories": ["cs.SD", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.11474", "abs": "https://arxiv.org/abs/2509.11474", "authors": ["Weilun Xu", "Tianhao Dai", "Oscar Goudet", "Xiaoxuan Wang"], "title": "Acoustic Overspecification in Electronic Dance Music Taxonomy", "comment": "5 pages, 3 figures, conference paper", "summary": "Electronic Dance Music (EDM) classification typically relies on\nindustry-defined taxonomies with numerous subgenres, yet the acoustic basis for\nthese distinctions remains unclear. Current approaches use supervised learning\nwith prescribed genre labels, assuming their validity without systematic\nevaluation. In this paper, we propose an unsupervised approach to discover the\nnatural acoustic structure of EDM independent of commercial labels. Our method\ncombines novel tempogram-based features capturing EDM's layered rhythmic\npatterns with multi-criteria feature selection. To validate that our findings\nreflect genuine acoustic structure rather than methodological artifacts, we\ncompare our results against state-of-the-art pre-trained audio embeddings (MERT\nand CLAP). Both our feature space and embedding representations converge to\n19-23 natural acoustic families compared to the prescribed 35, providing\nconsistent evidence of significant overspecification in current EDM taxonomy by\napproximately one-third."}
{"id": "2509.11183", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11183", "abs": "https://arxiv.org/abs/2509.11183", "authors": ["Emmanouil Karystinaios"], "title": "WeaveMuse: An Open Agentic System for Multimodal Music Understanding and Generation", "comment": "Accepted at Large Language Models for Music & Audio Workshop (LLM4MA)\n  2025", "summary": "Agentic AI has been standardized in industry as a practical paradigm for\ncoordinating specialized models and tools to solve complex multimodal tasks. In\nthis work, we present WeaveMuse, a multi-agent system for music understanding,\nsymbolic composition, and audio synthesis. Each specialist agent interprets\nuser requests, derives machine-actionable requirements (modalities, formats,\nconstraints), and validates its own outputs, while a manager agent selects and\nsequences tools, mediates user interaction, and maintains state across turns.\nThe system is extendable and deployable either locally, using quantization and\ninference strategies to fit diverse hardware budgets, or via the HFApi to\npreserve free community access to open models. Beyond out-of-the-box use, the\nsystem emphasizes controllability and adaptation through constraint schemas,\nstructured decoding, policy-based inference, and parameter-efficient adapters\nor distilled variants that tailor models to MIR tasks. A central design goal is\nto facilitate intermodal interaction across text, symbolic notation and\nvisualization, and audio, enabling analysis-synthesis-render loops and\naddressing cross-format constraints. The framework aims to democratize,\nimplement, and make accessible MIR tools by supporting interchangeable\nopen-source models of various sizes, flexible memory management, and\nreproducible deployment paths."}
{"id": "2509.10770", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10770", "abs": "https://arxiv.org/abs/2509.10770", "authors": ["Lei Lyu", "Urbashi Mitra"], "title": "Hybrid Atomic Norm Sparse/Diffuse Channel Estimation", "comment": null, "summary": "In this paper, the hybrid sparse/diffuse (HSD) channel model in frequency\ndomain is proposed. Based on the structural analysis on the resolvable paths\nand diffuse scattering statistics in the channel, the Hybrid\nAtomic-Least-Squares (HALS) algorithm is designed to estimate sparse/diffuse\ncomponents with a combined atomic and l2 regularization. A theoretical analysis\nis conducted on the Lagrangian dual problem and the conditions needed to be\nsatisfied by primal and dual solutions are provided. This analysis, in turn,\nsuggests an algorithm for optimal frequency support estimation. Debiased\nmethods for improved channel estimation are provided. Given differing amounts\nof side information, performance bounds are derived in terms of a genie-aided\nestimator and constrained Cramer-Rao lower bounds (CRLB). Numerical results via\nsimulations on synthetic data as well as real experimental data validate the\nefficacy of the proposed method. There are clear tradeoffs with respect to the\nproperties of the channel with respect to performance: sparsity of specular\npaths and relative energy of diffuse components."}
{"id": "2509.11606", "categories": ["cs.SD", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11606", "abs": "https://arxiv.org/abs/2509.11606", "authors": ["Milan Marocchi", "Matthew Fynn", "Kayapanda Mandana", "Yue Rong"], "title": "Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals", "comment": "35 pages, 37 figures, 19 tables", "summary": "Cardiovascular diseases (CVDs) are the leading cause of death worldwide,\naccounting for approximately 17.9 million deaths each year. Early detection is\ncritical, creating a demand for accurate and inexpensive pre-screening methods.\nDeep learning has recently been applied to classify abnormal heart sounds\nindicative of CVDs using synchronised phonocardiogram (PCG) and\nelectrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However,\nstate-of-the-art architectures remain underutilised due to the limited\navailability of synchronised and multichannel datasets. Augmented datasets and\npre-trained models provide a pathway to overcome these limitations, enabling\ntransformer-based architectures to be trained effectively. This work combines\ntraditional signal processing with denoising diffusion models, WaveGrad and\nDiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based\nclassifier on multimodal and multichannel heart sound datasets. The approach\nachieves state-of-the-art performance. On the Computing in Cardiology (CinC)\n2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR),\nsensitivity, specificity and Matthew's correlation coefficient (MCC) reach\n92.48\\%, 93.05\\%, 93.63\\%, 92.48\\%, 94.93\\% and 0.8283, respectively. Using the\nsynchronised PCG and ECG signals of the training-a dataset from CinC, 93.14\\%,\n92.21\\%, 94.35\\%, 90.10\\%, 95.12\\% and 0.8380 are achieved for accuracy, UAR,\nsensitivity, specificity and MCC, respectively. Using a wearable vest dataset\nconsisting of mPCG data, the model achieves 77.13\\% accuracy, 74.25\\% UAR,\n86.47\\% sensitivity, 62.04\\% specificity, and 0.5082 MCC. These results\ndemonstrate the effectiveness of transformer-based models for CVD detection\nwhen supported by augmented datasets, highlighting their potential to advance\nmultimodal and multichannel heart sound classification."}
{"id": "2509.11241", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11241", "abs": "https://arxiv.org/abs/2509.11241", "authors": ["Satyajeet Prabhu"], "title": "Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches", "comment": null, "summary": "Beat and downbeat tracking, jointly referred to as Meter Tracking, is a\nfundamental task in Music Information Retrieval (MIR). Deep learning models\nhave far surpassed traditional signal processing and classical machine learning\napproaches in this domain, particularly for Western (Eurogenetic) genres, where\nlarge annotated datasets are widely available. These systems, however, perform\nless reliably on underrepresented musical traditions. Carnatic music, a rich\ntradition from the Indian subcontinent, is renowned for its rhythmic intricacy\nand unique metrical structures (t\\=alas). The most notable prior work on meter\ntracking in this context employed probabilistic Dynamic Bayesian Networks\n(DBNs). The performance of state-of-the-art (SOTA) deep learning models on\nCarnatic music, however, remains largely unexplored.\n  In this study, we evaluate two models for meter tracking in Carnatic music:\nthe Temporal Convolutional Network (TCN), a lightweight architecture that has\nbeen successfully adapted for Latin rhythms, and Beat This!, a\ntransformer-based model designed for broad stylistic coverage without the need\nfor post-processing. Replicating the experimental setup of the DBN baseline on\nthe Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess the\nperformance of these models in a directly comparable setting. We further\ninvestigate adaptation strategies, including fine-tuning the models on Carnatic\ndata and the use of musically informed parameters. Results show that while\noff-the-shelf models do not always outperform the DBN, their performance\nimproves substantially with transfer learning, matching or surpassing the\nbaseline. These findings indicate that SOTA deep learning models can be\neffectively adapted to underrepresented traditions, paving the way for more\ninclusive and broadly applicable meter tracking systems."}
{"id": "2509.10831", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10831", "abs": "https://arxiv.org/abs/2509.10831", "authors": ["Maya Mekel", "Vered Karp", "Satish Mulleti", "Alejandro Cohen"], "title": "Self-Calibrating Integrate-and-Fire Time Encoding Machine", "comment": "7 pages, 3 figures", "summary": "In this paper, we introduce a novel self-calibrating integrate-and-fire time\nencoding machine (S-IF-TEM) that enables simultaneous parameter estimation and\nsignal reconstruction during sampling, thereby effectively mitigating mismatch\neffects. The proposed framework is developed over a new practical IF-TEM\n(P-IF-TEM) setting, which extends classical models by incorporating device\nmismatches and imperfections that can otherwise lead to significant\nreconstruction errors. Unlike existing IF-TEM settings, P-IF-TEM accounts for\nscenarios where (i) system parameters are inaccurately known and may vary over\ntime, (ii) the integrator discharge time after firings can vary, and (iii) the\nsampler may operate in its nonlinear region under large input dynamic ranges.\nFor this practical model, we derive sampling rate bounds and reconstruction\nconditions that ensure perfect recovery. Analytical results establish the\nconditions for perfect reconstruction under self-calibration, and evaluation\nstudies demonstrate substantial improvements - exceeding 59dB - highlighting\nthe effectiveness of the proposed approach."}
{"id": "2509.11717", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11717", "abs": "https://arxiv.org/abs/2509.11717", "authors": ["Adhiraj Banerjee", "Vipul Arora"], "title": "Neural Audio Codecs for Prompt-Driven Universal Source Separation", "comment": "21 pages, 1 figure, pre-print, under review", "summary": "Text-guided source separation supports flexible audio editing across media\nand assistive applications, but existing models like AudioSep are too\ncompute-heavy for edge deployment. Neural audio codec (NAC) models such as\nCodecFormer and SDCodec are compute-efficient but limited to fixed-class\nseparation. We introduce CodecSep, the first NAC-based model for on-device\nuniversal, text-driven separation. CodecSep combines DAC compression with a\nTransformer masker modulated by CLAP-derived FiLM parameters. Across six\nopen-domain benchmarks under matched training/prompt protocols,\n\\textbf{CodecSep} surpasses \\textbf{AudioSep} in separation fidelity (SI-SDR)\nwhile remaining competitive in perceptual quality (ViSQOL) and matching or\nexceeding fixed-stem baselines (TDANet, CodecFormer, SDCodec). In code-stream\ndeployments, it needs just 1.35~GMACs end-to-end -- approximately $54\\times$\nless compute ($25\\times$ architecture-only) than spectrogram-domain separators\nlike AudioSep -- while remaining fully bitstream-compatible."}
{"id": "2509.11425", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.11425", "abs": "https://arxiv.org/abs/2509.11425", "authors": ["Md Mubtasim Ahasan", "Rafat Hasan Khan", "Tasnim Mohiuddin", "Aman Chadha", "Tariq Iqbal", "M Ashraful Amin", "Amin Ahsan Ali", "Md Mofijul Islam", "A K M Mahbubur Rahman"], "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs", "comment": null, "summary": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec."}
{"id": "2509.10834", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.10834", "abs": "https://arxiv.org/abs/2509.10834", "authors": ["Xiao Liang", "Zhen Qin", "Zhihui Zhu", "Shuang Li"], "title": "Landscape Analysis of Simultaneous Blind Deconvolution and Phase Retrieval via Structured Low-Rank Tensor Recovery", "comment": "17 pages, 18 figures", "summary": "This paper presents a geometric analysis of the simultaneous blind\ndeconvolution and phase retrieval (BDPR) problem via a structured low-rank\ntensor recovery framework. Due to the highly complicated structure of the\nassociated sensing tensor, directly characterizing its optimization landscape\nis intractable. To address this, we introduce a tensor sensing problem as a\ntractable surrogate that preserves the essential structural features of the\ntarget low-rank tensor while enabling rigorous theoretical analysis. As a first\nstep toward understanding this surrogate model, we study the corresponding\npopulation risk, which captures key aspects of the underlying low-rank tensor\nstructure. We characterize the global landscape of the population risk on the\nunit sphere and show that Riemannian gradient descent (RGD) converges linearly\nunder mild conditions. We then extend the analysis to the tensor sensing\nproblem, establishing local geometric properties, proving convergence\nguarantees for RGD, and quantifying robustness under measurement noise. Our\ntheoretical results are further supported by extensive numerical experiments.\nThese findings offer foundational insights into the optimization landscape of\nthe structured low-rank tensor recovery problem, which equivalently\ncharacterizes the original BDPR problem, thereby providing principled guidance\nfor solving the original BDPR problem."}
{"id": "2509.11976", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.11976", "abs": "https://arxiv.org/abs/2509.11976", "authors": ["Dinghao Zou", "Yicheng Gong", "Xiaokang Li", "Xin Cao", "Sunbowen Lee"], "title": "PoolingVQ: A VQVAE Variant for Reducing Audio Redundancy and Boosting Multi-Modal Fusion in Music Emotion Analysis", "comment": null, "summary": "Multimodal music emotion analysis leverages audio and MIDI modalities to\nenhance performance. While mainstream approaches focus on complex feature\nextraction networks, we posit that shortening the length of audio sequence\nfeatures to mitigate redundancy, especially in contrast to MIDI's compact\nrepresentation, may effectively boost task performance. To achieve this, we\ndeveloped PoolingVQ by combining Vector Quantized Variational Autoencoder\n(VQVAE) with spatial pooling, which directly compresses audio feature sequences\nthrough local aggregation to reduce redundancy, then devised a two-stage\nco-attention approach to fuse audio and MIDI information. Experimental results\non the public datasets EMOPIA and VGMIDI demonstrate that our multimodal\nframework achieves state-of-the-art overall performance, with PoolingVQ\nyielding some improvement."}
{"id": "2509.10857", "categories": ["eess.SP", "physics.chem-ph", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.10857", "abs": "https://arxiv.org/abs/2509.10857", "authors": ["Hugues Kouakou", "José Henrique de Morais Goulart", "Raffaele Vitale", "Thomas Oberlin", "David Rousseau", "Cyril Ruckebusch", "Nicolas Dobigeon"], "title": "Online simplex-structured matrix factorization", "comment": null, "summary": "Simplex-structured matrix factorization (SSMF) is a common task encountered\nin signal processing and machine learning. Minimum-volume constrained unmixing\n(MVCU) algorithms are among the most widely used methods to perform this task.\nWhile MVCU algorithms generally perform well in an offline setting, their\ndirect application to online scenarios suffers from scalability limitations due\nto memory and computational demands. To overcome these limitations, this paper\nproposes an approach which can build upon any off-the-shelf MVCU algorithm to\noperate sequentially, i.e., to handle one observation at a time. The key idea\nof the proposed method consists in updating the solution of MVCU only when\nnecessary, guided by an online check of the corresponding optimization problem\nconstraints. It only stores and processes observations identified as\ninformative with respect to the geometrical constraints underlying SSMF. We\ndemonstrate the effectiveness of the approach when analyzing synthetic and real\ndatasets, showing that it achieves estimation accuracy comparable to the\noffline MVCU method upon which it relies, while significantly reducing the\ncomputational cost."}
{"id": "2509.12003", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12003", "abs": "https://arxiv.org/abs/2509.12003", "authors": ["Pierre Serrano", "Raphaël Duroselle", "Florian Angulo", "Jean-François Bonastre", "Olivier Boeffard"], "title": "Improving Out-of-Domain Audio Deepfake Detection via Layer Selection and Fusion of SSL-Based Countermeasures", "comment": null, "summary": "Audio deepfake detection systems based on frozen pre-trained self-supervised\nlearning (SSL) encoders show a high level of performance when combined with\nlayer-weighted pooling methods, such as multi-head factorized attentive pooling\n(MHFA). However, they still struggle to generalize to out-of-domain (OOD)\nconditions. We tackle this problem by studying the behavior of six different\npre-trained SSLs, on four different test corpora. We perform a layer-by-layer\nanalysis to determine which layers contribute most. Next, we study the pooling\nhead, comparing a strategy based on a single layer with automatic selection via\nMHFA. We observed that selecting the best layer gave very good results, while\nreducing system parameters by up to 80%. A wide variation in performance as a\nfunction of test corpus and SSL model is also observed, showing that the\npre-training strategy of the encoder plays a role. Finally, score-level fusion\nof several encoders improved generalization to OOD attacks."}
{"id": "2509.10874", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10874", "abs": "https://arxiv.org/abs/2509.10874", "authors": ["Baskaran Sripathmanathan", "Xiaowen Dong", "Michael Bronstein"], "title": "On the Impact of Downstream Tasks on Sampling and Reconstructing Noisy Graph Signals", "comment": "This work has been accepted for publication at IEEE CAMSAP 2025", "summary": "We investigate graph signal reconstruction and sample selection for\nclassification tasks. We present general theoretical characterisations of\nclassification error applicable to multiple commonly used reconstruction\nmethods, and compare that to the classical reconstruction error. We demonstrate\nthe applicability of our results by using them to derive new optimal sampling\nmethods for linearized graph convolutional networks, and show improvement over\nother graph signal processing based methods."}
{"id": "2509.10706", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10706", "abs": "https://arxiv.org/abs/2509.10706", "authors": ["Chin-Yun Yu", "György Fazekas"], "title": "Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method", "comment": "Published at 2025 AES International Conference on Artificial\n  Intelligence and Machine Learning for Audio\n  (https://aes2.org/publications/elibrary-page/?id=22991)", "summary": "Automatic differentiation through digital signal processing algorithms for\nvirtual analogue modelling has recently gained popularity. These algorithms are\ntypically more computationally efficient than black-box neural networks that\nrely on dense matrix multiplications. Due to their differentiable nature, they\ncan be integrated with neural networks and jointly trained using gradient\ndescent algorithms, resulting in more efficient systems. Furthermore, signal\nprocessing algorithms have significantly fewer parameters than neural networks,\nallowing the application of the Newton-Raphson method. This method offers\nfaster and more robust convergence than gradient descent at the cost of\nquadratic storage. This paper presents a method to emulate analogue levelling\namplifiers using a feed-forward digital compressor with parameters optimised\nvia the Newton-Raphson method. We demonstrate that a digital compressor can\nsuccessfully approximate the behaviour of our target unit, the Teletronix\nLA-2A. Different strategies for computing the Hessian matrix are benchmarked.\nWe leverage parallel algorithms for recursive filters to achieve efficient\ntraining on modern GPUs. The resulting model is made into a VST plugin and is\nopen-sourced at https://github.com/aim-qmul/4a2a."}
{"id": "2509.10917", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10917", "abs": "https://arxiv.org/abs/2509.10917", "authors": ["Yekta Demirci", "Guillaume Mantelet", "Stéphane Martel", "Jean-François Frigon", "Gunes Karabulut Kurt"], "title": "Forecasting Self-Similar User Traffic Demand Using Transformers in LEO Satellite Networks", "comment": "6 pages", "summary": "In this paper, we propose the use of a transformer-based model to address the\nneed for forecasting user traffic demand in the next generation Low Earth Orbit\n(LEO) satellite networks. Considering a LEO satellite constellation, we present\nthe need to forecast the demand for the satellites in-orbit to utilize dynamic\nbeam-hopping in high granularity. We adopt a traffic dataset with second-order\nself-similar characteristics. Given this traffic dataset, the Fractional\nAuto-regressive Integrated Moving Average (FARIMA) model is considered a\nbenchmark forecasting solution. However, the constrained on-board processing\ncapabilities of LEO satellites, combined with the need to fit a new model for\neach input sequence due to the nature of FARIMA, motivate the investigation of\nalternative solutions. As an alternative, a pretrained probabilistic time\nseries model that utilizes transformers with a Prob-Sparse self-attention\nmechanism is considered. The considered solution is investigated under\ndifferent time granularities with varying sequence and prediction lengths.\nConcluding this paper, we provide extensive simulation results where the\ntransformer-based solution achieved up to six percent better forecasting\naccuracy on certain traffic conditions using mean squared error as the\nperformance indicator."}
{"id": "2509.10951", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.10951", "abs": "https://arxiv.org/abs/2509.10951", "authors": ["Kevin Wilkinghoff", "Haici Yang", "Janek Ebbers", "François G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "Local Density-Based Anomaly Score Normalization for Domain Generalization", "comment": null, "summary": "State-of-the-art anomalous sound detection (ASD) systems in domain-shifted\nconditions rely on projecting audio signals into an embedding space and using\ndistance-based outlier detection to compute anomaly scores. One of the major\ndifficulties to overcome is the so-called domain mismatch between the anomaly\nscore distributions of a source domain and a target domain that differ\nacoustically and in terms of the amount of training data provided. A decision\nthreshold that is optimal for one domain may be highly sub-optimal for the\nother domain and vice versa. This significantly degrades the performance when\nonly using a single decision threshold, as is required when generalizing to\nmultiple data domains that are possibly unseen during training while still\nusing the same trained ASD system as in the source domain. To reduce this\nmismatch between the domains, we propose a simple local-density-based anomaly\nscore normalization scheme. In experiments conducted on several ASD datasets,\nwe show that the proposed normalization scheme consistently improves\nperformance for various types of embedding-based ASD systems and yields better\nresults than existing anomaly score normalization approaches."}
{"id": "2509.10926", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.10926", "abs": "https://arxiv.org/abs/2509.10926", "authors": ["Ashish Patwari", "Ananya Pandey", "Aditya Dabade", "Priyadarshini Raiguru"], "title": "Design and Validation of a MATLAB-based GUI for Coarray Domain Analysis of Sparse Linear Arrays", "comment": "12 pages, 11 Figures, Currently Under Peer Review", "summary": "This work presents a first-of-its-kind graphical user interface (GUI)-based\nsimulator developed using MATLAB App designer for the comprehensive analysis of\nsparse linear arrays (SLAs) in the difference coarray (DCA) domain. Sparse\nsensor arrays have emerged as a critical solution in enhancing signal\ndetection, direction of arrival (DOA) estimation, and beamforming in fields\nsuch as wireless communication, radar, sonar, and integrated sensing systems.\nThey offer several advantages over traditional uniform arrays, including\nreduced system complexity, lower deployment costs, and improved mitigation of\nmutual coupling effects. The tool enables users to input array configurations,\ncompute DCAs, visualize weight function graphs, and assess the hole-free status\nof arrays, as applicable for coarray processing. Unlike conventional simulators\nthat focus on radiation pattern visualization (array pattern, main lobe and\nsidelobe characteristics, azimuth cut, rectangular view, polar view etc.), this\ntool addresses the behavior of SLAs from a coarray domain perspective.\nNumerical validations demonstrate the tool's correctness, effectiveness, and\nits potential to foster further research in sparse arrays. This simulator could\nalso be used as a teaching aid to drive home complicated topics and attract\nyoung minds towards the fascinating field of sparse array design."}
{"id": "2509.11084", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.11084", "abs": "https://arxiv.org/abs/2509.11084", "authors": ["Hyeongju Kim", "Juheon Lee", "Jinhyeok Yang", "Jacob Morton"], "title": "Length-Aware Rotary Position Embedding for Text-Speech Alignment", "comment": "5 pages, 3 figures, preprint", "summary": "Many recent text-to-speech (TTS) systems are built on transformer\narchitectures and employ cross-attention mechanisms for text-speech alignment.\nWithin these systems, rotary position embedding (RoPE) is commonly used to\nencode positional information in text and speech representations. In this work,\nwe introduce length-aware RoPE (LARoPE), a simple yet effective extension of\nRoPE that improves text-speech alignment. Unlike RoPE, which relies on absolute\nindices, LARoPE computes relative distances between query and key positions\nusing length-normalized indices. Experimental results show that LARoPE\nconsistently outperforms RoPE, offering faster loss convergence, more accurate\ntext-speech alignment, and higher overall TTS quality. Furthermore, LARoPE\ndemonstrates greater resilience to variations in utterance duration and\nmaintains stable performance in extended speech generation up to 30 seconds,\nwhereas RoPE suffers from notable degradation. Notably, our method achieves a\nstate-of-the-art word error rate on a standard zero-shot TTS benchmark."}
{"id": "2509.11081", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11081", "abs": "https://arxiv.org/abs/2509.11081", "authors": ["Yifan Ye", "Bin Chen", "Xiang Li", "Yi Lei", "Zhiwei Liang", "Qingqing Hu", "Can Zhao", "Yanni Ou"], "title": "Experimental Demonstration of Rate-Adaptation via Hybrid Polar-BCH Product Code for Flexible PON", "comment": "4 Pages,2 figures", "summary": "The flexible-rate Polar-BCH product codes are experimentally demonstrated in\na coherent passive optical network system with 16QAM for the first time. Using\na new hybrid soft- and hard-decision decoder, we achieve a power gain of upto\n1.75 dB over traditional BCH-BCH product codes after 48 km transmission."}
{"id": "2509.11117", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11117", "abs": "https://arxiv.org/abs/2509.11117", "authors": ["Haoyu Wang", "Jiawei Hu", "Jiaqi Xu", "Ying Ju", "A. Lee Swindlehurst"], "title": "Nonreciprocal RIS-Aided Covert Channel Reciprocity Attacks and Countermeasures", "comment": "submitted to IEEE Trans for review", "summary": "Reconfigurable intelligent surface (RIS) technology enhances wireless\ncommunication performance, but it also introduces new vulnerabilities that can\nbe exploited by adversaries. This paper investigates channel reciprocity attack\n(CRACK) threats in multi-antenna wireless systems operating in time-division\nduplexing mode using a physically consistent non-reciprocal RIS (NR-RIS) model.\nCRACK can degrade communication rate and facilitate passive eavesdropping\nbehavior by distorting the downlink precoding, without requiring any additional\nsignal transmission or channel state information (CSI). Unlike conventional RIS\njamming strategies, the NR-RIS does not need synchronization with the\nlegitimate system and thus can operate with slow or fixed configurations to\nimplement CRACK, obscuring the distinction between the direct and RIS-induced\nchannels and thereby complicating corresponding defensive precoding designs. To\ncounter the CRACK threat posed by NR-RIS, we develop ``SecureCoder,'' a deep\nreinforcement learning-based framework that can mitigate CRACK and determine an\nimproved downlink precoder matrix using the estimated uplink CSI and rate\nfeedback from the users. Simulation results demonstrate the severe performance\ndegradation caused by NR-RIS CRACK and validate the effectiveness of\nSecureCoder in improving both throughput and reducing security threats, thereby\nenhancing system robustness."}
{"id": "2509.11193", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11193", "abs": "https://arxiv.org/abs/2509.11193", "authors": ["Haifan Yin", "Jindiao Huang", "Ruikun Zhang", "Jiwang Wu", "Li Tan"], "title": "Holographic interference surface: A proof of concept based on the principle of interferometry", "comment": null, "summary": "Revolutionizing communication architectures to achieve a balance between\nenhanced performance and improved efficiency is becoming increasingly critical\nfor wireless communications as the era of ultra-large-scale arrays approaches.\nIn traditional communication architectures, radio frequency (RF) signals are\ntypically converted to baseband for subsequent processing through operations\nsuch as filtering, analog-to-digital conversion and down-conversion, all of\nwhich depend on expensive and power-intensive RF chains. The increased hardware\ncomplexity and escalated power consumption resulting from this dependency\nsignificantly limit the practical deployment of ultra-large-scale arrays. To\naddress these limitations, we propose a holographic communication system based\non the principle of interferometry, designated as holographic interference\nsurfaces (HIS). Utilizing the interference effect of electromagnetic waves, HIS\nestimates the channel state information (CSI) by dealing solely with power\ninformation, which enables the replacement of RF chains with power sensors and\ncompletes the signal processing in radio frequency. As proof-of-concept\ndemonstrations, we implemented a prototype system based on principles of\nholographic interference. Experimental results align well with theoretical\npredictions, confirming the practical viability and effectiveness of the\nproposed HIS. This work provides a new paradigm for building a more\ncost-effective wireless communication architecture."}
{"id": "2509.11243", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11243", "abs": "https://arxiv.org/abs/2509.11243", "authors": ["Haozhen Li", "Ruide Zhang", "Rongqing Zhang", "Xiang Cheng"], "title": "Synesthesia of Machines (SoM)-Empowered Wireless Image Transmission over Complex Dynamic Channel", "comment": null, "summary": "Wireless image transmission underpins diverse networked intelligent services\nand becomes an increasingly critical issue. Existing works have shown that deep\nlearning-based joint source-channel coding (JSCC) is an effective framework to\nbalance image transmission fidelity and data overhead. However, these studies\noversimplify the communication system as a mere pipeline with noise, failing to\naccount for the complex dynamics of wireless channels and concrete\nphysical-layer transmission process. To address these limitations, we propose a\nSynesthesia of Machines (SoM)-empowered Dynamic Channel Adaptive Transmission\n(DCAT) scheme, designed for practical implementation in real communication\nscenarios. Building upon the Swin Transformer backbone, our DCAT scheme\ndemonstrates robust adaptability to time-selective fading and channel aging\neffects by effectively utilizing the physical-layer transmission\ncharacteristics of wireless channels. Comprehensive experimental results\nconfirm that DCAT consistently achieves superior performance compared with JSCC\nbaseline approaches across all conditions. Furthermore, our neural network\narchitecture demonstrates high scalability due to its interpretable design,\noffering substantial potential for cost-efficient deployment in practical\napplications."}
{"id": "2509.11373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11373", "abs": "https://arxiv.org/abs/2509.11373", "authors": ["Hadi Zayyani", "Felipe A. P. de Figueiredo", "Mohammad Salman", "Rausley A. A. de Souza"], "title": "Resistor Hopping KLJN Noise Communication Using Small Bias Voltages Supported by ML and Optimum Threshold-Based Detectors", "comment": null, "summary": "In this paper, a Resistor Hopping (RH) scheme with the addition of biases is\nproposed for secure Kirchhoff Law Johnson-Noise (KLJN) communication. The RH\napproach enables us to increase the bit rate of secure communication between\nAlice and Bob, while also ensuring that the inherent unconditional security of\nKLJN is satisfied. The biases are added to the proposed scheme to better\ndistinguish between Gaussian distributed noises in terms of their means, rather\nthan just using variances. Throughout the paper, we strive to minimize biases\nto achieve a power-efficient scheme. For the detection part of the proposed\nalgorithm, a Maximum-Likelihood (ML) detector is derived. The separability\ncondition of Gaussian distributions is investigated, along with the provision\nof a threshold-based detector that offers both simple and optimal thresholds in\nterms of minimizing the error probability. Some analysis of the proposed\nRH-KLJN communication scheme is provided, including Physical Layer Security\n(PLS) equations. Simulation results demonstrate the advantages of the proposed\nscheme over the classical KLJN scheme, offering a higher data rate and lower\nbit error probability at the expense of increased complexity."}
{"id": "2509.11378", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11378", "abs": "https://arxiv.org/abs/2509.11378", "authors": ["Hadi Zayyani", "Mohammad Salman", "Felipe A. P. de Figueiredo", "Rausley A. A. de Souza"], "title": "A Generalized Framework for Quadratic Noise Modulation Using Non-Gaussian Distributions", "comment": null, "summary": "This letter generalizes noise modulation by introducing two voltage biases\nand employing non-Gaussian noise distributions, such as Mixture of Gaussian\n(MoG) and Laplacian, in addition to traditional Gaussian noise. The proposed\nframework doubles the data rate by enabling discrimination in both the mean and\nvariance of transmitted noise symbols. This novel modulation scheme is referred\nto as Generalized Quadratic Noise Modulation (GQNM). Closed-form expressions\nfor the Bit Error Probability (BEP) are derived for the Generalized Gaussian\n(GG) and Gaussian Mixture of Two Gaussians (GMoTG) cases. Simulation results\ndemonstrate the advantages of the generalized modulation scheme, particularly\nunder non-Gaussian noise assumptions, highlighting its potential for enhanced\nperformance in low-power and secure communication systems."}
{"id": "2509.11397", "categories": ["eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.11397", "abs": "https://arxiv.org/abs/2509.11397", "authors": ["Rafi Beinhorn", "Shay Kreymer", "Amnon Balanov", "Michael Cohen", "Alon Zabatani", "Tamir Bendory"], "title": "Solving ill-conditioned polynomial equations using score-based priors with application to multi-target detection", "comment": null, "summary": "Recovering signals from low-order moments is a fundamental yet notoriously\ndifficult task in inverse problems. This recovery process often reduces to\nsolving ill-conditioned systems of polynomial equations. In this work, we\npropose a new framework that integrates score-based diffusion priors with\nmoment-based estimators to regularize and solve these nonlinear inverse\nproblems. This introduces a new role for generative models: stabilizing\npolynomial recovery from noisy statistical features. As a concrete application,\nwe study the multi-target detection (MTD) model in the high-noise regime. We\ndemonstrate two main results: (i) diffusion priors substantially improve\nrecovery from third-order moments, and (ii) they make the super-resolution MTD\nproblem, otherwise ill-posed, feasible. Numerical experiments on MNIST data\nconfirm consistent gains in reconstruction accuracy across SNR levels. Our\nresults suggest a promising new direction for combining generative priors with\nnonlinear polynomial inverse problems."}
{"id": "2509.11419", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11419", "abs": "https://arxiv.org/abs/2509.11419", "authors": ["Mengyuan Ma", "Nhan Thanh Nguyen", "Nir Shlezinger", "Yonina C. Eldar", "A. Lee Swindlehurst", "Markku Juntti"], "title": "Knowledge Distillation for Sensing-Assisted Long-Term Beam Tracking in mmWave Communications", "comment": "14 pages, 17 figures", "summary": "Infrastructure-mounted sensors can capture rich environmental information to\nenhance communications and facilitate beamforming in millimeter-wave systems.\nThis work presents an efficient sensing-assisted long-term beam tracking\nframework that selects optimal beams from a codebook for current and multiple\nfuture time slots. We first design a large attention-enhanced neural network\n(NN) to fully exploit past visual observations for beam tracking. A\nconvolutional NN extracts compact image features, while gated recurrent units\nwith attention capture the temporal dependencies within sequences. The large NN\nthen acts as the teacher to guide the training of a lightweight student NN via\nknowledge distillation. The student requires shorter input sequences yet\npreserves long-term beam prediction ability. Numerical results demonstrate that\nthe teacher achieves Top-5 accuracies exceeding 93% for current and six future\ntime slots, approaching state-of-the-art performance with a 90% complexity\nreduction. The student closely matches the teacher's performance while cutting\ncomplexity by another 90%, despite operating with 60% shorter input sequences.\nThis improvement significantly enhances data efficiency, reduces latency, and\nlowers power consumption in sensing and processing."}
{"id": "2509.11500", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.11500", "abs": "https://arxiv.org/abs/2509.11500", "authors": ["Tian Han", "Peter J Smith", "Urbashi Mitra", "Jamie S Evans", "Robin Evans", "Rajitha Senanayake"], "title": "Dynamic Length FSK Waveforms for Joint Communications and Radar", "comment": "15 pages, 7 figures Submitted to IEEE Transactions on Wireless\n  Communications", "summary": "Motivated by the constant modulus property of frequency shift keying (FSK)\nbased waveforms and the stabilisation of its radar performance with an increase\nin the number of subpulses, in this paper an FSK-based dynamic subpulse number\njoint communications and radar waveform design is proposed. From a\ncommunications point of view, the system operates based on traditional FSK\nmodulation. From a sensing point of view, although the subpulses are\ncontinuously generated and transmitted, radar waveforms are dynamically formed\nby monitoring the flatness of the spectrum which in turn guarantees the\naccuracy of the delay estimation. Other constraints on the waveform length are\nused to ensure satisfactory values of the root mean square time duration,\nambiguity function sidelobe levels and prevent overly long waveforms. To\nprovide an estimation of the probability of generating extremely long\nwaveforms, the distribution of the number of subpulses is approximated using a\nBrownian motion process and an existing result on its one-sided exit density.\nNumerical examples are provided to evaluate the accuracy of the approximate\ndistribution, as well as the ambiguity function sidelobe levels and the delay\nand Doppler shift estimation performance of the transmitted waveforms."}
{"id": "2509.11510", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11510", "abs": "https://arxiv.org/abs/2509.11510", "authors": ["Rishab Parthasarathy", "Michael Popik", "Noah Haefner"], "title": "Radio Frequency Amplitude-Modulation to Frequency-Modulation Signal Converter", "comment": "23 pages, 27 figures, equal contribution", "summary": "In this project, we wanted to discover an analog topology that could\neffectively convert amplitude-modulated (AM) signals to frequency-modulated\n(FM) signals, while also ensuring that both sets of signals were within their\nrespective radio frequency (RF) bands. To that end, an effective topology for\ndoing so was developed, characterized, and demonstrated, requiring the ability\nto de-modulate incoming signals from the AM radio band--spanning from 530 kHz\nto 1700 kHz--and re-modulate these signals into the FM radio band--spanning\nfrom 88 MHz to 108 MHz. These bands are separated by roughly 86 MHz, presenting\nthe need for the topology to radically alter the incoming frequency before\nre-broadcasting. At its simplest implementation, this required an AM\ndemodulation circuit coupled to a voltage controlled oscillator (VCO).\nTogether, these two circuits translated variations in the incoming envelope\nsignal to variations in the output frequency while still maintaining\nhigh-fidelity audio, similar to how existing radio receiving and broadcasting\nare done. Altogether, the project not only developed a working system but also\nprovided valuable instruction in the design, analysis, and construction of\neffective RF circuits--invaluable to future endeavors within analog\nelectronics."}
{"id": "2509.11533", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11533", "abs": "https://arxiv.org/abs/2509.11533", "authors": ["Hongyang Pan", "Yanheng Liu", "Geng Sun", "Qingqing Wu", "Tierui Gong", "Pengfei Wang", "Dusit Niyato", "Chau Yuen"], "title": "Cooperative UAV-mounted RISs-assisted Energy-efficient Communications", "comment": null, "summary": "Cooperative reconfigurable intelligent surfaces (RISs) are promising\ntechnologies for 6G networks to support a great number of users. Compared with\nthe fixed RISs, the properly deployed RISs may improve the communication\nperformance with less communication energy consumption, thereby improving the\nenergy efficiency. In this paper, we consider a cooperative unmanned aerial\nvehicle-mounted RISs (UAV-RISs)-assisted cellular network, where multiple RISs\nare carried and enhanced by UAVs to serve multiple ground users (GUs)\nsimultaneously such that achieving the three-dimensional (3D) mobility and\nopportunistic deployment. Specifically, we formulate an energy-efficient\ncommunication problem based on multi-objective optimization framework\n(EEComm-MOF) to jointly consider the beamforming vector of base station (BS),\nthe location deployment and the discrete phase shifts of UAV-RIS system so as\nto simultaneously maximize the minimum available rate over all GUs, maximize\nthe total available rate of all GUs, and minimize the total energy consumption\nof the system, while the transmit power constraint of BS is considered. To\ncomprehensively solve EEComm-MOF which is an NP-hard and non-convex problem\nwith constraints, a non-dominated sorting genetic algorithm-II with a\ncontinuous solution processing mechanism, a discrete solution processing\nmechanism, and a complex solution processing mechanism (INSGA-II-CDC) is\nproposed. Simulations results demonstrate that the proposed INSGA-II-CDC can\nsolve EEComm-MOF effectively and outperforms other benchmarks under different\nparameter settings. Moreover, the stability of INSGA-II-CDC and the\neffectiveness of the improved mechanisms are verified. Finally, the\nimplementability analysis of the algorithm is given."}
{"id": "2509.11542", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11542", "abs": "https://arxiv.org/abs/2509.11542", "authors": ["Giorgi Tsintsadze", "Reza Vahdani", "James L. Drewniak", "Richard Zai"], "title": "Simplified Design Approach for Via Transitions up to 67 GHz", "comment": null, "summary": "A systematic approach for high-speed via transition design is proposed. The\neffects of via barrel radius, anti-pad size, and the distance from adjacent\nstitching (GND) vias on bandwidth are analyzed and characterized. Guidelines\nfor selecting parameter values are provided and validated by correlating 3D\nfull-wave FEM simulation results with actual measurements of the coupon board.\nWhen a sufficient number of stitching vias are used, the via structure can be\napproximated as a coaxial transmission line. The proposed methodology builds on\nthis approximation and also considers high-order modes. With this framework,\nengineers can easily optimize design parameters while intuitively understanding\nhow geometry affects bandwidth. This approach also allows engineers with\nlimited access to expensive and computationally intensive 3D FEM tools to\ndesign high bandwidth vias up to 67 GHz."}
{"id": "2509.11551", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11551", "abs": "https://arxiv.org/abs/2509.11551", "authors": ["Yida Zhang", "Qiuyan Liu", "Hongtao Luo", "Yuqi Xia", "Qiang Wang"], "title": "Stacked Intelligent Metasurface for End-to-End OFDM System", "comment": null, "summary": "Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabled\nwave-domain signal processing have emerged as promising research directions for\noffloading baseband digital processing tasks and efficiently simplifying\ntransceiver design. However, existing architectures are limited to employing\nSIM (DPSIM) for a single communication function, such as precoding or\ncombining. To further enhance the overall performance of SIM (DPSIM)-assisted\nsystems and achieve end-to-end (E2E) joint optimization from the transmitted\nbitstream to the received bitstream, we propose an SIM (DPSIM)- assisted E2E\northogonal frequency-division multiplexing (OFDM) system, where traditional\ncommunication tasks such as modulation, precoding, combining, and demodulation\nare performed simultaneously during electromagnetic (EM) forward propagation.\nFurthermore, inspired by the idea of abstracting real metasurfaces as hidden\nlayers of a neural network, we propose the electromagnetic neural network\n(EMNN) to enable the control of the E2E OFDM communication system. In addition,\ntransfer learning is introduced into the model training, and a training and\ndeployment framework for the EMNN is designed. Simulation results demonstrate\nthat both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems can\nachieve robust bitstream transmission under complex channel conditions. Our\nstudy highlights the application potential of EMNN and SIM (DPSIM)-assisted E2E\nOFDM systems in the design of next-generation transceivers."}
{"id": "2509.11571", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11571", "abs": "https://arxiv.org/abs/2509.11571", "authors": ["Zhiyuan Liu", "Qingyu Liu", "Shuhang Zhang", "Hongliang Zhang", "Lingyang Song"], "title": "RadioLAM: A Large AI Model for Fine-Grained 3D Radio Map Estimation", "comment": "Submitted to IEEE JSAC", "summary": "A radio map captures the spatial distribution of wireless channel parameters,\nsuch as the strength of the signal received, across a geographic area. The\nproblem of fine-grained three-dimensional (3D) radio map estimation involves\ninferring a high-resolution radio map for the two-dimensional (2D) area at an\narbitrary target height within a 3D region of interest, using radio samples\ncollected by sensors sparsely distributed in that 3D region. Solutions to the\nproblem are crucial for efficient spectrum management in 3D spaces,\nparticularly for drones in the rapidly developing low-altitude economy.\nHowever, this problem is challenging due to ultra-sparse sampling, where the\nnumber of collected radio samples is far fewer than the desired resolution of\nthe radio map to be estimated. In this paper, we design a Large Artificial\nIntelligence Model (LAM) called RadioLAM for the problem. RadioLAM employs the\ncreative power and the strong generalization capability of LAM to address the\nultra-sparse sampling challenge. It consists of three key blocks: 1) an\naugmentation block, using the radio propagation model to project the radio\nsamples collected at different heights to the 2D area at the target height; 2)\na generation block, leveraging an LAM under an Mixture of Experts (MoE)\narchitecture to generate a candidate set of fine-grained radio maps for the\ntarget 2D area; and 3) an election block, utilizing the radio propagation model\nas a guide to find the best map from the candidate set. Extensive simulations\nshow that RadioLAM is able to solve the fine-grained 3D radio map estimation\nproblem efficiently from an ultra-low sampling rate of 0.1%, and significantly\noutperforms the state-of-the-art."}
{"id": "2509.11607", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11607", "abs": "https://arxiv.org/abs/2509.11607", "authors": ["Jun Wu", "Yaoqi Yang", "Weijie Yuan", "Wenchao Liu", "Jiacheng Wang", "Tianqi Mao", "Lin Zhou", "Yuanhao Cui", "Fan Liu", "Geng Sun", "Nan Wu", "Dezhi Zheng", "Jindan Xu", "Nan Ma", "Zhiyong Feng", "Wei Xu", "Dusit Niyato", "Chau Yuen", "Xiaojun Jing", "Zhiguo Shi", "Yingchang Liang", "Shi Jin", "Dong In Kim", "Jiangzhou Wang", "Ping Zhang", "Hao Yin", "Jun Zhang"], "title": "Low-Altitude Wireless Networks: A Survey", "comment": null, "summary": "The rapid development of the low-altitude economy has imposed unprecedented\ndemands on wireless infrastructure to accommodate large-scale drone deployments\nand facilitate intelligent services in dynamic airspace environments. However,\nunlocking its full potential in practical applications presents significant\nchallenges. Traditional aerial systems predominantly focus on air-ground\ncommunication services, often neglecting the integration of sensing,\ncomputation, control, and energy-delivering functions, which hinders the\nability to meet diverse mission-critical demands. Besides, the absence of\nsystematic low-altitude airspace planning and management exacerbates issues\nregarding dynamic interference in three-dimensional space, coverage\ninstability, and scalability. To overcome these challenges, a comprehensive\nframework, termed low-altitude wireless network (LAWN), has emerged to\nseamlessly integrate communication, sensing, computation, control, and air\ntraffic management into a unified design. This article provides a comprehensive\noverview of LAWN systems, introducing LAWN system fundamentals and the\nevolution of functional designs. Subsequently, we delve into performance\nevaluation metrics and review critical concerns surrounding privacy and\nsecurity in the open-air network environment. Finally, we present the\ncutting-edge developments in airspace structuring and air traffic management,\nproviding insights to facilitate the practical deployment of LAWNs."}
{"id": "2509.11725", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11725", "abs": "https://arxiv.org/abs/2509.11725", "authors": ["Mengyuan Ma", "Nhan Thanh Nguyen", "Nir Shlezinger", "Yonina C. Eldar", "Markku Juntti"], "title": "Attention-Enhanced Learning for Sensing-Assisted Long-Term Beam Tracking in mmWave Communications", "comment": "5 pages, 6 figures, submitted to ICASSP2026", "summary": "Beam training and prediction in millimeter-wave communications are highly\nchallenging due to fast time-varying channels and sensitivity to blockages and\nmobility. In this context, infrastructure-mounted cameras can capture rich\nenvironmental information that can facilitate beam tracking design. In this\nwork, we develop an efficient attention-enhanced machine learning model for\nlong-term beam tracking built upon convolutional neural networks and gated\nrecurrent units to predict both current and future beams from past observed\nimages. The integrated temporal attention mechanism substantially improves its\npredictive performance. Numerical results demonstrate that the proposed design\nachieves Top-5 beam prediction accuracies exceeding 90% across both current and\nsix future time slots, significantly reducing overhead arising from sensing and\nprocessing for beam training. It further attains 97% of state-of-the-art\nperformance with only 3% of the computational complexity."}
{"id": "2509.11923", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11923", "abs": "https://arxiv.org/abs/2509.11923", "authors": ["Mingjun Ying", "Peijie Ma", "Dipankar Shakya", "Theodore S. Rappaport"], "title": "Multi-Stage Location Optimization Through Power Delay Profile Alignment Using Site-Specific Wireless Ray Tracing", "comment": "6 pages, 3 figures, 2 tables", "summary": "Ray tracing (RT) simulations require accurate transmitter (TX) and receiver\n(RX) location information from real-world measurements to accurately\ncharacterize wireless propagation behavior in an environment. Such wireless\npropagation measurements typically employ GPS-based logging for TX/RX\nlocations, which can produce meter-level errors that lead to unreliable RT\ncalibration and validation. These location misalignments cause inaccurate\ninteractions between RT-generated multipath components (MPCs) and the modeled\n3D environment, which lead to erroneous channel predictions, and severe\ndiscrepancies between simulated and measured power delay profiles (PDPs) and\nchannel characteristics. Moreover, the same RT-generated PDPs using inaccurate\nlocations result in calibration errors when adjusting material properties such\nas conductivity and permittivity.\n  This paper presents a systematic multi-stage TX/RX location calibration\nframework to correct location errors and consequently align measured and\nsimulated omnidirectional PDPs.\n  Optimization is performed using a computationally efficient multi-stage grid\nsearch and the Powell method. Applying the location calibration framework to\nNYU WIRELESS urban-microcell (UMi) measurements at 6.75 GHz and 16.95 GHz\ncorrected TX/RX location errors of up to 7 m. The framework reduced the\ncomposite loss function by 42.3\\% for line-of-sight (LOS) and 13.5\\% for\nnon-line-of-sight (NLOS) scenarios. Furthermore, peak power prediction accuracy\nimproved by approximately 1 dB on average. Such improved geometric alignment\nenables accurate channel prediction, vital for beam management and\ninfrastructure deployment for next-generation wireless networks."}
{"id": "2509.11994", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11994", "abs": "https://arxiv.org/abs/2509.11994", "authors": ["Souvik Paul", "Iván Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu"], "title": "Optimized Sparse Network Coverage via L1-norm Minimization", "comment": "Submitted for IEEE CAMSAP 2025 conference", "summary": "The selection of nodes that can serve as cluster heads, local sinks and\ngateways is a critical challenge in distributed sensor and communication\nnetworks. This paper presents a novel framework for identifying a minimal set\nof nexus nodes to ensure full network coverage while minimizing cost. By\nformulating the problem as a convex relaxation of the NP-hard set cover\nproblem, we integrate the graph theoretic centrality measures of node degree\nand betweenness centrality into a cost function optimized via a relaxed L1-norm\nminimization. The proposed approach is applicable to static and dynamic network\nscenarios and does not require location or distance estimation. Through\nsimulations across various graph models and dynamic conditions, it is shown\nthat the method achieves faster execution times (lower complexity) and\ncompetitive sparsity compared to classical greedy and genetic algorithms (GA),\noffering a robust, distributed, and cost-efficient node selection solution."}
{"id": "2509.12032", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12032", "abs": "https://arxiv.org/abs/2509.12032", "authors": ["Baiyang Liu", "Jiewei Huang", "Tuo Wu", "Huan Meng", "Fengcheng Mei", "Lei Ning", "Kai-Kit Wong", "Hang Wong", "Kin-Fai Tong", "Kwai-Man Luk"], "title": "Meta Fluid Antenna: Architecture Design, Performance Analysis, Experimental Examination", "comment": "13 pages", "summary": "Fluid antenna systems (FAS) have recently emerged as a promising solution for\nsixth-generation (6G) ultra-dense connectivity. These systems utilize dynamic\nradiating and/or shaping techniques to mitigate interference and improve\nspectral efficiency without relying on channel state information (CSI). The\nreported improvements achieved by employing a single dynamically activated\nradiating position in fluid antenna multiple access (FAMA) are significant. To\nfully realize the potential of FAMA in multi-user multiplexing, we propose\nleveraging the unique fast-switching capabilities of a single radio-frequency\n(RF)-chain meta-fluid antenna structure to achieve multi-activation. This\nallows for a significantly larger set of independent radiating states without\nrequiring additional signal processing. Simulations demonstrate that\nmulti-activation FAMA enables robust multi-user multiplexing with a higher\nsignal-to-interference ratio (SIR) under various Rayleigh-fading environments\ncompared to other single RF-chain technologies. We further show that the SIR\ncan be optimized within a 15~$\\mu s$ timeframe under a multi-user\nRayleigh-fading channel, making the proposed scheme highly suitable for\nfast-changing wireless environments. Verified through the theoretical Jakes'\nmodel, full three-dimensional (3D) electromagnetic (EM) simulations and\nexperimental validation, multi-activation FAMA enables effective CSI-free,\nmulti-user communication, offering a scalable solution for high-capacity\nwireless networks."}
{"id": "2509.12089", "categories": ["eess.SP", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12089", "abs": "https://arxiv.org/abs/2509.12089", "authors": ["Qiying Hu"], "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss", "comment": null, "summary": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions."}
{"id": "2509.12110", "categories": ["eess.SP", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12110", "abs": "https://arxiv.org/abs/2509.12110", "authors": ["Qiying Hu", "Linping Zhang", "Xueqian Wang", "Gang Li", "Yu Liu", "Xiao-Ping Zhang"], "title": "When marine radar target detection meets pretrained large language models", "comment": null, "summary": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests."}
{"id": "2509.11606", "categories": ["cs.SD", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.11606", "abs": "https://arxiv.org/abs/2509.11606", "authors": ["Milan Marocchi", "Matthew Fynn", "Kayapanda Mandana", "Yue Rong"], "title": "Scaling to Multimodal and Multichannel Heart Sound Classification: Fine-Tuning Wav2Vec 2.0 with Synthetic and Augmented Biosignals", "comment": "35 pages, 37 figures, 19 tables", "summary": "Cardiovascular diseases (CVDs) are the leading cause of death worldwide,\naccounting for approximately 17.9 million deaths each year. Early detection is\ncritical, creating a demand for accurate and inexpensive pre-screening methods.\nDeep learning has recently been applied to classify abnormal heart sounds\nindicative of CVDs using synchronised phonocardiogram (PCG) and\nelectrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However,\nstate-of-the-art architectures remain underutilised due to the limited\navailability of synchronised and multichannel datasets. Augmented datasets and\npre-trained models provide a pathway to overcome these limitations, enabling\ntransformer-based architectures to be trained effectively. This work combines\ntraditional signal processing with denoising diffusion models, WaveGrad and\nDiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based\nclassifier on multimodal and multichannel heart sound datasets. The approach\nachieves state-of-the-art performance. On the Computing in Cardiology (CinC)\n2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR),\nsensitivity, specificity and Matthew's correlation coefficient (MCC) reach\n92.48\\%, 93.05\\%, 93.63\\%, 92.48\\%, 94.93\\% and 0.8283, respectively. Using the\nsynchronised PCG and ECG signals of the training-a dataset from CinC, 93.14\\%,\n92.21\\%, 94.35\\%, 90.10\\%, 95.12\\% and 0.8380 are achieved for accuracy, UAR,\nsensitivity, specificity and MCC, respectively. Using a wearable vest dataset\nconsisting of mPCG data, the model achieves 77.13\\% accuracy, 74.25\\% UAR,\n86.47\\% sensitivity, 62.04\\% specificity, and 0.5082 MCC. These results\ndemonstrate the effectiveness of transformer-based models for CVD detection\nwhen supported by augmented datasets, highlighting their potential to advance\nmultimodal and multichannel heart sound classification."}

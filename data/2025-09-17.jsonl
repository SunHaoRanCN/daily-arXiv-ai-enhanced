{"id": "2509.12261", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12261", "abs": "https://arxiv.org/abs/2509.12261", "authors": ["Marko Djukanovic", "Christian Blum", "Aleksandar Kartelj", "Ana Nikolikj", "Guenther Raidl"], "title": "An Adaptive CMSA for Solving the Longest Filled Common Subsequence Problem with an Application in Audio Querying", "comment": null, "summary": "This paper addresses the Longest Filled Common Subsequence (LFCS) problem, a\nchallenging NP-hard problem with applications in bioinformatics, including gene\nmutation prediction and genomic data reconstruction. Existing approaches,\nincluding exact, metaheuristic, and approximation algorithms, have primarily\nbeen evaluated on small-sized instances, which offer limited insights into\ntheir scalability. In this work, we introduce a new benchmark dataset with\nsignificantly larger instances and demonstrate that existing datasets lack the\ndiscriminative power needed to meaningfully assess algorithm performance at\nscale. To solve large instances efficiently, we utilize an adaptive Construct,\nMerge, Solve, Adapt (CMSA) framework that iteratively generates promising\nsubproblems via component-based construction and refines them using feedback\nfrom prior iterations. Subproblems are solved using an external black-box\nsolver. Extensive experiments on both standard and newly introduced benchmarks\nshow that the proposed adaptive CMSA achieves state-of-the-art performance,\noutperforming five leading methods. Notably, on 1,510 problem instances with\nknown optimal solutions, our approach solves 1,486 of them -- achieving over\n99.9% optimal solution quality and demonstrating exceptional scalability. We\nadditionally propose a novel application of LFCS for song identification from\ndegraded audio excerpts as an engineering contribution, using real-world\nenergy-profile instances from popular music. Finally, we conducted an empirical\nexplainability analysis to identify critical feature combinations influencing\nalgorithm performance, i.e., the key problem features contributing to success\nor failure of the approaches across different instance types are revealed."}
{"id": "2509.12267", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12267", "abs": "https://arxiv.org/abs/2509.12267", "authors": ["Christian Zhou-Zheng", "John Backsund", "Dun Li Chan", "Alex Coventry", "Avid Eslami", "Jyotin Goel", "Xingwen Han", "Danysh Soomro", "Galen Wei"], "title": "A Traditional Approach to Symbolic Piano Continuation", "comment": "3 pages, extended abstract, MIREX session at ISMIR 2025 LBD", "summary": "We present a traditional approach to symbolic piano music continuation for\nthe MIREX 2025 Symbolic Music Generation challenge. While computational music\ngeneration has recently focused on developing large foundation models with\nsophisticated architectural modifications, we argue that simpler approaches\nremain more effective for constrained, single-instrument tasks. We thus return\nto a simple, unaugmented next-token-prediction objective on tokenized raw MIDI,\naiming to outperform large foundation models by using better data and better\nfundamentals. We release model weights and code at\nhttps://github.com/christianazinn/mirex2025."}
{"id": "2509.12275", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12275", "abs": "https://arxiv.org/abs/2509.12275", "authors": ["Jinghua Zhao", "Hang Su", "Lichun Fan", "Zhenbo Luo", "Jian Luan", "Hui Wang", "Haoqin Sun", "Yong Qin"], "title": "Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio questuin answering", "comment": "5 pages, 1 figure, 2 tables", "summary": "We propose Omni-CLST, an error-aware Curriculum Learning framework with\nguided Selective Chain-of-Thought for audio question answering. The framework\nefficiently leverages existing high-quality dataset through two key strategies:\nan error-aware curriculum that organizes samples by difficulty, and a guided\nthought dropout mechanism that focuses reasoning on challenging cases.\nIntegrated with GRPO training, these strategies enable the model to learn more\neffectively from informative samples. Experiments on MMAU-mini and MMAR\ndemonstrate that Omni-CLST achieves competitive accuracy (73.80% on MMAU-mini)\nand establishes a new state of the art (64.30% on MMAR), highlighting its\nrobustness and generalization capability in multimodal audio-language\nunderstanding."}
{"id": "2509.12295", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12295", "abs": "https://arxiv.org/abs/2509.12295", "authors": ["James Tavernor", "Emily Mower Provost"], "title": "More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech Emotion Recognition", "comment": "\\copyright 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Speech emotion recognition systems often predict a consensus value generated\nfrom the ratings of multiple annotators. However, these models have limited\nability to predict the annotation of any one person. Alternatively, models can\nlearn to predict the annotations of all annotators. Adapting such models to new\nannotators is difficult as new annotators must individually provide sufficient\nlabeled training data. We propose to leverage inter-annotator similarity by\nusing a model pre-trained on a large annotator population to identify a\nsimilar, previously seen annotator. Given a new, previously unseen, annotator\nand limited enrollment data, we can make predictions for a similar annotator,\nenabling off-the-shelf annotation of unseen data in target datasets, providing\na mechanism for extremely low-cost personalization. We demonstrate our approach\nsignificantly outperforms other off-the-shelf approaches, paving the way for\nlightweight emotion adaptation, practical for real-world deployment."}
{"id": "2509.12583", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.12583", "abs": "https://arxiv.org/abs/2509.12583", "authors": ["Zhan Jin"], "title": "Multi-Modal Embedding-based Target Speaker Enhancement", "comment": null, "summary": "Target Speaker Extraction (TSE) is a critical challenge in cocktail party\nscenarios. While leveraging multiple modalities, such as voice, lip, face, and\nexpression embeddings, can enhance performance, real-world applications often\nsuffer from intermittent modality dropout. This paper presents a comprehensive\nstudy on the interactions and robustness of various multimodal fusion\nstrategies under varying degrees of modality dropout. We build upon a\nstate-of-the-art audio-visual speech enhancement system and integrate four\ndistinct speaker identity cues: lip embeddings for synchronized contextual\ninformation, a voice speaker embedding extracted via cross-attention for\nacoustic consistency, a static face embedding for speaker identity, and a novel\ndynamic expression embedding for frame-wise emotional features. We\nsystematically evaluate different combinations of these modalities under two\nkey training regimes: zero dropout and 80% modality dropout. Extensive\nexperiments demonstrate that while a full multimodal ensemble achieves optimal\nperformance under ideal (zero dropout) conditions, its effectiveness diminishes\nsignificantly when test-time dropout occurs without prior exposure during\ntraining. Crucially, we show that training with a high (80%) modality dropout\nrate dramatically enhances model robustness, enabling the system to maintain\nsuperior performance even under severe test-time missing modalities. Our\nfindings highlight that voice embeddings exhibit consistent robustness, while\nthe proposed expression embedding provides valuable complementary information.\nThis work underscores the importance of training strategies that account for\nreal-world imperfection, moving beyond pure performance maximization to achieve\npractical reliability in multimodal speech enhancement systems."}
{"id": "2509.12348", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12348", "abs": "https://arxiv.org/abs/2509.12348", "authors": ["Hua Chen", "Tao Gong", "Tuo Wu", "Maged Elkashlan", "Baiyang Liu", "Chan-Byoung Chae", "Kin-Fai Tong", "Kai-Kit Wong"], "title": "FAS-ARIS: Turning Multipath Challenges Into Localization Opportunities", "comment": "13 pages", "summary": "Traditional single-input single-output (SISO) systems face fundamental\nlimitations in achieving accurate three-dimensional (3D) localization due to\nlimited spatial degrees of freedom (DoF) and the adverse impact of multipath\npropagation. This paper proposes a novel fluid antenna system (FAS)-active\nreconfigurable intelligent surface (ARIS) framework that transforms multipath\neffects from a hindrance into a resource for enhanced localization. By\nsynergistically combining the signal amplification capabilities of ARIS with\nthe spatial diversity enabled by FAS, the proposed system achieves robust 3D\nuser equipment (UE) positioning -- without relying on auxiliary information\nsuch as time-of-arrival (ToA) or frequency diversity. The system exploits both\nline-of-sight (LoS) and non-line-of-sight (NLoS) components through a tailored\nsignal decoupling strategy. We design novel UE pilot sequences and ARIS phase\nconfigurations to effectively separate LoS and NLoS channels, enabling\nindependent parameter estimation. A multi-stage estimation algorithm is then\napplied: the multiple signal classification (MUSIC) algorithm estimates\nangle-of-arrival (AoA) from the direct path, while maximum likelihood\nestimation with interior-point refinement recovers cascaded channel parameters\nfrom the reflected path. Finally, geometric triangulation using least-squares\nestimation determines the UE's 3D position based on the extracted AoA\ninformation. Comprehensive performance analysis, including the derivation of\nCram\\'{e}r-Rao bounds for both channel and position estimation, establishes\ntheoretical benchmarks. Simulation results confirm that the proposed FAS-ARIS\nframework achieves near-optimal localization accuracy while maintaining\nrobustness in rich multipath environments -- effectively turning conventional\nlocalization challenges into advantages."}
{"id": "2509.12667", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.12667", "abs": "https://arxiv.org/abs/2509.12667", "authors": ["Ziyun Liu", "Chris Donahue"], "title": "Osu2MIR: Beat Tracking Dataset Derived From Osu! Data", "comment": "2 pages", "summary": "In this work, we explore the use of Osu!, a community-based rhythm game, as\nan alternative source of beat and downbeat annotations. Osu! beatmaps are\ncreated and refined by a large, diverse community and span underrepresented\ngenres such as anime, Vocaloid, and video game music. We introduce a pipeline\nfor extracting annotations from Osu! beatmaps and partition them into\nmeaningful subsets. Through manual analysis, we find that beatmaps with a\nsingle timing point or widely spaced multiple timing points (>=5 seconds apart)\nprovide reliable annotations, while closely spaced timing points (<5 seconds\napart) often require additional curation. We also observe high consistency\nacross multiple annotations of the same song. This study demonstrates the\npotential of Osu! data as a scalable, diverse, and community-driven resource\nfor MIR research. We release our pipeline and a high-quality subset\nosu2beat2025 to support further exploration:\nhttps://github.com/ziyunliu4444/osu2mir."}
{"id": "2509.12668", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12668", "abs": "https://arxiv.org/abs/2509.12668", "authors": ["Oguzhan Kurnaz", "Tomi Kinnunen", "Cemal Hanilci"], "title": "Investigating the Potential of Multi-Stage Score Fusion in Spoofing-Aware Speaker Verification", "comment": "published in SIU2025", "summary": "Despite improvements in automatic speaker verification (ASV), vulnerability\nagainst spoofing attacks remains a major concern. In this study, we investigate\nthe integration of ASV and countermeasure (CM) subsystems into a modular\nspoof-aware speaker verification (SASV) framework. Unlike conventional\nsingle-stage score-level fusion methods, we explore the potential of a\nmulti-stage approach that utilizes the ASV and CM systems in multiple stages.\nBy leveraging ECAPA-TDNN (ASV) and AASIST (CM) subsystems, we consider support\nvector machine and logistic regression classifiers to achieve SASV. In the\nsecond stage, we integrate their outputs with the original score to revise\nfusion back-end classifiers. Additionally, we incorporate another auxiliary\nscore from RawGAT (CM) to further enhance our SASV framework. Our approach\nyields an equal error rate (EER) of 1.30% on the evaluation dataset of the\nSASV2022 challenge, representing a 24% relative improvement over the baseline\nsystem."}
{"id": "2509.12359", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12359", "abs": "https://arxiv.org/abs/2509.12359", "authors": ["Henry Carvajal Mora", "Nathaly Orozco", "Fernando Almeida García", "José Vega-Sánchez", "Felipe Grijalva", "Edgar Benitez Olivo"], "title": "Partial Secrecy Analysis in Wireless Systems: Diversity-Enhanced PLS over Generalized Fading Channels", "comment": null, "summary": "Securing information in future mobile networks is challenging, especially for\ndevices with limited computational resources. Physical layer security (PLS)\noffers a viable solution by leveraging wireless channel randomness. When full\nsecrecy is unattainable, the partial secrecy regime provides a realistic\nalternative. This work analyzes partial secrecy performance under the\ngeneralized multicluster fluctuating two-ray (MFTR) fading model, which\nsubsumes many classical fading cases. We study a system with a transmitter (A),\nlegitimate receiver (B), and eavesdropper (E), both B and E using antenna\narrays with maximal ratio combining (MRC), under i.n.i.d. fading. Exact and\nclosed-form approximations are derived for key secrecy metrics: generalized\nsecrecy outage probability (GSOP), average fractional equivocation (AFE), and\naverage information leakage rate (AILR). The results, validated by Monte Carlo\nsimulations, retain constant complexity regardless of diversity order. The MFTR\nmodel's flexibility enables comprehensive assessment across fading conditions,\nshowing that more MRC branches at B enhance secrecy performance depending on\nthe A-E link characteristics."}
{"id": "2509.12712", "categories": ["cs.SD", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.12712", "abs": "https://arxiv.org/abs/2509.12712", "authors": ["Ruigang Li", "Yongxu Zhu"], "title": "Timbre-Adaptive Transcription: A Lightweight Architecture with Associative Memory for Dynamic Instrument Separation", "comment": null, "summary": "Existing multi-timbre transcription models struggle with generalization\nbeyond pre-trained instruments and rigid source-count constraints. We address\nthese limitations with a lightweight deep clustering solution featuring: 1) a\ntimbre-agnostic backbone achieving state-of-the-art performance with only half\nthe parameters of comparable models, and 2) a novel associative memory\nmechanism that mimics human auditory cognition to dynamically encode unseen\ntimbres via attention-based clustering. Our biologically-inspired framework\nenables adaptive polyphonic separation with minimal training data (12.5\nminutes), supported by a new synthetic dataset method offering cost-effective,\nhigh-precision multi-timbre generation. Experiments show the timbre-agnostic\ntranscription model outperforms existing models on public benchmarks, while the\nseparation module demonstrates promising timbre discrimination. This work\nprovides an efficient framework for timbre-related music transcription and\nexplores new directions for timbre-aware separation through cognitive-inspired\narchitectures."}
{"id": "2509.13068", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13068", "abs": "https://arxiv.org/abs/2509.13068", "authors": ["Jingyu Li", "Guangyan Zhang", "Zhen Ye", "Yiwen Guo"], "title": "MSR-Codec: A Low-Bitrate Multi-Stream Residual Codec for High-Fidelity Speech Generation with Information Disentanglement", "comment": null, "summary": "Audio codecs are a critical component of modern speech generation systems.\nThis paper introduces a low-bitrate, multi-scale residual codec that encodes\nspeech into four distinct streams: semantic, timbre, prosody, and residual.\nThis architecture achieves high-fidelity speech reconstruction at competitive\nlow bitrates while demonstrating an inherent ability for information\ndisentanglement. We construct a two-stage language model for text-to-speech\n(TTS) synthesis using this codec, which, despite its lightweight design and\nminimal data requirements, achieves a state-of-the-art Word Error Rate (WER)\nand superior speaker similarity compared to several larger models. Furthermore,\nthe codec's design proves highly effective for voice conversion, enabling\nindependent manipulation of speaker timbre and prosody."}
{"id": "2509.12510", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12510", "abs": "https://arxiv.org/abs/2509.12510", "authors": ["Wei Shao", "Ruoyu Zhang", "Zequan Liang", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun"], "title": "Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device", "comment": "In the proceedings of IEEE-EMBS BSN 2025", "summary": "Wearable photoplethysmography (PPG) is embedded in billions of devices, yet\nits optical waveform is easily corrupted by motion, perfusion loss, and ambient\nlight, jeopardizing downstream cardiometric analytics. Existing signal-quality\nassessment (SQA) methods rely either on brittle heuristics or on data-hungry\nsupervised models. We introduce the first fully unsupervised SQA pipeline for\nwrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,\nunlabeled data from heterogeneous sources (varying in device and sampling\nfrequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,\nthe learned representation is stable across differences in LED wavelength,\ndrive intensity, and device optics, as well as wrist motion). Stage 2 converts\neach 512-D encoder embedding into a 4-D topological signature via persistent\nhomology (PH) and clusters these signatures with HDBSCAN. To produce a binary\nsignal-quality index (SQI), the acceptable PPG signals are represented by the\ndensest cluster while the remaining clusters are assumed to mainly contain\npoor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,\nDavies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,\nrespectively, on a stratified sample of 10,000 windows. In this study, we\npropose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)\nframework that offers a drop-in, scalable, cross-device quality gate for PPG\nsignals."}
{"id": "2509.12786", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.12786", "abs": "https://arxiv.org/abs/2509.12786", "authors": ["Adrian Nachtwey", "Fabian C. Moss", "Anna Viktoria Katrin Plaksin"], "title": "Beyond Bars: Distribution of Edit Operations in Historical Prints", "comment": null, "summary": "In this paper, we present a method for conducting comparative corpus studies\nin musicology that reduces the time-consuming digitization process. Instead of\nencoding whole corpora of musical sources, we suggest sampling bars from these\nsources. We address the challenge of selecting representative samples and\nevaluate three different sampling methods. We used Beethoven's Bagatelles Op.\n33 as a case study to find the method that works best in finding samples\nrepresentative with respect to differences. We believe that this approach\noffers significant value to musicological research by enabling large-scale\nanalyses and thereby statistically sound results. Moreover, we believe our work\nto be a valuable step toward understanding nineteenth-century editorial\npractices and enriching the field of scholarly editing of historical musical\nworks."}
{"id": "2509.13085", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13085", "abs": "https://arxiv.org/abs/2509.13085", "authors": ["Kyo-Won Koo", "Chan-yeong Lim", "Jee-weon Jung", "Hye-jin Shim", "Ha-Jin Yu"], "title": "Token-based Attractors and Cross-attention in Spoof Diarization", "comment": "Accepted to IEEE ASRU 2025", "summary": "Spoof diarization identifies ``what spoofed when\" in a given speech by\ntemporally locating spoofed regions and determining their manipulation\ntechniques. As a first step toward this task, prior work proposed a two-branch\nmodel for localization and spoof type clustering, which laid the foundation for\nspoof diarization. However, its simple structure limits the ability to capture\ncomplex spoofing patterns and lacks explicit reference points for\ndistinguishing between bona fide and various spoofing types. To address these\nlimitations, our approach introduces learnable tokens where each token\nrepresents acoustic features of bona fide and spoofed speech. These attractors\ninteract with frame-level embeddings to extract discriminative representations,\nimproving separation between genuine and generated speech. Vast experiments on\nPartialSpoof dataset consistently demonstrate that our approach outperforms\nexisting methods in bona fide detection and spoofing method clustering."}
{"id": "2509.12515", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12515", "abs": "https://arxiv.org/abs/2509.12515", "authors": ["Zequan Liang", "Ruoyu Zhang", "Wei Shao", "krishna Karthik", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun"], "title": "Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG", "comment": "In the proceedings of IEEE-EMBS International Conference on Body\n  Sensor Networks 2025", "summary": "Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.\nTraditional SpO2 estimation methods often rely on complex clinical calibration,\nmaking them unsuitable for low-power, wearable applications. In this paper, we\npropose a transfer learning-based framework for the rapid adaptation of SpO2\nestimation to energy-efficient wearable devices using low-sampling-rate (25Hz)\ndual-channel photoplethysmography (PPG). We first pretrain a bidirectional Long\nShort-Term Memory (BiLSTM) model with self-attention on a public clinical\ndataset, then fine-tune it using data collected from our wearable We-Be band\nand an FDA-approved reference pulse oximeter. Experimental results show that\nour approach achieves a mean absolute error (MAE) of 2.967% on the public\ndataset and 2.624% on the private dataset, significantly outperforming\ntraditional calibration and non-transferred machine learning baselines.\nMoreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,\nexcluding baseline draw. Our method also attains an MAE of 3.284% in\ninstantaneous SpO2 prediction, effectively capturing rapid fluctuations. These\nresults demonstrate the rapid adaptation of accurate, low-power SpO2 monitoring\non wearable devices without the need for clinical calibration."}
{"id": "2509.12831", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12831", "abs": "https://arxiv.org/abs/2509.12831", "authors": ["Javeria Amir", "Farwa Attaria", "Mah Jabeen", "Umara Noor", "Zahid Rashid"], "title": "A Lightweight Pipeline for Noisy Speech Voice Cloning and Accurate Lip Sync Synthesis", "comment": null, "summary": "Recent developments in voice cloning and talking head generation demonstrate\nimpressive capabilities in synthesizing natural speech and realistic lip\nsynchronization. Current methods typically require and are trained on large\nscale datasets and computationally intensive processes using clean studio\nrecorded inputs that is infeasible in noisy or low resource environments. In\nthis paper, we introduce a new modular pipeline comprising Tortoise text to\nspeech. It is a transformer based latent diffusion model that can perform high\nfidelity zero shot voice cloning given only a few training samples. We use a\nlightweight generative adversarial network architecture for robust real time\nlip synchronization. The solution will contribute to many essential tasks\nconcerning less reliance on massive pre training generation of emotionally\nexpressive speech and lip synchronization in noisy and unconstrained scenarios.\nThe modular structure of the pipeline allows an easy extension for future multi\nmodal and text guided voice modulation and it could be used in real world\nsystems."}
{"id": "2509.13215", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13215", "abs": "https://arxiv.org/abs/2509.13215", "authors": ["Bingxiang Zhong", "Thomas Dietzen"], "title": "Importance-Weighted Domain Adaptation for Sound Source Tracking", "comment": "Accepted paper: Workshop on Detection and Classification of Acoustic\n  Scenes and Events (DCASE 2025)", "summary": "In recent years, deep learning has significantly advanced sound source\nlocalization (SSL). However, training such models requires large labeled\ndatasets, and real recordings are costly to annotate in particular if sources\nmove. While synthetic data using simulated room impulse responses (RIRs) and\nnoise offers a practical alternative, models trained on synthetic data suffer\nfrom domain shift in real environments. Unsupervised domain adaptation (UDA)\ncan address this by aligning synthetic and real domains without relying on\nlabels from the latter. The few existing UDA approaches however focus on static\nSSL and do not account for the problem of sound source tracking (SST), which\npresents two specific domain adaptation challenges. First, variable-length\ninput sequences create mismatches in feature dimensionality across domains.\nSecond, the angular coverages of the synthetic and the real data may not be\nwell aligned either due to partial domain overlap or due to batch size\nconstraints, which we refer to as directional diversity mismatch. To address\nthese, we propose a novel UDA approach tailored for SST based on two key\nfeatures. We employ the final hidden state of a recurrent neural network as a\nfixed-dimensional feature representation to handle variable-length sequences.\nFurther, we use importance-weighted adversarial training to tackle directional\ndiversity mismatch by prioritizing synthetic samples similar to the real\ndomain. Experimental results demonstrate that our approach successfully adapts\nsynthetic-trained models to real environments, improving SST performance."}
{"id": "2509.12518", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12518", "abs": "https://arxiv.org/abs/2509.12518", "authors": ["Zequan Liang", "Ruoyu Zhang", "Wei Shao", "Mahdi Pirayesh Shirazi Nejad", "Ehsan Kourkchi", "Setareh Rafatirad", "Houman Homayoun"], "title": "Generalizable Blood Pressure Estimation from Multi-Wavelength PPG Using Curriculum-Adversarial Learning", "comment": "In the proceedings of IEEE-EMBS International Conference on Body\n  Sensor Networks 2025", "summary": "Accurate and generalizable blood pressure (BP) estimation is vital for the\nearly detection and management of cardiovascular diseases. In this study, we\nenforce subject-level data splitting on a public multi-wavelength\nphotoplethysmography (PPG) dataset and propose a generalizable BP estimation\nframework based on curriculum-adversarial learning. Our approach combines\ncurriculum learning, which transitions from hypertension classification to BP\nregression, with domain-adversarial training that confuses subject identity to\nencourage the learning of subject-invariant features. Experiments show that\nmulti-channel fusion consistently outperforms single-channel models. On the\nfour-wavelength PPG dataset, our method achieves strong performance under\nstrict subject-level splitting, with mean absolute errors (MAE) of 14.2mmHg for\nsystolic blood pressure (SBP) and 6.4mmHg for diastolic blood pressure (DBP).\nAdditionally, ablation studies validate the effectiveness of both the\ncurriculum and adversarial components. These results highlight the potential of\nleveraging complementary information in multi-wavelength PPG and\ncurriculum-adversarial strategies for accurate and robust BP estimation."}
{"id": "2509.12845", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12845", "abs": "https://arxiv.org/abs/2509.12845", "authors": ["Xin Fang", "Guirui Zhong", "Qing Wang", "Fan Chu", "Lei Wang", "Mengui Qian", "Mingqi Cai", "Jiangzhao Wu", "Jianqing Gao", "Jun Du"], "title": "Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training", "comment": "5 pages, 3 figures", "summary": "Anomalous Sound Detection (ASD) is often formulated as a machine attribute\nclassification task, a strategy necessitated by the common scenario where only\nnormal data is available for training. However, the exhaustive collection of\nmachine attribute labels is laborious and impractical. To address the challenge\nof missing attribute labels, this paper proposes an agglomerative hierarchical\nclustering method for the assignment of pseudo-attribute labels using\nrepresentations derived from a domain-adaptive pre-trained model, which are\nexpected to capture machine attribute characteristics. We then apply model\nadaptation to this pre-trained model through supervised fine-tuning for machine\nattribute classification, resulting in a new state-of-the-art performance.\nEvaluation on the Detection and Classification of Acoustic Scenes and Events\n(DCASE) 2025 Challenge dataset demonstrates that our proposed approach yields\nsignificant performance gains, ultimately outperforming our previous\ntop-ranking system in the challenge."}
{"id": "2509.12261", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12261", "abs": "https://arxiv.org/abs/2509.12261", "authors": ["Marko Djukanovic", "Christian Blum", "Aleksandar Kartelj", "Ana Nikolikj", "Guenther Raidl"], "title": "An Adaptive CMSA for Solving the Longest Filled Common Subsequence Problem with an Application in Audio Querying", "comment": null, "summary": "This paper addresses the Longest Filled Common Subsequence (LFCS) problem, a\nchallenging NP-hard problem with applications in bioinformatics, including gene\nmutation prediction and genomic data reconstruction. Existing approaches,\nincluding exact, metaheuristic, and approximation algorithms, have primarily\nbeen evaluated on small-sized instances, which offer limited insights into\ntheir scalability. In this work, we introduce a new benchmark dataset with\nsignificantly larger instances and demonstrate that existing datasets lack the\ndiscriminative power needed to meaningfully assess algorithm performance at\nscale. To solve large instances efficiently, we utilize an adaptive Construct,\nMerge, Solve, Adapt (CMSA) framework that iteratively generates promising\nsubproblems via component-based construction and refines them using feedback\nfrom prior iterations. Subproblems are solved using an external black-box\nsolver. Extensive experiments on both standard and newly introduced benchmarks\nshow that the proposed adaptive CMSA achieves state-of-the-art performance,\noutperforming five leading methods. Notably, on 1,510 problem instances with\nknown optimal solutions, our approach solves 1,486 of them -- achieving over\n99.9% optimal solution quality and demonstrating exceptional scalability. We\nadditionally propose a novel application of LFCS for song identification from\ndegraded audio excerpts as an engineering contribution, using real-world\nenergy-profile instances from popular music. Finally, we conducted an empirical\nexplainability analysis to identify critical feature combinations influencing\nalgorithm performance, i.e., the key problem features contributing to success\nor failure of the approaches across different instance types are revealed."}
{"id": "2509.12605", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12605", "abs": "https://arxiv.org/abs/2509.12605", "authors": ["Yang Chen", "Yeonju Lee", "Yao Shi", "Qiyu Sun"], "title": "Kalman Filtering of Stationary Graph Signals", "comment": null, "summary": "In this paper, we propose a novel definition of stationary graph signals,\nformulated with respect to a symmetric graph shift, such as the graph\nLaplacian. We show that stationary graph signals can be generated by\ntransmitting white noise through polynomial graph channels, and that their\nstationarity is preserved under polynomial channel transmission.\n  In this paper, we also investigate Kalman filtering to dynamical systems\ncharacterized by polynomial state and observation matrices. We demonstrate that\nKalman filtering maintains the stationarity of graph signals, while effectively\nincorporating both system dynamics and noise structure. In comparison to the\nstatic inverse filtering method and naive zero-signal strategy, the Kalman\nfiltering procedure yields more accurate and adaptive signal estimates,\nhighlighting its robustness and versatility in graph signal processing."}
{"id": "2509.12974", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12974", "abs": "https://arxiv.org/abs/2509.12974", "authors": ["Junan Zhang", "Mengyao Zhu", "Xin Xu", "Hui Bu", "Zhenhua Ling", "Zhizheng Wu"], "title": "The CCF AATC 2025: Speech Restoration Challenge", "comment": "Technical Report", "summary": "Real-world speech communication is often hampered by a variety of distortions\nthat degrade quality and intelligibility. While many speech enhancement\nalgorithms target specific degradations like noise or reverberation, they often\nfall short in realistic scenarios where multiple distortions co-exist and\ninteract. To spur research in this area, we introduce the Speech Restoration\nChallenge as part of the China Computer Federation (CCF) Advanced Audio\nTechnology Competition (AATC) 2025. This challenge focuses on restoring speech\nsignals affected by a composite of three degradation types: (1) complex\nacoustic degradations including non-stationary noise and reverberation; (2)\nsignal-chain artifacts such as those from MP3 compression; and (3) secondary\nartifacts introduced by other pre-processing enhancement models. We describe\nthe challenge's background, the design of the task, the comprehensive dataset\ncreation methodology, and the detailed evaluation protocol, which assesses both\nobjective performance and model complexity. Homepage: https://ccf-aatc.org.cn/."}
{"id": "2509.12267", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12267", "abs": "https://arxiv.org/abs/2509.12267", "authors": ["Christian Zhou-Zheng", "John Backsund", "Dun Li Chan", "Alex Coventry", "Avid Eslami", "Jyotin Goel", "Xingwen Han", "Danysh Soomro", "Galen Wei"], "title": "A Traditional Approach to Symbolic Piano Continuation", "comment": "3 pages, extended abstract, MIREX session at ISMIR 2025 LBD", "summary": "We present a traditional approach to symbolic piano music continuation for\nthe MIREX 2025 Symbolic Music Generation challenge. While computational music\ngeneration has recently focused on developing large foundation models with\nsophisticated architectural modifications, we argue that simpler approaches\nremain more effective for constrained, single-instrument tasks. We thus return\nto a simple, unaugmented next-token-prediction objective on tokenized raw MIDI,\naiming to outperform large foundation models by using better data and better\nfundamentals. We release model weights and code at\nhttps://github.com/christianazinn/mirex2025."}
{"id": "2509.12646", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12646", "abs": "https://arxiv.org/abs/2509.12646", "authors": ["Yixin Ding", "Haoyu Jiang", "Xiaoli Xu", "Yanan Liang", "Yong Zeng"], "title": "Data Fusion for BS-UE Cooperative MIMO-OFDM ISAC", "comment": "6 pages, 4 figures", "summary": "Integrated sensing and communication (ISAC) is a promising technique for\nexpanding the functionalities of wireless networks with enhanced spectral\nefficiency. The 3rd Generation Partnership Project (3GPP) has defined six basic\nsensing operation modes in wireless networks. To further enhance the sensing\ncapability of wireless networks, this paper proposes a new sensing operation\nmode, i.e., the base station (BS) and user equipment (UE) cooperative sensing.\nSpecifically, after decoding the communication data, the UE further processes\nthe received signal to extract the target sensing information. We propose an\nefficient algorithm for fusing the sensing results obtained by the BS and UE,\nby exploiting the geometric relationship among BS, UE and targets as well as\nthe expected sensing quality in the BS monostatic and BS-UE bistatic sensing.\nThe results show that the proposed data fusion method for cooperative sensing\ncan effectively improve the position and velocity estimation accuracy of\nmultiple targets, and provide a new approach on the expansion of the sensing\npattern."}
{"id": "2509.13093", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13093", "abs": "https://arxiv.org/abs/2509.13093", "authors": ["Yujie Guo", "Jiaming Zhou", "Yuhang Jia", "Shiwan Zhao", "Yong Qin"], "title": "GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR", "comment": null, "summary": "End-to-end multi-talker automatic speech recognition (MTASR) faces\nsignificant challenges in accurately transcribing overlapping speech,\nespecially under high-overlap conditions. To address these challenges, we\nproposed Global-Local Aware Dynamic (GLAD) Mixture-of-Experts, which\ndynamically fuse speaker-aware global information and fine-grained local\nfeatures to guide expert selection. This mechanism enables speaker-specific\nrouting by leveraging both global context and local acoustic cues. Experiments\non LibriSpeechMix show that GLAD outperforms existing MTASR approaches,\nparticularly in challenging multi-talker scenarios. To our best knowledge, this\nis the first work to apply Mixture-of-Experts (MoE) to end-to-end MTASR with a\nglobal-local fusion strategy. Our code and train dataset can be found at\nhttps://github.com/NKU-HLT/GLAD."}
{"id": "2509.12275", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12275", "abs": "https://arxiv.org/abs/2509.12275", "authors": ["Jinghua Zhao", "Hang Su", "Lichun Fan", "Zhenbo Luo", "Jian Luan", "Hui Wang", "Haoqin Sun", "Yong Qin"], "title": "Omni-CLST: Error-aware Curriculum Learning with guided Selective chain-of-Thought for audio questuin answering", "comment": "5 pages, 1 figure, 2 tables", "summary": "We propose Omni-CLST, an error-aware Curriculum Learning framework with\nguided Selective Chain-of-Thought for audio question answering. The framework\nefficiently leverages existing high-quality dataset through two key strategies:\nan error-aware curriculum that organizes samples by difficulty, and a guided\nthought dropout mechanism that focuses reasoning on challenging cases.\nIntegrated with GRPO training, these strategies enable the model to learn more\neffectively from informative samples. Experiments on MMAU-mini and MMAR\ndemonstrate that Omni-CLST achieves competitive accuracy (73.80% on MMAU-mini)\nand establishes a new state of the art (64.30% on MMAR), highlighting its\nrobustness and generalization capability in multimodal audio-language\nunderstanding."}
{"id": "2509.12658", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.12658", "abs": "https://arxiv.org/abs/2509.12658", "authors": ["Po-Heng Chou", "Jiun-Jia Wu", "Wan-Jen Huang", "Ronald Y. Chang"], "title": "Sustainable LSTM-Based Precoding for RIS-Aided mmWave MIMO Systems with Implicit CSI", "comment": "6 pages, 5 figures, 2 tables, and accepted by 2025 IEEE Globecom\n  Workshops", "summary": "In this paper, we propose a sustainable long short-term memory (LSTM)-based\nprecoding framework for reconfigurable intelligent surface (RIS)-assisted\nmillimeter-wave (mmWave) MIMO systems. Instead of explicit channel state\ninformation (CSI) estimation, the framework exploits uplink pilot sequences to\nimplicitly learn channel characteristics, reducing both pilot overhead and\ninference complexity. Practical hardware constraints are addressed by\nincorporating the phase-dependent amplitude model of RIS elements, while a\nmulti-label training strategy improves robustness when multiple near-optimal\ncodewords yield comparable performance. Simulations show that the proposed\ndesign achieves over 90% of the spectral efficiency of exhaustive search (ES)\nwith only 2.2% of its computation time, cutting energy consumption by nearly\ntwo orders of magnitude. The method also demonstrates resilience under\ndistribution mismatch and scalability to larger RIS arrays, making it a\npractical and energy-efficient solution for sustainable 6G wireless networks."}
{"id": "2509.13145", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13145", "abs": "https://arxiv.org/abs/2509.13145", "authors": ["Yudong Yang", "Xiaokang Liu", "Shaofeng zhao", "Rongfeng Su", "Nan Yan", "Lan Wang"], "title": "UTI-LLM: A Personalized Articulatory-Speech Therapy Assistance System Based on Multimodal Large Language Model", "comment": null, "summary": "Speech therapy plays a critical role in training speech disorders caused by\nneurological impairments such as stroke. However, traditional manual and\ncomputer-assisted systems are limited in real-time accessibility and\narticulatory motion feedback, constraining their practical utility. Recent\nadvances in multimodal large language models (MLLMs) have demonstrated\nsignificant potential in healthcare, particularly through their ability to\nintegrate multimodal data for adaptive assessment and therapeutic feedback.\nNevertheless, challenges including insufficient acquisition and fusion of\narticulatory information, inadequate parsing of articulatory organ motion\ntrajectories, and the scarcity of high-quality domain-specific datasets hinder\nthe application of MLLMs in speech therapy. To address these limitations, we\npropose an MLLM-based speech rehabilitation assistance system that\nsynergistically leverages ultrasound tongue imaging and speech signals to\ndeliver precise, interactive articulatory feedback. We construct a high-quality\ndomain-specific dataset comprising UTI-speech dialogue pairs. This dataset\nfacilitates fine-tuning to enhance the model's clinical adaptability. Building\non this dataset, our methods achieves spatiotemporal fusion training strategy\nof ultrasound videos and speech signals, enabling fine-grained articulatory\nimpairment analysis and ultimately generating actionable feedback."}
{"id": "2509.12295", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12295", "abs": "https://arxiv.org/abs/2509.12295", "authors": ["James Tavernor", "Emily Mower Provost"], "title": "More Similar than Dissimilar: Modeling Annotators for Cross-Corpus Speech Emotion Recognition", "comment": "\\copyright 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Speech emotion recognition systems often predict a consensus value generated\nfrom the ratings of multiple annotators. However, these models have limited\nability to predict the annotation of any one person. Alternatively, models can\nlearn to predict the annotations of all annotators. Adapting such models to new\nannotators is difficult as new annotators must individually provide sufficient\nlabeled training data. We propose to leverage inter-annotator similarity by\nusing a model pre-trained on a large annotator population to identify a\nsimilar, previously seen annotator. Given a new, previously unseen, annotator\nand limited enrollment data, we can make predictions for a similar annotator,\nenabling off-the-shelf annotation of unseen data in target datasets, providing\na mechanism for extremely low-cost personalization. We demonstrate our approach\nsignificantly outperforms other off-the-shelf approaches, paving the way for\nlightweight emotion adaptation, practical for real-world deployment."}
{"id": "2509.12698", "categories": ["eess.SP", "cs.ET", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.12698", "abs": "https://arxiv.org/abs/2509.12698", "authors": ["Yifan Jiang", "Qingqing Wu", "Hongxun Hui", "Wen Chen", "Derrick Wing Kwan Ng"], "title": "Low-Altitude UAV Tracking via Sensing-Assisted Predictive Beamforming", "comment": "13 pages, submitted to IEEE Transaction journals", "summary": "Sensing-assisted predictive beamforming, as one of the enabling technologies\nfor emerging integrated sensing and communication (ISAC) paradigm, shows\nsignificant promise for enhancing various future unmanned aerial vehicle (UAV)\napplications. However, current works predominately emphasized on spectral\nefficiency enhancement, while the impact of such beamforming techniques on the\ncommunication reliability was largely unexplored and challenging to\ncharacterize. To fill this research gap and tackle this issue, this paper\ninvestigates outage capacity maximization for UAV tracking under the\nsensing-assisted predictive beamforming scheme. Specifically, a\ncellular-connected UAV tracking scheme is proposed leveraging extended Kalman\nfiltering (EKF), where the predicted UAV trajectory, sensing duration ratio,\nand target constant received signal-to-noise ratio (SNR) are jointly optimized\nto maximize the outage capacity at each time slot. To address the implicit\nnature of the objective function, closed-form approximations of the outage\nprobabilities (OPs) at both prediction and measurement stages of each time slot\nare proposed based on second-order Taylor expansions, providing an efficient\nand full characterization of outage capacity. Subsequently, an efficient\nalgorithm is proposed based on a combination of bisection search and successive\nconvex approximation (SCA) to address the non-convex optimization problem with\nguaranteed convergence. To further reduce computational complexity, a second\nefficient algorithm is developed based on alternating optimization (AO).\nSimulation results validate the accuracy of the derived OP approximations, the\neffectiveness of the proposed algorithms, and the significant outage capacity\nenhancement over various benchmarks, while also indicating a trade-off between\ndecreasing path loss and enjoying wide beam coverage for outage capacity\nmaximization."}
{"id": "2509.13148", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13148", "abs": "https://arxiv.org/abs/2509.13148", "authors": ["Han Yin", "Jung-Woo Choi"], "title": "Can Large Audio Language Models Understand Audio Well? Speech, Scene and Events Understanding Benchmark for LALMs", "comment": "submitted to ICASSP 2026", "summary": "Recently, Large Audio Language Models (LALMs) have progressed rapidly,\ndemonstrating their strong efficacy in universal audio understanding through\ncross-modal integration. To evaluate the LALM's audio understanding\nperformance, researchers have proposed different benchmarks. However, key\naspects for real-world interactions are underexplored in existing benchmarks,\ni.e., audio signals typically contain both speech and non-speech components,\nand energy levels of these components can vary significantly across different\nscenarios. Moreover, most benchmarks do not consider the joint understanding of\nspeech, scene, and events within the same audio clip. In this work, we\nintroduce SSEU-Bench, the first versatile audio understanding benchmark that\nexplicitly accounts for energy differences between speech and non-speech audio,\nwith both independent and joint understanding settings for speech, scene, and\nevents. Furthermore, we demonstrate that some LALMs tend to underperform on\ncertain tasks in a joint understanding setting. To address this issue, we\nintroduce Chain-of-Thought, which effectively improves the LALM's joint audio\nunderstanding performance by decomposing complex tasks into simpler reasoning\nsteps"}
{"id": "2509.12974", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12974", "abs": "https://arxiv.org/abs/2509.12974", "authors": ["Junan Zhang", "Mengyao Zhu", "Xin Xu", "Hui Bu", "Zhenhua Ling", "Zhizheng Wu"], "title": "The CCF AATC 2025: Speech Restoration Challenge", "comment": "Technical Report", "summary": "Real-world speech communication is often hampered by a variety of distortions\nthat degrade quality and intelligibility. While many speech enhancement\nalgorithms target specific degradations like noise or reverberation, they often\nfall short in realistic scenarios where multiple distortions co-exist and\ninteract. To spur research in this area, we introduce the Speech Restoration\nChallenge as part of the China Computer Federation (CCF) Advanced Audio\nTechnology Competition (AATC) 2025. This challenge focuses on restoring speech\nsignals affected by a composite of three degradation types: (1) complex\nacoustic degradations including non-stationary noise and reverberation; (2)\nsignal-chain artifacts such as those from MP3 compression; and (3) secondary\nartifacts introduced by other pre-processing enhancement models. We describe\nthe challenge's background, the design of the task, the comprehensive dataset\ncreation methodology, and the detailed evaluation protocol, which assesses both\nobjective performance and model complexity. Homepage: https://ccf-aatc.org.cn/."}
{"id": "2509.12748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12748", "abs": "https://arxiv.org/abs/2509.12748", "authors": ["Haiyang Li", "Tianqi Mao", "Pengyu Wang", "Ruiqi Liu", "Shunyu Li", "Zhaocheng Wang"], "title": "NEFT: A Unified Transformer Framework for Efficient Near-Field CSI Feedback in XL-MIMO Systems", "comment": null, "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) systems,\noperating in the near-field region due to their massive antenna arrays, are a\nkey enabler of next-generation wireless communications but face significant\nchallenges in channel state information (CSI) feedback. Deep learning has\nemerged as a powerful tool by learning compact CSI representations for\nfeedback. However, existing methods struggle to capture the intricate structure\nof near-field CSI while incurring prohibitive computational overhead on\npractical mobile devices. To overcome these limitations, we propose the\nNear-Field Efficient Feedback Transformer (NEFT) family for accurate and\nefficient near-field CSI feedback across diverse hardware platforms. Built on a\nhierarchical Vision Transformer backbone, NEFT is extended with lightweight\nvariants to meet various deployment constraints: NEFT-Compact applies\nmulti-level knowledge distillation (KD) to reduce complexity while maintaining\naccuracy, and NEFT-Hybrid and NEFT-Edge address encoder- and edge-constrained\nscenarios via attention-free encoding and KD. Extensive simulations show that\nNEFT achieves a 15--21 dB improvement in normalized mean-squared error (NMSE)\nover state-of-the-art methods, while NEFT-Compact and NEFT-Edge reduce total\nFLOPs by 25--36% with negligible accuracy loss. Moreover, NEFT-Hybrid lowers\nencoder-side complexity by up to 64%, enabling deployment in highly asymmetric\ndevice scenarios. These results establish NEFT as a practical and scalable\nsolution for near-field CSI feedback in XL-MIMO systems."}
{"id": "2509.13285", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13285", "abs": "https://arxiv.org/abs/2509.13285", "authors": ["Gwendal Le Vaillant", "Yannick Molle"], "title": "Contrastive timbre representations for musical instrument and synthesizer retrieval", "comment": null, "summary": "Efficiently retrieving specific instrument timbres from audio mixtures\nremains a challenge in digital music production. This paper introduces a\ncontrastive learning framework for musical instrument retrieval, enabling\ndirect querying of instrument databases using a single model for both single-\nand multi-instrument sounds. We propose techniques to generate realistic\npositive/negative pairs of sounds for virtual musical instruments, such as\nsamplers and synthesizers, addressing limitations in common audio data\naugmentation methods.\n  The first experiment focuses on instrument retrieval from a dataset of 3,884\ninstruments, using single-instrument audio as input. Contrastive approaches are\ncompetitive with previous works based on classification pre-training. The\nsecond experiment considers multi-instrument retrieval with a mixture of\ninstruments as audio input. In this case, the proposed contrastive framework\noutperforms related works, achieving 81.7\\% top-1 and 95.7\\% top-5 accuracies\nfor three-instrument mixtures."}
{"id": "2509.12770", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12770", "abs": "https://arxiv.org/abs/2509.12770", "authors": ["Giorgi Tsintsadze", "Haran Manoharan", "Aaron Harmon", "Daniel Commerou", "Connor Buneta", "Brian Booth", "Daryl Beetner"], "title": "EMC Limit Level Guidelines for In-System Interference with GPS Receivers", "comment": null, "summary": "Because GPS signals are weak, electronic systems and components that are\nplaced near GPS receivers can easily cause disruptive electromagnetic\ninterference through their unintended radiated emissions. In this paper, EMC\nlimit level guidelines are presented for electronics that are intended to be\nplaced near to GPS receivers, as often happens in automotive and other\napplications. One of the challenges of defining limit-levels for systems\nintended to be integrated with GPS receivers is that the impact of noise at the\ninput of the receiver may vary substantially depending on the form of the noise\ndue to the correlator function implemented by GPS receiver. The quality of the\ncorrelated signal is typically represented using the carrier-to-noise ratio ($C\n/ N_0$). A theoretical model predicting the degredation of the carrier-to-noise\nratio with radio frequency interference is presented in this paper and is\nvalidated with realistic noise sources. The model is then used to develop\nguidelines to assess the impact of unintended emissions from electronic devices\non nearby GPS receivers based on the frequency, bandwidth, and magnitude of the\nnoise. These guidelines provide a more nuanced method of evaluating emissions\nthan simple limit lines that are used by many emissions standards."}
{"id": "2509.12583", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.12583", "abs": "https://arxiv.org/abs/2509.12583", "authors": ["Zhan Jin"], "title": "Multi-Modal Embedding-based Target Speaker Enhancement", "comment": null, "summary": "Target Speaker Extraction (TSE) is a critical challenge in cocktail party\nscenarios. While leveraging multiple modalities, such as voice, lip, face, and\nexpression embeddings, can enhance performance, real-world applications often\nsuffer from intermittent modality dropout. This paper presents a comprehensive\nstudy on the interactions and robustness of various multimodal fusion\nstrategies under varying degrees of modality dropout. We build upon a\nstate-of-the-art audio-visual speech enhancement system and integrate four\ndistinct speaker identity cues: lip embeddings for synchronized contextual\ninformation, a voice speaker embedding extracted via cross-attention for\nacoustic consistency, a static face embedding for speaker identity, and a novel\ndynamic expression embedding for frame-wise emotional features. We\nsystematically evaluate different combinations of these modalities under two\nkey training regimes: zero dropout and 80% modality dropout. Extensive\nexperiments demonstrate that while a full multimodal ensemble achieves optimal\nperformance under ideal (zero dropout) conditions, its effectiveness diminishes\nsignificantly when test-time dropout occurs without prior exposure during\ntraining. Crucially, we show that training with a high (80%) modality dropout\nrate dramatically enhances model robustness, enabling the system to maintain\nsuperior performance even under severe test-time missing modalities. Our\nfindings highlight that voice embeddings exhibit consistent robustness, while\nthe proposed expression embedding provides valuable complementary information.\nThis work underscores the importance of training strategies that account for\nreal-world imperfection, moving beyond pure performance maximization to achieve\npractical reliability in multimodal speech enhancement systems."}
{"id": "2509.12821", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12821", "abs": "https://arxiv.org/abs/2509.12821", "authors": ["Martin Zach", "Youssef Haouchat", "Michael Unser"], "title": "A Statistical Benchmark for Diffusion Posterior Sampling Algorithms", "comment": null, "summary": "We propose a statistical benchmark for diffusion posterior sampling (DPS)\nalgorithms for Bayesian linear inverse problems. The benchmark synthesizes\nsignals from sparse L\\'evy-process priors whose posteriors admit efficient\nGibbs methods. These Gibbs methods can be used to obtain gold-standard\nposterior samples that can be compared to the samples obtained by the DPS\nalgorithms. By using the Gibbs methods for the resolution of the denoising\nproblems in the reverse diffusion, the framework also isolates the error that\narises from the approximations to the likelihood score. We instantiate the\nbenchmark with the minimum-mean-squared-error optimality gap and posterior\ncoverage tests and provide numerical experiments for popular DPS algorithms on\nthe inverse problems of denoising, deconvolution, imputation, and\nreconstruction from partial Fourier measurements. We release the benchmark code\nat https://github.com/zacmar/dps-benchmark. The repository exposes simple\nplug-in interfaces, reference scripts, and config-driven runs so that new\nalgorithms can be added and evaluated with minimal effort. We invite\nresearchers to contribute and report results."}
{"id": "2509.12857", "categories": ["eess.SP", "G.3"], "pdf": "https://arxiv.org/pdf/2509.12857", "abs": "https://arxiv.org/abs/2509.12857", "authors": ["Yi Zhang", "Rui Guo", "Yonina C. Eldar"], "title": "Bayesian Signal Separation via Plug-and-Play Diffusion-Within-Gibbs Sampling", "comment": "5 pages, 1 figure, submitted to conference", "summary": "We propose a posterior sampling algorithm for the problem of estimating\nmultiple independent source signals from their noisy superposition. The\nproposed algorithm is a combination of Gibbs sampling method and plug-and-play\n(PnP) diffusion priors. Unlike most existing diffusion-model-based approaches\nfor signal separation, our method allows source priors to be learned separately\nand flexibly combined without retraining. Moreover, under the assumption of\nperfect diffusion model training, the proposed method provably produces samples\nfrom the posterior distribution. Experiments on the task of heartbeat\nextraction from mixtures with synthetic motion artifacts demonstrate the\nsuperior performance of our method over existing approaches."}
{"id": "2509.12870", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12870", "abs": "https://arxiv.org/abs/2509.12870", "authors": ["Ruichen Wang", "Zhikang Ni", "Pengzhou Wang", "Xiya Cao", "Zhi Li", "Bao Zhang"], "title": "Towards personalized, precise and survey-free environment recognition: AI-enhanced sensor fusion without pre-deployment", "comment": "5 pages, 7 figures, conference", "summary": "Accurate and personalized environment recognition is essential for seamless\nindoor positioning and optimized connectivity, yet traditional fingerprinting\nrequires costly site surveys and lacks user-level adaptation. We present a\nsurvey-free, on-device sensor-fusion framework that builds a personalized,\nlightweight multi-source fingerprint (FP) database from pedestrian dead\nreckoning (PDR), WiFi/cellular, GNSS, and interaction time tags. Matching is\nperformed by an AI-enhanced dynamic time warping module (AIDTW) that aligns\nnoisy, asynchronous sequences. To turn perception into continually improving\nactions, a cloud-edge online Reinforcement Learning from Human Feedback (RLHF)\nloop aggregates desensitized summaries and human feedback in the cloud to\noptimize a policy via proximal policy optimization (PPO), and periodically\ndistills updates to devices. Across indoor/outdoor scenarios, our system\nreduces network-transition latency (measured by time-to-switch, TTS) by 32-65%\nin daily environments compared with conventional baselines, without\nsite-specific pre-deployment."}
{"id": "2509.12954", "categories": ["eess.SP", "H.1.1"], "pdf": "https://arxiv.org/pdf/2509.12954", "abs": "https://arxiv.org/abs/2509.12954", "authors": ["Traian E. Abrudan", "Kartik Patel", "John Kimionis", "Tara Esmaeilbeig", "Eleftherios Kampianakis", "Sahan Damith Liyanaarachchi", "Michael Eggleston"], "title": "Next-Generation Backscatter Networks for Integrated Communications and RF Sensing", "comment": "15 pages + appendix", "summary": "This paper provides a comprehensive analysis and theoretical foundation for\nnext-generation backscatter networks that move beyond communication and\nintegrate RF location sensing capabilities. An end-to-end system model for\nwideband OFDM backscatter systems is derived, including detailed\ncharacterization of propagation channels, receiver chain impairments, RF tag\noperation, and unsynchronized network nodes. The theoretical system model is\nvalidated through experimental evaluation using actual hardware, demonstrating\nthe detailed model's accuracy. A practical bistatic ranging method that can\noperate with unsynchronized nodes is presented, along with the Cram\\'er-Rao\nLower Bound (CRLB) derived to show the achievable performance limits. Our\nexperimental results demonstrate the system performance for communication, RF\nsensing, and ranging, while also benchmarking against the derived theoretical\nlimits. This analytical framework and experimental validation establish\nfundamental understanding of distributed, unsynchronized backscatter systems\nfor future machine-type communication networks that are deployed in massive\nscale, while remaining energy-efficient."}
{"id": "2509.12971", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12971", "abs": "https://arxiv.org/abs/2509.12971", "authors": ["Wenyi Yan", "Zeyuan Li", "Lu Gan", "Honqing Liu", "Guoquan Li"], "title": "Difference-Based Recovery for Modulo Sampling: Tightened Bounds and Robustness Guarantees", "comment": null, "summary": "Conventional analog-to-digital converters (ADCs) clip when signals exceed\ntheir input range. Modulo (unlimited) sampling overcomes this limitation by\nfolding the signal before digitization, but existing recovery methods are\neither computationally intensive or constrained by loose oversampling bounds\nthat demand high sampling rates. In addition, none account for sampling jitter,\nwhich is unavoidable in practice. This paper revisits difference-based recovery\nand establishes new theoretical and practical guarantees. In the noiseless\nsetting, we prove that arbitrarily high difference order reduces the sufficient\noversampling factor from $2\\pi e$ to $\\pi$, substantially tightening classical\nbounds. For fixed order $N$, we derive a noise-aware sampling condition that\nguarantees stable recovery. For second-order difference-based recovery ($N=2$),\nwe further extend the analysis to non-uniform sampling, proving robustness\nunder bounded jitter. An FPGA-based hardware prototype demonstrates reliable\nreconstruction with amplitude expansion up to $\\rho = 108$, confirming the\nfeasibility of high-performance unlimited sensing with a simple and robust\nrecovery pipeline."}
{"id": "2509.13004", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13004", "abs": "https://arxiv.org/abs/2509.13004", "authors": ["Jona Cappelle", "Jarne Van Mulders", "Sarah Goossens", "Thomas Reher", "Liesbet Van der Perre", "Lieven De Strycker", "Bram Van de Poel", "Gilles Callebaut"], "title": "RF-Powered Batteryless Plant Movement Sensor for Precision Agriculture", "comment": null, "summary": "Precision agriculture demands non-invasive, energy-efficient, and sustainable\nplant monitoring solutions. In this work, we present the design and\nimplementation of a lightweight, batteryless plant movement sensor powered\nsolely by RF energy. This sensor targets Controlled Environment Agriculture\n(CEA) and utilizes inertial measurements units (IMUs) to monitor leaf motion,\nwhich correlates with plant physiological responses to environmental stress. By\neliminating the battery, we reduce the ecological footprint, weight, and\nmaintenance requirements, transitioning from lifetime-based to operation-based\nenergy storage. Our design minimizes circuit complexity while enabling\nflexible, adaptive readout scheduling based on energy availability and sensor\ndata. We detail the energy requirements, RF power transfer considerations,\nintegration constraints, and outline future directions, including multi-antenna\npower delivery and networked sensor synchronization."}
{"id": "2509.13030", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13030", "abs": "https://arxiv.org/abs/2509.13030", "authors": ["Ge Chen", "Panqi Chen", "Lei Cheng"], "title": "Deep Tensor Learning for Reliable Channel Charting from Incomplete and Noisy Measurements", "comment": null, "summary": "Channel charting has emerged as a powerful tool for user equipment\nlocalization and wireless environment sensing. Its efficacy lies in mapping\nhigh-dimensional channel data into low-dimensional features that preserve the\nrelative similarities of the original data. However, existing channel charting\nmethods are largely developed using simulated or indoor measurements, often\nassuming clean and complete channel data across all frequency bands. In\ncontrast, real-world channels collected from base stations are typically\nincomplete due to frequency hopping and are significantly noisy, particularly\nat cell edges. These challenging conditions greatly degrade the performance of\ncurrent methods. To address this, we propose a deep tensor learning method that\nleverages the inherent tensor structure of wireless channels to effectively\nextract informative while low-dimensional features (i.e., channel charts) from\nnoisy and incomplete measurements. Experimental results demonstrate the\nreliability and effectiveness of the proposed approach in these challenging\nscenarios."}
{"id": "2509.13071", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13071", "abs": "https://arxiv.org/abs/2509.13071", "authors": ["Yuan Liu", "Linlong Wu", "Xuesong Cai", "M. R. Bhavani Shankar"], "title": "Scatterer Localization Using Multi-Bounce Paths", "comment": "Presented in ISCS25", "summary": "Indoor sensing is challenging because of the multi-bounce effect, spherical\nwavefront, and spatial nonstationarity (SNS) of the near-field effect. This\npaper addresses radio-based environment sensing considering these issues.\nSpecifically, graph theory (GT) is used to model the multi-bounce propagation\nof the near field. In this manner, indoor reflectors/scatterers are modeled as\nvertices in a propagation graph, the multi-bounce paths are modeled by the\nedges linking the vertices. Besides, the coupled multipath parameters in the\nnear field, i.e., range and angles, are denoted directly by the coordinates of\nvertices. Then, the space-alternating generalized expectation-maximization\n(SAGE) algorithm is adapted to the proposed Graph theory-based dictionary-aided\nMulti-bounce SAGE (GM-SAGE), where the searching parameters including range and\nangle of departure/arrival (AoD/AoA) are transformed to the coordinates of\nscatterers in the graph. The proposed algorithm is validated through\nmeasurement-calibrated ray tracing (RT) in a complex indoor office. The results\ndemonstrate that the proposed GM-SAGE can deal with multi-bounce channels."}
{"id": "2509.13287", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13287", "abs": "https://arxiv.org/abs/2509.13287", "authors": ["Nandan Sriranga", "Haodong Yang", "Pramod K. Varshney"], "title": "Transmitter Subspace-Aware Target Detection in Two-Channel Passive Radars with Inter-Receiver Collaboration", "comment": null, "summary": "We address target detection in a single Delay-Doppler cell using spatially\ndistributed two-channel passive radars. An unknown illuminator of opportunity\n(IO) is assumed to emit a waveform lying in a known low-dimensional subspace\n(e.g., OFDM). Each receiver transforms its reference and surveillance signals\nonto the IO subspace after noise-whitening, to obtain cross-correlation (CC)\nmeasurements. To save bandwidth, receivers collaboratively exchange and\nlinearly combine the CC output, and only a subset transmits them to a fusion\ncenter (FC) over a multiple-access channel (MAC). Collaboration weights are\ndesigned using the moments of the FC measurement to enhance detection\nperformance."}

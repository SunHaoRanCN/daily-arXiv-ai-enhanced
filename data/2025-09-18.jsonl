{"id": "2509.13328", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13328", "abs": "https://arxiv.org/abs/2509.13328", "authors": ["Danish Rizvi", "David Boyle"], "title": "Dual Actor DDPG for Airborne STAR-RIS Assisted Communications", "comment": null, "summary": "This study departs from the prevailing assumption of independent Transmission\nand Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect\nReconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a\nnovel multi-user downlink communication system that leverages a UAV-mounted\nSTAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key\ncontributions include the joint optimization of UAV trajectory, active\nbeamforming vectors at the base station, and passive RIS TRCs to enhance\ncommunication efficiency, while considering UAV energy constraints. We design\nthe TRC as a combination of discrete and continuous actions, and propose a\nnovel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The\nalgorithm relies on two separate actor networks for high-dimensional hybrid\naction space. We also propose a novel harmonic mean index (HFI)-based reward\nfunction to ensure communication fairness amongst users. For comprehensive\nanalysis, we study the impact of RIS size on UAV aerodynamics showing that it\nincreases drag and energy demand. Simulation results demonstrate that the\nproposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based\nsolutions by 24% and 97%, respectively, in accumulated reward.\nThree-dimensional UAV trajectory optimization achieves 28% higher communication\nefficiency compared to two-dimensional and altitude optimization. The HFI based\nreward function provides 41% lower QoS denial rates as compared to other\nbenchmarks. The mobile Aerial-STAR system shows superior performance over fixed\ndeployed counterparts, with the coupled phase STAR-RIS outperforming dual\nTransmit/Reflect RIS and conventional RIS setups. These findings highlight the\npotential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG\napproach in optimizing their performance."}
{"id": "2509.13559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13559", "abs": "https://arxiv.org/abs/2509.13559", "authors": ["Yuan Liu", "Linlong Wu", "Xuesong Cai", "M. R. Bhavani Shankar"], "title": "Environment Reconstruction in Multi-Bounce Channels with Array Partial Blockage", "comment": "Presented in EUSIPCO2025", "summary": "Extremely-large antenna arrays (ELAA) are important in applications requiring\nhigh angular resolution. However, a prominent issue is the spatial\nnon-stationary (SNS) channels due to partial blockage to the ELAA. In this\npaper, we address the scatterer localization and subsequent environment\nreconstruction considering partially blocked SNS channels. Specifically, the\nSNS effects are parametrically modeled through spatial-varying amplitudes with\nsparsity. Based on the established signal model, the graph-based\ndictionary-aided multi-bounce space-alternating generalized\nexpectation-maximization (GM-SAGE) algorithm is applied to estimate the channel\nparameters and the channel sparsity is empirically detected along with\namplitude estimation. To validate the proposed approach, we generate\nmulti-bounce paths through ray tracing (RT) simulations, where the SNS channels\ncaused by partial blockage could be configured flexibly. The simulation results\ndemonstrate the robustness of the proposed approach in dealing with the SNS\nchannels."}
{"id": "2509.13592", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13592", "abs": "https://arxiv.org/abs/2509.13592", "authors": ["Youval Klioui"], "title": "Fast Single-Snapshot Harmonic Recovery with 2D Sparse Arrays using BCCB Matrices", "comment": null, "summary": "We introduce an efficient implementation of sparse recovery methods for the\nproblem of harmonic estimation with 2D sparse arrays using a single snapshot.\nBy imposing a uniformity constraint on the harmonic grids of the\nsubdictionaries used in the sparse recovery problem, in addition to a mild\nconstraint on the array topology that consists in having the elements lie on a\ngrid specified in half-wavelength units, we show that the Gram matrices that\nappear in these sparse recovery methods exhibit a block-circulant with\ncirculant blocks (BCCB) structure. The BCCB structure is then exploited to\nreduce the computational complexity of the matrix-vector products that appear\nin these methods through the use of 2D fast Fourier transforms (FFT) from\nO((L1L2)^2) down to O(L1L2 log(L1L2)) operations per iterations, where L1, L2\nare the lengths of the subdictionaries used for estimating the harmonics in the\nfirst and second dimension, respectively. We experimentally verify the proposed\nimplementation using the iterative shrinkage thresholding algorithm (ISTA), the\nfast iterative shrinkage-thresholding algorithm (FISTA), and the alternating\ndirection method of multipliers (ADMM) where we observe improvements"}
{"id": "2509.13600", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13600", "abs": "https://arxiv.org/abs/2509.13600", "authors": ["Argyris Kriezis", "Yu-Hsuan Chen", "Dennis Akos", "Sherman Lo", "Todd Walter"], "title": "GNSS Jamming and Spoofing Monitoring Using Low-Cost COTS Receivers", "comment": "Submitted to ION NAVIGATION Journal", "summary": "The Global Navigation Satellite System (GNSS) is increasingly vulnerable to\nradio frequency interference (RFI), including jamming and spoofing, which\nthreaten the integrity of navigation and timing services. This paper presents a\nmethodology for detecting and classifying RFI events using low-cost commercial\noff-the-shelf (COTS) GNSS receivers. By combining carrier-to-noise ratio (C/N0)\nmeasurements with a calibrated received power metric, a two-dimensional\ndetection space is constructed to identify and distinguish nominal, jammed,\nspoofed, and blocked signal conditions. The method is validated through both\ncontrolled jamming tests in Norway and real-world deployments in Poland, and\nthe Southeast Mediterranean which have experienced such conditions. Results\ndemonstrate that COTS-based detection, when properly calibrated, offers a\nviable and effective approach for GNSS RFI monitoring."}
{"id": "2509.13395", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.13395", "abs": "https://arxiv.org/abs/2509.13395", "authors": ["Haolong Zheng", "Yekaterina Yegorova", "Mark Hasegawa-Johnson"], "title": "TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models", "comment": null, "summary": "Speech foundation models have recently demonstrated the ability to perform\nSpeech In-Context Learning (SICL). Selecting effective in-context examples is\ncrucial for SICL performance, yet selection methodologies remain underexplored.\nIn this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline\nthat uses semantic context to enhance off-the-shelf large multimodal models'\nspeech recognition ability without fine-tuning. Across challenging automatic\nspeech recognition tasks, including accented English, multilingual speech, and\nchildren's speech, our method enables models to surpass zero-shot performance\nwith up to 84.7% relative WER reduction. We conduct ablation studies to show\nthe robustness and efficiency of our method."}
{"id": "2509.13390", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS", "I.2.1; I.2.6; I.2.10; I.5.1; I.5.2; J.2; J.7"], "pdf": "https://arxiv.org/pdf/2509.13390", "abs": "https://arxiv.org/abs/2509.13390", "authors": ["Deepti Kunte", "Bram Cornelis", "Claudio Colangeli", "Karl Janssens", "Brecht Van Baelen", "Konstantinos Gryllias"], "title": "A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds", "comment": "Submitted to: Mechanical Systems and Signal Processing", "summary": "The detection of anomalies in automotive cabin sounds is critical for\nensuring vehicle quality and maintaining passenger comfort. In many real-world\nsettings, this task is more appropriately framed as an unsupervised learning\nproblem rather than the supervised case due to the scarcity or complete absence\nof labeled faulty data. In such an unsupervised setting, the model is trained\nexclusively on healthy samples and detects anomalies as deviations from normal\nbehavior. However, in the absence of labeled faulty samples for validation and\nthe limited reliability of commonly used metrics, such as validation\nreconstruction error, effective model selection remains a significant\nchallenge. To overcome these limitations, a domain-knowledge-informed approach\nfor model selection is proposed, in which proxy-anomalies engineered through\nstructured perturbations of healthy spectrograms are used in the validation set\nto support model selection. The proposed methodology is evaluated on a\nhigh-fidelity electric vehicle dataset comprising healthy and faulty cabin\nsounds across five representative fault types viz., Imbalance, Modulation,\nWhine, Wind, and Pulse Width Modulation. This dataset, generated using advanced\nsound synthesis techniques, and validated via expert jury assessments, has been\nmade publicly available to facilitate further research. Experimental\nevaluations on the five fault cases demonstrate the selection of optimal models\nusing proxy-anomalies, significantly outperform conventional model selection\nstrategies."}
{"id": "2509.13745", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13745", "abs": "https://arxiv.org/abs/2509.13745", "authors": ["Hiroki Kuroda", "Renato Luis Garrido Cavalcante", "Masahiro Yukawa"], "title": "Theoretical Validation of the Latent Optimally Partitioned-$\\ell_2/\\ell_1$ Penalty with Application to Angular Power Spectrum Estimation", "comment": null, "summary": "This paper demonstrates that, in both theory and practice, the latent\noptimally partitioned (LOP)-$\\ell_2/\\ell_1$ penalty is effective for exploiting\nblock-sparsity without the knowledge of the concrete block structure. More\nprecisely, we first present a novel theoretical result showing that the\noptimized block partition in the LOP-$\\ell_2/\\ell_1$ penalty satisfy a\ncondition required for accurate recovery of block-sparse signals. Motivated by\nthis result, we present a new application of the LOP-$\\ell_2/\\ell_1$ penalty to\nestimation of angular power spectrum, which is block-sparse with unknown block\npartition, in MIMO communication systems. Numerical simulations show that the\nproposed use of block-sparsity with the LOP-$\\ell_2/\\ell_1$ penalty\nsignificantly improves the estimation accuracy of the angular power spectrum."}
{"id": "2509.13442", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13442", "abs": "https://arxiv.org/abs/2509.13442", "authors": ["Arnab Kumar Roy", "Hemant Kumar Kathania", "Paban Sapkota"], "title": "Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation", "comment": "Speaker-independent experiments on classification of dysarthric\n  speech severity", "summary": "Dysarthric speech severity classification is crucial for objective clinical\nassessment and progress monitoring in individuals with motor speech disorders.\nAlthough prior methods have addressed this task, achieving robust\ngeneralization in speaker-independent (SID) scenarios remains challenging. This\nwork introduces DSSCNet, a novel deep neural architecture that combines\nConvolutional, Squeeze-Excitation (SE), and Residual network, helping it\nextract discriminative representations of dysarthric speech from mel\nspectrograms. The addition of SE block selectively focuses on the important\nfeatures of the dysarthric speech, thereby minimizing loss and enhancing\noverall model performance. We also propose a cross-corpus fine-tuning framework\nfor severity classification, adapted from detection-based transfer learning\napproaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora:\nTORGO and UA-Speech under speaker-independent evaluation protocols:\nOne-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols.\nDSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and\n64.18% under LOSO setting on TORGO and UA-Speech respectively outperforming\nexisting state-of-the-art methods. Upon fine-tuning, the performance improves\nsubstantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25%\non UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. These\nresults demonstrate the effectiveness and generalizability of DSSCNet for\nfine-grained severity classification across diverse dysarthric speech datasets."}
{"id": "2509.13548", "categories": ["cs.SD", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.13548", "abs": "https://arxiv.org/abs/2509.13548", "authors": ["Manan Mittal", "Thomas Deppisch", "Joseph Forrer", "Chris Le Sueur", "Zamir Ben-Hur", "David Lou Along", "Daniel D. E. Wong"], "title": "Field of View Enhanced Signal Dependent Binauralization with Mixture of Experts Framework for Continuous Source Motion", "comment": "5 pages, 3 figures", "summary": "We propose a novel mixture of experts framework for field-of-view enhancement\nin binaural signal matching. Our approach enables dynamic spatial audio\nrendering that adapts to continuous talker motion, allowing users to emphasize\nor suppress sounds from selected directions while preserving natural binaural\ncues. Unlike traditional methods that rely on explicit direction-of-arrival\nestimation or operate in the Ambisonics domain, our signal-dependent framework\ncombines multiple binaural filters in an online manner using implicit\nlocalization. This allows for real-time tracking and enhancement of moving\nsound sources, supporting applications such as speech focus, noise reduction,\nand world-locked audio in augmented and virtual reality. The method is agnostic\nto array geometry offering a flexible solution for spatial audio capture and\npersonalized playback in next-generation consumer audio devices."}
{"id": "2509.13786", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13786", "abs": "https://arxiv.org/abs/2509.13786", "authors": ["SaiKrishna Saketh Yellapragada", "Esa Ollila", "Mario Costa"], "title": "Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization", "comment": "Submitted for 51st International Conference on Acoustics, Speech, and\n  Signal Processing, ICASSP 2026", "summary": "As wireless communication systems advance toward Sixth Generation (6G) Radio\nAccess Networks (RAN), Deep Learning (DL)-based neural receivers are emerging\nas transformative solutions for Physical Layer (PHY) processing, delivering\nsuperior Block Error Rate (BLER) performance compared to traditional\nmodel-based approaches. Practical deployment on resource-constrained hardware,\nhowever, requires efficient quantization to reduce latency, energy, and memory\nwithout sacrificing reliability. We extend Post-Training Quantization (PTQ)\nbaselines with Quantization-Aware Training (QAT), which incorporates\nlow-precision simulation during training for robustness at ultra-low bitwidths.\nOur study applies QAT/PTQ to a neural receiver architecture and evaluates\nacross 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS\nenvironments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit\nQAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT\nmodels are also shown to outperform PTQ models by up to 3 dB, and yield 8x\ncompression. These results demonstrate that QAT is a key enabler of\nlow-complexity and latency-constrained inference at the PHY layer, facilitating\nreal-time processing in 6G edge devices"}
{"id": "2509.13658", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13658", "abs": "https://arxiv.org/abs/2509.13658", "authors": ["Shulei Ji", "Zihao Wang", "Le Ma", "Jiaxing Yu", "Kejun Zhang"], "title": "Assessing Data Replication in Symbolic Music via Adapted Structural Similarity Index Measure", "comment": null, "summary": "AI-generated music may inadvertently replicate samples from the training\ndata, raising concerns of plagiarism. Similarity measures can quantify such\nreplication, thereby offering supervision and guidance for music generation\nmodels. Existing similarity measure methods for symbolic music mainly target\nmelody repetition, leaving a gap in assessing complex music with rich textures\nand expressive performance characteristics. To address this gap, we introduce\nSSIMuse, the first adaptation of the Structural Similarity Index Measure (SSIM)\nfrom images to symbolic music. Specifically, we represent symbolic music as\nimage-like piano rolls in binary and velocity-based forms. Build upon these\nrepresentations, we reinterprete and suitably modify the SSIM components in the\nmusical context to develop two variants, i.e., SSIMuse-B and SSIMuse-V, for\nevaluating data replication in composition and dynamic performance,\nrespectively. Controlled experiments on synthetic samples from multiple\ndatasets show that SSIMuse can reliably detect exact replication at a\ngranularity of at least one bar. SSIMuse enables open evaluation of replication\nin music generation and draws attention to its broader ethical, social, legal,\nand economic implications. The code is available at\nhttps://github.com/Tayjsl97/SSIMuse."}
{"id": "2509.13825", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13825", "abs": "https://arxiv.org/abs/2509.13825", "authors": ["Fei Liu", "Yang Ai", "Zhen-Hua Ling"], "title": "Neural Speech Separation with Parallel Amplitude and Phase Spectrum Estimation", "comment": "Accepted by APSIPA2025", "summary": "This paper proposes APSS, a novel neural speech separation model with\nparallel amplitude and phase spectrum estimation. Unlike most existing speech\nseparation methods, the APSS distinguishes itself by explicitly estimating the\nphase spectrum for more complete and accurate separation. Specifically, APSS\nfirst extracts the amplitude and phase spectra from the mixed speech signal.\nSubsequently, the extracted amplitude and phase spectra are fused by a feature\ncombiner into joint representations, which are then further processed by a deep\nprocessor with time-frequency Transformers to capture temporal and spectral\ndependencies. Finally, leveraging parallel amplitude and phase separators, the\nAPSS estimates the respective spectra for each speaker from the resulting\nfeatures, which are then combined via inverse short-time Fourier transform\n(iSTFT) to reconstruct the separated speech signals. Experimental results\nindicate that APSS surpasses both time-domain separation methods and\nimplicit-phase-estimation-based time-frequency approaches. Also, APSS achieves\nstable and competitive results on multiple datasets, highlighting its strong\ngeneralization capability and practical applicability."}
{"id": "2509.13807", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.13807", "abs": "https://arxiv.org/abs/2509.13807", "authors": ["Ruiqi Kong", "He Chen"], "title": "Domino: Dominant Path-based Compensation for Hardware Impairments in Modern WiFi Sensing", "comment": "5 pages, 5 figures", "summary": "WiFi sensing faces a critical reliability challenge due to hardware-induced\nRF distortions, especially with modern, market-dominant WiFi cards supporting\n802.11ac/ax protocols. These cards employ sensitive automatic gain control and\nseparate RF chains, introducing complex and dynamic distortions that render\nexisting compensation methods ineffective. In this paper, we introduce Domino,\na new framework that transforms channel state information (CSI) into channel\nimpulse response (CIR) and leverages it for precise distortion compensation.\nDomino is built on the key insight that hardware-induced distortions impact all\nsignal paths uniformly, allowing the dominant static path to serve as a\nreliable reference for effective compensation through delay-domain processing.\nReal-world respiration monitoring experiments show that Domino achieves at\nleast 2x higher mean accuracy over existing methods, maintaining robust\nperformance with a median error below 0.24 bpm, even using a single antenna in\nboth direct line-of-sight and obstructed scenarios."}
{"id": "2509.13667", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13667", "abs": "https://arxiv.org/abs/2509.13667", "authors": ["Hui-Peng Du", "Yang Ai", "Zhen-Hua Ling"], "title": "A Distilled Low-Latency Neural Vocoder with Explicit Amplitude and Phase Prediction", "comment": "Accepted by APSIPA ASC 2025", "summary": "The majority of mainstream neural vocoders primarily focus on speech quality\nand generation speed, while overlooking latency, which is a critical factor in\nreal-time applications. Excessive latency leads to noticeable delays in user\ninteraction, severely degrading the user experience and rendering such systems\nimpractical for real-time use. Therefore, this paper proposes DLL-APNet, a\nDistilled Low-Latency neural vocoder which first predicts the Amplitude and\nPhase spectra explicitly from input mel spectrogram and then reconstructs the\nspeech waveform via inverse short-time Fourier transform (iSTFT). The DLL-APNet\nvocoder leverages causal convolutions to constrain the utilization of\ninformation to current and historical contexts, effectively minimizing latency.\nTo mitigate speech quality degradation caused by causal constraints, a\nknowledge distillation strategy is proposed, where a pre-trained non-causal\nteacher vocoder guides intermediate feature generation of the causal student\nDLL-APNet vocoder. Experimental results demonstrate that the proposed DLL-APNet\nvocoder produces higher-quality speech than other causal vocoders, while\nrequiring fewer computational resources. Furthermore, the proposed DLL-APNet\nvocoder achieves speech quality on par with mainstream non-causal neural\nvocoders, validating its ability to deliver both high perceptual quality and\nlow latency."}
{"id": "2509.13853", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13853", "abs": "https://arxiv.org/abs/2509.13853", "authors": ["Shun Huang", "Zhihua Fang", "Liang He"], "title": "Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection", "comment": "Accept ICASSP 2025", "summary": "Unsupervised anomalous sound detection aims to detect unknown anomalous\nsounds by training a model using only normal audio data. Despite advancements\nin self-supervised methods, the issue of frequent false alarms when handling\nsamples of the same type from different machines remains unresolved. This paper\nintroduces a novel training technique called one-stage supervised contrastive\nlearning (OS-SCL), which significantly addresses this problem by perturbing\nfeatures in the embedding space and employing a one-stage noisy supervised\ncontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved\n94.64\\% AUC, 88.42\\% pAUC, and 89.24\\% mAUC using only Log-Mel features.\nAdditionally, a time-frequency feature named TFgram is proposed, which is\nextracted from raw audio. This feature effectively captures critical\ninformation for anomalous sound detection, ultimately achieving 95.71\\% AUC,\n90.23\\% pAUC, and 91.23\\% mAUC. The source code is available at:\n\\underline{www.github.com/huangswt/OS-SCL}."}
{"id": "2509.13822", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13822", "abs": "https://arxiv.org/abs/2509.13822", "authors": ["Hao Sun", "Shicong Liu", "Xianghao Yu", "Ying Sun"], "title": "Flow Matching-Based Active Learning for Radio Map Construction with Low-Altitude UAVs", "comment": null, "summary": "The employment of unmanned aerial vehicles (UAVs) in the lowaltitude economy\nnecessitates precise and real-time radio maps for reliable communication and\nsafe navigation. However, constructing such maps is hindered by the\ninfeasibility of exhaustive measurements due to UAVs' limited flight endurance.\nTo address this, we propose a novel active learning framework for low-altitude\nradio map construction based on limited measurements. First, a Plug-and-Play\n(PnP)-refined flow matching algorithm is introduced, which leverages flow\nmatching as a powerful generative prior within a PnP scheme to reconstruct\nhigh-fidelity radio maps. Second, the generative nature of flow matching is\nexploited to quantify uncertainty by generating an ensemble of radio maps and\ncomputing the location-wise variance. The resulting uncertainty map guides a\nmulti-objective candidate selection and then a trajectory is planned via\nutility-aware path search (UAPS), directing the UAV to the most informative\nlocations while taking travel costs into account. Simulation results\ndemonstrate that our method significantly outperforms the baselines, achieving\nmore than a 70% reduction in normalized mean squared error (NMSE)."}
{"id": "2509.13670", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13670", "abs": "https://arxiv.org/abs/2509.13670", "authors": ["En-Wei Zhang", "Hui-Peng Du", "Xiao-Hang Jiang", "Yang Ai", "Zhen-Hua Ling"], "title": "A High-Quality and Low-Complexity Streamable Neural Speech Codec with Knowledge Distillation", "comment": "Accepted by APSIPA ASC 2025", "summary": "While many current neural speech codecs achieve impressive reconstructed\nspeech quality, they often neglect latency and complexity considerations,\nlimiting their practical deployment in downstream tasks such as real-time\nspeech communication and efficient speech compression. In our previous work, we\nproposed StreamCodec, which enables streamable speech coding by leveraging\nmodel causalization and a scalar-vector-combined quantization strategy, but its\nreconstructed quality and complexity still have room for improvement.\nTherefore, this paper proposes an improved iteration of StreamCodec, named\nStreamCodec2. The StreamCodec2 supports streamable and lightweight speech\ncoding by adopting a fully causal architecture and reducing the convolutional\nchannels. To compensate for the speech quality degradation caused by model\ncausalization and pruning, we introduce a non-causal, high-complexity teacher\ncodec to guide the training of StreamCodec2 through knowledge distillation.\nExperimental results demonstrate that our proposed StreamCodec2, trained with\nthe knowledge distillation strategy, can achieve high-quality speech\nreconstruction while maintaining low latency (only 20 ms), low computational\ncomplexity (only 910 MFLOPs), and low model complexity (only 5.4 M parameters)."}
{"id": "2509.14003", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14003", "abs": "https://arxiv.org/abs/2509.14003", "authors": ["Liting Gao", "Yi Yuan", "Yaru Chen", "Yuelan Cheng", "Zhenbo Li", "Juan Wen", "Shubin Zhang", "Wenwu Wang"], "title": "RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing", "comment": null, "summary": "Diffusion models have shown remarkable progress in text-to-audio generation.\nHowever, text-guided audio editing remains in its early stages. This task\nfocuses on modifying the target content within an audio signal while preserving\nthe rest, thus demanding precise localization and faithful editing according to\nthe text prompt. Existing training-based and zero-shot methods that rely on\nfull-caption or costly optimization often struggle with complex editing or lack\npracticality. In this work, we propose a novel end-to-end efficient rectified\nflow matching-based diffusion framework for audio editing, and construct a\ndataset featuring overlapping multi-event audio to support training and\nbenchmarking in complex scenarios. Experiments show that our model achieves\nfaithful semantic alignment without requiring auxiliary captions or masks,\nwhile maintaining competitive editing quality across metrics."}
{"id": "2509.13851", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.13851", "abs": "https://arxiv.org/abs/2509.13851", "authors": ["Hao Su", "Jiangtao Wang", "Yongchao Wang"], "title": "FFT-Free PAPR Reduction Methods for OFDM Signals", "comment": "6 page, 7 figures", "summary": "In this paper, we propose two low-complexity peak to average power\nratio(PAPR) reduction algorithms for orthogonal frequency division\nmultiplexing(OFDM) signals. The main content is as follows: First, a non-convex\noptimization model is established by minimizing the signal distortion power.\nThen, a customized alternating direction method of multipliers(ADMM) algorithm\nis proposed to solve the problem, named time domain ADMM(T-ADMM) along with an\nimproved version called T-ADMM with constrain update(TCU-ADMM). In the\nalgorithms, all subproblems can be solved analytically, and each iteration has\nlinear computational complexity. These algorithms circumvents the challenges\nposed by repeated fast Fourier transform(FFT) and inverse FFT(IFFT) operations\nin traditional PAPR reduction algorithms. Additionally, we prove that the\nT-ADMM algorithm is theoretically guaranteed convergent if proper parameter is\nchosen. Finally, simulation results demonstrate the effectiveness of the\nproposed methods."}
{"id": "2509.13741", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13741", "abs": "https://arxiv.org/abs/2509.13741", "authors": ["Younghoo Kwon", "Dongheon Lee", "Dohwan Kim", "Jung-Woo Choi"], "title": "Self-Guided Target Sound Extraction and Classification Through Universal Sound Separation Model and Multiple Clues", "comment": "5 pages, 2 figures, submitted to DCASE workshop 2025", "summary": "This paper introduces a multi-stage self-directed framework designed to\naddress the spatial semantic segmentation of sound scene (S5) task in the DCASE\n2025 Task 4 challenge. This framework integrates models focused on three\ndistinct tasks: Universal Sound Separation (USS), Single-label Classification\n(SC), and Target Sound Extraction (TSE). Initially, USS breaks down a complex\naudio mixture into separate source waveforms. Each of these separated waveforms\nis then processed by a SC block, generating two critical pieces of information:\nthe waveform itself and its corresponding class label. These serve as inputs\nfor the TSE stage, which isolates the source that matches this information.\nSince these inputs are produced within the system, the extraction target is\nidentified autonomously, removing the necessity for external guidance. The\nextracted waveform can be looped back into the classification task, creating a\ncycle of iterative refinement that progressively enhances both separability and\nlabeling accuracy. We thus call our framework a multi-stage self-guided system\ndue to these self-contained characteristics. On the official evaluation\ndataset, the proposed system achieves an 11.00 dB increase in class-aware\nsignal-to-distortion ratio improvement (CA-SDRi) and a 55.8\\% accuracy in label\nprediction, outperforming the ResUNetK baseline by 4.4 dB and 4.3\\%,\nrespectively, and achieving first place among all submissions."}
{"id": "2509.14049", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14049", "abs": "https://arxiv.org/abs/2509.14049", "authors": ["Jordi Grau-Haro", "Ruben Ribes-Serrano", "Javier Naranjo-Alcazar", "Marta Garcia-Ballesteros", "Pedro Zuccarello"], "title": "Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices", "comment": "Accepted at Computing Conference 2026, London, UK", "summary": "Convolutional Neural Networks (CNNs) have demonstrated exceptional\nperformance in audio tagging tasks. However, deploying these models on\nresource-constrained devices like the Raspberry Pi poses challenges related to\ncomputational efficiency and thermal management. In this paper, a comprehensive\nevaluation of multiple convolutional neural network (CNN) architectures for\naudio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D\nmodels from the Pretrained Audio Neural Networks (PANNs) framework, a\nConvNeXt-based model adapted for audio classification, as well as MobileNetV3\narchitectures. In addition, two PANNs-derived networks, CNN9 and CNN13,\nrecently proposed, are also evaluated. To enhance deployment efficiency and\nportability across diverse hardware platforms, all models are converted to the\nOpen Neural Network Exchange (ONNX) format. Unlike previous works that focus on\na single model, our analysis encompasses a broader range of architectures and\ninvolves continuous 24-hour inference sessions to assess performance stability.\nOur experiments reveal that, with appropriate model selection and optimization,\nit is possible to maintain consistent inference latency and manage thermal\nbehavior effectively over extended periods. These findings provide valuable\ninsights for deploying audio tagging models in real-world edge computing\nscenarios."}
{"id": "2509.13940", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.13940", "abs": "https://arxiv.org/abs/2509.13940", "authors": ["Weifeng Zhu", "Junyuan Gao", "Shuowen Zhang", "Liang Liu"], "title": "Reconfigurable Intelligent Surface-Assisted Multiuser Tracking and Signal Detection in ISAC", "comment": "6 pages, 6 figures, accepted by IEEE conference", "summary": "This paper investigates the multiuser tracking and signal detection problem\nin integrated sensing and communication (ISAC) systems with the assistance of\nreconfigurable intelligent surfaces (RISs). Due to the diverse and high user\nmobility, the tracking and signal detection performance can be significantly\ndeteriorated without choreographed user state (position and velocity) updating\nprinciple. To tackle this challenge, we manage to establish a comprehensive\nprobabilistic signal model to characterize the interdependencies among user\nstates, transmit signals, and received signals during the tracking procedure.\nBased on the Bayesian problem formulation, we further propose a novel hybrid\nvariational message passing algorithm for the online estimation of user states,\nwhich can iteratively update the posterior probabilities of user states during\neach tracking frame with computational efficiency. Numerical results are\nprovided to demonstrate that the proposed algorithm can significantly improve\nboth of the tracking and signal detection performance over the representative\nBayesian estimation counterparts."}
{"id": "2509.13785", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13785", "abs": "https://arxiv.org/abs/2509.13785", "authors": ["Bingshen Mu", "Pengcheng Guo", "Zhaokai Sun", "Shuai Wang", "Hexin Liu", "Mingchen Shao", "Lei Xie", "Eng Siong Chng", "Longshuai Xiao", "Qiangze Feng", "Daliang Wang"], "title": "Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods", "comment": null, "summary": "This paper summarizes the Interspeech2025 Multilingual Conversational Speech\nLanguage Model (MLC-SLM) challenge, which aims to advance the exploration of\nbuilding effective multilingual conversational speech LLMs (SLLMs). We provide\na detailed description of the task settings for the MLC-SLM challenge, the\nreleased real-world multilingual conversational speech dataset totaling\napproximately 1,604 hours, and the baseline systems for participants. The\nMLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489\nvalid leaderboard results and 14 technical reports for the two tasks. We\ndistill valuable insights on building multilingual conversational SLLMs based\non submissions from participants, aiming to contribute to the advancement of\nthe community."}
{"id": "2509.14052", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14052", "abs": "https://arxiv.org/abs/2509.14052", "authors": ["Junan Zhang", "Yunjia Zhang", "Xueyao Zhang", "Zhizheng Wu"], "title": "AnyAccomp: Generalizable Accompaniment Generation via Quantized Melodic Bottleneck", "comment": "Demo audio and code: https://anyaccomp.github.io", "summary": "Singing Accompaniment Generation (SAG) is the process of generating\ninstrumental music for a given clean vocal input. However, existing SAG\ntechniques use source-separated vocals as input and overfit to separation\nartifacts. This creates a critical train-test mismatch, leading to failure on\nclean, real-world vocal inputs. We introduce AnyAccomp, a framework that\nresolves this by decoupling accompaniment generation from source-dependent\nartifacts. AnyAccomp first employs a quantized melodic bottleneck, using a\nchromagram and a VQ-VAE to extract a discrete and timbre-invariant\nrepresentation of the core melody. A subsequent flow-matching model then\ngenerates the accompaniment conditioned on these robust codes. Experiments show\nAnyAccomp achieves competitive performance on separated-vocal benchmarks while\nsignificantly outperforming baselines on generalization test sets of clean\nstudio vocals and, notably, solo instrumental tracks. This demonstrates a\nqualitative leap in generalization, enabling robust accompaniment for\ninstruments - a task where existing models completely fail - and paving the way\nfor more versatile music co-creation tools. Demo audio and code:\nhttps://anyaccomp.github.io"}
{"id": "2509.13961", "categories": ["eess.SP", "92C55, 68T10, 93C85", "I.5.4; J.3; H.1.2"], "pdf": "https://arxiv.org/pdf/2509.13961", "abs": "https://arxiv.org/abs/2509.13961", "authors": ["Lorenza Angelini", "Dimitar Stanev", "Marta Płonka", "Rafał Klimas", "Natan Napiórkowski", "Gabriela González Chan", "Lisa Bunn", "Paul S Glazier", "Richard Hosking", "Jenny Freeman", "Jeremy Hobart", "Jonathan Marsden", "Licinio Craveiro", "Mike D Rinderknecht", "Mattia Zanon"], "title": "Adaptive and robust smartphone-based step detection in multiple sclerosis", "comment": "66 pages total, 6 figures, 1 table, 23 supplementary appendix pages,\n  2 supplementary figures, 6 supplementary tables", "summary": "Background: Many attempts to validate gait pipelines that process sensor data\nto detect gait events have focused on the detection of initial contacts only in\nsupervised settings using a single sensor. Objective: To evaluate the\nperformance of a gait pipeline in detecting initial/final contacts using a step\ndetection algorithm adaptive to different test settings, smartphone wear\nlocations, and gait impairment levels. Methods: In GaitLab (ISRCTN15993728),\nhealthy controls (HC) and people with multiple sclerosis (PwMS; Expanded\nDisability Status Scale 0.0-6.5) performed supervised Two-Minute Walk Test\n[2MWT] (structured in-lab overground and treadmill 2MWT) during two on-site\nvisits carrying six smartphones and unsupervised walking activities (structured\nand unstructured real-world walking) daily for 10-14 days using a single\nsmartphone. Reference gait data were collected with a motion capture system or\nGait Up sensors. The pipeline's performance in detecting initial/final contacts\nwas evaluated through F1 scores and absolute temporal error with respect to\nreference measurement systems. Results: We studied 35 HC and 93 PwMS.\nInitial/final contacts were accurately detected across all smartphone wear\nlocations. Median F1 scores for initial/final contacts on in-lab 2MWT were\n>=98.2%/96.5% in HC and >=98.5%/97.7% in PwMS. F1 scores remained high on\nstructured (HC: 100% [0.3%]/100% [0.2%]; PwMS: 99.5% [1.9%]/99.4% [2.5%]) and\nunstructured real-world walking (HC: 97.8% [2.6%]/97.8% [2.8%]; PwMS: 94.4%\n[6.2%]/94.0% [6.5%]). Median temporal errors were <=0.08 s. Neither age, sex,\ndisease severity, walking aid use, nor setting (outdoor/indoor) impacted\npipeline performance (all p>0.05). Conclusion: This gait pipeline accurately\nand consistently detects initial and final contacts in PwMS across different\nsmartphone locations and environments, highlighting its potential for\nreal-world gait assessment."}
{"id": "2509.13878", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13878", "abs": "https://arxiv.org/abs/2509.13878", "authors": ["Janne Laakkonen", "Ivan Kukanov", "Ville Hautamäki"], "title": "Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection", "comment": "6 pages, 3 figures, 1 table", "summary": "Foundation models such as Wav2Vec2 excel at representation learning in speech\ntasks, including audio deepfake detection. However, after being fine-tuned on a\nfixed set of bonafide and spoofed audio clips, they often fail to generalize to\nnovel deepfake methods not represented in training. To address this, we propose\na mixture-of-LoRA-experts approach that integrates multiple low-rank adapters\n(LoRA) into the model's attention layers. A routing mechanism selectively\nactivates specialized experts, enhancing adaptability to evolving deepfake\nattacks. Experimental results show that our method outperforms standard\nfine-tuning in both in-domain and out-of-domain scenarios, reducing equal error\nrates relative to baseline models. Notably, our best MoE-LoRA model lowers the\naverage out-of-domain EER from 8.55\\% to 6.08\\%, demonstrating its\neffectiveness in achieving generalizable audio deepfake detection."}
{"id": "2509.13442", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13442", "abs": "https://arxiv.org/abs/2509.13442", "authors": ["Arnab Kumar Roy", "Hemant Kumar Kathania", "Paban Sapkota"], "title": "Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation", "comment": "Speaker-independent experiments on classification of dysarthric\n  speech severity", "summary": "Dysarthric speech severity classification is crucial for objective clinical\nassessment and progress monitoring in individuals with motor speech disorders.\nAlthough prior methods have addressed this task, achieving robust\ngeneralization in speaker-independent (SID) scenarios remains challenging. This\nwork introduces DSSCNet, a novel deep neural architecture that combines\nConvolutional, Squeeze-Excitation (SE), and Residual network, helping it\nextract discriminative representations of dysarthric speech from mel\nspectrograms. The addition of SE block selectively focuses on the important\nfeatures of the dysarthric speech, thereby minimizing loss and enhancing\noverall model performance. We also propose a cross-corpus fine-tuning framework\nfor severity classification, adapted from detection-based transfer learning\napproaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora:\nTORGO and UA-Speech under speaker-independent evaluation protocols:\nOne-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols.\nDSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and\n64.18% under LOSO setting on TORGO and UA-Speech respectively outperforming\nexisting state-of-the-art methods. Upon fine-tuning, the performance improves\nsubstantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25%\non UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. These\nresults demonstrate the effectiveness and generalizability of DSSCNet for\nfine-grained severity classification across diverse dysarthric speech datasets."}
{"id": "2509.13975", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13975", "abs": "https://arxiv.org/abs/2509.13975", "authors": ["Ilker Bayram"], "title": "Classification Filtering", "comment": null, "summary": "We consider a streaming signal in which each sample is linked to a latent\nclass. We assume that multiple classifiers are available, each providing class\nprobabilities with varying degrees of accuracy. These classifiers are employed\nfollowing a straightforward and fixed policy. In this setting, we consider the\nproblem of fusing the output of the classifiers while incorporating the\ntemporal aspect to improve classification accuracy. We propose a state-space\nmodel and develop a filter tailored for realtime execution. We demonstrate the\neffectiveness of the proposed filter in an activity classification application\nbased on inertial measurement unit (IMU) data from a wearable device."}
{"id": "2509.13927", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13927", "abs": "https://arxiv.org/abs/2509.13927", "authors": ["Kevin Wilkinghoff", "Zheng-Hua Tan"], "title": "DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models", "comment": null, "summary": "Reasoning about spatial audio with large language models requires a spatial\naudio encoder as an acoustic front-end to obtain audio embeddings for further\nprocessing. Such an encoder needs to capture all information required to detect\nthe type of sound events, as well as the direction and distance of their\ncorresponding sources. Accomplishing this with a single audio encoder is\ndemanding as the information required for each of these tasks is mostly\nindependent of each other. As a result, the performance obtained with a single\nencoder is often worse than when using task-specific audio encoders. In this\nwork, we present DSpAST, a novel audio encoder based on SpatialAST that learns\ndisentangled representations of spatial audio while having only 0.2% additional\nparameters. Experiments on SpatialSoundQA with the spatial audio reasoning\nsystem BAT demonstrate that DSpAST significantly outperforms SpatialAST."}
{"id": "2509.13785", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13785", "abs": "https://arxiv.org/abs/2509.13785", "authors": ["Bingshen Mu", "Pengcheng Guo", "Zhaokai Sun", "Shuai Wang", "Hexin Liu", "Mingchen Shao", "Lei Xie", "Eng Siong Chng", "Longshuai Xiao", "Qiangze Feng", "Daliang Wang"], "title": "Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods", "comment": null, "summary": "This paper summarizes the Interspeech2025 Multilingual Conversational Speech\nLanguage Model (MLC-SLM) challenge, which aims to advance the exploration of\nbuilding effective multilingual conversational speech LLMs (SLLMs). We provide\na detailed description of the task settings for the MLC-SLM challenge, the\nreleased real-world multilingual conversational speech dataset totaling\napproximately 1,604 hours, and the baseline systems for participants. The\nMLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489\nvalid leaderboard results and 14 technical reports for the two tasks. We\ndistill valuable insights on building multilingual conversational SLLMs based\non submissions from participants, aiming to contribute to the advancement of\nthe community."}
{"id": "2509.13984", "categories": ["eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.13984", "abs": "https://arxiv.org/abs/2509.13984", "authors": ["Drake Silbernagel", "Yu Rong", "Isabella Lenz", "Prithvi Hemanth", "Carl Morgenstern", "Owen Ma", "Nolan Matthews", "Nader Zaki", "Kyle W. Martin", "John D. Elgin", "Jacob Holtom", "Daniel W. Bliss", "Kimberly Frey"], "title": "Distributed Coherent Beamforming at 60 GHz Enabled by Optically-Established Coherence", "comment": null, "summary": "We implement and experimentally demonstrate a 60 GHz distributed system\nleveraging an optical time synchronization system that provides precise time\nand frequency alignment between independent elements of the distributed mesh.\nUtilizing such accurate coherence, we perform receive beamforming with\ninterference rejection and transmit nulling. In these configurations, the\nsystem achieves a coherent gain over an incoherent network of N nodes,\nsignificantly improving the relevant signal power ratios. Our system\ndemonstrates extended array phase coherence times, enabling advanced\ntechniques. Results from over-the-air experiments demonstrate a 14.3 dB\nsignal-to-interference-plus-noise improvement in interference-laden scenarios\nwith a contributing 13.5 dB null towards interference in receive beamforming.\nIn transmit nulling, a signal-to-noise ratio (SNR) gain of 7.9 dB is measured\ntowards an intended receiver while maintaining an SNR reduction of 8.9 dB at\nanother receiver. These findings represent the use of distributed coherence in\nthe V band without the use of GPS timing."}
{"id": "2509.13989", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.13989", "abs": "https://arxiv.org/abs/2509.13989", "authors": ["Yi-Cheng Lin", "Huang-Cheng Chou", "Tzu-Chieh Wei", "Kuan-Yu Chen", "Hung-yi Lee"], "title": "Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems", "comment": "Submission to ICASSP 2026", "summary": "Instruction-guided text-to-speech (ITTS) enables users to control speech\ngeneration through natural language prompts, offering a more intuitive\ninterface than traditional TTS. However, the alignment between user style\ninstructions and listener perception remains largely unexplored. This work\nfirst presents a perceptual analysis of ITTS controllability across two\nexpressive dimensions (adverbs of degree and graded emotion intensity) and\ncollects human ratings on speaker age and word-level emphasis attributes. To\ncomprehensively reveal the instruction-perception gap, we provide a data\ncollection with large-scale human evaluations, named Expressive VOice Control\n(E-VOC) corpus. Furthermore, we reveal that (1) gpt-4o-mini-tts is the most\nreliable ITTS model with great alignment between instruction and generated\nutterances across acoustic dimensions. (2) The 5 analyzed ITTS systems tend to\ngenerate Adult voices even when the instructions ask to use child or Elderly\nvoices. (3) Fine-grained control remains a major challenge, indicating that\nmost ITTS systems have substantial room for improvement in interpreting\nslightly different attribute instructions."}
{"id": "2509.13878", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13878", "abs": "https://arxiv.org/abs/2509.13878", "authors": ["Janne Laakkonen", "Ivan Kukanov", "Ville Hautamäki"], "title": "Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection", "comment": "6 pages, 3 figures, 1 table", "summary": "Foundation models such as Wav2Vec2 excel at representation learning in speech\ntasks, including audio deepfake detection. However, after being fine-tuned on a\nfixed set of bonafide and spoofed audio clips, they often fail to generalize to\nnovel deepfake methods not represented in training. To address this, we propose\na mixture-of-LoRA-experts approach that integrates multiple low-rank adapters\n(LoRA) into the model's attention layers. A routing mechanism selectively\nactivates specialized experts, enhancing adaptability to evolving deepfake\nattacks. Experimental results show that our method outperforms standard\nfine-tuning in both in-domain and out-of-domain scenarios, reducing equal error\nrates relative to baseline models. Notably, our best MoE-LoRA model lowers the\naverage out-of-domain EER from 8.55\\% to 6.08\\%, demonstrating its\neffectiveness in achieving generalizable audio deepfake detection."}
{"id": "2509.14062", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14062", "abs": "https://arxiv.org/abs/2509.14062", "authors": ["Saifur Rahman", "Syed Luqman Shah", "Salman Khan", "Jalal Khan", "Muhammad Irfan", "Maaz Shafi", "Said Muhammad", "Fazal Muhammad", "Mohammad Shahed Akond"], "title": "Distributed Deep Learning with RIS Grouping for Accurate Cascaded Channel Estimation", "comment": "Submitted for Publication", "summary": "Reconfigurable Intelligent Surface (RIS) panels are envisioned as a key\ntechnology for sixth-generation (6G) wireless networks, providing a\ncost-effective means to enhance coverage and spectral efficiency. A critical\nchallenge is the estimation of the cascaded base station (BS)-RIS-user channel,\nsince the passive nature of RIS elements prevents direct channel acquisition,\nincurring prohibitive pilot overhead, computational complexity, and energy\nconsumption. To address this, we propose a deep learning (DL)-based channel\nestimation framework that reduces pilot overhead by grouping RIS elements and\nreconstructing the cascaded channel from partial pilot observations.\nFurthermore, conventional DL models trained under single-user settings suffer\nfrom poor generalization across new user locations and propagation scenarios.\nWe develop a distributed machine learning (DML) strategy in which the BS and\nusers collaboratively train a shared neural network using diverse channel\ndatasets collected across the network, thereby achieving robust generalization.\nBuilding on this foundation, we design a hierarchical DML neural architecture\nthat first classifies propagation conditions and then employs scenario-specific\nfeature extraction to further improve estimation accuracy. Simulation results\nconfirm that the proposed framework substantially reduces pilot overhead and\ncomplexity while outperforming conventional methods and single-user models in\nchannel estimation accuracy. These results demonstrate the practicality and\neffectiveness of the proposed approach for 6G RIS-assisted systems."}
{"id": "2509.14069", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14069", "abs": "https://arxiv.org/abs/2509.14069", "authors": ["Xikun Lu", "Fang Liu", "Weizhi Shi", "Jinqiu Sang"], "title": "Lightweight Implicit Neural Network for Binaural Audio Synthesis", "comment": "Submitted to ICASSP 2026", "summary": "High-fidelity binaural audio synthesis is crucial for immersive listening,\nbut existing methods require extensive computational resources, limiting their\nedge-device application. To address this, we propose the Lightweight Implicit\nNeural Network (LINN), a novel two-stage framework. LINN first generates\ninitial estimates using a time-domain warping, which is then refined by an\nImplicit Binaural Corrector (IBC) module. IBC is an implicit neural network\nthat predicts amplitude and phase corrections directly, resulting in a highly\ncompact model architecture. Experimental results show that LINN achieves\nstatistically comparable perceptual quality to the best-performing baseline\nmodel while significantly improving computational efficiency. Compared to the\nmost efficient existing method, LINN achieves a 72.7% reduction in parameters\nand significantly fewer compute operations (MACs). This demonstrates that our\napproach effectively addresses the trade-off between synthesis quality and\ncomputational efficiency, providing a new solution for high-fidelity\nedge-device spatial audio applications."}
{"id": "2509.13927", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.13927", "abs": "https://arxiv.org/abs/2509.13927", "authors": ["Kevin Wilkinghoff", "Zheng-Hua Tan"], "title": "DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models", "comment": null, "summary": "Reasoning about spatial audio with large language models requires a spatial\naudio encoder as an acoustic front-end to obtain audio embeddings for further\nprocessing. Such an encoder needs to capture all information required to detect\nthe type of sound events, as well as the direction and distance of their\ncorresponding sources. Accomplishing this with a single audio encoder is\ndemanding as the information required for each of these tasks is mostly\nindependent of each other. As a result, the performance obtained with a single\nencoder is often worse than when using task-specific audio encoders. In this\nwork, we present DSpAST, a novel audio encoder based on SpatialAST that learns\ndisentangled representations of spatial audio while having only 0.2% additional\nparameters. Experiments on SpatialSoundQA with the spatial audio reasoning\nsystem BAT demonstrate that DSpAST significantly outperforms SpatialAST."}
{"id": "2509.14072", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14072", "abs": "https://arxiv.org/abs/2509.14072", "authors": ["Vincent Lauinger", "Lennart Schmitz", "Patrick Matalla", "Andrej Rode", "Sebastian Randel", "Laurent Schmalen"], "title": "Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission", "comment": "Accepted and to be presented at the European Conference on Optical\n  Communication (ECOC) 2025", "summary": "We demonstrate the effectiveness of a novel phase-noise-tolerant,\nvariational-autoencoder-based equalization scheme for\nspace-division-multiplexed (SDM) transmission in an experiment over 150km of\nrandomly-coupled multi-core fibers."}
{"id": "2509.14076", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14076", "abs": "https://arxiv.org/abs/2509.14076", "authors": ["Xikun Lu", "Yujian Ma", "Xianquan Jiang", "Xuelong Wang", "Jinqiu Sang"], "title": "A Lightweight Fourier-based Network for Binaural Speech Enhancement with Spatial Cue Preservation", "comment": "Submitted to ICASSP 2026", "summary": "Binaural speech enhancement faces a severe trade-off challenge, where\nstate-of-the-art performance is achieved by computationally intensive\narchitectures, while lightweight solutions often come at the cost of\nsignificant performance degradation. To bridge this gap, we propose the Global\nAdaptive Fourier Network (GAF-Net), a lightweight deep complex network that\naims to establish a balance between performance and computational efficiency.\nThe GAF-Net architecture consists of three components. First, a dual-feature\nencoder combining short-time Fourier transform and gammatone features enhances\nthe robustness of acoustic representation. Second, a channel-independent\nglobally adaptive Fourier modulator efficiently captures long-term temporal\ndependencies while preserving the spatial cues. Finally, a dynamic gating\nmechanism is implemented to reduce processing artifacts. Experimental results\nshow that GAF-Net achieves competitive performance, particularly in terms of\nbinaural cues (ILD and IPD error) and objective intelligibility (MBSTOI), with\nfewer parameters and computational cost. These results confirm that GAF-Net\nprovides a feasible way to achieve high-fidelity binaural processing on\nresource-constrained devices."}
{"id": "2509.14069", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14069", "abs": "https://arxiv.org/abs/2509.14069", "authors": ["Xikun Lu", "Fang Liu", "Weizhi Shi", "Jinqiu Sang"], "title": "Lightweight Implicit Neural Network for Binaural Audio Synthesis", "comment": "Submitted to ICASSP 2026", "summary": "High-fidelity binaural audio synthesis is crucial for immersive listening,\nbut existing methods require extensive computational resources, limiting their\nedge-device application. To address this, we propose the Lightweight Implicit\nNeural Network (LINN), a novel two-stage framework. LINN first generates\ninitial estimates using a time-domain warping, which is then refined by an\nImplicit Binaural Corrector (IBC) module. IBC is an implicit neural network\nthat predicts amplitude and phase corrections directly, resulting in a highly\ncompact model architecture. Experimental results show that LINN achieves\nstatistically comparable perceptual quality to the best-performing baseline\nmodel while significantly improving computational efficiency. Compared to the\nmost efficient existing method, LINN achieves a 72.7% reduction in parameters\nand significantly fewer compute operations (MACs). This demonstrates that our\napproach effectively addresses the trade-off between synthesis quality and\ncomputational efficiency, providing a new solution for high-fidelity\nedge-device spatial audio applications."}
{"id": "2509.14160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14160", "abs": "https://arxiv.org/abs/2509.14160", "authors": ["Adam Umra", "Aya Mostafa Ahmed", "Stefan Roth", "Aydin Sezgin"], "title": "Hardware-Efficient Cognitive Radar: Multi-Target Detection with RL-Driven Transmissive RIS", "comment": "5 pages, 3 figures, submitted to ICASSP 2026", "summary": "Cognitive radar has emerged as a key paradigm for next-generation sensing,\nenabling adaptive, intelligent operation in dynamic and complex environments.\nYet, conventional cognitive multiple-input multiple-output (MIMO) radars offer\nstrong detection performance but suffer from high hardware complexity and power\ndemands. To overcome these limitations, we develop a reinforcement learning\n(RL)-based framework that leverages a transmissive reconfigurable intelligent\nsurface (TRIS) for adaptive beamforming. A state-action-reward-state-action\n(SARSA) agent tunes TRIS phase shifts to improve multi-target detection in low\nsignal-to-noise ratio (SNR) conditions while operating with far fewer radio\nfrequency (RF) chains. Simulations confirm that the proposed TRIS-RL radar\nmatches or, for large number of elements, even surpasses MIMO performance with\nreduced cost and energy requirements."}
{"id": "2509.14136", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14136", "abs": "https://arxiv.org/abs/2509.14136", "authors": ["Jungwoo Heo", "Hyun-seo Shin", "Chan-yeong Lim", "Kyo-won Koo", "Seung-bin Kim", "Jisoo Son", "Ha-Jin Yu"], "title": "SV-Mixer: Replacing the Transformer Encoder with Lightweight MLPs for Self-Supervised Model Compression in Speaker Verification", "comment": "8 pages, 5 figures, accepted at IEEE ASRU 2025", "summary": "Self-supervised learning (SSL) has pushed speaker verification accuracy close\nto state-of-the-art levels, but the Transformer backbones used in most SSL\nencoders hinder on-device and real-time deployment. Prior compression work\ntrims layer depth or width yet still inherits the quadratic cost of\nself-attention. We propose SV-Mixer, the first fully MLP-based student encoder\nfor SSL distillation. SV-Mixer replaces Transformer with three lightweight\nmodules: Multi-Scale Mixing for multi-resolution temporal features,\nLocal-Global Mixing for frame-to-utterance context, and Group Channel Mixing\nfor spectral subspaces. Distilled from WavLM, SV-Mixer outperforms a\nTransformer student by 14.6% while cutting parameters and GMACs by over half,\nand at 75% compression, it closely matches the teacher's performance. Our\nresults show that attention-free SSL students can deliver teacher-level\naccuracy with hardware-friendly footprints, opening the door to robust\non-device speaker verification."}
{"id": "2509.14076", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14076", "abs": "https://arxiv.org/abs/2509.14076", "authors": ["Xikun Lu", "Yujian Ma", "Xianquan Jiang", "Xuelong Wang", "Jinqiu Sang"], "title": "A Lightweight Fourier-based Network for Binaural Speech Enhancement with Spatial Cue Preservation", "comment": "Submitted to ICASSP 2026", "summary": "Binaural speech enhancement faces a severe trade-off challenge, where\nstate-of-the-art performance is achieved by computationally intensive\narchitectures, while lightweight solutions often come at the cost of\nsignificant performance degradation. To bridge this gap, we propose the Global\nAdaptive Fourier Network (GAF-Net), a lightweight deep complex network that\naims to establish a balance between performance and computational efficiency.\nThe GAF-Net architecture consists of three components. First, a dual-feature\nencoder combining short-time Fourier transform and gammatone features enhances\nthe robustness of acoustic representation. Second, a channel-independent\nglobally adaptive Fourier modulator efficiently captures long-term temporal\ndependencies while preserving the spatial cues. Finally, a dynamic gating\nmechanism is implemented to reduce processing artifacts. Experimental results\nshow that GAF-Net achieves competitive performance, particularly in terms of\nbinaural cues (ILD and IPD error) and objective intelligibility (MBSTOI), with\nfewer parameters and computational cost. These results confirm that GAF-Net\nprovides a feasible way to achieve high-fidelity binaural processing on\nresource-constrained devices."}
{"id": "2509.14186", "categories": ["eess.SP", "cs.SY", "eess.SY", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.14186", "abs": "https://arxiv.org/abs/2509.14186", "authors": ["Patrick Vincent N. Lubenia", "Taposh Banerjee"], "title": "Quickest Change Detection with Cost-Constrained Experiment Design", "comment": null, "summary": "In the classical quickest change detection problem, an observer performs only\none experiment to monitor a stochastic process. This paper considers the case\nwhere, at each observation time, the decision-maker needs to choose between\nmultiple experiments with different information qualities and costs. The goal\nis to minimize the worst-case average detection delay subject to false alarm\nand cost constraints. An algorithm called the 2E-CUSUM Algorithm has been\ndeveloped to achieve this goal for the two-experiment case. Extensions to\nmultiple-experiment designs are also studied, and 2E-CUSUM is extended\naccordingly. Data efficiency, where the observer has the choice not to perform\nan experiment, is explored as well. The proposed algorithms are analyzed and\nshown to be asymptotically optimal."}
{"id": "2509.14187", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14187", "abs": "https://arxiv.org/abs/2509.14187", "authors": ["Yu-Wen Chen", "Melody Ma", "Julia Hirschberg"], "title": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual Descriptions and LLMs", "comment": "EMNLP 2025 MainConference", "summary": "Automatic pronunciation assessment is typically performed by acoustic models\ntrained on audio-score pairs. Although effective, these systems provide only\nnumerical scores, without the information needed to help learners understand\ntheir errors. Meanwhile, large language models (LLMs) have proven effective in\nsupporting language learning, but their potential for assessing pronunciation\nremains unexplored. In this work, we introduce TextPA, a zero-shot, Textual\ndescription-based Pronunciation Assessment approach. TextPA utilizes\nhuman-readable representations of speech signals, which are fed into an LLM to\nassess pronunciation accuracy and fluency, while also providing reasoning\nbehind the assigned scores. Finally, a phoneme sequence match scoring method is\nused to refine the accuracy scores. Our work highlights a previously overlooked\ndirection for pronunciation assessment. Instead of relying on supervised\ntraining with audio-score examples, we exploit the rich pronunciation knowledge\nembedded in written text. Experimental results show that our approach is both\ncost-efficient and competitive in performance. Furthermore, TextPA\nsignificantly improves the performance of conventional audio-score-trained\nmodels on out-of-domain data by offering a complementary perspective."}
{"id": "2509.14201", "categories": ["eess.SP", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14201", "abs": "https://arxiv.org/abs/2509.14201", "authors": ["Guangjin Pan", "Liping Bai", "Zhuojun Tian", "Hui Chen", "Mehdi Bennis", "Henk Wymeersch"], "title": "Active Inference Framework for Closed-Loop Sensing, Communication, and Control in UAV Systems", "comment": "5 pages, 2 figures", "summary": "Integrated sensing and communication (ISAC) is a core technology for 6G, and\nits application to closed-loop sensing, communication, and control (SCC)\nenables various services. Existing SCC solutions often treat sensing and\ncontrol separately, leading to suboptimal performance and resource usage. In\nthis work, we introduce the active inference framework (AIF) into SCC-enabled\nunmanned aerial vehicle (UAV) systems for joint state estimation, control, and\nsensing resource allocation. By formulating a unified generative model, the\nproblem reduces to minimizing variational free energy for inference and\nexpected free energy for action planning. Simulation results show that both\ncontrol cost and sensing cost are reduced relative to baselines."}
{"id": "2509.13390", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS", "I.2.1; I.2.6; I.2.10; I.5.1; I.5.2; J.2; J.7"], "pdf": "https://arxiv.org/pdf/2509.13390", "abs": "https://arxiv.org/abs/2509.13390", "authors": ["Deepti Kunte", "Bram Cornelis", "Claudio Colangeli", "Karl Janssens", "Brecht Van Baelen", "Konstantinos Gryllias"], "title": "A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds", "comment": "Submitted to: Mechanical Systems and Signal Processing", "summary": "The detection of anomalies in automotive cabin sounds is critical for\nensuring vehicle quality and maintaining passenger comfort. In many real-world\nsettings, this task is more appropriately framed as an unsupervised learning\nproblem rather than the supervised case due to the scarcity or complete absence\nof labeled faulty data. In such an unsupervised setting, the model is trained\nexclusively on healthy samples and detects anomalies as deviations from normal\nbehavior. However, in the absence of labeled faulty samples for validation and\nthe limited reliability of commonly used metrics, such as validation\nreconstruction error, effective model selection remains a significant\nchallenge. To overcome these limitations, a domain-knowledge-informed approach\nfor model selection is proposed, in which proxy-anomalies engineered through\nstructured perturbations of healthy spectrograms are used in the validation set\nto support model selection. The proposed methodology is evaluated on a\nhigh-fidelity electric vehicle dataset comprising healthy and faulty cabin\nsounds across five representative fault types viz., Imbalance, Modulation,\nWhine, Wind, and Pulse Width Modulation. This dataset, generated using advanced\nsound synthesis techniques, and validated via expert jury assessments, has been\nmade publicly available to facilitate further research. Experimental\nevaluations on the five fault cases demonstrate the selection of optimal models\nusing proxy-anomalies, significantly outperform conventional model selection\nstrategies."}
{"id": "2509.14217", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14217", "abs": "https://arxiv.org/abs/2509.14217", "authors": ["Andriy Enttsel", "Weichen Wang", "Mauro Mangia", "Riccardo Rovatti", "Deniz Gündüz"], "title": "Goal-Oriented Joint Source-Channel Coding: Distortion-Classification-Power Trade-off", "comment": "13 pages, 3 figures", "summary": "Joint source-channel coding is a compelling paradigm when low-latency and\nlow-complexity communication is required. This work proposes a theoretical\nframework that integrates classification and anomaly detection within the\nconventional signal reconstruction objective. Assuming a Gaussian scalar source\nand constraining the encoder to piecewise linear mappings, we derive tractable\ndesign rules and explicitly characterize the trade-offs between distortion,\nclassification error, and transmission power."}
{"id": "2509.14052", "categories": ["cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14052", "abs": "https://arxiv.org/abs/2509.14052", "authors": ["Junan Zhang", "Yunjia Zhang", "Xueyao Zhang", "Zhizheng Wu"], "title": "AnyAccomp: Generalizable Accompaniment Generation via Quantized Melodic Bottleneck", "comment": "Demo audio and code: https://anyaccomp.github.io", "summary": "Singing Accompaniment Generation (SAG) is the process of generating\ninstrumental music for a given clean vocal input. However, existing SAG\ntechniques use source-separated vocals as input and overfit to separation\nartifacts. This creates a critical train-test mismatch, leading to failure on\nclean, real-world vocal inputs. We introduce AnyAccomp, a framework that\nresolves this by decoupling accompaniment generation from source-dependent\nartifacts. AnyAccomp first employs a quantized melodic bottleneck, using a\nchromagram and a VQ-VAE to extract a discrete and timbre-invariant\nrepresentation of the core melody. A subsequent flow-matching model then\ngenerates the accompaniment conditioned on these robust codes. Experiments show\nAnyAccomp achieves competitive performance on separated-vocal benchmarks while\nsignificantly outperforming baselines on generalization test sets of clean\nstudio vocals and, notably, solo instrumental tracks. This demonstrates a\nqualitative leap in generalization, enabling robust accompaniment for\ninstruments - a task where existing models completely fail - and paving the way\nfor more versatile music co-creation tools. Demo audio and code:\nhttps://anyaccomp.github.io"}

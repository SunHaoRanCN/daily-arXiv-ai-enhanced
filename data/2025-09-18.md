<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [eess.AS](#eess.AS) [Total: 14]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Dual Actor DDPG for Airborne STAR-RIS Assisted Communications](https://arxiv.org/abs/2509.13328)
*Danish Rizvi,David Boyle*

Main category: eess.SP

TL;DR: 本文提出了一种基于无人机搭载STAR-RIS的空中智能表面系统，通过新的DA-DDPG算法优化无人机轨迹、基站收发波束形成和被动RIS参数，在考虑能源约束的同时提高了系统性能和用户公平性。


<details>
  <summary>Details</summary>
Motivation: 现有STAR-RIS研究偏重于独立的传输和反射系数偏重，本文认为这种偏重存在缺陷，因此探索了结合了耦合TRC相位模型的新题系统。

Method: 设计了TRC作为离散和连续动作的组合，提出了新的双演员深度确定性策略梯度（DA-DDPG）算法，利用两个独立的演员网络处理高维混合动作空间，并提出了基于调和平均指数的奖励函数以确保用户公平性。

Result: 模拟结果显示，提出的DA-DDPG算法在累积奖励方面超过传统DDPG和DQN基准解决方案的24%和97%。三维无人机轨迹优化实现了比二维和高度优化高28%的通信效率。HFI奖励函数提供了比其他基准得到41%更低的QoS拒绝率。

Conclusion: 运动的Aerial-STAR系统显示出比固定部署对手更优异的性能，具有耦合相位的STAR-RIS超过了双传输/反射RIS和传统RIS配置。这些发现强调了Aerial-STAR系统的潜力以及我们提出的DA-DDPG方法在优化其性能方面的有效性。

Abstract: This study departs from the prevailing assumption of independent Transmission
and Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect
Reconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a
novel multi-user downlink communication system that leverages a UAV-mounted
STAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key
contributions include the joint optimization of UAV trajectory, active
beamforming vectors at the base station, and passive RIS TRCs to enhance
communication efficiency, while considering UAV energy constraints. We design
the TRC as a combination of discrete and continuous actions, and propose a
novel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The
algorithm relies on two separate actor networks for high-dimensional hybrid
action space. We also propose a novel harmonic mean index (HFI)-based reward
function to ensure communication fairness amongst users. For comprehensive
analysis, we study the impact of RIS size on UAV aerodynamics showing that it
increases drag and energy demand. Simulation results demonstrate that the
proposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based
solutions by 24% and 97%, respectively, in accumulated reward.
Three-dimensional UAV trajectory optimization achieves 28% higher communication
efficiency compared to two-dimensional and altitude optimization. The HFI based
reward function provides 41% lower QoS denial rates as compared to other
benchmarks. The mobile Aerial-STAR system shows superior performance over fixed
deployed counterparts, with the coupled phase STAR-RIS outperforming dual
Transmit/Reflect RIS and conventional RIS setups. These findings highlight the
potential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG
approach in optimizing their performance.

</details>


### [2] [Environment Reconstruction in Multi-Bounce Channels with Array Partial Blockage](https://arxiv.org/abs/2509.13559)
*Yuan Liu,Linlong Wu,Xuesong Cai,M. R. Bhavani Shankar*

Main category: eess.SP

TL;DR: 本文提出了一种图基字典辅助的多跳空间交替预期最大化(GM-SAGE)算法，用于处理极大天线数组的空间非稳态通道，实现散射体定位和环境重建。


<details>
  <summary>Details</summary>
Motivation: 极大天线数组(ELAA)在高角分辨率应用中重要，但部分阻塞导致的空间非稳态(SNS)通道成为突出问题，需要有效处理这种非稳态性。

Method: 通过带稀疏性的空间变化幅度模型对SNS效应进行参数化建模，并采用图基字典辅助的多跳空间交替预期最大化算法估计通道参数，同时经验地检测通道稀疏性。

Result: 使用光线追踪模拟生成多跳路径验证方法，模拟结果表明所提方法在处理SNS通道时具有良好的稳健性。

Conclusion: 该研究为处理极大天线数组的空间非稳态通道提供了一种有效的散射体定位和环境重建方案，通过参数化建模和图基算法实现了对部分阻塞情况下通道特征的准确估计。

Abstract: Extremely-large antenna arrays (ELAA) are important in applications requiring
high angular resolution. However, a prominent issue is the spatial
non-stationary (SNS) channels due to partial blockage to the ELAA. In this
paper, we address the scatterer localization and subsequent environment
reconstruction considering partially blocked SNS channels. Specifically, the
SNS effects are parametrically modeled through spatial-varying amplitudes with
sparsity. Based on the established signal model, the graph-based
dictionary-aided multi-bounce space-alternating generalized
expectation-maximization (GM-SAGE) algorithm is applied to estimate the channel
parameters and the channel sparsity is empirically detected along with
amplitude estimation. To validate the proposed approach, we generate
multi-bounce paths through ray tracing (RT) simulations, where the SNS channels
caused by partial blockage could be configured flexibly. The simulation results
demonstrate the robustness of the proposed approach in dealing with the SNS
channels.

</details>


### [3] [Fast Single-Snapshot Harmonic Recovery with 2D Sparse Arrays using BCCB Matrices](https://arxiv.org/abs/2509.13592)
*Youval Klioui*

Main category: eess.SP

TL;DR: 提出了一种高效的稀疏恢复方法实现，用于二维稀疏阵列的谐波估计问题，通过利用BCCB矩阵结构和2D FFT将计算复杂度从O((L1L2)^2)降低到O(L1L2 log(L1L2))


<details>
  <summary>Details</summary>
Motivation: 解决二维稀疏阵列谐波估计中Gram矩阵计算复杂度高的问题，提高稀疏恢复方法的计算效率

Method: 对谐波网格施加均匀性约束和阵列拓扑约束，使Gram矩阵呈现BCCB结构，然后利用2D FFT加速矩阵向量乘积计算

Result: 实验验证了在ISTA、FISTA和ADMM算法中的计算效率提升，实现了计算复杂度的显著降低

Conclusion: 该方法通过利用BCCB矩阵结构和FFT技术，有效降低了二维谐波估计问题的计算复杂度，为实际应用提供了高效的实现方案

Abstract: We introduce an efficient implementation of sparse recovery methods for the
problem of harmonic estimation with 2D sparse arrays using a single snapshot.
By imposing a uniformity constraint on the harmonic grids of the
subdictionaries used in the sparse recovery problem, in addition to a mild
constraint on the array topology that consists in having the elements lie on a
grid specified in half-wavelength units, we show that the Gram matrices that
appear in these sparse recovery methods exhibit a block-circulant with
circulant blocks (BCCB) structure. The BCCB structure is then exploited to
reduce the computational complexity of the matrix-vector products that appear
in these methods through the use of 2D fast Fourier transforms (FFT) from
O((L1L2)^2) down to O(L1L2 log(L1L2)) operations per iterations, where L1, L2
are the lengths of the subdictionaries used for estimating the harmonics in the
first and second dimension, respectively. We experimentally verify the proposed
implementation using the iterative shrinkage thresholding algorithm (ISTA), the
fast iterative shrinkage-thresholding algorithm (FISTA), and the alternating
direction method of multipliers (ADMM) where we observe improvements

</details>


### [4] [GNSS Jamming and Spoofing Monitoring Using Low-Cost COTS Receivers](https://arxiv.org/abs/2509.13600)
*Argyris Kriezis,Yu-Hsuan Chen,Dennis Akos,Sherman Lo,Todd Walter*

Main category: eess.SP

TL;DR: 提出了一种基于低成本商用GNSS接收机的RFI检测分类方法，通过C/N0和接收功率构建二维检测空间，有效识别干扰、欺骗等信号异常


<details>
  <summary>Details</summary>
Motivation: GNSS系统日益容易受到射频干扰（包括干扰和欺骗），威胁导航和授时服务的完整性，需要有效的监测方法

Method: 结合载噪比(C/N0)测量和校准的接收功率指标，构建二维检测空间来识别和区分正常、干扰、欺骗和阻塞信号条件

Result: 通过在挪威的受控干扰测试和在波兰、东南地中海的实际部署验证，证明经过适当校准的COTS检测是可行有效的GNSS RFI监测方法

Conclusion: 低成本商用GNSS接收机经过适当校准后，可以提供可行且有效的GNSS射频干扰监测解决方案

Abstract: The Global Navigation Satellite System (GNSS) is increasingly vulnerable to
radio frequency interference (RFI), including jamming and spoofing, which
threaten the integrity of navigation and timing services. This paper presents a
methodology for detecting and classifying RFI events using low-cost commercial
off-the-shelf (COTS) GNSS receivers. By combining carrier-to-noise ratio (C/N0)
measurements with a calibrated received power metric, a two-dimensional
detection space is constructed to identify and distinguish nominal, jammed,
spoofed, and blocked signal conditions. The method is validated through both
controlled jamming tests in Norway and real-world deployments in Poland, and
the Southeast Mediterranean which have experienced such conditions. Results
demonstrate that COTS-based detection, when properly calibrated, offers a
viable and effective approach for GNSS RFI monitoring.

</details>


### [5] [Theoretical Validation of the Latent Optimally Partitioned-$\ell_2/\ell_1$ Penalty with Application to Angular Power Spectrum Estimation](https://arxiv.org/abs/2509.13745)
*Hiroki Kuroda,Renato Luis Garrido Cavalcante,Masahiro Yukawa*

Main category: eess.SP

TL;DR: LOP-ℓ₂/ℓ₁惩罚在理论和实践中都能有效利用块稀疏性，无需已知具体块结构。该研究展示了该惩罚在MIMO系统中估计未知块划分的角功率谱的新应用，显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有的块稀疏方法通常需要已知具体的块结构信息，但在实际应用中块划分往往是未知的。本文旨在开发一种无需先验块结构知识的块稀疏信号恢复方法。

Method: 提出了LOP-ℓ₂/ℓ₁惩罚方法，通过优化块划分来利用块稀疏性。首先从理论上证明了优化块划分满足准确恢复块稀疏信号的条件，然后将其应用于MIMO通信系统中角功率谱的估计。

Result: 数值模拟表明，使用LOP-ℓ₂/ℓ₁惩罚的块稀疏方法显著提高了角功率谱的估计精度，验证了该方法在未知块结构情况下的有效性。

Conclusion: LOP-ℓ₂/ℓ₁惩罚是一种有效的块稀疏信号恢复工具，能够在无需已知具体块结构的情况下实现准确的信号恢复，在MIMO系统等实际应用中具有重要价值。

Abstract: This paper demonstrates that, in both theory and practice, the latent
optimally partitioned (LOP)-$\ell_2/\ell_1$ penalty is effective for exploiting
block-sparsity without the knowledge of the concrete block structure. More
precisely, we first present a novel theoretical result showing that the
optimized block partition in the LOP-$\ell_2/\ell_1$ penalty satisfy a
condition required for accurate recovery of block-sparse signals. Motivated by
this result, we present a new application of the LOP-$\ell_2/\ell_1$ penalty to
estimation of angular power spectrum, which is block-sparse with unknown block
partition, in MIMO communication systems. Numerical simulations show that the
proposed use of block-sparsity with the LOP-$\ell_2/\ell_1$ penalty
significantly improves the estimation accuracy of the angular power spectrum.

</details>


### [6] [Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization](https://arxiv.org/abs/2509.13786)
*SaiKrishna Saketh Yellapragada,Esa Ollila,Mario Costa*

Main category: eess.SP

TL;DR: 该论文研究6G无线通信系统中基于深度学习的神经接收器的量化技术，通过量化感知训练(QAT)在超低位宽下实现高效量化，在保持性能的同时显著降低延迟、能耗和内存需求。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线通信系统的发展，基于深度学习的神经接收器在物理层处理中展现出优越性能，但在资源受限的硬件上部署需要高效的量化技术来平衡性能和效率。

Method: 扩展后训练量化(PTQ)基线方法，采用量化感知训练(QAT)，在训练过程中融入低精度模拟，在3GPP CDL-B/D信道环境下评估神经接收器架构。

Result: 4位和8位QAT模型在10%目标BLER下达到与FP32模型相似的性能，比PTQ模型性能提升高达3dB，实现8倍压缩比。

Conclusion: 量化感知训练是实现6G边缘设备物理层低复杂度、低延迟实时处理的关键技术，能够有效支持资源受限环境下的高效推理。

Abstract: As wireless communication systems advance toward Sixth Generation (6G) Radio
Access Networks (RAN), Deep Learning (DL)-based neural receivers are emerging
as transformative solutions for Physical Layer (PHY) processing, delivering
superior Block Error Rate (BLER) performance compared to traditional
model-based approaches. Practical deployment on resource-constrained hardware,
however, requires efficient quantization to reduce latency, energy, and memory
without sacrificing reliability. We extend Post-Training Quantization (PTQ)
baselines with Quantization-Aware Training (QAT), which incorporates
low-precision simulation during training for robustness at ultra-low bitwidths.
Our study applies QAT/PTQ to a neural receiver architecture and evaluates
across 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS
environments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit
QAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT
models are also shown to outperform PTQ models by up to 3 dB, and yield 8x
compression. These results demonstrate that QAT is a key enabler of
low-complexity and latency-constrained inference at the PHY layer, facilitating
real-time processing in 6G edge devices

</details>


### [7] [Domino: Dominant Path-based Compensation for Hardware Impairments in Modern WiFi Sensing](https://arxiv.org/abs/2509.13807)
*Ruiqi Kong,He Chen*

Main category: eess.SP

TL;DR: Domino是一个新的WiFi感知框架，通过将CSI转换为CIR并利用延迟域处理来精确补偿硬件引起的RF失真，在呼吸监测中实现了比现有方法至少2倍的平均精度提升。


<details>
  <summary>Details</summary>
Motivation: 现代WiFi卡（支持802.11ac/ax协议）的自动增益控制和独立RF链引入了复杂的动态失真，使得现有的补偿方法失效，导致WiFi感知面临严重的可靠性挑战。

Method: Domino框架将信道状态信息（CSI）转换为信道脉冲响应（CIR），利用硬件引起的失真对所有信号路径影响均匀的特性，通过延迟域处理使用主导静态路径作为可靠参考进行有效补偿。

Result: 真实世界呼吸监测实验显示，Domino实现了比现有方法至少2倍的平均精度提升，在单天线情况下，即使在直视和遮挡场景下，中位误差也保持在0.24 bpm以下。

Conclusion: Domino通过创新的延迟域处理方法有效解决了现代WiFi硬件引起的RF失真问题，显著提升了WiFi感知的可靠性和精度，为实际应用提供了可行的解决方案。

Abstract: WiFi sensing faces a critical reliability challenge due to hardware-induced
RF distortions, especially with modern, market-dominant WiFi cards supporting
802.11ac/ax protocols. These cards employ sensitive automatic gain control and
separate RF chains, introducing complex and dynamic distortions that render
existing compensation methods ineffective. In this paper, we introduce Domino,
a new framework that transforms channel state information (CSI) into channel
impulse response (CIR) and leverages it for precise distortion compensation.
Domino is built on the key insight that hardware-induced distortions impact all
signal paths uniformly, allowing the dominant static path to serve as a
reliable reference for effective compensation through delay-domain processing.
Real-world respiration monitoring experiments show that Domino achieves at
least 2x higher mean accuracy over existing methods, maintaining robust
performance with a median error below 0.24 bpm, even using a single antenna in
both direct line-of-sight and obstructed scenarios.

</details>


### [8] [Flow Matching-Based Active Learning for Radio Map Construction with Low-Altitude UAVs](https://arxiv.org/abs/2509.13822)
*Hao Sun,Shicong Liu,Xianghao Yu,Ying Sun*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The employment of unmanned aerial vehicles (UAVs) in the lowaltitude economy
necessitates precise and real-time radio maps for reliable communication and
safe navigation. However, constructing such maps is hindered by the
infeasibility of exhaustive measurements due to UAVs' limited flight endurance.
To address this, we propose a novel active learning framework for low-altitude
radio map construction based on limited measurements. First, a Plug-and-Play
(PnP)-refined flow matching algorithm is introduced, which leverages flow
matching as a powerful generative prior within a PnP scheme to reconstruct
high-fidelity radio maps. Second, the generative nature of flow matching is
exploited to quantify uncertainty by generating an ensemble of radio maps and
computing the location-wise variance. The resulting uncertainty map guides a
multi-objective candidate selection and then a trajectory is planned via
utility-aware path search (UAPS), directing the UAV to the most informative
locations while taking travel costs into account. Simulation results
demonstrate that our method significantly outperforms the baselines, achieving
more than a 70% reduction in normalized mean squared error (NMSE).

</details>


### [9] [FFT-Free PAPR Reduction Methods for OFDM Signals](https://arxiv.org/abs/2509.13851)
*Hao Su,Jiangtao Wang,Yongchao Wang*

Main category: eess.SP

TL;DR: 提出两种低复杂度PAPR降低算法T-ADMM和TCU-ADMM，避免传统算法中的重复FFT/IFFT操作，具有线性计算复杂度和理论收敛性保证


<details>
  <summary>Details</summary>
Motivation: 解决传统OFDM信号PAPR降低算法中重复FFT/IFFT操作导致的高计算复杂度问题

Method: 通过最小化信号扩异功率建立非凸优化模型，使用自定义交替方向乘子法(T-ADMM)和约束更新版本(TCU-ADMM)求解，所有子问题可分析求解

Result: 算法每次迭代都有线性计算复杂度，T-ADMM在适当参数下理论保证收敛，模拟结果验证了方法的有效性

Conclusion: 所提算法能够有效降低OFDM信号的PAPR，避免了传统方法的高计算复杂度问题，具有实际应用价值

Abstract: In this paper, we propose two low-complexity peak to average power
ratio(PAPR) reduction algorithms for orthogonal frequency division
multiplexing(OFDM) signals. The main content is as follows: First, a non-convex
optimization model is established by minimizing the signal distortion power.
Then, a customized alternating direction method of multipliers(ADMM) algorithm
is proposed to solve the problem, named time domain ADMM(T-ADMM) along with an
improved version called T-ADMM with constrain update(TCU-ADMM). In the
algorithms, all subproblems can be solved analytically, and each iteration has
linear computational complexity. These algorithms circumvents the challenges
posed by repeated fast Fourier transform(FFT) and inverse FFT(IFFT) operations
in traditional PAPR reduction algorithms. Additionally, we prove that the
T-ADMM algorithm is theoretically guaranteed convergent if proper parameter is
chosen. Finally, simulation results demonstrate the effectiveness of the
proposed methods.

</details>


### [10] [Reconfigurable Intelligent Surface-Assisted Multiuser Tracking and Signal Detection in ISAC](https://arxiv.org/abs/2509.13940)
*Weifeng Zhu,Junyuan Gao,Shuowen Zhang,Liang Liu*

Main category: eess.SP

TL;DR: 这篇论文研究了基于可重构智能表面(RIS)的集成感知通信系统中的多用户跟踪和信号检测问题，提出了一种新的混合变分消息传递算法来提高跟踪和信号检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决高机动性用户环境下跟踪和信号检测性能衰退的问题，需要一种有效的在线状态估计方法。

Method: 建立了一个综合的概率信号模型，并提出了新的混合变分消息传递算法，进行用户状态的在线后验概率更新。

Result: 数值结果表明，提出的算法在跟踪和信号检测性能方面显著超过了代表性的贝叶斯估计方法。

Conclusion: 该方法能够高效地处理高机动性用户环境下的跟踪和信号检测问题，为RIS辅助的ISAC系统提供了有效的解决方案。

Abstract: This paper investigates the multiuser tracking and signal detection problem
in integrated sensing and communication (ISAC) systems with the assistance of
reconfigurable intelligent surfaces (RISs). Due to the diverse and high user
mobility, the tracking and signal detection performance can be significantly
deteriorated without choreographed user state (position and velocity) updating
principle. To tackle this challenge, we manage to establish a comprehensive
probabilistic signal model to characterize the interdependencies among user
states, transmit signals, and received signals during the tracking procedure.
Based on the Bayesian problem formulation, we further propose a novel hybrid
variational message passing algorithm for the online estimation of user states,
which can iteratively update the posterior probabilities of user states during
each tracking frame with computational efficiency. Numerical results are
provided to demonstrate that the proposed algorithm can significantly improve
both of the tracking and signal detection performance over the representative
Bayesian estimation counterparts.

</details>


### [11] [Adaptive and robust smartphone-based step detection in multiple sclerosis](https://arxiv.org/abs/2509.13961)
*Lorenza Angelini,Dimitar Stanev,Marta Płonka,Rafał Klimas,Natan Napiórkowski,Gabriela González Chan,Lisa Bunn,Paul S Glazier,Richard Hosking,Jenny Freeman,Jeremy Hobart,Jonathan Marsden,Licinio Craveiro,Mike D Rinderknecht,Mattia Zanon*

Main category: eess.SP

TL;DR: 这项研究验证了一种适用于多种设置、手机携带位置和步态障碍程度的步态处理管线，准确检测初始/终止接触点，在实际环境中表现出艺人的性能。


<details>
  <summary>Details</summary>
Motivation: 以前的步态管线验证研究多重点关注在监督设置下使用单个传感器检测初始接触点，需要评估步态管线在不同测试设置、手机携带位置和步态障碍程度下检测初始/终止接触点的性能。

Method: 在GaitLab研究中，健康对照组和多硬化症患者进行了监督的两分钟行走测试（实验室过地面和跑步机）以及无监督的实际行走活动。使用运动捕获系统或Gait Up传感器收集参考步态数据，通过F1分数和绝对时间误差评估管线性能。

Result: 研究包括35名健康对照和93名多硬化症患者。在所有手机携带位置下均准确检测到初始/终止接触点。实验室2MWT中位数F1分数达到≥98.2%/96.5%（健康组）和≥98.5%/97.7%（病人组）。在结构化和非结构化实际行走中F1分数仍保持高水平，中位数时间误差≤0.08秒。年龄、性别、疾病严重程度、行走辅助器使用或环境（室外/室内）均未影响管线性能。

Conclusion: 该步态管线能够准确且一致地在不同手机位置和环境下检测多硬化症患者的初始和终止接触点，显示了其在实际步态评估中的潜力。

Abstract: Background: Many attempts to validate gait pipelines that process sensor data
to detect gait events have focused on the detection of initial contacts only in
supervised settings using a single sensor. Objective: To evaluate the
performance of a gait pipeline in detecting initial/final contacts using a step
detection algorithm adaptive to different test settings, smartphone wear
locations, and gait impairment levels. Methods: In GaitLab (ISRCTN15993728),
healthy controls (HC) and people with multiple sclerosis (PwMS; Expanded
Disability Status Scale 0.0-6.5) performed supervised Two-Minute Walk Test
[2MWT] (structured in-lab overground and treadmill 2MWT) during two on-site
visits carrying six smartphones and unsupervised walking activities (structured
and unstructured real-world walking) daily for 10-14 days using a single
smartphone. Reference gait data were collected with a motion capture system or
Gait Up sensors. The pipeline's performance in detecting initial/final contacts
was evaluated through F1 scores and absolute temporal error with respect to
reference measurement systems. Results: We studied 35 HC and 93 PwMS.
Initial/final contacts were accurately detected across all smartphone wear
locations. Median F1 scores for initial/final contacts on in-lab 2MWT were
>=98.2%/96.5% in HC and >=98.5%/97.7% in PwMS. F1 scores remained high on
structured (HC: 100% [0.3%]/100% [0.2%]; PwMS: 99.5% [1.9%]/99.4% [2.5%]) and
unstructured real-world walking (HC: 97.8% [2.6%]/97.8% [2.8%]; PwMS: 94.4%
[6.2%]/94.0% [6.5%]). Median temporal errors were <=0.08 s. Neither age, sex,
disease severity, walking aid use, nor setting (outdoor/indoor) impacted
pipeline performance (all p>0.05). Conclusion: This gait pipeline accurately
and consistently detects initial and final contacts in PwMS across different
smartphone locations and environments, highlighting its potential for
real-world gait assessment.

</details>


### [12] [Classification Filtering](https://arxiv.org/abs/2509.13975)
*Ilker Bayram*

Main category: eess.SP

TL;DR: 提出一种针对流式信号的分类器融合方法，通过状态空间模型和实时滤波器结合时序信息来提高分类精度


<details>
  <summary>Details</summary>
Motivation: 处理流式信号分类问题，多个分类器提供不同准确度的概率输出，需要融合这些输出并利用时序信息来提升分类准确性

Method: 采用状态空间模型，开发专门用于实时执行的滤波器，在惯性测量单元(IMU)数据的活动分类应用中实现

Result: 证明了所提出滤波器在基于可穿戴设备IMU数据的活动分类应用中的有效性

Conclusion: 通过状态空间模型和实时滤波器融合多分类器输出并利用时序信息，能够有效提高流式信号的分类精度

Abstract: We consider a streaming signal in which each sample is linked to a latent
class. We assume that multiple classifiers are available, each providing class
probabilities with varying degrees of accuracy. These classifiers are employed
following a straightforward and fixed policy. In this setting, we consider the
problem of fusing the output of the classifiers while incorporating the
temporal aspect to improve classification accuracy. We propose a state-space
model and develop a filter tailored for realtime execution. We demonstrate the
effectiveness of the proposed filter in an activity classification application
based on inertial measurement unit (IMU) data from a wearable device.

</details>


### [13] [Distributed Coherent Beamforming at 60 GHz Enabled by Optically-Established Coherence](https://arxiv.org/abs/2509.13984)
*Drake Silbernagel,Yu Rong,Isabella Lenz,Prithvi Hemanth,Carl Morgenstern,Owen Ma,Nolan Matthews,Nader Zaki,Kyle W. Martin,John D. Elgin,Jacob Holtom,Daniel W. Bliss,Kimberly Frey*

Main category: eess.SP

TL;DR: 通过60 GHz分布式系统通过光学时钟同步实现精确相位对齐，在收发射维中实现干涉拒止和传输零塔，获得显著的信号收益改善。


<details>
  <summary>Details</summary>
Motivation: 在V波段实现分布式系统的相位同步是一项挑战，本文通过光学时钟同步系统来解决这一问题，以支持高性能的收发射维技术。

Method: 实玹60 GHz分布式系统，采用光学时钟同步系统为独立元素提供精确时间和频率对齐。利用这种准确的相位一致性，进行接收射维形成中的干涉拒止和传输零塔。

Result: 系统在干涉干扰场景中实现了14.3 dB的信干比改善，其中接收射维形成对干扰实现了13.5 dB的零塔效果。在传输零塔中，对目标接收机获得7.9 dB的SNR增益，同时对其他接收机保持8.9 dB的SNR减少。

Conclusion: 这些结果表明了在V波段不依赖GPS时钟的情况下实现分布式相位同步的可行性，为高性能分布式系统的发展提供了重要技术支撑。

Abstract: We implement and experimentally demonstrate a 60 GHz distributed system
leveraging an optical time synchronization system that provides precise time
and frequency alignment between independent elements of the distributed mesh.
Utilizing such accurate coherence, we perform receive beamforming with
interference rejection and transmit nulling. In these configurations, the
system achieves a coherent gain over an incoherent network of N nodes,
significantly improving the relevant signal power ratios. Our system
demonstrates extended array phase coherence times, enabling advanced
techniques. Results from over-the-air experiments demonstrate a 14.3 dB
signal-to-interference-plus-noise improvement in interference-laden scenarios
with a contributing 13.5 dB null towards interference in receive beamforming.
In transmit nulling, a signal-to-noise ratio (SNR) gain of 7.9 dB is measured
towards an intended receiver while maintaining an SNR reduction of 8.9 dB at
another receiver. These findings represent the use of distributed coherence in
the V band without the use of GPS timing.

</details>


### [14] [Distributed Deep Learning with RIS Grouping for Accurate Cascaded Channel Estimation](https://arxiv.org/abs/2509.14062)
*Saifur Rahman,Syed Luqman Shah,Salman Khan,Jalal Khan,Muhammad Irfan,Maaz Shafi,Said Muhammad,Fazal Muhammad,Mohammad Shahed Akond*

Main category: eess.SP

TL;DR: 使用深度学习和分布式机器学习方法来低低RIS助国6G系统中的通道估计开销和复杂度，提高估计精度和通用性


<details>
  <summary>Details</summary>
Motivation: 解决RIS被动性质导致的通道估计挑战，包括高开销、高复杂度和能耗，以及单用户模型在新场景下的弱通用性

Method: 提出基于深度学习的通道估计框架，通过分组RIS元素和从部分导频观测重建渗透通道；开发分布式机器学习策略，基站和用户协作训练共享神经网络；设计层次DML神经网络结构，先分类传播条件再进行场景特定特征提取

Result: 模拟结果证实该框架显著减少导频开销和复杂度，在通道估计精度上超过传统方法和单用户模型

Conclusion: 该方法对于6G RIS助国系统具有实用性和有效性，能够在保持低开销和复杂度的同时提高通道估计性能

Abstract: Reconfigurable Intelligent Surface (RIS) panels are envisioned as a key
technology for sixth-generation (6G) wireless networks, providing a
cost-effective means to enhance coverage and spectral efficiency. A critical
challenge is the estimation of the cascaded base station (BS)-RIS-user channel,
since the passive nature of RIS elements prevents direct channel acquisition,
incurring prohibitive pilot overhead, computational complexity, and energy
consumption. To address this, we propose a deep learning (DL)-based channel
estimation framework that reduces pilot overhead by grouping RIS elements and
reconstructing the cascaded channel from partial pilot observations.
Furthermore, conventional DL models trained under single-user settings suffer
from poor generalization across new user locations and propagation scenarios.
We develop a distributed machine learning (DML) strategy in which the BS and
users collaboratively train a shared neural network using diverse channel
datasets collected across the network, thereby achieving robust generalization.
Building on this foundation, we design a hierarchical DML neural architecture
that first classifies propagation conditions and then employs scenario-specific
feature extraction to further improve estimation accuracy. Simulation results
confirm that the proposed framework substantially reduces pilot overhead and
complexity while outperforming conventional methods and single-user models in
channel estimation accuracy. These results demonstrate the practicality and
effectiveness of the proposed approach for 6G RIS-assisted systems.

</details>


### [15] [Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission](https://arxiv.org/abs/2509.14072)
*Vincent Lauinger,Lennart Schmitz,Patrick Matalla,Andrej Rode,Sebastian Randel,Laurent Schmalen*

Main category: eess.SP

TL;DR: 提出一种基于变分自编码器的相位噪声容忍均衡方案，用于空分复用传输系统


<details>
  <summary>Details</summary>
Motivation: 解决多芯光纤传输中的相位噪声问题，提高空分复用系统的传输性能

Method: 采用变分自编码器(VAE)架构，设计相位噪声容忍的均衡方案，在150km随机耦合多芯光纤上进行实验验证

Result: 实验证明了该方案的有效性，能够有效处理空分复用传输中的相位噪声问题

Conclusion: 基于VAE的均衡方案是解决多芯光纤空分复用传输中相位噪声问题的有效方法

Abstract: We demonstrate the effectiveness of a novel phase-noise-tolerant,
variational-autoencoder-based equalization scheme for
space-division-multiplexed (SDM) transmission in an experiment over 150km of
randomly-coupled multi-core fibers.

</details>


### [16] [Hardware-Efficient Cognitive Radar: Multi-Target Detection with RL-Driven Transmissive RIS](https://arxiv.org/abs/2509.14160)
*Adam Umra,Aya Mostafa Ahmed,Stefan Roth,Aydin Sezgin*

Main category: eess.SP

TL;DR: 基于渣透可重配智能表面(TRIS)和强化学习(RL)的认知雷达框架，通过适应性材料调节来提升多目标检测性能，在低信噪比条件下实现了更低硬件复杂度和能耗的高性能雷达系统。


<details>
  <summary>Details</summary>
Motivation: 传统认知MIMO雷达虽然检测性能强大，但硬件复杂度高、功耗大，需要找到一种更简单、更节能的替代方案。

Method: 提出了一种基于强化学习(SARSA算法)的框架，利用渣透可重配智能表面(TRIS)进行适应性材料调节。通过调节TRIS的相位移来改善多目标检测性能，同时减少对无线电频(RF)链路的需求。

Result: 模拟实验表明，所提出的TRIS-RL雷达框架在低信噪比条件下能够达到甚至超过传统MIMO雷达的检测性能，特别是在元素数量较多时表现更优。

Conclusion: 这种新型雷达框架在保持高检测性能的同时，显著降低了硬件成本和能源消耗，为下一代认知感知系统提供了一种更经济、更高效的解决方案。

Abstract: Cognitive radar has emerged as a key paradigm for next-generation sensing,
enabling adaptive, intelligent operation in dynamic and complex environments.
Yet, conventional cognitive multiple-input multiple-output (MIMO) radars offer
strong detection performance but suffer from high hardware complexity and power
demands. To overcome these limitations, we develop a reinforcement learning
(RL)-based framework that leverages a transmissive reconfigurable intelligent
surface (TRIS) for adaptive beamforming. A state-action-reward-state-action
(SARSA) agent tunes TRIS phase shifts to improve multi-target detection in low
signal-to-noise ratio (SNR) conditions while operating with far fewer radio
frequency (RF) chains. Simulations confirm that the proposed TRIS-RL radar
matches or, for large number of elements, even surpasses MIMO performance with
reduced cost and energy requirements.

</details>


### [17] [Quickest Change Detection with Cost-Constrained Experiment Design](https://arxiv.org/abs/2509.14186)
*Patrick Vincent N. Lubenia,Taposh Banerjee*

Main category: eess.SP

TL;DR: 本文研究了多实验选择的快速变化检测问题，提出了2E-CUSUM算法来处理双实验情况，并扩展到多实验设计，在满足误报和成本约束下最小化最坏情况平均检测延迟。


<details>
  <summary>Details</summary>
Motivation: 传统快速变化检测只使用单一实验监控随机过程，但实际应用中决策者需要在每个观测时间选择不同信息质量和成本的多重实验，需要开发新的算法来处理这种多实验选择问题。

Method: 开发了2E-CUSUM算法来处理双实验情况，并扩展到多实验设计。还探索了数据效率问题，即观察者可以选择不进行实验的情况。

Result: 提出的算法被分析并证明是渐近最优的，能够有效处理多实验选择下的变化检测问题。

Conclusion: 2E-CUSUM算法及其扩展为多实验选择的快速变化检测问题提供了有效的解决方案，具有渐近最优性，适用于实际应用中需要权衡信息质量和成本的多实验监控场景。

Abstract: In the classical quickest change detection problem, an observer performs only
one experiment to monitor a stochastic process. This paper considers the case
where, at each observation time, the decision-maker needs to choose between
multiple experiments with different information qualities and costs. The goal
is to minimize the worst-case average detection delay subject to false alarm
and cost constraints. An algorithm called the 2E-CUSUM Algorithm has been
developed to achieve this goal for the two-experiment case. Extensions to
multiple-experiment designs are also studied, and 2E-CUSUM is extended
accordingly. Data efficiency, where the observer has the choice not to perform
an experiment, is explored as well. The proposed algorithms are analyzed and
shown to be asymptotically optimal.

</details>


### [18] [Active Inference Framework for Closed-Loop Sensing, Communication, and Control in UAV Systems](https://arxiv.org/abs/2509.14201)
*Guangjin Pan,Liping Bai,Zhuojun Tian,Hui Chen,Mehdi Bennis,Henk Wymeersch*

Main category: eess.SP

TL;DR: 将主动推理框架(AIF)引入无人机系统的集成感知通信控制(SCC)中，实现联合状态估计、控制和感知资源分配


<details>
  <summary>Details</summary>
Motivation: 现有SCC解决方案通常将感知和控制分开处理，导致性能次优和资源使用效率低下

Method: 通过构建统一的生成模型，将问题简化为最小化变分自由能进行推理和最小化期望自由能进行行动规划

Result: 仿真结果显示，相对于基线方法，控制成本和感知成本均有所降低

Conclusion: 主动推理框架能够有效提升无人机系统在集成感知通信控制方面的性能表现

Abstract: Integrated sensing and communication (ISAC) is a core technology for 6G, and
its application to closed-loop sensing, communication, and control (SCC)
enables various services. Existing SCC solutions often treat sensing and
control separately, leading to suboptimal performance and resource usage. In
this work, we introduce the active inference framework (AIF) into SCC-enabled
unmanned aerial vehicle (UAV) systems for joint state estimation, control, and
sensing resource allocation. By formulating a unified generative model, the
problem reduces to minimizing variational free energy for inference and
expected free energy for action planning. Simulation results show that both
control cost and sensing cost are reduced relative to baselines.

</details>


### [19] [Goal-Oriented Joint Source-Channel Coding: Distortion-Classification-Power Trade-off](https://arxiv.org/abs/2509.14217)
*Andriy Enttsel,Weichen Wang,Mauro Mangia,Riccardo Rovatti,Deniz Gündüz*

Main category: eess.SP

TL;DR: 提出一个理论框架，将分类和异常检测整合到传统的信号重建目标中，通过分段线性编码器在高斯标量源下分析失真、分类错误和传输功率之间的权衡


<details>
  <summary>Details</summary>
Motivation: 在需要低延迟和低复杂度通信的场景下，联合源信道编码是一个有吸引力的范式

Method: 假设高斯标量源，约束编码器使用分段线性映射，推导出可处理的设计规则

Result: 明确表征了失真、分类错误和传输功率之间的权衡关系

Conclusion: 该框架为联合源信道编码中的多目标优化提供了理论分析基础

Abstract: Joint source-channel coding is a compelling paradigm when low-latency and
low-complexity communication is required. This work proposes a theoretical
framework that integrates classification and anomaly detection within the
conventional signal reconstruction objective. Assuming a Gaussian scalar source
and constraining the encoder to piecewise linear mappings, we derive tractable
design rules and explicitly characterize the trade-offs between distortion,
classification error, and transmission power.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [20] [TICL: Text-Embedding KNN For Speech In-Context Learning Unlocks Speech Recognition Abilities of Large Multimodal Models](https://arxiv.org/abs/2509.13395)
*Haolong Zheng,Yekaterina Yegorova,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: TICL方法通过文本嵌入K近邻选择语义相关的上下文示例，显著提升多模态模型的语音识别性能，无需微调即可在多种挑战性任务上实现高达84.7%的相对WER降低


<details>
  <summary>Details</summary>
Motivation: 语音基础模型已展示上下文学习能力，但有效的上下文示例选择方法仍未被充分探索，需要提升现成多模态模型的语音识别性能

Method: 提出TICL管道，使用文本嵌入K近邻算法基于语义上下文选择示例，无需对模型进行微调

Result: 在重口音英语、多语言语音和儿童语音等挑战性ASR任务中，相对WER降低最高达84.7%，消融研究证明了方法的鲁棒性和效率

Conclusion: TICL是一种简单有效的上下文示例选择方法，能够显著提升现有多模态模型的语音识别性能，无需额外训练成本

Abstract: Speech foundation models have recently demonstrated the ability to perform
Speech In-Context Learning (SICL). Selecting effective in-context examples is
crucial for SICL performance, yet selection methodologies remain underexplored.
In this work, we propose Text-Embedding KNN for SICL (TICL), a simple pipeline
that uses semantic context to enhance off-the-shelf large multimodal models'
speech recognition ability without fine-tuning. Across challenging automatic
speech recognition tasks, including accented English, multilingual speech, and
children's speech, our method enables models to surpass zero-shot performance
with up to 84.7% relative WER reduction. We conduct ablation studies to show
the robustness and efficiency of our method.

</details>


### [21] [Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation](https://arxiv.org/abs/2509.13442)
*Arnab Kumar Roy,Hemant Kumar Kathania,Paban Sapkota*

Main category: eess.AS

TL;DR: DSSCNet是一种新的深度神经网络结构，结合卷积、压缩激活和残差网络，用于从梅尔谱图中提取辨别性表征来分类语音失调的严重程度，通过跨语料库微调框架显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 语音失调严重程度分类对于运动语音障碍者的客观临床评估和进展监测至关重要，但现有方法在说话者独立场景下实现稳健的泛化仍面临挑战。

Method: 提出DSSCNet深度神经网络结构，结合卷积、SE块（压缩激活）和残差网络，从梅尔谱图中提取辨别性表征。SE块选择性关注语音失调语音的重要特征，减少损失并提升模型性能。还提出了跨语料库微调框架。

Result: 在TORGO和UA-Speech语料库上，DSSCNet在OSPS协议下分别达到56.84%和62.62%的准确率，在LOSO设置下分别达到63.47%和64.18%，超越现有最佳方法。经过微调后，性能显著提高，在OSPS下达到75.80%（TORGO）和68.25%（UA-Speech），在LOSO下达到77.76%（TORGO）和79.44%（UA-Speech）。

Conclusion: DSSCNet在不同语音失调语音数据集上都显示出有效性和良好的泛化能力，适用于细粒度的严重程度分类任务。

Abstract: Dysarthric speech severity classification is crucial for objective clinical
assessment and progress monitoring in individuals with motor speech disorders.
Although prior methods have addressed this task, achieving robust
generalization in speaker-independent (SID) scenarios remains challenging. This
work introduces DSSCNet, a novel deep neural architecture that combines
Convolutional, Squeeze-Excitation (SE), and Residual network, helping it
extract discriminative representations of dysarthric speech from mel
spectrograms. The addition of SE block selectively focuses on the important
features of the dysarthric speech, thereby minimizing loss and enhancing
overall model performance. We also propose a cross-corpus fine-tuning framework
for severity classification, adapted from detection-based transfer learning
approaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora:
TORGO and UA-Speech under speaker-independent evaluation protocols:
One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols.
DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and
64.18% under LOSO setting on TORGO and UA-Speech respectively outperforming
existing state-of-the-art methods. Upon fine-tuning, the performance improves
substantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25%
on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. These
results demonstrate the effectiveness and generalizability of DSSCNet for
fine-grained severity classification across diverse dysarthric speech datasets.

</details>


### [22] [Assessing Data Replication in Symbolic Music via Adapted Structural Similarity Index Measure](https://arxiv.org/abs/2509.13658)
*Shulei Ji,Zihao Wang,Le Ma,Jiaxing Yu,Kejun Zhang*

Main category: eess.AS

TL;DR: SSIMuse是将图像结构相似性指标(SSIM)首次适配到符号音乐中，通过钢琴卷帘表示法来检测AI生成音乐中的训练数据复制问题，提供作曲和动态性能两种变体。


<details>
  <summary>Details</summary>
Motivation: 现有符号音乐相似性度量方法主要针对旋律重复，无法有效评估具有丰富纹理和表现力特征的复杂音乐，需要开发能检测数据复制的新方法。

Method: 将符号音乐表示为类似图像的二进制和力度基钢琴卷帘，重新解释并适当修改SSIM组件，开发SSIMuse-B和SSIMuse-V两种变体分别评估作曲和动态性能的数据复制。

Result: 在多数据集合成样本上的控制实验表明，SSIMuse能够可靠地检测至少一个小节粒度的精确复制。

Conclusion: SSIMuse实现了音乐生成中复制问题的开放评估，并引起了对其更广泛的伦理、社会、法律和经济影响的关注。

Abstract: AI-generated music may inadvertently replicate samples from the training
data, raising concerns of plagiarism. Similarity measures can quantify such
replication, thereby offering supervision and guidance for music generation
models. Existing similarity measure methods for symbolic music mainly target
melody repetition, leaving a gap in assessing complex music with rich textures
and expressive performance characteristics. To address this gap, we introduce
SSIMuse, the first adaptation of the Structural Similarity Index Measure (SSIM)
from images to symbolic music. Specifically, we represent symbolic music as
image-like piano rolls in binary and velocity-based forms. Build upon these
representations, we reinterprete and suitably modify the SSIM components in the
musical context to develop two variants, i.e., SSIMuse-B and SSIMuse-V, for
evaluating data replication in composition and dynamic performance,
respectively. Controlled experiments on synthetic samples from multiple
datasets show that SSIMuse can reliably detect exact replication at a
granularity of at least one bar. SSIMuse enables open evaluation of replication
in music generation and draws attention to its broader ethical, social, legal,
and economic implications. The code is available at
https://github.com/Tayjsl97/SSIMuse.

</details>


### [23] [A Distilled Low-Latency Neural Vocoder with Explicit Amplitude and Phase Prediction](https://arxiv.org/abs/2509.13667)
*Hui-Peng Du,Yang Ai,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: DLL-APNet是一个低延迟神经声码器，通过因果卷积和知识蒸馏技术，在保持高质量语音合成的同时显著降低延迟，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 主流神经声码器主要关注语音质量和生成速度，但忽略了延迟这一实时应用中的关键因素。过高的延迟会导致用户交互中出现明显延迟，严重影响用户体验，使系统无法用于实时场景。

Method: 提出DLL-APNet声码器，首先从输入mel频谱显式预测幅度和相位谱，然后通过逆短时傅里叶变换(iSTFT)重建语音波形。使用因果卷积限制信息利用范围，并通过知识蒸馏策略，用预训练的非因果教师声码器指导因果学生声码器的中间特征生成。

Result: 实验结果表明，DLL-APNet比其他因果声码器产生更高质量的语音，同时需要更少的计算资源。其语音质量与主流非因果神经声码器相当，验证了其同时提供高感知质量和低延迟的能力。

Conclusion: DLL-APNet成功解决了神经声码器在实时应用中的延迟问题，通过因果约束和知识蒸馏技术的结合，实现了低延迟和高语音质量的平衡，为实时语音合成应用提供了实用解决方案。

Abstract: The majority of mainstream neural vocoders primarily focus on speech quality
and generation speed, while overlooking latency, which is a critical factor in
real-time applications. Excessive latency leads to noticeable delays in user
interaction, severely degrading the user experience and rendering such systems
impractical for real-time use. Therefore, this paper proposes DLL-APNet, a
Distilled Low-Latency neural vocoder which first predicts the Amplitude and
Phase spectra explicitly from input mel spectrogram and then reconstructs the
speech waveform via inverse short-time Fourier transform (iSTFT). The DLL-APNet
vocoder leverages causal convolutions to constrain the utilization of
information to current and historical contexts, effectively minimizing latency.
To mitigate speech quality degradation caused by causal constraints, a
knowledge distillation strategy is proposed, where a pre-trained non-causal
teacher vocoder guides intermediate feature generation of the causal student
DLL-APNet vocoder. Experimental results demonstrate that the proposed DLL-APNet
vocoder produces higher-quality speech than other causal vocoders, while
requiring fewer computational resources. Furthermore, the proposed DLL-APNet
vocoder achieves speech quality on par with mainstream non-causal neural
vocoders, validating its ability to deliver both high perceptual quality and
low latency.

</details>


### [24] [A High-Quality and Low-Complexity Streamable Neural Speech Codec with Knowledge Distillation](https://arxiv.org/abs/2509.13670)
*En-Wei Zhang,Hui-Peng Du,Xiao-Hang Jiang,Yang Ai,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: StreamCodec2是一个改进的流式语音编解码器，通过全因果架构和通道剪枝降低延迟和复杂度，并利用知识蒸馏从高质量教师模型学习，在保持低延迟(20ms)、低计算复杂度(910MFLOPs)和小模型(5.4M参数)的同时实现高质量语音重建。


<details>
  <summary>Details</summary>
Motivation: 现有神经语音编解码器往往忽视延迟和复杂度问题，限制了在实时语音通信等下游任务中的实际部署。虽然之前的StreamCodec实现了流式编码，但其重建质量和复杂度仍有改进空间。

Method: 采用全因果架构和减少卷积通道来实现流式和轻量级编码；引入非因果的高复杂度教师编解码器，通过知识蒸馏指导StreamCodec2的训练，以补偿模型因果化和剪枝带来的质量损失。

Result: 实验结果表明，StreamCodec2在知识蒸馏训练下能够实现高质量语音重建，同时保持低延迟(20ms)、低计算复杂度(910MFLOPs)和低模型复杂度(5.4M参数)。

Conclusion: StreamCodec2通过架构改进和知识蒸馏策略，成功实现了在低延迟、低复杂度约束下的高质量流式语音编码，为实际应用提供了可行的解决方案。

Abstract: While many current neural speech codecs achieve impressive reconstructed
speech quality, they often neglect latency and complexity considerations,
limiting their practical deployment in downstream tasks such as real-time
speech communication and efficient speech compression. In our previous work, we
proposed StreamCodec, which enables streamable speech coding by leveraging
model causalization and a scalar-vector-combined quantization strategy, but its
reconstructed quality and complexity still have room for improvement.
Therefore, this paper proposes an improved iteration of StreamCodec, named
StreamCodec2. The StreamCodec2 supports streamable and lightweight speech
coding by adopting a fully causal architecture and reducing the convolutional
channels. To compensate for the speech quality degradation caused by model
causalization and pruning, we introduce a non-causal, high-complexity teacher
codec to guide the training of StreamCodec2 through knowledge distillation.
Experimental results demonstrate that our proposed StreamCodec2, trained with
the knowledge distillation strategy, can achieve high-quality speech
reconstruction while maintaining low latency (only 20 ms), low computational
complexity (only 910 MFLOPs), and low model complexity (only 5.4 M parameters).

</details>


### [25] [Self-Guided Target Sound Extraction and Classification Through Universal Sound Separation Model and Multiple Clues](https://arxiv.org/abs/2509.13741)
*Younghoo Kwon,Dongheon Lee,Dohwan Kim,Jung-Woo Choi*

Main category: eess.AS

TL;DR: 这篇论文提出了一种多阶段自持导框架，用于解决DCASE 2025任务4中的声音场景空间语义分割任务，通过递代精炼提升分离效果和标签准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂音频混合中的空间语义分割问题，避免外部指导的依赖，实现自主的音频分离和分类。

Method: 集成三个模块：通用音频分离(USS)、单标签分类(SC)和目标音频提取(TSE)，通过迭代循环进行自持导的精炼。

Result: 在官方评估数据集上，CA-SDRi提升11.00 dB，标签预测准确率55.8%，超过ResUNetK基线模型4.4 dB和4.3%，获得第一名。

Conclusion: 多阶段自持导框架能够有效提升音频分离和分类性能，通过自主识别目标和迭代精炼实现优异的结果。

Abstract: This paper introduces a multi-stage self-directed framework designed to
address the spatial semantic segmentation of sound scene (S5) task in the DCASE
2025 Task 4 challenge. This framework integrates models focused on three
distinct tasks: Universal Sound Separation (USS), Single-label Classification
(SC), and Target Sound Extraction (TSE). Initially, USS breaks down a complex
audio mixture into separate source waveforms. Each of these separated waveforms
is then processed by a SC block, generating two critical pieces of information:
the waveform itself and its corresponding class label. These serve as inputs
for the TSE stage, which isolates the source that matches this information.
Since these inputs are produced within the system, the extraction target is
identified autonomously, removing the necessity for external guidance. The
extracted waveform can be looped back into the classification task, creating a
cycle of iterative refinement that progressively enhances both separability and
labeling accuracy. We thus call our framework a multi-stage self-guided system
due to these self-contained characteristics. On the official evaluation
dataset, the proposed system achieves an 11.00 dB increase in class-aware
signal-to-distortion ratio improvement (CA-SDRi) and a 55.8\% accuracy in label
prediction, outperforming the ResUNetK baseline by 4.4 dB and 4.3\%,
respectively, and achieving first place among all submissions.

</details>


### [26] [Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods](https://arxiv.org/abs/2509.13785)
*Bingshen Mu,Pengcheng Guo,Zhaokai Sun,Shuai Wang,Hexin Liu,Mingchen Shao,Lei Xie,Eng Siong Chng,Longshuai Xiao,Qiangze Feng,Daliang Wang*

Main category: eess.AS

TL;DR: Interspeech2025 MLC-SLM挑战赛总结，旨在推动多语言对话语音大模型发展，吸引了78个团队参与，发布了1604小时多语言对话数据集。


<details>
  <summary>Details</summary>
Motivation: 推动构建有效的多语言对话语音大模型（SLLMs）的研究进展，解决多语言语音对话系统的技术挑战。

Method: 组织多语言对话语音大模型挑战赛，提供任务设置、1604小时真实多语言对话数据集和基线系统，收集和分析参赛团队的提交结果。

Result: 吸引了来自13个国家的78个团队参与，获得489个有效排行榜结果和14份技术报告，提炼出构建多语言对话SLLMs的宝贵见解。

Conclusion: 该挑战赛成功推动了多语言对话语音大模型社区的发展，为未来研究提供了重要参考和数据集资源。

Abstract: This paper summarizes the Interspeech2025 Multilingual Conversational Speech
Language Model (MLC-SLM) challenge, which aims to advance the exploration of
building effective multilingual conversational speech LLMs (SLLMs). We provide
a detailed description of the task settings for the MLC-SLM challenge, the
released real-world multilingual conversational speech dataset totaling
approximately 1,604 hours, and the baseline systems for participants. The
MLC-SLM challenge attracts 78 teams from 13 countries to participate, with 489
valid leaderboard results and 14 technical reports for the two tasks. We
distill valuable insights on building multilingual conversational SLLMs based
on submissions from participants, aiming to contribute to the advancement of
the community.

</details>


### [27] [Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection](https://arxiv.org/abs/2509.13878)
*Janne Laakkonen,Ivan Kukanov,Ville Hautamäki*

Main category: eess.AS

TL;DR: 通过混合专家模型(MoE-LoRA)方法，在Wav2Vec2基础模型中集成多个低秩适配器，通过路由机制选择性激活专门专家，提升音频深度伪造检测的过通用性。


<details>
  <summary>Details</summary>
Motivation: 当前的基础模型在音频深度伪造检测任务中，经过精调后容易出现过拟合问题，无法良好地激活到训练集中未见的新型深度伪造攻击方法。

Method: 提出了一种混合专家模型方法，在模型的注意力层中集成多个低秩适配器(LoRA)，通过路由机制动态选择和激活不同的专门专家来处理不同类型的深度伪造攻击。

Result: 在域内和域外场景下都显著超过了标准精调方法，将平均域外误差率(EER)从8.55%降低到6.08%，显著提升了模型的通用性。

Conclusion: 混合专家模型方法能够有效提升音频深度伪造检测的过通用性，更好地适应不断演化的深度伪造攻击方法。

Abstract: Foundation models such as Wav2Vec2 excel at representation learning in speech
tasks, including audio deepfake detection. However, after being fine-tuned on a
fixed set of bonafide and spoofed audio clips, they often fail to generalize to
novel deepfake methods not represented in training. To address this, we propose
a mixture-of-LoRA-experts approach that integrates multiple low-rank adapters
(LoRA) into the model's attention layers. A routing mechanism selectively
activates specialized experts, enhancing adaptability to evolving deepfake
attacks. Experimental results show that our method outperforms standard
fine-tuning in both in-domain and out-of-domain scenarios, reducing equal error
rates relative to baseline models. Notably, our best MoE-LoRA model lowers the
average out-of-domain EER from 8.55\% to 6.08\%, demonstrating its
effectiveness in achieving generalizable audio deepfake detection.

</details>


### [28] [DSpAST: Disentangled Representations for Spatial Audio Reasoning with Large Language Models](https://arxiv.org/abs/2509.13927)
*Kevin Wilkinghoff,Zheng-Hua Tan*

Main category: eess.AS

TL;DR: DSpAST是一种基于SpatialAST的新型空间音频编码器，通过仅增加0.2%的参数学习解耦表示，在空间音频推理任务中显著优于SpatialAST。


<details>
  <summary>Details</summary>
Motivation: 现有的单一音频编码器难以同时捕捉声音事件类型、方向和距离等独立信息，导致性能不如任务特定编码器。

Method: 提出DSpAST编码器，基于SpatialAST架构，学习空间音频的解耦表示，仅增加0.2%的额外参数。

Result: 在SpatialSoundQA数据集上的实验表明，DSpAST结合BAT空间音频推理系统显著优于SpatialAST。

Conclusion: DSpAST通过解耦表示学习，以极小的参数增加实现了更好的空间音频编码性能。

Abstract: Reasoning about spatial audio with large language models requires a spatial
audio encoder as an acoustic front-end to obtain audio embeddings for further
processing. Such an encoder needs to capture all information required to detect
the type of sound events, as well as the direction and distance of their
corresponding sources. Accomplishing this with a single audio encoder is
demanding as the information required for each of these tasks is mostly
independent of each other. As a result, the performance obtained with a single
encoder is often worse than when using task-specific audio encoders. In this
work, we present DSpAST, a novel audio encoder based on SpatialAST that learns
disentangled representations of spatial audio while having only 0.2% additional
parameters. Experiments on SpatialSoundQA with the spatial audio reasoning
system BAT demonstrate that DSpAST significantly outperforms SpatialAST.

</details>


### [29] [Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems](https://arxiv.org/abs/2509.13989)
*Yi-Cheng Lin,Huang-Cheng Chou,Tzu-Chieh Wei,Kuan-Yu Chen,Hung-yi Lee*

Main category: eess.AS

TL;DR: 这篇论文对指令导向的文本语音系统进行了感知分析，发现GPT-4o-mini-TTS在指令-语音对齐方面表现最佳，但所有系统在细粒度控制和语音年龄控制方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探索指令导向文本语音系统中用户样式指令与听觉者感知之间的对齐问题，这个领域之前很少有研究涉及。

Method: 进行了两个表达性维度（程度副词和情感强度）的感知分析，收集了语者年龄和词级强调属性的人类评分，并构建了大规模人类评估数据集E-VOC语料库。

Result: 发现GPT-4o-mini-TTS是最可靠的ITTS模型，在音响维度上具有良好的指令-语音对齐；所有分析系统都偏向生成成年人声音，无论指令要求儿童或老年声音；细粒度控制仍是主要挑战。

Conclusion: 当前ITTS系统在解释略有差异的属性指令方面还有很大改进空间，需要进一步提升系统的细粒度控制能力。

Abstract: Instruction-guided text-to-speech (ITTS) enables users to control speech
generation through natural language prompts, offering a more intuitive
interface than traditional TTS. However, the alignment between user style
instructions and listener perception remains largely unexplored. This work
first presents a perceptual analysis of ITTS controllability across two
expressive dimensions (adverbs of degree and graded emotion intensity) and
collects human ratings on speaker age and word-level emphasis attributes. To
comprehensively reveal the instruction-perception gap, we provide a data
collection with large-scale human evaluations, named Expressive VOice Control
(E-VOC) corpus. Furthermore, we reveal that (1) gpt-4o-mini-tts is the most
reliable ITTS model with great alignment between instruction and generated
utterances across acoustic dimensions. (2) The 5 analyzed ITTS systems tend to
generate Adult voices even when the instructions ask to use child or Elderly
voices. (3) Fine-grained control remains a major challenge, indicating that
most ITTS systems have substantial room for improvement in interpreting
slightly different attribute instructions.

</details>


### [30] [Lightweight Implicit Neural Network for Binaural Audio Synthesis](https://arxiv.org/abs/2509.14069)
*Xikun Lu,Fang Liu,Weizhi Shi,Jinqiu Sang*

Main category: eess.AS

TL;DR: 提出轻量级隐式神经网络LINN，通过两阶段框架实现高质量双耳音频合成，参数减少72.7%，计算量显著降低，适用于边缘设备


<details>
  <summary>Details</summary>
Motivation: 现有双耳音频合成方法计算资源需求高，限制了在边缘设备上的应用，需要解决合成质量与计算效率之间的权衡问题

Method: 两阶段框架：1) 时域扭曲生成初始估计 2) 隐式双耳校正器(IBC)模块进行细化，IBC是隐式神经网络直接预测幅度和相位校正

Result: 达到与最佳基线模型相当的感知质量，参数减少72.7%，计算操作(MACs)显著减少

Conclusion: LINN有效解决了合成质量与计算效率的权衡问题，为高保真边缘设备空间音频应用提供了新解决方案

Abstract: High-fidelity binaural audio synthesis is crucial for immersive listening,
but existing methods require extensive computational resources, limiting their
edge-device application. To address this, we propose the Lightweight Implicit
Neural Network (LINN), a novel two-stage framework. LINN first generates
initial estimates using a time-domain warping, which is then refined by an
Implicit Binaural Corrector (IBC) module. IBC is an implicit neural network
that predicts amplitude and phase corrections directly, resulting in a highly
compact model architecture. Experimental results show that LINN achieves
statistically comparable perceptual quality to the best-performing baseline
model while significantly improving computational efficiency. Compared to the
most efficient existing method, LINN achieves a 72.7% reduction in parameters
and significantly fewer compute operations (MACs). This demonstrates that our
approach effectively addresses the trade-off between synthesis quality and
computational efficiency, providing a new solution for high-fidelity
edge-device spatial audio applications.

</details>


### [31] [A Lightweight Fourier-based Network for Binaural Speech Enhancement with Spatial Cue Preservation](https://arxiv.org/abs/2509.14076)
*Xikun Lu,Yujian Ma,Xianquan Jiang,Xuelong Wang,Jinqiu Sang*

Main category: eess.AS

TL;DR: GAF-Net是一个轻量级的深度复数网络，通过结合双特征编码器、全局自适应傅里叶调制器和动态门控机制，在双耳语音增强任务中实现了性能与计算效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 双耳语音增强面临性能与计算复杂度之间的严重权衡问题，高性能方法通常计算密集，而轻量级方法往往性能显著下降。需要开发一种既能保持高性能又计算高效的解决方案。

Method: 1) 双特征编码器：结合短时傅里叶变换和gammatone特征增强声学表示鲁棒性；2) 通道独立全局自适应傅里叶调制器：高效捕获长期时间依赖性同时保留空间线索；3) 动态门控机制：减少处理伪影。

Result: GAF-Net在双耳线索（ILD和IPD误差）和客观可懂度（MBSTOI）方面达到竞争性性能，同时具有更少的参数和计算成本。

Conclusion: GAF-Net为在资源受限设备上实现高保真双耳处理提供了可行途径，成功平衡了性能与计算效率。

Abstract: Binaural speech enhancement faces a severe trade-off challenge, where
state-of-the-art performance is achieved by computationally intensive
architectures, while lightweight solutions often come at the cost of
significant performance degradation. To bridge this gap, we propose the Global
Adaptive Fourier Network (GAF-Net), a lightweight deep complex network that
aims to establish a balance between performance and computational efficiency.
The GAF-Net architecture consists of three components. First, a dual-feature
encoder combining short-time Fourier transform and gammatone features enhances
the robustness of acoustic representation. Second, a channel-independent
globally adaptive Fourier modulator efficiently captures long-term temporal
dependencies while preserving the spatial cues. Finally, a dynamic gating
mechanism is implemented to reduce processing artifacts. Experimental results
show that GAF-Net achieves competitive performance, particularly in terms of
binaural cues (ILD and IPD error) and objective intelligibility (MBSTOI), with
fewer parameters and computational cost. These results confirm that GAF-Net
provides a feasible way to achieve high-fidelity binaural processing on
resource-constrained devices.

</details>


### [32] [SV-Mixer: Replacing the Transformer Encoder with Lightweight MLPs for Self-Supervised Model Compression in Speaker Verification](https://arxiv.org/abs/2509.14136)
*Jungwoo Heo,Hyun-seo Shin,Chan-yeong Lim,Kyo-won Koo,Seung-bin Kim,Jisoo Son,Ha-Jin Yu*

Main category: eess.AS

TL;DR: SV-Mixer是首个完全基于MLP的SSL学生编码器，通过替换Transformer为三个轻量级模块，在保持高精度的同时大幅降低计算成本和参数数量，实现了硬件友好的说话人验证部署。


<details>
  <summary>Details</summary>
Motivation: 当前SSL方法虽然准确率高，但Transformer架构的二次计算成本阻碍了设备端和实时部署。需要开发更轻量化的架构来保持性能的同时降低计算需求。

Method: 提出SV-Mixer架构，用三个轻量模块替代Transformer：多尺度混合处理多分辨率时序特征、局部-全局混合处理帧到话语上下文、分组通道混合处理频谱子空间。通过从WavLM蒸馏学习。

Result: SV-Mixer在参数和GMACs减少超过一半的情况下，性能比Transformer学生高出14.6%。在75%压缩率下，性能与教师模型接近匹配。

Conclusion: 无注意力的SSL学生模型能够提供教师级别的准确性，同时具有硬件友好的计算足迹，为鲁棒的设备端说话人验证打开了大门。

Abstract: Self-supervised learning (SSL) has pushed speaker verification accuracy close
to state-of-the-art levels, but the Transformer backbones used in most SSL
encoders hinder on-device and real-time deployment. Prior compression work
trims layer depth or width yet still inherits the quadratic cost of
self-attention. We propose SV-Mixer, the first fully MLP-based student encoder
for SSL distillation. SV-Mixer replaces Transformer with three lightweight
modules: Multi-Scale Mixing for multi-resolution temporal features,
Local-Global Mixing for frame-to-utterance context, and Group Channel Mixing
for spectral subspaces. Distilled from WavLM, SV-Mixer outperforms a
Transformer student by 14.6% while cutting parameters and GMACs by over half,
and at 75% compression, it closely matches the teacher's performance. Our
results show that attention-free SSL students can deliver teacher-level
accuracy with hardware-friendly footprints, opening the door to robust
on-device speaker verification.

</details>


### [33] [Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual Descriptions and LLMs](https://arxiv.org/abs/2509.14187)
*Yu-Wen Chen,Melody Ma,Julia Hirschberg*

Main category: eess.AS

TL;DR: TextPA是一种基于文本描述的零样本发音评估方法，利用LLM分析语音信号的人类可读表示来评估发音准确性和流畅度，并提供评分理由


<details>
  <summary>Details</summary>
Motivation: 传统发音评估系统只能提供数值分数，缺乏帮助学习者理解错误的信息；而LLM在语言学习方面表现出色但尚未用于发音评估

Method: 使用语音信号的人类可读表示作为输入，通过LLM进行发音准确性和流畅度评估，并采用音素序列匹配评分方法来优化准确度分数

Result: 实验表明该方法成本效益高且性能有竞争力，能显著提升传统音频-分数训练模型在域外数据上的性能

Conclusion: TextPA展示了一个被忽视的发音评估方向，通过利用文本中嵌入的丰富发音知识，而不是依赖音频-分数对的监督训练

Abstract: Automatic pronunciation assessment is typically performed by acoustic models
trained on audio-score pairs. Although effective, these systems provide only
numerical scores, without the information needed to help learners understand
their errors. Meanwhile, large language models (LLMs) have proven effective in
supporting language learning, but their potential for assessing pronunciation
remains unexplored. In this work, we introduce TextPA, a zero-shot, Textual
description-based Pronunciation Assessment approach. TextPA utilizes
human-readable representations of speech signals, which are fed into an LLM to
assess pronunciation accuracy and fluency, while also providing reasoning
behind the assigned scores. Finally, a phoneme sequence match scoring method is
used to refine the accuracy scores. Our work highlights a previously overlooked
direction for pronunciation assessment. Instead of relying on supervised
training with audio-score examples, we exploit the rich pronunciation knowledge
embedded in written text. Experimental results show that our approach is both
cost-efficient and competitive in performance. Furthermore, TextPA
significantly improves the performance of conventional audio-score-trained
models on out-of-domain data by offering a complementary perspective.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [34] [A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds](https://arxiv.org/abs/2509.13390)
*Deepti Kunte,Bram Cornelis,Claudio Colangeli,Karl Janssens,Brecht Van Baelen,Konstantinos Gryllias*

Main category: cs.SD

TL;DR: 通过工程化代理异常样本来解决无监督异常检测中模型选择挑战，在汽车间声音异常检测中取得显著成效


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测中缺乏标记故障数据进行验证，常规的验证重建错误指标可靠性有限，模型选择面临重大挑战

Method: 提出基于领域知识的模型选择方法，通过结构化批动健康谱图来工程化代理异常样本，并用于验证集支持模型选择

Result: 在高保真度电动汽车数据集上，对五种代表性故障类型进行实验评估，使用代理异常选择的最优模型显著超过传统模型选择策略

Conclusion: 工程化代理异常方法为无监督异常检测的模型选择提供了有效解决方案，在汽车间声音质量控制中具有重要应用价值

Abstract: The detection of anomalies in automotive cabin sounds is critical for
ensuring vehicle quality and maintaining passenger comfort. In many real-world
settings, this task is more appropriately framed as an unsupervised learning
problem rather than the supervised case due to the scarcity or complete absence
of labeled faulty data. In such an unsupervised setting, the model is trained
exclusively on healthy samples and detects anomalies as deviations from normal
behavior. However, in the absence of labeled faulty samples for validation and
the limited reliability of commonly used metrics, such as validation
reconstruction error, effective model selection remains a significant
challenge. To overcome these limitations, a domain-knowledge-informed approach
for model selection is proposed, in which proxy-anomalies engineered through
structured perturbations of healthy spectrograms are used in the validation set
to support model selection. The proposed methodology is evaluated on a
high-fidelity electric vehicle dataset comprising healthy and faulty cabin
sounds across five representative fault types viz., Imbalance, Modulation,
Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced
sound synthesis techniques, and validated via expert jury assessments, has been
made publicly available to facilitate further research. Experimental
evaluations on the five fault cases demonstrate the selection of optimal models
using proxy-anomalies, significantly outperform conventional model selection
strategies.

</details>


### [35] [Field of View Enhanced Signal Dependent Binauralization with Mixture of Experts Framework for Continuous Source Motion](https://arxiv.org/abs/2509.13548)
*Manan Mittal,Thomas Deppisch,Joseph Forrer,Chris Le Sueur,Zamir Ben-Hur,David Lou Along,Daniel D. E. Wong*

Main category: cs.SD

TL;DR: 一种新的混合专家框架，用于双耳信号匹配中的视野增强，支持动态空间音频渲染和移动声源追踪


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖显式声源方向估计或在Ambisonics域运行的限制，实现自适应连续说话者运动的动态空间音频渲染

Method: 使用隐式定位技术，在线组合多个双耳滤波器，构建信号依赖的混合专家框架

Result: 支持实时追踪和增强移动声源，可用于语音聚焦、噪声降低和增强虚拟现实中的世界锁定音频

Conclusion: 该方法不依赖数组布局，为下一代消费音频设备提供灵活的空间音频捕获和个性化播放解决方案

Abstract: We propose a novel mixture of experts framework for field-of-view enhancement
in binaural signal matching. Our approach enables dynamic spatial audio
rendering that adapts to continuous talker motion, allowing users to emphasize
or suppress sounds from selected directions while preserving natural binaural
cues. Unlike traditional methods that rely on explicit direction-of-arrival
estimation or operate in the Ambisonics domain, our signal-dependent framework
combines multiple binaural filters in an online manner using implicit
localization. This allows for real-time tracking and enhancement of moving
sound sources, supporting applications such as speech focus, noise reduction,
and world-locked audio in augmented and virtual reality. The method is agnostic
to array geometry offering a flexible solution for spatial audio capture and
personalized playback in next-generation consumer audio devices.

</details>


### [36] [Neural Speech Separation with Parallel Amplitude and Phase Spectrum Estimation](https://arxiv.org/abs/2509.13825)
*Fei Liu,Yang Ai,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: APSS是一种新颖的神经语音分离模型，通过并行估计振幅和相位频谱来实现更完整准确的语音分离，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大多数语音分离方法主要关注振幅频谱而忽略相位频谱，导致分离不完整和不准确。APSS旨在通过显式估计相位频谱来改进分离质量。

Method: 首先从混合语音信号中提取振幅和相位频谱，通过特征组合器融合成联合表示，使用时频Transformer捕获时频依赖关系，最后通过并行分离器估计各说话人的频谱并通过逆短时傅里叶变换重建分离信号。

Result: 实验结果表明APSS超越了时域分离方法和基于隐式相位估计的时频方法，在多个数据集上获得稳定且具有竞争力的结果。

Conclusion: APSS展示了强大的泛化能力和实际应用价值，通过并行振幅和相位频谱估计显著提升了语音分离性能。

Abstract: This paper proposes APSS, a novel neural speech separation model with
parallel amplitude and phase spectrum estimation. Unlike most existing speech
separation methods, the APSS distinguishes itself by explicitly estimating the
phase spectrum for more complete and accurate separation. Specifically, APSS
first extracts the amplitude and phase spectra from the mixed speech signal.
Subsequently, the extracted amplitude and phase spectra are fused by a feature
combiner into joint representations, which are then further processed by a deep
processor with time-frequency Transformers to capture temporal and spectral
dependencies. Finally, leveraging parallel amplitude and phase separators, the
APSS estimates the respective spectra for each speaker from the resulting
features, which are then combined via inverse short-time Fourier transform
(iSTFT) to reconstruct the separated speech signals. Experimental results
indicate that APSS surpasses both time-domain separation methods and
implicit-phase-estimation-based time-frequency approaches. Also, APSS achieves
stable and competitive results on multiple datasets, highlighting its strong
generalization capability and practical applicability.

</details>


### [37] [Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection](https://arxiv.org/abs/2509.13853)
*Shun Huang,Zhihua Fang,Liang He*

Main category: cs.SD

TL;DR: 提出OS-SCL训练技术和TFgram特征，在无监督异常声音检测中显著减少误报，在DCASE 2020任务2上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 解决自监督方法在处理不同机器同类型样本时频繁误报的问题

Method: 使用嵌入空间特征扰动和单阶段噪声监督对比学习(OS-SCL)，并提出从原始音频提取的时频特征TFgram

Result: 仅使用Log-Mel特征：94.64% AUC, 88.42% pAUC, 89.24% mAUC；使用TFgram特征：95.71% AUC, 90.23% pAUC, 91.23% mAUC

Conclusion: OS-SCL方法有效解决了异常声音检测中的误报问题，TFgram特征能更好地捕捉关键信息，性能显著提升

Abstract: Unsupervised anomalous sound detection aims to detect unknown anomalous
sounds by training a model using only normal audio data. Despite advancements
in self-supervised methods, the issue of frequent false alarms when handling
samples of the same type from different machines remains unresolved. This paper
introduces a novel training technique called one-stage supervised contrastive
learning (OS-SCL), which significantly addresses this problem by perturbing
features in the embedding space and employing a one-stage noisy supervised
contrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved
94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.
Additionally, a time-frequency feature named TFgram is proposed, which is
extracted from raw audio. This feature effectively captures critical
information for anomalous sound detection, ultimately achieving 95.71\% AUC,
90.23\% pAUC, and 91.23\% mAUC. The source code is available at:
\underline{www.github.com/huangswt/OS-SCL}.

</details>


### [38] [RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing](https://arxiv.org/abs/2509.14003)
*Liting Gao,Yi Yuan,Yaru Chen,Yuelan Cheng,Zhenbo Li,Juan Wen,Shubin Zhang,Wenwu Wang*

Main category: cs.SD

TL;DR: 基于算法流匹配的异步模型框架，无需辅助提示或掩码即可实现高质量的音频编辑


<details>
  <summary>Details</summary>
Motivation: 文本导向音频编辑任务对准确定位和忠实修改要求高，现有方法在复杂编辑场景下遇到困难或缺乏实用性

Method: 构建重叠多事件音频数据集，提出端到端的高效算法流匹配基于扩散框架

Result: 模型在不需要辅助提示或掩码的情况下实现了忠实的语义对齐，在各项指标上都保持竞争力的编辑质量

Conclusion: 该方法为复杂场景下的音频编辑任务提供了高效、实用的解决方案，推进了文本导向音频编辑技术的发展

Abstract: Diffusion models have shown remarkable progress in text-to-audio generation.
However, text-guided audio editing remains in its early stages. This task
focuses on modifying the target content within an audio signal while preserving
the rest, thus demanding precise localization and faithful editing according to
the text prompt. Existing training-based and zero-shot methods that rely on
full-caption or costly optimization often struggle with complex editing or lack
practicality. In this work, we propose a novel end-to-end efficient rectified
flow matching-based diffusion framework for audio editing, and construct a
dataset featuring overlapping multi-event audio to support training and
benchmarking in complex scenarios. Experiments show that our model achieves
faithful semantic alignment without requiring auxiliary captions or masks,
while maintaining competitive editing quality across metrics.

</details>


### [39] [Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices](https://arxiv.org/abs/2509.14049)
*Jordi Grau-Haro,Ruben Ribes-Serrano,Javier Naranjo-Alcazar,Marta Garcia-Ballesteros,Pedro Zuccarello*

Main category: cs.SD

TL;DR: 这篇论文维度评估多种卷积神经网络在Raspberry Pi边缘设处上的音频标签性能，包括连续24小时推理测试，以确保模型在资源受限环境下的稳定性和可部署性。


<details>
  <summary>Details</summary>
Motivation: 虽然CNN在音频标签任务中表现优异，但在Raspberry Pi等资源受限设处上部署时面临计算效率和热管理挑战，需要维度评估不同模型的实际部署性能。

Method: 评估了PANNs框架中的所有1D和2D模型、适配音频分类的ConvNeXt模型、MobileNetV3以及最新提出的CNN9和CNN13网络。所有模型转换为ONNX格式，并进行连续24小时的推理测试来评估性能稳定性。

Result: 实验结果显示，通过适当的模型选择和优化，可以在延长时间内保持一致的推理延迟和有效管理热行为。

Conclusion: 这些发现为在真实世界边缘计算场景中部署音频标签模型提供了有价值的见解，证明在资源受限设处上也能实现高效稳定的音频标签功能。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated exceptional
performance in audio tagging tasks. However, deploying these models on
resource-constrained devices like the Raspberry Pi poses challenges related to
computational efficiency and thermal management. In this paper, a comprehensive
evaluation of multiple convolutional neural network (CNN) architectures for
audio tagging on the Raspberry Pi is conducted, encompassing all 1D and 2D
models from the Pretrained Audio Neural Networks (PANNs) framework, a
ConvNeXt-based model adapted for audio classification, as well as MobileNetV3
architectures. In addition, two PANNs-derived networks, CNN9 and CNN13,
recently proposed, are also evaluated. To enhance deployment efficiency and
portability across diverse hardware platforms, all models are converted to the
Open Neural Network Exchange (ONNX) format. Unlike previous works that focus on
a single model, our analysis encompasses a broader range of architectures and
involves continuous 24-hour inference sessions to assess performance stability.
Our experiments reveal that, with appropriate model selection and optimization,
it is possible to maintain consistent inference latency and manage thermal
behavior effectively over extended periods. These findings provide valuable
insights for deploying audio tagging models in real-world edge computing
scenarios.

</details>


### [40] [AnyAccomp: Generalizable Accompaniment Generation via Quantized Melodic Bottleneck](https://arxiv.org/abs/2509.14052)
*Junan Zhang,Yunjia Zhang,Xueyao Zhang,Zhizheng Wu*

Main category: cs.SD

TL;DR: AnyAccomp是一个新的歌唱伴奏生成框架，通过解耦伴奏生成与源依赖伪影，解决了现有方法在真实干净人声输入上的泛化失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱伴奏生成技术使用源分离的人声作为输入，过度拟合分离伪影，导致在真实干净人声输入上出现严重的训练-测试不匹配问题。

Method: 首先使用量化旋律瓶颈（色度图和VQ-VAE）提取离散且音色不变的旋律核心表示，然后通过流匹配模型基于这些鲁棒编码生成伴奏。

Result: 在分离人声基准测试中表现竞争力，在干净录音室人声和独奏乐器轨道的泛化测试集上显著优于基线方法，特别是在乐器伴奏任务上取得突破。

Conclusion: 该方法实现了泛化能力的质的飞跃，能够为乐器生成鲁棒伴奏（现有模型完全失败的任务），为更通用的音乐共创工具铺平了道路。

Abstract: Singing Accompaniment Generation (SAG) is the process of generating
instrumental music for a given clean vocal input. However, existing SAG
techniques use source-separated vocals as input and overfit to separation
artifacts. This creates a critical train-test mismatch, leading to failure on
clean, real-world vocal inputs. We introduce AnyAccomp, a framework that
resolves this by decoupling accompaniment generation from source-dependent
artifacts. AnyAccomp first employs a quantized melodic bottleneck, using a
chromagram and a VQ-VAE to extract a discrete and timbre-invariant
representation of the core melody. A subsequent flow-matching model then
generates the accompaniment conditioned on these robust codes. Experiments show
AnyAccomp achieves competitive performance on separated-vocal benchmarks while
significantly outperforming baselines on generalization test sets of clean
studio vocals and, notably, solo instrumental tracks. This demonstrates a
qualitative leap in generalization, enabling robust accompaniment for
instruments - a task where existing models completely fail - and paving the way
for more versatile music co-creation tools. Demo audio and code:
https://anyaccomp.github.io

</details>

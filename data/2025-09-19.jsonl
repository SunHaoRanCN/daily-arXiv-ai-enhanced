{"id": "2509.14304", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14304", "abs": "https://arxiv.org/abs/2509.14304", "authors": ["Eric Zhang", "Li Wei", "Sarah Chen", "Michael Wang"], "title": "Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework", "comment": null, "summary": "Stuttered and dysfluent speech detection systems have traditionally suffered\nfrom the trade-off between accuracy and clinical interpretability. While\nend-to-end deep learning models achieve high performance, their black-box\nnature limits clinical adoption. This paper looks at the Unconstrained\nDysfluency Modeling (UDM) series-the current state-of-the-art framework\ndeveloped by Berkeley that combines modular architecture, explicit phoneme\nalignment, and interpretable outputs for real-world clinical deployment.\nThrough extensive experiments involving patients and certified speech-language\npathologists (SLPs), we demonstrate that UDM achieves state-of-the-art\nperformance (F1: 0.89+-0.04) while providing clinically meaningful\ninterpretability scores (4.2/5.0). Our deployment study shows 87% clinician\nacceptance rate and 34% reduction in diagnostic time. The results provide\nstrong evidence that UDM represents a practical pathway toward AI-assisted\nspeech therapy in clinical environments."}
{"id": "2509.14469", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14469", "abs": "https://arxiv.org/abs/2509.14469", "authors": ["Seungmin Seo", "Oleg Aulov", "P. Jonathon Phillips"], "title": "Measuring Soft Biometric Leakage in Speaker De-Identification Systems", "comment": null, "summary": "We use the term re-identification to refer to the process of recovering the\noriginal speaker's identity from anonymized speech outputs. Speaker\nde-identification systems aim to reduce the risk of re-identification, but most\nevaluations focus only on individual-level measures and overlook broader risks\nfrom soft biometric leakage. We introduce the Soft Biometric Leakage Score\n(SBLS), a unified method that quantifies resistance to zero-shot inference\nattacks on non-unique traits such as channel type, age range, dialect, sex of\nthe speaker, or speaking style. SBLS integrates three elements: direct\nattribute inference using pre-trained classifiers, linkage detection via mutual\ninformation analysis, and subgroup robustness across intersecting attributes.\nApplying SBLS with publicly available classifiers, we show that all five\nevaluated de-identification systems exhibit significant vulnerabilities. Our\nresults indicate that adversaries using only pre-trained models - without\naccess to original speech or system details - can still reliably recover soft\nbiometric information from anonymized output, exposing fundamental weaknesses\nthat standard distributional metrics fail to capture."}
{"id": "2509.14479", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14479", "abs": "https://arxiv.org/abs/2509.14479", "authors": ["Sean Foley", "Jihwan Lee", "Kevin Huang", "Xuan Shi", "Yoonjeong Lee", "Louis Goldstein", "Shrikanth Narayanan"], "title": "A long-form single-speaker real-time MRI speech dataset and benchmark", "comment": null, "summary": "We release the USC Long Single-Speaker (LSS) dataset containing real-time MRI\nvideo of the vocal tract dynamics and simultaneous audio obtained during speech\nproduction. This unique dataset contains roughly one hour of video and audio\ndata from a single native speaker of American English, making it one of the\nlonger publicly available single-speaker datasets of real-time MRI speech data.\nAlong with the articulatory and acoustic raw data, we release derived\nrepresentations of the data that are suitable for a range of downstream tasks.\nThis includes video cropped to the vocal tract region, sentence-level splits of\nthe data, restored and denoised audio, and regions-of-interest timeseries. We\nalso benchmark this dataset on articulatory synthesis and phoneme recognition\ntasks, providing baseline performance for these tasks on this dataset which\nfuture research can aim to improve upon."}
{"id": "2509.14579", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14579", "abs": "https://arxiv.org/abs/2509.14579", "authors": ["Qingyu Liu", "Yushen Chen", "Zhikang Niu", "Chunhui Wang", "Yunting Yang", "Bowen Zhang", "Jian Zhao", "Pengcheng Zhu", "Kai Yu", "Xie Chen"], "title": "Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech Synthesis", "comment": "5 pages, 2 figures", "summary": "Flow-matching-based text-to-speech (TTS) models have shown high-quality\nspeech synthesis. However, most current flow-matching-based TTS models still\nrely on reference transcripts corresponding to the audio prompt for synthesis.\nThis dependency prevents cross-lingual voice cloning when audio prompt\ntranscripts are unavailable, particularly for unseen languages. The key\nchallenges for flow-matching-based TTS models to remove audio prompt\ntranscripts are identifying word boundaries during training and determining\nappropriate duration during inference. In this paper, we introduce\nCross-Lingual F5-TTS, a framework that enables cross-lingual voice cloning\nwithout audio prompt transcripts. Our method preprocesses audio prompts by\nforced alignment to obtain word boundaries, enabling direct synthesis from\naudio prompts while excluding transcripts during training. To address the\nduration modeling challenge, we train speaking rate predictors at different\nlinguistic granularities to derive duration from speaker pace. Experiments show\nthat our approach matches the performance of F5-TTS while enabling\ncross-lingual voice cloning."}
{"id": "2509.14298", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14298", "abs": "https://arxiv.org/abs/2509.14298", "authors": ["Justin Lovelace", "Rithesh Kumar", "Jiaqi Su", "Ke Chen", "Kilian Q Weinberger", "Zeyu Jin"], "title": "SpeechOp: Inference-Time Task Composition for Generative Speech Processing", "comment": null, "summary": "While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild\"\ndata to achieve remarkable success, speech-to-speech processing tasks like\nenhancement face data limitations, which lead data-hungry generative approaches\nto distort speech content and speaker identity. To bridge this gap, we present\nSpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS\nmodels into a universal speech processor capable of performing a wide range of\nspeech tasks and composing them in novel ways at inference time. By adapting a\npre-trained TTS model, SpeechOp inherits a rich understanding of natural\nspeech, accelerating training and improving S2S task quality, while\nsimultaneously enhancing core TTS performance. Finally, we introduce Implicit\nTask Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g.,\nfrom Whisper) guide SpeechOp's enhancement via our principled inference-time\ntask composition. ITC achieves state-of-the-art content preservation by\nrobustly combining web-scale speech understanding with SpeechOp's generative\ncapabilities. Audio samples are available at\nhttps://justinlovelace.github.io/projects/speechop"}
{"id": "2509.14240", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14240", "abs": "https://arxiv.org/abs/2509.14240", "authors": ["Nafize Ishtiaque Hossain", "Kundan Saha", "Atul Sharma", "Sameer Sonkusale"], "title": "In Planta Tattoo and Kirigami Sensors for Self-Powered Monitoring of Vapor Pressure Deficit and Growth Dynamics", "comment": null, "summary": "We report a scalable, self-powered in planta sensor platform for continuous\nmonitoring of plant hydration and growth. The system integrates two components\na leaf mounted tattoo sensor for estimating vapor pressure deficit and a\nkirigami inspired strain sensor for tracking radial stem growth. Uniquely, the\ntattoo sensor serves a dual function measuring temperature and humidity beneath\nthe leaf surface while simultaneously harvesting power from ambient moisture\nvia a vanadium pentoxide nanosheet membrane. This moist-electric generator\nconfiguration enables energy-autonomous operation, delivering a power density\nof 0.1114 miroW per square cm. The V2O5 based sensor exhibits high sensitivity\nto humidity and temperature, enabling accurate VPD estimation for over 10 days\nuntil leaf senescence. The eutectogel based kirigami strain sensor, wrapped\naround the stem, offers a gauge factor of 1.5 and immunity to unrelated\nmechanical disturbances, allowing continuous growth tracking for more than 20\ndays. Both sensors are fabricated via cleanroom-free, roll to roll compatible\nmethods, underscoring their potential for large-scale agricultural deployment\nto monitor abiotic stress and improve crop management."}
{"id": "2509.14666", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14666", "abs": "https://arxiv.org/abs/2509.14666", "authors": ["Arvind Krishna Sridhar", "Yinyi Guo", "Erik Visser"], "title": "Spatial Audio Motion Understanding and Reasoning", "comment": "5 pages, 2 figures, 3 tables", "summary": "Spatial audio reasoning enables machines to interpret auditory scenes by\nunderstanding events and their spatial attributes. In this work, we focus on\nspatial audio understanding with an emphasis on reasoning about moving sources.\nFirst, we introduce a spatial audio encoder that processes spatial audio to\ndetect multiple overlapping events and estimate their spatial attributes,\nDirection of Arrival (DoA) and source distance, at the frame level. To\ngeneralize to unseen events, we incorporate an audio grounding model that\naligns audio features with semantic audio class text embeddings via a\ncross-attention mechanism. Second, to answer complex queries about dynamic\naudio scenes involving moving sources, we condition a large language model\n(LLM) on structured spatial attributes extracted by our model. Finally, we\nintroduce a spatial audio motion understanding and reasoning benchmark dataset\nand demonstrate our framework's performance against the baseline model."}
{"id": "2509.14379", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14379", "abs": "https://arxiv.org/abs/2509.14379", "authors": ["Yochai Yemini", "Rami Ben-Ari", "Sharon Gannot", "Ethan Fetaya"], "title": "Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior", "comment": null, "summary": "In this paper, we address the problem of single-microphone speech separation\nin the presence of ambient noise. We propose a generative unsupervised\ntechnique that directly models both clean speech and structured noise\ncomponents, training exclusively on these individual signals rather than noisy\nmixtures. Our approach leverages an audio-visual score model that incorporates\nvisual cues to serve as a strong generative speech prior. By explicitly\nmodelling the noise distribution alongside the speech distribution, we enable\neffective decomposition through the inverse problem paradigm. We perform speech\nseparation by sampling from the posterior distributions via a reverse diffusion\nprocess, which directly estimates and removes the modelled noise component to\nrecover clean constituent signals. Experimental results demonstrate promising\nperformance, highlighting the effectiveness of our direct noise modelling\napproach in challenging acoustic environments."}
{"id": "2509.14242", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14242", "abs": "https://arxiv.org/abs/2509.14242", "authors": ["Jinshuai Gu", "Zenghui Lin", "Jingying Ma", "Jingyu Wang", "Linyan Zhang", "Rui Bai", "Zelin Tu", "Youyou Jiang", "Donglin Xie", "Yuxi Zhou", "Guoli Liu", "Shenda Hong"], "title": "Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes", "comment": null, "summary": "Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment\ntechnique used globally, especially in underdeveloped countries. However, it is\ncurrently mainly used to identify the fetus's current status (e.g., fetal\nacidosis or hypoxia), and the potential of CTG in predicting future adverse\npregnancy outcomes has not been fully explored. We aim to develop an AI-based\nmodel that predicts biological age from CTG time series (named CTGage), then\ncalculate the age gap between CTGage and actual age (named CTGage-gap), and use\nthis gap as a new digital biomarker for future adverse pregnancy outcomes. The\nCTGage model is developed using 61,140 records from 11,385 pregnant women,\ncollected at Peking University People's Hospital between 2018 and 2022. For\nmodel training, a structurally designed 1D convolutional neural network is\nused, incorporating distribution-aligned augmented regression technology. The\nCTGage-gap is categorized into five groups: < -21 days (underestimation group),\n-21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days\n(overestimation group). We further defined the underestimation group and\noverestimation group together as the high-risk group. We then compare the\nincidence of adverse outcomes and maternal diseases across these groups. The\naverage absolute error of the CTGage model is 10.91 days. When comparing the\noverestimation group with the normal group, premature infants incidence is\n5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is\n31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the\nnormal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and\nanaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial\nintelligence-derived CTGage can predict the future risk of adverse pregnancy\noutcomes and hold potential as a novel, non-invasive, and easily accessible\ndigital biomarker."}
{"id": "2509.14675", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14675", "abs": "https://arxiv.org/abs/2509.14675", "authors": ["Xuanjun Chen", "Chia-Yu Hu", "I-Ming Lin", "Yi-Cheng Lin", "I-Hsiang Chiu", "You Zhang", "Sung-Feng Huang", "Yi-Hsuan Yang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "How Does Instrumental Music Help SingFake Detection?", "comment": "Work in progress", "summary": "Although many models exist to detect singing voice deepfakes (SingFake), how\nthese models operate, particularly with instrumental accompaniment, is unclear.\nWe investigate how instrumental music affects SingFake detection from two\nperspectives. To investigate the behavioral effect, we test different\nbackbones, unpaired instrumental tracks, and frequency subbands. To analyze the\nrepresentational effect, we probe how fine-tuning alters encoders' speech and\nmusic capabilities. Our results show that instrumental accompaniment acts\nmainly as data augmentation rather than providing intrinsic cues (e.g., rhythm\nor harmony). Furthermore, fine-tuning increases reliance on shallow speaker\nfeatures while reducing sensitivity to content, paralinguistic, and semantic\ninformation. These insights clarify how models exploit vocal versus\ninstrumental cues and can inform the design of more interpretable and robust\nSingFake detection systems."}
{"id": "2509.14430", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14430", "abs": "https://arxiv.org/abs/2509.14430", "authors": ["Yufeng Yang", "Yiteng Huang", "Yong Xu", "Li Wan", "Suwon Shon", "Yang Liu", "Yifeng Fan", "Zhaojun Yang", "Olivier Siohan", "Yue Liu", "Ming Sun", "Florian Metze"], "title": "Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses", "comment": null, "summary": "With the growing adoption of wearable devices such as smart glasses for AI\nassistants, wearer speech recognition (WSR) is becoming increasingly critical\nto next-generation human-computer interfaces. However, in real environments,\ninterference from side-talk speech remains a significant challenge to WSR and\nmay cause accumulated errors for downstream tasks such as natural language\nprocessing. In this work, we introduce a novel multi-channel differential\nautomatic speech recognition (ASR) method for robust WSR on smart glasses. The\nproposed system takes differential inputs from different frontends that\ncomplement each other to improve the robustness of WSR, including a beamformer,\nmicrophone selection, and a lightweight side-talk detection model. Evaluations\non both simulated and real datasets demonstrate that the proposed system\noutperforms the traditional approach, achieving up to an 18.0% relative\nreduction in word error rate."}
{"id": "2509.14243", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14243", "abs": "https://arxiv.org/abs/2509.14243", "authors": ["Xinjie Wang", "Zhongrui Li", "Peng Han", "Chunxin Yuan", "Jiexin Xu", "Zhiqiang Wei", "Jie Nie"], "title": "InWaveSR: Topography-Aware Super-Resolution Network for Internal Solitary Waves", "comment": null, "summary": "The effective utilization of observational data is frequently hindered by\ninsufficient resolution. To address this problem, we present a new\nspatio-temporal super-resolution (STSR) model, called InWaveSR. It is built on\na deep learning framework with physical restrictions and can efficiently\ngenerate high-resolution data from low-resolution input, especially for data\nfeaturing internal solitary waves (ISWs). To increase generality and\ninterpretation, the model InWaveSR uses the primitive Navier-Stokes equations\nas the constraint, ensuring that the output results are physically consistent.\nIn addition, the proposed model incorporates an HF-ResBlock component that\ncombines the attention mechanism and the Fast Fourier Transform (FFT) method to\nimprove the performance of the model in capturing high-frequency\ncharacteristics. Simultaneously, in order to enhance the adaptability of the\nmodel to complicated bottom topography, an edge sampling and numerical\npre-processing method are carried out to optimize the training process. On\nevaluations using the in-situ observational ISW data, the proposed InWaveSR\nachieved a peak signal-to-noise ratio (PSNR) score of 36.2, higher than those\nof the traditional interpolation method and the previous neural network. This\nhighlights its significant superiority over traditional methods, demonstrating\nits excellent performance and reliability in high-resolution ISW\nreconstruction."}
{"id": "2509.14737", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14737", "abs": "https://arxiv.org/abs/2509.14737", "authors": ["Samuel J. Broughton", "Lahiru Samarakoon"], "title": "Pushing the Limits of End-to-End Diarization", "comment": "As presented at Interspeech 2025", "summary": "In this paper, we present state-of-the-art diarization error rates (DERs) on\nmultiple publicly available datasets, including AliMeeting-far,\nAliMeeting-near, AMI-Mix, AMI-SDM, DIHARD III, and MagicData RAMC. Leveraging\nEEND-TA, a single unified non-autoregressive model for end-to-end speaker\ndiarization, we achieve new benchmark results, most notably a DER of 14.49% on\nDIHARD III. Our approach scales pretraining through 8-speaker simulation\nmixtures, ensuring each generated speaker mixture configuration is sufficiently\nrepresented. These experiments highlight that EEND-based architectures possess\na greater capacity for learning than previously explored, surpassing many\nexisting diarization solutions while maintaining efficient speeds during\ninference."}
{"id": "2509.14632", "categories": ["eess.AS", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14632", "abs": "https://arxiv.org/abs/2509.14632", "authors": ["Miseul Kim", "Soo Jin Park", "Kyungguen Byun", "Hyeon-Kyeong Shin", "Sunkuk Moon", "Shuhua Zhang", "Erik Visser"], "title": "Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation", "comment": "Submitted to ICASSP 2026", "summary": "Speaker diarization systems often struggle with high intrinsic intra-speaker\nvariability, such as shifts in emotion, health, or content. This can cause\nsegments from the same speaker to be misclassified as different individuals,\nfor example, when one raises their voice or speaks faster during conversation.\nTo address this, we propose a style-controllable speech generation model that\naugments speech across diverse styles while preserving the target speaker's\nidentity. The proposed system starts with diarized segments from a conventional\ndiarizer. For each diarized segment, it generates augmented speech samples\nenriched with phonetic and stylistic diversity. And then, speaker embeddings\nfrom both the original and generated audio are blended to enhance the system's\nrobustness in grouping segments with high intrinsic intra-speaker variability.\nWe validate our approach on a simulated emotional speech dataset and the\ntruncated AMI dataset, demonstrating significant improvements, with error rate\nreductions of 49% and 35% on each dataset, respectively."}
{"id": "2509.14402", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14402", "abs": "https://arxiv.org/abs/2509.14402", "authors": ["Jinshui Zhang", "Angel V Peterchev", "Stefan M Goetz"], "title": "Conditional Nearest Level Modulation for Improved Switching Dynamics in Asymmetric Multilevel Converters", "comment": null, "summary": "Modular multilevel converters have promising applications in clean energy,\nelectric vehicles, and biomedical instrumentation, but need many modules to\nachieve fine output granularity, particularly of the voltage. Asymmetric\nmultilevel circuits introduce differences in module voltages so that the\nquantity of output levels grows exponentially with the number of modules.\nNearest-level modulation (NLM) is preferred over carrier-based methods in\nasymmetric circuits for its simplicity. However, the large number of output\nlevels can overwhelm NLM and cause excessive transistor switching on some\nmodules and output voltage spikes. We propose a conditional nearest-level\nmodulation (cNLM) by incorporating mathematical penalty models to regulate\nswitching dynamics. This approach improves output quality and reduces switching\nrates. Additionally, we present cNLM variations tailored for specific\nfunctions, such as enforcing a minimum switching interval. Experimental\nvalidation on an asymmetric multilevel prototype demonstrates that cNLM reduces\nthe total output distortion from 66.3% to 15.1% while cutting the switching\nrate to just 8% of the original NLM."}
{"id": "2509.14785", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14785", "abs": "https://arxiv.org/abs/2509.14785", "authors": ["Kentaro Seki", "Yuki Okamoto", "Kouei Yamaoka", "Yuki Saito", "Shinnosuke Takamichi", "Hiroshi Saruwatari"], "title": "Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions", "comment": "Submitted to ICASSP 2026", "summary": "Contrastive language--audio pretraining (CLAP) has achieved remarkable\nsuccess as an audio--text embedding framework, but existing approaches are\nlimited to monaural or single-source conditions and cannot fully capture\nspatial information. The central challenge in modeling spatial information lies\nin multi-source conditions, where the correct correspondence between each sound\nsource and its location is required. To tackle this problem, we propose\nSpatial-CLAP, which introduces a content-aware spatial encoder that enables\nspatial representations coupled with audio content. We further propose spatial\ncontrastive learning (SCL), a training strategy that explicitly enforces the\nlearning of the correct correspondence and promotes more reliable embeddings\nunder multi-source conditions. Experimental evaluations, including downstream\ntasks, demonstrate that Spatial-CLAP learns effective embeddings even under\nmulti-source conditions, and confirm the effectiveness of SCL. Moreover,\nevaluation on unseen three-source mixtures highlights the fundamental\ndistinction between conventional single-source training and our proposed\nmulti-source training paradigm. These findings establish a new paradigm for\nspatially-aware audio--text embeddings."}
{"id": "2509.14650", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14650", "abs": "https://arxiv.org/abs/2509.14650", "authors": ["Jun-Wei Yeow", "Ee-Leng Tan", "Santi Peksi", "Zhen-Ting Ong", "Woon-Seng Gan"], "title": "Enhancing Situational Awareness in Wearable Audio Devices Using a Lightweight Sound Event Localization and Detection System", "comment": "Accepted and presented at the 2025 AES International Conference on\n  Headphone Technology", "summary": "Wearable audio devices with active noise control (ANC) enhance listening\ncomfort but often at the expense of situational awareness. However, this\nauditory isolation may mask crucial environmental cues, posing significant\nsafety risks. To address this, we propose an environmental intelligence\nframework that combines Acoustic Scene Classification (ASC) with Sound Event\nLocalization and Detection (SELD). Our system first employs a lightweight ASC\nmodel to infer the current environment. The scene prediction then dynamically\nconditions a SELD network, tuning its sensitivity to detect and localize sounds\nthat are most salient to the current context. On simulated headphone data, the\nproposed ASC-conditioned SELD system demonstrates improved spatial intelligence\nover a conventional baseline. This work represents a crucial step towards\ncreating intelligent hearables that can deliver crucial environmental\ninformation, fostering a safer and more context-aware listening experience."}
{"id": "2509.14442", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14442", "abs": "https://arxiv.org/abs/2509.14442", "authors": ["Arjun Teh", "Wael H. Ali", "Joshua Rapp", "Hassan Mansour"], "title": "Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography", "comment": "Presented in ISCS25", "summary": "We develop a framework for non-invasive volumetric indoor airflow estimation\nfrom a single viewpoint using background-oriented schlieren (BOS) measurements\nand physics-informed reconstruction. Our framework utilizes a light projector\nthat projects a pattern onto a target back-wall and a camera that observes\nsmall distortions in the light pattern. While the single-view BOS tomography\nproblem is severely ill-posed, our proposed framework addresses this using: (1)\nimproved ray tracing, (2) a physics-based light rendering approach and loss\nformulation, and (3) a physics-based regularization using a physics-informed\nneural network (PINN) to ensure that the reconstructed airflow is consistent\nwith the governing equations for buoyancy-driven flows."}
{"id": "2509.14804", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14804", "abs": "https://arxiv.org/abs/2509.14804", "authors": ["Mingchen Shao", "Bingshen Mu", "Chengyou Wang", "Hai Li", "Ying Yan", "Zhonghua Fu", "Lei Xie"], "title": "Towards Building Speech Large Language Models for Multitask Understanding in Low-Resource Languages", "comment": null, "summary": "Speech large language models (SLLMs) built on speech encoders, adapters, and\nLLMs demonstrate remarkable multitask understanding performance in\nhigh-resource languages such as English and Chinese. However, their\neffectiveness substantially degrades in low-resource languages such as Thai.\nThis limitation arises from three factors: (1) existing commonly used speech\nencoders, like the Whisper family, underperform in low-resource languages and\nlack support for broader spoken language understanding tasks; (2) the ASR-based\nalignment paradigm requires training the entire SLLM, leading to high\ncomputational cost; (3) paired speech-text data in low-resource languages is\nscarce. To overcome these challenges in the low-resource language Thai, we\nintroduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder\nfor Thai. It is obtained by continuously training the standard SSL XLSR model\non 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a\nspeech-text alignment method that is more resource-efficient and\nmultitask-effective than typical ASR-based alignment. Finally, we present\nThai-SUP, a pipeline for generating Thai spoken language understanding data\nfrom high-resource languages, yielding the first Thai spoken language\nunderstanding dataset of over 1,000 hours. Multiple experiments demonstrate the\neffectiveness of our methods in building a Thai multitask-understanding SLLM.\nWe open-source XLSR-Thai and Thai-SUP to facilitate future research."}
{"id": "2509.14659", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14659", "abs": "https://arxiv.org/abs/2509.14659", "authors": ["Kartik Hegde", "Rehana Mahfuz", "Yinyi Guo", "Erik Visser"], "title": "Aligning Audio Captions with Human Preferences", "comment": "Submitted to ICASSP 2026", "summary": "Current audio captioning systems rely heavily on supervised learning with\npaired audio-caption datasets, which are expensive to curate and may not\nreflect human preferences in real-world scenarios. To address this limitation,\nwe propose a preference-aligned audio captioning framework based on\nReinforcement Learning from Human Feedback (RLHF). To effectively capture\nnuanced human preferences, we train a Contrastive Language-Audio Pretraining\n(CLAP)-based reward model using human-labeled pairwise preference data. This\nreward model is integrated into a reinforcement learning framework to fine-tune\nany baseline captioning system without relying on ground-truth caption\nannotations. Extensive human evaluations across multiple datasets show that our\nmethod produces captions preferred over those from baseline models,\nparticularly in cases where the baseline models fail to provide correct and\nnatural captions. Furthermore, our framework achieves performance comparable to\nsupervised approaches with ground-truth data, demonstrating its effectiveness\nin aligning audio captioning with human preferences and its scalability in\nreal-world scenarios."}
{"id": "2509.14447", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14447", "abs": "https://arxiv.org/abs/2509.14447", "authors": ["Sriram V. C. Nallani", "Gautham Ramachandran", "Sahil S. Shah"], "title": "Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces", "comment": "9 pages, 5 figures, submitted to ICLR 2026", "summary": "Brain-Computer Interfaces face challenges from neural signal instability and\nmemory constraints for real-time implantable applications. We introduce an\nonline SNN decoder using local three-factor learning rules with dual-timescale\neligibility traces that avoid backpropagation through time while maintaining\ncompetitive performance. Our approach combines error-modulated Hebbian updates,\nfast/slow trace consolidation, and adaptive learning rate control, requiring\nonly O(1) memory versus O(T) for BPTT methods. Evaluations on two primate\ndatasets achieve comparable decoding accuracy (Pearson $R \\geq 0.63$ Zenodo, $R\n\\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence than\nBPTT-trained SNNs. Closed-loop simulations with synthetic neural populations\ndemonstrate adaptation to neural disruptions and learning from scratch without\noffline calibration. This work enables memory-efficient, continuously adaptive\nneural decoding suitable for resource-constrained implantable BCI systems."}
{"id": "2509.14858", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14858", "abs": "https://arxiv.org/abs/2509.14858", "authors": ["Duojia Li", "Shenghui Lu", "Hongchen Pan", "Zongyi Zhan", "Qingyang Hong", "Lin Li"], "title": "MeanFlowSE: one-step generative speech enhancement via conditional mean flow", "comment": null, "summary": "Multistep inference is a bottleneck for real-time generative speech\nenhancement because flow- and diffusion-based systems learn an instantaneous\nvelocity field and therefore rely on iterative ordinary differential equation\n(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that\nlearns the average velocity over finite intervals along a trajectory. Using a\nJacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a\nlocal training objective that directly supervises finite-interval displacement\nwhile remaining consistent with the instantaneous-field constraint on the\ndiagonal. At inference, MeanFlowSE performs single-step generation via a\nbackward-in-time displacement, removing the need for multistep solvers; an\noptional few-step variant offers additional refinement. On VoiceBank-DEMAND,\nthe single-step model achieves strong intelligibility, fidelity, and perceptual\nquality with substantially lower computational cost than multistep baselines.\nThe method requires no knowledge distillation or external teachers, providing\nan efficient, high-fidelity framework for real-time generative speech\nenhancement."}
{"id": "2509.14677", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14677", "abs": "https://arxiv.org/abs/2509.14677", "authors": ["Miseul Kim", "Seyun Um", "Hyeonjin Cha", "Hong-goo Kang"], "title": "SpeechMLC: Speech Multi-label Classification", "comment": "Accepted to INTERSPEECH 2025", "summary": "In this paper, we propose a multi-label classification framework to detect\nmultiple speaking styles in a speech sample. Unlike previous studies that have\nprimarily focused on identifying a single target style, our framework\neffectively captures various speaker characteristics within a unified\nstructure, making it suitable for generalized human-computer interaction\napplications. The proposed framework integrates cross-attention mechanisms\nwithin a transformer decoder to extract salient features associated with each\ntarget label from the input speech. To mitigate the data imbalance inherent in\nmulti-label speech datasets, we employ a data augmentation technique based on a\nspeech generation model. We validate our model's effectiveness through multiple\nobjective evaluations on seen and unseen corpora. In addition, we provide an\nanalysis of the influence of human perception on classification accuracy by\nconsidering the impact of human labeling agreement on model performance."}
{"id": "2509.14449", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14449", "abs": "https://arxiv.org/abs/2509.14449", "authors": ["Mahdi Shamsi", "Hadi Zayyani", "Hasan Abu Hilal", "Mohammad Salman"], "title": "Secure Blind Graph Signal Recovery and Adversary Detection Using Smoothness Maximization", "comment": null, "summary": "In this letter, we propose a secure blind Graph Signal Recovery (GSR)\nalgorithm that can detect adversary nodes. Some unknown adversaries are assumed\nto be injecting false data at their respective nodes in the graph. The number\nand location of adversaries are not known in advance and the goal is to recover\nthe graph signal in the presence of measurement noise and False Data Injection\n(FDI) caused by the adversaries. Consequently, the proposed algorithm would be\na perfect candidate to solve this challenging problem. Moreover, due to the\npresence of malicious nodes, the proposed method serves as a secure GSR\nalgorithm. For adversary detection, a statistical measure based on differential\nsmoothness is used. Specifically, the difference between the current observed\nsmoothness and the average smoothness excluding the corresponding node. This\ngenuine statistical approach leads to an effective and low-complexity adversary\ndetector. In addition, following malicious node detection, the GSR is performed\nusing a variant of smoothness maximization, which is solved efficiently as a\nfractional optimization problem using a Dinkelbach's algorithm. Analysis of the\ndetector, which determines the optimum threshold of the detector is also\npresented. Simulation results show a significant improvement of the proposed\nmethod in signal recovery compared to the median GSR algorithm and other\ncompeting methods."}
{"id": "2509.14880", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14880", "abs": "https://arxiv.org/abs/2509.14880", "authors": ["Rishabh Jain", "Naomi Harte"], "title": "From Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition", "comment": "submitted to ICASSP 2026. This work has been submitted to the IEEE\n  for possible publication", "summary": "Advances in self-supervised encoders have improved Visual Speech Recognition\n(VSR). Recent approaches integrating these encoders with LLM decoders improves\ntranscription accuracy; however, it remains unclear whether these gains stem\nfrom visual understanding or stronger language modeling. In this work, we\nsystematically evaluate LLM decoders by freezing or selectively updating the\nvisual encoder, scaling decoder size, comparing adaptation strategies and\narchitectures, and varying training data across LRS2, LRS3, and their\ncombination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and\nadaptation yield limited improvements, while combining datasets enhances\ngeneralization. Semantic analysis reveals that gains arise primarily from\nlexical rather than semantic processing. Our Llama-2-13B model trained on the\ncombined set achieves 24.7\\% WER on LRS3 and 47.0\\% on WildVSR, establishing\nSOTA among models trained without additional supervision. Our findings indicate\nLLM decoders refine contextual reasoning rather than visual features,\nemphasizing the need for stronger visual encoders to drive meaningful progress."}
{"id": "2509.14684", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14684", "abs": "https://arxiv.org/abs/2509.14684", "authors": ["Ye-Xin Lu", "Yu Gu", "Kun Wei", "Hui-Peng Du", "Yang Ai", "Zhen-Hua Ling"], "title": "DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis", "comment": "Submitted to ICASSP 2026", "summary": "This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework\nthat enables ENvironment-aware synthesis through Disentangled Audio Infilling.\nBy leveraging separate speaker and environment prompts, DAIEN-TTS allows\nindependent control over the timbre and the background environment of the\nsynthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first\nincorporates a pretrained speech-environment separation (SES) module to\ndisentangle the environmental speech into mel-spectrograms of clean speech and\nenvironment audio. Two random span masks of varying lengths are then applied to\nboth mel-spectrograms, which, together with the text embedding, serve as\nconditions for infilling the masked environmental mel-spectrogram, enabling the\nsimultaneous continuation of personalized speech and time-varying environmental\naudio. To further enhance controllability during inference, we adopt dual\nclass-free guidance (DCFG) for the speech and environment components and\nintroduce a signal-to-noise ratio (SNR) adaptation strategy to align the\nsynthesized speech with the environment prompt. Experimental results\ndemonstrate that DAIEN-TTS generates environmental personalized speech with\nhigh naturalness, strong speaker similarity, and high environmental fidelity."}
{"id": "2509.14503", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14503", "abs": "https://arxiv.org/abs/2509.14503", "authors": ["Zhongwen Sun", "Wei Chen", "Yuxuan Sun", "Bo Ai"], "title": "Age of Information Aided Intelligent Grant-Free Massive Access for Heterogeneous mMTC Traffic", "comment": null, "summary": "With the arrival of 6G, the Internet of Things (IoT) traffic is becoming more\nand more complex and diverse. To meet the diverse service requirements of IoT\ndevices, massive machine-type communications (mMTC) becomes a typical scenario,\nand more recently, grant-free random access (GF-RA) presents a promising\ndirection due to its low signaling overhead. However, existing GF-RA research\nprimarily focuses on improving the accuracy of user detection and data\nrecovery, without considering the heterogeneity of traffic. In this paper, we\ninvestigate a non-orthogonal GF-RA scenario where two distinct types of traffic\ncoexist: event-triggered traffic with alarm devices (ADs), and status update\ntraffic with monitor devices (MDs). The goal is to simultaneously achieve high\ndetection success rates for ADs and high information timeliness for MDs. First,\nwe analyze the age-based random access scheme and optimize the access\nparameters to minimize the average age of information (AoI) of MDs. Then, we\ndesign an age-based prior information aided autoencoder (A-PIAAE) to jointly\ndetect active devices, together with learned pilots used in GF-RA to reduce\ninterference between non-orthogonal pilots. In the decoder, an Age-based\nLearned Iterative Shrinkage Thresholding Algorithm (LISTA-AGE) utilizing the\nAoI of MDs as the prior information is proposed to enhance active user\ndetection. Theoretical analysis is provided to demonstrate the proposed A-PIAAE\nhas better convergence performance. Experiments demonstrate the advantage of\nthe proposed method in reducing the average AoI of MDs and improving the\nsuccessful detection rate of ADs."}
{"id": "2509.14893", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14893", "abs": "https://arxiv.org/abs/2509.14893", "authors": ["Yuanjian Chen", "Yang Xiao", "Jinjie Huang"], "title": "Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification", "comment": null, "summary": "Multimodal acoustic event classification plays a key role in audio-visual\nsystems. Although combining audio and visual signals improves recognition, it\nis still difficult to align them over time and to reduce the effect of noise\nacross modalities. Existing methods often treat audio and visual streams\nseparately, fusing features later with contrastive or mutual information\nobjectives. Recent advances explore multimodal graph learning, but most fail to\ndistinguish between intra- and inter-modal temporal dependencies. To address\nthis, we propose Temporally Heterogeneous Graph-based Contrastive Learning\n(THGCL). Our framework constructs a temporal graph for each event, where audio\nand video segments form nodes and their temporal links form edges. We introduce\nGaussian processes for intra-modal smoothness, Hawkes processes for inter-modal\ndecay, and contrastive learning to capture fine-grained relationships.\nExperiments on AudioSet show that THGCL achieves state-of-the-art performance."}
{"id": "2509.14784", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14784", "abs": "https://arxiv.org/abs/2509.14784", "authors": ["Keyu An", "Zhiyu Zhang", "Changfeng Gao", "Yabin Li", "Zhendong Peng", "Haoxu Wang", "Zhihao Du", "Han Zhao", "Zhifu Gao", "Xiangang Li"], "title": "MELA-TTS: Joint transformer-diffusion model with representation alignment for speech synthesis", "comment": "submitted to ICASSP 2026", "summary": "This work introduces MELA-TTS, a novel joint transformer-diffusion framework\nfor end-to-end text-to-speech synthesis. By autoregressively generating\ncontinuous mel-spectrogram frames from linguistic and speaker conditions, our\narchitecture eliminates the need for speech tokenization and multi-stage\nprocessing pipelines. To address the inherent difficulties of modeling\ncontinuous features, we propose a representation alignment module that aligns\noutput representations of the transformer decoder with semantic embeddings from\na pretrained ASR encoder during training. This mechanism not only speeds up\ntraining convergence, but also enhances cross-modal coherence between the\ntextual and acoustic domains. Comprehensive experiments demonstrate that\nMELA-TTS achieves state-of-the-art performance across multiple evaluation\nmetrics while maintaining robust zero-shot voice cloning capabilities, in both\noffline and streaming synthesis modes. Our results establish a new benchmark\nfor continuous feature generation approaches in TTS, offering a compelling\nalternative to discrete-token-based paradigms."}
{"id": "2509.14559", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14559", "abs": "https://arxiv.org/abs/2509.14559", "authors": ["Paolo Torrado", "Anders Pearson", "Jason Klein", "Alexander Moscibroda", "Joshua Smith"], "title": "Radiolunadiff: Estimation of wireless network signal strength in lunar terrain", "comment": null, "summary": "In this paper, we propose a novel physics-informed deep learning architecture\nfor predicting radio maps over lunar terrain. Our approach integrates a\nphysics-based lunar terrain generator, which produces realistic topography\ninformed by publicly available NASA data, with a ray-tracing engine to create a\nhigh-fidelity dataset of radio propagation scenarios. Building on this dataset,\nwe introduce a triplet-UNet architecture, consisting of two standard UNets and\na diffusion network, to model complex propagation effects. Experimental results\ndemonstrate that our method outperforms existing deep learning approaches on\nour terrain dataset across various metrics."}
{"id": "2509.14912", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14912", "abs": "https://arxiv.org/abs/2509.14912", "authors": ["Kangdi Wang", "Zhiyue Wu", "Dinghao Zhou", "Rui Lin", "Junyu Dai", "Tao Jiang"], "title": "Back to Ear: Perceptually Driven High Fidelity Music Reconstruction", "comment": "Check the Code here:\n  https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE and Model Weights\n  here: https://huggingface.co/earlab/EAR_VAE", "summary": "Variational Autoencoders (VAEs) are essential for large-scale audio tasks\nlike diffusion-based generation. However, existing open-source models often\nneglect auditory perceptual aspects during training, leading to weaknesses in\nphase accuracy and stereophonic spatial representation. To address these\nchallenges, we propose {\\epsilon}ar-VAE, an open-source music signal\nreconstruction model that rethinks and optimizes the VAE training paradigm. Our\ncontributions are threefold: (i) A K-weighting perceptual filter applied prior\nto loss calculation to align the objective with auditory perception. (ii) Two\nnovel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss\nusing its derivatives--Instantaneous Frequency and Group Delay--for precision.\n(iii) A new spectral supervision paradigm where magnitude is supervised by all\nfour Mid/Side/Left/Right components, while phase is supervised only by the LR\ncomponents. Experiments show {\\epsilon}ar-VAE at 44.1kHz substantially\noutperforms leading open-source models across diverse metrics, showing\nparticular strength in reconstructing high-frequency harmonics and the spatial\ncharacteristics."}
{"id": "2509.14789", "categories": ["eess.AS", "cs.CR", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14789", "abs": "https://arxiv.org/abs/2509.14789", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Acoustic Simulation Framework for Multi-channel Replay Speech Detection", "comment": "Submitted to ICASSP 2026", "summary": "Replay speech attacks pose a significant threat to voice-controlled systems,\nespecially in smart environments where voice assistants are widely deployed.\nWhile multi-channel audio offers spatial cues that can enhance replay detection\nrobustness, existing datasets and methods predominantly rely on single-channel\nrecordings. In this work, we introduce an acoustic simulation framework\ndesigned to simulate multi-channel replay speech configurations using publicly\navailable resources. Our setup models both genuine and spoofed speech across\nvaried environments, including realistic microphone and loudspeaker impulse\nresponses, room acoustics, and noise conditions. The framework employs measured\nloudspeaker directionalities during the replay attack to improve the realism of\nthe simulation. We define two spoofing settings, which simulate whether a\nreverberant or an anechoic speech is used in the replay scenario, and evaluate\nthe impact of omnidirectional and diffuse noise on detection performance. Using\nthe state-of-the-art M-ALRAD model for replay speech detection, we demonstrate\nthat synthetic data can support the generalization capabilities of the detector\nacross unseen enclosures."}
{"id": "2509.14665", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14665", "abs": "https://arxiv.org/abs/2509.14665", "authors": ["Tian-Yu Xiang", "Zheng Lei", "Xiao-Hu Zhou", "Xiao-Liang Xie", "Shi-Qi Liu", "Mei-Jiang Gui", "Hong-Yun Ou", "Xin-Zheng Huang", "Xin-Yi Fu", "Zeng-Guang Hou"], "title": "Task-Oriented Learning for Automatic EEG Denoising", "comment": null, "summary": "Electroencephalography (EEG) denoising methods typically depend on manual\nintervention or clean reference signals. This work introduces a task-oriented\nlearning framework for automatic EEG denoising that uses only task labels\nwithout clean reference signals. EEG recordings are first decomposed into\ncomponents based on blind source separation (BSS) techniques. Then, a\nlearning-based selector assigns a retention probability to each component, and\nthe denoised signal is reconstructed as a probability-weighted combination. A\ndownstream proxy-task model evaluates the reconstructed signal, with its task\nloss supervising the selector in a collaborative optimization scheme that\nrelies solely on task labels, eliminating the need for clean EEG references.\nExperiments on three datasets spanning two paradigms and multiple noise\nconditions show consistent gains in both task performance (accuracy:\n$2.56\\%\\uparrow$) and standard signal-quality metrics (signal-to-noise-ratio:\n$0.82$\\,dB\\,$\\uparrow$). Further analyses demonstrate that the task-oriented\nlearning framework is algorithm-agnostic, as it accommodates diverse\ndecomposition techniques and network backbones for both the selector and the\nproxy model. These promising results indicate that the proposed task-oriented\nlearning framework is a practical EEG denoising solution with potential\nimplications for neuroscience research and EEG-based interaction systems."}
{"id": "2509.14944", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14944", "abs": "https://arxiv.org/abs/2509.14944", "authors": ["Xiaolei Xu", "Chaoyue Niu", "Guy J. Brown", "Hector Romero", "Ning Ma"], "title": "Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening", "comment": "Submitted to ICASSP 2026", "summary": "Obstructive sleep apnoea (OSA) is a prevalent condition with significant\nhealth consequences, yet many patients remain undiagnosed due to the complexity\nand cost of over-night polysomnography. Acoustic-based screening provides a\nscalable alternative, yet performance is limited by environmental noise and the\nlack of physiological context. Respiratory effort is a key signal used in\nclinical scoring of OSA events, but current approaches require additional\ncontact sensors that reduce scalability and patient comfort. This paper\npresents the first study to estimate respiratory effort directly from nocturnal\naudio, enabling physiological context to be recovered from sound alone. We\npropose a latent-space fusion framework that integrates the estimated effort\nembeddings with acoustic features for OSA detection. Using a dataset of 157\nnights from 103 participants recorded in home environments, our respiratory\neffort estimator achieves a concordance correlation coefficient of 0.48,\ncapturing meaningful respiratory dynamics. Fusing effort and audio improves\nsensitivity and AUC over audio-only baselines, especially at low\napnoea-hypopnoea index thresholds. The proposed approach requires only\nsmartphone audio at test time, which enables sensor-free, scalable, and\nlongitudinal OSA monitoring."}
{"id": "2509.14855", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14855", "abs": "https://arxiv.org/abs/2509.14855", "authors": ["Michael Tatarjitzky", "Boaz Rafaely"], "title": "AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding and Dropout-Based Learning", "comment": "Submitted to ICASSP 2026", "summary": "Multichannel speech enhancement leverages spatial cues to improve\nintelligibility and quality, but most learning-based methods rely on specific\nmicrophone array geometry, unable to account for geometry changes. To mitigate\nthis limitation, current array-agnostic approaches employ large multi-geometry\ndatasets but may still fail to generalize to unseen layouts. We propose\nAmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes\narbitrary array recordings into the spherical harmonics domain using Ambisonics\nSignal Matching (ASM). A deep neural network is trained on simulated Ambisonics\ndata, combined with channel dropout for robustness against array-dependent\nencoding errors, therefore omitting the need for a diverse microphone array\ndatabase. Experiments show that while the baseline and proposed models perform\nsimilarly on the training arrays, the baseline degrades on unseen arrays. In\ncontrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating\nstrong generalization and practical potential for array-agnostic speech\nenhancement."}
{"id": "2509.14710", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14710", "abs": "https://arxiv.org/abs/2509.14710", "authors": ["Koya Sato"], "title": "Mitigating the Impact of Location Uncertainty on Radio Map-Based Predictive Rate Selection via Noisy-Input Gaussian Process", "comment": "6 pages, 8 figures. Accepted for presentation at 2025 IEEE GLOBECOM\n  Workshops: Workshop on Radio Maps for Communications and Sensing", "summary": "This paper proposes a predictive rate-selection framework based on Gaussian\nprocess (GP)-based radio map construction that is robust to location\nuncertainty. Radio maps are a promising tool for improving communication\nefficiency in 6G networks. Although they enable the design of location-based\nmaximum transmission rates by exploiting statistical channel information,\nexisting discussions often assume perfect (i.e., noiseless) location\ninformation during channel sensing. Since such information must be obtained\nfrom positioning systems such as global navigation satellite systems, it\ninevitably involves positioning errors; this location uncertainty can degrade\nthe reliability of radio map-based wireless systems. To mitigate this issue, we\nintroduce the noisy-input GP (NIGP), which treats location noise as additional\noutput noise by applying a Taylor approximation of the function of interest.\nNumerical results demonstrate that the proposed NIGP-based design achieves more\nreliable transmission-rate selection than pure GP and yields higher throughput\nthan path loss-based rate selection."}
{"id": "2509.15140", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15140", "abs": "https://arxiv.org/abs/2509.15140", "authors": ["Yuxin Luo", "Ruoyi Zhang", "Lu-Chuan Liu", "Tianyu Li", "Hangyu Liu"], "title": "FCPE: A Fast Context-based Pitch Estimation Model", "comment": "Under review", "summary": "Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription\nand singing voice conversion (SVC), but existing methods suffer significant\nperformance degradation under noise. In this paper, we propose FCPE, a fast\ncontext-based pitch estimation model that employs a Lynx-Net architecture with\ndepth-wise separable convolutions to effectively capture mel spectrogram\nfeatures while maintaining low computational cost and robust noise tolerance.\nExperiments show that our method achieves 96.79\\% Raw Pitch Accuracy (RPA) on\nthe MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time\nFactor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly\noutperforms existing algorithms in efficiency. Code is available at\nhttps://github.com/CNChTu/FCPE."}
{"id": "2509.14934", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14934", "abs": "https://arxiv.org/abs/2509.14934", "authors": ["Francisco Messina", "Francesca Ronchini", "Luca Comanducci", "Paolo Bestagini", "Fabio Antonacci"], "title": "Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance", "comment": null, "summary": "A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment."}
{"id": "2509.14711", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14711", "abs": "https://arxiv.org/abs/2509.14711", "authors": ["Ziwei Huang", "Shiliang Lu", "Lu Bai", "Xuesong Cai", "Xiang Cheng"], "title": "LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines", "comment": null, "summary": "Based on Synesthesia of Machines (SoM), a large language model (LLM) is\nadapted for multipath generation (LLM4MG) for the first time. Considering a\ntypical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new\nmulti-modal sensing-communication dataset is constructed, named SynthSoM-V2I,\nincluding channel multipath information, millimeter wave (mmWave) radar sensory\ndata, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based\non the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model\nMeta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The\nproposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic\nspace through feature extraction and fusion networks. To further achieve\ngeneral knowledge transfer from the pre-trained LLaMA for multipath generation\nvia multi-modal sensory data, the low-rank adaptation (LoRA)\nparameter-efficient fine-tuning and propagation-aware prompt engineering are\nexploited. Simulation results demonstrate that the proposed LLM4MG outperforms\nconventional deep learning-based methods in terms of line-of-sight\n(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath\npower/delay generation precision with normalized mean square error (NMSE) of\n0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and\ncross-scenario generalization. The utility of the proposed LLM4MG is validated\nby real-world generalization. The necessity of high-precision multipath\ngeneration for system design is also demonstrated by channel capacity\ncomparison."}
{"id": "2509.15151", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15151", "abs": "https://arxiv.org/abs/2509.15151", "authors": ["Stelios Katsis", "Vassilis Lyberatos", "Spyridon Kantarelis", "Edmund Dervakos", "Giorgos Stamou"], "title": "Exploring How Audio Effects Alter Emotion with Foundation Models", "comment": null, "summary": "Audio effects (FX) such as reverberation, distortion, modulation, and dynamic\nrange processing play a pivotal role in shaping emotional responses during\nmusic listening. While prior studies have examined links between low-level\naudio features and affective perception, the systematic impact of audio FX on\nemotion remains underexplored. This work investigates how foundation models -\nlarge-scale neural architectures pretrained on multimodal data - can be\nleveraged to analyze these effects. Such models encode rich associations\nbetween musical structure, timbre, and affective meaning, offering a powerful\nframework for probing the emotional consequences of sound design techniques. By\napplying various probing methods to embeddings from deep learning models, we\nexamine the complex, nonlinear relationships between audio FX and estimated\nemotion, uncovering patterns tied to specific effects and evaluating the\nrobustness of foundation audio models. Our findings aim to advance\nunderstanding of the perceptual impact of audio production practices, with\nimplications for music cognition, performance, and affective computing."}
{"id": "2509.14946", "categories": ["eess.AS", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14946", "abs": "https://arxiv.org/abs/2509.14946", "authors": ["Bingsong Bai", "Qihang Lu", "Wenbing Yang", "Zihan Sun", "YueRan Hou", "Peilei Jia", "Songbai Pu", "Ruibo Fu", "Yingming Gao", "Ya Li", "Jun Gao"], "title": "SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding", "comment": "submitted to ICASSP 2026", "summary": "Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing\nmore realistic and engaging speech. However, existing methods typically depend\non proprietary datasets, while publicly available resources often suffer from\nincomplete speech, inaccurate or missing timestamps, and limited real-world\nrelevance. To address these problems, we propose an automated framework for\ngenerating large-scale paralinguistic data and apply it to construct the\nSynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with\n118.75 hours of data and precise timestamps, all derived from natural\nconversational speech. Our contributions lie in introducing the first automated\nmethod for constructing large-scale paralinguistic datasets and releasing the\nSynParaSpeech corpus, which advances speech generation through more natural\nparalinguistic synthesis and enhances speech understanding by improving\nparalinguistic event detection. The dataset and audio samples are available at\nhttps://github.com/ShawnPi233/SynParaSpeech."}
{"id": "2509.14764", "categories": ["eess.SP", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14764", "abs": "https://arxiv.org/abs/2509.14764", "authors": ["Yuanyuan Yao", "Simon Geirnaert", "Tinne Tuytelaars", "Alexander Bertrand"], "title": "Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding", "comment": null, "summary": "Decoding the attended speaker in a multi-speaker environment from\nelectroencephalography (EEG) has attracted growing interest in recent years,\nwith neuro-steered hearing devices as a driver application. Current approaches\ntypically rely on ground-truth labels of the attended speaker during training,\nnecessitating calibration sessions for each user and each EEG set-up to achieve\noptimal performance. While unsupervised self-adaptive auditory attention\ndecoding (AAD) for stimulus reconstruction has been developed to eliminate the\nneed for labeled data, it suffers from an initialization bias that can\ncompromise performance. Although an unbiased variant has been proposed to\naddress this limitation, it introduces substantial computational complexity\nthat scales with data size. This paper presents three computationally efficient\nalternatives that achieve comparable performance, but with a significantly\nlower and constant computational cost. The code for the proposed algorithms is\navailable at https://github.com/YYao-42/Unsupervised_AAD."}
{"id": "2509.15210", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15210", "abs": "https://arxiv.org/abs/2509.15210", "authors": ["Chen Si", "Qianyi Wu", "Chaitanya Amballa", "Romit Roy Choudhury"], "title": "Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation", "comment": null, "summary": "Realistic sound simulation plays a critical role in many applications. A key\nelement in sound simulation is the room impulse response (RIR), which\ncharacterizes how sound propagates from a source to a listener within a given\nspace. Recent studies have applied neural implicit methods to learn RIR using\ncontext information collected from the environment, such as scene images.\nHowever, these approaches do not effectively leverage explicit geometric\ninformation from the environment. To further exploit the potential of neural\nimplicit models with direct geometric features, we present Mesh-infused Neural\nAcoustic Field (MiNAF), which queries a rough room mesh at given locations and\nextracts distance distributions as an explicit representation of local context.\nOur approach demonstrates that incorporating explicit local geometric features\ncan better guide the neural network in generating more accurate RIR\npredictions. Through comparisons with conventional and state-of-the-art\nbaseline methods, we show that MiNAF performs competitively across various\nevaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets\nwith limited training samples, demonstrating an advance in high-fidelity sound\nsimulation."}
{"id": "2509.14959", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14959", "abs": "https://arxiv.org/abs/2509.14959", "authors": ["Anton Selitskiy", "Akib Shahriyar", "Jishnuraj Prakasan"], "title": "Discrete optimal transport is a strong audio adversarial attack", "comment": null, "summary": "In this paper, we show that discrete optimal transport (DOT) is an effective\nblack-box adversarial attack against modern audio anti-spoofing countermeasures\n(CMs). Our attack operates as a post-processing, distribution-alignment step:\nframe-level WavLM embeddings of generated speech are aligned to an unpaired\nbona fide pool via entropic OT and a top-$k$ barycentric projection, then\ndecoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with\nAASIST baselines, DOT yields consistently high equal error rate (EER) across\ndatasets and remains competitive after CM fine-tuning, outperforming several\nconventional attacks in cross-dataset transfer. Ablation analysis highlights\nthe practical impact of vocoder overlap. Results indicate that\ndistribution-level alignment is a powerful and stable attack surface for\ndeployed CMs."}
{"id": "2509.14809", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14809", "abs": "https://arxiv.org/abs/2509.14809", "authors": ["Ning Wang", "Chenyu Zhang", "Yanshi Sun", "Minghui Min", "Shiyin Li"], "title": "Comparative Performance Analysis of Different Hybrid NOMA Schemes", "comment": "9 pages, 6 figures. Paper submitted to IEEE Internet of Things\n  Journal, paper ID IoT-55019-2025", "summary": "Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages\nof pure NOMA and conventional OMA organically, has emerged as a highly\npromising multiple access technology for future wireless networks. Recent\nstudies have proposed various H-NOMA systems by employing different successive\ninterference cancellation (SIC) methods for the NOMA transmission phase.\nHowever, existing analyses typically assume a fixed channel gain order between\npaired users, despite the fact that channel coefficients follow random\ndistribution, leading to their magnitude relationships inherently stochastic\nand time varying. This paper analyzes the performance of three H-NOMA schemes\nunder stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA\nscheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;\nc) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical\nanalysis derives closed-form expressions for the probability that H-NOMA\nschemes underperform conventional OMA. Asymptotic results in the high\nsignal-to-noise ratio (SNR) regime are also developed. Simulation results\nvalidate our analysis and demonstrate the performance of H-NOMA schemes across\ndifferent SNR scenarios, providing a theoretical foundation for the deployment\nof H-NOMA in next-generation wireless systems."}
{"id": "2509.15222", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.15222", "abs": "https://arxiv.org/abs/2509.15222", "authors": ["Junhyung Park", "Yonghyun Kim", "Joonhyung Bae", "Kirak Kim", "Taegyun Kwon", "Alexander Lerch", "Juhan Nam"], "title": "Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation", "comment": "Accepted to the Late-Breaking Demo Session of the 26th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2025", "summary": "Piano performance is a multimodal activity that intrinsically combines\nphysical actions with the acoustic rendition. Despite growing research interest\nin analyzing the multimodal nature of piano performance, the laborious process\nof acquiring large-scale multimodal data remains a significant bottleneck,\nhindering further progress in this field. To overcome this barrier, we present\nan integrated web toolkit comprising two graphical user interfaces (GUIs): (i)\nPiaRec, which supports the synchronized acquisition of audio, video, MIDI, and\nperformance metadata. (ii) ASDF, which enables the efficient annotation of\nperformer fingering from the visual data. Collectively, this system can\nstreamline the acquisition of multimodal piano performance datasets."}
{"id": "2509.15001", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15001", "abs": "https://arxiv.org/abs/2509.15001", "authors": ["Théo Charlot", "Tarek Kunze", "Maxime Poli", "Alejandrina Cristia", "Emmanuel Dupoux", "Marvin Lavechin"], "title": "BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings", "comment": "5 pages, 1 figure", "summary": "Child-centered long-form recordings are essential for studying early language\ndevelopment, but existing speech models trained on clean adult data perform\npoorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the\nfirst self-supervised speech representation model trained on 13,000 hours of\nmultilingual child-centered long-form recordings spanning over 40 languages. We\nevaluate BabyHuBERT on speaker segmentation, identifying when target children\nspeak versus female adults, male adults, or other children -- a fundamental\npreprocessing step for analyzing naturalistic language experiences. BabyHuBERT\nachieves F1-scores from 52.1% to 74.4% across six diverse datasets,\nconsistently outperforming W2V2-LL4300 (trained on English long-forms) and\nstandard HuBERT (trained on clean adult speech). Notable improvements include\n13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon\nIslands corpora, demonstrating effectiveness on underrepresented languages. By\nsharing code and models, BabyHuBERT serves as a foundation model for child\nspeech research, enabling fine-tuning on diverse downstream tasks."}
{"id": "2509.14836", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14836", "abs": "https://arxiv.org/abs/2509.14836", "authors": ["Keitaro Yamashita", "Kazuki Naganuma", "Shunsuke Ono"], "title": "Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization", "comment": "Submitted to the IEEE Open Journal of Signal Processing", "summary": "This paper proposes a method for vertex-wise flexible sampling of a broad\nclass of graph signals, designed to attain the best possible recovery based on\nthe generalized sampling theory. This is achieved by designing a sampling\noperator by an optimization problem, which is inherently non-convex, as the\nbest possible recovery imposes a rank constraint. An existing method for\nvertex-wise flexible sampling is able to control the number of active vertices\nbut cannot incorporate prior knowledge of mandatory or forbidden vertices. To\naddress these challenges, we formulate the operator design as a problem that\nhandles a constraint of the number of active vertices and prior knowledge on\nspecific vertices for sampling, mandatory inclusion or exclusion. We\ntransformed this constrained problem into a difference-of-convex (DC)\noptimization problem by using the nuclear norm and a DC penalty for vertex\nselection. To solve this, we develop a convergent solver based on the general\ndouble-proximal gradient DC algorithm. The effectiveness of our method is\ndemonstrated through experiments on various graph signal models, including\nreal-world data, showing superior performance in the recovery accuracy by\ncomparing to existing methods."}
{"id": "2509.14430", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14430", "abs": "https://arxiv.org/abs/2509.14430", "authors": ["Yufeng Yang", "Yiteng Huang", "Yong Xu", "Li Wan", "Suwon Shon", "Yang Liu", "Yifeng Fan", "Zhaojun Yang", "Olivier Siohan", "Yue Liu", "Ming Sun", "Florian Metze"], "title": "Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses", "comment": null, "summary": "With the growing adoption of wearable devices such as smart glasses for AI\nassistants, wearer speech recognition (WSR) is becoming increasingly critical\nto next-generation human-computer interfaces. However, in real environments,\ninterference from side-talk speech remains a significant challenge to WSR and\nmay cause accumulated errors for downstream tasks such as natural language\nprocessing. In this work, we introduce a novel multi-channel differential\nautomatic speech recognition (ASR) method for robust WSR on smart glasses. The\nproposed system takes differential inputs from different frontends that\ncomplement each other to improve the robustness of WSR, including a beamformer,\nmicrophone selection, and a lightweight side-talk detection model. Evaluations\non both simulated and real datasets demonstrate that the proposed system\noutperforms the traditional approach, achieving up to an 18.0% relative\nreduction in word error rate."}
{"id": "2509.15008", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15008", "abs": "https://arxiv.org/abs/2509.15008", "authors": ["Chaoyue Niu", "Veronica Rowe", "Guy J. Brown", "Heather Elphick", "Heather Kenyon", "Lowri Thomas", "Sam Johnson", "Ning Ma"], "title": "Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models", "comment": null, "summary": "Paediatric obstructive sleep apnoea (OSA) is clinically significant yet\ndifficult to diagnose, as children poorly tolerate sensor-based\npolysomnography. Acoustic monitoring provides a non-invasive alternative for\nhome-based OSA screening, but limited paediatric data hinders the development\nof robust deep learning approaches. This paper proposes a transfer learning\nframework that adapts acoustic models pretrained on adult sleep data to\npaediatric OSA detection, incorporating SpO2-based desaturation patterns to\nenhance model training. Using a large adult sleep dataset (157 nights) and a\nsmaller paediatric dataset (15 nights), we systematically evaluate (i) single-\nversus multi-task learning, (ii) encoder freezing versus full fine-tuning, and\n(iii) the impact of delaying SpO2 labels to better align them with the\nacoustics and capture physiologically meaningful features. Results show that\nfine-tuning with SpO2 integration consistently improves paediatric OSA\ndetection compared with baseline models without adaptation. These findings\ndemonstrate the feasibility of transfer learning for home-based OSA screening\nin children and offer its potential clinical value for early diagnosis."}
{"id": "2509.14909", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14909", "abs": "https://arxiv.org/abs/2509.14909", "authors": ["Flor Ortiz", "Eva Lagunas"], "title": "Hybrid Table-Assisted and RL-Based Dynamic Routing for NGSO Satellite Networks", "comment": null, "summary": "This letter investigates dynamic routing in Next-Generation Satellite Orbit\n(NGSO) constellations and proposes a hybrid strategy that combines precomputed\nrouting tables with a Deep Q-Learning (DQL) fallback mechanism. While fully\nRL-based schemes offer adaptability to topology dynamics, they often suffer\nfrom high complexity, long convergence times, and unstable performance under\nheavy traffic. In contrast, the proposed framework exploits deterministic table\nlookups under nominal conditions and selectively activates the DQL agent only\nwhen links become unavailable or congested. Simulation results in large-scale\nNGSO networks show that the hybrid approach consistently achieves higher packet\ndelivery ratio, lower end-to-end delay, shorter average hop count, and improved\nthroughput compared to a pure RL baseline. These findings highlight the\neffectiveness of hybrid routing as a scalable and resilient solution for\ndelay-sensitive satellite broadband services"}
{"id": "2509.14659", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14659", "abs": "https://arxiv.org/abs/2509.14659", "authors": ["Kartik Hegde", "Rehana Mahfuz", "Yinyi Guo", "Erik Visser"], "title": "Aligning Audio Captions with Human Preferences", "comment": "Submitted to ICASSP 2026", "summary": "Current audio captioning systems rely heavily on supervised learning with\npaired audio-caption datasets, which are expensive to curate and may not\nreflect human preferences in real-world scenarios. To address this limitation,\nwe propose a preference-aligned audio captioning framework based on\nReinforcement Learning from Human Feedback (RLHF). To effectively capture\nnuanced human preferences, we train a Contrastive Language-Audio Pretraining\n(CLAP)-based reward model using human-labeled pairwise preference data. This\nreward model is integrated into a reinforcement learning framework to fine-tune\nany baseline captioning system without relying on ground-truth caption\nannotations. Extensive human evaluations across multiple datasets show that our\nmethod produces captions preferred over those from baseline models,\nparticularly in cases where the baseline models fail to provide correct and\nnatural captions. Furthermore, our framework achieves performance comparable to\nsupervised approaches with ground-truth data, demonstrating its effectiveness\nin aligning audio captioning with human preferences and its scalability in\nreal-world scenarios."}
{"id": "2509.15082", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15082", "abs": "https://arxiv.org/abs/2509.15082", "authors": ["Yu-Wen Chen", "William Ho", "Maxim Topaz", "Julia Hirschberg", "Zoran Kostic"], "title": "From Who Said What to Who They Are: Modular Training-free Identity-Aware LLM Refinement of Speaker Diarization", "comment": null, "summary": "Speaker diarization (SD) struggles in real-world scenarios due to dynamic\nenvironments and unknown speaker counts. SD is rarely used alone and is often\npaired with automatic speech recognition (ASR), but non-modular methods that\njointly train on domain-specific data have limited flexibility. Moreover, many\napplications require true speaker identities rather than SD's pseudo labels. We\npropose a training-free modular pipeline combining off-the-shelf SD, ASR, and a\nlarge language model (LLM) to determine who spoke, what was said, and who they\nare. Using structured LLM prompting on reconciled SD and ASR outputs, our\nmethod leverages semantic continuity in conversational context to refine\nlow-confidence speaker labels and assigns role identities while correcting\nsplit speakers. On a real-world patient-clinician dataset, our approach\nachieves a 29.7% relative error reduction over baseline reconciled SD and ASR.\nIt enhances diarization performance without additional training and delivers a\ncomplete pipeline for SD, ASR, and speaker identity detection in practical\napplications."}
{"id": "2509.15069", "categories": ["eess.SP", "cs.DS", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.15069", "abs": "https://arxiv.org/abs/2509.15069", "authors": ["Deijany Rodriguez Linares", "Oksana Moryakova", "Håkan Johansson"], "title": "Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This letter presents a novel approach for \\mbox{efficiently} computing\ntime-index powered weighted sums of the form $\\sum_{n=0}^{N-1} n^{K} v[n]$\nusing cascaded accumulators. Traditional direct computation requires\n$K{\\times}N$ general multiplications, which become prohibitive for large $N$,\nwhile alternative strategies based on lookup tables or signal reversal require\nstoring entire data blocks. By exploiting accumulator properties, the proposed\nmethod eliminates the need for such storage and reduces the multiplicative cost\nto only $K{+}1$ constant multiplications, enabling efficient real-time\nimplementation. The approach is particularly useful when such sums need to be\nefficiently computed in sample-by-sample processing systems."}
{"id": "2509.14684", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14684", "abs": "https://arxiv.org/abs/2509.14684", "authors": ["Ye-Xin Lu", "Yu Gu", "Kun Wei", "Hui-Peng Du", "Yang Ai", "Zhen-Hua Ling"], "title": "DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis", "comment": "Submitted to ICASSP 2026", "summary": "This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework\nthat enables ENvironment-aware synthesis through Disentangled Audio Infilling.\nBy leveraging separate speaker and environment prompts, DAIEN-TTS allows\nindependent control over the timbre and the background environment of the\nsynthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first\nincorporates a pretrained speech-environment separation (SES) module to\ndisentangle the environmental speech into mel-spectrograms of clean speech and\nenvironment audio. Two random span masks of varying lengths are then applied to\nboth mel-spectrograms, which, together with the text embedding, serve as\nconditions for infilling the masked environmental mel-spectrogram, enabling the\nsimultaneous continuation of personalized speech and time-varying environmental\naudio. To further enhance controllability during inference, we adopt dual\nclass-free guidance (DCFG) for the speech and environment components and\nintroduce a signal-to-noise ratio (SNR) adaptation strategy to align the\nsynthesized speech with the environment prompt. Experimental results\ndemonstrate that DAIEN-TTS generates environmental personalized speech with\nhigh naturalness, strong speaker similarity, and high environmental fidelity."}
{"id": "2509.15085", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15085", "abs": "https://arxiv.org/abs/2509.15085", "authors": ["Simon Welker", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streaming Mel Vocoding with Generative Flow Matching", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram\nto an audio waveform, is still a key component in many text-to-speech (TTS)\nsystems today. Based on generative flow matching, our prior work on generative\nSTFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel\nfilterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for\nspeech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total\nlatency of 48 ms. We show real-time streaming capability at this latency not\nonly in theory, but in practice on a consumer laptop GPU. Furthermore, we show\nthat our model achieves substantially better PESQ and SI-SDR values compared to\nwell-established not streaming-capable baselines for Mel vocoding including\nHiFi-GAN."}
{"id": "2509.15129", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15129", "abs": "https://arxiv.org/abs/2509.15129", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "Doppler Radiance Field-Guided Antenna Selection for Improved Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition", "comment": null, "summary": "With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard\nfor advanced sensing, interest in using Wi-Fi Channel State Information (CSI)\nfor remote sensing has surged. Recent findings indicate that learning a unified\nthree-dimensional motion representation through Doppler Radiance Fields (DoRFs)\nderived from CSI significantly improves the generalization capabilities of\nWi-Fi-based human activity recognition (HAR). Despite this progress, CSI\nsignals remain affected by asynchronous access point (AP) clocks and additive\nnoise from environmental and hardware sources. Consequently, even with existing\npreprocessing techniques, both the CSI data and Doppler velocity projections\nused in DoRFs are still susceptible to noise and outliers, limiting HAR\nperformance. To address this challenge, we propose a novel framework for\nmulti-antenna APs to suppress noise and identify the most informative antennas\nbased on DoRF fitting errors, which capture inconsistencies among Doppler\nvelocity projections. Experimental results on a challenging small-scale hand\ngesture recognition dataset demonstrate that the proposed DoRF-guided\nWi-Fi-based HAR approach significantly improves generalization capability,\npaving the way for robust real-world sensing deployments."}
{"id": "2509.14764", "categories": ["eess.SP", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.14764", "abs": "https://arxiv.org/abs/2509.14764", "authors": ["Yuanyuan Yao", "Simon Geirnaert", "Tinne Tuytelaars", "Alexander Bertrand"], "title": "Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding", "comment": null, "summary": "Decoding the attended speaker in a multi-speaker environment from\nelectroencephalography (EEG) has attracted growing interest in recent years,\nwith neuro-steered hearing devices as a driver application. Current approaches\ntypically rely on ground-truth labels of the attended speaker during training,\nnecessitating calibration sessions for each user and each EEG set-up to achieve\noptimal performance. While unsupervised self-adaptive auditory attention\ndecoding (AAD) for stimulus reconstruction has been developed to eliminate the\nneed for labeled data, it suffers from an initialization bias that can\ncompromise performance. Although an unbiased variant has been proposed to\naddress this limitation, it introduces substantial computational complexity\nthat scales with data size. This paper presents three computationally efficient\nalternatives that achieve comparable performance, but with a significantly\nlower and constant computational cost. The code for the proposed algorithms is\navailable at https://github.com/YYao-42/Unsupervised_AAD."}
{"id": "2509.15095", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15095", "abs": "https://arxiv.org/abs/2509.15095", "authors": ["Yutong Liu", "Ziyue Zhang", "Yongbin Yu", "Xiangxiang Wang", "Yuqing Cai", "Nyima Tashi"], "title": "Listening, Imagining \\& Refining: A Heuristic Optimized ASR Correction Framework with LLMs", "comment": null, "summary": "Automatic Speech Recognition (ASR) systems remain prone to errors that affect\ndownstream applications. In this paper, we propose LIR-ASR, a heuristic\noptimized iterative correction framework using LLMs, inspired by human auditory\nperception. LIR-ASR applies a \"Listening-Imagining-Refining\" strategy,\ngenerating phonetic variants and refining them in context. A heuristic\noptimization with finite state machine (FSM) is introduced to prevent the\ncorrection process from being trapped in local optima and rule-based\nconstraints help maintain semantic fidelity. Experiments on both English and\nChinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of\nup to 1.5 percentage points compared to baselines, demonstrating substantial\naccuracy gains in transcription."}
{"id": "2509.15162", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15162", "abs": "https://arxiv.org/abs/2509.15162", "authors": ["Jingreng Lei", "Yang Li", "Ziyue Wang", "Qingfeng Lin", "Ya-Feng Liu", "Yik-Chung Wu"], "title": "A Unified Distributed Algorithm for Hybrid Near-Far Field Activity Detection in Cell-Free Massive MIMO", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A great amount of endeavor has recently been devoted to activity detection\nfor massive machine-type communications in cell-free multiple-input\nmultiple-output (MIMO) systems. However, as the number of antennas at the\naccess points (APs) increases, the Rayleigh distance that separates the\nnear-field and far-field regions also expands, rendering the conventional\nassumption of far-field propagation alone impractical. To address this\nchallenge, this paper establishes a covariance-based formulation that can\neffectively capture the statistical property of hybrid near-far field channels.\nBased on this formulation, we theoretically reveal that increasing the\nproportion of near-field channels enhances the detection performance.\nFurthermore, we propose a distributed algorithm, where each AP performs local\nactivity detection and only exchanges the detection results to the central\nprocessing unit, thus significantly reducing the computational complexity and\nthe communication overhead. Not only with convergence guarantee, the proposed\nalgorithm is unified in the sense that it can handle single-cell or cell-free\nsystems with either near-field or far-field devices as special cases.\nSimulation results validate the theoretical analyses and demonstrate the\nsuperior performance of the proposed approach compared with existing methods."}
{"id": "2509.14789", "categories": ["eess.AS", "cs.CR", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14789", "abs": "https://arxiv.org/abs/2509.14789", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Acoustic Simulation Framework for Multi-channel Replay Speech Detection", "comment": "Submitted to ICASSP 2026", "summary": "Replay speech attacks pose a significant threat to voice-controlled systems,\nespecially in smart environments where voice assistants are widely deployed.\nWhile multi-channel audio offers spatial cues that can enhance replay detection\nrobustness, existing datasets and methods predominantly rely on single-channel\nrecordings. In this work, we introduce an acoustic simulation framework\ndesigned to simulate multi-channel replay speech configurations using publicly\navailable resources. Our setup models both genuine and spoofed speech across\nvaried environments, including realistic microphone and loudspeaker impulse\nresponses, room acoustics, and noise conditions. The framework employs measured\nloudspeaker directionalities during the replay attack to improve the realism of\nthe simulation. We define two spoofing settings, which simulate whether a\nreverberant or an anechoic speech is used in the replay scenario, and evaluate\nthe impact of omnidirectional and diffuse noise on detection performance. Using\nthe state-of-the-art M-ALRAD model for replay speech detection, we demonstrate\nthat synthetic data can support the generalization capabilities of the detector\nacross unseen enclosures."}
{"id": "2509.14304", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14304", "abs": "https://arxiv.org/abs/2509.14304", "authors": ["Eric Zhang", "Li Wei", "Sarah Chen", "Michael Wang"], "title": "Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework", "comment": null, "summary": "Stuttered and dysfluent speech detection systems have traditionally suffered\nfrom the trade-off between accuracy and clinical interpretability. While\nend-to-end deep learning models achieve high performance, their black-box\nnature limits clinical adoption. This paper looks at the Unconstrained\nDysfluency Modeling (UDM) series-the current state-of-the-art framework\ndeveloped by Berkeley that combines modular architecture, explicit phoneme\nalignment, and interpretable outputs for real-world clinical deployment.\nThrough extensive experiments involving patients and certified speech-language\npathologists (SLPs), we demonstrate that UDM achieves state-of-the-art\nperformance (F1: 0.89+-0.04) while providing clinically meaningful\ninterpretability scores (4.2/5.0). Our deployment study shows 87% clinician\nacceptance rate and 34% reduction in diagnostic time. The results provide\nstrong evidence that UDM represents a practical pathway toward AI-assisted\nspeech therapy in clinical environments."}
{"id": "2509.14632", "categories": ["eess.AS", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14632", "abs": "https://arxiv.org/abs/2509.14632", "authors": ["Miseul Kim", "Soo Jin Park", "Kyungguen Byun", "Hyeon-Kyeong Shin", "Sunkuk Moon", "Shuhua Zhang", "Erik Visser"], "title": "Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation", "comment": "Submitted to ICASSP 2026", "summary": "Speaker diarization systems often struggle with high intrinsic intra-speaker\nvariability, such as shifts in emotion, health, or content. This can cause\nsegments from the same speaker to be misclassified as different individuals,\nfor example, when one raises their voice or speaks faster during conversation.\nTo address this, we propose a style-controllable speech generation model that\naugments speech across diverse styles while preserving the target speaker's\nidentity. The proposed system starts with diarized segments from a conventional\ndiarizer. For each diarized segment, it generates augmented speech samples\nenriched with phonetic and stylistic diversity. And then, speaker embeddings\nfrom both the original and generated audio are blended to enhance the system's\nrobustness in grouping segments with high intrinsic intra-speaker variability.\nWe validate our approach on a simulated emotional speech dataset and the\ntruncated AMI dataset, demonstrating significant improvements, with error rate\nreductions of 49% and 35% on each dataset, respectively."}
{"id": "2509.14934", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14934", "abs": "https://arxiv.org/abs/2509.14934", "authors": ["Francisco Messina", "Francesca Ronchini", "Luca Comanducci", "Paolo Bestagini", "Fabio Antonacci"], "title": "Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance", "comment": null, "summary": "A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment."}
{"id": "2509.14675", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14675", "abs": "https://arxiv.org/abs/2509.14675", "authors": ["Xuanjun Chen", "Chia-Yu Hu", "I-Ming Lin", "Yi-Cheng Lin", "I-Hsiang Chiu", "You Zhang", "Sung-Feng Huang", "Yi-Hsuan Yang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "How Does Instrumental Music Help SingFake Detection?", "comment": "Work in progress", "summary": "Although many models exist to detect singing voice deepfakes (SingFake), how\nthese models operate, particularly with instrumental accompaniment, is unclear.\nWe investigate how instrumental music affects SingFake detection from two\nperspectives. To investigate the behavioral effect, we test different\nbackbones, unpaired instrumental tracks, and frequency subbands. To analyze the\nrepresentational effect, we probe how fine-tuning alters encoders' speech and\nmusic capabilities. Our results show that instrumental accompaniment acts\nmainly as data augmentation rather than providing intrinsic cues (e.g., rhythm\nor harmony). Furthermore, fine-tuning increases reliance on shallow speaker\nfeatures while reducing sensitivity to content, paralinguistic, and semantic\ninformation. These insights clarify how models exploit vocal versus\ninstrumental cues and can inform the design of more interpretable and robust\nSingFake detection systems."}
{"id": "2509.14675", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14675", "abs": "https://arxiv.org/abs/2509.14675", "authors": ["Xuanjun Chen", "Chia-Yu Hu", "I-Ming Lin", "Yi-Cheng Lin", "I-Hsiang Chiu", "You Zhang", "Sung-Feng Huang", "Yi-Hsuan Yang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "How Does Instrumental Music Help SingFake Detection?", "comment": "Work in progress", "summary": "Although many models exist to detect singing voice deepfakes (SingFake), how\nthese models operate, particularly with instrumental accompaniment, is unclear.\nWe investigate how instrumental music affects SingFake detection from two\nperspectives. To investigate the behavioral effect, we test different\nbackbones, unpaired instrumental tracks, and frequency subbands. To analyze the\nrepresentational effect, we probe how fine-tuning alters encoders' speech and\nmusic capabilities. Our results show that instrumental accompaniment acts\nmainly as data augmentation rather than providing intrinsic cues (e.g., rhythm\nor harmony). Furthermore, fine-tuning increases reliance on shallow speaker\nfeatures while reducing sensitivity to content, paralinguistic, and semantic\ninformation. These insights clarify how models exploit vocal versus\ninstrumental cues and can inform the design of more interpretable and robust\nSingFake detection systems."}
{"id": "2509.15001", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15001", "abs": "https://arxiv.org/abs/2509.15001", "authors": ["Théo Charlot", "Tarek Kunze", "Maxime Poli", "Alejandrina Cristia", "Emmanuel Dupoux", "Marvin Lavechin"], "title": "BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings", "comment": "5 pages, 1 figure", "summary": "Child-centered long-form recordings are essential for studying early language\ndevelopment, but existing speech models trained on clean adult data perform\npoorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the\nfirst self-supervised speech representation model trained on 13,000 hours of\nmultilingual child-centered long-form recordings spanning over 40 languages. We\nevaluate BabyHuBERT on speaker segmentation, identifying when target children\nspeak versus female adults, male adults, or other children -- a fundamental\npreprocessing step for analyzing naturalistic language experiences. BabyHuBERT\nachieves F1-scores from 52.1% to 74.4% across six diverse datasets,\nconsistently outperforming W2V2-LL4300 (trained on English long-forms) and\nstandard HuBERT (trained on clean adult speech). Notable improvements include\n13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon\nIslands corpora, demonstrating effectiveness on underrepresented languages. By\nsharing code and models, BabyHuBERT serves as a foundation model for child\nspeech research, enabling fine-tuning on diverse downstream tasks."}
{"id": "2509.14893", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14893", "abs": "https://arxiv.org/abs/2509.14893", "authors": ["Yuanjian Chen", "Yang Xiao", "Jinjie Huang"], "title": "Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification", "comment": null, "summary": "Multimodal acoustic event classification plays a key role in audio-visual\nsystems. Although combining audio and visual signals improves recognition, it\nis still difficult to align them over time and to reduce the effect of noise\nacross modalities. Existing methods often treat audio and visual streams\nseparately, fusing features later with contrastive or mutual information\nobjectives. Recent advances explore multimodal graph learning, but most fail to\ndistinguish between intra- and inter-modal temporal dependencies. To address\nthis, we propose Temporally Heterogeneous Graph-based Contrastive Learning\n(THGCL). Our framework constructs a temporal graph for each event, where audio\nand video segments form nodes and their temporal links form edges. We introduce\nGaussian processes for intra-modal smoothness, Hawkes processes for inter-modal\ndecay, and contrastive learning to capture fine-grained relationships.\nExperiments on AudioSet show that THGCL achieves state-of-the-art performance."}
{"id": "2509.14677", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14677", "abs": "https://arxiv.org/abs/2509.14677", "authors": ["Miseul Kim", "Seyun Um", "Hyeonjin Cha", "Hong-goo Kang"], "title": "SpeechMLC: Speech Multi-label Classification", "comment": "Accepted to INTERSPEECH 2025", "summary": "In this paper, we propose a multi-label classification framework to detect\nmultiple speaking styles in a speech sample. Unlike previous studies that have\nprimarily focused on identifying a single target style, our framework\neffectively captures various speaker characteristics within a unified\nstructure, making it suitable for generalized human-computer interaction\napplications. The proposed framework integrates cross-attention mechanisms\nwithin a transformer decoder to extract salient features associated with each\ntarget label from the input speech. To mitigate the data imbalance inherent in\nmulti-label speech datasets, we employ a data augmentation technique based on a\nspeech generation model. We validate our model's effectiveness through multiple\nobjective evaluations on seen and unseen corpora. In addition, we provide an\nanalysis of the influence of human perception on classification accuracy by\nconsidering the impact of human labeling agreement on model performance."}
{"id": "2509.15085", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15085", "abs": "https://arxiv.org/abs/2509.15085", "authors": ["Simon Welker", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streaming Mel Vocoding with Generative Flow Matching", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram\nto an audio waveform, is still a key component in many text-to-speech (TTS)\nsystems today. Based on generative flow matching, our prior work on generative\nSTFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel\nfilterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for\nspeech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total\nlatency of 48 ms. We show real-time streaming capability at this latency not\nonly in theory, but in practice on a consumer laptop GPU. Furthermore, we show\nthat our model achieves substantially better PESQ and SI-SDR values compared to\nwell-established not streaming-capable baselines for Mel vocoding including\nHiFi-GAN."}
{"id": "2509.14944", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14944", "abs": "https://arxiv.org/abs/2509.14944", "authors": ["Xiaolei Xu", "Chaoyue Niu", "Guy J. Brown", "Hector Romero", "Ning Ma"], "title": "Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening", "comment": "Submitted to ICASSP 2026", "summary": "Obstructive sleep apnoea (OSA) is a prevalent condition with significant\nhealth consequences, yet many patients remain undiagnosed due to the complexity\nand cost of over-night polysomnography. Acoustic-based screening provides a\nscalable alternative, yet performance is limited by environmental noise and the\nlack of physiological context. Respiratory effort is a key signal used in\nclinical scoring of OSA events, but current approaches require additional\ncontact sensors that reduce scalability and patient comfort. This paper\npresents the first study to estimate respiratory effort directly from nocturnal\naudio, enabling physiological context to be recovered from sound alone. We\npropose a latent-space fusion framework that integrates the estimated effort\nembeddings with acoustic features for OSA detection. Using a dataset of 157\nnights from 103 participants recorded in home environments, our respiratory\neffort estimator achieves a concordance correlation coefficient of 0.48,\ncapturing meaningful respiratory dynamics. Fusing effort and audio improves\nsensitivity and AUC over audio-only baselines, especially at low\napnoea-hypopnoea index thresholds. The proposed approach requires only\nsmartphone audio at test time, which enables sensor-free, scalable, and\nlongitudinal OSA monitoring."}
{"id": "2509.14789", "categories": ["eess.AS", "cs.CR", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14789", "abs": "https://arxiv.org/abs/2509.14789", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Acoustic Simulation Framework for Multi-channel Replay Speech Detection", "comment": "Submitted to ICASSP 2026", "summary": "Replay speech attacks pose a significant threat to voice-controlled systems,\nespecially in smart environments where voice assistants are widely deployed.\nWhile multi-channel audio offers spatial cues that can enhance replay detection\nrobustness, existing datasets and methods predominantly rely on single-channel\nrecordings. In this work, we introduce an acoustic simulation framework\ndesigned to simulate multi-channel replay speech configurations using publicly\navailable resources. Our setup models both genuine and spoofed speech across\nvaried environments, including realistic microphone and loudspeaker impulse\nresponses, room acoustics, and noise conditions. The framework employs measured\nloudspeaker directionalities during the replay attack to improve the realism of\nthe simulation. We define two spoofing settings, which simulate whether a\nreverberant or an anechoic speech is used in the replay scenario, and evaluate\nthe impact of omnidirectional and diffuse noise on detection performance. Using\nthe state-of-the-art M-ALRAD model for replay speech detection, we demonstrate\nthat synthetic data can support the generalization capabilities of the detector\nacross unseen enclosures."}
{"id": "2509.15222", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.15222", "abs": "https://arxiv.org/abs/2509.15222", "authors": ["Junhyung Park", "Yonghyun Kim", "Joonhyung Bae", "Kirak Kim", "Taegyun Kwon", "Alexander Lerch", "Juhan Nam"], "title": "Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation", "comment": "Accepted to the Late-Breaking Demo Session of the 26th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2025", "summary": "Piano performance is a multimodal activity that intrinsically combines\nphysical actions with the acoustic rendition. Despite growing research interest\nin analyzing the multimodal nature of piano performance, the laborious process\nof acquiring large-scale multimodal data remains a significant bottleneck,\nhindering further progress in this field. To overcome this barrier, we present\nan integrated web toolkit comprising two graphical user interfaces (GUIs): (i)\nPiaRec, which supports the synchronized acquisition of audio, video, MIDI, and\nperformance metadata. (ii) ASDF, which enables the efficient annotation of\nperformer fingering from the visual data. Collectively, this system can\nstreamline the acquisition of multimodal piano performance datasets."}
{"id": "2509.14934", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14934", "abs": "https://arxiv.org/abs/2509.14934", "authors": ["Francisco Messina", "Francesca Ronchini", "Luca Comanducci", "Paolo Bestagini", "Fabio Antonacci"], "title": "Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance", "comment": null, "summary": "A persistent challenge in generative audio models is data replication, where\nthe model unintentionally generates parts of its training data during\ninference. In this work, we address this issue in text-to-audio diffusion\nmodels by exploring the use of anti-memorization strategies. We adopt\nAnti-Memorization Guidance (AMG), a technique that modifies the sampling\nprocess of pre-trained diffusion models to discourage memorization. Our study\nexplores three types of guidance within AMG, each designed to reduce\nreplication while preserving generation quality. We use Stable Audio Open as\nour backbone, leveraging its fully open-source architecture and training\ndataset. Our comprehensive experimental analysis suggests that AMG\nsignificantly mitigates memorization in diffusion-based text-to-audio\ngeneration without compromising audio fidelity or semantic alignment."}
{"id": "2509.15085", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15085", "abs": "https://arxiv.org/abs/2509.15085", "authors": ["Simon Welker", "Tal Peer", "Timo Gerkmann"], "title": "Real-Time Streaming Mel Vocoding with Generative Flow Matching", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram\nto an audio waveform, is still a key component in many text-to-speech (TTS)\nsystems today. Based on generative flow matching, our prior work on generative\nSTFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel\nfilterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for\nspeech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total\nlatency of 48 ms. We show real-time streaming capability at this latency not\nonly in theory, but in practice on a consumer laptop GPU. Furthermore, we show\nthat our model achieves substantially better PESQ and SI-SDR values compared to\nwell-established not streaming-capable baselines for Mel vocoding including\nHiFi-GAN."}

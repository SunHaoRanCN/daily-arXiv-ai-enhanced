<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [eess.AS](#eess.AS) [Total: 19]
- [cs.SD](#cs.SD) [Total: 18]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [In Planta Tattoo and Kirigami Sensors for Self-Powered Monitoring of Vapor Pressure Deficit and Growth Dynamics](https://arxiv.org/abs/2509.14240)
*Nafize Ishtiaque Hossain,Kundan Saha,Atul Sharma,Sameer Sonkusale*

Main category: eess.SP

TL;DR: 开发了一种可扩展的自供电植物传感器平台，用于连续监测植物水分和生长状况，包括叶片纹身传感器和茎干应变传感器，能够能量自主运行超过10-20天。


<details>
  <summary>Details</summary>
Motivation: 传统植物监测方法通常需要外部电源且难以实现长期连续监测，需要开发自供电、可扩展的传感器系统来改善农业作物管理和应对非生物胁迫。

Method: 集成两种传感器：1)叶片纹身传感器使用五氧化二钒纳米片膜同时测量温湿度并收集环境水分发电；2)受剪纸启发的应变传感器包裹茎干测量径向生长。采用无洁净室、卷对卷兼容的制造方法。

Result: 系统实现能量自主运行，功率密度达0.1114μW/cm²。湿度温度传感器灵敏度高，可准确估算蒸汽压差超过10天；应变传感器测量因子1.5，抗干扰能力强，可连续跟踪生长超过20天。

Conclusion: 该自供电传感器平台具有大规模农业部署潜力，能够有效监测植物水分状况和生长动态，为作物管理和非生物胁迫监测提供可持续的解决方案。

Abstract: We report a scalable, self-powered in planta sensor platform for continuous
monitoring of plant hydration and growth. The system integrates two components
a leaf mounted tattoo sensor for estimating vapor pressure deficit and a
kirigami inspired strain sensor for tracking radial stem growth. Uniquely, the
tattoo sensor serves a dual function measuring temperature and humidity beneath
the leaf surface while simultaneously harvesting power from ambient moisture
via a vanadium pentoxide nanosheet membrane. This moist-electric generator
configuration enables energy-autonomous operation, delivering a power density
of 0.1114 miroW per square cm. The V2O5 based sensor exhibits high sensitivity
to humidity and temperature, enabling accurate VPD estimation for over 10 days
until leaf senescence. The eutectogel based kirigami strain sensor, wrapped
around the stem, offers a gauge factor of 1.5 and immunity to unrelated
mechanical disturbances, allowing continuous growth tracking for more than 20
days. Both sensors are fabricated via cleanroom-free, roll to roll compatible
methods, underscoring their potential for large-scale agricultural deployment
to monitor abiotic stress and improve crop management.

</details>


### [2] [Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes](https://arxiv.org/abs/2509.14242)
*Jinshuai Gu,Zenghui Lin,Jingying Ma,Jingyu Wang,Linyan Zhang,Rui Bai,Zelin Tu,Youyou Jiang,Donglin Xie,Yuxi Zhou,Guoli Liu,Shenda Hong*

Main category: eess.SP

TL;DR: 基于彩色多婴勒测讥数据的AI模型CTGage可预测胎儿生物学年龄，并通过年龄差值预测不良孕产结局风险


<details>
  <summary>Details</summary>
Motivation: 彩色多婴勒测讥(CTG)目前主要用于识别胎儿当前状态，其在预测未来不良孕产结局方面的潜力未得到充分挖掘

Method: 使用61,140条CTG记录开发了基于1D卷积神经网络的CTGage模型，通过年龄差值(CTGage-gap)将人群分为五组并比较不良结局发生率

Result: 模型平均绝对误差10.91天，高估组早产儿发生率5.33% vs 正常组1.42%，孕期糖尿病31.93% vs 20.86%；低估组低体重儿0.17% vs 0.15%，贫血37.51% vs 34.74%

Conclusion: AI模型CTGage可作为一种新型、非侵入性、易获取的数字生物标记物预测不良孕产结局风险

Abstract: Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment
technique used globally, especially in underdeveloped countries. However, it is
currently mainly used to identify the fetus's current status (e.g., fetal
acidosis or hypoxia), and the potential of CTG in predicting future adverse
pregnancy outcomes has not been fully explored. We aim to develop an AI-based
model that predicts biological age from CTG time series (named CTGage), then
calculate the age gap between CTGage and actual age (named CTGage-gap), and use
this gap as a new digital biomarker for future adverse pregnancy outcomes. The
CTGage model is developed using 61,140 records from 11,385 pregnant women,
collected at Peking University People's Hospital between 2018 and 2022. For
model training, a structurally designed 1D convolutional neural network is
used, incorporating distribution-aligned augmented regression technology. The
CTGage-gap is categorized into five groups: < -21 days (underestimation group),
-21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days
(overestimation group). We further defined the underestimation group and
overestimation group together as the high-risk group. We then compare the
incidence of adverse outcomes and maternal diseases across these groups. The
average absolute error of the CTGage model is 10.91 days. When comparing the
overestimation group with the normal group, premature infants incidence is
5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is
31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the
normal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and
anaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial
intelligence-derived CTGage can predict the future risk of adverse pregnancy
outcomes and hold potential as a novel, non-invasive, and easily accessible
digital biomarker.

</details>


### [3] [InWaveSR: Topography-Aware Super-Resolution Network for Internal Solitary Waves](https://arxiv.org/abs/2509.14243)
*Xinjie Wang,Zhongrui Li,Peng Han,Chunxin Yuan,Jiexin Xu,Zhiqiang Wei,Jie Nie*

Main category: eess.SP

TL;DR: 提出InWaveSR模型，通过深度学习框架结合物理约束，从低分辨率数据生成高分辨率内孤立波数据，在PSNR指标上达到36.2，优于传统方法


<details>
  <summary>Details</summary>
Motivation: 观测数据分辨率不足阻碍有效利用，需要开发能够生成高分辨率数据的方法，特别是针对内孤立波数据

Method: 基于深度学习框架，使用原始Navier-Stokes方程作为物理约束，结合注意力机制和快速傅里叶变换的HF-ResBlock组件，采用边缘采样和数值预处理优化训练

Result: 在实地观测ISW数据评估中，PSNR达到36.2，优于传统插值方法和先前神经网络

Conclusion: InWaveSR模型在内孤立波高分辨率重建方面表现出优异性能和可靠性，显著优于传统方法

Abstract: The effective utilization of observational data is frequently hindered by
insufficient resolution. To address this problem, we present a new
spatio-temporal super-resolution (STSR) model, called InWaveSR. It is built on
a deep learning framework with physical restrictions and can efficiently
generate high-resolution data from low-resolution input, especially for data
featuring internal solitary waves (ISWs). To increase generality and
interpretation, the model InWaveSR uses the primitive Navier-Stokes equations
as the constraint, ensuring that the output results are physically consistent.
In addition, the proposed model incorporates an HF-ResBlock component that
combines the attention mechanism and the Fast Fourier Transform (FFT) method to
improve the performance of the model in capturing high-frequency
characteristics. Simultaneously, in order to enhance the adaptability of the
model to complicated bottom topography, an edge sampling and numerical
pre-processing method are carried out to optimize the training process. On
evaluations using the in-situ observational ISW data, the proposed InWaveSR
achieved a peak signal-to-noise ratio (PSNR) score of 36.2, higher than those
of the traditional interpolation method and the previous neural network. This
highlights its significant superiority over traditional methods, demonstrating
its excellent performance and reliability in high-resolution ISW
reconstruction.

</details>


### [4] [Conditional Nearest Level Modulation for Improved Switching Dynamics in Asymmetric Multilevel Converters](https://arxiv.org/abs/2509.14402)
*Jinshui Zhang,Angel V Peterchev,Stefan M Goetz*

Main category: eess.SP

TL;DR: 方法通过条件性最近层调制（cNLM）简化不对称多级转换器的调制，使用数学惩罚模型来控制开关动态和改善输出质量。


<details>
  <summary>Details</summary>
Motivation: 不对称多级转换器在清洁能源、电动汽车等领域有应用潜力，但需要很多模块来实现精细的输出约束。最近层调制（NLM）在这种设计中很简单，但输出层数过多会导致开关频率过高和输出电压尖筹。

Method: 提出条件性最近层调制（cNLM），结合数学惩罚模型来调节开关动态。还提出了适用于特定功能的cNLM变体，如强制最小开关间隔。

Result: 在不对称多级原型上的实验验证显示，cNLM将总输出失真从66.3%降低到15.1%，并将开关频率降低到原始NLM的8%。

Conclusion: cNLM能够有效控制不对称多级转换器的开关动态，显著提升输出质量并降低开关损耗，为该类转换器的应用提供了更有效的调制方案。

Abstract: Modular multilevel converters have promising applications in clean energy,
electric vehicles, and biomedical instrumentation, but need many modules to
achieve fine output granularity, particularly of the voltage. Asymmetric
multilevel circuits introduce differences in module voltages so that the
quantity of output levels grows exponentially with the number of modules.
Nearest-level modulation (NLM) is preferred over carrier-based methods in
asymmetric circuits for its simplicity. However, the large number of output
levels can overwhelm NLM and cause excessive transistor switching on some
modules and output voltage spikes. We propose a conditional nearest-level
modulation (cNLM) by incorporating mathematical penalty models to regulate
switching dynamics. This approach improves output quality and reduces switching
rates. Additionally, we present cNLM variations tailored for specific
functions, such as enforcing a minimum switching interval. Experimental
validation on an asymmetric multilevel prototype demonstrates that cNLM reduces
the total output distortion from 66.3% to 15.1% while cutting the switching
rate to just 8% of the original NLM.

</details>


### [5] [Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography](https://arxiv.org/abs/2509.14442)
*Arjun Teh,Wael H. Ali,Joshua Rapp,Hassan Mansour*

Main category: eess.SP

TL;DR: 基于单视点BOS测量和物理信息重建的非侵入式室内流场估计框架


<details>
  <summary>Details</summary>
Motivation: 解决单视点BOS断层扫描问题的严重不适定性，实现从单个视角准确估计室内空气流场

Method: 经过改进的光线追踪、物理基础的光线渲染方法和损失构造，以及使用PINN进行物理正则化确保重建流场符合浮力驱动流的控制方程

Result: 开发了能够从单个视角准确重建室内空气流场的框架

Conclusion: 该框架通过结合光学测量和物理信息重建技术，有效解决了单视点BOS断层扫描的不适定性问题

Abstract: We develop a framework for non-invasive volumetric indoor airflow estimation
from a single viewpoint using background-oriented schlieren (BOS) measurements
and physics-informed reconstruction. Our framework utilizes a light projector
that projects a pattern onto a target back-wall and a camera that observes
small distortions in the light pattern. While the single-view BOS tomography
problem is severely ill-posed, our proposed framework addresses this using: (1)
improved ray tracing, (2) a physics-based light rendering approach and loss
formulation, and (3) a physics-based regularization using a physics-informed
neural network (PINN) to ensure that the reconstructed airflow is consistent
with the governing equations for buoyancy-driven flows.

</details>


### [6] [Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces](https://arxiv.org/abs/2509.14447)
*Sriram V. C. Nallani,Gautham Ramachandran,Sahil S. Shah*

Main category: eess.SP

TL;DR: 基于双时间规模资格追踪的三因子学习规则的在线SNN解码器，避免BPTT的同时保持竞争性能，实现O(1)内存消耗和更快收敛


<details>
  <summary>Details</summary>
Motivation: 解决脑机接口中神经信号不稳定性和实时嵌入式应用的内存约束问题

Method: 采用局部三因子学习规则，结合错误调制Hebbian更新、快/慢追踪整合和自适应学习率控制

Result: 在两个玛美神经数据集上达到相似解码精度（Pearson R≥0.63和R≥0.81），内存减少28-35%，收敛更快

Conclusion: 该方法能够实现内存高效、持续自适应的神经解码，适合资源受限的嵌入式BCI系统

Abstract: Brain-Computer Interfaces face challenges from neural signal instability and
memory constraints for real-time implantable applications. We introduce an
online SNN decoder using local three-factor learning rules with dual-timescale
eligibility traces that avoid backpropagation through time while maintaining
competitive performance. Our approach combines error-modulated Hebbian updates,
fast/slow trace consolidation, and adaptive learning rate control, requiring
only O(1) memory versus O(T) for BPTT methods. Evaluations on two primate
datasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R
\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence than
BPTT-trained SNNs. Closed-loop simulations with synthetic neural populations
demonstrate adaptation to neural disruptions and learning from scratch without
offline calibration. This work enables memory-efficient, continuously adaptive
neural decoding suitable for resource-constrained implantable BCI systems.

</details>


### [7] [Secure Blind Graph Signal Recovery and Adversary Detection Using Smoothness Maximization](https://arxiv.org/abs/2509.14449)
*Mahdi Shamsi,Hadi Zayyani,Hasan Abu Hilal,Mohammad Salman*

Main category: eess.SP

TL;DR: 提出一种安全盲图形信号恢复算法，能够在存在测量噪声和恶意节点注入假数据的情况下检测敌对节点并恢复图形信号


<details>
  <summary>Details</summary>
Motivation: 解决图形信号恢复中的安全问题，当一些未知敌对节点注入假数据时，需要一种能够在不先知敌对节点数量和位置的情况下进行安全恢复的算法

Method: 使用基于差分平滑性的统计量来检测敌对节点，通过比较当前观测平滑性与排除对应节点后的平均平滑性的差异。检测后采用平滑性最大化的变体通过Dinkelbach算法求解分数优化问题来进行图形信号恢复

Result: 模拟结果显示，该方法在信号恢复方面比中位数GSR算法和其他竞争方法有显著改善

Conclusion: 该算法提供了一种有效且计算复杂度低的安全盲图形信号恢复方案，能够在存在恶意节点攻击的情况下进行可靠的信号恢复

Abstract: In this letter, we propose a secure blind Graph Signal Recovery (GSR)
algorithm that can detect adversary nodes. Some unknown adversaries are assumed
to be injecting false data at their respective nodes in the graph. The number
and location of adversaries are not known in advance and the goal is to recover
the graph signal in the presence of measurement noise and False Data Injection
(FDI) caused by the adversaries. Consequently, the proposed algorithm would be
a perfect candidate to solve this challenging problem. Moreover, due to the
presence of malicious nodes, the proposed method serves as a secure GSR
algorithm. For adversary detection, a statistical measure based on differential
smoothness is used. Specifically, the difference between the current observed
smoothness and the average smoothness excluding the corresponding node. This
genuine statistical approach leads to an effective and low-complexity adversary
detector. In addition, following malicious node detection, the GSR is performed
using a variant of smoothness maximization, which is solved efficiently as a
fractional optimization problem using a Dinkelbach's algorithm. Analysis of the
detector, which determines the optimum threshold of the detector is also
presented. Simulation results show a significant improvement of the proposed
method in signal recovery compared to the median GSR algorithm and other
competing methods.

</details>


### [8] [Age of Information Aided Intelligent Grant-Free Massive Access for Heterogeneous mMTC Traffic](https://arxiv.org/abs/2509.14503)
*Zhongwen Sun,Wei Chen,Yuxuan Sun,Bo Ai*

Main category: eess.SP

TL;DR: 本文研究非正交免授权随机接入场景下的异构交通流兼容问题，通过基于信息龄的自动编码器方案同时优化告警设备检测率和监控设备信息时效性。


<details>
  <summary>Details</summary>
Motivation: 现有的免授权随机接入研究主要关注用户检测和数据恢复的准确性，而忽视了交通流的异质性。本文考虑了告警设备和监控设备两种不同类型交通流兼容的问题。

Method: 首先分析基于信息龄的随机接入方案并优化接入参数以最小化MDs的平均AoI。然后设计了基于信息龄的先验信息辅助自动编码器(A-PIAAE)，以联合检测活跃设备，并使用学习导频来减少非正交导频之间的干扰。在解码器中提出了利用MDs的AoI作为先验信息的基于信息龄的学习迭代收缩阈值算法(LISTA-AGE)。

Result: 理论分析证明了所提出的A-PIAAE具有更好的收敛性能。实验结果显示该方法在降住MDs的平均AoI和提高ADs的成功检测率方面具有优势。

Conclusion: 本文提出的方法能够有效处理异构交通流兼容问题，同时实现了对告警设备高检测成功率和监控设备高信息时效性的双重优化目标。

Abstract: With the arrival of 6G, the Internet of Things (IoT) traffic is becoming more
and more complex and diverse. To meet the diverse service requirements of IoT
devices, massive machine-type communications (mMTC) becomes a typical scenario,
and more recently, grant-free random access (GF-RA) presents a promising
direction due to its low signaling overhead. However, existing GF-RA research
primarily focuses on improving the accuracy of user detection and data
recovery, without considering the heterogeneity of traffic. In this paper, we
investigate a non-orthogonal GF-RA scenario where two distinct types of traffic
coexist: event-triggered traffic with alarm devices (ADs), and status update
traffic with monitor devices (MDs). The goal is to simultaneously achieve high
detection success rates for ADs and high information timeliness for MDs. First,
we analyze the age-based random access scheme and optimize the access
parameters to minimize the average age of information (AoI) of MDs. Then, we
design an age-based prior information aided autoencoder (A-PIAAE) to jointly
detect active devices, together with learned pilots used in GF-RA to reduce
interference between non-orthogonal pilots. In the decoder, an Age-based
Learned Iterative Shrinkage Thresholding Algorithm (LISTA-AGE) utilizing the
AoI of MDs as the prior information is proposed to enhance active user
detection. Theoretical analysis is provided to demonstrate the proposed A-PIAAE
has better convergence performance. Experiments demonstrate the advantage of
the proposed method in reducing the average AoI of MDs and improving the
successful detection rate of ADs.

</details>


### [9] [Radiolunadiff: Estimation of wireless network signal strength in lunar terrain](https://arxiv.org/abs/2509.14559)
*Paolo Torrado,Anders Pearson,Jason Klein,Alexander Moscibroda,Joshua Smith*

Main category: eess.SP

TL;DR: 提出了一种新颖的物理信息深度学习架构，用于预测月球地形上的无线电地图，结合物理地形生成器和射线追踪引擎，采用三重UNet架构，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够准确预测月球地形上无线电传播的高保真方法，以支持月球通信和导航系统的设计。

Method: 集成基于物理的月球地形生成器（使用NASA数据）和射线追踪引擎创建数据集，采用三重UNet架构（两个标准UNet加扩散网络）建模复杂传播效应。

Result: 实验结果表明，该方法在月球地形数据集上的多个评估指标均优于现有的深度学习方法。

Conclusion: 所提出的物理信息深度学习架构能够有效预测月球地形无线电地图，为月球通信系统提供了可靠的技术支持。

Abstract: In this paper, we propose a novel physics-informed deep learning architecture
for predicting radio maps over lunar terrain. Our approach integrates a
physics-based lunar terrain generator, which produces realistic topography
informed by publicly available NASA data, with a ray-tracing engine to create a
high-fidelity dataset of radio propagation scenarios. Building on this dataset,
we introduce a triplet-UNet architecture, consisting of two standard UNets and
a diffusion network, to model complex propagation effects. Experimental results
demonstrate that our method outperforms existing deep learning approaches on
our terrain dataset across various metrics.

</details>


### [10] [Task-Oriented Learning for Automatic EEG Denoising](https://arxiv.org/abs/2509.14665)
*Tian-Yu Xiang,Zheng Lei,Xiao-Hu Zhou,Xiao-Liang Xie,Shi-Qi Liu,Mei-Jiang Gui,Hong-Yun Ou,Xin-Zheng Huang,Xin-Yi Fu,Zeng-Guang Hou*

Main category: eess.SP

TL;DR: 提出基于任务导向学习的EEG自动去噪框架，无需干净参考信号，仅使用任务标签进行监督，在多个数据集上实现任务性能和信号质量的同步提升


<details>
  <summary>Details</summary>
Motivation: 传统EEG去噪方法依赖人工干预或干净参考信号，限制了实际应用。需要开发仅使用任务标签的自动去噪方法

Method: 采用盲源分离技术分解EEG信号，学习选择器为每个成分分配保留概率，通过下游代理任务模型评估重建信号，仅使用任务损失进行协作优化

Result: 在三个数据集、两种范式和多种噪声条件下，任务准确率提升2.56%，信噪比提升0.82dB，框架对算法具有普适性

Conclusion: 该任务导向学习框架是实用的EEG去噪解决方案，对神经科学研究和基于EEG的交互系统具有潜在影响

Abstract: Electroencephalography (EEG) denoising methods typically depend on manual
intervention or clean reference signals. This work introduces a task-oriented
learning framework for automatic EEG denoising that uses only task labels
without clean reference signals. EEG recordings are first decomposed into
components based on blind source separation (BSS) techniques. Then, a
learning-based selector assigns a retention probability to each component, and
the denoised signal is reconstructed as a probability-weighted combination. A
downstream proxy-task model evaluates the reconstructed signal, with its task
loss supervising the selector in a collaborative optimization scheme that
relies solely on task labels, eliminating the need for clean EEG references.
Experiments on three datasets spanning two paradigms and multiple noise
conditions show consistent gains in both task performance (accuracy:
$2.56\%\uparrow$) and standard signal-quality metrics (signal-to-noise-ratio:
$0.82$\,dB\,$\uparrow$). Further analyses demonstrate that the task-oriented
learning framework is algorithm-agnostic, as it accommodates diverse
decomposition techniques and network backbones for both the selector and the
proxy model. These promising results indicate that the proposed task-oriented
learning framework is a practical EEG denoising solution with potential
implications for neuroscience research and EEG-based interaction systems.

</details>


### [11] [Mitigating the Impact of Location Uncertainty on Radio Map-Based Predictive Rate Selection via Noisy-Input Gaussian Process](https://arxiv.org/abs/2509.14710)
*Koya Sato*

Main category: eess.SP

TL;DR: 基于噪声输入高斯过程(NIGP)的新题建模方法，通过泰勒近似处理位置不确定性，提高无线通信系统的传输速率选择可靠性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图技术假设位置信息完美，实际上位置系统存在定位误差，这种位置不确定性会降低无线系统的可靠性。

Method: 提出噪声输入高斯过程(NIGP)方法，将位置噪声视为额外的输出噪声，利用泰勒展开进行近似处理。

Result: 数值实验结果显示，NIGP基于设计比纯粹GP方法具有更高的传输速率选择可靠性，同时比路径损耗基于方法获得更高的吞吐量。

Conclusion: 该研究为6G网络中应对位置不确定性提供了有效的解决方案，通过NIGP方法显著提升了无线通信系统的性能和可靠性。

Abstract: This paper proposes a predictive rate-selection framework based on Gaussian
process (GP)-based radio map construction that is robust to location
uncertainty. Radio maps are a promising tool for improving communication
efficiency in 6G networks. Although they enable the design of location-based
maximum transmission rates by exploiting statistical channel information,
existing discussions often assume perfect (i.e., noiseless) location
information during channel sensing. Since such information must be obtained
from positioning systems such as global navigation satellite systems, it
inevitably involves positioning errors; this location uncertainty can degrade
the reliability of radio map-based wireless systems. To mitigate this issue, we
introduce the noisy-input GP (NIGP), which treats location noise as additional
output noise by applying a Taylor approximation of the function of interest.
Numerical results demonstrate that the proposed NIGP-based design achieves more
reliable transmission-rate selection than pure GP and yields higher throughput
than path loss-based rate selection.

</details>


### [12] [LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines](https://arxiv.org/abs/2509.14711)
*Ziwei Huang,Shiliang Lu,Lu Bai,Xuesong Cai,Xiang Cheng*

Main category: eess.SP

TL;DR: 基于机器联觉(SoM)的大语言模型多路径生成方案(LLM4MG)，利用LLaMA 3.2模型通过多模态感知数据生成6G V2I场景下的高精度多路径信息


<details>
  <summary>Details</summary>
Motivation: 为了解决第六代(6G)车辆与基础设施(V2I)场景下高精度多路径生成的挑战，将大语言模型首次应用于多路径生成任务

Method: 构建SynthSoM-V2I多模态感知-通信数据集，基于LLaMA 3.2模型，通过特征提取融合网络对齐多模态特征空间，使用LoRA参数效率微调和传播感知提示工程实现知识转移

Result: 在LoS/NLoS分类上达到92.76%的准确率，多路径功率/延迟生成精度NMSE为0.099/0.032，在跨车辆流量密度、跨带宽和跨场景情况下都体现了良好的泛化性能

Conclusion: LLM4MG方案在多路径生成任务上显著超过传统深度学习方法，通过实际场景验证了其实用性，高精度多路径生成对系统设计具有重要价值

Abstract: Based on Synesthesia of Machines (SoM), a large language model (LLM) is
adapted for multipath generation (LLM4MG) for the first time. Considering a
typical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new
multi-modal sensing-communication dataset is constructed, named SynthSoM-V2I,
including channel multipath information, millimeter wave (mmWave) radar sensory
data, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based
on the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model
Meta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The
proposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic
space through feature extraction and fusion networks. To further achieve
general knowledge transfer from the pre-trained LLaMA for multipath generation
via multi-modal sensory data, the low-rank adaptation (LoRA)
parameter-efficient fine-tuning and propagation-aware prompt engineering are
exploited. Simulation results demonstrate that the proposed LLM4MG outperforms
conventional deep learning-based methods in terms of line-of-sight
(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath
power/delay generation precision with normalized mean square error (NMSE) of
0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and
cross-scenario generalization. The utility of the proposed LLM4MG is validated
by real-world generalization. The necessity of high-precision multipath
generation for system design is also demonstrated by channel capacity
comparison.

</details>


### [13] [Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding](https://arxiv.org/abs/2509.14764)
*Yuanyuan Yao,Simon Geirnaert,Tinne Tuytelaars,Alexander Bertrand*

Main category: eess.SP

TL;DR: 这篇论文提出了三种计算效率更高的无监督听视注意力解码算法，解决了现有无偏见方法计算复杂度过高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督听视注意力解码方法存在初始化偏见或计算复杂度过高的问题，需要开发更高效的算法。

Method: 提出了三种计算效率更高的无监督听解码算法，具有更低且固定的计算成本。

Result: 新算法达到了类似的性能水平，但计算复杂度显著降低。

Conclusion: 这些新算法为无监督听听视注意力解码提供了更实用的解决方案，有助于神经导向助听设备的开发。

Abstract: Decoding the attended speaker in a multi-speaker environment from
electroencephalography (EEG) has attracted growing interest in recent years,
with neuro-steered hearing devices as a driver application. Current approaches
typically rely on ground-truth labels of the attended speaker during training,
necessitating calibration sessions for each user and each EEG set-up to achieve
optimal performance. While unsupervised self-adaptive auditory attention
decoding (AAD) for stimulus reconstruction has been developed to eliminate the
need for labeled data, it suffers from an initialization bias that can
compromise performance. Although an unbiased variant has been proposed to
address this limitation, it introduces substantial computational complexity
that scales with data size. This paper presents three computationally efficient
alternatives that achieve comparable performance, but with a significantly
lower and constant computational cost. The code for the proposed algorithms is
available at https://github.com/YYao-42/Unsupervised_AAD.

</details>


### [14] [Comparative Performance Analysis of Different Hybrid NOMA Schemes](https://arxiv.org/abs/2509.14809)
*Ning Wang,Chenyu Zhang,Yanshi Sun,Minghui Min,Shiyin Li*

Main category: eess.SP

TL;DR: 本文分析了三种混合非正交多址(H-NOMA)方案在随机信道增益排序下的性能，包括固定顺序SIC、非功率自适应混合SIC和功率自适应混合SIC方案，推导了闭式表达式并验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 现有H-NOMA分析通常假设固定信道增益顺序，但实际信道系数是随机分布的，其大小关系具有随机性和时变性，需要研究随机信道排序下的性能。

Method: 理论分析推导了三种H-NOMA方案相对于传统OMA性能较差的概率闭式表达式，并开发了高信噪比区域的渐近结果，通过仿真验证分析。

Result: 仿真结果验证了理论分析的正确性，展示了不同信噪比场景下H-NOMA方案的性能表现。

Conclusion: 研究为下一代无线系统中H-NOMA的部署提供了理论基础，证明了在随机信道条件下不同H-NOMA方案的性能特征。

Abstract: Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages
of pure NOMA and conventional OMA organically, has emerged as a highly
promising multiple access technology for future wireless networks. Recent
studies have proposed various H-NOMA systems by employing different successive
interference cancellation (SIC) methods for the NOMA transmission phase.
However, existing analyses typically assume a fixed channel gain order between
paired users, despite the fact that channel coefficients follow random
distribution, leading to their magnitude relationships inherently stochastic
and time varying. This paper analyzes the performance of three H-NOMA schemes
under stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA
scheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;
c) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical
analysis derives closed-form expressions for the probability that H-NOMA
schemes underperform conventional OMA. Asymptotic results in the high
signal-to-noise ratio (SNR) regime are also developed. Simulation results
validate our analysis and demonstrate the performance of H-NOMA schemes across
different SNR scenarios, providing a theoretical foundation for the deployment
of H-NOMA in next-generation wireless systems.

</details>


### [15] [Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization](https://arxiv.org/abs/2509.14836)
*Keitaro Yamashita,Kazuki Naganuma,Shunsuke Ono*

Main category: eess.SP

TL;DR: 提出一种基于广义采样理论的图信号顶点灵活采样方法，通过优化设计采样算子，能够控制活跃顶点数量并整合先验知识（必须包含或排除的顶点），使用DC优化和双近端梯度算法求解，在多种图信号模型上展现优越的恢复精度。


<details>
  <summary>Details</summary>
Motivation: 现有顶点灵活采样方法虽然能控制活跃顶点数量，但无法整合先验知识（如必须采样或禁止采样的特定顶点），限制了实际应用中的灵活性。

Method: 将采样算子设计转化为带约束的优化问题，使用核范数和DC惩罚处理顶点选择，转化为DC优化问题，并开发基于双近端梯度DC算法的收敛求解器。

Result: 在多种图信号模型（包括真实世界数据）上的实验表明，该方法在恢复精度方面优于现有方法。

Conclusion: 所提出的方法能够有效整合先验知识并控制采样顶点数量，在保证理论最优恢复性能的同时，显著提升了图信号采样的灵活性和实用性。

Abstract: This paper proposes a method for vertex-wise flexible sampling of a broad
class of graph signals, designed to attain the best possible recovery based on
the generalized sampling theory. This is achieved by designing a sampling
operator by an optimization problem, which is inherently non-convex, as the
best possible recovery imposes a rank constraint. An existing method for
vertex-wise flexible sampling is able to control the number of active vertices
but cannot incorporate prior knowledge of mandatory or forbidden vertices. To
address these challenges, we formulate the operator design as a problem that
handles a constraint of the number of active vertices and prior knowledge on
specific vertices for sampling, mandatory inclusion or exclusion. We
transformed this constrained problem into a difference-of-convex (DC)
optimization problem by using the nuclear norm and a DC penalty for vertex
selection. To solve this, we develop a convergent solver based on the general
double-proximal gradient DC algorithm. The effectiveness of our method is
demonstrated through experiments on various graph signal models, including
real-world data, showing superior performance in the recovery accuracy by
comparing to existing methods.

</details>


### [16] [Hybrid Table-Assisted and RL-Based Dynamic Routing for NGSO Satellite Networks](https://arxiv.org/abs/2509.14909)
*Flor Ortiz,Eva Lagunas*

Main category: eess.SP

TL;DR: 这篇论文提出了一种混合动态路由策略，结合预计算路由表和深度Q学习备用机制，在NGSO卫星组网中实现更高的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决纯助图学习路由方案在卫星网络中存在的高复杂度、长收敛时间和重载情况下性能不稳定的问题。

Method: 提出混合路由框架，在正常条件下使用确定性路由表查找，仅在链路不可用或塞塞时启动深度Q学习备用机制。

Result: 在大规模NGSO网络中，混合方案在数据包交付率、端到端延迟、平均跳数和吞吐量方面都显著优于纯助图学习基线方案。

Conclusion: 混合路由策略是一种可扩展、具有弹性的解决方案，适用于对延迟敏感的卫星广播服务。

Abstract: This letter investigates dynamic routing in Next-Generation Satellite Orbit
(NGSO) constellations and proposes a hybrid strategy that combines precomputed
routing tables with a Deep Q-Learning (DQL) fallback mechanism. While fully
RL-based schemes offer adaptability to topology dynamics, they often suffer
from high complexity, long convergence times, and unstable performance under
heavy traffic. In contrast, the proposed framework exploits deterministic table
lookups under nominal conditions and selectively activates the DQL agent only
when links become unavailable or congested. Simulation results in large-scale
NGSO networks show that the hybrid approach consistently achieves higher packet
delivery ratio, lower end-to-end delay, shorter average hop count, and improved
throughput compared to a pure RL baseline. These findings highlight the
effectiveness of hybrid routing as a scalable and resilient solution for
delay-sensitive satellite broadband services

</details>


### [17] [Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators](https://arxiv.org/abs/2509.15069)
*Deijany Rodriguez Linares,Oksana Moryakova,Håkan Johansson*

Main category: eess.SP

TL;DR: 通过涞漏累加器层联结构高效计算时间指数权重和，将乘法操作从K×N降到K+1次常数乘法


<details>
  <summary>Details</summary>
Motivation: 传统方法需要K×N次一般乘法或存储整个数据块，在大N时计算成本高、存储需求大

Method: 利用涞漏累加器的特性，通过层联结构消除数据存储需求，将乘法操作减少为K+1次常数乘法

Result: 实现了高效的实时计算，无需存储整个数据块，乘法操作量显著降低

Conclusion: 该方法特别适合在样本逐个处理系统中高效计算时间指数权重和，具有重要的实时应用价值

Abstract: This letter presents a novel approach for \mbox{efficiently} computing
time-index powered weighted sums of the form $\sum_{n=0}^{N-1} n^{K} v[n]$
using cascaded accumulators. Traditional direct computation requires
$K{\times}N$ general multiplications, which become prohibitive for large $N$,
while alternative strategies based on lookup tables or signal reversal require
storing entire data blocks. By exploiting accumulator properties, the proposed
method eliminates the need for such storage and reduces the multiplicative cost
to only $K{+}1$ constant multiplications, enabling efficient real-time
implementation. The approach is particularly useful when such sums need to be
efficiently computed in sample-by-sample processing systems.

</details>


### [18] [Doppler Radiance Field-Guided Antenna Selection for Improved Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition](https://arxiv.org/abs/2509.15129)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 提出基于多普勒辐射场(DoRF)的Wi-Fi手势识别新框架，通过多天线AP噪声抑制和天线选择，显著提升泛化能力


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI信号受AP时钟异步和环境噪声影响，现有预处理方法仍无法完全消除噪声和异常值，限制了基于Wi-Fi的人类活动识别性能

Method: 提出多天线AP框架，基于DoRF拟合误差抑制噪声并选择信息量最大的天线，利用多普勒速度投影的不一致性来识别噪声

Result: 在具有挑战性的小规模手势识别数据集上实验表明，该方法显著提高了泛化能力

Conclusion: 该DoRF引导的Wi-Fi HAR方法为鲁棒的实时感知部署铺平了道路

Abstract: With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard
for advanced sensing, interest in using Wi-Fi Channel State Information (CSI)
for remote sensing has surged. Recent findings indicate that learning a unified
three-dimensional motion representation through Doppler Radiance Fields (DoRFs)
derived from CSI significantly improves the generalization capabilities of
Wi-Fi-based human activity recognition (HAR). Despite this progress, CSI
signals remain affected by asynchronous access point (AP) clocks and additive
noise from environmental and hardware sources. Consequently, even with existing
preprocessing techniques, both the CSI data and Doppler velocity projections
used in DoRFs are still susceptible to noise and outliers, limiting HAR
performance. To address this challenge, we propose a novel framework for
multi-antenna APs to suppress noise and identify the most informative antennas
based on DoRF fitting errors, which capture inconsistencies among Doppler
velocity projections. Experimental results on a challenging small-scale hand
gesture recognition dataset demonstrate that the proposed DoRF-guided
Wi-Fi-based HAR approach significantly improves generalization capability,
paving the way for robust real-world sensing deployments.

</details>


### [19] [A Unified Distributed Algorithm for Hybrid Near-Far Field Activity Detection in Cell-Free Massive MIMO](https://arxiv.org/abs/2509.15162)
*Jingreng Lei,Yang Li,Ziyue Wang,Qingfeng Lin,Ya-Feng Liu,Yik-Chung Wu*

Main category: eess.SP

TL;DR: 本文针对大规模MIMO系统中混合近远场信道下的活动检测问题，提出了基于协方差的分布式算法，理论证明近场信道比例增加能提升检测性能，并通过仿真验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着接入点天线数量增加，瑞利距离扩展使得传统远场传播假设不再实用，需要处理混合近远场信道的活动检测挑战。

Method: 建立基于协方差的混合近远场信道统计特性模型，提出分布式算法：各接入点执行本地活动检测，仅向中央处理单元交换检测结果，降低计算复杂度和通信开销。

Result: 理论分析表明近场信道比例增加能提升检测性能，仿真结果验证了理论分析并证明所提方法相比现有方法具有优越性能。

Conclusion: 提出的分布式算法不仅具有收敛保证，而且能够统一处理单小区或小区自由系统中的近场或远场设备，为大规模机器类型通信提供了有效的活动检测解决方案。

Abstract: A great amount of endeavor has recently been devoted to activity detection
for massive machine-type communications in cell-free multiple-input
multiple-output (MIMO) systems. However, as the number of antennas at the
access points (APs) increases, the Rayleigh distance that separates the
near-field and far-field regions also expands, rendering the conventional
assumption of far-field propagation alone impractical. To address this
challenge, this paper establishes a covariance-based formulation that can
effectively capture the statistical property of hybrid near-far field channels.
Based on this formulation, we theoretically reveal that increasing the
proportion of near-field channels enhances the detection performance.
Furthermore, we propose a distributed algorithm, where each AP performs local
activity detection and only exchanges the detection results to the central
processing unit, thus significantly reducing the computational complexity and
the communication overhead. Not only with convergence guarantee, the proposed
algorithm is unified in the sense that it can handle single-cell or cell-free
systems with either near-field or far-field devices as special cases.
Simulation results validate the theoretical analyses and demonstrate the
superior performance of the proposed approach compared with existing methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [20] [SpeechOp: Inference-Time Task Composition for Generative Speech Processing](https://arxiv.org/abs/2509.14298)
*Justin Lovelace,Rithesh Kumar,Jiaqi Su,Ke Chen,Kilian Q Weinberger,Zeyu Jin*

Main category: eess.AS

TL;DR: SpeechOp是一个基于预训练TTS模型的多任务潜在扩散模型，可将TTS转换为通用语音处理器，通过隐式任务组合实现语音增强等任务，在内容保持方面达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决语音到语音处理任务（如增强）面临的数据限制问题，避免生成式方法扭曲语音内容和说话人身份

Method: 通过适配预训练TTS模型构建多任务潜在扩散模型，引入隐式任务组合（ITC）管道，利用ASR转录本指导增强过程

Result: 继承了自然语音的丰富理解，加速训练并提高S2S任务质量，同时提升核心TTS性能，在内容保持方面达到最先进水平

Conclusion: SpeechOp成功将TTS模型转化为通用语音处理器，通过ITC实现了web规模语音理解与生成能力的稳健结合

Abstract: While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild"
data to achieve remarkable success, speech-to-speech processing tasks like
enhancement face data limitations, which lead data-hungry generative approaches
to distort speech content and speaker identity. To bridge this gap, we present
SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS
models into a universal speech processor capable of performing a wide range of
speech tasks and composing them in novel ways at inference time. By adapting a
pre-trained TTS model, SpeechOp inherits a rich understanding of natural
speech, accelerating training and improving S2S task quality, while
simultaneously enhancing core TTS performance. Finally, we introduce Implicit
Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g.,
from Whisper) guide SpeechOp's enhancement via our principled inference-time
task composition. ITC achieves state-of-the-art content preservation by
robustly combining web-scale speech understanding with SpeechOp's generative
capabilities. Audio samples are available at
https://justinlovelace.github.io/projects/speechop

</details>


### [21] [Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior](https://arxiv.org/abs/2509.14379)
*Yochai Yemini,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya*

Main category: eess.AS

TL;DR: 提出一种生成式无监督单麦克风语音分离方法，通过直接建模纯净语音和结构化噪声分布，利用音频-视觉评分模型作为强生成先验，通过反向扩散过程从后验分布采样实现噪声分离。


<details>
  <summary>Details</summary>
Motivation: 解决在环境噪声存在下的单麦克风语音分离问题，传统方法需要大量带噪混合数据进行训练，而本文旨在通过直接建模噪声和语音分布来实现更有效的分离。

Method: 使用生成式无监督技术，直接建模纯净语音和结构化噪声组件，训练仅使用单独信号而非带噪混合。采用音频-视觉评分模型整合视觉线索作为强生成语音先验，通过反向扩散过程从后验分布采样进行语音分离。

Result: 实验结果表明该方法在挑战性声学环境中表现出有希望的性能，验证了直接噪声建模方法的有效性。

Conclusion: 通过直接建模噪声分布并结合视觉信息的生成式方法，能够有效实现单麦克风环境下的语音分离，为复杂声学环境中的语音处理提供了新思路。

Abstract: In this paper, we address the problem of single-microphone speech separation
in the presence of ambient noise. We propose a generative unsupervised
technique that directly models both clean speech and structured noise
components, training exclusively on these individual signals rather than noisy
mixtures. Our approach leverages an audio-visual score model that incorporates
visual cues to serve as a strong generative speech prior. By explicitly
modelling the noise distribution alongside the speech distribution, we enable
effective decomposition through the inverse problem paradigm. We perform speech
separation by sampling from the posterior distributions via a reverse diffusion
process, which directly estimates and removes the modelled noise component to
recover clean constituent signals. Experimental results demonstrate promising
performance, highlighting the effectiveness of our direct noise modelling
approach in challenging acoustic environments.

</details>


### [22] [Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses](https://arxiv.org/abs/2509.14430)
*Yufeng Yang,Yiteng Huang,Yong Xu,Li Wan,Suwon Shon,Yang Liu,Yifeng Fan,Zhaojun Yang,Olivier Siohan,Yue Liu,Ming Sun,Florian Metze*

Main category: eess.AS

TL;DR: 一种用于智能眼镜的多通道差分语音识别方法，通过维护中心设计和边话检测提升了语音识别的鲁棒性，在实际环境中字符错误率相对降低18.0%。


<details>
  <summary>Details</summary>
Motivation: 随着智能眼镜等可穿戴设备的普及，使用者语音识别变得越来越重要。但在实际环境中，周围的边话干扰仍然是主要挑战，可能导致下游任务的累积错误。

Method: 提出了一种新的多通道差分自动语音识别方法，采用不同前端的差分输入来提升识别鲁棒性，包括权重调整器、麦克风选择和轻量边话检测模型。

Result: 在模拟和实际数据集上的评估显示，该系统表现超过传统方法，字符错误率相对降低了18.0%。

Conclusion: 该研究提出的多通道差分ASR方法能够有效提升智能眼镜上的语音识别鲁棒性，对于应对实际环境中的边话干扰具有重要意义。

Abstract: With the growing adoption of wearable devices such as smart glasses for AI
assistants, wearer speech recognition (WSR) is becoming increasingly critical
to next-generation human-computer interfaces. However, in real environments,
interference from side-talk speech remains a significant challenge to WSR and
may cause accumulated errors for downstream tasks such as natural language
processing. In this work, we introduce a novel multi-channel differential
automatic speech recognition (ASR) method for robust WSR on smart glasses. The
proposed system takes differential inputs from different frontends that
complement each other to improve the robustness of WSR, including a beamformer,
microphone selection, and a lightweight side-talk detection model. Evaluations
on both simulated and real datasets demonstrate that the proposed system
outperforms the traditional approach, achieving up to an 18.0% relative
reduction in word error rate.

</details>


### [23] [Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation](https://arxiv.org/abs/2509.14632)
*Miseul Kim,Soo Jin Park,Kyungguen Byun,Hyeon-Kyeong Shin,Sunkuk Moon,Shuhua Zhang,Erik Visser*

Main category: eess.AS

TL;DR: 通过风格可控语音生成模型增强语音样本的语音和风格多样性，合并原始和生成音频的讲者嵌入表征，有效降低语音分离系统的误差率


<details>
  <summary>Details</summary>
Motivation: 解决讲者分离系统在高内部讲者变异性（如情感、健康、内容变化）时的性能问题，避免同一讲者的语音段落被错误分类为不同个体

Method: 使用风格可控语音生成模型，对每个分离后的语音段落生成增强的语音样本，然后合并原始和生成音频的讲者嵌入表征来提升系统稳健性

Result: 在模拟情感语音数据集和截断的AMI数据集上验证，误差率分别降低了49%和35%

Conclusion: 通过风格可控语音生成模型进行语音增强可以有效提升讲者分离系统在高内部讲者变异性场景下的性能

Abstract: Speaker diarization systems often struggle with high intrinsic intra-speaker
variability, such as shifts in emotion, health, or content. This can cause
segments from the same speaker to be misclassified as different individuals,
for example, when one raises their voice or speaks faster during conversation.
To address this, we propose a style-controllable speech generation model that
augments speech across diverse styles while preserving the target speaker's
identity. The proposed system starts with diarized segments from a conventional
diarizer. For each diarized segment, it generates augmented speech samples
enriched with phonetic and stylistic diversity. And then, speaker embeddings
from both the original and generated audio are blended to enhance the system's
robustness in grouping segments with high intrinsic intra-speaker variability.
We validate our approach on a simulated emotional speech dataset and the
truncated AMI dataset, demonstrating significant improvements, with error rate
reductions of 49% and 35% on each dataset, respectively.

</details>


### [24] [Enhancing Situational Awareness in Wearable Audio Devices Using a Lightweight Sound Event Localization and Detection System](https://arxiv.org/abs/2509.14650)
*Jun-Wei Yeow,Ee-Leng Tan,Santi Peksi,Zhen-Ting Ong,Woon-Seng Gan*

Main category: eess.AS

TL;DR: 提出环境智能框架，结合声学场景分类和声音事件定位检测，通过场景预测动态调整SELD网络灵敏度，提升可穿戴音频设备的情境感知能力


<details>
  <summary>Details</summary>
Motivation: 主动降噪耳机在提供舒适聆听体验的同时会屏蔽环境声音线索，带来安全隐患，需要增强设备的环境感知能力

Method: 使用轻量级ASC模型推断当前环境，然后基于场景预测动态调节SELD网络，使其能够检测和定位当前环境下最重要的声音

Result: 在模拟耳机数据上，ASC条件化的SELD系统相比传统基线表现出更好的空间智能

Conclusion: 这是创建智能可听设备的重要一步，能够提供关键环境信息，实现更安全、更具情境感知的聆听体验

Abstract: Wearable audio devices with active noise control (ANC) enhance listening
comfort but often at the expense of situational awareness. However, this
auditory isolation may mask crucial environmental cues, posing significant
safety risks. To address this, we propose an environmental intelligence
framework that combines Acoustic Scene Classification (ASC) with Sound Event
Localization and Detection (SELD). Our system first employs a lightweight ASC
model to infer the current environment. The scene prediction then dynamically
conditions a SELD network, tuning its sensitivity to detect and localize sounds
that are most salient to the current context. On simulated headphone data, the
proposed ASC-conditioned SELD system demonstrates improved spatial intelligence
over a conventional baseline. This work represents a crucial step towards
creating intelligent hearables that can deliver crucial environmental
information, fostering a safer and more context-aware listening experience.

</details>


### [25] [Aligning Audio Captions with Human Preferences](https://arxiv.org/abs/2509.14659)
*Kartik Hegde,Rehana Mahfuz,Yinyi Guo,Erik Visser*

Main category: eess.AS

TL;DR: 基于RLHF的音频描述框架，通过对比学习训练奖励模型，使用人类偏好数据微调模型，在无需真实标注的情况下生成更符合人类偏好的音频描述


<details>
  <summary>Details</summary>
Motivation: 解决目前音频描述系统依赖贵金的成对数据集训练，这些数据集缺乏人类真实偏好且成本高的问题

Method: 使用对比语言-音频预训练(CLAP)的奖励模型，通过人类标注的成对偏好数据训练，然后将奖励模型集成到强化学习框架中微调基线描述系统

Result: 在多个数据集上的人类评估显示，该方法生成的描述更受人类偏好，尤其在基线模型失败的情况下能提供正确自然的描述，性能可与使用真实标注的监督方法相比

Conclusion: 该框架能够有效对齐音频描述与人类偏好，并在真实场景中具有良好的可扩展性，为解决监督学习数据缺乏和偏好对齐问题提供了有效方案

Abstract: Current audio captioning systems rely heavily on supervised learning with
paired audio-caption datasets, which are expensive to curate and may not
reflect human preferences in real-world scenarios. To address this limitation,
we propose a preference-aligned audio captioning framework based on
Reinforcement Learning from Human Feedback (RLHF). To effectively capture
nuanced human preferences, we train a Contrastive Language-Audio Pretraining
(CLAP)-based reward model using human-labeled pairwise preference data. This
reward model is integrated into a reinforcement learning framework to fine-tune
any baseline captioning system without relying on ground-truth caption
annotations. Extensive human evaluations across multiple datasets show that our
method produces captions preferred over those from baseline models,
particularly in cases where the baseline models fail to provide correct and
natural captions. Furthermore, our framework achieves performance comparable to
supervised approaches with ground-truth data, demonstrating its effectiveness
in aligning audio captioning with human preferences and its scalability in
real-world scenarios.

</details>


### [26] [SpeechMLC: Speech Multi-label Classification](https://arxiv.org/abs/2509.14677)
*Miseul Kim,Seyun Um,Hyeonjin Cha,Hong-goo Kang*

Main category: eess.AS

TL;DR: 提出基于跨注意力机制的多标签语音风格分类框架，能够同时检测语音样本中的多种说话风格，解决了传统单标签分类的局限性


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注单一目标风格的识别，但实际应用中需要同时检测多种说话者特征，因此需要开发适用于广义人机交互应用的多标签分类框架

Method: 在Transformer解码器中集成跨注意力机制来提取与每个目标标签相关的显著特征，并采用基于语音生成模型的数据增强技术来缓解多标签语音数据集中的数据不平衡问题

Result: 通过在已见和未见语料库上的多目标评估验证了模型的有效性，并分析了人类标注一致性对模型性能的影响

Conclusion: 该框架能够有效捕获各种说话者特征，为广义人机交互应用提供了合适的解决方案，同时通过人类感知分析为分类准确性提供了深入见解

Abstract: In this paper, we propose a multi-label classification framework to detect
multiple speaking styles in a speech sample. Unlike previous studies that have
primarily focused on identifying a single target style, our framework
effectively captures various speaker characteristics within a unified
structure, making it suitable for generalized human-computer interaction
applications. The proposed framework integrates cross-attention mechanisms
within a transformer decoder to extract salient features associated with each
target label from the input speech. To mitigate the data imbalance inherent in
multi-label speech datasets, we employ a data augmentation technique based on a
speech generation model. We validate our model's effectiveness through multiple
objective evaluations on seen and unseen corpora. In addition, we provide an
analysis of the influence of human perception on classification accuracy by
considering the impact of human labeling agreement on model performance.

</details>


### [27] [DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis](https://arxiv.org/abs/2509.14684)
*Ye-Xin Lu,Yu Gu,Kun Wei,Hui-Peng Du,Yang Ai,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: DAIEN-TTS是一个零样本文本转语音框架，通过解耦音频填充实现环境感知合成，可独立控制音色和背景环境。


<details>
  <summary>Details</summary>
Motivation: 现有的TTS系统通常无法独立控制语音的音色和背景环境，限制了语音合成的灵活性和真实感。

Method: 基于F5-TTS构建，使用预训练语音环境分离模块解耦环境语音，应用随机跨度掩码，采用双无分类引导和信噪比适配策略。

Result: 实验结果表明DAIEN-TTS能够生成具有高自然度、强说话人相似度和高环境保真度的环境个性化语音。

Conclusion: DAIEN-TTS框架成功实现了环境感知的零样本语音合成，为语音合成提供了更好的环境控制能力。

Abstract: This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework
that enables ENvironment-aware synthesis through Disentangled Audio Infilling.
By leveraging separate speaker and environment prompts, DAIEN-TTS allows
independent control over the timbre and the background environment of the
synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first
incorporates a pretrained speech-environment separation (SES) module to
disentangle the environmental speech into mel-spectrograms of clean speech and
environment audio. Two random span masks of varying lengths are then applied to
both mel-spectrograms, which, together with the text embedding, serve as
conditions for infilling the masked environmental mel-spectrogram, enabling the
simultaneous continuation of personalized speech and time-varying environmental
audio. To further enhance controllability during inference, we adopt dual
class-free guidance (DCFG) for the speech and environment components and
introduce a signal-to-noise ratio (SNR) adaptation strategy to align the
synthesized speech with the environment prompt. Experimental results
demonstrate that DAIEN-TTS generates environmental personalized speech with
high naturalness, strong speaker similarity, and high environmental fidelity.

</details>


### [28] [MELA-TTS: Joint transformer-diffusion model with representation alignment for speech synthesis](https://arxiv.org/abs/2509.14784)
*Keyu An,Zhiyu Zhang,Changfeng Gao,Yabin Li,Zhendong Peng,Haoxu Wang,Zhihao Du,Han Zhao,Zhifu Gao,Xiangang Li*

Main category: eess.AS

TL;DR: MELA-TTS是一个新颖的联合transformer-diffusion框架，用于端到端文本转语音合成，无需语音标记化和多阶段处理流程，通过表示对齐模块提升训练效率和跨模态一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统TTS系统中需要语音标记化和多阶段处理的问题，以及连续特征建模的困难，旨在实现更高效的端到端语音合成。

Method: 使用自回归方式从语言和说话人条件生成连续mel频谱图帧，提出表示对齐模块，在训练时将transformer解码器的输出表示与预训练ASR编码器的语义嵌入对齐。

Result: 在多个评估指标上达到最先进性能，保持强大的零样本语音克隆能力，支持离线和流式合成模式。

Conclusion: 为TTS中的连续特征生成方法设立了新基准，提供了离散标记范式的有力替代方案。

Abstract: This work introduces MELA-TTS, a novel joint transformer-diffusion framework
for end-to-end text-to-speech synthesis. By autoregressively generating
continuous mel-spectrogram frames from linguistic and speaker conditions, our
architecture eliminates the need for speech tokenization and multi-stage
processing pipelines. To address the inherent difficulties of modeling
continuous features, we propose a representation alignment module that aligns
output representations of the transformer decoder with semantic embeddings from
a pretrained ASR encoder during training. This mechanism not only speeds up
training convergence, but also enhances cross-modal coherence between the
textual and acoustic domains. Comprehensive experiments demonstrate that
MELA-TTS achieves state-of-the-art performance across multiple evaluation
metrics while maintaining robust zero-shot voice cloning capabilities, in both
offline and streaming synthesis modes. Our results establish a new benchmark
for continuous feature generation approaches in TTS, offering a compelling
alternative to discrete-token-based paradigms.

</details>


### [29] [Acoustic Simulation Framework for Multi-channel Replay Speech Detection](https://arxiv.org/abs/2509.14789)
*Michael Neri,Tuomas Virtanen*

Main category: eess.AS

TL;DR: 提出了一个多通道重放语音攻击的声学模拟框架，通过模拟真实环境中的麦克风、扬声器脉冲响应、房间声学和噪声条件，生成合成数据来提升重放语音检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 重放语音攻击对语音控制系统构成严重威胁，现有数据集和方法主要依赖单通道录音，而多通道音频的空间线索可以增强重放检测的鲁棒性。

Method: 开发了声学模拟框架，模拟多通道重放语音配置，包括真实和伪造语音在不同环境中的传播，使用测量的扬声器方向性，定义两种欺骗设置（混响和无混响语音），并评估全向和扩散噪声的影响。

Result: 使用最先进的M-ALRAD模型进行测试，证明合成数据可以支持检测器在未见过的环境中的泛化能力。

Conclusion: 该模拟框架能够有效生成多通道重放语音数据，提升检测器对未知环境的适应性和检测性能。

Abstract: Replay speech attacks pose a significant threat to voice-controlled systems,
especially in smart environments where voice assistants are widely deployed.
While multi-channel audio offers spatial cues that can enhance replay detection
robustness, existing datasets and methods predominantly rely on single-channel
recordings. In this work, we introduce an acoustic simulation framework
designed to simulate multi-channel replay speech configurations using publicly
available resources. Our setup models both genuine and spoofed speech across
varied environments, including realistic microphone and loudspeaker impulse
responses, room acoustics, and noise conditions. The framework employs measured
loudspeaker directionalities during the replay attack to improve the realism of
the simulation. We define two spoofing settings, which simulate whether a
reverberant or an anechoic speech is used in the replay scenario, and evaluate
the impact of omnidirectional and diffuse noise on detection performance. Using
the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate
that synthetic data can support the generalization capabilities of the detector
across unseen enclosures.

</details>


### [30] [AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding and Dropout-Based Learning](https://arxiv.org/abs/2509.14855)
*Michael Tatarjitzky,Boaz Rafaely*

Main category: eess.AS

TL;DR: AmbiDrop是一个基于Ambisonics的阵列无关语音增强框架，通过球谐域编码和通道dropout技术，无需多样化麦克风阵列数据库即可实现良好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决传统多通道语音增强方法依赖特定麦克风阵列几何形状、无法适应几何变化的问题，提高对未见阵列布局的泛化能力

Method: 使用Ambisonics信号匹配将任意阵列录音编码到球谐域，在模拟的Ambisonics数据上训练深度神经网络，结合通道dropout技术增强对阵列相关编码错误的鲁棒性

Result: 在训练阵列上性能与基线相当，但在未见阵列上基线性能下降，而AmbiDrop在SI-SDR、PESQ和STOI指标上持续提升

Conclusion: AmbiDrop展示了强大的泛化能力和阵列无关语音增强的实际应用潜力，无需多样化麦克风阵列数据库

Abstract: Multichannel speech enhancement leverages spatial cues to improve
intelligibility and quality, but most learning-based methods rely on specific
microphone array geometry, unable to account for geometry changes. To mitigate
this limitation, current array-agnostic approaches employ large multi-geometry
datasets but may still fail to generalize to unseen layouts. We propose
AmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes
arbitrary array recordings into the spherical harmonics domain using Ambisonics
Signal Matching (ASM). A deep neural network is trained on simulated Ambisonics
data, combined with channel dropout for robustness against array-dependent
encoding errors, therefore omitting the need for a diverse microphone array
database. Experiments show that while the baseline and proposed models perform
similarly on the training arrays, the baseline degrades on unseen arrays. In
contrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating
strong generalization and practical potential for array-agnostic speech
enhancement.

</details>


### [31] [Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance](https://arxiv.org/abs/2509.14934)
*Francisco Messina,Francesca Ronchini,Luca Comanducci,Paolo Bestagini,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本文探索使用抗记忆化策略来解决文本到音频扩散模型中的数据复制问题，采用AMG技术修改预训练扩散模型的采样过程以减少记忆化，在保持生成质量的同时显著降低复制现象。


<details>
  <summary>Details</summary>
Motivation: 生成音频模型中存在数据复制问题，模型在推理过程中会无意中生成训练数据的一部分，这需要解决。

Method: 采用Anti-Memorization Guidance (AMG)技术，探索三种不同的引导方式，使用Stable Audio Open作为骨干网络，利用其完全开源的架构和训练数据集。

Result: 综合实验分析表明，AMG显著减轻了基于扩散的文本到音频生成中的记忆化现象，同时不损害音频保真度或语义对齐。

Conclusion: AMG策略能有效解决文本到音频扩散模型中的数据复制问题，在保持生成质量的前提下减少记忆化现象。

Abstract: A persistent challenge in generative audio models is data replication, where
the model unintentionally generates parts of its training data during
inference. In this work, we address this issue in text-to-audio diffusion
models by exploring the use of anti-memorization strategies. We adopt
Anti-Memorization Guidance (AMG), a technique that modifies the sampling
process of pre-trained diffusion models to discourage memorization. Our study
explores three types of guidance within AMG, each designed to reduce
replication while preserving generation quality. We use Stable Audio Open as
our backbone, leveraging its fully open-source architecture and training
dataset. Our comprehensive experimental analysis suggests that AMG
significantly mitigates memorization in diffusion-based text-to-audio
generation without compromising audio fidelity or semantic alignment.

</details>


### [32] [SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding](https://arxiv.org/abs/2509.14946)
*Bingsong Bai,Qihang Lu,Wenbing Yang,Zihan Sun,YueRan Hou,Peilei Jia,Songbai Pu,Ruibo Fu,Yingming Gao,Ya Li,Jun Gao*

Main category: eess.AS

TL;DR: 自动化框架生成大规模语调声音数据集SynParaSpeech，包含6类118.75小时语音，提高语音生成和识别性能


<details>
  <summary>Details</summary>
Motivation: 解决现有语调声音数据集存在的问题：依赖专有数据、语音不完整、时间戳不准确、实际应用性差

Method: 提出自动化框架，从自然对话语音中提取语调声音，构建SynParaSpeech数据集，包含6个类别的精确时间戳数据

Result: 成功构建了118.75小时的大规模语调声音数据集，提供了精确的时间戳标注

Conclusion: 该研究首次提出自动化构建大规模语调声音数据集的方法，SynParaSpeech数据集能够提升语音生成的自然性和语调声音事件检测的准确性

Abstract: Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing
more realistic and engaging speech. However, existing methods typically depend
on proprietary datasets, while publicly available resources often suffer from
incomplete speech, inaccurate or missing timestamps, and limited real-world
relevance. To address these problems, we propose an automated framework for
generating large-scale paralinguistic data and apply it to construct the
SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with
118.75 hours of data and precise timestamps, all derived from natural
conversational speech. Our contributions lie in introducing the first automated
method for constructing large-scale paralinguistic datasets and releasing the
SynParaSpeech corpus, which advances speech generation through more natural
paralinguistic synthesis and enhances speech understanding by improving
paralinguistic event detection. The dataset and audio samples are available at
https://github.com/ShawnPi233/SynParaSpeech.

</details>


### [33] [Discrete optimal transport is a strong audio adversarial attack](https://arxiv.org/abs/2509.14959)
*Anton Selitskiy,Akib Shahriyar,Jishnuraj Prakasan*

Main category: eess.AS

TL;DR: 离散最优传输(DOT)是一种有效的黑盒对抗攻击方法，能够成功攻击现代音频反欺骗检测系统，通过分布对齐实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现代音频反欺骗检测系统(CMs)容易受到对抗攻击，但现有攻击方法在跨数据集迁移和系统微调后效果不佳，需要开发更稳定有效的攻击手段。

Method: 采用离散最优传输作为后处理步骤：将生成语音的WavLM帧级嵌入通过熵最优传输与真实语音池对齐，使用top-k重心投影，最后通过神经声码器解码。

Result: 在ASVspoof2019和ASVspoof5数据集上，DOT攻击始终获得高EER(等错误率)，在CM微调后仍保持竞争力，在跨数据集迁移中优于多种传统攻击方法。

Conclusion: 分布级别的对齐是部署CMs的强大且稳定的攻击面，声码器重叠对实际效果有重要影响。

Abstract: In this paper, we show that discrete optimal transport (DOT) is an effective
black-box adversarial attack against modern audio anti-spoofing countermeasures
(CMs). Our attack operates as a post-processing, distribution-alignment step:
frame-level WavLM embeddings of generated speech are aligned to an unpaired
bona fide pool via entropic OT and a top-$k$ barycentric projection, then
decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with
AASIST baselines, DOT yields consistently high equal error rate (EER) across
datasets and remains competitive after CM fine-tuning, outperforming several
conventional attacks in cross-dataset transfer. Ablation analysis highlights
the practical impact of vocoder overlap. Results indicate that
distribution-level alignment is a powerful and stable attack surface for
deployed CMs.

</details>


### [34] [BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings](https://arxiv.org/abs/2509.15001)
*Théo Charlot,Tarek Kunze,Maxime Poli,Alejandrina Cristia,Emmanuel Dupoux,Marvin Lavechin*

Main category: eess.AS

TL;DR: BabyHuBERT是首个基于13,000小时多语言儿童长时录音训练的自监督语音表示模型，在儿童语音识别任务上显著优于现有模型


<details>
  <summary>Details</summary>
Motivation: 现有基于成人清晰语音训练的模型在儿童语音数据上表现不佳，因为儿童语音在声学和语言学特征上与成人存在显著差异

Method: 使用13,000小时涵盖40多种语言的多语言儿童中心长时录音训练自监督语音表示模型BabyHuBERT

Result: 在六个不同数据集上，BabyHuBERT的F1分数达到52.1%到74.4%，比W2V2-LL4300和标准HuBERT表现更好，在Vanuatu和Solomon Islands语料上分别提升13.2和15.9个F1点

Conclusion: BabyHuBERT作为儿童语音研究的基础模型，能够在下游任务上进行微调，并为研究自然语言环境中的语言发展提供了重要工具

Abstract: Child-centered long-form recordings are essential for studying early language
development, but existing speech models trained on clean adult data perform
poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the
first self-supervised speech representation model trained on 13,000 hours of
multilingual child-centered long-form recordings spanning over 40 languages. We
evaluate BabyHuBERT on speaker segmentation, identifying when target children
speak versus female adults, male adults, or other children -- a fundamental
preprocessing step for analyzing naturalistic language experiences. BabyHuBERT
achieves F1-scores from 52.1% to 74.4% across six diverse datasets,
consistently outperforming W2V2-LL4300 (trained on English long-forms) and
standard HuBERT (trained on clean adult speech). Notable improvements include
13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon
Islands corpora, demonstrating effectiveness on underrepresented languages. By
sharing code and models, BabyHuBERT serves as a foundation model for child
speech research, enabling fine-tuning on diverse downstream tasks.

</details>


### [35] [Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models](https://arxiv.org/abs/2509.15008)
*Chaoyue Niu,Veronica Rowe,Guy J. Brown,Heather Elphick,Heather Kenyon,Lowri Thomas,Sam Johnson,Ning Ma*

Main category: eess.AS

TL;DR: 提出基于迁移学习的声学监测框架，利用成人睡眠数据预训练模型，结合SpO2去饱和模式，有效提升儿童阻塞性睡眠呼吸暂停的家庭筛查准确率


<details>
  <summary>Details</summary>
Motivation: 儿童阻塞性睡眠呼吸暂停(OSA)临床意义重大但诊断困难，传统传感器多导睡眠图儿童耐受性差。声学监测提供无创的家庭筛查方案，但儿童数据有限阻碍深度学习模型发展

Method: 采用迁移学习框架，将成人睡眠声学数据预训练的模型适配到儿童OSA检测，整合SpO2去饱和模式增强训练。系统评估单任务vs多任务学习、编码器冻结vs全微调、SpO2标签延迟对齐等策略

Result: 实验表明，结合SpO2整合的微调方法相比无适配基线模型，能持续改善儿童OSA检测性能

Conclusion: 验证了迁移学习在儿童家庭OSA筛查中的可行性，展示了其在早期诊断中的潜在临床价值

Abstract: Paediatric obstructive sleep apnoea (OSA) is clinically significant yet
difficult to diagnose, as children poorly tolerate sensor-based
polysomnography. Acoustic monitoring provides a non-invasive alternative for
home-based OSA screening, but limited paediatric data hinders the development
of robust deep learning approaches. This paper proposes a transfer learning
framework that adapts acoustic models pretrained on adult sleep data to
paediatric OSA detection, incorporating SpO2-based desaturation patterns to
enhance model training. Using a large adult sleep dataset (157 nights) and a
smaller paediatric dataset (15 nights), we systematically evaluate (i) single-
versus multi-task learning, (ii) encoder freezing versus full fine-tuning, and
(iii) the impact of delaying SpO2 labels to better align them with the
acoustics and capture physiologically meaningful features. Results show that
fine-tuning with SpO2 integration consistently improves paediatric OSA
detection compared with baseline models without adaptation. These findings
demonstrate the feasibility of transfer learning for home-based OSA screening
in children and offer its potential clinical value for early diagnosis.

</details>


### [36] [From Who Said What to Who They Are: Modular Training-free Identity-Aware LLM Refinement of Speaker Diarization](https://arxiv.org/abs/2509.15082)
*Yu-Wen Chen,William Ho,Maxim Topaz,Julia Hirschberg,Zoran Kostic*

Main category: eess.AS

TL;DR: 一种无需训练的模块化方案，结合现成语音分割、语音识别和大语言模型，通过语义上下文提升讲者识别准确性。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中语音分割的挑战：动态环境、未知讲者数量、非模块化方法缺乏灵活性，以及需要真实讲者身份而非假标签的应用需求。

Method: 使用结构化的LLM提示对协调后的SD和ASR输出进行处理，利用对话上下文中的语义连续性来精炼低信心度的讲者标签，分配角色身份并缩正分割错误的讲者。

Result: 在真实医患数据集上，该方法相比基线协调SD和ASR实现了29.7%的相对错误降低。

Conclusion: 该方法在不需额外训练的情况下提升了语音分割性能，为实际应用提供了完整的SD、ASR和讲者身份检测流程。

Abstract: Speaker diarization (SD) struggles in real-world scenarios due to dynamic
environments and unknown speaker counts. SD is rarely used alone and is often
paired with automatic speech recognition (ASR), but non-modular methods that
jointly train on domain-specific data have limited flexibility. Moreover, many
applications require true speaker identities rather than SD's pseudo labels. We
propose a training-free modular pipeline combining off-the-shelf SD, ASR, and a
large language model (LLM) to determine who spoke, what was said, and who they
are. Using structured LLM prompting on reconciled SD and ASR outputs, our
method leverages semantic continuity in conversational context to refine
low-confidence speaker labels and assigns role identities while correcting
split speakers. On a real-world patient-clinician dataset, our approach
achieves a 29.7% relative error reduction over baseline reconciled SD and ASR.
It enhances diarization performance without additional training and delivers a
complete pipeline for SD, ASR, and speaker identity detection in practical
applications.

</details>


### [37] [Real-Time Streaming Mel Vocoding with Generative Flow Matching](https://arxiv.org/abs/2509.15085)
*Simon Welker,Tal Peer,Timo Gerkmann*

Main category: eess.AS

TL;DR: MelFlow是一个基于生成流匹配的流式Mel声码器，具有32ms算法延迟和48ms总延迟，在16kHz语音上实现实时流式处理，性能优于HiFi-GAN等非流式基线模型。


<details>
  <summary>Details</summary>
Motivation: Mel声码化（将Mel幅度谱转换为音频波形）仍然是许多TTS系统的关键组件，需要开发低延迟的流式处理方案。

Method: 基于生成流匹配、先前的DiffPhase生成STFT相位检索工作以及Mel滤波器组的伪逆算子，开发了MelFlow流式生成模型。

Result: 在消费级笔记本GPU上实现了实时流式处理，PESQ和SI-SDR指标显著优于HiFi-GAN等非流式基线模型。

Conclusion: MelFlow成功实现了低延迟的流式Mel声码化，在保持高质量的同时大幅降低了处理延迟。

Abstract: The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram
to an audio waveform, is still a key component in many text-to-speech (TTS)
systems today. Based on generative flow matching, our prior work on generative
STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel
filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for
speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total
latency of 48 ms. We show real-time streaming capability at this latency not
only in theory, but in practice on a consumer laptop GPU. Furthermore, we show
that our model achieves substantially better PESQ and SI-SDR values compared to
well-established not streaming-capable baselines for Mel vocoding including
HiFi-GAN.

</details>


### [38] [Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs](https://arxiv.org/abs/2509.15095)
*Yutong Liu,Ziyue Zhang,Yongbin Yu,Xiangxiang Wang,Yuqing Cai,Nyima Tashi*

Main category: eess.AS

TL;DR: LIR-ASR是一个基于LLM的启发式优化迭代校正框架，通过"听-想象-精炼"策略减少ASR错误，在英中ASR输出上平均降低CER/WER达1.5个百分点


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统容易产生错误，影响下游应用性能，需要有效的错误校正方法

Method: 采用受人类听觉感知启发的"Listening-Imagining-Refining"策略，生成语音变体并在上下文中精炼，引入启发式优化和有限状态机避免局部最优，使用基于规则的约束保持语义保真度

Result: 在英语和中文ASR输出上实验显示，相比基线方法平均降低CER/WER达1.5个百分点，显著提升转录准确性

Conclusion: LIR-ASR框架有效减少了ASR错误，通过迭代校正策略和启发式优化实现了显著的准确率提升

Abstract: Automatic Speech Recognition (ASR) systems remain prone to errors that affect
downstream applications. In this paper, we propose LIR-ASR, a heuristic
optimized iterative correction framework using LLMs, inspired by human auditory
perception. LIR-ASR applies a "Listening-Imagining-Refining" strategy,
generating phonetic variants and refining them in context. A heuristic
optimization with finite state machine (FSM) is introduced to prevent the
correction process from being trapped in local optima and rule-based
constraints help maintain semantic fidelity. Experiments on both English and
Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of
up to 1.5 percentage points compared to baselines, demonstrating substantial
accuracy gains in transcription.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [39] [Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework](https://arxiv.org/abs/2509.14304)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.SD

TL;DR: UDM框架在口吃和言语不流畅检测中实现了高精度（F1: 0.89）与临床可解释性（4.2/5.0）的平衡，临床接受率达87%，诊断时间减少34%


<details>
  <summary>Details</summary>
Motivation: 解决传统口吃检测系统在准确性和临床可解释性之间的权衡问题，克服端到端深度学习模型的黑箱性质对临床应用的限制

Method: 采用模块化架构、显式音素对齐和可解释输出，通过患者和认证言语病理学家的广泛实验验证

Result: 达到最先进性能（F1: 0.89±0.04），临床可解释性评分4.2/5.0，临床接受率87%，诊断时间减少34%

Conclusion: UDM代表了在临床环境中实现AI辅助言语治疗的实用途径

Abstract: Stuttered and dysfluent speech detection systems have traditionally suffered
from the trade-off between accuracy and clinical interpretability. While
end-to-end deep learning models achieve high performance, their black-box
nature limits clinical adoption. This paper looks at the Unconstrained
Dysfluency Modeling (UDM) series-the current state-of-the-art framework
developed by Berkeley that combines modular architecture, explicit phoneme
alignment, and interpretable outputs for real-world clinical deployment.
Through extensive experiments involving patients and certified speech-language
pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art
performance (F1: 0.89+-0.04) while providing clinically meaningful
interpretability scores (4.2/5.0). Our deployment study shows 87% clinician
acceptance rate and 34% reduction in diagnostic time. The results provide
strong evidence that UDM represents a practical pathway toward AI-assisted
speech therapy in clinical environments.

</details>


### [40] [Measuring Soft Biometric Leakage in Speaker De-Identification Systems](https://arxiv.org/abs/2509.14469)
*Seungmin Seo,Oleg Aulov,P. Jonathon Phillips*

Main category: cs.SD

TL;DR: 该文章提出了软生物特征泄漌指数（SBLS），用于量化语音匿名化系统在非唯一性软生物特征上的漌漌风险，发现当前系统都存在显著漌漌漌洞。


<details>
  <summary>Details</summary>
Motivation: 当前语音匿名化系统评估仅关注个体级别重识别风险，忽视了软生物特征泄漌带来的更广泛风险。

Method: 提出SBLS方法，整合三个元素：直接属性推理（使用预训练分类器）、链接检测（通过相互信息分析）和子组稳健性（跨交属性的精度分析）。

Result: 对五个匿名化系统的评测显示，所有系统都存在显著的软生物特征泄漌漌洞，攻击者仅使用预训练模型即可可靠恢复匿名化输出中的软生物特征信息。

Conclusion: 标准的分布指标无法抓住语音匿名化系统的根本弱点，SBLS提供了一种统一的风险量化方法。

Abstract: We use the term re-identification to refer to the process of recovering the
original speaker's identity from anonymized speech outputs. Speaker
de-identification systems aim to reduce the risk of re-identification, but most
evaluations focus only on individual-level measures and overlook broader risks
from soft biometric leakage. We introduce the Soft Biometric Leakage Score
(SBLS), a unified method that quantifies resistance to zero-shot inference
attacks on non-unique traits such as channel type, age range, dialect, sex of
the speaker, or speaking style. SBLS integrates three elements: direct
attribute inference using pre-trained classifiers, linkage detection via mutual
information analysis, and subgroup robustness across intersecting attributes.
Applying SBLS with publicly available classifiers, we show that all five
evaluated de-identification systems exhibit significant vulnerabilities. Our
results indicate that adversaries using only pre-trained models - without
access to original speech or system details - can still reliably recover soft
biometric information from anonymized output, exposing fundamental weaknesses
that standard distributional metrics fail to capture.

</details>


### [41] [A long-form single-speaker real-time MRI speech dataset and benchmark](https://arxiv.org/abs/2509.14479)
*Sean Foley,Jihwan Lee,Kevin Huang,Xuan Shi,Yoonjeong Lee,Louis Goldstein,Shrikanth Narayanan*

Main category: cs.SD

TL;DR: USC LSS数据集发布，包含单说话者1小时的实时MRI声道动态视频和同步音频数据，提供多种衍生表示，并在发音合成和音素识别任务上建立基准性能


<details>
  <summary>Details</summary>
Motivation: 提供更长的公开单说话者实时MRI语音数据集，支持发音研究和下游任务

Method: 收集单名美国英语母语者的实时MRI视频和同步音频数据，进行数据预处理和衍生表示提取

Result: 发布了约1小时的数据集，包含原始数据和多种衍生表示（声道区域裁剪视频、句子级分割、修复降噪音频、ROI时间序列等）

Conclusion: 该数据集为发音合成和音素识别等任务提供了有价值的基准，可供未来研究改进

Abstract: We release the USC Long Single-Speaker (LSS) dataset containing real-time MRI
video of the vocal tract dynamics and simultaneous audio obtained during speech
production. This unique dataset contains roughly one hour of video and audio
data from a single native speaker of American English, making it one of the
longer publicly available single-speaker datasets of real-time MRI speech data.
Along with the articulatory and acoustic raw data, we release derived
representations of the data that are suitable for a range of downstream tasks.
This includes video cropped to the vocal tract region, sentence-level splits of
the data, restored and denoised audio, and regions-of-interest timeseries. We
also benchmark this dataset on articulatory synthesis and phoneme recognition
tasks, providing baseline performance for these tasks on this dataset which
future research can aim to improve upon.

</details>


### [42] [Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech Synthesis](https://arxiv.org/abs/2509.14579)
*Qingyu Liu,Yushen Chen,Zhikang Niu,Chunhui Wang,Yunting Yang,Bowen Zhang,Jian Zhao,Pengcheng Zhu,Kai Yu,Xie Chen*

Main category: cs.SD

TL;DR: 提出了Cross-Lingual F5-TTS框架，能够在不需要音频提示文本的情况下实现跨语言语音克隆，通过强制对齐获取词边界，并训练多粒度语速预测器解决时长建模问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于流匹配的TTS模型需要音频提示对应的参考文本，这限制了在无文本情况下（特别是未见语言）的跨语言语音克隆能力。

Method: 使用强制对齐预处理音频提示获取词边界，在训练时排除文本依赖；训练不同语言学粒度的语速预测器来从说话人语速推导时长。

Result: 实验表明该方法在保持F5-TTS性能的同时实现了跨语言语音克隆。

Conclusion: 提出的框架成功解决了基于流匹配的TTS模型在无音频提示文本情况下的跨语言语音克隆问题。

Abstract: Flow-matching-based text-to-speech (TTS) models have shown high-quality
speech synthesis. However, most current flow-matching-based TTS models still
rely on reference transcripts corresponding to the audio prompt for synthesis.
This dependency prevents cross-lingual voice cloning when audio prompt
transcripts are unavailable, particularly for unseen languages. The key
challenges for flow-matching-based TTS models to remove audio prompt
transcripts are identifying word boundaries during training and determining
appropriate duration during inference. In this paper, we introduce
Cross-Lingual F5-TTS, a framework that enables cross-lingual voice cloning
without audio prompt transcripts. Our method preprocesses audio prompts by
forced alignment to obtain word boundaries, enabling direct synthesis from
audio prompts while excluding transcripts during training. To address the
duration modeling challenge, we train speaking rate predictors at different
linguistic granularities to derive duration from speaker pace. Experiments show
that our approach matches the performance of F5-TTS while enabling
cross-lingual voice cloning.

</details>


### [43] [Spatial Audio Motion Understanding and Reasoning](https://arxiv.org/abs/2509.14666)
*Arvind Krishna Sridhar,Yinyi Guo,Erik Visser*

Main category: cs.SD

TL;DR: 提出了一种空间音频推理框架，通过空间音频编码器和音频接地模型检测事件并估计空间属性，结合大语言模型处理动态音频场景中的移动源问题


<details>
  <summary>Details</summary>
Motivation: 让机器能够理解听觉场景中的事件及其空间属性，特别是针对移动声源的复杂空间音频推理

Method: 1) 空间音频编码器处理空间音频，检测重叠事件并估计方向角和距离；2) 音频接地模型通过跨注意力机制对齐音频特征和语义文本嵌入；3) 使用大语言模型处理提取的结构化空间属性来回答复杂查询

Result: 引入了空间音频运动理解和推理基准数据集，并展示了框架相对于基线模型的性能表现

Conclusion: 该框架能够有效处理动态音频场景中的移动声源推理问题，为空间音频理解提供了新的解决方案

Abstract: Spatial audio reasoning enables machines to interpret auditory scenes by
understanding events and their spatial attributes. In this work, we focus on
spatial audio understanding with an emphasis on reasoning about moving sources.
First, we introduce a spatial audio encoder that processes spatial audio to
detect multiple overlapping events and estimate their spatial attributes,
Direction of Arrival (DoA) and source distance, at the frame level. To
generalize to unseen events, we incorporate an audio grounding model that
aligns audio features with semantic audio class text embeddings via a
cross-attention mechanism. Second, to answer complex queries about dynamic
audio scenes involving moving sources, we condition a large language model
(LLM) on structured spatial attributes extracted by our model. Finally, we
introduce a spatial audio motion understanding and reasoning benchmark dataset
and demonstrate our framework's performance against the baseline model.

</details>


### [44] [How Does Instrumental Music Help SingFake Detection?](https://arxiv.org/abs/2509.14675)
*Xuanjun Chen,Chia-Yu Hu,I-Ming Lin,Yi-Cheng Lin,I-Hsiang Chiu,You Zhang,Sung-Feng Huang,Yi-Hsuan Yang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 该研究探讨了伴奏音乐对歌声深度伪造检测的影响，发现伴奏主要起数据增强作用而非提供内在线索，微调会增强模型对说话人特征的依赖而降低对其他信息的敏感性。


<details>
  <summary>Details</summary>
Motivation: 虽然存在许多歌声深度伪造检测模型，但这些模型在伴奏音乐存在时的运作机制尚不明确，需要研究伴奏音乐如何影响检测性能。

Method: 从行为效应和表征效应两个角度进行研究：测试不同骨干网络、无配对伴奏音轨和频率子带；分析微调如何改变编码器的语音和音乐能力。

Result: 伴奏音乐主要作为数据增强而非提供节奏或和声等内在线索；微调增加了对浅层说话人特征的依赖，同时降低了对内容、副语言和语义信息的敏感性。

Conclusion: 这些发现阐明了模型如何利用声乐与器乐线索，可为设计更可解释和鲁棒的歌声深度伪造检测系统提供指导。

Abstract: Although many models exist to detect singing voice deepfakes (SingFake), how
these models operate, particularly with instrumental accompaniment, is unclear.
We investigate how instrumental music affects SingFake detection from two
perspectives. To investigate the behavioral effect, we test different
backbones, unpaired instrumental tracks, and frequency subbands. To analyze the
representational effect, we probe how fine-tuning alters encoders' speech and
music capabilities. Our results show that instrumental accompaniment acts
mainly as data augmentation rather than providing intrinsic cues (e.g., rhythm
or harmony). Furthermore, fine-tuning increases reliance on shallow speaker
features while reducing sensitivity to content, paralinguistic, and semantic
information. These insights clarify how models exploit vocal versus
instrumental cues and can inform the design of more interpretable and robust
SingFake detection systems.

</details>


### [45] [Pushing the Limits of End-to-End Diarization](https://arxiv.org/abs/2509.14737)
*Samuel J. Broughton,Lahiru Samarakoon*

Main category: cs.SD

TL;DR: 基于EEND-TA的统一非自回归模型在多个公开数据集上实现了最先进的说话人日志错误率，特别是在DIHARD III上达到14.49%的DER


<details>
  <summary>Details</summary>
Motivation: 探索基于EEND的架构在说话人日志任务中的学习能力，超越现有解决方案同时保持推理效率

Method: 使用EEND-TA单一统一非自回归模型，通过8说话人模拟混合进行预训练扩展，确保每个生成的说话人混合配置得到充分表示

Result: 在多个公开数据集上取得新的基准结果，包括AliMeeting-far、AliMeeting-near、AMI-Mix、AMI-SDM、DIHARD III和MagicData RAMC，其中DIHARD III上的DER为14.49%

Conclusion: EEND-based架构具有比先前探索更大的学习能力，在保持推理效率的同时超越了现有的许多日志解决方案

Abstract: In this paper, we present state-of-the-art diarization error rates (DERs) on
multiple publicly available datasets, including AliMeeting-far,
AliMeeting-near, AMI-Mix, AMI-SDM, DIHARD III, and MagicData RAMC. Leveraging
EEND-TA, a single unified non-autoregressive model for end-to-end speaker
diarization, we achieve new benchmark results, most notably a DER of 14.49% on
DIHARD III. Our approach scales pretraining through 8-speaker simulation
mixtures, ensuring each generated speaker mixture configuration is sufficiently
represented. These experiments highlight that EEND-based architectures possess
a greater capacity for learning than previously explored, surpassing many
existing diarization solutions while maintaining efficient speeds during
inference.

</details>


### [46] [Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions](https://arxiv.org/abs/2509.14785)
*Kentaro Seki,Yuki Okamoto,Kouei Yamaoka,Yuki Saito,Shinnosuke Takamichi,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: Spatial-CLAP是一个新的音频-文本嵌入框架，通过内容感知空间编码器和空间对比学习，解决了现有CLAP方法在多源条件下无法捕捉空间信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的对比语言-音频预训练(CLAP)方法仅限于单声道或单源条件，无法充分捕捉空间信息。多源条件下的主要挑战在于需要建立每个声源与其位置之间的正确对应关系。

Method: 提出了Spatial-CLAP，包含内容感知空间编码器来生成与音频内容耦合的空间表示，以及空间对比学习(SCL)训练策略来显式学习正确对应关系。

Result: 实验评估表明Spatial-CLAP在多源条件下学习到了有效的嵌入表示，SCL策略有效。在未见过的三源混合音频上的评估突显了多源训练范式与传统单源训练的根本区别。

Conclusion: 这项工作为空间感知的音频-文本嵌入建立了新的范式，解决了多源条件下的空间信息建模挑战。

Abstract: Contrastive language--audio pretraining (CLAP) has achieved remarkable
success as an audio--text embedding framework, but existing approaches are
limited to monaural or single-source conditions and cannot fully capture
spatial information. The central challenge in modeling spatial information lies
in multi-source conditions, where the correct correspondence between each sound
source and its location is required. To tackle this problem, we propose
Spatial-CLAP, which introduces a content-aware spatial encoder that enables
spatial representations coupled with audio content. We further propose spatial
contrastive learning (SCL), a training strategy that explicitly enforces the
learning of the correct correspondence and promotes more reliable embeddings
under multi-source conditions. Experimental evaluations, including downstream
tasks, demonstrate that Spatial-CLAP learns effective embeddings even under
multi-source conditions, and confirm the effectiveness of SCL. Moreover,
evaluation on unseen three-source mixtures highlights the fundamental
distinction between conventional single-source training and our proposed
multi-source training paradigm. These findings establish a new paradigm for
spatially-aware audio--text embeddings.

</details>


### [47] [Towards Building Speech Large Language Models for Multitask Understanding in Low-Resource Languages](https://arxiv.org/abs/2509.14804)
*Mingchen Shao,Bingshen Mu,Chengyou Wang,Hai Li,Ying Yan,Zhonghua Fu,Lei Xie*

Main category: cs.SD

TL;DR: 提出了XLSR-Thai泰语自监督语音编码器、U-Align语音文本对齐方法和Thai-SUP数据生成管道，解决了泰语等低资源语言中语音大语言模型性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在高资源语言（如英语、中文）表现优异，但在低资源语言（如泰语）性能显著下降，主要由于语音编码器性能不足、ASR对齐范式计算成本高、以及配对语音文本数据稀缺。

Method: 1）在3.6万小时泰语数据上持续训练得到XLSR-Thai自监督语音编码器；2）提出资源效率更高的U-Align语音文本对齐方法；3）开发Thai-SUP管道从高资源语言生成泰语语音理解数据。

Result: 构建了首个超过1000小时的泰语语音理解数据集，实验证明所提方法能有效构建泰语多任务理解语音大语言模型。

Conclusion: 提出的XLSR-Thai、U-Align和Thai-SUP方法成功解决了低资源语言泰语中语音大语言模型的性能瓶颈，为未来研究提供了开源工具和数据资源。

Abstract: Speech large language models (SLLMs) built on speech encoders, adapters, and
LLMs demonstrate remarkable multitask understanding performance in
high-resource languages such as English and Chinese. However, their
effectiveness substantially degrades in low-resource languages such as Thai.
This limitation arises from three factors: (1) existing commonly used speech
encoders, like the Whisper family, underperform in low-resource languages and
lack support for broader spoken language understanding tasks; (2) the ASR-based
alignment paradigm requires training the entire SLLM, leading to high
computational cost; (3) paired speech-text data in low-resource languages is
scarce. To overcome these challenges in the low-resource language Thai, we
introduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder
for Thai. It is obtained by continuously training the standard SSL XLSR model
on 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a
speech-text alignment method that is more resource-efficient and
multitask-effective than typical ASR-based alignment. Finally, we present
Thai-SUP, a pipeline for generating Thai spoken language understanding data
from high-resource languages, yielding the first Thai spoken language
understanding dataset of over 1,000 hours. Multiple experiments demonstrate the
effectiveness of our methods in building a Thai multitask-understanding SLLM.
We open-source XLSR-Thai and Thai-SUP to facilitate future research.

</details>


### [48] [MeanFlowSE: one-step generative speech enhancement via conditional mean flow](https://arxiv.org/abs/2509.14858)
*Duojia Li,Shenghui Lu,Hongchen Pan,Zongyi Zhan,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: MeanFlowSE是一种条件生成模型，通过学习轨迹上有限间隔内的平均速度，实现单步语音增强，避免了传统多步推理的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 多步推理是实时生成式语音增强的瓶颈，因为基于流和扩散的系统需要迭代ODE求解器。本文旨在开发一种高效的单步生成方法。

Method: 提出MeanFlowSE模型，使用Jacobian-vector product实现MeanFlow恒等式，学习有限间隔位移的直接监督目标，保持与瞬时场约束的一致性。

Result: 在VoiceBank-DEMAND数据集上，单步模型实现了良好的可懂度、保真度和感知质量，计算成本显著低于多步基线方法。

Conclusion: 该方法无需知识蒸馏或外部教师，为实时生成式语音增强提供了一个高效、高保真的框架。

Abstract: Multistep inference is a bottleneck for real-time generative speech
enhancement because flow- and diffusion-based systems learn an instantaneous
velocity field and therefore rely on iterative ordinary differential equation
(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that
learns the average velocity over finite intervals along a trajectory. Using a
Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a
local training objective that directly supervises finite-interval displacement
while remaining consistent with the instantaneous-field constraint on the
diagonal. At inference, MeanFlowSE performs single-step generation via a
backward-in-time displacement, removing the need for multistep solvers; an
optional few-step variant offers additional refinement. On VoiceBank-DEMAND,
the single-step model achieves strong intelligibility, fidelity, and perceptual
quality with substantially lower computational cost than multistep baselines.
The method requires no knowledge distillation or external teachers, providing
an efficient, high-fidelity framework for real-time generative speech
enhancement.

</details>


### [49] [From Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition](https://arxiv.org/abs/2509.14880)
*Rishabh Jain,Naomi Harte*

Main category: cs.SD

TL;DR: 通过系统性实验评估LLM解码器在视觉语音识别中的作用，发现性能提升主要来自词汇处理而非视觉特征理解，建议需要更强的视觉编码器


<details>
  <summary>Details</summary>
Motivation: 识别LLM解码器在视觉语音识别中的真正作用，分清性能提升是来自视觉理解还是语言模型能力

Method: 冻结/选择性更新视觉编码器、缩放解码器规模、比较适配策略和架构、在LRS2、LRS3及其组合数据集上训练

Result: Llama-2-13B模型在LRS3达到24.7% WER，WildVSR上47.0% WER，设立无额外监督SOTA，证明数据组合提升通用性

Conclusion: LLM解码器主要精炼上下文推理而非视觉特征，需要更强的视觉编码器来推动真正进步

Abstract: Advances in self-supervised encoders have improved Visual Speech Recognition
(VSR). Recent approaches integrating these encoders with LLM decoders improves
transcription accuracy; however, it remains unclear whether these gains stem
from visual understanding or stronger language modeling. In this work, we
systematically evaluate LLM decoders by freezing or selectively updating the
visual encoder, scaling decoder size, comparing adaptation strategies and
architectures, and varying training data across LRS2, LRS3, and their
combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and
adaptation yield limited improvements, while combining datasets enhances
generalization. Semantic analysis reveals that gains arise primarily from
lexical rather than semantic processing. Our Llama-2-13B model trained on the
combined set achieves 24.7\% WER on LRS3 and 47.0\% on WildVSR, establishing
SOTA among models trained without additional supervision. Our findings indicate
LLM decoders refine contextual reasoning rather than visual features,
emphasizing the need for stronger visual encoders to drive meaningful progress.

</details>


### [50] [Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification](https://arxiv.org/abs/2509.14893)
*Yuanjian Chen,Yang Xiao,Jinjie Huang*

Main category: cs.SD

TL;DR: 提出了基于时间异构图对比学习(THGCL)的多模态声学事件分类方法，通过构建音频视频时序图，结合高斯过程和霍克斯过程来分别处理模态内平滑性和模态间衰减关系，在AudioSet数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 多模态声学事件分类中音频和视觉信号的时间对齐困难，现有方法往往分别处理不同模态，后期融合特征，且缺乏对模态内和模态间时间依赖关系的区分

Method: 构建每个事件的时间图，音频和视频段作为节点，时间链接作为边；使用高斯过程处理模态内平滑性，霍克斯过程处理模态间衰减关系，并采用对比学习捕捉细粒度关系

Result: 在AudioSet数据集上实现了最先进的性能

Conclusion: THGCL框架通过有效建模模态内和模态间的时间依赖关系，显著提升了多模态声学事件分类的性能

Abstract: Multimodal acoustic event classification plays a key role in audio-visual
systems. Although combining audio and visual signals improves recognition, it
is still difficult to align them over time and to reduce the effect of noise
across modalities. Existing methods often treat audio and visual streams
separately, fusing features later with contrastive or mutual information
objectives. Recent advances explore multimodal graph learning, but most fail to
distinguish between intra- and inter-modal temporal dependencies. To address
this, we propose Temporally Heterogeneous Graph-based Contrastive Learning
(THGCL). Our framework constructs a temporal graph for each event, where audio
and video segments form nodes and their temporal links form edges. We introduce
Gaussian processes for intra-modal smoothness, Hawkes processes for inter-modal
decay, and contrastive learning to capture fine-grained relationships.
Experiments on AudioSet show that THGCL achieves state-of-the-art performance.

</details>


### [51] [Back to Ear: Perceptually Driven High Fidelity Music Reconstruction](https://arxiv.org/abs/2509.14912)
*Kangdi Wang,Zhiyue Wu,Dinghao Zhou,Rui Lin,Junyu Dai,Tao Jiang*

Main category: cs.SD

TL;DR: 提出εar-VAE，一种改进的音频VAE模型，通过感知滤波、相位损失和新的频谱监督范式，显著提升了音频重建质量，特别是在高频谐波和空间特性方面。


<details>
  <summary>Details</summary>
Motivation: 现有开源VAE模型在音频任务中忽视听觉感知方面，导致相位精度和立体声空间表示存在缺陷，需要改进训练范式。

Method: 使用K-weighting感知滤波器；提出两种新相位损失（相关性损失和相位导数损失）；设计新的频谱监督范式（幅度用MSLR四分量监督，相位只用LR分量监督）。

Result: 在44.1kHz采样率下，εar-VAE在多项指标上显著优于领先的开源模型，特别是在高频谐波重建和空间特性保持方面表现突出。

Conclusion: εar-VAE通过重新思考和优化VAE训练范式，成功解决了音频重建中的感知质量问题，为大规模音频生成任务提供了更好的基础模型。

Abstract: Variational Autoencoders (VAEs) are essential for large-scale audio tasks
like diffusion-based generation. However, existing open-source models often
neglect auditory perceptual aspects during training, leading to weaknesses in
phase accuracy and stereophonic spatial representation. To address these
challenges, we propose {\epsilon}ar-VAE, an open-source music signal
reconstruction model that rethinks and optimizes the VAE training paradigm. Our
contributions are threefold: (i) A K-weighting perceptual filter applied prior
to loss calculation to align the objective with auditory perception. (ii) Two
novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss
using its derivatives--Instantaneous Frequency and Group Delay--for precision.
(iii) A new spectral supervision paradigm where magnitude is supervised by all
four Mid/Side/Left/Right components, while phase is supervised only by the LR
components. Experiments show {\epsilon}ar-VAE at 44.1kHz substantially
outperforms leading open-source models across diverse metrics, showing
particular strength in reconstructing high-frequency harmonics and the spatial
characteristics.

</details>


### [52] [Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening](https://arxiv.org/abs/2509.14944)
*Xiaolei Xu,Chaoyue Niu,Guy J. Brown,Hector Romero,Ning Ma*

Main category: cs.SD

TL;DR: 通过音频估计呼吸努力，给合声学特征提高阻塞性睡眠呼吸暂停检测效果，只需智能手机音频即可实现无传感器监测


<details>
  <summary>Details</summary>
Motivation: 解决阻塞性睡眠呼吸暂停检测中现有方法成本高、扩展性差的问题，尝试从夜间音频中直接估算呼吸努力以提供生理上下文

Method: 提出潜在空间融合框架，将估算的呼吸努力嵌入与声学特征结合，使用103名参与者157夜家庭录音数据进行验证

Result: 呼吸努力估计器达到0.48的一致性相关系数，融合方法在敏感性和AUC指标上超过单纯音频基线，尤其在低呼吸暂停指数阈值时表现更优

Conclusion: 该方法仅需智能手机音频即可实现高效检测，为无传感器、可扩展的长期睡眠呼吸监测提供了可行方案

Abstract: Obstructive sleep apnoea (OSA) is a prevalent condition with significant
health consequences, yet many patients remain undiagnosed due to the complexity
and cost of over-night polysomnography. Acoustic-based screening provides a
scalable alternative, yet performance is limited by environmental noise and the
lack of physiological context. Respiratory effort is a key signal used in
clinical scoring of OSA events, but current approaches require additional
contact sensors that reduce scalability and patient comfort. This paper
presents the first study to estimate respiratory effort directly from nocturnal
audio, enabling physiological context to be recovered from sound alone. We
propose a latent-space fusion framework that integrates the estimated effort
embeddings with acoustic features for OSA detection. Using a dataset of 157
nights from 103 participants recorded in home environments, our respiratory
effort estimator achieves a concordance correlation coefficient of 0.48,
capturing meaningful respiratory dynamics. Fusing effort and audio improves
sensitivity and AUC over audio-only baselines, especially at low
apnoea-hypopnoea index thresholds. The proposed approach requires only
smartphone audio at test time, which enables sensor-free, scalable, and
longitudinal OSA monitoring.

</details>


### [53] [FCPE: A Fast Context-based Pitch Estimation Model](https://arxiv.org/abs/2509.15140)
*Yuxin Luo,Ruoyi Zhang,Lu-Chuan Liu,Tianyu Li,Hangyu Liu*

Main category: cs.SD

TL;DR: FCPE是一种基于快速上下文的音高估计模型，采用Lynx-Net架构和深度可分离卷积，在保持低计算成本的同时有效提取梅尔频谱特征，具有强大的噪声容忍能力。


<details>
  <summary>Details</summary>
Motivation: 单声道音频中的音高估计对于MIDI转录和歌声转换至关重要，但现有方法在噪声环境下性能显著下降，需要开发更鲁棒高效的解决方案。

Method: 提出FCPE模型，使用Lynx-Net架构结合深度可分离卷积来有效捕获梅尔频谱特征，同时保持低计算成本和良好的噪声容忍性。

Result: 在MIR-1K数据集上达到96.79%的原始音高准确率(RPA)，与最先进方法相当；在单RTX 4090 GPU上实时因子(RTF)为0.0062，效率显著优于现有算法。

Conclusion: FCPE是一种高效且鲁棒的音高估计方法，在保持高精度的同时大幅提升了计算效率，特别适用于噪声环境下的实时应用。

Abstract: Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription
and singing voice conversion (SVC), but existing methods suffer significant
performance degradation under noise. In this paper, we propose FCPE, a fast
context-based pitch estimation model that employs a Lynx-Net architecture with
depth-wise separable convolutions to effectively capture mel spectrogram
features while maintaining low computational cost and robust noise tolerance.
Experiments show that our method achieves 96.79\% Raw Pitch Accuracy (RPA) on
the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time
Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly
outperforms existing algorithms in efficiency. Code is available at
https://github.com/CNChTu/FCPE.

</details>


### [54] [Exploring How Audio Effects Alter Emotion with Foundation Models](https://arxiv.org/abs/2509.15151)
*Stelios Katsis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou*

Main category: cs.SD

TL;DR: 研究使用基础模型分析音频效果对情感的影响，通过深度学习方法探索音频FX与情感感知的非线性关系


<details>
  <summary>Details</summary>
Motivation: 音频效果在音乐情感塑造中起关键作用，但现有研究主要关注低级音频特征与情感的联系，音频FX对情感的系统性影响尚未充分探索

Method: 应用多种探测方法分析深度学习模型嵌入，研究基础模型编码的音乐结构、音色与情感关联，评估音频基础模型的鲁棒性

Result: 揭示了特定音频效果与情感之间的复杂非线性关系模式

Conclusion: 研究结果有助于推进对音频制作实践感知影响的理解，对音乐认知、表演和情感计算具有重要意义

Abstract: Audio effects (FX) such as reverberation, distortion, modulation, and dynamic
range processing play a pivotal role in shaping emotional responses during
music listening. While prior studies have examined links between low-level
audio features and affective perception, the systematic impact of audio FX on
emotion remains underexplored. This work investigates how foundation models -
large-scale neural architectures pretrained on multimodal data - can be
leveraged to analyze these effects. Such models encode rich associations
between musical structure, timbre, and affective meaning, offering a powerful
framework for probing the emotional consequences of sound design techniques. By
applying various probing methods to embeddings from deep learning models, we
examine the complex, nonlinear relationships between audio FX and estimated
emotion, uncovering patterns tied to specific effects and evaluating the
robustness of foundation audio models. Our findings aim to advance
understanding of the perceptual impact of audio production practices, with
implications for music cognition, performance, and affective computing.

</details>


### [55] [Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation](https://arxiv.org/abs/2509.15210)
*Chen Si,Qianyi Wu,Chaitanya Amballa,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: MiNAF是一种结合显式几何特征的神经隐式声场方法，通过查询粗糙房间网格提取距离分布来提升房间脉冲响应预测的准确性，在有限训练样本下也表现出色


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式方法在声音模拟中未能有效利用环境的显式几何信息，限制了房间脉冲响应(RIR)预测的准确性

Method: 提出Mesh-infused Neural Acoustic Field (MiNAF)，在给定位置查询粗糙房间网格并提取距离分布作为局部上下文的显式表示，将显式几何特征融入神经网络

Result: MiNAF在各种评估指标上与现有方法相比具有竞争力，在有限训练样本的数据集中表现出良好的鲁棒性

Conclusion: 结合显式局部几何特征能更好地指导神经网络生成更准确的RIR预测，推动了高保真声音模拟的发展

Abstract: Realistic sound simulation plays a critical role in many applications. A key
element in sound simulation is the room impulse response (RIR), which
characterizes how sound propagates from a source to a listener within a given
space. Recent studies have applied neural implicit methods to learn RIR using
context information collected from the environment, such as scene images.
However, these approaches do not effectively leverage explicit geometric
information from the environment. To further exploit the potential of neural
implicit models with direct geometric features, we present Mesh-infused Neural
Acoustic Field (MiNAF), which queries a rough room mesh at given locations and
extracts distance distributions as an explicit representation of local context.
Our approach demonstrates that incorporating explicit local geometric features
can better guide the neural network in generating more accurate RIR
predictions. Through comparisons with conventional and state-of-the-art
baseline methods, we show that MiNAF performs competitively across various
evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets
with limited training samples, demonstrating an advance in high-fidelity sound
simulation.

</details>


### [56] [Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation](https://arxiv.org/abs/2509.15222)
*Junhyung Park,Yonghyun Kim,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: 一个集成式网页工具包，包含PiaRec多模态数据同步采集和ASDF指法注释界面，以解决钢琴表演多模态数据获取的困难


<details>
  <summary>Details</summary>
Motivation: 钢琴表演本质上是一种多模态活动，但大规模多模态数据的获取过程很劳务，限制了该领域的进一步发展

Method: 开发了两个图形用户界面：PiaRec（支持音频、视频、MIDI和表演元数据的同步采集）和ASDF（从视觉数据中高效注释执法的工具）

Result: 构建了一个完整的系统，能够流程化多模态钢琴表演数据集的获取过程

Conclusion: 该工具包有效解决了钢琴表演多模态数据获取的困难，为该领域的研究提供了重要的技术支持

Abstract: Piano performance is a multimodal activity that intrinsically combines
physical actions with the acoustic rendition. Despite growing research interest
in analyzing the multimodal nature of piano performance, the laborious process
of acquiring large-scale multimodal data remains a significant bottleneck,
hindering further progress in this field. To overcome this barrier, we present
an integrated web toolkit comprising two graphical user interfaces (GUIs): (i)
PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and
performance metadata. (ii) ASDF, which enables the efficient annotation of
performer fingering from the visual data. Collectively, this system can
streamline the acquisition of multimodal piano performance datasets.

</details>

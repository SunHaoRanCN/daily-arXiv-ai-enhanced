<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [eess.AS](#eess.AS) [Total: 19]
- [cs.SD](#cs.SD) [Total: 18]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [In Planta Tattoo and Kirigami Sensors for Self-Powered Monitoring of Vapor Pressure Deficit and Growth Dynamics](https://arxiv.org/abs/2509.14240)
*Nafize Ishtiaque Hossain,Kundan Saha,Atul Sharma,Sameer Sonkusale*

Main category: eess.SP

TL;DR: 自动力植物传感器平台，集成叶面温湿度传感器和菌形变形菌纳米底底传感器，可持续监测植物水分状况和生长情况。


<details>
  <summary>Details</summary>
Motivation: 为了实现植物水分状况和生长状况的持续、自主监测，提高农业管理效率和应对非生物急逼的能力。

Method: 使用五氧化二锰纳米纳米膜制造叶面温湿度传感器，同时利用湿度发电；采用共液胶基菌形变形菌纳米底底传感器监测茎干径向生长。

Result: 传感器能够持续工作10天以上，温湿度传感器功率密度达到0.1114微瓦/平方厚米，变形传感器测量茎干生长持续20天以上。

Conclusion: 该自动力植物传感器平台具有高效、长时间监测能力，且制造方法简单易扩展，在大规模农业应用中具有广阔前景。

Abstract: We report a scalable, self-powered in planta sensor platform for continuous
monitoring of plant hydration and growth. The system integrates two components
a leaf mounted tattoo sensor for estimating vapor pressure deficit and a
kirigami inspired strain sensor for tracking radial stem growth. Uniquely, the
tattoo sensor serves a dual function measuring temperature and humidity beneath
the leaf surface while simultaneously harvesting power from ambient moisture
via a vanadium pentoxide nanosheet membrane. This moist-electric generator
configuration enables energy-autonomous operation, delivering a power density
of 0.1114 miroW per square cm. The V2O5 based sensor exhibits high sensitivity
to humidity and temperature, enabling accurate VPD estimation for over 10 days
until leaf senescence. The eutectogel based kirigami strain sensor, wrapped
around the stem, offers a gauge factor of 1.5 and immunity to unrelated
mechanical disturbances, allowing continuous growth tracking for more than 20
days. Both sensors are fabricated via cleanroom-free, roll to roll compatible
methods, underscoring their potential for large-scale agricultural deployment
to monitor abiotic stress and improve crop management.

</details>


### [2] [Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes](https://arxiv.org/abs/2509.14242)
*Jinshuai Gu,Zenghui Lin,Jingying Ma,Jingyu Wang,Linyan Zhang,Rui Bai,Zelin Tu,Youyou Jiang,Donglin Xie,Yuxi Zhou,Guoli Liu,Shenda Hong*

Main category: eess.SP

TL;DR: 基于心电图监护(CTG)数据开发人工智能模型CTGage预测胎儿生物学年龄，并用实际年龄与预测年龄的差值(CTGage-gap)作为新的数字生物标记物来预测不良孕期结局。


<details>
  <summary>Details</summary>
Motivation: 目前CTG主要用于识别胎儿当前状态，其在预测未来不良孕期结局方面的潜力未得到充分开发。研究主要目标是开发一种新的非侵入性数字生物标记物来预测不良孕期结局。

Method: 使用61,140份来自11,385名孕妇的CTG记录，采用结构化设计的1D卷积神经网络和分布对齐增强回归技术开发CTGage模型。将CTGage-gap分为五个组别，并比较不同组别中不良结局和母体疾病的发生率。

Result: 模型平均绝对误差为10.91天。高估组早产儿发生率5.33%·vs·正常组1.42%，孕期糖尿病31.93%·vs·正常组20.86%。低估组低体重儿发生率0.17%·vs·正常组0.15%，贫血37.51%·vs·正常组34.74%，均有显著差异。

Conclusion: 人工智能源的CTGage可以预测不良孕期结局的未来风险，有潜力成为一种新的非侵入性且容易获取的数字生物标记物。

Abstract: Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment
technique used globally, especially in underdeveloped countries. However, it is
currently mainly used to identify the fetus's current status (e.g., fetal
acidosis or hypoxia), and the potential of CTG in predicting future adverse
pregnancy outcomes has not been fully explored. We aim to develop an AI-based
model that predicts biological age from CTG time series (named CTGage), then
calculate the age gap between CTGage and actual age (named CTGage-gap), and use
this gap as a new digital biomarker for future adverse pregnancy outcomes. The
CTGage model is developed using 61,140 records from 11,385 pregnant women,
collected at Peking University People's Hospital between 2018 and 2022. For
model training, a structurally designed 1D convolutional neural network is
used, incorporating distribution-aligned augmented regression technology. The
CTGage-gap is categorized into five groups: < -21 days (underestimation group),
-21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days
(overestimation group). We further defined the underestimation group and
overestimation group together as the high-risk group. We then compare the
incidence of adverse outcomes and maternal diseases across these groups. The
average absolute error of the CTGage model is 10.91 days. When comparing the
overestimation group with the normal group, premature infants incidence is
5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is
31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the
normal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and
anaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial
intelligence-derived CTGage can predict the future risk of adverse pregnancy
outcomes and hold potential as a novel, non-invasive, and easily accessible
digital biomarker.

</details>


### [3] [InWaveSR: Topography-Aware Super-Resolution Network for Internal Solitary Waves](https://arxiv.org/abs/2509.14243)
*Xinjie Wang,Zhongrui Li,Peng Han,Chunxin Yuan,Jiexin Xu,Zhiqiang Wei,Jie Nie*

Main category: eess.SP

TL;DR: 提出了InWaveSR模型，一种基于深度学习框架的时空超分辨率方法，用于从低分辨率观测数据生成高分辨率内部孤立波数据，通过物理约束和频域处理提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 观测数据分辨率不足限制了其有效利用，特别是在内部孤立波(ISW)等复杂海洋现象研究中，需要开发能够生成物理一致的高分辨率数据的方法。

Method: 基于深度学习框架，使用原始Navier-Stokes方程作为物理约束；结合注意力机制和快速傅里叶变换的HF-ResBlock组件捕捉高频特征；采用边缘采样和数值预处理方法增强对复杂地形的适应性。

Result: 在实地观测ISW数据评估中，InWaveSR达到了36.2的峰值信噪比(PSNR)，优于传统插值方法和先前神经网络方法。

Conclusion: InWaveSR模型在内部孤立波高分辨率重建方面表现出显著优势，具有优异的性能和可靠性，为海洋观测数据处理提供了有效解决方案。

Abstract: The effective utilization of observational data is frequently hindered by
insufficient resolution. To address this problem, we present a new
spatio-temporal super-resolution (STSR) model, called InWaveSR. It is built on
a deep learning framework with physical restrictions and can efficiently
generate high-resolution data from low-resolution input, especially for data
featuring internal solitary waves (ISWs). To increase generality and
interpretation, the model InWaveSR uses the primitive Navier-Stokes equations
as the constraint, ensuring that the output results are physically consistent.
In addition, the proposed model incorporates an HF-ResBlock component that
combines the attention mechanism and the Fast Fourier Transform (FFT) method to
improve the performance of the model in capturing high-frequency
characteristics. Simultaneously, in order to enhance the adaptability of the
model to complicated bottom topography, an edge sampling and numerical
pre-processing method are carried out to optimize the training process. On
evaluations using the in-situ observational ISW data, the proposed InWaveSR
achieved a peak signal-to-noise ratio (PSNR) score of 36.2, higher than those
of the traditional interpolation method and the previous neural network. This
highlights its significant superiority over traditional methods, demonstrating
its excellent performance and reliability in high-resolution ISW
reconstruction.

</details>


### [4] [Conditional Nearest Level Modulation for Improved Switching Dynamics in Asymmetric Multilevel Converters](https://arxiv.org/abs/2509.14402)
*Jinshui Zhang,Angel V Peterchev,Stefan M Goetz*

Main category: eess.SP

TL;DR: 来文提出了条件最近水平调制(cNLM)方法，通过数学罚次模型调节开关动态，解决不对称多级转换器中过高的切换频率和输出正弦异常问题，显著提升输出质量并降低切换次数。


<details>
  <summary>Details</summary>
Motivation: 不对称多级电路虽然能够按指数增长输出水平数，但传统的最近水平调制(NLM)方法在输出水平数过多时会导致某些模块的过度切换和输出电压射线，影响转换器的性能和可靠性。

Method: 提出条件最近水平调制(cNLM)方法，通过引入数学罚次模型来规制开关动态。还提出了适配特定功能的cNLM变体，如强制最小切换间隔等。

Result: 在不对称多级原型上的实验验证显示，cNLM将总输出失真从66.3%降低到15.1%，同时将切换频率降低到原始NLM的仅8%。

Conclusion: cNLM方法能够有效解决不对称多级转换器中的过高切换频率和输出质量问题，显著提升了转换器的性能和可靠性，在清洁能源、电动汽车等领域具有应用价值。

Abstract: Modular multilevel converters have promising applications in clean energy,
electric vehicles, and biomedical instrumentation, but need many modules to
achieve fine output granularity, particularly of the voltage. Asymmetric
multilevel circuits introduce differences in module voltages so that the
quantity of output levels grows exponentially with the number of modules.
Nearest-level modulation (NLM) is preferred over carrier-based methods in
asymmetric circuits for its simplicity. However, the large number of output
levels can overwhelm NLM and cause excessive transistor switching on some
modules and output voltage spikes. We propose a conditional nearest-level
modulation (cNLM) by incorporating mathematical penalty models to regulate
switching dynamics. This approach improves output quality and reduces switching
rates. Additionally, we present cNLM variations tailored for specific
functions, such as enforcing a minimum switching interval. Experimental
validation on an asymmetric multilevel prototype demonstrates that cNLM reduces
the total output distortion from 66.3% to 15.1% while cutting the switching
rate to just 8% of the original NLM.

</details>


### [5] [Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography](https://arxiv.org/abs/2509.14442)
*Arjun Teh,Wael H. Ali,Joshua Rapp,Hassan Mansour*

Main category: eess.SP

TL;DR: 基于单视点背景旋斗技术和物理信息重建的非入侵式室内流场估计框架


<details>
  <summary>Details</summary>
Motivation: 解决单视点BOS旋斗成像问题的严重不适定性，实现从单个观测点进行净确的室内流场估计

Method: 组合改进光线追踪、物理基础光线渲染技术、物理信息神经网络(PINN)正则化，确保重建流场符合浮力驱动流的控制方程

Result: 开发了一个完整的非入侵式室内流场估计框架，能够通过单个视点观测光线形变来重建体积流场

Conclusion: 该框架有效解决了单视点BOS旋斗的不适定性问题，为非入侵式流场可视化提供了一种准确的方法

Abstract: We develop a framework for non-invasive volumetric indoor airflow estimation
from a single viewpoint using background-oriented schlieren (BOS) measurements
and physics-informed reconstruction. Our framework utilizes a light projector
that projects a pattern onto a target back-wall and a camera that observes
small distortions in the light pattern. While the single-view BOS tomography
problem is severely ill-posed, our proposed framework addresses this using: (1)
improved ray tracing, (2) a physics-based light rendering approach and loss
formulation, and (3) a physics-based regularization using a physics-informed
neural network (PINN) to ensure that the reconstructed airflow is consistent
with the governing equations for buoyancy-driven flows.

</details>


### [6] [Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces](https://arxiv.org/abs/2509.14447)
*Sriram V. C. Nallani,Gautham Ramachandran,Sahil S. Shah*

Main category: eess.SP

TL;DR: 提出一种在线SNN解码器，使用局部三因子学习规则和双时间尺度资格迹，避免BPTT反向传播，在保持竞争性能的同时实现O(1)内存需求。


<details>
  <summary>Details</summary>
Motivation: 脑机接口面临神经信号不稳定性和实时植入应用的内存限制挑战，需要内存高效且持续自适应的神经解码方法。

Method: 结合误差调制Hebbian更新、快/慢迹整合和自适应学习率控制，采用局部三因子学习规则与双时间尺度资格迹。

Result: 在两个灵长类数据集上达到可比较的解码精度（Pearson R≥0.63和R≥0.81），内存减少28-35%，收敛速度比BPTT训练的SNN更快。闭环仿真显示能够适应神经干扰并从零开始学习。

Conclusion: 该方法实现了内存高效、持续自适应的神经解码，适用于资源受限的植入式BCI系统。

Abstract: Brain-Computer Interfaces face challenges from neural signal instability and
memory constraints for real-time implantable applications. We introduce an
online SNN decoder using local three-factor learning rules with dual-timescale
eligibility traces that avoid backpropagation through time while maintaining
competitive performance. Our approach combines error-modulated Hebbian updates,
fast/slow trace consolidation, and adaptive learning rate control, requiring
only O(1) memory versus O(T) for BPTT methods. Evaluations on two primate
datasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R
\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence than
BPTT-trained SNNs. Closed-loop simulations with synthetic neural populations
demonstrate adaptation to neural disruptions and learning from scratch without
offline calibration. This work enables memory-efficient, continuously adaptive
neural decoding suitable for resource-constrained implantable BCI systems.

</details>


### [7] [Secure Blind Graph Signal Recovery and Adversary Detection Using Smoothness Maximization](https://arxiv.org/abs/2509.14449)
*Mahdi Shamsi,Hadi Zayyani,Hasan Abu Hilal,Mohammad Salman*

Main category: eess.SP

TL;DR: 提出一种安全盲图形信号恢复算法，能够在存在测量噪声和恶意节点注入假数据的情况下检测敌对节点并恢复图形信号


<details>
  <summary>Details</summary>
Motivation: 解决图形信号恢复中的安全问题，当未知数量和位置的恶意节点注入假数据时，需要一种能够检测敌对节点并安全恢复信号的算法

Method: 使用基于差分平滑性的统计量来检测恶意节点，通过比较当前观测平滑性与排除对应节点后的平均平滑性的差异。检测后使用Dinkelbach算法求解分数优化问题来实现平滑性最大化的图形信号恢复

Result: 模拟结果显示，该方法在信号恢复性能上显著优于中位数GSR算法和其他竞争方法

Conclusion: 该算法通过有效的恶意节点检测和平滑性最大化恢复，成为解决图形信号恢复中安全挑战问题的理想选择

Abstract: In this letter, we propose a secure blind Graph Signal Recovery (GSR)
algorithm that can detect adversary nodes. Some unknown adversaries are assumed
to be injecting false data at their respective nodes in the graph. The number
and location of adversaries are not known in advance and the goal is to recover
the graph signal in the presence of measurement noise and False Data Injection
(FDI) caused by the adversaries. Consequently, the proposed algorithm would be
a perfect candidate to solve this challenging problem. Moreover, due to the
presence of malicious nodes, the proposed method serves as a secure GSR
algorithm. For adversary detection, a statistical measure based on differential
smoothness is used. Specifically, the difference between the current observed
smoothness and the average smoothness excluding the corresponding node. This
genuine statistical approach leads to an effective and low-complexity adversary
detector. In addition, following malicious node detection, the GSR is performed
using a variant of smoothness maximization, which is solved efficiently as a
fractional optimization problem using a Dinkelbach's algorithm. Analysis of the
detector, which determines the optimum threshold of the detector is also
presented. Simulation results show a significant improvement of the proposed
method in signal recovery compared to the median GSR algorithm and other
competing methods.

</details>


### [8] [Age of Information Aided Intelligent Grant-Free Massive Access for Heterogeneous mMTC Traffic](https://arxiv.org/abs/2509.14503)
*Zhongwen Sun,Wei Chen,Yuxuan Sun,Bo Ai*

Main category: eess.SP

TL;DR: 本文研究非正交免授权随机接入场景下的异构交通流兼容问题，通过基于信息龄的自动编码器方案同时优化告警设备检测率和监控设备信息时效性。


<details>
  <summary>Details</summary>
Motivation: 现有的免授权随机接入研究主要关注用户检测和数据恢复的准确性，而忽视了交通流的异质性。本文考虑了两种不同类型交通流的共存问题：事件触发的告警设备和状态更新的监控设备。

Method: 首先分析基于信息龄的随机接入方案并优化接入参数以最小化监控设备的平均信息龄。然后设计了基于信息龄的先验信息辅助自动编码器(A-PIAAE)，使用学习导频来减少非正交导频之间的干扰。在解码器中提出了利用监控设备信息龄作为先验信息的基于信息龄的学习迭代收缩阈值算法(LISTA-AGE)。

Result: 理论分析证明了所提出的A-PIAAE方案具有更好的收敛性能。实验结果显示该方法在降低监控设备平均信息龄和提高告警设备成功检测率方面具有优势。

Conclusion: 本文提出的方法能够有效处理非正交免授权随机接入场景下的异构交通流问题，同时优化了告警设备的检测性能和监控设备的信息时效性，为6G时代IoT交通流的复杂性和多样性提供了有效解决方案。

Abstract: With the arrival of 6G, the Internet of Things (IoT) traffic is becoming more
and more complex and diverse. To meet the diverse service requirements of IoT
devices, massive machine-type communications (mMTC) becomes a typical scenario,
and more recently, grant-free random access (GF-RA) presents a promising
direction due to its low signaling overhead. However, existing GF-RA research
primarily focuses on improving the accuracy of user detection and data
recovery, without considering the heterogeneity of traffic. In this paper, we
investigate a non-orthogonal GF-RA scenario where two distinct types of traffic
coexist: event-triggered traffic with alarm devices (ADs), and status update
traffic with monitor devices (MDs). The goal is to simultaneously achieve high
detection success rates for ADs and high information timeliness for MDs. First,
we analyze the age-based random access scheme and optimize the access
parameters to minimize the average age of information (AoI) of MDs. Then, we
design an age-based prior information aided autoencoder (A-PIAAE) to jointly
detect active devices, together with learned pilots used in GF-RA to reduce
interference between non-orthogonal pilots. In the decoder, an Age-based
Learned Iterative Shrinkage Thresholding Algorithm (LISTA-AGE) utilizing the
AoI of MDs as the prior information is proposed to enhance active user
detection. Theoretical analysis is provided to demonstrate the proposed A-PIAAE
has better convergence performance. Experiments demonstrate the advantage of
the proposed method in reducing the average AoI of MDs and improving the
successful detection rate of ADs.

</details>


### [9] [Radiolunadiff: Estimation of wireless network signal strength in lunar terrain](https://arxiv.org/abs/2509.14559)
*Paolo Torrado,Anders Pearson,Jason Klein,Alexander Moscibroda,Joshua Smith*

Main category: eess.SP

TL;DR: 提出了一种基于物理信息的深度学习架构，用于预测月球地形上的无线电地图，结合地形生成器和射线追踪技术，使用triplet-UNet架构在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 需要准确预测月球地形上的无线电传播特性，以支持月球探测任务的通信规划，现有深度学习方法在复杂地形环境下表现不足。

Method: 整合基于物理的月球地形生成器（使用NASA数据）和射线追踪引擎创建高保真数据集，采用triplet-UNet架构（两个标准UNet加一个扩散网络）建模复杂传播效应。

Result: 实验结果表明，该方法在月球地形数据集上的各项指标均优于现有的深度学习方法。

Conclusion: 提出的物理信息深度学习架构能够有效预测月球地形上的无线电地图，为月球探测任务的通信系统设计提供了可靠的技术支持。

Abstract: In this paper, we propose a novel physics-informed deep learning architecture
for predicting radio maps over lunar terrain. Our approach integrates a
physics-based lunar terrain generator, which produces realistic topography
informed by publicly available NASA data, with a ray-tracing engine to create a
high-fidelity dataset of radio propagation scenarios. Building on this dataset,
we introduce a triplet-UNet architecture, consisting of two standard UNets and
a diffusion network, to model complex propagation effects. Experimental results
demonstrate that our method outperforms existing deep learning approaches on
our terrain dataset across various metrics.

</details>


### [10] [Task-Oriented Learning for Automatic EEG Denoising](https://arxiv.org/abs/2509.14665)
*Tian-Yu Xiang,Zheng Lei,Xiao-Hu Zhou,Xiao-Liang Xie,Shi-Qi Liu,Mei-Jiang Gui,Hong-Yun Ou,Xin-Zheng Huang,Xin-Yi Fu,Zeng-Guang Hou*

Main category: eess.SP

TL;DR: 一种仅使用任务标签的任务导向学习框架，通过协同优化策略实现自动EEG去噪，无需清洁参考信号


<details>
  <summary>Details</summary>
Motivation: 解决EEG去噪方法依赖人工干预或清洁参考信号的问题，提出一种仅需任务标签的自动去噪方案

Method: 先通过盲源分离技术分解EEG为组件，然后用学习基选择器给每个组件赋予保留概率，最后通过下游代理任务模型的任务损失来监督选择器的协同优化

Result: 在三个数据集上都得到一致收益，任务性能提升2.56%，信噪比提升0.82dB，并且框架算法无关，适用于多种分解技术和网络背榜

Conclusion: 该任务导向学习框架是一种实用的EEG去噪解决方案，对神经科学研究和EEG基交互系统具有潜在影响

Abstract: Electroencephalography (EEG) denoising methods typically depend on manual
intervention or clean reference signals. This work introduces a task-oriented
learning framework for automatic EEG denoising that uses only task labels
without clean reference signals. EEG recordings are first decomposed into
components based on blind source separation (BSS) techniques. Then, a
learning-based selector assigns a retention probability to each component, and
the denoised signal is reconstructed as a probability-weighted combination. A
downstream proxy-task model evaluates the reconstructed signal, with its task
loss supervising the selector in a collaborative optimization scheme that
relies solely on task labels, eliminating the need for clean EEG references.
Experiments on three datasets spanning two paradigms and multiple noise
conditions show consistent gains in both task performance (accuracy:
$2.56\%\uparrow$) and standard signal-quality metrics (signal-to-noise-ratio:
$0.82$\,dB\,$\uparrow$). Further analyses demonstrate that the task-oriented
learning framework is algorithm-agnostic, as it accommodates diverse
decomposition techniques and network backbones for both the selector and the
proxy model. These promising results indicate that the proposed task-oriented
learning framework is a practical EEG denoising solution with potential
implications for neuroscience research and EEG-based interaction systems.

</details>


### [11] [Mitigating the Impact of Location Uncertainty on Radio Map-Based Predictive Rate Selection via Noisy-Input Gaussian Process](https://arxiv.org/abs/2509.14710)
*Koya Sato*

Main category: eess.SP

TL;DR: 基于噪声输入高斯过程(NIGP)的广播地图构建方法，能够在存在位置不确定性的情况下实现更可靠的传输速率选择，提高6G网络通信效率。


<details>
  <summary>Details</summary>
Motivation: 广播地图在6G网络中具有重要价值，但现有方法假设位置信息完美无噪。实际中通过定位系统获取的位置信息带有定位错误，这种位置不确定性会降低广播地图基础的无线系统的可靠性。

Method: 提出了噪声输入高斯过程(NIGP)方法，通过对关注函数进行泰勒近似，将位置噪声视为额外的输出噪声来处理。

Result: 数值实验结果显示，提出的NIGP基础设计比纯粹GP方法实现了更可靠的传输速率选择，并比基于路径损耗的速率选择方案获得了更高的吞吐量。

Conclusion: NIGP方法能够有效减轻位置不确定性对广播地图构建的负面影响，提高了6G网络中通信效率和可靠性，为实际应用中存在定位错误的情况提供了有效解决方案。

Abstract: This paper proposes a predictive rate-selection framework based on Gaussian
process (GP)-based radio map construction that is robust to location
uncertainty. Radio maps are a promising tool for improving communication
efficiency in 6G networks. Although they enable the design of location-based
maximum transmission rates by exploiting statistical channel information,
existing discussions often assume perfect (i.e., noiseless) location
information during channel sensing. Since such information must be obtained
from positioning systems such as global navigation satellite systems, it
inevitably involves positioning errors; this location uncertainty can degrade
the reliability of radio map-based wireless systems. To mitigate this issue, we
introduce the noisy-input GP (NIGP), which treats location noise as additional
output noise by applying a Taylor approximation of the function of interest.
Numerical results demonstrate that the proposed NIGP-based design achieves more
reliable transmission-rate selection than pure GP and yields higher throughput
than path loss-based rate selection.

</details>


### [12] [LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines](https://arxiv.org/abs/2509.14711)
*Ziwei Huang,Shiliang Lu,Lu Bai,Xuesong Cai,Xiang Cheng*

Main category: eess.SP

TL;DR: 基于机器聚合感知(SoM)的大语言模型多路径生成方案(LLM4MG)，利用LLaMA 3.2模型通过多模态感知数据生成6G V2I场景下的高精度多路径信息


<details>
  <summary>Details</summary>
Motivation: 解决6G车联网V2I场景下高精度多路径生成的挑战，利用大语言模型的强大语义理解能力来处理多模态感知数据并生成准确的通信通道特征

Method: 构建SynthSoM-V2I多模态数据集，包含通道多路径、mmWave雷达、RGB-D图像和LiDAR点云数据。基于LLaMA 3.2模型，通过特征提取融合网络将多模态特征空间与语义空间对齐，使用LoRA简化微调和传播觉知提示工程实现知识迁移

Result: 在LoS/NLoS分类中达到92.76%的准确率，多路径功率/延迟生成精度NMSE为0.099/0.032，在跨车辆流量密度、跨带宽和跨场景情况下都显示出良好的泛化性能，通道容量对比验证了高精度多路径生成的必要性

Conclusion: LLM4MG方案成功将大语言模型应用于6G V2I多路径生成，通过多模态感知数据实现了超过传统深度学习方法的性能，为未来智能通信系统设计提供了新的解决方案

Abstract: Based on Synesthesia of Machines (SoM), a large language model (LLM) is
adapted for multipath generation (LLM4MG) for the first time. Considering a
typical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new
multi-modal sensing-communication dataset is constructed, named SynthSoM-V2I,
including channel multipath information, millimeter wave (mmWave) radar sensory
data, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based
on the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model
Meta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The
proposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic
space through feature extraction and fusion networks. To further achieve
general knowledge transfer from the pre-trained LLaMA for multipath generation
via multi-modal sensory data, the low-rank adaptation (LoRA)
parameter-efficient fine-tuning and propagation-aware prompt engineering are
exploited. Simulation results demonstrate that the proposed LLM4MG outperforms
conventional deep learning-based methods in terms of line-of-sight
(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath
power/delay generation precision with normalized mean square error (NMSE) of
0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and
cross-scenario generalization. The utility of the proposed LLM4MG is validated
by real-world generalization. The necessity of high-precision multipath
generation for system design is also demonstrated by channel capacity
comparison.

</details>


### [13] [Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding](https://arxiv.org/abs/2509.14764)
*Yuanyuan Yao,Simon Geirnaert,Tinne Tuytelaars,Alexander Bertrand*

Main category: eess.SP

TL;DR: 这篇论文提出了三种计算效率更高的无监督听视注意力解码方法，解决了现有无偏见方法计算复杂度过高的问题。


<details>
  <summary>Details</summary>
Motivation: 目前的无监督听视注意力解码方法存在初始化偏见或计算复杂度过高的问题，需要发展更高效的方法来消除对标签数据的依赖。

Method: 提出了三种新的计算效率更高的无监督听听觉注意力解码算法，这些算法保持了类似的性能但计算成本更低且固定。

Result: 新方法在保持与现有无偏见方法相似性能的同时，显著降低了计算复杂度，计算成本更低且不随数据量增长而增加。

Conclusion: 这三种新算法为无监督听听觉注意力解码提供了更高效的解决方案，有助于实现更实用的神经导向听力设备。

Abstract: Decoding the attended speaker in a multi-speaker environment from
electroencephalography (EEG) has attracted growing interest in recent years,
with neuro-steered hearing devices as a driver application. Current approaches
typically rely on ground-truth labels of the attended speaker during training,
necessitating calibration sessions for each user and each EEG set-up to achieve
optimal performance. While unsupervised self-adaptive auditory attention
decoding (AAD) for stimulus reconstruction has been developed to eliminate the
need for labeled data, it suffers from an initialization bias that can
compromise performance. Although an unbiased variant has been proposed to
address this limitation, it introduces substantial computational complexity
that scales with data size. This paper presents three computationally efficient
alternatives that achieve comparable performance, but with a significantly
lower and constant computational cost. The code for the proposed algorithms is
available at https://github.com/YYao-42/Unsupervised_AAD.

</details>


### [14] [Comparative Performance Analysis of Different Hybrid NOMA Schemes](https://arxiv.org/abs/2509.14809)
*Ning Wang,Chenyu Zhang,Yanshi Sun,Minghui Min,Shiyin Li*

Main category: eess.SP

TL;DR: 本文分析了三种混合非正交多均访问(H-NOMA)方案在随机频道收益排序条件下的性能，包括固定顺序SIC、非功率适应混合SIC和功率适应混合SIC方案，并与传统OMA进行性能对比。


<details>
  <summary>Details</summary>
Motivation: 现有H-NOMA分析多假设固定频道收益排序，而实际频道系数随机分布导致收益大小关系随机变化，需要在随机频道收益排序条件下分析H-NOMA性能。

Method: 对三种H-NOMA方案（FSIC、HSIC-NPA、HSIC-PA）进行理论分析，求解其性能低于传统OMA的概率的闭式表达式，并在高信噪比治理下得到近似结果。

Result: 模拟结果验证了理论分析，并展示了不同信噪比场景下H-NOMA方案的性能表现。

Conclusion: 研究为下一代无线系统部署H-NOMA提供了理论基础，对比了不同SIC方案在随机频道收益排序条件下的性能差异。

Abstract: Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages
of pure NOMA and conventional OMA organically, has emerged as a highly
promising multiple access technology for future wireless networks. Recent
studies have proposed various H-NOMA systems by employing different successive
interference cancellation (SIC) methods for the NOMA transmission phase.
However, existing analyses typically assume a fixed channel gain order between
paired users, despite the fact that channel coefficients follow random
distribution, leading to their magnitude relationships inherently stochastic
and time varying. This paper analyzes the performance of three H-NOMA schemes
under stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA
scheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;
c) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical
analysis derives closed-form expressions for the probability that H-NOMA
schemes underperform conventional OMA. Asymptotic results in the high
signal-to-noise ratio (SNR) regime are also developed. Simulation results
validate our analysis and demonstrate the performance of H-NOMA schemes across
different SNR scenarios, providing a theoretical foundation for the deployment
of H-NOMA in next-generation wireless systems.

</details>


### [15] [Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization](https://arxiv.org/abs/2509.14836)
*Keitaro Yamashita,Kazuki Naganuma,Shunsuke Ono*

Main category: eess.SP

TL;DR: 本文提出了一种基于广义采样理论的图信号顶点灵活采样方法，通过优化问题设计采样算子，能够控制活动顶点数量并整合先验知识，使用DC优化和双近端梯度算法求解，在多种图信号模型上表现出优越的恢复精度。


<details>
  <summary>Details</summary>
Motivation: 现有顶点灵活采样方法虽然能控制活动顶点数量，但无法整合关于必须采样或禁止采样的顶点的先验知识，限制了实际应用效果。

Method: 将采样算子设计问题转化为带约束的优化问题，使用核范数和DC惩罚处理顶点选择，开发基于双近端梯度DC算法的收敛求解器。

Result: 在多种图信号模型（包括真实数据）上的实验表明，该方法在恢复精度方面优于现有方法。

Conclusion: 所提出的方法能够有效整合先验知识并控制采样顶点数量，在广义采样理论框架下实现了优越的图信号恢复性能。

Abstract: This paper proposes a method for vertex-wise flexible sampling of a broad
class of graph signals, designed to attain the best possible recovery based on
the generalized sampling theory. This is achieved by designing a sampling
operator by an optimization problem, which is inherently non-convex, as the
best possible recovery imposes a rank constraint. An existing method for
vertex-wise flexible sampling is able to control the number of active vertices
but cannot incorporate prior knowledge of mandatory or forbidden vertices. To
address these challenges, we formulate the operator design as a problem that
handles a constraint of the number of active vertices and prior knowledge on
specific vertices for sampling, mandatory inclusion or exclusion. We
transformed this constrained problem into a difference-of-convex (DC)
optimization problem by using the nuclear norm and a DC penalty for vertex
selection. To solve this, we develop a convergent solver based on the general
double-proximal gradient DC algorithm. The effectiveness of our method is
demonstrated through experiments on various graph signal models, including
real-world data, showing superior performance in the recovery accuracy by
comparing to existing methods.

</details>


### [16] [Hybrid Table-Assisted and RL-Based Dynamic Routing for NGSO Satellite Networks](https://arxiv.org/abs/2509.14909)
*Flor Ortiz,Eva Lagunas*

Main category: eess.SP

TL;DR: 这篇论文提出了一种混合动态路由策略，结合了预计算路由表和深度Q学习预备机制，在NGSO卫星网络中实现了更高的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 完全基于强化学习的路由方案虽然能适应拓扑动态性，但存在复杂度高、收敛时间长和重载下性能不稳等问题，需要查找更可缩放的解决方案。

Method: 提出混合框架：在正常条件下使用确定性路由表查找，仅在链路不可用或塞塞时才启动深度Q学习预备机制。

Result: 大规模NGSO网络模拟显示，混合方法在包交付比、端到端延迟、平均跳数和吞吐量方面均明显优于纯强化学习基线方案。

Conclusion: 混合路由策略是一种可缩放且具有弹性的解决方案，特别适用于对延迟敏感的卫星宽带服务。

Abstract: This letter investigates dynamic routing in Next-Generation Satellite Orbit
(NGSO) constellations and proposes a hybrid strategy that combines precomputed
routing tables with a Deep Q-Learning (DQL) fallback mechanism. While fully
RL-based schemes offer adaptability to topology dynamics, they often suffer
from high complexity, long convergence times, and unstable performance under
heavy traffic. In contrast, the proposed framework exploits deterministic table
lookups under nominal conditions and selectively activates the DQL agent only
when links become unavailable or congested. Simulation results in large-scale
NGSO networks show that the hybrid approach consistently achieves higher packet
delivery ratio, lower end-to-end delay, shorter average hop count, and improved
throughput compared to a pure RL baseline. These findings highlight the
effectiveness of hybrid routing as a scalable and resilient solution for
delay-sensitive satellite broadband services

</details>


### [17] [Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators](https://arxiv.org/abs/2509.15069)
*Deijany Rodriguez Linares,Oksana Moryakova,Håkan Johansson*

Main category: eess.SP

TL;DR: 提出一种基于级联累加器的新方法，高效计算时间索引加权和形式，大幅降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 传统直接计算方法需要K×N次乘法，计算成本高，而基于查找表或信号反转的方法需要存储整个数据块

Method: 利用级联累加器的特性，消除了数据存储需求，将乘法计算量降低到仅K+1次常数乘法

Result: 实现了高效的实时计算，特别适用于根据每个样本进行处理的系统

Conclusion: 该方法为时间索引加权和计算提供了一种高效、节省存储的解决方案，适用于大规模数据处理场景

Abstract: This letter presents a novel approach for \mbox{efficiently} computing
time-index powered weighted sums of the form $\sum_{n=0}^{N-1} n^{K} v[n]$
using cascaded accumulators. Traditional direct computation requires
$K{\times}N$ general multiplications, which become prohibitive for large $N$,
while alternative strategies based on lookup tables or signal reversal require
storing entire data blocks. By exploiting accumulator properties, the proposed
method eliminates the need for such storage and reduces the multiplicative cost
to only $K{+}1$ constant multiplications, enabling efficient real-time
implementation. The approach is particularly useful when such sums need to be
efficiently computed in sample-by-sample processing systems.

</details>


### [18] [Doppler Radiance Field-Guided Antenna Selection for Improved Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition](https://arxiv.org/abs/2509.15129)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 提出基于多普勒辐射场(DoRF)的Wi-Fi手势识别框架，通过多天线噪声抑制和天线选择提升泛化能力


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI信号存在AP时钟异步和环境硬件噪声问题，现有预处理方法仍无法有效处理噪声和异常值，限制了基于DoRF的人体活动识别性能

Method: 为多天线AP提出新框架，基于DoRF拟合误差抑制噪声并选择信息量最大的天线，利用多普勒速度投影的不一致性来识别噪声

Result: 在具有挑战性的小规模手势识别数据集上实验表明，该方法显著提高了泛化能力

Conclusion: DoRF引导的Wi-Fi HAR方法为鲁棒的实时感知部署铺平了道路

Abstract: With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard
for advanced sensing, interest in using Wi-Fi Channel State Information (CSI)
for remote sensing has surged. Recent findings indicate that learning a unified
three-dimensional motion representation through Doppler Radiance Fields (DoRFs)
derived from CSI significantly improves the generalization capabilities of
Wi-Fi-based human activity recognition (HAR). Despite this progress, CSI
signals remain affected by asynchronous access point (AP) clocks and additive
noise from environmental and hardware sources. Consequently, even with existing
preprocessing techniques, both the CSI data and Doppler velocity projections
used in DoRFs are still susceptible to noise and outliers, limiting HAR
performance. To address this challenge, we propose a novel framework for
multi-antenna APs to suppress noise and identify the most informative antennas
based on DoRF fitting errors, which capture inconsistencies among Doppler
velocity projections. Experimental results on a challenging small-scale hand
gesture recognition dataset demonstrate that the proposed DoRF-guided
Wi-Fi-based HAR approach significantly improves generalization capability,
paving the way for robust real-world sensing deployments.

</details>


### [19] [A Unified Distributed Algorithm for Hybrid Near-Far Field Activity Detection in Cell-Free Massive MIMO](https://arxiv.org/abs/2509.15162)
*Jingreng Lei,Yang Li,Ziyue Wang,Qingfeng Lin,Ya-Feng Liu,Yik-Chung Wu*

Main category: eess.SP

TL;DR: 本文针对大规模MIMO系统中的活动检测问题，提出了一个协方差建模框架来处理混合近远场信道，并开发了分布式算法来降低计算复杂度和通信开销。


<details>
  <summary>Details</summary>
Motivation: 随着接入点天线数量的增加，瑞利距离扩大使得传统的纯远场传播假设不再实用，需要处理混合近远场信道的活动检测问题。

Method: 建立基于协方差的混合近远场信道统计特性建模，提出分布式算法让每个接入点进行本地活动检测，仅向中央处理单元交换检测结果。

Result: 理论分析表明增加近场信道比例能提升检测性能，仿真结果验证了理论分析并显示所提方法优于现有方法。

Conclusion: 所提出的算法具有收敛保证，是统一的解决方案，能够处理单小区或小区自由系统中的近场或远场设备作为特例。

Abstract: A great amount of endeavor has recently been devoted to activity detection
for massive machine-type communications in cell-free multiple-input
multiple-output (MIMO) systems. However, as the number of antennas at the
access points (APs) increases, the Rayleigh distance that separates the
near-field and far-field regions also expands, rendering the conventional
assumption of far-field propagation alone impractical. To address this
challenge, this paper establishes a covariance-based formulation that can
effectively capture the statistical property of hybrid near-far field channels.
Based on this formulation, we theoretically reveal that increasing the
proportion of near-field channels enhances the detection performance.
Furthermore, we propose a distributed algorithm, where each AP performs local
activity detection and only exchanges the detection results to the central
processing unit, thus significantly reducing the computational complexity and
the communication overhead. Not only with convergence guarantee, the proposed
algorithm is unified in the sense that it can handle single-cell or cell-free
systems with either near-field or far-field devices as special cases.
Simulation results validate the theoretical analyses and demonstrate the
superior performance of the proposed approach compared with existing methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [20] [SpeechOp: Inference-Time Task Composition for Generative Speech Processing](https://arxiv.org/abs/2509.14298)
*Justin Lovelace,Rithesh Kumar,Jiaqi Su,Ke Chen,Kilian Q Weinberger,Zeyu Jin*

Main category: eess.AS

TL;DR: SpeechOp是一个基于预训练TTS模型的多任务潜在扩散模型，可将TTS转换为通用语音处理器，支持多种语音任务组合，并通过隐式任务组合实现最先进的内容保持性能


<details>
  <summary>Details</summary>
Motivation: 解决语音增强等语音到语音处理任务面临的数据限制问题，避免生成式方法扭曲语音内容和说话人身份

Method: 通过适配预训练TTS模型构建多任务潜在扩散模型，引入隐式任务组合(ITC)管道，利用ASR转录本指导增强过程

Result: 继承了丰富的自然语音理解能力，加速训练并提高S2S任务质量，同时增强核心TTS性能，实现最先进的内容保持

Conclusion: SpeechOp成功将预训练TTS模型转化为通用语音处理器，通过ITC实现了网络规模语音理解与生成能力的稳健结合

Abstract: While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild"
data to achieve remarkable success, speech-to-speech processing tasks like
enhancement face data limitations, which lead data-hungry generative approaches
to distort speech content and speaker identity. To bridge this gap, we present
SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS
models into a universal speech processor capable of performing a wide range of
speech tasks and composing them in novel ways at inference time. By adapting a
pre-trained TTS model, SpeechOp inherits a rich understanding of natural
speech, accelerating training and improving S2S task quality, while
simultaneously enhancing core TTS performance. Finally, we introduce Implicit
Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g.,
from Whisper) guide SpeechOp's enhancement via our principled inference-time
task composition. ITC achieves state-of-the-art content preservation by
robustly combining web-scale speech understanding with SpeechOp's generative
capabilities. Audio samples are available at
https://justinlovelace.github.io/projects/speechop

</details>


### [21] [Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior](https://arxiv.org/abs/2509.14379)
*Yochai Yemini,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya*

Main category: eess.AS

TL;DR: 提出了一种基于生成式无监督学习的单麦克风语音分离方法，通过直接建模纯净语音和结构化噪声分量，在嘈杂环境中实现有效的语音分离


<details>
  <summary>Details</summary>
Motivation: 解决在环境噪声存在下的单麦克风语音分离问题，传统方法需要大量带噪混合数据进行训练，而本文旨在通过直接建模噪声分布来提升分离性能

Method: 使用音频-视觉评分模型，结合视觉线索作为强生成式语音先验；通过反向扩散过程从后验分布中采样，直接估计并移除建模的噪声分量来恢复纯净信号

Result: 实验结果表明该方法在具有挑战性的声学环境中表现出良好的性能，验证了直接噪声建模方法的有效性

Conclusion: 提出的生成式无监督技术通过直接建模噪声分布，为单麦克风语音分离提供了一种有效的解决方案，特别是在复杂声学环境下表现出优越性能

Abstract: In this paper, we address the problem of single-microphone speech separation
in the presence of ambient noise. We propose a generative unsupervised
technique that directly models both clean speech and structured noise
components, training exclusively on these individual signals rather than noisy
mixtures. Our approach leverages an audio-visual score model that incorporates
visual cues to serve as a strong generative speech prior. By explicitly
modelling the noise distribution alongside the speech distribution, we enable
effective decomposition through the inverse problem paradigm. We perform speech
separation by sampling from the posterior distributions via a reverse diffusion
process, which directly estimates and removes the modelled noise component to
recover clean constituent signals. Experimental results demonstrate promising
performance, highlighting the effectiveness of our direct noise modelling
approach in challenging acoustic environments.

</details>


### [22] [Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses](https://arxiv.org/abs/2509.14430)
*Yufeng Yang,Yiteng Huang,Yong Xu,Li Wan,Suwon Shon,Yang Liu,Yifeng Fan,Zhaojun Yang,Olivier Siohan,Yue Liu,Ming Sun,Florian Metze*

Main category: eess.AS

TL;DR: 一种用于智能眼镜的多通道差分语音识别方法，通过组合调期器、麦克风选择和边话检测模型，有效应对现实环境中的边诚干扰问题


<details>
  <summary>Details</summary>
Motivation: 随着智能眼镜等可戴设备的普及，戴者语音识别变得越来越重要，但现实环境中的边诚干扰仍是严重挑战

Method: 提出新的多通道差分自动语音识别方法，采用互补的前端处理技术包括调期器、麦克风选择和轻量级边诚检测模型

Result: 在模拟和真实数据集上评估显示，该系统超越传统方法，实现了词语错误率相对降低18.0%

Conclusion: 该方法能够有效提高智能眼镜上戴者语音识别的稳健性，应对现实环境中的边诚干扰挑战

Abstract: With the growing adoption of wearable devices such as smart glasses for AI
assistants, wearer speech recognition (WSR) is becoming increasingly critical
to next-generation human-computer interfaces. However, in real environments,
interference from side-talk speech remains a significant challenge to WSR and
may cause accumulated errors for downstream tasks such as natural language
processing. In this work, we introduce a novel multi-channel differential
automatic speech recognition (ASR) method for robust WSR on smart glasses. The
proposed system takes differential inputs from different frontends that
complement each other to improve the robustness of WSR, including a beamformer,
microphone selection, and a lightweight side-talk detection model. Evaluations
on both simulated and real datasets demonstrate that the proposed system
outperforms the traditional approach, achieving up to an 18.0% relative
reduction in word error rate.

</details>


### [23] [Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation](https://arxiv.org/abs/2509.14632)
*Miseul Kim,Soo Jin Park,Kyungguen Byun,Hyeon-Kyeong Shin,Sunkuk Moon,Shuhua Zhang,Erik Visser*

Main category: eess.AS

TL;DR: 通过风格可控语音生成模型增强讲者识别的驱动性，有效减少讲者识别系统因语音风格变化而产生的误分类


<details>
  <summary>Details</summary>
Motivation: 解决讲者识别系统在高内部讲者变异性（如情感、健康、内容变化）时容易将同一讲者的语音段落误分为不同个体的问题

Method: 使用风格可控语音生成模型，在保持目标讲者身份的前提下生成具有语音和风格多样性的增强语音样本，然后混合原始和生成音频的讲者嵌入表示来提高系统的驱动性

Result: 在模拟情感语音数据集和被截断的AMI数据集上验证，误差率分别减少49%和35%

Conclusion: 风格可控语音生成模型能够有效地增强讲者识别系统的驱动性，明显提升了在高内部讲者变异性情况下的性能

Abstract: Speaker diarization systems often struggle with high intrinsic intra-speaker
variability, such as shifts in emotion, health, or content. This can cause
segments from the same speaker to be misclassified as different individuals,
for example, when one raises their voice or speaks faster during conversation.
To address this, we propose a style-controllable speech generation model that
augments speech across diverse styles while preserving the target speaker's
identity. The proposed system starts with diarized segments from a conventional
diarizer. For each diarized segment, it generates augmented speech samples
enriched with phonetic and stylistic diversity. And then, speaker embeddings
from both the original and generated audio are blended to enhance the system's
robustness in grouping segments with high intrinsic intra-speaker variability.
We validate our approach on a simulated emotional speech dataset and the
truncated AMI dataset, demonstrating significant improvements, with error rate
reductions of 49% and 35% on each dataset, respectively.

</details>


### [24] [Enhancing Situational Awareness in Wearable Audio Devices Using a Lightweight Sound Event Localization and Detection System](https://arxiv.org/abs/2509.14650)
*Jun-Wei Yeow,Ee-Leng Tan,Santi Peksi,Zhen-Ting Ong,Woon-Seng Gan*

Main category: eess.AS

TL;DR: 通过结合音响场景分类(ASC)和声音事件定位检测(SELD)的环境智能框架，提升可穿戴音频设备的情境感知能力，解决ANC导致的环境感知丢失问题。


<details>
  <summary>Details</summary>
Motivation: 主动噪声控制(ANC)可穿戴音频设备虽提升听觉舒适性，但宽帮隔绝了环境声音线索，可能浪蓄重要的环境印证，构成安全风险。

Method: 提出一种环境智能框架，首先使用轻量级ASC模型推断当前环境，然后根据场景预测动态调整SELD网络的敏感度，优化对当前上下文最关键声音的检测和定位能力。

Result: 在模拟耳机数据上，ASC条件化的SELD系统比传统基线系统显示出更好的空间智能性能。

Conclusion: 这项工作是向智能可穿戴音频设备发展的关锠步骤，能够传递重要的环境信息，为用户提供更安全、更具情境感知能力的听觉体验。

Abstract: Wearable audio devices with active noise control (ANC) enhance listening
comfort but often at the expense of situational awareness. However, this
auditory isolation may mask crucial environmental cues, posing significant
safety risks. To address this, we propose an environmental intelligence
framework that combines Acoustic Scene Classification (ASC) with Sound Event
Localization and Detection (SELD). Our system first employs a lightweight ASC
model to infer the current environment. The scene prediction then dynamically
conditions a SELD network, tuning its sensitivity to detect and localize sounds
that are most salient to the current context. On simulated headphone data, the
proposed ASC-conditioned SELD system demonstrates improved spatial intelligence
over a conventional baseline. This work represents a crucial step towards
creating intelligent hearables that can deliver crucial environmental
information, fostering a safer and more context-aware listening experience.

</details>


### [25] [Aligning Audio Captions with Human Preferences](https://arxiv.org/abs/2509.14659)
*Kartik Hegde,Rehana Mahfuz,Yinyi Guo,Erik Visser*

Main category: eess.AS

TL;DR: 基于RLHF的音频描述框架，通过对比预训练奖励模型和强化学习，在不依赖标注数据的情况下实现了与人类偏好对齐的音频描述生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统音频描述系统对成对音频-描述数据集的依赖问题，这些数据集成本高且可能不能反映实际场景中的人类偏好。

Method: 训练基于CLAP的对比奖励模型，使用人类标注的成对偏好数据。将奖励模型集成到强化学习框架中，微调任意基线描述系统而不依赖真实标注。

Result: 多个数据集的人类评估显示，该方法生成的描述更受偏好，尤其在基线模型失败的情况下。继续性能可与使用真实标注的监督方法相比。

Conclusion: 该框架有效实现了音频描述与人类偏好的对齐，并在实际场景中具有良好的扩展性。

Abstract: Current audio captioning systems rely heavily on supervised learning with
paired audio-caption datasets, which are expensive to curate and may not
reflect human preferences in real-world scenarios. To address this limitation,
we propose a preference-aligned audio captioning framework based on
Reinforcement Learning from Human Feedback (RLHF). To effectively capture
nuanced human preferences, we train a Contrastive Language-Audio Pretraining
(CLAP)-based reward model using human-labeled pairwise preference data. This
reward model is integrated into a reinforcement learning framework to fine-tune
any baseline captioning system without relying on ground-truth caption
annotations. Extensive human evaluations across multiple datasets show that our
method produces captions preferred over those from baseline models,
particularly in cases where the baseline models fail to provide correct and
natural captions. Furthermore, our framework achieves performance comparable to
supervised approaches with ground-truth data, demonstrating its effectiveness
in aligning audio captioning with human preferences and its scalability in
real-world scenarios.

</details>


### [26] [SpeechMLC: Speech Multi-label Classification](https://arxiv.org/abs/2509.14677)
*Miseul Kim,Seyun Um,Hyeonjin Cha,Hong-goo Kang*

Main category: eess.AS

TL;DR: 提出了一种多标签分类框架，用于检测语音样本中的多种说话风格，通过交叉注意力机制和数据增强技术解决多标签语音数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注识别单一目标风格，而本文旨在开发能够有效捕捉多种说话者特征的统一框架，以适应广义的人机交互应用需求。

Method: 在transformer解码器中集成交叉注意力机制来提取与每个目标标签相关的显著特征，并采用基于语音生成模型的数据增强技术来缓解数据不平衡问题。

Result: 通过在已见和未见语料库上的多项目标评估验证了模型的有效性，并分析了人类标注一致性对模型性能的影响。

Conclusion: 该框架能够有效检测多种说话风格，为广义人机交互应用提供了实用的解决方案，同时揭示了人类感知对分类准确性的重要影响。

Abstract: In this paper, we propose a multi-label classification framework to detect
multiple speaking styles in a speech sample. Unlike previous studies that have
primarily focused on identifying a single target style, our framework
effectively captures various speaker characteristics within a unified
structure, making it suitable for generalized human-computer interaction
applications. The proposed framework integrates cross-attention mechanisms
within a transformer decoder to extract salient features associated with each
target label from the input speech. To mitigate the data imbalance inherent in
multi-label speech datasets, we employ a data augmentation technique based on a
speech generation model. We validate our model's effectiveness through multiple
objective evaluations on seen and unseen corpora. In addition, we provide an
analysis of the influence of human perception on classification accuracy by
considering the impact of human labeling agreement on model performance.

</details>


### [27] [DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis](https://arxiv.org/abs/2509.14684)
*Ye-Xin Lu,Yu Gu,Kun Wei,Hui-Peng Du,Yang Ai,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: DAIEN-TTS是一个零样本文本转语音框架，通过解耦音频填充实现环境感知合成，可独立控制音色和背景环境。


<details>
  <summary>Details</summary>
Motivation: 传统TTS系统难以同时控制语音的音色特征和背景环境特征，需要开发能够独立控制这两个维度的合成方法。

Method: 基于F5-TTS构建，使用预训练语音环境分离模块解耦环境语音，应用随机跨度掩码，采用双无分类引导和信噪比自适应策略。

Result: 实验结果表明DAIEN-TTS能够生成具有高自然度、强说话人相似性和高环境保真度的环境个性化语音。

Conclusion: 该框架成功实现了对环境语音的零样本合成，提供了对说话人音色和背景环境的独立控制能力。

Abstract: This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework
that enables ENvironment-aware synthesis through Disentangled Audio Infilling.
By leveraging separate speaker and environment prompts, DAIEN-TTS allows
independent control over the timbre and the background environment of the
synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first
incorporates a pretrained speech-environment separation (SES) module to
disentangle the environmental speech into mel-spectrograms of clean speech and
environment audio. Two random span masks of varying lengths are then applied to
both mel-spectrograms, which, together with the text embedding, serve as
conditions for infilling the masked environmental mel-spectrogram, enabling the
simultaneous continuation of personalized speech and time-varying environmental
audio. To further enhance controllability during inference, we adopt dual
class-free guidance (DCFG) for the speech and environment components and
introduce a signal-to-noise ratio (SNR) adaptation strategy to align the
synthesized speech with the environment prompt. Experimental results
demonstrate that DAIEN-TTS generates environmental personalized speech with
high naturalness, strong speaker similarity, and high environmental fidelity.

</details>


### [28] [MELA-TTS: Joint transformer-diffusion model with representation alignment for speech synthesis](https://arxiv.org/abs/2509.14784)
*Keyu An,Zhiyu Zhang,Changfeng Gao,Yabin Li,Zhendong Peng,Haoxu Wang,Zhihao Du,Han Zhao,Zhifu Gao,Xiangang Li*

Main category: eess.AS

TL;DR: MELA-TTS是一个新颖的联合transformer-diffusion框架，用于端到端文本转语音合成，通过自回归生成连续mel频谱图，无需语音标记化和多阶段处理流程。


<details>
  <summary>Details</summary>
Motivation: 解决连续特征建模的固有困难，消除对语音标记化和多阶段处理管道的需求，提供更好的跨模态一致性。

Method: 提出表示对齐模块，在训练期间将transformer解码器的输出表示与预训练ASR编码器的语义嵌入对齐，使用联合transformer-diffusion框架进行端到端合成。

Result: 在多个评估指标上达到最先进性能，保持强大的零样本语音克隆能力，支持离线和流式合成模式。

Conclusion: 为TTS中的连续特征生成方法建立了新基准，提供了离散标记范式的有吸引力的替代方案。

Abstract: This work introduces MELA-TTS, a novel joint transformer-diffusion framework
for end-to-end text-to-speech synthesis. By autoregressively generating
continuous mel-spectrogram frames from linguistic and speaker conditions, our
architecture eliminates the need for speech tokenization and multi-stage
processing pipelines. To address the inherent difficulties of modeling
continuous features, we propose a representation alignment module that aligns
output representations of the transformer decoder with semantic embeddings from
a pretrained ASR encoder during training. This mechanism not only speeds up
training convergence, but also enhances cross-modal coherence between the
textual and acoustic domains. Comprehensive experiments demonstrate that
MELA-TTS achieves state-of-the-art performance across multiple evaluation
metrics while maintaining robust zero-shot voice cloning capabilities, in both
offline and streaming synthesis modes. Our results establish a new benchmark
for continuous feature generation approaches in TTS, offering a compelling
alternative to discrete-token-based paradigms.

</details>


### [29] [Acoustic Simulation Framework for Multi-channel Replay Speech Detection](https://arxiv.org/abs/2509.14789)
*Michael Neri,Tuomas Virtanen*

Main category: eess.AS

TL;DR: 通过音响模拟框架生成多通道重放攻击语音数据，提升语音助手的防重放攻击检测能力


<details>
  <summary>Details</summary>
Motivation: 重放语音攻击对语音控制系统构成严重威胁，而现有方法主要基于单通道数据，多通道音频可提供空间线索以增强检测稳健性

Method: 开发音响模拟框架，模拟真实和欺骗语音，包括麦克风和扬声器冲击响应、房间音响和噪声条件，使用测量的扬声器方向性提高模拟真实性

Result: 使用M-ALRAD模型评估，证明合成数据可以支持检测器在未见环境中的泛化能力

Conclusion: 该模拟框架能够生成高质量的多通道重放攻击数据，有助于提升语音助手防欺骗的性能和适应性

Abstract: Replay speech attacks pose a significant threat to voice-controlled systems,
especially in smart environments where voice assistants are widely deployed.
While multi-channel audio offers spatial cues that can enhance replay detection
robustness, existing datasets and methods predominantly rely on single-channel
recordings. In this work, we introduce an acoustic simulation framework
designed to simulate multi-channel replay speech configurations using publicly
available resources. Our setup models both genuine and spoofed speech across
varied environments, including realistic microphone and loudspeaker impulse
responses, room acoustics, and noise conditions. The framework employs measured
loudspeaker directionalities during the replay attack to improve the realism of
the simulation. We define two spoofing settings, which simulate whether a
reverberant or an anechoic speech is used in the replay scenario, and evaluate
the impact of omnidirectional and diffuse noise on detection performance. Using
the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate
that synthetic data can support the generalization capabilities of the detector
across unseen enclosures.

</details>


### [30] [AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding and Dropout-Based Learning](https://arxiv.org/abs/2509.14855)
*Michael Tatarjitzky,Boaz Rafaely*

Main category: eess.AS

TL;DR: AmbiDrop是一个基于Ambisonics的阵列无关语音增强框架，通过球谐域编码和通道dropout技术，无需多样麦克风阵列数据库即可实现良好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决传统多通道语音增强方法对特定麦克风阵列几何形状的依赖问题，以及现有阵列无关方法在未见布局上泛化能力不足的局限性

Method: 使用Ambisonics信号匹配将任意阵列录音编码到球谐域，在模拟的Ambisonics数据上训练深度神经网络，并结合通道dropout技术来增强对阵列相关编码错误的鲁棒性

Result: 在未见阵列上，AmbiDrop在SI-SDR、PESQ和STOI指标上持续提升，而基线方法性能下降，证明了强大的泛化能力

Conclusion: AmbiDrop框架通过Ambisonics编码和dropout技术，有效实现了阵列无关的语音增强，具有实际应用潜力

Abstract: Multichannel speech enhancement leverages spatial cues to improve
intelligibility and quality, but most learning-based methods rely on specific
microphone array geometry, unable to account for geometry changes. To mitigate
this limitation, current array-agnostic approaches employ large multi-geometry
datasets but may still fail to generalize to unseen layouts. We propose
AmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes
arbitrary array recordings into the spherical harmonics domain using Ambisonics
Signal Matching (ASM). A deep neural network is trained on simulated Ambisonics
data, combined with channel dropout for robustness against array-dependent
encoding errors, therefore omitting the need for a diverse microphone array
database. Experiments show that while the baseline and proposed models perform
similarly on the training arrays, the baseline degrades on unseen arrays. In
contrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating
strong generalization and practical potential for array-agnostic speech
enhancement.

</details>


### [31] [Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance](https://arxiv.org/abs/2509.14934)
*Francisco Messina,Francesca Ronchini,Luca Comanducci,Paolo Bestagini,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本研究针对文本到音频扩散模型中的数据复制问题，采用抗记忆化引导(AMG)技术来减少训练数据记忆，在保持生成质量的同时显著降低记忆化现象。


<details>
  <summary>Details</summary>
Motivation: 生成音频模型中存在数据复制问题，模型在推理时会无意中生成训练数据片段，这需要有效的抗记忆化策略来解决。

Method: 采用抗记忆化引导(AMG)技术，修改预训练扩散模型的采样过程来抑制记忆化。探索了三种不同的引导方式，使用Stable Audio Open作为基础架构。

Result: 实验分析表明，AMG能够显著减轻基于扩散的文本到音频生成中的记忆化现象，同时不损害音频保真度或语义对齐。

Conclusion: AMG是一种有效的技术，可以在保持生成质量的同时解决扩散音频模型中的数据复制问题。

Abstract: A persistent challenge in generative audio models is data replication, where
the model unintentionally generates parts of its training data during
inference. In this work, we address this issue in text-to-audio diffusion
models by exploring the use of anti-memorization strategies. We adopt
Anti-Memorization Guidance (AMG), a technique that modifies the sampling
process of pre-trained diffusion models to discourage memorization. Our study
explores three types of guidance within AMG, each designed to reduce
replication while preserving generation quality. We use Stable Audio Open as
our backbone, leveraging its fully open-source architecture and training
dataset. Our comprehensive experimental analysis suggests that AMG
significantly mitigates memorization in diffusion-based text-to-audio
generation without compromising audio fidelity or semantic alignment.

</details>


### [32] [SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding](https://arxiv.org/abs/2509.14946)
*Bingsong Bai,Qihang Lu,Wenbing Yang,Zihan Sun,YueRan Hou,Peilei Jia,Songbai Pu,Ruibo Fu,Yingming Gao,Ya Li,Jun Gao*

Main category: eess.AS

TL;DR: 自动化框架生成大规模语调声音数据集SynParaSpeech，包含6个类别118.75小时数据，具有精确时间戳和自然对话特性


<details>
  <summary>Details</summary>
Motivation: 解决现有语调声音数据集存在的问题：依赖专有数据集、语音不完整、时间戳不准确或缺失、实际应用价值有限

Method: 提出自动化框架，从自然对话语音中生成大规模语调声音数据，构建SynParaSpeech数据集

Result: 成功构建包含6个语调声音类别、118.75小时数据量的数据集，具有精确时间戳和高实际应用价值

Conclusion: 该研究首次提出自动化构建大规模语调数据集的方法，SynParaSpeech数据集能够提升语调声音合成的自然性和语调事件检测的准确性

Abstract: Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing
more realistic and engaging speech. However, existing methods typically depend
on proprietary datasets, while publicly available resources often suffer from
incomplete speech, inaccurate or missing timestamps, and limited real-world
relevance. To address these problems, we propose an automated framework for
generating large-scale paralinguistic data and apply it to construct the
SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with
118.75 hours of data and precise timestamps, all derived from natural
conversational speech. Our contributions lie in introducing the first automated
method for constructing large-scale paralinguistic datasets and releasing the
SynParaSpeech corpus, which advances speech generation through more natural
paralinguistic synthesis and enhances speech understanding by improving
paralinguistic event detection. The dataset and audio samples are available at
https://github.com/ShawnPi233/SynParaSpeech.

</details>


### [33] [Discrete optimal transport is a strong audio adversarial attack](https://arxiv.org/abs/2509.14959)
*Anton Selitskiy,Akib Shahriyar,Jishnuraj Prakasan*

Main category: eess.AS

TL;DR: 离散最优传输(DOT)是一种有效的黑盒对抗攻击方法，通过分布对齐方式攻击音频反欺骗系统，在多个数据集上表现稳定且优于传统攻击方法


<details>
  <summary>Details</summary>
Motivation: 现代音频反欺骗系统容易受到对抗攻击，需要开发更有效的攻击方法来评估系统安全性

Method: 使用熵最优传输和top-k重心投影将生成语音的WavLM嵌入与真实语音池对齐，然后通过神经声码器解码

Result: 在ASVspoof2019和ASVspoof5数据集上，DOT攻击始终获得高错误率，在跨数据集迁移和微调后仍保持竞争力

Conclusion: 分布级对齐是部署的反欺骗系统的一个强大而稳定的攻击面

Abstract: In this paper, we show that discrete optimal transport (DOT) is an effective
black-box adversarial attack against modern audio anti-spoofing countermeasures
(CMs). Our attack operates as a post-processing, distribution-alignment step:
frame-level WavLM embeddings of generated speech are aligned to an unpaired
bona fide pool via entropic OT and a top-$k$ barycentric projection, then
decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with
AASIST baselines, DOT yields consistently high equal error rate (EER) across
datasets and remains competitive after CM fine-tuning, outperforming several
conventional attacks in cross-dataset transfer. Ablation analysis highlights
the practical impact of vocoder overlap. Results indicate that
distribution-level alignment is a powerful and stable attack surface for
deployed CMs.

</details>


### [34] [BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings](https://arxiv.org/abs/2509.15001)
*Théo Charlot,Tarek Kunze,Maxime Poli,Alejandrina Cristia,Emmanuel Dupoux,Marvin Lavechin*

Main category: eess.AS

TL;DR: BabyHuBERT是首个基于13,000小时多语言儿童语音数据训练的自监督语音表示模型，在儿童语音识别任务上显著优于现有模型


<details>
  <summary>Details</summary>
Motivation: 现有语音模型基于成人清晰语音训练，在儿童语音数据上表现不佳，需要专门针对儿童语音特点的模型

Method: 使用13,000小时多语言儿童长时录音训练自监督语音表示模型，覆盖40多种语言

Result: 在6个数据集上F1分数达52.1%-74.4%，比W2V2-LL4300和标准HuBERT表现更好，在Vanuatu和Solomon Islands语料上分别提升13.2和15.9个F1点

Conclusion: BabyHuBERT为儿童语音研究提供了基础模型，可微调用于多种下游任务，特别对 underrepresented languages 有显著效果

Abstract: Child-centered long-form recordings are essential for studying early language
development, but existing speech models trained on clean adult data perform
poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the
first self-supervised speech representation model trained on 13,000 hours of
multilingual child-centered long-form recordings spanning over 40 languages. We
evaluate BabyHuBERT on speaker segmentation, identifying when target children
speak versus female adults, male adults, or other children -- a fundamental
preprocessing step for analyzing naturalistic language experiences. BabyHuBERT
achieves F1-scores from 52.1% to 74.4% across six diverse datasets,
consistently outperforming W2V2-LL4300 (trained on English long-forms) and
standard HuBERT (trained on clean adult speech). Notable improvements include
13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon
Islands corpora, demonstrating effectiveness on underrepresented languages. By
sharing code and models, BabyHuBERT serves as a foundation model for child
speech research, enabling fine-tuning on diverse downstream tasks.

</details>


### [35] [Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models](https://arxiv.org/abs/2509.15008)
*Chaoyue Niu,Veronica Rowe,Guy J. Brown,Heather Elphick,Heather Kenyon,Lowri Thomas,Sam Johnson,Ning Ma*

Main category: eess.AS

TL;DR: 提出基于迁移学习的声学监测框架，利用成人睡眠数据预训练模型，结合SpO2去饱和模式，有效提升儿童阻塞性睡眠呼吸暂停的家庭筛查准确率


<details>
  <summary>Details</summary>
Motivation: 儿童阻塞性睡眠呼吸暂停(OSA)临床意义重大但诊断困难，传统多导睡眠图儿童耐受性差，声学监测作为无创替代方案缺乏足够儿科数据支持深度学习模型开发

Method: 采用迁移学习框架，将成人睡眠数据预训练的声学模型适配到儿科OSA检测，整合SpO2去饱和模式增强训练；系统评估单任务vs多任务学习、编码器冻结vs全微调、SpO2标签延迟对齐声学信号

Result: 实验使用157晚成人数据和15晚儿科数据，结果显示SpO2整合的微调方法相比无适配基线模型持续改善儿科OSA检测性能

Conclusion: 研究证明了迁移学习在儿童家庭OSA筛查中的可行性，展现了其在早期诊断中的潜在临床价值

Abstract: Paediatric obstructive sleep apnoea (OSA) is clinically significant yet
difficult to diagnose, as children poorly tolerate sensor-based
polysomnography. Acoustic monitoring provides a non-invasive alternative for
home-based OSA screening, but limited paediatric data hinders the development
of robust deep learning approaches. This paper proposes a transfer learning
framework that adapts acoustic models pretrained on adult sleep data to
paediatric OSA detection, incorporating SpO2-based desaturation patterns to
enhance model training. Using a large adult sleep dataset (157 nights) and a
smaller paediatric dataset (15 nights), we systematically evaluate (i) single-
versus multi-task learning, (ii) encoder freezing versus full fine-tuning, and
(iii) the impact of delaying SpO2 labels to better align them with the
acoustics and capture physiologically meaningful features. Results show that
fine-tuning with SpO2 integration consistently improves paediatric OSA
detection compared with baseline models without adaptation. These findings
demonstrate the feasibility of transfer learning for home-based OSA screening
in children and offer its potential clinical value for early diagnosis.

</details>


### [36] [From Who Said What to Who They Are: Modular Training-free Identity-Aware LLM Refinement of Speaker Diarization](https://arxiv.org/abs/2509.15082)
*Yu-Wen Chen,William Ho,Maxim Topaz,Julia Hirschberg,Zoran Kostic*

Main category: eess.AS

TL;DR: 一种无需训练的模块化管线方案，结合现成SD、ASR和大语言模型，通过结构化提示利用会话语义上下文来提升讲话人分离精度和识别真实讲话人身份


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中讲话人分离遇到的动态环境、未知讲话人数问题，以及传统方法不能提供真实讲话人身份的限制

Method: 使用现成的SD和ASR组件，通过结构化的LLM提示处理协调后的输出，利用会话语义连续性来精炼低信心度讲话人标签、分配角色身份并修正分裂讲话人

Result: 在真实医患数据集上，相比基线方法相对错误减少29.7%，在不需额外训练的情况下显著提升了讲话人分离性能

Conclusion: 该方法提供了一个完整的实用解决方案，能够同时处理讲话人分离、语音识别和讲话人身份识别，适用于实际应用场景

Abstract: Speaker diarization (SD) struggles in real-world scenarios due to dynamic
environments and unknown speaker counts. SD is rarely used alone and is often
paired with automatic speech recognition (ASR), but non-modular methods that
jointly train on domain-specific data have limited flexibility. Moreover, many
applications require true speaker identities rather than SD's pseudo labels. We
propose a training-free modular pipeline combining off-the-shelf SD, ASR, and a
large language model (LLM) to determine who spoke, what was said, and who they
are. Using structured LLM prompting on reconciled SD and ASR outputs, our
method leverages semantic continuity in conversational context to refine
low-confidence speaker labels and assigns role identities while correcting
split speakers. On a real-world patient-clinician dataset, our approach
achieves a 29.7% relative error reduction over baseline reconciled SD and ASR.
It enhances diarization performance without additional training and delivers a
complete pipeline for SD, ASR, and speaker identity detection in practical
applications.

</details>


### [37] [Real-Time Streaming Mel Vocoding with Generative Flow Matching](https://arxiv.org/abs/2509.15085)
*Simon Welker,Tal Peer,Timo Gerkmann*

Main category: eess.AS

TL;DR: MelFlow是一个基于流匹配和STFT相位检索的流式Mel声码器，具有极低延迟（48ms），在消费级GPU上实现实时流式处理，性能优于HiFi-GAN等非流式基线模型。


<details>
  <summary>Details</summary>
Motivation: Mel声码化（将Mel频谱图转换为音频波形）仍是TTS系统的关键组件，需要开发低延迟的流式处理方案以满足实时应用需求。

Method: 基于生成流匹配、STFT相位检索（DiffPhase）和Mel滤波器组的伪逆算子，开发了MelFlow流式生成模型。

Result: 实现了32ms算法延迟和48ms总延迟，在消费级笔记本GPU上验证了实时流式能力，PESQ和SI-SDR指标显著优于HiFi-GAN等基线模型。

Conclusion: MelFlow成功实现了低延迟的流式Mel声码化，为实时TTS系统提供了高效的音频生成解决方案。

Abstract: The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram
to an audio waveform, is still a key component in many text-to-speech (TTS)
systems today. Based on generative flow matching, our prior work on generative
STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel
filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for
speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total
latency of 48 ms. We show real-time streaming capability at this latency not
only in theory, but in practice on a consumer laptop GPU. Furthermore, we show
that our model achieves substantially better PESQ and SI-SDR values compared to
well-established not streaming-capable baselines for Mel vocoding including
HiFi-GAN.

</details>


### [38] [Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs](https://arxiv.org/abs/2509.15095)
*Yutong Liu,Ziyue Zhang,Yongbin Yu,Xiangxiang Wang,Yuqing Cai,Nyima Tashi*

Main category: eess.AS

TL;DR: LIR-ASR是一个基于LLM的启发式优化迭代校正框架，通过"听-想象-精炼"策略减少ASR错误，在英中ASR输出上平均降低CER/WER达1.5个百分点


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统容易产生错误，影响下游应用性能，需要有效的错误校正方法

Method: 采用受人类听觉感知启发的"Listening-Imagining-Refining"策略，生成音位变体并在上下文中精炼，结合启发式优化和有限状态机避免局部最优，使用基于规则的约束保持语义保真度

Result: 在英语和中文ASR输出上实验显示，相比基线方法平均降低CER/WER达1.5个百分点，显著提升了转录准确性

Conclusion: LIR-ASR框架通过结合LLM和启发式优化，有效减少了ASR错误，为语音识别后处理提供了有效的校正方案

Abstract: Automatic Speech Recognition (ASR) systems remain prone to errors that affect
downstream applications. In this paper, we propose LIR-ASR, a heuristic
optimized iterative correction framework using LLMs, inspired by human auditory
perception. LIR-ASR applies a "Listening-Imagining-Refining" strategy,
generating phonetic variants and refining them in context. A heuristic
optimization with finite state machine (FSM) is introduced to prevent the
correction process from being trapped in local optima and rule-based
constraints help maintain semantic fidelity. Experiments on both English and
Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of
up to 1.5 percentage points compared to baselines, demonstrating substantial
accuracy gains in transcription.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [39] [Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework](https://arxiv.org/abs/2509.14304)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.SD

TL;DR: UDM框架在口吃和言语不流畅检测中实现了高精度(0.89 F1)与临床可解释性(4.2/5.0)的平衡，临床接受率达87%，诊断时间减少34%


<details>
  <summary>Details</summary>
Motivation: 解决传统口吃检测系统在准确性和临床可解释性之间的权衡问题，克服端到端深度学习模型的黑箱性质对临床应用的限制

Method: 采用模块化架构、显式音素对齐和可解释输出相结合的UDM框架，通过患者和认证言语病理学家的广泛实验验证

Result: 达到最先进性能(F1: 0.89±0.04)，提供临床有意义的可解释性评分(4.2/5.0)，临床接受率87%，诊断时间减少34%

Conclusion: UDM代表了在临床环境中实现AI辅助言语治疗的实用路径，为临床部署提供了强有力证据

Abstract: Stuttered and dysfluent speech detection systems have traditionally suffered
from the trade-off between accuracy and clinical interpretability. While
end-to-end deep learning models achieve high performance, their black-box
nature limits clinical adoption. This paper looks at the Unconstrained
Dysfluency Modeling (UDM) series-the current state-of-the-art framework
developed by Berkeley that combines modular architecture, explicit phoneme
alignment, and interpretable outputs for real-world clinical deployment.
Through extensive experiments involving patients and certified speech-language
pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art
performance (F1: 0.89+-0.04) while providing clinically meaningful
interpretability scores (4.2/5.0). Our deployment study shows 87% clinician
acceptance rate and 34% reduction in diagnostic time. The results provide
strong evidence that UDM represents a practical pathway toward AI-assisted
speech therapy in clinical environments.

</details>


### [40] [Measuring Soft Biometric Leakage in Speaker De-Identification Systems](https://arxiv.org/abs/2509.14469)
*Seungmin Seo,Oleg Aulov,P. Jonathon Phillips*

Main category: cs.SD

TL;DR: 这篇论文提出了软生物特征泄漏指数(SBLS)来评估语音去识别化系统的安全性，发现当前系统存在显著漏洞，攻击者可通过预训练模型可靠恢复软生物特征信息。


<details>
  <summary>Details</summary>
Motivation: 现有的语音去识别化系统评估主要关注个体级别的重识别风险，而忽视了来自软生物特征泄漏的更广泛风险。需要一种统一的方法来量化这些系统对非唯一特征推理攻击的抵御能力。

Method: 提出软生物特征泄漏指数(SBLS)，该方法整合了三个元素：1)使用预训练分类器进行直接属性推理；2)通过相互信息分析进行链接检测；3)在交叉属性上的子组稳健性分析。使用公开可用的分类器对五个去识别化系统进行评测。

Result: 所有五个评估的去识别化系统都显示出显著的漏洞。攻击者仅使用预训练模型（无需原始语音或系统详细信息）仍可可靠地从匿名化输出中恢复软生物特征信息。标准的分布指标无法抓住这些基本的弱点。

Conclusion: 当前的语音去识别化系统在软生物特征保护方面存在重大漏洞，SBLS提供了一种有效的评估方法来识别这些风险，并强调了在设计去识别化系统时需要更全面的安全考虑。

Abstract: We use the term re-identification to refer to the process of recovering the
original speaker's identity from anonymized speech outputs. Speaker
de-identification systems aim to reduce the risk of re-identification, but most
evaluations focus only on individual-level measures and overlook broader risks
from soft biometric leakage. We introduce the Soft Biometric Leakage Score
(SBLS), a unified method that quantifies resistance to zero-shot inference
attacks on non-unique traits such as channel type, age range, dialect, sex of
the speaker, or speaking style. SBLS integrates three elements: direct
attribute inference using pre-trained classifiers, linkage detection via mutual
information analysis, and subgroup robustness across intersecting attributes.
Applying SBLS with publicly available classifiers, we show that all five
evaluated de-identification systems exhibit significant vulnerabilities. Our
results indicate that adversaries using only pre-trained models - without
access to original speech or system details - can still reliably recover soft
biometric information from anonymized output, exposing fundamental weaknesses
that standard distributional metrics fail to capture.

</details>


### [41] [A long-form single-speaker real-time MRI speech dataset and benchmark](https://arxiv.org/abs/2509.14479)
*Sean Foley,Jihwan Lee,Kevin Huang,Xuan Shi,Yoonjeong Lee,Louis Goldstein,Shrikanth Narayanan*

Main category: cs.SD

TL;DR: USC LSS数据集发布，包含单说话人1小时的实时MRI声道动态视频和同步音频数据，提供多种衍生表示，并在发音合成和音素识别任务上建立基准性能


<details>
  <summary>Details</summary>
Motivation: 为发音研究提供更长的单说话人实时MRI语音数据集，支持多种下游任务研究

Method: 收集单名美国英语母语者的实时MRI视频和同步音频数据，进行数据预处理和衍生表示提取（包括声道区域裁剪视频、句子级分割、音频修复降噪、感兴趣区域时间序列等）

Result: 发布了约1小时的单说话人数据集，是目前较长的公开单说话人实时MRI语音数据集，提供了多种数据表示形式

Conclusion: 该数据集为发音合成、音素识别等任务提供了有价值的基准数据，未来研究可在此基础上进一步提升性能

Abstract: We release the USC Long Single-Speaker (LSS) dataset containing real-time MRI
video of the vocal tract dynamics and simultaneous audio obtained during speech
production. This unique dataset contains roughly one hour of video and audio
data from a single native speaker of American English, making it one of the
longer publicly available single-speaker datasets of real-time MRI speech data.
Along with the articulatory and acoustic raw data, we release derived
representations of the data that are suitable for a range of downstream tasks.
This includes video cropped to the vocal tract region, sentence-level splits of
the data, restored and denoised audio, and regions-of-interest timeseries. We
also benchmark this dataset on articulatory synthesis and phoneme recognition
tasks, providing baseline performance for these tasks on this dataset which
future research can aim to improve upon.

</details>


### [42] [Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech Synthesis](https://arxiv.org/abs/2509.14579)
*Qingyu Liu,Yushen Chen,Zhikang Niu,Chunhui Wang,Yunting Yang,Bowen Zhang,Jian Zhao,Pengcheng Zhu,Kai Yu,Xie Chen*

Main category: cs.SD

TL;DR: 该文章提出了Cross-Lingual F5-TTS框架，解决了流匹配TTS模型对音频提示语词转写的依赖问题，实现了无需语词转写的跨语言声音克隆


<details>
  <summary>Details</summary>
Motivation: 当前流匹配TTS模型依赖音频提示的语词转写，导致跨语言声音克隆在无法获得转写时受限，特别是对于未见语言

Method: 通过强制对齐预处理获取词边界，在训练时排除转写依赖；训练不同语言粒度的语速预测器来模型持续时间

Result: 实验结果显示该方法能够达到与F5-TTS相当的性能，同时支持跨语言声音克隆

Conclusion: 该框架成功解决了流匹配TTS模型在跨语言声音克隆中的关键挑战，为无需语词转写的语音合成提供了可行方案

Abstract: Flow-matching-based text-to-speech (TTS) models have shown high-quality
speech synthesis. However, most current flow-matching-based TTS models still
rely on reference transcripts corresponding to the audio prompt for synthesis.
This dependency prevents cross-lingual voice cloning when audio prompt
transcripts are unavailable, particularly for unseen languages. The key
challenges for flow-matching-based TTS models to remove audio prompt
transcripts are identifying word boundaries during training and determining
appropriate duration during inference. In this paper, we introduce
Cross-Lingual F5-TTS, a framework that enables cross-lingual voice cloning
without audio prompt transcripts. Our method preprocesses audio prompts by
forced alignment to obtain word boundaries, enabling direct synthesis from
audio prompts while excluding transcripts during training. To address the
duration modeling challenge, we train speaking rate predictors at different
linguistic granularities to derive duration from speaker pace. Experiments show
that our approach matches the performance of F5-TTS while enabling
cross-lingual voice cloning.

</details>


### [43] [Spatial Audio Motion Understanding and Reasoning](https://arxiv.org/abs/2509.14666)
*Arvind Krishna Sridhar,Yinyi Guo,Erik Visser*

Main category: cs.SD

TL;DR: 通过空间音频编码器和音频基准模型检测多重重叠事件的空间属性，并利用LLM进行动态音频场景的复杂推理


<details>
  <summary>Details</summary>
Motivation: 解决机器对动态音频场景的理解和推理问题，特别是关于移动源的空间属性分析

Method: 使用空间音频编码器检测事件和估计属性，通过音频基准模型实现语义对齐，然后用LLM进行结构化推理

Result: 开发了空间音频运动理解和推理标准数据集，并验证了框架的性能

Conclusion: 提出的框架能够有效地理解和推理动态音频场景中移动源的空间属性

Abstract: Spatial audio reasoning enables machines to interpret auditory scenes by
understanding events and their spatial attributes. In this work, we focus on
spatial audio understanding with an emphasis on reasoning about moving sources.
First, we introduce a spatial audio encoder that processes spatial audio to
detect multiple overlapping events and estimate their spatial attributes,
Direction of Arrival (DoA) and source distance, at the frame level. To
generalize to unseen events, we incorporate an audio grounding model that
aligns audio features with semantic audio class text embeddings via a
cross-attention mechanism. Second, to answer complex queries about dynamic
audio scenes involving moving sources, we condition a large language model
(LLM) on structured spatial attributes extracted by our model. Finally, we
introduce a spatial audio motion understanding and reasoning benchmark dataset
and demonstrate our framework's performance against the baseline model.

</details>


### [44] [How Does Instrumental Music Help SingFake Detection?](https://arxiv.org/abs/2509.14675)
*Xuanjun Chen,Chia-Yu Hu,I-Ming Lin,Yi-Cheng Lin,I-Hsiang Chiu,You Zhang,Sung-Feng Huang,Yi-Hsuan Yang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 该研究探讨了乐器伴奏对歌声深度伪造检测的影响，发现乐器主要起到数据增强作用而非提供内在线索，微调会增强对说话人特征的依赖而降低对内容和语义信息的敏感性。


<details>
  <summary>Details</summary>
Motivation: 虽然存在许多歌声深度伪造检测模型，但这些模型如何处理乐器伴奏尚不清楚，需要研究乐器音乐对SingFake检测的行为和表征影响。

Method: 从两个角度进行研究：行为效应方面测试不同骨干网络、未配对乐器轨道和频率子带；表征效应方面分析微调如何改变编码器的语音和音乐能力。

Result: 乐器伴奏主要作为数据增强而非提供节奏或和声等内在线索；微调增加了对浅层说话人特征的依赖，同时降低了对内容、副语言和语义信息的敏感性。

Conclusion: 这些发现阐明了模型如何利用人声与乐器线索，可为设计更可解释和鲁棒的SingFake检测系统提供指导。

Abstract: Although many models exist to detect singing voice deepfakes (SingFake), how
these models operate, particularly with instrumental accompaniment, is unclear.
We investigate how instrumental music affects SingFake detection from two
perspectives. To investigate the behavioral effect, we test different
backbones, unpaired instrumental tracks, and frequency subbands. To analyze the
representational effect, we probe how fine-tuning alters encoders' speech and
music capabilities. Our results show that instrumental accompaniment acts
mainly as data augmentation rather than providing intrinsic cues (e.g., rhythm
or harmony). Furthermore, fine-tuning increases reliance on shallow speaker
features while reducing sensitivity to content, paralinguistic, and semantic
information. These insights clarify how models exploit vocal versus
instrumental cues and can inform the design of more interpretable and robust
SingFake detection systems.

</details>


### [45] [Pushing the Limits of End-to-End Diarization](https://arxiv.org/abs/2509.14737)
*Samuel J. Broughton,Lahiru Samarakoon*

Main category: cs.SD

TL;DR: 本文提出了基于EEND-TA的统一非自回归端到端说话人日志模型，在多个公开数据集上实现了最先进的性能，特别是在DIHARD III数据集上达到了14.49%的DER。


<details>
  <summary>Details</summary>
Motivation: 探索基于EEND的架构具有比先前研究更大的学习能力，旨在超越现有说话人日志解决方案，同时保持推理时的高效速度。

Method: 使用EEND-TA单一统一非自回归模型进行端到端说话人日志，通过8说话人模拟混合进行预训练扩展，确保每个生成的说话人混合配置得到充分表示。

Result: 在多个公开数据集上实现了最先进的DER性能：AliMeeting-far、AliMeeting-near、AMIMix、AMI-SDM、DIHARD III和MagicData RAMC，其中DIHARD III上的DER达到14.49%。

Conclusion: 实验证明基于EEND的架构具有比先前探索更大的学习能力，超越了现有的许多说话人日志解决方案，同时在推理时保持了高效速度。

Abstract: In this paper, we present state-of-the-art diarization error rates (DERs) on
multiple publicly available datasets, including AliMeeting-far,
AliMeeting-near, AMI-Mix, AMI-SDM, DIHARD III, and MagicData RAMC. Leveraging
EEND-TA, a single unified non-autoregressive model for end-to-end speaker
diarization, we achieve new benchmark results, most notably a DER of 14.49% on
DIHARD III. Our approach scales pretraining through 8-speaker simulation
mixtures, ensuring each generated speaker mixture configuration is sufficiently
represented. These experiments highlight that EEND-based architectures possess
a greater capacity for learning than previously explored, surpassing many
existing diarization solutions while maintaining efficient speeds during
inference.

</details>


### [46] [Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions](https://arxiv.org/abs/2509.14785)
*Kentaro Seki,Yuki Okamoto,Kouei Yamaoka,Yuki Saito,Shinnosuke Takamichi,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: Spatial-CLAP是一个创新的音频-文本嵌入框架，通过内容感知空间编码器和空间对比学习，解决了多源音频条件下空间信息建模的挑战，实现了有效的空间感知音频-文本嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有的CLAP方法仅限于单声道或单源条件，无法充分捕捉空间信息。多源条件下的核心挑战在于需要正确建立每个声源与其位置之间的对应关系。

Method: 提出Spatial-CLAP，引入内容感知空间编码器实现与音频内容耦合的空间表示；提出空间对比学习(SCL)训练策略，显式强制学习正确对应关系并在多源条件下促进更可靠的嵌入。

Result: 实验评估表明Spatial-CLAP在多源条件下学习到有效嵌入，证实了SCL的有效性。对未见过的三源混合的评估突出了传统单源训练与所提多源训练范式的根本区别。

Conclusion: 这些发现为空间感知音频-文本嵌入建立了新的范式，解决了多源音频空间信息建模的关键问题。

Abstract: Contrastive language--audio pretraining (CLAP) has achieved remarkable
success as an audio--text embedding framework, but existing approaches are
limited to monaural or single-source conditions and cannot fully capture
spatial information. The central challenge in modeling spatial information lies
in multi-source conditions, where the correct correspondence between each sound
source and its location is required. To tackle this problem, we propose
Spatial-CLAP, which introduces a content-aware spatial encoder that enables
spatial representations coupled with audio content. We further propose spatial
contrastive learning (SCL), a training strategy that explicitly enforces the
learning of the correct correspondence and promotes more reliable embeddings
under multi-source conditions. Experimental evaluations, including downstream
tasks, demonstrate that Spatial-CLAP learns effective embeddings even under
multi-source conditions, and confirm the effectiveness of SCL. Moreover,
evaluation on unseen three-source mixtures highlights the fundamental
distinction between conventional single-source training and our proposed
multi-source training paradigm. These findings establish a new paradigm for
spatially-aware audio--text embeddings.

</details>


### [47] [Towards Building Speech Large Language Models for Multitask Understanding in Low-Resource Languages](https://arxiv.org/abs/2509.14804)
*Mingchen Shao,Bingshen Mu,Chengyou Wang,Hai Li,Ying Yan,Zhonghua Fu,Lei Xie*

Main category: cs.SD

TL;DR: 这篇论文提出了XLSR-Thai语音编码器、U-Align对齐方法和Thai-SUP数据生成流程，解决低资源语言泰语的语音大语言模型性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在高资源语言表现良好，但在泰语等低资源语言上性能显著下降，原因包括语音编码器性能不佳、对齐方法计算成本高、语音-文本数据缺乏。

Method: 1) 基于36,000小时泰语语音数据连续训练XLSR模型，得到自监督语音编码器XLSR-Thai
2) 提出U-Align语音-文本对齐方法，比ASR基准对齐更节约资源和多任务有效
3) 开发Thai-SUP流程，从高资源语言生成超过1,000小时的泰语语音理解数据集

Result: 多个实验证明了这些方法在构建泰语多任务理解语音大语言模型旹的有效性。

Conclusion: 该研究成功解决了低资源语言泰语的SLLM构建挑战，并开源了XLSR-Thai和Thai-SUP，为未来研究提供了基础。

Abstract: Speech large language models (SLLMs) built on speech encoders, adapters, and
LLMs demonstrate remarkable multitask understanding performance in
high-resource languages such as English and Chinese. However, their
effectiveness substantially degrades in low-resource languages such as Thai.
This limitation arises from three factors: (1) existing commonly used speech
encoders, like the Whisper family, underperform in low-resource languages and
lack support for broader spoken language understanding tasks; (2) the ASR-based
alignment paradigm requires training the entire SLLM, leading to high
computational cost; (3) paired speech-text data in low-resource languages is
scarce. To overcome these challenges in the low-resource language Thai, we
introduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder
for Thai. It is obtained by continuously training the standard SSL XLSR model
on 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a
speech-text alignment method that is more resource-efficient and
multitask-effective than typical ASR-based alignment. Finally, we present
Thai-SUP, a pipeline for generating Thai spoken language understanding data
from high-resource languages, yielding the first Thai spoken language
understanding dataset of over 1,000 hours. Multiple experiments demonstrate the
effectiveness of our methods in building a Thai multitask-understanding SLLM.
We open-source XLSR-Thai and Thai-SUP to facilitate future research.

</details>


### [48] [MeanFlowSE: one-step generative speech enhancement via conditional mean flow](https://arxiv.org/abs/2509.14858)
*Duojia Li,Shenghui Lu,Hongchen Pan,Zongyi Zhan,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: MeanFlowSE是一种条件生成模型，通过学习轨迹上有限间隔内的平均速度，实现单步语音增强，避免了传统方法需要多步ODE求解器的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于流和扩散的语音增强系统需要迭代ODE求解器进行多步推理，这在实时应用中成为计算瓶颈。

Method: 使用Jacobian-vector product (JVP)实例化MeanFlow恒等式，学习有限间隔内的位移监督，同时保持瞬时场约束的一致性。推理时通过反向时间位移进行单步生成。

Result: 在VoiceBank-DEMAND数据集上，单步模型实现了良好的可懂度、保真度和感知质量，计算成本显著低于多步基线方法。

Conclusion: MeanFlowSE提供了一个无需知识蒸馏或外部教师的高效、高保真实时生成式语音增强框架。

Abstract: Multistep inference is a bottleneck for real-time generative speech
enhancement because flow- and diffusion-based systems learn an instantaneous
velocity field and therefore rely on iterative ordinary differential equation
(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that
learns the average velocity over finite intervals along a trajectory. Using a
Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a
local training objective that directly supervises finite-interval displacement
while remaining consistent with the instantaneous-field constraint on the
diagonal. At inference, MeanFlowSE performs single-step generation via a
backward-in-time displacement, removing the need for multistep solvers; an
optional few-step variant offers additional refinement. On VoiceBank-DEMAND,
the single-step model achieves strong intelligibility, fidelity, and perceptual
quality with substantially lower computational cost than multistep baselines.
The method requires no knowledge distillation or external teachers, providing
an efficient, high-fidelity framework for real-time generative speech
enhancement.

</details>


### [49] [From Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition](https://arxiv.org/abs/2509.14880)
*Rishabh Jain,Naomi Harte*

Main category: cs.SD

TL;DR: LLM解码器在视觉语音识别中的改进主要来自语言建模而非视觉理解，数据集组合比模型缩放更重要，需要更强的视觉编码器


<details>
  <summary>Details</summary>
Motivation: 评估LLM解码器在视觉语音识别中的真实贡献，区分性能提升是来自视觉理解还是语言建模能力

Method: 通过冻结/选择性更新视觉编码器、缩放解码器大小、比较适应策略和架构、在不同数据集(LRS2/LRS3/WildVSR)上训练来系统评估

Result: Llama-2-13B模型在组合数据集上达到LRS3 24.7% WER和WildVSR 47.0% WER的SOTA性能，语义分析显示改进主要来自词汇而非语义处理

Conclusion: LLM解码器主要优化上下文推理而非视觉特征，视觉语音识别的实质性进展需要更强的视觉编码器

Abstract: Advances in self-supervised encoders have improved Visual Speech Recognition
(VSR). Recent approaches integrating these encoders with LLM decoders improves
transcription accuracy; however, it remains unclear whether these gains stem
from visual understanding or stronger language modeling. In this work, we
systematically evaluate LLM decoders by freezing or selectively updating the
visual encoder, scaling decoder size, comparing adaptation strategies and
architectures, and varying training data across LRS2, LRS3, and their
combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and
adaptation yield limited improvements, while combining datasets enhances
generalization. Semantic analysis reveals that gains arise primarily from
lexical rather than semantic processing. Our Llama-2-13B model trained on the
combined set achieves 24.7\% WER on LRS3 and 47.0\% on WildVSR, establishing
SOTA among models trained without additional supervision. Our findings indicate
LLM decoders refine contextual reasoning rather than visual features,
emphasizing the need for stronger visual encoders to drive meaningful progress.

</details>


### [50] [Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification](https://arxiv.org/abs/2509.14893)
*Yuanjian Chen,Yang Xiao,Jinjie Huang*

Main category: cs.SD

TL;DR: 提出了基于时间异构图对比学习(THGCL)的多模态声学事件分类方法，通过构建音频视频段的时间图，使用高斯过程和霍克斯过程分别处理模态内平滑性和模态间衰减，结合对比学习提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法通常分别处理音频和视觉流，后期融合特征，但难以对齐时间信息并减少跨模态噪声影响。大多数多模态图学习方法未能区分模态内和模态间的时间依赖性

Method: 构建每个事件的时间图，音频和视频段作为节点，时间链接作为边。使用高斯过程处理模态内平滑性，霍克斯过程处理模态间衰减，结合对比学习捕捉细粒度关系

Result: 在AudioSet数据集上实现了最先进的性能

Conclusion: THGCL框架通过有效建模模态内和模态间的时间依赖性，在多模态声学事件分类任务中取得了优异的表现

Abstract: Multimodal acoustic event classification plays a key role in audio-visual
systems. Although combining audio and visual signals improves recognition, it
is still difficult to align them over time and to reduce the effect of noise
across modalities. Existing methods often treat audio and visual streams
separately, fusing features later with contrastive or mutual information
objectives. Recent advances explore multimodal graph learning, but most fail to
distinguish between intra- and inter-modal temporal dependencies. To address
this, we propose Temporally Heterogeneous Graph-based Contrastive Learning
(THGCL). Our framework constructs a temporal graph for each event, where audio
and video segments form nodes and their temporal links form edges. We introduce
Gaussian processes for intra-modal smoothness, Hawkes processes for inter-modal
decay, and contrastive learning to capture fine-grained relationships.
Experiments on AudioSet show that THGCL achieves state-of-the-art performance.

</details>


### [51] [Back to Ear: Perceptually Driven High Fidelity Music Reconstruction](https://arxiv.org/abs/2509.14912)
*Kangdi Wang,Zhiyue Wu,Dinghao Zhou,Rui Lin,Junyu Dai,Tao Jiang*

Main category: cs.SD

TL;DR: 提出了一个名为{\epsilon}ar-VAE的开源音乐信号重建模型，通过听觉感知优化、相位损失改进和频谱监督新范式，显著提升了音频重建质量，特别是在高频谐波和空间特性重建方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的开源VAE模型在训练过程中往往忽视听觉感知方面，导致相位精度和立体声空间表示存在缺陷，需要开发更好的音频重建模型。

Method: 采用K权重感知滤波器进行损失计算；提出两种新的相位损失（相关性损失和相位损失）；设计新的频谱监督范式，幅度由所有四个Mid/Side/Left/Right分量监督，相位仅由LR分量监督。

Result: 在44.1kHz采样率下，{\epsilon}ar-VAE在多种指标上显著优于领先的开源模型，特别是在重建高频谐波和空间特性方面表现优异。

Conclusion: {\epsilon}ar-VAE通过重新思考和优化VAE训练范式，成功解决了现有模型在听觉感知方面的不足，为音频重建任务提供了更优秀的解决方案。

Abstract: Variational Autoencoders (VAEs) are essential for large-scale audio tasks
like diffusion-based generation. However, existing open-source models often
neglect auditory perceptual aspects during training, leading to weaknesses in
phase accuracy and stereophonic spatial representation. To address these
challenges, we propose {\epsilon}ar-VAE, an open-source music signal
reconstruction model that rethinks and optimizes the VAE training paradigm. Our
contributions are threefold: (i) A K-weighting perceptual filter applied prior
to loss calculation to align the objective with auditory perception. (ii) Two
novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss
using its derivatives--Instantaneous Frequency and Group Delay--for precision.
(iii) A new spectral supervision paradigm where magnitude is supervised by all
four Mid/Side/Left/Right components, while phase is supervised only by the LR
components. Experiments show {\epsilon}ar-VAE at 44.1kHz substantially
outperforms leading open-source models across diverse metrics, showing
particular strength in reconstructing high-frequency harmonics and the spatial
characteristics.

</details>


### [52] [Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening](https://arxiv.org/abs/2509.14944)
*Xiaolei Xu,Chaoyue Niu,Guy J. Brown,Hector Romero,Ning Ma*

Main category: cs.SD

TL;DR: 通过智能手机音频直接估计呼吸努力，给合声学特征提高阻塞性睡眠呼吸暂停检测效果，实现无传感器、可扩展的睡眠监测方案


<details>
  <summary>Details</summary>
Motivation: 阻塞性睡眠呼吸暂停(OSA)普遍但诊断困难，传统多导睡眠监测复杂费用高，声音监测又受环境噪声限制缺乏生理上下文信息

Method: 提出潜在空间融合框架，从夜间音频估计呼吸努力嵌入，然后与声音特征融合进行OSA检测，使用103名参与者157夜家庭记录数据

Result: 呼吸努力估计器协调相关系数达0.48，融合呼吸努力和声音的方法在敏感性和AUC指标上都超过仅使用声音的基准方法，特别是在低呼吸暂停指数阈值时

Conclusion: 该方法仅需智能手机音频即可实现OSA检测，为无传感器、可扩展和长期睡眠监测提供了可行方案

Abstract: Obstructive sleep apnoea (OSA) is a prevalent condition with significant
health consequences, yet many patients remain undiagnosed due to the complexity
and cost of over-night polysomnography. Acoustic-based screening provides a
scalable alternative, yet performance is limited by environmental noise and the
lack of physiological context. Respiratory effort is a key signal used in
clinical scoring of OSA events, but current approaches require additional
contact sensors that reduce scalability and patient comfort. This paper
presents the first study to estimate respiratory effort directly from nocturnal
audio, enabling physiological context to be recovered from sound alone. We
propose a latent-space fusion framework that integrates the estimated effort
embeddings with acoustic features for OSA detection. Using a dataset of 157
nights from 103 participants recorded in home environments, our respiratory
effort estimator achieves a concordance correlation coefficient of 0.48,
capturing meaningful respiratory dynamics. Fusing effort and audio improves
sensitivity and AUC over audio-only baselines, especially at low
apnoea-hypopnoea index thresholds. The proposed approach requires only
smartphone audio at test time, which enables sensor-free, scalable, and
longitudinal OSA monitoring.

</details>


### [53] [FCPE: A Fast Context-based Pitch Estimation Model](https://arxiv.org/abs/2509.15140)
*Yuxin Luo,Ruoyi Zhang,Lu-Chuan Liu,Tianyu Li,Hangyu Liu*

Main category: cs.SD

TL;DR: FCPE是一个基于上下文的快速音高估计模型，采用Lynx-Net架构和深度可分离卷积，在噪声环境下表现优异，计算效率极高


<details>
  <summary>Details</summary>
Motivation: 单声道音频中的音高估计对MIDI转录和歌声转换至关重要，但现有方法在噪声环境下性能显著下降

Method: 使用Lynx-Net架构和深度可分离卷积来有效捕捉梅尔频谱图特征，同时保持低计算成本和强大的噪声容忍度

Result: 在MIR-1K数据集上达到96.79%的原始音高准确率，与最先进方法相当；在RTX 4090 GPU上的实时因子为0.0062，效率显著优于现有算法

Conclusion: FCPE模型在保持高精度的同时实现了极高的计算效率，为噪声环境下的音高估计提供了有效的解决方案

Abstract: Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription
and singing voice conversion (SVC), but existing methods suffer significant
performance degradation under noise. In this paper, we propose FCPE, a fast
context-based pitch estimation model that employs a Lynx-Net architecture with
depth-wise separable convolutions to effectively capture mel spectrogram
features while maintaining low computational cost and robust noise tolerance.
Experiments show that our method achieves 96.79\% Raw Pitch Accuracy (RPA) on
the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time
Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly
outperforms existing algorithms in efficiency. Code is available at
https://github.com/CNChTu/FCPE.

</details>


### [54] [Exploring How Audio Effects Alter Emotion with Foundation Models](https://arxiv.org/abs/2509.15151)
*Stelios Katsis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou*

Main category: cs.SD

TL;DR: 基础模型探针分析音频效果对情感的影响，通过深度学习模型嵌入探索音频FX与情感的复杂关系


<details>
  <summary>Details</summary>
Motivation: 音频效果（FX）如混响、失真、调制等在音乐中形成情感反应的关键作用很重要，但对其系统性情感影响的研究仍不充分

Method: 利用基础模型（大规模多模态预训练神经网络）进行探针分析，通过多种探针方法分析深度学习模型的嵌入表示

Result: 揭示了音频FX与情感估计之间的复杂非线性关系，发现了与具体效果相关的模式，并评估了基础音频模型的稳健性

Conclusion: 研究提高了对音频制作技术知觉影响的理解，对音乐认知、表演和情感计算领域具有重要意义

Abstract: Audio effects (FX) such as reverberation, distortion, modulation, and dynamic
range processing play a pivotal role in shaping emotional responses during
music listening. While prior studies have examined links between low-level
audio features and affective perception, the systematic impact of audio FX on
emotion remains underexplored. This work investigates how foundation models -
large-scale neural architectures pretrained on multimodal data - can be
leveraged to analyze these effects. Such models encode rich associations
between musical structure, timbre, and affective meaning, offering a powerful
framework for probing the emotional consequences of sound design techniques. By
applying various probing methods to embeddings from deep learning models, we
examine the complex, nonlinear relationships between audio FX and estimated
emotion, uncovering patterns tied to specific effects and evaluating the
robustness of foundation audio models. Our findings aim to advance
understanding of the perceptual impact of audio production practices, with
implications for music cognition, performance, and affective computing.

</details>


### [55] [Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation](https://arxiv.org/abs/2509.15210)
*Chen Si,Qianyi Wu,Chaitanya Amballa,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: MiNAF提出了一种结合显式几何特征的神经隐式声场方法，通过查询粗糙房间网格提取距离分布作为局部上下文，显著提升了房间脉冲响应预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式方法在声音模拟中未能有效利用环境的显式几何信息，限制了房间脉冲响应(RIR)预测的精度。

Method: 提出Mesh-infused Neural Acoustic Field (MiNAF)，在给定位置查询粗糙房间网格并提取距离分布作为显式局部几何特征表示，指导神经网络生成更准确的RIR预测。

Result: 与常规和最先进基线方法相比，MiNAF在各种评估指标上表现优异，且在训练样本有限的数据集中展现出良好的鲁棒性。

Conclusion: 结合显式局部几何特征能够显著提升神经隐式模型在声音模拟中的性能，为高保真声音仿真提供了有效解决方案。

Abstract: Realistic sound simulation plays a critical role in many applications. A key
element in sound simulation is the room impulse response (RIR), which
characterizes how sound propagates from a source to a listener within a given
space. Recent studies have applied neural implicit methods to learn RIR using
context information collected from the environment, such as scene images.
However, these approaches do not effectively leverage explicit geometric
information from the environment. To further exploit the potential of neural
implicit models with direct geometric features, we present Mesh-infused Neural
Acoustic Field (MiNAF), which queries a rough room mesh at given locations and
extracts distance distributions as an explicit representation of local context.
Our approach demonstrates that incorporating explicit local geometric features
can better guide the neural network in generating more accurate RIR
predictions. Through comparisons with conventional and state-of-the-art
baseline methods, we show that MiNAF performs competitively across various
evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets
with limited training samples, demonstrating an advance in high-fidelity sound
simulation.

</details>


### [56] [Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation](https://arxiv.org/abs/2509.15222)
*Junhyung Park,Yonghyun Kim,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: 开发了一个集成网络工具包，包含PiaRec和ASDF两个GUI，用于简化钢琴表演多模态数据的采集和指法标注


<details>
  <summary>Details</summary>
Motivation: 钢琴表演是多模态活动，但大规模多模态数据采集过程繁琐，阻碍了该领域的研究进展

Method: 开发了两个图形用户界面：PiaRec支持音频、视频、MIDI和表演元数据的同步采集；ASDF支持从视觉数据中高效标注演奏指法

Result: 创建了一个能够简化多模态钢琴表演数据集采集的系统

Conclusion: 该集成系统可以有效克服钢琴表演多模态数据采集的瓶颈问题

Abstract: Piano performance is a multimodal activity that intrinsically combines
physical actions with the acoustic rendition. Despite growing research interest
in analyzing the multimodal nature of piano performance, the laborious process
of acquiring large-scale multimodal data remains a significant bottleneck,
hindering further progress in this field. To overcome this barrier, we present
an integrated web toolkit comprising two graphical user interfaces (GUIs): (i)
PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and
performance metadata. (ii) ASDF, which enables the efficient annotation of
performer fingering from the visual data. Collectively, this system can
streamline the acquisition of multimodal piano performance datasets.

</details>

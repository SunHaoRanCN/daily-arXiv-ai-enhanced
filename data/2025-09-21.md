<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [eess.AS](#eess.AS) [Total: 19]
- [cs.SD](#cs.SD) [Total: 18]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [In Planta Tattoo and Kirigami Sensors for Self-Powered Monitoring of Vapor Pressure Deficit and Growth Dynamics](https://arxiv.org/abs/2509.14240)
*Nafize Ishtiaque Hossain,Kundan Saha,Atul Sharma,Sameer Sonkusale*

Main category: eess.SP

TL;DR: 自供电植物体内传感器平台，集成叶面温湿度传感器和草纸艺术风格的茎干伸缩传感器，可持续监测植物水分状况和生长情况


<details>
  <summary>Details</summary>
Motivation: 开发一种可扩展、自供电的植物体内传感器平台，用于持续监测植物水合状况和生长，以支持大规模农业部署和改善作物管理

Method: 集成两种传感器：1）叶面刷印传感器，使用五氧化二锰纳米膜来测量温度、湿度并通过水分收集能量；2）基于草纸艺术的弹性体基胶传感器，包裙在茎干上跟踪径向生长

Result: 系统能够自主运行，能量收集密度达到0.1114微瓦/平方厘米。温湿度传感器可持续工作10天以上，茎干传感器可持续工作20天以上，且充分抗干扰

Conclusion: 这种无需清洁室、可卷对卷生产的传感器平台具有很大的农业应用潜力，可用于监测非生物强迫和改善作物管理

Abstract: We report a scalable, self-powered in planta sensor platform for continuous
monitoring of plant hydration and growth. The system integrates two components
a leaf mounted tattoo sensor for estimating vapor pressure deficit and a
kirigami inspired strain sensor for tracking radial stem growth. Uniquely, the
tattoo sensor serves a dual function measuring temperature and humidity beneath
the leaf surface while simultaneously harvesting power from ambient moisture
via a vanadium pentoxide nanosheet membrane. This moist-electric generator
configuration enables energy-autonomous operation, delivering a power density
of 0.1114 miroW per square cm. The V2O5 based sensor exhibits high sensitivity
to humidity and temperature, enabling accurate VPD estimation for over 10 days
until leaf senescence. The eutectogel based kirigami strain sensor, wrapped
around the stem, offers a gauge factor of 1.5 and immunity to unrelated
mechanical disturbances, allowing continuous growth tracking for more than 20
days. Both sensors are fabricated via cleanroom-free, roll to roll compatible
methods, underscoring their potential for large-scale agricultural deployment
to monitor abiotic stress and improve crop management.

</details>


### [2] [Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes](https://arxiv.org/abs/2509.14242)
*Jinshuai Gu,Zenghui Lin,Jingying Ma,Jingyu Wang,Linyan Zhang,Rui Bai,Zelin Tu,Youyou Jiang,Donglin Xie,Yuxi Zhou,Guoli Liu,Shenda Hong*

Main category: eess.SP

TL;DR: 基于异常CTG年龄预测模型(CTGage)的年龄差值(CTGage-gap)可作为新的数字生物标记物，预测孕期不良结局风险


<details>
  <summary>Details</summary>
Motivation: 开发一种能够预测未来孕期不良结局的无创新型数字生物标记物，充分利用价格低廉、无创伤的胎儿心电监护(CTG)技术

Method: 使用61,140份CTG记录开发一维卷积神经网络模型(CTGage)预测生物年龄，计算年龄差值并分为五个风险组别，分析不同组别中孕期不良结局的发生率

Result: 模型平均绝对误差10.91天。高估组早产儿发生率5.33% vs 正常组1.42%，孕期糖尿病31.93% vs 20.86%，低估组低体重儿0.17% vs 0.15%，贫血37.51% vs 34.74%，均显著差异

Conclusion: AI推导的CTGage模型能够预测孕期不良结局的未来风险，有潜力成为一种新型、无创伤且易获得的数字生物标记物

Abstract: Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment
technique used globally, especially in underdeveloped countries. However, it is
currently mainly used to identify the fetus's current status (e.g., fetal
acidosis or hypoxia), and the potential of CTG in predicting future adverse
pregnancy outcomes has not been fully explored. We aim to develop an AI-based
model that predicts biological age from CTG time series (named CTGage), then
calculate the age gap between CTGage and actual age (named CTGage-gap), and use
this gap as a new digital biomarker for future adverse pregnancy outcomes. The
CTGage model is developed using 61,140 records from 11,385 pregnant women,
collected at Peking University People's Hospital between 2018 and 2022. For
model training, a structurally designed 1D convolutional neural network is
used, incorporating distribution-aligned augmented regression technology. The
CTGage-gap is categorized into five groups: < -21 days (underestimation group),
-21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and > 21 days
(overestimation group). We further defined the underestimation group and
overestimation group together as the high-risk group. We then compare the
incidence of adverse outcomes and maternal diseases across these groups. The
average absolute error of the CTGage model is 10.91 days. When comparing the
overestimation group with the normal group, premature infants incidence is
5.33% vs. 1.42% (p < 0.05) and gestational diabetes mellitus (GDM) incidence is
31.93% vs. 20.86% (p < 0.05). When comparing the underestimation group with the
normal group, low birth weight incidence is 0.17% vs. 0.15% (p < 0.05) and
anaemia incidence is 37.51% vs. 34.74% (p < 0.05). Artificial
intelligence-derived CTGage can predict the future risk of adverse pregnancy
outcomes and hold potential as a novel, non-invasive, and easily accessible
digital biomarker.

</details>


### [3] [InWaveSR: Topography-Aware Super-Resolution Network for Internal Solitary Waves](https://arxiv.org/abs/2509.14243)
*Xinjie Wang,Zhongrui Li,Peng Han,Chunxin Yuan,Jiexin Xu,Zhiqiang Wei,Jie Nie*

Main category: eess.SP

TL;DR: 提出InWaveSR模型，基于深度学习框架和物理约束，用于从低分辨率观测数据生成高分辨率数据，特别针对内孤立波数据。


<details>
  <summary>Details</summary>
Motivation: 观测数据分辨率不足限制了有效利用，需要开发能够生成高分辨率数据的方法，特别是对于内孤立波等复杂物理现象。

Method: 使用原始Navier-Stokes方程作为物理约束，结合注意力机制和快速傅里叶变换的HF-ResBlock组件，采用边缘采样和数值预处理优化训练过程。

Result: 在实测内孤立波数据评估中，PSNR达到36.2，优于传统插值方法和先前神经网络方法。

Conclusion: InWaveSR模型在高质量内孤立波重建方面表现出优异性能和可靠性，显著优于传统方法。

Abstract: The effective utilization of observational data is frequently hindered by
insufficient resolution. To address this problem, we present a new
spatio-temporal super-resolution (STSR) model, called InWaveSR. It is built on
a deep learning framework with physical restrictions and can efficiently
generate high-resolution data from low-resolution input, especially for data
featuring internal solitary waves (ISWs). To increase generality and
interpretation, the model InWaveSR uses the primitive Navier-Stokes equations
as the constraint, ensuring that the output results are physically consistent.
In addition, the proposed model incorporates an HF-ResBlock component that
combines the attention mechanism and the Fast Fourier Transform (FFT) method to
improve the performance of the model in capturing high-frequency
characteristics. Simultaneously, in order to enhance the adaptability of the
model to complicated bottom topography, an edge sampling and numerical
pre-processing method are carried out to optimize the training process. On
evaluations using the in-situ observational ISW data, the proposed InWaveSR
achieved a peak signal-to-noise ratio (PSNR) score of 36.2, higher than those
of the traditional interpolation method and the previous neural network. This
highlights its significant superiority over traditional methods, demonstrating
its excellent performance and reliability in high-resolution ISW
reconstruction.

</details>


### [4] [Conditional Nearest Level Modulation for Improved Switching Dynamics in Asymmetric Multilevel Converters](https://arxiv.org/abs/2509.14402)
*Jinshui Zhang,Angel V Peterchev,Stefan M Goetz*

Main category: eess.SP

TL;DR: 来文提出了条件最近层调制(cNLM)方法，通过数学惩罚模型调节开关动态，有效解决了非对称多级转换器中过高开关频率和输出电压突变问题。


<details>
  <summary>Details</summary>
Motivation: 非对称多级转换器虽然能够按指数增长输出级别，但传统的最近层调制(NLM)方法在输出级别过多时会导致某些模块开关频率过高和输出电压突变，影响输出质量。

Method: 提出条件最近层调制(cNLM)方法，通过引入数学惩罚模型来规制开关动态。还提出了适配特定功能的cNLM变体，如强制最小开关间隔。

Result: 在非对称多级原型上的实验验证显示，与原始NLM相比，cNLM将总输出x异变从66.3%降至15.1%，并将开关频率降低到原始的8%。

Conclusion: cNLM方法能够有效提升非对称多级转换器的输出质量，显著降低开关频率和输出x异变，为清洁能源、电动汽车等应用领域提供了更优科的调制方案。

Abstract: Modular multilevel converters have promising applications in clean energy,
electric vehicles, and biomedical instrumentation, but need many modules to
achieve fine output granularity, particularly of the voltage. Asymmetric
multilevel circuits introduce differences in module voltages so that the
quantity of output levels grows exponentially with the number of modules.
Nearest-level modulation (NLM) is preferred over carrier-based methods in
asymmetric circuits for its simplicity. However, the large number of output
levels can overwhelm NLM and cause excessive transistor switching on some
modules and output voltage spikes. We propose a conditional nearest-level
modulation (cNLM) by incorporating mathematical penalty models to regulate
switching dynamics. This approach improves output quality and reduces switching
rates. Additionally, we present cNLM variations tailored for specific
functions, such as enforcing a minimum switching interval. Experimental
validation on an asymmetric multilevel prototype demonstrates that cNLM reduces
the total output distortion from 66.3% to 15.1% while cutting the switching
rate to just 8% of the original NLM.

</details>


### [5] [Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography](https://arxiv.org/abs/2509.14442)
*Arjun Teh,Wael H. Ali,Joshua Rapp,Hassan Mansour*

Main category: eess.SP

TL;DR: 基于单视点背景光线缓动法和物理领域知识的室内流场体积量估计框架


<details>
  <summary>Details</summary>
Motivation: 解决单视点BOS断层扫描问题的严重不适定性，实现无侵入式室内流场可视化

Method: 改进光线追踪算法，物理基础的光线渲染和损失函数，使用PINN进行物理正则化以确保流场符合浮力驱动流控制方程

Result: 开发了一个能够从单个视点准确重建室内空气流场体积分布的框架

Conclusion: 该框架通过结合光学测量和物理模型，有效解决了单视点BOS断层扫描的问题，为无侵入式流场可视化提供了新方法

Abstract: We develop a framework for non-invasive volumetric indoor airflow estimation
from a single viewpoint using background-oriented schlieren (BOS) measurements
and physics-informed reconstruction. Our framework utilizes a light projector
that projects a pattern onto a target back-wall and a camera that observes
small distortions in the light pattern. While the single-view BOS tomography
problem is severely ill-posed, our proposed framework addresses this using: (1)
improved ray tracing, (2) a physics-based light rendering approach and loss
formulation, and (3) a physics-based regularization using a physics-informed
neural network (PINN) to ensure that the reconstructed airflow is consistent
with the governing equations for buoyancy-driven flows.

</details>


### [6] [Biologically Plausible Online Hebbian Meta-Learning: Two-Timescale Local Rules for Spiking Neural Brain Interfaces](https://arxiv.org/abs/2509.14447)
*Sriram V. C. Nallani,Gautham Ramachandran,Sahil S. Shah*

Main category: eess.SP

TL;DR: 提出了一种用于脑机接口的在线脉冲神经网络解码器，使用局部三因子学习规则和双时间尺度资格迹，无需时间反向传播，在保持竞争力的同时显著减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 脑机接口面临神经信号不稳定性和实时植入应用的内存限制挑战，需要开发内存高效且能持续自适应的神经解码方法。

Method: 采用基于误差调制的Hebbian更新、快速/慢速迹整合和自适应学习率控制的局部三因子学习规则，结合双时间尺度资格迹，避免使用时间反向传播。

Result: 在两个灵长类数据集上达到可比解码精度（Pearson R≥0.63和R≥0.81），内存减少28-35%，收敛速度比BPTT方法更快，闭环仿真显示能适应神经干扰并从零开始学习。

Conclusion: 该方法实现了内存高效的持续自适应神经解码，适用于资源受限的植入式脑机接口系统，仅需O(1)内存而BPTT方法需要O(T)内存。

Abstract: Brain-Computer Interfaces face challenges from neural signal instability and
memory constraints for real-time implantable applications. We introduce an
online SNN decoder using local three-factor learning rules with dual-timescale
eligibility traces that avoid backpropagation through time while maintaining
competitive performance. Our approach combines error-modulated Hebbian updates,
fast/slow trace consolidation, and adaptive learning rate control, requiring
only O(1) memory versus O(T) for BPTT methods. Evaluations on two primate
datasets achieve comparable decoding accuracy (Pearson $R \geq 0.63$ Zenodo, $R
\geq 0.81$ MC Maze) with 28-35% memory reduction and faster convergence than
BPTT-trained SNNs. Closed-loop simulations with synthetic neural populations
demonstrate adaptation to neural disruptions and learning from scratch without
offline calibration. This work enables memory-efficient, continuously adaptive
neural decoding suitable for resource-constrained implantable BCI systems.

</details>


### [7] [Secure Blind Graph Signal Recovery and Adversary Detection Using Smoothness Maximization](https://arxiv.org/abs/2509.14449)
*Mahdi Shamsi,Hadi Zayyani,Hasan Abu Hilal,Mohammad Salman*

Main category: eess.SP

TL;DR: 提出一种安全的盲图形信号恢复算法，能够在存在测量噪声和恶意节点欺诈攻击的情况下检测敌对节点并恢复图形信号


<details>
  <summary>Details</summary>
Motivation: 解决图形信号恢复中的安全问题，因为未知数量和位置的恶意节点可能注入假数据，影响信号恢复的准确性

Method: 使用基于差分平滑性的统计量来检测敌对节点，然后采用平滑性最大化的变体通过Dinkelbach算法来解决分数优化问题进行图形信号恢复

Result: 模拟结果显示，与中位数GSR算法和其他竞争方法相比，提出的方法在信号恢复方面显示出显著改善

Conclusion: 该算法是解决存在测量噪声和假数据注入攻击时图形信号恢复问题的理想选择，具有高效和低复杂度的敌对检测能力

Abstract: In this letter, we propose a secure blind Graph Signal Recovery (GSR)
algorithm that can detect adversary nodes. Some unknown adversaries are assumed
to be injecting false data at their respective nodes in the graph. The number
and location of adversaries are not known in advance and the goal is to recover
the graph signal in the presence of measurement noise and False Data Injection
(FDI) caused by the adversaries. Consequently, the proposed algorithm would be
a perfect candidate to solve this challenging problem. Moreover, due to the
presence of malicious nodes, the proposed method serves as a secure GSR
algorithm. For adversary detection, a statistical measure based on differential
smoothness is used. Specifically, the difference between the current observed
smoothness and the average smoothness excluding the corresponding node. This
genuine statistical approach leads to an effective and low-complexity adversary
detector. In addition, following malicious node detection, the GSR is performed
using a variant of smoothness maximization, which is solved efficiently as a
fractional optimization problem using a Dinkelbach's algorithm. Analysis of the
detector, which determines the optimum threshold of the detector is also
presented. Simulation results show a significant improvement of the proposed
method in signal recovery compared to the median GSR algorithm and other
competing methods.

</details>


### [8] [Age of Information Aided Intelligent Grant-Free Massive Access for Heterogeneous mMTC Traffic](https://arxiv.org/abs/2509.14503)
*Zhongwen Sun,Wei Chen,Yuxuan Sun,Bo Ai*

Main category: eess.SP

TL;DR: 本文研究了非正交掉照免授权随机接入场景下的异构流量兼容问题，提出了基于信息龄的自动编码器方案A-PIAAE，同时优化了监控设备的信息时效性和报警设备的检测成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的免授权随机接入研究主要关注用户检测和数据恢复的准确性，而忽视了流量的异质性。在6G下物联网流量更加复杂多样的背景下，需要同时满足事件触发流量和状态更新流量的不同服务需求。

Method: 首先分析基于信息龄的随机接入方案，优化接入参数以最小化监控设备的平均信息龄。然后设计了A-PIAAE自动编码器，利用学习导频减少非正交导频干扰，并在解码器中提出LISTA-AGE算法，以监控设备的信息龄作为先验信息来提升主动用户检测。

Result: 理论分析证明了A-PIAAE具有更好的收敛性能。实验结果显示，该方法能够有效降低监控设备的平均信息龄，同时提高报警设备的成功检测率。

Conclusion: 该研究成功地解决了异构流量兼容的挑战，通过利用信息龄作为先验信息，实现了在非正交免授权随机接入系统中同时优化事件触发流量检测和状态更新流量时效性的目标。

Abstract: With the arrival of 6G, the Internet of Things (IoT) traffic is becoming more
and more complex and diverse. To meet the diverse service requirements of IoT
devices, massive machine-type communications (mMTC) becomes a typical scenario,
and more recently, grant-free random access (GF-RA) presents a promising
direction due to its low signaling overhead. However, existing GF-RA research
primarily focuses on improving the accuracy of user detection and data
recovery, without considering the heterogeneity of traffic. In this paper, we
investigate a non-orthogonal GF-RA scenario where two distinct types of traffic
coexist: event-triggered traffic with alarm devices (ADs), and status update
traffic with monitor devices (MDs). The goal is to simultaneously achieve high
detection success rates for ADs and high information timeliness for MDs. First,
we analyze the age-based random access scheme and optimize the access
parameters to minimize the average age of information (AoI) of MDs. Then, we
design an age-based prior information aided autoencoder (A-PIAAE) to jointly
detect active devices, together with learned pilots used in GF-RA to reduce
interference between non-orthogonal pilots. In the decoder, an Age-based
Learned Iterative Shrinkage Thresholding Algorithm (LISTA-AGE) utilizing the
AoI of MDs as the prior information is proposed to enhance active user
detection. Theoretical analysis is provided to demonstrate the proposed A-PIAAE
has better convergence performance. Experiments demonstrate the advantage of
the proposed method in reducing the average AoI of MDs and improving the
successful detection rate of ADs.

</details>


### [9] [Radiolunadiff: Estimation of wireless network signal strength in lunar terrain](https://arxiv.org/abs/2509.14559)
*Paolo Torrado,Anders Pearson,Jason Klein,Alexander Moscibroda,Joshua Smith*

Main category: eess.SP

TL;DR: 提出了一种基于物理信息的深度学习架构，用于预测月球地形上的无线电地图，结合了物理地形生成器和射线追踪引擎，使用triplet-UNet架构在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 需要准确预测月球地形上的无线电传播特性，以支持月球探测任务的通信规划，现有深度学习方法在复杂地形环境下表现不足。

Method: 整合基于物理的月球地形生成器（使用NASA数据）和射线追踪引擎创建高保真数据集，采用triplet-UNet架构（两个标准UNet加扩散网络）建模复杂传播效应。

Result: 实验结果表明，该方法在月球地形数据集上的各项指标均优于现有的深度学习方法。

Conclusion: 所提出的物理信息深度学习架构能够有效预测月球地形上的无线电地图，为月球探测任务的通信系统设计提供了可靠的技术支持。

Abstract: In this paper, we propose a novel physics-informed deep learning architecture
for predicting radio maps over lunar terrain. Our approach integrates a
physics-based lunar terrain generator, which produces realistic topography
informed by publicly available NASA data, with a ray-tracing engine to create a
high-fidelity dataset of radio propagation scenarios. Building on this dataset,
we introduce a triplet-UNet architecture, consisting of two standard UNets and
a diffusion network, to model complex propagation effects. Experimental results
demonstrate that our method outperforms existing deep learning approaches on
our terrain dataset across various metrics.

</details>


### [10] [Task-Oriented Learning for Automatic EEG Denoising](https://arxiv.org/abs/2509.14665)
*Tian-Yu Xiang,Zheng Lei,Xiao-Hu Zhou,Xiao-Liang Xie,Shi-Qi Liu,Mei-Jiang Gui,Hong-Yun Ou,Xin-Zheng Huang,Xin-Yi Fu,Zeng-Guang Hou*

Main category: eess.SP

TL;DR: 一种仅使用任务标签的自动EEG去噪框架，通过任务向导的协同优化实现无需清洁参考信号的EEG去噪


<details>
  <summary>Details</summary>
Motivation: 传统EEG去噪方法依赖人工帮助或清洁参考信号，本文提出一种仅需任务标签的自动去噪方法

Method: 首先通过盲源分离技术将EEG分解为组件，然后用学习基于的选择器给每个组件赋予保留概率，最后通过概率加权组合重构去噪信号，下游代理任务模型评估重构信号

Result: 在三个数据集上进行实验，在多种噪声条件下均获得一致收益，任务性能提升精确度2.56%，信噪比提升0.82dB

Conclusion: 该任务向导学习框架是一种实用的EEG去噪解决方案，对神经科学研究和EEG基于交互系统具有潜在影响

Abstract: Electroencephalography (EEG) denoising methods typically depend on manual
intervention or clean reference signals. This work introduces a task-oriented
learning framework for automatic EEG denoising that uses only task labels
without clean reference signals. EEG recordings are first decomposed into
components based on blind source separation (BSS) techniques. Then, a
learning-based selector assigns a retention probability to each component, and
the denoised signal is reconstructed as a probability-weighted combination. A
downstream proxy-task model evaluates the reconstructed signal, with its task
loss supervising the selector in a collaborative optimization scheme that
relies solely on task labels, eliminating the need for clean EEG references.
Experiments on three datasets spanning two paradigms and multiple noise
conditions show consistent gains in both task performance (accuracy:
$2.56\%\uparrow$) and standard signal-quality metrics (signal-to-noise-ratio:
$0.82$\,dB\,$\uparrow$). Further analyses demonstrate that the task-oriented
learning framework is algorithm-agnostic, as it accommodates diverse
decomposition techniques and network backbones for both the selector and the
proxy model. These promising results indicate that the proposed task-oriented
learning framework is a practical EEG denoising solution with potential
implications for neuroscience research and EEG-based interaction systems.

</details>


### [11] [Mitigating the Impact of Location Uncertainty on Radio Map-Based Predictive Rate Selection via Noisy-Input Gaussian Process](https://arxiv.org/abs/2509.14710)
*Koya Sato*

Main category: eess.SP

TL;DR: 基于噪声输入高斯过程(NIGP)的新题途径选择框架，通过泰勒近似处理位置噪声，在位置不确定性下实现更可靠的6G网络速率选择


<details>
  <summary>Details</summary>
Motivation: 现有无线系统假设位置信息完美，实际个体定位往往存在误差，这种位置不确定性会降低基于无线电图的速率选择的可靠性

Method: 提出使用噪声输入高斯过程(NIGP)，通过泰勒展开近似将位置噪声当作额外的输出噪声来处理，构建更加稳健的无线电图

Result: 数值实验结果显示，NIGP基础设计比纯粹GP方法实现了更可靠的传输速率选择，同时比基于路径损耗的方法获得了更高的端到端速率

Conclusion: 该研究提出的NIGP方法能够有效减少位置不确定性对无线电图系统的负面影响，为6G网络提供了更加稳健和可靠的速率选择方案

Abstract: This paper proposes a predictive rate-selection framework based on Gaussian
process (GP)-based radio map construction that is robust to location
uncertainty. Radio maps are a promising tool for improving communication
efficiency in 6G networks. Although they enable the design of location-based
maximum transmission rates by exploiting statistical channel information,
existing discussions often assume perfect (i.e., noiseless) location
information during channel sensing. Since such information must be obtained
from positioning systems such as global navigation satellite systems, it
inevitably involves positioning errors; this location uncertainty can degrade
the reliability of radio map-based wireless systems. To mitigate this issue, we
introduce the noisy-input GP (NIGP), which treats location noise as additional
output noise by applying a Taylor approximation of the function of interest.
Numerical results demonstrate that the proposed NIGP-based design achieves more
reliable transmission-rate selection than pure GP and yields higher throughput
than path loss-based rate selection.

</details>


### [12] [LLM4MG: Adapting Large Language Model for Multipath Generation via Synesthesia of Machines](https://arxiv.org/abs/2509.14711)
*Ziwei Huang,Shiliang Lu,Lu Bai,Xuesong Cai,Xiang Cheng*

Main category: eess.SP

TL;DR: 基于机器联觉(SoM)的大语言模型多路径生成方案(LLM4MG)，利用LLaMA 3.2模型通过多模态感知数据生成6G V2I场景中的频道多路径信息，在分类准确度和生成精度方面超过传统深度学习方法


<details>
  <summary>Details</summary>
Motivation: 为解决6G车联网(V2I)场景中高精度频道多路径生成的挑战，利用大语言模型的强大语义理解能力来处理多模态感知数据并生成多路径信息

Method: 构建SynthSoM-V2I多模态感知-901a信数据集，基于LLaMA 3.2模型通过特征提取融合网络对齐多模态特征空间与语义空间，采用LoRA参数效率微调和传播觉知提示工程实现知识迁移

Result: 在LoS/NLoS分类中达到92.76%的准确度，多路径功率/延迟生成精度NMSE为0.099/0.032，在跨车辆流量密度、跨带宽和跨场景通用性方面超过传统深度学习方法，并通过真实场景验证了其实用性

Conclusion: LLM4MG方案成功将大语言模型应用于多路径生成任务，为6G V2I通信系统提供了高精度的频道模拟能力，高精度多路径生成对系统设计的必要性在频道容量比较中得到了证明

Abstract: Based on Synesthesia of Machines (SoM), a large language model (LLM) is
adapted for multipath generation (LLM4MG) for the first time. Considering a
typical sixth-generation (6G) vehicle-to-infrastructure (V2I) scenario, a new
multi-modal sensing-communication dataset is constructed, named SynthSoM-V2I,
including channel multipath information, millimeter wave (mmWave) radar sensory
data, RGB-D images, and light detection and ranging (LiDAR) point clouds. Based
on the SynthSoM-V2I dataset, the proposed LLM4MG leverages Large Language Model
Meta AI (LLaMA) 3.2 for multipath generation via multi-modal sensory data. The
proposed LLM4MG aligns the multi-modal feature space with the LLaMA semantic
space through feature extraction and fusion networks. To further achieve
general knowledge transfer from the pre-trained LLaMA for multipath generation
via multi-modal sensory data, the low-rank adaptation (LoRA)
parameter-efficient fine-tuning and propagation-aware prompt engineering are
exploited. Simulation results demonstrate that the proposed LLM4MG outperforms
conventional deep learning-based methods in terms of line-of-sight
(LoS)/non-LoS (NLoS) classification with accuracy of 92.76%, multipath
power/delay generation precision with normalized mean square error (NMSE) of
0.099/0.032, and cross-vehicular traffic density (VTD), cross-band, and
cross-scenario generalization. The utility of the proposed LLM4MG is validated
by real-world generalization. The necessity of high-precision multipath
generation for system design is also demonstrated by channel capacity
comparison.

</details>


### [13] [Efficient Solutions for Mitigating Initialization Bias in Unsupervised Self-Adaptive Auditory Attention Decoding](https://arxiv.org/abs/2509.14764)
*Yuanyuan Yao,Simon Geirnaert,Tinne Tuytelaars,Alexander Bertrand*

Main category: eess.SP

TL;DR: 这篇论文提出了三种计算效率更高的无监督听视注意力解码方法，解决了现有方法的初始化偏差和高计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 目前的无监督听视注意力解码方法存在初始化偏差或者计算复杂度过高的问题，需要发展更高效的算法来消除对标签数据的依赖。

Method: 提出了三种计算效率更高的无监督听视注意力解码方法，这些方法具有更低且固定的计算成本。

Result: 新方法在保持相似性能的同时，显著降低了计算复杂度，解决了现有方法的性能偏差问题。

Conclusion: 这些新方法为无监督听视注意力解码领域提供了更加高效可行的解决方案，有助于发展更实用的神经导向听力设备。

Abstract: Decoding the attended speaker in a multi-speaker environment from
electroencephalography (EEG) has attracted growing interest in recent years,
with neuro-steered hearing devices as a driver application. Current approaches
typically rely on ground-truth labels of the attended speaker during training,
necessitating calibration sessions for each user and each EEG set-up to achieve
optimal performance. While unsupervised self-adaptive auditory attention
decoding (AAD) for stimulus reconstruction has been developed to eliminate the
need for labeled data, it suffers from an initialization bias that can
compromise performance. Although an unbiased variant has been proposed to
address this limitation, it introduces substantial computational complexity
that scales with data size. This paper presents three computationally efficient
alternatives that achieve comparable performance, but with a significantly
lower and constant computational cost. The code for the proposed algorithms is
available at https://github.com/YYao-42/Unsupervised_AAD.

</details>


### [14] [Comparative Performance Analysis of Different Hybrid NOMA Schemes](https://arxiv.org/abs/2509.14809)
*Ning Wang,Chenyu Zhang,Yanshi Sun,Minghui Min,Shiyin Li*

Main category: eess.SP

TL;DR: 这篇论文分析了三种混合非正交多均访问(H-NOMA)方案在随机频道收益顺序下的性能，包括固定顺序SIC、非功率适应混合SIC和功率适应混合SIC方案，并与传统OMA进行性能对比。


<details>
  <summary>Details</summary>
Motivation: 现有H-NOMA分析多假设固定频道收益顺序，而实际频道系数是随机分布的，导致大小关系内在随机变化，需要研究随机频道收益顺序下的性能。

Method: 分析了三种H-NOMA方案：FSIC、HSIC-NPA和HSIC-PA。通过理论分析得到了H-NOMA方案超过传统OMA的概率表达式，并在高信噪比渡渐分析了异常性能。

Result: 模拟结果验证了理论分析，并展示了不同SNR场景下H-NOMA方案的性能表现，为下一代无线系统部署H-NOMA提供了理论基础。

Conclusion: 论文通过考虑随机频道收益顺序的实际情况，对三种H-NOMA方案进行了更为准确的性能分析，为未来无线网络中H-NOMA技术的应用提供了重要的理论支撑。

Abstract: Hybrid non-orthogonal multiple access (H-NOMA), which combines the advantages
of pure NOMA and conventional OMA organically, has emerged as a highly
promising multiple access technology for future wireless networks. Recent
studies have proposed various H-NOMA systems by employing different successive
interference cancellation (SIC) methods for the NOMA transmission phase.
However, existing analyses typically assume a fixed channel gain order between
paired users, despite the fact that channel coefficients follow random
distribution, leading to their magnitude relationships inherently stochastic
and time varying. This paper analyzes the performance of three H-NOMA schemes
under stochastic channel gain ordering: a) fixed order SIC (FSIC) aided H-NOMA
scheme; b) hybrid SIC with non-power adaptation (HSIC-NPA) aided H-NOMA scheme;
c) hybrid SIC with power adaptation (HSIC-PA) aided H-NOMA scheme. Theoretical
analysis derives closed-form expressions for the probability that H-NOMA
schemes underperform conventional OMA. Asymptotic results in the high
signal-to-noise ratio (SNR) regime are also developed. Simulation results
validate our analysis and demonstrate the performance of H-NOMA schemes across
different SNR scenarios, providing a theoretical foundation for the deployment
of H-NOMA in next-generation wireless systems.

</details>


### [15] [Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization](https://arxiv.org/abs/2509.14836)
*Keitaro Yamashita,Kazuki Naganuma,Shunsuke Ono*

Main category: eess.SP

TL;DR: 提出一种基于广义采样理论的图信号顶点灵活采样方法，通过优化设计采样算子实现最佳恢复性能，支持控制活跃顶点数量和融入先验知识


<details>
  <summary>Details</summary>
Motivation: 现有顶点灵活采样方法虽然能控制活跃顶点数量，但无法融入必须采样或禁止采样的先验知识，需要解决这一限制

Method: 将算子设计问题转化为DC优化问题，使用核范数和DC惩罚处理顶点选择，开发基于双近端梯度DC算法的收敛求解器

Result: 在各种图信号模型和真实数据上的实验表明，该方法在恢复精度上优于现有方法

Conclusion: 所提出的方法能够有效处理顶点数量约束和先验知识，在图信号采样恢复方面表现出优越性能

Abstract: This paper proposes a method for vertex-wise flexible sampling of a broad
class of graph signals, designed to attain the best possible recovery based on
the generalized sampling theory. This is achieved by designing a sampling
operator by an optimization problem, which is inherently non-convex, as the
best possible recovery imposes a rank constraint. An existing method for
vertex-wise flexible sampling is able to control the number of active vertices
but cannot incorporate prior knowledge of mandatory or forbidden vertices. To
address these challenges, we formulate the operator design as a problem that
handles a constraint of the number of active vertices and prior knowledge on
specific vertices for sampling, mandatory inclusion or exclusion. We
transformed this constrained problem into a difference-of-convex (DC)
optimization problem by using the nuclear norm and a DC penalty for vertex
selection. To solve this, we develop a convergent solver based on the general
double-proximal gradient DC algorithm. The effectiveness of our method is
demonstrated through experiments on various graph signal models, including
real-world data, showing superior performance in the recovery accuracy by
comparing to existing methods.

</details>


### [16] [Hybrid Table-Assisted and RL-Based Dynamic Routing for NGSO Satellite Networks](https://arxiv.org/abs/2509.14909)
*Flor Ortiz,Eva Lagunas*

Main category: eess.SP

TL;DR: 这篇论文提出了一种混合动态路由策略，在NGSO卫星网络中结合预计算路由表和深度Q学习备用机制，以应对网络拓扑动态性和流量压力。


<details>
  <summary>Details</summary>
Motivation: 完全基于强化学习的路由方案虽然能够适应拓扑动态性，但存在复杂度高、收敛时间长和重载情况下性能不稳定等问题。需要一种更可扩展和强固的解决方案来支持延迟敏感的卫星广播服务。

Method: 提出混合路由框架，在正常条件下使用确定性的预计算路由表查找，仅在链路不可用或塞时才选择性激活深度Q学习代理。

Result: 在大规模NGSO网络中的模拟结果显示，混合方法相比纯强化学习基准方案，一质实现了更高的包交付率、更低的端到端延迟、更短的平均跳数和更高的吞吐量。

Conclusion: 这些发现高度识别了混合路由作为一种可扩展和强固的解决方案的有效性，适用于延迟敏感的卫星广播服务。

Abstract: This letter investigates dynamic routing in Next-Generation Satellite Orbit
(NGSO) constellations and proposes a hybrid strategy that combines precomputed
routing tables with a Deep Q-Learning (DQL) fallback mechanism. While fully
RL-based schemes offer adaptability to topology dynamics, they often suffer
from high complexity, long convergence times, and unstable performance under
heavy traffic. In contrast, the proposed framework exploits deterministic table
lookups under nominal conditions and selectively activates the DQL agent only
when links become unavailable or congested. Simulation results in large-scale
NGSO networks show that the hybrid approach consistently achieves higher packet
delivery ratio, lower end-to-end delay, shorter average hop count, and improved
throughput compared to a pure RL baseline. These findings highlight the
effectiveness of hybrid routing as a scalable and resilient solution for
delay-sensitive satellite broadband services

</details>


### [17] [Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators](https://arxiv.org/abs/2509.15069)
*Deijany Rodriguez Linares,Oksana Moryakova,Håkan Johansson*

Main category: eess.SP

TL;DR: 通过涛漏累加器结构，高效计算时间索引加权和，将乘法操作从K×N次降低到K+1次常数乘法


<details>
  <summary>Details</summary>
Motivation: 传统方法需要K×N次一般乘法，计算成本较高，而基于查找表或信号反转的方法需要存储整个数据块

Method: 利用涛漏累加器的特性，构建级联累加器结构来消除数据存储需求，并将乘法操作减少为K+1次常数乘法

Result: 提出的方法能够在不需要存储整个数据块的情况下，实现高效的实时计算，适用于根据样本逐个处理的系统

Conclusion: 该方法通过涛漏累加器结构显著提高了时间索引加权和的计算效率，适合实时处理应用

Abstract: This letter presents a novel approach for \mbox{efficiently} computing
time-index powered weighted sums of the form $\sum_{n=0}^{N-1} n^{K} v[n]$
using cascaded accumulators. Traditional direct computation requires
$K{\times}N$ general multiplications, which become prohibitive for large $N$,
while alternative strategies based on lookup tables or signal reversal require
storing entire data blocks. By exploiting accumulator properties, the proposed
method eliminates the need for such storage and reduces the multiplicative cost
to only $K{+}1$ constant multiplications, enabling efficient real-time
implementation. The approach is particularly useful when such sums need to be
efficiently computed in sample-by-sample processing systems.

</details>


### [18] [Doppler Radiance Field-Guided Antenna Selection for Improved Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition](https://arxiv.org/abs/2509.15129)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 提出基于多普勒辐射场(DoRF)的Wi-Fi感知框架，通过多天线AP噪声抑制和最优天线选择，显著提升小规模手势识别的泛化能力


<details>
  <summary>Details</summary>
Motivation: Wi-Fi CSI信号受AP时钟异步和环境硬件噪声影响，现有预处理方法仍无法有效处理CSI数据和多普勒速度投影中的噪声和异常值，限制了基于Wi-Fi的人类活动识别性能

Method: 提出多天线AP框架，基于DoRF拟合误差抑制噪声并识别最具信息量的天线，利用多普勒速度投影的不一致性来指导Wi-Fi HAR

Result: 在具有挑战性的小规模手势识别数据集上，所提出的DoRF引导方法显著提高了泛化能力

Conclusion: 该方法为鲁棒的实时感知部署铺平了道路，通过噪声抑制和天线选择优化提升了Wi-Fi基于感知的性能

Abstract: With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard
for advanced sensing, interest in using Wi-Fi Channel State Information (CSI)
for remote sensing has surged. Recent findings indicate that learning a unified
three-dimensional motion representation through Doppler Radiance Fields (DoRFs)
derived from CSI significantly improves the generalization capabilities of
Wi-Fi-based human activity recognition (HAR). Despite this progress, CSI
signals remain affected by asynchronous access point (AP) clocks and additive
noise from environmental and hardware sources. Consequently, even with existing
preprocessing techniques, both the CSI data and Doppler velocity projections
used in DoRFs are still susceptible to noise and outliers, limiting HAR
performance. To address this challenge, we propose a novel framework for
multi-antenna APs to suppress noise and identify the most informative antennas
based on DoRF fitting errors, which capture inconsistencies among Doppler
velocity projections. Experimental results on a challenging small-scale hand
gesture recognition dataset demonstrate that the proposed DoRF-guided
Wi-Fi-based HAR approach significantly improves generalization capability,
paving the way for robust real-world sensing deployments.

</details>


### [19] [A Unified Distributed Algorithm for Hybrid Near-Far Field Activity Detection in Cell-Free Massive MIMO](https://arxiv.org/abs/2509.15162)
*Jingreng Lei,Yang Li,Ziyue Wang,Qingfeng Lin,Ya-Feng Liu,Yik-Chung Wu*

Main category: eess.SP

TL;DR: 这篇论文提出了一种分布式算法，用于处理细胞免MIMO系统中混合近远场通信的设备活动检测问题，通过协方差基源模型和分布式处理显著提高了性能并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着MIMO系统天线数量增加，远场传播假设已不够实用，需要合并考虑近场和远场混合通信的设备活动检测方案。

Method: 建立协方差基源模型描述混合近远场通道特性，提出分布式算法，每个接入点进行本地检测后与中央处理单元交换结果，降低计算复杂度和通信开销。

Result: 理论分析表明近场通道比例增加能提升检测性能，模拟结果验证了该算法的优越性和收敛性，超越现有方法。

Conclusion: 该算法不仅有收敛保证，而且能够统一处理单细胞或细胞免系统中的近远场设备检测问题，为大规模机器类通信提供了高效解决方案。

Abstract: A great amount of endeavor has recently been devoted to activity detection
for massive machine-type communications in cell-free multiple-input
multiple-output (MIMO) systems. However, as the number of antennas at the
access points (APs) increases, the Rayleigh distance that separates the
near-field and far-field regions also expands, rendering the conventional
assumption of far-field propagation alone impractical. To address this
challenge, this paper establishes a covariance-based formulation that can
effectively capture the statistical property of hybrid near-far field channels.
Based on this formulation, we theoretically reveal that increasing the
proportion of near-field channels enhances the detection performance.
Furthermore, we propose a distributed algorithm, where each AP performs local
activity detection and only exchanges the detection results to the central
processing unit, thus significantly reducing the computational complexity and
the communication overhead. Not only with convergence guarantee, the proposed
algorithm is unified in the sense that it can handle single-cell or cell-free
systems with either near-field or far-field devices as special cases.
Simulation results validate the theoretical analyses and demonstrate the
superior performance of the proposed approach compared with existing methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [20] [SpeechOp: Inference-Time Task Composition for Generative Speech Processing](https://arxiv.org/abs/2509.14298)
*Justin Lovelace,Rithesh Kumar,Jiaqi Su,Ke Chen,Kilian Q Weinberger,Zeyu Jin*

Main category: eess.AS

TL;DR: SpeechOp是一个基于预训练TTS模型的多任务潜在扩散模型，可将TTS系统转化为通用语音处理器，通过隐式任务组合实现语音增强等多种任务。


<details>
  <summary>Details</summary>
Motivation: 解决语音到语音处理任务（如增强）面临的数据限制问题，避免生成式方法扭曲语音内容和说话人身份。

Method: 通过适配预训练TTS模型构建多任务潜在扩散模型，引入隐式任务组合（ITC）管道，利用ASR转录本指导语音增强。

Result: 实现了最先进的内容保持性能，同时提升了核心TTS性能，加速了训练过程。

Conclusion: SpeechOp成功将预训练TTS模型转化为通用语音处理器，通过结合网络规模语音理解和生成能力，在多种语音任务上表现出色。

Abstract: While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild"
data to achieve remarkable success, speech-to-speech processing tasks like
enhancement face data limitations, which lead data-hungry generative approaches
to distort speech content and speaker identity. To bridge this gap, we present
SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS
models into a universal speech processor capable of performing a wide range of
speech tasks and composing them in novel ways at inference time. By adapting a
pre-trained TTS model, SpeechOp inherits a rich understanding of natural
speech, accelerating training and improving S2S task quality, while
simultaneously enhancing core TTS performance. Finally, we introduce Implicit
Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g.,
from Whisper) guide SpeechOp's enhancement via our principled inference-time
task composition. ITC achieves state-of-the-art content preservation by
robustly combining web-scale speech understanding with SpeechOp's generative
capabilities. Audio samples are available at
https://justinlovelace.github.io/projects/speechop

</details>


### [21] [Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior](https://arxiv.org/abs/2509.14379)
*Yochai Yemini,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya*

Main category: eess.AS

TL;DR: 提出一种生成式无监督单麦克风语音分离方法，通过直接建模纯净语音和环境噪声分布，利用音频-视觉评分模型和反向扩散过程实现噪声环境下的有效语音分离。


<details>
  <summary>Details</summary>
Motivation: 解决单麦克风在环境噪声存在情况下的语音分离问题，传统方法需要噪声混合数据进行训练，而本文旨在通过直接建模纯净语音和结构化噪声组件来实现无监督分离。

Method: 使用生成式无监督技术，直接建模纯净语音和结构化噪声分布，利用音频-视觉评分模型整合视觉线索作为强生成语音先验，通过反向扩散过程从后验分布采样进行语音分离。

Result: 实验结果表明该方法在挑战性声学环境中表现出有希望的性能，验证了直接噪声建模方法的有效性。

Conclusion: 提出的直接噪声建模方法能够有效实现噪声环境下的单麦克风语音分离，为无监督语音处理提供了新的解决方案。

Abstract: In this paper, we address the problem of single-microphone speech separation
in the presence of ambient noise. We propose a generative unsupervised
technique that directly models both clean speech and structured noise
components, training exclusively on these individual signals rather than noisy
mixtures. Our approach leverages an audio-visual score model that incorporates
visual cues to serve as a strong generative speech prior. By explicitly
modelling the noise distribution alongside the speech distribution, we enable
effective decomposition through the inverse problem paradigm. We perform speech
separation by sampling from the posterior distributions via a reverse diffusion
process, which directly estimates and removes the modelled noise component to
recover clean constituent signals. Experimental results demonstrate promising
performance, highlighting the effectiveness of our direct noise modelling
approach in challenging acoustic environments.

</details>


### [22] [Multi-Channel Differential ASR for Robust Wearer Speech Recognition on Smart Glasses](https://arxiv.org/abs/2509.14430)
*Yufeng Yang,Yiteng Huang,Yong Xu,Li Wan,Suwon Shon,Yang Liu,Yifeng Fan,Zhaojun Yang,Olivier Siohan,Yue Liu,Ming Sun,Florian Metze*

Main category: eess.AS

TL;DR: 提出一种多通道差分语音识别方法，通过组合调度器、麦克风选择和轻量级旁边话检测模型，在智能眼镜上实现更稳健的戳戴者语音识别，并在实际环境中将词误率相对降低18.0%。


<details>
  <summary>Details</summary>
Motivation: 随着智能眼镜等可戴设备的普及，戳戴者语音识别变得越来越重要。但在实际环境中，旁边话语的干扰仍然是一个重大挑战，可能导致下游任务的累积错误。

Method: 提出一种新的多通道差分自动语音识别方法，系统采用互相补充的不同前端输入，包括调度器、麦克风选择和轻量级旁边话检测模型，以提高WSR的稳健性。

Result: 在模拟和实际数据集上的评估显示，该系统超越了传统方法，实现了词误率相对降低18.0%。

Conclusion: 该研究提出的多通道差分ASR方法能够有效地应对实际环境中的旁边话干扰问题，为智能眼镜上的戳戴者语音识别提供了更稳健的解决方案。

Abstract: With the growing adoption of wearable devices such as smart glasses for AI
assistants, wearer speech recognition (WSR) is becoming increasingly critical
to next-generation human-computer interfaces. However, in real environments,
interference from side-talk speech remains a significant challenge to WSR and
may cause accumulated errors for downstream tasks such as natural language
processing. In this work, we introduce a novel multi-channel differential
automatic speech recognition (ASR) method for robust WSR on smart glasses. The
proposed system takes differential inputs from different frontends that
complement each other to improve the robustness of WSR, including a beamformer,
microphone selection, and a lightweight side-talk detection model. Evaluations
on both simulated and real datasets demonstrate that the proposed system
outperforms the traditional approach, achieving up to an 18.0% relative
reduction in word error rate.

</details>


### [23] [Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation](https://arxiv.org/abs/2509.14632)
*Miseul Kim,Soo Jin Park,Kyungguen Byun,Hyeon-Kyeong Shin,Sunkuk Moon,Shuhua Zhang,Erik Visser*

Main category: eess.AS

TL;DR: 提出了一种风格可控的语音生成模型，通过生成具有语音和风格多样性的增强语音样本来提高说话人日志系统对说话人内部高变异性的鲁棒性，在两个数据集上分别实现了49%和35%的错误率降低。


<details>
  <summary>Details</summary>
Motivation: 解决说话人日志系统在处理高内在说话人内部变异性（如情绪、健康或内容变化）时的困难，避免同一说话人的片段因风格变化而被错误分类为不同个体。

Method: 使用风格可控的语音生成模型，从传统日志器获得的日志化片段生成具有语音和风格多样性的增强语音样本，然后融合原始音频和生成音频的说话人嵌入来增强系统鲁棒性。

Result: 在模拟情感语音数据集和截断的AMI数据集上验证，错误率分别降低了49%和35%，显示出显著改进。

Conclusion: 提出的风格可控语音生成和嵌入融合方法有效提高了说话人日志系统对说话人内部变异性的处理能力，显著降低了错误率。

Abstract: Speaker diarization systems often struggle with high intrinsic intra-speaker
variability, such as shifts in emotion, health, or content. This can cause
segments from the same speaker to be misclassified as different individuals,
for example, when one raises their voice or speaks faster during conversation.
To address this, we propose a style-controllable speech generation model that
augments speech across diverse styles while preserving the target speaker's
identity. The proposed system starts with diarized segments from a conventional
diarizer. For each diarized segment, it generates augmented speech samples
enriched with phonetic and stylistic diversity. And then, speaker embeddings
from both the original and generated audio are blended to enhance the system's
robustness in grouping segments with high intrinsic intra-speaker variability.
We validate our approach on a simulated emotional speech dataset and the
truncated AMI dataset, demonstrating significant improvements, with error rate
reductions of 49% and 35% on each dataset, respectively.

</details>


### [24] [Enhancing Situational Awareness in Wearable Audio Devices Using a Lightweight Sound Event Localization and Detection System](https://arxiv.org/abs/2509.14650)
*Jun-Wei Yeow,Ee-Leng Tan,Santi Peksi,Zhen-Ting Ong,Woon-Seng Gan*

Main category: eess.AS

TL;DR: 这篇论文提出了一种结合音响场景分类(ASC)和声音事件定位与检测(SELD)的环境智能框架，用于提升可穿戴音频设备的情境感知能力和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决主动噪声控制(ANC)可穿戴设备在提供舒适性的同时导致环境感知不足的问题，避免消除重要环境线索而造成安全风险。

Method: 首先使用轻量级ASC模型推断当前环境，然后根据场景预测动态调整SELD网络的敏感度，以检测和定位当前上下文中最重要的声音。

Result: 在模拟耳机数据上，该ASC条件SELD系统比传统基准系统展现出更好的空间智能。

Conclusion: 这项工作是向智能可穿戴音频设备发展的重要步骤，能够传递关键环境信息，创造更安全、更具上下文感知的听觉体验。

Abstract: Wearable audio devices with active noise control (ANC) enhance listening
comfort but often at the expense of situational awareness. However, this
auditory isolation may mask crucial environmental cues, posing significant
safety risks. To address this, we propose an environmental intelligence
framework that combines Acoustic Scene Classification (ASC) with Sound Event
Localization and Detection (SELD). Our system first employs a lightweight ASC
model to infer the current environment. The scene prediction then dynamically
conditions a SELD network, tuning its sensitivity to detect and localize sounds
that are most salient to the current context. On simulated headphone data, the
proposed ASC-conditioned SELD system demonstrates improved spatial intelligence
over a conventional baseline. This work represents a crucial step towards
creating intelligent hearables that can deliver crucial environmental
information, fostering a safer and more context-aware listening experience.

</details>


### [25] [Aligning Audio Captions with Human Preferences](https://arxiv.org/abs/2509.14659)
*Kartik Hegde,Rehana Mahfuz,Yinyi Guo,Erik Visser*

Main category: eess.AS

TL;DR: 基于RLHF的音频描述框架，通过对比预训练奖励模型和强化学习，在不依赖标注数据的情况下生成更符合人类偏好的音频描述


<details>
  <summary>Details</summary>
Motivation: 解决传统监督学习方法对成对音频-描述数据集的依赖性，这些数据集成本高且可能与实际场景中的人类偏好不一致

Method: 训练CLAP基础的奖励模型（使用人类标注的成对偏好数据），然后将其集成到强化学习框架中对基线描述系统进行微调

Result: 多个数据集的人类评估显示，该方法生成的描述更受偏好，尤其在基线模型失败的情况下能提供正确且自然的描述

Conclusion: 该框架能够在不依赖标准标注的情况下达到与监督方法相当的性能，有效对齐音频描述与人类偏好，具有良好的实际应用潜力

Abstract: Current audio captioning systems rely heavily on supervised learning with
paired audio-caption datasets, which are expensive to curate and may not
reflect human preferences in real-world scenarios. To address this limitation,
we propose a preference-aligned audio captioning framework based on
Reinforcement Learning from Human Feedback (RLHF). To effectively capture
nuanced human preferences, we train a Contrastive Language-Audio Pretraining
(CLAP)-based reward model using human-labeled pairwise preference data. This
reward model is integrated into a reinforcement learning framework to fine-tune
any baseline captioning system without relying on ground-truth caption
annotations. Extensive human evaluations across multiple datasets show that our
method produces captions preferred over those from baseline models,
particularly in cases where the baseline models fail to provide correct and
natural captions. Furthermore, our framework achieves performance comparable to
supervised approaches with ground-truth data, demonstrating its effectiveness
in aligning audio captioning with human preferences and its scalability in
real-world scenarios.

</details>


### [26] [SpeechMLC: Speech Multi-label Classification](https://arxiv.org/abs/2509.14677)
*Miseul Kim,Seyun Um,Hyeonjin Cha,Hong-goo Kang*

Main category: eess.AS

TL;DR: 提出了一个多标签分类框架来检测语音样本中的多种说话风格，使用交叉注意力机制和语音生成数据增强技术，在多个语料库上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统研究主要关注单一说话风格的识别，但实际应用中需要同时检测多种说话风格特征，以支持更通用的人机交互应用。

Method: 在transformer解码器中集成交叉注意力机制来提取每个目标标签的显著特征，并使用基于语音生成模型的数据增强技术来解决数据不平衡问题。

Result: 在可见和未见语料库上通过多个客观评估验证了模型的有效性，并分析了人类标注一致性对分类准确率的影响。

Conclusion: 该框架能够有效捕捉多种说话者特征，为通用人机交互应用提供了实用的多标签说话风格检测解决方案。

Abstract: In this paper, we propose a multi-label classification framework to detect
multiple speaking styles in a speech sample. Unlike previous studies that have
primarily focused on identifying a single target style, our framework
effectively captures various speaker characteristics within a unified
structure, making it suitable for generalized human-computer interaction
applications. The proposed framework integrates cross-attention mechanisms
within a transformer decoder to extract salient features associated with each
target label from the input speech. To mitigate the data imbalance inherent in
multi-label speech datasets, we employ a data augmentation technique based on a
speech generation model. We validate our model's effectiveness through multiple
objective evaluations on seen and unseen corpora. In addition, we provide an
analysis of the influence of human perception on classification accuracy by
considering the impact of human labeling agreement on model performance.

</details>


### [27] [DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis](https://arxiv.org/abs/2509.14684)
*Ye-Xin Lu,Yu Gu,Kun Wei,Hui-Peng Du,Yang Ai,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: DAIEN-TTS是一个零样本文本转语音框架，通过解耦音频填充实现环境感知的语音合成，可独立控制音色和背景环境。


<details>
  <summary>Details</summary>
Motivation: 传统TTS系统难以同时控制语音的音色特征和背景环境特征，需要一种能够解耦控制这两个维度的合成方法。

Method: 基于F5-TTS构建，使用预训练语音-环境分离模块解耦环境语音，应用随机跨度掩码，采用双无分类引导和信噪比自适应策略。

Result: 实验结果表明DAIEN-TTS能够生成具有高自然度、强说话人相似度和高环境保真度的环境个性化语音。

Conclusion: 该框架成功实现了对语音音色和背景环境的独立控制，为零样本环境感知语音合成提供了有效解决方案。

Abstract: This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework
that enables ENvironment-aware synthesis through Disentangled Audio Infilling.
By leveraging separate speaker and environment prompts, DAIEN-TTS allows
independent control over the timbre and the background environment of the
synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first
incorporates a pretrained speech-environment separation (SES) module to
disentangle the environmental speech into mel-spectrograms of clean speech and
environment audio. Two random span masks of varying lengths are then applied to
both mel-spectrograms, which, together with the text embedding, serve as
conditions for infilling the masked environmental mel-spectrogram, enabling the
simultaneous continuation of personalized speech and time-varying environmental
audio. To further enhance controllability during inference, we adopt dual
class-free guidance (DCFG) for the speech and environment components and
introduce a signal-to-noise ratio (SNR) adaptation strategy to align the
synthesized speech with the environment prompt. Experimental results
demonstrate that DAIEN-TTS generates environmental personalized speech with
high naturalness, strong speaker similarity, and high environmental fidelity.

</details>


### [28] [MELA-TTS: Joint transformer-diffusion model with representation alignment for speech synthesis](https://arxiv.org/abs/2509.14784)
*Keyu An,Zhiyu Zhang,Changfeng Gao,Yabin Li,Zhendong Peng,Haoxu Wang,Zhihao Du,Han Zhao,Zhifu Gao,Xiangang Li*

Main category: eess.AS

TL;DR: MELA-TTS是一个基于transformer-diffusion的端到端文本转语音框架，无需语音标记化和多阶段处理，通过表示对齐模块提升训练效率和跨模态一致性，在多项指标上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统通常需要语音标记化和多阶段处理流程，存在复杂性和性能限制。作者希望开发一个能够直接生成连续梅尔频谱图的端到端框架，避免离散标记的局限性。

Method: 提出联合transformer-diffusion架构，自回归生成连续梅尔频谱帧；引入表示对齐模块，在训练时将transformer解码器输出与预训练ASR编码器的语义嵌入对齐；支持离线和流式合成模式。

Result: 在多个评估指标上达到最先进性能；保持强大的零样本语音克隆能力；训练收敛速度更快；跨模态一致性得到显著提升。

Conclusion: MELA-TTS为连续特征生成方法在TTS领域建立了新基准，为基于离散标记的范式提供了有吸引力的替代方案，展示了端到端连续特征生成的优越性。

Abstract: This work introduces MELA-TTS, a novel joint transformer-diffusion framework
for end-to-end text-to-speech synthesis. By autoregressively generating
continuous mel-spectrogram frames from linguistic and speaker conditions, our
architecture eliminates the need for speech tokenization and multi-stage
processing pipelines. To address the inherent difficulties of modeling
continuous features, we propose a representation alignment module that aligns
output representations of the transformer decoder with semantic embeddings from
a pretrained ASR encoder during training. This mechanism not only speeds up
training convergence, but also enhances cross-modal coherence between the
textual and acoustic domains. Comprehensive experiments demonstrate that
MELA-TTS achieves state-of-the-art performance across multiple evaluation
metrics while maintaining robust zero-shot voice cloning capabilities, in both
offline and streaming synthesis modes. Our results establish a new benchmark
for continuous feature generation approaches in TTS, offering a compelling
alternative to discrete-token-based paradigms.

</details>


### [29] [Acoustic Simulation Framework for Multi-channel Replay Speech Detection](https://arxiv.org/abs/2509.14789)
*Michael Neri,Tuomas Virtanen*

Main category: eess.AS

TL;DR: 本文提出了一种声学模拟框架，用公开资源模拟多通道重放语音攻击，以提高语音助手系统的防护能力。


<details>
  <summary>Details</summary>
Motivation: 重放语音攻击对语音控制系统构成严重威胁，而现有数据集和方法主要依赖单通道录音，多通道音频可以提供空间线索增强检测程度。

Method: 设计声学模拟框架，模拟真实和欺骗语音，包括麦克风和扬声器冲击响应、房间声学和噪声条件，使用测量的扬声器方向性提高模拟真实性。

Result: 使用最新的M-ALRAD模型进行检测，证明合成数据可以支持检测器在未见环境中的泛化能力。

Conclusion: 该模拟框架能够有效地模拟多通道重放语音攻击，为语音助手系统提供更强大的安全防护。

Abstract: Replay speech attacks pose a significant threat to voice-controlled systems,
especially in smart environments where voice assistants are widely deployed.
While multi-channel audio offers spatial cues that can enhance replay detection
robustness, existing datasets and methods predominantly rely on single-channel
recordings. In this work, we introduce an acoustic simulation framework
designed to simulate multi-channel replay speech configurations using publicly
available resources. Our setup models both genuine and spoofed speech across
varied environments, including realistic microphone and loudspeaker impulse
responses, room acoustics, and noise conditions. The framework employs measured
loudspeaker directionalities during the replay attack to improve the realism of
the simulation. We define two spoofing settings, which simulate whether a
reverberant or an anechoic speech is used in the replay scenario, and evaluate
the impact of omnidirectional and diffuse noise on detection performance. Using
the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate
that synthetic data can support the generalization capabilities of the detector
across unseen enclosures.

</details>


### [30] [AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding and Dropout-Based Learning](https://arxiv.org/abs/2509.14855)
*Michael Tatarjitzky,Boaz Rafaely*

Main category: eess.AS

TL;DR: AmbiDrop是一个基于Ambisonics的阵列无关语音增强框架，通过球谐域编码和通道dropout技术，无需多样化麦克风阵列数据库即可实现对新阵列布局的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统多通道语音增强方法依赖特定麦克风阵列几何结构，无法适应几何变化。现有阵列无关方法需要大量多几何数据集但仍难以泛化到未见过的阵列布局。

Method: 提出AmbiDrop框架：1）使用Ambisonics信号匹配将任意阵列录音编码到球谐域；2）在模拟Ambisonics数据上训练深度神经网络；3）结合通道dropout技术增强对阵列相关编码错误的鲁棒性

Result: 实验表明：基线模型在未见阵列上性能下降，而AmbiDrop在SI-SDR、PESQ和STOI指标上持续提升，展现出强泛化能力

Conclusion: AmbiDrop通过Ambisonics编码和dropout技术，实现了阵列无关的语音增强，具有实际应用潜力，无需依赖多样化麦克风阵列数据库

Abstract: Multichannel speech enhancement leverages spatial cues to improve
intelligibility and quality, but most learning-based methods rely on specific
microphone array geometry, unable to account for geometry changes. To mitigate
this limitation, current array-agnostic approaches employ large multi-geometry
datasets but may still fail to generalize to unseen layouts. We propose
AmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes
arbitrary array recordings into the spherical harmonics domain using Ambisonics
Signal Matching (ASM). A deep neural network is trained on simulated Ambisonics
data, combined with channel dropout for robustness against array-dependent
encoding errors, therefore omitting the need for a diverse microphone array
database. Experiments show that while the baseline and proposed models perform
similarly on the training arrays, the baseline degrades on unseen arrays. In
contrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating
strong generalization and practical potential for array-agnostic speech
enhancement.

</details>


### [31] [Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance](https://arxiv.org/abs/2509.14934)
*Francisco Messina,Francesca Ronchini,Luca Comanducci,Paolo Bestagini,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本文探讨了在文本到音频扩散模型中使用反记忆化策略来防止数据复制问题，通过Anti-Memorization Guidance技术显著减少了记忆化现象，同时保持了音频质量。


<details>
  <summary>Details</summary>
Motivation: 生成音频模型面临数据复制的挑战，模型在推理过程中无意中生成训练数据的部分内容，这需要有效的反记忆化解决方案。

Method: 采用Anti-Memorization Guidance (AMG)技术，修改预训练扩散模型的采样过程来抑制记忆化。研究探索了三种不同的指导类型，使用Stable Audio Open作为基础架构。

Result: 综合实验分析表明，AMG显著减轻了基于扩散的文本到音频生成中的记忆化现象，同时没有损害音频保真度或语义对齐。

Conclusion: AMG技术有效解决了文本到音频扩散模型中的数据复制问题，为生成音频模型提供了实用的反记忆化解决方案。

Abstract: A persistent challenge in generative audio models is data replication, where
the model unintentionally generates parts of its training data during
inference. In this work, we address this issue in text-to-audio diffusion
models by exploring the use of anti-memorization strategies. We adopt
Anti-Memorization Guidance (AMG), a technique that modifies the sampling
process of pre-trained diffusion models to discourage memorization. Our study
explores three types of guidance within AMG, each designed to reduce
replication while preserving generation quality. We use Stable Audio Open as
our backbone, leveraging its fully open-source architecture and training
dataset. Our comprehensive experimental analysis suggests that AMG
significantly mitigates memorization in diffusion-based text-to-audio
generation without compromising audio fidelity or semantic alignment.

</details>


### [32] [SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding](https://arxiv.org/abs/2509.14946)
*Bingsong Bai,Qihang Lu,Wenbing Yang,Zihan Sun,YueRan Hou,Peilei Jia,Songbai Pu,Ruibo Fu,Yingming Gao,Ya Li,Jun Gao*

Main category: eess.AS

TL;DR: 自动化框架生成大规模语调语音数据集SynParaSpeech，包含6个类别、118.75小时数据，提高语音生成和识别的自然性


<details>
  <summary>Details</summary>
Motivation: 解决现有语调语音数据集存在的问题：依赖专有数据、语音不完整、时间戳不准确或缺失、实际应用性有限

Method: 提出自动化框架，从自然对话语音中生成大规模语调数据集SynParaSpeech，包含6个语调类别，具有精确时间戳

Result: 构建了包含118.75小时数据的SynParaSpeech数据集，为语调语音生成提供了更自然的合成能力，同时提升了语调事件检测的性能

Conclusion: 该研究首次提出自动化构建大规模语调数据集的方法，提供的SynParaSpeech数据集在语音生成和识别领域都发挥重要作用，数据集和音频样本已开源

Abstract: Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing
more realistic and engaging speech. However, existing methods typically depend
on proprietary datasets, while publicly available resources often suffer from
incomplete speech, inaccurate or missing timestamps, and limited real-world
relevance. To address these problems, we propose an automated framework for
generating large-scale paralinguistic data and apply it to construct the
SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with
118.75 hours of data and precise timestamps, all derived from natural
conversational speech. Our contributions lie in introducing the first automated
method for constructing large-scale paralinguistic datasets and releasing the
SynParaSpeech corpus, which advances speech generation through more natural
paralinguistic synthesis and enhances speech understanding by improving
paralinguistic event detection. The dataset and audio samples are available at
https://github.com/ShawnPi233/SynParaSpeech.

</details>


### [33] [Discrete optimal transport is a strong audio adversarial attack](https://arxiv.org/abs/2509.14959)
*Anton Selitskiy,Akib Shahriyar,Jishnuraj Prakasan*

Main category: eess.AS

TL;DR: 离散最优传输(DOT)作为黑盒对抗攻击，通过分布对齐方式有效攻击音频反欺骗系统，在跨数据集迁移中表现优于传统攻击方法


<details>
  <summary>Details</summary>
Motivation: 针对现代音频反欺骗系统(CMs)开发有效的黑盒对抗攻击方法，探索分布级对齐作为攻击面的潜力

Method: 采用离散最优传输作为后处理步骤，将生成语音的WavLM帧级嵌入通过熵最优传输和top-k重心投影与真实语音池对齐，然后用神经声码器解码

Result: 在ASVspoof2019和ASVspoof5数据集上，DOT攻击始终获得高EER，在CM微调后仍保持竞争力，在跨数据集迁移中优于多种传统攻击方法

Conclusion: 分布级对齐是部署CMs的强大且稳定的攻击面，声码器重叠对实际效果有重要影响

Abstract: In this paper, we show that discrete optimal transport (DOT) is an effective
black-box adversarial attack against modern audio anti-spoofing countermeasures
(CMs). Our attack operates as a post-processing, distribution-alignment step:
frame-level WavLM embeddings of generated speech are aligned to an unpaired
bona fide pool via entropic OT and a top-$k$ barycentric projection, then
decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with
AASIST baselines, DOT yields consistently high equal error rate (EER) across
datasets and remains competitive after CM fine-tuning, outperforming several
conventional attacks in cross-dataset transfer. Ablation analysis highlights
the practical impact of vocoder overlap. Results indicate that
distribution-level alignment is a powerful and stable attack surface for
deployed CMs.

</details>


### [34] [BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings](https://arxiv.org/abs/2509.15001)
*Théo Charlot,Tarek Kunze,Maxime Poli,Alejandrina Cristia,Emmanuel Dupoux,Marvin Lavechin*

Main category: eess.AS

TL;DR: BabyHuBERT是首个基于13,000小时多语言儿童长时录音训练的自监督语音表示模型，在儿童语音识别任务上显著优于现有模型


<details>
  <summary>Details</summary>
Motivation: 现有基于成人清晰语音训练的模型在儿童语音数据上表现不佳，因为儿童语音在声学和语言学特征上与成人存在显著差异

Method: 使用13,000小时涵盖40多种语言的多语言儿童中心长时录音训练自监督语音表示模型BabyHuBERT

Result: 在6个不同数据集上F1分数达到52.1%-74.4%，比W2V2-LL4300和标准HuBERT表现更好，在Vanuatu和Solomon Islands语料上分别提升13.2和15.9个F1点

Conclusion: BabyHuBERT作为儿童语音研究的基础模型，可为各种下游任务提供微调支持，并通过代码和模型共享促进研究发展

Abstract: Child-centered long-form recordings are essential for studying early language
development, but existing speech models trained on clean adult data perform
poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the
first self-supervised speech representation model trained on 13,000 hours of
multilingual child-centered long-form recordings spanning over 40 languages. We
evaluate BabyHuBERT on speaker segmentation, identifying when target children
speak versus female adults, male adults, or other children -- a fundamental
preprocessing step for analyzing naturalistic language experiences. BabyHuBERT
achieves F1-scores from 52.1% to 74.4% across six diverse datasets,
consistently outperforming W2V2-LL4300 (trained on English long-forms) and
standard HuBERT (trained on clean adult speech). Notable improvements include
13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon
Islands corpora, demonstrating effectiveness on underrepresented languages. By
sharing code and models, BabyHuBERT serves as a foundation model for child
speech research, enabling fine-tuning on diverse downstream tasks.

</details>


### [35] [Transfer Learning for Paediatric Sleep Apnoea Detection Using Physiology-Guided Acoustic Models](https://arxiv.org/abs/2509.15008)
*Chaoyue Niu,Veronica Rowe,Guy J. Brown,Heather Elphick,Heather Kenyon,Lowri Thomas,Sam Johnson,Ning Ma*

Main category: eess.AS

TL;DR: 提出基于迁移学习的声学监测框架，利用成人睡眠数据预训练模型，结合SpO2去饱和模式增强训练，用于儿童阻塞性睡眠呼吸暂停的家庭筛查。


<details>
  <summary>Details</summary>
Motivation: 儿童阻塞性睡眠呼吸暂停(OSA)临床意义重大但诊断困难，传统传感器多导睡眠图儿童耐受性差，声学监测提供无创替代方案，但儿科数据有限阻碍深度学习模型发展。

Method: 使用迁移学习框架，将成人睡眠数据(157晚)预训练的声学模型适配到儿科OSA检测，整合SpO2去饱和模式增强训练，系统评估单任务vs多任务学习、编码器冻结vs全微调、SpO2标签延迟对齐策略。

Result: 结果显示，结合SpO2整合的微调方法相比无适配基线模型，能持续改善儿科OSA检测性能。

Conclusion: 研究证明了迁移学习在儿童家庭OSA筛查中的可行性，展示了其在早期诊断中的潜在临床价值。

Abstract: Paediatric obstructive sleep apnoea (OSA) is clinically significant yet
difficult to diagnose, as children poorly tolerate sensor-based
polysomnography. Acoustic monitoring provides a non-invasive alternative for
home-based OSA screening, but limited paediatric data hinders the development
of robust deep learning approaches. This paper proposes a transfer learning
framework that adapts acoustic models pretrained on adult sleep data to
paediatric OSA detection, incorporating SpO2-based desaturation patterns to
enhance model training. Using a large adult sleep dataset (157 nights) and a
smaller paediatric dataset (15 nights), we systematically evaluate (i) single-
versus multi-task learning, (ii) encoder freezing versus full fine-tuning, and
(iii) the impact of delaying SpO2 labels to better align them with the
acoustics and capture physiologically meaningful features. Results show that
fine-tuning with SpO2 integration consistently improves paediatric OSA
detection compared with baseline models without adaptation. These findings
demonstrate the feasibility of transfer learning for home-based OSA screening
in children and offer its potential clinical value for early diagnosis.

</details>


### [36] [From Who Said What to Who They Are: Modular Training-free Identity-Aware LLM Refinement of Speaker Diarization](https://arxiv.org/abs/2509.15082)
*Yu-Wen Chen,William Ho,Maxim Topaz,Julia Hirschberg,Zoran Kostic*

Main category: eess.AS

TL;DR: 一种无需训练的模块化管线，结合现成语音分离、语音识别和大语言模型，通过结构化提示完喂语境中的讲者识别、语音内容和角色识别


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中语音分离面临的动态环境、未知讲者数量问题，以及非模块化方法的缺乏灵活性，同时满足应用对真实讲者身份的需求

Method: 使用结构化的LLM提示技术，对协调后的SD和ASR输出进行处理，利用对话语境中的语义连续性来精炼低信心度的讲者标签，分配角色身份并修正分裂讲者问题

Result: 在真实医患数据集上，该方法相比基线协调SD和ASR系统实现了29.7%的相对错误减少，在不需额外训练的情况下提升了分离性能

Conclusion: 该方法提供了一个完整的模块化管线，能够在实际应用中同时处理语音分离、语音识别和讲者身份检测，具有强大的实用价值

Abstract: Speaker diarization (SD) struggles in real-world scenarios due to dynamic
environments and unknown speaker counts. SD is rarely used alone and is often
paired with automatic speech recognition (ASR), but non-modular methods that
jointly train on domain-specific data have limited flexibility. Moreover, many
applications require true speaker identities rather than SD's pseudo labels. We
propose a training-free modular pipeline combining off-the-shelf SD, ASR, and a
large language model (LLM) to determine who spoke, what was said, and who they
are. Using structured LLM prompting on reconciled SD and ASR outputs, our
method leverages semantic continuity in conversational context to refine
low-confidence speaker labels and assigns role identities while correcting
split speakers. On a real-world patient-clinician dataset, our approach
achieves a 29.7% relative error reduction over baseline reconciled SD and ASR.
It enhances diarization performance without additional training and delivers a
complete pipeline for SD, ASR, and speaker identity detection in practical
applications.

</details>


### [37] [Real-Time Streaming Mel Vocoding with Generative Flow Matching](https://arxiv.org/abs/2509.15085)
*Simon Welker,Tal Peer,Timo Gerkmann*

Main category: eess.AS

TL;DR: MelFlow是一个基于流匹配的生成式Mel声码器，具有32ms算法延迟和48ms总延迟，支持实时流式处理，在PESQ和SI-SDR指标上优于HiFi-GAN等非流式基线模型


<details>
  <summary>Details</summary>
Motivation: Mel声码化（将Mel幅度谱转换为音频波形）仍然是现代TTS系统的关键组件，需要开发低延迟的流式处理能力

Method: 基于生成流匹配、DiffPhase的STFT相位恢复方法以及Mel滤波器组的伪逆算子，开发了MelFlow模型

Result: 在16kHz采样率下实现32ms算法延迟和48ms总延迟，在消费级笔记本GPU上验证了实时流式处理能力，PESQ和SI-SDR指标显著优于HiFi-GAN等基线

Conclusion: MelFlow成功实现了低延迟的流式Mel声码化，在保持高质量音频重建的同时具备实际部署的实时处理能力

Abstract: The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram
to an audio waveform, is still a key component in many text-to-speech (TTS)
systems today. Based on generative flow matching, our prior work on generative
STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel
filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for
speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total
latency of 48 ms. We show real-time streaming capability at this latency not
only in theory, but in practice on a consumer laptop GPU. Furthermore, we show
that our model achieves substantially better PESQ and SI-SDR values compared to
well-established not streaming-capable baselines for Mel vocoding including
HiFi-GAN.

</details>


### [38] [Listening, Imagining \& Refining: A Heuristic Optimized ASR Correction Framework with LLMs](https://arxiv.org/abs/2509.15095)
*Yutong Liu,Ziyue Zhang,Yongbin Yu,Xiangxiang Wang,Yuqing Cai,Nyima Tashi*

Main category: eess.AS

TL;DR: LIR-ASR是一个基于LLM的启发式优化迭代校正框架，通过"听-想象-精炼"策略来改进ASR系统输出，在英中两种语言上平均降低CER/WER达1.5个百分点


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统容易产生错误，影响下游应用性能，需要有效的后处理校正方法

Method: 提出LIR-ASR框架，采用"听-想象-精炼"策略，生成语音变体并在上下文中精炼，使用启发式优化和有限状态机避免局部最优，结合规则约束保持语义保真度

Result: 在英文和中文ASR输出上的实验显示，相比基线方法平均降低CER/WER达1.5个百分点，显著提升了转录准确性

Conclusion: LIR-ASR框架有效改进了ASR系统的输出质量，通过模仿人类听觉感知的迭代校正策略实现了显著的准确率提升

Abstract: Automatic Speech Recognition (ASR) systems remain prone to errors that affect
downstream applications. In this paper, we propose LIR-ASR, a heuristic
optimized iterative correction framework using LLMs, inspired by human auditory
perception. LIR-ASR applies a "Listening-Imagining-Refining" strategy,
generating phonetic variants and refining them in context. A heuristic
optimization with finite state machine (FSM) is introduced to prevent the
correction process from being trapped in local optima and rule-based
constraints help maintain semantic fidelity. Experiments on both English and
Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of
up to 1.5 percentage points compared to baselines, demonstrating substantial
accuracy gains in transcription.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [39] [Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework](https://arxiv.org/abs/2509.14304)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.SD

TL;DR: UDM框架在口吃和言语不流畅检测中实现了高精度(0.89 F1)与临床可解释性(4.2/5.0)的平衡，87%临床医生接受率，诊断时间减少34%


<details>
  <summary>Details</summary>
Motivation: 解决传统口吃检测系统在准确性和临床可解释性之间的权衡问题，克服端到端深度学习模型的黑盒性质对临床应用的限制

Method: 采用模块化架构、显式音素对齐和可解释输出，通过患者和认证言语病理学家的广泛实验验证

Result: 达到最先进性能(F1: 0.89±0.04)，提供临床有意义的可解释性评分(4.2/5.0)，87%临床医生接受率，诊断时间减少34%

Conclusion: UDM代表了在临床环境中实现AI辅助言语治疗的实用途径，为临床部署提供了可行方案

Abstract: Stuttered and dysfluent speech detection systems have traditionally suffered
from the trade-off between accuracy and clinical interpretability. While
end-to-end deep learning models achieve high performance, their black-box
nature limits clinical adoption. This paper looks at the Unconstrained
Dysfluency Modeling (UDM) series-the current state-of-the-art framework
developed by Berkeley that combines modular architecture, explicit phoneme
alignment, and interpretable outputs for real-world clinical deployment.
Through extensive experiments involving patients and certified speech-language
pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art
performance (F1: 0.89+-0.04) while providing clinically meaningful
interpretability scores (4.2/5.0). Our deployment study shows 87% clinician
acceptance rate and 34% reduction in diagnostic time. The results provide
strong evidence that UDM represents a practical pathway toward AI-assisted
speech therapy in clinical environments.

</details>


### [40] [Measuring Soft Biometric Leakage in Speaker De-Identification Systems](https://arxiv.org/abs/2509.14469)
*Seungmin Seo,Oleg Aulov,P. Jonathon Phillips*

Main category: cs.SD

TL;DR: 本文提出了软生物特征泄露评分（SBLS），用于量化语音去识别系统对零样本推理攻击的抵抗能力，发现现有系统在软生物特征保护方面存在显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的说话人身份去识别系统评估主要关注个体层面的指标，忽视了软生物特征泄露带来的更广泛风险，需要一种统一的方法来量化系统对非唯一性特征推断攻击的抵抗能力。

Method: 提出了SBLS评分方法，整合三个要素：使用预训练分类器进行直接属性推断、通过互信息分析进行链接检测、以及跨交叉属性的子群体鲁棒性分析。使用公开可用的分类器对五个去识别系统进行评估。

Result: 所有五个评估的去识别系统都表现出显著的脆弱性。攻击者仅使用预训练模型（无需原始语音或系统细节）就能可靠地从匿名化输出中恢复软生物特征信息。

Conclusion: 标准分布度量无法捕捉到的根本性弱点被暴露，表明当前语音去识别系统在软生物特征保护方面存在严重不足，需要更全面的评估方法。

Abstract: We use the term re-identification to refer to the process of recovering the
original speaker's identity from anonymized speech outputs. Speaker
de-identification systems aim to reduce the risk of re-identification, but most
evaluations focus only on individual-level measures and overlook broader risks
from soft biometric leakage. We introduce the Soft Biometric Leakage Score
(SBLS), a unified method that quantifies resistance to zero-shot inference
attacks on non-unique traits such as channel type, age range, dialect, sex of
the speaker, or speaking style. SBLS integrates three elements: direct
attribute inference using pre-trained classifiers, linkage detection via mutual
information analysis, and subgroup robustness across intersecting attributes.
Applying SBLS with publicly available classifiers, we show that all five
evaluated de-identification systems exhibit significant vulnerabilities. Our
results indicate that adversaries using only pre-trained models - without
access to original speech or system details - can still reliably recover soft
biometric information from anonymized output, exposing fundamental weaknesses
that standard distributional metrics fail to capture.

</details>


### [41] [A long-form single-speaker real-time MRI speech dataset and benchmark](https://arxiv.org/abs/2509.14479)
*Sean Foley,Jihwan Lee,Kevin Huang,Xuan Shi,Yoonjeong Lee,Louis Goldstein,Shrikanth Narayanan*

Main category: cs.SD

TL;DR: USC LSS数据集发布，包含单说话人1小时的实时MRI声道动态视频和同步音频数据，提供多种衍生表示，并在发音合成和音素识别任务上建立基准性能


<details>
  <summary>Details</summary>
Motivation: 提供更长的单说话人实时MRI语音数据集，支持发音研究和下游任务开发

Method: 收集单名美国英语母语者的实时MRI视频和同步音频数据，进行数据预处理和衍生表示提取

Result: 发布了约1小时的数据集，包含原始数据和多种衍生表示（声道区域裁剪视频、句子级分割、修复降噪音频、ROI时间序列等）

Conclusion: 该数据集为发音研究和相关任务提供了有价值的资源，并通过基准测试为未来研究提供了性能参考

Abstract: We release the USC Long Single-Speaker (LSS) dataset containing real-time MRI
video of the vocal tract dynamics and simultaneous audio obtained during speech
production. This unique dataset contains roughly one hour of video and audio
data from a single native speaker of American English, making it one of the
longer publicly available single-speaker datasets of real-time MRI speech data.
Along with the articulatory and acoustic raw data, we release derived
representations of the data that are suitable for a range of downstream tasks.
This includes video cropped to the vocal tract region, sentence-level splits of
the data, restored and denoised audio, and regions-of-interest timeseries. We
also benchmark this dataset on articulatory synthesis and phoneme recognition
tasks, providing baseline performance for these tasks on this dataset which
future research can aim to improve upon.

</details>


### [42] [Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech Synthesis](https://arxiv.org/abs/2509.14579)
*Qingyu Liu,Yushen Chen,Zhikang Niu,Chunhui Wang,Yunting Yang,Bowen Zhang,Jian Zhao,Pengcheng Zhu,Kai Yu,Xie Chen*

Main category: cs.SD

TL;DR: 这篇论文提出了Cross-Lingual F5-TTS框架，解决了流匹配基于TTS模型对音频提示语词转写的依赖问题，实现了无需语词转写的跨语言语音克隆。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配基于TTS模型仍依赖音频提示的参考语词转写，这阻碍了在语词转写不可用时的跨语言语音克隆。

Method: 通过强制对齐预处理获取词边界，在训练中排除语词转写；训练不同语言粒度的语速预测器来建模持续时间。

Result: 实验结果显示该方法能够达到与F5-TTS相当的性能，同时支持跨语言语音克隆。

Conclusion: 该框架成功解决了流匹配TTS模型依赖音频语词转写的限制，为跨语言语音克隆提供了可行方案。

Abstract: Flow-matching-based text-to-speech (TTS) models have shown high-quality
speech synthesis. However, most current flow-matching-based TTS models still
rely on reference transcripts corresponding to the audio prompt for synthesis.
This dependency prevents cross-lingual voice cloning when audio prompt
transcripts are unavailable, particularly for unseen languages. The key
challenges for flow-matching-based TTS models to remove audio prompt
transcripts are identifying word boundaries during training and determining
appropriate duration during inference. In this paper, we introduce
Cross-Lingual F5-TTS, a framework that enables cross-lingual voice cloning
without audio prompt transcripts. Our method preprocesses audio prompts by
forced alignment to obtain word boundaries, enabling direct synthesis from
audio prompts while excluding transcripts during training. To address the
duration modeling challenge, we train speaking rate predictors at different
linguistic granularities to derive duration from speaker pace. Experiments show
that our approach matches the performance of F5-TTS while enabling
cross-lingual voice cloning.

</details>


### [43] [Spatial Audio Motion Understanding and Reasoning](https://arxiv.org/abs/2509.14666)
*Arvind Krishna Sridhar,Yinyi Guo,Erik Visser*

Main category: cs.SD

TL;DR: 这篇论文提出了一种空间音频理解框架，通过结合空间音频编码器、音频基准模型和大语言模型，能够理解动态音频场景中移动源的事件和空间属性。


<details>
  <summary>Details</summary>
Motivation: 解决机器理解动态音频场景中移动源的空间属性和复杂查询的挑战，提高空间音频理解能力。

Method: 1) 空间音频编码器检测事件和估计空间属性；2) 音频基准模型通过跨注意机制对齐音频特征与语义文本；3) 大语言模型处理结构化空间属性来回答复杂查询。

Result: 开发了空间音频运动理解理解标准数据集，并在基准模型上展示了优秀的性能。

Conclusion: 该框架能够有效地理解动态音频场景，对移动源进行空间理解，为空间音频理解领域提供了新的解决方案。

Abstract: Spatial audio reasoning enables machines to interpret auditory scenes by
understanding events and their spatial attributes. In this work, we focus on
spatial audio understanding with an emphasis on reasoning about moving sources.
First, we introduce a spatial audio encoder that processes spatial audio to
detect multiple overlapping events and estimate their spatial attributes,
Direction of Arrival (DoA) and source distance, at the frame level. To
generalize to unseen events, we incorporate an audio grounding model that
aligns audio features with semantic audio class text embeddings via a
cross-attention mechanism. Second, to answer complex queries about dynamic
audio scenes involving moving sources, we condition a large language model
(LLM) on structured spatial attributes extracted by our model. Finally, we
introduce a spatial audio motion understanding and reasoning benchmark dataset
and demonstrate our framework's performance against the baseline model.

</details>


### [44] [How Does Instrumental Music Help SingFake Detection?](https://arxiv.org/abs/2509.14675)
*Xuanjun Chen,Chia-Yu Hu,I-Ming Lin,Yi-Cheng Lin,I-Hsiang Chiu,You Zhang,Sung-Feng Huang,Yi-Hsuan Yang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 该研究探讨了乐器伴奏对歌声深度伪造检测的影响，发现乐器主要起到数据增强作用而非提供内在线索，微调会增强模型对说话人特征的依赖而降低对其他信息的敏感性。


<details>
  <summary>Details</summary>
Motivation: 虽然存在许多歌声深度伪造检测模型，但这些模型在乐器伴奏下的工作原理尚不清楚，需要研究乐器音乐如何影响SingFake检测。

Method: 从行为效应和表征效应两个角度进行研究：测试不同骨干网络、非配对乐器音轨和频率子带；分析微调如何改变编码器的语音和音乐能力。

Result: 乐器伴奏主要作为数据增强而非提供内在线索（如节奏或和声）；微调增加了对浅层说话人特征的依赖，同时降低了对内容、副语言和语义信息的敏感性。

Conclusion: 这些发现阐明了模型如何利用声乐与乐器线索，可为设计更可解释和鲁棒的SingFake检测系统提供指导。

Abstract: Although many models exist to detect singing voice deepfakes (SingFake), how
these models operate, particularly with instrumental accompaniment, is unclear.
We investigate how instrumental music affects SingFake detection from two
perspectives. To investigate the behavioral effect, we test different
backbones, unpaired instrumental tracks, and frequency subbands. To analyze the
representational effect, we probe how fine-tuning alters encoders' speech and
music capabilities. Our results show that instrumental accompaniment acts
mainly as data augmentation rather than providing intrinsic cues (e.g., rhythm
or harmony). Furthermore, fine-tuning increases reliance on shallow speaker
features while reducing sensitivity to content, paralinguistic, and semantic
information. These insights clarify how models exploit vocal versus
instrumental cues and can inform the design of more interpretable and robust
SingFake detection systems.

</details>


### [45] [Pushing the Limits of End-to-End Diarization](https://arxiv.org/abs/2509.14737)
*Samuel J. Broughton,Lahiru Samarakoon*

Main category: cs.SD

TL;DR: 本文使用EEND-TA模型在多个公开数据集上实现了最先进的说话人日志错误率，特别是在DIHARD III数据集上达到14.49%的DER，证明了基于EEND架构的强大学习能力


<details>
  <summary>Details</summary>
Motivation: 探索基于EEND架构的说话人日志模型的潜力，验证其是否具有比先前研究更大的学习容量，同时保持推理效率

Method: 采用EEND-TA（端到端说话人日志的统一非自回归模型），通过8说话人模拟混合进行预训练扩展，确保每个生成的说话人混合配置得到充分表示

Result: 在多个公开数据集上实现了最先进的DER性能：AliMeeting-far、AliMeeting-near、AMI-Mix、AMI-SDM、DIHARD III和MagicData RAMC，其中DIHARD III上的DER为14.49%

Conclusion: EEND-based架构具有比先前探索更大的学习容量，超越了现有的许多说话人日志解决方案，同时在推理过程中保持高效速度

Abstract: In this paper, we present state-of-the-art diarization error rates (DERs) on
multiple publicly available datasets, including AliMeeting-far,
AliMeeting-near, AMI-Mix, AMI-SDM, DIHARD III, and MagicData RAMC. Leveraging
EEND-TA, a single unified non-autoregressive model for end-to-end speaker
diarization, we achieve new benchmark results, most notably a DER of 14.49% on
DIHARD III. Our approach scales pretraining through 8-speaker simulation
mixtures, ensuring each generated speaker mixture configuration is sufficiently
represented. These experiments highlight that EEND-based architectures possess
a greater capacity for learning than previously explored, surpassing many
existing diarization solutions while maintaining efficient speeds during
inference.

</details>


### [46] [Spatial-CLAP: Learning Spatially-Aware audio--text Embeddings for Multi-Source Conditions](https://arxiv.org/abs/2509.14785)
*Kentaro Seki,Yuki Okamoto,Kouei Yamaoka,Yuki Saito,Shinnosuke Takamichi,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: Spatial-CLAP是一个创新的音频-文本嵌入框架，通过内容感知空间编码器和空间对比学习，解决了多源条件下空间信息建模的挑战，实现了有效的空间感知音频表示。


<details>
  <summary>Details</summary>
Motivation: 现有的CLAP方法仅限于单声道或单源条件，无法充分捕捉空间信息。多源条件下需要正确建立每个声源与其位置的对应关系，这是建模空间信息的主要挑战。

Method: 提出Spatial-CLAP，引入内容感知空间编码器实现与音频内容耦合的空间表示。提出空间对比学习(SCL)训练策略，显式强制学习正确对应关系，在多源条件下促进更可靠的嵌入。

Result: 实验评估表明Spatial-CLAP即使在多源条件下也能学习有效嵌入，证实了SCL的有效性。对未见的三源混合评估突出了传统单源训练与多源训练范式的根本区别。

Conclusion: 这些发现为空间感知音频-文本嵌入建立了新的范式，解决了多源空间信息建模的关键问题。

Abstract: Contrastive language--audio pretraining (CLAP) has achieved remarkable
success as an audio--text embedding framework, but existing approaches are
limited to monaural or single-source conditions and cannot fully capture
spatial information. The central challenge in modeling spatial information lies
in multi-source conditions, where the correct correspondence between each sound
source and its location is required. To tackle this problem, we propose
Spatial-CLAP, which introduces a content-aware spatial encoder that enables
spatial representations coupled with audio content. We further propose spatial
contrastive learning (SCL), a training strategy that explicitly enforces the
learning of the correct correspondence and promotes more reliable embeddings
under multi-source conditions. Experimental evaluations, including downstream
tasks, demonstrate that Spatial-CLAP learns effective embeddings even under
multi-source conditions, and confirm the effectiveness of SCL. Moreover,
evaluation on unseen three-source mixtures highlights the fundamental
distinction between conventional single-source training and our proposed
multi-source training paradigm. These findings establish a new paradigm for
spatially-aware audio--text embeddings.

</details>


### [47] [Towards Building Speech Large Language Models for Multitask Understanding in Low-Resource Languages](https://arxiv.org/abs/2509.14804)
*Mingchen Shao,Bingshen Mu,Chengyou Wang,Hai Li,Ying Yan,Zhonghua Fu,Lei Xie*

Main category: cs.SD

TL;DR: 这篇论文提出了XLSR-Thai语音编码器和U-Align对齐方法，解决低资源语言如泰语中SLLM模型的性能问题，并创建了超过1,000小时的泰语语音理解数据集。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在高资源语言表现优异，但在低资源语言如泰语中性能大幅下降，主要因因包括：Whisper等语音编码器在低资源语言表现差、ASR对齐方法计算成本高、泰语语音-文本数据稀缺。

Method: 1）基于XLSR模型连续训练36,000小时泰语语音数据，开发XLSR-Thai自监督语音编码器
2）提出U-Align语音-文本对齐方法，比ASR基准方法更节约资源且支持多任务
3）开发Thai-SUP流水线，从高资源语言生成泰语语音理解数据

Result: 实验结果表明方法在构建泰语多任务理解SLLM模型方面具有效果，创建了首个超过1,000小时的泰语语音理解数据集。

Conclusion: 该研究成功解决了低资源语言泰语中SLLM模型的性能问题，通过开发专门的语音编码器、更高效的对齐方法和数据生成流水线，为低资源语言的语音大语言模型研究提供了有效解决方案。开源了XLSR-Thai和Thai-SUP以便后续研究。

Abstract: Speech large language models (SLLMs) built on speech encoders, adapters, and
LLMs demonstrate remarkable multitask understanding performance in
high-resource languages such as English and Chinese. However, their
effectiveness substantially degrades in low-resource languages such as Thai.
This limitation arises from three factors: (1) existing commonly used speech
encoders, like the Whisper family, underperform in low-resource languages and
lack support for broader spoken language understanding tasks; (2) the ASR-based
alignment paradigm requires training the entire SLLM, leading to high
computational cost; (3) paired speech-text data in low-resource languages is
scarce. To overcome these challenges in the low-resource language Thai, we
introduce XLSR-Thai, the first self-supervised learning (SSL) speech encoder
for Thai. It is obtained by continuously training the standard SSL XLSR model
on 36,000 hours of Thai speech data. Furthermore, we propose U-Align, a
speech-text alignment method that is more resource-efficient and
multitask-effective than typical ASR-based alignment. Finally, we present
Thai-SUP, a pipeline for generating Thai spoken language understanding data
from high-resource languages, yielding the first Thai spoken language
understanding dataset of over 1,000 hours. Multiple experiments demonstrate the
effectiveness of our methods in building a Thai multitask-understanding SLLM.
We open-source XLSR-Thai and Thai-SUP to facilitate future research.

</details>


### [48] [MeanFlowSE: one-step generative speech enhancement via conditional mean flow](https://arxiv.org/abs/2509.14858)
*Duojia Li,Shenghui Lu,Hongchen Pan,Zongyi Zhan,Qingyang Hong,Lin Li*

Main category: cs.SD

TL;DR: MeanFlowSE是一种单步生成语音增强模型，通过学习有限时间间隔内的平均速度场，避免了传统方法需要多步ODE求解器的问题，在保持高质量的同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于流和扩散的生成语音增强系统需要迭代ODE求解器进行多步推理，这在实时应用中成为计算瓶颈。

Method: 提出MeanFlowSE条件生成模型，使用Jacobian-vector product实现MeanFlow恒等式，学习轨迹上有限时间间隔内的平均速度场，支持单步生成和可选的多步细化。

Result: 在VoiceBank-DEMAND数据集上，单步模型实现了良好的可懂度、保真度和感知质量，计算成本显著低于多步基线方法。

Conclusion: 该方法无需知识蒸馏或外部教师模型，为实时生成语音增强提供了一个高效、高保真的框架。

Abstract: Multistep inference is a bottleneck for real-time generative speech
enhancement because flow- and diffusion-based systems learn an instantaneous
velocity field and therefore rely on iterative ordinary differential equation
(ODE) solvers. We introduce MeanFlowSE, a conditional generative model that
learns the average velocity over finite intervals along a trajectory. Using a
Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a
local training objective that directly supervises finite-interval displacement
while remaining consistent with the instantaneous-field constraint on the
diagonal. At inference, MeanFlowSE performs single-step generation via a
backward-in-time displacement, removing the need for multistep solvers; an
optional few-step variant offers additional refinement. On VoiceBank-DEMAND,
the single-step model achieves strong intelligibility, fidelity, and perceptual
quality with substantially lower computational cost than multistep baselines.
The method requires no knowledge distillation or external teachers, providing
an efficient, high-fidelity framework for real-time generative speech
enhancement.

</details>


### [49] [From Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition](https://arxiv.org/abs/2509.14880)
*Rishabh Jain,Naomi Harte*

Main category: cs.SD

TL;DR: 通过系统性实验研究LLM解码器在视觉语音识别中的作用，发现继续训练和扩大解码器对提升效果有限，而数据集结合能提升模型通用性，成绩提升主要来自词汇处理而非语义理解


<details>
  <summary>Details</summary>
Motivation: 识别当前视觉语音识别系统的提升是来自视觉理解还是语言模型能力，以确定LLM解码器的真正作用机制

Method: 冻结或选择性更新视觉编码器，扩大解码器规模，比较适配策略和架构，在LRS2、LRS3及其组合数据集上进行训练

Result: Llama-2-13B模型在LRS3上达到24.7% WER，在WildVSR上达到47.0% WER，达到无额外监督训练模型的SOTA性能，语义分析显示成绩提升主要来自词汇处理

Conclusion: LLM解码器主要精炼上下文推理能力而非视觉特征处理，需要更强大的视觉编码器来推动实质性进步

Abstract: Advances in self-supervised encoders have improved Visual Speech Recognition
(VSR). Recent approaches integrating these encoders with LLM decoders improves
transcription accuracy; however, it remains unclear whether these gains stem
from visual understanding or stronger language modeling. In this work, we
systematically evaluate LLM decoders by freezing or selectively updating the
visual encoder, scaling decoder size, comparing adaptation strategies and
architectures, and varying training data across LRS2, LRS3, and their
combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and
adaptation yield limited improvements, while combining datasets enhances
generalization. Semantic analysis reveals that gains arise primarily from
lexical rather than semantic processing. Our Llama-2-13B model trained on the
combined set achieves 24.7\% WER on LRS3 and 47.0\% on WildVSR, establishing
SOTA among models trained without additional supervision. Our findings indicate
LLM decoders refine contextual reasoning rather than visual features,
emphasizing the need for stronger visual encoders to drive meaningful progress.

</details>


### [50] [Temporally Heterogeneous Graph Contrastive Learning for Multimodal Acoustic event Classification](https://arxiv.org/abs/2509.14893)
*Yuanjian Chen,Yang Xiao,Jinjie Huang*

Main category: cs.SD

TL;DR: 提出了THGCL方法，通过构建时间异构图和对比学习，解决了多模态声学事件分类中的时间对齐和噪声问题，在AudioSet上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态声学事件分类中，音频和视觉信号的时间对齐困难，现有方法无法区分模态内和模态间的时间依赖关系，且容易受到跨模态噪声的影响。

Method: 构建时间异构图，音频和视频段作为节点，时间链接作为边；使用高斯过程处理模态内平滑性，霍克斯过程处理模态间衰减，结合对比学习捕捉细粒度关系。

Result: 在AudioSet数据集上的实验表明，THGCL方法达到了最先进的性能水平。

Conclusion: THGCL框架通过建模时间异构图和对比学习，有效解决了多模态声学事件分类中的时间对齐和噪声抑制问题，取得了优异的性能表现。

Abstract: Multimodal acoustic event classification plays a key role in audio-visual
systems. Although combining audio and visual signals improves recognition, it
is still difficult to align them over time and to reduce the effect of noise
across modalities. Existing methods often treat audio and visual streams
separately, fusing features later with contrastive or mutual information
objectives. Recent advances explore multimodal graph learning, but most fail to
distinguish between intra- and inter-modal temporal dependencies. To address
this, we propose Temporally Heterogeneous Graph-based Contrastive Learning
(THGCL). Our framework constructs a temporal graph for each event, where audio
and video segments form nodes and their temporal links form edges. We introduce
Gaussian processes for intra-modal smoothness, Hawkes processes for inter-modal
decay, and contrastive learning to capture fine-grained relationships.
Experiments on AudioSet show that THGCL achieves state-of-the-art performance.

</details>


### [51] [Back to Ear: Perceptually Driven High Fidelity Music Reconstruction](https://arxiv.org/abs/2509.14912)
*Kangdi Wang,Zhiyue Wu,Dinghao Zhou,Rui Lin,Junyu Dai,Tao Jiang*

Main category: cs.SD

TL;DR: 提出了εar-VAE，一种改进的音频VAE模型，通过听觉感知滤波、新型相位损失和频谱监督范式，显著提升了音频重建质量，特别是在高频谐波和空间特性方面。


<details>
  <summary>Details</summary>
Motivation: 现有开源VAE模型在训练时忽视听觉感知方面，导致相位精度和立体声空间表示存在缺陷，需要改进音频重建质量。

Method: 采用K加权感知滤波器、相关损失和相位损失（瞬时频率和群延迟）、新的频谱监督范式（幅度由四个Mid/Side/Left/Right分量监督，相位仅由LR分量监督）。

Result: 在44.1kHz采样率下，εar-VAE在多种指标上显著优于领先的开源模型，特别是在重建高频谐波和空间特性方面表现突出。

Conclusion: εar-VAE通过重新思考和优化VAE训练范式，有效解决了音频重建中的感知质量和空间表示问题，为大规模音频任务提供了更好的基础模型。

Abstract: Variational Autoencoders (VAEs) are essential for large-scale audio tasks
like diffusion-based generation. However, existing open-source models often
neglect auditory perceptual aspects during training, leading to weaknesses in
phase accuracy and stereophonic spatial representation. To address these
challenges, we propose {\epsilon}ar-VAE, an open-source music signal
reconstruction model that rethinks and optimizes the VAE training paradigm. Our
contributions are threefold: (i) A K-weighting perceptual filter applied prior
to loss calculation to align the objective with auditory perception. (ii) Two
novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss
using its derivatives--Instantaneous Frequency and Group Delay--for precision.
(iii) A new spectral supervision paradigm where magnitude is supervised by all
four Mid/Side/Left/Right components, while phase is supervised only by the LR
components. Experiments show {\epsilon}ar-VAE at 44.1kHz substantially
outperforms leading open-source models across diverse metrics, showing
particular strength in reconstructing high-frequency harmonics and the spatial
characteristics.

</details>


### [52] [Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening](https://arxiv.org/abs/2509.14944)
*Xiaolei Xu,Chaoyue Niu,Guy J. Brown,Hector Romero,Ning Ma*

Main category: cs.SD

TL;DR: 通过声音估计呼吸努力，给合声学特征提高阻塞性睡眠呼吸暂停检测效果，只需智能手机音频即可实现无传感器监测


<details>
  <summary>Details</summary>
Motivation: 阻塞性睡眠呼吸暂停(OSA)普遍但诊断复杂费用高，现有声音筛查受环境噪声限制缺乏生理上下文。呼吸努力是临床OSA评分关键信号，但需要接触传感器影响扩展性和患者舒适性

Method: 提出隐含空间融合框架，从夜间音频直接估计呼吸努力嵌入，然后与声学特征融合进行OSA检测。基于103名参与者157夜家庭环境数据集

Result: 呼吸努力估计器协调相关系数达0.48，抓取有意义呼吸动力学。融合方法在敏感性和AUC指标上超过单纯音频基线，特别是在低呼吸暂停指数阈值时

Conclusion: 该方法仅需智能手机音频即可实现OSA监测，支持无传感器、可扩展和纵向监测，为大规模OSA筛查提供了可行方案

Abstract: Obstructive sleep apnoea (OSA) is a prevalent condition with significant
health consequences, yet many patients remain undiagnosed due to the complexity
and cost of over-night polysomnography. Acoustic-based screening provides a
scalable alternative, yet performance is limited by environmental noise and the
lack of physiological context. Respiratory effort is a key signal used in
clinical scoring of OSA events, but current approaches require additional
contact sensors that reduce scalability and patient comfort. This paper
presents the first study to estimate respiratory effort directly from nocturnal
audio, enabling physiological context to be recovered from sound alone. We
propose a latent-space fusion framework that integrates the estimated effort
embeddings with acoustic features for OSA detection. Using a dataset of 157
nights from 103 participants recorded in home environments, our respiratory
effort estimator achieves a concordance correlation coefficient of 0.48,
capturing meaningful respiratory dynamics. Fusing effort and audio improves
sensitivity and AUC over audio-only baselines, especially at low
apnoea-hypopnoea index thresholds. The proposed approach requires only
smartphone audio at test time, which enables sensor-free, scalable, and
longitudinal OSA monitoring.

</details>


### [53] [FCPE: A Fast Context-based Pitch Estimation Model](https://arxiv.org/abs/2509.15140)
*Yuxin Luo,Ruoyi Zhang,Lu-Chuan Liu,Tianyu Li,Hangyu Liu*

Main category: cs.SD

TL;DR: FCPE是一种基于快速上下文的音高估计模型，采用Lynx-Net架构和深度可分离卷积，在保持低计算成本的同时实现高精度噪声鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有单声道音频音高估计方法在噪声环境下性能显著下降，影响MIDI转录和歌声转换应用

Method: 使用Lynx-Net架构和深度可分离卷积有效提取mel频谱图特征，兼顾计算效率和噪声容忍度

Result: 在MIR-1K数据集上达到96.79%的原始音高准确率，与最先进方法相当；在RTX 4090 GPU上实时因子为0.0062，效率显著优于现有算法

Conclusion: FCPE模型在音高估计任务中实现了高精度和高效率的平衡，特别在噪声环境下表现出色，为实际应用提供了有效解决方案

Abstract: Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription
and singing voice conversion (SVC), but existing methods suffer significant
performance degradation under noise. In this paper, we propose FCPE, a fast
context-based pitch estimation model that employs a Lynx-Net architecture with
depth-wise separable convolutions to effectively capture mel spectrogram
features while maintaining low computational cost and robust noise tolerance.
Experiments show that our method achieves 96.79\% Raw Pitch Accuracy (RPA) on
the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time
Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly
outperforms existing algorithms in efficiency. Code is available at
https://github.com/CNChTu/FCPE.

</details>


### [54] [Exploring How Audio Effects Alter Emotion with Foundation Models](https://arxiv.org/abs/2509.15151)
*Stelios Katsis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou*

Main category: cs.SD

TL;DR: 利用基础模型分析音频效果器对音乐情感的影响，通过深度学习方法探索音频FX与情感感知之间的复杂非线性关系


<details>
  <summary>Details</summary>
Motivation: 音频效果器在塑造音乐情感反应中起关键作用，但现有研究主要关注低级音频特征与情感感知的联系，对音频FX系统影响的研究不足

Method: 应用多种探测方法分析深度学习模型的嵌入表示，研究基础音频模型中音频效果器与情感估计之间的复杂关系

Result: 发现了与特定效果器相关的情感模式，并评估了基础音频模型的鲁棒性

Conclusion: 研究结果有助于深入理解音频制作实践对感知的影响，对音乐认知、表演和情感计算具有重要意义

Abstract: Audio effects (FX) such as reverberation, distortion, modulation, and dynamic
range processing play a pivotal role in shaping emotional responses during
music listening. While prior studies have examined links between low-level
audio features and affective perception, the systematic impact of audio FX on
emotion remains underexplored. This work investigates how foundation models -
large-scale neural architectures pretrained on multimodal data - can be
leveraged to analyze these effects. Such models encode rich associations
between musical structure, timbre, and affective meaning, offering a powerful
framework for probing the emotional consequences of sound design techniques. By
applying various probing methods to embeddings from deep learning models, we
examine the complex, nonlinear relationships between audio FX and estimated
emotion, uncovering patterns tied to specific effects and evaluating the
robustness of foundation audio models. Our findings aim to advance
understanding of the perceptual impact of audio production practices, with
implications for music cognition, performance, and affective computing.

</details>


### [55] [Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation](https://arxiv.org/abs/2509.15210)
*Chen Si,Qianyi Wu,Chaitanya Amballa,Romit Roy Choudhury*

Main category: cs.SD

TL;DR: 提出了Mesh-infused Neural Acoustic Field (MiNAF)方法，通过结合显式几何特征来改进神经隐式模型，实现更准确的房间脉冲响应预测


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式方法在声音模拟中未能有效利用环境的显式几何信息，限制了房间脉冲响应(RIR)预测的准确性

Method: MiNAF方法查询粗糙房间网格，提取距离分布作为局部上下文的显式表示，将显式几何特征融入神经隐式模型

Result: 与常规和最先进的基线方法相比，MiNAF在各种评估指标上表现优异，在训练样本有限的数据集中也显示出良好的鲁棒性

Conclusion: 结合显式局部几何特征能够更好地指导神经网络生成更准确的RIR预测，推动了高保真声音模拟的发展

Abstract: Realistic sound simulation plays a critical role in many applications. A key
element in sound simulation is the room impulse response (RIR), which
characterizes how sound propagates from a source to a listener within a given
space. Recent studies have applied neural implicit methods to learn RIR using
context information collected from the environment, such as scene images.
However, these approaches do not effectively leverage explicit geometric
information from the environment. To further exploit the potential of neural
implicit models with direct geometric features, we present Mesh-infused Neural
Acoustic Field (MiNAF), which queries a rough room mesh at given locations and
extracts distance distributions as an explicit representation of local context.
Our approach demonstrates that incorporating explicit local geometric features
can better guide the neural network in generating more accurate RIR
predictions. Through comparisons with conventional and state-of-the-art
baseline methods, we show that MiNAF performs competitively across various
evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets
with limited training samples, demonstrating an advance in high-fidelity sound
simulation.

</details>


### [56] [Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation](https://arxiv.org/abs/2509.15222)
*Junhyung Park,Yonghyun Kim,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: 开发了一个集成网络工具包，包含PiaRec和ASDF两个GUI，用于简化钢琴表演多模态数据的采集和指法标注


<details>
  <summary>Details</summary>
Motivation: 钢琴表演是多模态活动，但获取大规模多模态数据的过程繁琐，阻碍了该领域的研究进展

Method: 创建包含两个图形用户界面的网络工具包：PiaRec用于同步采集音频、视频、MIDI和表演元数据；ASDF用于从视觉数据中高效标注演奏指法

Result: 开发了一个完整的系统，能够简化多模态钢琴表演数据集的采集流程

Conclusion: 该集成工具包可以有效克服钢琴表演多模态数据采集的瓶颈，推动该领域的研究发展

Abstract: Piano performance is a multimodal activity that intrinsically combines
physical actions with the acoustic rendition. Despite growing research interest
in analyzing the multimodal nature of piano performance, the laborious process
of acquiring large-scale multimodal data remains a significant bottleneck,
hindering further progress in this field. To overcome this barrier, we present
an integrated web toolkit comprising two graphical user interfaces (GUIs): (i)
PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and
performance metadata. (ii) ASDF, which enables the efficient annotation of
performer fingering from the visual data. Collectively, this system can
streamline the acquisition of multimodal piano performance datasets.

</details>

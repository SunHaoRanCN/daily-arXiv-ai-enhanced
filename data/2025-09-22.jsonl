{"id": "2509.15261", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15261", "abs": "https://arxiv.org/abs/2509.15261", "authors": ["Xiaoyang Liu", "Yuma Kinoshita"], "title": "Pre-training Autoencoder for Acoustic Event Classification via Blinky", "comment": "Accepted to APSIPA ASC 2025. 6 pages, 1 figures", "summary": "In the acoustic event classification (AEC) framework that employs Blinkies,\naudio signals are converted into LED light emissions and subsequently captured\nby a single video camera. However, the 30 fps optical transmission channel\nconveys only about 0.2% of the normal audio bandwidth and is highly susceptible\nto noise. We propose a novel sound-to-light conversion method that leverages\nthe encoder of a pre-trained autoencoder (AE) to distill compact,\ndiscriminative features from the recorded audio. To pre-train the AE, we adopt\na noise-robust learning strategy in which artificial noise is injected into the\nencoder's latent representations during training, thereby enhancing the model's\nrobustness against channel noise. The encoder architecture is specifically\ndesigned for the memory footprint of contemporary edge devices such as the\nRaspberry Pi 4. In a simulation experiment on the ESC-50 dataset under a\nstringent 15 Hz bandwidth constraint, the proposed method achieved higher\nmacro-F1 scores than conventional sound-to-light conversion approaches."}
{"id": "2509.15473", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15473", "abs": "https://arxiv.org/abs/2509.15473", "authors": ["Yuyu Wang", "Wuyue Xia", "Huaxiu Yao", "Jingping Nie"], "title": "Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech", "comment": "6 pages, 3rd ACM International Workshop on Intelligent Acoustic\n  Systems and Applications (IASA 25)", "summary": "Post-exercise speech contains rich physiological and linguistic cues, often\nmarked by semantic pauses, breathing pauses, and combined breathing-semantic\npauses. Detecting these events enables assessment of recovery rate, lung\nfunction, and exertion-related abnormalities. However, existing works on\nidentifying and distinguishing different types of pauses in this context are\nlimited. In this work, building on a recently released dataset with\nsynchronized audio and respiration signals, we provide systematic annotations\nof pause types. Using these annotations, we systematically conduct exploratory\nbreathing and semantic pause detection and exertion-level classification across\ndeep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features\n(MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three\nsetups-single feature, feature fusion, and a two-stage detection-classification\ncascade-under both classification and regression formulations. Results show\nper-type detection accuracy up to 89$\\%$ for semantic, 55$\\%$ for breathing,\n86$\\%$ for combined pauses, and 73$\\%$overall, while exertion-level\nclassification achieves 90.5$\\%$ accuracy, outperformin prior work."}
{"id": "2509.15516", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15516", "abs": "https://arxiv.org/abs/2509.15516", "authors": ["Dhruuv Agarwal", "Harry Zhang", "Yang Yu", "Quan Wang"], "title": "State-of-the-Art Dysarthric Speech Recognition with MetaICL for on-the-fly Personalization", "comment": null, "summary": "Personalizing Automatic Speech Recognition (ASR) for dysarthric speech is\ncrucial but challenging due to training and storing of individual user\nadapters. We propose a hybrid meta-training method for a single model,\nexcelling in zero-shot and few-shot on-the-fly personalization via in-context\nlearning (ICL). Measuring Word Error Rate (WER) on state-of-the-art subsets,\nthe model achieves 13.9% WER on Euphonia which surpasses speaker-independent\nbaselines (17.5% WER) and rivals user-specific personalized models. On SAP Test\n1, its 5.3% WER significantly bests the 8% from even personalized adapters. We\nalso demonstrate the importance of example curation, where an oracle\ntext-similarity method shows 5 curated examples can achieve performance similar\nto 19 randomly selected ones, highlighting a key area for future efficiency\ngains. Finally, we conduct data ablations to measure the data efficiency of\nthis approach. This work presents a practical, scalable, and personalized\nsolution."}
{"id": "2509.15523", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15523", "abs": "https://arxiv.org/abs/2509.15523", "authors": ["Xinyi Chen", "Xi Chen", "Zhenyu Weng", "Yang Xiao"], "title": "AFT: An Exemplar-Free Class Incremental Learning Method for Environmental Sound Classification", "comment": "Submitted to ICASSP 2026", "summary": "As sounds carry rich information, environmental sound classification (ESC) is\ncrucial for numerous applications such as rare wild animals detection. However,\nour world constantly changes, asking ESC models to adapt to new sounds\nperiodically. The major challenge here is catastrophic forgetting, where models\nlose the ability to recognize old sounds when learning new ones. Many methods\naddress this using replay-based continual learning. This could be impractical\nin scenarios such as data privacy concerns. Exemplar-free methods are commonly\nused but can distort old features, leading to worse performance. To overcome\nsuch limitations, we propose an Acoustic Feature Transformation (AFT) technique\nthat aligns the temporal features of old classes to the new space, including a\nselectively compressed feature space. AFT mitigates the forgetting of old\nknowledge without retaining past data. We conducted experiments on two\ndatasets, showing consistent improvements over baseline models with accuracy\ngains of 3.7\\% to 3.9\\%."}
{"id": "2509.15253", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15253", "abs": "https://arxiv.org/abs/2509.15253", "authors": ["Zhiwen Qian", "Jinhua Liang", "Huan Zhang"], "title": "Emotion-Aware Speech Generation with Character-Specific Voices for Comics", "comment": null, "summary": "This paper presents an end-to-end pipeline for generating character-specific,\nemotion-aware speech from comics. The proposed system takes full comic volumes\nas input and produces speech aligned with each character's dialogue and\nemotional state. An image processing module performs character detection, text\nrecognition, and emotion intensity recognition. A large language model performs\ndialogue attribution and emotion analysis by integrating visual information\nwith the evolving plot context. Speech is synthesized through a text-to-speech\nmodel with distinct voice profiles tailored to each character and emotion. This\nwork enables automated voiceover generation for comics, offering a step toward\ninteractive and immersive comic reading experience."}
{"id": "2509.15475", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.15475", "abs": "https://arxiv.org/abs/2509.15475", "authors": ["Lioz Berman", "Sharon Gannot", "Tom Tirer"], "title": "(SP)$^2$-Net: A Neural Spatial Spectrum Method for DOA Estimation", "comment": "Code can be found at https://github.com/Liozb/SP2-Net", "summary": "We consider the problem of estimating the directions of arrival (DOAs) of\nmultiple sources from a single snapshot of an antenna array, a task with many\npractical applications. In such settings, the classical Bartlett beamformer is\ncommonly used, as maximum likelihood estimation becomes impractical when the\nnumber of sources is unknown or large, and spectral methods based on the sample\ncovariance are not applicable due to the lack of multiple snapshots. However,\nthe accuracy and resolution of the Bartlett beamformer are fundamentally\nlimited by the array aperture. In this paper, we propose a deep learning\ntechnique, comprising a novel architecture and training strategy, for\ngenerating a high-resolution spatial spectrum from a single snapshot.\nSpecifically, we train a deep neural network that takes the measurements and a\nhypothesis angle as input and learns to output a score consistent with the\ncapabilities of a much wider array. At inference time, a heatmap can be\nproduced by scanning an arbitrary set of angles. We demonstrate the advantages\nof our trained model, named (SP)$^2$-Net, over the Bartlett beamformer and\nsparsity-based DOA estimation methods."}
{"id": "2509.15599", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15599", "abs": "https://arxiv.org/abs/2509.15599", "authors": ["Jun-Wei Yeow", "Ee-Leng Tan", "Santi Peksi", "Woon-Seng Gan"], "title": "MAGENTA: Magnitude and Geometry-ENhanced Training Approach for Robust Long-Tailed Sound Event Localization and Detection", "comment": "This work has been submitted to IEEE ICASSP 2026 for possible\n  publication", "summary": "Deep learning-based Sound Event Localization and Detection (SELD) systems\ndegrade significantly on real-world, long-tailed datasets. Standard regression\nlosses bias learning toward frequent classes, causing rare events to be\nsystematically under-recognized. To address this challenge, we introduce\nMAGENTA (Magnitude And Geometry-ENhanced Training Approach), a unified loss\nfunction that counteracts this bias within a physically interpretable vector\nspace. MAGENTA geometrically decomposes the regression error into radial and\nangular components, enabling targeted, rarity-aware penalties and strengthened\ndirectional modeling. Empirically, MAGENTA substantially improves SELD\nperformance on imbalanced real-world data, providing a principled foundation\nfor a new class of geometry-aware SELD objectives. Code is available at:\nhttps://github.com/itsjunwei/MAGENTA_ICASSP"}
{"id": "2509.15389", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15389", "abs": "https://arxiv.org/abs/2509.15389", "authors": ["Youngwon Choi", "Jaeyoon Jung", "Hyeonyu Kim", "Huu-Kim Nguyen", "Hwayeon Kim"], "title": "Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data", "comment": "4 pages (excluding references), 2 figures, submitted to ICASSP 2026", "summary": "Large Audio Language Models (LALMs) have emerged as powerful tools for\nspeech-related tasks but remain underexplored for fine-tuning, especially with\nlimited speech data. To bridge this gap, we systematically examine how\ndifferent fine-tuning schemes including text-only, direct mixing, and\ncurriculum learning affect spoken language understanding (SLU), focusing on\nscenarios where text-label pairs are abundant while paired speech-label data\nare limited. Results show that LALMs already achieve competitive performance\nwith text-only fine-tuning, highlighting their strong generalization ability.\nAdding even small amounts of speech data (2-5%) yields substantial further\ngains, with curriculum learning particularly effective under scarce data. In\ncross-lingual SLU, combining source-language speech data with target-language\ntext and minimal target-language speech data enables effective adaptation.\nOverall, this study provides practical insights into the LALM fine-tuning under\nrealistic data constraints."}
{"id": "2509.15564", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15564", "abs": "https://arxiv.org/abs/2509.15564", "authors": ["Jeongjae Lee", "Wonseok Choi", "Songnam Hong"], "title": "CSIT-Free Downlink Transmission for mmWave MU-MISO Systems in High-Mobility Scenario", "comment": "Submitted to IEEE Conference", "summary": "This paper investigates the downlink (DL) transmission in millimeter-wave\n(mmWave) multi-user multiple-input single-output (MU-MISO) systems especially\nfocusing on a high speed mobile scenario. To complete the DL transmission\nwithin an extremely short channel coherence time, we propose a novel DL\ntransmission framework that eliminates the need for channel state information\nat the transmitter (CSIT), of which acquisition process requires a substantial\noverhead, instead fully exploiting the given channel coherence time. Harnessing\nthe characteristic of mmWave channel and uniquely designed CSIT-free unitary\nprecoding, we propose a symbol detection method along with the simultaneous CSI\nat the receiver (CSIR) and Doppler shift estimation method to completely cancel\nthe interferences while achieving a full combining gain. Via simulations, we\ndemonstrate the effectiveness of the proposed method comparing with the\nexisting baselines."}
{"id": "2509.15628", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15628", "abs": "https://arxiv.org/abs/2509.15628", "authors": ["Pengyu Wang", "Xiaofei Li"], "title": "Rec-RIR: Monaural Blind Room Impulse Response Identification via DNN-based Reverberant Speech Reconstruction in STFT Domain", "comment": "Submitted to ICASSP 2026", "summary": "Room impulse response (RIR) characterizes the complete propagation process of\nsound in an enclosed space. This paper presents Rec-RIR for monaural blind RIR\nidentification. Rec-RIR is developed based on the convolutive transfer function\n(CTF) approximation, which models reverberation effect within narrow-band\nfilter banks in the short-time Fourier transform (STFT) domain. Specifically,\nwe propose a deep neural network (DNN) with cross-band and narrow-band blocks\nto estimate the CTF filter. The DNN is trained through reconstructing the\nnoise-free reverberant speech spectra. This objective enables stable and\nstraightforward supervised training. Subsequently, a pseudo intrusive\nmeasurement process is employed to convert the CTF filter estimate into\ntime-domain RIR by simulating a common intrusive RIR measurement procedure.\nExperimental results demonstrate that Rec-RIR achieves state-of-the-art (SOTA)\nperformance in both RIR identification and acoustic parameter estimation.\nOpen-source codes are available online at\nhttps://github.com/Audio-WestlakeU/Rec-RIR."}
{"id": "2509.15437", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS", "I.2.0; I.2.7; I.5.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.15437", "abs": "https://arxiv.org/abs/2509.15437", "authors": ["Daniyal Kabir Dar", "Qiben Yan", "Li Xiao", "Arun Ross"], "title": "Impact of Phonetics on Speaker Identity in Adversarial Voice Attack", "comment": "Additional figures for extended visualization:\n  https://daniyalkabir.github.io/icassp-2025-results/", "summary": "Adversarial perturbations in speech pose a serious threat to automatic speech\nrecognition (ASR) and speaker verification by introducing subtle waveform\nmodifications that remain imperceptible to humans but can significantly alter\nsystem outputs. While targeted attacks on end-to-end ASR models have been\nwidely studied, the phonetic basis of these perturbations and their effect on\nspeaker identity remain underexplored. In this work, we analyze adversarial\naudio at the phonetic level and show that perturbations exploit systematic\nconfusions such as vowel centralization and consonant substitutions. These\ndistortions not only mislead transcription but also degrade phonetic cues\ncritical for speaker verification, leading to identity drift. Using DeepSpeech\nas our ASR target, we generate targeted adversarial examples and evaluate their\nimpact on speaker embeddings across genuine and impostor samples. Results\nacross 16 phonetically diverse target phrases demonstrate that adversarial\naudio induces both transcription errors and identity drift, highlighting the\nneed for phonetic-aware defenses to ensure the robustness of ASR and speaker\nrecognition systems."}
{"id": "2509.15601", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.15601", "abs": "https://arxiv.org/abs/2509.15601", "authors": ["Wanghan Lv", "Kumar Vijay Mishra", "Jinsong Hu"], "title": "Twisting Signals for Joint Radar-Communications: An OAM Vortex Beam Approach", "comment": "13 pages, 12 figures, 1 table", "summary": "Orbital angular momentum (OAM) technology has attracted much research\ninterest in recent years because of its characteristic helical phase front\ntwisting around the propagation axis and natural orthogonality among different\nOAM states to encode more degrees of freedom than classical planar beams.\nLeveraging upon these features, OAM technique has been applied to wireless\ncommunication systems to enhance spectral efficiency and radar systems to\ndistinguish spatial targets without beam scanning. Leveraging upon these unique\nproperties, we propose an OAM-based millimeter-wave joint radar-communications\n(JRC) system comprising a bi-static automotive radar and vehicle-to-vehicle\n(V2V) communications. Different from existing uniform circular array (UCA)\nbased OAM systems where each element is an isotropic antenna, an OAM spatial\nmodulation scheme utilizing a uniform linear array (ULA) is adopted with each\nelement being a traveling-wave antenna, producing multiple Laguerre-Gaussian\n(LG) vortex beams simultaneously. Specifically, we first build a novel\nbi-static automotive OAM-JRC model that embeds communication messages in a\nradar signal, following which a target position and velocity parameters\nestimation algorithm is designed with only radar frames. Then, an OAM-based\nmode-division multiplexing (MDM) strategy between radar and JRC frames is\npresented to ensure the JRC parameters identifiability and recovery.\nFurthermore, we analyze the performance of the JRC system through deriving\nrecovery guarantees and Cram\\'er-Rao lower bound (CRLB) of radar target\nparameters and evaluating the bit error rate (BER) of communication,\nrespectively. Our numerical experiments validate the effectiveness of the\nproposed OAM-based JRC system and parameter estimation method."}
{"id": "2509.15702", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15702", "abs": "https://arxiv.org/abs/2509.15702", "authors": ["Kaspar Müller", "Markus Buck", "Simon Doclo", "Jan Østergaard", "Tobias Wolff"], "title": "A Steered Response Power Method for Sound Source Localization With Generic Acoustic Models", "comment": "Accepted for publication in IEEE Transactions on Audio, Speech and\n  Language Processing", "summary": "The steered response power (SRP) method is one of the most popular approaches\nfor acoustic source localization with microphone arrays. It is often based on\nsimplifying acoustic assumptions, such as an omnidirectional sound source in\nthe far field of the microphone array(s), free field propagation, and spatially\nuncorrelated noise. In reality, however, there are many acoustic scenarios\nwhere such assumptions are violated. This paper proposes a generalization of\nthe conventional SRP method that allows to apply generic acoustic models for\nlocalization with arbitrary microphone constellations. These models may\nconsider, for instance, level differences in distributed microphones, the\ndirectivity of sources and receivers, or acoustic shadowing effects. Moreover,\nalso measured acoustic transfer functions may be applied as acoustic model. We\nshow that the delay-and-sum beamforming of the conventional SRP is not optimal\nfor localization with generic acoustic models. To this end, we propose a\ngeneralized SRP beamforming criterion that considers generic acoustic models\nand spatially correlated noise, and derive an optimal SRP beamformer.\nFurthermore, we propose and analyze appropriate frequency weightings. Unlike\nthe conventional SRP, the proposed method can jointly exploit observed level\nand time differences between the microphone signals to infer the source\nlocation. Realistic simulations of three different microphone setups with\nspeech under various noise conditions indicate that the proposed method can\nsignificantly reduce the mean localization error compared to the conventional\nSRP and, in particular, a reduction of more than 60% can be archived in noisy\nconditions."}
{"id": "2509.15462", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15462", "abs": "https://arxiv.org/abs/2509.15462", "authors": ["Ryan Collette", "Ross Greenwood", "Serena Nicoll"], "title": "A Novel Semantic Compression Approach for Ultra-low Bandwidth Voice Communication", "comment": "5 pages, 2 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "While existing speech audio codecs designed for compression exploit limited\nforms of temporal redundancy and allow for multi-scale representations, they\ntend to represent all features of audio in the same way. In contrast,\ngenerative voice models designed for text-to-speech and voice transfer tasks\nhave recently proved effective at factorizing audio signals into high-level\nsemantic representations of fundamentally distinct features. In this paper, we\nleverage such representations in a novel semantic communications approach to\nachieve lower bitrates without sacrificing perceptual quality or suitability\nfor specific downstream tasks. Our technique matches or outperforms existing\naudio codecs on transcription, sentiment analysis, and speaker verification\nwhen encoding at 2-4x lower bitrate -- notably surpassing Encodec in perceptual\nquality and speaker verification while using up to 4x less bitrate."}
{"id": "2509.15603", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15603", "abs": "https://arxiv.org/abs/2509.15603", "authors": ["Sven Hinderer"], "title": "Blind Source Separation of Radar Signals in Time Domain Using Deep Learning", "comment": null, "summary": "Identification and further analysis of radar emitters in a contested\nenvironment requires detection and separation of incoming signals. If they\narrive from the same direction and at similar frequencies, deinterleaving them\nremains challenging. A solution to overcome this limitation becomes\nincreasingly important with the advancement of emitter capabilities. We propose\ntreating the problem as blind source separation in time domain and apply\nsupervisedly trained neural networks to extract the underlying signals from the\nreceived mixture. This allows us to handle highly overlapping and also\ncontinuous wave (CW) signals from both radar and communication emitters. We\nmake use of advancements in the field of audio source separation and extend a\ncurrent state-of-the-art model with the objective of deinterleaving arbitrary\nradio frequency (RF) signals. Results show, that our approach is capable of\nseparating two unknown waveforms in a given frequency band with a single\nchannel receiver."}
{"id": "2509.15845", "categories": ["eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.15845", "abs": "https://arxiv.org/abs/2509.15845", "authors": ["Ziqi Dai", "Yiting Chen", "Jiacheng Xu", "Liufei Xie", "Yuchen Wang", "Zhenchuan Yang", "Bingsong Bai", "Yangsheng Gao", "Wenjiang Zhou", "Weifeng Zhao", "Ruohua Zhou"], "title": "Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and Context-Aware Instruct-TTS", "comment": "Submitted to ICASSP 2026.Copyright 2026 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work. DOI will be added upon IEEE Xplore publication", "summary": "The pipeline for multi-participant audiobook production primarily consists of\nthree stages: script analysis, character voice timbre selection, and speech\nsynthesis. Among these, script analysis can be automated with high accuracy\nusing NLP models, whereas character voice timbre selection still relies on\nmanual effort. Speech synthesis uses either manual dubbing or text-to-speech\n(TTS). While TTS boosts efficiency, it struggles with emotional expression,\nintonation control, and contextual scene adaptation. To address these\nchallenges, we propose DeepDubbing, an end-to-end automated system for\nmulti-participant audiobook production. The system comprises two main\ncomponents: a Text-to-Timbre (TTT) model and a Context-Aware Instruct-TTS\n(CA-Instruct-TTS) model. The TTT model generates role-specific timbre\nembeddings conditioned on text descriptions. The CA-Instruct-TTS model\nsynthesizes expressive speech by analyzing contextual dialogue and\nincorporating fine-grained emotional instructions. This system enables the\nautomated generation of multi-participant audiobooks with both timbre-matched\ncharacter voices and emotionally expressive narration, offering a novel\nsolution for audiobook production."}
{"id": "2509.15492", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15492", "abs": "https://arxiv.org/abs/2509.15492", "authors": ["Xinlei Niu", "Jianbo Ma", "Dylan Harper-Harris", "Xiangyu Zhang", "Charles Patrick Martin", "Jing Zhang"], "title": "Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech", "comment": null, "summary": "The generation of realistic, context-aware audio is important in real-world\napplications such as video game development. While existing video-to-audio\n(V2A) methods mainly focus on Foley sound generation, they struggle to produce\nintelligible speech. Meanwhile, current environmental speech synthesis\napproaches remain text-driven and fail to temporally align with dynamic video\ncontent. In this paper, we propose Beyond Video-to-SFX (BVS), a method to\ngenerate synchronized audio with environmentally aware intelligible speech for\ngiven videos. We introduce a two-stage modeling method: (1) stage one is a\nvideo-guided audio semantic (V2AS) model to predict unified audio semantic\ntokens conditioned on phonetic cues; (2) stage two is a video-conditioned\nsemantic-to-acoustic (VS2A) model that refines semantic tokens into detailed\nacoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios\nsuch as video-to-context-aware speech synthesis and immersive audio background\nconversion, with ablation studies further validating our design. Our\ndemonstration is available\nat~\\href{https://xinleiniu.github.io/BVS-demo/}{BVS-Demo}."}
{"id": "2509.15627", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15627", "abs": "https://arxiv.org/abs/2509.15627", "authors": ["Ziyuan Zheng", "Qingqing Wu", "Yanze Zhu", "Wen Chen", "Ying Gao", "Honghao Wang"], "title": "Wireless Sensing with Movable Intelligent Surface", "comment": "13 pages, 11 figures, submitted to an IEEE Journal for possible\n  publications", "summary": "Future wireless networks are envisioned to deliver not only gigabit\ncommunications but also ubiquitous sensing. Reconfigurable intelligent surfaces\n(RISs) have emerged to reshape radio propagation, recently showing considerable\npromise for wireless sensing. Still, their per-element electronic tuning incurs\nprohibitive hardware cost and power consumption. Motivated by the concept of\nfluid antenna system (FAS), this paper introduces a low-cost movable\nintelligent surface (MIS) for wireless sensing, which replaces element-wise\nelectronic phase tuning with panel-wise mechanical reconfiguration. The MIS\nstacks a large fixed and a smaller movable pre-phased metasurface layers, whose\ndifferential position shifts synthesize distinct composite phase patterns,\nenabling multiple beam patterns for multi-target detection. We characterize a\nMIS-enabled multi-hop echo signal model with multi-target interference and then\nformulate a worst-case sensing signal-to-interference-plus-noise ratio (SINR)\nmaximization problem that jointly designs MIS phase shifts and schedules MS2's\nposition. A Riemannian Augmented Lagrangian Method (RALM)-based algorithm is\ndeveloped to solve the formulated mixed-integer non-convex problem. We also\nderive a heuristic MIS beam steering design with closed-form phase distribution\nand position scheduling. Simulations validate MIS's beam pattern\nreconfiguration capability, show that the RALM-based scheme significantly\noutperforms the closed-form scheme in improving sensing SINR, and uncover a\ngain-diversity trade-off in beam patterns that informs the optimal choice of\nMIS configuration."}
{"id": "2509.15899", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15899", "abs": "https://arxiv.org/abs/2509.15899", "authors": ["Younghoo Kwon", "Jung-Woo Choi"], "title": "Sound Separation and Classification with Object and Semantic Guidance", "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "The spatial semantic segmentation task focuses on separating and classifying\nsound objects from multichannel signals. To achieve two different goals,\nconventional methods fine-tune a large classification model cascaded with the\nseparation model and inject classified labels as separation clues for the next\niteration step. However, such integration is not ideal, in that fine-tuning\nover a smaller dataset loses the diversity of large classification models,\nfeatures from the source separation model are different from the inputs of the\npretrained classifier, and injected one-hot class labels lack semantic depth,\noften leading to error propagation. To resolve these issues, we propose a\nDual-Path Classifier (DPC) architecture that combines object features from a\nsource separation model with semantic representations acquired from a\npretrained classification model without fine-tuning. We also introduce a\nSemantic Clue Encoder (SCE) that enriches the semantic depth of injected clues.\nOur system achieves a state-of-the-art 11.19 dB CA-SDRi and enhanced semantic\nfidelity on the DCASE 2025 task4 evaluation set, surpassing the top-rank\nperformance of 11.00 dB. These results highlight the effectiveness of\nintegrating separator-derived features and rich semantic clues."}
{"id": "2509.15570", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15570", "abs": "https://arxiv.org/abs/2509.15570", "authors": ["Xinxin Meng", "Jiangtao Guo", "Yunxiang Zhang", "Shun Huang"], "title": "Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection", "comment": "Accepted CVIPPR 2024 April Xiamen China", "summary": "The outlier exposure method is an effective approach to address the\nunsupervised anomaly sound detection problem. The key focus of this method is\nhow to make the model learn the distribution space of normal data. Based on\nbiological perception and data analysis, it is found that anomalous audio and\nnoise often have higher frequencies. Therefore, we propose a data augmentation\nmethod for high-frequency information in contrastive learning. This enables the\nmodel to pay more attention to the low-frequency information of the audio,\nwhich represents the normal operational mode of the machine. We evaluated the\nproposed method on the DCASE 2020 Task 2. The results showed that our method\noutperformed other contrastive learning methods used on this dataset. We also\nevaluated the generalizability of our method on the DCASE 2022 Task 2 dataset."}
{"id": "2509.15636", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15636", "abs": "https://arxiv.org/abs/2509.15636", "authors": ["Tobias Lafer", "Erik Leitinger", "Klaus Witrisal"], "title": "Optimizing Sparse Antenna Arrays for Localization and Sensing using Vector Spherical Wave Functions", "comment": null, "summary": "In increasing number of electronic devices implement wideband radio\ntechnologies for localization and sensing purposes, like ultra-wideband (UWB).\nSuch radio technologies benefit from a large number of antennas, but space for\nantennas is often limited, especially in devices for mobile and IoT\napplications. A common challenge is therefore to optimize the placement and\norientations of a small number of antenna elements inside a device, leading to\nthe best localization performance. We propose a method for systematically\napproaching the optimization of such sparse arrays by means of Cram\\'er-Rao\nlower bounds (CRLBs) and vector spherical wave functions (VSWFs). The VSWFs\nform the basis of a wideband signal model considering frequency, direction and\npolarization-dependent characteristics of the antenna array under test (AUT),\ntogether with mutual coupling and distortions from surrounding obstacles. We\nderive the CRLBs for localization parameters like delay and angle-of-arrival\nfor this model under additive white Gaussian noise channel conditions, and\nformulate optimization problems for determining optimal antenna positions and\norientations via minimization of the CRLBs. The proposed optimization procedure\nis demonstrated by means of an exemplary arrangement of three Crossed\nExponentially Tapered Slot (XETS) antennas."}
{"id": "2509.15969", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15969", "abs": "https://arxiv.org/abs/2509.15969", "authors": ["Nikita Torgashov", "Gustav Eje Henter", "Gabriel Skantze"], "title": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency", "comment": "5 pages, 1 figure, submitted to IEEE ICASSP 2026", "summary": "We present VoXtream, a fully autoregressive, zero-shot streaming\ntext-to-speech (TTS) system for real-time use that begins speaking from the\nfirst word. VoXtream directly maps incoming phonemes to audio tokens using a\nmonotonic alignment scheme and a dynamic look-ahead that does not delay onset.\nBuilt around an incremental phoneme transformer, a temporal transformer\npredicting semantic and duration tokens, and a depth transformer producing\nacoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay\namong publicly available streaming TTS: 102 ms on GPU. Despite being trained on\na mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several\nmetrics, while delivering competitive quality in both output- and\nfull-streaming settings. Demo and code are available at\nhttps://herimor.github.io/voxtream."}
{"id": "2509.15612", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15612", "abs": "https://arxiv.org/abs/2509.15612", "authors": ["Yiru Zhang", "Hang Su", "Lichun Fan", "Zhenbo Luo", "Jian Luan"], "title": "Thinking in cocktail party: Chain-of-Thought and reinforcement learning for target speaker automatic speech recognition", "comment": "submitted to ICASSP 2026", "summary": "Target Speaker Automatic Speech Recognition (TS-ASR) aims to transcribe the\nspeech of a specified target speaker from multi-speaker mixtures in cocktail\nparty scenarios. Recent advancement of Large Audio-Language Models (LALMs) has\nalready brought some new insights to TS-ASR. However, significant room for\noptimization remains for the TS-ASR task within the LALMs architecture. While\nChain of Thoughts (CoT) and Reinforcement Learning (RL) have proven effective\nin certain speech tasks, TS-ASR, which requires the model to deeply comprehend\nspeech signals, differentiate various speakers, and handle overlapping\nutterances is particularly well-suited to a reasoning-guided approach.\nTherefore, we propose a novel framework that incorporates CoT and RL training\ninto TS-ASR for performance improvement. A novel CoT dataset of TS-ASR is\nconstructed, and the TS-ASR model is first trained on regular data and then\nfine-tuned on CoT data. Finally, the model is further trained with RL using\nselected data to enhance generalized reasoning capabilities. Experiment results\ndemonstrate a significant improvement of TS-ASR performance with CoT and RL\ntraining, establishing a state-of-the-art performance compared with previous\nworks of TS-ASR on comparable datasets."}
{"id": "2509.15650", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15650", "abs": "https://arxiv.org/abs/2509.15650", "authors": ["Sven Hinderer", "Zheming Yin", "Athanasios Papanikolaou", "Jan Hesselbarth", "Bin Yang"], "title": "Hybrid Baseband Simulation for Single-Channel Radar-Based Indoor Localization System", "comment": null, "summary": "Indoor localization with chirp sequence radar at millimeter wavelength offers\nhigh localization accuracy at low system cost. We propose a hybrid radar\nbaseband signal simulator for our novel single-channel radar-based indoor\nlocalization system consisting of an active radar and passive reflectors as\nreferences. By combining ray tracing channel simulations with real measurements\nof the two-way antenna gain of the radar and accurate simulation of the radar\ncross section of chosen reflectors, realistic modeling of the baseband receive\nsignal in complex scenarios is achieved."}
{"id": "2509.16023", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16023", "abs": "https://arxiv.org/abs/2509.16023", "authors": ["Aristeidis Papadopoulos", "Naomi Harte"], "title": "Interpreting the Role of Visemes in Audio-Visual Speech Recognition", "comment": "Accepted into Automatic Speech Recognition and Understanding- ASRU\n  2025", "summary": "Audio-Visual Speech Recognition (AVSR) models have surpassed their audio-only\ncounterparts in terms of performance. However, the interpretability of AVSR\nsystems, particularly the role of the visual modality, remains under-explored.\nIn this paper, we apply several interpretability techniques to examine how\nvisemes are encoded in AV-HuBERT a state-of-the-art AVSR model. First, we use\nt-distributed Stochastic Neighbour Embedding (t-SNE) to visualize learned\nfeatures, revealing natural clustering driven by visual cues, which is further\nrefined by the presence of audio. Then, we employ probing to show how audio\ncontributes to refining feature representations, particularly for visemes that\nare visually ambiguous or under-represented. Our findings shed light on the\ninterplay between modalities in AVSR and could point to new strategies for\nleveraging visual information to improve AVSR performance."}
{"id": "2509.15622", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15622", "abs": "https://arxiv.org/abs/2509.15622", "authors": ["Valtteri Kallinen", "Lauri Juvela"], "title": "De-crackling Virtual Analog Controls with Asymptotically Stable Recurrent Neural Networks", "comment": null, "summary": "Recurrent neural networks are used in virtual analog modeling applications to\ndigitally replicate the sound of analog hardware audio processors. The controls\nof hardware devices can be used as a conditioning input to these networks. A\ncommon method for introducing control conditioning to these models is the\ndirect static concatenation of controls with input audio samples, which we show\nproduces audio artifacts under time-varied conditioning. Here we derive\nconstraints for asymptotically stable variants of commonly used recurrent\nneural networks and demonstrate that asymptotical stability in recurrent neural\nnetworks can eliminate audio artifacts from the model output under zero input\nand time-varied conditioning. Furthermore, our results suggest a possible\ngeneral solution to mitigate conditioning-induced artifacts in other audio\nneural network architectures, such as convolutional and state-space models."}
{"id": "2509.15681", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15681", "abs": "https://arxiv.org/abs/2509.15681", "authors": ["Jiahuan Wu", "Xinchun Yu", "Xiao-Ping Zhang"], "title": "Extended k-u Fading Model in mmWave Communication: Statistical Properties and Performance Evaluations", "comment": null, "summary": "This study proposes a small-scale fading model, named the extended k-u model,\nwhich incorporates the imbalance of multipath clusters by adding a new\nparameter based on the original k-u model. The extended k-u model outperforms\nthe k-u model in characterizing small-scale fading in the millimeter-wave\n(mmWave) band and has more accurate modeling capability than the extended n-u\nmodel in scenarios with line-of-sight (LoS) paths. And it is mathematically\nmore tractable than the a-k-n-u model. Experiments are conducted for mmWave\ncommunication scenarios with LoS paths, covering the outdoor 28 GHz band, the\nindoor 65 GHz band, and the indoor 92 GHz band. The results demonstrate that\nthe extended k-u model achieves a smaller mean square error in fitting the\nmeasured data compared to both the k-u model and the extended n-u model across\nall scenarios. In addition, through theoretical derivations, closed-form\nexpressions are obtained for the key statistical characteristics of the\nextended k-u model, including the probability density function, cumulative\ndistribution function, moments of arbitrary order, and moment generating\nfunction. Based on these statistics, this study further derives and analyzes\nthe expressions for some performance metrics of the communication system,\nincluding the amount of fading, the probability of outage, and the average bit\nerror rate."}
{"id": "2509.16182", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16182", "abs": "https://arxiv.org/abs/2509.16182", "authors": ["Orchid Chetia Phukan", "Mohd Mujtaba Akhtar", "Girish", "Swarup Ranjan Behera", "Parabattina Bhagath", "Pailla Balakrishna Reddy", "Arun Balaji Buduru"], "title": "Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are Paralinguistic Pre-Trained Representations Sufficient?", "comment": "Accepted to APSIPA-ASC 2025", "summary": "Recent benchmarks evaluating pre-trained models (PTMs) for cross-corpus\nspeech emotion recognition (SER) have overlooked PTM pre-trained for\nparalinguistic speech processing (PSP), raising concerns about their\nreliability, since SER is inherently a paralinguistic task. We hypothesize that\nPSP-focused PTM will perform better in cross-corpus SER settings. To test this,\nwe analyze state-of-the-art PTMs representations including paralinguistic,\nmonolingual, multilingual, and speaker recognition. Our results confirm that\nTRILLsson (a paralinguistic PTM) outperforms others, reinforcing the need to\nconsider PSP-focused PTMs in cross-corpus SER benchmarks. This study enhances\nbenchmark trustworthiness and guides PTMs evaluations for reliable cross-corpus\nSER."}
{"id": "2509.15625", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15625", "abs": "https://arxiv.org/abs/2509.15625", "authors": ["Patrick O'Reilly", "Julia Barnett", "Hugo Flores García", "Annie Chu", "Nathan Pruyne", "Prem Seetharaman", "Bryan Pardo"], "title": "The Rhythm In Anything: Audio-Prompted Drums Generation with Masked Language Modeling", "comment": "ISMIR 2025", "summary": "Musicians and nonmusicians alike use rhythmic sound gestures, such as tapping\nand beatboxing, to express drum patterns. While these gestures effectively\ncommunicate musical ideas, realizing these ideas as fully-produced drum\nrecordings can be time-consuming, potentially disrupting many creative\nworkflows. To bridge this gap, we present TRIA (The Rhythm In Anything), a\nmasked transformer model for mapping rhythmic sound gestures to high-fidelity\ndrum recordings. Given an audio prompt of the desired rhythmic pattern and a\nsecond prompt to represent drumkit timbre, TRIA produces audio of a drumkit\nplaying the desired rhythm (with appropriate elaborations) in the desired\ntimbre. Subjective and objective evaluations show that a TRIA model trained on\nless than 10 hours of publicly-available drum data can generate high-quality,\nfaithful realizations of sound gestures across a wide range of timbres in a\nzero-shot manner."}
{"id": "2509.15718", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15718", "abs": "https://arxiv.org/abs/2509.15718", "authors": ["Hao Zhang", "Fuhui Zhou", "Qihui Wu", "Chau Yuen"], "title": "Distributed Multi-Task Learning for Joint Wireless Signal Enhancement and Recognition", "comment": "accepted by Transactions on Cognitive Communications and Networking", "summary": "Wireless signal recognition (WSR) is crucial in modern and future wireless\ncommunication networks since it aims to identify the properties of the received\nsignal in a no-collaborative manner. However, it is challenging to accurately\nclassify signals in low signal-to-noise ratio (SNR) conditions and distributed\nnetwork settings. In this paper, we propose a novel distributed multi-task\nlearning framework for joint wireless signal enhancement and recognition\n(WSER), addressing the crucial need for non-collaborative signal identification\nin modern wireless networks. Our approach integrates a wireless signal\nenhancement and recognition network (WSERNet) with FedProx+, an enhanced\nfederated learning algorithm designed for heterogeneous data distributions.\nSpecifically, WSERNet leverages an asymmetric convolution block (ACBlock) to\ncapture long-range dependencies in the input signal and improve the performance\nof the deep learning model. FedProx+ introduces a proximal term to the loss\nfunction to encourage the model updates to be closer to the previous model,\nenhancing the convergence speed and robustness of federated learning. Extensive\nexperiments demonstrate the effectiveness of the proposed framework for joint\nWSER, achieving superior performance compared to state-of-the-art methods under\nboth centralized and distributed settings including independent and identically\ndistributed (IID) and non-IID data distributions."}
{"id": "2509.16193", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16193", "abs": "https://arxiv.org/abs/2509.16193", "authors": ["Mohd Mujtaba Akhtar", "Girish", "Orchid Chetia Phukan", "Swarup Ranjan Behera", "Pailla Balakrishna Reddy", "Ananda Chandra Nayak", "Sanjib Kumar Nayak", "Arun Balaji Buduru"], "title": "Are Multimodal Foundation Models All That Is Needed for Emofake Detection?", "comment": "Accepted to APSIPA-ASC 2025", "summary": "In this work, we investigate multimodal foundation models (MFMs) for EmoFake\ndetection (EFD) and hypothesize that they will outperform audio foundation\nmodels (AFMs). MFMs due to their cross-modal pre-training, learns emotional\npatterns from multiple modalities, while AFMs rely only on audio. As such, MFMs\ncan better recognize unnatural emotional shifts and inconsistencies in\nmanipulated audio, making them more effective at distinguishing real from fake\nemotional expressions. To validate our hypothesis, we conduct a comprehensive\ncomparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind)\nalongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs for\nEFD. Beyond individual foundation models (FMs) performance, we explore FMs\nfusion, motivated by findings in related research areas such synthetic speech\ndetection and speech emotion recognition. To this end, we propose SCAR, a novel\nframework for effective fusion. SCAR introduces a nested cross-attention\nmechanism, where representations from FMs interact at two stages sequentially\nto refine information exchange. Additionally, a self-attention refinement\nmodule further enhances feature representations by reinforcing important\ncross-FM cues while suppressing noise. Through SCAR with synergistic fusion of\nMFMs, we achieve SOTA performance, surpassing both standalone FMs and\nconventional fusion approaches and previous works on EFD."}
{"id": "2509.15626", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15626", "abs": "https://arxiv.org/abs/2509.15626", "authors": ["Junki Ohmura", "Yuki Ito", "Emiru Tsunoo", "Toshiyuki Sekiya", "Toshiyuki Kumakura"], "title": "LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice Impression Control", "comment": "Submitted to ICASSP 2026", "summary": "Fine-grained control over voice impressions (e.g., making a voice brighter or\ncalmer) is a key frontier for creating more controllable text-to-speech.\nHowever, this nascent field faces two key challenges. The first is the problem\nof impression leakage, where the synthesized voice is undesirably influenced by\nthe speaker's reference audio, rather than the separately specified target\nimpression, and the second is the lack of a public, annotated corpus. To\nmitigate impression leakage, we propose two methods: 1) a training strategy\nthat separately uses an utterance for speaker identity and another utterance of\nthe same speaker for target impression, and 2) a novel reference-free model\nthat generates a speaker embedding solely from the target impression, achieving\nthe benefits of improved robustness against the leakage and the convenience of\nreference-free generation. Objective and subjective evaluations demonstrate a\nsignificant improvement in controllability. Our best method reduced the mean\nsquared error of 11-dimensional voice impression vectors from 0.61 to 0.41\nobjectively and from 1.15 to 0.92 subjectively, while maintaining high\nfidelity. To foster reproducible research, we introduce LibriTTS-VI, the first\npublic voice impression dataset released with clear annotation standards, built\nupon the LibriTTS-R corpus."}
{"id": "2509.15766", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15766", "abs": "https://arxiv.org/abs/2509.15766", "authors": ["Peihao Dong", "Jingchun Wang", "Shen Gao", "Fuhui Zhou", "Qihui Wu"], "title": "Explainable Deep Learning Based Adversarial Defense for Automatic Modulation Classification", "comment": "Accepted by IEEE Internet of Things Journal", "summary": "Deep learning (DL) has been widely applied to enhance automatic modulation\nclassification (AMC). However, the elaborate AMC neural networks are\nsusceptible to various adversarial attacks, which are challenging to handle due\nto the generalization capability and computational cost. In this article, an\nexplainable DL based defense scheme, called SHapley Additive exPlanation\nenhanced Adversarial Fine-Tuning (SHAP-AFT), is developed in the perspective of\ndisclosing the attacking impact on the AMC network. By introducing the concept\nof cognitive negative information, the motivation of using SHAP for defense is\ntheoretically analyzed first. The proposed scheme includes three stages, i.e.,\nthe attack detection, the information importance evaluation, and the AFT. The\nfirst stage indicates the existence of the attack. The second stage evaluates\ncontributions of the received data and removes those data positions using\nnegative Shapley values corresponding to the dominating negative information\ncaused by the attack. Then the AMC network is fine-tuned based on adversarial\nadaptation samples using the refined received data pattern. Simulation results\nshow the effectiveness of the Shapley value as the key indicator as well as the\nsuperior defense performance of the proposed SHAP-AFT scheme in face of\ndifferent attack types and intensities."}
{"id": "2509.15253", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15253", "abs": "https://arxiv.org/abs/2509.15253", "authors": ["Zhiwen Qian", "Jinhua Liang", "Huan Zhang"], "title": "Emotion-Aware Speech Generation with Character-Specific Voices for Comics", "comment": null, "summary": "This paper presents an end-to-end pipeline for generating character-specific,\nemotion-aware speech from comics. The proposed system takes full comic volumes\nas input and produces speech aligned with each character's dialogue and\nemotional state. An image processing module performs character detection, text\nrecognition, and emotion intensity recognition. A large language model performs\ndialogue attribution and emotion analysis by integrating visual information\nwith the evolving plot context. Speech is synthesized through a text-to-speech\nmodel with distinct voice profiles tailored to each character and emotion. This\nwork enables automated voiceover generation for comics, offering a step toward\ninteractive and immersive comic reading experience."}
{"id": "2509.15629", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15629", "abs": "https://arxiv.org/abs/2509.15629", "authors": ["Lester Phillip Violeta", "Xueyao Zhang", "Jiatong Shi", "Yusuke Yasuda", "Wen-Chin Huang", "Zhizheng Wu", "Tomoki Toda"], "title": "The Singing Voice Conversion Challenge 2025: From Singer Identity Conversion To Singing Style Conversion", "comment": null, "summary": "We present the findings of the latest iteration of the Singing Voice\nConversion Challenge, a scientific event aiming to compare and understand\ndifferent voice conversion systems in a controlled environment. Compared to\nprevious iterations which solely focused on converting the singer identity,\nthis year we also focused on converting the singing style of the singer. To\ncreate a controlled environment and thorough evaluations, we developed a new\nchallenge database, introduced two tasks, open-sourced baselines, and conducted\nlarge-scale crowd-sourced listening tests and objective evaluations. The\nchallenge was ran for two months and in total we evaluated 26 different\nsystems. The results of the large-scale crowd-sourced listening test showed\nthat top systems had comparable singer identity scores to ground truth samples.\nHowever, modeling the singing style and consequently achieving high naturalness\nstill remains a challenge in this task, primarily due to the difficulty in\nmodeling dynamic information in breathy, glissando, and vibrato singing styles."}
{"id": "2509.15902", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15902", "abs": "https://arxiv.org/abs/2509.15902", "authors": ["Haofan Dong", "Ozgur B. Akan"], "title": "Fundamental Limits of THz Inter-Satellite ISAC Under Hardware Impairments", "comment": null, "summary": "This paper establishes a theoretical framework for analyzing the fundamental\nperformance limits of terahertz (THz) Low Earth Orbit (LEO) inter-satellite\nlink (ISL) Integrated Sensing and Communications (ISAC) systems. We develop a\nunified, end-to-end signal model that, jointly captures the effects of extreme\norbital dynamics, cascaded non-ideal hardware impairments, and micro-radian\nbeam pointing errors. Through Bayesian Cram\\'er-Rao Lower Bound (BCRLB)\nanalysis, we derive the ultimate sensing accuracy for range and range-rate,\nrevealing a quadratic ($1/f_c^2$) improvement in estimation variance with\ncarrier frequency, which is ultimately floored by signal-dependent hardware\ndistortion. For communication, we show that system performance is not\npower-limited but hardware-limited, deriving a closed-form capacity ceiling\nunder the joint effect of phase noise and PA nonlinearity: $C_{\\text{sat}} =\n\\log_2(1 + e^{-\\sigma_\\phi^2}/\\Gamma_{\\text{eff}})$, where\n$\\Gamma_{\\text{eff}}$ is a proposed hardware quality factor. Our numerical\nresults, based on state-of-the-art component data and the identified\ntrade-offs, suggest that favorable operational conditions may exist in the\nsub-THz frequency range (200-600 GHz) where the quadratic sensing gain with\nfrequency is balanced against hardware quality degradation. Power Amplifier\n(PA) nonlinearity emerges as the dominant performance bottleneck, exceeding\nother impairments by one to two orders of magnitude."}
{"id": "2509.15389", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15389", "abs": "https://arxiv.org/abs/2509.15389", "authors": ["Youngwon Choi", "Jaeyoon Jung", "Hyeonyu Kim", "Huu-Kim Nguyen", "Hwayeon Kim"], "title": "Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data", "comment": "4 pages (excluding references), 2 figures, submitted to ICASSP 2026", "summary": "Large Audio Language Models (LALMs) have emerged as powerful tools for\nspeech-related tasks but remain underexplored for fine-tuning, especially with\nlimited speech data. To bridge this gap, we systematically examine how\ndifferent fine-tuning schemes including text-only, direct mixing, and\ncurriculum learning affect spoken language understanding (SLU), focusing on\nscenarios where text-label pairs are abundant while paired speech-label data\nare limited. Results show that LALMs already achieve competitive performance\nwith text-only fine-tuning, highlighting their strong generalization ability.\nAdding even small amounts of speech data (2-5%) yields substantial further\ngains, with curriculum learning particularly effective under scarce data. In\ncross-lingual SLU, combining source-language speech data with target-language\ntext and minimal target-language speech data enables effective adaptation.\nOverall, this study provides practical insights into the LALM fine-tuning under\nrealistic data constraints."}
{"id": "2509.15654", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15654", "abs": "https://arxiv.org/abs/2509.15654", "authors": ["Pengcheng Li", "Botao Zhao", "Zuheng Kang", "Junqing Peng", "Xiaoyang Qu", "Yayun He", "Jianzong Wang"], "title": "EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model for Generalized Speech Emotion Recognition", "comment": "Accpeted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)", "summary": "Although Large Audio-Language Models (LALMs) have exhibited outstanding\nperformance in auditory understanding, their performance in affective computing\nscenarios, particularly in emotion recognition, reasoning, and subtle sentiment\ndifferentiation, remains suboptimal. Recent advances in Reinforcement Learning\n(RL) have shown promise in improving LALMs' reasoning abilities. However, two\ncritical challenges hinder the direct application of RL techniques to Speech\nEmotion Recognition (SER) tasks: (1) convergence instability caused by\nambiguous emotional boundaries and (2) limited reasoning ability when using\nrelatively small models (e.g., 7B-parameter architectures). To overcome these\nlimitations, we introduce EMO-RL, a novel framework incorporating reinforcement\nlearning with two key innovations: Emotion Similarity-Weighted Reward (ESWR)\nand Explicit Structured Reasoning (ESR). Built upon pretrained LALMs, our\nmethod employs group-relative policy optimization with emotion constraints.\nComprehensive experiments demonstrate that our EMO-RL training strategies can\nsignificantly enhance the emotional reasoning capabilities of LALMs, attaining\nstate-of-the-art results on both the MELD and IEMOCAP datasets, and\ncross-dataset experiments prove the strong superiority of generalization."}
{"id": "2509.15964", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15964", "abs": "https://arxiv.org/abs/2509.15964", "authors": ["Tianyu Li", "Yan Xin", "Jianzhong", "Zhang"], "title": "MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework", "comment": null, "summary": "Reliable channel estimation (CE) is fundamental for robust communication in\ndynamic wireless environments, where models must generalize across varying\nconditions such as signal-to-noise ratios (SNRs), the number of resource blocks\n(RBs), and channel profiles. Traditional deep learning (DL)-based methods\nstruggle to generalize effectively across such diverse settings, particularly\nunder multitask and zero-shot scenarios. In this work, we propose MoE-CE, a\nflexible mixture-of-experts (MoE) framework designed to enhance the\ngeneralization capability of DL-based CE methods. MoE-CE provides an\nappropriate inductive bias by leveraging multiple expert subnetworks, each\nspecialized in distinct channel characteristics, and a learned router that\ndynamically selects the most relevant experts per input. This architecture\nenhances model capacity and adaptability without a proportional rise in\ncomputational cost while being agnostic to the choice of the backbone model and\nthe learning algorithm. Through extensive experiments on synthetic datasets\ngenerated under diverse SNRs, RB numbers, and channel profiles, including\nmultitask and zero-shot evaluations, we demonstrate that MoE-CE consistently\noutperforms conventional DL approaches, achieving significant performance gains\nwhile maintaining efficiency."}
{"id": "2509.15437", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS", "I.2.0; I.2.7; I.5.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.15437", "abs": "https://arxiv.org/abs/2509.15437", "authors": ["Daniyal Kabir Dar", "Qiben Yan", "Li Xiao", "Arun Ross"], "title": "Impact of Phonetics on Speaker Identity in Adversarial Voice Attack", "comment": "Additional figures for extended visualization:\n  https://daniyalkabir.github.io/icassp-2025-results/", "summary": "Adversarial perturbations in speech pose a serious threat to automatic speech\nrecognition (ASR) and speaker verification by introducing subtle waveform\nmodifications that remain imperceptible to humans but can significantly alter\nsystem outputs. While targeted attacks on end-to-end ASR models have been\nwidely studied, the phonetic basis of these perturbations and their effect on\nspeaker identity remain underexplored. In this work, we analyze adversarial\naudio at the phonetic level and show that perturbations exploit systematic\nconfusions such as vowel centralization and consonant substitutions. These\ndistortions not only mislead transcription but also degrade phonetic cues\ncritical for speaker verification, leading to identity drift. Using DeepSpeech\nas our ASR target, we generate targeted adversarial examples and evaluate their\nimpact on speaker embeddings across genuine and impostor samples. Results\nacross 16 phonetically diverse target phrases demonstrate that adversarial\naudio induces both transcription errors and identity drift, highlighting the\nneed for phonetic-aware defenses to ensure the robustness of ASR and speaker\nrecognition systems."}
{"id": "2509.15661", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15661", "abs": "https://arxiv.org/abs/2509.15661", "authors": ["Qiaolin Wang", "Xilin Jiang", "Linyang He", "Junkai Wu", "Nima Mesgarani"], "title": "SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models", "comment": null, "summary": "While large audio-language models (LALMs) have demonstrated state-of-the-art\naudio understanding, their reasoning capability in complex soundscapes still\nfalls behind large vision-language models (LVLMs). Compared to the visual\ndomain, one bottleneck is the lack of large-scale chain-of-thought audio data\nto teach LALM stepwise reasoning. To circumvent this data and modality gap, we\npresent SightSound-R1, a cross-modal distillation framework that transfers\nadvanced reasoning from a stronger LVLM teacher to a weaker LALM student on the\nsame audio-visual question answering (AVQA) dataset. SightSound-R1 consists of\nthree core steps: (i) test-time scaling to generate audio-focused chains of\nthought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter\nhallucinations, and (iii) a distillation pipeline with supervised fine-tuning\n(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM\nstudent. Results show that SightSound-R1 improves LALM reasoning performance\nboth in the in-domain AVQA test set as well as in unseen auditory scenes and\nquestions, outperforming both pretrained and label-only distilled baselines.\nThus, we conclude that vision reasoning can be effectively transferred to audio\nmodels and scaled with abundant audio-visual data."}
{"id": "2509.15973", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15973", "abs": "https://arxiv.org/abs/2509.15973", "authors": ["Yiming Zhou", "Wei Dai"], "title": "Scalable Hessian-free Proximal Conjugate Gradient Method for Nonconvex and Nonsmooth Optimization", "comment": "Manuscript for ICASSP 2026 Submission", "summary": "This work studies a composite minimization problem involving a differentiable\nfunction q and a nonsmooth function h, both of which may be nonconvex. This\nproblem is ubiquitous in signal processing and machine learning yet remains\nchallenging to solve efficiently, particularly when large-scale instances, poor\nconditioning, and nonconvexity coincide. To address these challenges, we\npropose a proximal conjugate gradient method (PCG) that matches the fast\nconvergence of proximal (quasi-)Newton algorithms while reducing computation\nand memory complexity, and is especially effective for spectrally clustered\nHessians. Our key innovation is to form, at each iteration, an approximation to\nthe Newton direction based on CG iterations to build a majorization surrogate.\nWe define this surrogate in a curvature-aware manner and equip it with a\nCG-derived isotropic weight, guaranteeing majorization of a local second-order\nmodel of q along the given direction. To better preserve majorization after the\nproximal step and enable further approximation refinement, we scale the CG\ndirection by the ratio between the Cauchy step length and a step size derived\nfrom the largest Ritz value of the CG tridiagonal. All curvature is accessed\nvia Hessian-vector products computed by automatic differentiation, keeping the\nmethod Hessian-free. Convergence to first-order critical points is established.\nNumerical experiments on CS-MRI with nonconvex regularization and on dictionary\nlearning, against benchmark methods, demonstrate the efficiency of the proposed\napproach."}
{"id": "2509.15462", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15462", "abs": "https://arxiv.org/abs/2509.15462", "authors": ["Ryan Collette", "Ross Greenwood", "Serena Nicoll"], "title": "A Novel Semantic Compression Approach for Ultra-low Bandwidth Voice Communication", "comment": "5 pages, 2 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "While existing speech audio codecs designed for compression exploit limited\nforms of temporal redundancy and allow for multi-scale representations, they\ntend to represent all features of audio in the same way. In contrast,\ngenerative voice models designed for text-to-speech and voice transfer tasks\nhave recently proved effective at factorizing audio signals into high-level\nsemantic representations of fundamentally distinct features. In this paper, we\nleverage such representations in a novel semantic communications approach to\nachieve lower bitrates without sacrificing perceptual quality or suitability\nfor specific downstream tasks. Our technique matches or outperforms existing\naudio codecs on transcription, sentiment analysis, and speaker verification\nwhen encoding at 2-4x lower bitrate -- notably surpassing Encodec in perceptual\nquality and speaker verification while using up to 4x less bitrate."}
{"id": "2509.15666", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15666", "abs": "https://arxiv.org/abs/2509.15666", "authors": ["Yongsheng Feng", "Yuetonghui Xu", "Jiehui Luo", "Hongjia Liu", "Xiaobing Li", "Feng Yu", "Wei Li"], "title": "TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation", "comment": "submitted to ICASSP 2026", "summary": "Source separation is a fundamental task in speech, music, and audio\nprocessing, and it also provides cleaner and larger data for training\ngenerative models. However, improving separation performance in practice often\ndepends on increasingly large networks, inflating training and deployment\ncosts. Motivated by recent advances in inference-time scaling for generative\nmodeling, we propose Training-Time and Inference-Time Scalable Discriminative\nSource Separation (TISDiSS), a unified framework that integrates early-split\nmulti-loss supervision, shared-parameter design, and dynamic inference\nrepetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting\ninference depth without retraining additional models. We further provide\nsystematic analyses of architectural and training choices and show that\ntraining with more inference repetitions improves shallow-inference\nperformance, benefiting low-latency applications. Experiments on standard\nspeech separation benchmarks demonstrate state-of-the-art performance with a\nreduced parameter count, establishing TISDiSS as a scalable and practical\nframework for adaptive source separation."}
{"id": "2509.15993", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.15993", "abs": "https://arxiv.org/abs/2509.15993", "authors": ["Yuwei Wang", "Li Sun", "Tingting Yang"], "title": "Wireless Channel Foundation Model with Embedded Noise-Plus-Interference Suppression Structure", "comment": null, "summary": "Wireless channel foundation model (WCFM) is a task-agnostic AI model that is\npretrained on large-scale wireless channel datasets to learn a universal\nchannel feature representation that can be used for a wide range of downstream\ntasks related to communications and sensing. While existing works on WCFM have\ndemonstrated its great potentials in various tasks including beam prediction,\nchannel prediction, localization, etc, the models are all trained using perfect\n(i.e., error-free and complete) channel information state (CSI) data which are\ngenerated with simulation tools. However, in practical systems where the WCFM\nis deployed, perfect CSI is not available. Instead, channel estimation needs to\nbe first performed based on pilot signals over a subset of the resource\nelements (REs) to acquire a noisy version of the CSI (termed as degraded CSI),\nwhich significantly differs from the perfect CSI in some real-world\nenvironments with severe noise and interference. As a result, the feature\nrepresentation generated by the WCFM is unable to reflect the characteristics\nof the true channel, yielding performance degradation in downstream tasks. To\naddress this issue, in this paper we propose an enhanced wireless channel\nfoundation model architecture with noise-plus-interference (NPI) suppression\ncapability. In our approach, coarse estimates of the CSIs are first obtained.\nWith these information, two projection matrices are computed to extract the NPI\nterms in the received signals, which are further processed by a NPI estimation\nand subtraction module. Finally, the resultant signal is passed through a CSI\ncompletion network to get a clean version of the CSI, which is used for feature\nextraction. Simulation results demonstrated that compared to the\nstate-of-the-art solutions, WCFM with NPI suppression structure achieves\nimproved performance on channel prediction task."}
{"id": "2509.15492", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15492", "abs": "https://arxiv.org/abs/2509.15492", "authors": ["Xinlei Niu", "Jianbo Ma", "Dylan Harper-Harris", "Xiangyu Zhang", "Charles Patrick Martin", "Jing Zhang"], "title": "Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech", "comment": null, "summary": "The generation of realistic, context-aware audio is important in real-world\napplications such as video game development. While existing video-to-audio\n(V2A) methods mainly focus on Foley sound generation, they struggle to produce\nintelligible speech. Meanwhile, current environmental speech synthesis\napproaches remain text-driven and fail to temporally align with dynamic video\ncontent. In this paper, we propose Beyond Video-to-SFX (BVS), a method to\ngenerate synchronized audio with environmentally aware intelligible speech for\ngiven videos. We introduce a two-stage modeling method: (1) stage one is a\nvideo-guided audio semantic (V2AS) model to predict unified audio semantic\ntokens conditioned on phonetic cues; (2) stage two is a video-conditioned\nsemantic-to-acoustic (VS2A) model that refines semantic tokens into detailed\nacoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios\nsuch as video-to-context-aware speech synthesis and immersive audio background\nconversion, with ablation studies further validating our design. Our\ndemonstration is available\nat~\\href{https://xinleiniu.github.io/BVS-demo/}{BVS-Demo}."}
{"id": "2509.15680", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15680", "abs": "https://arxiv.org/abs/2509.15680", "authors": ["Taehan Lee", "Jaehan Jung", "Hyukjun Lee"], "title": "Mamba-2 audio captioning: design space exploration and analysis", "comment": "Submitted to the 2026 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2026). Under review", "summary": "We present an audio captioning model built on the Mamba-2 large language\nmodel backbone, which is a state-of-the-art (SOTA) state-space model (SSM). We\nsystematically explore the design space: LLM sizes, LoRA ranks, and connector\ndesigns leveraging Mamba-2's linear-time complexity with respect to sequence\nlength. Across benchmarks, our models achieve strong captioning performance\ncompared with larger language models trained on the same dataset, despite using\nfewer parameters. For the first time, we conduct an in-depth analysis of how\nthe number of LLM parameters, audio encoder fine-tuning strategies, audio\nfeature diversity, and different feature reduction or expansion techniques\naffect performance."}
{"id": "2509.16045", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16045", "abs": "https://arxiv.org/abs/2509.16045", "authors": ["Shan Shan", "Chongjun Ouyang", "Yong Li", "Yuanwei Liu"], "title": "Secure Multicast Communications with Pinching-Antenna Systems (PASS)", "comment": null, "summary": "This article investigates secure multicast communications in pinching-antenna\nsystems (PASS), where pinching beamforming is enabled by adaptively adjusting\npinching antenna (PAs) positions along waveguides to improve multicast\nsecurity. Specifically, a PASS-based secure multicast framework is proposed, in\nwhich joint optimization of transmit and pinching beamforming is conducted to\nmaximize the secrecy multicast rate. i) For the single-group multicast\nscenario, an alternating optimization (AO) framework is employed, where the\npinching beamformer is updated via an element-wise sequential optimization\nmethod. The transmit beamformer is designed via a semidefinite relaxation (SDR)\nformulation for an upper-bound solution, while a Dinkelbach-alternating\ndirection method of multipliers (ADMM) offers a low-complexity alternative. ii)\nFor the multi-group multicast scenario, transmit and pinching beamformers are\nalternately optimized under a majorization-minimization (MM) framework. The\ntransmit beamformer is obtained via SDR or an efficient second-order cone\nprogramming (SOCP) method, while the pinching beamformer is updated through\nMM-based element-wise sequential update strategy. Numerical results are\nprovided to demonstrate that: (i) PASS consistently outperform conventional\nfixed-location antenna architectures in terms of secrecy performance across\nvarious configurations; and (ii) the performance advantage of PASS over\nfixed-location architectures becomes more significant with increased service\nregion, larger antenna arrays, and higher user and eavesdropper densities."}
{"id": "2509.15570", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15570", "abs": "https://arxiv.org/abs/2509.15570", "authors": ["Xinxin Meng", "Jiangtao Guo", "Yunxiang Zhang", "Shun Huang"], "title": "Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection", "comment": "Accepted CVIPPR 2024 April Xiamen China", "summary": "The outlier exposure method is an effective approach to address the\nunsupervised anomaly sound detection problem. The key focus of this method is\nhow to make the model learn the distribution space of normal data. Based on\nbiological perception and data analysis, it is found that anomalous audio and\nnoise often have higher frequencies. Therefore, we propose a data augmentation\nmethod for high-frequency information in contrastive learning. This enables the\nmodel to pay more attention to the low-frequency information of the audio,\nwhich represents the normal operational mode of the machine. We evaluated the\nproposed method on the DCASE 2020 Task 2. The results showed that our method\noutperformed other contrastive learning methods used on this dataset. We also\nevaluated the generalizability of our method on the DCASE 2022 Task 2 dataset."}
{"id": "2509.15692", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15692", "abs": "https://arxiv.org/abs/2509.15692", "authors": ["Pei Zhang", "Yiming Wang", "Jialong Tang", "Baosong Yang", "Rui Wang", "Derek F. Wong", "Fei Huang"], "title": "Direct Simultaneous Translation Activation for Large Audio-Language Models", "comment": null, "summary": "Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech\ninto target text in real time, outputting translations while receiving source\nspeech input, rather than waiting for the entire utterance to be spoken.\nSimul-S2TT research often modifies model architectures to implement read-write\nstrategies. However, with the rise of large audio-language models (LALMs), a\nkey challenge is how to directly activate Simul-S2TT capabilities in base\nmodels without additional architectural changes. In this paper, we introduce\n{\\bf Simul}taneous {\\bf S}elf-{\\bf A}ugmentation ({\\bf SimulSA}), a strategy\nthat utilizes LALMs' inherent capabilities to obtain simultaneous data by\nrandomly truncating speech and constructing partially aligned translation. By\nincorporating them into offline SFT data, SimulSA effectively bridges the\ndistribution gap between offline translation during pretraining and\nsimultaneous translation during inference. Experimental results demonstrate\nthat augmenting only about {\\bf 1\\%} of the simultaneous data, compared to the\nfull offline SFT data, can significantly activate LALMs' Simul-S2TT\ncapabilities without modifications to model architecture or decoding strategy."}
{"id": "2509.16086", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16086", "abs": "https://arxiv.org/abs/2509.16086", "authors": ["Sahil P. Wankhede", "Xiangdong Xie", "Ali H. Alshehri", "Keith W Brashler", "Mohammad Ba'adani", "Doru C Turcan", "Kamal Youcef-Toumi", "Xian Du"], "title": "In-Situ Fault Detection of Submerged Pump Impellers Using Encapsulated Accelerometers and Machine Learning", "comment": "Under review", "summary": "Vertical turbine pumps in oil and gas operations rely on motor-mounted\naccelerometers for condition monitoring. However, these sensors cannot detect\nfaults at submerged impellers exposed to harsh downhole environments. We\npresent the first study deploying encapsulated accelerometers mounted directly\non submerged impeller bowls, enabling in-situ vibration monitoring. Using a\nlab-scale pump setup with 1-meter oil submergence, we collected vibration data\nunder normal and simulated fault conditions. The data were analyzed using a\nsuite of machine learning models -- spanning traditional and deep learning\nmethods -- to evaluate sensor effectiveness. Impeller-mounted sensors achieved\n91.3% average accuracy and 0.973 AUC-ROC, outperforming the best non-submerged\nsensor. Crucially, encapsulation caused no statistically significant\nperformance loss in sensor performance, confirming its viability for\noil-submerged environments. While the lab setup used shallow submergence,\nreal-world pump impellers operate up to hundreds of meters underground -- well\nbeyond the range of surface-mounted sensors. This first-of-its-kind in-situ\nmonitoring system demonstrates that impeller-mounted sensors -- encapsulated\nfor protection while preserving diagnostic fidelity -- can reliably detect\nfaults in critical submerged pump components. By capturing localized vibration\nsignatures that are undetectable from surface-mounted sensors, this approach\nenables earlier fault detection, reduces unplanned downtime, and optimizes\nmaintenance for downhole systems in oil and gas operations."}
{"id": "2509.15603", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15603", "abs": "https://arxiv.org/abs/2509.15603", "authors": ["Sven Hinderer"], "title": "Blind Source Separation of Radar Signals in Time Domain Using Deep Learning", "comment": null, "summary": "Identification and further analysis of radar emitters in a contested\nenvironment requires detection and separation of incoming signals. If they\narrive from the same direction and at similar frequencies, deinterleaving them\nremains challenging. A solution to overcome this limitation becomes\nincreasingly important with the advancement of emitter capabilities. We propose\ntreating the problem as blind source separation in time domain and apply\nsupervisedly trained neural networks to extract the underlying signals from the\nreceived mixture. This allows us to handle highly overlapping and also\ncontinuous wave (CW) signals from both radar and communication emitters. We\nmake use of advancements in the field of audio source separation and extend a\ncurrent state-of-the-art model with the objective of deinterleaving arbitrary\nradio frequency (RF) signals. Results show, that our approach is capable of\nseparating two unknown waveforms in a given frequency band with a single\nchannel receiver."}
{"id": "2509.15703", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15703", "abs": "https://arxiv.org/abs/2509.15703", "authors": ["Yizhou Zhang", "Yuan Gao", "Wangjin Zhou", "Zicheng Yuan", "Keisuke Imoto", "Tatsuya Kawahara"], "title": "SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation", "comment": "Submitted to ICASSP 2026", "summary": "Self-supervised learning (SSL) on large-scale datasets like AudioSet has\nbecome the dominant paradigm for audio representation learning. While the\ncontinuous influx of new, unlabeled audio presents an opportunity to enrich\nthese static representations, a naive approach is to retrain the model from\nscratch using all available data. However, this method is computationally\nprohibitive and discards the valuable knowledge embedded in the previously\ntrained model weights. To address this inefficiency, we propose SONAR\n(Self-distilled cONtinual pre-training for domain adaptive Audio\nRepresentation), a continual pre-training framework built upon BEATs. SONAR\neffectively adapts to new domains while mitigating catastrophic forgetting by\ntackling three key challenges: implementing a joint sampling strategy for new\nand prior data, applying regularization to balance specificity and generality,\nand dynamically expanding the tokenizer codebook for novel acoustic patterns.\nExperiments across four distinct domains demonstrate that our method achieves\nboth high adaptability and robust resistance to forgetting."}
{"id": "2509.16183", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16183", "abs": "https://arxiv.org/abs/2509.16183", "authors": ["Tyler G. R. Reid", "Matteo Gala", "Mathieu Favreau", "Argyris Kriezis", "Michael O'Meara", "Andre Pant", "Paul Tarantino", "Christina Youn"], "title": "Xona Pulsar Compatibility with GNSS", "comment": "15 pages, 12 figures", "summary": "At least ten emerging providers are developing satellite navigation systems\nfor low Earth orbit (LEO). Compatibility with existing GNSS in L-band is\ncritical to their successful deployment and for the larger ecosystem. Xona is\ndeploying Pulsar, a near 260-satellite LEO constellation offering dual L-band\nnavigation services near L1 and L5. Designed for interoperability, Pulsar\nprovides centimeter-level accuracy, resilience, and authentication, while\nmaintaining a format that existing GNSS receivers can support through a\nfirmware update. This study examines Pulsar's compatibility with GPS and\nGalileo by evaluating C/N0 degradation caused by the introduction of its X1 and\nX5 signals. Using spectrally compact QPSK modulation, Pulsar minimizes\ninterference despite higher signal power. Theoretical analysis is supported by\nhardware testing across a range of commercial GNSS receivers in both lab-based\nsimulation and in-orbit live-sky conditions. The study confirms Pulsar causes\nno adverse interference effects to existing GNSS, supporting coexistence and\nintegration within the global PNT ecosystem."}
{"id": "2509.15612", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15612", "abs": "https://arxiv.org/abs/2509.15612", "authors": ["Yiru Zhang", "Hang Su", "Lichun Fan", "Zhenbo Luo", "Jian Luan"], "title": "Thinking in cocktail party: Chain-of-Thought and reinforcement learning for target speaker automatic speech recognition", "comment": "submitted to ICASSP 2026", "summary": "Target Speaker Automatic Speech Recognition (TS-ASR) aims to transcribe the\nspeech of a specified target speaker from multi-speaker mixtures in cocktail\nparty scenarios. Recent advancement of Large Audio-Language Models (LALMs) has\nalready brought some new insights to TS-ASR. However, significant room for\noptimization remains for the TS-ASR task within the LALMs architecture. While\nChain of Thoughts (CoT) and Reinforcement Learning (RL) have proven effective\nin certain speech tasks, TS-ASR, which requires the model to deeply comprehend\nspeech signals, differentiate various speakers, and handle overlapping\nutterances is particularly well-suited to a reasoning-guided approach.\nTherefore, we propose a novel framework that incorporates CoT and RL training\ninto TS-ASR for performance improvement. A novel CoT dataset of TS-ASR is\nconstructed, and the TS-ASR model is first trained on regular data and then\nfine-tuned on CoT data. Finally, the model is further trained with RL using\nselected data to enhance generalized reasoning capabilities. Experiment results\ndemonstrate a significant improvement of TS-ASR performance with CoT and RL\ntraining, establishing a state-of-the-art performance compared with previous\nworks of TS-ASR on comparable datasets."}
{"id": "2509.15775", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15775", "abs": "https://arxiv.org/abs/2509.15775", "authors": ["Yiqing Yang", "Man-Wai Mak"], "title": "EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large Language Model", "comment": "5 pages, 2 figures", "summary": "The performance of speech emotion recognition (SER) is limited by the\ninsufficient emotion information in unimodal systems and the feature alignment\ndifficulties in multimodal systems. Recently, multimodal large language models\n(MLLMs) have made progress in SER. However, MLLMs still suffer from\nhallucination and misclassification problems in complex emotion reasoning. To\naddress these problems, we propose an MLLM-based framework called EmoQ, which\ngenerates query embeddings that fuse multimodal information through an\nEmoQ-Former and uses multi-objective affective learning (MAL) to achieve\nco-optimization. The framework also provides a soft-prompt injection strategy\nto inject multimodal representations into the LLM. This end-to-end architecture\nachieves state-of-the-art performance on the IEMOCAP and MELD datasets,\nproviding a new multimodal fusion paradigm for SER."}
{"id": "2509.15628", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15628", "abs": "https://arxiv.org/abs/2509.15628", "authors": ["Pengyu Wang", "Xiaofei Li"], "title": "Rec-RIR: Monaural Blind Room Impulse Response Identification via DNN-based Reverberant Speech Reconstruction in STFT Domain", "comment": "Submitted to ICASSP 2026", "summary": "Room impulse response (RIR) characterizes the complete propagation process of\nsound in an enclosed space. This paper presents Rec-RIR for monaural blind RIR\nidentification. Rec-RIR is developed based on the convolutive transfer function\n(CTF) approximation, which models reverberation effect within narrow-band\nfilter banks in the short-time Fourier transform (STFT) domain. Specifically,\nwe propose a deep neural network (DNN) with cross-band and narrow-band blocks\nto estimate the CTF filter. The DNN is trained through reconstructing the\nnoise-free reverberant speech spectra. This objective enables stable and\nstraightforward supervised training. Subsequently, a pseudo intrusive\nmeasurement process is employed to convert the CTF filter estimate into\ntime-domain RIR by simulating a common intrusive RIR measurement procedure.\nExperimental results demonstrate that Rec-RIR achieves state-of-the-art (SOTA)\nperformance in both RIR identification and acoustic parameter estimation.\nOpen-source codes are available online at\nhttps://github.com/Audio-WestlakeU/Rec-RIR."}
{"id": "2509.15622", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15622", "abs": "https://arxiv.org/abs/2509.15622", "authors": ["Valtteri Kallinen", "Lauri Juvela"], "title": "De-crackling Virtual Analog Controls with Asymptotically Stable Recurrent Neural Networks", "comment": null, "summary": "Recurrent neural networks are used in virtual analog modeling applications to\ndigitally replicate the sound of analog hardware audio processors. The controls\nof hardware devices can be used as a conditioning input to these networks. A\ncommon method for introducing control conditioning to these models is the\ndirect static concatenation of controls with input audio samples, which we show\nproduces audio artifacts under time-varied conditioning. Here we derive\nconstraints for asymptotically stable variants of commonly used recurrent\nneural networks and demonstrate that asymptotical stability in recurrent neural\nnetworks can eliminate audio artifacts from the model output under zero input\nand time-varied conditioning. Furthermore, our results suggest a possible\ngeneral solution to mitigate conditioning-induced artifacts in other audio\nneural network architectures, such as convolutional and state-space models."}
{"id": "2509.15804", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15804", "abs": "https://arxiv.org/abs/2509.15804", "authors": ["Xueping Zhang", "Liwei Jin", "Yechen Wang", "Linxi Li", "Ming Li"], "title": "CompSpoof: A Dataset and Joint Learning Framework for Component-Level Audio Anti-spoofing Countermeasures", "comment": null, "summary": "Component-level audio Spoofing (Comp-Spoof) targets a new form of audio\nmanipulation where only specific components of a signal, such as speech or\nenvironmental sound, are forged or substituted while other components remain\ngenuine. Existing anti-spoofing datasets and methods treat an utterance or a\nsegment as entirely bona fide or entirely spoofed, and thus cannot accurately\ndetect component-level spoofing. To address this, we construct a new dataset,\nCompSpoof, covering multiple combinations of bona fide and spoofed speech and\nenvironmental sound. We further propose a separation-enhanced joint learning\nframework that separates audio components apart and applies anti-spoofing\nmodels to each one. Joint learning is employed, preserving information relevant\nfor detection. Extensive experiments demonstrate that our method outperforms\nthe baseline, highlighting the necessity of separate components and the\nimportance of detecting spoofing for each component separately. Datasets and\ncode are available at: https://github.com/XuepingZhang/CompSpoof."}
{"id": "2509.15946", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15946", "abs": "https://arxiv.org/abs/2509.15946", "authors": ["Sungho Lee", "Matteo Scerbo", "Seungu Han", "Min Jun Choi", "Kyogu Lee", "Enzo De Sena"], "title": "Differentiable Acoustic Radiance Transfer", "comment": null, "summary": "Geometric acoustics is an efficient approach to room acoustics modeling,\ngoverned by the canonical time-dependent rendering equation. Acoustic radiance\ntransfer (ART) solves the equation through discretization, modeling the time-\nand direction-dependent energy exchange between surface patches given with\nflexible material properties. We introduce DART, a differentiable and efficient\nimplementation of ART that enables gradient-based optimization of material\nproperties. We evaluate DART on a simpler variant of the acoustic field\nlearning task, which aims to predict the energy responses of novel\nsource-receiver settings. Experimental results show that DART exhibits\nfavorable properties, e.g., better generalization under a sparse measurement\nscenario, compared to existing signal processing and neural network baselines,\nwhile remaining a simple, fully interpretable system."}
{"id": "2509.15625", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15625", "abs": "https://arxiv.org/abs/2509.15625", "authors": ["Patrick O'Reilly", "Julia Barnett", "Hugo Flores García", "Annie Chu", "Nathan Pruyne", "Prem Seetharaman", "Bryan Pardo"], "title": "The Rhythm In Anything: Audio-Prompted Drums Generation with Masked Language Modeling", "comment": "ISMIR 2025", "summary": "Musicians and nonmusicians alike use rhythmic sound gestures, such as tapping\nand beatboxing, to express drum patterns. While these gestures effectively\ncommunicate musical ideas, realizing these ideas as fully-produced drum\nrecordings can be time-consuming, potentially disrupting many creative\nworkflows. To bridge this gap, we present TRIA (The Rhythm In Anything), a\nmasked transformer model for mapping rhythmic sound gestures to high-fidelity\ndrum recordings. Given an audio prompt of the desired rhythmic pattern and a\nsecond prompt to represent drumkit timbre, TRIA produces audio of a drumkit\nplaying the desired rhythm (with appropriate elaborations) in the desired\ntimbre. Subjective and objective evaluations show that a TRIA model trained on\nless than 10 hours of publicly-available drum data can generate high-quality,\nfaithful realizations of sound gestures across a wide range of timbres in a\nzero-shot manner."}
{"id": "2509.15808", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15808", "abs": "https://arxiv.org/abs/2509.15808", "authors": ["Máté Gedeon", "Péter Mihajlik"], "title": "From Independence to Interaction: Speaker-Aware Simulation of Multi-Speaker Conversational Timing", "comment": "Submitted to ICASSP 2026", "summary": "We present a speaker-aware approach for simulating multi-speaker\nconversations that captures temporal consistency and realistic turn-taking\ndynamics. Prior work typically models aggregate conversational statistics under\nan independence assumption across speakers and turns. In contrast, our method\nuses speaker-specific deviation distributions enforcing intra-speaker temporal\nconsistency, while a Markov chain governs turn-taking and a fixed room impulse\nresponse preserves spatial realism. We also unify pauses and overlaps into a\nsingle gap distribution, modeled with kernel density estimation for smooth\ncontinuity. Evaluation on Switchboard using intrinsic metrics - global gap\nstatistics, correlations between consecutive gaps, copula-based higher-order\ndependencies, turn-taking entropy, and gap survival functions - shows that\nspeaker-aware simulation better aligns with real conversational patterns than\nthe baseline method, capturing fine-grained temporal dependencies and realistic\nspeaker alternation, while revealing open challenges in modeling long-range\nconversational structure."}
{"id": "2509.15948", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15948", "abs": "https://arxiv.org/abs/2509.15948", "authors": ["Sungho Lee", "Marco Martínez-Ramírez", "Wei-Hsiang Liao", "Stefan Uhlich", "Giorgio Fabbro", "Kyogu Lee", "Yuki Mitsufuji"], "title": "Reverse Engineering of Music Mixing Graphs with Differentiable Processors and Iterative Pruning", "comment": "JAES, extension of arxiv.org/abs/2408.03204 and\n  arxiv.org/abs/2406.01049", "summary": "Reverse engineering of music mixes aims to uncover how dry source signals are\nprocessed and combined to produce a final mix. We extend the prior works to\nreflect the compositional nature of mixing and search for a graph of audio\nprocessors. First, we construct a mixing console, applying all available\nprocessors to every track and subgroup. With differentiable processor\nimplementations, we optimize their parameters with gradient descent. Then, we\nrepeat the process of removing negligible processors and fine-tuning the\nremaining ones. This way, the quality of the full mixing console can be\npreserved while removing approximately two-thirds of the processors. The\nproposed method can be used not only to analyze individual music mixes but also\nto collect large-scale graph data that can be used for downstream tasks, e.g.,\nautomatic mixing. Especially for the latter purpose, efficient implementation\nof the search is crucial. To this end, we present an efficient batch-processing\nmethod that computes multiple processors in parallel. We also exploit the\n\"dry/wet\" parameter of the processors to accelerate the search. Extensive\nquantitative and qualitative analyses are conducted to evaluate the proposed\nmethod's performance, behavior, and computational cost."}
{"id": "2509.15626", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15626", "abs": "https://arxiv.org/abs/2509.15626", "authors": ["Junki Ohmura", "Yuki Ito", "Emiru Tsunoo", "Toshiyuki Sekiya", "Toshiyuki Kumakura"], "title": "LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice Impression Control", "comment": "Submitted to ICASSP 2026", "summary": "Fine-grained control over voice impressions (e.g., making a voice brighter or\ncalmer) is a key frontier for creating more controllable text-to-speech.\nHowever, this nascent field faces two key challenges. The first is the problem\nof impression leakage, where the synthesized voice is undesirably influenced by\nthe speaker's reference audio, rather than the separately specified target\nimpression, and the second is the lack of a public, annotated corpus. To\nmitigate impression leakage, we propose two methods: 1) a training strategy\nthat separately uses an utterance for speaker identity and another utterance of\nthe same speaker for target impression, and 2) a novel reference-free model\nthat generates a speaker embedding solely from the target impression, achieving\nthe benefits of improved robustness against the leakage and the convenience of\nreference-free generation. Objective and subjective evaluations demonstrate a\nsignificant improvement in controllability. Our best method reduced the mean\nsquared error of 11-dimensional voice impression vectors from 0.61 to 0.41\nobjectively and from 1.15 to 0.92 subjectively, while maintaining high\nfidelity. To foster reproducible research, we introduce LibriTTS-VI, the first\npublic voice impression dataset released with clear annotation standards, built\nupon the LibriTTS-R corpus."}
{"id": "2509.15922", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15922", "abs": "https://arxiv.org/abs/2509.15922", "authors": ["Dohwan Kim", "Jung-Woo Choi"], "title": "DISPATCH: Distilling Selective Patches for Speech Enhancement", "comment": "submitted to ICASSP 2026", "summary": "In speech enhancement, knowledge distillation (KD) compresses models by\ntransferring a high-capacity teacher's knowledge to a compact student. However,\nconventional KD methods train the student to mimic the teacher's output\nentirely, which forces the student to imitate the regions where the teacher\nperforms poorly and to apply distillation to the regions where the student\nalready performs well, which yields only marginal gains. We propose Distilling\nSelective Patches (DISPatch), a KD framework for speech enhancement that\napplies the distillation loss to spectrogram patches where the teacher\noutperforms the student, as determined by a Knowledge Gap Score. This approach\nguides optimization toward areas with the most significant potential for\nstudent improvement while minimizing the influence of regions where the teacher\nmay provide unreliable instruction. Furthermore, we introduce Multi-Scale\nSelective Patches (MSSP), a frequency-dependent method that uses different\npatch sizes across low- and high-frequency bands to account for spectral\nheterogeneity. We incorporate DISPatch into conventional KD methods and observe\nconsistent gains in compact students. Moreover, integrating DISPatch and MSSP\ninto a state-of-the-art frequency-dependent KD method considerably improves\nperformance across all metrics."}
{"id": "2509.15629", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15629", "abs": "https://arxiv.org/abs/2509.15629", "authors": ["Lester Phillip Violeta", "Xueyao Zhang", "Jiatong Shi", "Yusuke Yasuda", "Wen-Chin Huang", "Zhizheng Wu", "Tomoki Toda"], "title": "The Singing Voice Conversion Challenge 2025: From Singer Identity Conversion To Singing Style Conversion", "comment": null, "summary": "We present the findings of the latest iteration of the Singing Voice\nConversion Challenge, a scientific event aiming to compare and understand\ndifferent voice conversion systems in a controlled environment. Compared to\nprevious iterations which solely focused on converting the singer identity,\nthis year we also focused on converting the singing style of the singer. To\ncreate a controlled environment and thorough evaluations, we developed a new\nchallenge database, introduced two tasks, open-sourced baselines, and conducted\nlarge-scale crowd-sourced listening tests and objective evaluations. The\nchallenge was ran for two months and in total we evaluated 26 different\nsystems. The results of the large-scale crowd-sourced listening test showed\nthat top systems had comparable singer identity scores to ground truth samples.\nHowever, modeling the singing style and consequently achieving high naturalness\nstill remains a challenge in this task, primarily due to the difficulty in\nmodeling dynamic information in breathy, glissando, and vibrato singing styles."}
{"id": "2509.15946", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15946", "abs": "https://arxiv.org/abs/2509.15946", "authors": ["Sungho Lee", "Matteo Scerbo", "Seungu Han", "Min Jun Choi", "Kyogu Lee", "Enzo De Sena"], "title": "Differentiable Acoustic Radiance Transfer", "comment": null, "summary": "Geometric acoustics is an efficient approach to room acoustics modeling,\ngoverned by the canonical time-dependent rendering equation. Acoustic radiance\ntransfer (ART) solves the equation through discretization, modeling the time-\nand direction-dependent energy exchange between surface patches given with\nflexible material properties. We introduce DART, a differentiable and efficient\nimplementation of ART that enables gradient-based optimization of material\nproperties. We evaluate DART on a simpler variant of the acoustic field\nlearning task, which aims to predict the energy responses of novel\nsource-receiver settings. Experimental results show that DART exhibits\nfavorable properties, e.g., better generalization under a sparse measurement\nscenario, compared to existing signal processing and neural network baselines,\nwhile remaining a simple, fully interpretable system."}
{"id": "2509.15654", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15654", "abs": "https://arxiv.org/abs/2509.15654", "authors": ["Pengcheng Li", "Botao Zhao", "Zuheng Kang", "Junqing Peng", "Xiaoyang Qu", "Yayun He", "Jianzong Wang"], "title": "EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model for Generalized Speech Emotion Recognition", "comment": "Accpeted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)", "summary": "Although Large Audio-Language Models (LALMs) have exhibited outstanding\nperformance in auditory understanding, their performance in affective computing\nscenarios, particularly in emotion recognition, reasoning, and subtle sentiment\ndifferentiation, remains suboptimal. Recent advances in Reinforcement Learning\n(RL) have shown promise in improving LALMs' reasoning abilities. However, two\ncritical challenges hinder the direct application of RL techniques to Speech\nEmotion Recognition (SER) tasks: (1) convergence instability caused by\nambiguous emotional boundaries and (2) limited reasoning ability when using\nrelatively small models (e.g., 7B-parameter architectures). To overcome these\nlimitations, we introduce EMO-RL, a novel framework incorporating reinforcement\nlearning with two key innovations: Emotion Similarity-Weighted Reward (ESWR)\nand Explicit Structured Reasoning (ESR). Built upon pretrained LALMs, our\nmethod employs group-relative policy optimization with emotion constraints.\nComprehensive experiments demonstrate that our EMO-RL training strategies can\nsignificantly enhance the emotional reasoning capabilities of LALMs, attaining\nstate-of-the-art results on both the MELD and IEMOCAP datasets, and\ncross-dataset experiments prove the strong superiority of generalization."}
{"id": "2509.15948", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15948", "abs": "https://arxiv.org/abs/2509.15948", "authors": ["Sungho Lee", "Marco Martínez-Ramírez", "Wei-Hsiang Liao", "Stefan Uhlich", "Giorgio Fabbro", "Kyogu Lee", "Yuki Mitsufuji"], "title": "Reverse Engineering of Music Mixing Graphs with Differentiable Processors and Iterative Pruning", "comment": "JAES, extension of arxiv.org/abs/2408.03204 and\n  arxiv.org/abs/2406.01049", "summary": "Reverse engineering of music mixes aims to uncover how dry source signals are\nprocessed and combined to produce a final mix. We extend the prior works to\nreflect the compositional nature of mixing and search for a graph of audio\nprocessors. First, we construct a mixing console, applying all available\nprocessors to every track and subgroup. With differentiable processor\nimplementations, we optimize their parameters with gradient descent. Then, we\nrepeat the process of removing negligible processors and fine-tuning the\nremaining ones. This way, the quality of the full mixing console can be\npreserved while removing approximately two-thirds of the processors. The\nproposed method can be used not only to analyze individual music mixes but also\nto collect large-scale graph data that can be used for downstream tasks, e.g.,\nautomatic mixing. Especially for the latter purpose, efficient implementation\nof the search is crucial. To this end, we present an efficient batch-processing\nmethod that computes multiple processors in parallel. We also exploit the\n\"dry/wet\" parameter of the processors to accelerate the search. Extensive\nquantitative and qualitative analyses are conducted to evaluate the proposed\nmethod's performance, behavior, and computational cost."}
{"id": "2509.15661", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15661", "abs": "https://arxiv.org/abs/2509.15661", "authors": ["Qiaolin Wang", "Xilin Jiang", "Linyang He", "Junkai Wu", "Nima Mesgarani"], "title": "SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models", "comment": null, "summary": "While large audio-language models (LALMs) have demonstrated state-of-the-art\naudio understanding, their reasoning capability in complex soundscapes still\nfalls behind large vision-language models (LVLMs). Compared to the visual\ndomain, one bottleneck is the lack of large-scale chain-of-thought audio data\nto teach LALM stepwise reasoning. To circumvent this data and modality gap, we\npresent SightSound-R1, a cross-modal distillation framework that transfers\nadvanced reasoning from a stronger LVLM teacher to a weaker LALM student on the\nsame audio-visual question answering (AVQA) dataset. SightSound-R1 consists of\nthree core steps: (i) test-time scaling to generate audio-focused chains of\nthought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter\nhallucinations, and (iii) a distillation pipeline with supervised fine-tuning\n(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM\nstudent. Results show that SightSound-R1 improves LALM reasoning performance\nboth in the in-domain AVQA test set as well as in unseen auditory scenes and\nquestions, outperforming both pretrained and label-only distilled baselines.\nThus, we conclude that vision reasoning can be effectively transferred to audio\nmodels and scaled with abundant audio-visual data."}
{"id": "2509.15952", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15952", "abs": "https://arxiv.org/abs/2509.15952", "authors": ["Gang Yang", "Yue Lei", "Wenxin Tai", "Jin Wu", "Jia Chen", "Ting Zhong", "Fan Zhou"], "title": "Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement", "comment": "5 pages, 2 figures, submitted to ICASSP 2026", "summary": "Diffusion and flow matching (FM) models have achieved remarkable progress in\nspeech enhancement (SE), yet their dependence on multi-step generation is\ncomputationally expensive and vulnerable to discretization errors. Recent\nadvances in one-step generative modeling, particularly MeanFlow, provide a\npromising alternative by reformulating dynamics through average velocity\nfields. In this work, we present COSE, a one-step FM framework tailored for SE.\nTo address the high training overhead of Jacobian-vector product (JVP)\ncomputations in MeanFlow, we introduce a velocity composition identity to\ncompute average velocity efficiently, eliminating expensive computation while\npreserving theoretical consistency and achieving competitive enhancement\nquality. Extensive experiments on standard benchmarks show that COSE delivers\nup to 5x faster sampling and reduces training cost by 40%, all without\ncompromising speech quality. Code is available at\nhttps://github.com/ICDM-UESTC/COSE."}
{"id": "2509.15666", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15666", "abs": "https://arxiv.org/abs/2509.15666", "authors": ["Yongsheng Feng", "Yuetonghui Xu", "Jiehui Luo", "Hongjia Liu", "Xiaobing Li", "Feng Yu", "Wei Li"], "title": "TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation", "comment": "submitted to ICASSP 2026", "summary": "Source separation is a fundamental task in speech, music, and audio\nprocessing, and it also provides cleaner and larger data for training\ngenerative models. However, improving separation performance in practice often\ndepends on increasingly large networks, inflating training and deployment\ncosts. Motivated by recent advances in inference-time scaling for generative\nmodeling, we propose Training-Time and Inference-Time Scalable Discriminative\nSource Separation (TISDiSS), a unified framework that integrates early-split\nmulti-loss supervision, shared-parameter design, and dynamic inference\nrepetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting\ninference depth without retraining additional models. We further provide\nsystematic analyses of architectural and training choices and show that\ntraining with more inference repetitions improves shallow-inference\nperformance, benefiting low-latency applications. Experiments on standard\nspeech separation benchmarks demonstrate state-of-the-art performance with a\nreduced parameter count, establishing TISDiSS as a scalable and practical\nframework for adaptive source separation."}
{"id": "2509.16010", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16010", "abs": "https://arxiv.org/abs/2509.16010", "authors": ["Qi Wang", "Shituo Ma", "Guoxin Yu", "Hanyang Peng", "Yue Yu"], "title": "Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation", "comment": null, "summary": "Voice cloning for Text-to-Speech (TTS) aims to generate expressive and\npersonalized speech from text using limited data from a target speaker.\nFederated Learning (FL) offers a collaborative and privacy-preserving framework\nfor this task, but existing approaches suffer from high communication costs and\ntend to suppress stylistic heterogeneity, resulting in insufficient\npersonalization. To address these issues, we propose Fed-PISA, which stands for\nFederated Personalized Identity-Style Adaptation. To minimize communication\ncosts, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:\nthe speaker's timbre is retained locally through a private ID-LoRA, while only\na lightweight style-LoRA is transmitted to the server, thereby minimizing\nparameter exchange. To harness heterogeneity, our aggregation method, inspired\nby collaborative filtering, is introduced to create custom models for each\nclient by learning from stylistically similar peers. Experiments show that\nFed-PISA improves style expressivity, naturalness, and speaker similarity,\noutperforming standard federated baselines with minimal communication costs."}
{"id": "2509.15680", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15680", "abs": "https://arxiv.org/abs/2509.15680", "authors": ["Taehan Lee", "Jaehan Jung", "Hyukjun Lee"], "title": "Mamba-2 audio captioning: design space exploration and analysis", "comment": "Submitted to the 2026 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2026). Under review", "summary": "We present an audio captioning model built on the Mamba-2 large language\nmodel backbone, which is a state-of-the-art (SOTA) state-space model (SSM). We\nsystematically explore the design space: LLM sizes, LoRA ranks, and connector\ndesigns leveraging Mamba-2's linear-time complexity with respect to sequence\nlength. Across benchmarks, our models achieve strong captioning performance\ncompared with larger language models trained on the same dataset, despite using\nfewer parameters. For the first time, we conduct an in-depth analysis of how\nthe number of LLM parameters, audio encoder fine-tuning strategies, audio\nfeature diversity, and different feature reduction or expansion techniques\naffect performance."}
{"id": "2509.16195", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16195", "abs": "https://arxiv.org/abs/2509.16195", "authors": ["Luca Della Libera", "Cem Subakan", "Mirco Ravanelli"], "title": "FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation", "comment": "5 pages, 1 figure", "summary": "Neural audio codecs are a fundamental component of modern generative audio\npipelines. Although recent codecs achieve strong low-bitrate reconstruction and\nprovide powerful representations for downstream tasks, most are non-streamable,\nlimiting their use in real-time applications. We present FocalCodec-Stream, a\nhybrid codec based on focal modulation that compresses speech into a single\nbinary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our\napproach combines multi-stage causal distillation of WavLM with targeted\narchitectural improvements, including a lightweight refiner module that\nenhances quality under latency constraints. Experiments show that\nFocalCodec-Stream outperforms existing streamable codecs at comparable\nbitrates, while preserving both semantic and acoustic information. The result\nis a favorable trade-off between reconstruction quality, downstream task\nperformance, latency, and efficiency. Code and checkpoints will be released at\nhttps://github.com/lucadellalib/focalcodec."}
{"id": "2509.15692", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15692", "abs": "https://arxiv.org/abs/2509.15692", "authors": ["Pei Zhang", "Yiming Wang", "Jialong Tang", "Baosong Yang", "Rui Wang", "Derek F. Wong", "Fei Huang"], "title": "Direct Simultaneous Translation Activation for Large Audio-Language Models", "comment": null, "summary": "Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech\ninto target text in real time, outputting translations while receiving source\nspeech input, rather than waiting for the entire utterance to be spoken.\nSimul-S2TT research often modifies model architectures to implement read-write\nstrategies. However, with the rise of large audio-language models (LALMs), a\nkey challenge is how to directly activate Simul-S2TT capabilities in base\nmodels without additional architectural changes. In this paper, we introduce\n{\\bf Simul}taneous {\\bf S}elf-{\\bf A}ugmentation ({\\bf SimulSA}), a strategy\nthat utilizes LALMs' inherent capabilities to obtain simultaneous data by\nrandomly truncating speech and constructing partially aligned translation. By\nincorporating them into offline SFT data, SimulSA effectively bridges the\ndistribution gap between offline translation during pretraining and\nsimultaneous translation during inference. Experimental results demonstrate\nthat augmenting only about {\\bf 1\\%} of the simultaneous data, compared to the\nfull offline SFT data, can significantly activate LALMs' Simul-S2TT\ncapabilities without modifications to model architecture or decoding strategy."}
{"id": "2509.15261", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15261", "abs": "https://arxiv.org/abs/2509.15261", "authors": ["Xiaoyang Liu", "Yuma Kinoshita"], "title": "Pre-training Autoencoder for Acoustic Event Classification via Blinky", "comment": "Accepted to APSIPA ASC 2025. 6 pages, 1 figures", "summary": "In the acoustic event classification (AEC) framework that employs Blinkies,\naudio signals are converted into LED light emissions and subsequently captured\nby a single video camera. However, the 30 fps optical transmission channel\nconveys only about 0.2% of the normal audio bandwidth and is highly susceptible\nto noise. We propose a novel sound-to-light conversion method that leverages\nthe encoder of a pre-trained autoencoder (AE) to distill compact,\ndiscriminative features from the recorded audio. To pre-train the AE, we adopt\na noise-robust learning strategy in which artificial noise is injected into the\nencoder's latent representations during training, thereby enhancing the model's\nrobustness against channel noise. The encoder architecture is specifically\ndesigned for the memory footprint of contemporary edge devices such as the\nRaspberry Pi 4. In a simulation experiment on the ESC-50 dataset under a\nstringent 15 Hz bandwidth constraint, the proposed method achieved higher\nmacro-F1 scores than conventional sound-to-light conversion approaches."}
{"id": "2509.15703", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15703", "abs": "https://arxiv.org/abs/2509.15703", "authors": ["Yizhou Zhang", "Yuan Gao", "Wangjin Zhou", "Zicheng Yuan", "Keisuke Imoto", "Tatsuya Kawahara"], "title": "SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation", "comment": "Submitted to ICASSP 2026", "summary": "Self-supervised learning (SSL) on large-scale datasets like AudioSet has\nbecome the dominant paradigm for audio representation learning. While the\ncontinuous influx of new, unlabeled audio presents an opportunity to enrich\nthese static representations, a naive approach is to retrain the model from\nscratch using all available data. However, this method is computationally\nprohibitive and discards the valuable knowledge embedded in the previously\ntrained model weights. To address this inefficiency, we propose SONAR\n(Self-distilled cONtinual pre-training for domain adaptive Audio\nRepresentation), a continual pre-training framework built upon BEATs. SONAR\neffectively adapts to new domains while mitigating catastrophic forgetting by\ntackling three key challenges: implementing a joint sampling strategy for new\nand prior data, applying regularization to balance specificity and generality,\nand dynamically expanding the tokenizer codebook for novel acoustic patterns.\nExperiments across four distinct domains demonstrate that our method achieves\nboth high adaptability and robust resistance to forgetting."}
{"id": "2509.15473", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15473", "abs": "https://arxiv.org/abs/2509.15473", "authors": ["Yuyu Wang", "Wuyue Xia", "Huaxiu Yao", "Jingping Nie"], "title": "Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech", "comment": "6 pages, 3rd ACM International Workshop on Intelligent Acoustic\n  Systems and Applications (IASA 25)", "summary": "Post-exercise speech contains rich physiological and linguistic cues, often\nmarked by semantic pauses, breathing pauses, and combined breathing-semantic\npauses. Detecting these events enables assessment of recovery rate, lung\nfunction, and exertion-related abnormalities. However, existing works on\nidentifying and distinguishing different types of pauses in this context are\nlimited. In this work, building on a recently released dataset with\nsynchronized audio and respiration signals, we provide systematic annotations\nof pause types. Using these annotations, we systematically conduct exploratory\nbreathing and semantic pause detection and exertion-level classification across\ndeep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features\n(MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three\nsetups-single feature, feature fusion, and a two-stage detection-classification\ncascade-under both classification and regression formulations. Results show\nper-type detection accuracy up to 89$\\%$ for semantic, 55$\\%$ for breathing,\n86$\\%$ for combined pauses, and 73$\\%$overall, while exertion-level\nclassification achieves 90.5$\\%$ accuracy, outperformin prior work."}
{"id": "2509.15808", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15808", "abs": "https://arxiv.org/abs/2509.15808", "authors": ["Máté Gedeon", "Péter Mihajlik"], "title": "From Independence to Interaction: Speaker-Aware Simulation of Multi-Speaker Conversational Timing", "comment": "Submitted to ICASSP 2026", "summary": "We present a speaker-aware approach for simulating multi-speaker\nconversations that captures temporal consistency and realistic turn-taking\ndynamics. Prior work typically models aggregate conversational statistics under\nan independence assumption across speakers and turns. In contrast, our method\nuses speaker-specific deviation distributions enforcing intra-speaker temporal\nconsistency, while a Markov chain governs turn-taking and a fixed room impulse\nresponse preserves spatial realism. We also unify pauses and overlaps into a\nsingle gap distribution, modeled with kernel density estimation for smooth\ncontinuity. Evaluation on Switchboard using intrinsic metrics - global gap\nstatistics, correlations between consecutive gaps, copula-based higher-order\ndependencies, turn-taking entropy, and gap survival functions - shows that\nspeaker-aware simulation better aligns with real conversational patterns than\nthe baseline method, capturing fine-grained temporal dependencies and realistic\nspeaker alternation, while revealing open challenges in modeling long-range\nconversational structure."}
{"id": "2509.15516", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15516", "abs": "https://arxiv.org/abs/2509.15516", "authors": ["Dhruuv Agarwal", "Harry Zhang", "Yang Yu", "Quan Wang"], "title": "State-of-the-Art Dysarthric Speech Recognition with MetaICL for on-the-fly Personalization", "comment": null, "summary": "Personalizing Automatic Speech Recognition (ASR) for dysarthric speech is\ncrucial but challenging due to training and storing of individual user\nadapters. We propose a hybrid meta-training method for a single model,\nexcelling in zero-shot and few-shot on-the-fly personalization via in-context\nlearning (ICL). Measuring Word Error Rate (WER) on state-of-the-art subsets,\nthe model achieves 13.9% WER on Euphonia which surpasses speaker-independent\nbaselines (17.5% WER) and rivals user-specific personalized models. On SAP Test\n1, its 5.3% WER significantly bests the 8% from even personalized adapters. We\nalso demonstrate the importance of example curation, where an oracle\ntext-similarity method shows 5 curated examples can achieve performance similar\nto 19 randomly selected ones, highlighting a key area for future efficiency\ngains. Finally, we conduct data ablations to measure the data efficiency of\nthis approach. This work presents a practical, scalable, and personalized\nsolution."}
{"id": "2509.15946", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15946", "abs": "https://arxiv.org/abs/2509.15946", "authors": ["Sungho Lee", "Matteo Scerbo", "Seungu Han", "Min Jun Choi", "Kyogu Lee", "Enzo De Sena"], "title": "Differentiable Acoustic Radiance Transfer", "comment": null, "summary": "Geometric acoustics is an efficient approach to room acoustics modeling,\ngoverned by the canonical time-dependent rendering equation. Acoustic radiance\ntransfer (ART) solves the equation through discretization, modeling the time-\nand direction-dependent energy exchange between surface patches given with\nflexible material properties. We introduce DART, a differentiable and efficient\nimplementation of ART that enables gradient-based optimization of material\nproperties. We evaluate DART on a simpler variant of the acoustic field\nlearning task, which aims to predict the energy responses of novel\nsource-receiver settings. Experimental results show that DART exhibits\nfavorable properties, e.g., better generalization under a sparse measurement\nscenario, compared to existing signal processing and neural network baselines,\nwhile remaining a simple, fully interpretable system."}
{"id": "2509.15523", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15523", "abs": "https://arxiv.org/abs/2509.15523", "authors": ["Xinyi Chen", "Xi Chen", "Zhenyu Weng", "Yang Xiao"], "title": "AFT: An Exemplar-Free Class Incremental Learning Method for Environmental Sound Classification", "comment": "Submitted to ICASSP 2026", "summary": "As sounds carry rich information, environmental sound classification (ESC) is\ncrucial for numerous applications such as rare wild animals detection. However,\nour world constantly changes, asking ESC models to adapt to new sounds\nperiodically. The major challenge here is catastrophic forgetting, where models\nlose the ability to recognize old sounds when learning new ones. Many methods\naddress this using replay-based continual learning. This could be impractical\nin scenarios such as data privacy concerns. Exemplar-free methods are commonly\nused but can distort old features, leading to worse performance. To overcome\nsuch limitations, we propose an Acoustic Feature Transformation (AFT) technique\nthat aligns the temporal features of old classes to the new space, including a\nselectively compressed feature space. AFT mitigates the forgetting of old\nknowledge without retaining past data. We conducted experiments on two\ndatasets, showing consistent improvements over baseline models with accuracy\ngains of 3.7\\% to 3.9\\%."}
{"id": "2509.15948", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.15948", "abs": "https://arxiv.org/abs/2509.15948", "authors": ["Sungho Lee", "Marco Martínez-Ramírez", "Wei-Hsiang Liao", "Stefan Uhlich", "Giorgio Fabbro", "Kyogu Lee", "Yuki Mitsufuji"], "title": "Reverse Engineering of Music Mixing Graphs with Differentiable Processors and Iterative Pruning", "comment": "JAES, extension of arxiv.org/abs/2408.03204 and\n  arxiv.org/abs/2406.01049", "summary": "Reverse engineering of music mixes aims to uncover how dry source signals are\nprocessed and combined to produce a final mix. We extend the prior works to\nreflect the compositional nature of mixing and search for a graph of audio\nprocessors. First, we construct a mixing console, applying all available\nprocessors to every track and subgroup. With differentiable processor\nimplementations, we optimize their parameters with gradient descent. Then, we\nrepeat the process of removing negligible processors and fine-tuning the\nremaining ones. This way, the quality of the full mixing console can be\npreserved while removing approximately two-thirds of the processors. The\nproposed method can be used not only to analyze individual music mixes but also\nto collect large-scale graph data that can be used for downstream tasks, e.g.,\nautomatic mixing. Especially for the latter purpose, efficient implementation\nof the search is crucial. To this end, we present an efficient batch-processing\nmethod that computes multiple processors in parallel. We also exploit the\n\"dry/wet\" parameter of the processors to accelerate the search. Extensive\nquantitative and qualitative analyses are conducted to evaluate the proposed\nmethod's performance, behavior, and computational cost."}
{"id": "2509.15599", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15599", "abs": "https://arxiv.org/abs/2509.15599", "authors": ["Jun-Wei Yeow", "Ee-Leng Tan", "Santi Peksi", "Woon-Seng Gan"], "title": "MAGENTA: Magnitude and Geometry-ENhanced Training Approach for Robust Long-Tailed Sound Event Localization and Detection", "comment": "This work has been submitted to IEEE ICASSP 2026 for possible\n  publication", "summary": "Deep learning-based Sound Event Localization and Detection (SELD) systems\ndegrade significantly on real-world, long-tailed datasets. Standard regression\nlosses bias learning toward frequent classes, causing rare events to be\nsystematically under-recognized. To address this challenge, we introduce\nMAGENTA (Magnitude And Geometry-ENhanced Training Approach), a unified loss\nfunction that counteracts this bias within a physically interpretable vector\nspace. MAGENTA geometrically decomposes the regression error into radial and\nangular components, enabling targeted, rarity-aware penalties and strengthened\ndirectional modeling. Empirically, MAGENTA substantially improves SELD\nperformance on imbalanced real-world data, providing a principled foundation\nfor a new class of geometry-aware SELD objectives. Code is available at:\nhttps://github.com/itsjunwei/MAGENTA_ICASSP"}
{"id": "2509.16010", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16010", "abs": "https://arxiv.org/abs/2509.16010", "authors": ["Qi Wang", "Shituo Ma", "Guoxin Yu", "Hanyang Peng", "Yue Yu"], "title": "Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation", "comment": null, "summary": "Voice cloning for Text-to-Speech (TTS) aims to generate expressive and\npersonalized speech from text using limited data from a target speaker.\nFederated Learning (FL) offers a collaborative and privacy-preserving framework\nfor this task, but existing approaches suffer from high communication costs and\ntend to suppress stylistic heterogeneity, resulting in insufficient\npersonalization. To address these issues, we propose Fed-PISA, which stands for\nFederated Personalized Identity-Style Adaptation. To minimize communication\ncosts, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:\nthe speaker's timbre is retained locally through a private ID-LoRA, while only\na lightweight style-LoRA is transmitted to the server, thereby minimizing\nparameter exchange. To harness heterogeneity, our aggregation method, inspired\nby collaborative filtering, is introduced to create custom models for each\nclient by learning from stylistically similar peers. Experiments show that\nFed-PISA improves style expressivity, naturalness, and speaker similarity,\noutperforming standard federated baselines with minimal communication costs."}
{"id": "2509.15603", "categories": ["eess.SP", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15603", "abs": "https://arxiv.org/abs/2509.15603", "authors": ["Sven Hinderer"], "title": "Blind Source Separation of Radar Signals in Time Domain Using Deep Learning", "comment": null, "summary": "Identification and further analysis of radar emitters in a contested\nenvironment requires detection and separation of incoming signals. If they\narrive from the same direction and at similar frequencies, deinterleaving them\nremains challenging. A solution to overcome this limitation becomes\nincreasingly important with the advancement of emitter capabilities. We propose\ntreating the problem as blind source separation in time domain and apply\nsupervisedly trained neural networks to extract the underlying signals from the\nreceived mixture. This allows us to handle highly overlapping and also\ncontinuous wave (CW) signals from both radar and communication emitters. We\nmake use of advancements in the field of audio source separation and extend a\ncurrent state-of-the-art model with the objective of deinterleaving arbitrary\nradio frequency (RF) signals. Results show, that our approach is capable of\nseparating two unknown waveforms in a given frequency band with a single\nchannel receiver."}
{"id": "2509.15702", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15702", "abs": "https://arxiv.org/abs/2509.15702", "authors": ["Kaspar Müller", "Markus Buck", "Simon Doclo", "Jan Østergaard", "Tobias Wolff"], "title": "A Steered Response Power Method for Sound Source Localization With Generic Acoustic Models", "comment": "Accepted for publication in IEEE Transactions on Audio, Speech and\n  Language Processing", "summary": "The steered response power (SRP) method is one of the most popular approaches\nfor acoustic source localization with microphone arrays. It is often based on\nsimplifying acoustic assumptions, such as an omnidirectional sound source in\nthe far field of the microphone array(s), free field propagation, and spatially\nuncorrelated noise. In reality, however, there are many acoustic scenarios\nwhere such assumptions are violated. This paper proposes a generalization of\nthe conventional SRP method that allows to apply generic acoustic models for\nlocalization with arbitrary microphone constellations. These models may\nconsider, for instance, level differences in distributed microphones, the\ndirectivity of sources and receivers, or acoustic shadowing effects. Moreover,\nalso measured acoustic transfer functions may be applied as acoustic model. We\nshow that the delay-and-sum beamforming of the conventional SRP is not optimal\nfor localization with generic acoustic models. To this end, we propose a\ngeneralized SRP beamforming criterion that considers generic acoustic models\nand spatially correlated noise, and derive an optimal SRP beamformer.\nFurthermore, we propose and analyze appropriate frequency weightings. Unlike\nthe conventional SRP, the proposed method can jointly exploit observed level\nand time differences between the microphone signals to infer the source\nlocation. Realistic simulations of three different microphone setups with\nspeech under various noise conditions indicate that the proposed method can\nsignificantly reduce the mean localization error compared to the conventional\nSRP and, in particular, a reduction of more than 60% can be archived in noisy\nconditions."}
{"id": "2509.15969", "categories": ["eess.AS", "cs.CL", "cs.HC", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.15969", "abs": "https://arxiv.org/abs/2509.15969", "authors": ["Nikita Torgashov", "Gustav Eje Henter", "Gabriel Skantze"], "title": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency", "comment": "5 pages, 1 figure, submitted to IEEE ICASSP 2026", "summary": "We present VoXtream, a fully autoregressive, zero-shot streaming\ntext-to-speech (TTS) system for real-time use that begins speaking from the\nfirst word. VoXtream directly maps incoming phonemes to audio tokens using a\nmonotonic alignment scheme and a dynamic look-ahead that does not delay onset.\nBuilt around an incremental phoneme transformer, a temporal transformer\npredicting semantic and duration tokens, and a depth transformer producing\nacoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay\namong publicly available streaming TTS: 102 ms on GPU. Despite being trained on\na mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several\nmetrics, while delivering competitive quality in both output- and\nfull-streaming settings. Demo and code are available at\nhttps://herimor.github.io/voxtream."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 17]
- [eess.AS](#eess.AS) [Total: 13]
- [cs.SD](#cs.SD) [Total: 26]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [(SP)$^2$-Net: A Neural Spatial Spectrum Method for DOA Estimation](https://arxiv.org/abs/2509.15475)
*Lioz Berman,Sharon Gannot,Tom Tirer*

Main category: eess.SP

TL;DR: 提出了一种名为(SP)^2-Net的深度学习技术，用于从单次快照中生成高分辨率空间谱，以估计多个源的到达方向(DOA)，克服了传统Bartlett波束形成器的孔径限制。


<details>
  <summary>Details</summary>
Motivation: 在单次快照场景下，经典Bartlett波束形成器的精度和分辨率受限于阵列孔径，而最大似然估计在源数量未知或较大时不实用，基于样本协方差的谱方法因缺乏多次快照而不适用。

Method: 设计了一种新颖的深度神经网络架构和训练策略，该网络以测量数据和假设角度作为输入，学习输出与更宽阵列能力一致的得分。推理时通过扫描任意角度集生成热图。

Result: 实验证明，(SP)^2-Net在DOA估计方面优于Bartlett波束形成器和基于稀疏性的DOA估计方法。

Conclusion: 所提出的深度学习技术能够有效提升单次快照下的DOA估计性能，突破了传统方法的孔径限制。

Abstract: We consider the problem of estimating the directions of arrival (DOAs) of
multiple sources from a single snapshot of an antenna array, a task with many
practical applications. In such settings, the classical Bartlett beamformer is
commonly used, as maximum likelihood estimation becomes impractical when the
number of sources is unknown or large, and spectral methods based on the sample
covariance are not applicable due to the lack of multiple snapshots. However,
the accuracy and resolution of the Bartlett beamformer are fundamentally
limited by the array aperture. In this paper, we propose a deep learning
technique, comprising a novel architecture and training strategy, for
generating a high-resolution spatial spectrum from a single snapshot.
Specifically, we train a deep neural network that takes the measurements and a
hypothesis angle as input and learns to output a score consistent with the
capabilities of a much wider array. At inference time, a heatmap can be
produced by scanning an arbitrary set of angles. We demonstrate the advantages
of our trained model, named (SP)$^2$-Net, over the Bartlett beamformer and
sparsity-based DOA estimation methods.

</details>


### [2] [CSIT-Free Downlink Transmission for mmWave MU-MISO Systems in High-Mobility Scenario](https://arxiv.org/abs/2509.15564)
*Jeongjae Lee,Wonseok Choi,Songnam Hong*

Main category: eess.SP

TL;DR: 本文提出了一种毫米波多用户MISO系统的下行传输框架，无需信道状态信息(CSIT)，通过专门设计的CSIT-free酉预编码和符号检测方法，在高移动性场景下实现干扰消除和全合并增益。


<details>
  <summary>Details</summary>
Motivation: 针对毫米波多用户MISO系统在高速移动场景下的挑战，传统CSIT获取过程开销大，无法在极短的信道相干时间内完成下行传输，需要开发无需CSIT的传输方案。

Method: 提出新型下行传输框架，利用毫米波信道特性和专门设计的CSIT-free酉预编码，结合符号检测方法以及同时进行的CSIR和多普勒频移估计，实现干扰完全消除。

Result: 通过仿真验证，所提方法相比现有基线方案具有显著优势，能够有效应对高速移动场景下的传输挑战。

Conclusion: 该CSIT-free传输框架为毫米波高速移动通信提供了一种有效的解决方案，能够在极短信道相干时间内实现可靠的下行传输。

Abstract: This paper investigates the downlink (DL) transmission in millimeter-wave
(mmWave) multi-user multiple-input single-output (MU-MISO) systems especially
focusing on a high speed mobile scenario. To complete the DL transmission
within an extremely short channel coherence time, we propose a novel DL
transmission framework that eliminates the need for channel state information
at the transmitter (CSIT), of which acquisition process requires a substantial
overhead, instead fully exploiting the given channel coherence time. Harnessing
the characteristic of mmWave channel and uniquely designed CSIT-free unitary
precoding, we propose a symbol detection method along with the simultaneous CSI
at the receiver (CSIR) and Doppler shift estimation method to completely cancel
the interferences while achieving a full combining gain. Via simulations, we
demonstrate the effectiveness of the proposed method comparing with the
existing baselines.

</details>


### [3] [Twisting Signals for Joint Radar-Communications: An OAM Vortex Beam Approach](https://arxiv.org/abs/2509.15601)
*Wanghan Lv,Kumar Vijay Mishra,Jinsong Hu*

Main category: eess.SP

TL;DR: 提出了一种基于轨道角动量(OAM)的毫米波联合雷达通信系统，结合双基地汽车雷达和车对车通信，通过空间调制和模式分割复用技术实现雷达参数估计和通信功能。


<details>
  <summary>Details</summary>
Motivation: 利用OAM技术的螺旋相位波前和自然正交性，在无线通信和雷达系统中增强频谱效率和空间目标识别能力，实现雷达与通信的联合系统。

Method: 采用均匀线性阵列和行波天线产生多个拉盖尔-高斯涡旋光束，构建双基地汽车OAM-JRC模型，设计目标位置和速度参数估计算法，并采用基于OAM的模式分割复用策略。

Result: 通过数值实验验证了所提OAM-JRC系统和参数估计方法的有效性，推导了恢复保证和克拉美-罗下界，评估了通信的误码率性能。

Conclusion: 提出的基于OAM的联合雷达通信系统能够有效实现雷达目标参数估计和可靠通信，为汽车雷达和车对车通信提供了新的解决方案。

Abstract: Orbital angular momentum (OAM) technology has attracted much research
interest in recent years because of its characteristic helical phase front
twisting around the propagation axis and natural orthogonality among different
OAM states to encode more degrees of freedom than classical planar beams.
Leveraging upon these features, OAM technique has been applied to wireless
communication systems to enhance spectral efficiency and radar systems to
distinguish spatial targets without beam scanning. Leveraging upon these unique
properties, we propose an OAM-based millimeter-wave joint radar-communications
(JRC) system comprising a bi-static automotive radar and vehicle-to-vehicle
(V2V) communications. Different from existing uniform circular array (UCA)
based OAM systems where each element is an isotropic antenna, an OAM spatial
modulation scheme utilizing a uniform linear array (ULA) is adopted with each
element being a traveling-wave antenna, producing multiple Laguerre-Gaussian
(LG) vortex beams simultaneously. Specifically, we first build a novel
bi-static automotive OAM-JRC model that embeds communication messages in a
radar signal, following which a target position and velocity parameters
estimation algorithm is designed with only radar frames. Then, an OAM-based
mode-division multiplexing (MDM) strategy between radar and JRC frames is
presented to ensure the JRC parameters identifiability and recovery.
Furthermore, we analyze the performance of the JRC system through deriving
recovery guarantees and Cram\'er-Rao lower bound (CRLB) of radar target
parameters and evaluating the bit error rate (BER) of communication,
respectively. Our numerical experiments validate the effectiveness of the
proposed OAM-based JRC system and parameter estimation method.

</details>


### [4] [Blind Source Separation of Radar Signals in Time Domain Using Deep Learning](https://arxiv.org/abs/2509.15603)
*Sven Hinderer*

Main category: eess.SP

TL;DR: 该论文提出使用监督训练的神经网络进行盲源分离，以解决雷达信号在相同方向和频率下的解交织问题，能够处理高度重叠和连续波信号。


<details>
  <summary>Details</summary>
Motivation: 在对抗环境中识别和分析雷达发射器需要检测和分离输入信号，当信号来自相同方向和相似频率时，解交织仍然具有挑战性。随着发射器能力的进步，克服这一限制的解决方案变得越来越重要。

Method: 将问题视为时域盲源分离，应用监督训练的神经网络从接收到的混合信号中提取底层信号。利用音频源分离领域的进展，扩展当前最先进的模型，旨在解交织任意射频信号。

Result: 结果显示，该方法能够在给定频带内使用单通道接收器分离两个未知波形。

Conclusion: 提出的方法能够有效处理雷达和通信发射器的高度重叠和连续波信号，为单通道接收器下的信号解交织提供了可行解决方案。

Abstract: Identification and further analysis of radar emitters in a contested
environment requires detection and separation of incoming signals. If they
arrive from the same direction and at similar frequencies, deinterleaving them
remains challenging. A solution to overcome this limitation becomes
increasingly important with the advancement of emitter capabilities. We propose
treating the problem as blind source separation in time domain and apply
supervisedly trained neural networks to extract the underlying signals from the
received mixture. This allows us to handle highly overlapping and also
continuous wave (CW) signals from both radar and communication emitters. We
make use of advancements in the field of audio source separation and extend a
current state-of-the-art model with the objective of deinterleaving arbitrary
radio frequency (RF) signals. Results show, that our approach is capable of
separating two unknown waveforms in a given frequency band with a single
channel receiver.

</details>


### [5] [Wireless Sensing with Movable Intelligent Surface](https://arxiv.org/abs/2509.15627)
*Ziyuan Zheng,Qingqing Wu,Yanze Zhu,Wen Chen,Ying Gao,Honghao Wang*

Main category: eess.SP

TL;DR: 该论文提出了一种低成本的可移动智能表面（MIS）用于无线感知，通过机械重构替代电子相位调谐，实现多目标检测的波束模式重构。


<details>
  <summary>Details</summary>
Motivation: 未来无线网络需要同时支持千兆通信和泛在感知。可重构智能表面（RISs）虽然能重塑无线电传播，但其逐元素电子调谐存在硬件成本和功耗过高的问题。受流体天线系统（FAS）概念启发，本文旨在开发一种更经济高效的感知解决方案。

Method: MIS采用堆叠式结构，包含大的固定层和小的可移动预相位超表面层，通过位置偏移合成不同的复合相位模式。开发了基于黎曼增广拉格朗日方法（RALM）的算法来联合优化MIS相位偏移和位置调度，同时提出了具有闭式相位分布和位置调度的启发式波束导向设计。

Result: 仿真验证了MIS的波束模式重构能力，显示RALM方案在提升感知信干噪比（SINR）方面显著优于闭式方案，并揭示了波束模式中的增益-多样性权衡关系。

Conclusion: MIS提供了一种低成本、低功耗的无线感知解决方案，通过机械重构实现多波束模式，为多目标检测提供了有效的技术路径，其增益-多样性权衡关系为MIS配置的优化选择提供了指导。

Abstract: Future wireless networks are envisioned to deliver not only gigabit
communications but also ubiquitous sensing. Reconfigurable intelligent surfaces
(RISs) have emerged to reshape radio propagation, recently showing considerable
promise for wireless sensing. Still, their per-element electronic tuning incurs
prohibitive hardware cost and power consumption. Motivated by the concept of
fluid antenna system (FAS), this paper introduces a low-cost movable
intelligent surface (MIS) for wireless sensing, which replaces element-wise
electronic phase tuning with panel-wise mechanical reconfiguration. The MIS
stacks a large fixed and a smaller movable pre-phased metasurface layers, whose
differential position shifts synthesize distinct composite phase patterns,
enabling multiple beam patterns for multi-target detection. We characterize a
MIS-enabled multi-hop echo signal model with multi-target interference and then
formulate a worst-case sensing signal-to-interference-plus-noise ratio (SINR)
maximization problem that jointly designs MIS phase shifts and schedules MS2's
position. A Riemannian Augmented Lagrangian Method (RALM)-based algorithm is
developed to solve the formulated mixed-integer non-convex problem. We also
derive a heuristic MIS beam steering design with closed-form phase distribution
and position scheduling. Simulations validate MIS's beam pattern
reconfiguration capability, show that the RALM-based scheme significantly
outperforms the closed-form scheme in improving sensing SINR, and uncover a
gain-diversity trade-off in beam patterns that informs the optimal choice of
MIS configuration.

</details>


### [6] [Optimizing Sparse Antenna Arrays for Localization and Sensing using Vector Spherical Wave Functions](https://arxiv.org/abs/2509.15636)
*Tobias Lafer,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: 提出了一种基于克拉美-罗下界和矢量球面波函数的稀疏天线阵列优化方法，用于提升移动和物联网设备的定位性能


<details>
  <summary>Details</summary>
Motivation: 随着电子设备越来越多地采用超宽带等宽带无线电技术进行定位和感知，但移动和物联网设备中天线空间有限，需要优化少量天线元件的布局和方向以获得最佳定位性能

Method: 使用矢量球面波函数建立宽带信号模型，考虑频率、方向、极化特性、互耦效应和周围障碍物影响，推导克拉美-罗下界作为定位参数（时延、到达角）的性能指标，通过最小化CRLB来优化天线位置和方向

Result: 通过三个交叉指数锥形缝隙天线的示例布置验证了所提出的优化程序的有效性

Conclusion: 该方法为稀疏天线阵列的优化提供了一种系统化的解决方案，能够显著提升移动和物联网设备的定位性能

Abstract: In increasing number of electronic devices implement wideband radio
technologies for localization and sensing purposes, like ultra-wideband (UWB).
Such radio technologies benefit from a large number of antennas, but space for
antennas is often limited, especially in devices for mobile and IoT
applications. A common challenge is therefore to optimize the placement and
orientations of a small number of antenna elements inside a device, leading to
the best localization performance. We propose a method for systematically
approaching the optimization of such sparse arrays by means of Cram\'er-Rao
lower bounds (CRLBs) and vector spherical wave functions (VSWFs). The VSWFs
form the basis of a wideband signal model considering frequency, direction and
polarization-dependent characteristics of the antenna array under test (AUT),
together with mutual coupling and distortions from surrounding obstacles. We
derive the CRLBs for localization parameters like delay and angle-of-arrival
for this model under additive white Gaussian noise channel conditions, and
formulate optimization problems for determining optimal antenna positions and
orientations via minimization of the CRLBs. The proposed optimization procedure
is demonstrated by means of an exemplary arrangement of three Crossed
Exponentially Tapered Slot (XETS) antennas.

</details>


### [7] [Hybrid Baseband Simulation for Single-Channel Radar-Based Indoor Localization System](https://arxiv.org/abs/2509.15650)
*Sven Hinderer,Zheming Yin,Athanasios Papanikolaou,Jan Hesselbarth,Bin Yang*

Main category: eess.SP

TL;DR: 提出了一种用于单通道雷达室内定位系统的混合雷达基带信号模拟器，结合了射线追踪信道模拟和实际测量数据，实现复杂场景下的真实基带信号建模。


<details>
  <summary>Details</summary>
Motivation: 毫米波啁啾序列雷达在室内定位中能以低成本实现高精度定位，需要开发能够真实模拟基带接收信号的仿真工具来支持系统设计和验证。

Method: 结合射线追踪信道模拟、雷达双向天线增益的实际测量数据以及选定反射器的雷达截面精确模拟，构建混合基带信号模拟器。

Result: 实现了在复杂场景下对基带接收信号的现实建模，为单通道雷达室内定位系统提供了有效的仿真平台。

Conclusion: 所提出的混合模拟器能够有效支持新型单通道雷达室内定位系统的开发和验证，为低成本高精度室内定位技术提供了重要工具。

Abstract: Indoor localization with chirp sequence radar at millimeter wavelength offers
high localization accuracy at low system cost. We propose a hybrid radar
baseband signal simulator for our novel single-channel radar-based indoor
localization system consisting of an active radar and passive reflectors as
references. By combining ray tracing channel simulations with real measurements
of the two-way antenna gain of the radar and accurate simulation of the radar
cross section of chosen reflectors, realistic modeling of the baseband receive
signal in complex scenarios is achieved.

</details>


### [8] [Extended k-u Fading Model in mmWave Communication: Statistical Properties and Performance Evaluations](https://arxiv.org/abs/2509.15681)
*Jiahuan Wu,Xinchun Yu,Xiao-Ping Zhang*

Main category: eess.SP

TL;DR: 提出了一种扩展k-u小尺度衰落模型，通过在原有k-u模型基础上增加新参数来考虑多径簇的不平衡性，在毫米波通信场景中具有更好的性能表现和数学可处理性。


<details>
  <summary>Details</summary>
Motivation: 现有的小尺度衰落模型在毫米波频段对多径簇不平衡性的建模能力有限，需要开发更准确且数学上更易处理的模型来改善毫米波通信系统的性能分析。

Method: 基于原始k-u模型增加新参数构建扩展k-u模型，在28GHz、65GHz和92GHz频段的毫米波通信场景中进行实验验证，并通过理论推导获得关键统计特性的闭式表达式。

Result: 扩展k-u模型在所有测试场景中都比k-u模型和扩展n-u模型具有更小的均方误差，且在存在视距路径的场景中建模能力更准确。

Conclusion: 扩展k-u模型为毫米波通信系统的小尺度衰落提供了更精确的建模工具，其数学可处理性便于系统性能分析，在毫米波通信领域具有重要应用价值。

Abstract: This study proposes a small-scale fading model, named the extended k-u model,
which incorporates the imbalance of multipath clusters by adding a new
parameter based on the original k-u model. The extended k-u model outperforms
the k-u model in characterizing small-scale fading in the millimeter-wave
(mmWave) band and has more accurate modeling capability than the extended n-u
model in scenarios with line-of-sight (LoS) paths. And it is mathematically
more tractable than the a-k-n-u model. Experiments are conducted for mmWave
communication scenarios with LoS paths, covering the outdoor 28 GHz band, the
indoor 65 GHz band, and the indoor 92 GHz band. The results demonstrate that
the extended k-u model achieves a smaller mean square error in fitting the
measured data compared to both the k-u model and the extended n-u model across
all scenarios. In addition, through theoretical derivations, closed-form
expressions are obtained for the key statistical characteristics of the
extended k-u model, including the probability density function, cumulative
distribution function, moments of arbitrary order, and moment generating
function. Based on these statistics, this study further derives and analyzes
the expressions for some performance metrics of the communication system,
including the amount of fading, the probability of outage, and the average bit
error rate.

</details>


### [9] [Distributed Multi-Task Learning for Joint Wireless Signal Enhancement and Recognition](https://arxiv.org/abs/2509.15718)
*Hao Zhang,Fuhui Zhou,Qihui Wu,Chau Yuen*

Main category: eess.SP

TL;DR: 提出了一种新颖的分布式多任务学习框架，用于联合无线信号增强和识别（WSER），解决了现代无线网络中非协作信号识别的关键需求。


<details>
  <summary>Details</summary>
Motivation: 无线信号识别在无线通信网络中至关重要，但在低信噪比条件和分布式网络设置下准确分类信号具有挑战性。

Method: 集成无线信号增强和识别网络（WSERNet）与FedProx+增强型联邦学习算法。WSERNet利用非对称卷积块（ACBlock）捕获输入信号中的长程依赖关系，FedProx+在损失函数中引入近端项以提高收敛速度和鲁棒性。

Result: 大量实验证明该框架在联合WSER方面有效，在集中式和分布式设置下（包括IID和非IID数据分布）均优于最先进方法。

Conclusion: 所提出的分布式多任务学习框架能够有效解决无线信号识别在低信噪比和分布式环境下的挑战，具有优越的性能表现。

Abstract: Wireless signal recognition (WSR) is crucial in modern and future wireless
communication networks since it aims to identify the properties of the received
signal in a no-collaborative manner. However, it is challenging to accurately
classify signals in low signal-to-noise ratio (SNR) conditions and distributed
network settings. In this paper, we propose a novel distributed multi-task
learning framework for joint wireless signal enhancement and recognition
(WSER), addressing the crucial need for non-collaborative signal identification
in modern wireless networks. Our approach integrates a wireless signal
enhancement and recognition network (WSERNet) with FedProx+, an enhanced
federated learning algorithm designed for heterogeneous data distributions.
Specifically, WSERNet leverages an asymmetric convolution block (ACBlock) to
capture long-range dependencies in the input signal and improve the performance
of the deep learning model. FedProx+ introduces a proximal term to the loss
function to encourage the model updates to be closer to the previous model,
enhancing the convergence speed and robustness of federated learning. Extensive
experiments demonstrate the effectiveness of the proposed framework for joint
WSER, achieving superior performance compared to state-of-the-art methods under
both centralized and distributed settings including independent and identically
distributed (IID) and non-IID data distributions.

</details>


### [10] [Explainable Deep Learning Based Adversarial Defense for Automatic Modulation Classification](https://arxiv.org/abs/2509.15766)
*Peihao Dong,Jingchun Wang,Shen Gao,Fuhui Zhou,Qihui Wu*

Main category: eess.SP

TL;DR: 本文提出了一种基于可解释性深度学习的防御方案SHAP-AFT，用于对抗自动调制分类网络中的对抗攻击。该方案通过Shapley值评估攻击影响，并采用对抗性微调来增强网络鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动调制分类神经网络容易受到各种对抗攻击，且由于泛化能力和计算成本的问题难以处理。需要开发一种能够揭示攻击影响的可解释防御方案。

Method: SHAP-AFT方案包含三个阶段：攻击检测、信息重要性评估和对抗性微调。通过引入认知负信息概念，使用Shapley值评估数据贡献度，移除负值对应的攻击影响位置，然后基于精炼数据模式进行对抗性微调。

Result: 仿真结果表明，Shapley值作为关键指标的有效性，以及SHAP-AFT方案在面对不同类型和强度攻击时的优越防御性能。

Conclusion: 所提出的SHAP-AFT方案通过可解释性分析和对抗性微调，成功提升了自动调制分类网络对对抗攻击的防御能力，具有实际应用价值。

Abstract: Deep learning (DL) has been widely applied to enhance automatic modulation
classification (AMC). However, the elaborate AMC neural networks are
susceptible to various adversarial attacks, which are challenging to handle due
to the generalization capability and computational cost. In this article, an
explainable DL based defense scheme, called SHapley Additive exPlanation
enhanced Adversarial Fine-Tuning (SHAP-AFT), is developed in the perspective of
disclosing the attacking impact on the AMC network. By introducing the concept
of cognitive negative information, the motivation of using SHAP for defense is
theoretically analyzed first. The proposed scheme includes three stages, i.e.,
the attack detection, the information importance evaluation, and the AFT. The
first stage indicates the existence of the attack. The second stage evaluates
contributions of the received data and removes those data positions using
negative Shapley values corresponding to the dominating negative information
caused by the attack. Then the AMC network is fine-tuned based on adversarial
adaptation samples using the refined received data pattern. Simulation results
show the effectiveness of the Shapley value as the key indicator as well as the
superior defense performance of the proposed SHAP-AFT scheme in face of
different attack types and intensities.

</details>


### [11] [Fundamental Limits of THz Inter-Satellite ISAC Under Hardware Impairments](https://arxiv.org/abs/2509.15902)
*Haofan Dong,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 本文为太赫兹低轨卫星间链路集成感知通信系统建立了理论框架，分析了其基本性能极限。通过贝叶斯克拉默-拉奥下界分析，揭示了感知精度随载波频率的二次方改善，同时指出系统性能主要受硬件限制而非功率限制。


<details>
  <summary>Details</summary>
Motivation: 研究太赫兹低轨卫星间链路集成感知通信系统的性能极限，解决极端轨道动力学、级联非理想硬件损伤和微弧度波束指向误差等挑战，为未来卫星通信系统设计提供理论基础。

Method: 开发了统一的端到端信号模型，结合贝叶斯克拉默-拉奥下界分析，推导了距离和距离变化率的感知精度极限，并建立了考虑相位噪声和功率放大器非线性的闭合形式容量上限公式。

Result: 研究发现感知估计方差随载波频率呈1/f_c^2的二次方改善，但最终受信号相关硬件失真限制。通信性能受硬件限制而非功率限制，功率放大器非线性成为主要性能瓶颈，比其他损伤高1-2个数量级。

Conclusion: 在200-600 GHz的亚太赫兹频段可能存在有利的工作条件，其中频率的二次方感知增益与硬件质量退化达到平衡。功率放大器非线性是主导性能瓶颈，需要重点关注和改进。

Abstract: This paper establishes a theoretical framework for analyzing the fundamental
performance limits of terahertz (THz) Low Earth Orbit (LEO) inter-satellite
link (ISL) Integrated Sensing and Communications (ISAC) systems. We develop a
unified, end-to-end signal model that, jointly captures the effects of extreme
orbital dynamics, cascaded non-ideal hardware impairments, and micro-radian
beam pointing errors. Through Bayesian Cram\'er-Rao Lower Bound (BCRLB)
analysis, we derive the ultimate sensing accuracy for range and range-rate,
revealing a quadratic ($1/f_c^2$) improvement in estimation variance with
carrier frequency, which is ultimately floored by signal-dependent hardware
distortion. For communication, we show that system performance is not
power-limited but hardware-limited, deriving a closed-form capacity ceiling
under the joint effect of phase noise and PA nonlinearity: $C_{\text{sat}} =
\log_2(1 + e^{-\sigma_\phi^2}/\Gamma_{\text{eff}})$, where
$\Gamma_{\text{eff}}$ is a proposed hardware quality factor. Our numerical
results, based on state-of-the-art component data and the identified
trade-offs, suggest that favorable operational conditions may exist in the
sub-THz frequency range (200-600 GHz) where the quadratic sensing gain with
frequency is balanced against hardware quality degradation. Power Amplifier
(PA) nonlinearity emerges as the dominant performance bottleneck, exceeding
other impairments by one to two orders of magnitude.

</details>


### [12] [MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework](https://arxiv.org/abs/2509.15964)
*Tianyu Li,Yan Xin,Jianzhong,Zhang*

Main category: eess.SP

TL;DR: MoE-CE是一个基于混合专家框架的信道估计方法，旨在提升深度学习模型在动态无线环境中的泛化能力，通过多个专家子网络和动态路由器来适应不同的信道条件。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在多变无线环境（如不同信噪比、资源块数量和信道配置）下的泛化能力不足，特别是在多任务和零样本场景中表现不佳。

Method: 提出MoE-CE框架，使用多个专家子网络分别处理不同的信道特性，并通过学习到的路由器动态选择最相关的专家，提高模型容量和适应性而不显著增加计算成本。

Result: 在多样化SNR、RB数量和信道配置的合成数据集上进行实验，包括多任务和零样本评估，MoE-CE始终优于传统深度学习方法，获得显著性能提升并保持效率。

Conclusion: MoE-CE框架有效提升了信道估计的泛化能力，适用于动态无线环境，且不依赖于特定的骨干模型和学习算法。

Abstract: Reliable channel estimation (CE) is fundamental for robust communication in
dynamic wireless environments, where models must generalize across varying
conditions such as signal-to-noise ratios (SNRs), the number of resource blocks
(RBs), and channel profiles. Traditional deep learning (DL)-based methods
struggle to generalize effectively across such diverse settings, particularly
under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a
flexible mixture-of-experts (MoE) framework designed to enhance the
generalization capability of DL-based CE methods. MoE-CE provides an
appropriate inductive bias by leveraging multiple expert subnetworks, each
specialized in distinct channel characteristics, and a learned router that
dynamically selects the most relevant experts per input. This architecture
enhances model capacity and adaptability without a proportional rise in
computational cost while being agnostic to the choice of the backbone model and
the learning algorithm. Through extensive experiments on synthetic datasets
generated under diverse SNRs, RB numbers, and channel profiles, including
multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently
outperforms conventional DL approaches, achieving significant performance gains
while maintaining efficiency.

</details>


### [13] [Scalable Hessian-free Proximal Conjugate Gradient Method for Nonconvex and Nonsmooth Optimization](https://arxiv.org/abs/2509.15973)
*Yiming Zhou,Wei Dai*

Main category: eess.SP

TL;DR: 本文提出了一种近端共轭梯度方法（PCG），用于解决包含可微函数q和非光滑函数h的复合最小化问题，该方法在保持快速收敛的同时降低了计算和内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 复合最小化问题在信号处理和机器学习中普遍存在，但在大规模实例、病态条件和非凸性同时存在时难以高效求解。现有方法如近端（拟）牛顿算法计算和内存成本较高。

Method: PCG方法在每次迭代中基于共轭梯度（CG）迭代构建牛顿方向的近似，形成曲率感知的主化代理函数。通过CG导出的各向同性权重保证对q的局部二阶模型的主化，并使用CG三对角矩阵的最大Ritz值调整步长。

Result: 数值实验在非凸正则化的CS-MRI和字典学习问题上进行，与基准方法相比证明了所提方法的效率。

Conclusion: PCG方法能够匹配近端（拟）牛顿算法的快速收敛速度，同时显著降低计算和内存需求，特别适用于谱聚类Hessian矩阵的情况。

Abstract: This work studies a composite minimization problem involving a differentiable
function q and a nonsmooth function h, both of which may be nonconvex. This
problem is ubiquitous in signal processing and machine learning yet remains
challenging to solve efficiently, particularly when large-scale instances, poor
conditioning, and nonconvexity coincide. To address these challenges, we
propose a proximal conjugate gradient method (PCG) that matches the fast
convergence of proximal (quasi-)Newton algorithms while reducing computation
and memory complexity, and is especially effective for spectrally clustered
Hessians. Our key innovation is to form, at each iteration, an approximation to
the Newton direction based on CG iterations to build a majorization surrogate.
We define this surrogate in a curvature-aware manner and equip it with a
CG-derived isotropic weight, guaranteeing majorization of a local second-order
model of q along the given direction. To better preserve majorization after the
proximal step and enable further approximation refinement, we scale the CG
direction by the ratio between the Cauchy step length and a step size derived
from the largest Ritz value of the CG tridiagonal. All curvature is accessed
via Hessian-vector products computed by automatic differentiation, keeping the
method Hessian-free. Convergence to first-order critical points is established.
Numerical experiments on CS-MRI with nonconvex regularization and on dictionary
learning, against benchmark methods, demonstrate the efficiency of the proposed
approach.

</details>


### [14] [Wireless Channel Foundation Model with Embedded Noise-Plus-Interference Suppression Structure](https://arxiv.org/abs/2509.15993)
*Yuwei Wang,Li Sun,Tingting Yang*

Main category: eess.SP

TL;DR: 本文提出了一种具有噪声加干扰抑制能力的增强型无线信道基础模型架构，解决了实际系统中由于信道估计误差导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的无线信道基础模型都使用仿真工具生成的完美CSI数据进行训练，但实际部署时只能获得有噪声的降级CSI，导致特征表示无法反映真实信道特性，造成下游任务性能下降。

Method: 首先获取CSI的粗略估计，计算两个投影矩阵提取接收信号中的NPI项，通过NPI估计和减法模块处理，最后通过CSI补全网络获得干净的CSI用于特征提取。

Result: 仿真结果表明，与最先进解决方案相比，具有NPI抑制结构的WCFM在信道预测任务上实现了性能提升。

Conclusion: 提出的NPI抑制架构能够有效处理实际系统中的噪声和干扰问题，为无线信道基础模型的实际部署提供了可行方案。

Abstract: Wireless channel foundation model (WCFM) is a task-agnostic AI model that is
pretrained on large-scale wireless channel datasets to learn a universal
channel feature representation that can be used for a wide range of downstream
tasks related to communications and sensing. While existing works on WCFM have
demonstrated its great potentials in various tasks including beam prediction,
channel prediction, localization, etc, the models are all trained using perfect
(i.e., error-free and complete) channel information state (CSI) data which are
generated with simulation tools. However, in practical systems where the WCFM
is deployed, perfect CSI is not available. Instead, channel estimation needs to
be first performed based on pilot signals over a subset of the resource
elements (REs) to acquire a noisy version of the CSI (termed as degraded CSI),
which significantly differs from the perfect CSI in some real-world
environments with severe noise and interference. As a result, the feature
representation generated by the WCFM is unable to reflect the characteristics
of the true channel, yielding performance degradation in downstream tasks. To
address this issue, in this paper we propose an enhanced wireless channel
foundation model architecture with noise-plus-interference (NPI) suppression
capability. In our approach, coarse estimates of the CSIs are first obtained.
With these information, two projection matrices are computed to extract the NPI
terms in the received signals, which are further processed by a NPI estimation
and subtraction module. Finally, the resultant signal is passed through a CSI
completion network to get a clean version of the CSI, which is used for feature
extraction. Simulation results demonstrated that compared to the
state-of-the-art solutions, WCFM with NPI suppression structure achieves
improved performance on channel prediction task.

</details>


### [15] [Secure Multicast Communications with Pinching-Antenna Systems (PASS)](https://arxiv.org/abs/2509.16045)
*Shan Shan,Chongjun Ouyang,Yong Li,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文研究了捏合天线系统中的安全组播通信，通过自适应调整天线位置来优化波束成形，提高组播安全性。提出了单组和多组组播场景下的联合优化框架，采用交替优化等方法最大化保密组播速率。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线架构在安全组播通信中存在性能限制，需要开发更灵活的天线系统来应对日益增长的安全需求。捏合天线系统通过动态调整天线位置，能够提供更好的波束成形能力和安全性能。

Method: 对于单组组播场景，采用交替优化框架，使用元素级顺序优化方法更新捏合波束成形器，通过半定松弛或Dinkelbach-ADMM方法设计发射波束成形器。对于多组组播场景，在MM框架下交替优化发射和捏合波束成形器，使用SDR或SOCP方法。

Result: 数值结果表明，PASS在各种配置下始终优于传统固定位置天线架构，特别是在服务区域增大、天线阵列规模扩大以及用户和窃听者密度增加时，性能优势更加显著。

Conclusion: 捏合天线系统为安全组播通信提供了一种有效的解决方案，通过动态天线位置调整显著提升了系统安全性能，具有重要的实际应用价值。

Abstract: This article investigates secure multicast communications in pinching-antenna
systems (PASS), where pinching beamforming is enabled by adaptively adjusting
pinching antenna (PAs) positions along waveguides to improve multicast
security. Specifically, a PASS-based secure multicast framework is proposed, in
which joint optimization of transmit and pinching beamforming is conducted to
maximize the secrecy multicast rate. i) For the single-group multicast
scenario, an alternating optimization (AO) framework is employed, where the
pinching beamformer is updated via an element-wise sequential optimization
method. The transmit beamformer is designed via a semidefinite relaxation (SDR)
formulation for an upper-bound solution, while a Dinkelbach-alternating
direction method of multipliers (ADMM) offers a low-complexity alternative. ii)
For the multi-group multicast scenario, transmit and pinching beamformers are
alternately optimized under a majorization-minimization (MM) framework. The
transmit beamformer is obtained via SDR or an efficient second-order cone
programming (SOCP) method, while the pinching beamformer is updated through
MM-based element-wise sequential update strategy. Numerical results are
provided to demonstrate that: (i) PASS consistently outperform conventional
fixed-location antenna architectures in terms of secrecy performance across
various configurations; and (ii) the performance advantage of PASS over
fixed-location architectures becomes more significant with increased service
region, larger antenna arrays, and higher user and eavesdropper densities.

</details>


### [16] [In-Situ Fault Detection of Submerged Pump Impellers Using Encapsulated Accelerometers and Machine Learning](https://arxiv.org/abs/2509.16086)
*Sahil P. Wankhede,Xiangdong Xie,Ali H. Alshehri,Keith W Brashler,Mohammad Ba'adani,Doru C Turcan,Kamal Youcef-Toumi,Xian Du*

Main category: eess.SP

TL;DR: 本研究首次在油井泵的浸没叶轮上部署封装式加速度计，实现原位振动监测，通过机器学习模型分析显示该传感器能有效检测故障，准确率达91.3%，优于非浸没传感器。


<details>
  <summary>Details</summary>
Motivation: 传统安装在电机上的加速度计无法检测浸没在恶劣井下环境中的叶轮故障，需要开发能够直接监测关键浸没部件的方法。

Method: 在实验室规模的泵装置上部署封装式加速度计直接安装在浸没叶轮上，收集正常和模拟故障条件下的振动数据，使用传统和深度学习的机器学习模型进行分析。

Result: 叶轮安装的传感器达到91.3%的平均准确率和0.973的AUC-ROC，优于最佳非浸没传感器，封装对传感器性能无显著影响。

Conclusion: 这种首创的原位监测系统证明封装式叶轮安装传感器能够可靠检测关键浸没泵部件的故障，实现更早的故障检测，减少非计划停机时间。

Abstract: Vertical turbine pumps in oil and gas operations rely on motor-mounted
accelerometers for condition monitoring. However, these sensors cannot detect
faults at submerged impellers exposed to harsh downhole environments. We
present the first study deploying encapsulated accelerometers mounted directly
on submerged impeller bowls, enabling in-situ vibration monitoring. Using a
lab-scale pump setup with 1-meter oil submergence, we collected vibration data
under normal and simulated fault conditions. The data were analyzed using a
suite of machine learning models -- spanning traditional and deep learning
methods -- to evaluate sensor effectiveness. Impeller-mounted sensors achieved
91.3% average accuracy and 0.973 AUC-ROC, outperforming the best non-submerged
sensor. Crucially, encapsulation caused no statistically significant
performance loss in sensor performance, confirming its viability for
oil-submerged environments. While the lab setup used shallow submergence,
real-world pump impellers operate up to hundreds of meters underground -- well
beyond the range of surface-mounted sensors. This first-of-its-kind in-situ
monitoring system demonstrates that impeller-mounted sensors -- encapsulated
for protection while preserving diagnostic fidelity -- can reliably detect
faults in critical submerged pump components. By capturing localized vibration
signatures that are undetectable from surface-mounted sensors, this approach
enables earlier fault detection, reduces unplanned downtime, and optimizes
maintenance for downhole systems in oil and gas operations.

</details>


### [17] [Xona Pulsar Compatibility with GNSS](https://arxiv.org/abs/2509.16183)
*Tyler G. R. Reid,Matteo Gala,Mathieu Favreau,Argyris Kriezis,Michael O'Meara,Andre Pant,Paul Tarantino,Christina Youn*

Main category: eess.SP

TL;DR: Xona的Pulsar低轨卫星导航系统与现有GPS和伽利略系统兼容，使用QPSK调制技术避免干扰，硬件测试证实不会对现有GNSS产生不良干扰。


<details>
  <summary>Details</summary>
Motivation: 随着低轨卫星导航系统的发展，确保新系统与现有GNSS在L波段的兼容性至关重要，这是成功部署和生态系统整合的关键。

Method: 使用频谱紧凑的QPSK调制技术，通过实验室仿真和实际在轨条件下的硬件测试，评估Pulsar的X1和X5信号对GPS和伽利略系统C/N0的退化影响。

Result: 研究表明Pulsar不会对现有GNSS产生不良干扰效应，支持其在全球PNT生态系统中的共存和整合。

Conclusion: Pulsar系统设计具有互操作性，能够与现有GNSS接收器兼容，通过固件更新即可支持，为厘米级精度导航提供了可行的解决方案。

Abstract: At least ten emerging providers are developing satellite navigation systems
for low Earth orbit (LEO). Compatibility with existing GNSS in L-band is
critical to their successful deployment and for the larger ecosystem. Xona is
deploying Pulsar, a near 260-satellite LEO constellation offering dual L-band
navigation services near L1 and L5. Designed for interoperability, Pulsar
provides centimeter-level accuracy, resilience, and authentication, while
maintaining a format that existing GNSS receivers can support through a
firmware update. This study examines Pulsar's compatibility with GPS and
Galileo by evaluating C/N0 degradation caused by the introduction of its X1 and
X5 signals. Using spectrally compact QPSK modulation, Pulsar minimizes
interference despite higher signal power. Theoretical analysis is supported by
hardware testing across a range of commercial GNSS receivers in both lab-based
simulation and in-orbit live-sky conditions. The study confirms Pulsar causes
no adverse interference effects to existing GNSS, supporting coexistence and
integration within the global PNT ecosystem.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [18] [Pre-training Autoencoder for Acoustic Event Classification via Blinky](https://arxiv.org/abs/2509.15261)
*Xiaoyang Liu,Yuma Kinoshita*

Main category: eess.AS

TL;DR: 提出了一种基于预训练自编码器编码器的声音-光转换方法，用于在带宽受限（15Hz）的光学传输通道中实现鲁棒的声学事件分类，相比传统方法在ESC-50数据集上获得了更高的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Blinkies的声学事件分类框架中，30fps的光学传输通道仅能传输正常音频带宽的0.2%，且对噪声高度敏感，需要开发更鲁棒的声音-光转换方法。

Method: 利用预训练自编码器的编码器从录音音频中提取紧凑、判别性特征；在编码器潜在表示中注入人工噪声进行噪声鲁棒学习；编码器架构针对Raspberry Pi 4等边缘设备的内存占用进行专门设计。

Result: 在ESC-50数据集上的仿真实验中，在严格的15Hz带宽约束下，所提方法比传统声音-光转换方法获得了更高的宏平均F1分数。

Conclusion: 该方法通过噪声鲁棒学习和紧凑特征提取，有效提升了带宽受限光学通道中的声学事件分类性能，适用于边缘计算设备。

Abstract: In the acoustic event classification (AEC) framework that employs Blinkies,
audio signals are converted into LED light emissions and subsequently captured
by a single video camera. However, the 30 fps optical transmission channel
conveys only about 0.2% of the normal audio bandwidth and is highly susceptible
to noise. We propose a novel sound-to-light conversion method that leverages
the encoder of a pre-trained autoencoder (AE) to distill compact,
discriminative features from the recorded audio. To pre-train the AE, we adopt
a noise-robust learning strategy in which artificial noise is injected into the
encoder's latent representations during training, thereby enhancing the model's
robustness against channel noise. The encoder architecture is specifically
designed for the memory footprint of contemporary edge devices such as the
Raspberry Pi 4. In a simulation experiment on the ESC-50 dataset under a
stringent 15 Hz bandwidth constraint, the proposed method achieved higher
macro-F1 scores than conventional sound-to-light conversion approaches.

</details>


### [19] [Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech](https://arxiv.org/abs/2509.15473)
*Yuyu Wang,Wuyue Xia,Huaxiu Yao,Jingping Nie*

Main category: eess.AS

TL;DR: 该论文提出了一种基于深度学习模型检测运动后语音中不同类型停顿（语义停顿、呼吸停顿、混合停顿）的方法，并评估了恢复率和运动强度分类性能。


<details>
  <summary>Details</summary>
Motivation: 运动后语音包含丰富的生理和语言线索，检测这些停顿事件可以评估恢复率、肺功能和运动相关异常，但目前缺乏系统识别和区分不同类型停顿的研究。

Method: 使用同步音频和呼吸信号数据集，系统标注停顿类型，探索多种深度学习模型（GRU、1D CNN-LSTM、AlexNet、VGG16）、声学特征（MFCC、MFB）和Wav2Vec2表示，评估单特征、特征融合和两阶段检测-分类级联三种设置。

Result: 语义停顿检测准确率最高达89%，呼吸停顿55%，混合停顿86%，总体准确率73%；运动强度分类准确率达到90.5%，优于先前工作。

Conclusion: 该方法能有效检测运动后语音中的不同类型停顿，并为运动恢复评估提供可靠工具，在运动强度分类方面表现优异。

Abstract: Post-exercise speech contains rich physiological and linguistic cues, often
marked by semantic pauses, breathing pauses, and combined breathing-semantic
pauses. Detecting these events enables assessment of recovery rate, lung
function, and exertion-related abnormalities. However, existing works on
identifying and distinguishing different types of pauses in this context are
limited. In this work, building on a recently released dataset with
synchronized audio and respiration signals, we provide systematic annotations
of pause types. Using these annotations, we systematically conduct exploratory
breathing and semantic pause detection and exertion-level classification across
deep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features
(MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three
setups-single feature, feature fusion, and a two-stage detection-classification
cascade-under both classification and regression formulations. Results show
per-type detection accuracy up to 89$\%$ for semantic, 55$\%$ for breathing,
86$\%$ for combined pauses, and 73$\%$overall, while exertion-level
classification achieves 90.5$\%$ accuracy, outperformin prior work.

</details>


### [20] [State-of-the-Art Dysarthric Speech Recognition with MetaICL for on-the-fly Personalization](https://arxiv.org/abs/2509.15516)
*Dhruuv Agarwal,Harry Zhang,Yang Yu,Quan Wang*

Main category: eess.AS

TL;DR: 提出一种混合元训练方法，用于构建单一模型，通过上下文学习实现零样本和少样本的个性化语音识别，特别针对构音障碍语音


<details>
  <summary>Details</summary>
Motivation: 为构音障碍语音提供个性化自动语音识别（ASR）解决方案，解决传统方法需要为每个用户训练和存储单独适配器的问题

Method: 采用混合元训练方法，结合上下文学习（ICL）技术，使单一模型能够进行零样本和少样本的即时个性化

Result: 在Euphonia数据集上达到13.9%的词错误率，优于说话人无关基线（17.5%），接近用户特定个性化模型；在SAP Test 1上达到5.3%的词错误率，显著优于个性化适配器的8%

Conclusion: 该方法提供了一个实用、可扩展的个性化解决方案，同时发现示例选择的重要性，5个精心挑选的示例可达到19个随机示例的效果

Abstract: Personalizing Automatic Speech Recognition (ASR) for dysarthric speech is
crucial but challenging due to training and storing of individual user
adapters. We propose a hybrid meta-training method for a single model,
excelling in zero-shot and few-shot on-the-fly personalization via in-context
learning (ICL). Measuring Word Error Rate (WER) on state-of-the-art subsets,
the model achieves 13.9% WER on Euphonia which surpasses speaker-independent
baselines (17.5% WER) and rivals user-specific personalized models. On SAP Test
1, its 5.3% WER significantly bests the 8% from even personalized adapters. We
also demonstrate the importance of example curation, where an oracle
text-similarity method shows 5 curated examples can achieve performance similar
to 19 randomly selected ones, highlighting a key area for future efficiency
gains. Finally, we conduct data ablations to measure the data efficiency of
this approach. This work presents a practical, scalable, and personalized
solution.

</details>


### [21] [AFT: An Exemplar-Free Class Incremental Learning Method for Environmental Sound Classification](https://arxiv.org/abs/2509.15523)
*Xinyi Chen,Xi Chen,Zhenyu Weng,Yang Xiao*

Main category: eess.AS

TL;DR: 提出了一种声学特征转换（AFT）技术来解决环境声音分类中的灾难性遗忘问题，无需保留历史数据即可将旧类别的时域特征对齐到新空间，在基准模型上实现了3.7%到3.9%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 环境声音分类模型需要定期适应新声音，但传统基于重放的持续学习方法存在数据隐私问题，而无示例方法会扭曲旧特征导致性能下降。

Method: AFT技术通过选择性压缩的特征空间，将旧类别的时域特征对齐到新空间，避免保留历史数据的同时减轻旧知识的遗忘。

Result: 在两个数据集上的实验显示，该方法相比基线模型实现了3.7%到3.9%的准确率提升，表现一致优于基准。

Conclusion: AFT技术有效解决了环境声音分类中的灾难性遗忘问题，在不保留历史数据的情况下实现了更好的持续学习性能。

Abstract: As sounds carry rich information, environmental sound classification (ESC) is
crucial for numerous applications such as rare wild animals detection. However,
our world constantly changes, asking ESC models to adapt to new sounds
periodically. The major challenge here is catastrophic forgetting, where models
lose the ability to recognize old sounds when learning new ones. Many methods
address this using replay-based continual learning. This could be impractical
in scenarios such as data privacy concerns. Exemplar-free methods are commonly
used but can distort old features, leading to worse performance. To overcome
such limitations, we propose an Acoustic Feature Transformation (AFT) technique
that aligns the temporal features of old classes to the new space, including a
selectively compressed feature space. AFT mitigates the forgetting of old
knowledge without retaining past data. We conducted experiments on two
datasets, showing consistent improvements over baseline models with accuracy
gains of 3.7\% to 3.9\%.

</details>


### [22] [MAGENTA: Magnitude and Geometry-ENhanced Training Approach for Robust Long-Tailed Sound Event Localization and Detection](https://arxiv.org/abs/2509.15599)
*Jun-Wei Yeow,Ee-Leng Tan,Santi Peksi,Woon-Seng Gan*

Main category: eess.AS

TL;DR: MAGENTA是一种针对声音事件定位与检测（SELD）系统的统一损失函数，通过几何分解回归误差为径向和角度分量，解决现实世界长尾数据集中罕见事件识别不足的问题。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的SELD系统在现实世界长尾数据集上性能显著下降，标准回归损失偏向频繁类别，导致罕见事件被系统性低估。

Method: MAGENTA在物理可解释的向量空间中几何分解回归误差为径向和角度分量，实现有针对性的、考虑稀有性的惩罚和增强的方向建模。

Result: 实证表明MAGENTA在非平衡现实世界数据上显著提高了SELD性能。

Conclusion: MAGENTA为几何感知SELD目标函数提供了原则性基础，代码已开源。

Abstract: Deep learning-based Sound Event Localization and Detection (SELD) systems
degrade significantly on real-world, long-tailed datasets. Standard regression
losses bias learning toward frequent classes, causing rare events to be
systematically under-recognized. To address this challenge, we introduce
MAGENTA (Magnitude And Geometry-ENhanced Training Approach), a unified loss
function that counteracts this bias within a physically interpretable vector
space. MAGENTA geometrically decomposes the regression error into radial and
angular components, enabling targeted, rarity-aware penalties and strengthened
directional modeling. Empirically, MAGENTA substantially improves SELD
performance on imbalanced real-world data, providing a principled foundation
for a new class of geometry-aware SELD objectives. Code is available at:
https://github.com/itsjunwei/MAGENTA_ICASSP

</details>


### [23] [Rec-RIR: Monaural Blind Room Impulse Response Identification via DNN-based Reverberant Speech Reconstruction in STFT Domain](https://arxiv.org/abs/2509.15628)
*Pengyu Wang,Xiaofei Li*

Main category: eess.AS

TL;DR: Rec-RIR是一个基于卷积传递函数（CTF）近似的单声道盲房间脉冲响应（RIR）识别方法，使用深度神经网络估计CTF滤波器，并通过伪侵入式测量过程将CTF估计转换为时域RIR。


<details>
  <summary>Details</summary>
Motivation: 房间脉冲响应（RIR）完整描述了声音在封闭空间中的传播过程，但盲RIR识别具有挑战性。本文旨在开发一种有效的单声道盲RIR识别方法。

Method: 基于CTF近似在STFT域建模混响效应，提出具有跨带和窄带块的DNN来估计CTF滤波器，通过重构无噪声混响语音谱进行训练，然后使用伪侵入式测量过程将CTF滤波器估计转换为时域RIR。

Result: 实验结果表明，Rec-RIR在RIR识别和声学参数估计方面均达到了最先进的性能。

Conclusion: Rec-RIR通过CTF近似和DNN估计实现了有效的盲RIR识别，开源代码可供使用。

Abstract: Room impulse response (RIR) characterizes the complete propagation process of
sound in an enclosed space. This paper presents Rec-RIR for monaural blind RIR
identification. Rec-RIR is developed based on the convolutive transfer function
(CTF) approximation, which models reverberation effect within narrow-band
filter banks in the short-time Fourier transform (STFT) domain. Specifically,
we propose a deep neural network (DNN) with cross-band and narrow-band blocks
to estimate the CTF filter. The DNN is trained through reconstructing the
noise-free reverberant speech spectra. This objective enables stable and
straightforward supervised training. Subsequently, a pseudo intrusive
measurement process is employed to convert the CTF filter estimate into
time-domain RIR by simulating a common intrusive RIR measurement procedure.
Experimental results demonstrate that Rec-RIR achieves state-of-the-art (SOTA)
performance in both RIR identification and acoustic parameter estimation.
Open-source codes are available online at
https://github.com/Audio-WestlakeU/Rec-RIR.

</details>


### [24] [A Steered Response Power Method for Sound Source Localization With Generic Acoustic Models](https://arxiv.org/abs/2509.15702)
*Kaspar Müller,Markus Buck,Simon Doclo,Jan Østergaard,Tobias Wolff*

Main category: eess.AS

TL;DR: 本文提出了一种广义的SRP方法，能够应用通用声学模型进行声源定位，相比传统SRP方法在噪声条件下可将平均定位误差降低60%以上。


<details>
  <summary>Details</summary>
Motivation: 传统SRP方法基于简化的声学假设（如全向声源、远场、自由场传播等），但在实际声学场景中这些假设经常被违反，需要更通用的方法。

Method: 提出广义SRP波束形成准则，考虑通用声学模型和空间相关噪声，推导最优SRP波束形成器，并提出适当的频率加权方法。

Result: 在三种不同麦克风设置下对语音进行真实模拟，结果表明所提方法能显著降低平均定位误差，在噪声条件下误差减少超过60%。

Conclusion: 所提出的广义SRP方法能够联合利用观测到的麦克风信号间的幅度差和时间差来推断声源位置，相比传统方法具有显著优势。

Abstract: The steered response power (SRP) method is one of the most popular approaches
for acoustic source localization with microphone arrays. It is often based on
simplifying acoustic assumptions, such as an omnidirectional sound source in
the far field of the microphone array(s), free field propagation, and spatially
uncorrelated noise. In reality, however, there are many acoustic scenarios
where such assumptions are violated. This paper proposes a generalization of
the conventional SRP method that allows to apply generic acoustic models for
localization with arbitrary microphone constellations. These models may
consider, for instance, level differences in distributed microphones, the
directivity of sources and receivers, or acoustic shadowing effects. Moreover,
also measured acoustic transfer functions may be applied as acoustic model. We
show that the delay-and-sum beamforming of the conventional SRP is not optimal
for localization with generic acoustic models. To this end, we propose a
generalized SRP beamforming criterion that considers generic acoustic models
and spatially correlated noise, and derive an optimal SRP beamformer.
Furthermore, we propose and analyze appropriate frequency weightings. Unlike
the conventional SRP, the proposed method can jointly exploit observed level
and time differences between the microphone signals to infer the source
location. Realistic simulations of three different microphone setups with
speech under various noise conditions indicate that the proposed method can
significantly reduce the mean localization error compared to the conventional
SRP and, in particular, a reduction of more than 60% can be archived in noisy
conditions.

</details>


### [25] [Deep Dubbing: End-to-End Auto-Audiobook System with Text-to-Timbre and Context-Aware Instruct-TTS](https://arxiv.org/abs/2509.15845)
*Ziqi Dai,Yiting Chen,Jiacheng Xu,Liufei Xie,Yuchen Wang,Zhenchuan Yang,Bingsong Bai,Yangsheng Gao,Wenjiang Zhou,Weifeng Zhao,Ruohua Zhou*

Main category: eess.AS

TL;DR: DeepDubbing是一个端到端的自动化多参与者有声读物生产系统，通过文本到音色模型和上下文感知指令TTS模型解决角色音色选择和情感表达问题。


<details>
  <summary>Details</summary>
Motivation: 当前有声读物生产流程中，脚本分析可以自动化，但角色音色选择仍需人工，且TTS在情感表达、语调控制和场景适应方面存在困难。

Method: 系统包含两个主要组件：文本到音色模型生成角色特定音色嵌入，上下文感知指令TTS模型通过分析上下文对话和细粒度情感指令合成富有表现力的语音。

Result: 该系统能够自动生成具有音色匹配角色声音和情感丰富叙述的多参与者有声读物。

Conclusion: DeepDubbing为有声读物生产提供了一种新颖的自动化解决方案，实现了角色音色选择和情感表达的自动化。

Abstract: The pipeline for multi-participant audiobook production primarily consists of
three stages: script analysis, character voice timbre selection, and speech
synthesis. Among these, script analysis can be automated with high accuracy
using NLP models, whereas character voice timbre selection still relies on
manual effort. Speech synthesis uses either manual dubbing or text-to-speech
(TTS). While TTS boosts efficiency, it struggles with emotional expression,
intonation control, and contextual scene adaptation. To address these
challenges, we propose DeepDubbing, an end-to-end automated system for
multi-participant audiobook production. The system comprises two main
components: a Text-to-Timbre (TTT) model and a Context-Aware Instruct-TTS
(CA-Instruct-TTS) model. The TTT model generates role-specific timbre
embeddings conditioned on text descriptions. The CA-Instruct-TTS model
synthesizes expressive speech by analyzing contextual dialogue and
incorporating fine-grained emotional instructions. This system enables the
automated generation of multi-participant audiobooks with both timbre-matched
character voices and emotionally expressive narration, offering a novel
solution for audiobook production.

</details>


### [26] [Sound Separation and Classification with Object and Semantic Guidance](https://arxiv.org/abs/2509.15899)
*Younghoo Kwon,Jung-Woo Choi*

Main category: eess.AS

TL;DR: 本文提出了一种双路径分类器（DPC）架构，结合了源分离模型的对象特征和预训练分类模型的语义表示，无需微调，并引入了语义线索编码器（SCE）来丰富注入线索的语义深度，在DCASE 2025任务4评估集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过微调大型分类模型并与分离模型级联，将分类标签作为分离线索注入下一迭代步骤，但这种集成存在三个问题：在小数据集上微调会失去大型分类模型的多样性、源分离模型的特征与预训练分类器的输入不同、注入的one-hot类标签缺乏语义深度导致错误传播。

Method: 提出了双路径分类器（DPC）架构，将源分离模型的对象特征与预训练分类模型的语义表示相结合，无需微调；引入了语义线索编码器（SCE）来丰富注入线索的语义深度。

Result: 在DCASE 2025任务4评估集上实现了11.19 dB的CA-SDRi，超越了之前最佳性能的11.00 dB，并提高了语义保真度。

Conclusion: 结果表明，将分离器导出的特征与丰富的语义线索相结合是有效的，解决了传统方法中的问题，实现了更好的空间语义分割性能。

Abstract: The spatial semantic segmentation task focuses on separating and classifying
sound objects from multichannel signals. To achieve two different goals,
conventional methods fine-tune a large classification model cascaded with the
separation model and inject classified labels as separation clues for the next
iteration step. However, such integration is not ideal, in that fine-tuning
over a smaller dataset loses the diversity of large classification models,
features from the source separation model are different from the inputs of the
pretrained classifier, and injected one-hot class labels lack semantic depth,
often leading to error propagation. To resolve these issues, we propose a
Dual-Path Classifier (DPC) architecture that combines object features from a
source separation model with semantic representations acquired from a
pretrained classification model without fine-tuning. We also introduce a
Semantic Clue Encoder (SCE) that enriches the semantic depth of injected clues.
Our system achieves a state-of-the-art 11.19 dB CA-SDRi and enhanced semantic
fidelity on the DCASE 2025 task4 evaluation set, surpassing the top-rank
performance of 11.00 dB. These results highlight the effectiveness of
integrating separator-derived features and rich semantic clues.

</details>


### [27] [VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency](https://arxiv.org/abs/2509.15969)
*Nikita Torgashov,Gustav Eje Henter,Gabriel Skantze*

Main category: eess.AS

TL;DR: VoXtream是一个完全自回归、零样本的实时流式文本转语音系统，能够从第一个单词开始说话，实现了102毫秒的最低初始延迟。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在实时应用中快速响应的流式TTS系统，减少语音合成的初始延迟，使其能够立即开始说话。

Method: 使用单调对齐方案和动态前瞻机制，通过增量音素转换器、时间转换器（预测语义和持续时间标记）和深度转换器（生成声学标记）来直接映射输入音素到音频标记。

Result: 在GPU上实现102毫秒的初始延迟，在9k小时的中等规模语料库上训练，在多个指标上匹配或超越更大的基线模型，在输出流和全流设置下都具有竞争力的质量。

Conclusion: VoXtream展示了在实时流式TTS系统中实现低延迟和高性能的可行性，为实时语音合成应用提供了有效的解决方案。

Abstract: We present VoXtream, a fully autoregressive, zero-shot streaming
text-to-speech (TTS) system for real-time use that begins speaking from the
first word. VoXtream directly maps incoming phonemes to audio tokens using a
monotonic alignment scheme and a dynamic look-ahead that does not delay onset.
Built around an incremental phoneme transformer, a temporal transformer
predicting semantic and duration tokens, and a depth transformer producing
acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay
among publicly available streaming TTS: 102 ms on GPU. Despite being trained on
a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several
metrics, while delivering competitive quality in both output- and
full-streaming settings. Demo and code are available at
https://herimor.github.io/voxtream.

</details>


### [28] [Interpreting the Role of Visemes in Audio-Visual Speech Recognition](https://arxiv.org/abs/2509.16023)
*Aristeidis Papadopoulos,Naomi Harte*

Main category: eess.AS

TL;DR: 本文应用多种可解释性技术分析AV-HuBERT模型中视位的编码方式，揭示了视觉和听觉模态在音频-视觉语音识别中的交互作用。


<details>
  <summary>Details</summary>
Motivation: 虽然音频-视觉语音识别模型性能已超越纯音频模型，但其可解释性特别是视觉模态的作用仍未被充分探索。

Method: 使用t-SNE可视化学习特征，并通过探针分析展示音频如何精炼特征表示，特别是对于视觉模糊或代表性不足的视位。

Result: 发现视觉线索驱动的自然聚类，且音频进一步精炼了这些特征表示。

Conclusion: 研究结果揭示了AVSR中模态间的相互作用，可能为利用视觉信息改善AVSR性能提供新策略。

Abstract: Audio-Visual Speech Recognition (AVSR) models have surpassed their audio-only
counterparts in terms of performance. However, the interpretability of AVSR
systems, particularly the role of the visual modality, remains under-explored.
In this paper, we apply several interpretability techniques to examine how
visemes are encoded in AV-HuBERT a state-of-the-art AVSR model. First, we use
t-distributed Stochastic Neighbour Embedding (t-SNE) to visualize learned
features, revealing natural clustering driven by visual cues, which is further
refined by the presence of audio. Then, we employ probing to show how audio
contributes to refining feature representations, particularly for visemes that
are visually ambiguous or under-represented. Our findings shed light on the
interplay between modalities in AVSR and could point to new strategies for
leveraging visual information to improve AVSR performance.

</details>


### [29] [Rethinking Cross-Corpus Speech Emotion Recognition Benchmarking: Are Paralinguistic Pre-Trained Representations Sufficient?](https://arxiv.org/abs/2509.16182)
*Orchid Chetia Phukan,Mohd Mujtaba Akhtar,Girish,Swarup Ranjan Behera,Parabattina Bhagath,Pailla Balakrishna Reddy,Arun Balaji Buduru*

Main category: eess.AS

TL;DR: 本文研究发现，专注于副语言语音处理的预训练模型在跨语料库语音情感识别任务中表现优于其他模型，特别是TRILLsson模型表现最佳，强调了在跨语料库语音情感识别基准测试中考虑副语言预训练模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的跨语料库语音情感识别基准测试忽略了为副语言语音处理预训练的模型，而语音情感识别本质上是一个副语言任务，这引发了基准测试可靠性的担忧。

Method: 分析包括副语言、单语、多语和说话人识别在内的最先进预训练模型的表示，特别比较TRILLsson（副语言预训练模型）与其他模型的性能。

Result: 研究结果证实TRILLsson在跨语料库语音情感识别设置中优于其他预训练模型。

Conclusion: 这项研究增强了基准测试的可信度，并为可靠的跨语料库语音情感识别预训练模型评估提供了指导，强调需要考虑副语言预训练模型。

Abstract: Recent benchmarks evaluating pre-trained models (PTMs) for cross-corpus
speech emotion recognition (SER) have overlooked PTM pre-trained for
paralinguistic speech processing (PSP), raising concerns about their
reliability, since SER is inherently a paralinguistic task. We hypothesize that
PSP-focused PTM will perform better in cross-corpus SER settings. To test this,
we analyze state-of-the-art PTMs representations including paralinguistic,
monolingual, multilingual, and speaker recognition. Our results confirm that
TRILLsson (a paralinguistic PTM) outperforms others, reinforcing the need to
consider PSP-focused PTMs in cross-corpus SER benchmarks. This study enhances
benchmark trustworthiness and guides PTMs evaluations for reliable cross-corpus
SER.

</details>


### [30] [Are Multimodal Foundation Models All That Is Needed for Emofake Detection?](https://arxiv.org/abs/2509.16193)
*Mohd Mujtaba Akhtar,Girish,Orchid Chetia Phukan,Swarup Ranjan Behera,Pailla Balakrishna Reddy,Ananda Chandra Nayak,Sanjib Kumar Nayak,Arun Balaji Buduru*

Main category: eess.AS

TL;DR: 该论文研究了多模态基础模型（MFMs）在情感伪造检测（EFD）中的应用，假设MFMs由于跨模态预训练能比仅依赖音频的音频基础模型（AFMs）表现更好。实验证实了MFMs的优势，并提出了SCAR融合框架，通过嵌套交叉注意力和自注意力精炼模块实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是基于MFMs能够从多种模态学习情感模式，而AFMs仅依赖音频，因此MFMs能更好地识别被操纵音频中的不自然情感变化和不一致性，从而更有效地区分真实和伪造的情感表达。

Method: 方法包括对SOTA MFMs（如LanguageBind）和AFMs（如WavLM）进行综合比较分析，并提出了SCAR融合框架。SCAR引入嵌套交叉注意力机制，让基础模型的表示在两个阶段顺序交互以优化信息交换，同时自注意力精炼模块通过强化重要的跨模型线索并抑制噪声来增强特征表示。

Result: 实验结果表明，MFMs在EFD任务上超越了AFMs。通过SCAR框架对MFMs进行协同融合，实现了SOTA性能，超越了独立的基础模型、传统融合方法以及先前在EFD上的工作。

Conclusion: 结论是MFMs在情感伪造检测中确实优于AFMs，而提出的SCAR框架通过有效的多模态融合进一步提升了检测性能，为EFD任务提供了新的SOTA解决方案。

Abstract: In this work, we investigate multimodal foundation models (MFMs) for EmoFake
detection (EFD) and hypothesize that they will outperform audio foundation
models (AFMs). MFMs due to their cross-modal pre-training, learns emotional
patterns from multiple modalities, while AFMs rely only on audio. As such, MFMs
can better recognize unnatural emotional shifts and inconsistencies in
manipulated audio, making them more effective at distinguishing real from fake
emotional expressions. To validate our hypothesis, we conduct a comprehensive
comparative analysis of state-of-the-art (SOTA) MFMs (e.g. LanguageBind)
alongside AFMs (e.g. WavLM). Our experiments confirm that MFMs surpass AFMs for
EFD. Beyond individual foundation models (FMs) performance, we explore FMs
fusion, motivated by findings in related research areas such synthetic speech
detection and speech emotion recognition. To this end, we propose SCAR, a novel
framework for effective fusion. SCAR introduces a nested cross-attention
mechanism, where representations from FMs interact at two stages sequentially
to refine information exchange. Additionally, a self-attention refinement
module further enhances feature representations by reinforcing important
cross-FM cues while suppressing noise. Through SCAR with synergistic fusion of
MFMs, we achieve SOTA performance, surpassing both standalone FMs and
conventional fusion approaches and previous works on EFD.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [31] [Emotion-Aware Speech Generation with Character-Specific Voices for Comics](https://arxiv.org/abs/2509.15253)
*Zhiwen Qian,Jinhua Liang,Huan Zhang*

Main category: cs.SD

TL;DR: 提出一个端到端的管道，用于从漫画生成角色特定、情感感知的语音。系统输入完整漫画卷，输出与角色对话和情感状态对齐的语音。


<details>
  <summary>Details</summary>
Motivation: 实现漫画的自动化配音生成，为交互式和沉浸式漫画阅读体验迈出一步。

Method: 图像处理模块执行角色检测、文本识别和情感强度识别；大语言模型通过整合视觉信息和情节上下文进行对话归属和情感分析；使用文本转语音模型合成语音，为每个角色和情感定制独特的语音配置文件。

Result: 开发了一个完整的系统，能够处理漫画输入并生成与角色和情感匹配的语音输出。

Conclusion: 该工作展示了自动化漫画配音生成的可行性，为提升漫画阅读体验提供了技术基础。

Abstract: This paper presents an end-to-end pipeline for generating character-specific,
emotion-aware speech from comics. The proposed system takes full comic volumes
as input and produces speech aligned with each character's dialogue and
emotional state. An image processing module performs character detection, text
recognition, and emotion intensity recognition. A large language model performs
dialogue attribution and emotion analysis by integrating visual information
with the evolving plot context. Speech is synthesized through a text-to-speech
model with distinct voice profiles tailored to each character and emotion. This
work enables automated voiceover generation for comics, offering a step toward
interactive and immersive comic reading experience.

</details>


### [32] [Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data](https://arxiv.org/abs/2509.15389)
*Youngwon Choi,Jaeyoon Jung,Hyeonyu Kim,Huu-Kim Nguyen,Hwayeon Kim*

Main category: cs.SD

TL;DR: 本文系统研究了大型音频语言模型（LALMs）在有限语音数据下的微调策略，包括纯文本微调、直接混合和课程学习等方法，重点关注文本标签对丰富但语音标签对有限的情况。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在语音相关任务中表现出强大能力，但在有限语音数据下的微调策略尚未得到充分探索，特别是在实际应用中语音数据往往稀缺的现实约束下。

Method: 比较了三种微调方案：纯文本微调、直接混合语音文本数据、以及课程学习方法。在跨语言口语理解任务中，结合源语言语音数据和目标语言文本数据，仅需少量目标语言语音数据进行适配。

Result: LALMs仅通过纯文本微调就能达到竞争性性能，显示其强大的泛化能力。添加少量语音数据（2-5%）即可带来显著提升，特别是在数据稀缺时课程学习效果最佳。跨语言SLU中，结合源语言语音和目标语言文本可实现有效适配。

Conclusion: 本研究为在实际数据约束下进行LALM微调提供了实用指导，证明即使语音数据有限，通过合适的微调策略也能获得良好性能。

Abstract: Large Audio Language Models (LALMs) have emerged as powerful tools for
speech-related tasks but remain underexplored for fine-tuning, especially with
limited speech data. To bridge this gap, we systematically examine how
different fine-tuning schemes including text-only, direct mixing, and
curriculum learning affect spoken language understanding (SLU), focusing on
scenarios where text-label pairs are abundant while paired speech-label data
are limited. Results show that LALMs already achieve competitive performance
with text-only fine-tuning, highlighting their strong generalization ability.
Adding even small amounts of speech data (2-5%) yields substantial further
gains, with curriculum learning particularly effective under scarce data. In
cross-lingual SLU, combining source-language speech data with target-language
text and minimal target-language speech data enables effective adaptation.
Overall, this study provides practical insights into the LALM fine-tuning under
realistic data constraints.

</details>


### [33] [Impact of Phonetics on Speaker Identity in Adversarial Voice Attack](https://arxiv.org/abs/2509.15437)
*Daniyal Kabir Dar,Qiben Yan,Li Xiao,Arun Ross*

Main category: cs.SD

TL;DR: 本文分析了语音对抗性扰动在音素层面的影响，揭示了这些扰动通过元音中央化和辅音替换等系统性混淆来误导语音识别系统，同时还会导致说话人身份漂移。


<details>
  <summary>Details</summary>
Motivation: 虽然针对端到端ASR模型的定向攻击已被广泛研究，但这些扰动的音素基础及其对说话人身份的影响仍未充分探索。本文旨在填补这一研究空白。

Method: 使用DeepSpeech作为ASR目标，生成定向对抗样本，并在真实和冒名样本上评估其对说话人嵌入的影响。分析了16个音素多样化的目标短语。

Result: 结果表明，对抗性音频不仅导致转录错误，还会引起身份漂移，说明扰动会降解对说话人验证至关重要的音素线索。

Conclusion: 需要开发音素感知的防御机制来确保ASR和说话人识别系统的鲁棒性。

Abstract: Adversarial perturbations in speech pose a serious threat to automatic speech
recognition (ASR) and speaker verification by introducing subtle waveform
modifications that remain imperceptible to humans but can significantly alter
system outputs. While targeted attacks on end-to-end ASR models have been
widely studied, the phonetic basis of these perturbations and their effect on
speaker identity remain underexplored. In this work, we analyze adversarial
audio at the phonetic level and show that perturbations exploit systematic
confusions such as vowel centralization and consonant substitutions. These
distortions not only mislead transcription but also degrade phonetic cues
critical for speaker verification, leading to identity drift. Using DeepSpeech
as our ASR target, we generate targeted adversarial examples and evaluate their
impact on speaker embeddings across genuine and impostor samples. Results
across 16 phonetically diverse target phrases demonstrate that adversarial
audio induces both transcription errors and identity drift, highlighting the
need for phonetic-aware defenses to ensure the robustness of ASR and speaker
recognition systems.

</details>


### [34] [A Novel Semantic Compression Approach for Ultra-low Bandwidth Voice Communication](https://arxiv.org/abs/2509.15462)
*Ryan Collette,Ross Greenwood,Serena Nicoll*

Main category: cs.SD

TL;DR: 该论文提出了一种基于语义通信的新型语音编解码方法，利用生成式语音模型的语义表示能力，在保持感知质量和下游任务适用性的同时实现2-4倍更低的比特率。


<details>
  <summary>Details</summary>
Motivation: 现有语音音频编解码器虽然能够利用时间冗余和多尺度表示，但以相同方式处理所有音频特征。而生成式语音模型能够将音频信号分解为不同特征的高层语义表示，这为降低比特率提供了新思路。

Method: 采用语义通信方法，利用生成式语音模型的语义表示能力来分解音频信号，实现对音频特征的有效因子化表示。

Result: 在2-4倍更低比特率下，该方法在转录、情感分析和说话人验证任务上匹配或优于现有音频编解码器，特别是在感知质量和说话人验证方面显著超越Encodec，同时使用最多4倍更少的比特率。

Conclusion: 基于语义表示的通信方法能够在不牺牲感知质量或下游任务适用性的前提下，显著降低语音编码的比特率，为高效语音通信提供了新方向。

Abstract: While existing speech audio codecs designed for compression exploit limited
forms of temporal redundancy and allow for multi-scale representations, they
tend to represent all features of audio in the same way. In contrast,
generative voice models designed for text-to-speech and voice transfer tasks
have recently proved effective at factorizing audio signals into high-level
semantic representations of fundamentally distinct features. In this paper, we
leverage such representations in a novel semantic communications approach to
achieve lower bitrates without sacrificing perceptual quality or suitability
for specific downstream tasks. Our technique matches or outperforms existing
audio codecs on transcription, sentiment analysis, and speaker verification
when encoding at 2-4x lower bitrate -- notably surpassing Encodec in perceptual
quality and speaker verification while using up to 4x less bitrate.

</details>


### [35] [Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech](https://arxiv.org/abs/2509.15492)
*Xinlei Niu,Jianbo Ma,Dylan Harper-Harris,Xiangyu Zhang,Charles Patrick Martin,Jing Zhang*

Main category: cs.SD

TL;DR: BVS是一个视频到音频生成方法，能够生成包含环境感知智能语音的同步音频，解决了现有方法在生成可理解语音方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到音频方法主要关注Foley声音生成，难以产生可理解的语音；而当前的环境语音合成方法仍然是文本驱动的，无法与动态视频内容时间对齐。

Method: 采用两阶段建模方法：第一阶段是视频引导的音频语义模型，根据语音提示预测统一的音频语义标记；第二阶段是视频条件的语义到声学模型，将语义标记细化为详细的声学标记。

Result: 实验证明BVS在视频到上下文感知语音合成和沉浸式音频背景转换等场景中有效，消融研究进一步验证了设计。

Conclusion: BVS方法能够生成与视频同步的环境感知智能语音，在现实应用中具有重要价值。

Abstract: The generation of realistic, context-aware audio is important in real-world
applications such as video game development. While existing video-to-audio
(V2A) methods mainly focus on Foley sound generation, they struggle to produce
intelligible speech. Meanwhile, current environmental speech synthesis
approaches remain text-driven and fail to temporally align with dynamic video
content. In this paper, we propose Beyond Video-to-SFX (BVS), a method to
generate synchronized audio with environmentally aware intelligible speech for
given videos. We introduce a two-stage modeling method: (1) stage one is a
video-guided audio semantic (V2AS) model to predict unified audio semantic
tokens conditioned on phonetic cues; (2) stage two is a video-conditioned
semantic-to-acoustic (VS2A) model that refines semantic tokens into detailed
acoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios
such as video-to-context-aware speech synthesis and immersive audio background
conversion, with ablation studies further validating our design. Our
demonstration is available
at~\href{https://xinleiniu.github.io/BVS-demo/}{BVS-Demo}.

</details>


### [36] [Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection](https://arxiv.org/abs/2509.15570)
*Xinxin Meng,Jiangtao Guo,Yunxiang Zhang,Shun Huang*

Main category: cs.SD

TL;DR: 提出了一种基于高频信息数据增强的对比学习方法，用于无监督异常声音检测，通过让模型更关注代表机器正常运行的低频信息来提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 基于生物感知和数据分析发现异常音频和噪声通常具有更高频率，因此希望通过数据增强使模型更关注代表机器正常操作模式的低频信息。

Method: 在对比学习中提出高频信息的数据增强方法，使模型学习正常数据的分布空间，重点关注音频的低频信息。

Result: 在DCASE 2020 Task 2数据集上评估，该方法优于其他对比学习方法，并在DCASE 2022 Task 2数据集上验证了方法的泛化能力。

Conclusion: 提出的高频信息数据增强方法能有效提升无监督异常声音检测性能，具有良好的泛化能力。

Abstract: The outlier exposure method is an effective approach to address the
unsupervised anomaly sound detection problem. The key focus of this method is
how to make the model learn the distribution space of normal data. Based on
biological perception and data analysis, it is found that anomalous audio and
noise often have higher frequencies. Therefore, we propose a data augmentation
method for high-frequency information in contrastive learning. This enables the
model to pay more attention to the low-frequency information of the audio,
which represents the normal operational mode of the machine. We evaluated the
proposed method on the DCASE 2020 Task 2. The results showed that our method
outperformed other contrastive learning methods used on this dataset. We also
evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.

</details>


### [37] [Thinking in cocktail party: Chain-of-Thought and reinforcement learning for target speaker automatic speech recognition](https://arxiv.org/abs/2509.15612)
*Yiru Zhang,Hang Su,Lichun Fan,Zhenbo Luo,Jian Luan*

Main category: cs.SD

TL;DR: 本文提出了一种结合思维链（CoT）和强化学习（RL）的新型TS-ASR框架，通过在CoT数据集上进行训练和RL微调，显著提升了目标说话人自动语音识别在鸡尾酒会场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 目标说话人自动语音识别（TS-ASR）需要从多人语音混合中识别特定说话人，现有大型音频语言模型（LALMs）在该任务上仍有优化空间。由于TS-ASR需要深度理解语音信号、区分不同说话人并处理重叠语音，因此特别适合采用推理引导的方法。

Method: 构建了TS-ASR的CoT数据集，模型先在常规数据上训练，然后在CoT数据上微调，最后使用RL对选定数据进行进一步训练以增强泛化推理能力。

Result: 实验结果表明，CoT和RL训练显著提升了TS-ASR性能，在可比数据集上达到了最先进的性能水平。

Conclusion: 提出的CoT+RL框架有效提升了TS-ASR任务的表现，证明了推理引导方法在该领域的有效性。

Abstract: Target Speaker Automatic Speech Recognition (TS-ASR) aims to transcribe the
speech of a specified target speaker from multi-speaker mixtures in cocktail
party scenarios. Recent advancement of Large Audio-Language Models (LALMs) has
already brought some new insights to TS-ASR. However, significant room for
optimization remains for the TS-ASR task within the LALMs architecture. While
Chain of Thoughts (CoT) and Reinforcement Learning (RL) have proven effective
in certain speech tasks, TS-ASR, which requires the model to deeply comprehend
speech signals, differentiate various speakers, and handle overlapping
utterances is particularly well-suited to a reasoning-guided approach.
Therefore, we propose a novel framework that incorporates CoT and RL training
into TS-ASR for performance improvement. A novel CoT dataset of TS-ASR is
constructed, and the TS-ASR model is first trained on regular data and then
fine-tuned on CoT data. Finally, the model is further trained with RL using
selected data to enhance generalized reasoning capabilities. Experiment results
demonstrate a significant improvement of TS-ASR performance with CoT and RL
training, establishing a state-of-the-art performance compared with previous
works of TS-ASR on comparable datasets.

</details>


### [38] [De-crackling Virtual Analog Controls with Asymptotically Stable Recurrent Neural Networks](https://arxiv.org/abs/2509.15622)
*Valtteri Kallinen,Lauri Juvela*

Main category: cs.SD

TL;DR: 本文提出通过确保递归神经网络的渐近稳定性来消除时间变化控制条件下的音频伪影问题


<details>
  <summary>Details</summary>
Motivation: 在虚拟模拟建模中，递归神经网络用于模拟模拟硬件音频处理器，但传统方法在时间变化控制条件下会产生音频伪影

Method: 推导常用递归神经网络的渐近稳定性约束条件，并证明渐近稳定性可以消除零输入和时间变化控制条件下的音频伪影

Result: 实验结果表明渐近稳定的递归神经网络能够有效消除控制引起的音频伪影

Conclusion: 该方法为解决其他音频神经网络架构（如卷积和状态空间模型）中的控制诱导伪影问题提供了通用解决方案

Abstract: Recurrent neural networks are used in virtual analog modeling applications to
digitally replicate the sound of analog hardware audio processors. The controls
of hardware devices can be used as a conditioning input to these networks. A
common method for introducing control conditioning to these models is the
direct static concatenation of controls with input audio samples, which we show
produces audio artifacts under time-varied conditioning. Here we derive
constraints for asymptotically stable variants of commonly used recurrent
neural networks and demonstrate that asymptotical stability in recurrent neural
networks can eliminate audio artifacts from the model output under zero input
and time-varied conditioning. Furthermore, our results suggest a possible
general solution to mitigate conditioning-induced artifacts in other audio
neural network architectures, such as convolutional and state-space models.

</details>


### [39] [The Rhythm In Anything: Audio-Prompted Drums Generation with Masked Language Modeling](https://arxiv.org/abs/2509.15625)
*Patrick O'Reilly,Julia Barnett,Hugo Flores García,Annie Chu,Nathan Pruyne,Prem Seetharaman,Bryan Pardo*

Main category: cs.SD

TL;DR: TRIA是一个基于掩码变换器的模型，能够将节奏声音手势映射为高质量鼓录音，通过音频提示生成所需节奏和音色的鼓声。


<details>
  <summary>Details</summary>
Motivation: 音乐家和普通人使用节奏声音手势（如敲击和口技）表达鼓点模式，但将这些想法实现为完整制作的鼓录音耗时且可能中断创作流程。

Method: 使用掩码变换器模型，通过两个音频提示：一个表示所需节奏模式，另一个表示鼓组音色，生成具有适当装饰的鼓声录音。

Result: 在不到10小时的公开鼓数据上训练的TRIA模型能够以零样本方式生成高质量、忠实的声音手势实现，覆盖广泛的音色范围。

Conclusion: TRIA成功填补了节奏声音手势与专业鼓录音之间的技术鸿沟，为音乐创作提供了高效的解决方案。

Abstract: Musicians and nonmusicians alike use rhythmic sound gestures, such as tapping
and beatboxing, to express drum patterns. While these gestures effectively
communicate musical ideas, realizing these ideas as fully-produced drum
recordings can be time-consuming, potentially disrupting many creative
workflows. To bridge this gap, we present TRIA (The Rhythm In Anything), a
masked transformer model for mapping rhythmic sound gestures to high-fidelity
drum recordings. Given an audio prompt of the desired rhythmic pattern and a
second prompt to represent drumkit timbre, TRIA produces audio of a drumkit
playing the desired rhythm (with appropriate elaborations) in the desired
timbre. Subjective and objective evaluations show that a TRIA model trained on
less than 10 hours of publicly-available drum data can generate high-quality,
faithful realizations of sound gestures across a wide range of timbres in a
zero-shot manner.

</details>


### [40] [LibriTTS-VI: A Public Corpus and Novel Methods for Efficient Voice Impression Control](https://arxiv.org/abs/2509.15626)
*Junki Ohmura,Yuki Ito,Emiru Tsunoo,Toshiyuki Sekiya,Toshiyuki Kumakura*

Main category: cs.SD

TL;DR: 论文提出两种方法解决语音印象控制中的印象泄漏问题：1）使用不同语句分别处理说话人身份和目标印象的训练策略；2）无需参考音频的模型，仅从目标印象生成说话人嵌入。同时发布了首个公开语音印象数据集LibriTTS-VI。


<details>
  <summary>Details</summary>
Motivation: 解决语音合成中细粒度语音印象控制的两个关键挑战：印象泄漏问题（合成语音受说话人参考音频影响而非目标印象）和缺乏公开标注语料库。

Method: 提出两种方法：1）训练策略：使用不同语句分别处理说话人身份和目标印象；2）无参考模型：仅从目标印象生成说话人嵌入。基于LibriTTS-R语料库构建了LibriTTS-VI数据集。

Result: 客观评估：11维语音印象向量的均方误差从0.61降至0.41；主观评估：从1.15降至0.92，同时保持高保真度。

Conclusion: 提出的方法显著提高了语音印象控制的准确性，解决了印象泄漏问题，并发布了首个公开语音印象数据集促进可重复研究。

Abstract: Fine-grained control over voice impressions (e.g., making a voice brighter or
calmer) is a key frontier for creating more controllable text-to-speech.
However, this nascent field faces two key challenges. The first is the problem
of impression leakage, where the synthesized voice is undesirably influenced by
the speaker's reference audio, rather than the separately specified target
impression, and the second is the lack of a public, annotated corpus. To
mitigate impression leakage, we propose two methods: 1) a training strategy
that separately uses an utterance for speaker identity and another utterance of
the same speaker for target impression, and 2) a novel reference-free model
that generates a speaker embedding solely from the target impression, achieving
the benefits of improved robustness against the leakage and the convenience of
reference-free generation. Objective and subjective evaluations demonstrate a
significant improvement in controllability. Our best method reduced the mean
squared error of 11-dimensional voice impression vectors from 0.61 to 0.41
objectively and from 1.15 to 0.92 subjectively, while maintaining high
fidelity. To foster reproducible research, we introduce LibriTTS-VI, the first
public voice impression dataset released with clear annotation standards, built
upon the LibriTTS-R corpus.

</details>


### [41] [The Singing Voice Conversion Challenge 2025: From Singer Identity Conversion To Singing Style Conversion](https://arxiv.org/abs/2509.15629)
*Lester Phillip Violeta,Xueyao Zhang,Jiatong Shi,Yusuke Yasuda,Wen-Chin Huang,Zhizheng Wu,Tomoki Toda*

Main category: cs.SD

TL;DR: 最新一届歌唱声音转换挑战赛的结果分析，该挑战赛比较了26个不同的声音转换系统，重点关注歌手身份和歌唱风格的转换。


<details>
  <summary>Details</summary>
Motivation: 创建一个受控环境来比较和理解不同的声音转换系统，特别是扩展了对歌唱风格转换的研究，而不仅仅是歌手身份转换。

Method: 开发了新的挑战数据库，引入了两个任务，开源了基线系统，并进行了大规模众包听力测试和客观评估。

Result: 顶级系统在歌手身份评分上与真实样本相当，但在建模歌唱风格和实现高自然度方面仍面临挑战。

Conclusion: 建模动态信息（如气声、滑音和颤音）是当前歌唱风格转换的主要难点，需要进一步研究。

Abstract: We present the findings of the latest iteration of the Singing Voice
Conversion Challenge, a scientific event aiming to compare and understand
different voice conversion systems in a controlled environment. Compared to
previous iterations which solely focused on converting the singer identity,
this year we also focused on converting the singing style of the singer. To
create a controlled environment and thorough evaluations, we developed a new
challenge database, introduced two tasks, open-sourced baselines, and conducted
large-scale crowd-sourced listening tests and objective evaluations. The
challenge was ran for two months and in total we evaluated 26 different
systems. The results of the large-scale crowd-sourced listening test showed
that top systems had comparable singer identity scores to ground truth samples.
However, modeling the singing style and consequently achieving high naturalness
still remains a challenge in this task, primarily due to the difficulty in
modeling dynamic information in breathy, glissando, and vibrato singing styles.

</details>


### [42] [EMO-RL: Emotion-Rule-Based Reinforcement Learning Enhanced Audio-Language Model for Generalized Speech Emotion Recognition](https://arxiv.org/abs/2509.15654)
*Pengcheng Li,Botao Zhao,Zuheng Kang,Junqing Peng,Xiaoyang Qu,Yayun He,Jianzong Wang*

Main category: cs.SD

TL;DR: EMO-RL是一个结合强化学习的新框架，通过情感相似性加权奖励和显式结构化推理来提升大型音频语言模型在情感计算任务中的表现，特别是在语音情感识别方面取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型在情感计算场景（特别是情感识别、推理和细微情感区分）中的表现仍不理想，且直接应用强化学习技术面临情感边界模糊导致的收敛不稳定性和小模型推理能力有限的问题。

Method: 提出了EMO-RL框架，包含两个关键创新：情感相似性加权奖励(ESWR)和显式结构化推理(ESR)。基于预训练的LALMs，采用带有情感约束的组相对策略优化方法。

Result: 在MELD和IEMOCAP数据集上取得了最先进的结果，跨数据集实验证明了该方法的强泛化能力。

Conclusion: EMO-RL训练策略能显著增强LALMs的情感推理能力，有效解决了强化学习在语音情感识别任务中应用的两个关键挑战。

Abstract: Although Large Audio-Language Models (LALMs) have exhibited outstanding
performance in auditory understanding, their performance in affective computing
scenarios, particularly in emotion recognition, reasoning, and subtle sentiment
differentiation, remains suboptimal. Recent advances in Reinforcement Learning
(RL) have shown promise in improving LALMs' reasoning abilities. However, two
critical challenges hinder the direct application of RL techniques to Speech
Emotion Recognition (SER) tasks: (1) convergence instability caused by
ambiguous emotional boundaries and (2) limited reasoning ability when using
relatively small models (e.g., 7B-parameter architectures). To overcome these
limitations, we introduce EMO-RL, a novel framework incorporating reinforcement
learning with two key innovations: Emotion Similarity-Weighted Reward (ESWR)
and Explicit Structured Reasoning (ESR). Built upon pretrained LALMs, our
method employs group-relative policy optimization with emotion constraints.
Comprehensive experiments demonstrate that our EMO-RL training strategies can
significantly enhance the emotional reasoning capabilities of LALMs, attaining
state-of-the-art results on both the MELD and IEMOCAP datasets, and
cross-dataset experiments prove the strong superiority of generalization.

</details>


### [43] [SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models](https://arxiv.org/abs/2509.15661)
*Qiaolin Wang,Xilin Jiang,Linyang He,Junkai Wu,Nima Mesgarani*

Main category: cs.SD

TL;DR: SightSound-R1是一个跨模态蒸馏框架，通过视觉-语言模型的推理能力来增强音频-语言模型在复杂声音场景中的推理性能


<details>
  <summary>Details</summary>
Motivation: 大型音频-语言模型在复杂声音场景中的推理能力落后于视觉-语言模型，主要瓶颈是缺乏大规模链式思维音频数据

Method: 采用三步蒸馏框架：1）测试时缩放生成音频聚焦的链式思维；2）音频基础验证过滤幻觉；3）监督微调加GRPO优化的蒸馏流程

Result: SightSound-R1在领域内AVQA测试集和未见过的听觉场景中都提升了LALM的推理性能，超越了预训练和仅标签蒸馏的基线

Conclusion: 视觉推理可以有效地转移到音频模型，并通过丰富的音频-视觉数据进行扩展

Abstract: While large audio-language models (LALMs) have demonstrated state-of-the-art
audio understanding, their reasoning capability in complex soundscapes still
falls behind large vision-language models (LVLMs). Compared to the visual
domain, one bottleneck is the lack of large-scale chain-of-thought audio data
to teach LALM stepwise reasoning. To circumvent this data and modality gap, we
present SightSound-R1, a cross-modal distillation framework that transfers
advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the
same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of
three core steps: (i) test-time scaling to generate audio-focused chains of
thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter
hallucinations, and (iii) a distillation pipeline with supervised fine-tuning
(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM
student. Results show that SightSound-R1 improves LALM reasoning performance
both in the in-domain AVQA test set as well as in unseen auditory scenes and
questions, outperforming both pretrained and label-only distilled baselines.
Thus, we conclude that vision reasoning can be effectively transferred to audio
models and scaled with abundant audio-visual data.

</details>


### [44] [TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation](https://arxiv.org/abs/2509.15666)
*Yongsheng Feng,Yuetonghui Xu,Jiehui Luo,Hongjia Liu,Xiaobing Li,Feng Yu,Wei Li*

Main category: cs.SD

TL;DR: TISDiSS是一个可扩展的源分离框架，通过训练时和推理时的动态调整实现速度-性能权衡，减少参数数量同时保持最先进性能。


<details>
  <summary>Details</summary>
Motivation: 源分离任务通常依赖大型网络，导致训练和部署成本高昂。受生成模型中推理时缩放技术的启发，希望开发一个可扩展的框架来平衡性能与效率。

Method: 提出TISDiSS框架，整合早期分割多损失监督、共享参数设计和动态推理重复次数，允许通过调整推理深度实现灵活的速度-性能权衡。

Result: 在标准语音分离基准测试中达到最先进性能，同时减少了参数数量。更多推理重复次数的训练改善了浅层推理性能，有利于低延迟应用。

Conclusion: TISDiSS被确立为一个可扩展且实用的自适应源分离框架，为实际应用提供了有效的速度-性能平衡解决方案。

Abstract: Source separation is a fundamental task in speech, music, and audio
processing, and it also provides cleaner and larger data for training
generative models. However, improving separation performance in practice often
depends on increasingly large networks, inflating training and deployment
costs. Motivated by recent advances in inference-time scaling for generative
modeling, we propose Training-Time and Inference-Time Scalable Discriminative
Source Separation (TISDiSS), a unified framework that integrates early-split
multi-loss supervision, shared-parameter design, and dynamic inference
repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting
inference depth without retraining additional models. We further provide
systematic analyses of architectural and training choices and show that
training with more inference repetitions improves shallow-inference
performance, benefiting low-latency applications. Experiments on standard
speech separation benchmarks demonstrate state-of-the-art performance with a
reduced parameter count, establishing TISDiSS as a scalable and practical
framework for adaptive source separation.

</details>


### [45] [Mamba-2 audio captioning: design space exploration and analysis](https://arxiv.org/abs/2509.15680)
*Taehan Lee,Jaehan Jung,Hyukjun Lee*

Main category: cs.SD

TL;DR: 基于Mamba-2大语言模型构建的音频描述模型，通过系统探索设计空间，在多个基准测试中实现了强大的描述性能，同时使用更少的参数。


<details>
  <summary>Details</summary>
Motivation: 利用Mamba-2的线性时间复杂度优势，探索音频描述任务的最佳模型设计，包括LLM大小、LoRA秩和连接器设计等关键因素。

Method: 基于Mamba-2状态空间模型构建音频描述系统，系统研究LLM参数数量、音频编码器微调策略、音频特征多样性以及特征缩减或扩展技术对性能的影响。

Result: 模型在多个基准测试中表现出色，与在相同数据集上训练的更大语言模型相比，实现了强大的描述性能，同时参数量更少。

Conclusion: 首次深入分析了音频描述任务中各种设计因素对性能的影响，证明了Mamba-2架构在音频描述任务中的有效性，为后续研究提供了重要参考。

Abstract: We present an audio captioning model built on the Mamba-2 large language
model backbone, which is a state-of-the-art (SOTA) state-space model (SSM). We
systematically explore the design space: LLM sizes, LoRA ranks, and connector
designs leveraging Mamba-2's linear-time complexity with respect to sequence
length. Across benchmarks, our models achieve strong captioning performance
compared with larger language models trained on the same dataset, despite using
fewer parameters. For the first time, we conduct an in-depth analysis of how
the number of LLM parameters, audio encoder fine-tuning strategies, audio
feature diversity, and different feature reduction or expansion techniques
affect performance.

</details>


### [46] [Direct Simultaneous Translation Activation for Large Audio-Language Models](https://arxiv.org/abs/2509.15692)
*Pei Zhang,Yiming Wang,Jialong Tang,Baosong Yang,Rui Wang,Derek F. Wong,Fei Huang*

Main category: cs.SD

TL;DR: SimulSA是一种利用大型音频语言模型内在能力，通过随机截断语音和构建部分对齐翻译来获得同步数据的方法，仅需约1%的同步数据即可激活模型的同步语音到文本翻译能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型音频语言模型的兴起，关键挑战是如何在不改变模型架构的情况下直接激活同步语音到文本翻译能力，而不是等待整个话语说完。

Method: 提出SimulSA策略，利用LALMs的固有能力，通过随机截断语音和构建部分对齐翻译来获得同步数据，并将其整合到离线监督微调数据中。

Result: 实验结果表明，仅需约1%的同步数据（相比完整的离线SFT数据）即可显著激活LALMs的Simul-S2TT能力，无需修改模型架构或解码策略。

Conclusion: SimulSA有效弥合了预训练期间离线翻译和推理期间同步翻译之间的分布差距，为激活大型音频语言模型的同步翻译能力提供了一种高效方法。

Abstract: Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech
into target text in real time, outputting translations while receiving source
speech input, rather than waiting for the entire utterance to be spoken.
Simul-S2TT research often modifies model architectures to implement read-write
strategies. However, with the rise of large audio-language models (LALMs), a
key challenge is how to directly activate Simul-S2TT capabilities in base
models without additional architectural changes. In this paper, we introduce
{\bf Simul}taneous {\bf S}elf-{\bf A}ugmentation ({\bf SimulSA}), a strategy
that utilizes LALMs' inherent capabilities to obtain simultaneous data by
randomly truncating speech and constructing partially aligned translation. By
incorporating them into offline SFT data, SimulSA effectively bridges the
distribution gap between offline translation during pretraining and
simultaneous translation during inference. Experimental results demonstrate
that augmenting only about {\bf 1\%} of the simultaneous data, compared to the
full offline SFT data, can significantly activate LALMs' Simul-S2TT
capabilities without modifications to model architecture or decoding strategy.

</details>


### [47] [SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation](https://arxiv.org/abs/2509.15703)
*Yizhou Zhang,Yuan Gao,Wangjin Zhou,Zicheng Yuan,Keisuke Imoto,Tatsuya Kawahara*

Main category: cs.SD

TL;DR: SONAR是一个基于BEATs的持续预训练框架，通过联合采样、正则化和动态扩展tokenizer来解决音频表示学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习模型在静态数据集上训练，无法有效利用新出现的未标记音频数据，而从头重新训练模型计算成本过高且会丢失已有知识。

Method: 提出SONAR框架，采用三个关键技术：新旧数据联合采样策略、平衡特异性和通用性的正则化方法、为新颖声学模式动态扩展tokenizer码本。

Result: 在四个不同领域的实验中，该方法展现出高适应性和对遗忘的强鲁棒性。

Conclusion: SONAR框架能够有效适应新领域同时缓解灾难性遗忘，为音频表示学习的持续预训练提供了可行方案。

Abstract: Self-supervised learning (SSL) on large-scale datasets like AudioSet has
become the dominant paradigm for audio representation learning. While the
continuous influx of new, unlabeled audio presents an opportunity to enrich
these static representations, a naive approach is to retrain the model from
scratch using all available data. However, this method is computationally
prohibitive and discards the valuable knowledge embedded in the previously
trained model weights. To address this inefficiency, we propose SONAR
(Self-distilled cONtinual pre-training for domain adaptive Audio
Representation), a continual pre-training framework built upon BEATs. SONAR
effectively adapts to new domains while mitigating catastrophic forgetting by
tackling three key challenges: implementing a joint sampling strategy for new
and prior data, applying regularization to balance specificity and generality,
and dynamically expanding the tokenizer codebook for novel acoustic patterns.
Experiments across four distinct domains demonstrate that our method achieves
both high adaptability and robust resistance to forgetting.

</details>


### [48] [EmoQ: Speech Emotion Recognition via Speech-Aware Q-Former and Large Language Model](https://arxiv.org/abs/2509.15775)
*Yiqing Yang,Man-Wai Mak*

Main category: cs.SD

TL;DR: 提出EmoQ框架，通过EmoQ-Former融合多模态信息生成查询嵌入，使用多目标情感学习实现协同优化，在IEMOCAP和MELD数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决单模态系统情感信息不足和多模态系统特征对齐困难的问题，以及MLLMs在复杂情感推理中的幻觉和误分类问题

Method: EmoQ框架包括EmoQ-Former生成融合多模态信息的查询嵌入，多目标情感学习实现协同优化，软提示注入策略将多模态表示注入LLM

Result: 在IEMOCAP和MELD数据集上实现了最先进的性能

Conclusion: 为语音情感识别提供了一种新的多模态融合范式

Abstract: The performance of speech emotion recognition (SER) is limited by the
insufficient emotion information in unimodal systems and the feature alignment
difficulties in multimodal systems. Recently, multimodal large language models
(MLLMs) have made progress in SER. However, MLLMs still suffer from
hallucination and misclassification problems in complex emotion reasoning. To
address these problems, we propose an MLLM-based framework called EmoQ, which
generates query embeddings that fuse multimodal information through an
EmoQ-Former and uses multi-objective affective learning (MAL) to achieve
co-optimization. The framework also provides a soft-prompt injection strategy
to inject multimodal representations into the LLM. This end-to-end architecture
achieves state-of-the-art performance on the IEMOCAP and MELD datasets,
providing a new multimodal fusion paradigm for SER.

</details>


### [49] [CompSpoof: A Dataset and Joint Learning Framework for Component-Level Audio Anti-spoofing Countermeasures](https://arxiv.org/abs/2509.15804)
*Xueping Zhang,Liwei Jin,Yechen Wang,Linxi Li,Ming Li*

Main category: cs.SD

TL;DR: 该论文提出了一种新的音频伪造形式——组件级音频伪造（Comp-Spoof），并构建了相应的数据集CompSpoof，同时提出了分离增强的联合学习框架来检测这种伪造方式。


<details>
  <summary>Details</summary>
Motivation: 现有的反伪造数据集和方法将整个语音段视为完全真实或完全伪造，无法准确检测组件级伪造，即只伪造信号中特定组件（如语音或环境声音）的情况。

Method: 提出分离增强的联合学习框架，首先分离音频组件，然后对每个组件分别应用反伪造模型，通过联合学习保留检测相关信息。

Result: 大量实验表明，该方法优于基线方法，证明了分离组件的重要性以及单独检测每个组件伪造的必要性。

Conclusion: 组件级音频伪造是一个重要的安全问题，需要专门的数据集和检测方法，论文提出的框架为解决这一问题提供了有效方案。

Abstract: Component-level audio Spoofing (Comp-Spoof) targets a new form of audio
manipulation where only specific components of a signal, such as speech or
environmental sound, are forged or substituted while other components remain
genuine. Existing anti-spoofing datasets and methods treat an utterance or a
segment as entirely bona fide or entirely spoofed, and thus cannot accurately
detect component-level spoofing. To address this, we construct a new dataset,
CompSpoof, covering multiple combinations of bona fide and spoofed speech and
environmental sound. We further propose a separation-enhanced joint learning
framework that separates audio components apart and applies anti-spoofing
models to each one. Joint learning is employed, preserving information relevant
for detection. Extensive experiments demonstrate that our method outperforms
the baseline, highlighting the necessity of separate components and the
importance of detecting spoofing for each component separately. Datasets and
code are available at: https://github.com/XuepingZhang/CompSpoof.

</details>


### [50] [Differentiable Acoustic Radiance Transfer](https://arxiv.org/abs/2509.15946)
*Sungho Lee,Matteo Scerbo,Seungu Han,Min Jun Choi,Kyogu Lee,Enzo De Sena*

Main category: cs.SD

TL;DR: DART是一种可微分且高效的声学辐射传递（ART）实现，能够通过梯度优化材料属性，在稀疏测量场景下表现出比现有方法更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 几何声学是高效的室内声学建模方法，但现有的声学辐射传递方法缺乏可微分性，无法通过梯度优化来调整材料属性。

Method: 基于声学辐射传递（ART）方法，通过离散化处理时间和方向相关的能量交换，开发了可微分实现DART，支持基于梯度的材料属性优化。

Result: 在声场学习任务中，DART在稀疏测量场景下比传统信号处理和神经网络基线表现出更好的泛化能力，同时保持完全可解释性。

Conclusion: DART作为一个简单且完全可解释的系统，在声学建模中展示了优越的性能，特别是在数据稀缺的情况下。

Abstract: Geometric acoustics is an efficient approach to room acoustics modeling,
governed by the canonical time-dependent rendering equation. Acoustic radiance
transfer (ART) solves the equation through discretization, modeling the time-
and direction-dependent energy exchange between surface patches given with
flexible material properties. We introduce DART, a differentiable and efficient
implementation of ART that enables gradient-based optimization of material
properties. We evaluate DART on a simpler variant of the acoustic field
learning task, which aims to predict the energy responses of novel
source-receiver settings. Experimental results show that DART exhibits
favorable properties, e.g., better generalization under a sparse measurement
scenario, compared to existing signal processing and neural network baselines,
while remaining a simple, fully interpretable system.

</details>


### [51] [From Independence to Interaction: Speaker-Aware Simulation of Multi-Speaker Conversational Timing](https://arxiv.org/abs/2509.15808)
*Máté Gedeon,Péter Mihajlik*

Main category: cs.SD

TL;DR: 提出了一种说话人感知的多说话人对话模拟方法，该方法捕捉时间一致性和真实的轮换动态，相比基线方法能更好地对齐真实对话模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在说话人和轮次间独立性假设下建模聚合对话统计，无法捕捉精细的时间依赖关系和真实的说话人交替模式。

Method: 使用说话人特定的偏差分布确保说话人内部时间一致性，马尔可夫链控制轮换，固定房间脉冲响应保持空间真实性，将停顿和重叠统一为单一间隙分布并使用核密度估计建模。

Result: 在Switchboard数据集上的评估显示，说话人感知模拟在全局间隙统计、连续间隙相关性、高阶依赖关系、轮换熵和间隙生存函数等内在指标上优于基线方法。

Conclusion: 该方法能更好地捕捉精细时间依赖关系和真实说话人交替，但建模长距离对话结构仍存在挑战。

Abstract: We present a speaker-aware approach for simulating multi-speaker
conversations that captures temporal consistency and realistic turn-taking
dynamics. Prior work typically models aggregate conversational statistics under
an independence assumption across speakers and turns. In contrast, our method
uses speaker-specific deviation distributions enforcing intra-speaker temporal
consistency, while a Markov chain governs turn-taking and a fixed room impulse
response preserves spatial realism. We also unify pauses and overlaps into a
single gap distribution, modeled with kernel density estimation for smooth
continuity. Evaluation on Switchboard using intrinsic metrics - global gap
statistics, correlations between consecutive gaps, copula-based higher-order
dependencies, turn-taking entropy, and gap survival functions - shows that
speaker-aware simulation better aligns with real conversational patterns than
the baseline method, capturing fine-grained temporal dependencies and realistic
speaker alternation, while revealing open challenges in modeling long-range
conversational structure.

</details>


### [52] [Reverse Engineering of Music Mixing Graphs with Differentiable Processors and Iterative Pruning](https://arxiv.org/abs/2509.15948)
*Sungho Lee,Marco Martínez-Ramírez,Wei-Hsiang Liao,Stefan Uhlich,Giorgio Fabbro,Kyogu Lee,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 该论文提出了一种用于音乐混音逆向工程的图结构搜索方法，通过构建混合控制台、优化处理器参数、移除不必要处理器等步骤，在保持混音质量的同时减少约三分之二的处理器使用。


<details>
  <summary>Details</summary>
Motivation: 现有音乐混音逆向工程方法未能充分反映混音的组成性质，需要开发能够搜索音频处理器图结构的方法，用于分析音乐混音和构建大规模图数据集以支持下游任务。

Method: 构建混合控制台对所有音轨应用可用处理器，使用可微分处理器实现通过梯度下降优化参数，然后迭代移除可忽略处理器并微调剩余处理器，同时提出高效的批处理方法和利用干/湿参数加速搜索。

Result: 该方法在保持完整混合控制台质量的同时能够移除约三分之二的处理器，通过大量定量和定性分析验证了方法的性能、行为和计算成本。

Conclusion: 所提方法不仅可用于分析单个音乐混音，还能为自动混音等下游任务收集大规模图数据，高效的搜索实现对于实际应用至关重要。

Abstract: Reverse engineering of music mixes aims to uncover how dry source signals are
processed and combined to produce a final mix. We extend the prior works to
reflect the compositional nature of mixing and search for a graph of audio
processors. First, we construct a mixing console, applying all available
processors to every track and subgroup. With differentiable processor
implementations, we optimize their parameters with gradient descent. Then, we
repeat the process of removing negligible processors and fine-tuning the
remaining ones. This way, the quality of the full mixing console can be
preserved while removing approximately two-thirds of the processors. The
proposed method can be used not only to analyze individual music mixes but also
to collect large-scale graph data that can be used for downstream tasks, e.g.,
automatic mixing. Especially for the latter purpose, efficient implementation
of the search is crucial. To this end, we present an efficient batch-processing
method that computes multiple processors in parallel. We also exploit the
"dry/wet" parameter of the processors to accelerate the search. Extensive
quantitative and qualitative analyses are conducted to evaluate the proposed
method's performance, behavior, and computational cost.

</details>


### [53] [DISPATCH: Distilling Selective Patches for Speech Enhancement](https://arxiv.org/abs/2509.15922)
*Dohwan Kim,Jung-Woo Choi*

Main category: cs.SD

TL;DR: DISPatch是一个用于语音增强的知识蒸馏框架，通过选择性应用蒸馏损失到教师模型表现优于学生模型的频谱图区域，避免模仿教师表现差的区域和学生已表现好的区域，从而提高压缩模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法让学生完全模仿教师的输出，这导致学生被迫模仿教师表现差的区域，并对学生已表现好的区域进行蒸馏，收益有限。需要一种选择性蒸馏方法，只在教师真正优于学生的区域进行知识转移。

Method: 提出DISPatch框架，基于知识差距分数选择性地对频谱图区域应用蒸馏损失；进一步提出多尺度选择性补丁(MSSP)方法，针对不同频带使用不同大小的补丁来处理频谱异质性。

Result: 将DISPatch集成到传统知识蒸馏方法中，在紧凑学生模型上观察到一致的性能提升；将DISPatch和MSSP集成到最先进的频域相关知识蒸馏方法中，在所有指标上都显著提高了性能。

Conclusion: DISPatch通过选择性蒸馏策略有效提升了知识蒸馏在语音增强任务中的效果，特别是在处理频谱异质性和避免无效知识转移方面表现出色。

Abstract: In speech enhancement, knowledge distillation (KD) compresses models by
transferring a high-capacity teacher's knowledge to a compact student. However,
conventional KD methods train the student to mimic the teacher's output
entirely, which forces the student to imitate the regions where the teacher
performs poorly and to apply distillation to the regions where the student
already performs well, which yields only marginal gains. We propose Distilling
Selective Patches (DISPatch), a KD framework for speech enhancement that
applies the distillation loss to spectrogram patches where the teacher
outperforms the student, as determined by a Knowledge Gap Score. This approach
guides optimization toward areas with the most significant potential for
student improvement while minimizing the influence of regions where the teacher
may provide unreliable instruction. Furthermore, we introduce Multi-Scale
Selective Patches (MSSP), a frequency-dependent method that uses different
patch sizes across low- and high-frequency bands to account for spectral
heterogeneity. We incorporate DISPatch into conventional KD methods and observe
consistent gains in compact students. Moreover, integrating DISPatch and MSSP
into a state-of-the-art frequency-dependent KD method considerably improves
performance across all metrics.

</details>


### [54] [Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement](https://arxiv.org/abs/2509.15952)
*Gang Yang,Yue Lei,Wenxin Tai,Jin Wu,Jia Chen,Ting Zhong,Fan Zhou*

Main category: cs.SD

TL;DR: COSE是一个用于语音增强的单步流匹配框架，通过引入速度组合恒等式来高效计算平均速度，避免了昂贵的雅可比向量积计算，实现了5倍更快的采样速度和40%的训练成本降低。


<details>
  <summary>Details</summary>
Motivation: 扩散和流匹配模型在语音增强方面取得了显著进展，但多步生成依赖计算成本高且容易受到离散化误差影响。MeanFlow等单步生成模型通过平均速度场重新表述动力学，提供了有前景的替代方案。

Method: 提出了COSE框架，引入速度组合恒等式来高效计算平均速度，消除了昂贵的雅可比向量积计算，同时保持理论一致性。

Result: 在标准基准测试上的广泛实验表明，COSE实现了高达5倍的更快采样速度，训练成本降低40%，且不损害语音质量。

Conclusion: COSE提供了一个高效的单步流匹配解决方案，在保持语音增强质量的同时显著降低了计算成本。

Abstract: Diffusion and flow matching (FM) models have achieved remarkable progress in
speech enhancement (SE), yet their dependence on multi-step generation is
computationally expensive and vulnerable to discretization errors. Recent
advances in one-step generative modeling, particularly MeanFlow, provide a
promising alternative by reformulating dynamics through average velocity
fields. In this work, we present COSE, a one-step FM framework tailored for SE.
To address the high training overhead of Jacobian-vector product (JVP)
computations in MeanFlow, we introduce a velocity composition identity to
compute average velocity efficiently, eliminating expensive computation while
preserving theoretical consistency and achieving competitive enhancement
quality. Extensive experiments on standard benchmarks show that COSE delivers
up to 5x faster sampling and reduces training cost by 40%, all without
compromising speech quality. Code is available at
https://github.com/ICDM-UESTC/COSE.

</details>


### [55] [Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation](https://arxiv.org/abs/2509.16010)
*Qi Wang,Shituo Ma,Guoxin Yu,Hanyang Peng,Yue Yu*

Main category: cs.SD

TL;DR: Fed-PISA是一个联邦学习框架，用于语音克隆任务，通过解耦的LoRA机制降低通信成本，并利用协同过滤聚合方法提升个性化表现。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在语音克隆中存在高通信成本和抑制风格异质性的问题，导致个性化不足。

Method: 提出Fed-PISA框架：1）使用解耦的LoRA机制，本地保留说话人音色（ID-LoRA），仅传输轻量级风格LoRA；2）基于协同过滤的聚合方法，为每个客户端创建定制模型。

Result: 实验表明Fed-PISA在风格表现力、自然度和说话人相似度方面优于标准联邦基线，且通信成本极低。

Conclusion: Fed-PISA有效解决了联邦语音克隆中的通信成本和个性化不足问题，实现了更好的个性化语音生成效果。

Abstract: Voice cloning for Text-to-Speech (TTS) aims to generate expressive and
personalized speech from text using limited data from a target speaker.
Federated Learning (FL) offers a collaborative and privacy-preserving framework
for this task, but existing approaches suffer from high communication costs and
tend to suppress stylistic heterogeneity, resulting in insufficient
personalization. To address these issues, we propose Fed-PISA, which stands for
Federated Personalized Identity-Style Adaptation. To minimize communication
costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:
the speaker's timbre is retained locally through a private ID-LoRA, while only
a lightweight style-LoRA is transmitted to the server, thereby minimizing
parameter exchange. To harness heterogeneity, our aggregation method, inspired
by collaborative filtering, is introduced to create custom models for each
client by learning from stylistically similar peers. Experiments show that
Fed-PISA improves style expressivity, naturalness, and speaker similarity,
outperforming standard federated baselines with minimal communication costs.

</details>


### [56] [FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation](https://arxiv.org/abs/2509.16195)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.SD

TL;DR: FocalCodec-Stream是一个基于焦点调制的混合音频编解码器，能够在0.55-0.80 kbps的极低比特率下压缩语音，理论延迟为80毫秒，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有的神经音频编解码器虽然重建质量好且为下游任务提供强大表示，但大多数不支持流式处理，限制了在实时应用中的使用。

Method: 结合WavLM的多阶段因果蒸馏和针对性架构改进，包括轻量级精炼模块来在延迟约束下提升质量。基于焦点调制构建混合编解码器。

Result: 实验表明FocalCodec-Stream在可比比特率下优于现有流式编解码器，同时保留了语义和声学信息。

Conclusion: 该方法在重建质量、下游任务性能、延迟和效率之间实现了有利的权衡。

Abstract: Neural audio codecs are a fundamental component of modern generative audio
pipelines. Although recent codecs achieve strong low-bitrate reconstruction and
provide powerful representations for downstream tasks, most are non-streamable,
limiting their use in real-time applications. We present FocalCodec-Stream, a
hybrid codec based on focal modulation that compresses speech into a single
binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our
approach combines multi-stage causal distillation of WavLM with targeted
architectural improvements, including a lightweight refiner module that
enhances quality under latency constraints. Experiments show that
FocalCodec-Stream outperforms existing streamable codecs at comparable
bitrates, while preserving both semantic and acoustic information. The result
is a favorable trade-off between reconstruction quality, downstream task
performance, latency, and efficiency. Code and checkpoints will be released at
https://github.com/lucadellalib/focalcodec.

</details>

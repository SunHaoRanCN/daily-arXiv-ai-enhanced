{"id": "2509.16223", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16223", "abs": "https://arxiv.org/abs/2509.16223", "authors": ["Huaiyu Chen", "Fahed Hassanat", "Robert Laganiere", "Martin Bouchard"], "title": "MRADNET: a Compact Radar Object Detector with MetaFormer", "comment": "5 pages, 2 figures, submitted to IEEE Icassp 2026", "summary": "Frequency-modulated continuous wave radars have gained increasing popularity\nin the automotive industry. Its robustness against adverse weather conditions\nmakes it a suitable choice for radar object detection in advanced driver\nassistance systems. These real-time embedded systems have requirements for the\ncompactness and efficiency of the model, which have been largely overlooked in\nprevious work. In this work, we propose mRadNet, a novel radar object detection\nmodel with compactness in mind. mRadNet employs a U-net style architecture with\nMetaFormer blocks, in which separable convolution and attention token mixers\nare used to capture both local and global features effectively. More efficient\ntoken embedding and merging strategies are introduced to further facilitate the\nlightweight design of the model. The performance of mRadNet is validated on the\nCRUW dataset, improving state-of-the-art performance."}
{"id": "2509.16359", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16359", "abs": "https://arxiv.org/abs/2509.16359", "authors": ["David Campos Anchieta", "John R. Buck"], "title": "Power Spectral Density Estimation via Universal Truncated Order Statistics Filtering", "comment": null, "summary": "Loud transient signals in underwater acoustic data increase the bias and\nvariance of background noise power spectral density (PSD) estimates based on\nsample mean. Recently, two PSD estimators mitigated the loud transient impact\non PSD estimates by applying order statistics filtering (OSF). The first, the\nSchwock and Abadi Welch Percentile, scales a single rank order statistic (OS)\nof consecutive periodograms. The second, the truncated linear order statistics\nfilter, is a weighted sum of OS up to a chosen rank. In order to minimize\nvariance, both OSFs must carefully choose the highest rank that still\neliminates the loud transients. However, in real-time applications in dynamic\nenvironments, loud transients occur at unpredictable rates, requiring dynamic\nadjustment of the OSF ranks to keep low bias and variance. To circumvent the\nchallenges of real-time rank selection, this paper proposes a convex sum of\nOSFs across ranks with blending weights that are sequentially adjusted to favor\nthe lowest variance OSFs over a recent time window. The performance of the\nblended sum provably approaches the performance of the best fixed rank OSF.\nSimulations and real data confirm the blended OSFs effectively filter loud\ntransients out of spectrograms without explicitly choosing a threshold rank."}
{"id": "2509.16417", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16417", "abs": "https://arxiv.org/abs/2509.16417", "authors": ["Ayla Eftekhari", "Maryam Cheraghy", "Armin Farhadi", "Mohammad Robat Mili", "Qingqing Wu"], "title": "Hybrid FIM and STAR-BD-RIS-Aided Wireless Communications with Short Packet Length: A Meta-TD3 Approach", "comment": null, "summary": "Reconfigurable intelligent surfaces (RIS) and flexible intelligent\nmetasurfaces (FIM) have been widely adopted in multi-user wireless\ncommunication systems to enhance channel quality through simultaneous\ntransmission and reflection of signals and three-dimensional reconfiguration of\nantennas. In this paper, we propose a novel system architecture that integrates\nthe benefits of both technologies by deploying an FIM antenna at the base\nstation (BS) and a simultaneously transmitting and reflecting beyond diagonal\nRIS (STAR-BD-RIS) along the transmission path to ensure sufficient received\npower for single-antenna users. The objective is to maximize the achievable sum\nrate considering the short block length by jointly optimizing the FIM surface\nconfiguration, the transmit beamforming vector, and STAR-BD-RIS phase shift\nmatrix subject to practical constraints including minimum\nsignal-to-interference-plus-noise ratio (SINR), power limitations, FIM\nconstraint, and the STAR-BD-RIS phase-shift matrix. To solve the resulting\nnon-convex optimization problem, we develop a learning-based approach that\nincorporates meta-learning into the twin delayed deep deterministic policy\ngradient (TD3) algorithm, referred to as Meta-TD3. The simulation results\ndemonstrate that the proposed hybrid system outperforms conventional\nconfigurations employing either FIM or RIS alone, while the Meta-TD3 algorithm\nachieves superior performance compared to classic learning techniques."}
{"id": "2509.16536", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.16536", "abs": "https://arxiv.org/abs/2509.16536", "authors": ["Johannes Mootz", "Reza Akhavian"], "title": "Advancing Accessible Hand-Arm Vibration Safety Monitoring: ISO-Compliance with Wearable Sensors and Transfer Functions", "comment": null, "summary": "Field workers are frequently exposed to hazardous vibrations, increasing the\nrisk of Hand-Arm Vibration Syndrome (HAVS) and other long-term health problems.\nISO 5349-1 provides guidelines for measuring vibration exposure. However, this\nstandard was established in controlled conditions using high-quality\naccelerometers directly attached to power tool handles. This study investigates\nan alternative, wearable sensor-based data collection process and develops an\nerror-minimization transfer function that derives values comparable to ISO\nbenchmarks for safety monitoring. Experiments are performed with subjects\nhammer drilling into concrete while vibrations are measured using three\naccelerometers at different sampling frequencies. The transfer function maps\nvibration data across sensor positions by accounting for damping effects. The\nfindings indicate a significant reduction in acceleration between the palm and\nupper arm, highlight the impact of sampling frequency on data accuracy, and\nenable accurate comparison of true hand-arm vibration levels with existing\nstandard limits to allow accessible, real-time, and cost-effective HAVS\nprevention."}
{"id": "2509.16329", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16329", "abs": "https://arxiv.org/abs/2509.16329", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Panchal Nayak", "Priyabrata Mallick", "Swarup Ranjan Behera", "Parabattina Bhagath", "Pailla Balakrishna Reddy", "Arun Balaji Buduru"], "title": "Investigating Polyglot Speech Foundation Models for Learning Collective Emotion from Crowds", "comment": "Accepted to APSIPA-ASC 2025", "summary": "This paper investigates the polyglot (multilingual) speech foundation models\n(SFMs) for Crowd Emotion Recognition (CER). We hypothesize that polyglot SFMs,\npre-trained on diverse languages, accents, and speech patterns, are\nparticularly adept at navigating the noisy and complex acoustic environments\ncharacteristic of crowd settings, thereby offering a significant advantage for\nCER. To substantiate this, we perform a comprehensive analysis, comparing\npolyglot, monolingual, and speaker recognition SFMs through extensive\nexperiments on a benchmark CER dataset across varying audio durations (1 sec,\n500 ms, and 250 ms). The results consistently demonstrate the superiority of\npolyglot SFMs, outperforming their counterparts across all audio lengths and\nexcelling even with extremely short-duration inputs. These findings pave the\nway for adaptation of SFMs in setting up new benchmarks for CER."}
{"id": "2509.16522", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16522", "abs": "https://arxiv.org/abs/2509.16522", "authors": ["Tse-Yang Chen", "Yuh-Jzer Joung"], "title": "Etude: Piano Cover Generation with a Three-Stage Approach - Extract, strucTUralize, and DEcode", "comment": null, "summary": "Piano cover generation aims to automatically transform a pop song into a\npiano arrangement. While numerous deep learning approaches have been proposed,\nexisting models often fail to maintain structural consistency with the original\nsong, likely due to the absence of beat-aware mechanisms or the difficulty of\nmodeling complex rhythmic patterns. Rhythmic information is crucial, as it\ndefines structural similarity (e.g., tempo, BPM) and directly impacts the\noverall quality of the generated music.\n  In this paper, we introduce Etude, a three-stage architecture consisting of\nExtract, strucTUralize, and DEcode stages. By pre-extracting rhythmic\ninformation and applying a novel, simplified REMI-based tokenization, our model\nproduces covers that preserve proper song structure, enhance fluency and\nmusical dynamics, and support highly controllable generation through style\ninjection. Subjective evaluations with human listeners show that Etude\nsubstantially outperforms prior models, achieving a quality level comparable to\nthat of human composers."}
{"id": "2509.16570", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16570", "abs": "https://arxiv.org/abs/2509.16570", "authors": ["Rohit Kumar Singh", "Subrata Kumar", "Shovan Bhaumik"], "title": "Bearing-only Tracking using Towed Sensor-Array with Non-Gaussian Measurement Noise Statistics", "comment": null, "summary": "Passive bearing-only tracking (BOT) estimates the target states by utilising\nnoisy bearing measurements captured by a sensor array. The sensor array is\noften towed behind the ship, using a long flexible cable to reduce interference\nfrom the own-ship's inherent noises. This forms a towed cable sensor-array\nsystem (TCSAS). During BOT, the tow-ship has to perform a manoeuvre to make the\ntracking system observable. Such a manoeuvre destabilises the TCSAS, thus\nmaking its exact location unknown \\emph{w.r.t.} tow-ship. However, it is very\ncrucial to know the exact location of the towed sensor-array to perform\nefficient and reliable target state estimation. The existing BOT approaches\nperform TMA during own-ship manoeuvre either by pausing the measurement\nupdation step of the estimation algorithm or assuming a fixed aft position for\nthe towed sensor-array. These assumptions lead to unreliable state estimation.\nTo address this, we propose a dynamic model for TCSAS, using a lumped mass\napproach, which will provide the location of the sensor array during the\nown-ship manoeuvre. This location will be fed to the state estimation\nalgorithm. The dynamic of TCSAS in 3D space is obtained by solving the\nequations obtained from the moment balance condition and quasi-static\nequilibrium condition at the lumped mass points. Moreover, the bearing data\ncaptured by the towed sensor-array is corrupted with non-Gaussian noise. It is\nhandled using the maximum correntropy criterion based Kalman filter with a\nkernel bandwidth selection technique, proposed in this paper. The proposed\nsensor-array dynamic model is verified for a real-world BOT engagement\nscenario."}
{"id": "2509.16342", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16342", "abs": "https://arxiv.org/abs/2509.16342", "authors": ["Sean Turland", "Eloi Moliner", "Vesa Välimäki"], "title": "Similarity-Guided Diffusion for Long-Gap Music Inpainting", "comment": "5 pages, 2 figures. Submitted to IEEE ICASSP 2026. Audio examples and\n  supplementary material are available at: https://s-turland.github.io/SimDPS/", "summary": "Music inpainting aims to reconstruct missing segments of a corrupted\nrecording. While diffusion-based generative models improve reconstruction for\nmedium-length gaps, they often struggle to preserve musical plausibility over\nmulti-second gaps. We introduce Similarity-Guided Diffusion Posterior Sampling\n(SimDPS), a hybrid method that combines diffusion-based inference with\nsimilarity search. Candidate segments are first retrieved from a corpus based\non contextual similarity, then incorporated into a modified likelihood that\nguides the diffusion process toward contextually consistent reconstructions.\nSubjective evaluation on piano music inpainting with 2-s gaps shows that the\nproposed SimDPS method enhances perceptual plausibility compared to unguided\ndiffusion and frequently outperforms similarity search alone when moderately\nsimilar candidates are available. These results demonstrate the potential of a\nhybrid similarity approach for diffusion-based audio enhancement with long\ngaps."}
{"id": "2509.16566", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16566", "abs": "https://arxiv.org/abs/2509.16566", "authors": ["Omar Eldeeb", "Martin Malandro"], "title": "Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks", "comment": null, "summary": "Current methods for Music Structure Analysis (MSA) focus primarily on audio\ndata. While symbolic music can be synthesized into audio and analyzed using\nexisting MSA techniques, such an approach does not exploit symbolic music's\nrich explicit representation of pitch, timing, and instrumentation. A key\nsubproblem of MSA is section boundary detection-determining whether a given\npoint in time marks the transition between musical sections. In this paper, we\nstudy automatic section boundary detection for symbolic music. First, we\nintroduce a human-annotated MIDI dataset for section boundary detection,\nconsisting of metadata from 6134 MIDI files that we manually curated from the\nLakh MIDI dataset. Second, we train a deep learning model to classify the\npresence of section boundaries within a fixed-length musical window. Our data\nrepresentation involves a novel encoding scheme based on synthesized overtones\nto encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model\nachieves an F1 score of 0.77, improving over the analogous audio-based\nsupervised learning approach and the unsupervised block-matching segmentation\n(CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset,\ncode, and models."}
{"id": "2509.16580", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16580", "abs": "https://arxiv.org/abs/2509.16580", "authors": ["Dilshara Herath", "Chinthaka Abeyrathne", "Chamindu Adithya", "Chathura Seneviratne"], "title": "Fusing Spectral Correlation Density Imaging with Deep Learning for Intelligent Fault Diagnosis in Rotating Machinery", "comment": null, "summary": "Bearing fault diagnosis in rotating machinery is critical for ensuring\noperational reliability, therefore early fault detection is essential to avoid\ncatastrophic failures and expensive emergency repairs. Traditional methods like\nFast Fourier Transform (FFT) often fail to capture the complex, non-stationary\nnature of vibration signals. This study leverages the cyclostationary\nproperties of vibration data through Spectral Correlation Density (SCD) images\nto enhance fault detection and apply deep learning for classification. Using a\npublicly available dataset with bearing faults seeded in two distinct housings\n(A and B) under varying load conditions (0 Nm, 2 Nm, 4 Nm), we processed\nvibration signals into 2D SCD images to reveal fault-specific periodicities,\nsuch as broadband spectra (2000--8000 Hz) for larger faults. Three\nconvolutional neural network (CNN) models, Custom CNN, ResNet152V2, and\nEfficientNetB0, were developed to classify seven bearing conditions. The custom\nCNN achieved the highest accuracies of 96.58\\% and 94.95\\% on Housing A and B,\nrespectively, followed by ResNet152V2 at 96.49\\% and 95.35\\%, and\nEfficientNetB0 at 94.16\\% and 91.65\\%, respectively. The models' high\naccuracies across different housings demonstrate a robust solution suitable for\ncost-effective condition monitoring deployable near sensing platforms,\ncontributing to applied machine learning for edge intelligence and showcasing\neffective signal processing strategies for handling complex, potentially\nlarge-scale vibration data."}
{"id": "2509.16358", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16358", "abs": "https://arxiv.org/abs/2509.16358", "authors": ["Jesper Brunnström", "Martin Bo Møller", "Jan Østergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Sound field estimation with moving microphones using kernel ridge regression", "comment": null, "summary": "Sound field estimation with moving microphones can increase flexibility,\ndecrease measurement time, and reduce equipment constraints compared to using\nstationary microphones. In this paper a sound field estimation method based on\nkernel ridge regression (KRR) is proposed for moving microphones. The proposed\nKRR method is constructed using a discrete time continuous space sound field\nmodel based on the discrete Fourier transform and the Herglotz wave function.\nThe proposed method allows for the inclusion of prior knowledge as a\nregularization penalty, similar to kernel-based methods with stationary\nmicrophones, which is novel for moving microphones. Using a directional\nweighting for the proposed method, the sound field estimates are improved,\nwhich is demonstrated on both simulated and real data. Due to the high\ncomputational cost of sound field estimation with moving microphones, an\napproximate KRR method is proposed, using random Fourier features (RFF) to\napproximate the kernel. The RFF method is shown to decrease computational cost\nwhile obtaining less accurate estimates compared to KRR, providing a trade-off\nbetween cost and performance."}
{"id": "2509.16649", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16649", "abs": "https://arxiv.org/abs/2509.16649", "authors": ["Hyun Jun Kim", "Hyeong Yong Choi", "Changwon Lim"], "title": "AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval", "comment": "5 pages, 1 figure, DCASE2025 Task2 technical report", "summary": "This report presents the AISTAT team's submission to the language-based audio\nretrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder\narchitecture, where audio and text modalities are encoded separately, and their\nrepresentations are aligned using contrastive learning. Drawing inspiration\nfrom methodologies of the previous year's challenge, we implemented a\ndistillation approach and leveraged large language models (LLMs) for effective\ndata augmentation techniques, including back-translation and LLM mix.\nAdditionally, we incorporated clustering to introduce an auxiliary\nclassification task for further finetuning. Our best single system achieved a\nmAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on\nthe Clotho development test split."}
{"id": "2509.16585", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.16585", "abs": "https://arxiv.org/abs/2509.16585", "authors": ["Ta Giang Thuy Loan", "Hoang-Lan Nguyen", "Nguyen Thi Ngoc Lan", "Do Hai Son", "Tran Thi Thuy Quynh", "Nguyen Linh Trung", "Karim Abed-Meraim", "Thanh Trung Le"], "title": "Robust Sparse Subspace Tracking from Corrupted Data Observations", "comment": null, "summary": "Subspace tracking is a fundamental problem in signal processing, where the\ngoal is to estimate and track the underlying subspace that spans a sequence of\ndata streams over time. In high-dimensional settings, data samples are often\ncorrupted by non-Gaussian noises and may exhibit sparsity. This paper explores\nthe alpha divergence for sparse subspace estimation and tracking, offering\nrobustness to data corruption. The proposed method outperforms the\nstate-of-the-art robust subspace tracking methods while achieving a low\ncomputational complexity and memory storage. Several experiments are conducted\nto demonstrate its effectiveness in robust subspace tracking and\ndirection-of-arrival (DOA) estimation."}
{"id": "2509.16480", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16480", "abs": "https://arxiv.org/abs/2509.16480", "authors": ["Anup Singh", "Kris Demuynck"], "title": "Harmonic Summation-Based Robust Pitch Estimation in Noisy and Reverberant Environments", "comment": null, "summary": "Accurate pitch estimation is essential for numerous speech processing\napplications, yet it remains challenging in high-distortion environments. This\npaper proposes a robust pitch estimation method that delivers robust pitch\nestimates in challenging noise environments. Our approach computes the\nNormalized Average Magnitude Difference Function (NAMDF), transforms it into a\nlikelihood function, and generates probabilistic pitch states for frames at\neach sample shift. To enhance noise robustness, we aggregate likelihood values\nacross integer multiples of the pitch period and neighboring frames.\nFurthermore, we introduce a simple yet effective continuity constraint in the\nViterbi algorithm to refine pitch selection among multiple candidates.\nExperimental results show that our method consistently achieves lower Gross\nPitch Error (GPE) and Voicing Decision Error (VDE) across various SNR levels,\noutperforming existing methods in both noisy and reverberant conditions."}
{"id": "2509.16662", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16662", "abs": "https://arxiv.org/abs/2509.16662", "authors": ["Eunjin Choi", "Hyerin Kim", "Jiwoo Ryu", "Juhan Nam", "Dasaem Jeong"], "title": "On the de-duplication of the Lakh MIDI dataset", "comment": "The paper has been accepted for publication at ISMIR 2025", "summary": "A large-scale dataset is essential for training a well-generalized\ndeep-learning model. Most such datasets are collected via scraping from various\ninternet sources, inevitably introducing duplicated data. In the symbolic music\ndomain, these duplicates often come from multiple user arrangements and\nmetadata changes after simple editing. However, despite critical issues such as\nunreliable training evaluation from data leakage during random splitting,\ndataset duplication has not been extensively addressed in the MIR community.\nThis study investigates the dataset duplication issues regarding Lakh MIDI\nDataset (LMD), one of the largest publicly available sources in the symbolic\nmusic domain. To find and evaluate the best retrieval method for duplicated\ndata, we employed the Clean MIDI subset of the LMD as a benchmark test set, in\nwhich different versions of the same songs are grouped together. We first\nevaluated rule-based approaches and previous symbolic music retrieval models\nfor de-duplication and also investigated with a contrastive learning-based BERT\nmodel with various augmentations to find duplicate files. As a result, we\npropose three different versions of the filtered list of LMD, which filters out\nat least 38,134 samples in the most conservative settings among 178,561 files."}
{"id": "2509.16643", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16643", "abs": "https://arxiv.org/abs/2509.16643", "authors": ["Yu Zhou", "Chao Zou", "Nanhao Zhou", "Yanqun Tang", "Xiaoying Zhang", "Haoran Yin", "Xiaoran Liu", "Ruisi He", "Pan Tang", "Weijie Yuan", "Yong Zeng"], "title": "Affine Frequency Division Multiplexing for Communication and Channel Sounding: Requirements, Challenges, and Key Technologies", "comment": "Under revision in an IEEE Magazine", "summary": "Channel models are crucial for theoretical analysis, performance evaluation,\nand deployment of wireless communication systems. Traditional channel sounding\nsystems are insufficient for handling the dynamic changes of channels in the\nnext-generation space-air-ground-sea integrated networks (SAGSIN), which often\nresults in outdated channel models that fail to provide reliable prior\ninformation for communication systems. To address this challenge, this paper\nproposes an integrated channel sounding and communication (ICSC) method as a\npractical solution. Unlike orthogonal frequency division multiplexing, affine\nfrequency division multiplexing (AFDM) provides a full delay-Doppler\nrepresentation of the channel, achieving optimal diversity in time-frequency\ndoubly dispersive channels and effectively addressing the aforementioned\nchallenges. Thus, we investigate the fundamental principles of AFDM, showing\nhow it enables simultaneous communication and channel sounding, and explore key\nperformance metrics for both functionalities. We also clarify the distinction\nand relationship between channel sounding, estimation, tracking and scatterer\nsensing. Additionally, several potential application scenarios for AFDM-ICSC\nare explored. Finally, we highlight the key challenges in implementing\nAFDM-ICSC, outline future research directions, and provide valuable insights\nfor the continued development of this technology."}
{"id": "2509.16481", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16481", "abs": "https://arxiv.org/abs/2509.16481", "authors": ["Ui-Hyeop Shin", "Bon Hyeok Ku", "Hyung-Min Park"], "title": "TF-CorrNet: Leveraging Spatial Correlation for Continuous Speech Separation", "comment": "Accepted in SPL", "summary": "In general, multi-channel source separation has utilized inter-microphone\nphase differences (IPDs) concatenated with magnitude information in\ntime-frequency domain, or real and imaginary components stacked along the\nchannel axis. However, the spatial information of a sound source is\nfundamentally contained in the differences between microphones, specifically in\nthe correlation between them, while the power of each microphone also provides\nvaluable information about the source spectrum, which is why the magnitude is\nalso included. Therefore, we propose a network that directly leverages a\ncorrelation input with phase transform (PHAT)-beta to estimate the separation\nfilter. In addition, the proposed TF-CorrNet processes the features alternately\nacross time and frequency axes as a dual-path strategy in terms of spatial\ninformation. Furthermore, we add a spectral module to model source-related\ndirect time-frequency patterns for improved speech separation. Experimental\nresults demonstrate that the proposed TF-CorrNet effectively separates the\nspeech sounds, showing high performance with a low computational cost in the\nLibriCSS dataset."}
{"id": "2509.16670", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16670", "abs": "https://arxiv.org/abs/2509.16670", "authors": ["Wenhuan Lu", "Xinyue Song", "Wenjun Ke", "Zhizhi Yu", "Wenhao Yang", "Jianguo Wei"], "title": "Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection", "comment": null, "summary": "Audio grounding, or speech-driven open-set object detection, aims to localize\nand identify objects directly from speech, enabling generalization beyond\npredefined categories. This task is crucial for applications like human-robot\ninteraction where textual input is impractical. However, progress in this\ndomain faces a fundamental bottleneck from the scarcity of large-scale, paired\naudio-image data, and is further constrained by previous methods that rely on\nindirect, text-mediated pipelines. In this paper, we introduce Speech-to-See\n(Speech2See), an end-to-end approach built on a pre-training and fine-tuning\nparadigm. Specifically, in the pre-training stage, we design a Query-Guided\nSemantic Aggregation module that employs learnable queries to condense\nredundant speech embeddings into compact semantic representations. During\nfine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts\n(MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation.\nExtensive experiments show that Speech2See achieves robust and adaptable\nperformance across multiple benchmarks, demonstrating its strong generalization\nability and broad applicability."}
{"id": "2509.16688", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.16688", "abs": "https://arxiv.org/abs/2509.16688", "authors": ["Özlem Tuğfe Demir", "Emil Björnson"], "title": "Near-Field Channel Estimation with ELAA Modular Arrays Under Hardware Impairments", "comment": "7 pages, 5 figures, IEEE PIMRC 2025", "summary": "Extremely large-scale antenna arrays (ELAAs) enable high spatial resolution\nand multiplexing, especially for user equipments (UEs) in the radiative\nnear-field. To reduce hardware cost, modular ELAA architectures with\ndistributed baseband units (BBUs) are gaining traction. This paper addresses\nnear-field line-of-sight (LOS) channel estimation under low noise amplifier\n(LNA)-induced hardware impairments in such modular systems. We propose\ncomputationally efficient estimators that exploit the array geometry and\nconstant-modulus structure of near-field LOS channels, including a novel\ntwo-dimensional (2D) discrete Fourier transform (DFT) masking technique that\nimproves estimation accuracy and significantly reduces fronthaul signaling.\nNumerical results show that the proposed methods significantly outperform the\nconventional least squares (LS) method."}
{"id": "2509.16603", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16603", "abs": "https://arxiv.org/abs/2509.16603", "authors": ["Maurício do V. M. da Costa", "Eloi Moliner"], "title": "An Octave-based Multi-Resolution CQT Architecture for Diffusion-based Audio Generation", "comment": "accepted at IEEE International Symposium on the Internet of Sounds", "summary": "This paper introduces MR-CQTdiff, a novel neural-network architecture for\ndiffusion-based audio generation that leverages a multi-resolution Constant-$Q$\nTransform (C$Q$T). The proposed architecture employs an efficient, invertible\nCQT framework that adjusts the time-frequency resolution on an octave-by-octave\nbasis. This design addresses the issue of low temporal resolution at lower\nfrequencies, enabling more flexible and expressive audio generation. We conduct\nan evaluation using the Fr\\'echet Audio Distance (FAD) metric across various\narchitectures and two datasets. Experimental results demonstrate that\nMR-CQTdiff achieves state-of-the-art audio quality, outperforming competing\narchitectures."}
{"id": "2509.16718", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16718", "abs": "https://arxiv.org/abs/2509.16718", "authors": ["Vishnu Raja", "Adithya V Ganesan", "Anand Syamkumar", "Ritwik Banerjee", "H Andrew Schwartz"], "title": "Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies", "comment": "Will appear in EMNLP 2025 Main Proceedings", "summary": "State-of-the-art automatic speech recognition (ASR) models like Whisper,\nperform poorly on atypical speech, such as that produced by individuals with\ndysarthria. Past works for atypical speech have mostly investigated fully\npersonalized (or idiosyncratic) models, but modeling strategies that can both\ngeneralize and handle idiosyncracy could be more effective for capturing\natypical speech. To investigate this, we compare four strategies: (a)\n$\\textit{normative}$ models trained on typical speech (no personalization), (b)\n$\\textit{idiosyncratic}$ models completely personalized to individuals, (c)\n$\\textit{dysarthric-normative}$ models trained on other dysarthric speakers,\nand (d) $\\textit{dysarthric-idiosyncratic}$ models which combine strategies by\nfirst modeling normative patterns before adapting to individual speech. In this\ncase study, we find the dysarthric-idiosyncratic model performs better than\nidiosyncratic approach while requiring less than half as much personalized data\n(36.43 WER with 128 train size vs 36.99 with 256). Further, we found that\ntuning the speech encoder alone (as opposed to the LM decoder) yielded the best\nresults reducing word error rate from 71% to 32% on average. Our findings\nhighlight the value of leveraging both normative (cross-speaker) and\nidiosyncratic (speaker-specific) patterns to improve ASR for underrepresented\nspeech populations."}
{"id": "2509.16776", "categories": ["eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.16776", "abs": "https://arxiv.org/abs/2509.16776", "authors": ["Hassaan Hashmi", "Spyridon Pougkakiotis", "Dionysis Kalogerias"], "title": "Data-Driven Two-Stage IRS-Aided Sumrate Maximization with Inexact Precoding", "comment": null, "summary": "We propose iZoSGA, a data-driven learning algorithm for joint passive\nlong-term intelligent reflective surface (IRS)-aided beamforming and active\nshort-term precoding in wireless networks. iZoSGA is based on a zeroth-order\nstochastic quasigradient ascent methodology designed for tackling two-stage\nnonconvex stochastic programs with continuous uncertainty and objective\nfunctions with \"black-box\" terms, and where second-stage optimization is\ninexact. As such, iZoSGA utilizes inexact precoding oracles, enabling practical\nimplementation when short-term (e.g., WMMSE-based) beamforming is solved\napproximately. The proposed method is agnostic to channel models or statistics,\nand applies to arbitrary IRS/network configurations. We prove non-asymptotic\nconvergence of iZoSGA to a neighborhood of a stationary solution of the\noriginal exact problem under minimal assumptions. Our numerics confirm the\nefficacy iZoSGA in several \"inexact regimes\", enabling passive yet fully\neffective IRS operation in diverse and realistic IRS-aided scenarios."}
{"id": "2509.16622", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16622", "abs": "https://arxiv.org/abs/2509.16622", "authors": ["Mengqi Wang", "Zhan Liu", "Zengrui Jin", "Guangzhi Sun", "Chao Zhang", "Philip C. Woodland"], "title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing", "comment": null, "summary": "Diffusion-based large language models (DLLMs) have recently attracted growing\ninterest as an alternative to autoregressive decoders. In this work, we present\nan empirical study on using the diffusion-based large language model LLaDA for\nautomatic speech recognition (ASR). We first investigate its use as an external\ndeliberation-based processing module for Whisper-LLaMA transcripts. By\nleveraging the bidirectional attention and denoising capabilities of LLaDA, we\nexplore random masking, low-confidence masking, and semi-autoregressive\nstrategies, showing that Whisper-LLaDA substantially reduces WER compared with\nthe baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER\non test-clean/test-other, representing a 12.3% relative improvement over the\nWhisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA\nwithout acoustic features fails to improve accuracy, highlighting the\nimportance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA\nas a standalone decoder for ASR with diffusion-based and semi-autoregressive\ndecoding. Most experimental configurations achieve faster inference than the\nWhisper-LLaMA baseline, although recognition accuracy is slightly lower. These\nfindings offer an empirical view of diffusion-based LLMs for ASR and point to\npromising directions for improvements."}
{"id": "2509.16862", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16862", "abs": "https://arxiv.org/abs/2509.16862", "authors": ["Rinka Nobukawa", "Makito Kitamura", "Tomohiko Nakamura", "Shinnosuke Takamichi", "Hiroshi Saruwatari"], "title": "Drum-to-Vocal Percussion Sound Conversion and Its Evaluation Methodology", "comment": "6 pages, 5 figures, accepted for 2025 Asia Pacific Signal and\n  Information Processing Association Annual Summit and Conference (APSIPA ASC)", "summary": "This paper defines the novel task of drum-to-vocal percussion (VP) sound\nconversion. VP imitates percussion instruments through human vocalization and\nis frequently employed in contemporary a cappella music. It exhibits acoustic\nproperties distinct from speech and singing (e.g., aperiodicity, noisy\ntransients, and the absence of linguistic structure), making conventional\nspeech or singing synthesis methods unsuitable. We thus formulate VP synthesis\nas a timbre transfer problem from drum sounds, leveraging their rhythmic and\ntimbral correspondence. To support this formulation, we define three\nrequirements for successful conversion: rhythmic fidelity, timbral consistency,\nand naturalness as VP. We also propose corresponding subjective evaluation\ncriteria. We implement two baseline conversion methods using a neural audio\nsynthesizer, the real-time audio variational autoencoder (RAVE), with and\nwithout vector quantization (VQ). Subjective experiments show that both methods\nproduce plausible VP outputs, with the VQ-based RAVE model yielding more\nconsistent conversion."}
{"id": "2509.16854", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16854", "abs": "https://arxiv.org/abs/2509.16854", "authors": ["Nianzu Li", "Weidong Mei", "Lipeng Zhu", "Peiran Wu", "Boyu Ning"], "title": "On the Secrecy Performance of Pinching-Antenna Systems", "comment": null, "summary": "Pinching-antenna systems have recently gained significant attention as a\nnovel reconfigurable-antenna technology due to its exceptional capability of\nmitigating signal-propagation path loss. In this letter, we investigate the\nsecrecy performance of a pinching-antenna system in the presence of an\neavesdropper. In particular, we derive an approximate expression of the\nsystem's secrecy outage probability (SOP) with respect to the random locations\nof the legitimate user and eavesdropper and analyze its asymptotic behavior.\nMoreover, we derive a constant performance lower bound on the SOP of the\nconsidered system, i.e., $\\frac{2\\pi-1}{24}$, which is significantly lower than\nthat of conventional fixed-position antenna systems, i.e., $0.5$. Finally,\nsimulation results are provided to validate the correctness of our analytical\nresults."}
{"id": "2509.16705", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16705", "abs": "https://arxiv.org/abs/2509.16705", "authors": ["Shuubham Ojha", "Felix Gervits", "Carol Espy-Wilson"], "title": "Reverse Attention for Lightweight Speech Enhancement on Edge Devices", "comment": null, "summary": "This paper introduces a lightweight deep learning model for real-time speech\nenhancement, designed to operate efficiently on resource-constrained devices.\nThe proposed model leverages a compact architecture that facilitates rapid\ninference without compromising performance. Key contributions include infusing\nsoft attention-based attention gates in the U-Net architecture which is known\nto perform well for segmentation tasks and is optimized for GPUs. Experimental\nevaluations demonstrate that the model achieves competitive speech quality and\nintelligibility metrics, such as PESQ and Word Error Rates (WER), improving the\nperformance of similarly sized baseline models. We are able to achieve a 6.24%\nWER improvement and a 0.64 PESQ score improvement over un-enhanced waveforms."}
{"id": "2509.16913", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16913", "abs": "https://arxiv.org/abs/2509.16913", "authors": ["Pedro Ramoneda", "Masahiro Suzuki", "Akira Maezawa", "Xavier Serra"], "title": "Difficulty-Aware Score Generation for Piano Sight-Reading", "comment": null, "summary": "Adapting learning materials to the level of skill of a student is important\nin education. In the context of music training, one essential ability is\nsight-reading -- playing unfamiliar scores at first sight -- which benefits\nfrom progressive and level-appropriate practice. However, creating exercises at\nthe appropriate level of difficulty demands significant time and effort. We\naddress this challenge as a controlled symbolic music generation task that aims\nto produce piano scores with a desired difficulty level. Controlling symbolic\ngeneration through conditioning is commonly done using control tokens, but\nthese do not always have a clear impact on global properties, such as\ndifficulty. To improve conditioning, we introduce an auxiliary optimization\ntarget for difficulty prediction that helps prevent conditioning collapse -- a\ncommon issue in which models ignore control signals in the absence of explicit\nsupervision. This auxiliary objective helps the model to learn internal\nrepresentations aligned with the target difficulty, enabling more precise and\nadaptive score generation. Evaluation with automatic metrics and expert\njudgments shows better control of difficulty and potential educational value.\nOur approach represents a step toward personalized music education through the\ngeneration of difficulty-aware practice material."}
{"id": "2509.16910", "categories": ["eess.SP", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16910", "abs": "https://arxiv.org/abs/2509.16910", "authors": ["Daxiang Li", "Zhichao Zhang"], "title": "Graph Fractional Hilbert Transform: Theory and Application", "comment": "32 pages, 6 figures", "summary": "The graph Hilbert transform (GHT) is a key tool in constructing analytic\nsignals and extracting envelope and phase information in graph signal\nprocessing. However, its utility is limited by confinement to the graph Fourier\ndomain, a fixed phase shift, information loss for real-valued spectral\ncomponents, and the absence of tunable parameters. The graph fractional Fourier\ntransform introduces domain flexibility through a fractional order parameter\n$\\alpha$ but does not resolve the issues of phase rigidity and information\nloss. Inspired by the dual-parameter fractional Hilbert transform (FRHT) in\nclassical signal processing, we propose the graph FRHT (GFRHT). The GFRHT\nincorporates a dual-parameter framework: the fractional order $\\alpha$ enables\nanalysis across arbitrary fractional domains, interpolating between vertex and\nspectral spaces, while the angle parameter $\\beta$ provides adjustable phase\nshifts and a non-zero real-valued response ($\\cos\\beta$) for real eigenvalues,\nthereby eliminating information loss. We formally define the GFRHT, establish\nits core properties, and design a method for graph analytic signal\nconstruction, enabling precise envelope extraction and demodulation.\nExperiments on edge detection, anomaly identification, and speech\nclassification demonstrate that GFRHT outperforms GHT, offering greater\nflexibility and superior performance in graph signal processing."}
{"id": "2509.16715", "categories": ["eess.AS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16715", "abs": "https://arxiv.org/abs/2509.16715", "authors": ["Adrien Llave", "Emma Granier", "Grégory Pallone"], "title": "QASTAnet: A DNN-based Quality Metric for Spatial Audio", "comment": null, "summary": "In the development of spatial audio technologies, reliable and shared methods\nfor evaluating audio quality are essential. Listening tests are currently the\nstandard but remain costly in terms of time and resources. Several models\npredicting subjective scores have been proposed, but they do not generalize\nwell to real-world signals. In this paper, we propose QASTAnet (Quality\nAssessment for SpaTial Audio network), a new metric based on a deep neural\nnetwork, specialized on spatial audio (ambisonics and binaural). As training\ndata is scarce, we aim for the model to be trainable with a small amount of\ndata. To do so, we propose to rely on expert modeling of the low-level auditory\nsystem and use a neurnal network to model the high-level cognitive function of\nthe quality judgement. We compare its performance to two reference metrics on a\nwide range of content types (speech, music, ambiance, anechoic, reverberated)\nand focusing on codec artifacts. Results demonstrate that QASTAnet overcomes\nthe aforementioned limitations of the existing methods. The strong correlation\nbetween the proposed metric prediction and subjective scores makes it a good\ncandidate for comparing codecs in their development."}
{"id": "2509.16922", "categories": ["cs.SD", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16922", "abs": "https://arxiv.org/abs/2509.16922", "authors": ["Tianheng Zhu", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control", "comment": "Main paper (15 pages). Accepted for publication by ICONIP(\n  International Conference on Neural Information Processing) 2025", "summary": "Audio-driven talking head generation is crucial for applications in virtual\nreality, digital avatars, and film production. While NeRF-based methods enable\nhigh-fidelity reconstruction, they suffer from low rendering efficiency and\nsuboptimal audio-visual synchronization. This work presents PGSTalker, a\nreal-time audio-driven talking head synthesis framework based on 3D Gaussian\nSplatting (3DGS). To improve rendering performance, we propose a pixel-aware\ndensity control strategy that adaptively allocates point density, enhancing\ndetail in dynamic facial regions while reducing redundancy elsewhere.\nAdditionally, we introduce a lightweight Multimodal Gated Fusion Module to\neffectively fuse audio and spatial features, thereby improving the accuracy of\nGaussian deformation prediction. Extensive experiments on public datasets\ndemonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches\nin rendering quality, lip-sync precision, and inference speed. Our method\nexhibits strong generalization capabilities and practical potential for\nreal-world deployment."}
{"id": "2509.16919", "categories": ["eess.SP", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.16919", "abs": "https://arxiv.org/abs/2509.16919", "authors": ["Huong Hoang", "Keito Suzuki", "Truong Nguyen", "Pamela Cosman"], "title": "Bi-modal Prediction and Transformation Coding for Compressing Complex Human Dynamics", "comment": null, "summary": "For dynamic human motion sequences, the original KeyNode-Driven codec often\nstruggles to retain compression efficiency when confronted with rapid movements\nor strong non-rigid deformations. This paper proposes a novel Bi-modal coding\nframework that enhances the flexibility of motion representation by integrating\nsemantic segmentation and region-specific transformation modeling. The rigid\ntransformation model (rotation & translation) is extended with a hybrid scheme\nthat selectively applies affine transformations-rotation, translation, scaling,\nand shearing-only to deformation-rich regions (e.g., the torso, where loose\nclothing induces high variability), while retaining rigid models elsewhere. The\naffine model is decomposed into minimal parameter sets for efficient coding and\ncombined through a component selection strategy guided by a Lagrangian\nRate-Distortion optimization. The results show that the Bi-modal method\nachieves more accurate mesh deformation, especially in sequences involving\ncomplex non-rigid motion, without compromising compression efficiency in\nsimpler regions, with an average bit-rate saving of 33.81% compared to the\nbaseline."}
{"id": "2509.16760", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16760", "abs": "https://arxiv.org/abs/2509.16760", "authors": ["Samuel Rey", "Luca Martino", "Roberto San Millan", "Eduardo Morgado"], "title": "Feature Selection via Graph Topology Inference for Soundscape Emotion Recognition", "comment": null, "summary": "Research on soundscapes has shifted the focus of environmental acoustics from\nnoise levels to the perception of sounds, incorporating contextual factors.\nSoundscape emotion recognition (SER) models perception using a set of features,\nwith arousal and valence commonly regarded as sufficient descriptors of affect.\nIn this work, we blend \\emph{graph learning} techniques with a novel\n\\emph{information criterion} to develop a feature selection framework for SER.\nSpecifically, we estimate a sparse graph representation of feature relations\nusing linear structural equation models (SEM) tailored to the widely used\nEmo-Soundscapes dataset. The resulting graph captures the relations between\ninput features and the two emotional outputs. To determine the appropriate\nlevel of sparsity, we propose a novel \\emph{generalized elbow detector}, which\nprovides both a point estimate and an uncertainty interval. We conduct an\nextensive evaluation of our methods, including visualizations of the inferred\nrelations. While several of our findings align with previous studies, the graph\nrepresentation also reveals a strong connection between arousal and valence,\nchallenging common SER assumptions."}
{"id": "2509.16926", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16926", "abs": "https://arxiv.org/abs/2509.16926", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment", "comment": "Accepted on Workshop on Detection and Classification of Acoustic\n  Scenes and Events (DCASE 2025)", "summary": "Multi-channel audio alignment is a key requirement in bioacoustic monitoring,\nspatial audio systems, and acoustic localization. However, existing methods\noften struggle to address nonlinear clock drift and lack mechanisms for\nquantifying uncertainty. Traditional methods like Cross-correlation and Dynamic\nTime Warping assume simple drift patterns and provide no reliability measures.\nMeanwhile, recent deep learning models typically treat alignment as a binary\nclassification task, overlooking inter-channel dependencies and uncertainty\nestimation. We introduce a method that combines cross-attention mechanisms with\nconfidence-weighted scoring to improve multi-channel audio synchronization. We\nextend BEATs encoders with cross-attention layers to model temporal\nrelationships between channels. We also develop a confidence-weighted scoring\nfunction that uses the full prediction distribution instead of binary\nthresholding. Our method achieved first place in the BioDCASE 2025 Task 1\nchallenge with 0.30 MSE average across test datasets, compared to 0.58 for the\ndeep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU\ndata (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The\nframework supports probabilistic temporal alignment, moving beyond point\nestimates. While validated in a bioacoustic context, the approach is applicable\nto a broader range of multi-channel audio tasks where alignment confidence is\ncritical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA"}
{"id": "2509.16921", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16921", "abs": "https://arxiv.org/abs/2509.16921", "authors": ["Seyong Kim", "Jeonghun Park"], "title": "Asymptotic Scaling Law Analysis of Multicast Satellite Communications with Massive MIMO", "comment": null, "summary": "In this paper, we consider a geostationary orbit (GEO) satellite\ncommunication system that employs massive multiple-input multiple-output (MIMO)\nfor multicast transmission. By modeling the spatial distribution of ground\nusers using a Poisson point process (PPP) and assuming a fixed-beam precoding\nis adopted, we find a closed-form expression for the asymptotical rate scaling\nlaw as a function of the number of antennas and the scaling factors of user\ndensity and multicast users. From the derived analytical expression, we reveal\nthat the rate degradation caused by multicast transmission can be precisely\ncompensated by increasing the user density accordingly."}
{"id": "2509.16901", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16901", "abs": "https://arxiv.org/abs/2509.16901", "authors": ["Mandip Goswami"], "title": "Automotive Sound Quality for EVs: Psychoacoustic Metrics with Reproducible AI/ML Baselines", "comment": null, "summary": "We present an open, reproducible reference for automotive sound quality that\nconnects standardized psychoacoustic metrics with lightweight AI/ML baselines,\nwith a specific focus on electric vehicles (EVs). We implement loudness (ISO\n532-1/2), tonality (DIN 45681), and modulation-based descriptors (roughness,\nfluctuation strength), and document assumptions and parameterizations for\nreliable reuse. For modeling, we provide simple, fully reproducible baselines\n(logistic regression, random forest, SVM) on synthetic EV-like cases using\nfixed splits and seeds, reporting accuracy and rank correlations as examples of\nend-to-end workflows rather than a comparative benchmark. Program-level\nnormalization is reported in LUFS via ITU-R BS.1770, while psychoacoustic\nanalysis uses ISO-532 loudness (sones). All figures and tables are regenerated\nby scripts with pinned environments; code and minimal audio stimuli are\nreleased under permissive licenses to support teaching, replication, and\nextension to EV-specific noise phenomena (e.g., inverter whine, reduced\nmasking)."}
{"id": "2509.16971", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16971", "abs": "https://arxiv.org/abs/2509.16971", "authors": ["Yan Rong", "Chenxing Li", "Dong Yu", "Li Liu"], "title": "AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for Coarse-to-Fine Audio Deep Reasoning", "comment": null, "summary": "Audio deep reasoning is a challenging task that requires expert-level\nperception, multi-step logical inference, and the integration of contextual\nknowledge. However, existing models suffer from a gap between audio perception\nand reasoning abilities due to the lack of training data with explicit\nreasoning chains and the absence of mechanisms for active exploration and\niterative refinement. To address these challenges, we propose\nAudioGenie-Reasoner (AGR), the first unified training-free multi-agent system\nthat coordinates perception and reasoning over an evolving chain of textual\nevidence. Our key idea is a paradigm shift that transforms audio deep reasoning\ninto complex text understanding task from a new perspective, thereby unlocking\nthe full potential of large language models. Specifically, the design of AGR\nmimics the human coarse-to-fine cognitive process. It first transforms the\ninput audio into a coarse text-based document. Then, we design a novel\nproactive iterative document refinement loop, featuring tool-augmented routes\nand specialized agents, to continuously search for missing information and\naugment the evidence chain in a coarse-to-fine manner until sufficient\nquestion-related information is gathered for making final predictions.\nExperimental results show that AGR achieves state-of-the-art (SOTA) performance\nover existing open-source audio deep reasoning models across various\nbenchmarks. The code will be made publicly available."}
{"id": "2509.17101", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17101", "abs": "https://arxiv.org/abs/2509.17101", "authors": ["Shiyong Chen"], "title": "Functional WMMSE Algorithm for Continuous Aperture Array Systems", "comment": null, "summary": "In this paper, we propose a functional extension of the weighted minimum\nmean-squared error (WMMSE) algorithm for downlink beamforming in multiuser\nmultiple-input multiple-output (MU-MIMO) systems where both the base station\n(BS) and the users employ continuous-aperture arrays (CAPAs). The method lifts\nthe matrices and vectors in the classical discrete WMMSE recursion to\ncontinuous functions by replacing matrix products and inner products with\nintegrals over the apertures. In practice, we apply a Galerkin projection to\nmap functions to coefficient matrices, solve the resulting discrete WMMSE\nproblem via closed-form updates, and then lift these updates back to the\nfunctional domain. All integrals are implemented using Gauss-Legendre\nquadrature, which preserves the closed-form structure through weighted matrix\nproducts. Simulations show that the proposed method outperforms baselines in\nboth spectral efficiency (SE) and computational complexity."}
{"id": "2509.16945", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16945", "abs": "https://arxiv.org/abs/2509.16945", "authors": ["Jeongmin Lee", "Chanhong Jeon", "Hyungjoo Seo", "Taewook Kang"], "title": "DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time UAV Speech Enhancement", "comment": null, "summary": "This paper proposes DroFiT (Drone Frequency lightweight Transformer for\nspeech enhancement, a single microphone speech enhancement network for severe\ndrone self-noise. DroFit integrates a frequency-wise Transformer with a\nfull/sub-band hybrid encoder-decoder and a TCN back-end for memory-efficient\nstreaming. A learnable skip-and-gate fusion with a combined spectral-temporal\nloss further refines reconstruction. The model is trained on VoiceBank-DEMAND\nmixed with recorded drone noise (-5 to -25 dB SNR) and evaluate using standard\nspeech enhancement metrics and computational efficiency. Experimental results\nshow that DroFiT achieves competitive enhancement performance while\nsignificantly reducing computational and memory demands, paving the way for\nreal-time processing on resource-constrained UAV platforms. Audio demo samples\nare available on our demo page."}
{"id": "2509.16975", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16975", "abs": "https://arxiv.org/abs/2509.16975", "authors": ["Yuhang Jia", "Xu Zhang", "Yang Chen", "Hui Wang", "Enzhi Wang", "Yong Qin"], "title": "Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs", "comment": null, "summary": "Automatic mean opinion score (MOS) prediction provides a more perceptual\nalternative to objective metrics, offering deeper insights into the evaluated\nmodels. With the rapid progress of multimodal large language models (MLLMs),\ntheir enhanced perceptual and reasoning abilities enable more comprehensive and\ninterpretable audio quality assessment. In this work, we tackle the challenging\ntask of audio editing evaluation and propose the first natural language-based\nautomated evaluation framework built on MLLMs. Our approach introduces two\nfine-tuning tasks to boost multi-audio understanding, combined with\nChain-of-Thought prompting, and lightweight instruction tuning, to enhance\nstep-by-step reasoning. Experiment demonstrate that our framework delivers\naccurate, interpretable, and text-based editing evaluation, closely aligning\nwith human judgments and objective metrics while substantially improving over\nbaselines. The code and demo are available at\nhttps://github.com/NKU-HLT/Eval_Reasoning."}
{"id": "2509.17181", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17181", "abs": "https://arxiv.org/abs/2509.17181", "authors": ["Mahdi Shamsi", "Hadi Zayyani", "Farokh Marvasti"], "title": "Resilient Signal Reflection under CSI Perturbations: A Robust Approach for Secure RIS Communication", "comment": null, "summary": "Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative\ntechnology in wireless communication, enabling dynamic control over signal\npropagation. This paper tackles the challenge of mitigating Channel State\nInformation (CSI) perturbations in RIS-aided systems, particularly for secure\ncommunication scenarios. Leveraging a first-order approximation technique, we\ndevelop a robust approach that strengthens the resilience of RIS configurations\nagainst CSI imperfections. The study considers both untrusted user interception\nand stealth radar applications, focusing on optimizing signal reflection and\ntransmission in the presence of eavesdroppers. Simulation results demonstrate\nnotable gains in security and efficiency while maintaining low computational\ncomplexity. By extending the stability range, the proposed method updates RIS\nelements using only a few matrix-vector multiplications, eliminating the need\nfor repeated inverse or pseudo-inverse computations under small channel\nperturbations. Additionally, the framework provides a baseline for quantifying\nalgorithmic sensitivity to CSI variations. Overall, the findings underscore the\npotential of RIS to enable secure and reliable communication in next-generation\nnetworks such as 6G."}
{"id": "2509.16994", "categories": ["eess.AS", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.16994", "abs": "https://arxiv.org/abs/2509.16994", "authors": ["Ina Salaj", "Arijit Biswas"], "title": "Attentive AV-FusionNet: Audio-Visual Quality Prediction with Hybrid Attention", "comment": "Pre-review version submitted to ICASSP 2026", "summary": "We introduce a novel deep learning-based audio-visual quality (AVQ)\nprediction model that leverages internal features from state-of-the-art\nunimodal predictors. Unlike prior approaches that rely on simple fusion\nstrategies, our model employs a hybrid representation that combines learned\nGenerative Machine Listener (GML) audio features with hand-crafted Video\nMultimethod Assessment Fusion (VMAF) video features. Attention mechanisms\ncapture cross-modal interactions and intra-modal relationships, yielding\ncontext-aware quality representations. A modality relevance estimator\nquantifies each modality's contribution per content, potentially enabling\nadaptive bitrate allocation. Experiments demonstrate improved AVQ prediction\naccuracy and robustness across diverse content types."}
{"id": "2509.16979", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16979", "abs": "https://arxiv.org/abs/2509.16979", "authors": ["Boxuan Cao", "Linkai Li", "Hanlin Yu", "Changgeng Mo", "Haoshuai Zhou", "Shan Xiang Wang"], "title": "Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility Prediction for Hearing-Impaired Listeners", "comment": null, "summary": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is\nessential for assessing hearing aid performance, traditionally relying on\nlistening tests or intrusive methods like HASPI. However, these methods require\nclean reference signals, which are often unavailable in real-world conditions,\ncreating a gap between lab-based and real-world assessments. To address this,\nwe propose a non-intrusive intelligibility prediction framework that leverages\nspeech enhancers to provide a parallel enhanced-signal pathway, enabling robust\npredictions without reference signals. We evaluate three state-of-the-art\nenhancers and demonstrate that prediction performance depends on the choice of\nenhancer, with ensembles of strong enhancers yielding the best results. To\nimprove cross-dataset generalization, we introduce a 2-clips augmentation\nstrategy that enhances listener-specific variability, boosting robustness on\nunseen datasets. Our approach consistently outperforms the non-intrusive\nbaseline, CPC2 Champion across multiple datasets, highlighting the potential of\nenhancer-guided non-intrusive intelligibility prediction for real-world\napplications."}
{"id": "2509.17267", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17267", "abs": "https://arxiv.org/abs/2509.17267", "authors": ["Taorui Chen", "Yuki Gao", "Yi Wang", "Hai-Han Sun"], "title": "Estimation of Specific Gravity of Potato Tubers Using Dielectric Properties", "comment": null, "summary": "Potatoes are an economically important crop, and their quality is closely\nrelated to the starch content, which is typically inferred from specific\ngravity (SG). Although microwave sensing technologies have been increasingly\ndeveloped for underground potato detection and quality assessment in recent\nyears, no accurate model has yet been established to link the dielectric\nproperties of potatoes with their key agronomic traits. To address this gap, we\ndeveloped a model for estimating potato tubers' SG based on their dielectric\nconstant. To construct and validate the model, we conducted SG measurements and\ndielectric spectroscopy measurements in the frequency range of 0.3 GHz to 3.0\nGHz on 250 potatoes of five different types (red, russet, yellow, purple, and\nchipping potatoes, with 50 samples per type). Out of the 250 data sets, 200\ndata sets were used for model development, and 50 data sets were used for model\nvalidation. A linear regression model was used to summarize the relationship\nbetween SG and dielectric constant, where the regression coefficients are\nexpressed as fourth-order polynomial functions of frequency. Experimental\nresults on the 50 validation data sets show that the model achieves high\nestimation accuracy with mean absolute errors (MAE) less than\n\\(4.8\\times10^{-3}\\) and mean absolute percentage errors (MAPE) less than\n0.45\\%. The study of the dielectric properties of potatoes, along with the\nderived SG estimation model, provides a foundation for the future development\nof microwave sensing technologies for agronomic trait assessment in the potato\nproduction and processing industries. All measured data will be made publicly\navailable upon acceptance of the paper."}
{"id": "2509.17143", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.17143", "abs": "https://arxiv.org/abs/2509.17143", "authors": ["Junhyeok Lee", "Helin Wang", "Yaohan Guan", "Thomas Thebaud", "Laureano Moro-Velazquez", "Jesús Villalba", "Najim Dehak"], "title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances", "comment": null, "summary": "We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers\nmulti-factor controllability through multiple classifier-free guidances (CFGs).\nWhile previous VC models rely on a fixed conditioning scheme, MaskVCT\nintegrates diverse conditions in a single model. To further enhance robustness\nand control, the model can leverage continuous or quantized linguistic features\nto enhance intellgibility and speaker similarity, and can use or omit pitch\ncontour to control prosody. These choices allow users to seamlessly balance\nspeaker identity, linguistic content, and prosodic factors in a zero-shot VC\nsetting. Extensive experiments demonstrate that MaskVCT achieves the best\ntarget speaker and accent similarities while obtaining competitive word and\ncharacter error rates compared to existing baselines. Audio samples are\navailable at https://maskvct.github.io/."}
{"id": "2509.17006", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17006", "abs": "https://arxiv.org/abs/2509.17006", "authors": ["Ruonan Zhang", "Xiaoyang Hao", "Yichen Han", "Junjie Cao", "Yue Liu", "Kai Zhang"], "title": "MBCodec:Thorough disentangle for high-fidelity audio compression", "comment": "5 pages, 2 figures", "summary": "High-fidelity neural audio codecs in Text-to-speech (TTS) aim to compress\nspeech signals into discrete representations for faithful reconstruction.\nHowever, prior approaches faced challenges in effectively disentangling\nacoustic and semantic information within tokens, leading to a lack of\nfine-grained details in synthesized speech. In this study, we propose MBCodec,\na novel multi-codebook audio codec based on Residual Vector Quantization (RVQ)\nthat learns a hierarchically structured representation. MBCodec leverages\nself-supervised semantic tokenization and audio subband features from the raw\nsignals to construct a functionally-disentangled latent space. In order to\nencourage comprehensive learning across various layers of the codec embedding\nspace, we introduce adaptive dropout depths to differentially train codebooks\nacross layers, and employ a multi-channel pseudo-quadrature mirror filter\n(PQMF) during training. By thoroughly decoupling semantic and acoustic\nfeatures, our method not only achieves near-lossless speech reconstruction but\nalso enables a remarkable 170x compression of 24 kHz audio, resulting in a low\nbit rate of just 2.2 kbps. Experimental evaluations confirm its consistent and\nsubstantial outperformance of baselines across all evaluations."}
{"id": "2509.17344", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17344", "abs": "https://arxiv.org/abs/2509.17344", "authors": ["Sven Hinderer", "Manuel Buchfink", "Bin Yang"], "title": "On Mutual Information Neural Estimation for Localization", "comment": null, "summary": "Mutual information (MI) is a promising candidate measure for the assessment\nand optimization of localization systems, as it captures nonlinear dependencies\nbetween random variables. However, the high cost of computing MI, especially\nfor high-dimensional problems, prohibits its application for many real-world\nlocalization systems. We evaluate an algorithm from a new class of neural MI\nestimators called Mutual Information Neural Estimation (MINE) to approximate\nthe MI between the set of feasible user element (UE) locations and the\ncorresponding set of measurements from said UE locations used for positioning.\nWe apply this estimator to a simulated multilateration (MLAT) system, where the\ntrue MI for benchmarking can be approximated by Monte Carlo simulation. The\nestimator is experimentally evaluated w.r.t. its convergence and consistency\nand we investigate the usefulness of MI for assessing simple MLAT systems."}
{"id": "2509.17247", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17247", "abs": "https://arxiv.org/abs/2509.17247", "authors": ["Dongheon Lee", "Younghoo Kwon", "Jung-Woo Choi"], "title": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene Analysis", "comment": "26 pages, 13 figures, 8 tables, accepted in NeurIPS 2025", "summary": "We propose DeepASA, a one-for-all model for auditory scene analysis that\nperforms multi-input multi-output (MIMO) source separation, dereverberation,\nsound event detection (SED), audio classification, and direction-of-arrival\nestimation (DoAE) within a unified framework. DeepASA is designed for complex\nauditory scenes where multiple, often similar, sound sources overlap in time\nand move dynamically in space. To achieve robust and consistent inference\nacross tasks, we introduce an object-oriented processing (OOP) strategy. This\napproach encapsulates diverse auditory features into object-centric\nrepresentations and refines them through a chain-of-inference (CoI) mechanism.\nThe pipeline comprises a dynamic temporal kernel-based feature extractor, a\ntransformer-based aggregator, and an object separator that yields per-object\nfeatures. These features feed into multiple task-specific decoders. Our\nobject-centric representations naturally resolve the parameter association\nambiguity inherent in traditional track-wise processing. However, early-stage\nobject separation can lead to failure in downstream ASA tasks. To address this,\nwe implement temporal coherence matching (TCM) within the chain-of-inference,\nenabling multi-task fusion and iterative refinement of object features using\nestimated auditory parameters. We evaluate DeepASA on representative spatial\naudio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental\nresults show that our model achieves state-of-the-art performance across all\nevaluated tasks, demonstrating its effectiveness in both source separation and\nauditory parameter estimation under diverse spatial auditory scenes."}
{"id": "2509.17021", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17021", "abs": "https://arxiv.org/abs/2509.17021", "authors": ["Ruonan Zhang", "Lingzhou Mu", "Xixin Wu", "Kai Zhang"], "title": "Bridging the gap between training and inference in LM-based TTS models", "comment": "5 pages, 4 figures", "summary": "Recent advancements in text-to-speech (TTS) have shown that language model\n(LM) based systems offer competitive performance compared to traditional\napproaches. However, in training, TTS models use ground-truth (GT) tokens as\nprefixes to predict the next token, while in inference these tokens are not\navailable, a gap between training and inference that is often neglected. In\nthis study, we propose a prompt-guided hybrid training scheme to mitigate\nexposure bias in popular LM-based TTS systems. Our core idea is to adopt a\nhybrid training paradigm that combines teacher forcing with free running,\nthereby introducing self-generated tokens into the training process. This makes\nthe training mode more consistent with inference, reducing the\ntraining-inference gap. In addition, we incorporate an EOS prediction mechanism\nduring training to detect incorrect sequence termination and adaptively control\nthe free running process. Experimental results provide a comprehensive\nevaluation of the impact of exposure bias on LM-based TTS, and demonstrate that\nour method effectively narrows the training-inference gap, thereby improving\nthe quality of synthesized long-form speech."}
{"id": "2509.17483", "categories": ["eess.SP", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.17483", "abs": "https://arxiv.org/abs/2509.17483", "authors": ["Qianqian Li", "Lintao Li", "Lixiang Liu", "Lei Yang", "Caihong Gong", "Hua Li", "Shiya Hao", "Xiaoming Dai"], "title": "On the Design of Capacity-Achieving Distributions for Discrete-Time Poisson Channel with Low-Precision ADCs", "comment": null, "summary": "This paper investigates the design of the capacity-achieving input\ndistribution for the discrete-time Poisson channel (DTPC) under dark current\neffects with low-precision analog-to-digital converters (ADCs). This study\nintroduces an efficient optimization algorithm that integrates the\nNewton-Raphson and Blahut-Arimoto (BA) methods to determine the\ncapacity-achieving input distribution and the corresponding amplitudes of input\nmass points for the DTPC, subject to both peak and average power constraints.\nAdditionally, the Karush-Kuhn-Tucker (KKT) conditions are established to\nprovide necessary and sufficient conditions for the optimality of the obtained\ncapacity-achieving distribution. Simulation results illustrate that the\nproposed algorithm attains $72\\%$ and $83\\%$ of the theoretical capacity at 5\ndB for 1-bit and 2-bit quantized DTPC, respectively. Furthermore, for a\nfinite-precision quantized DTPC (i.e., ${\\log _2}K$ bits), the capacity can be\nachieved by a non-uniform discrete input distribution with support for $K$ mass\npoints, under the given power constraints."}
{"id": "2509.17270", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17270", "abs": "https://arxiv.org/abs/2509.17270", "authors": ["Hanlin Yu", "Haoshuai Zhou", "Boxuan Cao", "Changgeng Mo", "Linkai Li", "Shan X. Wang"], "title": "Reference-aware SFM layers for intrusive intelligibility prediction", "comment": "Preprint; submitted to ICASSP 2026. 5 pages. CPC3 system: Dev RMSE\n  22.36, Eval RMSE 24.98 (ranked 1st)", "summary": "Intrusive speech-intelligibility predictors that exploit explicit reference\nsignals are now widespread, yet they have not consistently surpassed\nnon-intrusive systems. We argue that a primary cause is the limited\nexploitation of speech foundation models (SFMs). This work revisits intrusive\nprediction by combining reference conditioning with multi-layer SFM\nrepresentations. Our final system achieves RMSE 22.36 on the development set\nand 24.98 on the evaluation set, ranking 1st on CPC3. These findings provide\npractical guidance for constructing SFM-based intrusive intelligibility\npredictors."}
{"id": "2509.17052", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17052", "abs": "https://arxiv.org/abs/2509.17052", "authors": ["Wataru Nakata", "Yuki Saito", "Yota Ueda", "Hiroshi Saruwatari"], "title": "Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing", "comment": "5 pages, 1 figures", "summary": "Large-scale text-to-speech (TTS) systems are limited by the scarcity of\nclean, multilingual recordings. We introduce Sidon, a fast, open-source speech\nrestoration model that converts noisy in-the-wild speech into studio-quality\nspeech and scales to dozens of languages. Sidon consists of two models:\nw2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google's internal\nspeech restoration model with the aim of dataset cleansing for speech\nsynthesis. Sidon is also computationally efficient, running up to 3,390 times\nfaster than real time on a single GPU. We further show that training a TTS\nmodel using a Sidon-cleansed automatic speech recognition corpus improves the\nquality of synthetic speech in a zero-shot setting. Code and model are released\nto facilitate reproducible dataset cleansing for the research community."}
{"id": "2509.17511", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17511", "abs": "https://arxiv.org/abs/2509.17511", "authors": ["Yunqiao Hu", "Xuesu Xiao", "Steven Jones", "Shunqiao Sun"], "title": "Single-Snapshot Localization Using Sparse Extremely Large Aperture Arrays", "comment": "ICASSP 2026 manuscript under review", "summary": "This paper investigates single-snapshot direction-of-arrival (DOA) estimation\nand target localization with coherent sparse extremely large aperture arrays\n(ELAAs) in automotive radar applications. Far-field and near-field signal\nmodels are formulated for distributed bistatic configurations. To enable\nnoncoherent processing, a single-snapshot MUSIC (SS-MUSIC) algorithm is\nproposed to fuse local spectra from individual subarrays and extended to\nnear-field localization via geometric intersection. For coherent processing, a\nsingle-snapshot ESPRIT (SS-ESPRIT) method with ambiguity dealiasing is\ndeveloped to fully exploit the aperture of sparse ELAAs for high-resolution\nangle estimation. Simulation results demonstrate that SS-ESPRIT provides\nsuperior angular resolution for closely spaced far-field targets, while\nSS-MUSIC offers robustness in near-field localization and flexibility in hybrid\nscenarios."}
{"id": "2509.17277", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17277", "abs": "https://arxiv.org/abs/2509.17277", "authors": ["Mandip Goswami"], "title": "BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and Psychoacoustics Research", "comment": "Data note; 6 to 8 pages; 1 to 2 figures; dataset: CC0-1.0; code: MIT", "summary": "We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset\n(300-500 clips) designed for rapid, rights-clean experimentation in\nhuman-computer interaction and audio machine learning. Each clip is generated\nfrom a parametric recipe controlling waveform family (sine, square, triangle,\nFM), fundamental frequency, duration, amplitude envelope, amplitude modulation\n(AM), and lightweight Schroeder-style reverberation. We use three reverberation\nsettings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir\nmedium' ('medium') throughout the paper and in the metadata. We release mono 48\nkHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and\ntiny reproducible baselines for (i) waveform-family classification and (ii) f0\nregression on single tones. The corpus targets tasks such as earcon\nclassification, timbre analyses, and onset detection, with clearly stated\nlicensing and limitations. Audio is dedicated to the public domain via CC0-1.0;\ncode is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code:\nhttps://github.com/mandip42/earcons-mini-500."}
{"id": "2509.17091", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.17091", "abs": "https://arxiv.org/abs/2509.17091", "authors": ["Massa Baali", "Sarthak Bisht", "Francisco Teixeira", "Kateryna Shapovalenko", "Rita Singh", "Bhiksha Raj"], "title": "SVeritas: Benchmark for Robust Speaker Verification under Diverse Conditions", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Speaker verification (SV) models are increasingly integrated into security,\npersonalization, and access control systems, yet their robustness to many\nreal-world challenges remains inadequately benchmarked. These include a variety\nof natural and maliciously created conditions causing signal degradations or\nmismatches between enrollment and test data, impacting performance. Existing\nbenchmarks evaluate only subsets of these conditions, missing others entirely.\nWe introduce SVeritas, a comprehensive Speaker Verification tasks benchmark\nsuite, assessing SV systems under stressors like recording duration,\nspontaneity, content, noise, microphone distance, reverberation, channel\nmismatches, audio bandwidth, codecs, speaker age, and susceptibility to\nspoofing and adversarial attacks. While several benchmarks do exist that each\ncover some of these issues, SVeritas is the first comprehensive evaluation that\nnot only includes all of these, but also several other entirely new, but\nnonetheless important, real-life conditions that have not previously been\nbenchmarked. We use SVeritas to evaluate several state-of-the-art SV models and\nobserve that while some architectures maintain stability under common\ndistortions, they suffer substantial performance degradation in scenarios\ninvolving cross-language trials, age mismatches, and codec-induced compression.\nExtending our analysis across demographic subgroups, we further identify\ndisparities in robustness across age groups, gender, and linguistic\nbackgrounds. By standardizing evaluation under realistic and synthetic stress\nconditions, SVeritas enables precise diagnosis of model weaknesses and\nestablishes a foundation for advancing equitable and reliable speaker\nverification systems."}
{"id": "2509.17674", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17674", "abs": "https://arxiv.org/abs/2509.17674", "authors": ["Julia Matejas", "Olaf Żurawski", "Nils Strodthoff", "Juan Miguel Lopez Alcaraz"], "title": "Predicting Chest Radiograph Findings from Electrocardiograms Using Interpretable Machine Learning", "comment": "19 pages, 3 figures, source code under\n  https://github.com/UOLMDA2025/CardioCXR", "summary": "Purpose: Chest X-rays are essential for diagnosing pulmonary conditions, but\nlimited access in resource-constrained settings can delay timely diagnosis.\nElectrocardiograms (ECGs), in contrast, are widely available, non-invasive, and\noften acquired earlier in clinical workflows. This study aims to assess whether\nECG features and patient demographics can predict chest radiograph findings\nusing an interpretable machine learning approach.\n  Methods: Using the MIMIC-IV database, Extreme Gradient Boosting (XGBoost)\nclassifiers were trained to predict diverse chest radiograph findings from\nECG-derived features and demographic variables. Recursive feature elimination\nwas performed independently for each target to identify the most predictive\nfeatures. Model performance was evaluated using the area under the receiver\noperating characteristic curve (AUROC) with bootstrapped 95% confidence\nintervals. Shapley Additive Explanations (SHAP) were applied to interpret\nfeature contributions.\n  Results: Models successfully predicted multiple chest radiograph findings\nwith varying accuracy. Feature selection tailored predictors to each target,\nand including demographic variables consistently improved performance. SHAP\nanalysis revealed clinically meaningful contributions from ECG features to\nradiographic predictions.\n  Conclusion: ECG-derived features combined with patient demographics can serve\nas a proxy for certain chest radiograph findings, enabling early triage or\npre-screening in settings where radiographic imaging is limited. Interpretable\nmachine learning demonstrates potential to support radiology workflows and\nimprove patient care."}
{"id": "2509.17286", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17286", "abs": "https://arxiv.org/abs/2509.17286", "authors": ["David Rowe", "Tibor Bece"], "title": "RADE for Land Mobile Radio: A Neural Codec for Transmission of Speech over Baseband FM Radio Channels", "comment": "6 pages, 9 figures", "summary": "In the 1990s Land Mobile Radio (LMR) systems evolved from analog frequency\nmodulation (FM) to standardised digital systems. Both digital and analog FM\nsystems now co-exist in various services and exhibit similar speech quality.\nThe architecture of many digital radios retains the analog FM modulator and\ndemodulator from legacy analog radios, but driven by a multi-level digital\npulse train rather than an analog voice signal. We denote this architecture\nbaseband FM (BBFM). In this paper we describe a modern machine learning\napproach that uses an autoencoder to send high quality, 8 kHz bandwidth speech\nover the BBFM channel. The speech quality is shown to be superior to analog FM\nover simulated LMR channels in the presence of fading, and a demonstration of\nthe system running over commodity UHF radios is presented."}
{"id": "2509.17112", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17112", "abs": "https://arxiv.org/abs/2509.17112", "authors": ["Alexander Wang", "Chris Donahue", "Dhruv Jain"], "title": "RISE: Adaptive music playback for Realtime Intensity Synchronization with Exercise", "comment": "ISMIR 2025", "summary": "We propose a system to adapt a user's music to their exercise by aligning\nhigh-energy music segments with intense intervals of the workout. Listening to\nmusic during exercise can boost motivation and performance. However, the\nstructure of the music may be different from the user's natural phases of rest\nand work, causing users to rest longer than needed while waiting for a\nmotivational section, or lose motivation mid-work if the section ends too soon.\nTo address this, our system, called RISE, automatically estimates the intense\nsegments in music and uses component-based music rearrangement techniques to\ndynamically extend and shorten different segments of the user's song to fit the\nongoing exercise routine. Our system takes as input the rest and work durations\nto guide adaptation. Currently, this is determined either via a pre-defined\nplan or manual input during the workout. We evaluated RISE with 12 participants\nand compared our system to a non-adaptive music baseline while exercising in\nour lab. Participants found our rearrangements keeps intensity estimation\naccurate, and many recalled moments when intensity alignment helped them push\nthrough their workout."}
{"id": "2509.17797", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17797", "abs": "https://arxiv.org/abs/2509.17797", "authors": ["Yuan Gao", "Yiming Liu", "Runze Yu", "Shengli Liu", "Yanliang Jin", "Shunqing Zhang", "Shugong Xu", "Xiaoli Chu"], "title": "SSNet: Flexible and robust channel extrapolation for fluid antenna systems enabled by an self-supervised learning framework", "comment": null, "summary": "Fluid antenna systems (FAS) signify a pivotal advancement in 6G communication\nby enhancing spectral efficiency and robustness. However, obtaining accurate\nchannel state information (CSI) in FAS poses challenges due to its complex\nphysical structure. Traditional methods, such as pilot-based interpolation and\ncompressive sensing, are not only computationally intensive but also lack\nadaptability. Current extrapolation techniques relying on rigid parametric\nmodels do not accommodate the dynamic environment of FAS, while data-driven\ndeep learning approaches demand extensive training and are vulnerable to noise\nand hardware imperfections. To address these challenges, this paper introduces\na novel self-supervised learning network (SSNet) designed for efficient and\nadaptive channel extrapolation in FAS. We formulate the problem of channel\nextrapolation in FAS as an image reconstruction task. Here, a limited number of\nunmasked pixels (representing the known CSI of the selected ports) are used to\nextrapolate the masked pixels (the CSI of unselected ports). SSNet capitalizes\non the intrinsic structure of FAS channels, learning generalized\nrepresentations from raw CSI data, thus reducing dependency on large labelled\ndatasets. For enhanced feature extraction and noise resilience, we propose a\nmix-of-expert (MoE) module. In this setup, multiple feedforward neural networks\n(FFNs) operate in parallel. The outputs of the MoE module are combined using a\nweighted sum, determined by a gating function that computes the weights of each\nFFN using a softmax function. Extensive simulations validate the superiority of\nthe proposed model. Results indicate that SSNet significantly outperforms\nbenchmark models, such as AGMAE and long short-term memory (LSTM) networks by\nusing a much smaller labelled dataset."}
{"id": "2509.17375", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17375", "abs": "https://arxiv.org/abs/2509.17375", "authors": ["Aayush Jaiswal", "Parampreet Singh", "Vipul Arora"], "title": "Improving Active Learning for Melody Estimation by Disentangling Uncertainties", "comment": "This work has been submitted to the IEEE ICASSP 2026 for possible\n  publication", "summary": "Estimating the fundamental frequency, or melody, is a core task in Music\nInformation Retrieval (MIR). Various studies have explored signal processing,\nmachine learning, and deep-learning-based approaches, with a very recent focus\non utilizing uncertainty in active learning settings for melody estimation.\nHowever, these approaches do not investigate the relative effectiveness of\ndifferent uncertainties. In this work, we follow a framework that disentangles\naleatoric and epistemic uncertainties to guide active learning for melody\nestimation. Trained on a source dataset, our model adapts to new domains using\nonly a small number of labeled samples. Experimental results demonstrate that\nepistemic uncertainty is more reliable for domain adaptation with reduced\nlabeling effort as compared to aleatoric uncertainty."}
{"id": "2509.17162", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.17162", "abs": "https://arxiv.org/abs/2509.17162", "authors": ["Zeyu Xie", "Yaoyun Zhang", "Xuenan Xu", "Yongkang Yin", "Chenxing Li", "Mengyue Wu", "Yuexian Zou"], "title": "FakeSound2: A Benchmark for Explainable and Generalizable Deepfake Sound Detection", "comment": null, "summary": "The rapid development of generative audio raises ethical and security\nconcerns stemming from forged data, making deepfake sound detection an\nimportant safeguard against the malicious use of such technologies. Although\nprior studies have explored this task, existing methods largely focus on binary\nclassification and fall short in explaining how manipulations occur, tracing\nwhere the sources originated, or generalizing to unseen sources-thereby\nlimiting the explainability and reliability of detection. To address these\nlimitations, we present FakeSound2, a benchmark designed to advance deepfake\nsound detection beyond binary accuracy. FakeSound2 evaluates models across\nthree dimensions: localization, traceability, and generalization, covering 6\nmanipulation types and 12 diverse sources. Experimental results show that\nalthough current systems achieve high classification accuracy, they struggle to\nrecognize forged pattern distributions and provide reliable explanations. By\nhighlighting these gaps, FakeSound2 establishes a comprehensive benchmark that\nreveals key challenges and aims to foster robust, explainable, and\ngeneralizable approaches for trustworthy audio authentication."}
{"id": "2509.17804", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17804", "abs": "https://arxiv.org/abs/2509.17804", "authors": ["Xiaohua Zhou", "Tianyu Fang", "Yijie Mao", "Bruno Clerckx"], "title": "Generalized Beyond-Diagonal RIS Architectures: Theory and Design via Structure-oriented Symmetric Unitary Projection", "comment": null, "summary": "Beyond-diagonal reconfigurable intelligent surface (BD-RIS), which enables\nadvanced wave control through interconnection of RIS elements, are gaining\ngrowing recognition as a promising technology for 6G and beyond. However, the\nenhanced flexibility of BD-RIS in controlling the phase and amplitude of\nreflected signals comes at the cost of high circuit complexity. In this paper,\nwe propose two novel BD-RIS architectures, namely, the stem-connected RIS and\ncluster-connected RIS, to explore trade-off between circuit complexity and\nperformance. Specifically, the proposed stem-connected RIS is capable of\nachieving the same performance as fully-connected RIS while significantly\nreducing circuit complexity. The proposed cluster-connected RIS offers a\nunified framework that generalizes existing BD-RIS architectures--including\nsingle-connected, fully-connected, group-connected, tree-connected (arrowhead),\nand forest-connected (arrowhead) RISs--as special cases. This framework enables\na much more flexible trade-offs between circuit complexity and system\nperformance than existing ones. Based on the proposed BD-RIS architectures, we\nintroduce a novel and generalized structure-oriented symmetric unitary\nprojection method for designing the scattering matrix across all BD-RIS\nconfigurations. This method is effectively applied to solve the sum channel\ngain maximization problem and other utility-based optimization problems.\nNumerical results demonstrate that the proposed stem-connected RIS is the\nsimplest architecture that achieves optimal BD-RIS performance, while the\ncluster-connected RIS further enlarges the performance-complexity trade-off\nrange. Furthermore, the proposed projection-based algorithms demonstrate high\nefficiency."}
{"id": "2509.17404", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17404", "abs": "https://arxiv.org/abs/2509.17404", "authors": ["Wei Tan", "Shun Lei", "Huaicheng Zhang", "Guangzheng Li", "Yixuan Zhang", "Hangting Chen", "Jianwei Yu", "Rongzhi Gu", "Dong Yu"], "title": "SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription", "comment": null, "summary": "Artificial Intelligence Generated Content (AIGC) is currently a popular\nresearch area. Among its various branches, song generation has attracted\ngrowing interest. Despite the abundance of available songs, effective data\npreparation remains a significant challenge. Converting these songs into\ntraining-ready datasets typically requires extensive manual labeling, which is\nboth time consuming and costly. To address this issue, we propose SongPrep, an\nautomated preprocessing pipeline designed specifically for song data. This\nframework streamlines key processes such as source separation, structure\nanalysis, and lyric recognition, producing structured data that can be directly\nused to train song generation models. Furthermore, we introduce SongPrepE2E, an\nend-to-end structured lyrics recognition model based on pretrained language\nmodels. Without the need for additional source separation, SongPrepE2E is able\nto analyze the structure and lyrics of entire songs and provide precise\ntimestamps. By leveraging context from the whole song alongside pretrained\nsemantic knowledge, SongPrepE2E achieves low Diarization Error Rate (DER) and\nWord Error Rate (WER) on the proposed SSLD-200 dataset. Downstream tasks\ndemonstrate that training song generation models with the data output by\nSongPrepE2E enables the generated songs to closely resemble those produced by\nhumans."}
{"id": "2509.17164", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.17164", "abs": "https://arxiv.org/abs/2509.17164", "authors": ["Zeyu Xie", "Xuenan Xu", "Yixuan Li", "Mengyue Wu", "Yuexian Zou"], "title": "STAR: Speech-to-Audio Generation via Representation Learning", "comment": null, "summary": "This work presents STAR, the first end-to-end speech-to-audio generation\nframework, designed to enhance efficiency and address error propagation\ninherent in cascaded systems. Unlike prior approaches relying on text or\nvision, STAR leverages speech as it constitutes a natural modality for\ninteraction. As an initial step to validate the feasibility of the system, we\ndemonstrate through representation learning experiments that spoken sound event\nsemantics can be effectively extracted from raw speech, capturing both auditory\nevents and scene cues. Leveraging the semantic representations, STAR\nincorporates a bridge network for representation mapping and a two-stage\ntraining strategy to achieve end-to-end synthesis. With a 76.9% reduction in\nspeech processing latency, STAR demonstrates superior generation performance\nover the cascaded systems. Overall, STAR establishes speech as a direct\ninteraction signal for audio generation, thereby bridging representation\nlearning and multimodal synthesis. Generated samples are available at\nhttps://zeyuxie29.github.io/STAR."}
{"id": "2509.17916", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17916", "abs": "https://arxiv.org/abs/2509.17916", "authors": ["Kabuto Arai", "Koji Ishibashi", "Hiroki Iimori", "Yuto Hama", "Paulo Valente Klaine", "Szabolcs Malomsoky"], "title": "Joint Pilot Allocation and Sequence Design for MIMO-OFDM Systems With Channel Sparsity", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a joint optimization of pilot subcarrier allocation and\nnon-orthogonal sequence for multiple-input-multiple-output (MIMO)-orthogonal\nfrequency-division multiplexing (OFDM) systems under compressed sensing\n(CS)-based channel estimation exploiting delay and angle sparsity. Since the\nperformance of CS-based approaches depends on a coherence metric of the sensing\nmatrix in the measurement process, we formulate a joint optimization problem to\nminimize this coherence. Due to the discrete nature of subcarrier allocation, a\nstraightforward formulation of the joint optimization results in a\nmixed-integer nonlinear program (MINLP), which is computationally intractable\ndue to the combinatorial explosion of allocation candidates. To overcome the\nintractability of discrete variables, we introduce a block sparse penalty for\npilots across all subcarriers, which ensures that the power of some unnecessary\npilots approaches zero. This framework enables joint optimization using only\ncontinuous variables. In addition, we propose an efficient computation method\nfor the coherence metric by exploiting the structure of the sensing matrix,\nwhich allows its gradient to be derived in closed form, making the joint\noptimization problem solvable in an efficient way via a gradient descent\napproach. Numerical results confirm that the proposed pilot sequence exhibits\nsuperior coherence properties and enhances the CS-based channel estimation\nperformance."}
{"id": "2509.17410", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17410", "abs": "https://arxiv.org/abs/2509.17410", "authors": ["Geonwoo Baek", "Jung-Woo Choi"], "title": "Neural acoustic multipole splatting for room impulse response synthesis", "comment": "5 pages, 5 figures", "summary": "Room Impulse Response (RIR) prediction at arbitrary receiver positions is\nessential for practical applications such as spatial audio rendering. We\npropose Neural Acoustic Multipole Splatting (NAMS), which synthesizes RIRs at\nunseen receiver positions by learning the positions of neural acoustic\nmultipoles and predicting their emitted signals and directivities using a\nneural network. Representing sound fields through a combination of multipoles\noffers sufficient flexibility to express complex acoustic scenes while adhering\nto physical constraints such as the Helmholtz equation. We also introduce a\npruning strategy that starts from a dense splatting of neural acoustic\nmultipoles and progressively eliminates redundant ones during training.\nExperiments conducted on both real and synthetic datasets indicate that the\nproposed method surpasses previous approaches on most metrics while maintaining\nrapid inference. Ablation studies reveal that multipole splatting with pruning\nachieves better performance than the monopole model with just 20% of the poles."}
{"id": "2509.17219", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17219", "abs": "https://arxiv.org/abs/2509.17219", "authors": ["Matthieu Cervera", "Francesco Paissan", "Mirco Ravanelli", "Cem Subakan"], "title": "Virtual Consistency for Audio Editing", "comment": null, "summary": "Free-form, text-based audio editing remains a persistent challenge, despite\nprogress in inversion-based neural methods. Current approaches rely on slow\ninversion procedures, limiting their practicality. We present a\nvirtual-consistency based audio editing system that bypasses inversion by\nadapting the sampling process of diffusion models. Our pipeline is\nmodel-agnostic, requiring no fine-tuning or architectural changes, and achieves\nsubstantial speed-ups over recent neural editing baselines. Crucially, it\nachieves this efficiency without compromising quality, as demonstrated by\nquantitative benchmarks and a user study involving 16 participants."}
{"id": "2509.17953", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17953", "abs": "https://arxiv.org/abs/2509.17953", "authors": ["Kathrin Klein", "Benedikt Böck", "Nurettin Turan", "Wolfgang Utschick"], "title": "Autoregressive-Gaussian Mixture Models: Efficient Generative Modeling of WSS Signals", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work addresses the challenge of making generative models suitable for\nresource-constrained environments like mobile wireless communication systems.\nWe propose a generative model that integrates Autoregressive (AR)\nparameterization into a Gaussian Mixture Model (GMM) for modeling Wide-Sense\nStationary (WSS) processes. By exploiting model-based insights allowing for\nstructural constraints, the approach significantly reduces parameters while\nmaintaining high modeling accuracy. Channel estimation experiments show that\nthe model can outperform standard GMMs and variants using Toeplitz or circulant\ncovariances, particularly with small sample sizes. For larger datasets, it\nmatches the performance of conventional methods while improving computational\nefficiency and reducing the memory requirements."}
{"id": "2509.17490", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17490", "abs": "https://arxiv.org/abs/2509.17490", "authors": ["Yuseon Choi", "Hyeonseung Kim", "Jewoo Jun", "Jong Won Shin"], "title": "FUN-SSL: Full-band Layer Followed by U-Net with Narrow-band Layers for Multiple Moving Sound Source Localization", "comment": "Submitted to ICASSP 2026", "summary": "Dual-path processing along the temporal and spectral dimensions has shown to\nbe effective in various speech processing applications. While the sound source\nlocalization (SSL) models utilizing dual-path processing such as the FN-SSL and\nIPDnet demonstrated impressive performances in localizing multiple moving\nsources, they require significant amount of computation. In this paper, we\npropose an architecture for SSL which introduces a U-Net to perform narrow-band\nprocessing in multiple resolutions to reduce computational complexity. The\nproposed model replaces the full-narrow network block in the IPDnet consisting\nof one full-band LSTM layer along the spectral dimension followed by one\nnarrow-band LSTM layer along the temporal dimension with the FUN block composed\nof one Full-band layer followed by a U-net with Narrow-band layers in multiple\nscales. On top of the skip connections within each U-Net, we also introduce the\nskip connections between FUN blocks to enrich information. Experimental results\nshowed that the proposed FUN-SSL outperformed previously proposed approaches\nwith computational complexity much lower than that of the IPDnet."}
{"id": "2509.17585", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17585", "abs": "https://arxiv.org/abs/2509.17585", "authors": ["Viola Negroni", "Davide Salvi", "Alessandro Ilic Mezza", "Paolo Bestagini", "Stefano Tubaro"], "title": "Attention-based Mixture of Experts for Robust Speech Deepfake Detection", "comment": "Accepted @ IEEE WIFS 2025", "summary": "AI-generated speech is becoming increasingly used in everyday life, powering\nvirtual assistants, accessibility tools, and other applications. However, it is\nalso being exploited for malicious purposes such as impersonation,\nmisinformation, and biometric spoofing. As speech deepfakes become nearly\nindistinguishable from real human speech, the need for robust detection methods\nand effective countermeasures has become critically urgent. In this paper, we\npresent the ISPL's submission to the SAFE challenge at IH&MMSec 2025, where our\nsystem ranked first across all tasks. Our solution introduces a novel approach\nto audio deepfake detection based on a Mixture of Experts architecture. The\nproposed system leverages multiple state-of-the-art detectors, combining their\noutputs through an attention-based gating network that dynamically weights each\nexpert based on the input speech signal. In this design, each expert develops a\nspecialized understanding of the shared training data by learning to capture\ndifferent complementary aspects of the same input through inductive biases.\nExperimental results indicate that our method outperforms existing approaches\nacross multiple datasets. We further evaluate and analyze the performance of\nour system in the SAFE challenge."}
{"id": "2509.17983", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17983", "abs": "https://arxiv.org/abs/2509.17983", "authors": ["Boxuan Sun", "Hongliang Luo", "Shaodan Ma", "Feifei Gao"], "title": "Bridge Micro-Deformation Monitoring Scheme with Integrated Sensing and Communications", "comment": "The manuscript was submitted to IEEE Transactions on Wireless\n  Communications (IEEE TWC) on April 22, 2025, and a major revision request was\n  received on August 4, 2025", "summary": "In this paper, we propose a novel integrated sensing and communications\n(ISAC) scheme to perform bridge micro-deformation monitoring (BMDM) in complex\nenvironments. We first provide an excitation-bridge coupling model to represent\nthe micro-deformation process of the bridge. Next, we design a novel frame\nstructure for BMDM applications, and construct the OFDM echo channel model for\nbasic scene of BMDM, including micro-deformation, dynamic objects, and static\nenvironment. Then, we develop a phasor statistical analysis method based on\naverage cancellation algorithm to suppress the interference of dynamic objects,\nas well as a circle fitting method based on least squares algorithm to remove\nthe interference of static environment near the monitoring area. Furthermore,\nwe extract the micro-deformation feature vector from the OFDM echo signals\nafter inverse discrete fourier transform (IDFT), and derive vertical\nmicro-deformation value with the time-frequency phase resources. Simulation\nresults demonstrate the effectiveness of the proposed BMDM scheme and its\nrobustness against both dynamic interferences and static interferences."}
{"id": "2509.17516", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17516", "abs": "https://arxiv.org/abs/2509.17516", "authors": ["Min Liu", "JingJing Yin", "Xiang Zhang", "Siyu Hao", "Yanni Hu", "Bin Lin", "Yuan Feng", "Hongbin Zhou", "Jianhao Ye"], "title": "Audiobook-CC: Controllable Long-context Speech Generation for Multicast Audiobook", "comment": null, "summary": "Existing text-to-speech systems predominantly focus on single-sentence\nsynthesis and lack adequate contextual modeling as well as fine-grained\nperformance control capabilities for generating coherent multicast audiobooks.\nTo address these limitations, we propose a context-aware and emotion\ncontrollable speech synthesis framework specifically engineered for multicast\naudiobooks with three key innovations: a context mechanism for contextual\nconsistency, a disentanglement paradigm to decouple style control from speech\nprompts for semantic consistency, and self-distillation to boost emotional\nexpressiveness and instruction controllability. Experimental results show\nsuperior performance across the generation of narration, dialogue, and the\nwhole chapter, significantly outperforming existing baselines. Ablation studies\nare conducted to validate the effectiveness of our proposed methods. Demo\nsamples can be found in https://everest-ai.github.io/."}
{"id": "2509.17609", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17609", "abs": "https://arxiv.org/abs/2509.17609", "authors": ["Chang Li", "Zehua Chen", "Liyuan Wang", "Jun Zhu"], "title": "Audio Super-Resolution with Latent Bridge Models", "comment": "Accepted at NeurIPS 2025", "summary": "Audio super-resolution (SR), i.e., upsampling the low-resolution (LR)\nwaveform to the high-resolution (HR) version, has recently been explored with\ndiffusion and bridge models, while previous methods often suffer from\nsub-optimal upsampling quality due to their uninformative generation prior.\nTowards high-quality audio super-resolution, we present a new system with\nlatent bridge models (LBMs), where we compress the audio waveform into a\ncontinuous latent space and design an LBM to enable a latent-to-latent\ngeneration process that naturally matches the LR-toHR upsampling process,\nthereby fully exploiting the instructive prior information contained in the LR\nwaveform. To further enhance the training results despite the limited\navailability of HR samples, we introduce frequency-aware LBMs, where the prior\nand target frequency are taken as model input, enabling LBMs to explicitly\nlearn an any-to-any upsampling process at the training stage. Furthermore, we\ndesign cascaded LBMs and present two prior augmentation strategies, where we\nmake the first attempt to unlock the audio upsampling beyond 48 kHz and empower\na seamless cascaded SR process, providing higher flexibility for audio\npost-production. Comprehensive experimental results evaluated on the VCTK,\nESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate\nthat we achieve state-of-the-art objective and perceptual quality for\nany-to-48kHz SR across speech, audio, and music signals, as well as setting the\nfirst record for any-to-192kHz audio SR. Demo at https://AudioLBM.github.io/."}
{"id": "2509.16358", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.16358", "abs": "https://arxiv.org/abs/2509.16358", "authors": ["Jesper Brunnström", "Martin Bo Møller", "Jan Østergaard", "Shoichi Koyama", "Toon van Waterschoot", "Marc Moonen"], "title": "Sound field estimation with moving microphones using kernel ridge regression", "comment": null, "summary": "Sound field estimation with moving microphones can increase flexibility,\ndecrease measurement time, and reduce equipment constraints compared to using\nstationary microphones. In this paper a sound field estimation method based on\nkernel ridge regression (KRR) is proposed for moving microphones. The proposed\nKRR method is constructed using a discrete time continuous space sound field\nmodel based on the discrete Fourier transform and the Herglotz wave function.\nThe proposed method allows for the inclusion of prior knowledge as a\nregularization penalty, similar to kernel-based methods with stationary\nmicrophones, which is novel for moving microphones. Using a directional\nweighting for the proposed method, the sound field estimates are improved,\nwhich is demonstrated on both simulated and real data. Due to the high\ncomputational cost of sound field estimation with moving microphones, an\napproximate KRR method is proposed, using random Fourier features (RFF) to\napproximate the kernel. The RFF method is shown to decrease computational cost\nwhile obtaining less accurate estimates compared to KRR, providing a trade-off\nbetween cost and performance."}
{"id": "2509.17661", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17661", "abs": "https://arxiv.org/abs/2509.17661", "authors": ["Jacob J Webber", "Oliver Watts", "Lovisa Wihlborg", "David Wheatley", "Johnny Tam", "Christine Weaver", "Suvankar Pal", "Siddharthan Chandran", "Cassia Valentini-Botinhao"], "title": "Comparator Loss: An Ordinal Contrastive Loss to Derive a Severity Score for Speech-based Health Monitoring", "comment": "Submitted to ICASSP 2026. This work is supported by NEURii, a\n  collaborative partnership involving the University of Edinburgh, Gates\n  Ventures, Eisai, LifeArc and Health Data Research UK (HDR UK)", "summary": "Monitoring the progression of neurodegenerative disease has important\napplications in the planning of treatment and the evaluation of future\nmedications. Whereas much of the state-of-the-art in health monitoring from\nspeech has been focused on classifying patients versus healthy controls, or\npredicting real-world health metrics, we propose here a novel measure of\ndisease progression: the severity score. This score is derived from a model\ntrained to minimize what we call the comparator loss. The comparator loss\nensures scores follow an ordering relation, which can be based on diagnosis,\nclinically annotated scores, or simply the chronological order of the\nrecordings. In addition to giving a more detailed picture than a simple\ndiscrete classification, the proposed comparator loss-based system has the\npotential to incorporate information from disparate health metrics, which is\ncritical for making full use of small health-related datasets. We evaluated our\nproposed models based on their ability to affirmatively track the progression\nof patients with motor neuron disease (MND), the correlation of their output\nwith clinical annotations such as ALSFRS-R, as well as their ability to\ndistinguish between subjects with MND and healthy controls."}
{"id": "2509.17800", "categories": ["cs.SD", "cs.OH"], "pdf": "https://arxiv.org/pdf/2509.17800", "abs": "https://arxiv.org/abs/2509.17800", "authors": ["Harshit", "Rahul Jana", "Ritesh Kumar"], "title": "Convolutional Neural Network Optimization for Beehive Classification Using Bioacoustic Signals", "comment": null, "summary": "The behavior of honeybees is an important ecological phenomenon not only in\nterms of honey and beeswax production but also due to the proliferation of\nflora and fauna around it. The best way to study this significant phenomenon is\nby non-invasive monitoring of beehives using the sounds produced by various\nbody movements that give out audio signals which can be exploited for various\npredictions related to the objectives mentioned above. This study investigates\nthe application of Convolutional Neural Networks to classify and monitor\ndifferent hive states with the help of joint time and frequency image\nrepresentations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and\nCochleagram. Our findings indicate that the Cochleagram outperformed all the\nother representations, achieving an accuracy of 98.31% on unseen data.\nFurthermore, we employed various strategies including pruning, quantization,\nand knowledge distillation to optimize the network and prevent any potential\nissues with model size. With these optimizations, the network size was lowered\nby 91.8% and the inference time was accelerated by 66%, increasing its\nsuitability for real-time applications. Thus our study emphasizes the\nsignificance of using optimization approaches to minimize model size, avoid\ndeployment problems, and expedite inference for real-time application as well\nas the selection of an appropriate time-frequency representation for optimal\nperformance."}
{"id": "2509.17490", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.17490", "abs": "https://arxiv.org/abs/2509.17490", "authors": ["Yuseon Choi", "Hyeonseung Kim", "Jewoo Jun", "Jong Won Shin"], "title": "FUN-SSL: Full-band Layer Followed by U-Net with Narrow-band Layers for Multiple Moving Sound Source Localization", "comment": "Submitted to ICASSP 2026", "summary": "Dual-path processing along the temporal and spectral dimensions has shown to\nbe effective in various speech processing applications. While the sound source\nlocalization (SSL) models utilizing dual-path processing such as the FN-SSL and\nIPDnet demonstrated impressive performances in localizing multiple moving\nsources, they require significant amount of computation. In this paper, we\npropose an architecture for SSL which introduces a U-Net to perform narrow-band\nprocessing in multiple resolutions to reduce computational complexity. The\nproposed model replaces the full-narrow network block in the IPDnet consisting\nof one full-band LSTM layer along the spectral dimension followed by one\nnarrow-band LSTM layer along the temporal dimension with the FUN block composed\nof one Full-band layer followed by a U-net with Narrow-band layers in multiple\nscales. On top of the skip connections within each U-Net, we also introduce the\nskip connections between FUN blocks to enrich information. Experimental results\nshowed that the proposed FUN-SSL outperformed previously proposed approaches\nwith computational complexity much lower than that of the IPDnet."}
{"id": "2509.17741", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17741", "abs": "https://arxiv.org/abs/2509.17741", "authors": ["Shrishti Saha Shetu", "Emanuël A. P. Habets", "Andreas Brendel"], "title": "GAN-Based Multi-Microphone Spatial Target Speaker Extraction", "comment": null, "summary": "Spatial target speaker extraction isolates a desired speaker's voice in\nmulti-speaker environments using spatial information, such as the direction of\narrival (DoA). Although recent deep neural network (DNN)-based discriminative\nmethods have shown significant performance improvements, the potential of\ngenerative approaches, such as generative adversarial networks (GANs), remains\nlargely unexplored for this problem. In this work, we demonstrate that a GAN\ncan effectively leverage both noisy mixtures and spatial information to extract\nand generate the target speaker's speech. By conditioning the GAN on\nintermediate features of a discriminative spatial filtering model in addition\nto DoA, we enable steerable target extraction with high spatial resolution of 5\ndegrees, outperforming state-of-the-art discriminative methods in perceptual\nquality-based objective metrics."}
{"id": "2509.17883", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.17883", "abs": "https://arxiv.org/abs/2509.17883", "authors": ["Qiushi Han", "Yuan Liao", "Youhao Si", "Liya Huang"], "title": "Brainprint-Modulated Target Speaker Extraction", "comment": "5 pages, 2 figures, conference", "summary": "Achieving robust and personalized performance in neuro-steered Target Speaker\nExtraction (TSE) remains a significant challenge for next-generation hearing\naids. This is primarily due to two factors: the inherent non-stationarity of\nEEG signals across sessions, and the high inter-subject variability that limits\nthe efficacy of generalized models. To address these issues, we propose\nBrainprint-Modulated Target Speaker Extraction (BM-TSE), a novel framework for\npersonalized and high-fidelity extraction. BM-TSE first employs a\nspatio-temporal EEG encoder with an Adaptive Spectral Gain (ASG) module to\nextract stable features resilient to non-stationarity. The core of our\nframework is a personalized modulation mechanism, where a unified brainmap\nembedding is learned under the joint supervision of subject identification\n(SID) and auditory attention decoding (AAD) tasks. This learned brainmap,\nencoding both static user traits and dynamic attentional states, actively\nrefines the audio separation process, dynamically tailoring the output to each\nuser. Evaluations on the public KUL and Cocktail Party datasets demonstrate\nthat BM-TSE achieves state-of-the-art performance, significantly outperforming\nexisting methods. Our code is publicly accessible at:\nhttps://github.com/rosshan-orz/BM-TSE."}
{"id": "2509.17965", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17965", "abs": "https://arxiv.org/abs/2509.17965", "authors": ["Sai Samrat Kankanala", "Ram Chandra", "Sriram Ganapathy"], "title": "Benchmarking Humans and Machines on Complex Multilingual Speech Understanding Tasks", "comment": "5 Pages, 1 Figure", "summary": "Auditory attention and selective phase-locking are central to human speech\nunderstanding in complex acoustic scenes and cocktail party settings, yet these\ncapabilities in multilingual subjects remain poorly understood. While machine\nunderstanding of natural speech has advanced in recent years, questions persist\nabout comprehension of overlapped and mixed-channel speech. We propose a\nsystematic paradigm for studying humans and machines in speech\nquestion-answering tasks in multilingual settings with clean and mixed-channel\nspeech. For human listeners, selective attention to a target speaker was\nsignificantly better in their native language (L1) than in their second\nlanguage (L2). For machine listening, speech-based large language models (LLMs)\nmatch or exceed human performance in clean, single-speaker conditions but often\nstruggle to selectively attend in two-speaker settings. These results reveal a\nkey divergence: humans rely on attentional cues that are more streamlined in\ntheir native language, whereas LLMs default to parallel information extraction\nwhich exceed human skills."}
{"id": "2509.16329", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16329", "abs": "https://arxiv.org/abs/2509.16329", "authors": ["Orchid Chetia Phukan", "Girish", "Mohd Mujtaba Akhtar", "Panchal Nayak", "Priyabrata Mallick", "Swarup Ranjan Behera", "Parabattina Bhagath", "Pailla Balakrishna Reddy", "Arun Balaji Buduru"], "title": "Investigating Polyglot Speech Foundation Models for Learning Collective Emotion from Crowds", "comment": "Accepted to APSIPA-ASC 2025", "summary": "This paper investigates the polyglot (multilingual) speech foundation models\n(SFMs) for Crowd Emotion Recognition (CER). We hypothesize that polyglot SFMs,\npre-trained on diverse languages, accents, and speech patterns, are\nparticularly adept at navigating the noisy and complex acoustic environments\ncharacteristic of crowd settings, thereby offering a significant advantage for\nCER. To substantiate this, we perform a comprehensive analysis, comparing\npolyglot, monolingual, and speaker recognition SFMs through extensive\nexperiments on a benchmark CER dataset across varying audio durations (1 sec,\n500 ms, and 250 ms). The results consistently demonstrate the superiority of\npolyglot SFMs, outperforming their counterparts across all audio lengths and\nexcelling even with extremely short-duration inputs. These findings pave the\nway for adaptation of SFMs in setting up new benchmarks for CER."}
{"id": "2509.17988", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17988", "abs": "https://arxiv.org/abs/2509.17988", "authors": ["Zirui Li", "Jens Edlund", "Yicheng Gu", "Nhan Phan", "Lauri Juvela", "Mikko Kurimo"], "title": "Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech", "comment": null, "summary": "Text-to-speech (TTS) development is limited by scarcity of high-quality,\npublicly available speech data for most languages outside a few high-resource\nlanguages. We present Nord-Parl-TTS, an open TTS dataset for Finnish and\nSwedish based on speech found in the wild. Using recordings of Nordic\nparliamentary proceedings, we extract 900 hours of Finnish and 5090 hours of\nSwedish speech suitable for TTS training. The dataset is built using an adapted\nversion of the Emilia data processing pipeline and includes unified evaluation\nsets to support model development and benchmarking. By offering open,\nlarge-scale data for Finnish and Swedish, Nord-Parl-TTS narrows the resource\ngap in TTS between high- and lower-resourced languages."}
{"id": "2509.16342", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16342", "abs": "https://arxiv.org/abs/2509.16342", "authors": ["Sean Turland", "Eloi Moliner", "Vesa Välimäki"], "title": "Similarity-Guided Diffusion for Long-Gap Music Inpainting", "comment": "5 pages, 2 figures. Submitted to IEEE ICASSP 2026. Audio examples and\n  supplementary material are available at: https://s-turland.github.io/SimDPS/", "summary": "Music inpainting aims to reconstruct missing segments of a corrupted\nrecording. While diffusion-based generative models improve reconstruction for\nmedium-length gaps, they often struggle to preserve musical plausibility over\nmulti-second gaps. We introduce Similarity-Guided Diffusion Posterior Sampling\n(SimDPS), a hybrid method that combines diffusion-based inference with\nsimilarity search. Candidate segments are first retrieved from a corpus based\non contextual similarity, then incorporated into a modified likelihood that\nguides the diffusion process toward contextually consistent reconstructions.\nSubjective evaluation on piano music inpainting with 2-s gaps shows that the\nproposed SimDPS method enhances perceptual plausibility compared to unguided\ndiffusion and frequently outperforms similarity search alone when moderately\nsimilar candidates are available. These results demonstrate the potential of a\nhybrid similarity approach for diffusion-based audio enhancement with long\ngaps."}
{"id": "2509.16522", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16522", "abs": "https://arxiv.org/abs/2509.16522", "authors": ["Tse-Yang Chen", "Yuh-Jzer Joung"], "title": "Etude: Piano Cover Generation with a Three-Stage Approach - Extract, strucTUralize, and DEcode", "comment": null, "summary": "Piano cover generation aims to automatically transform a pop song into a\npiano arrangement. While numerous deep learning approaches have been proposed,\nexisting models often fail to maintain structural consistency with the original\nsong, likely due to the absence of beat-aware mechanisms or the difficulty of\nmodeling complex rhythmic patterns. Rhythmic information is crucial, as it\ndefines structural similarity (e.g., tempo, BPM) and directly impacts the\noverall quality of the generated music.\n  In this paper, we introduce Etude, a three-stage architecture consisting of\nExtract, strucTUralize, and DEcode stages. By pre-extracting rhythmic\ninformation and applying a novel, simplified REMI-based tokenization, our model\nproduces covers that preserve proper song structure, enhance fluency and\nmusical dynamics, and support highly controllable generation through style\ninjection. Subjective evaluations with human listeners show that Etude\nsubstantially outperforms prior models, achieving a quality level comparable to\nthat of human composers."}
{"id": "2509.16480", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16480", "abs": "https://arxiv.org/abs/2509.16480", "authors": ["Anup Singh", "Kris Demuynck"], "title": "Harmonic Summation-Based Robust Pitch Estimation in Noisy and Reverberant Environments", "comment": null, "summary": "Accurate pitch estimation is essential for numerous speech processing\napplications, yet it remains challenging in high-distortion environments. This\npaper proposes a robust pitch estimation method that delivers robust pitch\nestimates in challenging noise environments. Our approach computes the\nNormalized Average Magnitude Difference Function (NAMDF), transforms it into a\nlikelihood function, and generates probabilistic pitch states for frames at\neach sample shift. To enhance noise robustness, we aggregate likelihood values\nacross integer multiples of the pitch period and neighboring frames.\nFurthermore, we introduce a simple yet effective continuity constraint in the\nViterbi algorithm to refine pitch selection among multiple candidates.\nExperimental results show that our method consistently achieves lower Gross\nPitch Error (GPE) and Voicing Decision Error (VDE) across various SNR levels,\noutperforming existing methods in both noisy and reverberant conditions."}
{"id": "2509.16566", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16566", "abs": "https://arxiv.org/abs/2509.16566", "authors": ["Omar Eldeeb", "Martin Malandro"], "title": "Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks", "comment": null, "summary": "Current methods for Music Structure Analysis (MSA) focus primarily on audio\ndata. While symbolic music can be synthesized into audio and analyzed using\nexisting MSA techniques, such an approach does not exploit symbolic music's\nrich explicit representation of pitch, timing, and instrumentation. A key\nsubproblem of MSA is section boundary detection-determining whether a given\npoint in time marks the transition between musical sections. In this paper, we\nstudy automatic section boundary detection for symbolic music. First, we\nintroduce a human-annotated MIDI dataset for section boundary detection,\nconsisting of metadata from 6134 MIDI files that we manually curated from the\nLakh MIDI dataset. Second, we train a deep learning model to classify the\npresence of section boundaries within a fixed-length musical window. Our data\nrepresentation involves a novel encoding scheme based on synthesized overtones\nto encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model\nachieves an F1 score of 0.77, improving over the analogous audio-based\nsupervised learning approach and the unsupervised block-matching segmentation\n(CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset,\ncode, and models."}
{"id": "2509.16481", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16481", "abs": "https://arxiv.org/abs/2509.16481", "authors": ["Ui-Hyeop Shin", "Bon Hyeok Ku", "Hyung-Min Park"], "title": "TF-CorrNet: Leveraging Spatial Correlation for Continuous Speech Separation", "comment": "Accepted in SPL", "summary": "In general, multi-channel source separation has utilized inter-microphone\nphase differences (IPDs) concatenated with magnitude information in\ntime-frequency domain, or real and imaginary components stacked along the\nchannel axis. However, the spatial information of a sound source is\nfundamentally contained in the differences between microphones, specifically in\nthe correlation between them, while the power of each microphone also provides\nvaluable information about the source spectrum, which is why the magnitude is\nalso included. Therefore, we propose a network that directly leverages a\ncorrelation input with phase transform (PHAT)-beta to estimate the separation\nfilter. In addition, the proposed TF-CorrNet processes the features alternately\nacross time and frequency axes as a dual-path strategy in terms of spatial\ninformation. Furthermore, we add a spectral module to model source-related\ndirect time-frequency patterns for improved speech separation. Experimental\nresults demonstrate that the proposed TF-CorrNet effectively separates the\nspeech sounds, showing high performance with a low computational cost in the\nLibriCSS dataset."}
{"id": "2509.16649", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16649", "abs": "https://arxiv.org/abs/2509.16649", "authors": ["Hyun Jun Kim", "Hyeong Yong Choi", "Changwon Lim"], "title": "AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval", "comment": "5 pages, 1 figure, DCASE2025 Task2 technical report", "summary": "This report presents the AISTAT team's submission to the language-based audio\nretrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder\narchitecture, where audio and text modalities are encoded separately, and their\nrepresentations are aligned using contrastive learning. Drawing inspiration\nfrom methodologies of the previous year's challenge, we implemented a\ndistillation approach and leveraged large language models (LLMs) for effective\ndata augmentation techniques, including back-translation and LLM mix.\nAdditionally, we incorporated clustering to introduce an auxiliary\nclassification task for further finetuning. Our best single system achieved a\nmAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on\nthe Clotho development test split."}
{"id": "2509.16603", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16603", "abs": "https://arxiv.org/abs/2509.16603", "authors": ["Maurício do V. M. da Costa", "Eloi Moliner"], "title": "An Octave-based Multi-Resolution CQT Architecture for Diffusion-based Audio Generation", "comment": "accepted at IEEE International Symposium on the Internet of Sounds", "summary": "This paper introduces MR-CQTdiff, a novel neural-network architecture for\ndiffusion-based audio generation that leverages a multi-resolution Constant-$Q$\nTransform (C$Q$T). The proposed architecture employs an efficient, invertible\nCQT framework that adjusts the time-frequency resolution on an octave-by-octave\nbasis. This design addresses the issue of low temporal resolution at lower\nfrequencies, enabling more flexible and expressive audio generation. We conduct\nan evaluation using the Fr\\'echet Audio Distance (FAD) metric across various\narchitectures and two datasets. Experimental results demonstrate that\nMR-CQTdiff achieves state-of-the-art audio quality, outperforming competing\narchitectures."}
{"id": "2509.16662", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16662", "abs": "https://arxiv.org/abs/2509.16662", "authors": ["Eunjin Choi", "Hyerin Kim", "Jiwoo Ryu", "Juhan Nam", "Dasaem Jeong"], "title": "On the de-duplication of the Lakh MIDI dataset", "comment": "The paper has been accepted for publication at ISMIR 2025", "summary": "A large-scale dataset is essential for training a well-generalized\ndeep-learning model. Most such datasets are collected via scraping from various\ninternet sources, inevitably introducing duplicated data. In the symbolic music\ndomain, these duplicates often come from multiple user arrangements and\nmetadata changes after simple editing. However, despite critical issues such as\nunreliable training evaluation from data leakage during random splitting,\ndataset duplication has not been extensively addressed in the MIR community.\nThis study investigates the dataset duplication issues regarding Lakh MIDI\nDataset (LMD), one of the largest publicly available sources in the symbolic\nmusic domain. To find and evaluate the best retrieval method for duplicated\ndata, we employed the Clean MIDI subset of the LMD as a benchmark test set, in\nwhich different versions of the same songs are grouped together. We first\nevaluated rule-based approaches and previous symbolic music retrieval models\nfor de-duplication and also investigated with a contrastive learning-based BERT\nmodel with various augmentations to find duplicate files. As a result, we\npropose three different versions of the filtered list of LMD, which filters out\nat least 38,134 samples in the most conservative settings among 178,561 files."}
{"id": "2509.16622", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16622", "abs": "https://arxiv.org/abs/2509.16622", "authors": ["Mengqi Wang", "Zhan Liu", "Zengrui Jin", "Guangzhi Sun", "Chao Zhang", "Philip C. Woodland"], "title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing", "comment": null, "summary": "Diffusion-based large language models (DLLMs) have recently attracted growing\ninterest as an alternative to autoregressive decoders. In this work, we present\nan empirical study on using the diffusion-based large language model LLaDA for\nautomatic speech recognition (ASR). We first investigate its use as an external\ndeliberation-based processing module for Whisper-LLaMA transcripts. By\nleveraging the bidirectional attention and denoising capabilities of LLaDA, we\nexplore random masking, low-confidence masking, and semi-autoregressive\nstrategies, showing that Whisper-LLaDA substantially reduces WER compared with\nthe baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER\non test-clean/test-other, representing a 12.3% relative improvement over the\nWhisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA\nwithout acoustic features fails to improve accuracy, highlighting the\nimportance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA\nas a standalone decoder for ASR with diffusion-based and semi-autoregressive\ndecoding. Most experimental configurations achieve faster inference than the\nWhisper-LLaMA baseline, although recognition accuracy is slightly lower. These\nfindings offer an empirical view of diffusion-based LLMs for ASR and point to\npromising directions for improvements."}
{"id": "2509.16670", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16670", "abs": "https://arxiv.org/abs/2509.16670", "authors": ["Wenhuan Lu", "Xinyue Song", "Wenjun Ke", "Zhizhi Yu", "Wenhao Yang", "Jianguo Wei"], "title": "Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection", "comment": null, "summary": "Audio grounding, or speech-driven open-set object detection, aims to localize\nand identify objects directly from speech, enabling generalization beyond\npredefined categories. This task is crucial for applications like human-robot\ninteraction where textual input is impractical. However, progress in this\ndomain faces a fundamental bottleneck from the scarcity of large-scale, paired\naudio-image data, and is further constrained by previous methods that rely on\nindirect, text-mediated pipelines. In this paper, we introduce Speech-to-See\n(Speech2See), an end-to-end approach built on a pre-training and fine-tuning\nparadigm. Specifically, in the pre-training stage, we design a Query-Guided\nSemantic Aggregation module that employs learnable queries to condense\nredundant speech embeddings into compact semantic representations. During\nfine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts\n(MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation.\nExtensive experiments show that Speech2See achieves robust and adaptable\nperformance across multiple benchmarks, demonstrating its strong generalization\nability and broad applicability."}
{"id": "2509.16705", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16705", "abs": "https://arxiv.org/abs/2509.16705", "authors": ["Shuubham Ojha", "Felix Gervits", "Carol Espy-Wilson"], "title": "Reverse Attention for Lightweight Speech Enhancement on Edge Devices", "comment": null, "summary": "This paper introduces a lightweight deep learning model for real-time speech\nenhancement, designed to operate efficiently on resource-constrained devices.\nThe proposed model leverages a compact architecture that facilitates rapid\ninference without compromising performance. Key contributions include infusing\nsoft attention-based attention gates in the U-Net architecture which is known\nto perform well for segmentation tasks and is optimized for GPUs. Experimental\nevaluations demonstrate that the model achieves competitive speech quality and\nintelligibility metrics, such as PESQ and Word Error Rates (WER), improving the\nperformance of similarly sized baseline models. We are able to achieve a 6.24%\nWER improvement and a 0.64 PESQ score improvement over un-enhanced waveforms."}
{"id": "2509.16718", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16718", "abs": "https://arxiv.org/abs/2509.16718", "authors": ["Vishnu Raja", "Adithya V Ganesan", "Anand Syamkumar", "Ritwik Banerjee", "H Andrew Schwartz"], "title": "Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies", "comment": "Will appear in EMNLP 2025 Main Proceedings", "summary": "State-of-the-art automatic speech recognition (ASR) models like Whisper,\nperform poorly on atypical speech, such as that produced by individuals with\ndysarthria. Past works for atypical speech have mostly investigated fully\npersonalized (or idiosyncratic) models, but modeling strategies that can both\ngeneralize and handle idiosyncracy could be more effective for capturing\natypical speech. To investigate this, we compare four strategies: (a)\n$\\textit{normative}$ models trained on typical speech (no personalization), (b)\n$\\textit{idiosyncratic}$ models completely personalized to individuals, (c)\n$\\textit{dysarthric-normative}$ models trained on other dysarthric speakers,\nand (d) $\\textit{dysarthric-idiosyncratic}$ models which combine strategies by\nfirst modeling normative patterns before adapting to individual speech. In this\ncase study, we find the dysarthric-idiosyncratic model performs better than\nidiosyncratic approach while requiring less than half as much personalized data\n(36.43 WER with 128 train size vs 36.99 with 256). Further, we found that\ntuning the speech encoder alone (as opposed to the LM decoder) yielded the best\nresults reducing word error rate from 71% to 32% on average. Our findings\nhighlight the value of leveraging both normative (cross-speaker) and\nidiosyncratic (speaker-specific) patterns to improve ASR for underrepresented\nspeech populations."}
{"id": "2509.16760", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16760", "abs": "https://arxiv.org/abs/2509.16760", "authors": ["Samuel Rey", "Luca Martino", "Roberto San Millan", "Eduardo Morgado"], "title": "Feature Selection via Graph Topology Inference for Soundscape Emotion Recognition", "comment": null, "summary": "Research on soundscapes has shifted the focus of environmental acoustics from\nnoise levels to the perception of sounds, incorporating contextual factors.\nSoundscape emotion recognition (SER) models perception using a set of features,\nwith arousal and valence commonly regarded as sufficient descriptors of affect.\nIn this work, we blend \\emph{graph learning} techniques with a novel\n\\emph{information criterion} to develop a feature selection framework for SER.\nSpecifically, we estimate a sparse graph representation of feature relations\nusing linear structural equation models (SEM) tailored to the widely used\nEmo-Soundscapes dataset. The resulting graph captures the relations between\ninput features and the two emotional outputs. To determine the appropriate\nlevel of sparsity, we propose a novel \\emph{generalized elbow detector}, which\nprovides both a point estimate and an uncertainty interval. We conduct an\nextensive evaluation of our methods, including visualizations of the inferred\nrelations. While several of our findings align with previous studies, the graph\nrepresentation also reveals a strong connection between arousal and valence,\nchallenging common SER assumptions."}
{"id": "2509.16862", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16862", "abs": "https://arxiv.org/abs/2509.16862", "authors": ["Rinka Nobukawa", "Makito Kitamura", "Tomohiko Nakamura", "Shinnosuke Takamichi", "Hiroshi Saruwatari"], "title": "Drum-to-Vocal Percussion Sound Conversion and Its Evaluation Methodology", "comment": "6 pages, 5 figures, accepted for 2025 Asia Pacific Signal and\n  Information Processing Association Annual Summit and Conference (APSIPA ASC)", "summary": "This paper defines the novel task of drum-to-vocal percussion (VP) sound\nconversion. VP imitates percussion instruments through human vocalization and\nis frequently employed in contemporary a cappella music. It exhibits acoustic\nproperties distinct from speech and singing (e.g., aperiodicity, noisy\ntransients, and the absence of linguistic structure), making conventional\nspeech or singing synthesis methods unsuitable. We thus formulate VP synthesis\nas a timbre transfer problem from drum sounds, leveraging their rhythmic and\ntimbral correspondence. To support this formulation, we define three\nrequirements for successful conversion: rhythmic fidelity, timbral consistency,\nand naturalness as VP. We also propose corresponding subjective evaluation\ncriteria. We implement two baseline conversion methods using a neural audio\nsynthesizer, the real-time audio variational autoencoder (RAVE), with and\nwithout vector quantization (VQ). Subjective experiments show that both methods\nproduce plausible VP outputs, with the VQ-based RAVE model yielding more\nconsistent conversion."}
{"id": "2509.16901", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16901", "abs": "https://arxiv.org/abs/2509.16901", "authors": ["Mandip Goswami"], "title": "Automotive Sound Quality for EVs: Psychoacoustic Metrics with Reproducible AI/ML Baselines", "comment": null, "summary": "We present an open, reproducible reference for automotive sound quality that\nconnects standardized psychoacoustic metrics with lightweight AI/ML baselines,\nwith a specific focus on electric vehicles (EVs). We implement loudness (ISO\n532-1/2), tonality (DIN 45681), and modulation-based descriptors (roughness,\nfluctuation strength), and document assumptions and parameterizations for\nreliable reuse. For modeling, we provide simple, fully reproducible baselines\n(logistic regression, random forest, SVM) on synthetic EV-like cases using\nfixed splits and seeds, reporting accuracy and rank correlations as examples of\nend-to-end workflows rather than a comparative benchmark. Program-level\nnormalization is reported in LUFS via ITU-R BS.1770, while psychoacoustic\nanalysis uses ISO-532 loudness (sones). All figures and tables are regenerated\nby scripts with pinned environments; code and minimal audio stimuli are\nreleased under permissive licenses to support teaching, replication, and\nextension to EV-specific noise phenomena (e.g., inverter whine, reduced\nmasking)."}
{"id": "2509.16926", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16926", "abs": "https://arxiv.org/abs/2509.16926", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment", "comment": "Accepted on Workshop on Detection and Classification of Acoustic\n  Scenes and Events (DCASE 2025)", "summary": "Multi-channel audio alignment is a key requirement in bioacoustic monitoring,\nspatial audio systems, and acoustic localization. However, existing methods\noften struggle to address nonlinear clock drift and lack mechanisms for\nquantifying uncertainty. Traditional methods like Cross-correlation and Dynamic\nTime Warping assume simple drift patterns and provide no reliability measures.\nMeanwhile, recent deep learning models typically treat alignment as a binary\nclassification task, overlooking inter-channel dependencies and uncertainty\nestimation. We introduce a method that combines cross-attention mechanisms with\nconfidence-weighted scoring to improve multi-channel audio synchronization. We\nextend BEATs encoders with cross-attention layers to model temporal\nrelationships between channels. We also develop a confidence-weighted scoring\nfunction that uses the full prediction distribution instead of binary\nthresholding. Our method achieved first place in the BioDCASE 2025 Task 1\nchallenge with 0.30 MSE average across test datasets, compared to 0.58 for the\ndeep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU\ndata (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The\nframework supports probabilistic temporal alignment, moving beyond point\nestimates. While validated in a bioacoustic context, the approach is applicable\nto a broader range of multi-channel audio tasks where alignment confidence is\ncritical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA"}
{"id": "2509.16945", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.16945", "abs": "https://arxiv.org/abs/2509.16945", "authors": ["Jeongmin Lee", "Chanhong Jeon", "Hyungjoo Seo", "Taewook Kang"], "title": "DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time UAV Speech Enhancement", "comment": null, "summary": "This paper proposes DroFiT (Drone Frequency lightweight Transformer for\nspeech enhancement, a single microphone speech enhancement network for severe\ndrone self-noise. DroFit integrates a frequency-wise Transformer with a\nfull/sub-band hybrid encoder-decoder and a TCN back-end for memory-efficient\nstreaming. A learnable skip-and-gate fusion with a combined spectral-temporal\nloss further refines reconstruction. The model is trained on VoiceBank-DEMAND\nmixed with recorded drone noise (-5 to -25 dB SNR) and evaluate using standard\nspeech enhancement metrics and computational efficiency. Experimental results\nshow that DroFiT achieves competitive enhancement performance while\nsignificantly reducing computational and memory demands, paving the way for\nreal-time processing on resource-constrained UAV platforms. Audio demo samples\nare available on our demo page."}
{"id": "2509.16971", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16971", "abs": "https://arxiv.org/abs/2509.16971", "authors": ["Yan Rong", "Chenxing Li", "Dong Yu", "Li Liu"], "title": "AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for Coarse-to-Fine Audio Deep Reasoning", "comment": null, "summary": "Audio deep reasoning is a challenging task that requires expert-level\nperception, multi-step logical inference, and the integration of contextual\nknowledge. However, existing models suffer from a gap between audio perception\nand reasoning abilities due to the lack of training data with explicit\nreasoning chains and the absence of mechanisms for active exploration and\niterative refinement. To address these challenges, we propose\nAudioGenie-Reasoner (AGR), the first unified training-free multi-agent system\nthat coordinates perception and reasoning over an evolving chain of textual\nevidence. Our key idea is a paradigm shift that transforms audio deep reasoning\ninto complex text understanding task from a new perspective, thereby unlocking\nthe full potential of large language models. Specifically, the design of AGR\nmimics the human coarse-to-fine cognitive process. It first transforms the\ninput audio into a coarse text-based document. Then, we design a novel\nproactive iterative document refinement loop, featuring tool-augmented routes\nand specialized agents, to continuously search for missing information and\naugment the evidence chain in a coarse-to-fine manner until sufficient\nquestion-related information is gathered for making final predictions.\nExperimental results show that AGR achieves state-of-the-art (SOTA) performance\nover existing open-source audio deep reasoning models across various\nbenchmarks. The code will be made publicly available."}
{"id": "2509.17247", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17247", "abs": "https://arxiv.org/abs/2509.17247", "authors": ["Dongheon Lee", "Younghoo Kwon", "Jung-Woo Choi"], "title": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene Analysis", "comment": "26 pages, 13 figures, 8 tables, accepted in NeurIPS 2025", "summary": "We propose DeepASA, a one-for-all model for auditory scene analysis that\nperforms multi-input multi-output (MIMO) source separation, dereverberation,\nsound event detection (SED), audio classification, and direction-of-arrival\nestimation (DoAE) within a unified framework. DeepASA is designed for complex\nauditory scenes where multiple, often similar, sound sources overlap in time\nand move dynamically in space. To achieve robust and consistent inference\nacross tasks, we introduce an object-oriented processing (OOP) strategy. This\napproach encapsulates diverse auditory features into object-centric\nrepresentations and refines them through a chain-of-inference (CoI) mechanism.\nThe pipeline comprises a dynamic temporal kernel-based feature extractor, a\ntransformer-based aggregator, and an object separator that yields per-object\nfeatures. These features feed into multiple task-specific decoders. Our\nobject-centric representations naturally resolve the parameter association\nambiguity inherent in traditional track-wise processing. However, early-stage\nobject separation can lead to failure in downstream ASA tasks. To address this,\nwe implement temporal coherence matching (TCM) within the chain-of-inference,\nenabling multi-task fusion and iterative refinement of object features using\nestimated auditory parameters. We evaluate DeepASA on representative spatial\naudio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental\nresults show that our model achieves state-of-the-art performance across all\nevaluated tasks, demonstrating its effectiveness in both source separation and\nauditory parameter estimation under diverse spatial auditory scenes."}
{"id": "2509.16975", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16975", "abs": "https://arxiv.org/abs/2509.16975", "authors": ["Yuhang Jia", "Xu Zhang", "Yang Chen", "Hui Wang", "Enzhi Wang", "Yong Qin"], "title": "Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs", "comment": null, "summary": "Automatic mean opinion score (MOS) prediction provides a more perceptual\nalternative to objective metrics, offering deeper insights into the evaluated\nmodels. With the rapid progress of multimodal large language models (MLLMs),\ntheir enhanced perceptual and reasoning abilities enable more comprehensive and\ninterpretable audio quality assessment. In this work, we tackle the challenging\ntask of audio editing evaluation and propose the first natural language-based\nautomated evaluation framework built on MLLMs. Our approach introduces two\nfine-tuning tasks to boost multi-audio understanding, combined with\nChain-of-Thought prompting, and lightweight instruction tuning, to enhance\nstep-by-step reasoning. Experiment demonstrate that our framework delivers\naccurate, interpretable, and text-based editing evaluation, closely aligning\nwith human judgments and objective metrics while substantially improving over\nbaselines. The code and demo are available at\nhttps://github.com/NKU-HLT/Eval_Reasoning."}
{"id": "2509.17270", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17270", "abs": "https://arxiv.org/abs/2509.17270", "authors": ["Hanlin Yu", "Haoshuai Zhou", "Boxuan Cao", "Changgeng Mo", "Linkai Li", "Shan X. Wang"], "title": "Reference-aware SFM layers for intrusive intelligibility prediction", "comment": "Preprint; submitted to ICASSP 2026. 5 pages. CPC3 system: Dev RMSE\n  22.36, Eval RMSE 24.98 (ranked 1st)", "summary": "Intrusive speech-intelligibility predictors that exploit explicit reference\nsignals are now widespread, yet they have not consistently surpassed\nnon-intrusive systems. We argue that a primary cause is the limited\nexploitation of speech foundation models (SFMs). This work revisits intrusive\nprediction by combining reference conditioning with multi-layer SFM\nrepresentations. Our final system achieves RMSE 22.36 on the development set\nand 24.98 on the evaluation set, ranking 1st on CPC3. These findings provide\npractical guidance for constructing SFM-based intrusive intelligibility\npredictors."}
{"id": "2509.16979", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.16979", "abs": "https://arxiv.org/abs/2509.16979", "authors": ["Boxuan Cao", "Linkai Li", "Hanlin Yu", "Changgeng Mo", "Haoshuai Zhou", "Shan Xiang Wang"], "title": "Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility Prediction for Hearing-Impaired Listeners", "comment": null, "summary": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is\nessential for assessing hearing aid performance, traditionally relying on\nlistening tests or intrusive methods like HASPI. However, these methods require\nclean reference signals, which are often unavailable in real-world conditions,\ncreating a gap between lab-based and real-world assessments. To address this,\nwe propose a non-intrusive intelligibility prediction framework that leverages\nspeech enhancers to provide a parallel enhanced-signal pathway, enabling robust\npredictions without reference signals. We evaluate three state-of-the-art\nenhancers and demonstrate that prediction performance depends on the choice of\nenhancer, with ensembles of strong enhancers yielding the best results. To\nimprove cross-dataset generalization, we introduce a 2-clips augmentation\nstrategy that enhances listener-specific variability, boosting robustness on\nunseen datasets. Our approach consistently outperforms the non-intrusive\nbaseline, CPC2 Champion across multiple datasets, highlighting the potential of\nenhancer-guided non-intrusive intelligibility prediction for real-world\napplications."}
{"id": "2509.17277", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17277", "abs": "https://arxiv.org/abs/2509.17277", "authors": ["Mandip Goswami"], "title": "BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and Psychoacoustics Research", "comment": "Data note; 6 to 8 pages; 1 to 2 figures; dataset: CC0-1.0; code: MIT", "summary": "We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset\n(300-500 clips) designed for rapid, rights-clean experimentation in\nhuman-computer interaction and audio machine learning. Each clip is generated\nfrom a parametric recipe controlling waveform family (sine, square, triangle,\nFM), fundamental frequency, duration, amplitude envelope, amplitude modulation\n(AM), and lightweight Schroeder-style reverberation. We use three reverberation\nsettings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir\nmedium' ('medium') throughout the paper and in the metadata. We release mono 48\nkHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and\ntiny reproducible baselines for (i) waveform-family classification and (ii) f0\nregression on single tones. The corpus targets tasks such as earcon\nclassification, timbre analyses, and onset detection, with clearly stated\nlicensing and limitations. Audio is dedicated to the public domain via CC0-1.0;\ncode is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code:\nhttps://github.com/mandip42/earcons-mini-500."}
{"id": "2509.17006", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17006", "abs": "https://arxiv.org/abs/2509.17006", "authors": ["Ruonan Zhang", "Xiaoyang Hao", "Yichen Han", "Junjie Cao", "Yue Liu", "Kai Zhang"], "title": "MBCodec:Thorough disentangle for high-fidelity audio compression", "comment": "5 pages, 2 figures", "summary": "High-fidelity neural audio codecs in Text-to-speech (TTS) aim to compress\nspeech signals into discrete representations for faithful reconstruction.\nHowever, prior approaches faced challenges in effectively disentangling\nacoustic and semantic information within tokens, leading to a lack of\nfine-grained details in synthesized speech. In this study, we propose MBCodec,\na novel multi-codebook audio codec based on Residual Vector Quantization (RVQ)\nthat learns a hierarchically structured representation. MBCodec leverages\nself-supervised semantic tokenization and audio subband features from the raw\nsignals to construct a functionally-disentangled latent space. In order to\nencourage comprehensive learning across various layers of the codec embedding\nspace, we introduce adaptive dropout depths to differentially train codebooks\nacross layers, and employ a multi-channel pseudo-quadrature mirror filter\n(PQMF) during training. By thoroughly decoupling semantic and acoustic\nfeatures, our method not only achieves near-lossless speech reconstruction but\nalso enables a remarkable 170x compression of 24 kHz audio, resulting in a low\nbit rate of just 2.2 kbps. Experimental evaluations confirm its consistent and\nsubstantial outperformance of baselines across all evaluations."}
{"id": "2509.17286", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17286", "abs": "https://arxiv.org/abs/2509.17286", "authors": ["David Rowe", "Tibor Bece"], "title": "RADE for Land Mobile Radio: A Neural Codec for Transmission of Speech over Baseband FM Radio Channels", "comment": "6 pages, 9 figures", "summary": "In the 1990s Land Mobile Radio (LMR) systems evolved from analog frequency\nmodulation (FM) to standardised digital systems. Both digital and analog FM\nsystems now co-exist in various services and exhibit similar speech quality.\nThe architecture of many digital radios retains the analog FM modulator and\ndemodulator from legacy analog radios, but driven by a multi-level digital\npulse train rather than an analog voice signal. We denote this architecture\nbaseband FM (BBFM). In this paper we describe a modern machine learning\napproach that uses an autoencoder to send high quality, 8 kHz bandwidth speech\nover the BBFM channel. The speech quality is shown to be superior to analog FM\nover simulated LMR channels in the presence of fading, and a demonstration of\nthe system running over commodity UHF radios is presented."}
{"id": "2509.17021", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17021", "abs": "https://arxiv.org/abs/2509.17021", "authors": ["Ruonan Zhang", "Lingzhou Mu", "Xixin Wu", "Kai Zhang"], "title": "Bridging the gap between training and inference in LM-based TTS models", "comment": "5 pages, 4 figures", "summary": "Recent advancements in text-to-speech (TTS) have shown that language model\n(LM) based systems offer competitive performance compared to traditional\napproaches. However, in training, TTS models use ground-truth (GT) tokens as\nprefixes to predict the next token, while in inference these tokens are not\navailable, a gap between training and inference that is often neglected. In\nthis study, we propose a prompt-guided hybrid training scheme to mitigate\nexposure bias in popular LM-based TTS systems. Our core idea is to adopt a\nhybrid training paradigm that combines teacher forcing with free running,\nthereby introducing self-generated tokens into the training process. This makes\nthe training mode more consistent with inference, reducing the\ntraining-inference gap. In addition, we incorporate an EOS prediction mechanism\nduring training to detect incorrect sequence termination and adaptively control\nthe free running process. Experimental results provide a comprehensive\nevaluation of the impact of exposure bias on LM-based TTS, and demonstrate that\nour method effectively narrows the training-inference gap, thereby improving\nthe quality of synthesized long-form speech."}
{"id": "2509.17404", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17404", "abs": "https://arxiv.org/abs/2509.17404", "authors": ["Wei Tan", "Shun Lei", "Huaicheng Zhang", "Guangzheng Li", "Yixuan Zhang", "Hangting Chen", "Jianwei Yu", "Rongzhi Gu", "Dong Yu"], "title": "SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription", "comment": null, "summary": "Artificial Intelligence Generated Content (AIGC) is currently a popular\nresearch area. Among its various branches, song generation has attracted\ngrowing interest. Despite the abundance of available songs, effective data\npreparation remains a significant challenge. Converting these songs into\ntraining-ready datasets typically requires extensive manual labeling, which is\nboth time consuming and costly. To address this issue, we propose SongPrep, an\nautomated preprocessing pipeline designed specifically for song data. This\nframework streamlines key processes such as source separation, structure\nanalysis, and lyric recognition, producing structured data that can be directly\nused to train song generation models. Furthermore, we introduce SongPrepE2E, an\nend-to-end structured lyrics recognition model based on pretrained language\nmodels. Without the need for additional source separation, SongPrepE2E is able\nto analyze the structure and lyrics of entire songs and provide precise\ntimestamps. By leveraging context from the whole song alongside pretrained\nsemantic knowledge, SongPrepE2E achieves low Diarization Error Rate (DER) and\nWord Error Rate (WER) on the proposed SSLD-200 dataset. Downstream tasks\ndemonstrate that training song generation models with the data output by\nSongPrepE2E enables the generated songs to closely resemble those produced by\nhumans."}
{"id": "2509.17052", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.17052", "abs": "https://arxiv.org/abs/2509.17052", "authors": ["Wataru Nakata", "Yuki Saito", "Yota Ueda", "Hiroshi Saruwatari"], "title": "Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing", "comment": "5 pages, 1 figures", "summary": "Large-scale text-to-speech (TTS) systems are limited by the scarcity of\nclean, multilingual recordings. We introduce Sidon, a fast, open-source speech\nrestoration model that converts noisy in-the-wild speech into studio-quality\nspeech and scales to dozens of languages. Sidon consists of two models:\nw2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google's internal\nspeech restoration model with the aim of dataset cleansing for speech\nsynthesis. Sidon is also computationally efficient, running up to 3,390 times\nfaster than real time on a single GPU. We further show that training a TTS\nmodel using a Sidon-cleansed automatic speech recognition corpus improves the\nquality of synthetic speech in a zero-shot setting. Code and model are released\nto facilitate reproducible dataset cleansing for the research community."}
{"id": "2509.17661", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.17661", "abs": "https://arxiv.org/abs/2509.17661", "authors": ["Jacob J Webber", "Oliver Watts", "Lovisa Wihlborg", "David Wheatley", "Johnny Tam", "Christine Weaver", "Suvankar Pal", "Siddharthan Chandran", "Cassia Valentini-Botinhao"], "title": "Comparator Loss: An Ordinal Contrastive Loss to Derive a Severity Score for Speech-based Health Monitoring", "comment": "Submitted to ICASSP 2026. This work is supported by NEURii, a\n  collaborative partnership involving the University of Edinburgh, Gates\n  Ventures, Eisai, LifeArc and Health Data Research UK (HDR UK)", "summary": "Monitoring the progression of neurodegenerative disease has important\napplications in the planning of treatment and the evaluation of future\nmedications. Whereas much of the state-of-the-art in health monitoring from\nspeech has been focused on classifying patients versus healthy controls, or\npredicting real-world health metrics, we propose here a novel measure of\ndisease progression: the severity score. This score is derived from a model\ntrained to minimize what we call the comparator loss. The comparator loss\nensures scores follow an ordering relation, which can be based on diagnosis,\nclinically annotated scores, or simply the chronological order of the\nrecordings. In addition to giving a more detailed picture than a simple\ndiscrete classification, the proposed comparator loss-based system has the\npotential to incorporate information from disparate health metrics, which is\ncritical for making full use of small health-related datasets. We evaluated our\nproposed models based on their ability to affirmatively track the progression\nof patients with motor neuron disease (MND), the correlation of their output\nwith clinical annotations such as ALSFRS-R, as well as their ability to\ndistinguish between subjects with MND and healthy controls."}
{"id": "2509.17162", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.17162", "abs": "https://arxiv.org/abs/2509.17162", "authors": ["Zeyu Xie", "Yaoyun Zhang", "Xuenan Xu", "Yongkang Yin", "Chenxing Li", "Mengyue Wu", "Yuexian Zou"], "title": "FakeSound2: A Benchmark for Explainable and Generalizable Deepfake Sound Detection", "comment": null, "summary": "The rapid development of generative audio raises ethical and security\nconcerns stemming from forged data, making deepfake sound detection an\nimportant safeguard against the malicious use of such technologies. Although\nprior studies have explored this task, existing methods largely focus on binary\nclassification and fall short in explaining how manipulations occur, tracing\nwhere the sources originated, or generalizing to unseen sources-thereby\nlimiting the explainability and reliability of detection. To address these\nlimitations, we present FakeSound2, a benchmark designed to advance deepfake\nsound detection beyond binary accuracy. FakeSound2 evaluates models across\nthree dimensions: localization, traceability, and generalization, covering 6\nmanipulation types and 12 diverse sources. Experimental results show that\nalthough current systems achieve high classification accuracy, they struggle to\nrecognize forged pattern distributions and provide reliable explanations. By\nhighlighting these gaps, FakeSound2 establishes a comprehensive benchmark that\nreveals key challenges and aims to foster robust, explainable, and\ngeneralizable approaches for trustworthy audio authentication."}
{"id": "2509.17164", "categories": ["cs.SD", "eess.AS", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.17164", "abs": "https://arxiv.org/abs/2509.17164", "authors": ["Zeyu Xie", "Xuenan Xu", "Yixuan Li", "Mengyue Wu", "Yuexian Zou"], "title": "STAR: Speech-to-Audio Generation via Representation Learning", "comment": null, "summary": "This work presents STAR, the first end-to-end speech-to-audio generation\nframework, designed to enhance efficiency and address error propagation\ninherent in cascaded systems. Unlike prior approaches relying on text or\nvision, STAR leverages speech as it constitutes a natural modality for\ninteraction. As an initial step to validate the feasibility of the system, we\ndemonstrate through representation learning experiments that spoken sound event\nsemantics can be effectively extracted from raw speech, capturing both auditory\nevents and scene cues. Leveraging the semantic representations, STAR\nincorporates a bridge network for representation mapping and a two-stage\ntraining strategy to achieve end-to-end synthesis. With a 76.9% reduction in\nspeech processing latency, STAR demonstrates superior generation performance\nover the cascaded systems. Overall, STAR establishes speech as a direct\ninteraction signal for audio generation, thereby bridging representation\nlearning and multimodal synthesis. Generated samples are available at\nhttps://zeyuxie29.github.io/STAR."}

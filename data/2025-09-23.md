<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 26]
- [eess.AS](#eess.AS) [Total: 27]
- [cs.SD](#cs.SD) [Total: 25]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [MRADNET: a Compact Radar Object Detector with MetaFormer](https://arxiv.org/abs/2509.16223)
*Huaiyu Chen,Fahed Hassanat,Robert Laganiere,Martin Bouchard*

Main category: eess.SP

TL;DR: mRadNet是一种新型的紧凑型雷达目标检测模型，专为汽车行业中的实时嵌入式系统设计，采用U-net架构和MetaFormer块，在CRUW数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 调频连续波雷达在汽车行业中越来越受欢迎，其对抗恶劣天气条件的鲁棒性使其成为高级驾驶辅助系统中雷达目标检测的合适选择。这些实时嵌入式系统对模型的紧凑性和效率有要求，而先前的工作在很大程度上忽视了这些要求。

Method: mRadNet采用U-net风格的架构和MetaFormer块，其中使用可分离卷积和注意力令牌混合器来有效捕获局部和全局特征。引入了更高效的令牌嵌入和合并策略，以进一步促进模型的轻量级设计。

Result: mRadNet在CRUW数据集上的性能得到了验证，提高了最先进的性能。

Conclusion: mRadNet是一种有效的紧凑型雷达目标检测模型，满足了实时嵌入式系统对紧凑性和效率的要求，并在性能上超越了现有方法。

Abstract: Frequency-modulated continuous wave radars have gained increasing popularity
in the automotive industry. Its robustness against adverse weather conditions
makes it a suitable choice for radar object detection in advanced driver
assistance systems. These real-time embedded systems have requirements for the
compactness and efficiency of the model, which have been largely overlooked in
previous work. In this work, we propose mRadNet, a novel radar object detection
model with compactness in mind. mRadNet employs a U-net style architecture with
MetaFormer blocks, in which separable convolution and attention token mixers
are used to capture both local and global features effectively. More efficient
token embedding and merging strategies are introduced to further facilitate the
lightweight design of the model. The performance of mRadNet is validated on the
CRUW dataset, improving state-of-the-art performance.

</details>


### [2] [Power Spectral Density Estimation via Universal Truncated Order Statistics Filtering](https://arxiv.org/abs/2509.16359)
*David Campos Anchieta,John R. Buck*

Main category: eess.SP

TL;DR: 本文提出了一种混合顺序统计滤波器，通过动态调整不同秩的滤波器权重来有效过滤水下声学数据中的强瞬态信号，无需显式选择阈值秩。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序统计滤波器需要仔细选择最高秩来消除强瞬态信号，但在动态环境中强瞬态信号以不可预测的速率出现，需要动态调整滤波器秩以保持低偏差和方差。

Method: 提出了一种跨秩的顺序统计滤波器的凸和，通过顺序调整混合权重，在最近时间窗口内偏好最低方差的滤波器。

Result: 混合顺序统计滤波器的性能可证明地接近最佳固定秩滤波器的性能，仿真和真实数据证实其能有效过滤频谱图中的强瞬态信号。

Conclusion: 该方法无需显式选择阈值秩，就能有效处理动态环境中的强瞬态信号，提高了背景噪声功率谱密度估计的准确性。

Abstract: Loud transient signals in underwater acoustic data increase the bias and
variance of background noise power spectral density (PSD) estimates based on
sample mean. Recently, two PSD estimators mitigated the loud transient impact
on PSD estimates by applying order statistics filtering (OSF). The first, the
Schwock and Abadi Welch Percentile, scales a single rank order statistic (OS)
of consecutive periodograms. The second, the truncated linear order statistics
filter, is a weighted sum of OS up to a chosen rank. In order to minimize
variance, both OSFs must carefully choose the highest rank that still
eliminates the loud transients. However, in real-time applications in dynamic
environments, loud transients occur at unpredictable rates, requiring dynamic
adjustment of the OSF ranks to keep low bias and variance. To circumvent the
challenges of real-time rank selection, this paper proposes a convex sum of
OSFs across ranks with blending weights that are sequentially adjusted to favor
the lowest variance OSFs over a recent time window. The performance of the
blended sum provably approaches the performance of the best fixed rank OSF.
Simulations and real data confirm the blended OSFs effectively filter loud
transients out of spectrograms without explicitly choosing a threshold rank.

</details>


### [3] [Hybrid FIM and STAR-BD-RIS-Aided Wireless Communications with Short Packet Length: A Meta-TD3 Approach](https://arxiv.org/abs/2509.16417)
*Ayla Eftekhari,Maryam Cheraghy,Armin Farhadi,Mohammad Robat Mili,Qingqing Wu*

Main category: eess.SP

TL;DR: 本文提出了一种结合FIM天线和STAR-BD-RIS的混合系统架构，通过联合优化FIM表面配置、发射波束成形向量和RIS相移矩阵来最大化短块长度下的可达和速率，并开发了基于元学习的Meta-TD3算法来解决非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RIS和FIM技术在多用户无线通信系统中的优势，通过同时传输和反射信号以及三维天线重构来增强信道质量，确保单天线用户获得足够的接收功率。

Method: 在基站部署FIM天线，在传输路径上部署STAR-BD-RIS，采用元学习增强的TD3算法（Meta-TD3）来联合优化FIM表面配置、发射波束成形向量和RIS相移矩阵。

Result: 仿真结果表明，所提出的混合系统在性能上优于仅使用FIM或RIS的传统配置，且Meta-TD3算法相比经典学习技术具有更优的性能。

Conclusion: FIM与STAR-BD-RIS的混合系统架构结合Meta-TD3算法能够有效提升多用户无线通信系统的性能，为未来智能超表面技术的应用提供了新的解决方案。

Abstract: Reconfigurable intelligent surfaces (RIS) and flexible intelligent
metasurfaces (FIM) have been widely adopted in multi-user wireless
communication systems to enhance channel quality through simultaneous
transmission and reflection of signals and three-dimensional reconfiguration of
antennas. In this paper, we propose a novel system architecture that integrates
the benefits of both technologies by deploying an FIM antenna at the base
station (BS) and a simultaneously transmitting and reflecting beyond diagonal
RIS (STAR-BD-RIS) along the transmission path to ensure sufficient received
power for single-antenna users. The objective is to maximize the achievable sum
rate considering the short block length by jointly optimizing the FIM surface
configuration, the transmit beamforming vector, and STAR-BD-RIS phase shift
matrix subject to practical constraints including minimum
signal-to-interference-plus-noise ratio (SINR), power limitations, FIM
constraint, and the STAR-BD-RIS phase-shift matrix. To solve the resulting
non-convex optimization problem, we develop a learning-based approach that
incorporates meta-learning into the twin delayed deep deterministic policy
gradient (TD3) algorithm, referred to as Meta-TD3. The simulation results
demonstrate that the proposed hybrid system outperforms conventional
configurations employing either FIM or RIS alone, while the Meta-TD3 algorithm
achieves superior performance compared to classic learning techniques.

</details>


### [4] [Advancing Accessible Hand-Arm Vibration Safety Monitoring: ISO-Compliance with Wearable Sensors and Transfer Functions](https://arxiv.org/abs/2509.16536)
*Johannes Mootz,Reza Akhavian*

Main category: eess.SP

TL;DR: 本研究开发了一种基于可穿戴传感器的振动监测方法，通过误差最小化传递函数将数据转换为可与ISO标准比较的值，用于实时、经济的HAVS预防。


<details>
  <summary>Details</summary>
Motivation: 现场工人经常暴露于有害振动中，增加手部振动综合症(HAVS)风险。ISO 5349-1标准使用高质量加速度计直接安装在工具手柄上，但这种方法在现实条件下不实用。

Method: 使用三个不同采样频率的加速度计测量受试者锤钻混凝土时的振动数据，开发传递函数来映射不同传感器位置间的振动数据，考虑阻尼效应。

Result: 研究发现手掌和上臂之间的加速度显著降低，采样频率对数据准确性有重要影响，传递函数能够准确比较真实手部振动水平与现有标准限值。

Conclusion: 该方法实现了可访问、实时且经济有效的HAVS预防，为现场振动监测提供了实用解决方案。

Abstract: Field workers are frequently exposed to hazardous vibrations, increasing the
risk of Hand-Arm Vibration Syndrome (HAVS) and other long-term health problems.
ISO 5349-1 provides guidelines for measuring vibration exposure. However, this
standard was established in controlled conditions using high-quality
accelerometers directly attached to power tool handles. This study investigates
an alternative, wearable sensor-based data collection process and develops an
error-minimization transfer function that derives values comparable to ISO
benchmarks for safety monitoring. Experiments are performed with subjects
hammer drilling into concrete while vibrations are measured using three
accelerometers at different sampling frequencies. The transfer function maps
vibration data across sensor positions by accounting for damping effects. The
findings indicate a significant reduction in acceleration between the palm and
upper arm, highlight the impact of sampling frequency on data accuracy, and
enable accurate comparison of true hand-arm vibration levels with existing
standard limits to allow accessible, real-time, and cost-effective HAVS
prevention.

</details>


### [5] [Bearing-only Tracking using Towed Sensor-Array with Non-Gaussian Measurement Noise Statistics](https://arxiv.org/abs/2509.16570)
*Rohit Kumar Singh,Subrata Kumar,Shovan Bhaumik*

Main category: eess.SP

TL;DR: 本文提出了一种用于拖曳电缆传感器阵列系统（TCSAS）的动态模型，通过集总质量方法解决拖船机动时传感器阵列位置不确定的问题，并结合最大相关熵准则的卡尔曼滤波器处理非高斯噪声，提高被动纯方位跟踪（BOT）的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在被动纯方位跟踪中，拖船需要机动以使跟踪系统可观测，但这会 destabilise 拖曳电缆传感器阵列系统，导致传感器阵列位置不确定。现有方法要么暂停测量更新步骤，要么假设传感器阵列位置固定，导致状态估计不可靠。

Method: 使用集总质量方法建立TCSAS的动态模型，通过求解力矩平衡条件和准静态平衡条件的方程获得3D空间中的动态特性。结合最大相关熵准则的卡尔曼滤波器处理非高斯噪声，并提出了核带宽选择技术。

Result: 所提出的传感器阵列动态模型在真实世界的BOT交战场景中得到了验证。

Conclusion: 该方法通过动态建模传感器阵列位置和有效处理非高斯噪声，显著提高了被动纯方位跟踪的可靠性和估计精度。

Abstract: Passive bearing-only tracking (BOT) estimates the target states by utilising
noisy bearing measurements captured by a sensor array. The sensor array is
often towed behind the ship, using a long flexible cable to reduce interference
from the own-ship's inherent noises. This forms a towed cable sensor-array
system (TCSAS). During BOT, the tow-ship has to perform a manoeuvre to make the
tracking system observable. Such a manoeuvre destabilises the TCSAS, thus
making its exact location unknown \emph{w.r.t.} tow-ship. However, it is very
crucial to know the exact location of the towed sensor-array to perform
efficient and reliable target state estimation. The existing BOT approaches
perform TMA during own-ship manoeuvre either by pausing the measurement
updation step of the estimation algorithm or assuming a fixed aft position for
the towed sensor-array. These assumptions lead to unreliable state estimation.
To address this, we propose a dynamic model for TCSAS, using a lumped mass
approach, which will provide the location of the sensor array during the
own-ship manoeuvre. This location will be fed to the state estimation
algorithm. The dynamic of TCSAS in 3D space is obtained by solving the
equations obtained from the moment balance condition and quasi-static
equilibrium condition at the lumped mass points. Moreover, the bearing data
captured by the towed sensor-array is corrupted with non-Gaussian noise. It is
handled using the maximum correntropy criterion based Kalman filter with a
kernel bandwidth selection technique, proposed in this paper. The proposed
sensor-array dynamic model is verified for a real-world BOT engagement
scenario.

</details>


### [6] [Fusing Spectral Correlation Density Imaging with Deep Learning for Intelligent Fault Diagnosis in Rotating Machinery](https://arxiv.org/abs/2509.16580)
*Dilshara Herath,Chinthaka Abeyrathne,Chamindu Adithya,Chathura Seneviratne*

Main category: eess.SP

TL;DR: 该研究利用振动数据的循环平稳特性，通过谱相关密度图像增强轴承故障检测，并应用深度学习进行分类。使用三种CNN模型在公开数据集上对七种轴承状态进行分类，自定义CNN模型在两种不同轴承座中分别达到96.58%和94.95%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 旋转机械中的轴承故障诊断对确保运行可靠性至关重要，早期故障检测可避免灾难性故障和昂贵的紧急维修。传统方法如快速傅里叶变换往往无法捕捉振动信号的复杂非平稳特性。

Method: 利用振动数据的循环平稳特性，通过谱相关密度图像处理振动信号为2D图像以揭示故障特定周期性。开发三种卷积神经网络模型（自定义CNN、ResNet152V2和EfficientNetB0）对七种轴承状态进行分类。

Result: 自定义CNN在轴承座A和B中分别达到96.58%和94.95%的最高准确率，ResNet152V2分别为96.49%和95.35%，EfficientNetB0分别为94.16%和91.65%。模型在不同轴承座中的高准确率证明了其鲁棒性。

Conclusion: 该方法为成本有效的状态监测提供了稳健解决方案，适用于传感平台附近的部署，为边缘智能的应用机器学习做出贡献，并展示了处理复杂、可能大规模振动数据的有效信号处理策略。

Abstract: Bearing fault diagnosis in rotating machinery is critical for ensuring
operational reliability, therefore early fault detection is essential to avoid
catastrophic failures and expensive emergency repairs. Traditional methods like
Fast Fourier Transform (FFT) often fail to capture the complex, non-stationary
nature of vibration signals. This study leverages the cyclostationary
properties of vibration data through Spectral Correlation Density (SCD) images
to enhance fault detection and apply deep learning for classification. Using a
publicly available dataset with bearing faults seeded in two distinct housings
(A and B) under varying load conditions (0 Nm, 2 Nm, 4 Nm), we processed
vibration signals into 2D SCD images to reveal fault-specific periodicities,
such as broadband spectra (2000--8000 Hz) for larger faults. Three
convolutional neural network (CNN) models, Custom CNN, ResNet152V2, and
EfficientNetB0, were developed to classify seven bearing conditions. The custom
CNN achieved the highest accuracies of 96.58\% and 94.95\% on Housing A and B,
respectively, followed by ResNet152V2 at 96.49\% and 95.35\%, and
EfficientNetB0 at 94.16\% and 91.65\%, respectively. The models' high
accuracies across different housings demonstrate a robust solution suitable for
cost-effective condition monitoring deployable near sensing platforms,
contributing to applied machine learning for edge intelligence and showcasing
effective signal processing strategies for handling complex, potentially
large-scale vibration data.

</details>


### [7] [Robust Sparse Subspace Tracking from Corrupted Data Observations](https://arxiv.org/abs/2509.16585)
*Ta Giang Thuy Loan,Hoang-Lan Nguyen,Nguyen Thi Ngoc Lan,Do Hai Son,Tran Thi Thuy Quynh,Nguyen Linh Trung,Karim Abed-Meraim,Thanh Trung Le*

Main category: eess.SP

TL;DR: 该论文提出了一种基于alpha散度的稀疏子空间估计和跟踪方法，在非高斯噪声和稀疏数据的高维场景下表现出色，具有鲁棒性强、计算复杂度低和内存需求小的优势。


<details>
  <summary>Details</summary>
Motivation: 在高维信号处理中，数据样本常受到非高斯噪声污染且具有稀疏性，传统子空间跟踪方法在这些挑战性场景下性能受限，需要开发更鲁棒的解决方案。

Method: 采用alpha散度进行稀疏子空间估计和跟踪，该方法能够有效处理数据损坏问题，同时保持较低的计算复杂度和内存存储需求。

Result: 实验证明该方法在鲁棒子空间跟踪和波达方向估计方面优于现有最先进方法，展现出优异的性能。

Conclusion: 基于alpha散度的方法为高维稀疏数据下的子空间跟踪问题提供了有效的解决方案，在保持计算效率的同时显著提升了鲁棒性。

Abstract: Subspace tracking is a fundamental problem in signal processing, where the
goal is to estimate and track the underlying subspace that spans a sequence of
data streams over time. In high-dimensional settings, data samples are often
corrupted by non-Gaussian noises and may exhibit sparsity. This paper explores
the alpha divergence for sparse subspace estimation and tracking, offering
robustness to data corruption. The proposed method outperforms the
state-of-the-art robust subspace tracking methods while achieving a low
computational complexity and memory storage. Several experiments are conducted
to demonstrate its effectiveness in robust subspace tracking and
direction-of-arrival (DOA) estimation.

</details>


### [8] [Affine Frequency Division Multiplexing for Communication and Channel Sounding: Requirements, Challenges, and Key Technologies](https://arxiv.org/abs/2509.16643)
*Yu Zhou,Chao Zou,Nanhao Zhou,Yanqun Tang,Xiaoying Zhang,Haoran Yin,Xiaoran Liu,Ruisi He,Pan Tang,Weijie Yuan,Yong Zeng*

Main category: eess.SP

TL;DR: 本文提出了一种集成信道探测与通信（ICSC）方法，使用仿频分复用（AFDM）技术解决下一代空天地海一体化网络（SAGSIN）中信道动态变化的问题。AFDM提供完整的时延-多普勒信道表示，在时频双弥散信道中实现最优分集。


<details>
  <summary>Details</summary>
Motivation: 传统信道探测系统无法处理下一代空天地海一体化网络中信道的动态变化，导致信道模型过时，无法为通信系统提供可靠的先验信息。

Method: 采用仿频分复用（AFDM）技术，该技术不同于正交频分复用，能够提供信道的完整时延-多普勒表示，实现同时通信和信道探测。

Result: AFDM在时频双弥散信道中实现最优分集性能，有效解决了信道动态变化带来的挑战。论文探讨了AFDM-ICSC的关键性能指标和潜在应用场景。

Conclusion: AFDM-ICSC技术为下一代无线通信系统提供了实用的解决方案，但实施过程中仍面临关键挑战，需要进一步研究发展方向。

Abstract: Channel models are crucial for theoretical analysis, performance evaluation,
and deployment of wireless communication systems. Traditional channel sounding
systems are insufficient for handling the dynamic changes of channels in the
next-generation space-air-ground-sea integrated networks (SAGSIN), which often
results in outdated channel models that fail to provide reliable prior
information for communication systems. To address this challenge, this paper
proposes an integrated channel sounding and communication (ICSC) method as a
practical solution. Unlike orthogonal frequency division multiplexing, affine
frequency division multiplexing (AFDM) provides a full delay-Doppler
representation of the channel, achieving optimal diversity in time-frequency
doubly dispersive channels and effectively addressing the aforementioned
challenges. Thus, we investigate the fundamental principles of AFDM, showing
how it enables simultaneous communication and channel sounding, and explore key
performance metrics for both functionalities. We also clarify the distinction
and relationship between channel sounding, estimation, tracking and scatterer
sensing. Additionally, several potential application scenarios for AFDM-ICSC
are explored. Finally, we highlight the key challenges in implementing
AFDM-ICSC, outline future research directions, and provide valuable insights
for the continued development of this technology.

</details>


### [9] [Near-Field Channel Estimation with ELAA Modular Arrays Under Hardware Impairments](https://arxiv.org/abs/2509.16688)
*Özlem Tuğfe Demir,Emil Björnson*

Main category: eess.SP

TL;DR: 该论文针对模块化超大规模天线阵列系统中的近场视距信道估计问题，提出了一种计算高效的估计方法，利用阵列几何和近场LOS信道的恒定模结构，通过2D DFT掩码技术提高估计精度并减少前传信号量。


<details>
  <summary>Details</summary>
Motivation: 模块化超大规模天线阵列架构虽然降低了硬件成本，但在存在低噪声放大器引起的硬件损伤情况下，近场视距信道估计面临挑战，需要开发高效的估计方法。

Method: 提出计算高效的估计器，利用阵列几何和近场LOS信道的恒定模结构，包括新颖的2D DFT掩码技术。

Result: 数值结果表明，所提方法在估计精度方面显著优于传统的LS方法，同时大幅减少了前传信号量。

Conclusion: 该方法为模块化ELAA系统中的近场LOS信道估计提供了有效的解决方案，在保持计算效率的同时提高了性能。

Abstract: Extremely large-scale antenna arrays (ELAAs) enable high spatial resolution
and multiplexing, especially for user equipments (UEs) in the radiative
near-field. To reduce hardware cost, modular ELAA architectures with
distributed baseband units (BBUs) are gaining traction. This paper addresses
near-field line-of-sight (LOS) channel estimation under low noise amplifier
(LNA)-induced hardware impairments in such modular systems. We propose
computationally efficient estimators that exploit the array geometry and
constant-modulus structure of near-field LOS channels, including a novel
two-dimensional (2D) discrete Fourier transform (DFT) masking technique that
improves estimation accuracy and significantly reduces fronthaul signaling.
Numerical results show that the proposed methods significantly outperform the
conventional least squares (LS) method.

</details>


### [10] [Data-Driven Two-Stage IRS-Aided Sumrate Maximization with Inexact Precoding](https://arxiv.org/abs/2509.16776)
*Hassaan Hashmi,Spyridon Pougkakiotis,Dionysis Kalogerias*

Main category: eess.SP

TL;DR: iZoSGA是一种数据驱动的学习算法，用于无线网络中联合被动长期智能反射面(IRS)波束成形和主动短期预编码。该算法基于零阶随机拟梯度上升方法，适用于处理具有连续不确定性和黑盒目标函数的两阶段非凸随机规划问题。


<details>
  <summary>Details</summary>
Motivation: 解决无线网络中IRS辅助波束成形的联合优化问题，特别是在短期预编码只能近似求解的实际情况下的挑战。

Method: 采用零阶随机拟梯度上升方法，利用不精确的预编码预言机，适用于任意信道模型和IRS配置，无需依赖信道统计信息。

Result: 理论证明iZoSGA在最小假设下能收敛到原问题稳定解的邻域，数值实验验证了算法在多种不精确场景下的有效性。

Conclusion: iZoSGA为IRS辅助无线网络提供了一种实用且有效的优化方法，能够在实际不精确条件下实现被动但完全有效的IRS操作。

Abstract: We propose iZoSGA, a data-driven learning algorithm for joint passive
long-term intelligent reflective surface (IRS)-aided beamforming and active
short-term precoding in wireless networks. iZoSGA is based on a zeroth-order
stochastic quasigradient ascent methodology designed for tackling two-stage
nonconvex stochastic programs with continuous uncertainty and objective
functions with "black-box" terms, and where second-stage optimization is
inexact. As such, iZoSGA utilizes inexact precoding oracles, enabling practical
implementation when short-term (e.g., WMMSE-based) beamforming is solved
approximately. The proposed method is agnostic to channel models or statistics,
and applies to arbitrary IRS/network configurations. We prove non-asymptotic
convergence of iZoSGA to a neighborhood of a stationary solution of the
original exact problem under minimal assumptions. Our numerics confirm the
efficacy iZoSGA in several "inexact regimes", enabling passive yet fully
effective IRS operation in diverse and realistic IRS-aided scenarios.

</details>


### [11] [On the Secrecy Performance of Pinching-Antenna Systems](https://arxiv.org/abs/2509.16854)
*Nianzu Li,Weidong Mei,Lipeng Zhu,Peiran Wu,Boyu Ning*

Main category: eess.SP

TL;DR: 本文研究了捏合天线系统在窃听者存在下的保密性能，推导了保密中断概率的近似表达式和渐近行为，并得出了比传统固定位置天线系统更低的性能下界。


<details>
  <summary>Details</summary>
Motivation: 捏合天线系统作为一种新型可重构天线技术，因其在减轻信号传播路径损耗方面的卓越能力而受到关注，但对其在窃听环境下的保密性能研究不足。

Method: 推导了系统保密中断概率的近似表达式，分析了其渐近行为，并建立了常数性能下界。

Result: 捏合天线系统的保密中断概率下界为(2π-1)/24 ≈ 0.2618，显著低于传统固定位置天线系统的0.5。仿真结果验证了分析的正确性。

Conclusion: 捏合天线系统在保密通信方面具有显著优势，其性能下界远优于传统天线系统，为安全无线通信提供了新的技术途径。

Abstract: Pinching-antenna systems have recently gained significant attention as a
novel reconfigurable-antenna technology due to its exceptional capability of
mitigating signal-propagation path loss. In this letter, we investigate the
secrecy performance of a pinching-antenna system in the presence of an
eavesdropper. In particular, we derive an approximate expression of the
system's secrecy outage probability (SOP) with respect to the random locations
of the legitimate user and eavesdropper and analyze its asymptotic behavior.
Moreover, we derive a constant performance lower bound on the SOP of the
considered system, i.e., $\frac{2\pi-1}{24}$, which is significantly lower than
that of conventional fixed-position antenna systems, i.e., $0.5$. Finally,
simulation results are provided to validate the correctness of our analytical
results.

</details>


### [12] [Graph Fractional Hilbert Transform: Theory and Application](https://arxiv.org/abs/2509.16910)
*Daxiang Li,Zhichao Zhang*

Main category: eess.SP

TL;DR: 本文提出了图分数希尔伯特变换（GFRHT），解决了传统图希尔伯特变换（GHT）在相位固定、信息丢失和缺乏可调参数等问题，通过双参数框架（分数阶α和角度β）实现了更灵活的分析和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GHT存在局限性：局限于图傅里叶域、固定相位偏移、实值谱分量信息丢失以及缺乏可调参数。图分数傅里叶变换虽然通过分数阶参数α引入域灵活性，但未能解决相位刚性和信息丢失问题。

Method: 提出图分数希尔伯特变换（GFRHT），采用双参数框架：分数阶参数α可在顶点空间和谱空间之间插值分析，角度参数β提供可调相位偏移，并为实特征值提供非零实值响应（cosβ），从而消除信息丢失。

Result: 在边缘检测、异常识别和语音分类等实验中，GFRHT优于传统GHT，表现出更大的灵活性和优越的性能。

Conclusion: GFRHT通过双参数框架解决了GHT的局限性，为图信号处理提供了更灵活的分析工具，在多个应用场景中展现出优越性能。

Abstract: The graph Hilbert transform (GHT) is a key tool in constructing analytic
signals and extracting envelope and phase information in graph signal
processing. However, its utility is limited by confinement to the graph Fourier
domain, a fixed phase shift, information loss for real-valued spectral
components, and the absence of tunable parameters. The graph fractional Fourier
transform introduces domain flexibility through a fractional order parameter
$\alpha$ but does not resolve the issues of phase rigidity and information
loss. Inspired by the dual-parameter fractional Hilbert transform (FRHT) in
classical signal processing, we propose the graph FRHT (GFRHT). The GFRHT
incorporates a dual-parameter framework: the fractional order $\alpha$ enables
analysis across arbitrary fractional domains, interpolating between vertex and
spectral spaces, while the angle parameter $\beta$ provides adjustable phase
shifts and a non-zero real-valued response ($\cos\beta$) for real eigenvalues,
thereby eliminating information loss. We formally define the GFRHT, establish
its core properties, and design a method for graph analytic signal
construction, enabling precise envelope extraction and demodulation.
Experiments on edge detection, anomaly identification, and speech
classification demonstrate that GFRHT outperforms GHT, offering greater
flexibility and superior performance in graph signal processing.

</details>


### [13] [Bi-modal Prediction and Transformation Coding for Compressing Complex Human Dynamics](https://arxiv.org/abs/2509.16919)
*Huong Hoang,Keito Suzuki,Truong Nguyen,Pamela Cosman*

Main category: eess.SP

TL;DR: 提出了一种双模态编码框架，通过结合语义分割和区域特定变换建模来增强运动表示的灵活性，在处理快速运动或强非刚性变形时提高压缩效率。


<details>
  <summary>Details</summary>
Motivation: 原始KeyNode驱动编解码器在处理快速运动或强非刚性变形时难以保持压缩效率，需要更灵活的运动表示方法。

Method: 扩展刚性变换模型，在变形丰富区域选择性应用仿射变换（旋转、平移、缩放和剪切），在其他区域保留刚性模型。通过拉格朗日率失真优化指导的组件选择策略，将仿射模型分解为最小参数集进行高效编码。

Result: 双模态方法实现了更准确的网格变形，特别是在涉及复杂非刚性运动的序列中，同时在简单区域不损害压缩效率，相比基线平均比特率节省33.81%。

Conclusion: 该双模态编码框架有效解决了动态人体运动序列中非刚性变形的压缩挑战，在保持压缩效率的同时显著提高了变形精度。

Abstract: For dynamic human motion sequences, the original KeyNode-Driven codec often
struggles to retain compression efficiency when confronted with rapid movements
or strong non-rigid deformations. This paper proposes a novel Bi-modal coding
framework that enhances the flexibility of motion representation by integrating
semantic segmentation and region-specific transformation modeling. The rigid
transformation model (rotation & translation) is extended with a hybrid scheme
that selectively applies affine transformations-rotation, translation, scaling,
and shearing-only to deformation-rich regions (e.g., the torso, where loose
clothing induces high variability), while retaining rigid models elsewhere. The
affine model is decomposed into minimal parameter sets for efficient coding and
combined through a component selection strategy guided by a Lagrangian
Rate-Distortion optimization. The results show that the Bi-modal method
achieves more accurate mesh deformation, especially in sequences involving
complex non-rigid motion, without compromising compression efficiency in
simpler regions, with an average bit-rate saving of 33.81% compared to the
baseline.

</details>


### [14] [Asymptotic Scaling Law Analysis of Multicast Satellite Communications with Massive MIMO](https://arxiv.org/abs/2509.16921)
*Seyong Kim,Jeonghun Park*

Main category: eess.SP

TL;DR: 本文分析了采用大规模MIMO的GEO卫星通信系统在组播传输下的渐近速率缩放规律，发现通过增加用户密度可以精确补偿组播传输带来的速率下降。


<details>
  <summary>Details</summary>
Motivation: 研究GEO卫星通信系统中大规模MIMO组播传输的性能，特别是用户空间分布对系统速率的影响。

Method: 使用泊松点过程建模地面用户的空间分布，假设采用固定波束预编码，推导出渐近速率缩放规律的闭式表达式。

Result: 获得了速率随天线数量、用户密度和组播用户缩放因子的函数关系，揭示了组播传输速率下降可通过增加用户密度来补偿。

Conclusion: 在GEO卫星大规模MIMO组播系统中，用户密度的适当增加可以有效抵消组播传输带来的性能损失。

Abstract: In this paper, we consider a geostationary orbit (GEO) satellite
communication system that employs massive multiple-input multiple-output (MIMO)
for multicast transmission. By modeling the spatial distribution of ground
users using a Poisson point process (PPP) and assuming a fixed-beam precoding
is adopted, we find a closed-form expression for the asymptotical rate scaling
law as a function of the number of antennas and the scaling factors of user
density and multicast users. From the derived analytical expression, we reveal
that the rate degradation caused by multicast transmission can be precisely
compensated by increasing the user density accordingly.

</details>


### [15] [Functional WMMSE Algorithm for Continuous Aperture Array Systems](https://arxiv.org/abs/2509.17101)
*Shiyong Chen*

Main category: eess.SP

TL;DR: 本文提出了一种功能扩展的加权最小均方误差（WMMSE）算法，用于多用户MIMO系统中的下行波束成形，其中基站和用户都采用连续孔径阵列。该方法通过将矩阵和向量提升为连续函数，并用积分代替矩阵乘积和内积，实现了对经典离散WMMSE递归的功能扩展。


<details>
  <summary>Details</summary>
Motivation: 在多用户MIMO系统中，传统离散WMMSE算法在处理连续孔径阵列时存在局限性。为了更有效地利用连续孔径阵列的优势，需要将离散算法扩展到连续函数域，以提高波束成形的性能和计算效率。

Method: 通过Galerkin投影将函数映射到系数矩阵，解决离散WMMSE问题，然后通过闭式更新将结果提升回功能域。所有积分使用Gauss-Legendre求积法实现，通过加权矩阵乘积保持闭式结构。

Result: 仿真结果表明，所提出的方法在频谱效率和计算复杂度方面均优于基线方法。

Conclusion: 功能扩展的WMMSE算法为连续孔径阵列的多用户MIMO系统提供了一种有效的波束成形解决方案，在性能和计算效率上均有显著提升。

Abstract: In this paper, we propose a functional extension of the weighted minimum
mean-squared error (WMMSE) algorithm for downlink beamforming in multiuser
multiple-input multiple-output (MU-MIMO) systems where both the base station
(BS) and the users employ continuous-aperture arrays (CAPAs). The method lifts
the matrices and vectors in the classical discrete WMMSE recursion to
continuous functions by replacing matrix products and inner products with
integrals over the apertures. In practice, we apply a Galerkin projection to
map functions to coefficient matrices, solve the resulting discrete WMMSE
problem via closed-form updates, and then lift these updates back to the
functional domain. All integrals are implemented using Gauss-Legendre
quadrature, which preserves the closed-form structure through weighted matrix
products. Simulations show that the proposed method outperforms baselines in
both spectral efficiency (SE) and computational complexity.

</details>


### [16] [Resilient Signal Reflection under CSI Perturbations: A Robust Approach for Secure RIS Communication](https://arxiv.org/abs/2509.17181)
*Mahdi Shamsi,Hadi Zayyani,Farokh Marvasti*

Main category: eess.SP

TL;DR: 本文提出了一种针对RIS辅助系统中CSI扰动的鲁棒方法，通过一阶近似技术增强RIS配置对CSI不完善的弹性，在安全通信场景下显著提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助无线通信系统中CSI扰动带来的挑战，特别是在存在窃听者的安全通信场景下，需要提高系统对信道状态信息不完善的鲁棒性。

Method: 采用一阶近似技术开发鲁棒方法，优化RIS的信号反射和传输配置，仅需少量矩阵向量乘法更新RIS元件，避免重复的逆矩阵或伪逆计算。

Result: 仿真结果显示在安全性和效率方面获得显著提升，同时保持低计算复杂度，扩展了系统的稳定范围。

Conclusion: 该方法为量化算法对CSI变化的敏感性提供了基准，证明了RIS在6G等下一代网络中实现安全可靠通信的潜力。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative
technology in wireless communication, enabling dynamic control over signal
propagation. This paper tackles the challenge of mitigating Channel State
Information (CSI) perturbations in RIS-aided systems, particularly for secure
communication scenarios. Leveraging a first-order approximation technique, we
develop a robust approach that strengthens the resilience of RIS configurations
against CSI imperfections. The study considers both untrusted user interception
and stealth radar applications, focusing on optimizing signal reflection and
transmission in the presence of eavesdroppers. Simulation results demonstrate
notable gains in security and efficiency while maintaining low computational
complexity. By extending the stability range, the proposed method updates RIS
elements using only a few matrix-vector multiplications, eliminating the need
for repeated inverse or pseudo-inverse computations under small channel
perturbations. Additionally, the framework provides a baseline for quantifying
algorithmic sensitivity to CSI variations. Overall, the findings underscore the
potential of RIS to enable secure and reliable communication in next-generation
networks such as 6G.

</details>


### [17] [Estimation of Specific Gravity of Potato Tubers Using Dielectric Properties](https://arxiv.org/abs/2509.17267)
*Taorui Chen,Yuki Gao,Yi Wang,Hai-Han Sun*

Main category: eess.SP

TL;DR: 开发了一个基于介电常数估算马铃薯比重(SG)的模型，通过介电谱测量和线性回归分析，在0.3-3.0GHz频率范围内实现了高精度的SG估算。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏将马铃薯介电特性与其关键农艺性状(如比重)关联的准确模型，而比重与淀粉含量密切相关，对马铃薯质量评估很重要。

Method: 对5种类型共250个马铃薯样本进行比重测量和介电谱测量(0.3-3.0GHz)，使用200个数据集开发线性回归模型，回归系数表示为频率的四阶多项式函数。

Result: 在50个验证数据集上，模型达到高估算精度，平均绝对误差小于4.8×10⁻³，平均绝对百分比误差小于0.45%。

Conclusion: 马铃薯介电特性研究及推导的SG估算模型为未来开发用于马铃薯生产和加工行业农艺性状评估的微波传感技术奠定了基础。

Abstract: Potatoes are an economically important crop, and their quality is closely
related to the starch content, which is typically inferred from specific
gravity (SG). Although microwave sensing technologies have been increasingly
developed for underground potato detection and quality assessment in recent
years, no accurate model has yet been established to link the dielectric
properties of potatoes with their key agronomic traits. To address this gap, we
developed a model for estimating potato tubers' SG based on their dielectric
constant. To construct and validate the model, we conducted SG measurements and
dielectric spectroscopy measurements in the frequency range of 0.3 GHz to 3.0
GHz on 250 potatoes of five different types (red, russet, yellow, purple, and
chipping potatoes, with 50 samples per type). Out of the 250 data sets, 200
data sets were used for model development, and 50 data sets were used for model
validation. A linear regression model was used to summarize the relationship
between SG and dielectric constant, where the regression coefficients are
expressed as fourth-order polynomial functions of frequency. Experimental
results on the 50 validation data sets show that the model achieves high
estimation accuracy with mean absolute errors (MAE) less than
\(4.8\times10^{-3}\) and mean absolute percentage errors (MAPE) less than
0.45\%. The study of the dielectric properties of potatoes, along with the
derived SG estimation model, provides a foundation for the future development
of microwave sensing technologies for agronomic trait assessment in the potato
production and processing industries. All measured data will be made publicly
available upon acceptance of the paper.

</details>


### [18] [On Mutual Information Neural Estimation for Localization](https://arxiv.org/abs/2509.17344)
*Sven Hinderer,Manuel Buchfink,Bin Yang*

Main category: eess.SP

TL;DR: 该论文提出使用神经网络互信息估计器（MINE）来近似计算定位系统中用户位置与测量值之间的互信息，解决了传统互信息计算在高维问题中的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 互信息是评估和优化定位系统的有前景的度量标准，但传统计算方法在高维问题中计算成本过高，限制了其在现实世界定位系统中的应用。

Method: 采用神经网络互信息估计器（MINE）来近似计算用户位置与对应测量值之间的互信息，并在模拟的多边定位系统上进行实验验证。

Result: 通过蒙特卡洛模拟对估计器进行了收敛性和一致性评估，验证了MINE在简单多边定位系统中评估互信息的有效性。

Conclusion: 神经网络互信息估计器为高维定位系统中的互信息计算提供了可行的解决方案，有助于定位系统的性能评估和优化。

Abstract: Mutual information (MI) is a promising candidate measure for the assessment
and optimization of localization systems, as it captures nonlinear dependencies
between random variables. However, the high cost of computing MI, especially
for high-dimensional problems, prohibits its application for many real-world
localization systems. We evaluate an algorithm from a new class of neural MI
estimators called Mutual Information Neural Estimation (MINE) to approximate
the MI between the set of feasible user element (UE) locations and the
corresponding set of measurements from said UE locations used for positioning.
We apply this estimator to a simulated multilateration (MLAT) system, where the
true MI for benchmarking can be approximated by Monte Carlo simulation. The
estimator is experimentally evaluated w.r.t. its convergence and consistency
and we investigate the usefulness of MI for assessing simple MLAT systems.

</details>


### [19] [On the Design of Capacity-Achieving Distributions for Discrete-Time Poisson Channel with Low-Precision ADCs](https://arxiv.org/abs/2509.17483)
*Qianqian Li,Lintao Li,Lixiang Liu,Lei Yang,Caihong Gong,Hua Li,Shiya Hao,Xiaoming Dai*

Main category: eess.SP

TL;DR: 本文研究了在暗电流效应和低精度ADC下的离散时间泊松通道容量实现输入分布的设计，提出了一种结合牛顿-拉弗森和Blahut-Arimoto方法的优化算法。


<details>
  <summary>Details</summary>
Motivation: 研究在暗电流效应和低精度ADC约束下，如何设计能够实现信道容量的最优输入分布，这对于实际通信系统的性能优化具有重要意义。

Method: 提出了一种高效的优化算法，整合牛顿-拉弗森和Blahut-Arimoto方法，在峰值和平均功率约束下确定容量实现输入分布及其幅度，并建立了KKT条件作为最优性的充要条件。

Result: 仿真结果显示，在5dB条件下，所提算法分别实现了1位和2位量化DTPC理论容量的72%和83%。对于有限精度量化DTPC，容量可以通过具有K个质量点的非均匀离散输入分布实现。

Conclusion: 该算法有效解决了低精度ADC下DTPC的容量实现问题，为实际通信系统的优化提供了理论依据和实用方法。

Abstract: This paper investigates the design of the capacity-achieving input
distribution for the discrete-time Poisson channel (DTPC) under dark current
effects with low-precision analog-to-digital converters (ADCs). This study
introduces an efficient optimization algorithm that integrates the
Newton-Raphson and Blahut-Arimoto (BA) methods to determine the
capacity-achieving input distribution and the corresponding amplitudes of input
mass points for the DTPC, subject to both peak and average power constraints.
Additionally, the Karush-Kuhn-Tucker (KKT) conditions are established to
provide necessary and sufficient conditions for the optimality of the obtained
capacity-achieving distribution. Simulation results illustrate that the
proposed algorithm attains $72\%$ and $83\%$ of the theoretical capacity at 5
dB for 1-bit and 2-bit quantized DTPC, respectively. Furthermore, for a
finite-precision quantized DTPC (i.e., ${\log _2}K$ bits), the capacity can be
achieved by a non-uniform discrete input distribution with support for $K$ mass
points, under the given power constraints.

</details>


### [20] [Single-Snapshot Localization Using Sparse Extremely Large Aperture Arrays](https://arxiv.org/abs/2509.17511)
*Yunqiao Hu,Xuesu Xiao,Steven Jones,Shunqiao Sun*

Main category: eess.SP

TL;DR: 本文研究了汽车雷达应用中相干稀疏极大孔径阵列的单快拍DOA估计和目标定位问题，提出了适用于远场和近场的SS-MUSIC和SS-ESPRIT算法。


<details>
  <summary>Details</summary>
Motivation: 解决汽车雷达中稀疏极大孔径阵列的单快拍DOA估计问题，特别是在相干处理和近场定位方面的挑战。

Method: 提出了SS-MUSIC算法用于非相干处理，通过融合子阵列局部谱进行定位；开发了SS-ESPRIT方法用于相干处理，利用稀疏ELAAs孔径进行高分辨率角度估计。

Result: 仿真结果表明，SS-ESPRIT在远场紧密目标角度分辨率方面表现优越，而SS-MUSIC在近场定位和混合场景中具有更好的鲁棒性和灵活性。

Conclusion: 两种算法各有优势，SS-ESPRIT适合高分辨率远场估计，SS-MUSIC更适合近场定位和复杂场景应用。

Abstract: This paper investigates single-snapshot direction-of-arrival (DOA) estimation
and target localization with coherent sparse extremely large aperture arrays
(ELAAs) in automotive radar applications. Far-field and near-field signal
models are formulated for distributed bistatic configurations. To enable
noncoherent processing, a single-snapshot MUSIC (SS-MUSIC) algorithm is
proposed to fuse local spectra from individual subarrays and extended to
near-field localization via geometric intersection. For coherent processing, a
single-snapshot ESPRIT (SS-ESPRIT) method with ambiguity dealiasing is
developed to fully exploit the aperture of sparse ELAAs for high-resolution
angle estimation. Simulation results demonstrate that SS-ESPRIT provides
superior angular resolution for closely spaced far-field targets, while
SS-MUSIC offers robustness in near-field localization and flexibility in hybrid
scenarios.

</details>


### [21] [Predicting Chest Radiograph Findings from Electrocardiograms Using Interpretable Machine Learning](https://arxiv.org/abs/2509.17674)
*Julia Matejas,Olaf Żurawski,Nils Strodthoff,Juan Miguel Lopez Alcaraz*

Main category: eess.SP

TL;DR: 使用心电图特征和人口统计学数据预测胸部X光片结果的机器学习方法研究


<details>
  <summary>Details</summary>
Motivation: 在资源有限地区，胸部X光检查受限可能延误诊断，而心电图设备更普及且常在临床流程中更早获取，因此探索心电图能否作为胸部X光结果的替代指标

Method: 基于MIMIC-IV数据库，使用XGBoost分类器训练模型，通过递归特征消除选择预测特征，采用SHAP方法解释特征贡献，使用AUROC评估模型性能

Result: 模型成功预测多种胸部X光结果，准确率各异，特征选择针对不同目标优化，加入人口统计学变量持续提升性能，SHAP分析显示心电图特征对放射学预测有临床意义

Conclusion: 心电图特征结合人口统计学数据可作为某些胸部X光结果的替代指标，在放射成像受限环境中实现早期分诊或预筛查，可解释机器学习有潜力支持放射学工作流程和改善患者护理

Abstract: Purpose: Chest X-rays are essential for diagnosing pulmonary conditions, but
limited access in resource-constrained settings can delay timely diagnosis.
Electrocardiograms (ECGs), in contrast, are widely available, non-invasive, and
often acquired earlier in clinical workflows. This study aims to assess whether
ECG features and patient demographics can predict chest radiograph findings
using an interpretable machine learning approach.
  Methods: Using the MIMIC-IV database, Extreme Gradient Boosting (XGBoost)
classifiers were trained to predict diverse chest radiograph findings from
ECG-derived features and demographic variables. Recursive feature elimination
was performed independently for each target to identify the most predictive
features. Model performance was evaluated using the area under the receiver
operating characteristic curve (AUROC) with bootstrapped 95% confidence
intervals. Shapley Additive Explanations (SHAP) were applied to interpret
feature contributions.
  Results: Models successfully predicted multiple chest radiograph findings
with varying accuracy. Feature selection tailored predictors to each target,
and including demographic variables consistently improved performance. SHAP
analysis revealed clinically meaningful contributions from ECG features to
radiographic predictions.
  Conclusion: ECG-derived features combined with patient demographics can serve
as a proxy for certain chest radiograph findings, enabling early triage or
pre-screening in settings where radiographic imaging is limited. Interpretable
machine learning demonstrates potential to support radiology workflows and
improve patient care.

</details>


### [22] [SSNet: Flexible and robust channel extrapolation for fluid antenna systems enabled by an self-supervised learning framework](https://arxiv.org/abs/2509.17797)
*Yuan Gao,Yiming Liu,Runze Yu,Shengli Liu,Yanliang Jin,Shunqing Zhang,Shugong Xu,Xiaoli Chu*

Main category: eess.SP

TL;DR: 本文提出了一种自监督学习网络SSNet，用于解决流体天线系统中信道状态信息外推的挑战，通过将问题建模为图像重建任务，利用混合专家模块提升特征提取和噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统在6G通信中具有重要价值，但传统方法如基于导频的插值、压缩感知等计算量大且缺乏适应性，现有外推技术无法适应FAS的动态环境，而数据驱动的深度学习方法需要大量训练且对噪声敏感。

Method: 将FAS信道外推问题建模为图像重建任务，使用自监督学习网络SSNet，引入混合专家模块并行处理多个前馈神经网络输出，通过门控函数加权组合结果。

Result: 大量仿真验证表明，SSNet显著优于AGMAE和LSTM等基准模型，且仅需使用更小的标注数据集。

Conclusion: SSNet为FAS提供了一种高效自适应的信道外推解决方案，降低了对外部标注数据的依赖，具有更好的噪声鲁棒性。

Abstract: Fluid antenna systems (FAS) signify a pivotal advancement in 6G communication
by enhancing spectral efficiency and robustness. However, obtaining accurate
channel state information (CSI) in FAS poses challenges due to its complex
physical structure. Traditional methods, such as pilot-based interpolation and
compressive sensing, are not only computationally intensive but also lack
adaptability. Current extrapolation techniques relying on rigid parametric
models do not accommodate the dynamic environment of FAS, while data-driven
deep learning approaches demand extensive training and are vulnerable to noise
and hardware imperfections. To address these challenges, this paper introduces
a novel self-supervised learning network (SSNet) designed for efficient and
adaptive channel extrapolation in FAS. We formulate the problem of channel
extrapolation in FAS as an image reconstruction task. Here, a limited number of
unmasked pixels (representing the known CSI of the selected ports) are used to
extrapolate the masked pixels (the CSI of unselected ports). SSNet capitalizes
on the intrinsic structure of FAS channels, learning generalized
representations from raw CSI data, thus reducing dependency on large labelled
datasets. For enhanced feature extraction and noise resilience, we propose a
mix-of-expert (MoE) module. In this setup, multiple feedforward neural networks
(FFNs) operate in parallel. The outputs of the MoE module are combined using a
weighted sum, determined by a gating function that computes the weights of each
FFN using a softmax function. Extensive simulations validate the superiority of
the proposed model. Results indicate that SSNet significantly outperforms
benchmark models, such as AGMAE and long short-term memory (LSTM) networks by
using a much smaller labelled dataset.

</details>


### [23] [Generalized Beyond-Diagonal RIS Architectures: Theory and Design via Structure-oriented Symmetric Unitary Projection](https://arxiv.org/abs/2509.17804)
*Xiaohua Zhou,Tianyu Fang,Yijie Mao,Bruno Clerckx*

Main category: eess.SP

TL;DR: 本文提出了两种新型BD-RIS架构（主干连接RIS和集群连接RIS），在电路复杂度和性能之间实现更好的权衡，并开发了结构导向的对称酉投影设计方法。


<details>
  <summary>Details</summary>
Motivation: 传统BD-RIS在控制反射信号相位和幅度方面具有灵活性，但电路复杂度高，需要探索复杂度与性能之间的平衡方案。

Method: 提出主干连接RIS和集群连接RIS两种新架构，采用结构导向的对称酉投影方法设计散射矩阵，解决信道增益最大化等优化问题。

Result: 主干连接RIS能以最低复杂度实现最优性能，集群连接RIS进一步扩大了性能-复杂度权衡范围，所提算法效率高。

Conclusion: 新架构在保持BD-RIS性能优势的同时显著降低电路复杂度，为6G及未来通信系统提供了实用的RIS解决方案。

Abstract: Beyond-diagonal reconfigurable intelligent surface (BD-RIS), which enables
advanced wave control through interconnection of RIS elements, are gaining
growing recognition as a promising technology for 6G and beyond. However, the
enhanced flexibility of BD-RIS in controlling the phase and amplitude of
reflected signals comes at the cost of high circuit complexity. In this paper,
we propose two novel BD-RIS architectures, namely, the stem-connected RIS and
cluster-connected RIS, to explore trade-off between circuit complexity and
performance. Specifically, the proposed stem-connected RIS is capable of
achieving the same performance as fully-connected RIS while significantly
reducing circuit complexity. The proposed cluster-connected RIS offers a
unified framework that generalizes existing BD-RIS architectures--including
single-connected, fully-connected, group-connected, tree-connected (arrowhead),
and forest-connected (arrowhead) RISs--as special cases. This framework enables
a much more flexible trade-offs between circuit complexity and system
performance than existing ones. Based on the proposed BD-RIS architectures, we
introduce a novel and generalized structure-oriented symmetric unitary
projection method for designing the scattering matrix across all BD-RIS
configurations. This method is effectively applied to solve the sum channel
gain maximization problem and other utility-based optimization problems.
Numerical results demonstrate that the proposed stem-connected RIS is the
simplest architecture that achieves optimal BD-RIS performance, while the
cluster-connected RIS further enlarges the performance-complexity trade-off
range. Furthermore, the proposed projection-based algorithms demonstrate high
efficiency.

</details>


### [24] [Joint Pilot Allocation and Sequence Design for MIMO-OFDM Systems With Channel Sparsity](https://arxiv.org/abs/2509.17916)
*Kabuto Arai,Koji Ishibashi,Hiroki Iimori,Yuto Hama,Paulo Valente Klaine,Szabolcs Malomsoky*

Main category: eess.SP

TL;DR: 本文提出了一种针对MIMO-OFDM系统的联合优化方法，通过压缩感知技术进行信道估计，优化导频子载波分配和非正交序列以最小化感知矩阵的相干性。


<details>
  <summary>Details</summary>
Motivation: 压缩感知方法的性能依赖于感知矩阵的相干性度量，而传统的混合整数非线性规划方法由于离散变量组合爆炸而计算不可行。

Method: 引入块稀疏惩罚项使不必要导频功率趋近于零，将问题转化为连续变量优化；利用感知矩阵结构推导相干性度量的闭式梯度，采用梯度下降法高效求解。

Result: 数值结果表明所提导频序列具有优越的相干性特性，显著提升了基于压缩感知的信道估计性能。

Conclusion: 该方法成功解决了传统MINLP的计算难题，为MIMO-OFDM系统的压缩感知信道估计提供了高效的联合优化方案。

Abstract: This paper proposes a joint optimization of pilot subcarrier allocation and
non-orthogonal sequence for multiple-input-multiple-output (MIMO)-orthogonal
frequency-division multiplexing (OFDM) systems under compressed sensing
(CS)-based channel estimation exploiting delay and angle sparsity. Since the
performance of CS-based approaches depends on a coherence metric of the sensing
matrix in the measurement process, we formulate a joint optimization problem to
minimize this coherence. Due to the discrete nature of subcarrier allocation, a
straightforward formulation of the joint optimization results in a
mixed-integer nonlinear program (MINLP), which is computationally intractable
due to the combinatorial explosion of allocation candidates. To overcome the
intractability of discrete variables, we introduce a block sparse penalty for
pilots across all subcarriers, which ensures that the power of some unnecessary
pilots approaches zero. This framework enables joint optimization using only
continuous variables. In addition, we propose an efficient computation method
for the coherence metric by exploiting the structure of the sensing matrix,
which allows its gradient to be derived in closed form, making the joint
optimization problem solvable in an efficient way via a gradient descent
approach. Numerical results confirm that the proposed pilot sequence exhibits
superior coherence properties and enhances the CS-based channel estimation
performance.

</details>


### [25] [Autoregressive-Gaussian Mixture Models: Efficient Generative Modeling of WSS Signals](https://arxiv.org/abs/2509.17953)
*Kathrin Klein,Benedikt Böck,Nurettin Turan,Wolfgang Utschick*

Main category: eess.SP

TL;DR: 提出一种将自回归参数化集成到高斯混合模型中的生成模型，用于建模宽平稳过程，旨在降低资源消耗同时保持高建模精度


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在资源受限环境（如移动无线通信系统）中的应用挑战

Method: 将自回归参数化集成到高斯混合模型中，利用基于模型的结构约束来减少参数数量

Result: 信道估计实验表明，该模型在小样本情况下优于标准GMM及使用Toeplitz或循环协方差的变体；对于大数据集，性能与传统方法相当但计算效率和内存需求更优

Conclusion: 该模型在保持高精度的同时显著降低了参数数量和计算资源需求，适用于资源受限的无线通信系统

Abstract: This work addresses the challenge of making generative models suitable for
resource-constrained environments like mobile wireless communication systems.
We propose a generative model that integrates Autoregressive (AR)
parameterization into a Gaussian Mixture Model (GMM) for modeling Wide-Sense
Stationary (WSS) processes. By exploiting model-based insights allowing for
structural constraints, the approach significantly reduces parameters while
maintaining high modeling accuracy. Channel estimation experiments show that
the model can outperform standard GMMs and variants using Toeplitz or circulant
covariances, particularly with small sample sizes. For larger datasets, it
matches the performance of conventional methods while improving computational
efficiency and reducing the memory requirements.

</details>


### [26] [Bridge Micro-Deformation Monitoring Scheme with Integrated Sensing and Communications](https://arxiv.org/abs/2509.17983)
*Boxuan Sun,Hongliang Luo,Shaodan Ma,Feifei Gao*

Main category: eess.SP

TL;DR: 提出了一种新颖的集成感知与通信方案，用于复杂环境下的桥梁微变形监测，通过OFDM回波信号处理和干扰抑制方法实现高精度监测。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中对桥梁微变形进行精确监测具有重要工程意义，但现有方法难以有效处理动态物体和静态环境的干扰。

Method: 建立激励-桥梁耦合模型，设计专用帧结构，开发基于平均消除算法的相量统计分析和最小二乘圆拟合方法，从OFDM回波信号中提取微变形特征。

Result: 仿真结果表明该BMDM方案有效，对动态和静态干扰具有鲁棒性，能够准确提取垂直微变形值。

Conclusion: 所提出的ISAC方案为桥梁微变形监测提供了一种有效的技术途径，在复杂环境下表现出良好的性能。

Abstract: In this paper, we propose a novel integrated sensing and communications
(ISAC) scheme to perform bridge micro-deformation monitoring (BMDM) in complex
environments. We first provide an excitation-bridge coupling model to represent
the micro-deformation process of the bridge. Next, we design a novel frame
structure for BMDM applications, and construct the OFDM echo channel model for
basic scene of BMDM, including micro-deformation, dynamic objects, and static
environment. Then, we develop a phasor statistical analysis method based on
average cancellation algorithm to suppress the interference of dynamic objects,
as well as a circle fitting method based on least squares algorithm to remove
the interference of static environment near the monitoring area. Furthermore,
we extract the micro-deformation feature vector from the OFDM echo signals
after inverse discrete fourier transform (IDFT), and derive vertical
micro-deformation value with the time-frequency phase resources. Simulation
results demonstrate the effectiveness of the proposed BMDM scheme and its
robustness against both dynamic interferences and static interferences.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [27] [Investigating Polyglot Speech Foundation Models for Learning Collective Emotion from Crowds](https://arxiv.org/abs/2509.16329)
*Orchid Chetia Phukan,Girish,Mohd Mujtaba Akhtar,Panchal Nayak,Priyabrata Mallick,Swarup Ranjan Behera,Parabattina Bhagath,Pailla Balakrishna Reddy,Arun Balaji Buduru*

Main category: eess.AS

TL;DR: 本文研究多语言语音基础模型在人群情绪识别中的应用，发现多语言模型在嘈杂的群体环境中表现优于单语言和说话人识别模型，特别是在极短音频输入下仍能保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 研究假设多语言语音基础模型由于预训练时接触了多种语言、口音和语音模式，能够更好地处理人群场景中嘈杂复杂的声学环境，从而在人群情绪识别任务中具有优势。

Method: 在基准CER数据集上进行全面分析，比较多语言、单语言和说话人识别语音基础模型在不同音频时长（1秒、500毫秒、250毫秒）下的表现。

Result: 多语言语音基础模型在所有音频长度下均优于其他模型，即使在极短时长的音频输入下也能表现出色。

Conclusion: 这些发现为语音基础模型在建立新的人群情绪识别基准方面提供了适应路径。

Abstract: This paper investigates the polyglot (multilingual) speech foundation models
(SFMs) for Crowd Emotion Recognition (CER). We hypothesize that polyglot SFMs,
pre-trained on diverse languages, accents, and speech patterns, are
particularly adept at navigating the noisy and complex acoustic environments
characteristic of crowd settings, thereby offering a significant advantage for
CER. To substantiate this, we perform a comprehensive analysis, comparing
polyglot, monolingual, and speaker recognition SFMs through extensive
experiments on a benchmark CER dataset across varying audio durations (1 sec,
500 ms, and 250 ms). The results consistently demonstrate the superiority of
polyglot SFMs, outperforming their counterparts across all audio lengths and
excelling even with extremely short-duration inputs. These findings pave the
way for adaptation of SFMs in setting up new benchmarks for CER.

</details>


### [28] [Similarity-Guided Diffusion for Long-Gap Music Inpainting](https://arxiv.org/abs/2509.16342)
*Sean Turland,Eloi Moliner,Vesa Välimäki*

Main category: eess.AS

TL;DR: 提出了SimDPS方法，结合扩散模型和相似性搜索，用于音乐修复中长间隙的重建，相比单独使用扩散模型或相似性搜索，能更好地保持音乐合理性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在中等长度间隙的音乐修复中表现良好，但在多秒级长间隙时难以保持音乐合理性。

Method: SimDPS方法首先基于上下文相似性从语料库中检索候选片段，然后将其整合到改进的似然函数中，引导扩散过程生成上下文一致的重建结果。

Result: 在2秒间隙的钢琴音乐修复主观评估中，SimDPS相比无引导扩散模型提升了感知合理性，且在存在适度相似候选时通常优于单独使用相似性搜索。

Conclusion: 结果表明混合相似性方法在基于扩散的音频增强中处理长间隙具有潜力。

Abstract: Music inpainting aims to reconstruct missing segments of a corrupted
recording. While diffusion-based generative models improve reconstruction for
medium-length gaps, they often struggle to preserve musical plausibility over
multi-second gaps. We introduce Similarity-Guided Diffusion Posterior Sampling
(SimDPS), a hybrid method that combines diffusion-based inference with
similarity search. Candidate segments are first retrieved from a corpus based
on contextual similarity, then incorporated into a modified likelihood that
guides the diffusion process toward contextually consistent reconstructions.
Subjective evaluation on piano music inpainting with 2-s gaps shows that the
proposed SimDPS method enhances perceptual plausibility compared to unguided
diffusion and frequently outperforms similarity search alone when moderately
similar candidates are available. These results demonstrate the potential of a
hybrid similarity approach for diffusion-based audio enhancement with long
gaps.

</details>


### [29] [Sound field estimation with moving microphones using kernel ridge regression](https://arxiv.org/abs/2509.16358)
*Jesper Brunnström,Martin Bo Møller,Jan Østergaard,Shoichi Koyama,Toon van Waterschoot,Marc Moonen*

Main category: eess.AS

TL;DR: 提出了一种基于核岭回归（KRR）的移动麦克风声场估计方法，通过离散时间连续空间声场模型和方向加权提高估计精度，并引入随机傅里叶特征（RFF）近似方法降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 移动麦克风声场估计相比固定麦克风具有更高的灵活性、更短的测量时间和更少的设备限制，但现有方法缺乏先验知识整合能力且计算成本高。

Method: 使用基于离散傅里叶变换和Herglotz波函数的离散时间连续空间声场模型构建KRR方法，引入方向加权改进估计精度，并提出RFF近似方法降低计算复杂度。

Result: 在仿真和真实数据上验证了方向加权KRR方法的有效性，RFF方法在降低计算成本的同时获得了相对较低的估计精度，提供了成本与性能的权衡方案。

Conclusion: 提出的KRR方法成功实现了移动麦克风声场估计，RFF近似为实际应用提供了可行的计算效率解决方案，该方法在声场估计领域具有重要应用价值。

Abstract: Sound field estimation with moving microphones can increase flexibility,
decrease measurement time, and reduce equipment constraints compared to using
stationary microphones. In this paper a sound field estimation method based on
kernel ridge regression (KRR) is proposed for moving microphones. The proposed
KRR method is constructed using a discrete time continuous space sound field
model based on the discrete Fourier transform and the Herglotz wave function.
The proposed method allows for the inclusion of prior knowledge as a
regularization penalty, similar to kernel-based methods with stationary
microphones, which is novel for moving microphones. Using a directional
weighting for the proposed method, the sound field estimates are improved,
which is demonstrated on both simulated and real data. Due to the high
computational cost of sound field estimation with moving microphones, an
approximate KRR method is proposed, using random Fourier features (RFF) to
approximate the kernel. The RFF method is shown to decrease computational cost
while obtaining less accurate estimates compared to KRR, providing a trade-off
between cost and performance.

</details>


### [30] [Harmonic Summation-Based Robust Pitch Estimation in Noisy and Reverberant Environments](https://arxiv.org/abs/2509.16480)
*Anup Singh,Kris Demuynck*

Main category: eess.AS

TL;DR: 提出一种在噪声环境中稳健的基音估计方法，通过NAMDF变换、概率状态生成和Viterbi算法优化，在多种信噪比下表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 高失真环境下的准确基音估计对语音处理应用至关重要，但现有方法在噪声环境中仍面临挑战

Method: 计算归一化平均幅度差函数(NAMDF)，将其转换为似然函数，生成每个样本偏移的基音概率状态，通过基音周期整数倍和相邻帧的似然值聚合增强噪声鲁棒性，并在Viterbi算法中引入连续性约束

Result: 实验结果显示该方法在不同SNR水平下均实现更低的基音粗大误差(GPE)和浊音决策错误(VDE)，在噪声和混响条件下均优于现有方法

Conclusion: 该方法为高失真环境提供了一种有效的稳健基音估计解决方案，在噪声鲁棒性方面表现优异

Abstract: Accurate pitch estimation is essential for numerous speech processing
applications, yet it remains challenging in high-distortion environments. This
paper proposes a robust pitch estimation method that delivers robust pitch
estimates in challenging noise environments. Our approach computes the
Normalized Average Magnitude Difference Function (NAMDF), transforms it into a
likelihood function, and generates probabilistic pitch states for frames at
each sample shift. To enhance noise robustness, we aggregate likelihood values
across integer multiples of the pitch period and neighboring frames.
Furthermore, we introduce a simple yet effective continuity constraint in the
Viterbi algorithm to refine pitch selection among multiple candidates.
Experimental results show that our method consistently achieves lower Gross
Pitch Error (GPE) and Voicing Decision Error (VDE) across various SNR levels,
outperforming existing methods in both noisy and reverberant conditions.

</details>


### [31] [TF-CorrNet: Leveraging Spatial Correlation for Continuous Speech Separation](https://arxiv.org/abs/2509.16481)
*Ui-Hyeop Shin,Bon Hyeok Ku,Hyung-Min Park*

Main category: eess.AS

TL;DR: 提出了一种名为TF-CorrNet的多通道语音分离网络，通过直接利用PHAT-beta相关输入来估计分离滤波器，采用时频双路径策略处理空间信息，并添加频谱模块建模源相关的时频模式。


<details>
  <summary>Details</summary>
Motivation: 传统多通道源分离方法使用IPD与幅度信息或实虚部堆叠，但声音源的空间信息本质上包含在麦克风间的相关性中，而每个麦克风的功率也提供源频谱信息。

Method: 提出TF-CorrNet网络，直接利用PHAT-beta相关输入估计分离滤波器，采用时频交替处理的双路径策略处理空间信息，并添加频谱模块建模源相关的直接时频模式。

Result: 在LibriCSS数据集上的实验结果表明，TF-CorrNet能有效分离语音声音，表现出高性能和低计算成本。

Conclusion: 所提出的TF-CorrNet方法在多通道语音分离任务中取得了良好效果，证明了利用相关性输入和双路径策略的有效性。

Abstract: In general, multi-channel source separation has utilized inter-microphone
phase differences (IPDs) concatenated with magnitude information in
time-frequency domain, or real and imaginary components stacked along the
channel axis. However, the spatial information of a sound source is
fundamentally contained in the differences between microphones, specifically in
the correlation between them, while the power of each microphone also provides
valuable information about the source spectrum, which is why the magnitude is
also included. Therefore, we propose a network that directly leverages a
correlation input with phase transform (PHAT)-beta to estimate the separation
filter. In addition, the proposed TF-CorrNet processes the features alternately
across time and frequency axes as a dual-path strategy in terms of spatial
information. Furthermore, we add a spectral module to model source-related
direct time-frequency patterns for improved speech separation. Experimental
results demonstrate that the proposed TF-CorrNet effectively separates the
speech sounds, showing high performance with a low computational cost in the
LibriCSS dataset.

</details>


### [32] [An Octave-based Multi-Resolution CQT Architecture for Diffusion-based Audio Generation](https://arxiv.org/abs/2509.16603)
*Maurício do V. M. da Costa,Eloi Moliner*

Main category: eess.AS

TL;DR: MR-CQTdiff是一种基于扩散的音频生成神经网络架构，采用多分辨率常数Q变换(CQT)来提高音频生成质量


<details>
  <summary>Details</summary>
Motivation: 解决传统CQT在低频区域时间分辨率不足的问题，实现更灵活和富有表现力的音频生成

Method: 使用高效可逆的CQT框架，在八度音阶基础上调整时频分辨率，构建多分辨率扩散模型

Result: 在多个架构和数据集上的实验表明，MR-CQTdiff在FAD指标上达到了最先进的音频质量，优于竞争架构

Conclusion: MR-CQTdiff通过多分辨率CQT设计成功解决了低频时间分辨率问题，为音频生成提供了更优的解决方案

Abstract: This paper introduces MR-CQTdiff, a novel neural-network architecture for
diffusion-based audio generation that leverages a multi-resolution Constant-$Q$
Transform (C$Q$T). The proposed architecture employs an efficient, invertible
CQT framework that adjusts the time-frequency resolution on an octave-by-octave
basis. This design addresses the issue of low temporal resolution at lower
frequencies, enabling more flexible and expressive audio generation. We conduct
an evaluation using the Fr\'echet Audio Distance (FAD) metric across various
architectures and two datasets. Experimental results demonstrate that
MR-CQTdiff achieves state-of-the-art audio quality, outperforming competing
architectures.

</details>


### [33] [Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](https://arxiv.org/abs/2509.16622)
*Mengqi Wang,Zhan Liu,Zengrui Jin,Guangzhi Sun,Chao Zhang,Philip C. Woodland*

Main category: eess.AS

TL;DR: 本文研究了基于扩散的大语言模型LLaDA在自动语音识别中的应用，探索了其作为外部审议模块和独立解码器的效果，在LibriSpeech数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 扩散基大语言模型作为自回归解码器的替代方案受到关注，本文旨在实证研究其在ASR任务中的应用潜力，特别是利用LLaDA的双向注意力和去噪能力来改进语音识别性能。

Method: 1) 将LLaDA作为Whisper-LLaMA转录的外部审议处理模块，探索随机掩码、低置信度掩码和半自回归策略；2) 评估Whisper-LLaDA作为独立解码器的效果，使用扩散基和半自回归解码方法。

Result: 在LibriSpeech上，最佳级联系统在test-clean/test-other上达到2.25%/4.94% WER，相比Whisper-LLaMA基线在test-other上相对改进12.3%。大多数实验配置实现了比基线更快的推理速度，但识别精度略低。

Conclusion: 扩散基LLM在ASR中具有应用潜力，音频条件嵌入对性能提升至关重要，为未来改进指明了有前景的方向。

Abstract: Diffusion-based large language models (DLLMs) have recently attracted growing
interest as an alternative to autoregressive decoders. In this work, we present
an empirical study on using the diffusion-based large language model LLaDA for
automatic speech recognition (ASR). We first investigate its use as an external
deliberation-based processing module for Whisper-LLaMA transcripts. By
leveraging the bidirectional attention and denoising capabilities of LLaDA, we
explore random masking, low-confidence masking, and semi-autoregressive
strategies, showing that Whisper-LLaDA substantially reduces WER compared with
the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER
on test-clean/test-other, representing a 12.3% relative improvement over the
Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA
without acoustic features fails to improve accuracy, highlighting the
importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA
as a standalone decoder for ASR with diffusion-based and semi-autoregressive
decoding. Most experimental configurations achieve faster inference than the
Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These
findings offer an empirical view of diffusion-based LLMs for ASR and point to
promising directions for improvements.

</details>


### [34] [Reverse Attention for Lightweight Speech Enhancement on Edge Devices](https://arxiv.org/abs/2509.16705)
*Shuubham Ojha,Felix Gervits,Carol Espy-Wilson*

Main category: eess.AS

TL;DR: 本文提出了一种轻量级深度学习模型，用于在资源受限设备上实现实时语音增强，通过紧凑架构和软注意力门机制在U-Net中实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限设备上的实时语音增强需求，设计轻量级模型以平衡计算效率和性能。

Method: 在U-Net架构中注入基于软注意力的注意力门机制，该架构原本擅长分割任务且针对GPU优化，采用紧凑设计实现快速推理。

Result: 实验表明模型在PESQ和WER等语音质量和可懂度指标上表现优异，相比未增强波形WER提升6.24%，PESQ得分提升0.64。

Conclusion: 所提模型在保持竞争力的同时实现了高效推理，为资源受限环境下的实时语音增强提供了有效解决方案。

Abstract: This paper introduces a lightweight deep learning model for real-time speech
enhancement, designed to operate efficiently on resource-constrained devices.
The proposed model leverages a compact architecture that facilitates rapid
inference without compromising performance. Key contributions include infusing
soft attention-based attention gates in the U-Net architecture which is known
to perform well for segmentation tasks and is optimized for GPUs. Experimental
evaluations demonstrate that the model achieves competitive speech quality and
intelligibility metrics, such as PESQ and Word Error Rates (WER), improving the
performance of similarly sized baseline models. We are able to achieve a 6.24%
WER improvement and a 0.64 PESQ score improvement over un-enhanced waveforms.

</details>


### [35] [QASTAnet: A DNN-based Quality Metric for Spatial Audio](https://arxiv.org/abs/2509.16715)
*Adrien Llave,Emma Granier,Grégory Pallone*

Main category: eess.AS

TL;DR: QASTAnet是一种基于深度神经网络的空间音频质量评估新方法，专门针对Ambisonics和双耳音频，旨在解决现有方法在真实信号上泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前空间音频技术发展中，可靠的音频质量评估方法至关重要。主观听音测试成本高昂，而现有预测模型对真实世界信号的泛化能力不足。

Method: 结合低阶听觉系统的专家建模和神经网络对高阶认知功能建模，使模型能够在少量数据下训练。专门针对空间音频（Ambisonics和双耳音频）设计。

Result: QASTAnet在多种内容类型（语音、音乐、环境音、无混响、有混响）上表现优于两个参考指标，特别是在编解码器伪影方面。

Conclusion: 该指标与主观评分高度相关，是编解码器开发中比较的理想候选方法，克服了现有方法的局限性。

Abstract: In the development of spatial audio technologies, reliable and shared methods
for evaluating audio quality are essential. Listening tests are currently the
standard but remain costly in terms of time and resources. Several models
predicting subjective scores have been proposed, but they do not generalize
well to real-world signals. In this paper, we propose QASTAnet (Quality
Assessment for SpaTial Audio network), a new metric based on a deep neural
network, specialized on spatial audio (ambisonics and binaural). As training
data is scarce, we aim for the model to be trainable with a small amount of
data. To do so, we propose to rely on expert modeling of the low-level auditory
system and use a neurnal network to model the high-level cognitive function of
the quality judgement. We compare its performance to two reference metrics on a
wide range of content types (speech, music, ambiance, anechoic, reverberated)
and focusing on codec artifacts. Results demonstrate that QASTAnet overcomes
the aforementioned limitations of the existing methods. The strong correlation
between the proposed metric prediction and subjective scores makes it a good
candidate for comparing codecs in their development.

</details>


### [36] [Feature Selection via Graph Topology Inference for Soundscape Emotion Recognition](https://arxiv.org/abs/2509.16760)
*Samuel Rey,Luca Martino,Roberto San Millan,Eduardo Morgado*

Main category: eess.AS

TL;DR: 本文提出了一种结合图学习和信息准则的特征选择框架，用于声音景观情感识别（SER），通过线性结构方程模型构建特征关系图，并开发了广义肘部检测器来确定合适的稀疏度。


<details>
  <summary>Details</summary>
Motivation: 声音景观研究已从噪声水平转向声音感知，但现有SER模型主要使用唤醒度和效价作为情感描述符，缺乏对特征关系的深入理解。

Method: 使用线性结构方程模型构建特征关系的稀疏图表示，提出广义肘部检测器确定图稀疏度，并在Emo-Soundscapes数据集上进行评估。

Result: 构建的图揭示了输入特征与情感输出之间的关系，特别是发现唤醒度和效价之间存在强连接，挑战了传统SER假设。

Conclusion: 图学习方法为SER提供了新的特征选择框架，揭示了情感维度间的复杂关系，对声音景观感知研究有重要意义。

Abstract: Research on soundscapes has shifted the focus of environmental acoustics from
noise levels to the perception of sounds, incorporating contextual factors.
Soundscape emotion recognition (SER) models perception using a set of features,
with arousal and valence commonly regarded as sufficient descriptors of affect.
In this work, we blend \emph{graph learning} techniques with a novel
\emph{information criterion} to develop a feature selection framework for SER.
Specifically, we estimate a sparse graph representation of feature relations
using linear structural equation models (SEM) tailored to the widely used
Emo-Soundscapes dataset. The resulting graph captures the relations between
input features and the two emotional outputs. To determine the appropriate
level of sparsity, we propose a novel \emph{generalized elbow detector}, which
provides both a point estimate and an uncertainty interval. We conduct an
extensive evaluation of our methods, including visualizations of the inferred
relations. While several of our findings align with previous studies, the graph
representation also reveals a strong connection between arousal and valence,
challenging common SER assumptions.

</details>


### [37] [Automotive Sound Quality for EVs: Psychoacoustic Metrics with Reproducible AI/ML Baselines](https://arxiv.org/abs/2509.16901)
*Mandip Goswami*

Main category: eess.AS

TL;DR: 本文提出了一个开放、可复现的汽车声学质量参考框架，将标准化心理声学指标与轻量级AI/ML基准模型相结合，特别关注电动汽车的声音特性。


<details>
  <summary>Details</summary>
Motivation: 为电动汽车声学质量评估提供可靠、可复现的基准方法，支持教学、复现和扩展到EV特定噪声现象（如逆变器啸叫、掩蔽效应降低）。

Method: 实现标准化心理声学指标（ISO 532-1/2响度、DIN 45681音调、粗糙度和波动强度），提供简单可复现的基线模型（逻辑回归、随机森林、SVM），使用固定数据分割和种子进行合成EV案例测试。

Result: 报告了端到端工作流程的准确率和秩相关结果，所有图表和表格通过脚本在固定环境中重新生成，代码和音频刺激以宽松许可证发布。

Conclusion: 该框架为汽车声学质量评估提供了标准化、可复现的参考基准，特别适用于电动汽车的声学特性分析，支持后续研究和应用扩展。

Abstract: We present an open, reproducible reference for automotive sound quality that
connects standardized psychoacoustic metrics with lightweight AI/ML baselines,
with a specific focus on electric vehicles (EVs). We implement loudness (ISO
532-1/2), tonality (DIN 45681), and modulation-based descriptors (roughness,
fluctuation strength), and document assumptions and parameterizations for
reliable reuse. For modeling, we provide simple, fully reproducible baselines
(logistic regression, random forest, SVM) on synthetic EV-like cases using
fixed splits and seeds, reporting accuracy and rank correlations as examples of
end-to-end workflows rather than a comparative benchmark. Program-level
normalization is reported in LUFS via ITU-R BS.1770, while psychoacoustic
analysis uses ISO-532 loudness (sones). All figures and tables are regenerated
by scripts with pinned environments; code and minimal audio stimuli are
released under permissive licenses to support teaching, replication, and
extension to EV-specific noise phenomena (e.g., inverter whine, reduced
masking).

</details>


### [38] [DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time UAV Speech Enhancement](https://arxiv.org/abs/2509.16945)
*Jeongmin Lee,Chanhong Jeon,Hyungjoo Seo,Taewook Kang*

Main category: eess.AS

TL;DR: DroFiT是一种用于无人机自噪声环境下语音增强的轻量级Transformer网络，结合频域Transformer、混合编码器-解码器和TCN后端，在保持竞争力的增强性能同时显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限的无人机平台上严重自噪声环境下的实时语音增强问题，传统方法计算开销大，难以在无人机上实时运行。

Method: 集成频域Transformer与全/子带混合编码器-解码器，使用TCN后端实现内存高效流式处理，采用可学习的跳跃门控融合和联合频谱-时序损失函数。

Result: 在VoiceBank-DEMAND与无人机噪声混合数据集（-5到-25 dB SNR）上评估，DroFiT在保持竞争力的语音增强性能同时显著降低了计算和内存需求。

Conclusion: DroFiT为资源受限的无人机平台上的实时语音处理铺平了道路，实现了性能与效率的良好平衡。

Abstract: This paper proposes DroFiT (Drone Frequency lightweight Transformer for
speech enhancement, a single microphone speech enhancement network for severe
drone self-noise. DroFit integrates a frequency-wise Transformer with a
full/sub-band hybrid encoder-decoder and a TCN back-end for memory-efficient
streaming. A learnable skip-and-gate fusion with a combined spectral-temporal
loss further refines reconstruction. The model is trained on VoiceBank-DEMAND
mixed with recorded drone noise (-5 to -25 dB SNR) and evaluate using standard
speech enhancement metrics and computational efficiency. Experimental results
show that DroFiT achieves competitive enhancement performance while
significantly reducing computational and memory demands, paving the way for
real-time processing on resource-constrained UAV platforms. Audio demo samples
are available on our demo page.

</details>


### [39] [Attentive AV-FusionNet: Audio-Visual Quality Prediction with Hybrid Attention](https://arxiv.org/abs/2509.16994)
*Ina Salaj,Arijit Biswas*

Main category: eess.AS

TL;DR: 提出了一种新颖的深度学习音频视觉质量预测模型，利用最先进单模态预测器的内部特征，通过混合表征和注意力机制提升跨模态交互建模能力


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简单的融合策略，无法有效捕捉音频和视频模态之间的复杂交互关系，需要更智能的跨模态质量预测方法

Method: 采用混合表征方法，结合学习的GML音频特征和手工制作的VMAF视频特征，使用注意力机制捕获跨模态交互和模态内关系，并引入模态相关性估计器量化各模态贡献

Result: 实验表明该模型在多种内容类型上实现了改进的AVQ预测准确性和鲁棒性

Conclusion: 该方法为音频视觉质量预测提供了更有效的解决方案，并可能实现自适应比特率分配

Abstract: We introduce a novel deep learning-based audio-visual quality (AVQ)
prediction model that leverages internal features from state-of-the-art
unimodal predictors. Unlike prior approaches that rely on simple fusion
strategies, our model employs a hybrid representation that combines learned
Generative Machine Listener (GML) audio features with hand-crafted Video
Multimethod Assessment Fusion (VMAF) video features. Attention mechanisms
capture cross-modal interactions and intra-modal relationships, yielding
context-aware quality representations. A modality relevance estimator
quantifies each modality's contribution per content, potentially enabling
adaptive bitrate allocation. Experiments demonstrate improved AVQ prediction
accuracy and robustness across diverse content types.

</details>


### [40] [MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances](https://arxiv.org/abs/2509.17143)
*Junhyeok Lee,Helin Wang,Yaohan Guan,Thomas Thebaud,Laureano Moro-Velazquez,Jesús Villalba,Najim Dehak*

Main category: eess.AS

TL;DR: MaskVCT是一个零样本语音转换模型，通过多分类器自由引导实现多因素可控性，能够在单一模型中集成多种条件，提供说话人身份、语言内容和韵律因素的灵活控制。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换模型依赖固定的条件设置，缺乏灵活性。MaskVCT旨在开发一个能够集成多种条件并提供多因素可控性的零样本语音转换模型。

Method: 使用多分类器自由引导技术，整合连续或量化语言特征来增强可懂度和说话人相似性，可选择使用或忽略音高轮廓来控制韵律。

Result: 实验表明MaskVCT在目标说话人和口音相似性方面表现最佳，同时在词错误率和字符错误率方面与现有基线方法具有竞争力。

Conclusion: MaskVCT通过多条件集成和灵活控制机制，在零样本语音转换中实现了说话人身份、语言内容和韵律因素的良好平衡。

Abstract: We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers
multi-factor controllability through multiple classifier-free guidances (CFGs).
While previous VC models rely on a fixed conditioning scheme, MaskVCT
integrates diverse conditions in a single model. To further enhance robustness
and control, the model can leverage continuous or quantized linguistic features
to enhance intellgibility and speaker similarity, and can use or omit pitch
contour to control prosody. These choices allow users to seamlessly balance
speaker identity, linguistic content, and prosodic factors in a zero-shot VC
setting. Extensive experiments demonstrate that MaskVCT achieves the best
target speaker and accent similarities while obtaining competitive word and
character error rates compared to existing baselines. Audio samples are
available at https://maskvct.github.io/.

</details>


### [41] [DeepASA: An Object-Oriented One-for-All Network for Auditory Scene Analysis](https://arxiv.org/abs/2509.17247)
*Dongheon Lee,Younghoo Kwon,Jung-Woo Choi*

Main category: eess.AS

TL;DR: DeepASA是一个统一的多任务听觉场景分析模型，能够同时进行源分离、去混响、声音事件检测、音频分类和方向估计等任务。


<details>
  <summary>Details</summary>
Motivation: 针对复杂听觉场景中多个相似声源在时间和空间上重叠的问题，需要开发一个能够统一处理多种听觉分析任务的鲁棒模型。

Method: 采用面向对象处理策略，通过动态时间核特征提取器、基于transformer的聚合器和对象分离器构建对象中心表示，并引入时间一致性匹配机制进行多任务融合和迭代优化。

Result: 在ASA2、MC-FUSS和STARSS23等基准数据集上实现了最先进的性能，在源分离和听觉参数估计方面均表现出色。

Conclusion: DeepASA通过统一的对象中心框架成功解决了多任务听觉场景分析的挑战，为复杂空间听觉场景的处理提供了有效解决方案。

Abstract: We propose DeepASA, a one-for-all model for auditory scene analysis that
performs multi-input multi-output (MIMO) source separation, dereverberation,
sound event detection (SED), audio classification, and direction-of-arrival
estimation (DoAE) within a unified framework. DeepASA is designed for complex
auditory scenes where multiple, often similar, sound sources overlap in time
and move dynamically in space. To achieve robust and consistent inference
across tasks, we introduce an object-oriented processing (OOP) strategy. This
approach encapsulates diverse auditory features into object-centric
representations and refines them through a chain-of-inference (CoI) mechanism.
The pipeline comprises a dynamic temporal kernel-based feature extractor, a
transformer-based aggregator, and an object separator that yields per-object
features. These features feed into multiple task-specific decoders. Our
object-centric representations naturally resolve the parameter association
ambiguity inherent in traditional track-wise processing. However, early-stage
object separation can lead to failure in downstream ASA tasks. To address this,
we implement temporal coherence matching (TCM) within the chain-of-inference,
enabling multi-task fusion and iterative refinement of object features using
estimated auditory parameters. We evaluate DeepASA on representative spatial
audio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental
results show that our model achieves state-of-the-art performance across all
evaluated tasks, demonstrating its effectiveness in both source separation and
auditory parameter estimation under diverse spatial auditory scenes.

</details>


### [42] [Reference-aware SFM layers for intrusive intelligibility prediction](https://arxiv.org/abs/2509.17270)
*Hanlin Yu,Haoshuai Zhou,Boxuan Cao,Changgeng Mo,Linkai Li,Shan X. Wang*

Main category: eess.AS

TL;DR: 该论文提出了一种结合参考信号和多层语音基础模型表示的新型侵入式语音可懂度预测方法，在CPC3评测中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 当前侵入式语音可懂度预测器虽然利用显式参考信号，但未能持续超越非侵入式系统，主要原因是未能充分利用语音基础模型的潜力。

Method: 通过结合参考信号条件化和多层语音基础模型表示来重新审视侵入式预测方法。

Result: 最终系统在开发集上达到RMSE 22.36，在评估集上达到RMSE 24.98，在CPC3评测中排名第一。

Conclusion: 这些发现为构建基于语音基础模型的侵入式可懂度预测器提供了实用指导。

Abstract: Intrusive speech-intelligibility predictors that exploit explicit reference
signals are now widespread, yet they have not consistently surpassed
non-intrusive systems. We argue that a primary cause is the limited
exploitation of speech foundation models (SFMs). This work revisits intrusive
prediction by combining reference conditioning with multi-layer SFM
representations. Our final system achieves RMSE 22.36 on the development set
and 24.98 on the evaluation set, ranking 1st on CPC3. These findings provide
practical guidance for constructing SFM-based intrusive intelligibility
predictors.

</details>


### [43] [BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and Psychoacoustics Research](https://arxiv.org/abs/2509.17277)
*Mandip Goswami*

Main category: eess.AS

TL;DR: BeepBank-500是一个紧凑、完全合成的耳标/警报数据集，包含300-500个音频片段，专为快速、无版权问题的HCI和音频机器学习实验设计。


<details>
  <summary>Details</summary>
Motivation: 为人类-计算机交互和音频机器学习研究提供一个快速、无版权问题的合成耳标数据集，支持耳标分类、音色分析和起始检测等任务。

Method: 通过参数化配方生成音频片段，控制波形族（正弦、方波、三角波、FM）、基频、持续时间、振幅包络、振幅调制和轻量级Schroeder风格混响。使用三种混响设置：干声、小房间和中等房间。

Result: 发布了单声道48kHz WAV音频（16位）、丰富的元数据表（信号/频谱特征）以及用于波形族分类和基频回归的微小可复现基线。

Conclusion: BeepBank-500为耳标相关研究提供了一个紧凑、完全合成且无版权问题的数据集，音频采用CC0-1.0许可，代码采用MIT许可。

Abstract: We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset
(300-500 clips) designed for rapid, rights-clean experimentation in
human-computer interaction and audio machine learning. Each clip is generated
from a parametric recipe controlling waveform family (sine, square, triangle,
FM), fundamental frequency, duration, amplitude envelope, amplitude modulation
(AM), and lightweight Schroeder-style reverberation. We use three reverberation
settings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir
medium' ('medium') throughout the paper and in the metadata. We release mono 48
kHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and
tiny reproducible baselines for (i) waveform-family classification and (ii) f0
regression on single tones. The corpus targets tasks such as earcon
classification, timbre analyses, and onset detection, with clearly stated
licensing and limitations. Audio is dedicated to the public domain via CC0-1.0;
code is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code:
https://github.com/mandip42/earcons-mini-500.

</details>


### [44] [RADE for Land Mobile Radio: A Neural Codec for Transmission of Speech over Baseband FM Radio Channels](https://arxiv.org/abs/2509.17286)
*David Rowe,Tibor Bece*

Main category: eess.AS

TL;DR: 本文提出了一种基于自动编码器的机器学习方法，用于在基带FM数字无线电系统中传输高质量语音，相比传统模拟FM在衰落信道下具有更好的语音质量。


<details>
  <summary>Details</summary>
Motivation: 现有的数字和模拟FM系统共存但语音质量相似，许多数字无线电保留了模拟FM的调制解调器架构（基带FM），但使用数字脉冲驱动。作者希望利用现代机器学习技术提升这种架构的语音传输质量。

Method: 使用自动编码器架构，通过机器学习方法在基带FM信道上传输8kHz带宽的高质量语音信号。

Result: 在模拟的LMR衰落信道中，该方法显示出比模拟FM更优越的语音质量，并在商用UHF无线电上进行了实际演示验证。

Conclusion: 基于自动编码器的机器学习方法能够有效提升基带FM数字无线电系统的语音传输质量，特别是在存在信道衰落的场景下表现优异。

Abstract: In the 1990s Land Mobile Radio (LMR) systems evolved from analog frequency
modulation (FM) to standardised digital systems. Both digital and analog FM
systems now co-exist in various services and exhibit similar speech quality.
The architecture of many digital radios retains the analog FM modulator and
demodulator from legacy analog radios, but driven by a multi-level digital
pulse train rather than an analog voice signal. We denote this architecture
baseband FM (BBFM). In this paper we describe a modern machine learning
approach that uses an autoencoder to send high quality, 8 kHz bandwidth speech
over the BBFM channel. The speech quality is shown to be superior to analog FM
over simulated LMR channels in the presence of fading, and a demonstration of
the system running over commodity UHF radios is presented.

</details>


### [45] [Improving Active Learning for Melody Estimation by Disentangling Uncertainties](https://arxiv.org/abs/2509.17375)
*Aayush Jaiswal,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TL;DR: 本文研究在旋律估计任务中，利用不同不确定性（偶然不确定性和认知不确定性）来指导主动学习，发现认知不确定性在领域适应中更有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然探索了在主动学习设置中利用不确定性进行旋律估计，但没有研究不同不确定性的相对有效性。本文旨在通过解耦偶然不确定性和认知不确定性来指导主动学习。

Method: 采用一个框架来解耦偶然不确定性和认知不确定性，在源数据集上训练模型，然后使用少量标记样本适应新领域。

Result: 实验结果表明，与偶然不确定性相比，认知不确定性在减少标注工作量的情况下对领域适应更可靠。

Conclusion: 认知不确定性在旋律估计的主动学习领域适应中比偶然不确定性更有效，能够以更少的标注工作量实现更好的性能。

Abstract: Estimating the fundamental frequency, or melody, is a core task in Music
Information Retrieval (MIR). Various studies have explored signal processing,
machine learning, and deep-learning-based approaches, with a very recent focus
on utilizing uncertainty in active learning settings for melody estimation.
However, these approaches do not investigate the relative effectiveness of
different uncertainties. In this work, we follow a framework that disentangles
aleatoric and epistemic uncertainties to guide active learning for melody
estimation. Trained on a source dataset, our model adapts to new domains using
only a small number of labeled samples. Experimental results demonstrate that
epistemic uncertainty is more reliable for domain adaptation with reduced
labeling effort as compared to aleatoric uncertainty.

</details>


### [46] [SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription](https://arxiv.org/abs/2509.17404)
*Wei Tan,Shun Lei,Huaicheng Zhang,Guangzheng Li,Yixuan Zhang,Hangting Chen,Jianwei Yu,Rongzhi Gu,Dong Yu*

Main category: eess.AS

TL;DR: SongPrep是一个用于歌曲数据的自动化预处理流水线，解决了歌曲生成中数据准备的手动标注问题。SongPrepE2E是基于预训练语言模型的端到端结构化歌词识别模型，能够分析整首歌曲的结构和歌词并提供精确时间戳。


<details>
  <summary>Details</summary>
Motivation: 解决歌曲生成中数据准备的挑战，避免耗时耗力的手动标注工作。现有的歌曲数据转换为训练数据集需要大量人工标注，成本高昂。

Method: 提出SongPrep自动化预处理流水线，包括源分离、结构分析和歌词识别等关键流程。开发SongPrepE2E端到端结构化歌词识别模型，利用预训练语言模型和整首歌曲的上下文信息。

Result: SongPrepE2E在SSLD-200数据集上实现了低说话人错误率(DER)和词错误率(WER)。下游任务表明，使用SongPrepE2E输出的数据训练歌曲生成模型，生成的歌曲与人类创作的歌曲非常接近。

Conclusion: SongPrep和SongPrepE2E为歌曲生成提供了有效的自动化数据预处理解决方案，显著降低了数据准备成本，提高了歌曲生成模型的质量。

Abstract: Artificial Intelligence Generated Content (AIGC) is currently a popular
research area. Among its various branches, song generation has attracted
growing interest. Despite the abundance of available songs, effective data
preparation remains a significant challenge. Converting these songs into
training-ready datasets typically requires extensive manual labeling, which is
both time consuming and costly. To address this issue, we propose SongPrep, an
automated preprocessing pipeline designed specifically for song data. This
framework streamlines key processes such as source separation, structure
analysis, and lyric recognition, producing structured data that can be directly
used to train song generation models. Furthermore, we introduce SongPrepE2E, an
end-to-end structured lyrics recognition model based on pretrained language
models. Without the need for additional source separation, SongPrepE2E is able
to analyze the structure and lyrics of entire songs and provide precise
timestamps. By leveraging context from the whole song alongside pretrained
semantic knowledge, SongPrepE2E achieves low Diarization Error Rate (DER) and
Word Error Rate (WER) on the proposed SSLD-200 dataset. Downstream tasks
demonstrate that training song generation models with the data output by
SongPrepE2E enables the generated songs to closely resemble those produced by
humans.

</details>


### [47] [Neural acoustic multipole splatting for room impulse response synthesis](https://arxiv.org/abs/2509.17410)
*Geonwoo Baek,Jung-Woo Choi*

Main category: eess.AS

TL;DR: 提出了神经声学多极溅射（NAMS）方法，通过学习神经声学多极的位置并预测其发射信号和方向性，来合成任意接收器位置的房间脉冲响应（RIR）。


<details>
  <summary>Details</summary>
Motivation: 在空间音频渲染等实际应用中，需要在任意接收器位置预测RIR。现有方法在表达复杂声学场景时存在局限性。

Method: 使用神经声学多极组合表示声场，引入剪枝策略从密集的多极溅射开始，在训练过程中逐步消除冗余多极。

Result: 在真实和合成数据集上的实验表明，该方法在大多数指标上优于先前方法，同时保持快速推理。消融研究显示多极溅射加剪枝仅用20%的极数就能获得比单极模型更好的性能。

Conclusion: NAMS方法通过多极表示和剪枝策略，能够有效预测复杂声学场景中的RIR，在性能和效率上都取得了显著提升。

Abstract: Room Impulse Response (RIR) prediction at arbitrary receiver positions is
essential for practical applications such as spatial audio rendering. We
propose Neural Acoustic Multipole Splatting (NAMS), which synthesizes RIRs at
unseen receiver positions by learning the positions of neural acoustic
multipoles and predicting their emitted signals and directivities using a
neural network. Representing sound fields through a combination of multipoles
offers sufficient flexibility to express complex acoustic scenes while adhering
to physical constraints such as the Helmholtz equation. We also introduce a
pruning strategy that starts from a dense splatting of neural acoustic
multipoles and progressively eliminates redundant ones during training.
Experiments conducted on both real and synthetic datasets indicate that the
proposed method surpasses previous approaches on most metrics while maintaining
rapid inference. Ablation studies reveal that multipole splatting with pruning
achieves better performance than the monopole model with just 20% of the poles.

</details>


### [48] [FUN-SSL: Full-band Layer Followed by U-Net with Narrow-band Layers for Multiple Moving Sound Source Localization](https://arxiv.org/abs/2509.17490)
*Yuseon Choi,Hyeonseung Kim,Jewoo Jun,Jong Won Shin*

Main category: eess.AS

TL;DR: 提出了一种基于U-Net的声源定位架构FUN-SSL，通过在多个分辨率下进行窄带处理来降低计算复杂度，同时保持优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的双路径声源定位模型（如FN-SSL和IPDnet）虽然性能优秀但计算复杂度高，需要更高效的架构。

Method: 用FUN块替换IPDnet中的全窄网络块，FUN块包含一个全频带层和一个具有多尺度窄带层的U-Net，并在FUN块之间引入跳跃连接。

Result: 实验结果表明FUN-SSL在计算复杂度远低于IPDnet的情况下，性能优于先前提出的方法。

Conclusion: 提出的FUN-SSL架构在保持高性能的同时显著降低了计算复杂度，为声源定位任务提供了更高效的解决方案。

Abstract: Dual-path processing along the temporal and spectral dimensions has shown to
be effective in various speech processing applications. While the sound source
localization (SSL) models utilizing dual-path processing such as the FN-SSL and
IPDnet demonstrated impressive performances in localizing multiple moving
sources, they require significant amount of computation. In this paper, we
propose an architecture for SSL which introduces a U-Net to perform narrow-band
processing in multiple resolutions to reduce computational complexity. The
proposed model replaces the full-narrow network block in the IPDnet consisting
of one full-band LSTM layer along the spectral dimension followed by one
narrow-band LSTM layer along the temporal dimension with the FUN block composed
of one Full-band layer followed by a U-net with Narrow-band layers in multiple
scales. On top of the skip connections within each U-Net, we also introduce the
skip connections between FUN blocks to enrich information. Experimental results
showed that the proposed FUN-SSL outperformed previously proposed approaches
with computational complexity much lower than that of the IPDnet.

</details>


### [49] [Audiobook-CC: Controllable Long-context Speech Generation for Multicast Audiobook](https://arxiv.org/abs/2509.17516)
*Min Liu,JingJing Yin,Xiang Zhang,Siyu Hao,Yanni Hu,Bin Lin,Yuan Feng,Hongbin Zhou,Jianhao Ye*

Main category: eess.AS

TL;DR: 提出了一种针对多播有声读物的上下文感知和情感可控语音合成框架，解决了现有系统在上下文建模和细粒度性能控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本转语音系统主要关注单句合成，缺乏足够的上下文建模能力，也无法对多播有声读物的连贯性进行细粒度控制。

Method: 采用上下文机制保证上下文一致性，解耦范式将风格控制与语音提示分离以保证语义一致性，自蒸馏技术提升情感表达能力和指令可控性。

Result: 实验结果显示在叙述、对话和整章生成方面均优于现有基线方法，消融研究验证了所提方法的有效性。

Conclusion: 该框架为多播有声读物提供了有效的上下文感知和情感可控的语音合成解决方案，显著提升了生成质量。

Abstract: Existing text-to-speech systems predominantly focus on single-sentence
synthesis and lack adequate contextual modeling as well as fine-grained
performance control capabilities for generating coherent multicast audiobooks.
To address these limitations, we propose a context-aware and emotion
controllable speech synthesis framework specifically engineered for multicast
audiobooks with three key innovations: a context mechanism for contextual
consistency, a disentanglement paradigm to decouple style control from speech
prompts for semantic consistency, and self-distillation to boost emotional
expressiveness and instruction controllability. Experimental results show
superior performance across the generation of narration, dialogue, and the
whole chapter, significantly outperforming existing baselines. Ablation studies
are conducted to validate the effectiveness of our proposed methods. Demo
samples can be found in https://everest-ai.github.io/.

</details>


### [50] [Comparator Loss: An Ordinal Contrastive Loss to Derive a Severity Score for Speech-based Health Monitoring](https://arxiv.org/abs/2509.17661)
*Jacob J Webber,Oliver Watts,Lovisa Wihlborg,David Wheatley,Johnny Tam,Christine Weaver,Suvankar Pal,Siddharthan Chandran,Cassia Valentini-Botinhao*

Main category: eess.AS

TL;DR: 提出了一种基于比较器损失的新型疾病进展监测方法——严重程度评分，用于追踪神经退行性疾病的进展


<details>
  <summary>Details</summary>
Motivation: 现有语音健康监测方法主要关注患者与健康对照的分类或预测实际健康指标，缺乏对疾病进展的详细监测

Method: 使用比较器损失训练模型，确保评分遵循排序关系（基于诊断、临床注释评分或录音时间顺序），可整合不同健康指标信息

Result: 评估了模型在追踪运动神经元疾病患者进展、与ALSFRS-R等临床注释的相关性以及区分MND患者与健康对照的能力

Conclusion: 基于比较器损失的系统比简单离散分类提供更详细的疾病进展信息，并能充分利用小型健康相关数据集

Abstract: Monitoring the progression of neurodegenerative disease has important
applications in the planning of treatment and the evaluation of future
medications. Whereas much of the state-of-the-art in health monitoring from
speech has been focused on classifying patients versus healthy controls, or
predicting real-world health metrics, we propose here a novel measure of
disease progression: the severity score. This score is derived from a model
trained to minimize what we call the comparator loss. The comparator loss
ensures scores follow an ordering relation, which can be based on diagnosis,
clinically annotated scores, or simply the chronological order of the
recordings. In addition to giving a more detailed picture than a simple
discrete classification, the proposed comparator loss-based system has the
potential to incorporate information from disparate health metrics, which is
critical for making full use of small health-related datasets. We evaluated our
proposed models based on their ability to affirmatively track the progression
of patients with motor neuron disease (MND), the correlation of their output
with clinical annotations such as ALSFRS-R, as well as their ability to
distinguish between subjects with MND and healthy controls.

</details>


### [51] [GAN-Based Multi-Microphone Spatial Target Speaker Extraction](https://arxiv.org/abs/2509.17741)
*Shrishti Saha Shetu,Emanuël A. P. Habets,Andreas Brendel*

Main category: eess.AS

TL;DR: 本文提出了一种基于GAN的空间目标说话人提取方法，通过结合判别式空间滤波模型的中间特征和方向信息，实现了5度高空间分辨率的可控目标提取，在感知质量指标上优于现有判别式方法。


<details>
  <summary>Details</summary>
Motivation: 虽然基于深度神经网络的判别式方法在空间目标说话人提取方面取得了显著进展，但生成式方法（如GAN）在该问题上的潜力尚未被充分探索。

Method: 通过将GAN条件化在判别式空间滤波模型的中间特征和方向信息上，利用噪声混合信号和空间信息来提取和生成目标说话人的语音。

Result: 该方法实现了5度高空间分辨率的目标提取，在感知质量相关的客观指标上优于最先进的判别式方法。

Conclusion: GAN能够有效利用空间信息进行目标说话人提取，展示了生成式方法在该领域的应用潜力。

Abstract: Spatial target speaker extraction isolates a desired speaker's voice in
multi-speaker environments using spatial information, such as the direction of
arrival (DoA). Although recent deep neural network (DNN)-based discriminative
methods have shown significant performance improvements, the potential of
generative approaches, such as generative adversarial networks (GANs), remains
largely unexplored for this problem. In this work, we demonstrate that a GAN
can effectively leverage both noisy mixtures and spatial information to extract
and generate the target speaker's speech. By conditioning the GAN on
intermediate features of a discriminative spatial filtering model in addition
to DoA, we enable steerable target extraction with high spatial resolution of 5
degrees, outperforming state-of-the-art discriminative methods in perceptual
quality-based objective metrics.

</details>


### [52] [Benchmarking Humans and Machines on Complex Multilingual Speech Understanding Tasks](https://arxiv.org/abs/2509.17965)
*Sai Samrat Kankanala,Ram Chandra,Sriram Ganapathy*

Main category: eess.AS

TL;DR: 本文研究了多语言环境下人类和机器在语音理解中的听觉注意机制，发现在混合语音场景中，人类在母语中的选择性注意优于第二语言，而语音大语言模型在单说话人条件下表现优异但在双说话人场景中面临挑战。


<details>
  <summary>Details</summary>
Motivation: 研究多语言受试者在复杂声学场景中的听觉注意和选择性相位锁定能力，以及机器在重叠和混合通道语音理解方面的表现，填补了该领域的研究空白。

Method: 提出了一个系统化范式，在多语言环境中使用清晰和混合通道语音进行人类和机器的语音问答任务研究，比较人类在母语和第二语言中的表现，以及语音大语言模型在不同条件下的性能。

Result: 人类听众在母语中对目标说话人的选择性注意显著优于第二语言；语音大语言模型在清晰单说话人条件下达到或超过人类表现，但在双说话人设置中往往难以选择性注意。

Conclusion: 人类依赖母语中更简化的注意线索，而大语言模型倾向于并行信息提取策略，这种差异揭示了两种系统在处理复杂语音场景时的根本不同。

Abstract: Auditory attention and selective phase-locking are central to human speech
understanding in complex acoustic scenes and cocktail party settings, yet these
capabilities in multilingual subjects remain poorly understood. While machine
understanding of natural speech has advanced in recent years, questions persist
about comprehension of overlapped and mixed-channel speech. We propose a
systematic paradigm for studying humans and machines in speech
question-answering tasks in multilingual settings with clean and mixed-channel
speech. For human listeners, selective attention to a target speaker was
significantly better in their native language (L1) than in their second
language (L2). For machine listening, speech-based large language models (LLMs)
match or exceed human performance in clean, single-speaker conditions but often
struggle to selectively attend in two-speaker settings. These results reveal a
key divergence: humans rely on attentional cues that are more streamlined in
their native language, whereas LLMs default to parallel information extraction
which exceed human skills.

</details>


### [53] [Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech](https://arxiv.org/abs/2509.17988)
*Zirui Li,Jens Edlund,Yicheng Gu,Nhan Phan,Lauri Juvela,Mikko Kurimo*

Main category: eess.AS

TL;DR: Nord-Parl-TTS是一个用于芬兰语和瑞典语的开放文本转语音数据集，基于北欧议会录音构建，包含900小时芬兰语和5090小时瑞典语音频数据，旨在缩小高资源语言与低资源语言之间的TTS资源差距。


<details>
  <summary>Details</summary>
Motivation: 当前TTS发展受限于大多数语言缺乏高质量公开语音数据，特别是芬兰语和瑞典语等非高资源语言。

Method: 使用北欧议会录音，采用改进的Emilia数据处理流程提取适合TTS训练的语音数据，并构建统一的评估集。

Result: 成功构建了包含900小时芬兰语和5090小时瑞典语的大规模TTS数据集，为模型开发和基准测试提供支持。

Conclusion: Nord-Parl-TTS通过提供开放的大规模芬兰语和瑞典语数据，缩小了高资源语言与低资源语言在TTS资源方面的差距。

Abstract: Text-to-speech (TTS) development is limited by scarcity of high-quality,
publicly available speech data for most languages outside a few high-resource
languages. We present Nord-Parl-TTS, an open TTS dataset for Finnish and
Swedish based on speech found in the wild. Using recordings of Nordic
parliamentary proceedings, we extract 900 hours of Finnish and 5090 hours of
Swedish speech suitable for TTS training. The dataset is built using an adapted
version of the Emilia data processing pipeline and includes unified evaluation
sets to support model development and benchmarking. By offering open,
large-scale data for Finnish and Swedish, Nord-Parl-TTS narrows the resource
gap in TTS between high- and lower-resourced languages.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [54] [Etude: Piano Cover Generation with a Three-Stage Approach - Extract, strucTUralize, and DEcode](https://arxiv.org/abs/2509.16522)
*Tse-Yang Chen,Yuh-Jzer Joung*

Main category: cs.SD

TL;DR: Etude是一个三阶段钢琴编曲生成架构，通过提取节奏信息并应用简化的REMI标记化方法，能够生成保持原歌曲结构、增强流畅性和音乐动态的钢琴编曲。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在钢琴编曲生成中往往无法保持与原歌曲的结构一致性，主要原因是缺乏节拍感知机制或难以建模复杂的节奏模式。节奏信息对于定义结构相似性和生成音乐质量至关重要。

Method: 提出Etude三阶段架构：提取阶段预提取节奏信息，结构阶段应用简化的REMI标记化方法，解码阶段生成钢琴编曲。模型支持通过风格注入实现高度可控的生成。

Result: 主观评估显示，Etude显著优于现有模型，其生成质量达到与人类作曲家相当的水平。

Conclusion: Etude通过节奏信息提取和简化的标记化方法，成功解决了钢琴编曲生成中的结构一致性问题，生成的编曲在结构保持、流畅性和音乐动态方面表现出色。

Abstract: Piano cover generation aims to automatically transform a pop song into a
piano arrangement. While numerous deep learning approaches have been proposed,
existing models often fail to maintain structural consistency with the original
song, likely due to the absence of beat-aware mechanisms or the difficulty of
modeling complex rhythmic patterns. Rhythmic information is crucial, as it
defines structural similarity (e.g., tempo, BPM) and directly impacts the
overall quality of the generated music.
  In this paper, we introduce Etude, a three-stage architecture consisting of
Extract, strucTUralize, and DEcode stages. By pre-extracting rhythmic
information and applying a novel, simplified REMI-based tokenization, our model
produces covers that preserve proper song structure, enhance fluency and
musical dynamics, and support highly controllable generation through style
injection. Subjective evaluations with human listeners show that Etude
substantially outperforms prior models, achieving a quality level comparable to
that of human composers.

</details>


### [55] [Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks](https://arxiv.org/abs/2509.16566)
*Omar Eldeeb,Martin Malandro*

Main category: cs.SD

TL;DR: 本文提出了一种针对符号音乐（MIDI）的自动段落边界检测方法，通过引入人工标注的MIDI数据集和基于合成泛音的新型编码方案，显著提升了段落边界检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的音乐结构分析方法主要针对音频数据，而符号音乐包含丰富的音高、时序和乐器信息，但现有方法未能充分利用这些优势。本文旨在开发专门针对符号音乐的段落边界检测技术。

Method: 1) 构建包含6134个MIDI文件的人工标注数据集；2) 提出基于合成泛音的3通道钢琴卷帘编码方案；3) 训练深度学习模型在固定长度音乐窗口内分类段落边界。

Result: 模型F1分数达到0.77，比基于音频的监督学习方法提升0.22，比无监督的块匹配分割方法提升0.31。

Conclusion: 该方法证明了符号音乐在音乐结构分析中的优势，通过专门设计的编码方案和数据集，显著提升了段落边界检测的准确率。

Abstract: Current methods for Music Structure Analysis (MSA) focus primarily on audio
data. While symbolic music can be synthesized into audio and analyzed using
existing MSA techniques, such an approach does not exploit symbolic music's
rich explicit representation of pitch, timing, and instrumentation. A key
subproblem of MSA is section boundary detection-determining whether a given
point in time marks the transition between musical sections. In this paper, we
study automatic section boundary detection for symbolic music. First, we
introduce a human-annotated MIDI dataset for section boundary detection,
consisting of metadata from 6134 MIDI files that we manually curated from the
Lakh MIDI dataset. Second, we train a deep learning model to classify the
presence of section boundaries within a fixed-length musical window. Our data
representation involves a novel encoding scheme based on synthesized overtones
to encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model
achieves an F1 score of 0.77, improving over the analogous audio-based
supervised learning approach and the unsupervised block-matching segmentation
(CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset,
code, and models.

</details>


### [56] [AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval](https://arxiv.org/abs/2509.16649)
*Hyun Jun Kim,Hyeong Yong Choi,Changwon Lim*

Main category: cs.SD

TL;DR: 本文介绍了AISTAT团队在DCASE 2025 Task 6中基于语言的音频检索任务的提交系统，采用双编码器架构和对比学习进行音频-文本表示对齐，通过蒸馏、LLM数据增强和聚类辅助分类任务优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于语言的音频检索任务，通过改进现有方法提升音频和文本模态之间的表示对齐效果。

Method: 使用双编码器架构分别编码音频和文本，采用对比学习进行表示对齐，结合蒸馏技术、LLM数据增强（包括回译和LLM混合）以及聚类辅助分类任务进行微调。

Result: 最佳单系统在Clotho开发测试集上达到mAP@16为46.62，四个系统集成后达到mAP@16为48.83。

Conclusion: 所提出的系统通过多技术组合有效提升了音频检索性能，集成方法进一步优化了结果。

Abstract: This report presents the AISTAT team's submission to the language-based audio
retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder
architecture, where audio and text modalities are encoded separately, and their
representations are aligned using contrastive learning. Drawing inspiration
from methodologies of the previous year's challenge, we implemented a
distillation approach and leveraged large language models (LLMs) for effective
data augmentation techniques, including back-translation and LLM mix.
Additionally, we incorporated clustering to introduce an auxiliary
classification task for further finetuning. Our best single system achieved a
mAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on
the Clotho development test split.

</details>


### [57] [On the de-duplication of the Lakh MIDI dataset](https://arxiv.org/abs/2509.16662)
*Eunjin Choi,Hyerin Kim,Jiwoo Ryu,Juhan Nam,Dasaem Jeong*

Main category: cs.SD

TL;DR: 该研究分析了Lakh MIDI数据集中的重复数据问题，提出了基于规则、检索模型和对比学习BERT的三种去重方法，最终生成了三个不同版本的过滤列表。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集对深度学习模型训练至关重要，但网络爬取的数据往往包含重复内容。在符号音乐领域，重复数据主要来自用户的不同编曲版本和元数据修改，这会导致训练评估不可靠，但该问题在MIR社区中尚未得到充分重视。

Method: 使用LMD的Clean MIDI子集作为基准测试集，评估了基于规则的方法、现有符号音乐检索模型，以及采用对比学习的BERT模型结合多种数据增强技术来识别重复文件。

Result: 提出了LMD的三个过滤版本，在最保守的设置下从178,561个文件中过滤掉至少38,134个重复样本。

Conclusion: 该研究为符号音乐领域的去重问题提供了系统解决方案，有助于提高数据集质量和模型训练可靠性。

Abstract: A large-scale dataset is essential for training a well-generalized
deep-learning model. Most such datasets are collected via scraping from various
internet sources, inevitably introducing duplicated data. In the symbolic music
domain, these duplicates often come from multiple user arrangements and
metadata changes after simple editing. However, despite critical issues such as
unreliable training evaluation from data leakage during random splitting,
dataset duplication has not been extensively addressed in the MIR community.
This study investigates the dataset duplication issues regarding Lakh MIDI
Dataset (LMD), one of the largest publicly available sources in the symbolic
music domain. To find and evaluate the best retrieval method for duplicated
data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in
which different versions of the same songs are grouped together. We first
evaluated rule-based approaches and previous symbolic music retrieval models
for de-duplication and also investigated with a contrastive learning-based BERT
model with various augmentations to find duplicate files. As a result, we
propose three different versions of the filtered list of LMD, which filters out
at least 38,134 samples in the most conservative settings among 178,561 files.

</details>


### [58] [Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection](https://arxiv.org/abs/2509.16670)
*Wenhuan Lu,Xinyue Song,Wenjun Ke,Zhizhi Yu,Wenhao Yang,Jianguo Wei*

Main category: cs.SD

TL;DR: Speech2See是一个端到端的语音驱动开放集目标检测方法，通过预训练和微调范式，直接从语音中定位和识别物体，无需文本中介。


<details>
  <summary>Details</summary>
Motivation: 解决音频定位任务中大规模配对音频图像数据稀缺的问题，以及现有方法依赖文本中介管道的局限性，适用于人机交互等文本输入不便的场景。

Method: 采用预训练和微调范式：预训练阶段使用查询引导语义聚合模块压缩语音嵌入；微调阶段引入参数高效的Mixture-of-LoRA-Experts架构实现跨模态适配。

Result: 在多个基准测试中表现出鲁棒和适应性强的性能，展示了强大的泛化能力和广泛适用性。

Conclusion: Speech2See方法有效解决了音频定位任务的关键瓶颈，为语音驱动的开放集目标检测提供了实用解决方案。

Abstract: Audio grounding, or speech-driven open-set object detection, aims to localize
and identify objects directly from speech, enabling generalization beyond
predefined categories. This task is crucial for applications like human-robot
interaction where textual input is impractical. However, progress in this
domain faces a fundamental bottleneck from the scarcity of large-scale, paired
audio-image data, and is further constrained by previous methods that rely on
indirect, text-mediated pipelines. In this paper, we introduce Speech-to-See
(Speech2See), an end-to-end approach built on a pre-training and fine-tuning
paradigm. Specifically, in the pre-training stage, we design a Query-Guided
Semantic Aggregation module that employs learnable queries to condense
redundant speech embeddings into compact semantic representations. During
fine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts
(MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation.
Extensive experiments show that Speech2See achieves robust and adaptable
performance across multiple benchmarks, demonstrating its strong generalization
ability and broad applicability.

</details>


### [59] [Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies](https://arxiv.org/abs/2509.16718)
*Vishnu Raja,Adithya V Ganesan,Anand Syamkumar,Ritwik Banerjee,H Andrew Schwartz*

Main category: cs.SD

TL;DR: 该论文研究了改进自动语音识别（ASR）模型对非典型语音（如构音障碍患者语音）识别性能的策略，比较了四种建模方法，发现结合规范模式和个体特异性的混合方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR模型（如Whisper）在识别非典型语音时表现不佳，需要探索既能泛化又能处理个体特异性的建模策略来改善对构音障碍等非典型语音的识别效果。

Method: 比较了四种策略：（a）规范模型（仅训练于典型语音）；（b）个体特异性模型（完全个性化）；（c）构音障碍规范模型（训练于其他构音障碍者语音）；（d）构音障碍-个体特异性混合模型（先建模规范模式再适应个体语音）。

Result: 混合模型表现优于纯个体特异性方法，且所需个性化数据更少（128训练样本的WER为36.43%，优于256样本的36.99%）。仅调整语音编码器（而非语言模型解码器）可将词错误率从71%降至32%。

Conclusion: 结合规范（跨说话者）和个体特异性（说话者特定）模式能有效改进ASR对非典型语音的识别，为改善弱势语音群体的ASR性能提供了有价值的方法。

Abstract: State-of-the-art automatic speech recognition (ASR) models like Whisper,
perform poorly on atypical speech, such as that produced by individuals with
dysarthria. Past works for atypical speech have mostly investigated fully
personalized (or idiosyncratic) models, but modeling strategies that can both
generalize and handle idiosyncracy could be more effective for capturing
atypical speech. To investigate this, we compare four strategies: (a)
$\textit{normative}$ models trained on typical speech (no personalization), (b)
$\textit{idiosyncratic}$ models completely personalized to individuals, (c)
$\textit{dysarthric-normative}$ models trained on other dysarthric speakers,
and (d) $\textit{dysarthric-idiosyncratic}$ models which combine strategies by
first modeling normative patterns before adapting to individual speech. In this
case study, we find the dysarthric-idiosyncratic model performs better than
idiosyncratic approach while requiring less than half as much personalized data
(36.43 WER with 128 train size vs 36.99 with 256). Further, we found that
tuning the speech encoder alone (as opposed to the LM decoder) yielded the best
results reducing word error rate from 71% to 32% on average. Our findings
highlight the value of leveraging both normative (cross-speaker) and
idiosyncratic (speaker-specific) patterns to improve ASR for underrepresented
speech populations.

</details>


### [60] [Drum-to-Vocal Percussion Sound Conversion and Its Evaluation Methodology](https://arxiv.org/abs/2509.16862)
*Rinka Nobukawa,Makito Kitamura,Tomohiko Nakamura,Shinnosuke Takamichi,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: 本文提出了鼓声到人声打击乐转换的新任务，通过神经音频合成器实现从鼓声到VP的转换，VQ-based RAVE模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 人声打击乐在当代无伴奏合唱中广泛应用，但其声学特性与传统语音和歌唱不同，需要专门的合成方法。

Method: 使用实时音频变分自编码器（RAVE）实现鼓声到VP的转换，比较了带和不带向量量化（VQ）的两种基线方法。

Result: 主观实验表明两种方法都能产生合理的VP输出，但基于VQ的RAVE模型转换效果更一致。

Conclusion: 成功定义了鼓声到VP转换任务，并建立了相应的评估标准，为VP合成提供了有效解决方案。

Abstract: This paper defines the novel task of drum-to-vocal percussion (VP) sound
conversion. VP imitates percussion instruments through human vocalization and
is frequently employed in contemporary a cappella music. It exhibits acoustic
properties distinct from speech and singing (e.g., aperiodicity, noisy
transients, and the absence of linguistic structure), making conventional
speech or singing synthesis methods unsuitable. We thus formulate VP synthesis
as a timbre transfer problem from drum sounds, leveraging their rhythmic and
timbral correspondence. To support this formulation, we define three
requirements for successful conversion: rhythmic fidelity, timbral consistency,
and naturalness as VP. We also propose corresponding subjective evaluation
criteria. We implement two baseline conversion methods using a neural audio
synthesizer, the real-time audio variational autoencoder (RAVE), with and
without vector quantization (VQ). Subjective experiments show that both methods
produce plausible VP outputs, with the VQ-based RAVE model yielding more
consistent conversion.

</details>


### [61] [Difficulty-Aware Score Generation for Piano Sight-Reading](https://arxiv.org/abs/2509.16913)
*Pedro Ramoneda,Masahiro Suzuki,Akira Maezawa,Xavier Serra*

Main category: cs.SD

TL;DR: 本文提出了一种通过辅助优化目标来控制符号音乐生成难度的方法，用于生成适合学生技能水平的钢琴练习曲。


<details>
  <summary>Details</summary>
Motivation: 在音乐教育中，根据学生技能水平调整学习材料很重要，但手动创建难度合适的练习材料需要大量时间和精力。

Method: 将难度控制作为符号音乐生成任务，引入难度预测的辅助优化目标来防止条件崩溃，使模型学习与目标难度对齐的内部表示。

Result: 通过自动指标和专家评估显示，该方法能更好地控制难度，具有教育价值。

Conclusion: 该方法为通过生成难度感知的练习材料实现个性化音乐教育迈出了重要一步。

Abstract: Adapting learning materials to the level of skill of a student is important
in education. In the context of music training, one essential ability is
sight-reading -- playing unfamiliar scores at first sight -- which benefits
from progressive and level-appropriate practice. However, creating exercises at
the appropriate level of difficulty demands significant time and effort. We
address this challenge as a controlled symbolic music generation task that aims
to produce piano scores with a desired difficulty level. Controlling symbolic
generation through conditioning is commonly done using control tokens, but
these do not always have a clear impact on global properties, such as
difficulty. To improve conditioning, we introduce an auxiliary optimization
target for difficulty prediction that helps prevent conditioning collapse -- a
common issue in which models ignore control signals in the absence of explicit
supervision. This auxiliary objective helps the model to learn internal
representations aligned with the target difficulty, enabling more precise and
adaptive score generation. Evaluation with automatic metrics and expert
judgments shows better control of difficulty and potential educational value.
Our approach represents a step toward personalized music education through the
generation of difficulty-aware practice material.

</details>


### [62] [PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control](https://arxiv.org/abs/2509.16922)
*Tianheng Zhu,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.SD

TL;DR: PGSTalker是一个基于3D高斯泼溅的实时音频驱动说话头合成框架，通过像素感知密度控制策略和多模态门控融合模块，在渲染质量、唇部同步精度和推理速度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF方法虽然能实现高保真重建，但存在渲染效率低和音频视觉同步不佳的问题，需要开发更高效的实时说话头生成方法。

Method: 提出像素感知密度控制策略自适应分配点密度，增强动态面部区域细节；引入轻量级多模态门控融合模块有效融合音频和空间特征，提高高斯变形预测精度。

Result: 在公开数据集上的实验表明，PGSTalker在渲染质量、唇部同步精度和推理速度方面优于现有的NeRF和3DGS方法，具有强大的泛化能力和实际部署潜力。

Conclusion: PGSTalker框架成功解决了现有方法的效率瓶颈，实现了高质量的实时音频驱动说话头合成，为虚拟现实、数字化身和电影制作等应用提供了实用解决方案。

Abstract: Audio-driven talking head generation is crucial for applications in virtual
reality, digital avatars, and film production. While NeRF-based methods enable
high-fidelity reconstruction, they suffer from low rendering efficiency and
suboptimal audio-visual synchronization. This work presents PGSTalker, a
real-time audio-driven talking head synthesis framework based on 3D Gaussian
Splatting (3DGS). To improve rendering performance, we propose a pixel-aware
density control strategy that adaptively allocates point density, enhancing
detail in dynamic facial regions while reducing redundancy elsewhere.
Additionally, we introduce a lightweight Multimodal Gated Fusion Module to
effectively fuse audio and spatial features, thereby improving the accuracy of
Gaussian deformation prediction. Extensive experiments on public datasets
demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches
in rendering quality, lip-sync precision, and inference speed. Our method
exhibits strong generalization capabilities and practical potential for
real-world deployment.

</details>


### [63] [Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment](https://arxiv.org/abs/2509.16926)
*Ragib Amin Nihal,Benjamin Yen,Takeshi Ashizawa,Kazuhiro Nakadai*

Main category: cs.SD

TL;DR: 提出一种结合交叉注意力机制和置信度加权评分的方法，用于改进多通道音频同步，在BioDCASE 2025挑战赛中取得最佳性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理非线性时钟漂移且缺乏不确定性量化机制，传统方法假设简单的漂移模式且无可靠性度量，深度学习模型通常将对齐视为二元分类任务而忽略通道间依赖关系

Method: 扩展BEATs编码器加入交叉注意力层来建模通道间时间关系，开发置信度加权评分函数使用完整预测分布而非二元阈值

Result: 在BioDCASE 2025 Task 1挑战赛中取得第一名，测试数据集平均MSE为0.30（深度学习基线为0.58），在ARU数据上MSE为0.14（减少77%），在斑胸草雀数据上MSE为0.45（减少18%）

Conclusion: 该框架支持概率性时间对齐，超越了点估计方法，虽然验证于生物声学背景，但适用于更广泛的多通道音频任务

Abstract: Multi-channel audio alignment is a key requirement in bioacoustic monitoring,
spatial audio systems, and acoustic localization. However, existing methods
often struggle to address nonlinear clock drift and lack mechanisms for
quantifying uncertainty. Traditional methods like Cross-correlation and Dynamic
Time Warping assume simple drift patterns and provide no reliability measures.
Meanwhile, recent deep learning models typically treat alignment as a binary
classification task, overlooking inter-channel dependencies and uncertainty
estimation. We introduce a method that combines cross-attention mechanisms with
confidence-weighted scoring to improve multi-channel audio synchronization. We
extend BEATs encoders with cross-attention layers to model temporal
relationships between channels. We also develop a confidence-weighted scoring
function that uses the full prediction distribution instead of binary
thresholding. Our method achieved first place in the BioDCASE 2025 Task 1
challenge with 0.30 MSE average across test datasets, compared to 0.58 for the
deep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU
data (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The
framework supports probabilistic temporal alignment, moving beyond point
estimates. While validated in a bioacoustic context, the approach is applicable
to a broader range of multi-channel audio tasks where alignment confidence is
critical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA

</details>


### [64] [AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for Coarse-to-Fine Audio Deep Reasoning](https://arxiv.org/abs/2509.16971)
*Yan Rong,Chenxing Li,Dong Yu,Li Liu*

Main category: cs.SD

TL;DR: AudioGenie-Reasoner (AGR) 是一个无需训练的多智能体系统，通过将音频深度推理转化为文本理解任务，实现了音频感知与推理的协同工作，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频深度推理模型存在感知与推理能力之间的差距，缺乏显式推理链的训练数据和主动探索迭代优化的机制。

Method: 采用从粗到精的认知过程，先将音频转换为粗略文本文档，然后通过主动迭代文档精炼循环，使用工具增强路径和专门智能体持续搜索缺失信息并增强证据链。

Result: 在多个基准测试中实现了最先进的性能，优于现有的开源音频深度推理模型。

Conclusion: AGR 通过将音频深度推理转化为文本理解任务，成功解锁了大语言模型的潜力，为音频深度推理提供了新的解决方案。

Abstract: Audio deep reasoning is a challenging task that requires expert-level
perception, multi-step logical inference, and the integration of contextual
knowledge. However, existing models suffer from a gap between audio perception
and reasoning abilities due to the lack of training data with explicit
reasoning chains and the absence of mechanisms for active exploration and
iterative refinement. To address these challenges, we propose
AudioGenie-Reasoner (AGR), the first unified training-free multi-agent system
that coordinates perception and reasoning over an evolving chain of textual
evidence. Our key idea is a paradigm shift that transforms audio deep reasoning
into complex text understanding task from a new perspective, thereby unlocking
the full potential of large language models. Specifically, the design of AGR
mimics the human coarse-to-fine cognitive process. It first transforms the
input audio into a coarse text-based document. Then, we design a novel
proactive iterative document refinement loop, featuring tool-augmented routes
and specialized agents, to continuously search for missing information and
augment the evidence chain in a coarse-to-fine manner until sufficient
question-related information is gathered for making final predictions.
Experimental results show that AGR achieves state-of-the-art (SOTA) performance
over existing open-source audio deep reasoning models across various
benchmarks. The code will be made publicly available.

</details>


### [65] [Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs](https://arxiv.org/abs/2509.16975)
*Yuhang Jia,Xu Zhang,Yang Chen,Hui Wang,Enzhi Wang,Yong Qin*

Main category: cs.SD

TL;DR: 该论文提出了首个基于多模态大语言模型的音频编辑评估框架，通过微调任务和链式思维提示来提升评估的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的发展，其增强的感知和推理能力为音频质量评估提供了更全面和可解释的方法，特别是在音频编辑评估这一挑战性任务中。

Method: 提出了两种微调任务来增强多音频理解能力，结合链式思维提示和轻量级指令微调，以提升逐步推理能力。

Result: 实验表明，该框架能够提供准确、可解释且基于文本的编辑评估，与人类判断和客观指标高度一致，并显著优于基线方法。

Conclusion: 该研究成功构建了首个基于自然语言的自动化音频编辑评估框架，展示了多模态大语言模型在音频质量评估中的潜力。

Abstract: Automatic mean opinion score (MOS) prediction provides a more perceptual
alternative to objective metrics, offering deeper insights into the evaluated
models. With the rapid progress of multimodal large language models (MLLMs),
their enhanced perceptual and reasoning abilities enable more comprehensive and
interpretable audio quality assessment. In this work, we tackle the challenging
task of audio editing evaluation and propose the first natural language-based
automated evaluation framework built on MLLMs. Our approach introduces two
fine-tuning tasks to boost multi-audio understanding, combined with
Chain-of-Thought prompting, and lightweight instruction tuning, to enhance
step-by-step reasoning. Experiment demonstrate that our framework delivers
accurate, interpretable, and text-based editing evaluation, closely aligning
with human judgments and objective metrics while substantially improving over
baselines. The code and demo are available at
https://github.com/NKU-HLT/Eval_Reasoning.

</details>


### [66] [Leveraging Multiple Speech Enhancers for Non-Intrusive Intelligibility Prediction for Hearing-Impaired Listeners](https://arxiv.org/abs/2509.16979)
*Boxuan Cao,Linkai Li,Hanlin Yu,Changgeng Mo,Haoshuai Zhou,Shan Xiang Wang*

Main category: cs.SD

TL;DR: 提出了一种非侵入式语音清晰度预测框架，利用语音增强器提供并行增强信号通路，无需参考信号即可进行鲁棒预测。通过评估三种先进增强器并引入2-clips增强策略，在多个数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统听力受损听众的语音清晰度评估方法需要干净的参考信号，这在现实场景中往往不可得，导致实验室评估与现实应用之间存在差距。

Method: 使用语音增强器构建并行增强信号通路，评估三种先进增强器性能，引入2-clips增强策略提升跨数据集泛化能力。

Result: 该方法在多个数据集上一致优于非侵入式基线CPC2 Champion，强增强器集成效果最佳，增强策略显著提升了在未见数据集上的鲁棒性。

Conclusion: 基于增强器引导的非侵入式清晰度预测方法在现实应用中具有巨大潜力，能够有效解决参考信号缺失的问题。

Abstract: Speech intelligibility evaluation for hearing-impaired (HI) listeners is
essential for assessing hearing aid performance, traditionally relying on
listening tests or intrusive methods like HASPI. However, these methods require
clean reference signals, which are often unavailable in real-world conditions,
creating a gap between lab-based and real-world assessments. To address this,
we propose a non-intrusive intelligibility prediction framework that leverages
speech enhancers to provide a parallel enhanced-signal pathway, enabling robust
predictions without reference signals. We evaluate three state-of-the-art
enhancers and demonstrate that prediction performance depends on the choice of
enhancer, with ensembles of strong enhancers yielding the best results. To
improve cross-dataset generalization, we introduce a 2-clips augmentation
strategy that enhances listener-specific variability, boosting robustness on
unseen datasets. Our approach consistently outperforms the non-intrusive
baseline, CPC2 Champion across multiple datasets, highlighting the potential of
enhancer-guided non-intrusive intelligibility prediction for real-world
applications.

</details>


### [67] [MBCodec:Thorough disentangle for high-fidelity audio compression](https://arxiv.org/abs/2509.17006)
*Ruonan Zhang,Xiaoyang Hao,Yichen Han,Junjie Cao,Yue Liu,Kai Zhang*

Main category: cs.SD

TL;DR: MBCodec是一种基于残差向量量化的多码本音频编解码器，通过分层结构表示实现语义和声学特征解耦，在170倍压缩下达到近无损重建


<details>
  <summary>Details</summary>
Motivation: 解决现有神经音频编解码器在语义和声学信息解耦方面的不足，提升合成语音的细节质量

Method: 使用自监督语义标记化和音频子带特征构建功能解耦的潜在空间，引入自适应dropout深度分层训练码本，采用多通道伪正交镜像滤波器

Result: 实现了24kHz音频的170倍压缩，比特率仅为2.2kbps，在各项评估中显著优于基线方法

Conclusion: MBCodec通过有效的特征解耦实现了高质量的语音压缩和重建，为文本转语音系统提供了高效的音频表示方法

Abstract: High-fidelity neural audio codecs in Text-to-speech (TTS) aim to compress
speech signals into discrete representations for faithful reconstruction.
However, prior approaches faced challenges in effectively disentangling
acoustic and semantic information within tokens, leading to a lack of
fine-grained details in synthesized speech. In this study, we propose MBCodec,
a novel multi-codebook audio codec based on Residual Vector Quantization (RVQ)
that learns a hierarchically structured representation. MBCodec leverages
self-supervised semantic tokenization and audio subband features from the raw
signals to construct a functionally-disentangled latent space. In order to
encourage comprehensive learning across various layers of the codec embedding
space, we introduce adaptive dropout depths to differentially train codebooks
across layers, and employ a multi-channel pseudo-quadrature mirror filter
(PQMF) during training. By thoroughly decoupling semantic and acoustic
features, our method not only achieves near-lossless speech reconstruction but
also enables a remarkable 170x compression of 24 kHz audio, resulting in a low
bit rate of just 2.2 kbps. Experimental evaluations confirm its consistent and
substantial outperformance of baselines across all evaluations.

</details>


### [68] [Bridging the gap between training and inference in LM-based TTS models](https://arxiv.org/abs/2509.17021)
*Ruonan Zhang,Lingzhou Mu,Xixin Wu,Kai Zhang*

Main category: cs.SD

TL;DR: 提出一种提示引导的混合训练方案来缓解基于语言模型的TTS系统中的曝光偏差问题，通过结合教师强制和自由运行训练模式，使训练过程更接近推理过程。


<details>
  <summary>Details</summary>
Motivation: 基于语言模型的TTS系统在训练时使用真实标记作为前缀来预测下一个标记，而在推理时这些标记不可用，这种训练-推理差距往往被忽视，导致曝光偏差问题。

Method: 采用混合训练范式，结合教师强制和自由运行训练，在训练过程中引入自生成标记，并加入EOS预测机制来检测错误的序列终止并自适应控制自由运行过程。

Result: 实验结果表明该方法有效缩小了训练-推理差距，提高了长格式语音合成的质量。

Conclusion: 提出的提示引导混合训练方案能够有效缓解基于语言模型的TTS系统中的曝光偏差问题，改善语音合成性能。

Abstract: Recent advancements in text-to-speech (TTS) have shown that language model
(LM) based systems offer competitive performance compared to traditional
approaches. However, in training, TTS models use ground-truth (GT) tokens as
prefixes to predict the next token, while in inference these tokens are not
available, a gap between training and inference that is often neglected. In
this study, we propose a prompt-guided hybrid training scheme to mitigate
exposure bias in popular LM-based TTS systems. Our core idea is to adopt a
hybrid training paradigm that combines teacher forcing with free running,
thereby introducing self-generated tokens into the training process. This makes
the training mode more consistent with inference, reducing the
training-inference gap. In addition, we incorporate an EOS prediction mechanism
during training to detect incorrect sequence termination and adaptively control
the free running process. Experimental results provide a comprehensive
evaluation of the impact of exposure bias on LM-based TTS, and demonstrate that
our method effectively narrows the training-inference gap, thereby improving
the quality of synthesized long-form speech.

</details>


### [69] [Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing](https://arxiv.org/abs/2509.17052)
*Wataru Nakata,Yuki Saito,Yota Ueda,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: Sidon是一个快速开源语音修复模型，可将嘈杂语音转换为录音室质量语音，支持多种语言，运行速度比实时快3390倍，可用于TTS系统的数据集清洗。


<details>
  <summary>Details</summary>
Motivation: 大规模文本转语音系统受限于干净多语言录音的稀缺性，需要有效的方法来清洗嘈杂的语音数据。

Method: Sidon包含两个模型：基于w2v-BERT 2.0微调的特征预测器用于清理嘈杂语音特征，以及声码器从清理后的特征合成修复语音。

Result: Sidon的修复性能与Google内部语音修复模型Miipher相当，在单GPU上运行速度比实时快3390倍，使用Sidon清洗的ASR语料训练TTS模型可提升零样本合成语音质量。

Conclusion: Sidon是一个高效的多语言语音修复工具，可促进研究社区的可复现数据集清洗工作，代码和模型已开源。

Abstract: Large-scale text-to-speech (TTS) systems are limited by the scarcity of
clean, multilingual recordings. We introduce Sidon, a fast, open-source speech
restoration model that converts noisy in-the-wild speech into studio-quality
speech and scales to dozens of languages. Sidon consists of two models:
w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech
and vocoder trained to synthesize restored speech from the cleansed features.
Sidon achieves restoration performance comparable to Miipher: Google's internal
speech restoration model with the aim of dataset cleansing for speech
synthesis. Sidon is also computationally efficient, running up to 3,390 times
faster than real time on a single GPU. We further show that training a TTS
model using a Sidon-cleansed automatic speech recognition corpus improves the
quality of synthetic speech in a zero-shot setting. Code and model are released
to facilitate reproducible dataset cleansing for the research community.

</details>


### [70] [SVeritas: Benchmark for Robust Speaker Verification under Diverse Conditions](https://arxiv.org/abs/2509.17091)
*Massa Baali,Sarthak Bisht,Francisco Teixeira,Kateryna Shapovalenko,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: SVeritas是一个全面的说话人验证基准测试套件，评估SV系统在各种现实和恶意条件下的性能，包括录音时长、噪声、声道不匹配、年龄差异、欺骗攻击等，填补了现有基准测试的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的说话人验证模型在真实世界挑战下的鲁棒性缺乏全面评估，现有基准测试只覆盖部分条件，无法全面评估模型性能。

Method: 开发SVeritas基准测试套件，包含多种自然和恶意创建的信号退化条件，评估多个最先进的SV模型在这些条件下的表现。

Result: 发现某些架构在常见失真下保持稳定，但在跨语言试验、年龄不匹配和编解码器压缩等场景下性能显著下降，同时发现不同人口亚组之间的鲁棒性差异。

Conclusion: SVeritas通过标准化评估为诊断模型弱点提供了基础，有助于推进公平可靠的说话人验证系统发展。

Abstract: Speaker verification (SV) models are increasingly integrated into security,
personalization, and access control systems, yet their robustness to many
real-world challenges remains inadequately benchmarked. These include a variety
of natural and maliciously created conditions causing signal degradations or
mismatches between enrollment and test data, impacting performance. Existing
benchmarks evaluate only subsets of these conditions, missing others entirely.
We introduce SVeritas, a comprehensive Speaker Verification tasks benchmark
suite, assessing SV systems under stressors like recording duration,
spontaneity, content, noise, microphone distance, reverberation, channel
mismatches, audio bandwidth, codecs, speaker age, and susceptibility to
spoofing and adversarial attacks. While several benchmarks do exist that each
cover some of these issues, SVeritas is the first comprehensive evaluation that
not only includes all of these, but also several other entirely new, but
nonetheless important, real-life conditions that have not previously been
benchmarked. We use SVeritas to evaluate several state-of-the-art SV models and
observe that while some architectures maintain stability under common
distortions, they suffer substantial performance degradation in scenarios
involving cross-language trials, age mismatches, and codec-induced compression.
Extending our analysis across demographic subgroups, we further identify
disparities in robustness across age groups, gender, and linguistic
backgrounds. By standardizing evaluation under realistic and synthetic stress
conditions, SVeritas enables precise diagnosis of model weaknesses and
establishes a foundation for advancing equitable and reliable speaker
verification systems.

</details>


### [71] [RISE: Adaptive music playback for Realtime Intensity Synchronization with Exercise](https://arxiv.org/abs/2509.17112)
*Alexander Wang,Chris Donahue,Dhruv Jain*

Main category: cs.SD

TL;DR: RISE系统通过自动识别音乐中的高能量片段，并将其与锻炼中的高强度间隔对齐，动态调整音乐结构以适应运动节奏，从而提升锻炼动机和表现。


<details>
  <summary>Details</summary>
Motivation: 锻炼时听音乐能提升动机和表现，但音乐结构与用户自然的休息/工作阶段可能不匹配，导致用户休息时间过长或中途失去动力。

Method: RISE系统自动估计音乐中的高强度片段，使用基于组件的音乐重排技术，动态延长或缩短歌曲的不同片段以适应正在进行的锻炼计划。输入包括休息和工作时长（通过预定义计划或手动输入确定）。

Result: 在12名参与者的实验室评估中，与无自适应音乐基线相比，参与者认为RISE的重排保持强度估计准确，许多人回忆到强度对齐帮助他们坚持完成锻炼。

Conclusion: RISE系统通过音乐与锻炼节奏的动态对齐，有效提升了锻炼体验和表现，证明了自适应音乐调整在运动场景中的价值。

Abstract: We propose a system to adapt a user's music to their exercise by aligning
high-energy music segments with intense intervals of the workout. Listening to
music during exercise can boost motivation and performance. However, the
structure of the music may be different from the user's natural phases of rest
and work, causing users to rest longer than needed while waiting for a
motivational section, or lose motivation mid-work if the section ends too soon.
To address this, our system, called RISE, automatically estimates the intense
segments in music and uses component-based music rearrangement techniques to
dynamically extend and shorten different segments of the user's song to fit the
ongoing exercise routine. Our system takes as input the rest and work durations
to guide adaptation. Currently, this is determined either via a pre-defined
plan or manual input during the workout. We evaluated RISE with 12 participants
and compared our system to a non-adaptive music baseline while exercising in
our lab. Participants found our rearrangements keeps intensity estimation
accurate, and many recalled moments when intensity alignment helped them push
through their workout.

</details>


### [72] [FakeSound2: A Benchmark for Explainable and Generalizable Deepfake Sound Detection](https://arxiv.org/abs/2509.17162)
*Zeyu Xie,Yaoyun Zhang,Xuenan Xu,Yongkang Yin,Chenxing Li,Mengyue Wu,Yuexian Zou*

Main category: cs.SD

TL;DR: FakeSound2是一个用于深度伪造音频检测的基准测试，旨在超越传统的二元分类，评估模型在定位、溯源和泛化三个维度的性能。


<details>
  <summary>Details</summary>
Motivation: 生成音频技术的快速发展带来了伦理和安全问题，现有检测方法主要关注二元分类，缺乏对伪造方式、来源追溯的解释能力，且泛化能力有限。

Method: 提出FakeSound2基准测试，涵盖6种伪造类型和12种不同来源，从定位、溯源和泛化三个维度评估模型性能。

Result: 实验结果表明，当前系统虽然分类准确率高，但难以识别伪造模式分布并提供可靠解释。

Conclusion: FakeSound2揭示了深度伪造音频检测的关键挑战，旨在促进开发更鲁棒、可解释和可泛化的可信音频认证方法。

Abstract: The rapid development of generative audio raises ethical and security
concerns stemming from forged data, making deepfake sound detection an
important safeguard against the malicious use of such technologies. Although
prior studies have explored this task, existing methods largely focus on binary
classification and fall short in explaining how manipulations occur, tracing
where the sources originated, or generalizing to unseen sources-thereby
limiting the explainability and reliability of detection. To address these
limitations, we present FakeSound2, a benchmark designed to advance deepfake
sound detection beyond binary accuracy. FakeSound2 evaluates models across
three dimensions: localization, traceability, and generalization, covering 6
manipulation types and 12 diverse sources. Experimental results show that
although current systems achieve high classification accuracy, they struggle to
recognize forged pattern distributions and provide reliable explanations. By
highlighting these gaps, FakeSound2 establishes a comprehensive benchmark that
reveals key challenges and aims to foster robust, explainable, and
generalizable approaches for trustworthy audio authentication.

</details>


### [73] [STAR: Speech-to-Audio Generation via Representation Learning](https://arxiv.org/abs/2509.17164)
*Zeyu Xie,Xuenan Xu,Yixuan Li,Mengyue Wu,Yuexian Zou*

Main category: cs.SD

TL;DR: STAR是首个端到端语音到音频生成框架，通过直接使用语音作为输入模态，减少级联系统的错误传播并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常依赖文本或视觉输入，而语音作为自然交互模态未被充分利用。级联系统存在错误传播和效率低下的问题。

Method: STAR通过表示学习提取语音中的声音事件语义，使用桥接网络进行表示映射，并采用两阶段训练策略实现端到端合成。

Result: STAR将语音处理延迟降低了76.9%，在生成性能上优于级联系统。

Conclusion: STAR确立了语音作为音频生成直接交互信号的有效性，桥接了表示学习和多模态合成。

Abstract: This work presents STAR, the first end-to-end speech-to-audio generation
framework, designed to enhance efficiency and address error propagation
inherent in cascaded systems. Unlike prior approaches relying on text or
vision, STAR leverages speech as it constitutes a natural modality for
interaction. As an initial step to validate the feasibility of the system, we
demonstrate through representation learning experiments that spoken sound event
semantics can be effectively extracted from raw speech, capturing both auditory
events and scene cues. Leveraging the semantic representations, STAR
incorporates a bridge network for representation mapping and a two-stage
training strategy to achieve end-to-end synthesis. With a 76.9% reduction in
speech processing latency, STAR demonstrates superior generation performance
over the cascaded systems. Overall, STAR establishes speech as a direct
interaction signal for audio generation, thereby bridging representation
learning and multimodal synthesis. Generated samples are available at
https://zeyuxie29.github.io/STAR.

</details>


### [74] [Virtual Consistency for Audio Editing](https://arxiv.org/abs/2509.17219)
*Matthieu Cervera,Francesco Paissan,Mirco Ravanelli,Cem Subakan*

Main category: cs.SD

TL;DR: 提出了一种基于虚拟一致性的音频编辑系统，通过调整扩散模型的采样过程来绕过缓慢的反转过程，实现了高效且高质量的文本驱动音频编辑。


<details>
  <summary>Details</summary>
Motivation: 当前基于反转的神经音频编辑方法依赖缓慢的反转过程，限制了实际应用。需要一种更高效的文本驱动音频编辑解决方案。

Method: 开发了模型无关的虚拟一致性音频编辑管道，无需微调或架构修改，通过调整扩散模型的采样过程来绕过反转步骤。

Result: 相比现有神经编辑基线实现了显著的速度提升，在定量基准测试和16名参与者的用户研究中保持了编辑质量。

Conclusion: 该方法在不牺牲质量的前提下大幅提升了音频编辑效率，为文本驱动的音频编辑提供了实用的解决方案。

Abstract: Free-form, text-based audio editing remains a persistent challenge, despite
progress in inversion-based neural methods. Current approaches rely on slow
inversion procedures, limiting their practicality. We present a
virtual-consistency based audio editing system that bypasses inversion by
adapting the sampling process of diffusion models. Our pipeline is
model-agnostic, requiring no fine-tuning or architectural changes, and achieves
substantial speed-ups over recent neural editing baselines. Crucially, it
achieves this efficiency without compromising quality, as demonstrated by
quantitative benchmarks and a user study involving 16 participants.

</details>


### [75] [Attention-based Mixture of Experts for Robust Speech Deepfake Detection](https://arxiv.org/abs/2509.17585)
*Viola Negroni,Davide Salvi,Alessandro Ilic Mezza,Paolo Bestagini,Stefano Tubaro*

Main category: cs.SD

TL;DR: 本文介绍了ISPL团队在IH&MMSec 2025 SAFE挑战赛中排名第一的音频深度伪造检测系统，采用专家混合架构结合注意力机制来动态加权多个检测器。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成语音在恶意用途（如冒充、虚假信息和生物特征欺骗）中的滥用日益严重，需要开发鲁棒的检测方法和有效对策来应对几乎无法与真实人类语音区分的语音深度伪造。

Method: 提出基于专家混合架构的新颖音频深度伪造检测方法，利用多个最先进的检测器，通过基于注意力的门控网络动态加权每个专家的输出，每个专家通过归纳偏置学习捕捉输入的不同互补方面。

Result: 实验结果表明该方法在多个数据集上优于现有方法，在SAFE挑战赛的所有任务中排名第一。

Conclusion: 所提出的专家混合架构结合注意力机制的方法在音频深度伪造检测方面表现出卓越性能，为解决日益严重的语音伪造问题提供了有效解决方案。

Abstract: AI-generated speech is becoming increasingly used in everyday life, powering
virtual assistants, accessibility tools, and other applications. However, it is
also being exploited for malicious purposes such as impersonation,
misinformation, and biometric spoofing. As speech deepfakes become nearly
indistinguishable from real human speech, the need for robust detection methods
and effective countermeasures has become critically urgent. In this paper, we
present the ISPL's submission to the SAFE challenge at IH&MMSec 2025, where our
system ranked first across all tasks. Our solution introduces a novel approach
to audio deepfake detection based on a Mixture of Experts architecture. The
proposed system leverages multiple state-of-the-art detectors, combining their
outputs through an attention-based gating network that dynamically weights each
expert based on the input speech signal. In this design, each expert develops a
specialized understanding of the shared training data by learning to capture
different complementary aspects of the same input through inductive biases.
Experimental results indicate that our method outperforms existing approaches
across multiple datasets. We further evaluate and analyze the performance of
our system in the SAFE challenge.

</details>


### [76] [Audio Super-Resolution with Latent Bridge Models](https://arxiv.org/abs/2509.17609)
*Chang Li,Zehua Chen,Liyuan Wang,Jun Zhu*

Main category: cs.SD

TL;DR: 本文提出了一种基于潜在桥模型（LBMs）的音频超分辨率系统，通过将音频波形压缩到连续潜在空间，实现潜在到潜在的生成过程，从而充分利用低分辨率波形中的信息先验。


<details>
  <summary>Details</summary>
Motivation: 现有的音频超分辨率方法由于生成先验信息不足，往往存在上采样质量不佳的问题。本文旨在开发一种能够充分利用低分辨率波形信息先验的高质量音频超分辨率系统。

Method: 采用潜在桥模型（LBMs）将音频压缩到连续潜在空间，设计频率感知LBMs以学习任意到任意上采样过程，并引入级联LBMs和先验增强策略实现无缝级联超分辨率。

Result: 在VCTK、ESC-50、Song-Describer等基准数据集上的实验表明，该方法在任意到48kHz超分辨率任务上达到了最先进的客观和感知质量，并首次实现了任意到192kHz音频超分辨率。

Conclusion: 提出的潜在桥模型系统在音频超分辨率任务中表现出色，特别是在高采样率上采样方面取得了突破性进展，为音频后处理提供了更高的灵活性。

Abstract: Audio super-resolution (SR), i.e., upsampling the low-resolution (LR)
waveform to the high-resolution (HR) version, has recently been explored with
diffusion and bridge models, while previous methods often suffer from
sub-optimal upsampling quality due to their uninformative generation prior.
Towards high-quality audio super-resolution, we present a new system with
latent bridge models (LBMs), where we compress the audio waveform into a
continuous latent space and design an LBM to enable a latent-to-latent
generation process that naturally matches the LR-toHR upsampling process,
thereby fully exploiting the instructive prior information contained in the LR
waveform. To further enhance the training results despite the limited
availability of HR samples, we introduce frequency-aware LBMs, where the prior
and target frequency are taken as model input, enabling LBMs to explicitly
learn an any-to-any upsampling process at the training stage. Furthermore, we
design cascaded LBMs and present two prior augmentation strategies, where we
make the first attempt to unlock the audio upsampling beyond 48 kHz and empower
a seamless cascaded SR process, providing higher flexibility for audio
post-production. Comprehensive experimental results evaluated on the VCTK,
ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate
that we achieve state-of-the-art objective and perceptual quality for
any-to-48kHz SR across speech, audio, and music signals, as well as setting the
first record for any-to-192kHz audio SR. Demo at https://AudioLBM.github.io/.

</details>


### [77] [Convolutional Neural Network Optimization for Beehive Classification Using Bioacoustic Signals](https://arxiv.org/abs/2509.17800)
*Harshit,Rahul Jana,Ritesh Kumar*

Main category: cs.SD

TL;DR: 该研究使用卷积神经网络对蜂箱声音信号进行分类和监测，通过比较不同时频表示方法，发现Cochleagram表现最佳，准确率达98.31%，并通过模型优化技术显著减小模型大小和加速推理速度。


<details>
  <summary>Details</summary>
Motivation: 蜜蜂行为是重要的生态现象，非侵入式监测蜂箱声音信号可以用于预测蜂群状态，但需要选择合适的时频表示方法和优化模型以实现实时应用。

Method: 使用卷积神经网络，比较Spectrogram、Mel-Spectrogram、Smoothed-Spectrogram和Cochleagram四种时频图像表示方法，并采用剪枝、量化和知识蒸馏等优化策略。

Result: Cochleagram表现最佳，准确率达98.31%；模型优化后大小减少91.8%，推理速度提升66%。

Conclusion: 选择合适的时频表示方法和采用模型优化技术对于实现实时蜂箱监测应用至关重要，Cochleagram是最优选择。

Abstract: The behavior of honeybees is an important ecological phenomenon not only in
terms of honey and beeswax production but also due to the proliferation of
flora and fauna around it. The best way to study this significant phenomenon is
by non-invasive monitoring of beehives using the sounds produced by various
body movements that give out audio signals which can be exploited for various
predictions related to the objectives mentioned above. This study investigates
the application of Convolutional Neural Networks to classify and monitor
different hive states with the help of joint time and frequency image
representations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and
Cochleagram. Our findings indicate that the Cochleagram outperformed all the
other representations, achieving an accuracy of 98.31% on unseen data.
Furthermore, we employed various strategies including pruning, quantization,
and knowledge distillation to optimize the network and prevent any potential
issues with model size. With these optimizations, the network size was lowered
by 91.8% and the inference time was accelerated by 66%, increasing its
suitability for real-time applications. Thus our study emphasizes the
significance of using optimization approaches to minimize model size, avoid
deployment problems, and expedite inference for real-time application as well
as the selection of an appropriate time-frequency representation for optimal
performance.

</details>


### [78] [Brainprint-Modulated Target Speaker Extraction](https://arxiv.org/abs/2509.17883)
*Qiushi Han,Yuan Liao,Youhao Si,Liya Huang*

Main category: cs.SD

TL;DR: BM-TSE是一种用于神经引导目标说话人提取的新框架，通过脑电图信号实现个性化高保真提取，解决了EEG信号非平稳性和主体间变异性的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决神经引导目标说话人提取中EEG信号的非平稳性和主体间变异性问题，实现下一代助听器的鲁棒个性化性能。

Method: 采用时空EEG编码器和自适应频谱增益模块提取稳定特征，通过个性化调制机制学习统一脑图嵌入，在主体识别和听觉注意解码任务联合监督下动态优化音频分离过程。

Result: 在KUL和Cocktail Party数据集上的评估显示，BM-TSE达到了最先进的性能，显著优于现有方法。

Conclusion: BM-TSE框架通过结合静态用户特征和动态注意力状态，成功实现了个性化的高保真目标说话人提取，为下一代助听器技术提供了有效解决方案。

Abstract: Achieving robust and personalized performance in neuro-steered Target Speaker
Extraction (TSE) remains a significant challenge for next-generation hearing
aids. This is primarily due to two factors: the inherent non-stationarity of
EEG signals across sessions, and the high inter-subject variability that limits
the efficacy of generalized models. To address these issues, we propose
Brainprint-Modulated Target Speaker Extraction (BM-TSE), a novel framework for
personalized and high-fidelity extraction. BM-TSE first employs a
spatio-temporal EEG encoder with an Adaptive Spectral Gain (ASG) module to
extract stable features resilient to non-stationarity. The core of our
framework is a personalized modulation mechanism, where a unified brainmap
embedding is learned under the joint supervision of subject identification
(SID) and auditory attention decoding (AAD) tasks. This learned brainmap,
encoding both static user traits and dynamic attentional states, actively
refines the audio separation process, dynamically tailoring the output to each
user. Evaluations on the public KUL and Cocktail Party datasets demonstrate
that BM-TSE achieves state-of-the-art performance, significantly outperforming
existing methods. Our code is publicly accessible at:
https://github.com/rosshan-orz/BM-TSE.

</details>

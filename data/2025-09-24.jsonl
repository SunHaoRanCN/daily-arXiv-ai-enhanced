{"id": "2509.18310", "categories": ["eess.SP", "cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.18310", "abs": "https://arxiv.org/abs/2509.18310", "authors": ["Bahar Kor", "Bipin Gaikwad", "Abani Patra", "Eric L. Miller"], "title": "On Multi-entity, Multivariate Quickest Change Point Detection", "comment": null, "summary": "We propose a framework for online Change Point Detection (CPD) from\nmulti-entity, multivariate time series data, motivated by applications in crowd\nmonitoring where traditional sensing methods (e.g., video surveillance) may be\ninfeasible. Our approach addresses the challenge of detecting system-wide\nbehavioral shifts in complex, dynamic environments where the number and\nbehavior of individual entities may be uncertain or evolve. We introduce the\nconcept of Individual Deviation from Normality (IDfN), computed via a\nreconstruction-error-based autoencoder trained on normal behavior. We aggregate\nthese individual deviations using mean, variance, and Kernel Density Estimates\n(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or\nabrupt changes, we apply statistical deviation metrics and the Cumulative Sum\n(CUSUM) technique to these scores. Our unsupervised approach eliminates the\nneed for labeled data or feature extraction, enabling real-time operation on\nstreaming input. Evaluations on both synthetic datasets and crowd simulations,\nexplicitly designed for anomaly detection in group behaviors, demonstrate that\nour method accurately detects significant system-level changes, offering a\nscalable and privacy-preserving solution for monitoring complex multi-agent\nsystems. In addition to this methodological contribution, we introduce new,\nchallenging multi-entity multivariate time series datasets generated from crowd\nsimulations in Unity and coupled nonlinear oscillators. To the best of our\nknowledge, there is currently no publicly available dataset of this type\ndesigned explicitly to evaluate CPD in complex collective and interactive\nsystems, highlighting an essential gap that our work addresses."}
{"id": "2509.18381", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18381", "abs": "https://arxiv.org/abs/2509.18381", "authors": ["Nicholas L. K. Goradia", "Harpreet S. Dhillon", "R. Michael Buehrer"], "title": "Multi-Target Detection for Cognitive MIMO Radar Networks", "comment": "12 pages, 16 figures", "summary": "In this work, we develop centralized and decentralized signal fusion\ntechniques for constant false alarm rate (CFAR) multi-target detection with a\ncognitive radar network in unknown noise and clutter distributions. Further, we\nfirst develop a detection statistic for co-located monostatic MIMO radar in\nunknown noise and clutter distributions which is asymptotically CFAR as the\nnumber of received pulses over all antennas grows large, and we provide\nconditions under which this detection statistic is valid. We leverage\nreinforcement learning (RL) for improved multi-target detection performance,\nwhere the radar learns likely target locations in a search area. These results\nare then generalized to the setting of cognitive radar networks, where radars\ncollaborate to learn where targets are likely to appear in a search area. We\nshow a fundamental tradeoff between the spatial and temporal domain for CFAR\ndetection in unknown noise and clutter distributions; in other words, we show a\ntradeoff between the number of radar antennas and the number of temporal\nsamples. We show the benefits and tradeoffs with centralized and decentralized\ndetection with a network of cognitive radars."}
{"id": "2509.18426", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2509.18426", "abs": "https://arxiv.org/abs/2509.18426", "authors": ["Ziad Hatab", "Michael Ernst Gadringer", "Arash Arsanjani", "Wolfgang Boesch"], "title": "Automatic Model Extraction of the Match Standard in Symmetric--Reciprocal--Match Calibration", "comment": "https://github.com/ZiadHatab/srm-calibration", "summary": "This paper addresses the modeling of parasitics of the match standard in the\nsymmetric-reciprocal-match (SRM) calibration method of vector network analyzers\n(VNAs). In the general SRM procedure, the match standard is assumed to be fully\nknown. Here, we demonstrate that the match can be modeled with an arbitrary\nfrequency-dependent model using a non-linear global optimization procedure. To\nhighlight the validity of the suggested approach, numerical tests were\nconducted, demonstrating the ability to recover the match standard parasitic\nmodel down to software numerical precision. Additionally, we performed\nmicrostrip line measurements to compare the SRM calibration with match modeling\nto the multiline thru-reflect-line (TRL) calibration one, showing that\nautomatic model extraction can achieve accuracy similar to using a match\nstandard defined through multiline TRL calibration."}
{"id": "2509.18555", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18555", "abs": "https://arxiv.org/abs/2509.18555", "authors": ["Ping Wang", "Zulin Wang", "Yuanfang Ma", "Xiaosi Tian", "Yuanhan Ni"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "comment": "6 pages, 5 figures, 2025 IEEE International Conference on\n  Communications", "summary": "This paper introduces a secure affine frequency division multiplexing\n(SE-AFDM) for wireless communication systems to enhance communication security.\nBesides configuring the parameter c1 to obtain communication reliability under\ndoubly selective channels, we also utilize the time-varying parameter c2 to\nimprove the security of the communications system. The derived input-output\nrelation shows that the legitimate receiver can eliminate the nonlinear impact\nintroduced by the time-varying c2 without losing the bit error rate (BER)\nperformance. Moreover, it is theoretically proved that the eavesdropper cannot\nseparate the time-varying c2 and random information symbols, such that the BER\nperformance of the eavesdropper is severely deteriorated. Meanwhile, the\nanalysis of the effective signal-to-interference-plus-noise ratio (SINR) of the\neavesdropper illustrates that the SINR decreases as the value range of c2\nexpands. Numerical results verify that the proposed SE-AFDM waveform has\nsignificant security while maintaining good BER performance in high-mobility\nscenarios."}
{"id": "2509.18235", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18235", "abs": "https://arxiv.org/abs/2509.18235", "authors": ["Jialu Li", "Marvin Lavechin", "Xulin Fan", "Nancy L. McElwain", "Alejandrina Cristia", "Paola Garcia-Perera", "Mark Hasegawa-Johnson"], "title": "Automated Analysis of Naturalistic Recordings in Early Childhood: Applications, Challenges, and Opportunities", "comment": "Accepted to IEEE Signal Processing Magazine", "summary": "Naturalistic recordings capture audio in real-world environments where\nparticipants behave naturally without interference from researchers or\nexperimental protocols. Naturalistic long-form recordings extend this concept\nby capturing spontaneous and continuous interactions over extended periods,\noften spanning hours or even days, in participants' daily lives. Naturalistic\nrecordings have been extensively used to study children's behaviors, including\nhow they interact with others in their environment, in the fields of\npsychology, education, cognitive science, and clinical research. These\nrecordings provide an unobtrusive way to observe children in real-world\nsettings beyond controlled and constrained experimental environments.\nAdvancements in speech technology and machine learning have provided an initial\nstep for researchers to automatically and systematically analyze large-scale\nnaturalistic recordings of children. Despite the imperfect accuracy of machine\nlearning models, these tools still offer valuable opportunities to uncover\nimportant insights into children's cognitive and social development. Several\ncritical speech technologies involved include speaker diarization, vocalization\nclassification, word count estimate from adults, speaker verification, and\nlanguage diarization for code-switching. Most of these technologies have been\nprimarily developed for adults, and speech technologies applied to children\nspecifically are still vastly under-explored. To fill this gap, we discuss\ncurrent progress, challenges, and opportunities in advancing these technologies\nto analyze naturalistic recordings of children during early development (<3\nyears of age). We strive to inspire the signal processing community and foster\ninterdisciplinary collaborations to further develop this emerging technology\nand address its unique challenges and opportunities."}
{"id": "2509.18727", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18727", "abs": "https://arxiv.org/abs/2509.18727", "authors": ["Yasaman Ettefagh", "Sharief Saleh", "Musa Furkan Keskin", "Hui Chen", "Gonzalo Seco-Granados", "Henk Wymeersch"], "title": "Integrated Cellular and LEO-based Positioning and Synchronization under User Mobility", "comment": null, "summary": "This paper investigates the localization, synchronization, and speed\nestimation of a mobile user equipment (UE) leveraging integrated terrestrial\nand non-terrestrial networks (NTNs), in particular low Earth orbit (LEO)\nsatellites. We focus on a minimal setup in which the UE received signal from\nonly one base station (BS) and one LEO satellite. We derive a generic signal\nmodel accounting for mobility, clock and frequency offsets, based on which a\nhierarchy of simplified models are proposed and organized by computational\ncomplexity. Estimation algorithms are developed for each model to facilitate\nefficient and accurate parameter recovery. Rigorous simulations validate the\neffectiveness of the proposed models, demonstrating their suitability across\ndiverse scenarios. The findings highlight how the trade-off between complexity\nand performance can be optimized for varying deployment environments and\napplication requirements, offering valuable insights for 6G positioning and\nsynchronization systems under user mobility."}
{"id": "2509.18531", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18531", "abs": "https://arxiv.org/abs/2509.18531", "authors": ["Seungyoun Shin", "Dongha Ahn", "Jiwoo Kim", "Sungwook Jeon"], "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS", "comment": "submitted to ICASSP 2026", "summary": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative\nPolicy Optimization (GRPO). However, in the absence of a verifiable reward for\n\\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)\nlowers error rates yet collapses prosody into monotone, unnatural speech;\nadding speaker-similarity further destabilizes training and degrades CER. We\naddress this with an \\textit{iterative Direct Preference Optimization (DPO)}\nscheme that uses only a few hundred human-labeled preference pairs per round to\ndirectly optimize prosodic naturalness while regularizing to the current model.\nOn \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center\ninteractions capturing task-oriented dialogues, our method attains the highest\nhuman preference (ELO) with competitive CER, outperforming GRPO and strong\ncommercial baselines. These results suggest that when prosody cannot be\nrewarded automatically, \\textit{human preference optimization} offers a\npractical and data-efficient path to natural and robust TTS. The demo page is\navailable at \\href{https://tts.ch.dev}"}
{"id": "2509.18753", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18753", "abs": "https://arxiv.org/abs/2509.18753", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong", "Kaibin Huang"], "title": "Detection Capability Comparison Between Intensity Detection and Splitting Detection for Rydberg-Atomic Sensors", "comment": null, "summary": "Rydberg atomic quantum receivers have been seen as novel radio frequency\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, their unique physical\ncharacteristics enable two fundamental signal readout schemes: intensity-based\ndetection and splitting-based detection. The former measures the electric\nfields through laser intensity, while the latter utilizes Autler-Townes\nsplitting. In this work, we systematically categorize and model existing signal\nreadout methods, classifying them into these two paradigms. Then, we derive the\nmaximum likelihood estimation procedures and corresponding Cram\\'er-Rao lower\nbounds (CRLB) for each detection modality. Through the analysis of the CRLB, we\npropose strategy for both readout schemes to enhance sensitivity and minimize\nestimation variance: acquiring data in regions with maximal slope magnitudes.\nWhile this approach has been implemented in intensity-based detection (e.g.,\nsuperheterodyne schemes), its application to splitting-based detection remains\nunexplored. Implementation of non-uniform frequency scanning, with preferential\nsampling at regions exhibiting maximum peak slopes combined with our proposed\nmaximum likelihood splitting estimation method, achieves significantly reduced\nestimation variance compared to conventional polynomial fitting. The\ncomparative analysis reveals the optimal detection performance of the two\ndetection schemes. This work also contributes to enhancing the accuracy of\nmicrowave calibration. Numerical results reveal that both fundamental signal\nreadout methods achieve lower estimation variance based on our proposed maximum\nlikelihood estimation approach."}
{"id": "2509.18561", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18561", "abs": "https://arxiv.org/abs/2509.18561", "authors": ["Dayun Choi", "Jung-Woo Choi"], "title": "SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes", "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "Recent advances in target sound extraction (TSE) utilize directional clues\nderived from direction of arrival (DoA), which represent an inherent spatial\nproperty of sound available in any acoustic scene. However, previous DoA-based\nmethods rely on hand-crafted features or discrete encodings, which lose\nfine-grained spatial information and limit adaptability. We propose\nSoundCompass, an effective directional clue integration framework centered on a\nSpectral Pairwise INteraction (SPIN) module that captures cross-channel spatial\ncorrelations in the complex spectrogram domain to preserve full spatial\ninformation in multichannel signals. The input feature expressed in terms of\nspatial correlations is fused with a DoA clue represented as spherical\nharmonics (SH) encoding. The fusion is carried out across overlapping frequency\nsubbands, inheriting the benefits reported in the previous band-split\narchitectures. We also incorporate the iterative refinement strategy,\nchain-of-inference (CoI), in the TSE framework, which recursively fuses DoA\nwith sound event activation estimated from the previous inference stage.\nExperiments demonstrate that SoundCompass, combining SPIN, SH embedding, and\nCoI, robustly extracts target sources across diverse signal classes and spatial\nconfigurations."}
{"id": "2509.18102", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18102", "abs": "https://arxiv.org/abs/2509.18102", "authors": ["Wangjie Li", "Xingjia Xie", "Yishuang Li", "Wenhao Guan", "Kaidi Wang", "Pengyu Ren", "Lin Li", "Qingyang Hong"], "title": "XMUspeech Systems for the ASVspoof 5 Challenge", "comment": null, "summary": "In this paper, we present our submitted XMUspeech systems to the speech\ndeepfake detection track of the ASVspoof 5 Challenge. Compared to previous\nchallenges, the audio duration in ASVspoof 5 database has significantly\nincreased. And we observed that merely adjusting the input audio length can\nsubstantially improve system performance. To capture artifacts at multiple\nlevels, we explored the performance of AASIST, HM-Conformer, Hubert, and\nWav2vec2 with various input features and loss functions. Specifically, in order\nto obtain artifact-related information, we trained self-supervised models on\nthe dataset containing spoofing utterances as the feature extractors. And we\napplied an adaptive multi-scale feature fusion (AMFF) method to integrate\nfeatures from multiple Transformer layers with the hand-crafted feature to\nenhance the detection capability. In addition, we conducted extensive\nexperiments on one-class loss functions and provided optimized configurations\nto better align with the anti-spoofing task. Our fusion system achieved a\nminDCF of 0.4783 and an EER of 20.45% in the closed condition, and a minDCF of\n0.2245 and an EER of 9.36% in the open condition."}
{"id": "2509.18799", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18799", "abs": "https://arxiv.org/abs/2509.18799", "authors": ["Sijia Cheng", "Liang Liu", "Ove Edfors", "Juan Vidal Alegria"], "title": "Highly Parallel Singular Value Decomposition for Low-Latency MIMO Processing", "comment": "5 pages, 6 figures, accepted to SiPS2025", "summary": "Singular value decomposition (SVD) is widely used in wireless systems,\nincluding multiple-input multiple-output (MIMO) processing and dimension\nreduction in distributed MIMO (D-MIMO). However, the iterative nature of\ndecomposition methods results in increased execution time as system size grows,\nposing challenges for real-time and low-latency applications. To address this,\nwe analyze the latency of state-of-art SVD methods, and highlight the\nefficiency of a 4-step highly parallel method based on Gram matrix\ntridiagonalization. Furthermore, we develop a time complexity (processing\nlatency) analysis framework with hardware profiling, allowing scalable and\nrealistic evaluation without full implementation. The numerical results\ndemonstrate the superior time efficiency of the selected parallel method,\nparticularly in massive MIMO scenarios."}
{"id": "2509.18570", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18570", "abs": "https://arxiv.org/abs/2509.18570", "authors": ["Yuke Si", "Runyan Yang", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling", "comment": "5 pages; submitted to ICASSP 2026", "summary": "Recent advances in large language models have facilitated the development of\nunified speech language models (SLMs) capable of supporting multiple speech\ntasks within a shared architecture. However, tasks such as automatic speech\nrecognition (ASR) and speech emotion recognition (SER) rely on distinct types\nof information: ASR primarily depends on linguistic content, whereas SER\nrequires the integration of both linguistic and paralinguistic cues. Existing\nmultitask SLMs typically adopt naive parameter sharing or prompt-based\nconditioning without explicitly modeling the differences in information\ncomposition required by each task. Such designs risk task interference and\nperformance degradation, especially under limited data conditions. To address\nthese limitations, we propose HarmoniFuse, a component-selective and\nprompt-adaptive framework for multi-task speech language modeling. HarmoniFuse\nis designed to harmonize heterogeneous task demands by selecting and fusing\ntask-relevant components of speech representations. Specifically, it integrates\na gated speech encoder to extract task-specific acoustic features and a\nprompt-adaptive dynamic fusion module to aggregate transformer layers based on\ntask characteristics. In addition, a batch-interleaved training strategy\nenables leveraging separate ASR and SER datasets without requiring joint\nannotation. Experimental results demonstrate that HarmoniFuse improves both ASR\nand SER performance, offering a scalable and robust solution for multitask\nspeech understanding under realistic data constraints."}
{"id": "2509.18196", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18196", "abs": "https://arxiv.org/abs/2509.18196", "authors": ["Jialong Mai", "Jinxin Ji", "Xiaofen Xing", "Chen Yang", "Weidong Chen", "Jingyuan Xing", "Xiangmin Xu"], "title": "MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech", "comment": "Submitted to ICASSP 2026", "summary": "Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing\nlexical content, but largely fail to recognize nonverbal vocalizations (NVs)\nembedded in speech, such as sighs, laughs, and coughs. This capability is\nimportant for a comprehensive understanding of human communication, as NVs\nconvey crucial emotional and intentional cues. Progress in NV-aware ASR has\nbeen hindered by the lack of high-quality, well-annotated datasets. To address\nthis gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech\ndataset. Unlike most existing corpora that rely on model-based detection,\nMNV-17's performative nature ensures high-fidelity, clearly articulated NV\ninstances. To the best of our knowledge, MNV-17 provides the most extensive set\nof nonverbal vocalization categories, comprising 17 distinct and well-balanced\nclasses of common NVs. We benchmarked MNV-17 on four mainstream ASR\narchitectures, evaluating their joint performance on semantic transcription and\nNV classification. The dataset and the pretrained model checkpoints will be\nmade publicly available to facilitate future research in expressive ASR."}
{"id": "2509.18853", "categories": ["eess.SP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.18853", "abs": "https://arxiv.org/abs/2509.18853", "authors": ["Xiaolei Li", "Pengyu Wang", "Wenhua Song", "Yangjin Xu", "Wei Gao"], "title": "Normal mode parameters estimation by a VLA in single-shooting", "comment": null, "summary": "This paper proposes an orthogonality-constrained modal search (OCMS) method\nfor estimating modal wavenumbers and modal depth functions using a vertical\nlinear array (VLA). Under the assumption of a known sound speed profile, OCMS\nleverages the orthogonality of distinct modal depth functions to extract both\nthe modal depth functions and their corresponding wavenumbers, even when the\nVLA and a monochromatic sound source remain stationary.The performance of OCMS\nis evaluated through numerical simulations under varying signal-to-noise ratios\n(SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and\nsound speed profile (SSP) uncertainty. The results demonstrate that OCMS is\nrobust against noise, VLA aperture variations, and changes in the number of VLA\nelements, meanwhile, the algorithm maintains reliable performance when SSP\nuncertainty < 1 m/s and VLA tilt angle <5{\\deg}. Furthermore, the effectiveness\nof OCMS is validated using SwellEx96 experimental data. The relative error\nbetween the modal wavenumbers derived from experimental data and those computed\nvia Kraken is on the order of $10^{-4}$."}
{"id": "2509.18579", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18579", "abs": "https://arxiv.org/abs/2509.18579", "authors": ["Runyan Yang", "Yuke Si", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation", "comment": "5 pages; submitted to ICASSP 2026", "summary": "While large audio language models excel at tasks like ASR and emotion\nrecognition, they still struggle with complex reasoning due to the modality gap\nbetween audio and text as well as the lack of structured intermediate\nsupervision. To address this, we propose a unified knowledge distillation\nframework to transfer reasoning capabilities from a high-capacity textual\nteacher model to a student audio models while preserving its acoustic\ncompetence. Our method introduces two key dimensions: source-wise distillation,\nwhich leverages both textual and acoustic teachers to provide complementary\nmodality-specific supervision; and layer-wise distillation, which aligns\nteacher signals with appropriate student layers to improve transfer efficiency.\nThis dual-dimensional strategy enables fine-grained control over the\ndistillation process, effectively bridging the gap between symbolic reasoning\nand speech representations. Experimental results show significant improvements\nin audio reasoning performance, demonstrating the effectiveness of our\nframework as a reasoning transfer solution for audio modeling."}
{"id": "2509.18272", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18272", "abs": "https://arxiv.org/abs/2509.18272", "authors": ["Tornike Karchkhadze", "Kuan-Lin Chen", "Mojtaba", "Heydari", "Robert Henzel", "Alessandro Toso", "Mehrez Souden", "Joshua Atkins"], "title": "StereoFoley: Object-Aware Stereo Audio Generation from Video", "comment": null, "summary": "We present StereoFoley, a video-to-audio generation framework that produces\nsemantically aligned, temporally synchronized, and spatially accurate stereo\nsound at 48 kHz. While recent generative video-to-audio models achieve strong\nsemantic and temporal fidelity, they largely remain limited to mono or fail to\ndeliver object-aware stereo imaging, constrained by the lack of professionally\nmixed, spatially accurate video-to-audio datasets. First, we develop and train\na base model that generates stereo audio from video, achieving state-of-the-art\nin both semantic accuracy and synchronization. Next, to overcome dataset\nlimitations, we introduce a synthetic data generation pipeline that combines\nvideo analysis, object tracking, and audio synthesis with dynamic panning and\ndistance-based loudness controls, enabling spatially accurate object-aware\nsound. Finally, we fine-tune the base model on this synthetic dataset, yielding\nclear object-audio correspondence. Since no established metrics exist, we\nintroduce stereo object-awareness measures and validate it through a human\nlistening study, showing strong correlation with perception. This work\nestablishes the first end-to-end framework for stereo object-aware\nvideo-to-audio generation, addressing a critical gap and setting a new\nbenchmark in the field."}
{"id": "2509.18918", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18918", "abs": "https://arxiv.org/abs/2509.18918", "authors": ["Hamideh-Sadat Fazael-Ardekani", "Hadi Zayyani", "Hamid Soltanian-Zadeh"], "title": "Quaternion LMS for Graph Signal Recovery", "comment": null, "summary": "This letter generalizes the Graph Signal Recovery (GSR) problem in Graph\nSignal Processing (GSP) to the Quaternion domain. It extends the Quaternion\nLeast Mean Square (QLMS) in adaptive filtering literature, and Graph LMS (GLMS)\nalgorithm in GSP literature, to an algorithm called Quaternion GLMS (QGLMS).\nThe basic adaptation formula using Quaternion-based algebra is derived.\nMoreover, mean convergence analysis and mean-square convergence analysis are\nmathematically performed. Hence, a sufficient condition on the step-size\nparameter of QGLMS is suggested. Also, simulation results demonstrate the\neffectiveness of the proposed algorithm in graph signal reconstruction."}
{"id": "2509.18603", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18603", "abs": "https://arxiv.org/abs/2509.18603", "authors": ["Jiarui Hai", "Mounya Elhilali"], "title": "SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering", "comment": null, "summary": "Data synthesis and augmentation are essential for Sound Event Detection (SED)\ndue to the scarcity of temporally labeled data. While augmentation methods like\nSpecAugment and Mix-up can enhance model performance, they remain constrained\nby the diversity of existing samples. Recent generative models offer new\nopportunities, yet their direct application to SED is challenging due to the\nlack of precise temporal annotations and the risk of introducing noise through\nunreliable filtering. To address these challenges and enable generative-based\naugmentation for SED, we propose SynSonic, a data augmentation method tailored\nfor this task. SynSonic leverages text-to-audio diffusion models guided by an\nenergy-envelope ControlNet to generate temporally coherent sound events. A\njoint score filtering strategy with dual classifiers ensures sample quality,\nand we explore its practical integration into training pipelines. Experimental\nresults show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1\nand PSDS2), enhancing both temporal localization and sound class\ndiscrimination."}
{"id": "2509.18375", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18375", "abs": "https://arxiv.org/abs/2509.18375", "authors": ["Hailin Song", "Shelley Brady", "Tomás Ward", "Alan F. Smeaton"], "title": "A Dimensional Approach to Canine Bark Analysis for Assistance Dog Seizure Signaling", "comment": null, "summary": "Standard classification of canine vocalisations is severely limited for\nassistance dogs, where sample data is sparse and variable across dogs and where\ncapture of the full range of bark types is ethically constrained. We reframe\nthis problem as a continuous regression task within a two-dimensional\narousal-valence space. Central to our approach is an adjusted Siamese Network\ntrained not on binary similarity, but on the ordinal and numeric distance\nbetween input sample pairs. Trained on a public dataset, our model reduces\nTurn-around Percentage by up to 50% on the challenging valence dimension\ncompared to a regression baseline. Qualitative validation on a real-world\ndataset confirms the learned space is semantically meaningful, establishing a\nproof-of-concept for analysing canine barking under severe data limitations."}
{"id": "2509.19056", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19056", "abs": "https://arxiv.org/abs/2509.19056", "authors": ["Razieh Torkamani", "Arash Amini", "Hadi Zayyani", "Mehdi Korki"], "title": "Bayesian Convolutional Neural Networks for Prior Learning in Graph Signal Recovery", "comment": null, "summary": "Graph signal recovery (GSR) is a fundamental problem in graph signal\nprocessing, where the goal is to reconstruct a complete signal defined over a\ngraph from a subset of noisy or missing observations. A central challenge in\nGSR is that the underlying statistical model of the graph signal is often\nunknown or too complex to specify analytically. To address this, we propose a\nflexible, data-driven framework that learns the signal prior directly from\ntraining samples. We develop a Bayesian convolutional neural network (BCNN)\narchitecture that models the prior distribution of graph signals using\ngraph-aware filters based on Chebyshev polynomials. By interpreting the hidden\nlayers of the CNN as Gibbs distributions and employing Gaussian mixture model\n(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior\nis integrated into a variational Bayesian (VB) inference framework to estimate\nthe posterior distribution of the signal and noise precision. Extensive\nexperiments on synthetic and real-world graph datasets demonstrate that the\nproposed BCNN-GSR algorithm achieves accurate and robust recovery across a\nvariety of signal distributions. The method generalizes well to complex,\nnon-Gaussian signal models and remains computationally efficient, making it\nsuitable for practical large-scale graph recovery tasks."}
{"id": "2509.18606", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18606", "abs": "https://arxiv.org/abs/2509.18606", "authors": ["Jiarui Hai", "Helin Wang", "Weizhe Guo", "Mounya Elhilali"], "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection", "comment": null, "summary": "Despite recent progress in large-scale sound event detection (SED) systems\ncapable of handling hundreds of sound classes, existing multi-class\nclassification frameworks remain fundamentally limited. They cannot process\nfree-text sound queries, which enable more flexible and user-friendly\ninteraction, and they lack zero-shot capabilities and offer poor few-shot\nadaptability. Although text-query-based separation methods have been explored,\nthey primarily focus on source separation and are ill-suited for SED tasks that\nrequire precise temporal localization and efficient detection across large and\ndiverse sound vocabularies. In this paper, we propose FlexSED, an\nopen-vocabulary sound event detection system. FlexSED builds on a pretrained\naudio SSL model and the CLAP text encoder, introducing an encoder-decoder\ncomposition and an adaptive fusion strategy to enable effective continuous\ntraining from pretrained weights. To ensure robust supervision, it also employs\nlarge language models (LLMs) to assist in event query selection during\ntraining, addressing challenges related to missing labels. As a result, FlexSED\nachieves superior performance compared to vanilla SED models on\nAudioSet-Strong, while demonstrating strong zero-shot and few-shot\ncapabilities. We release the code and pretrained models to support future\nresearch and applications based on FlexSED."}
{"id": "2509.18412", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18412", "abs": "https://arxiv.org/abs/2509.18412", "authors": ["Mélisande Teng", "Julien Boussard", "David Rolnick", "Hugo Larochelle"], "title": "Identifying birdsong syllables without labelled data", "comment": null, "summary": "Identifying sequences of syllables within birdsongs is key to tackling a wide\narray of challenges, including bird individual identification and better\nunderstanding of animal communication and sensory-motor learning. Recently,\nmachine learning approaches have demonstrated great potential to alleviate the\nneed for experts to label long audio recordings by hand. However, they still\ntypically rely on the availability of labelled data for model training,\nrestricting applicability to a few species and datasets. In this work, we build\nthe first fully unsupervised algorithm to decompose birdsong recordings into\nsequences of syllables. We first detect syllable events, then cluster them to\nextract templates --syllable representations-- before performing matching\npursuit to decompose the recording as a sequence of syllables. We evaluate our\nautomatic annotations against human labels on a dataset of Bengalese finch\nsongs and find that our unsupervised method achieves high performance. We also\ndemonstrate that our approach can distinguish individual birds within a species\nthrough their unique vocal signatures, for both Bengalese finches and another\nspecies, the great tit."}
{"id": "2509.19092", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19092", "abs": "https://arxiv.org/abs/2509.19092", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems", "comment": "Submitted for possible publication", "summary": "Multimodal sensing reduces beam training overhead but is constrained by\nmachine learning complexity and dataset demands. To address this, we propose a\ndata-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided\nmmWave beam tracking, i.e., predicting the best current and future beams.\nSpecifically, we propose a knowledge inversion framework, where a generator\nsynthesizes LiDAR input data from random noise, guided by a loss function\ndefined on the features and outputs of a pre-trained teacher model. The student\nmodel is then trained using the synthetic data and knowledge distilled from the\nteacher. The generator loss combines three terms, called metadata loss,\nactivation loss, and entropy loss. For student training, in addition to the\nstandard Kullback-Leibler divergence loss, we also consider a mean-squared\nerror (MSE) loss between the teacher and student logits. Simulation results\nshow that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and\nTop-5 accuracies. Moreover, we observe that the metadata loss contributes\nsignificantly to the generator performance, and that the MSE loss for the\nstudent can effectively replace the standard KD loss while requiring fewer\nfine-tuned hyperparameters."}
{"id": "2509.18798", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18798", "abs": "https://arxiv.org/abs/2509.18798", "authors": ["Chang Liu", "Ya-Jun Hu", "Ying-Ying Gao", "Shi-Lei Zhang", "Zhen-Hua Ling"], "title": "Group Relative Policy Optimization for Text-to-Speech with Large Language Models", "comment": "5 pages,submitted to ICASSP2026", "summary": "This paper proposes a GRPO-based approach to enhance the performance of large\nlanguage model (LLM)-based text-to-speech (TTS) models by deriving rewards from\nan off-the-shelf automatic speech recognition (ASR) model. Compared to previous\nreinforcement learning methods for LLM-based TTS, our method requires no\ndedicated model for reward computation or training. Moreover, we design a\ncomposite reward function that combines character error rate (CER) with\nnegative log-likelihood (NLL) obtained from the ASR model, providing more\ninformative and accurate reward signals. We apply GRPO fine-tuning to\npre-trained LLM-based TTS models and evaluate their zero-shot TTS performance.\nExperimental results show that the proposed method substantially improves both\nthe intelligibility and naturalness of synthesized speech. Ablation studies and\nfurther analyses confirm the effectiveness of integrating the two reward\ncomponents."}
{"id": "2509.18424", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18424", "abs": "https://arxiv.org/abs/2509.18424", "authors": ["Rami Zewail"], "title": "Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection", "comment": null, "summary": "In an attempt to address the need for skilled clinicians in heart sound\ninterpretation, recent research efforts on automating cardiac auscultation have\nexplored deep learning approaches. The majority of these approaches have been\nbased on supervised learning that is always challenged in occasions where\ntraining data is limited. More recently, there has been a growing interest in\npotentials of pre-trained self-supervised audio foundation models for\nbiomedical end tasks. Despite exhibiting promising results, these foundational\nmodels are typically computationally intensive. Within the context of automatic\ncardiac auscultation, this study explores a lightweight alternative to these\ngeneral-purpose audio foundation models by introducing the Scattering\nTransformer, a novel, training-free transformer architecture for heart murmur\ndetection. The proposed method leverages standard wavelet scattering networks\nby introducing contextual dependencies in a transformer-like architecture\nwithout any backpropagation. We evaluate our approach on the public CirCor\nDigiScope dataset, directly comparing it against leading general-purpose\nfoundational models. The Scattering Transformer achieves a Weighted\nAccuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,\ndemonstrating performance highly competitive with contemporary state of the art\nmethods. This study establishes the Scattering Transformer as a viable and\npromising alternative in resource-constrained setups."}
{"id": "2509.19119", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19119", "abs": "https://arxiv.org/abs/2509.19119", "authors": ["Palatip Jopanya", "Diana P. M. Osorio"], "title": "Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC", "comment": "5 pages, 2 figures", "summary": "As definitions about new architectural aspects, use cases, and standards for\nintegrated sensing and communication (ISAC) continue to appear, cellular\nsystems based on massive multiple-input multiple-output (MIMO) antenna\ntechnology are also experiencing a parallel evolution through the integration\nof novel network components. This evolution should support emerging ISAC use\ncases and services. In particular, this paper explores a recent vision for\ncost-efficient cellular network densification through the deployment of swarms\nof repeaters. Leveraging their ability to retransmit signals instantaneously,\nwe investigate how these repeaters can enhance radar sensing capabilities for\ndrone detection in a swarm repeater-assisted MIMO ISAC system. Our results\ndemonstrate that, by optimizing the gains of repeaters given a sufficient\nmaximum amplification gain, increasing the number of repeaters can lead to\ngains in sensing performance."}
{"id": "2509.18806", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18806", "abs": "https://arxiv.org/abs/2509.18806", "authors": ["Lingling Dai", "Andong Li", "Tong Lei", "Meng Yu", "Xiaodong Li", "Chengshi Zheng"], "title": "Rethinking the joint estimation of magnitude and phase for time-frequency domain neural vocoders", "comment": "Submitted to ICASSP 2026", "summary": "Time-frequency (T-F) domain-based neural vocoders have shown promising\nresults in synthesizing high-fidelity audio. Nevertheless, it remains unclear\non the mechanism of effectively predicting magnitude and phase targets jointly.\nIn this paper, we start from two representative T-F domain vocoders, namely\nVocos and APNet2, which belong to the single-stream and dual-stream modes for\nmagnitude and phase estimation, respectively. When evaluating their performance\non a large-scale dataset, we accidentally observe severe performance collapse\nof APNet2. To stabilize its performance, in this paper, we introduce three\nsimple yet effective strategies, each targeting the topological space, the\nsource space, and the output space, respectively. Specifically, we modify the\narchitectural topology for better information exchange in the topological\nspace, introduce prior knowledge to facilitate the generation process in the\nsource space, and optimize the backpropagation process for parameter updates\nwith an improved output format in the output space. Experimental results\ndemonstrate that our proposed method effectively facilitates the joint\nestimation of magnitude and phase in APNet2, thus bridging the performance\ndisparities between the single-stream and dual-stream vocoders."}
{"id": "2509.18569", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18569", "abs": "https://arxiv.org/abs/2509.18569", "authors": ["Changfeng Gao", "Yabin Li", "Keyu An", "Zhifu Gao", "Zhihao Du", "Han Zhao", "Xiangang Li"], "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system", "comment": null, "summary": "In recent years, large language models (LLMs) have played an important role\nin automatic speech recognition (ASR) and text-to-speech (TTS) systems. While\nreinforcement learning (RL) has significantly enhanced LLM performance in\ntext-based tasks, its application to ASR and TTS remains underexplored due to\nthe complexity of training audio-based models. In this study, we propose a\nlightweight RL framework tailored for audio-based LLMs that can process audio\ninputs and generate audio outputs. Based on this framework, we evaluate the\neffectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR\ntask, we experiment with different rule-based reward functions within the Group\nRelative Policy Optimization (GRPO) framework and investigate the impact of RL\ndata construction. For the TTS task, we compare GRPO with Differentiable Reward\nOptimization (DiffRO) and further combine the two approaches to achieve\nimproved performance. Our experiments demonstrate that RL can significantly\nenhance the performance of both ASR and TTS systems, even with limited training\ndata and a small number of optimization steps."}
{"id": "2509.19130", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19130", "abs": "https://arxiv.org/abs/2509.19130", "authors": ["Abolfazl Zakeri", "Nhan Thanh Nguyen", "Ahmed Alkhateeb", "Markku Juntti"], "title": "Deep Reinforcement Learning for Dynamic Sensing and Communications", "comment": "Under review for possible publication", "summary": "Environmental sensing can significantly enhance mmWave communications by\nassisting beam training, yet its benefits must be balanced against the\nassociated sensing costs. To this end, we propose a unified machine learning\nframework that dynamically determines when to sense and leverages sensory data\nfor beam prediction. Specifically, we formulate a joint sensing and beamforming\nproblem that maximizes the av- erage signal-to-noise ratio under an average\nsensing budget. Lyapunov optimization is employed to enforce the sensing\nconstraint, while a deep Q-Network determines the sensing slots. A pretrained\ndeep neural network then maps the sens- ing data to optimal beams in the\ncodebook. Simulations based on the real-world DeepSense dataset demonstrate\nthat the pro- posed approach substantially reduces sensing overhead while\nmaintaining satisfactory communications performance."}
{"id": "2509.18823", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18823", "abs": "https://arxiv.org/abs/2509.18823", "authors": ["Arijit Biswas", "Lars Villemoes"], "title": "Towards Evaluating Generative Audio: Insights from Neural Audio Codec Embedding Distances", "comment": "Pre-review version submitted to ICASSP 2026", "summary": "Neural audio codecs (NACs) achieve low-bitrate compression by learning\ncompact audio representations, which can also serve as features for perceptual\nquality evaluation. We introduce DACe, an enhanced, higher-fidelity version of\nthe Descript Audio Codec (DAC), trained on diverse real and synthetic tonal\ndata with balanced sampling. We systematically compare Fr\\'echet Audio Distance\n(FAD) and Maximum Mean Discrepancy (MMD) on MUSHRA tests across speech, music,\nand mixed content. FAD consistently outperforms MMD, and embeddings from\nhigher-fidelity NACs (such as DACe) show stronger correlations with human\njudgments. While CLAP LAION Music (CLAP-M) and OpenL3 Mel128 (OpenL3-128M)\nembeddings achieve higher correlations, NAC embeddings provide a practical\nzero-shot approach to audio quality assessment, requiring only unencoded audio\nfor training. These results demonstrate the dual utility of NACs for\ncompression and perceptually informed audio evaluation."}
{"id": "2509.18620", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.5.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.18620", "abs": "https://arxiv.org/abs/2509.18620", "authors": ["Aditya Bhattacharjee", "Marco Pasini", "Emmanouil Benetos"], "title": "Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation", "comment": "Under review for International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), Barcelona, 2026", "summary": "The evaluation of audio fingerprinting at a realistic scale is limited by the\nscarcity of large public music databases. We present an audio-free approach\nthat synthesises latent fingerprints which approximate the distribution of real\nfingerprints. Our method trains a Rectified Flow model on embeddings extracted\nby pre-trained neural audio fingerprinting systems. The synthetic fingerprints\ngenerated using our system act as realistic distractors and enable the\nsimulation of retrieval performance at a large scale without requiring\nadditional audio. We assess the fidelity of synthetic fingerprints by comparing\nthe distributions to real data. We further benchmark the retrieval performances\nacross multiple state-of-the-art audio fingerprinting frameworks by augmenting\nreal reference databases with synthetic distractors, and show that the scaling\ntrends obtained with synthetic distractors closely track those obtained with\nreal distractors. Finally, we scale the synthetic distractor database to model\nretrieval performance for very large databases, providing a practical metric of\nsystem scalability that does not depend on access to audio corpora."}
{"id": "2509.19235", "categories": ["eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.19235", "abs": "https://arxiv.org/abs/2509.19235", "authors": ["Wamberto J. L. Queiroz", "Hugerles S. Silva", "Higo T. P. Silva", "Alexandros-Apostolos A. Boulogeorgos"], "title": "On the Performance of THz Wireless Systems over $α$-$\\mathcal{F}$ Channels with Beam Misalignment and Mobility", "comment": null, "summary": "This paper investigates the performance of terahertz~(THz) wireless systems\nover the $\\alpha$-$\\mathcal{F}$ fading channels with beam misalignment and\nmobility. New expressions are derived for the probability density, cumulative\ndistribution, and moment generating functions, as well as higher-order moments\nof the instantaneous signal-to-noise ratio. Building upon the aforementioned\nexpressions, we extract novel formulas for the outage probability, symbol error\nprobability, and average channel capacity. Asymptotic metrics are also deduced,\nwhich provide useful insights. Monte Carlo simulations results are presented to\nsupport the derived analytical framework."}
{"id": "2509.18885", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18885", "abs": "https://arxiv.org/abs/2509.18885", "authors": ["Mingchi Hou", "Ina Kodrasi"], "title": "Influence of Clean Speech Characteristics on Speech Enhancement Performance", "comment": null, "summary": "Speech enhancement (SE) performance is known to depend on noise\ncharacteristics and signal to noise ratio (SNR), yet intrinsic properties of\nthe clean speech signal itself remain an underexplored factor. In this work, we\nsystematically analyze how clean speech characteristics influence enhancement\ndifficulty across multiple state of the art SE models, languages, and noise\nconditions. We extract a set of pitch, formant, loudness, and spectral flux\nfeatures from clean speech and compute correlations with objective SE metrics,\nincluding frequency weighted segmental SNR and PESQ. Our results show that\nformant amplitudes are consistently predictive of SE performance, with higher\nand more stable formants leading to larger enhancement gains. We further\ndemonstrate that performance varies substantially even within a single\nspeaker's utterances, highlighting the importance of intraspeaker acoustic\nvariability. These findings provide new insights into SE challenges, suggesting\nthat intrinsic speech characteristics should be considered when designing\ndatasets, evaluation protocols, and enhancement models."}
{"id": "2509.18691", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18691", "abs": "https://arxiv.org/abs/2509.18691", "authors": ["Sarthak Yadav", "Sergios Theodoridis", "Zheng-Hua Tan"], "title": "An overview of neural architectures for self-supervised audio representation learning from masked spectrograms", "comment": null, "summary": "In recent years, self-supervised learning has amassed significant interest\nfor training deep neural representations without labeled data. One such\nself-supervised learning approach is masked spectrogram modeling, where the\nobjective is to learn semantically rich contextual representations by\npredicting removed or hidden portions of the input audio spectrogram. With the\nTransformer neural architecture at its core, masked spectrogram modeling has\nemerged as the prominent approach for learning general purpose audio\nrepresentations, a.k.a. audio foundation models. Meanwhile, addressing the\nissues of the Transformer architecture, in particular the underlying Scaled\nDot-product Attention operation, which scales quadratically with input sequence\nlength, has led to renewed interest in recurrent sequence modeling approaches.\nAmong them, Selective structured state space models (such as Mamba) and\nextended Long Short-Term Memory (xLSTM) are the two most promising approaches\nwhich have experienced widespread adoption. While the body of work on these two\ntopics continues to grow, there is currently a lack of an adequate overview\nencompassing the intersection of these topics. In this paper, we present a\ncomprehensive overview of the aforementioned research domains, covering masked\nspectrogram modeling and the previously mentioned neural sequence modeling\narchitectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and\nxLSTM based masked spectrogram models in a unified, reproducible framework on\nten diverse downstream audio classification tasks, which will help interested\nreaders to make informed decisions regarding suitability of the evaluated\napproaches to adjacent applications."}
{"id": "2509.19272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19272", "abs": "https://arxiv.org/abs/2509.19272", "authors": ["Sathwik Chadaga"], "title": "Faster-Than-Nyquist Signalling - Theoretical Limits on Capacity and Techniques to Approach Capacity", "comment": null, "summary": "Faster-Than-Nyquist (FTN) Signalling is a non-orthogonal transmission scheme\nthat violates the Nyquist zero-ISI criterion providing higher throughput and\nbetter spectral efficiency than a Nyquist transmission scheme. In this thesis,\nthe inter symbol interference (ISI) introduced by FTN signalling is studied,\nand conditions on pulse shapes and $\\tau$ (time acceleration factor) are\nderived so that the ISI can be avoided completely. Further, these conditions\nare reinforced by investigating the theoretical limits on the capacities of FTN\nsystems. Finally, the use of power allocation and adaptive loading techniques\nare explored in reducing the effect of ISI and increasing the throughput of\northogonal frequency division multiplexing (OFDM) FTN systems. The\nimplementation of these techniques and simulation results are also\ndemonstrated."}
{"id": "2509.18890", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18890", "abs": "https://arxiv.org/abs/2509.18890", "authors": ["Mingchi Hou", "Ante Jukic", "Ina Kodrasi"], "title": "Generalizability of Predictive and Generative Speech Enhancement Models to Pathological Speakers", "comment": null, "summary": "State of the art speech enhancement (SE) models achieve strong performance on\nneurotypical speech, but their effectiveness is substantially reduced for\npathological speech. In this paper, we investigate strategies to address this\ngap for both predictive and generative SE models, including i) training models\nfrom scratch using pathological data, ii) finetuning models pretrained on\nneurotypical speech with additional data from pathological speakers, and iii)\nspeaker specific personalization using only data from the individual\npathological test speaker. Our results show that, despite the limited size of\npathological speech datasets, SE models can be successfully trained or\nfinetuned on such data. Finetuning models with data from several pathological\nspeakers yields the largest performance improvements, while speaker specific\npersonalization is less effective, likely due to the small amount of data\navailable per speaker. These findings highlight the challenges and potential\nstrategies for improving SE performance for pathological speakers."}
{"id": "2509.18700", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18700", "abs": "https://arxiv.org/abs/2509.18700", "authors": ["Chih-Cheng Chang", "Bo-Yu Chen", "Lu-Rong Chen", "Li Su"], "title": "Enhancing Automatic Chord Recognition through LLM Chain-of-Thought Reasoning", "comment": null, "summary": "Music Information Retrieval (MIR) encompasses a broad range of computational\ntechniques for analyzing and understanding musical content, with recent deep\nlearning advances driving substantial improvements. Building upon these\nadvances, this paper explores how large language models (LLMs) can serve as an\nintegrative bridge to connect and integrate information from multiple MIR\ntools, with a focus on enhancing automatic chord recognition performance. We\npresent a novel approach that positions text-based LLMs as intelligent\ncoordinators that process and integrate outputs from diverse state-of-the-art\nMIR tools-including music source separation, key detection, chord recognition,\nand beat tracking. Our method converts audio-derived musical information into\ntextual representations, enabling LLMs to perform reasoning and correction\nspecifically for chord recognition tasks. We design a 5-stage chain-of-thought\nframework that allows GPT-4o to systematically analyze, compare, and refine\nchord recognition results by leveraging music-theoretical knowledge to\nintegrate information across different MIR components. Experimental evaluation\non three datasets demonstrates consistent improvements across multiple\nevaluation metrics, with overall accuracy gains of 1-2.77% on the MIREX metric.\nOur findings demonstrate that LLMs can effectively function as integrative\nbridges in MIR pipelines, opening new directions for multi-tool coordination in\nmusic information retrieval tasks."}
{"id": "2509.19275", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19275", "abs": "https://arxiv.org/abs/2509.19275", "authors": ["Junzhe Song", "Ruisi He", "Mi Yang", "Zhengyu Zhang", "Xinwen Chen", "Xiaoying Zhang", "Bo Ai"], "title": "A Novel Site-Specific Inference Model for Urban Canyon Channels: From Measurements to Modeling", "comment": null, "summary": "With the rapid development of intelligent transportation and smart city\napplications, urban canyon has become a critical scenario for the design and\nevaluation of wireless communication systems. Due to its unique environmental\nlayout, the channel characteristics in urban canyon are strongly a street\ngeometry and building distribution, thereby exhibiting significant\nsite-specific channel condition. However, this feature has not been well\ncaptured in existing channel models. In this paper, we propose a site-specific\nchannel inference model based on environmental geometry, the model is\nparameterized using sub-6GHz channel measurements. Multipath components (MPCs)\nare extracted and clustered according to geometric propagation, which are\nexplicitly derived from the influence of canyon width, thereby establishing an\ninterpretable mapping between the physical environment and statistical\ncharacteristics of MPCs. A step-by-step implementation scheme is presented.\nSubsequently, the proposed site-specific channel inference model is validated\nby comparing second-order statistics of channels, derived from the model and\nmeasurements. The results show that the proposed model achieves high accuracy\nand robustness in different urban canyon scenarios."}
{"id": "2509.18928", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18928", "abs": "https://arxiv.org/abs/2509.18928", "authors": ["Zhijun Liu", "Dongya Jia", "Xiaoqiang Wang", "Chenpeng Du", "Shuai Wang", "Zhuo Chen", "Haizhou Li"], "title": "Direct Preference Optimization for Speech Autoregressive Diffusion Models", "comment": null, "summary": "Autoregressive diffusion models (ARDMs) have recently been applied to speech\ngeneration, achieving state-of-the-art (SOTA) performance in zero-shot\ntext-to-speech. By autoregressively generating continuous speech tokens with\nnext-token diffusion, these models offer a promising alternative to next-token\nprediction, avoiding the technical complexities associated with discrete speech\ntokenization. As a relatively new paradigm, research on reinforcement learning\n(RL)-based fine-tuning of speech ARDMs remains limited. In this paper, we\npropose Autoregressive Diffusion-Direct Preference Optimization (ARDM-DPO) to\nadvance this research. By fine-tuning the recently proposed zero-shot\ntext-to-speech model DiTAR with DPO, we achieve significant improvements in\nterms of speech expressiveness and robustness for long texts."}
{"id": "2509.18729", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18729", "abs": "https://arxiv.org/abs/2509.18729", "authors": ["Haoqin Sun", "Chenyang Lyu", "Xiangyu Kong", "Shiwan Zhao", "Jiaming Zhou", "Hui Wang", "Aobo Kong", "Jinghua Zhao", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Yong Qin"], "title": "MECap-R1: Emotion-aware Policy with Reinforcement Learning for Multimodal Emotion Captioning", "comment": null, "summary": "Speech Emotion Captioning (SEC) has emerged as a notable research direction.\nThe inherent complexity of emotional content in human speech makes it\nchallenging for traditional discrete classification methods to provide an\nadequate representation. Consequently, utilizing natural language to describe\nspeech emotions presents a novel avenue for more effectively capturing and\nexpressing affect. In this paper, we propose MECap-R1, a pioneering\nemotion-aware policy with reinforcement learning for multimodal emotion\ncaptioning. By employing Group Relative Policy Optimization with emotion-aware\nreward (Emo-GRPO), the framework precisely captures the emotion and semantic\nfeatures, thereby addressing the shortcomings of rigid rules in handling the\ndynamic and flexible nature of captions. Experimental results on the\nEmotionTalk dataset demonstrate that MECap-R1 performs well in generating\nemotion descriptions and achieves substantial gains in both accuracy and\ndiversity."}
{"id": "2509.19281", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19281", "abs": "https://arxiv.org/abs/2509.19281", "authors": ["Xiyang Lan", "Xin Li"], "title": "STFT-AECNN: An Attention-Enhanced CNN for Efficient Φ-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing", "comment": null, "summary": "Phase-sensitive optical time-domain reflectometry ({\\Phi}-OTDR) has emerged\nas a promising sensing technology in Internet of Things (IoT) infrastructures,\nenabling large-scale distributed acoustic sensing (DAS) for smart city\nsurveillance, industrial pipeline monitoring, and critical infrastructure\nprotection. However, accurately recognizing events from massive {\\Phi}-OTDR\ndata streams remains challenging, as existing deep learning methods either\ndisrupt the inherent spatiotemporal structure of signals or incur prohibitive\ncomputational costs, limiting their applicability in resource-constrained IoT\nscenarios. To overcome these challenges, we propose a novel STFT-based\nAttention-Enhanced Convolutional Neural Network (STFT-AECNN), which represents\nmulti-channel time-series data as stacked spectrograms to fully exploit their\nspatiotemporal characteristics while enabling efficient 2D CNN processing. A\nSpatial Efficient Attention Module (SEAM) is further introduced to adaptively\nemphasize the most informative channels, and a joint Cross-Entropy and Triplet\nloss is adopted to enhance the discriminability of the learned feature space.\nExtensive experiments on the public BJTU {\\Phi}-OTDR dataset demonstrate that\nSTFT-AECNN achieves a peak accuracy of 99.94% while maintaining high\ncomputational efficiency. These results highlight its potential for real-time,\nscalable, and robust event recognition in IoT-enabled DAS systems, paving the\nway for reliable and intelligent IoT sensing applications."}
{"id": "2509.19001", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19001", "abs": "https://arxiv.org/abs/2509.19001", "authors": ["Sihang Nie", "Xiaofen Xing", "Jingyuan Xing", "Baiji Liu", "Xiangmin Xu"], "title": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS", "comment": "5 pages, 2 figures, submitted to ICASSP2026", "summary": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already\nreached a high degree of naturalness. However, the precision control of TTS\ninference is still challenging. Although instruction-based Text-to-Speech\n(Instruct-TTS) models are proposed, these models still lack fine-grained\ncontrol due to the modality gap between single-level text instructions and\nmultilevel speech tokens. To address this limitation, we propose HD-PPT, a\nframework that transforms speech synthesis into a structured, hierarchical\ntask. To enable fine-grained control, we introduce a novel speech codec to\nextract distinct prompt-preference and content-preference tokens from the\ncomplex speech tokens, supervised by automatic speech recognition (ASR) and\ncross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality\ngap of these tokens, we propose a hierarchical decoding strategy, where the LLM\ngenerates tokens in a structured order: first semantic, then fine-grained\nstyle, and finally complete acoustic representation. Extensive experiments\ndemonstrate that this hierarchical paradigm significantly improves instruction\nadherence and achieves state-of-the-art naturalness, validating our approach\nfor precise and controllable speech synthesis. Audio samples are available at\nhttps://xxh333.github.io/."}
{"id": "2509.18816", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18816", "abs": "https://arxiv.org/abs/2509.18816", "authors": ["Junyu Wang", "Ziyang Ma", "Zhengding Luo", "Tianrui Wang", "Meng Ge", "Xiaobao Wang", "Longbiao Wang"], "title": "Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models", "comment": "Submitted to ICASSP 2026", "summary": "Large Audio-Language Models (LALMs) often suffer from audio-textual attention\nimbalance, prioritizing text over acoustic information, particularly in the\nmulti-modal fusion layers of the Transformer architecture. This bias hinders\ntheir ability to fully utilize acoustic cues, causing suboptimal performance on\naudio reasoning tasks. To mitigate this, we propose \\textbf{MATA}, a novel\ntraining-free method that dynamically pushes LALMs to pay \\textbf{M}ore\n\\textbf{A}ttention \\textbf{T}o \\textbf{A}udio tokens within the self-attention\nmechanism. Specifically, MATA intervenes post raw attention scoring, targeting\nonly the last token in intermediate layers without introducing additional\nparameters or computational overhead. Experiments on the MMAU and MMAR\nbenchmarks confirm MATA's effectiveness, with consistent performance gains.\nNotably, on MMAR, MATA enables an open-source model to surpass the proprietary\nGemini 2.0 Flash for the first time. Our work provides an efficient solution to\nmitigate attention bias and opens a new research direction for enhancing the\naudio-processing capabilities of multi-modal models."}
{"id": "2509.19025", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.19025", "abs": "https://arxiv.org/abs/2509.19025", "authors": ["Rui-Chen Zheng", "Yang Ai", "Hui-Peng Du", "Zhen-Hua Ling"], "title": "Enhancing Noise Robustness for Neural Speech Codecs through Resource-Efficient Progressive Quantization Perturbation Simulation", "comment": null, "summary": "Noise robustness remains a critical challenge for deploying neural speech\ncodecs in real-world acoustic scenarios where background noise is often\ninevitable. A key observation we make is that even slight input noise\nperturbations can cause unintended shifts in quantized codewords, thereby\ndegrading the quality of reconstructed speech. Motivated by this finding, we\npropose a novel and resource-efficient training strategy to enhance the noise\nrobustness of speech codecs by simulating such perturbations directly at the\nquantization level. Our approach introduces two core mechanisms: (1) a\ndistance-weighted probabilistic top-K sampling strategy that replaces the\nconventional deterministic nearest-neighbor selection in residual vector\nquantization (RVQ); and (2) a progressive training scheme that introduces\nperturbations from the last to the first quantizer in a controlled manner.\nCrucially, our method is trained exclusively on clean speech, eliminating the\nneed for any paired noisy-clean data. Experiments on two advanced neural speech\ncodecs, Encodec and WavTokenizer, demonstrate that the proposed strategy\nsubstantially improves robustness under noisy conditions-for example, boosting\nUTMOS from 3.475 to 3.586 at 15 dB SNR on Encodec-while also enhancing coding\nquality for clean speech."}
{"id": "2509.19231", "categories": ["cs.SD", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19231", "abs": "https://arxiv.org/abs/2509.19231", "authors": ["Karen Rosero", "Eunjung Yeo", "David R. Mortensen", "Cortney Van't Slot", "Rami R. Hallac", "Carlos Busso"], "title": "Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation", "comment": null, "summary": "We present ChiReSSD, a speech reconstruction framework that preserves\nchildren speaker's identity while suppressing mispronunciations. Unlike prior\napproaches trained on healthy adult speech, ChiReSSD adapts to the voices of\nchildren with speech sound disorders (SSD), with particular emphasis on pitch\nand prosody. We evaluate our method on the STAR dataset and report substantial\nimprovements in lexical accuracy and speaker identity preservation.\nFurthermore, we automatically predict the phonetic content in the original and\nreconstructed pairs, where the proportion of corrected consonants is comparable\nto the percentage of correct consonants (PCC), a clinical speech assessment\nmetric. Our experiments show Pearson correlation of 0.63 between automatic and\nhuman expert annotations, highlighting the potential to reduce the manual\ntranscription burden. In addition, experiments on the TORGO dataset demonstrate\neffective generalization for reconstructing adult dysarthric speech. Our\nresults indicate that disentangled, style-based TTS reconstruction can provide\nidentity-preserving speech across diverse clinical populations."}
{"id": "2509.19091", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19091", "abs": "https://arxiv.org/abs/2509.19091", "authors": ["Hyeongju Kim", "Yechan Yu", "June Young Yi", "Juheon Lee"], "title": "Training Flow Matching Models with Reliable Labels via Self-Purification", "comment": "5 pages, 3 figures, preprint", "summary": "Training datasets are inherently imperfect, often containing mislabeled\nsamples due to human annotation errors, limitations of tagging models, and\nother sources of noise. Such label contamination can significantly degrade the\nperformance of a trained model. In this work, we introduce Self-Purifying Flow\nMatching (SPFM), a principled approach to filtering unreliable data within the\nflow-matching framework. SPFM identifies suspicious data using the model itself\nduring the training process, bypassing the need for pretrained models or\nadditional modules. Our experiments demonstrate that models trained with SPFM\ngenerate samples that accurately adhere to the specified conditioning, even\nwhen trained on noisy labels. Furthermore, we validate the robustness of SPFM\non the TITW dataset, which consists of in-the-wild speech data, achieving\nperformance that surpasses existing baselines."}
{"id": "2509.18235", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18235", "abs": "https://arxiv.org/abs/2509.18235", "authors": ["Jialu Li", "Marvin Lavechin", "Xulin Fan", "Nancy L. McElwain", "Alejandrina Cristia", "Paola Garcia-Perera", "Mark Hasegawa-Johnson"], "title": "Automated Analysis of Naturalistic Recordings in Early Childhood: Applications, Challenges, and Opportunities", "comment": "Accepted to IEEE Signal Processing Magazine", "summary": "Naturalistic recordings capture audio in real-world environments where\nparticipants behave naturally without interference from researchers or\nexperimental protocols. Naturalistic long-form recordings extend this concept\nby capturing spontaneous and continuous interactions over extended periods,\noften spanning hours or even days, in participants' daily lives. Naturalistic\nrecordings have been extensively used to study children's behaviors, including\nhow they interact with others in their environment, in the fields of\npsychology, education, cognitive science, and clinical research. These\nrecordings provide an unobtrusive way to observe children in real-world\nsettings beyond controlled and constrained experimental environments.\nAdvancements in speech technology and machine learning have provided an initial\nstep for researchers to automatically and systematically analyze large-scale\nnaturalistic recordings of children. Despite the imperfect accuracy of machine\nlearning models, these tools still offer valuable opportunities to uncover\nimportant insights into children's cognitive and social development. Several\ncritical speech technologies involved include speaker diarization, vocalization\nclassification, word count estimate from adults, speaker verification, and\nlanguage diarization for code-switching. Most of these technologies have been\nprimarily developed for adults, and speech technologies applied to children\nspecifically are still vastly under-explored. To fill this gap, we discuss\ncurrent progress, challenges, and opportunities in advancing these technologies\nto analyze naturalistic recordings of children during early development (<3\nyears of age). We strive to inspire the signal processing community and foster\ninterdisciplinary collaborations to further develop this emerging technology\nand address its unique challenges and opportunities."}
{"id": "2509.19097", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.19097", "abs": "https://arxiv.org/abs/2509.19097", "authors": ["Stephen Roddy"], "title": "On-device Internet of Sounds Sonification with Wavetable Synthesis Techniques for Soil Moisture Monitoring in Water Scarcity Contexts", "comment": "8 pages, 5 figures, 6 equations, 5th International Symposium on the\n  Internet of Sounds", "summary": "Sonification, the mapping of data to sound to communicate information about\nthe original data source, is becoming a viable strategy for the sonic\nrepresentation and communication of information derived from the complex flows\nof data exchanged across Internet of Sounds (IoS) networks. This paper presents\nan IoS sonification implementation for monitoring soil moisture levels within\nthe broader context of the globally increasing water scarcity. While previous\nwork has focused on sonifications operating on the applications and services\nlevel of the IoS network infrastructure, this paper explores device-level\nsonification using wavetable synthesis techniques to map sensor data to\nacoustic parameters. An approach to on-device wavetable sonification is\nformalized, and a prototype implementation is presented and explored before the\napproach is contextualised with regard to the soil moisture monitoring tasks."}
{"id": "2509.18531", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18531", "abs": "https://arxiv.org/abs/2509.18531", "authors": ["Seungyoun Shin", "Dongha Ahn", "Jiwoo Kim", "Sungwook Jeon"], "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS", "comment": "submitted to ICASSP 2026", "summary": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative\nPolicy Optimization (GRPO). However, in the absence of a verifiable reward for\n\\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)\nlowers error rates yet collapses prosody into monotone, unnatural speech;\nadding speaker-similarity further destabilizes training and degrades CER. We\naddress this with an \\textit{iterative Direct Preference Optimization (DPO)}\nscheme that uses only a few hundred human-labeled preference pairs per round to\ndirectly optimize prosodic naturalness while regularizing to the current model.\nOn \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center\ninteractions capturing task-oriented dialogues, our method attains the highest\nhuman preference (ELO) with competitive CER, outperforming GRPO and strong\ncommercial baselines. These results suggest that when prosody cannot be\nrewarded automatically, \\textit{human preference optimization} offers a\npractical and data-efficient path to natural and robust TTS. The demo page is\navailable at \\href{https://tts.ch.dev}"}
{"id": "2509.19186", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19186", "abs": "https://arxiv.org/abs/2509.19186", "authors": ["Hyeongju Kim", "Junhyeok Lee", "Jacob Morton", "Juheon Lee", "Jinhyeok Yang"], "title": "Improving Test-Time Performance of RVQ-based Neural Codecs", "comment": "5 pages, preprint", "summary": "The residual vector quantization (RVQ) technique plays a central role in\nrecent advances in neural audio codecs. These models effectively synthesize\nhigh-fidelity audio from a limited number of codes due to the hierarchical\nstructure among quantization levels. In this paper, we propose an encoding\nalgorithm to further enhance the synthesis quality of RVQ-based neural codecs\nat test-time. Firstly, we point out the suboptimal nature of quantized vectors\ngenerated by conventional methods. We demonstrate that quantization error can\nbe mitigated by selecting a different set of codes. Subsequently, we present\nour encoding algorithm, designed to identify a set of discrete codes that\nachieve a lower quantization error. We then apply the proposed method to\npre-trained models and evaluate its efficacy using diverse metrics. Our\nexperimental findings validate that our method not only reduces quantization\nerrors, but also improves synthesis quality."}
{"id": "2509.18561", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18561", "abs": "https://arxiv.org/abs/2509.18561", "authors": ["Dayun Choi", "Jung-Woo Choi"], "title": "SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes", "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "Recent advances in target sound extraction (TSE) utilize directional clues\nderived from direction of arrival (DoA), which represent an inherent spatial\nproperty of sound available in any acoustic scene. However, previous DoA-based\nmethods rely on hand-crafted features or discrete encodings, which lose\nfine-grained spatial information and limit adaptability. We propose\nSoundCompass, an effective directional clue integration framework centered on a\nSpectral Pairwise INteraction (SPIN) module that captures cross-channel spatial\ncorrelations in the complex spectrogram domain to preserve full spatial\ninformation in multichannel signals. The input feature expressed in terms of\nspatial correlations is fused with a DoA clue represented as spherical\nharmonics (SH) encoding. The fusion is carried out across overlapping frequency\nsubbands, inheriting the benefits reported in the previous band-split\narchitectures. We also incorporate the iterative refinement strategy,\nchain-of-inference (CoI), in the TSE framework, which recursively fuses DoA\nwith sound event activation estimated from the previous inference stage.\nExperiments demonstrate that SoundCompass, combining SPIN, SH embedding, and\nCoI, robustly extracts target sources across diverse signal classes and spatial\nconfigurations."}
{"id": "2509.19219", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.19219", "abs": "https://arxiv.org/abs/2509.19219", "authors": ["Laura Lechler", "Ivana Balic"], "title": "MUSHRA-1S: A scalable and sensitive test approach for evaluating top-tier speech processing systems", "comment": "Submitted to ICASSP 2026; under review", "summary": "Evaluating state-of-the-art speech systems necessitates scalable and\nsensitive evaluation methods to detect subtle but unacceptable artifacts.\nStandard MUSHRA is sensitive but lacks scalability, while ACR scales well but\nloses sensitivity and saturates at a high quality. To address this, we\nintroduce MUSHRA 1S, a single-stimulus variant that rates one system at a time\nagainst a fixed anchor and reference. Across our experiments, MUSHRA 1S matches\nstandard MUSHRA more closely than ACR, including in the high-quality regime,\nwhere ACR saturates. MUSHRA 1S also effectively identifies specific deviations\nand reduces range-equalizing biases by fixing context. Overall, MUSHRA 1S\ncombines MUSHRA level sensitivity with ACR like scalability, making it a robust\nand scalable solution for benchmarking top-tier speech processing systems."}
{"id": "2509.18570", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18570", "abs": "https://arxiv.org/abs/2509.18570", "authors": ["Yuke Si", "Runyan Yang", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling", "comment": "5 pages; submitted to ICASSP 2026", "summary": "Recent advances in large language models have facilitated the development of\nunified speech language models (SLMs) capable of supporting multiple speech\ntasks within a shared architecture. However, tasks such as automatic speech\nrecognition (ASR) and speech emotion recognition (SER) rely on distinct types\nof information: ASR primarily depends on linguistic content, whereas SER\nrequires the integration of both linguistic and paralinguistic cues. Existing\nmultitask SLMs typically adopt naive parameter sharing or prompt-based\nconditioning without explicitly modeling the differences in information\ncomposition required by each task. Such designs risk task interference and\nperformance degradation, especially under limited data conditions. To address\nthese limitations, we propose HarmoniFuse, a component-selective and\nprompt-adaptive framework for multi-task speech language modeling. HarmoniFuse\nis designed to harmonize heterogeneous task demands by selecting and fusing\ntask-relevant components of speech representations. Specifically, it integrates\na gated speech encoder to extract task-specific acoustic features and a\nprompt-adaptive dynamic fusion module to aggregate transformer layers based on\ntask characteristics. In addition, a batch-interleaved training strategy\nenables leveraging separate ASR and SER datasets without requiring joint\nannotation. Experimental results demonstrate that HarmoniFuse improves both ASR\nand SER performance, offering a scalable and robust solution for multitask\nspeech understanding under realistic data constraints."}
{"id": "2509.19295", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19295", "abs": "https://arxiv.org/abs/2509.19295", "authors": ["Yonghyun Kim", "Chaeyeon Han", "Akash Sarode", "Noah Posner", "Subhrajit Guhathakurta", "Alexander Lerch"], "title": "Audio-Based Pedestrian Detection in the Presence of Vehicular Noise", "comment": "Accepted to the 10th Workshop on Detection and Classification of\n  Acoustic Scenes and Events (DCASE), 2025", "summary": "Audio-based pedestrian detection is a challenging task and has, thus far,\nonly been explored in noise-limited environments. We present a new dataset,\nresults, and a detailed analysis of the state-of-the-art in audio-based\npedestrian detection in the presence of vehicular noise. In our study, we\nconduct three analyses: (i) cross-dataset evaluation between noisy and\nnoise-limited environments, (ii) an assessment of the impact of noisy data on\nmodel performance, highlighting the influence of acoustic context, and (iii) an\nevaluation of the model's predictive robustness on out-of-domain sounds. The\nnew dataset is a comprehensive 1321-hour roadside dataset. It incorporates\ntraffic-rich soundscapes. Each recording includes 16kHz audio synchronized with\nframe-level pedestrian annotations and 1fps video thumbnails."}
{"id": "2509.18579", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18579", "abs": "https://arxiv.org/abs/2509.18579", "authors": ["Runyan Yang", "Yuke Si", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation", "comment": "5 pages; submitted to ICASSP 2026", "summary": "While large audio language models excel at tasks like ASR and emotion\nrecognition, they still struggle with complex reasoning due to the modality gap\nbetween audio and text as well as the lack of structured intermediate\nsupervision. To address this, we propose a unified knowledge distillation\nframework to transfer reasoning capabilities from a high-capacity textual\nteacher model to a student audio models while preserving its acoustic\ncompetence. Our method introduces two key dimensions: source-wise distillation,\nwhich leverages both textual and acoustic teachers to provide complementary\nmodality-specific supervision; and layer-wise distillation, which aligns\nteacher signals with appropriate student layers to improve transfer efficiency.\nThis dual-dimensional strategy enables fine-grained control over the\ndistillation process, effectively bridging the gap between symbolic reasoning\nand speech representations. Experimental results show significant improvements\nin audio reasoning performance, demonstrating the effectiveness of our\nframework as a reasoning transfer solution for audio modeling."}
{"id": "2509.18102", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18102", "abs": "https://arxiv.org/abs/2509.18102", "authors": ["Wangjie Li", "Xingjia Xie", "Yishuang Li", "Wenhao Guan", "Kaidi Wang", "Pengyu Ren", "Lin Li", "Qingyang Hong"], "title": "XMUspeech Systems for the ASVspoof 5 Challenge", "comment": null, "summary": "In this paper, we present our submitted XMUspeech systems to the speech\ndeepfake detection track of the ASVspoof 5 Challenge. Compared to previous\nchallenges, the audio duration in ASVspoof 5 database has significantly\nincreased. And we observed that merely adjusting the input audio length can\nsubstantially improve system performance. To capture artifacts at multiple\nlevels, we explored the performance of AASIST, HM-Conformer, Hubert, and\nWav2vec2 with various input features and loss functions. Specifically, in order\nto obtain artifact-related information, we trained self-supervised models on\nthe dataset containing spoofing utterances as the feature extractors. And we\napplied an adaptive multi-scale feature fusion (AMFF) method to integrate\nfeatures from multiple Transformer layers with the hand-crafted feature to\nenhance the detection capability. In addition, we conducted extensive\nexperiments on one-class loss functions and provided optimized configurations\nto better align with the anti-spoofing task. Our fusion system achieved a\nminDCF of 0.4783 and an EER of 20.45% in the closed condition, and a minDCF of\n0.2245 and an EER of 9.36% in the open condition."}
{"id": "2509.18603", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18603", "abs": "https://arxiv.org/abs/2509.18603", "authors": ["Jiarui Hai", "Mounya Elhilali"], "title": "SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering", "comment": null, "summary": "Data synthesis and augmentation are essential for Sound Event Detection (SED)\ndue to the scarcity of temporally labeled data. While augmentation methods like\nSpecAugment and Mix-up can enhance model performance, they remain constrained\nby the diversity of existing samples. Recent generative models offer new\nopportunities, yet their direct application to SED is challenging due to the\nlack of precise temporal annotations and the risk of introducing noise through\nunreliable filtering. To address these challenges and enable generative-based\naugmentation for SED, we propose SynSonic, a data augmentation method tailored\nfor this task. SynSonic leverages text-to-audio diffusion models guided by an\nenergy-envelope ControlNet to generate temporally coherent sound events. A\njoint score filtering strategy with dual classifiers ensures sample quality,\nand we explore its practical integration into training pipelines. Experimental\nresults show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1\nand PSDS2), enhancing both temporal localization and sound class\ndiscrimination."}
{"id": "2509.18196", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18196", "abs": "https://arxiv.org/abs/2509.18196", "authors": ["Jialong Mai", "Jinxin Ji", "Xiaofen Xing", "Chen Yang", "Weidong Chen", "Jingyuan Xing", "Xiangmin Xu"], "title": "MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech", "comment": "Submitted to ICASSP 2026", "summary": "Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing\nlexical content, but largely fail to recognize nonverbal vocalizations (NVs)\nembedded in speech, such as sighs, laughs, and coughs. This capability is\nimportant for a comprehensive understanding of human communication, as NVs\nconvey crucial emotional and intentional cues. Progress in NV-aware ASR has\nbeen hindered by the lack of high-quality, well-annotated datasets. To address\nthis gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech\ndataset. Unlike most existing corpora that rely on model-based detection,\nMNV-17's performative nature ensures high-fidelity, clearly articulated NV\ninstances. To the best of our knowledge, MNV-17 provides the most extensive set\nof nonverbal vocalization categories, comprising 17 distinct and well-balanced\nclasses of common NVs. We benchmarked MNV-17 on four mainstream ASR\narchitectures, evaluating their joint performance on semantic transcription and\nNV classification. The dataset and the pretrained model checkpoints will be\nmade publicly available to facilitate future research in expressive ASR."}
{"id": "2509.18606", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18606", "abs": "https://arxiv.org/abs/2509.18606", "authors": ["Jiarui Hai", "Helin Wang", "Weizhe Guo", "Mounya Elhilali"], "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection", "comment": null, "summary": "Despite recent progress in large-scale sound event detection (SED) systems\ncapable of handling hundreds of sound classes, existing multi-class\nclassification frameworks remain fundamentally limited. They cannot process\nfree-text sound queries, which enable more flexible and user-friendly\ninteraction, and they lack zero-shot capabilities and offer poor few-shot\nadaptability. Although text-query-based separation methods have been explored,\nthey primarily focus on source separation and are ill-suited for SED tasks that\nrequire precise temporal localization and efficient detection across large and\ndiverse sound vocabularies. In this paper, we propose FlexSED, an\nopen-vocabulary sound event detection system. FlexSED builds on a pretrained\naudio SSL model and the CLAP text encoder, introducing an encoder-decoder\ncomposition and an adaptive fusion strategy to enable effective continuous\ntraining from pretrained weights. To ensure robust supervision, it also employs\nlarge language models (LLMs) to assist in event query selection during\ntraining, addressing challenges related to missing labels. As a result, FlexSED\nachieves superior performance compared to vanilla SED models on\nAudioSet-Strong, while demonstrating strong zero-shot and few-shot\ncapabilities. We release the code and pretrained models to support future\nresearch and applications based on FlexSED."}
{"id": "2509.18272", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18272", "abs": "https://arxiv.org/abs/2509.18272", "authors": ["Tornike Karchkhadze", "Kuan-Lin Chen", "Mojtaba", "Heydari", "Robert Henzel", "Alessandro Toso", "Mehrez Souden", "Joshua Atkins"], "title": "StereoFoley: Object-Aware Stereo Audio Generation from Video", "comment": null, "summary": "We present StereoFoley, a video-to-audio generation framework that produces\nsemantically aligned, temporally synchronized, and spatially accurate stereo\nsound at 48 kHz. While recent generative video-to-audio models achieve strong\nsemantic and temporal fidelity, they largely remain limited to mono or fail to\ndeliver object-aware stereo imaging, constrained by the lack of professionally\nmixed, spatially accurate video-to-audio datasets. First, we develop and train\na base model that generates stereo audio from video, achieving state-of-the-art\nin both semantic accuracy and synchronization. Next, to overcome dataset\nlimitations, we introduce a synthetic data generation pipeline that combines\nvideo analysis, object tracking, and audio synthesis with dynamic panning and\ndistance-based loudness controls, enabling spatially accurate object-aware\nsound. Finally, we fine-tune the base model on this synthetic dataset, yielding\nclear object-audio correspondence. Since no established metrics exist, we\nintroduce stereo object-awareness measures and validate it through a human\nlistening study, showing strong correlation with perception. This work\nestablishes the first end-to-end framework for stereo object-aware\nvideo-to-audio generation, addressing a critical gap and setting a new\nbenchmark in the field."}
{"id": "2509.19001", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19001", "abs": "https://arxiv.org/abs/2509.19001", "authors": ["Sihang Nie", "Xiaofen Xing", "Jingyuan Xing", "Baiji Liu", "Xiangmin Xu"], "title": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS", "comment": "5 pages, 2 figures, submitted to ICASSP2026", "summary": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already\nreached a high degree of naturalness. However, the precision control of TTS\ninference is still challenging. Although instruction-based Text-to-Speech\n(Instruct-TTS) models are proposed, these models still lack fine-grained\ncontrol due to the modality gap between single-level text instructions and\nmultilevel speech tokens. To address this limitation, we propose HD-PPT, a\nframework that transforms speech synthesis into a structured, hierarchical\ntask. To enable fine-grained control, we introduce a novel speech codec to\nextract distinct prompt-preference and content-preference tokens from the\ncomplex speech tokens, supervised by automatic speech recognition (ASR) and\ncross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality\ngap of these tokens, we propose a hierarchical decoding strategy, where the LLM\ngenerates tokens in a structured order: first semantic, then fine-grained\nstyle, and finally complete acoustic representation. Extensive experiments\ndemonstrate that this hierarchical paradigm significantly improves instruction\nadherence and achieves state-of-the-art naturalness, validating our approach\nfor precise and controllable speech synthesis. Audio samples are available at\nhttps://xxh333.github.io/."}
{"id": "2509.18375", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18375", "abs": "https://arxiv.org/abs/2509.18375", "authors": ["Hailin Song", "Shelley Brady", "Tomás Ward", "Alan F. Smeaton"], "title": "A Dimensional Approach to Canine Bark Analysis for Assistance Dog Seizure Signaling", "comment": null, "summary": "Standard classification of canine vocalisations is severely limited for\nassistance dogs, where sample data is sparse and variable across dogs and where\ncapture of the full range of bark types is ethically constrained. We reframe\nthis problem as a continuous regression task within a two-dimensional\narousal-valence space. Central to our approach is an adjusted Siamese Network\ntrained not on binary similarity, but on the ordinal and numeric distance\nbetween input sample pairs. Trained on a public dataset, our model reduces\nTurn-around Percentage by up to 50% on the challenging valence dimension\ncompared to a regression baseline. Qualitative validation on a real-world\ndataset confirms the learned space is semantically meaningful, establishing a\nproof-of-concept for analysing canine barking under severe data limitations."}
{"id": "2509.19091", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19091", "abs": "https://arxiv.org/abs/2509.19091", "authors": ["Hyeongju Kim", "Yechan Yu", "June Young Yi", "Juheon Lee"], "title": "Training Flow Matching Models with Reliable Labels via Self-Purification", "comment": "5 pages, 3 figures, preprint", "summary": "Training datasets are inherently imperfect, often containing mislabeled\nsamples due to human annotation errors, limitations of tagging models, and\nother sources of noise. Such label contamination can significantly degrade the\nperformance of a trained model. In this work, we introduce Self-Purifying Flow\nMatching (SPFM), a principled approach to filtering unreliable data within the\nflow-matching framework. SPFM identifies suspicious data using the model itself\nduring the training process, bypassing the need for pretrained models or\nadditional modules. Our experiments demonstrate that models trained with SPFM\ngenerate samples that accurately adhere to the specified conditioning, even\nwhen trained on noisy labels. Furthermore, we validate the robustness of SPFM\non the TITW dataset, which consists of in-the-wild speech data, achieving\nperformance that surpasses existing baselines."}
{"id": "2509.18412", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18412", "abs": "https://arxiv.org/abs/2509.18412", "authors": ["Mélisande Teng", "Julien Boussard", "David Rolnick", "Hugo Larochelle"], "title": "Identifying birdsong syllables without labelled data", "comment": null, "summary": "Identifying sequences of syllables within birdsongs is key to tackling a wide\narray of challenges, including bird individual identification and better\nunderstanding of animal communication and sensory-motor learning. Recently,\nmachine learning approaches have demonstrated great potential to alleviate the\nneed for experts to label long audio recordings by hand. However, they still\ntypically rely on the availability of labelled data for model training,\nrestricting applicability to a few species and datasets. In this work, we build\nthe first fully unsupervised algorithm to decompose birdsong recordings into\nsequences of syllables. We first detect syllable events, then cluster them to\nextract templates --syllable representations-- before performing matching\npursuit to decompose the recording as a sequence of syllables. We evaluate our\nautomatic annotations against human labels on a dataset of Bengalese finch\nsongs and find that our unsupervised method achieves high performance. We also\ndemonstrate that our approach can distinguish individual birds within a species\nthrough their unique vocal signatures, for both Bengalese finches and another\nspecies, the great tit."}
{"id": "2509.19186", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19186", "abs": "https://arxiv.org/abs/2509.19186", "authors": ["Hyeongju Kim", "Junhyeok Lee", "Jacob Morton", "Juheon Lee", "Jinhyeok Yang"], "title": "Improving Test-Time Performance of RVQ-based Neural Codecs", "comment": "5 pages, preprint", "summary": "The residual vector quantization (RVQ) technique plays a central role in\nrecent advances in neural audio codecs. These models effectively synthesize\nhigh-fidelity audio from a limited number of codes due to the hierarchical\nstructure among quantization levels. In this paper, we propose an encoding\nalgorithm to further enhance the synthesis quality of RVQ-based neural codecs\nat test-time. Firstly, we point out the suboptimal nature of quantized vectors\ngenerated by conventional methods. We demonstrate that quantization error can\nbe mitigated by selecting a different set of codes. Subsequently, we present\nour encoding algorithm, designed to identify a set of discrete codes that\nachieve a lower quantization error. We then apply the proposed method to\npre-trained models and evaluate its efficacy using diverse metrics. Our\nexperimental findings validate that our method not only reduces quantization\nerrors, but also improves synthesis quality."}
{"id": "2509.18424", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18424", "abs": "https://arxiv.org/abs/2509.18424", "authors": ["Rami Zewail"], "title": "Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection", "comment": null, "summary": "In an attempt to address the need for skilled clinicians in heart sound\ninterpretation, recent research efforts on automating cardiac auscultation have\nexplored deep learning approaches. The majority of these approaches have been\nbased on supervised learning that is always challenged in occasions where\ntraining data is limited. More recently, there has been a growing interest in\npotentials of pre-trained self-supervised audio foundation models for\nbiomedical end tasks. Despite exhibiting promising results, these foundational\nmodels are typically computationally intensive. Within the context of automatic\ncardiac auscultation, this study explores a lightweight alternative to these\ngeneral-purpose audio foundation models by introducing the Scattering\nTransformer, a novel, training-free transformer architecture for heart murmur\ndetection. The proposed method leverages standard wavelet scattering networks\nby introducing contextual dependencies in a transformer-like architecture\nwithout any backpropagation. We evaluate our approach on the public CirCor\nDigiScope dataset, directly comparing it against leading general-purpose\nfoundational models. The Scattering Transformer achieves a Weighted\nAccuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,\ndemonstrating performance highly competitive with contemporary state of the art\nmethods. This study establishes the Scattering Transformer as a viable and\npromising alternative in resource-constrained setups."}
{"id": "2509.19295", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19295", "abs": "https://arxiv.org/abs/2509.19295", "authors": ["Yonghyun Kim", "Chaeyeon Han", "Akash Sarode", "Noah Posner", "Subhrajit Guhathakurta", "Alexander Lerch"], "title": "Audio-Based Pedestrian Detection in the Presence of Vehicular Noise", "comment": "Accepted to the 10th Workshop on Detection and Classification of\n  Acoustic Scenes and Events (DCASE), 2025", "summary": "Audio-based pedestrian detection is a challenging task and has, thus far,\nonly been explored in noise-limited environments. We present a new dataset,\nresults, and a detailed analysis of the state-of-the-art in audio-based\npedestrian detection in the presence of vehicular noise. In our study, we\nconduct three analyses: (i) cross-dataset evaluation between noisy and\nnoise-limited environments, (ii) an assessment of the impact of noisy data on\nmodel performance, highlighting the influence of acoustic context, and (iii) an\nevaluation of the model's predictive robustness on out-of-domain sounds. The\nnew dataset is a comprehensive 1321-hour roadside dataset. It incorporates\ntraffic-rich soundscapes. Each recording includes 16kHz audio synchronized with\nframe-level pedestrian annotations and 1fps video thumbnails."}
{"id": "2509.18569", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18569", "abs": "https://arxiv.org/abs/2509.18569", "authors": ["Changfeng Gao", "Yabin Li", "Keyu An", "Zhifu Gao", "Zhihao Du", "Han Zhao", "Xiangang Li"], "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system", "comment": null, "summary": "In recent years, large language models (LLMs) have played an important role\nin automatic speech recognition (ASR) and text-to-speech (TTS) systems. While\nreinforcement learning (RL) has significantly enhanced LLM performance in\ntext-based tasks, its application to ASR and TTS remains underexplored due to\nthe complexity of training audio-based models. In this study, we propose a\nlightweight RL framework tailored for audio-based LLMs that can process audio\ninputs and generate audio outputs. Based on this framework, we evaluate the\neffectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR\ntask, we experiment with different rule-based reward functions within the Group\nRelative Policy Optimization (GRPO) framework and investigate the impact of RL\ndata construction. For the TTS task, we compare GRPO with Differentiable Reward\nOptimization (DiffRO) and further combine the two approaches to achieve\nimproved performance. Our experiments demonstrate that RL can significantly\nenhance the performance of both ASR and TTS systems, even with limited training\ndata and a small number of optimization steps."}
{"id": "2509.18620", "categories": ["cs.SD", "cs.IR", "eess.AS", "H.5.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.18620", "abs": "https://arxiv.org/abs/2509.18620", "authors": ["Aditya Bhattacharjee", "Marco Pasini", "Emmanouil Benetos"], "title": "Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation", "comment": "Under review for International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), Barcelona, 2026", "summary": "The evaluation of audio fingerprinting at a realistic scale is limited by the\nscarcity of large public music databases. We present an audio-free approach\nthat synthesises latent fingerprints which approximate the distribution of real\nfingerprints. Our method trains a Rectified Flow model on embeddings extracted\nby pre-trained neural audio fingerprinting systems. The synthetic fingerprints\ngenerated using our system act as realistic distractors and enable the\nsimulation of retrieval performance at a large scale without requiring\nadditional audio. We assess the fidelity of synthetic fingerprints by comparing\nthe distributions to real data. We further benchmark the retrieval performances\nacross multiple state-of-the-art audio fingerprinting frameworks by augmenting\nreal reference databases with synthetic distractors, and show that the scaling\ntrends obtained with synthetic distractors closely track those obtained with\nreal distractors. Finally, we scale the synthetic distractor database to model\nretrieval performance for very large databases, providing a practical metric of\nsystem scalability that does not depend on access to audio corpora."}
{"id": "2509.18691", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18691", "abs": "https://arxiv.org/abs/2509.18691", "authors": ["Sarthak Yadav", "Sergios Theodoridis", "Zheng-Hua Tan"], "title": "An overview of neural architectures for self-supervised audio representation learning from masked spectrograms", "comment": null, "summary": "In recent years, self-supervised learning has amassed significant interest\nfor training deep neural representations without labeled data. One such\nself-supervised learning approach is masked spectrogram modeling, where the\nobjective is to learn semantically rich contextual representations by\npredicting removed or hidden portions of the input audio spectrogram. With the\nTransformer neural architecture at its core, masked spectrogram modeling has\nemerged as the prominent approach for learning general purpose audio\nrepresentations, a.k.a. audio foundation models. Meanwhile, addressing the\nissues of the Transformer architecture, in particular the underlying Scaled\nDot-product Attention operation, which scales quadratically with input sequence\nlength, has led to renewed interest in recurrent sequence modeling approaches.\nAmong them, Selective structured state space models (such as Mamba) and\nextended Long Short-Term Memory (xLSTM) are the two most promising approaches\nwhich have experienced widespread adoption. While the body of work on these two\ntopics continues to grow, there is currently a lack of an adequate overview\nencompassing the intersection of these topics. In this paper, we present a\ncomprehensive overview of the aforementioned research domains, covering masked\nspectrogram modeling and the previously mentioned neural sequence modeling\narchitectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and\nxLSTM based masked spectrogram models in a unified, reproducible framework on\nten diverse downstream audio classification tasks, which will help interested\nreaders to make informed decisions regarding suitability of the evaluated\napproaches to adjacent applications."}
{"id": "2509.18700", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18700", "abs": "https://arxiv.org/abs/2509.18700", "authors": ["Chih-Cheng Chang", "Bo-Yu Chen", "Lu-Rong Chen", "Li Su"], "title": "Enhancing Automatic Chord Recognition through LLM Chain-of-Thought Reasoning", "comment": null, "summary": "Music Information Retrieval (MIR) encompasses a broad range of computational\ntechniques for analyzing and understanding musical content, with recent deep\nlearning advances driving substantial improvements. Building upon these\nadvances, this paper explores how large language models (LLMs) can serve as an\nintegrative bridge to connect and integrate information from multiple MIR\ntools, with a focus on enhancing automatic chord recognition performance. We\npresent a novel approach that positions text-based LLMs as intelligent\ncoordinators that process and integrate outputs from diverse state-of-the-art\nMIR tools-including music source separation, key detection, chord recognition,\nand beat tracking. Our method converts audio-derived musical information into\ntextual representations, enabling LLMs to perform reasoning and correction\nspecifically for chord recognition tasks. We design a 5-stage chain-of-thought\nframework that allows GPT-4o to systematically analyze, compare, and refine\nchord recognition results by leveraging music-theoretical knowledge to\nintegrate information across different MIR components. Experimental evaluation\non three datasets demonstrates consistent improvements across multiple\nevaluation metrics, with overall accuracy gains of 1-2.77% on the MIREX metric.\nOur findings demonstrate that LLMs can effectively function as integrative\nbridges in MIR pipelines, opening new directions for multi-tool coordination in\nmusic information retrieval tasks."}
{"id": "2509.18816", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.18816", "abs": "https://arxiv.org/abs/2509.18816", "authors": ["Junyu Wang", "Ziyang Ma", "Zhengding Luo", "Tianrui Wang", "Meng Ge", "Xiaobao Wang", "Longbiao Wang"], "title": "Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models", "comment": "Submitted to ICASSP 2026", "summary": "Large Audio-Language Models (LALMs) often suffer from audio-textual attention\nimbalance, prioritizing text over acoustic information, particularly in the\nmulti-modal fusion layers of the Transformer architecture. This bias hinders\ntheir ability to fully utilize acoustic cues, causing suboptimal performance on\naudio reasoning tasks. To mitigate this, we propose \\textbf{MATA}, a novel\ntraining-free method that dynamically pushes LALMs to pay \\textbf{M}ore\n\\textbf{A}ttention \\textbf{T}o \\textbf{A}udio tokens within the self-attention\nmechanism. Specifically, MATA intervenes post raw attention scoring, targeting\nonly the last token in intermediate layers without introducing additional\nparameters or computational overhead. Experiments on the MMAU and MMAR\nbenchmarks confirm MATA's effectiveness, with consistent performance gains.\nNotably, on MMAR, MATA enables an open-source model to surpass the proprietary\nGemini 2.0 Flash for the first time. Our work provides an efficient solution to\nmitigate attention bias and opens a new research direction for enhancing the\naudio-processing capabilities of multi-modal models."}

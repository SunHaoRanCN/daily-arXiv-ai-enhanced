<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 17]
- [eess.AS](#eess.AS) [Total: 20]
- [cs.SD](#cs.SD) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [On Multi-entity, Multivariate Quickest Change Point Detection](https://arxiv.org/abs/2509.18310)
*Bahar Kor,Bipin Gaikwad,Abani Patra,Eric L. Miller*

Main category: eess.SP

TL;DR: 提出了一种用于多实体多元时间序列数据的在线变点检测框架，通过个体偏离正常性概念和系统级异常评分来检测复杂动态环境中的系统级行为变化。


<details>
  <summary>Details</summary>
Motivation: 解决在人群监控等应用中传统传感方法不可行时的挑战，检测复杂动态环境中系统级行为变化，其中个体实体的数量和行为可能不确定或演化。

Method: 使用基于重构误差的自编码器计算个体偏离正常性，通过均值、方差和核密度估计聚合得到系统级异常评分，应用统计偏差度量和累积和技术检测持续或突变变化。

Result: 在合成数据集和人群模拟上的评估表明，该方法能准确检测显著的系统级变化，为复杂多智能体系统监控提供可扩展且保护隐私的解决方案。

Conclusion: 该方法无需标记数据或特征提取，支持实时流式输入处理，同时引入了新的多实体多元时间序列数据集，填补了该领域的数据空白。

Abstract: We propose a framework for online Change Point Detection (CPD) from
multi-entity, multivariate time series data, motivated by applications in crowd
monitoring where traditional sensing methods (e.g., video surveillance) may be
infeasible. Our approach addresses the challenge of detecting system-wide
behavioral shifts in complex, dynamic environments where the number and
behavior of individual entities may be uncertain or evolve. We introduce the
concept of Individual Deviation from Normality (IDfN), computed via a
reconstruction-error-based autoencoder trained on normal behavior. We aggregate
these individual deviations using mean, variance, and Kernel Density Estimates
(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or
abrupt changes, we apply statistical deviation metrics and the Cumulative Sum
(CUSUM) technique to these scores. Our unsupervised approach eliminates the
need for labeled data or feature extraction, enabling real-time operation on
streaming input. Evaluations on both synthetic datasets and crowd simulations,
explicitly designed for anomaly detection in group behaviors, demonstrate that
our method accurately detects significant system-level changes, offering a
scalable and privacy-preserving solution for monitoring complex multi-agent
systems. In addition to this methodological contribution, we introduce new,
challenging multi-entity multivariate time series datasets generated from crowd
simulations in Unity and coupled nonlinear oscillators. To the best of our
knowledge, there is currently no publicly available dataset of this type
designed explicitly to evaluate CPD in complex collective and interactive
systems, highlighting an essential gap that our work addresses.

</details>


### [2] [Multi-Target Detection for Cognitive MIMO Radar Networks](https://arxiv.org/abs/2509.18381)
*Nicholas L. K. Goradia,Harpreet S. Dhillon,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 本文针对未知噪声和杂波分布下的CFAR多目标检测，开发了集中式和分布式信号融合技术，利用强化学习提升检测性能，并揭示了空间域与时域之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 在未知噪声和杂波分布环境下，实现恒虚警率的多目标检测是雷达网络面临的关键挑战。传统方法难以适应未知分布条件，需要开发新的检测统计量和融合技术。

Method: 1) 为共址单基地MIMO雷达开发了在未知噪声和杂波分布下的检测统计量；2) 利用强化学习让雷达学习搜索区域中可能的目标位置；3) 将方法扩展到认知雷达网络，实现雷达间的协作学习；4) 分析了集中式和分布式检测的优劣。

Result: 提出的检测统计量在接收脉冲数足够大时具有渐近CFAR特性，强化学习方法显著提升了多目标检测性能，验证了空间域（天线数量）与时域（时间样本数）之间的基本权衡关系。

Conclusion: 认知雷达网络通过信号融合和强化学习能够在未知分布环境下实现有效的CFAR多目标检测，但需要在空间资源和时间资源之间进行权衡，集中式和分布式检测各有优势。

Abstract: In this work, we develop centralized and decentralized signal fusion
techniques for constant false alarm rate (CFAR) multi-target detection with a
cognitive radar network in unknown noise and clutter distributions. Further, we
first develop a detection statistic for co-located monostatic MIMO radar in
unknown noise and clutter distributions which is asymptotically CFAR as the
number of received pulses over all antennas grows large, and we provide
conditions under which this detection statistic is valid. We leverage
reinforcement learning (RL) for improved multi-target detection performance,
where the radar learns likely target locations in a search area. These results
are then generalized to the setting of cognitive radar networks, where radars
collaborate to learn where targets are likely to appear in a search area. We
show a fundamental tradeoff between the spatial and temporal domain for CFAR
detection in unknown noise and clutter distributions; in other words, we show a
tradeoff between the number of radar antennas and the number of temporal
samples. We show the benefits and tradeoffs with centralized and decentralized
detection with a network of cognitive radars.

</details>


### [3] [Automatic Model Extraction of the Match Standard in Symmetric--Reciprocal--Match Calibration](https://arxiv.org/abs/2509.18426)
*Ziad Hatab,Michael Ernst Gadringer,Arash Arsanjani,Wolfgang Boesch*

Main category: eess.SP

TL;DR: 本文提出了一种在VNA对称互易匹配(SRM)校准中建模匹配标准寄生参数的方法，通过非线性全局优化程序可以建立任意频率相关的匹配模型。


<details>
  <summary>Details</summary>
Motivation: 在传统SRM校准中，匹配标准被假定为完全已知，但实际应用中需要更灵活的建模方法来处理匹配标准的寄生参数。

Method: 采用非线性全局优化程序来建立匹配标准的任意频率相关模型，通过数值测试和微带线测量验证方法的有效性。

Result: 数值测试表明能够以软件数值精度恢复匹配标准寄生模型，微带线测量显示该方法能达到与使用多线TRL校准定义的匹配标准相似的精度。

Conclusion: 所提出的自动模型提取方法能够有效建模匹配标准的寄生参数，为VNA SRM校准提供了一种准确且灵活的技术方案。

Abstract: This paper addresses the modeling of parasitics of the match standard in the
symmetric-reciprocal-match (SRM) calibration method of vector network analyzers
(VNAs). In the general SRM procedure, the match standard is assumed to be fully
known. Here, we demonstrate that the match can be modeled with an arbitrary
frequency-dependent model using a non-linear global optimization procedure. To
highlight the validity of the suggested approach, numerical tests were
conducted, demonstrating the ability to recover the match standard parasitic
model down to software numerical precision. Additionally, we performed
microstrip line measurements to compare the SRM calibration with match modeling
to the multiline thru-reflect-line (TRL) calibration one, showing that
automatic model extraction can achieve accuracy similar to using a match
standard defined through multiline TRL calibration.

</details>


### [4] [A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems](https://arxiv.org/abs/2509.18555)
*Ping Wang,Zulin Wang,Yuanfang Ma,Xiaosi Tian,Yuanhan Ni*

Main category: eess.SP

TL;DR: 本文提出了一种安全仿射频分复用（SE-AFDM）技术，通过时变参数c2增强无线通信系统的安全性，同时保持合法接收者的良好误码率性能。


<details>
  <summary>Details</summary>
Motivation: 针对无线通信系统在高速移动场景下的安全需求，传统方法在双选择性信道下安全性不足，需要开发既能保证通信可靠性又能增强安全性的新型波形技术。

Method: 在AFDM基础上引入时变参数c2，合法接收者可以消除c2引入的非线性影响，而窃听者无法分离c2和随机信息符号，导致其误码率性能严重恶化。

Result: 数值结果表明SE-AFDM在高移动性场景下具有显著的安全性，同时保持良好误码率性能，窃听者的有效信干噪比随c2值范围扩大而降低。

Conclusion: SE-AFDM波形技术成功实现了通信安全性和可靠性的平衡，为高速移动环境下的安全无线通信提供了有效解决方案。

Abstract: This paper introduces a secure affine frequency division multiplexing
(SE-AFDM) for wireless communication systems to enhance communication security.
Besides configuring the parameter c1 to obtain communication reliability under
doubly selective channels, we also utilize the time-varying parameter c2 to
improve the security of the communications system. The derived input-output
relation shows that the legitimate receiver can eliminate the nonlinear impact
introduced by the time-varying c2 without losing the bit error rate (BER)
performance. Moreover, it is theoretically proved that the eavesdropper cannot
separate the time-varying c2 and random information symbols, such that the BER
performance of the eavesdropper is severely deteriorated. Meanwhile, the
analysis of the effective signal-to-interference-plus-noise ratio (SINR) of the
eavesdropper illustrates that the SINR decreases as the value range of c2
expands. Numerical results verify that the proposed SE-AFDM waveform has
significant security while maintaining good BER performance in high-mobility
scenarios.

</details>


### [5] [Integrated Cellular and LEO-based Positioning and Synchronization under User Mobility](https://arxiv.org/abs/2509.18727)
*Yasaman Ettefagh,Sharief Saleh,Musa Furkan Keskin,Hui Chen,Gonzalo Seco-Granados,Henk Wymeersch*

Main category: eess.SP

TL;DR: 本文研究利用集成地面和非地面网络（特别是低地球轨道卫星）对移动用户设备进行定位、同步和速度估计，重点关注仅从一个基站和一个LEO卫星接收信号的最小配置。


<details>
  <summary>Details</summary>
Motivation: 研究在6G网络中如何利用地面和非地面网络的集成来改善移动用户的定位、同步和速度估计性能，特别是在信号源有限的场景下。

Method: 推导了考虑移动性、时钟和频率偏移的通用信号模型，提出了一系列按计算复杂度组织的简化模型，并为每个模型开发了相应的估计算法。

Result: 通过严格仿真验证了所提模型的有效性，证明了它们在不同场景下的适用性。

Conclusion: 研究发现复杂度和性能之间的权衡可以根据不同的部署环境和应用需求进行优化，为6G移动用户定位和同步系统提供了有价值的见解。

Abstract: This paper investigates the localization, synchronization, and speed
estimation of a mobile user equipment (UE) leveraging integrated terrestrial
and non-terrestrial networks (NTNs), in particular low Earth orbit (LEO)
satellites. We focus on a minimal setup in which the UE received signal from
only one base station (BS) and one LEO satellite. We derive a generic signal
model accounting for mobility, clock and frequency offsets, based on which a
hierarchy of simplified models are proposed and organized by computational
complexity. Estimation algorithms are developed for each model to facilitate
efficient and accurate parameter recovery. Rigorous simulations validate the
effectiveness of the proposed models, demonstrating their suitability across
diverse scenarios. The findings highlight how the trade-off between complexity
and performance can be optimized for varying deployment environments and
application requirements, offering valuable insights for 6G positioning and
synchronization systems under user mobility.

</details>


### [6] [Detection Capability Comparison Between Intensity Detection and Splitting Detection for Rydberg-Atomic Sensors](https://arxiv.org/abs/2509.18753)
*Hao Wu,Xinyuan Yao,Rui Ni,Chen Gong,Kaibin Huang*

Main category: eess.SP

TL;DR: 本文系统分类和建模了里德堡原子量子接收器的两种信号读出方案（强度检测和分裂检测），推导了最大似然估计方法和克拉美-罗下界，提出了通过在高斜率区域优先采样来提升检测灵敏度的策略。


<details>
  <summary>Details</summary>
Motivation: 里德堡原子量子接收器具有高灵敏度和宽频带特性，但其独特的物理特性支持两种不同的信号读出方案，需要系统分析这两种方案的性能差异和优化策略。

Method: 将现有信号读出方法分类为强度检测和分裂检测两种范式，推导每种检测模式的最大似然估计程序和克拉美-罗下界，分析CRLB并提出在高斜率区域优先采样的优化策略。

Result: 数值结果显示，两种基本信号读出方法基于提出的最大似然估计方法都能获得更低的估计方差，特别是分裂检测中采用非均匀频率扫描策略相比传统多项式拟合显著降低了估计方差。

Conclusion: 本研究系统比较了两种检测方案的最优检测性能，为微波校准精度的提升做出了贡献，证明了最大似然估计方法在两种检测模式中的有效性。

Abstract: Rydberg atomic quantum receivers have been seen as novel radio frequency
measurements and the high sensitivity to a large range of frequencies makes it
attractive for communications reception. However, their unique physical
characteristics enable two fundamental signal readout schemes: intensity-based
detection and splitting-based detection. The former measures the electric
fields through laser intensity, while the latter utilizes Autler-Townes
splitting. In this work, we systematically categorize and model existing signal
readout methods, classifying them into these two paradigms. Then, we derive the
maximum likelihood estimation procedures and corresponding Cram\'er-Rao lower
bounds (CRLB) for each detection modality. Through the analysis of the CRLB, we
propose strategy for both readout schemes to enhance sensitivity and minimize
estimation variance: acquiring data in regions with maximal slope magnitudes.
While this approach has been implemented in intensity-based detection (e.g.,
superheterodyne schemes), its application to splitting-based detection remains
unexplored. Implementation of non-uniform frequency scanning, with preferential
sampling at regions exhibiting maximum peak slopes combined with our proposed
maximum likelihood splitting estimation method, achieves significantly reduced
estimation variance compared to conventional polynomial fitting. The
comparative analysis reveals the optimal detection performance of the two
detection schemes. This work also contributes to enhancing the accuracy of
microwave calibration. Numerical results reveal that both fundamental signal
readout methods achieve lower estimation variance based on our proposed maximum
likelihood estimation approach.

</details>


### [7] [Highly Parallel Singular Value Decomposition for Low-Latency MIMO Processing](https://arxiv.org/abs/2509.18799)
*Sijia Cheng,Liang Liu,Ove Edfors,Juan Vidal Alegria*

Main category: eess.SP

TL;DR: 本文分析了SVD在无线系统中的延迟问题，提出了一种基于Gram矩阵三对角化的4步并行方法，并开发了时间复杂度分析框架，在Massive MIMO场景下展示了优越的时间效率。


<details>
  <summary>Details</summary>
Motivation: SVD在MIMO处理等无线系统中广泛应用，但传统迭代分解方法随着系统规模增大会导致执行时间增加，难以满足实时和低延迟应用需求。

Method: 分析现有SVD方法的延迟，提出基于Gram矩阵三对角化的4步高度并行方法，并开发了结合硬件分析的时间复杂度分析框架。

Result: 数值结果表明所选并行方法具有优越的时间效率，特别是在Massive MIMO场景下表现突出。

Conclusion: 所提出的并行SVD方法和分析框架能够有效解决大规模无线系统中的实时处理延迟问题。

Abstract: Singular value decomposition (SVD) is widely used in wireless systems,
including multiple-input multiple-output (MIMO) processing and dimension
reduction in distributed MIMO (D-MIMO). However, the iterative nature of
decomposition methods results in increased execution time as system size grows,
posing challenges for real-time and low-latency applications. To address this,
we analyze the latency of state-of-art SVD methods, and highlight the
efficiency of a 4-step highly parallel method based on Gram matrix
tridiagonalization. Furthermore, we develop a time complexity (processing
latency) analysis framework with hardware profiling, allowing scalable and
realistic evaluation without full implementation. The numerical results
demonstrate the superior time efficiency of the selected parallel method,
particularly in massive MIMO scenarios.

</details>


### [8] [Normal mode parameters estimation by a VLA in single-shooting](https://arxiv.org/abs/2509.18853)
*Xiaolei Li,Pengyu Wang,Wenhua Song,Yangjin Xu,Wei Gao*

Main category: eess.SP

TL;DR: 本文提出了一种正交约束模态搜索（OCMS）方法，用于使用垂直线性阵列（VLA）估计模态波数和模态深度函数。该方法在已知声速剖面的假设下，利用不同模态深度函数的正交性来提取模态深度函数及其对应的波数。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在VLA和单频声源保持静止的情况下，准确估计模态波数和模态深度函数的方法，以克服传统方法的局限性。

Method: OCMS方法基于模态深度函数的正交性，通过约束优化搜索来同时提取模态波数和深度函数。方法在已知声速剖面的条件下工作。

Result: 数值模拟显示OCMS对噪声、VLA孔径变化和阵元数量变化具有鲁棒性。在SSP不确定度<1 m/s和VLA倾斜角<5°时性能可靠。实验数据验证显示模态波数的相对误差约为10^{-4}量级。

Conclusion: OCMS方法是一种有效的模态参数估计技术，在多种实际条件下表现出良好的性能和准确性，适用于实际海洋环境中的声学模态分析。

Abstract: This paper proposes an orthogonality-constrained modal search (OCMS) method
for estimating modal wavenumbers and modal depth functions using a vertical
linear array (VLA). Under the assumption of a known sound speed profile, OCMS
leverages the orthogonality of distinct modal depth functions to extract both
the modal depth functions and their corresponding wavenumbers, even when the
VLA and a monochromatic sound source remain stationary.The performance of OCMS
is evaluated through numerical simulations under varying signal-to-noise ratios
(SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and
sound speed profile (SSP) uncertainty. The results demonstrate that OCMS is
robust against noise, VLA aperture variations, and changes in the number of VLA
elements, meanwhile, the algorithm maintains reliable performance when SSP
uncertainty < 1 m/s and VLA tilt angle <5{\deg}. Furthermore, the effectiveness
of OCMS is validated using SwellEx96 experimental data. The relative error
between the modal wavenumbers derived from experimental data and those computed
via Kraken is on the order of $10^{-4}$.

</details>


### [9] [Quaternion LMS for Graph Signal Recovery](https://arxiv.org/abs/2509.18918)
*Hamideh-Sadat Fazael-Ardekani,Hadi Zayyani,Hamid Soltanian-Zadeh*

Main category: eess.SP

TL;DR: 该论文将图信号处理中的图信号恢复问题推广到四元数域，提出了四元数图LMS算法，并进行了收敛性分析和仿真验证。


<details>
  <summary>Details</summary>
Motivation: 将图信号处理中的图信号恢复问题扩展到四元数域，结合四元数自适应滤波和图信号处理的理论，解决更复杂的信号处理问题。

Method: 推导了基于四元数代数的基本自适应公式，提出了四元数图LMS算法，并进行了均值收敛分析和均方收敛分析，给出了步长参数的充分条件。

Result: 仿真结果表明所提出的算法在图信号重构中具有有效性。

Conclusion: 成功将图信号恢复问题推广到四元数域，提出的QGLMS算法在理论和实验上都表现出良好的性能。

Abstract: This letter generalizes the Graph Signal Recovery (GSR) problem in Graph
Signal Processing (GSP) to the Quaternion domain. It extends the Quaternion
Least Mean Square (QLMS) in adaptive filtering literature, and Graph LMS (GLMS)
algorithm in GSP literature, to an algorithm called Quaternion GLMS (QGLMS).
The basic adaptation formula using Quaternion-based algebra is derived.
Moreover, mean convergence analysis and mean-square convergence analysis are
mathematically performed. Hence, a sufficient condition on the step-size
parameter of QGLMS is suggested. Also, simulation results demonstrate the
effectiveness of the proposed algorithm in graph signal reconstruction.

</details>


### [10] [Bayesian Convolutional Neural Networks for Prior Learning in Graph Signal Recovery](https://arxiv.org/abs/2509.19056)
*Razieh Torkamani,Arash Amini,Hadi Zayyani,Mehdi Korki*

Main category: eess.SP

TL;DR: 提出了一个基于贝叶斯卷积神经网络的数据驱动框架，用于图信号恢复问题，能够直接从训练样本中学习信号先验分布，并在各种信号分布下实现准确鲁棒的恢复。


<details>
  <summary>Details</summary>
Motivation: 图信号恢复中的主要挑战是图信号的底层统计模型通常未知或过于复杂难以解析指定，需要一种灵活的数据驱动方法来学习信号先验。

Method: 开发了贝叶斯卷积神经网络架构，使用基于切比雪夫多项式的图感知滤波器建模图信号先验分布，将CNN隐藏层解释为Gibbs分布并采用高斯混合模型非线性，构建闭式表达的先验，然后集成到变分贝叶斯推理框架中进行信号和噪声精度后验估计。

Result: 在合成和真实世界图数据集上的广泛实验表明，BCNN-GSR算法在各种信号分布下都能实现准确鲁棒的恢复，对复杂非高斯信号模型具有良好的泛化能力，且计算效率高。

Conclusion: 该方法为大规模图恢复任务提供了一种实用且计算高效的解决方案，能够有效处理未知或复杂图信号统计模型的恢复问题。

Abstract: Graph signal recovery (GSR) is a fundamental problem in graph signal
processing, where the goal is to reconstruct a complete signal defined over a
graph from a subset of noisy or missing observations. A central challenge in
GSR is that the underlying statistical model of the graph signal is often
unknown or too complex to specify analytically. To address this, we propose a
flexible, data-driven framework that learns the signal prior directly from
training samples. We develop a Bayesian convolutional neural network (BCNN)
architecture that models the prior distribution of graph signals using
graph-aware filters based on Chebyshev polynomials. By interpreting the hidden
layers of the CNN as Gibbs distributions and employing Gaussian mixture model
(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior
is integrated into a variational Bayesian (VB) inference framework to estimate
the posterior distribution of the signal and noise precision. Extensive
experiments on synthetic and real-world graph datasets demonstrate that the
proposed BCNN-GSR algorithm achieves accurate and robust recovery across a
variety of signal distributions. The method generalizes well to complex,
non-Gaussian signal models and remains computationally efficient, making it
suitable for practical large-scale graph recovery tasks.

</details>


### [11] [Data-Free Knowledge Distillation for LiDAR-Aided Beam Tracking in MmWave Systems](https://arxiv.org/abs/2509.19092)
*Abolfazl Zakeri,Nhan Thanh Nguyen,Ahmed Alkhateeb,Markku Juntti*

Main category: eess.SP

TL;DR: 提出了一种无数据知识蒸馏框架，用于高效的LiDAR辅助毫米波波束跟踪，通过知识反演生成合成数据来训练学生模型，在减少机器学习复杂性和数据集需求的同时实现了优于教师模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态传感可以减少波束训练开销，但受到机器学习复杂性和数据集需求的限制。为了解决这些问题，需要开发更高效的数据处理方法。

Method: 提出了知识反演框架，其中生成器从随机噪声合成LiDAR输入数据，通过元数据损失、激活损失和熵损失三个损失项进行指导。学生模型使用合成数据和教师模型的知识进行训练，采用KL散度损失和MSE损失。

Result: 仿真结果表明，所提出的DF-KD方法在Top-1和Top-5准确率上略微优于教师模型。元数据损失对生成器性能贡献显著，MSE损失可以有效替代标准KD损失且需要更少的超参数调优。

Conclusion: 该无数据知识蒸馏框架为LiDAR辅助毫米波波束跟踪提供了一种高效解决方案，在减少数据依赖的同时实现了优异的性能表现。

Abstract: Multimodal sensing reduces beam training overhead but is constrained by
machine learning complexity and dataset demands. To address this, we propose a
data-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided
mmWave beam tracking, i.e., predicting the best current and future beams.
Specifically, we propose a knowledge inversion framework, where a generator
synthesizes LiDAR input data from random noise, guided by a loss function
defined on the features and outputs of a pre-trained teacher model. The student
model is then trained using the synthetic data and knowledge distilled from the
teacher. The generator loss combines three terms, called metadata loss,
activation loss, and entropy loss. For student training, in addition to the
standard Kullback-Leibler divergence loss, we also consider a mean-squared
error (MSE) loss between the teacher and student logits. Simulation results
show that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and
Top-5 accuracies. Moreover, we observe that the metadata loss contributes
significantly to the generator performance, and that the MSE loss for the
student can effectively replace the standard KD loss while requiring fewer
fine-tuned hyperparameters.

</details>


### [12] [Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC](https://arxiv.org/abs/2509.19119)
*Palatip Jopanya,Diana P. M. Osorio*

Main category: eess.SP

TL;DR: 本文探讨了在基于大规模MIMO的蜂窝系统中，通过部署中继器群来增强集成感知与通信（ISAC）系统的雷达感知能力，特别是用于无人机检测。


<details>
  <summary>Details</summary>
Motivation: 随着ISAC新架构、用例和标准的不断出现，基于大规模MIMO的蜂窝系统也在通过集成新型网络组件而演进，需要支持新兴的ISAC用例和服务。本文旨在研究如何通过部署成本效益高的中继器群来增强ISAC系统的感知能力。

Method: 利用中继器即时重传信号的能力，在群中继器辅助的MIMO ISAC系统中优化中继器的增益，研究其对雷达感知性能的影响。

Result: 研究结果表明，在给定足够最大放大增益的条件下，通过优化中继器增益，增加中继器数量可以带来感知性能的提升。

Conclusion: 部署中继器群是一种有效的蜂窝网络密集化方法，能够显著增强ISAC系统的雷达感知能力，特别是在无人机检测等应用场景中具有重要价值。

Abstract: As definitions about new architectural aspects, use cases, and standards for
integrated sensing and communication (ISAC) continue to appear, cellular
systems based on massive multiple-input multiple-output (MIMO) antenna
technology are also experiencing a parallel evolution through the integration
of novel network components. This evolution should support emerging ISAC use
cases and services. In particular, this paper explores a recent vision for
cost-efficient cellular network densification through the deployment of swarms
of repeaters. Leveraging their ability to retransmit signals instantaneously,
we investigate how these repeaters can enhance radar sensing capabilities for
drone detection in a swarm repeater-assisted MIMO ISAC system. Our results
demonstrate that, by optimizing the gains of repeaters given a sufficient
maximum amplification gain, increasing the number of repeaters can lead to
gains in sensing performance.

</details>


### [13] [Deep Reinforcement Learning for Dynamic Sensing and Communications](https://arxiv.org/abs/2509.19130)
*Abolfazl Zakeri,Nhan Thanh Nguyen,Ahmed Alkhateeb,Markku Juntti*

Main category: eess.SP

TL;DR: 提出了一种统一的机器学习框架，用于动态决定何时进行环境感知并利用感知数据进行波束预测，以在满足感知预算约束的同时最大化平均信噪比。


<details>
  <summary>Details</summary>
Motivation: 环境感知可以显著增强毫米波通信的波束训练性能，但需要平衡感知带来的收益与相关的感知成本。

Method: 使用Lyapunov优化来强制执行感知约束，深度Q网络决定感知时隙，预训练的深度神经网络将感知数据映射到码本中的最优波束。

Result: 基于真实世界DeepSense数据集的仿真表明，该方法显著降低了感知开销，同时保持了令人满意的通信性能。

Conclusion: 所提出的方法有效解决了毫米波通信中感知成本与性能的权衡问题，为智能环境感知辅助通信提供了可行方案。

Abstract: Environmental sensing can significantly enhance mmWave communications by
assisting beam training, yet its benefits must be balanced against the
associated sensing costs. To this end, we propose a unified machine learning
framework that dynamically determines when to sense and leverages sensory data
for beam prediction. Specifically, we formulate a joint sensing and beamforming
problem that maximizes the av- erage signal-to-noise ratio under an average
sensing budget. Lyapunov optimization is employed to enforce the sensing
constraint, while a deep Q-Network determines the sensing slots. A pretrained
deep neural network then maps the sens- ing data to optimal beams in the
codebook. Simulations based on the real-world DeepSense dataset demonstrate
that the pro- posed approach substantially reduces sensing overhead while
maintaining satisfactory communications performance.

</details>


### [14] [On the Performance of THz Wireless Systems over $α$-$\mathcal{F}$ Channels with Beam Misalignment and Mobility](https://arxiv.org/abs/2509.19235)
*Wamberto J. L. Queiroz,Hugerles S. Silva,Higo T. P. Silva,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: 本文研究了太赫兹无线系统在α-ℱ衰落信道中考虑波束失准和移动性的性能，推导了信噪比的统计特性表达式，并基于此分析了中断概率、符号错误概率和平均信道容量等关键性能指标。


<details>
  <summary>Details</summary>
Motivation: 研究太赫兹无线通信系统在实际信道条件（包括波束失准和移动性）下的性能表现，为系统设计和优化提供理论依据。

Method: 推导了α-ℱ衰落信道下信噪比的概率密度函数、累积分布函数、矩生成函数和高阶矩的新表达式，并基于这些表达式分析系统性能指标，同时进行蒙特卡洛仿真验证。

Result: 获得了系统性能指标的解析表达式和渐近分析结果，蒙特卡洛仿真结果验证了理论分析的正确性。

Conclusion: 提出的分析框架能够准确评估太赫兹无线系统在复杂信道条件下的性能，为系统设计提供了有价值的理论工具。

Abstract: This paper investigates the performance of terahertz~(THz) wireless systems
over the $\alpha$-$\mathcal{F}$ fading channels with beam misalignment and
mobility. New expressions are derived for the probability density, cumulative
distribution, and moment generating functions, as well as higher-order moments
of the instantaneous signal-to-noise ratio. Building upon the aforementioned
expressions, we extract novel formulas for the outage probability, symbol error
probability, and average channel capacity. Asymptotic metrics are also deduced,
which provide useful insights. Monte Carlo simulations results are presented to
support the derived analytical framework.

</details>


### [15] [Faster-Than-Nyquist Signalling - Theoretical Limits on Capacity and Techniques to Approach Capacity](https://arxiv.org/abs/2509.19272)
*Sathwik Chadaga*

Main category: eess.SP

TL;DR: 本文研究了FTN信号传输方案，通过推导脉冲形状和时间加速因子的条件来完全避免ISI，并探索了功率分配和自适应加载技术来提高OFDM FTN系统的吞吐量。


<details>
  <summary>Details</summary>
Motivation: FTN信号传输方案能够提供比奈奎斯特传输方案更高的吞吐量和更好的频谱效率，但会引入符号间干扰（ISI）。本文旨在研究如何避免ISI并提高系统性能。

Method: 推导脉冲形状和时间加速因子的条件以避免ISI，研究FTN系统的理论容量限制，并探索功率分配和自适应加载技术在OFDM FTN系统中的应用。

Result: 通过理论分析和仿真实验，证明了所提出的条件和技术能够有效减少ISI并提高系统吞吐量。

Conclusion: FTN信号传输方案在满足特定条件下可以避免ISI，并通过功率分配和自适应加载技术进一步提高系统性能，具有实际应用潜力。

Abstract: Faster-Than-Nyquist (FTN) Signalling is a non-orthogonal transmission scheme
that violates the Nyquist zero-ISI criterion providing higher throughput and
better spectral efficiency than a Nyquist transmission scheme. In this thesis,
the inter symbol interference (ISI) introduced by FTN signalling is studied,
and conditions on pulse shapes and $\tau$ (time acceleration factor) are
derived so that the ISI can be avoided completely. Further, these conditions
are reinforced by investigating the theoretical limits on the capacities of FTN
systems. Finally, the use of power allocation and adaptive loading techniques
are explored in reducing the effect of ISI and increasing the throughput of
orthogonal frequency division multiplexing (OFDM) FTN systems. The
implementation of these techniques and simulation results are also
demonstrated.

</details>


### [16] [A Novel Site-Specific Inference Model for Urban Canyon Channels: From Measurements to Modeling](https://arxiv.org/abs/2509.19275)
*Junzhe Song,Ruisi He,Mi Yang,Zhengyu Zhang,Xinwen Chen,Xiaoying Zhang,Bo Ai*

Main category: eess.SP

TL;DR: 提出了一种基于环境几何的站点特定信道推断模型，通过子6GHz信道测量参数化，建立物理环境与多径分量统计特征之间的可解释映射。


<details>
  <summary>Details</summary>
Motivation: 现有信道模型未能充分捕捉城市峡谷场景中由街道几何和建筑分布决定的站点特定信道特性，需要开发更精确的模型。

Method: 基于几何传播提取和聚类多径分量，明确考虑峡谷宽度的影响，建立物理环境与多径分量统计特征之间的映射关系，并提出了逐步实现方案。

Result: 通过与实测信道二阶统计量的比较验证，该模型在不同城市峡谷场景下均表现出高精度和鲁棒性。

Conclusion: 所提出的站点特定信道推断模型能够准确捕捉城市峡谷环境中的信道特性，为智能交通和智慧城市应用中的无线通信系统设计提供了有效工具。

Abstract: With the rapid development of intelligent transportation and smart city
applications, urban canyon has become a critical scenario for the design and
evaluation of wireless communication systems. Due to its unique environmental
layout, the channel characteristics in urban canyon are strongly a street
geometry and building distribution, thereby exhibiting significant
site-specific channel condition. However, this feature has not been well
captured in existing channel models. In this paper, we propose a site-specific
channel inference model based on environmental geometry, the model is
parameterized using sub-6GHz channel measurements. Multipath components (MPCs)
are extracted and clustered according to geometric propagation, which are
explicitly derived from the influence of canyon width, thereby establishing an
interpretable mapping between the physical environment and statistical
characteristics of MPCs. A step-by-step implementation scheme is presented.
Subsequently, the proposed site-specific channel inference model is validated
by comparing second-order statistics of channels, derived from the model and
measurements. The results show that the proposed model achieves high accuracy
and robustness in different urban canyon scenarios.

</details>


### [17] [STFT-AECNN: An Attention-Enhanced CNN for Efficient Φ-OTDR Event Recognition in IoT-Enabled Distributed Acoustic Sensing](https://arxiv.org/abs/2509.19281)
*Xiyang Lan,Xin Li*

Main category: eess.SP

TL;DR: 提出了一种基于STFT的注意力增强卷积神经网络(STFT-AECNN)，用于解决Φ-OTDR数据流中事件识别的挑战，在保持计算效率的同时达到99.94%的峰值准确率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法要么破坏信号的固有时空结构，要么计算成本过高，限制了在资源受限的IoT场景中的应用。

Method: 将多通道时间序列数据表示为堆叠的频谱图，充分利用时空特性；引入空间高效注意力模块(SEAM)自适应强调信息最丰富的通道；采用联合交叉熵和三重损失增强特征空间的可区分性。

Result: 在公开的BJTU Φ-OTDR数据集上，STFT-AECNN达到99.94%的峰值准确率，同时保持高计算效率。

Conclusion: 该方法在IoT支持的DAS系统中具有实时、可扩展和鲁棒的事件识别潜力，为可靠智能的IoT传感应用铺平了道路。

Abstract: Phase-sensitive optical time-domain reflectometry ({\Phi}-OTDR) has emerged
as a promising sensing technology in Internet of Things (IoT) infrastructures,
enabling large-scale distributed acoustic sensing (DAS) for smart city
surveillance, industrial pipeline monitoring, and critical infrastructure
protection. However, accurately recognizing events from massive {\Phi}-OTDR
data streams remains challenging, as existing deep learning methods either
disrupt the inherent spatiotemporal structure of signals or incur prohibitive
computational costs, limiting their applicability in resource-constrained IoT
scenarios. To overcome these challenges, we propose a novel STFT-based
Attention-Enhanced Convolutional Neural Network (STFT-AECNN), which represents
multi-channel time-series data as stacked spectrograms to fully exploit their
spatiotemporal characteristics while enabling efficient 2D CNN processing. A
Spatial Efficient Attention Module (SEAM) is further introduced to adaptively
emphasize the most informative channels, and a joint Cross-Entropy and Triplet
loss is adopted to enhance the discriminability of the learned feature space.
Extensive experiments on the public BJTU {\Phi}-OTDR dataset demonstrate that
STFT-AECNN achieves a peak accuracy of 99.94% while maintaining high
computational efficiency. These results highlight its potential for real-time,
scalable, and robust event recognition in IoT-enabled DAS systems, paving the
way for reliable and intelligent IoT sensing applications.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [18] [Automated Analysis of Naturalistic Recordings in Early Childhood: Applications, Challenges, and Opportunities](https://arxiv.org/abs/2509.18235)
*Jialu Li,Marvin Lavechin,Xulin Fan,Nancy L. McElwain,Alejandrina Cristia,Paola Garcia-Perera,Mark Hasegawa-Johnson*

Main category: eess.AS

TL;DR: 本文综述了自然主义长时录音在儿童发展研究中的应用，讨论了现有语音技术（如说话人日志、发声分类等）在儿童录音分析中的进展、挑战和机遇，旨在促进信号处理领域的跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 自然主义录音能够无干扰地观察儿童在真实环境中的行为，但现有的语音技术主要针对成人开发，针对儿童的应用仍处于探索不足的状态，需要填补这一技术空白。

Method: 通过综述现有语音技术（说话人日志、发声分类、成人词数估计、说话人验证、语言日志等）在儿童自然主义录音分析中的应用现状，分析技术挑战和发展方向。

Result: 发现现有技术虽不完美但已能为儿童发展研究提供有价值见解，指出针对儿童的特殊语音特征（如音高变化、发音不准确等）需要开发专门的技术解决方案。

Conclusion: 呼吁信号处理社区加强跨学科合作，进一步开发适用于儿童自然主义录音分析的专门技术，以更好地支持儿童认知和社会发展的研究。

Abstract: Naturalistic recordings capture audio in real-world environments where
participants behave naturally without interference from researchers or
experimental protocols. Naturalistic long-form recordings extend this concept
by capturing spontaneous and continuous interactions over extended periods,
often spanning hours or even days, in participants' daily lives. Naturalistic
recordings have been extensively used to study children's behaviors, including
how they interact with others in their environment, in the fields of
psychology, education, cognitive science, and clinical research. These
recordings provide an unobtrusive way to observe children in real-world
settings beyond controlled and constrained experimental environments.
Advancements in speech technology and machine learning have provided an initial
step for researchers to automatically and systematically analyze large-scale
naturalistic recordings of children. Despite the imperfect accuracy of machine
learning models, these tools still offer valuable opportunities to uncover
important insights into children's cognitive and social development. Several
critical speech technologies involved include speaker diarization, vocalization
classification, word count estimate from adults, speaker verification, and
language diarization for code-switching. Most of these technologies have been
primarily developed for adults, and speech technologies applied to children
specifically are still vastly under-explored. To fill this gap, we discuss
current progress, challenges, and opportunities in advancing these technologies
to analyze naturalistic recordings of children during early development (<3
years of age). We strive to inspire the signal processing community and foster
interdisciplinary collaborations to further develop this emerging technology
and address its unique challenges and opportunities.

</details>


### [19] [No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS](https://arxiv.org/abs/2509.18531)
*Seungyoun Shin,Dongha Ahn,Jiwoo Kim,Sungwook Jeon*

Main category: eess.AS

TL;DR: 本文提出了一种迭代直接偏好优化（DPO）方案，通过少量人工标注的偏好对来优化TTS系统的韵律自然度，在韩国呼叫中心数据集上取得了最佳的人类偏好评分和竞争力的字符错误率。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法虽然能降低字符错误率，但会导致韵律单调不自然，且加入说话人相似性后会进一步破坏训练稳定性。当韵律无法通过自动奖励函数评估时，需要更有效的方法来优化韵律自然度。

Method: 采用迭代直接偏好优化（DPO）方案，每轮仅使用几百个人工标注的偏好对来直接优化韵律自然度，同时对当前模型进行正则化。

Result: 在KoCC-TTS数据集上，该方法获得了最高的人类偏好评分（ELO）和竞争力的字符错误率，超越了GRPO和商业基线系统。

Conclusion: 当韵律无法自动奖励时，人类偏好优化为自然且鲁棒的TTS系统提供了一条实用且数据高效的路径。

Abstract: Recent work reports gains in neural text-to-speech (TTS) with Group Relative
Policy Optimization (GRPO). However, in the absence of a verifiable reward for
\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)
lowers error rates yet collapses prosody into monotone, unnatural speech;
adding speaker-similarity further destabilizes training and degrades CER. We
address this with an \textit{iterative Direct Preference Optimization (DPO)}
scheme that uses only a few hundred human-labeled preference pairs per round to
directly optimize prosodic naturalness while regularizing to the current model.
On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center
interactions capturing task-oriented dialogues, our method attains the highest
human preference (ELO) with competitive CER, outperforming GRPO and strong
commercial baselines. These results suggest that when prosody cannot be
rewarded automatically, \textit{human preference optimization} offers a
practical and data-efficient path to natural and robust TTS. The demo page is
available at \href{https://tts.ch.dev}

</details>


### [20] [SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes](https://arxiv.org/abs/2509.18561)
*Dayun Choi,Jung-Woo Choi*

Main category: eess.AS

TL;DR: SoundCompass是一个目标声音提取框架，通过SPIN模块捕捉复杂频谱域的空间相关性，结合球谐函数编码的DoA线索，并采用链式推理策略进行迭代优化，实现鲁棒的目标源提取。


<details>
  <summary>Details</summary>
Motivation: 现有的基于DoA的目标声音提取方法依赖手工特征或离散编码，丢失了细粒度空间信息且适应性有限。需要一种能够保留完整空间信息并提高适应性的新方法。

Method: 提出Spectral Pairwise INteraction (SPIN)模块捕捉跨通道空间相关性；使用球谐函数编码DoA线索；采用频带分割架构进行特征融合；引入链式推理(CoI)策略进行迭代优化。

Result: 实验表明SoundCompass能够鲁棒地提取不同信号类别和空间配置下的目标源，性能优于现有方法。

Conclusion: SoundCompass通过结合SPIN模块、SH嵌入和CoI策略，有效解决了现有DoA方法空间信息丢失和适应性不足的问题，为目标声音提取提供了更优的解决方案。

Abstract: Recent advances in target sound extraction (TSE) utilize directional clues
derived from direction of arrival (DoA), which represent an inherent spatial
property of sound available in any acoustic scene. However, previous DoA-based
methods rely on hand-crafted features or discrete encodings, which lose
fine-grained spatial information and limit adaptability. We propose
SoundCompass, an effective directional clue integration framework centered on a
Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial
correlations in the complex spectrogram domain to preserve full spatial
information in multichannel signals. The input feature expressed in terms of
spatial correlations is fused with a DoA clue represented as spherical
harmonics (SH) encoding. The fusion is carried out across overlapping frequency
subbands, inheriting the benefits reported in the previous band-split
architectures. We also incorporate the iterative refinement strategy,
chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA
with sound event activation estimated from the previous inference stage.
Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and
CoI, robustly extracts target sources across diverse signal classes and spatial
configurations.

</details>


### [21] [HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling](https://arxiv.org/abs/2509.18570)
*Yuke Si,Runyan Yang,Yingying Gao,Junlan Feng,Chao Deng,Shilei Zhang*

Main category: eess.AS

TL;DR: 提出了HarmoniFuse框架，通过组件选择和提示自适应机制来解决多任务语音语言模型中任务干扰问题，提升ASR和SER性能


<details>
  <summary>Details</summary>
Motivation: 现有统一语音语言模型在处理ASR和SER等依赖不同类型信息的任务时存在任务干扰和性能下降问题，特别是在数据有限条件下

Method: 采用门控语音编码器提取任务特定声学特征，结合提示自适应动态融合模块根据任务特性聚合transformer层，并使用批量交错训练策略

Result: 实验结果表明HarmoniFuse能够同时提升ASR和SER性能

Conclusion: HarmoniFuse为多任务语音理解提供了一个可扩展且稳健的解决方案，特别适用于现实数据约束条件

Abstract: Recent advances in large language models have facilitated the development of
unified speech language models (SLMs) capable of supporting multiple speech
tasks within a shared architecture. However, tasks such as automatic speech
recognition (ASR) and speech emotion recognition (SER) rely on distinct types
of information: ASR primarily depends on linguistic content, whereas SER
requires the integration of both linguistic and paralinguistic cues. Existing
multitask SLMs typically adopt naive parameter sharing or prompt-based
conditioning without explicitly modeling the differences in information
composition required by each task. Such designs risk task interference and
performance degradation, especially under limited data conditions. To address
these limitations, we propose HarmoniFuse, a component-selective and
prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse
is designed to harmonize heterogeneous task demands by selecting and fusing
task-relevant components of speech representations. Specifically, it integrates
a gated speech encoder to extract task-specific acoustic features and a
prompt-adaptive dynamic fusion module to aggregate transformer layers based on
task characteristics. In addition, a batch-interleaved training strategy
enables leveraging separate ASR and SER datasets without requiring joint
annotation. Experimental results demonstrate that HarmoniFuse improves both ASR
and SER performance, offering a scalable and robust solution for multitask
speech understanding under realistic data constraints.

</details>


### [22] [Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation](https://arxiv.org/abs/2509.18579)
*Runyan Yang,Yuke Si,Yingying Gao,Junlan Feng,Chao Deng,Shilei Zhang*

Main category: eess.AS

TL;DR: 提出统一知识蒸馏框架，将文本教师模型的推理能力迁移到音频学生模型，同时保持音频能力


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在ASR和情感识别等任务表现良好，但在复杂推理方面存在困难，主要由于音频与文本之间的模态差距以及缺乏结构化中间监督

Method: 提出双维度知识蒸馏：源维度蒸馏利用文本和音频教师提供互补的模态特定监督；层维度蒸馏将教师信号与适当的学生层对齐以提高迁移效率

Result: 实验结果显示音频推理性能显著提升

Conclusion: 该框架作为音频建模的推理迁移解决方案具有有效性

Abstract: While large audio language models excel at tasks like ASR and emotion
recognition, they still struggle with complex reasoning due to the modality gap
between audio and text as well as the lack of structured intermediate
supervision. To address this, we propose a unified knowledge distillation
framework to transfer reasoning capabilities from a high-capacity textual
teacher model to a student audio models while preserving its acoustic
competence. Our method introduces two key dimensions: source-wise distillation,
which leverages both textual and acoustic teachers to provide complementary
modality-specific supervision; and layer-wise distillation, which aligns
teacher signals with appropriate student layers to improve transfer efficiency.
This dual-dimensional strategy enables fine-grained control over the
distillation process, effectively bridging the gap between symbolic reasoning
and speech representations. Experimental results show significant improvements
in audio reasoning performance, demonstrating the effectiveness of our
framework as a reasoning transfer solution for audio modeling.

</details>


### [23] [SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering](https://arxiv.org/abs/2509.18603)
*Jiarui Hai,Mounya Elhilali*

Main category: eess.AS

TL;DR: SynSonic是一种针对声音事件检测的数据增强方法，利用文本到音频扩散模型和能量包络ControlNet生成时间一致的声音事件，通过联合分数过滤策略提高样本质量。


<details>
  <summary>Details</summary>
Motivation: 声音事件检测面临时间标注数据稀缺的问题，现有增强方法受限于现有样本多样性，而生成模型直接应用存在缺乏精确时间标注和引入噪声的风险。

Method: 使用文本到音频扩散模型，通过能量包络ControlNet引导生成时间一致的声音事件，采用双分类器的联合分数过滤策略确保样本质量。

Result: 实验结果显示SynSonic提高了多音声音检测分数（PSDS1和PSDS2），改善了时间定位和声音类别区分能力。

Conclusion: SynSonic成功解决了生成模型在声音事件检测中的应用挑战，为数据增强提供了有效解决方案。

Abstract: Data synthesis and augmentation are essential for Sound Event Detection (SED)
due to the scarcity of temporally labeled data. While augmentation methods like
SpecAugment and Mix-up can enhance model performance, they remain constrained
by the diversity of existing samples. Recent generative models offer new
opportunities, yet their direct application to SED is challenging due to the
lack of precise temporal annotations and the risk of introducing noise through
unreliable filtering. To address these challenges and enable generative-based
augmentation for SED, we propose SynSonic, a data augmentation method tailored
for this task. SynSonic leverages text-to-audio diffusion models guided by an
energy-envelope ControlNet to generate temporally coherent sound events. A
joint score filtering strategy with dual classifiers ensures sample quality,
and we explore its practical integration into training pipelines. Experimental
results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1
and PSDS2), enhancing both temporal localization and sound class
discrimination.

</details>


### [24] [FlexSED: Towards Open-Vocabulary Sound Event Detection](https://arxiv.org/abs/2509.18606)
*Jiarui Hai,Helin Wang,Weizhe Guo,Mounya Elhilali*

Main category: eess.AS

TL;DR: FlexSED是一个开放词汇的声音事件检测系统，通过结合预训练音频SSL模型和CLAP文本编码器，引入编码器-解码器结构和自适应融合策略，实现了对自由文本查询的支持，并具备零样本和少样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模声音事件检测系统存在根本性限制：无法处理自由文本查询、缺乏零样本能力、少样本适应性差。虽然已有基于文本查询的分离方法，但它们主要关注源分离，不适合需要精确时间定位和高效检测的SED任务。

Method: 基于预训练音频SSL模型和CLAP文本编码器，引入编码器-解码器组合和自适应融合策略，支持从预训练权重进行有效连续训练。使用大语言模型辅助训练中的事件查询选择，解决缺失标签问题。

Result: 在AudioSet-Strong数据集上优于传统SED模型，同时展现出强大的零样本和少样本能力。

Conclusion: FlexSED实现了开放词汇的声音事件检测，为未来研究和应用提供了支持，代码和预训练模型已发布。

Abstract: Despite recent progress in large-scale sound event detection (SED) systems
capable of handling hundreds of sound classes, existing multi-class
classification frameworks remain fundamentally limited. They cannot process
free-text sound queries, which enable more flexible and user-friendly
interaction, and they lack zero-shot capabilities and offer poor few-shot
adaptability. Although text-query-based separation methods have been explored,
they primarily focus on source separation and are ill-suited for SED tasks that
require precise temporal localization and efficient detection across large and
diverse sound vocabularies. In this paper, we propose FlexSED, an
open-vocabulary sound event detection system. FlexSED builds on a pretrained
audio SSL model and the CLAP text encoder, introducing an encoder-decoder
composition and an adaptive fusion strategy to enable effective continuous
training from pretrained weights. To ensure robust supervision, it also employs
large language models (LLMs) to assist in event query selection during
training, addressing challenges related to missing labels. As a result, FlexSED
achieves superior performance compared to vanilla SED models on
AudioSet-Strong, while demonstrating strong zero-shot and few-shot
capabilities. We release the code and pretrained models to support future
research and applications based on FlexSED.

</details>


### [25] [Group Relative Policy Optimization for Text-to-Speech with Large Language Models](https://arxiv.org/abs/2509.18798)
*Chang Liu,Ya-Jun Hu,Ying-Ying Gao,Shi-Lei Zhang,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: 本文提出了一种基于GRPO的方法，通过使用现成的自动语音识别模型来提升基于大语言模型的文本转语音系统的性能，无需专门的奖励计算模型或训练。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的LLM-TTS方法需要专门的奖励计算模型，这增加了复杂性和成本。本文旨在开发一种更简单有效的方法来提升合成语音的可懂度和自然度。

Method: 使用GRPO微调预训练的LLM-TTS模型，设计了一个结合字符错误率和负对数似然的复合奖励函数，从现成的ASR模型中获取奖励信号。

Result: 实验结果表明，该方法显著提高了合成语音的可懂度和自然度，消融研究证实了两个奖励组件的有效性。

Conclusion: 提出的GRPO方法为LLM-TTS提供了一种简单有效的性能提升方案，无需额外训练专门的奖励模型。

Abstract: This paper proposes a GRPO-based approach to enhance the performance of large
language model (LLM)-based text-to-speech (TTS) models by deriving rewards from
an off-the-shelf automatic speech recognition (ASR) model. Compared to previous
reinforcement learning methods for LLM-based TTS, our method requires no
dedicated model for reward computation or training. Moreover, we design a
composite reward function that combines character error rate (CER) with
negative log-likelihood (NLL) obtained from the ASR model, providing more
informative and accurate reward signals. We apply GRPO fine-tuning to
pre-trained LLM-based TTS models and evaluate their zero-shot TTS performance.
Experimental results show that the proposed method substantially improves both
the intelligibility and naturalness of synthesized speech. Ablation studies and
further analyses confirm the effectiveness of integrating the two reward
components.

</details>


### [26] [Rethinking the joint estimation of magnitude and phase for time-frequency domain neural vocoders](https://arxiv.org/abs/2509.18806)
*Lingling Dai,Andong Li,Tong Lei,Meng Yu,Xiaodong Li,Chengshi Zheng*

Main category: eess.AS

TL;DR: 该论文针对APNet2双流声码器在时频域中联合预测幅度和相位时出现的性能崩溃问题，提出了三种简单有效的策略来稳定其性能。


<details>
  <summary>Details</summary>
Motivation: 时频域神经声码器在合成高保真音频方面表现出色，但联合预测幅度和相位目标的有效机制尚不明确。作者在评估APNet2性能时意外观察到严重的性能崩溃问题，需要找到稳定其性能的方法。

Method: 提出了三种策略：1）在拓扑空间中修改架构拓扑以改善信息交换；2）在源空间中引入先验知识促进生成过程；3）在输出空间中优化反向传播过程并改进输出格式。

Result: 实验结果表明，所提出的方法有效促进了APNet2中幅度和相位的联合估计，从而缩小了单流和双流声码器之间的性能差距。

Conclusion: 通过三个空间维度的优化策略，成功解决了APNet2双流声码器的性能崩溃问题，实现了幅度和相位联合估计的稳定性能。

Abstract: Time-frequency (T-F) domain-based neural vocoders have shown promising
results in synthesizing high-fidelity audio. Nevertheless, it remains unclear
on the mechanism of effectively predicting magnitude and phase targets jointly.
In this paper, we start from two representative T-F domain vocoders, namely
Vocos and APNet2, which belong to the single-stream and dual-stream modes for
magnitude and phase estimation, respectively. When evaluating their performance
on a large-scale dataset, we accidentally observe severe performance collapse
of APNet2. To stabilize its performance, in this paper, we introduce three
simple yet effective strategies, each targeting the topological space, the
source space, and the output space, respectively. Specifically, we modify the
architectural topology for better information exchange in the topological
space, introduce prior knowledge to facilitate the generation process in the
source space, and optimize the backpropagation process for parameter updates
with an improved output format in the output space. Experimental results
demonstrate that our proposed method effectively facilitates the joint
estimation of magnitude and phase in APNet2, thus bridging the performance
disparities between the single-stream and dual-stream vocoders.

</details>


### [27] [Towards Evaluating Generative Audio: Insights from Neural Audio Codec Embedding Distances](https://arxiv.org/abs/2509.18823)
*Arijit Biswas,Lars Villemoes*

Main category: eess.AS

TL;DR: DACe是Descript Audio Codec的增强版，通过平衡采样训练在真实和合成音频数据上，实现了更高保真度的神经音频编解码器。研究发现FAD比MMD在音频质量评估中表现更好，高保真NAC嵌入与人类判断相关性更强。


<details>
  <summary>Details</summary>
Motivation: 研究神经音频编解码器(NACs)在音频质量评估中的双重效用：既可用于低比特率压缩，又可作为感知质量评估的特征。

Method: 系统比较Fréchet Audio Distance (FAD)和Maximum Mean Discrepancy (MMD)在MUSHRA测试中的表现，使用DACe等高质量NAC嵌入进行音频质量评估。

Result: FAD始终优于MMD，高保真NAC嵌入(如DACe)与人类判断有更强相关性。虽然CLAP-M和OpenL3-128M嵌入相关性更高，但NAC嵌入提供实用的零样本音频质量评估方法。

Conclusion: NACs具有压缩和感知音频评估的双重实用性，仅需未编码音频即可训练，为音频质量评估提供了实用解决方案。

Abstract: Neural audio codecs (NACs) achieve low-bitrate compression by learning
compact audio representations, which can also serve as features for perceptual
quality evaluation. We introduce DACe, an enhanced, higher-fidelity version of
the Descript Audio Codec (DAC), trained on diverse real and synthetic tonal
data with balanced sampling. We systematically compare Fr\'echet Audio Distance
(FAD) and Maximum Mean Discrepancy (MMD) on MUSHRA tests across speech, music,
and mixed content. FAD consistently outperforms MMD, and embeddings from
higher-fidelity NACs (such as DACe) show stronger correlations with human
judgments. While CLAP LAION Music (CLAP-M) and OpenL3 Mel128 (OpenL3-128M)
embeddings achieve higher correlations, NAC embeddings provide a practical
zero-shot approach to audio quality assessment, requiring only unencoded audio
for training. These results demonstrate the dual utility of NACs for
compression and perceptually informed audio evaluation.

</details>


### [28] [Influence of Clean Speech Characteristics on Speech Enhancement Performance](https://arxiv.org/abs/2509.18885)
*Mingchi Hou,Ina Kodrasi*

Main category: eess.AS

TL;DR: 本文研究了语音增强性能与干净语音本身内在特性的关系，发现共振峰振幅是预测增强性能的关键因素，高且稳定的共振峰能带来更大的增强增益。


<details>
  <summary>Details</summary>
Motivation: 语音增强性能通常被认为依赖于噪声特性和信噪比，但干净语音本身的内在特性这一因素尚未得到充分探索。

Method: 从干净语音中提取音高、共振峰、响度和频谱通量等特征，计算这些特征与客观语音增强指标（如频率加权分段信噪比和PESQ）的相关性。

Result: 共振峰振幅与语音增强性能具有一致的预测性，高且稳定的共振峰能带来更大的增强增益。即使在同一个说话人的不同语句中，性能也有显著差异。

Conclusion: 内在语音特性应在设计数据集、评估协议和增强模型时予以考虑，这些发现为语音增强挑战提供了新的见解。

Abstract: Speech enhancement (SE) performance is known to depend on noise
characteristics and signal to noise ratio (SNR), yet intrinsic properties of
the clean speech signal itself remain an underexplored factor. In this work, we
systematically analyze how clean speech characteristics influence enhancement
difficulty across multiple state of the art SE models, languages, and noise
conditions. We extract a set of pitch, formant, loudness, and spectral flux
features from clean speech and compute correlations with objective SE metrics,
including frequency weighted segmental SNR and PESQ. Our results show that
formant amplitudes are consistently predictive of SE performance, with higher
and more stable formants leading to larger enhancement gains. We further
demonstrate that performance varies substantially even within a single
speaker's utterances, highlighting the importance of intraspeaker acoustic
variability. These findings provide new insights into SE challenges, suggesting
that intrinsic speech characteristics should be considered when designing
datasets, evaluation protocols, and enhancement models.

</details>


### [29] [Generalizability of Predictive and Generative Speech Enhancement Models to Pathological Speakers](https://arxiv.org/abs/2509.18890)
*Mingchi Hou,Ante Jukic,Ina Kodrasi*

Main category: eess.AS

TL;DR: 本文研究了针对病理性语音的语音增强方法，比较了从头训练、微调和个性化三种策略，发现多说话者微调效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的语音增强模型在神经典型语音上表现良好，但对病理性语音的效果显著下降，需要解决这一性能差距。

Method: 研究了三种策略：i) 使用病理性数据从头训练模型，ii) 在神经典型语音预训练模型基础上使用病理性数据进行微调，iii) 仅使用单个病理性测试说话者数据进行个性化训练。

Result: 尽管病理性语音数据集规模有限，但语音增强模型仍可成功训练或微调。多说话者微调带来最大性能提升，而个性化训练效果较差，可能因为每个说话者的数据量太少。

Conclusion: 研究结果突出了改善病理性说话者语音增强性能的挑战和潜在策略，多说话者微调是最有效的方法。

Abstract: State of the art speech enhancement (SE) models achieve strong performance on
neurotypical speech, but their effectiveness is substantially reduced for
pathological speech. In this paper, we investigate strategies to address this
gap for both predictive and generative SE models, including i) training models
from scratch using pathological data, ii) finetuning models pretrained on
neurotypical speech with additional data from pathological speakers, and iii)
speaker specific personalization using only data from the individual
pathological test speaker. Our results show that, despite the limited size of
pathological speech datasets, SE models can be successfully trained or
finetuned on such data. Finetuning models with data from several pathological
speakers yields the largest performance improvements, while speaker specific
personalization is less effective, likely due to the small amount of data
available per speaker. These findings highlight the challenges and potential
strategies for improving SE performance for pathological speakers.

</details>


### [30] [Direct Preference Optimization for Speech Autoregressive Diffusion Models](https://arxiv.org/abs/2509.18928)
*Zhijun Liu,Dongya Jia,Xiaoqiang Wang,Chenpeng Du,Shuai Wang,Zhuo Chen,Haizhou Li*

Main category: eess.AS

TL;DR: 本文提出ARDM-DPO方法，通过DPO微调自回归扩散模型，在零样本文本转语音任务中显著提升语音表现力和长文本鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自回归扩散模型在语音生成中取得了SOTA性能，但基于强化学习的微调研究仍然有限，需要探索如何进一步提升模型性能。

Method: 提出Autoregressive Diffusion-Direct Preference Optimization (ARDM-DPO)方法，对DiTAR模型进行DPO微调。

Result: 实验结果显示，该方法在语音表达性和长文本鲁棒性方面取得了显著改进。

Conclusion: ARDM-DPO为自回归扩散模型的强化学习微调提供了有效方案，能够显著提升语音生成质量。

Abstract: Autoregressive diffusion models (ARDMs) have recently been applied to speech
generation, achieving state-of-the-art (SOTA) performance in zero-shot
text-to-speech. By autoregressively generating continuous speech tokens with
next-token diffusion, these models offer a promising alternative to next-token
prediction, avoiding the technical complexities associated with discrete speech
tokenization. As a relatively new paradigm, research on reinforcement learning
(RL)-based fine-tuning of speech ARDMs remains limited. In this paper, we
propose Autoregressive Diffusion-Direct Preference Optimization (ARDM-DPO) to
advance this research. By fine-tuning the recently proposed zero-shot
text-to-speech model DiTAR with DPO, we achieve significant improvements in
terms of speech expressiveness and robustness for long texts.

</details>


### [31] [HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS](https://arxiv.org/abs/2509.19001)
*Sihang Nie,Xiaofen Xing,Jingyuan Xing,Baiji Liu,Xiangmin Xu*

Main category: eess.AS

TL;DR: HD-PPT是一个基于大语言模型的文本转语音框架，通过分层解码策略解决语音合成中的精细控制问题，将语音合成转化为结构化任务，显著提升了指令遵循能力和自然度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于指令的文本转语音模型由于单层次文本指令与多层次语音标记之间的模态差距，缺乏精细控制能力。为了解决这一局限性，需要开发能够实现精确可控语音合成的新方法。

Method: 提出HD-PPT框架，引入新型语音编解码器从复杂语音标记中提取不同的提示偏好和内容偏好标记，采用分层解码策略：LLM按结构化顺序生成标记——先语义，再细粒度风格，最后完整声学表示。

Result: 大量实验表明，这种分层范式显著提高了指令遵循能力，并达到了最先进的自然度水平。

Conclusion: HD-PPT验证了通过结构化分层方法实现精确可控语音合成的有效性，为语音合成的精细控制提供了新的解决方案。

Abstract: Large Language Model (LLM)-based Text-to-Speech (TTS) models have already
reached a high degree of naturalness. However, the precision control of TTS
inference is still challenging. Although instruction-based Text-to-Speech
(Instruct-TTS) models are proposed, these models still lack fine-grained
control due to the modality gap between single-level text instructions and
multilevel speech tokens. To address this limitation, we propose HD-PPT, a
framework that transforms speech synthesis into a structured, hierarchical
task. To enable fine-grained control, we introduce a novel speech codec to
extract distinct prompt-preference and content-preference tokens from the
complex speech tokens, supervised by automatic speech recognition (ASR) and
cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality
gap of these tokens, we propose a hierarchical decoding strategy, where the LLM
generates tokens in a structured order: first semantic, then fine-grained
style, and finally complete acoustic representation. Extensive experiments
demonstrate that this hierarchical paradigm significantly improves instruction
adherence and achieves state-of-the-art naturalness, validating our approach
for precise and controllable speech synthesis. Audio samples are available at
https://xxh333.github.io/.

</details>


### [32] [Enhancing Noise Robustness for Neural Speech Codecs through Resource-Efficient Progressive Quantization Perturbation Simulation](https://arxiv.org/abs/2509.19025)
*Rui-Chen Zheng,Yang Ai,Hui-Peng Du,Zhen-Hua Ling*

Main category: eess.AS

TL;DR: 提出一种资源高效的训练策略，通过直接在量化层面模拟噪声扰动来增强语音编解码器的噪声鲁棒性，无需成对的噪声-干净数据。


<details>
  <summary>Details</summary>
Motivation: 现实场景中背景噪声不可避免，轻微输入噪声扰动会导致量化码字偏移，从而降低重建语音质量。

Method: 提出两种核心机制：1）距离加权的概率性top-K采样策略替代传统确定性最近邻选择；2）从最后一个到第一个量化器的渐进式训练方案。仅使用干净语音训练。

Result: 在Encodec和WavTokenizer上的实验表明，该方法显著提高了噪声条件下的鲁棒性（如Encodec在15dB SNR下UTMOS从3.475提升到3.586），同时提升了干净语音的编码质量。

Conclusion: 该方法有效解决了语音编解码器的噪声鲁棒性问题，具有实际部署价值。

Abstract: Noise robustness remains a critical challenge for deploying neural speech
codecs in real-world acoustic scenarios where background noise is often
inevitable. A key observation we make is that even slight input noise
perturbations can cause unintended shifts in quantized codewords, thereby
degrading the quality of reconstructed speech. Motivated by this finding, we
propose a novel and resource-efficient training strategy to enhance the noise
robustness of speech codecs by simulating such perturbations directly at the
quantization level. Our approach introduces two core mechanisms: (1) a
distance-weighted probabilistic top-K sampling strategy that replaces the
conventional deterministic nearest-neighbor selection in residual vector
quantization (RVQ); and (2) a progressive training scheme that introduces
perturbations from the last to the first quantizer in a controlled manner.
Crucially, our method is trained exclusively on clean speech, eliminating the
need for any paired noisy-clean data. Experiments on two advanced neural speech
codecs, Encodec and WavTokenizer, demonstrate that the proposed strategy
substantially improves robustness under noisy conditions-for example, boosting
UTMOS from 3.475 to 3.586 at 15 dB SNR on Encodec-while also enhancing coding
quality for clean speech.

</details>


### [33] [Training Flow Matching Models with Reliable Labels via Self-Purification](https://arxiv.org/abs/2509.19091)
*Hyeongju Kim,Yechan Yu,June Young Yi,Juheon Lee*

Main category: eess.AS

TL;DR: 本文提出了一种名为自净化流匹配（SPFM）的方法，用于在流匹配框架内过滤不可靠数据，解决训练数据中标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 训练数据集通常存在标签错误和噪声，这些不完美的标签会显著降低训练模型的性能。传统方法需要预训练模型或额外模块来识别噪声数据，而SPFM旨在在训练过程中直接利用模型自身来识别可疑数据。

Method: SPFM在流匹配框架内，通过模型自身在训练过程中识别可疑数据，无需预训练模型或额外模块。该方法能够自动过滤不可靠数据，提高模型对噪声标签的鲁棒性。

Result: 实验表明，使用SPFM训练的模型即使在噪声标签下也能生成符合条件约束的样本。在TITW数据集（包含野外语音数据）上的验证显示，SPFM的性能超越了现有基线方法。

Conclusion: SPFM提供了一种原则性的方法来处理训练数据中的标签噪声问题，通过模型自净化机制有效提升了模型在噪声环境下的性能表现。

Abstract: Training datasets are inherently imperfect, often containing mislabeled
samples due to human annotation errors, limitations of tagging models, and
other sources of noise. Such label contamination can significantly degrade the
performance of a trained model. In this work, we introduce Self-Purifying Flow
Matching (SPFM), a principled approach to filtering unreliable data within the
flow-matching framework. SPFM identifies suspicious data using the model itself
during the training process, bypassing the need for pretrained models or
additional modules. Our experiments demonstrate that models trained with SPFM
generate samples that accurately adhere to the specified conditioning, even
when trained on noisy labels. Furthermore, we validate the robustness of SPFM
on the TITW dataset, which consists of in-the-wild speech data, achieving
performance that surpasses existing baselines.

</details>


### [34] [On-device Internet of Sounds Sonification with Wavetable Synthesis Techniques for Soil Moisture Monitoring in Water Scarcity Contexts](https://arxiv.org/abs/2509.19097)
*Stephen Roddy*

Main category: eess.AS

TL;DR: 本文提出了一种在物联网声音网络设备层面实现土壤湿度监测的声化方法，使用波表合成技术将传感器数据映射到声学参数。


<details>
  <summary>Details</summary>
Motivation: 随着全球水资源短缺问题日益严重，需要新的方法来监测土壤湿度。传统的声化方法主要在网络应用和服务层面运行，本文探索在设备层面实现声化的可能性。

Method: 采用波表合成技术，在设备层面实现声化，将土壤湿度传感器数据直接映射到声学参数，并提出了设备端波表声化的形式化方法。

Result: 开发了一个原型实现，验证了设备层面声化在土壤湿度监测任务中的可行性。

Conclusion: 设备层面的声化为物联网声音网络中的数据表示和通信提供了新的可能性，特别是在环境监测应用中具有实用价值。

Abstract: Sonification, the mapping of data to sound to communicate information about
the original data source, is becoming a viable strategy for the sonic
representation and communication of information derived from the complex flows
of data exchanged across Internet of Sounds (IoS) networks. This paper presents
an IoS sonification implementation for monitoring soil moisture levels within
the broader context of the globally increasing water scarcity. While previous
work has focused on sonifications operating on the applications and services
level of the IoS network infrastructure, this paper explores device-level
sonification using wavetable synthesis techniques to map sensor data to
acoustic parameters. An approach to on-device wavetable sonification is
formalized, and a prototype implementation is presented and explored before the
approach is contextualised with regard to the soil moisture monitoring tasks.

</details>


### [35] [Improving Test-Time Performance of RVQ-based Neural Codecs](https://arxiv.org/abs/2509.19186)
*Hyeongju Kim,Junhyeok Lee,Jacob Morton,Juheon Lee,Jinhyeok Yang*

Main category: eess.AS

TL;DR: 提出一种编码算法来提升基于残差向量量化的神经音频编解码器在测试时的合成质量，通过选择不同的码本集合来降低量化误差


<details>
  <summary>Details</summary>
Motivation: 传统方法生成的量化向量存在次优问题，量化误差可以通过选择不同的码本集合来缓解

Method: 设计编码算法来识别能够实现更低量化误差的离散码本集合，并将该方法应用于预训练模型

Result: 实验验证该方法不仅能减少量化误差，还能提高合成质量

Conclusion: 提出的编码算法有效提升了RVQ基神经音频编解码器的性能

Abstract: The residual vector quantization (RVQ) technique plays a central role in
recent advances in neural audio codecs. These models effectively synthesize
high-fidelity audio from a limited number of codes due to the hierarchical
structure among quantization levels. In this paper, we propose an encoding
algorithm to further enhance the synthesis quality of RVQ-based neural codecs
at test-time. Firstly, we point out the suboptimal nature of quantized vectors
generated by conventional methods. We demonstrate that quantization error can
be mitigated by selecting a different set of codes. Subsequently, we present
our encoding algorithm, designed to identify a set of discrete codes that
achieve a lower quantization error. We then apply the proposed method to
pre-trained models and evaluate its efficacy using diverse metrics. Our
experimental findings validate that our method not only reduces quantization
errors, but also improves synthesis quality.

</details>


### [36] [MUSHRA-1S: A scalable and sensitive test approach for evaluating top-tier speech processing systems](https://arxiv.org/abs/2509.19219)
*Laura Lechler,Ivana Balic*

Main category: eess.AS

TL;DR: MUSHRA 1S是一种新的语音系统评估方法，结合了标准MUSHRA的敏感性和ACR的可扩展性，通过单刺激评估方式解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的语音系统需要可扩展且敏感的评价方法来检测细微但不可接受的伪影。标准MUSHRA方法敏感但缺乏可扩展性，而ACR方法可扩展性好但在高质量区域会饱和且失去敏感性。

Method: 提出MUSHRA 1S方法，这是一种单刺激变体，每次评估一个系统，与固定的锚点和参考进行比较。该方法固定上下文，减少范围均衡偏差。

Result: 实验表明，MUSHRA 1S在匹配标准MUSHRA方面比ACR更接近，包括在ACR会饱和的高质量区域。MUSHRA 1S还能有效识别特定偏差。

Conclusion: MUSHRA 1S结合了MUSHRA级别的敏感性和ACR类似的可扩展性，为顶级语音处理系统的基准测试提供了一个稳健且可扩展的解决方案。

Abstract: Evaluating state-of-the-art speech systems necessitates scalable and
sensitive evaluation methods to detect subtle but unacceptable artifacts.
Standard MUSHRA is sensitive but lacks scalability, while ACR scales well but
loses sensitivity and saturates at a high quality. To address this, we
introduce MUSHRA 1S, a single-stimulus variant that rates one system at a time
against a fixed anchor and reference. Across our experiments, MUSHRA 1S matches
standard MUSHRA more closely than ACR, including in the high-quality regime,
where ACR saturates. MUSHRA 1S also effectively identifies specific deviations
and reduces range-equalizing biases by fixing context. Overall, MUSHRA 1S
combines MUSHRA level sensitivity with ACR like scalability, making it a robust
and scalable solution for benchmarking top-tier speech processing systems.

</details>


### [37] [Audio-Based Pedestrian Detection in the Presence of Vehicular Noise](https://arxiv.org/abs/2509.19295)
*Yonghyun Kim,Chaeyeon Han,Akash Sarode,Noah Posner,Subhrajit Guhathakurta,Alexander Lerch*

Main category: eess.AS

TL;DR: 本文提出了一个包含车辆噪声的音频行人检测新数据集，并分析了现有方法在噪声环境下的表现，包括跨数据集评估、噪声数据影响分析以及模型对域外声音的鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: 现有的音频行人检测研究主要在噪声受限环境下进行，缺乏对真实交通噪声环境的探索。本文旨在填补这一空白，研究在车辆噪声存在下的行人检测问题。

Method: 构建了一个1321小时的路边数据集，包含丰富的交通声景，每个录音包含16kHz音频、帧级行人标注和1fps视频缩略图。进行了三项分析：跨数据集评估、噪声数据影响评估和域外声音鲁棒性评估。

Result: 提出了一个全面的音频行人检测数据集，为研究噪声环境下的检测性能提供了基础。

Conclusion: 该研究为音频行人检测在真实噪声环境中的应用提供了重要数据集和分析框架，推动了该领域的发展。

Abstract: Audio-based pedestrian detection is a challenging task and has, thus far,
only been explored in noise-limited environments. We present a new dataset,
results, and a detailed analysis of the state-of-the-art in audio-based
pedestrian detection in the presence of vehicular noise. In our study, we
conduct three analyses: (i) cross-dataset evaluation between noisy and
noise-limited environments, (ii) an assessment of the impact of noisy data on
model performance, highlighting the influence of acoustic context, and (iii) an
evaluation of the model's predictive robustness on out-of-domain sounds. The
new dataset is a comprehensive 1321-hour roadside dataset. It incorporates
traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with
frame-level pedestrian annotations and 1fps video thumbnails.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [38] [XMUspeech Systems for the ASVspoof 5 Challenge](https://arxiv.org/abs/2509.18102)
*Wangjie Li,Xingjia Xie,Yishuang Li,Wenhao Guan,Kaidi Wang,Pengyu Ren,Lin Li,Qingyang Hong*

Main category: cs.SD

TL;DR: 本文介绍了提交到ASVspoof 5挑战赛语音深度伪造检测赛道的XMUspeech系统，通过多尺度特征融合和自监督学习等方法显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: ASVspoof 5数据库的音频时长显著增加，仅调整输入音频长度就能大幅改善系统性能，因此需要开发更有效的检测方法来应对更长的音频样本。

Method: 探索了AASIST、HM-Conformer、Hubert和Wav2vec2等模型，使用包含伪造语音的数据集训练自监督模型作为特征提取器，采用自适应多尺度特征融合(AMFF)方法整合多Transformer层特征与手工特征，并在一类损失函数上进行了广泛实验。

Result: 融合系统在封闭条件下达到minDCF 0.4783和EER 20.45%，在开放条件下达到minDCF 0.2245和EER 9.36%。

Conclusion: 所提出的多尺度特征融合和自监督学习方法有效提升了语音深度伪造检测性能，特别是在处理更长音频样本时表现出色。

Abstract: In this paper, we present our submitted XMUspeech systems to the speech
deepfake detection track of the ASVspoof 5 Challenge. Compared to previous
challenges, the audio duration in ASVspoof 5 database has significantly
increased. And we observed that merely adjusting the input audio length can
substantially improve system performance. To capture artifacts at multiple
levels, we explored the performance of AASIST, HM-Conformer, Hubert, and
Wav2vec2 with various input features and loss functions. Specifically, in order
to obtain artifact-related information, we trained self-supervised models on
the dataset containing spoofing utterances as the feature extractors. And we
applied an adaptive multi-scale feature fusion (AMFF) method to integrate
features from multiple Transformer layers with the hand-crafted feature to
enhance the detection capability. In addition, we conducted extensive
experiments on one-class loss functions and provided optimized configurations
to better align with the anti-spoofing task. Our fusion system achieved a
minDCF of 0.4783 and an EER of 20.45% in the closed condition, and a minDCF of
0.2245 and an EER of 9.36% in the open condition.

</details>


### [39] [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
*Jialong Mai,Jinxin Ji,Xiaofen Xing,Chen Yang,Weidong Chen,Jingyuan Xing,Xiangmin Xu*

Main category: cs.SD

TL;DR: 该论文介绍了MNV-17数据集，这是一个7.55小时的中文表演性语音数据集，专门用于解决ASR系统在识别非语言发声方面的不足，包含17个平衡的非语言发声类别。


<details>
  <summary>Details</summary>
Motivation: 主流ASR系统擅长转录词汇内容，但无法识别嵌入语音中的非语言发声（如叹息、笑声、咳嗽），这些发声传递重要的情感和意图线索。现有研究受限于缺乏高质量标注数据集。

Method: 创建MNV-17数据集，采用表演性语音确保高质量、清晰发音的非语言发声实例。在四种主流ASR架构上对数据集进行基准测试，评估语义转录和非语言发声分类的联合性能。

Result: MNV-17提供了最广泛的非语言发声类别集合，包含17个不同且平衡的常见非语言发声类别。数据集和预训练模型检查点将公开提供。

Conclusion: MNV-17数据集将促进表达性ASR的未来研究，为解决ASR系统在非语言发声识别方面的局限性提供了重要资源。

Abstract: Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing
lexical content, but largely fail to recognize nonverbal vocalizations (NVs)
embedded in speech, such as sighs, laughs, and coughs. This capability is
important for a comprehensive understanding of human communication, as NVs
convey crucial emotional and intentional cues. Progress in NV-aware ASR has
been hindered by the lack of high-quality, well-annotated datasets. To address
this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech
dataset. Unlike most existing corpora that rely on model-based detection,
MNV-17's performative nature ensures high-fidelity, clearly articulated NV
instances. To the best of our knowledge, MNV-17 provides the most extensive set
of nonverbal vocalization categories, comprising 17 distinct and well-balanced
classes of common NVs. We benchmarked MNV-17 on four mainstream ASR
architectures, evaluating their joint performance on semantic transcription and
NV classification. The dataset and the pretrained model checkpoints will be
made publicly available to facilitate future research in expressive ASR.

</details>


### [40] [StereoFoley: Object-Aware Stereo Audio Generation from Video](https://arxiv.org/abs/2509.18272)
*Tornike Karchkhadze,Kuan-Lin Chen,Mojtaba,Heydari,Robert Henzel,Alessandro Toso,Mehrez Souden,Joshua Atkins*

Main category: cs.SD

TL;DR: StereoFoley是一个视频到音频生成框架，能够产生语义对齐、时间同步且空间准确的48kHz立体声音频。该框架通过合成数据生成管道和微调技术，解决了现有模型在立体声成像方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到音频生成模型虽然在语义和时间保真度上表现良好，但大多局限于单声道或无法实现物体感知的立体声成像，主要受限于缺乏专业混合的空间准确数据集。

Method: 首先训练一个基础模型生成立体声音频，然后开发合成数据生成管道（结合视频分析、物体跟踪、音频合成与动态声像定位和距离响度控制），最后在合成数据上微调基础模型。

Result: 该框架在语义准确性和同步性方面达到最先进水平，并通过人类听力研究验证了立体声物体感知度与感知的强相关性。

Conclusion: 这项工作建立了首个端到端的立体声物体感知视频到音频生成框架，填补了关键空白并为该领域设立了新基准。

Abstract: We present StereoFoley, a video-to-audio generation framework that produces
semantically aligned, temporally synchronized, and spatially accurate stereo
sound at 48 kHz. While recent generative video-to-audio models achieve strong
semantic and temporal fidelity, they largely remain limited to mono or fail to
deliver object-aware stereo imaging, constrained by the lack of professionally
mixed, spatially accurate video-to-audio datasets. First, we develop and train
a base model that generates stereo audio from video, achieving state-of-the-art
in both semantic accuracy and synchronization. Next, to overcome dataset
limitations, we introduce a synthetic data generation pipeline that combines
video analysis, object tracking, and audio synthesis with dynamic panning and
distance-based loudness controls, enabling spatially accurate object-aware
sound. Finally, we fine-tune the base model on this synthetic dataset, yielding
clear object-audio correspondence. Since no established metrics exist, we
introduce stereo object-awareness measures and validate it through a human
listening study, showing strong correlation with perception. This work
establishes the first end-to-end framework for stereo object-aware
video-to-audio generation, addressing a critical gap and setting a new
benchmark in the field.

</details>


### [41] [A Dimensional Approach to Canine Bark Analysis for Assistance Dog Seizure Signaling](https://arxiv.org/abs/2509.18375)
*Hailin Song,Shelley Brady,Tomás Ward,Alan F. Smeaton*

Main category: cs.SD

TL;DR: 该论文将犬类发声分类问题重新定义为在唤醒-效价二维空间中的连续回归任务，提出了一种调整后的孪生网络方法，在数据稀疏和伦理受限的情况下有效分析犬吠声。


<details>
  <summary>Details</summary>
Motivation: 传统犬类发声分类方法在辅助犬场景中存在严重限制，因为样本数据稀疏且在不同犬只间变化大，且由于伦理约束无法捕获完整的吠叫类型范围。

Method: 使用调整后的孪生网络，不是基于二元相似性训练，而是基于输入样本对之间的序数和数值距离进行训练，在唤醒-效价二维空间中进行连续回归。

Result: 在公共数据集上训练后，该模型在具有挑战性的效价维度上将Turn-around Percentage降低了高达50%，相比回归基线有显著改进。在真实世界数据集上的定性验证确认学习到的空间具有语义意义。

Conclusion: 该方法为在严重数据限制下分析犬吠声建立了概念验证，证明了在唤醒-效价空间中连续回归方法的有效性。

Abstract: Standard classification of canine vocalisations is severely limited for
assistance dogs, where sample data is sparse and variable across dogs and where
capture of the full range of bark types is ethically constrained. We reframe
this problem as a continuous regression task within a two-dimensional
arousal-valence space. Central to our approach is an adjusted Siamese Network
trained not on binary similarity, but on the ordinal and numeric distance
between input sample pairs. Trained on a public dataset, our model reduces
Turn-around Percentage by up to 50% on the challenging valence dimension
compared to a regression baseline. Qualitative validation on a real-world
dataset confirms the learned space is semantically meaningful, establishing a
proof-of-concept for analysing canine barking under severe data limitations.

</details>


### [42] [Identifying birdsong syllables without labelled data](https://arxiv.org/abs/2509.18412)
*Mélisande Teng,Julien Boussard,David Rolnick,Hugo Larochelle*

Main category: cs.SD

TL;DR: 本文提出了首个完全无监督的算法，用于将鸟鸣录音分解为音节序列，无需人工标注数据即可实现高精度的鸟鸣分解和个体识别。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家手动标注长音频数据，限制了在少数物种和数据集上的应用。需要开发无监督算法来扩大适用范围，解决鸟个体识别和动物通信理解等挑战。

Method: 采用三阶段方法：首先检测音节事件，然后聚类提取音节模板，最后通过匹配追踪将录音分解为音节序列。

Result: 在Bengalese finch歌曲数据集上，无监督方法表现优异，能够准确区分同一物种内的不同个体，并在great tit物种上也验证了有效性。

Conclusion: 该无监督算法为鸟类通信研究和个体识别提供了可扩展的解决方案，突破了传统方法对标注数据的依赖。

Abstract: Identifying sequences of syllables within birdsongs is key to tackling a wide
array of challenges, including bird individual identification and better
understanding of animal communication and sensory-motor learning. Recently,
machine learning approaches have demonstrated great potential to alleviate the
need for experts to label long audio recordings by hand. However, they still
typically rely on the availability of labelled data for model training,
restricting applicability to a few species and datasets. In this work, we build
the first fully unsupervised algorithm to decompose birdsong recordings into
sequences of syllables. We first detect syllable events, then cluster them to
extract templates --syllable representations-- before performing matching
pursuit to decompose the recording as a sequence of syllables. We evaluate our
automatic annotations against human labels on a dataset of Bengalese finch
songs and find that our unsupervised method achieves high performance. We also
demonstrate that our approach can distinguish individual birds within a species
through their unique vocal signatures, for both Bengalese finches and another
species, the great tit.

</details>


### [43] [Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection](https://arxiv.org/abs/2509.18424)
*Rami Zewail*

Main category: cs.SD

TL;DR: 该研究提出了一种轻量级的散射变换器架构，用于心脏杂音检测，无需训练即可达到与现有先进方法相竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 针对心脏听诊自动化中训练数据有限的问题，以及现有预训练音频基础模型计算量大的挑战，需要开发轻量级替代方案。

Method: 结合标准小波散射网络和类Transformer架构，引入上下文依赖性，无需反向传播训练。

Result: 在CirCor DigiScope数据集上，加权准确率达到0.786，未加权平均召回率达到0.697，性能与当前最先进方法相当。

Conclusion: 散射变换器在资源受限环境下是一种可行且有前景的替代方案。

Abstract: In an attempt to address the need for skilled clinicians in heart sound
interpretation, recent research efforts on automating cardiac auscultation have
explored deep learning approaches. The majority of these approaches have been
based on supervised learning that is always challenged in occasions where
training data is limited. More recently, there has been a growing interest in
potentials of pre-trained self-supervised audio foundation models for
biomedical end tasks. Despite exhibiting promising results, these foundational
models are typically computationally intensive. Within the context of automatic
cardiac auscultation, this study explores a lightweight alternative to these
general-purpose audio foundation models by introducing the Scattering
Transformer, a novel, training-free transformer architecture for heart murmur
detection. The proposed method leverages standard wavelet scattering networks
by introducing contextual dependencies in a transformer-like architecture
without any backpropagation. We evaluate our approach on the public CirCor
DigiScope dataset, directly comparing it against leading general-purpose
foundational models. The Scattering Transformer achieves a Weighted
Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,
demonstrating performance highly competitive with contemporary state of the art
methods. This study establishes the Scattering Transformer as a viable and
promising alternative in resource-constrained setups.

</details>


### [44] [Explore the Reinforcement Learning for the LLM based ASR and TTS system](https://arxiv.org/abs/2509.18569)
*Changfeng Gao,Yabin Li,Keyu An,Zhifu Gao,Zhihao Du,Han Zhao,Xiangang Li*

Main category: cs.SD

TL;DR: 提出了一种轻量级强化学习框架，用于音频基础的大语言模型，在自动语音识别和文本转语音任务中验证了强化学习的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在文本任务中显著提升了LLM性能，但由于音频模型训练的复杂性，其在ASR和TTS中的应用仍未被充分探索。

Method: 为音频LLM设计轻量级RL框架，支持音频输入和输出。在ASR任务中使用GRPO框架和基于规则的奖励函数，在TTS任务中比较GRPO和DiffRO方法，并组合两种方法。

Result: 实验表明，即使使用有限训练数据和少量优化步骤，RL也能显著提升ASR和TTS系统的性能。

Conclusion: 强化学习可以有效增强音频基础LLM在ASR和TTS任务中的表现，为音频处理领域提供了新的优化途径。

Abstract: In recent years, large language models (LLMs) have played an important role
in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While
reinforcement learning (RL) has significantly enhanced LLM performance in
text-based tasks, its application to ASR and TTS remains underexplored due to
the complexity of training audio-based models. In this study, we propose a
lightweight RL framework tailored for audio-based LLMs that can process audio
inputs and generate audio outputs. Based on this framework, we evaluate the
effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR
task, we experiment with different rule-based reward functions within the Group
Relative Policy Optimization (GRPO) framework and investigate the impact of RL
data construction. For the TTS task, we compare GRPO with Differentiable Reward
Optimization (DiffRO) and further combine the two approaches to achieve
improved performance. Our experiments demonstrate that RL can significantly
enhance the performance of both ASR and TTS systems, even with limited training
data and a small number of optimization steps.

</details>


### [45] [Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation](https://arxiv.org/abs/2509.18620)
*Aditya Bhattacharjee,Marco Pasini,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 提出了一种无需音频的音频指纹合成方法，通过训练Rectified Flow模型在预训练神经音频指纹系统提取的嵌入上，生成近似真实指纹分布的合成指纹，用于大规模检索性能评估。


<details>
  <summary>Details</summary>
Motivation: 由于大型公共音乐数据库稀缺，限制了音频指纹在现实规模下的评估。需要一种无需额外音频即可模拟大规模检索性能的方法。

Method: 使用预训练神经音频指纹系统提取嵌入，训练Rectified Flow模型生成合成指纹。通过将合成指纹作为干扰项添加到真实参考数据库中，评估检索性能。

Result: 合成指纹的分布与真实数据相似，使用合成干扰项获得的扩展趋势与使用真实干扰项的结果高度一致。能够模拟超大规模数据库的检索性能。

Conclusion: 该方法提供了一种不依赖音频语料库的实用系统可扩展性度量，能够有效评估音频指纹系统在大规模场景下的性能。

Abstract: The evaluation of audio fingerprinting at a realistic scale is limited by the
scarcity of large public music databases. We present an audio-free approach
that synthesises latent fingerprints which approximate the distribution of real
fingerprints. Our method trains a Rectified Flow model on embeddings extracted
by pre-trained neural audio fingerprinting systems. The synthetic fingerprints
generated using our system act as realistic distractors and enable the
simulation of retrieval performance at a large scale without requiring
additional audio. We assess the fidelity of synthetic fingerprints by comparing
the distributions to real data. We further benchmark the retrieval performances
across multiple state-of-the-art audio fingerprinting frameworks by augmenting
real reference databases with synthetic distractors, and show that the scaling
trends obtained with synthetic distractors closely track those obtained with
real distractors. Finally, we scale the synthetic distractor database to model
retrieval performance for very large databases, providing a practical metric of
system scalability that does not depend on access to audio corpora.

</details>


### [46] [An overview of neural architectures for self-supervised audio representation learning from masked spectrograms](https://arxiv.org/abs/2509.18691)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 本文提供了掩码频谱建模与Mamba、xLSTM等新型序列建模架构的综合概述，并在统一框架下比较了基于Transformer、Mamba和xLSTM的掩码频谱模型在十个音频分类任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对掩码频谱建模与新兴序列建模架构（如Mamba和xLSTM）交叉领域的全面综述，需要为研究者提供该领域的系统概述和实证比较。

Method: 采用统一可复现的框架，在十个不同的下游音频分类任务上系统比较基于Transformer、Mamba和xLSTM的掩码频谱模型。

Result: 通过实证比较展示了不同架构在音频分类任务上的性能表现，为读者选择适合特定应用的模型提供参考依据。

Conclusion: 该研究填补了掩码频谱建模与新型序列建模架构交叉领域的综述空白，为音频基础模型的发展提供了有价值的比较分析。

Abstract: In recent years, self-supervised learning has amassed significant interest
for training deep neural representations without labeled data. One such
self-supervised learning approach is masked spectrogram modeling, where the
objective is to learn semantically rich contextual representations by
predicting removed or hidden portions of the input audio spectrogram. With the
Transformer neural architecture at its core, masked spectrogram modeling has
emerged as the prominent approach for learning general purpose audio
representations, a.k.a. audio foundation models. Meanwhile, addressing the
issues of the Transformer architecture, in particular the underlying Scaled
Dot-product Attention operation, which scales quadratically with input sequence
length, has led to renewed interest in recurrent sequence modeling approaches.
Among them, Selective structured state space models (such as Mamba) and
extended Long Short-Term Memory (xLSTM) are the two most promising approaches
which have experienced widespread adoption. While the body of work on these two
topics continues to grow, there is currently a lack of an adequate overview
encompassing the intersection of these topics. In this paper, we present a
comprehensive overview of the aforementioned research domains, covering masked
spectrogram modeling and the previously mentioned neural sequence modeling
architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and
xLSTM based masked spectrogram models in a unified, reproducible framework on
ten diverse downstream audio classification tasks, which will help interested
readers to make informed decisions regarding suitability of the evaluated
approaches to adjacent applications.

</details>


### [47] [Enhancing Automatic Chord Recognition through LLM Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.18700)
*Chih-Cheng Chang,Bo-Yu Chen,Lu-Rong Chen,Li Su*

Main category: cs.SD

TL;DR: 本文提出了一种使用大型语言模型（LLMs）作为集成桥梁来协调多个音乐信息检索（MIR）工具的新方法，专注于提升自动和弦识别性能。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习在音乐信息检索领域的进展，探索如何利用LLMs作为集成工具，连接和整合多个MIR工具的信息，以解决和弦识别任务中的挑战。

Method: 设计了一个5阶段思维链框架，将音频提取的音乐信息转换为文本表示，使GPT-4o能够系统分析、比较和精炼和弦识别结果，利用音乐理论知识整合不同MIR组件的信息。

Result: 在三个数据集上的实验评估显示，在多个评估指标上取得一致改进，MIREX指标上的整体准确率提升了1-2.77%。

Conclusion: LLMs可以有效地作为MIR流程中的集成桥梁，为音乐信息检索任务中的多工具协调开辟了新方向。

Abstract: Music Information Retrieval (MIR) encompasses a broad range of computational
techniques for analyzing and understanding musical content, with recent deep
learning advances driving substantial improvements. Building upon these
advances, this paper explores how large language models (LLMs) can serve as an
integrative bridge to connect and integrate information from multiple MIR
tools, with a focus on enhancing automatic chord recognition performance. We
present a novel approach that positions text-based LLMs as intelligent
coordinators that process and integrate outputs from diverse state-of-the-art
MIR tools-including music source separation, key detection, chord recognition,
and beat tracking. Our method converts audio-derived musical information into
textual representations, enabling LLMs to perform reasoning and correction
specifically for chord recognition tasks. We design a 5-stage chain-of-thought
framework that allows GPT-4o to systematically analyze, compare, and refine
chord recognition results by leveraging music-theoretical knowledge to
integrate information across different MIR components. Experimental evaluation
on three datasets demonstrates consistent improvements across multiple
evaluation metrics, with overall accuracy gains of 1-2.77% on the MIREX metric.
Our findings demonstrate that LLMs can effectively function as integrative
bridges in MIR pipelines, opening new directions for multi-tool coordination in
music information retrieval tasks.

</details>


### [48] [MECap-R1: Emotion-aware Policy with Reinforcement Learning for Multimodal Emotion Captioning](https://arxiv.org/abs/2509.18729)
*Haoqin Sun,Chenyang Lyu,Xiangyu Kong,Shiwan Zhao,Jiaming Zhou,Hui Wang,Aobo Kong,Jinghua Zhao,Longyue Wang,Weihua Luo,Kaifu Zhang,Yong Qin*

Main category: cs.SD

TL;DR: MECap-R1是一种基于强化学习的多模态情感描述方法，通过情感感知奖励机制解决传统离散分类方法在捕捉语音情感复杂性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统离散分类方法难以充分表征语音中复杂的情感内容，而使用自然语言描述情感为更有效地捕捉和表达情感提供了新途径。

Method: 提出MECap-R1框架，采用带有情感感知奖励的分组相对策略优化（Emo-GRPO），精确捕捉情感和语义特征，解决处理动态灵活描述时的刚性规则缺陷。

Result: 在EmotionTalk数据集上的实验结果表明，MECap-R1在生成情感描述方面表现良好，在准确性和多样性方面都取得了显著提升。

Conclusion: 该方法为语音情感描述提供了一种有效的解决方案，能够更好地处理情感内容的复杂性和灵活性。

Abstract: Speech Emotion Captioning (SEC) has emerged as a notable research direction.
The inherent complexity of emotional content in human speech makes it
challenging for traditional discrete classification methods to provide an
adequate representation. Consequently, utilizing natural language to describe
speech emotions presents a novel avenue for more effectively capturing and
expressing affect. In this paper, we propose MECap-R1, a pioneering
emotion-aware policy with reinforcement learning for multimodal emotion
captioning. By employing Group Relative Policy Optimization with emotion-aware
reward (Emo-GRPO), the framework precisely captures the emotion and semantic
features, thereby addressing the shortcomings of rigid rules in handling the
dynamic and flexible nature of captions. Experimental results on the
EmotionTalk dataset demonstrate that MECap-R1 performs well in generating
emotion descriptions and achieves substantial gains in both accuracy and
diversity.

</details>


### [49] [Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models](https://arxiv.org/abs/2509.18816)
*Junyu Wang,Ziyang Ma,Zhengding Luo,Tianrui Wang,Meng Ge,Xiaobao Wang,Longbiao Wang*

Main category: cs.SD

TL;DR: MATA是一种无需训练的方法，通过动态调整自注意力机制，让大型音频语言模型更多地关注音频token，解决音频-文本注意力不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型音频语言模型在多模态融合层中往往偏向文本信息而忽视音频信息，导致音频推理任务性能不佳。

Method: MATA在原始注意力评分后介入，仅针对中间层的最后一个token进行干预，不引入额外参数或计算开销。

Result: 在MMAU和MMAR基准测试中，MATA带来了一致的性能提升，使开源模型首次超越专有的Gemini 2.0 Flash。

Conclusion: MATA为缓解注意力偏差提供了高效解决方案，并为增强多模态模型的音频处理能力开辟了新研究方向。

Abstract: Large Audio-Language Models (LALMs) often suffer from audio-textual attention
imbalance, prioritizing text over acoustic information, particularly in the
multi-modal fusion layers of the Transformer architecture. This bias hinders
their ability to fully utilize acoustic cues, causing suboptimal performance on
audio reasoning tasks. To mitigate this, we propose \textbf{MATA}, a novel
training-free method that dynamically pushes LALMs to pay \textbf{M}ore
\textbf{A}ttention \textbf{T}o \textbf{A}udio tokens within the self-attention
mechanism. Specifically, MATA intervenes post raw attention scoring, targeting
only the last token in intermediate layers without introducing additional
parameters or computational overhead. Experiments on the MMAU and MMAR
benchmarks confirm MATA's effectiveness, with consistent performance gains.
Notably, on MMAR, MATA enables an open-source model to surpass the proprietary
Gemini 2.0 Flash for the first time. Our work provides an efficient solution to
mitigate attention bias and opens a new research direction for enhancing the
audio-processing capabilities of multi-modal models.

</details>


### [50] [Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation](https://arxiv.org/abs/2509.19231)
*Karen Rosero,Eunjung Yeo,David R. Mortensen,Cortney Van't Slot,Rami R. Hallac,Carlos Busso*

Main category: cs.SD

TL;DR: ChiReSSD是一个针对儿童语音障碍的语音重建框架，能够在纠正发音错误的同时保持说话者身份特征，特别是在音高和韵律方面。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对健康成人语音训练，无法有效处理儿童语音障碍患者的特殊语音特征，特别是音高和韵律的差异。

Method: 采用解耦的、基于风格的TTS重建方法，专门适应语音障碍儿童的语音特征，重点处理音高和韵律的保持。

Result: 在STAR数据集上显著提升了词汇准确性和说话者身份保持度，自动预测与人工标注的相关系数达到0.63，在TORGO数据集上也展示了良好的泛化能力。

Conclusion: 基于风格的解耦TTS重建方法能够为不同临床人群提供身份保持的语音重建，具有减少人工转录负担的潜力。

Abstract: We present ChiReSSD, a speech reconstruction framework that preserves
children speaker's identity while suppressing mispronunciations. Unlike prior
approaches trained on healthy adult speech, ChiReSSD adapts to the voices of
children with speech sound disorders (SSD), with particular emphasis on pitch
and prosody. We evaluate our method on the STAR dataset and report substantial
improvements in lexical accuracy and speaker identity preservation.
Furthermore, we automatically predict the phonetic content in the original and
reconstructed pairs, where the proportion of corrected consonants is comparable
to the percentage of correct consonants (PCC), a clinical speech assessment
metric. Our experiments show Pearson correlation of 0.63 between automatic and
human expert annotations, highlighting the potential to reduce the manual
transcription burden. In addition, experiments on the TORGO dataset demonstrate
effective generalization for reconstructing adult dysarthric speech. Our
results indicate that disentangled, style-based TTS reconstruction can provide
identity-preserving speech across diverse clinical populations.

</details>

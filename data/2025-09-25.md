<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 42]
- [eess.AS](#eess.AS) [Total: 12]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Raspberry Pi Pico as a Radio Transmitter](https://arxiv.org/abs/2509.19304)
*M. Andrecut*

Main category: eess.SP

TL;DR: 将树莓派Pico微控制器通过廉价现成电子元件和开源软件转变为无线电发射器的简单方法，可能带来安全风险。


<details>
  <summary>Details</summary>
Motivation: 探索树莓派Pico微控制器作为无线电发射器的潜力，揭示其在极端情况下可能构成的安全威胁。

Method: 使用廉价现成电子元件和开源软件，通过简单方法将树莓派Pico转换为无线电发射器。

Result: 成功实现了将树莓派Pico转变为无线电发射器的技术方案，能够建立大量本地隐蔽无线电通信通道。

Conclusion: 虽然这种技术转换看似无害，但在某些极端情况下可能构成严重的安全风险，需要引起重视。

Abstract: In this paper we discuss several surprisingly simple methods for transforming
the Raspberry Pi Pico (RP2) microcontroller into a radio transmitter, by using
only cheap off the shelf electronic components, and open source software. While
initially this transformation may look as a harmless curiosity, in some extreme
cases it can also pose security risks, since it can be used to open a large
number of local stealth radio communication channels.

</details>


### [2] [A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks](https://arxiv.org/abs/2509.19306)
*Jingyi Wang,Zhongyuan Zhao,Qingtian Wang,Zexu Li,Yue Wang,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: 该论文提出了一种基于在线学习的优化方法，用于在异构无线网络中优化联邦微调，通过动态切换LoRA模块来应对设备异构性和传输不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 边缘智能需要低延迟和无处不在的服务，但无线网络中的设备异构性和资源约束对联邦微调性能构成威胁。

Method: 提出了基于切换的联邦微调框架，设备动态切换LoRA模块；推导了推理风险差距的上界；将非凸混合整数规划问题分解为模型切换、发射功率控制和带宽分配子问题，并开发了在线优化算法。

Result: 在SST-2和QNLI数据集上的仿真结果表明，该方法在测试准确率和能量效率方面取得了性能提升。

Conclusion: 所提出的在线优化方法有效解决了异构无线网络中联邦微调的挑战，提高了模型的泛化能力和能效。

Abstract: Edge intelligence has emerged as a promising strategy to deliver low-latency
and ubiquitous services for mobile devices. Recent advances in fine-tuning
mechanisms of foundation models have enabled edge intelligence by integrating
low-rank adaptation (LoRA) with federated learning. However, in wireless
networks, the device heterogeneity and resource constraints on edge devices
pose great threats to the performance of federated fine-tuning. To tackle these
issues, we propose to optimize federated fine-tuning in heterogenous wireless
networks via online learning. First, the framework of switching-based federated
fine-tuning in wireless networks is provided. The edge devices switches to LoRA
modules dynamically for federated fine-tuning with base station to jointly
mitigate the impact of device heterogeneity and transmission unreliability.
Second, a tractable upper bound on the inference risk gap is derived based on
theoretical analysis. To improve the generalization capability, we formulate a
non-convex mixed-integer programming problem with long-term constraints, and
decouple it into model switching, transmit power control, and bandwidth
allocation subproblems. An online optimization algorithm is developed to solve
the problems with polynomial computational complexity. Finally, the simulation
results on the SST-2 and QNLI data sets demonstrate the performance gains in
test accuracy and energy efficiency.

</details>


### [3] [Bandwidth of Gamma-Distribution-Shaped Functions via Lambert W Function](https://arxiv.org/abs/2509.19307)
*Anthony LoPrete,Johannes Burge*

Main category: eess.SP

TL;DR: 本文推导了伽马分布函数半高全宽（FWHM）的精确解析表达式，使用Lambert W函数计算伽马分布概率密度函数的逆函数，并比较了伽马形函数与高斯近似的FWHM。


<details>
  <summary>Details</summary>
Motivation: 伽马形函数的半高全宽（FWHM）是表征单峰函数带宽的重要参数，但目前缺乏其闭式表达式。本文旨在填补这一空白。

Method: 使用Lambert W函数计算伽马分布概率密度函数的逆函数，从而推导出伽马分布在任意比例最大值处的宽度精确解析表达式，进而得到FWHM。

Result: 成功推导出伽马形函数FWHM的精确解析表达式，并提供了八度带宽的表达式。通过与高斯近似的比较，验证了所提方法的有效性。

Conclusion: 本文提供了伽马形函数FWHM的闭式解，解决了该领域的一个长期问题，为相关研究提供了实用的数学工具。

Abstract: The full width at half maximum (FWHM) is a useful quantity for characterizing
the bandwidth of unimodal functions. However, a closed-form expression for the
FWHM of gamma-shaped functions-i.e. functions that are shaped like the gamma
distribution probability density function (PDF)-is not widely available. Here,
we derive and present just such an expression. To do so, we use the Lambert W
function to compute the inverse of the gamma PDF. We use this inverse to derive
an exact analytic expression for the width of a gamma distribution at an
arbitrary proportion of the maximum, from which the FWHM follows trivially. (An
expression for the octave bandwidth of gamma-shaped functions is also
provided.) The FWHM is then compared to the Gaussian approximation of
gamma-shaped functions. A few other related issues are discussed.

</details>


### [4] [Graph-Based Spatio-temporal Attention and Multi-Scale Fusion for Clinically Interpretable, High-Fidelity Fetal ECG Extraction](https://arxiv.org/abs/2509.19308)
*Chang Wang,Ming Zhu,Shahram Latifi,Buddhadeb Dawn,Shengjie Zhai*

Main category: eess.SP

TL;DR: FHNet是一种结合图神经网络和多尺度增强Transformer的深度学习框架，用于从腹部心电图中提取干净的胎儿心电图信号，在低信噪比条件下表现出色。


<details>
  <summary>Details</summary>
Motivation: 先天性心脏病是最常见的新生儿异常，需要早期检测来改善预后。但胎儿心电图信号在腹部心电图中常被母体心电图和噪声掩盖，传统方法在低信噪比条件下效果不佳。

Method: 提出FetalHealthNet（FHNet）框架，集成图神经网络和多尺度增强Transformer，动态建模导联间的时空相关性，提取干净的胎儿心电图信号。

Result: 在基准数据集上，FHNet持续优于LSTM模型、标准Transformer和最先进模型，R²>0.99，RMSE=0.015，即使在严重噪声条件下也表现优异。可解释性分析显示了生理学上有意义的时空和导联贡献。

Conclusion: FHNet展示了AI驱动建模在推进胎儿监测和实现早期先天性心脏病筛查方面的潜力，强调了新一代生物医学信号处理的变革性影响。

Abstract: Congenital Heart Disease (CHD) is the most common neonatal anomaly,
highlighting the urgent need for early detection to improve outcomes. Yet,
fetal ECG (fECG) signals in abdominal ECG (aECG) are often masked by maternal
ECG and noise, challenging conventional methods under low signal-to-noise ratio
(SNR) conditions. We propose FetalHealthNet (FHNet), a deep learning framework
that integrates Graph Neural Networks with a multi-scale enhanced transformer
to dynamically model spatiotemporal inter-lead correlations and extract clean
fECG signals. On benchmark aECG datasets, FHNet consistently outperforms long
short-term memory (LSTM) models, standard transformers, and state-of-the-art
models, achieving R2>0.99 and RMSE = 0.015 even under severe noise.
Interpretability analyses highlight physiologically meaningful temporal and
lead contributions, supporting model transparency and clinical trust. FHNet
illustrates the potential of AI-driven modeling to advance fetal monitoring and
enable early CHD screening, underscoring the transformative impact of
next-generation biomedical signal processing.

</details>


### [5] [A Novel Two-Dimensional Wigner Distribution Framework via the Quadratic Phase Fourier Transform with a Non-Separable Kernel](https://arxiv.org/abs/2509.19310)
*Mukul Chauhan,Waseem Z. Lone,Amit K. Verma*

Main category: eess.SP

TL;DR: 本文提出了一种基于二维不可分离二次相位傅里叶变换的新型时频分布方法——二维不可分离二次相位维格纳分布，能够有效捕捉复杂不可分离信号结构并抑制交叉项。


<details>
  <summary>Details</summary>
Motivation: 传统维格纳分布在处理复杂不可分离信号时存在局限性，需要一种能够更好捕捉复杂信号结构并抑制交叉项的新型时频分布方法。

Method: 通过用NSQPFT核替换经典傅里叶核，将经典维格纳分布推广到二维不可分离二次相位维格纳分布，并严格建立了其数学性质。

Result: 该方法在单分量、双分量和三分量二维线性调频信号上表现出优越的交叉项抑制和信号定位性能。

Conclusion: 2D-NSQPWD为分析复杂不可分离信号提供了一种有效的时频分析工具，在信号处理领域具有重要应用价值。

Abstract: This paper introduces a novel time-frequency distribution, referred to as the
Two-Dimensional Non-Separable Quadratic Phase Wigner Distribution (2D-NSQPWD),
formulated within the framework of the Two-Dimensional Non-Separable Quadratic
Phase Fourier Transform (2D-NSQPFT). By replacing the classical Fourier kernel
with the NSQPFT kernel, the proposed distribution generalizes the classical
Wigner distribution and effectively captures complex, non-separable signal
structures. We rigorously establish several key properties of the 2D-NSQPWD,
including time and frequency shift invariance, marginal behavior, conjugate
symmetry, convolution relations, and Moyal's identity. Furthermore, the
connection between the 2D-NSQPWD and the two-dimensional short-time Fourier
transform (2D-STFT) is explored. The distribution's effectiveness is
demonstrated through its application to single-, bi-, and tri-component
two-dimensional linear frequency modulated (2D-LFM) signals, where it shows
superior performance in cross-term suppression and signal localization.

</details>


### [6] [E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion](https://arxiv.org/abs/2509.19312)
*Minghui Wu,Zhen Gao*

Main category: eess.SP

TL;DR: 提出了一种端到端的上行-下行CSI融合预编码网络，联合建模下行CSI参考信号设计、CSI反馈和基站预编码，以解决大规模MIMO系统中高维下行CSI获取和预编码的复杂性。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统虽然能提供高频谱效率，但高维下行信道状态信息(CSI)使得实时信道获取和预编码变得复杂。传统方法通常单独处理上行和下行信道信息，未能充分利用两者的互补性。

Method: 构建基于MAXIM架构的投影网络，输入上行探测参考信号(SRS)，输出用于设计下行CSI-RS的投影矩阵。用户设备压缩/量化CSI-RS观测结果并反馈。基站端有两个互补分支：基于量化下行观测的反馈预编码网络和基于上行SRS的SRS预编码网络，通过融合预编码网络结合两者生成最终预编码器。所有模块采用三阶段训练策略，以频谱效率为导向进行优化。

Result: 仿真结果表明，该方法能有效利用SRS衍生信息和用户反馈，相比传统基线方法获得了显著更好的性能。

Conclusion: 所提出的端到端融合预编码网络能够协同利用上行和下行信道信息，在大规模MIMO系统中实现了更高效的预编码性能，为解决高维CSI获取和预编码问题提供了有效解决方案。

Abstract: Massive multiple-input multiple-output (MIMO) promises high spectral
efficiency but also leads to high-dimensional downlink channel state
information (CSI), which complicates real-time channel acquisition and
precoding. To address this, we propose an end-to-end (E2E) uplink-downlink CSI
fusion precoding network that jointly models downlink CSI reference signal
(CSI-RS) design, CSI feedback, and base-station (BS) precoding within a single
E2E neural architecture. Concretely, a projection network built on the MAXIM
architecture takes uplink sounding reference signals (SRS) as input and outputs
frequency-, beam-, and port-domain projection matrices for designing downlink
CSI-RS. User equipment (UE) then compresses/quantizes the resulting CSI-RS
observations and feeds back a compact representation. At the base station (BS),
two complementary branches produce candidate precoders: one is a feedback-only
precoding network driven by quantized downlink observations, and the other is
an SRS-only precoding network driven by uplink SRS. These candidate precoders
are subsequently combined by a fusion precoding network to yield the final
transmit precoder. All the modules are trained with a
spectral-efficiency-oriented loss under a three-stage schedule. Simulation
results show that the proposed approach effectively harnesses both SRS-derived
information and UE feedback, achieving markedly better performance than
conventional baselines.

</details>


### [7] [STL-FFT-STFT-TCN-LSTM: An Effective Wave Height High Accuracy Prediction Model Fusing Time-Frequency Domain Features](https://arxiv.org/abs/2509.19313)
*Huipeng Liu,Zhichao Zhu,Yuan Zhou,Changlu Li*

Main category: eess.SP

TL;DR: 本文提出了一种结合STL-FFT-STFT-TCN-LSTM的混合模型，用于精确预测有效波高，解决了波浪能信号的非线性、突变、多尺度周期性等问题，在极端波高捕捉和高频噪声抑制方面表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 传统能源消耗加剧且环境影响显著，波浪能因其高能量密度、稳定性、分布广泛和环境友好性而成为可再生能源的重要成员。精确预测有效波高是波浪能开发的关键，但现有方法面临非线性、突变、多尺度周期性、数据稀疏和高频噪声等挑战。

Method: 提出STL-FFT-STFT-TCN-LSTM混合模型，结合季节性趋势分解、快速傅里叶变换、短时傅里叶变换、时序卷积网络和长短期记忆网络技术，优化多尺度特征融合，捕捉极端波高，处理高频噪声和周期信号问题。

Result: 使用NOAA站点2019-2022年每小时数据进行实验，相比其他单模型和混合模型，该模型在极端波高捕捉和高频噪声抑制方面预测精度显著提高，MAE降低15.8%-40.5%，SMAPE降低8.3%-20.3%，R增加1.31%-2.9%。消融实验验证了各组件步骤的必要性。

Conclusion: STL-FFT-STFT-TCN-LSTM模型在多尺度特征融合方面具有优越性，能够高效准确地预测有效波高，为波浪能开发提供了有效的技术支撑。

Abstract: As the consumption of traditional energy sources intensifies and their
adverse environmental impacts become more pronounced, wave energy stands out as
a highly promising member of the renewable energy family due to its high energy
density, stability, widespread distribution, and environmental friendliness.
The key to its development lies in the precise prediction of Significant Wave
Height (WVHT). However, wave energy signals exhibit strong nonlinearity, abrupt
changes, multi-scale periodicity, data sparsity, and high-frequency noise
interference; additionally, physical models for wave energy prediction incur
extremely high computational costs. To address these challenges, this study
proposes a hybrid model combining STL-FFT-STFT-TCN-LSTM. This model exploits
the Seasonal-Trend Decomposition Procedure based on Loess (STL), Fast Fourier
Transform (FFT), Short-Time Fourier Transform (STFT), Temporal Convolutional
Network (TCN), and Long Short-Term Memory (LSTM) technologies. The model aims
to optimize multi-scale feature fusion, capture extreme wave heights, and
address issues related to high-frequency noise and periodic signals, thereby
achieving efficient and accurate prediction of significant wave height.
Experiments were conducted using hourly data from NOAA Station 41008 and 41047
spanning 2019 to 2022. The results showed that compared with other single
models and hybrid models, the STL-FFT-STFT-TCN-LSTM model achieved
significantly higher prediction accuracy in capturing extreme wave heights and
suppressing high-frequency noise, with MAE reduced by 15.8\%-40.5\%, SMAPE
reduced by 8.3\%-20.3\%, and R increased by 1.31\%-2.9\%; in ablation
experiments, the model also demonstrated the indispensability of each component
step, validating its superiority in multi-scale feature fusion.

</details>


### [8] [Advancing Few-Shot Pediatric Arrhythmia Classification with a Novel Contrastive Loss and Multimodal Learning](https://arxiv.org/abs/2509.19315)
*Yiqiao Chen,Zijian Huang,Zhenghui Feng*

Main category: eess.SP

TL;DR: 提出了一种多模态端到端深度学习框架，结合双分支卷积编码器、语义注意力和轻量级Transformer，用于儿科心律失常分类，在Leipzig Heart Center数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 儿科心律失常是导致残疾和心源性猝死的主要风险因素，但由于类别不平衡、少样本类别和复杂信号特征，自动分类面临挑战，限制了早期筛查和临床干预的效率与可靠性。

Method: 提出多模态端到端深度学习框架：双分支卷积编码器处理ECG和IEGM信号，语义注意力实现跨模态特征对齐，轻量级Transformer建模全局依赖关系，并引入自适应全局类感知对比损失(AGCACL)增强类内紧凑性和类间可分离性。

Result: 在Leipzig Heart Center儿科/先天性ECG+IEGM数据集上取得最佳性能：Top-1准确率97.76%，宏精确率94.08%，宏召回率91.97%，宏F1分数92.97%，宏F2分数92.36%，相比最强基线分别提升13.64、15.96、19.82和19.44个百分点。

Conclusion: 该框架显著提高了少数心律失常类别的可检测性和鲁棒性，为儿科和先天性心脏病人群的心律筛查、术前评估和术后随访提供了潜在的临床价值。

Abstract: Pediatric arrhythmias are a major risk factor for disability and sudden
cardiac death, yet their automated classification remains challenging due to
class imbalance, few-shot categories, and complex signal characteristics, which
severely limit the efficiency and reliability of early screening and clinical
intervention. To address this problem, we propose a multimodal end-to-end deep
learning framework that combines dual-branch convolutional encoders for ECG and
IEGM, semantic attention for cross-modal feature alignment, and a lightweight
Transformer encoder for global dependency modeling. In addition, we introduce a
new contrastive loss fucntion named Adaptive Global Class-Aware Contrastive
Loss (AGCACL) to enhance intra-class compactness and inter-class separability
through class prototypes and a global similarity matrix. To the best of our
knowledge, this is the first systematic study based on the Leipzig Heart Center
pediatric/congenital ECG+IEGM dataset, for which we also provide a complete and
reproducible preprocessing pipeline. Experimental results demonstrate that the
proposed method achieves the overall best performance on this dataset,
including 97.76\% Top-1 Accuracy, 94.08\% Macro Precision, 91.97\% Macro
Recall, 92.97\% Macro F1, and 92.36\% Macro F2, with improvements of +13.64,
+15.96, +19.82, and +19.44 percentage points over the strongest baseline in
Macro Precision/Recall/F1/F2, respectively. These findings indicate that the
framework significantly improves the detectability and robustness for minority
arrhythmia classes, offering potential clinical value for rhythm screening,
pre-procedural assessment, and postoperative follow-up in pediatric and
congenital heart disease populations.

</details>


### [9] [Electric Vehicle Identification from Behind Smart Meter Data](https://arxiv.org/abs/2509.19316)
*Ammar Kamoona,Hui Song,Ali Moradi Amani,Mahdi Jalili,Xinghuo Yu,Peter McTaggart*

Main category: eess.SP

TL;DR: 本文提出了一种基于异常检测的无监督学习方法，用于从智能电表数据中识别电动汽车充电负荷，无需先验的EV充电配置文件知识。


<details>
  <summary>Details</summary>
Motivation: 电动汽车在电表后充电时，充电负荷被视为用户总负荷的一部分，不被配电网络运营商单独测量。配电网络运营商需要了解网络中EV的存在情况，以更好地规划和管理配电网。

Method: 采用基于异常检测技术的无监督学习方法和深度时间卷积编码解码网络，仅需非EV用户的真实功耗数据。

Result: 该方法应用于澳大利亚维多利亚州家庭的智能电表数据，在识别有EV的家庭方面表现出优越性能。

Conclusion: 所提出的无监督学习方法能有效识别电表后的EV充电负荷，为配电网络运营商提供重要决策支持。

Abstract: Electric vehicle (EV) charging loads identification from behind smart meter
recordings is an indispensable aspect that enables effective decision-making
for energy distributors to reach an informed and intelligent decision about the
power grid's reliability. When EV charging happens behind the meter (BTM), the
charging occurs on the customer side of the meter, which measures the overall
electricity consumption. In other words, the charging of the EV is considered
part of the customer's load and not separately measured by the Distribution
Network Operators (DNOs). DNOs require complete knowledge about the EV presence
in their network. Identifying the EV charging demand is essential to better
plan and manage the distribution grid. Unlike supervised methods, this paper
addresses the problem of EV charging load identification in a non-nonintrusive
manner from low-frequency smart meter using an unsupervised learning approach
based on anomaly detection technique. Our approach does not require prior
knowledge of EV charging profiles. It only requires real power consumption data
of non-EV users, which are abundant in practice. We propose a deep temporal
convolution encoding decoding (TAE) network. The TAE is applied to power
consumption from smart BTM from Victorian households in Australia, and the TAE
shows superior performance in identifying households with EVs.

</details>


### [10] [Scensory: Automated Real-Time Fungal Identification and Spatial Mapping](https://arxiv.org/abs/2509.19318)
*Yanbaihui Liu,Erica Babusci,Claudia K. Gunsch,Boyuan Chen*

Main category: eess.SP

TL;DR: 提出了一种名为Scensory的机器人嗅觉系统，使用低成本VOC传感器阵列和深度学习，能够同时识别真菌种类并定位其空间来源，实现实时、空间感知的真菌监测。


<details>
  <summary>Details</summary>
Motivation: 现有的室内真菌检测方法速度慢、成本高且缺乏空间分辨率，不适用于实时监测和大规模部署。

Method: 利用机器人自动数据收集，通过分析VOC的时间动态特征，使用神经网络架构解码化学和空间信息，提供被动多阵列和主动单阵列两种操作模式。

Result: 在五种真菌物种上，系统在环境条件下达到89.85%的物种检测准确率和87.31%的定位准确率，每个预测仅需3-7秒传感器输入。

Conclusion: 该方法实现了实时、空间感知的真菌监测，建立了可扩展且经济实惠的自主环境感知框架。

Abstract: Indoor fungal contamination poses significant risks to public health, yet
existing detection methods are slow, costly, and lack spatial resolution.
Conventional approaches rely on laboratory analysis or high-concentration
sampling, making them unsuitable for real-time monitoring and scalable
deployment. We introduce \textbf{\textit{Scensory}}, a robot-enabled olfactory
system that simultaneously identifies fungal species and localizes their
spatial origin using affordable volatile organic compound (VOC) sensor arrays
and deep learning. Our key idea is that temporal VOC dynamics encode both
chemical and spatial signatures, which we decode through neural architectures
trained on robot-automated data collection. We demonstrate two operational
modes: a passive multi-array configuration for environmental monitoring, and a
mobile single-array configuration for active source tracking. Across five
fungal species, our system achieves up to 89.85\% accuracy in species detection
and 87.31\% accuracy in localization under ambient conditions, where each
prediction only takes 3--7\,s sensor inputs. Additionally, by computationally
analyzing model behavior, we can uncover key biochemical signatures without
additional laboratory experiments. Our approach enables real-time, spatially
aware fungal monitoring and establishes a scalable and affordable framework for
autonomous environmental sensing.

</details>


### [11] [Human Activity Recognition Based on Electrocardiogram Data Only](https://arxiv.org/abs/2509.19328)
*Sina Montazeri,Waltenegus Dargie,Yunhe Feng,Kewei Sha*

Main category: eess.SP

TL;DR: 本文首次展示了仅使用心电图（ECG）在六种不同活动中进行稳健识别的能力，超越了以往工作的范围。设计了三种新的深度学习模型，在54名受试者的数据上进行测试，所有模型对已见受试者准确率超过94%，CNNTransformer混合模型对未见受试者达到72%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统活动识别依赖惯性测量单元（IMU），资源密集且需要校准。虽然已探索基于心电图的方法，但通常作为IMU的补充或仅限于广泛的分类。本文旨在推进该领域，仅使用ECG进行多活动识别。

Method: 设计并评估了三种新的深度学习模型：1）带有Squeeze-and-Excitation块的CNN分类器进行通道特征重校准；2）带有扩张卷积的ResNet分类器捕获多尺度时间依赖；3）新颖的CNNTransformer混合模型结合卷积特征提取和注意力机制进行长程时间关系建模。

Result: 在54名受试者的六种活动数据上测试，所有三种模型对已见受试者准确率超过94%，CNNTransformer混合模型对未见受试者达到最佳准确率72%，通过增加训练人群可进一步提高结果。

Conclusion: 本研究首次成功实现了仅使用ECG的多体育活动分类，为开发下一代可穿戴设备提供了重要潜力，能够同时进行心脏监测和活动识别，无需额外的运动传感器。

Abstract: Human activity recognition is critical for applications such as early
intervention and health analytics. Traditional activity recognition relies on
inertial measurement units (IMUs), which are resource intensive and require
calibration. Although electrocardiogram (ECG)-based methods have been explored,
these have typically served as supplements to IMUs or have been limited to
broad categorical classification such as fall detection or active vs. inactive
in daily activities. In this paper, we advance the field by demonstrating, for
the first time, robust recognition of activity only with ECG in six distinct
activities, which is beyond the scope of previous work. We design and evaluate
three new deep learning models, including a CNN classifier with
Squeeze-and-Excitation blocks for channel-wise feature recalibration, a ResNet
classifier with dilated convolutions for multiscale temporal dependency
capture, and a novel CNNTransformer hybrid combining convolutional feature
extraction with attention mechanisms for long-range temporal relationship
modeling. Tested on data from 54 subjects for six activities, all three models
achieve over 94% accuracy for seen subjects, while CNNTransformer hybrid
reaching the best accuracy of 72% for unseen subjects, a result that can be
further improved by increasing the training population. This study demonstrates
the first successful ECG-only activity classification in multiple physical
activities, offering significant potential for developing next-generation
wearables capable of simultaneous cardiac monitoring and activity recognition
without additional motion sensors.

</details>


### [12] [LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition](https://arxiv.org/abs/2509.19330)
*Zejun Liu,Yunshan Chen,Chengxi Xie,Huan Liu*

Main category: eess.SP

TL;DR: 该论文介绍了LibEMER，一个用于EEG多模态情感识别的统一评估框架，旨在解决该领域缺乏开源实现、标准化基准和深入讨论的问题。


<details>
  <summary>Details</summary>
Motivation: EEG多模态情感识别领域存在三个关键问题：缺乏开源实现、缺少标准化透明基准、以及缺乏对主要挑战和研究方向的深入讨论。

Method: 开发了LibEMER框架，提供可完全复现的PyTorch实现，包含标准化的数据预处理、模型实现和实验设置协议。

Result: 该框架在三个广泛使用的公共数据集和两个学习任务上实现了无偏见的性能评估。

Conclusion: LibEMER为EEG多模态情感识别研究提供了统一的评估标准，促进了该领域的公平比较和进一步发展。

Abstract: EEG-based multimodal emotion recognition(EMER) has gained significant
attention and witnessed notable advancements, the inherent complexity of human
neural systems has motivated substantial efforts toward multimodal approaches.
However, this field currently suffers from three critical limitations: (i) the
absence of open-source implementations. (ii) the lack of standardized and
transparent benchmarks for fair performance analysis. (iii) in-depth discussion
regarding main challenges and promising research directions is a notable
scarcity. To address these challenges, we introduce LibEMER, a unified
evaluation framework that provides fully reproducible PyTorch implementations
of curated deep learning methods alongside standardized protocols for data
preprocessing, model realization, and experimental setups. This framework
enables unbiased performance assessment on three widely-used public datasets
across two learning tasks. The open-source library is publicly accessible at:
https://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384

</details>


### [13] [Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention](https://arxiv.org/abs/2509.19331)
*Enhao Huang,Zhiyu Zhang,Tianxiang Xu,Chunshu Xia,Kaichun Hu,Yuchen Yang,Tongtong Pan,Dong Dong,Zhan Qin*

Main category: eess.SP

TL;DR: 提出全息Transformer，将波干涉原理融入自注意力机制，通过相对相位调制交互并相干叠加值，确保幅度和相位的一致性，在复数信号处理中实现物理一致性


<details>
  <summary>Details</summary>
Motivation: 大多数深度学习模型将注意力视为实值相关性，忽略了复数信号中的干涉效应，无法有效处理同时包含幅度和相位信息的复数信号

Method: 引入全息注意力机制，通过相对相位调制交互并相干叠加值；采用双头解码器同时重建输入和预测任务输出，防止相位崩溃；证明全息注意力实现了离散干涉算子并在线性混合下保持相位一致性

Result: 在PolSAR图像分类和无线信道预测实验中表现出色，实现了高分类精度和F1分数、低回归误差，以及对相位扰动的鲁棒性增强

Conclusion: 在注意力中强制执行物理一致性可以带来复数学习中的泛化改进，为相干信号建模提供了一个统一的、基于物理的框架

Abstract: Complex-valued signals encode both amplitude and phase, yet most deep models
treat attention as real-valued correlation, overlooking interference effects.
We introduce the Holographic Transformer, a physics-inspired architecture that
incorporates wave interference principles into self-attention. Holographic
attention modulates interactions by relative phase and coherently superimposes
values, ensuring consistency between amplitude and phase. A dual-headed decoder
simultaneously reconstructs the input and predicts task outputs, preventing
phase collapse when losses prioritize magnitude over phase. We demonstrate that
holographic attention implements a discrete interference operator and maintains
phase consistency under linear mixing. Experiments on PolSAR image
classification and wireless channel prediction show strong performance,
achieving high classification accuracy and F1 scores, low regression error, and
increased robustness to phase perturbations. These results highlight that
enforcing physical consistency in attention leads to generalizable improvements
in complex-valued learning and provides a unified, physics-based framework for
coherent signal modeling. The code is available at
https://github.com/EonHao/Holographic-Transformers.

</details>


### [14] [A Spatio-Temporal Feature Fusion EEG Virtual Channel Signal Generation Network and Its Application in Anxiety Assessment](https://arxiv.org/abs/2509.19334)
*Shangqing Yuan,Wenshuang Zhai,Shengwen Guo*

Main category: eess.SP

TL;DR: 本研究提出了一种基于时空特征融合的EEG虚拟通道信号生成网络，旨在解决便携式EEG设备通道有限的问题，通过4个前额叶通道生成13个脑区的虚拟EEG信号，并在焦虑分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决便携式EEG设备通道有限、信息采集不足的问题，通过生成虚拟通道信号来扩展可用脑区信息。

Method: 采用二维卷积神经网络架构，包含并行的时间域和空间域特征提取模块，以及特征融合模块，使用PRED+CT数据库的119名受试者多通道EEG数据进行验证。

Result: 生成的虚拟通道EEG信号与真实信号的平均相关系数为0.6724，平均绝对误差为3.9470；结合原始信号进行焦虑分类时，显著提升了机器学习算法的性能。

Conclusion: 该网络生成的虚拟EEG信号与真实信号具有高度一致性，能有效缓解便携式EEG设备信息获取不足的问题，并增强机器学习算法的分类性能。

Abstract: To address the issue of limited channels and insufficient information
collection in portable EEG devices, this study explores an EEG virtual channel
signal generation network using a novel spatio-temporal feature fusion
strategy. Based on the EEG signals from four frontal lobe channels, the network
aims to generate virtual channel EEG signals for other 13 important brain
regions. The architecture of the network is a two-dimensional convolutional
neural network and it includes a parallel module for temporal and spatial
domain feature extraction, followed by a feature fusion module. The public
PRED+CT database, which includes multi-channel EEG signals from 119 subjects,
was selected to verify the constructed network. The results showed that the
average correlation coefficient between the generated virtual channel EEG
signals and the original real signals was 0.6724, with an average absolute
error of 3.9470. Furthermore, the 13 virtual channel EEG signals were combined
with the original EEG signals of four brain regions and then used for anxiety
classification with a support vector machine. The results indicate that the
virtual EEG signals generated by the constructed network not only have a high
degree of consistency with the real channel EEG signals but also significantly
enhance the performance of machine learning algorithms for anxiety
classification. This study effectively alleviates the problem of insufficient
information acquisition by portable EEG devices with few channels.

</details>


### [15] [CSIYOLO: An Intelligent CSI-based Scatter Sensing Framework for Integrated Sensing and Communication Systems](https://arxiv.org/abs/2509.19335)
*Xudong Zhang,Jingbo Tan,Zhizhen Ren,Jintao Wang,Yihua Ma,Jian Song*

Main category: eess.SP

TL;DR: CSIYOLO是一个基于CSI的散射体定位框架，通过将散射体参数提取转化为图像检测问题，使用YOLO架构实现单基站-用户设备对的散射体定位，无需修改波形或硬件。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC散射体感知方法大多依赖波形和硬件修改或传统信号处理方案，导致与现有通信系统兼容性差且感知精度有限。

Method: 提出基于锚点的散射体参数检测方法，将参数提取建模为图像检测问题；设计CSI定位算法确定散射体位置；采用可扩展网络结构和噪声注入训练策略提升精度和鲁棒性。

Result: 实验表明该方法在不同散射体数量和估计误差下，能以较低复杂度显著优于现有方法的定位精度。

Conclusion: CSIYOLO框架仅使用估计的CSI即可实现散射体定位，可作为插件无缝集成到现有通信系统中，具有良好兼容性和实用性。

Abstract: ISAC is regarded as a promising technology for next-generation communication
systems, enabling simultaneous data transmission and target sensing. Among
various tasks in ISAC, scatter sensing plays a crucial role in exploiting the
full potential of ISAC and supporting applications such as autonomous driving
and low-altitude economy. However, most existing methods rely on either
waveform and hardware modifications or traditional signal processing schemes,
leading to poor compatibility with current communication systems and limited
sensing accuracy. To address these challenges, we propose CSIYOLO, a framework
that performs scatter localization only using estimated CSI from a single base
station-user equipment pair. This framework comprises two main components:
anchor-based scatter parameter detection and CSI-based scatter localization.
First, by formulating scatter parameter extraction as an image detection
problem, we propose an anchor-based scatter parameter detection method inspired
by You Only Look Once architectures. After that, a CSI-based localization
algorithm is derived to determine scatter locations with extracted parameters.
Moreover, to improve localization accuracy and implementation efficiency, we
design an extendable network structure with task-oriented optimizations,
enabling multi-scale anchor detection and better adaptation to CSI
characteristics. A noise injection training strategy is further designed to
enhance robustness against channel estimation errors. Since the proposed
framework operates solely on estimated CSI without modifying waveforms or
signal processing pipelines, it can be seamlessly integrated into existing
communication systems as a plugin. Experiments show that our proposed method
can significantly outperform existing methods in scatter localization accuracy
with relatively low complexities under varying numbers of scatters and
estimation errors.

</details>


### [16] [Non-locally averaged pruned reassigned spectrograms: a tool for glottal pulse visualization and analysis](https://arxiv.org/abs/2509.19686)
*Gabriel J. Griswold,Mark A. Griswold*

Main category: eess.SP

TL;DR: 提出了一种名为NAPReS（非局部平均修剪重分配谱图）的新方法，通过堆叠、求和和修剪大量声门脉冲来简化重分配谱图的可视化，并在高噪声环境下比传统LPC方法更具可重复性。


<details>
  <summary>Details</summary>
Motivation: 重分配谱图在精确共振峰测量和说话人区分方面有优势，但无法以易于理解和可重复的方式可视化大量数据。

Method: 利用Fulop和Fitz开发的技术工具，提出NAPReS方法，通过堆叠、求和和修剪大量声门脉冲来简化重分配谱图的可视化，并采用高斯混合模型（GMM）进行共振峰拟合。

Result: NAPReS能够以易于理解和量化的方式显示大量数据，使低振幅循环结构的观察更加容易。在高噪声情况下，NAPReS与GMM结合的方法比传统LPC共振峰拟合更具可重复性。

Conclusion: NAPReS方法有效解决了重分配谱图在大数据可视化方面的局限性，为声学分析提供了更可靠的工具。

Abstract: Reassigned spectrograms have shown advantages in precise formant measuring
and inter-speaker differentiation. However, reassigned spectrograms suffer from
their inability to visualize larger amounts of data in an easily comprehensible
and reproducible manner. Utilizing the techniques and tools developed by Fulop
and Fitz, a variation of the reassigned spectrogram is proposed. Non-locally
Averaged Pruned Reassigned Spectrograms (NAPReS) provide a simplified view into
the characteristics of a speaker's glottal pulsation patterns throughout the
centroid of a vowel through the stacking, summing, and pruning of large numbers
of glottal pulses. In this exploratory study, NAPReS has been shown to display
a large amount of data in an easily comprehensible and quantifiable manner,
while also making the observation of low-amplitude cyclical structures more
accessible. NAPReS also allows for alternative formant fitting methods such as
Gaussian mixture modeling. In this study, NAPReS with GMM was compared against
conventional LPC fitting of formant values and was shown to be more
reproducible than conventional LPC fitting in high-noise situations.

</details>


### [17] [Joint Channel Estimation and Computation Offloading in Fluid Antenna-assisted MEC Networks](https://arxiv.org/abs/2509.19340)
*Ying Ju,Mingdong Li,Haoyu Wang,Lei Liu,Youyang Qu,Mianxiong Dong,Victor C. M. Leung,Chau Yuen*

Main category: eess.SP

TL;DR: 本文提出了一种流体天线辅助的移动边缘计算卸载框架，通过创新的信道估计算法和分层多智能体算法来最小化系统延迟。


<details>
  <summary>Details</summary>
Motivation: 流体天线能够动态调整端口位置，提供空间分集和频谱效率优势，这对于移动边缘计算系统特别有价值。但面临信道估计复杂度和联合优化问题的非凸性两大挑战。

Method: 提出IBM-CCS信道估计算法整合信息相关性到传感过程；设计HiTDMA分层多智能体算法，利用博弈论降低功率控制变量维度，通过深度强化学习实现高效优化。

Result: 数值结果表明所提方案显著降低系统延迟，提升卸载性能，优于基准方法。IBM-CCS在不同端口密度下表现出优越的准确性和鲁棒性。

Conclusion: 流体天线辅助的MEC卸载框架通过创新的信道估计和优化算法，有效解决了系统延迟最小化问题，为不完美CSI下的高效通信提供了解决方案。

Abstract: With the emergence of fluid antenna (FA) in wireless communications, the
capability to dynamically adjust port positions offers substantial benefits in
spatial diversity and spectrum efficiency, which are particularly valuable for
mobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC
offloading framework to minimize system delay. This framework faces two severe
challenges, which are the complexity of channel estimation due to dynamic port
configuration and the inherent non-convexity of the joint optimization problem.
Firstly, we propose Information Bottleneck Metric-enhanced Channel Compressed
Sensing (IBM-CCS), which advances FA channel estimation by integrating
information relevance into the sensing process and capturing key features of FA
channels effectively. Secondly, to address the non-convex and high-dimensional
optimization problem in FA-assisted MEC systems, which includes FA port
selection, beamforming, power control, and resource allocation, we propose a
game theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA)
based offloading scheme, where the hierarchical structure effectively decouples
and coordinates the optimization tasks between the user side and the base
station side. Crucially, the game theory effectively reduces the dimensionality
of power control variables, allowing deep reinforcement learning (DRL) agents
to achieve improved optimization efficiency. Numerical results confirm that the
proposed scheme significantly reduces system delay and enhances offloading
performance, outperforming benchmarks. Additionally, the IBM-CCS channel
estimation demonstrates superior accuracy and robustness under varying port
densities, contributing to efficient communication under imperfect CSI.

</details>


### [18] [A Measurement Report Data-Driven Framework for Localized Statistical Channel Modeling](https://arxiv.org/abs/2509.19342)
*Xinyu Qin,Ye Xue,Qi Yan,Shutao Zhang,Bingsheng Peng,Tsung-Hui Chang*

Main category: eess.SP

TL;DR: 本文提出了一种基于测量报告(MR)数据的局部统计信道建模框架，利用低成本、广泛收集的MR数据解决传统方法依赖高成本路测数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的局部统计信道建模方法严重依赖高成本的路测数据，且空间覆盖有限。为了利用低成本、广泛收集的MR数据，需要解决MR数据中位置信息缺失的问题，并在复杂环境中实现鲁棒的建模。

Method: 框架包含两个核心模块：1) MR定位模块使用基于超图神经网络的半监督方法，通过距离感知超图建模和超图卷积提取位置信息；2) 联合网格构建和信道APS估计模块，通过聚类和改进的稀疏恢复方法交替优化网格划分和APS估计，处理病态测量矩阵和不完整观测。

Result: 在真实世界MR数据集上的综合实验表明，该框架在定位和信道建模方面具有优越的性能和鲁棒性。

Conclusion: 所提出的MR数据驱动框架能够有效利用低成本MR数据实现局部统计信道建模，解决了传统方法的高成本和有限覆盖问题，在复杂环境中表现出良好的性能。

Abstract: Localized statistical channel modeling (LSCM) is crucial for effective
performance evaluation in digital twin-assisted network optimization. Solely
relying on the multi-beam reference signal receiving power (RSRP), LSCM aims to
model the localized statistical propagation environment by estimating the
channel angular power spectrum (APS). However, existing methods rely heavily on
drive test data with high collection costs and limited spatial coverage. In
this paper, we propose a measurement report (MR) data-driven framework for
LSCM, exploiting the low-cost and extensive collection of MR data. The
framework comprises two novel modules. The MR localization module addresses the
issue of missing locations in MR data by introducing a semi-supervised method
based on hypergraph neural networks, which exploits multi-modal information via
distance-aware hypergraph modeling and hypergraph convolution for location
extraction. To enhance the computational efficiency and solution robustness,
LSCM operates at the grid level. Compared to independently constructing
geographically uniform grids and estimating channel APS, the joint grid
construction and channel APS estimation module enhances robustness in complex
environments with spatially non-uniform data by exploiting their correlation.
This module alternately optimizes grid partitioning and APS estimation using
clustering and improved sparse recovery for the ill-conditioned measurement
matrix and incomplete observations. Through comprehensive experiments on a
real-world MR dataset, we demonstrate the superior performance and robustness
of our framework in localization and channel modeling.

</details>


### [19] [Low-Cost Sensor Fusion Framework for Organic Substance Classification and Quality Control Using Classification Methods](https://arxiv.org/abs/2509.19367)
*Borhan Uddin Chowdhury,Damian Valles,Md Raf E Ul Shougat*

Main category: eess.SP

TL;DR: 基于Arduino Mega 2560微控制器平台开发了一个传感器融合框架，用于有机物质的快速无损分类和质量控制，结合机器学习算法实现了93-94%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 开发低成本、非破坏性的有机物质分类和质量控制系统，利用商业环境传感器和微控制器平台实现快速检测。

Method: 使用Arduino Mega 2560微控制器配备三个商业传感器，收集10种有机物质的数据，进行相关性分析和特征选择，应用PCA/LDA降维，训练SVM、决策树、随机森林、ANN和集成投票分类器。

Result: 最优模型（调优随机森林、集成分类器和ANN）在测试集上达到93-94%的准确率。

Conclusion: 基于Arduino Mega 2560的低成本多传感器平台，结合先进的机器学习和相关性驱动的特征工程，能够可靠地识别和控制有机化合物的质量。

Abstract: We present a sensor-fusion framework for rapid, non-destructive
classification and quality control of organic substances, built on a standard
Arduino Mega 2560 microcontroller platform equipped with three commercial
environmental and gas sensors. All data used in this study were generated
in-house: sensor outputs for ten distinct classes - including fresh and expired
samples of apple juice, onion, garlic, and ginger, as well as cinnamon and
cardamom - were systematically collected and labeled using this hardware setup,
resulting in a unique, application-specific dataset. Correlation analysis was
employed as part of the preprocessing pipeline for feature selection. After
preprocessing and dimensionality reduction (PCA/LDA), multiple supervised
learning models - including Support Vector Machine (SVM), Decision Tree (DT),
and Random Forest (RF), each with hyperparameter tuning, as well as an
Artificial Neural Network (ANN) and an ensemble voting classifier - were
trained and cross-validated on the collected dataset. The best-performing
models, including tuned Random Forest, ensemble, and ANN, achieved test
accuracies in the 93 to 94 percent range. These results demonstrate that
low-cost, multisensory platforms based on the Arduino Mega 2560, combined with
advanced machine learning and correlation-driven feature engineering, enable
reliable identification and quality control of organic compounds.

</details>


### [20] [Short-Term Regional Electricity Demand Forecasting in Argentina Using LSTM Networks](https://arxiv.org/abs/2509.19374)
*Oscar A. Oviedo*

Main category: eess.SP

TL;DR: 开发基于LSTM的深度学习模型来预测阿根廷科尔多瓦的短期小时电力需求，整合历史消费数据和外部变量，实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 为电网运营商提供优化的规划和控制策略，应对不同需求场景下的电力管理挑战。

Method: 使用LSTM网络结合历史消费数据、气候因素、时间周期和人口统计等外部变量，进行模型设计和超参数优化，并辅以随机森林回归的可解释性分析和日需求极值时间预测评估。

Result: 模型预测精度高，平均绝对百分比误差为3.20%，决定系数为0.95；在超过三分之二的测试日中准确预测日需求极值时间，90%以上案例误差在1小时内。

Conclusion: 提出的框架具有高预测准确性和操作相关性，为电网运营商提供了有价值的见解，增强了模型在季节性模式和极端消费事件中的鲁棒性和泛化能力。

Abstract: This study presents the development and optimization of a deep learning model
based on Long Short-Term Memory (LSTM) networks to predict short-term hourly
electricity demand in C\'ordoba, Argentina. Integrating historical consumption
data with exogenous variables (climatic factors, temporal cycles, and
demographic statistics), the model achieved high predictive precision, with a
mean absolute percentage error of 3.20\% and a determination coefficient of
0.95. The inclusion of periodic temporal encodings and weather variables proved
crucial to capture seasonal patterns and extreme consumption events, enhancing
the robustness and generalizability of the model. In addition to the design and
hyperparameter optimization of the LSTM architecture, two complementary
analyses were carried out: (i) an interpretability study using Random Forest
regression to quantify the relative importance of exogenous drivers, and (ii)
an evaluation of model performance in predicting the timing of daily demand
maxima and minima, achieving exact-hour accuracy in more than two-thirds of the
test days and within abs(1) hour in over 90\% of cases. Together, these results
highlight both the predictive accuracy and operational relevance of the
proposed framework, providing valuable insights for grid operators seeking
optimized planning and control strategies under diverse demand scenarios.

</details>


### [21] [On the Invariance of Cross-Correlation Peak Positions Under Monotonic Signal Transformations, with Application to Fast Time Difference Estimation](https://arxiv.org/abs/2509.19974)
*Natsuki Ueno,Ryotaro Sato,Nobutaka Ono*

Main category: eess.SP

TL;DR: 提出关于互相关峰值位置不变性的定理，为比传统FFT方法更快速的时间差估计方法提供理论基础。该方法利用信号量化到低比特整数的互相关函数，仅需整数运算而非实数运算。


<details>
  <summary>Details</summary>
Motivation: 传统基于FFT的时间差估计方法计算复杂度较高，需要寻找更高效的替代方案。通过理论分析发现互相关峰值位置在输入信号任意单调变换下保持不变，这为开发更快速算法提供了可能。

Method: 基于互相关峰值位置不变性定理，设计使用低比特整数量化信号的互相关函数进行时间差估计。该方法仅需整数算术运算，并可利用数论算法进一步提升计算效率。

Result: 数值实验表明，所提出的方法在处理时间上比传统基于FFT的方法更短，验证了理论分析的有效性。

Conclusion: 该定理为时间差估计提供了新的理论基础，基于整数运算的方法在计算效率上优于传统FFT方法，具有实际应用价值。

Abstract: We present a theorem concerning the invariance of cross-correlation peak
positions, which provides a foundation for a new method for time difference
estimation that is potentially faster than the conventional fast Fourier
transform (FFT) approach for real/complex sequences. This theoretical result
shows that the peak position of the cross-correlation function between two
shifted discrete-time signals remains unchanged under arbitrary monotonic
transformations of the input signals. By exploiting this property, we design an
efficient estimation algorithm based on the cross-correlation function between
signals quantized into low-bit integers. The proposed method requires only
integer arithmetic instead of real-valued operations, and further computational
efficiency can be achieved through number-theoretic algorithms. Numerical
experiments demonstrate that the proposed method achieves a shorter processing
time than conventional FFT-based approaches.

</details>


### [22] [Neural Network Based Framework for Passive Intermodulation Cancellation in MIMO Systems](https://arxiv.org/abs/2509.19382)
*Xiaolong Li,Zhi-qin John Xu,Peiting You,Yifei Zhu*

Main category: eess.SP

TL;DR: 提出了一种轻量级深度学习框架，利用深度可分离卷积和空洞卷积来有效抑制MIMO-OFDM系统中的无源互调干扰。


<details>
  <summary>Details</summary>
Motivation: 5G及后续通信系统中，无源互调成为关键的自干扰源，传统非线性模型方法计算复杂且扩展性有限。

Method: 采用深度可分离卷积和空洞卷积来高效捕捉天线和子载波间的非线性依赖关系，结合循环学习率调度和梯度裁剪来提升收敛性。

Result: 在MIMO实验设置中，该方法有效抑制了三阶无源互调失真，仅用11k可训练参数就实现了高达29dB的平均功率误差。

Conclusion: 紧凑的神经网络架构在未来无线通信系统中具有可扩展干扰抑制的潜力。

Abstract: Passive intermodulation (PIM) has emerged as a critical source of
self-interference in modern MIMO-OFDM systems, especially under the stringent
requirements of 5G and beyond. Conventional cancellation methods often rely on
complex nonlinear models with limited scalability and high computational cost.
In this work, we propose a lightweight deep learning framework for PIM
cancellation that leverages depthwise separable convolutions and dilated
convolutions to efficiently capture nonlinear dependencies across antennas and
subcarriers. To further enhance convergence, we adopt a cyclic learning rate
schedule and gradient clipping. In a controlled MIMO experimental setup, the
method effectively suppresses third-order passive intermodulation (PIM)
distortion, achieving up to 29dB of average power error (APE) with only 11k
trainable parameters. These results highlight the potential of compact neural
architectures for scalable interference mitigation in future wireless
communication systems.

</details>


### [23] [Impact of RHIs and ipSIC on Active RIS-NOMA Systems with Low-Precision ADCs](https://arxiv.org/abs/2509.19383)
*Qianqian Li,Hua Li,Shiya Hao,Lintao Li,Xiaoming Dai*

Main category: eess.SP

TL;DR: 本研究评估了采用低精度ADC的主动可重构智能表面辅助非正交多址系统的性能，推导了考虑硬件损伤和干扰消除不完美的中断概率近似表达式，并通过仿真证明主动RIS-NOMA系统在性能上优于被动RIS-NOMA系统。


<details>
  <summary>Details</summary>
Motivation: 传统被动RIS系统存在性能限制，而主动RIS具有信号放大能力，结合低精度ADC可以降低系统成本和功耗，但需要分析其在NOMA系统中的实际性能表现。

Method: 推导了考虑残余硬件损伤和干扰消除不完美的中断概率解析近似表达式，分析了高信噪比下的渐近性能、系统吞吐量和分集阶数，并通过仿真验证理论分析。

Result: 主动RIS-NOMA系统相比被动版本具有更低的中断概率和更高的吞吐量，且所需发射功率和反射单元数量更少。增加反射单元数量能显著改善两种系统的中断性能。

Conclusion: 通过优化发射功率和调整反射单元数量，可以有效缓解低精度ADC的负面影响，主动RIS-NOMA系统在性能上具有明显优势。

Abstract: This study evaluates the performance of an active reconfigurable intelligent
surface (ARIS)-assisted non-orthogonal multiple access (NOMA) system employing
low-precision analog-to-digital converters (ADCs). Analytical approximations
for the outage probability (OP) are derived, considering residual hardware
impairments (RHIs) and imperfect successive interference cancellation (ipSIC).
Additionally, we analyze the asymptotic OP, system throughput, and diversity
order at high signal-to-noise ratios (SNRs). Simulation results demonstrate
that the proposed quantized ARIS-NOMA system outperforms its passive
counterpart (PRIS-NOMA), achieving lower OP and higher throughput with reduced
transmit power requirements and fewer reflecting elements. Moreover, the outage
performance of both quantized ARIS-NOMA and PRIS-NOMA systems demonstrates
significant improvement as the number of reflecting elements increases. The
negative impacts of low-precision ADCs can be effectively mitigated by
optimizing transmit power and scaling the number of reflecting elements.

</details>


### [24] [Data-Driven Reconstruction of Significant Wave Heights from Sparse Observations](https://arxiv.org/abs/2509.19384)
*Hongyuan Shi,Yilin Zhai,Ping Dong,Zaijin You,Chao Zhan,Qing Wang*

Main category: eess.SP

TL;DR: AUWave是一个混合深度学习框架，用于从稀疏浮标观测重建高分辨率区域有效波高场，通过多尺度U-Net和自注意力机制实现32×32区域的重建。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏不均匀浮标观测重建高分辨率区域有效波高场的核心挑战，为海洋监测和风险感知操作提供支持。

Method: 融合站点序列编码器（MLP）与增强多尺度U-Net（包含瓶颈自注意力层）的混合深度学习框架，使用贝叶斯超参数搜索优化学习率等参数。

Result: 在夏威夷区域使用NDBC浮标观测和ERA5再分析数据，获得最小验证损失0.043285，RMSE分布略微右偏，在观测点附近误差最低，随距离增加而升高。

Conclusion: AUWave在数据较丰富配置下优于基准方法，多尺度和注意力组件在最小但非平凡空间锚定可用时提供精度增益，为网络设计提供可操作指导，为数据同化和应急重建提供可扩展路径。

Abstract: Reconstructing high-resolution regional significant wave height fields from
sparse and uneven buoy observations remains a core challenge for ocean
monitoring and risk-aware operations. We introduce AUWave, a hybrid deep
learning framework that fuses a station-wise sequence encoder (MLP) with a
multi-scale U-Net enhanced by a bottleneck self-attention layer to recover
32$\times$32 regional SWH fields. A systematic Bayesian hyperparameter search
with Optuna identifies the learning rate as the dominant driver of
generalization, followed by the scheduler decay and the latent dimension. Using
NDBC buoy observations and ERA5 reanalysis over the Hawaii region, AUWave
attains a minimum validation loss of 0.043285 and a slightly right-skewed RMSE
distribution. Spatial errors are lowest near observation sites and increase
with distance, reflecting identifiability limits under sparse sampling.
Sensitivity experiments show that AUWave consistently outperforms a
representative baseline in data-richer configurations, while the baseline is
only marginally competitive in the most underdetermined single-buoy cases. The
architecture's multi-scale and attention components translate into accuracy
gains when minimal but non-trivial spatial anchoring is available. Error maps
and buoy ablations reveal key anchor stations whose removal disproportionately
degrades performance, offering actionable guidance for network design. AUWave
provides a scalable pathway for gap filling, high-resolution priors for data
assimilation, and contingency reconstruction.

</details>


### [25] [A Statistical Mixture-of-Experts Framework for EMG Artifact Removal in EEG: Empirical Insights and a Proof-of-Concept Application](https://arxiv.org/abs/2509.19385)
*Benjamin J. Choi,Griffin Milsap,Clara A. Scholl,Francesco Tenore,Mattson Ogg*

Main category: eess.SP

TL;DR: 本文提出了一种基于混合专家（MoE）框架的脑电图（EEG）去噪算法，专门针对高噪声环境下的肌电（EMG）伪影去除问题。该算法通过三个新的统计洞察来改进现有方法，并在EEGdenoiseNet数据集上验证了其在高噪声场景下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经接口控制受限于信号质量差的问题。虽然基于神经网络的EEG去噪方法有所改进，但在高噪声环境下现有最先进模型表现不佳。需要开发更有效的EMG伪影去除算法来提升神经接口性能。

Method: 提出基于混合专家框架的信号滤波算法，利用三个关键洞察：1）将EMG伪影划分为可量化的子类型；2）在更窄信噪比范围内训练局部专家实现专业化；3）使用基于相关性的目标函数和重缩放算法加速收敛。算法结合了CNN和RNN神经网络。

Result: 在EEGdenoiseNet数据集（67名受试者）上的测试显示，MoE去噪模型在整体性能上与最先进ML算法相当，在高噪声环境下表现出更优的下界性能。

Conclusion: MoE框架在EMG伪影去除方面展现出潜力，特别是在高噪声环境中。需要进一步研究来评估该框架在更广泛真实场景中的应用，探索其在开发更有效神经接口方面的下游潜力。

Abstract: Effective control of neural interfaces is limited by poor signal quality.
While neural network-based electroencephalography (EEG) denoising methods for
electromyogenic (EMG) artifacts have improved in recent years, current
state-of-the-art (SOTA) models perform suboptimally in settings with high
noise. To address the shortcomings of current machine learning (ML)-based
denoising algorithms, we present a signal filtration algorithm driven by a new
mixture-of-experts (MoE) framework. Our algorithm leverages three new
statistical insights into the EEG-EMG denoising problem: (1) EMG artifacts can
be partitioned into quantifiable subtypes to aid downstream MoE classification,
(2) local experts trained on narrower signal-to-noise ratio (SNR) ranges can
achieve performance increases through specialization, and (3) correlation-based
objective functions, in conjunction with rescaling algorithms, can enable
faster convergence in a neural network-based denoising context. We empirically
demonstrate these three insights into EMG artifact removal and use our findings
to create a new downstream MoE denoising algorithm consisting of convolutional
(CNN) and recurrent (RNN) neural networks. We tested all results on a major
benchmark dataset (EEGdenoiseNet) collected from 67 subjects. We found that our
MoE denoising model achieved competitive overall performance with SOTA ML
denoising algorithms and superior lower bound performance in high noise
settings. These preliminary results highlight the promise of our MoE framework
for enabling advances in EMG artifact removal for EEG processing, especially in
high noise settings. Further research and development will be necessary to
assess our MoE framework on a wider range of real-world test cases and explore
its downstream potential to unlock more effective neural interfaces.

</details>


### [26] [Hybrid Pipeline SWD Detection in Long-Term EEG Signals](https://arxiv.org/abs/2509.19387)
*Antonio Quintero Rincon,Nicolas Masino,Veronica Marsico,Hadj Batatia*

Main category: eess.SP

TL;DR: 提出了一种轻量级混合管道，结合分析特征和浅层人工神经网络，用于在长期单极脑电图中准确检测棘慢波放电。


<details>
  <summary>Details</summary>
Motivation: 棘慢波放电是失神癫痫的脑电图特征，但在多日记录中手动识别既费时又容易出错，需要自动化检测方法。

Method: 使用双边移动平均滤波器抑制正常背景活动的高频成分，然后计算残差信号的正态分布样本均值和标准差作为特征向量，输入单隐藏层人工神经网络进行分类。

Result: 在12名患者的780个通道上评估，正确检测384个事件（灵敏度98%），特异性96.2%，总体准确率97.2%。

Conclusion: 正态分布描述符结合小型人工神经网络为扩展脑电图记录的自动SWD筛查提供了有效且计算成本低的解决方案。

Abstract: Spike-and-wave discharges (SWDs) are the electroencephalographic hallmark of
absence epilepsy, yet their manual identification in multi-day recordings
remains labour-intensive and error-prone. We present a lightweight hybrid
pipeline that couples analytical features with a shallow artificial neural
network (ANN) for accurate, patient-specific SWD detection in long-term,
monopolar EEG. A two-sided moving-average (MA) filter first suppresses the
high-frequency components of normal background activity. The residual signal is
then summarised by the mean and the standard deviation of its normally
distributed samples, yielding a compact, two-dimensional feature vector for
every 20s window. These features are fed to a single-hidden-layer ANN trained
via back-propagation to classify each window as SWD or non-SWD. The method was
evaluated on 780 channels sampled at 256 Hz from 12 patients, comprising 392
annotated SWD events. It correctly detected 384 events (sensitivity: 98%) while
achieving a specificity of 96.2 % and an overall accuracy of 97.2%. Because
feature extraction is analytic, and the classifier is small, the pipeline runs
in real-time and requires no manual threshold tuning. These results indicate
that normal-distribution descriptors combined with a modest ANN provide an
effective and computationally inexpensive solution for automated SWD screening
in extended EEG recordings.

</details>


### [27] [Self-Alignment Learning to Improve Myocardial Infarction Detection from Single-Lead ECG](https://arxiv.org/abs/2509.19397)
*Jiarui Jin,Xiaocheng Fang,Haoyu Wang,Jun Li,Che Liu,Donglin Xie,Hongyan Li,Shenda Hong*

Main category: eess.SP

TL;DR: SelfMIS提出了一种简单有效的对齐学习框架，通过自切割策略将多导联心电图与对应的单导联片段配对，直接在潜在空间中对齐，以改善单导联心电图的心肌梗死检测性能。


<details>
  <summary>Details</summary>
Motivation: 单导联心电图检测心肌梗死因空间信息有限而具有挑战性。虽然将单导联转换为多导联进行检测是直观想法，但信号级生成方法存在潜在空间差距，影响诊断性能。现有心电图对齐方法主要关注变换不变性学习，与单导联检测目标不匹配。

Method: SelfMIS采用自切割策略，无需手动数据增强，将多导联心电图与对应的单导联片段配对，直接在潜在空间中对齐。这种设计将学习目标从追求变换不变性转变为丰富单导联表示，使单导联编码器能够从局部信号推断全局心脏上下文。

Result: 实验表明，SelfMIS在九种心肌梗死类型上均优于基线模型，同时保持更简单的架构和更低的计算开销。

Conclusion: 直接潜在空间对齐方法有效提升了单导联心电图心肌梗死检测性能，证明了该方法的有效性。

Abstract: Myocardial infarction is a critical manifestation of coronary artery disease,
yet detecting it from single-lead electrocardiogram (ECG) remains challenging
due to limited spatial information. An intuitive idea is to convert single-lead
into multiple-lead ECG for classification by pre-trained models, but generative
methods optimized at the signal level in most cases leave a large latent space
gap, ultimately degrading diagnostic performance. This naturally raises the
question of whether latent space alignment could help. However, most prior ECG
alignment methods focus on learning transformation invariance, which mismatches
the goal of single-lead detection. To address this issue, we propose SelfMIS, a
simple yet effective alignment learning framework to improve myocardial
infarction detection from single-lead ECG. Discarding manual data
augmentations, SelfMIS employs a self-cutting strategy to pair multiple-lead
ECG with their corresponding single-lead segments and directly align them in
the latent space. This design shifts the learning objective from pursuing
transformation invariance to enriching the single-lead representation,
explicitly driving the single-lead ECG encoder to learn a representation
capable of inferring global cardiac context from the local signal.
Experimentally, SelfMIS achieves superior performance over baseline models
across nine myocardial infarction types while maintaining a simpler
architecture and lower computational overhead, thereby substantiating the
efficacy of direct latent space alignment. Our code and checkpoint will be
publicly available after acceptance.

</details>


### [28] [SpellerSSL: Self-Supervised Learning with P300 Aggregation for Speller BCIs](https://arxiv.org/abs/2509.19401)
*Jiazhen Hong,Geoff Mackellar,Soheila Ghane*

Main category: eess.SP

TL;DR: SpellerSSL框架结合自监督学习和P300聚合技术，解决了EEG P300拼写器面临的低信噪比、泛化能力差和校准耗时三大挑战，显著提升了字符识别率和信息传输速率。


<details>
  <summary>Details</summary>
Motivation: 解决基于EEG的P300拼写器脑机接口面临的三个主要挑战：低信噪比、泛化能力差和耗时校准过程。

Method: 提出SpellerSSL框架，结合自监督学习和P300聚合策略。使用定制的1D U-Net骨干网络，在跨域和域内EEG数据上进行预训练，然后通过轻量级ERP-Head分类器进行微调。

Result: 在II-B公开数据集上，域内自监督学习达到94%的字符识别率（仅需7次重复），最高信息传输速率达21.86 bits/min。P300聚合策略将所需校准数据量减少60%。

Conclusion: 这是首个将自监督学习应用于P300拼写器的研究，展示了其在提高脑机接口效率和泛化能力方面的潜力，为P300拼写器EEG基础模型的发展铺平了道路。

Abstract: Electroencephalogram (EEG)-based P300 speller brain-computer interfaces
(BCIs) face three main challenges: low signal-to-noise ratio (SNR), poor
generalization, and time-consuming calibration. We propose SpellerSSL, a
framework that combines self-supervised learning (SSL) with P300 aggregation to
address these issues. First, we introduce an aggregation strategy to enhance
SNR. Second, to achieve generalization in training, we employ a customized 1D
U-Net backbone and pretrain the model on both cross-domain and in-domain EEG
data. The pretrained model is subsequently fine-tuned with a lightweight
ERP-Head classifier for P300 detection, which adapts the learned
representations to subject-specific data. Our evaluations on calibration time
demonstrate that combining the aggregation strategy with SSL significantly
reduces the calibration burden per subject and improves robustness across
subjects. Experimental results show that SSL learns effective EEG
representations in both in-domain and cross-domain, with in-domain achieving a
state-of-the-art character recognition rate of 94% with only 7 repetitions and
the highest information transfer rate (ITR) of 21.86 bits/min on the public
II-B dataset. Moreover, in-domain SSL with P300 aggregation reduces the
required calibration size by 60% while maintaining a comparable character
recognition rate. To the best of our knowledge, this is the first study to
apply SSL to P300 spellers, highlighting its potential to improve both
efficiency and generalization in speller BCIs and paving the way toward an EEG
foundation model for P300 speller BCIs.

</details>


### [29] [Online Adaptation via Dual-Stage Alignment and Self-Supervision for Fast-Calibration Brain-Computer Interfaces](https://arxiv.org/abs/2509.19403)
*Sheng-Bin Duan,Jian-Long Hao,Tian-Yu Xiang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Zeng-Guang Hou*

Main category: eess.SP

TL;DR: 该研究提出了一种基于双阶段对齐和自监督的在线自适应算法，用于解决脑电图中个体差异对脑机接口系统在线应用的阻碍。


<details>
  <summary>Details</summary>
Motivation: 脑电图中的个体差异阻碍了基于脑电图的脑机接口系统的在线应用，需要一种能够快速适应新受试者的方法。

Method: 采用双阶段对齐方法：先在EEG数据空间进行欧几里得对齐，然后在表示空间更新批归一化统计量。同时设计自监督损失函数，使用解码器生成的软伪标签作为未知真实标签的代理，并通过香农熵校准以促进自监督训练。

Result: 在五个公共数据集和七个解码器上的实验表明，该算法可以无缝集成，不受BCI范式和解码器架构的限制。每次迭代仅需一个在线试验更新解码器，在稳态视觉诱发电位任务上平均准确率提升4.9%，在运动想象任务上提升3.6%。

Conclusion: 该算法支持快速校准操作，在脑机接口应用中具有巨大潜力。

Abstract: Individual differences in brain activity hinder the online application of
electroencephalogram (EEG)-based brain computer interface (BCI) systems. To
overcome this limitation, this study proposes an online adaptation algorithm
for unseen subjects via dual-stage alignment and self-supervision. The
alignment process begins by applying Euclidean alignment in the EEG data space
and then updates batch normalization statistics in the representation space.
Moreover, a self-supervised loss is designed to update the decoder. The loss is
computed by soft pseudo-labels derived from the decoder as a proxy for the
unknown ground truth, and is calibrated by Shannon entropy to facilitate
self-supervised training. Experiments across five public datasets and seven
decoders show the proposed algorithm can be integrated seamlessly regardless of
BCI paradigm and decoder architecture. In each iteration, the decoder is
updated with a single online trial, which yields average accuracy gains of 4.9%
on steady-state visual evoked potentials (SSVEP) and 3.6% on motor imagery.
These results support fast-calibration operation and show that the proposed
algorithm has great potential for BCI applications.

</details>


### [30] [Insights into Xona Pulsar LEO PNT: Constellation, Signals, and Receiver Design](https://arxiv.org/abs/2509.19551)
*Jérôme Leclère,Thyagaraja Marathe,Tyler G. R. Reid*

Main category: eess.SP

TL;DR: 本文分析了LEO星座Pulsar的GNSS特性，通过与GPS对比，研究了卫星参数特性及其对接收机设计的影响，提出了优化策略以降低功耗和缩短捕获时间。


<details>
  <summary>Details</summary>
Motivation: 随着LEO星座如Pulsar在PNT领域的重要性增加，需要深入理解其信号特性和星座几何结构的变化，以指导接收机设计优化。

Method: 使用GNSS模拟器分析Pulsar卫星的通过时长、仰角、多普勒频移、多普勒变化率、距离和可见卫星数等参数，并与GPS进行对比，考察不同纬度和仰角掩模的影响。

Result: LEO系统具有更强的信号、更快的动态特性、更短的首次定位时间，但需要处理更大的多普勒范围和更高的多普勒变化率，星座结构也与传统GNSS不同。

Conclusion: 通过参数分析和优化策略（如预测和优先级技术），可以显著减少捕获时间并降低接收机功耗，为LEO GNSS接收机设计提供重要指导。

Abstract: The landscape of global navigation satellite systems (GNSS) is expanding with
the emergence of low Earth orbit (LEO) constellations such as Pulsar, which are
expected to play a key role in the future of positioning, navigation, and
timing (PNT). LEO-based systems provide advantages including stronger signals
for greater robustness, faster dynamics that aid convergence and multipath
mitigation, and shorter time to first fix (TTFF) enabled by high data rates.
These benefits, however, come with changes in signal behavior and constellation
geometry that require careful consideration in receiver design. This paper
investigates Pulsar properties using a GNSS simulator, analyzing parameters
such as satellite pass duration, elevation, Doppler shift, Doppler rate, range,
and number of satellites in view. Comparisons with GPS highlight the
differences introduced by LEO operation. The analysis examines temporal
evolution, statistical distributions, and maximum and minimum values. Beyond
these statistical insights, the study explores interdependencies between
parameters and differences across satellites, providing additional perspective.
Evaluations are performed at multiple latitudes to ensure a worldwide
perspective, and the impact of applying different elevation masks is discussed
where relevant. Building on these findings, the paper assesses Pulsar's impact
on receiver design from two standpoints: design considerations, addressing
expanded Doppler ranges, higher Doppler rates, and unique constellation
structure; and design optimizations, exploiting parameter analyses and
interdependencies (e.g., Doppler rate vs Doppler) to refine acquisition
strategies and applying prediction and prioritization techniques to avoid
unnecessary computations. Together, these optimizations can reduce acquisition
time and lower receiver power consumption.

</details>


### [31] [DNN-Based Nulling Control Beam Focusing for Near-Field Multi-User Interference Mitigation](https://arxiv.org/abs/2509.19594)
*Mohammadhossein Karimi,Yuanzhe Gong,Tho Le-Ngoc*

Main category: eess.SP

TL;DR: 提出基于深度学习的近场零陷控制波束聚焦框架，用于超大MIMO系统中的多用户干扰抑制


<details>
  <summary>Details</summary>
Motivation: 解决近场多用户无线通信中的干扰问题，实现可扩展的实时波束聚焦

Method: 开发双估计器架构，包含两个全连接深度神经网络，分别预测NCBF权重的相位和幅度分量，使用期望用户和干扰用户的位置信息

Result: DNN模型预测精度高，相位估计误差0.067弧度，幅度估计误差0.206dB，平均MUI抑制36.7dB，所有测试案例干扰抑制超过17.5dB

Conclusion: 该方法能够实现可扩展的实时波束聚焦和有效干扰抑制，为未来近场多用户无线通信提供有前景的解决方案

Abstract: This paper proposes a deep learning-based framework for near-field nulling
control beam focusing (NCBF) in extra-large MIMO (XL-MIMO) systems to mitigate
multi-user interference (MUI). A dual-estimator architecture comprising two
fully connected deep neural networks (FCDNNs) is developed to separately
predict the phase and magnitude components of NCBF weights, using locations of
both desired and interfering users. The models are trained on a large dataset
generated via a Linearly Constrained Minimum Variance (LCMV) beamforming
algorithm to accommodate diverse user configurations, including both collinear
and non-collinear scenarios. Illustrative results demonstrate that the proposed
DNN models achieve high prediction accuracy, with test errors of only 0.067
radians for phase estimation and 0.206 dB for magnitude estimation. Full-wave
simulations incorporating realistic element radiation patterns and
inter-element coupling confirm the close agreement between the beam patterns
produced by the DNN-predicted and LCMV-based NCBF schemes under practical
deployment conditions. An average MUI suppression of 36.7 dB is achieved, with
interference mitigation exceeding 17.5 dB across all tested cases. The proposed
approach enables scalable and real-time beam focusing with effective
interference suppression, offering a promising solution for future near-field
multi-user wireless communications.

</details>


### [32] [Timeliness-Aware Joint Source and Channel Coding for Adaptive Image Transmission](https://arxiv.org/abs/2509.19754)
*Xiaolei Yang,Zijing Wang,Zhijin Qin,Xiaoming Tao*

Main category: eess.SP

TL;DR: 提出了一种基于价值信息（VoI）的自适应联合源信道编码方法，用于时间敏感应用中的图像传输，同时考虑重建质量和时效性。


<details>
  <summary>Details</summary>
Motivation: 现有无线系统带宽限制难以满足高保真和低延迟图像传输需求，语义通信有望通过传输目标导向的语义信息来突破性能瓶颈。

Method: 设计自适应码长的JSCC框架，构建VoI最大化问题，并提出基于深度强化学习的算法来优化传输码长。

Result: 实验结果表明，该方法在重建质量和时效性方面显著优于基线方案，特别是在低信噪比条件下表现优异。

Conclusion: 该方法为时间敏感无线网络中的高效鲁棒图像传输提供了有前景的解决方案。

Abstract: Accurate and timely image transmission is critical for emerging
time-sensitive applications such as remote sensing in satellite-assisted
Internet of Things. However, the bandwidth limitation poses a significant
challenge in existing wireless systems, making it difficult to fulfill the
requirements of both high-fidelity and low-latency image transmission. Semantic
communication is expected to break through the performance bottleneck by
focusing on the transmission of goal-oriented semantic information rather than
raw data. In this paper, we employ a new timeliness metric named the value of
information (VoI) and propose an adaptive joint source and channel coding
(JSCC) method for image transmission that simultaneously considers both
reconstruction quality and timeliness. Specifically, we first design a JSCC
framework for image transmission with adaptive code length. Next, we formulate
a VoI maximization problem by optimizing the transmission code length of the
adaptive JSCC under the reconstruction quality constraint. Then, a deep
reinforcement learning-based algorithm is proposed to solve the optimization
problem efficiently. Experimental results show that the proposed method
significantly outperforms baseline schemes in terms of reconstruction quality
and timeliness, particularly in low signal-to-noise ratio conditions, offering
a promising solution for efficient and robust image transmission in
time-sensitive wireless networks.

</details>


### [33] [Electromagnetics-Compliant Optimization of Dynamic Metasurface Antennas for Bistatic Sensing](https://arxiv.org/abs/2509.19801)
*Ioannis Gavras,George C. Alexandropoulos*

Main category: eess.SP

TL;DR: 本文提出了一种基于动态超表面天线（DMA）的波束成形优化方法，用于双基地传感系统，考虑了物理约束和不确定性因素。


<details>
  <summary>Details</summary>
Motivation: 现有DMA研究大多依赖理想化模型，忽略了超材料固有的结构约束和物理特性，如互耦效应和波导传播损耗。

Method: 首先提出DMA响应的可处理近似模型，然后构建鲁棒波束成形优化问题，最小化最坏情况位置误差界，并设计了两种低复杂度波束成形方法。

Result: 蒙特卡洛仿真验证了所提设计的准确性，表明准确建模互耦效应对保持定位性能至关重要，所提方法在满足DMA结构约束的同时，性能可与全数字和模拟方案相媲美。

Conclusion: 本文提出的DMA优化设计在存在定位和同步不确定性的情况下，仍能保持高精度定位性能，为下一代无线系统提供了可行的低功耗解决方案。

Abstract: Dynamic Metasurface Antennas (DMAs) are recently attracting considerable
research interests due to their potential to enable low-cost, reconfigurable,
and highly scalable antenna array architectures for next generation wireless
systems. However, most of the existing literature relies on idealized models
for the DMA operation, often overlooking critical structural and physical
constraints inherent to their constituent metamaterials. In this paper,
leveraging a recently proposed model for this antenna architecture
incorporating physically consistent modeling of mutual coupling and waveguide
propagation losses, we optimize DMA-based transmission for bistatic sensing. A
tractable approximation for the DMA response is first presented, which enables
efficient optimization of the dynamically reconfigurable Lorentzian-constrained
responses of the array's metamaterials. In particular, we formulate a robust
beamforming optimization problem with the objective to minimize the worst-case
position error bound, in the presence of spatial uncertainties for the
environment's scatterers as well as synchronization uncertainties at the analog
combining multi-antenna receiver. To address the resulting high computational
complexity due to the possibly excessive number of metamaterial-based antennas
and their operation constraints, two low complexity beamforming design
approaches are presented that perform offline searching over a novel beam
codebook. The accuracy of all presented DMA designs is assessed by means of
Monte Carlo simulations for various system parameters, confirming that
accurately modeling mutual coupling is essential for maintaining increased
localization performance. It is also shown that, even under positioning and
synchronization uncertainties, the proposed designs yield accuracy comparable
to their fully digital and analog counterparts, while adhering to the
structural DMA constraints.

</details>


### [34] [Generalized Nonnegative Structured Kruskal Tensor Regression](https://arxiv.org/abs/2509.19900)
*Xinjue Wang,Esa Ollila,Sergiy A. Vorobyov,Ammar Mian*

Main category: eess.SP

TL;DR: 本文提出了广义非负结构化Kruskal张量回归(NS-KTR)框架，通过模式特定的混合正则化和非负约束来提高可解释性和性能。该方法适用于线性和逻辑回归，处理多维张量数据的结构异质性，在合成信号和真实高光谱数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统张量回归方法在处理多维张量数据的结构异质性方面存在局限，缺乏对特定张量模式的结构特征保留能力，且可解释性不足。需要开发一个能够同时保持结构特征和物理可解释性的框架。

Method: 集成融合LASSO、全变分和岭正则化器，每个正则化器针对特定张量模式定制。开发了基于交替方向乘子法(ADMM)的高效参数估计算法，支持线性和逻辑回归公式。

Result: 在合成信号和真实高光谱数据集上的综合实验表明，NS-KTR在性能上持续优于传统张量回归方法，能够有效保留张量维度的结构特征。

Conclusion: NS-KTR框架在保持张量维度不同结构特征的同时确保物理可解释性，特别适用于信号处理和高光谱图像分析应用。

Abstract: This paper introduces Generalized Nonnegative Structured Kruskal Tensor
Regression (NS-KTR), a novel tensor regression framework that enhances
interpretability and performance through mode-specific hybrid regularization
and nonnegativity constraints. Our approach accommodates both linear and
logistic regression formulations for diverse response variables while
addressing the structural heterogeneity inherent in multidimensional tensor
data. We integrate fused LASSO, total variation, and ridge regularizers, each
tailored to specific tensor modes, and develop an efficient alternating
direction method of multipliers (ADMM) based algorithm for parameter
estimation. Comprehensive experiments on synthetic signals and real
hyperspectral datasets demonstrate that NS-KTR consistently outperforms
conventional tensor regression methods. The framework's ability to preserve
distinct structural characteristics across tensor dimensions while ensuring
physical interpretability makes it especially suitable for applications in
signal processing and hyperspectral image analysis.

</details>


### [35] [Rotatable Antenna Enabled Spectrum Sharing: Joint Antenna Orientation and Beamforming Design](https://arxiv.org/abs/2509.19912)
*Xingxiang Peng,Qingqing Wu,Ziyuan Zheng,Wen Chen,Yanze Zhu,Ying Gao*

Main category: eess.SP

TL;DR: 本文研究了可旋转天线在MISO干扰信道中的应用，通过联合优化发射波束成形和天线方向来最大化加权和速率，提出了交替优化框架和离散方向选择的交叉熵方法。


<details>
  <summary>Details</summary>
Motivation: 传统天线阵列通过增加元件数量来改善性能会导致硬件和功耗成本过高，而可旋转天线通过动态调整元件方向引入新的自由度，可以在不扩大阵列规模的情况下利用空间灵活性。

Method: 开发了交替优化框架，结合加权最小均方误差波束成形和Frank-Wolfe方向更新；针对有限分辨率执行器，构建球面斐波那契码本并设计基于交叉熵方法的离散方向选择算法。

Result: 仿真表明，将可旋转天线与传统波束成形结合显著提高了加权和速率，增益随元件方向性增强而增加；在离散方向控制下，提出的交叉熵方法算法始终优于最近投影基线。

Conclusion: 可旋转天线为干扰信道提供了一种有效的空间自由度利用方式，通过联合优化波束成形和天线方向可以显著改善系统性能，特别是在高方向性天线场景下效果更佳。

Abstract: Conventional antenna arrays rely primarily on digital beamforming for spatial
control. While adding more elements can narrow beamwidth and suppress
interference, such scaling incurs prohibitive hardware and power costs.
Rotatable antennas (RAs), which allow mechanical or electronic adjustment of
element orientations, introduce a new degree of freedom to exploit spatial
flexibility without enlarging the array. By dynamically optimizing
orientations, RAs can substantially improve desired link alignment and
interference suppression. This paper investigates RA-enabled multiple-input
single-output (MISO) interference channels under co-channel spectrum sharing
and formulates a weighted sum-rate maximization problem that jointly optimizes
transmit beamforming and antenna orientations. To tackle this nonconvex
problem, we develop an alternating optimization (AO) framework that integrates
weighted minimum mean-square error (WMMSE)-based beamforming with
Frank-Wolfe-based orientation updates. To reduce complexity, we further study
orientation optimization under maximum-ratio transmission (MRT) and
zero-forcing (ZF) beamforming schemes. For finite-resolution actuators, we
construct spherical Fibonacci codebooks and design a cross-entropy method
(CEM)-based algorithm for discrete orientation selection. Simulations show that
integrating RAs with conventional beamforming markedly increases weighted
sum-rate, with gains rising with element directivity. Under discrete
orientation control, the proposed CEM algorithm consistently outperforms the
nearest-projection baseline.

</details>


### [36] [Near-field Spatial-domain Channel Extrapolation for XL-MIMO Systems](https://arxiv.org/abs/2509.20026)
*Jiayi Lu,Jiayi Zhang,Hao Lei,Huahua Xiao,Bo Ai,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 提出一种用于多子载波XL-MIMO系统的自适应近场信道外推框架，通过天线子集选择和互相关最小化随机模式实现高效准确的信道估计


<details>
  <summary>Details</summary>
Motivation: XL-MIMO系统中动态RF链架构需要低复杂度获取准确CSI，现有方法忽略近场球面波前或过度依赖稀疏先验导致性能下降

Method: 开发了网格和离网格算法，离网格算法优化网格估计结果；引入交叉验证方案降低复杂度；分析传感矩阵互相关性并提出互相关最小化随机模式

Result: 数值结果表明所提算法在外推精度和可达速率上显著优于现有方法，同时保持低计算复杂度

Conclusion: 提出的CV比在精度和效率之间提供灵活权衡，对应的离网格算法以与传统网格方法相当的复杂度实现高精度

Abstract: Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are
pivotal to next-generation wireless communications, where dynamic RF chain
architectures offer enhanced performance. However, efficient precoding in such
systems requires accurate channel state information (CSI) obtained with low
complexity. To address this challenge, spatial-domain channel extrapolation has
attracted growing interest. Existing methods often overlook near-field
spherical wavefronts or rely heavily on sparsity priors, leading to performance
degradation. In this paper, we propose an adaptive near-field channel
extrapolation framework for multi-subcarrier XL-MIMO systems, leveraging a
strategically selected subset of antennas. Subsequently, we develop both
on-grid and off-grid algorithms, where the latter refines the former's
estimates for improved accuracy. To further reduce complexity, a
cross-validation (CV)-based scheme is introduced. Additionally, we analytically
formulate the mutual coherence of the sensing matrix and propose a
coherence-minimizing-based random pattern to ensure robust extrapolation.
Numerical results validate that the proposed algorithms significantly
outperform existing methods in both extrapolation accuracy and achievable rate,
while maintaining low computational complexity. In particular, our proposed CV
ratio offers a flexible trade-off between accuracy and efficiency, and the
corresponding off-grid algorithm achieves high accuracy with complexity
comparable to conventional on-grid methods.

</details>


### [37] [Multi-Stage CD-Kennedy Receiver for QPSK Modulated CV-QKD in Turbulent Channels](https://arxiv.org/abs/2509.20030)
*Renzhi Yuan,Zhixing Wang,Shouye Miao,Mufei Zhao,Haifeng Yao,Bin Cao,Mugen Peng*

Main category: eess.SP

TL;DR: 本文探讨了在湍流信道中使用多级CD-Kennedy量子接收器增强QPSK调制CV-QKD协议安全密钥率性能的可能性，提出了三种不同类型的接收器并分析了其性能。


<details>
  <summary>Details</summary>
Motivation: 连续变量量子密钥分发协议具有高安全密钥率和良好兼容性，但传统相干接收器性能受标准量子极限限制。量子接收器在湍流信道中的潜在应用尚未充分探索，而实际CV-QKD协议必须在大气湍流环境中工作。

Method: 首先推导了多级CD-Kennedy接收器在湍流信道中检测QPSK信号的错误概率，提出了三种不同类型的接收器（Type-I、Type-II、Type-III），然后推导了使用该接收器和后选择策略的CV-QKD协议的安全密钥率。

Result: 数值结果表明，多级CD-Kennedy接收器在湍流信道中的错误概率和安全密钥率性能均优于传统相干接收器，其中Type-II接收器在错误概率性能方面能够容忍更差的信道条件。

Conclusion: 量子接收器特别是多级CD-Kennedy接收器能够有效提升CV-QKD协议在湍流信道中的性能，为卫星对地光通信链路的实际应用提供了有前景的解决方案。

Abstract: Continuous variable-quantum key distribution (CV-QKD) protocols attract
increasing attentions in recent years because they enjoy high secret key rate
(SKR) and good compatibility with existing optical communication
infrastructure. Classical coherent receivers are widely employed in coherent
states based CV-QKD protocols, whose detection performance is bounded by the
standard quantum limit (SQL). Recently, quantum receivers based on displacement
operators are experimentally demonstrated with detection performance
outperforming the SQL in various practical conditions. However, potential
applications of quantum receivers in CV-QKD protocols under turbulent channels
are still not well explored, while practical CV-QKD protocols must survive from
the atmospheric turbulence in satellite-to-ground optical communication links.
In this paper, we consider the possibility of using a quantum receiver called
multi-stage CD-Kennedy receiver to enhance the SKR performance of a quadrature
phase shift keying (QPSK) modulated CV-QKD protocol in turbulent channels. We
first derive the error probability of the multi-stage CD-Kennedy receiver for
detecting QPSK signals in turbulent channels and further propose three types of
multi-stage CD-Kennedy receiver with different displacement choices, i.e., the
Type-I, Type-II, and Type-III receivers. Then we derive the SKR of a QPSK
modulated CV-QKD protocol using the multi-stage CD-Kennedy receiver and
post-selection strategy in turbulent channels. Numerical results show that the
multi-stage CD-Kennedy receiver can outperform the classical coherent receiver
in turbulent channels in terms of both error probability and SKR performance
and the Type-II receiver can tolerate worse channel conditions compared with
Type-I and Type-III receivers in terms of error probability performance.

</details>


### [38] [Reproduction Number and Spatial Connectivity Structure Estimation via Graph Sparsity-Promoting Penalized Functional](https://arxiv.org/abs/2509.20034)
*Etienne Lasalle,Barbara Pascal*

Main category: eess.SP

TL;DR: 该论文提出了一种联合估计有效再生数和空间连接结构的创新方法，以解决COVID-19疫情期间由于感染数据质量差而导致的监测挑战。


<details>
  <summary>Details</summary>
Motivation: 在流行病爆发期间，决策者需要准确可靠的工具来监测病原体传播。由于全球报告的感染数据质量较差，COVID-19疫情监测面临前所未有的挑战。当同时监测不同地区的疫情时，利用数据的空间结构可以显著提高再生数估计的准确性和鲁棒性，但这需要良好的空间结构估计。

Method: 提出了一种联合估计再生数和连接结构的程序。该方法通过在精心设计的合成数据上进行密集数值模拟来评估，并在真实的COVID-19时空感染数据上进行验证。

Result: 通过数值模拟和真实数据验证表明，该联合估计方法能够显著提高再生数估计的准确性和鲁棒性，特别是在数据质量较差的情况下。

Conclusion: 该研究为解决流行病监测中的空间结构估计问题提供了有效的解决方案，为决策者提供了更可靠的疫情监测工具，特别是在数据质量不理想的情况下。

Abstract: During an epidemic outbreak, decision makers crucially need accurate and
robust tools to monitor the pathogen propagation. The effective reproduction
number, defined as the expected number of secondary infections stemming from
one contaminated individual, is a state-of-the-art indicator quantifying the
epidemic intensity. Numerous estimators have been developed to precisely track
the reproduction number temporal evolution. Yet, COVID-19 pandemic surveillance
raised unprecedented challenges due to the poor quality of worldwide reported
infection counts. When monitoring the epidemic in different territories
simultaneously, leveraging the spatial structure of data significantly enhances
both the accuracy and robustness of reproduction number estimates. However,
this requires a good estimate of the spatial structure. To tackle this major
limitation, the present work proposes a joint estimator of the reproduction
number and connectivity structure. The procedure is assessed through intensive
numerical simulations on carefully designed synthetic data and illustrated on
real COVID-19 spatiotemporal infection counts.

</details>


### [39] [A dual bistatic optical forward transceiver configuration for determining the position of an acoustic communication source detected by optical communication fibers](https://arxiv.org/abs/2509.20046)
*Knut H. Grythe,Jan Erik Håkegård*

Main category: eess.SP

TL;DR: 本文提出了一种基于双光纤布局的水声通信系统，利用双向配置和双基地雷达原理实现声源定位，通过到达时间差(TDOA)估计位置，分析了定位精度的理论极限和系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统分布式声学传感(DAS)在双向配置中无法将源定位集成到信号解码过程中，对于需要源定位的场景，需要一种集成通信和定位的替代方案。

Method: 采用双光纤布局，每端配备光学发射器和接收器，利用双向配置接收两端的声信号，基于传播延迟差异使用TDOA方法估计源位置，推导Cramér-Rao界分析定位精度理论极限，使用交叉模糊函数作为最大似然估计器。

Result: 分析表明增加声学带宽和更高载波频率可以提高空间分辨率，仿真结果展示了在不同系统条件下的性能表现，定位精度虽低于DAS但为集成通信和定位提供了可行方案。

Conclusion: 该方法为水下声学通信系统提供了一种集成定位功能的可行方案，但实际应用中仍需解决关键技术挑战。

Abstract: Optical fibers have long been employed as sensors in a wide range of
commercial systems. Distributed Acoustic Sensing (DAS) extends this concept by
enabling the detection and localization of acoustic sources along the fiber,
using backscattered light from small segments to achieve spatial resolution on
the order of meters. Recently, DAS has also been explored as a component in
underwater acoustic communication systems. Emerging interest in bidirectional
configurations where both transmitter and receiver are placed at opposite ends
of the fiber has opened new possibilities. However, in such setups, source
localization is not inherently integrated into the signal decoding process. For
scenarios where source positioning is valuable, we propose an approach inspired
by bi-static radar principles. This configuration utilizes acoustic signals
received at both ends of the fiber to estimate source position based on
propagation delay differences. Although the localization accuracy is lower than
that of DAS due to reduced sampling rates, the method offers a viable
alternative for integrated communication and positioning. We present the system
topology and configuration for a dual-fiber layout, each end equipped with
optical transmitters and receivers. The position estimation is derived from the
time difference of arrival (TDOA) between the two receivers. The Cram\'er-Rao
Bound is derived to characterize the theoretical limits of localization
accuracy, highlighting dependencies on system parameters such as optical power
loss. Our analysis shows that increased acoustic bandwidth and higher carrier
frequencies enhance spatial resolution. We formulate the Cross Ambiguity
Function as a maximum likelihood estimator for TDOA and provide simulation
results illustrating its performance under varying system conditions. Finally,
we discuss key challenges that must be addressed for practical implementation.

</details>


### [40] [Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors](https://arxiv.org/abs/2509.20059)
*Koki Kanzaki,Koya Sato*

Main category: eess.SP

TL;DR: 提出了一种针对定位信息存在突发性误差环境的高精度无线电地图构建方法，通过将定位误差建模为可调参数嵌入边际对数似然函数，实现无线电地图构建过程中的位置不确定性事后校准。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图构建方法大多假设感知过程中位置信息无噪声，但实际设备定位系统（如GNSS）会产生几米到几十米的定位误差，忽略这些误差会导致无线电地图精度显著下降。

Method: 引入了一个新颖框架，将定位误差与无线电传播的空间相关性一起建模，通过将它们作为可调参数嵌入边际对数似然函数中，实现位置不确定性的事后校准。

Result: 基于实际人类移动数据的数值结果表明，所提方法能将RMSE退化限制在约0.25-0.29 dB，而基线方法的性能损失超过1 dB。

Conclusion: 该方法能有效处理定位误差问题，在存在位置不确定性的情况下仍能保持较高的无线电地图构建精度。

Abstract: This paper proposes a high-accuracy radio map construction method tailored
for environments where location information is affected by bursty errors. Radio
maps are an effective tool for visualizing wireless environments. Although
extensive research has been conducted on accurate radio map construction, most
existing approaches assume noise-free location information during sensing. In
practice, however, positioning errors ranging from a few to several tens of
meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring
such errors during inference can lead to significant degradation in radio map
accuracy. This study highlights that these errors often tend to be biased when
using mobile devices as sensors. We introduce a novel framework that models
these errors together with spatial correlation in radio propagation by
embedding them as tunable parameters in the marginal log-likelihood function.
This enables ex-post calibration of location uncertainty during radio map
construction. Numerical results based on practical human mobility data
demonstrate that the proposed method can limit RMSE degradation to
approximately 0.25-0.29 dB, compared with Gaussian process regression using
noise-free location data, whereas baseline methods suffer performance losses
exceeding 1 dB.

</details>


### [41] [Reciprocal Beyond-Diagonal Reconfigurable Intelligent Surface (BD-RIS): Scattering Matrix Design via Manifold Optimization](https://arxiv.org/abs/2509.20246)
*Marko Fidanovski,Iván Alexander Morales Sandoval,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu,Emil Björnson*

Main category: eess.SP

TL;DR: 本文研究了通过BD-RIS实现和速率最大化的问题，重点在于通过强制BD-RIS设计中的互易性来实现低复杂度物理实现的可行性。


<details>
  <summary>Details</summary>
Motivation: BD-RIS技术因其相对较低的成本和先进的信号处理能力，在恶劣城市环境中能够增强无线系统的性能和QoS。本文旨在通过设计对称散射矩阵来优化系统性能。

Method: 采用流形优化框架，在目标函数中添加惩罚项以确保对称约束，并通过将获得的解投影到可行散射矩阵集上来进一步强制执行互易性。

Result: 仿真结果表明，所提出的方法在和速率最大化方面优于当前最先进的方法。

Conclusion: 通过强制执行互易性的BD-RIS设计，能够有效实现低复杂度的物理实现，并在和速率最大化方面表现出优越性能。

Abstract: Beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) are emerging as
a transformative technology in wireless communications, enabling enhanced
performance and quality of service (QoS) of wireless systems in harsh urban
environments due to their relatively low cost and advanced signal processing
capabilities. Generally, BD-RIS systems are employed to improve robustness,
increase achievable rates, and enhance energy efficiency of wireless systems in
both direct and indirect ways. The direct way is to produce a favorable
propagation environment via the design of optimized scattering matrices, while
the indirect way is to reap additional improvements via the design of
multiple-input multiple-output (MIMO) beamformers that further exploit the
latter "engineered" medium. In this article, the problem of sum-rate
maximization via BD-RIS is examined, with a focus on feasibility, namely
low-complexity physical implementation, by enforcing reciprocity in the BD-RIS
design. We begin by outlining the system model and formulating an optimization
problem that aims to enhance the system's sum-rate by designing a symmetric
scattering matrix. In particular, the approach leverages a manifold
optimization framework, where a penalty term is added to the objective function
to ensure that the symmetry constraint is upheld, with reciprocity further
enforced by projecting the obtained solution onto a set of feasible scattering
matrices. Simulation results demonstrate the effectiveness of the proposed
method in outperforming current state-of-the-art (SotA) approaches in terms of
sum-rate maximization.

</details>


### [42] [Geometric Port Selection in CUMA Systems](https://arxiv.org/abs/2509.20299)
*Chenguang Rao,Kai-Kit Wong,Mohd Hamza Naim Shaikh,Hanjiang Hong,Hyundong Shin,Yangyang Zhang*

Main category: eess.SP

TL;DR: 本文提出了两种自适应单射频端口选择方案（EOHS和PCA），用于改进紧凑超大规模天线阵列（CUMA）技术，在保持低复杂度的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统CUMA的随机端口选择策略存在优化空间，需要开发更智能的自适应方案来提升干扰抑制性能。

Method: 提出了两种端口选择方案：EOHS方案动态选择最大化瞬时信号累积的投影方向；PCA方案基于主成分分析将端口分区与信道向量的主要统计方向对齐。

Result: 仿真结果表明，EOHS和PCA方案在各种用户密度、端口数量和FAS孔径尺寸下均优于传统CUMA，PCA方案能以较低计算成本实现接近EOHS的性能。

Conclusion: 所提方案能有效扩展到大规模用户场景，为下一代多址接入系统提供了有吸引力的复杂度-性能权衡。

Abstract: Compact ultra-massive antenna-array (CUMA) is a novel multiple access
technology built on the fluid antenna system (FAS) concept, offering an
improved scheme over fluid antenna multiple access (FAMA) that can support
massive connectivity on the same physical channel without the need of precoding
and interference cancellation. By employing a simple port-selection mechanism
that leverages random channel superposition, CUMA can suppress inter-user
interference while keeping hardware costs low. Nevertheless, its ad-hoc
port-selection strategy leaves considerable room for optimization. In this
work, we revisit CUMA and propose two adaptive single-RF port-selection schemes
that retain its simplicity while significantly enhancing performance. The first
one, referred to as exact optimal half-space (EOHS), dynamically selects the
projection direction that maximizes the instantaneous signal build-up across
active ports. To reduce complexity while preserving most of the gains, we
furthermore introduce a principal component analysis (PCA)-based scheme, which
aligns port partitioning with the dominant statistical direction of per-port
channel vectors. This method yields a closed-form low-complexity solution,
complemented by a tractable analytical framework that provides a closed-form
expression for the signal-to-interference ratio (SIR) probability density
function (PDF). Simulation results corroborate the analysis, demonstrating that
both EOHS and PCA consistently outperform conventional CUMA across diverse user
densities, port counts, and FAS aperture sizes. Notably, PCA achieves
performance close to EOHS at a fraction of the computational cost. The proposed
schemes scale effectively to large-user regimes, offering a compelling
complexity-performance trade-off for next-generation multiple access systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [43] [Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech Generation](https://arxiv.org/abs/2509.19592)
*Roy Fejgin,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Ryan Langman Jaehyeon Kim,Subhankar Ghosh,Shehzeen Hussain,Jason Li*

Main category: eess.AS

TL;DR: 本文系统研究了基于大语言模型的语音生成模型中处理多码本结构的两种局部变换器架构，分析了并行预测与迭代采样策略在吞吐量和质量之间的权衡，并提出了基于部署优先级的实用解码策略选择指南。


<details>
  <summary>Details</summary>
Motivation: 语音生成模型基于离散声学代码运行，这些代码具有多码本结构，每个时间步需要联合预测N个码本条目。简单的并行预测方法假设码本间独立性，虽然解码效率高但会降低保真度。需要更好的方法来捕捉码本间依赖关系。

Method: 研究了两种局部变换器架构：1）自回归变换器（顺序生成码本）2）基于MaskGIT的变换器（迭代掩码预测）。两种设计都支持帧堆叠技术，主变换器联合预测多帧，局部变换器解码其码本。

Result: 通过广泛分析，表征了并行和迭代采样策略在不同吞吐量和质量机制下的权衡关系。帧堆叠技术在不影响感知质量的前提下提高了速度。

Conclusion: 提出了基于计算效率和合成保真度等部署优先级的实用解码策略选择指南，为语音生成模型的优化部署提供了系统指导。

Abstract: Speech generation models based on large language models (LLMs) typically
operate on discrete acoustic codes, which differ fundamentally from text tokens
due to their multicodebook structure. At each timestep, models must predict N
codebook entries jointly, introducing dependencies that challenge simple
parallel prediction approaches. Parallel prediction assumes independence among
codebooks, yielding efficient decoding but often at the cost of reduced
fidelity. To address this, hierarchical strategies employ a local transformer
(LT) to refine predictions and capture intra-timestep dependencies. In this
work, we systematically investigate two LT architectures: an autoregressive
transformer that generates codebooks sequentially, and a MaskGIT-based
transformer that performs iterative masked prediction. Both designs further
enable frame stacking, where the primary transformer predicts multiple frames
jointly, and the LT decodes their codebooks, offering improvements in speed
without compromising perceptual quality. Through extensive analysis, we
characterize the tradeoffs between parallel and iterative sampling strategies
across different throughput and quality regimes. Finally, we propose practical
guidelines for selecting decoding strategies based on deployment priorities
such as computational efficiency and synthesis fidelity.

</details>


### [44] [Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning](https://arxiv.org/abs/2509.19631)
*Shaoshi Ling,Gang Liu,Guoli Ye,Jinyu Li*

Main category: eess.AS

TL;DR: 提出了一种新颖的多阶段强化学习训练框架，用于增强多模态大语言模型在语音摘要任务中的能力，显著缩小了与基于文本的最先进LLM之间的差距。


<details>
  <summary>Details</summary>
Motivation: 开源多模态大语言模型在语音摘要任务中仍落后于最先进的基于文本的LLM，限制了其实际部署应用。

Method: 采用多阶段强化学习训练框架来增强MLLMs的语音摘要能力，直接从语音生成文本摘要而无需中间转录。

Result: 模型在强基线基础上取得显著改进，超越了更大的MLLMs，并显著缩小了与最先进文本LLM的差距。

Conclusion: 该多阶段强化学习框架有效提升了MLLMs的语音摘要性能，为语音内容理解提供了更实用的解决方案。

Abstract: Speech summarization is a critical component of spoken content understanding,
particularly in the era of rapidly growing spoken and audiovisual data. Recent
advances in multi-modal large language models (MLLMs), leveraging the power of
LLMs, enable generating textual summaries directly from speech without
intermediate transcriptions, while supporting controllable styles and zero-shot
generalization. However, open-source MLLMs continue to lag behind the
state-of-the-art text-based LLMs, limiting their practical deployment for
speech summarization. In this work, we present a novel multi-stage
reinforcement learning training framework to enhance the speech summarization
capabilities in MLLMs. Our model delivers substantial improvements over strong
baselines, outperforms much larger MLLMs, and significantly narrows the gap
with state-of-the-art text-based LLMs.

</details>


### [45] [Selective Classifier-free Guidance for Zero-shot Text-to-speech](https://arxiv.org/abs/2509.19668)
*John Zheng,Farhad Maleki*

Main category: eess.AS

TL;DR: 本文评估了图像生成中的无分类器引导策略在语音合成中的应用，发现这些策略在语音合成中普遍失效，但通过选择性CFG策略可以在保持文本忠实度的同时提高说话人相似度。


<details>
  <summary>Details</summary>
Motivation: 在零样本文本到语音合成中，平衡目标说话人保真度和文本内容忠实度仍然是一个挑战。虽然无分类器引导策略在图像生成中表现出色，但在语音合成中的应用尚未充分探索。

Method: 评估图像生成CFG策略在语音合成中的适应性，并扩展分离条件CFG方法。采用标准CFG在早期时间步应用，后期切换到选择性CFG的策略。

Result: 图像生成中有效的CFG策略在语音合成中普遍失效。但通过选择性CFG策略可以在提高说话人相似度的同时限制文本忠实度的退化。有趣的是，选择性CFG的有效性高度依赖于文本表示，英语和普通话的不同会导致不同结果。

Conclusion: CFG策略不能直接迁移到语音合成领域，需要针对语音特性进行专门设计。文本表示对CFG策略效果有重要影响，不同语言需要不同的优化方法。

Abstract: In zero-shot text-to-speech, achieving a balance between fidelity to the
target speaker and adherence to text content remains a challenge. While
classifier-free guidance (CFG) strategies have shown promising results in image
generation, their application to speech synthesis are underexplored. Separating
the conditions used for CFG enables trade-offs between different desired
characteristics in speech synthesis. In this paper, we evaluate the
adaptability of CFG strategies originally developed for image generation to
speech synthesis and extend separated-condition CFG approaches for this domain.
Our results show that CFG strategies effective in image generation generally
fail to improve speech synthesis. We also find that we can improve speaker
similarity while limiting degradation of text adherence by applying standard
CFG during early timesteps and switching to selective CFG only in later
timesteps. Surprisingly, we observe that the effectiveness of a selective CFG
strategy is highly text-representation dependent, as differences between the
two languages of English and Mandarin can lead to different results even with
the same model.

</details>


### [46] [Short-Segment Speaker Verification with Pre-trained Models and Multi-Resolution Encoder](https://arxiv.org/abs/2509.19721)
*Jisoo Myoung,Sangwook Han,Kihyuk Kim,Jong Won Shin*

Main category: eess.AS

TL;DR: 该论文提出了一种结合预训练模型特征、滤波器组特征和多分辨率时域编码器特征的说话人验证系统，旨在解决短语音段验证中预训练模型时间分辨率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练模型通常具有20ms的时间分辨率，低于典型的滤波器组特征。这在短语音段（小于2秒）的说话人验证中尤为不利，因为需要从有限长度的输入中提取尽可能多的信息。

Method: 提出了一种说话人验证系统，结合使用预训练模型特征、滤波器组特征以及多分辨率时域编码器特征（窗口移位为25、50、100和200个样本）。

Result: 在VoxCeleb数据集上对不同输入长度进行的实验结果显示，该系统相比各种输入特征组合的系统都取得了持续的改进。

Conclusion: 通过结合多分辨率特征，特别是引入更高时间分辨率的特征，可以有效提升短语音段说话人验证的性能。

Abstract: Speaker verification (SV) utilizing features obtained from models pre-trained
via self-supervised learning has recently demonstrated impressive performances.
However, these pre-trained models (PTMs) usually have a temporal resolution of
20 ms, which is lower than typical filterbank features. It may be problematic
especially for short-segment SV with an input segment shorter than 2 s, in
which we need to extract as much information as possible from the input with a
limited length. Although there have been approaches to utilize multi-resolution
features from the HuBERT models, the window shifts were 320, 640, and 1600
samples when the sampling rate was 16 kHz and thus only lower resolution
features were considered. In this study, we propose an SV system which utilizes
PTM features along with filterbank features and those from the multi-resolution
time domain encoder with window shifts of 25, 50, 100, and 200 samples.
Experimental results on the VoxCeleb dataset with various input lengths showed
consistent improvements over systems with various combinations of input
features.

</details>


### [47] [MMedFD: A Real-world Healthcare Benchmark for Multi-turn Full-Duplex Automatic Speech Recognition](https://arxiv.org/abs/2509.19817)
*Hongzhao Chen,XiaoYang Wang,Jing Lan,Hexiao Ding,Yufeng Jiang MingHui Yang,DanHui Xu,Jun Luo,Nga-Chun Ng,Gerald W. Y. Cheng,Yunlin Mao,Jung Sun Yoo*

Main category: eess.AS

TL;DR: MMedFD是首个面向多轮全双工设置的真实世界中文医疗ASR语料库，包含5,805个标注会话，提供了流式分割、说话人归属和对话记忆的模型无关流程，并建立了可复现的医疗ASR基准框架。


<details>
  <summary>Details</summary>
Motivation: 临床对话中的自动语音识别需要应对全双工交互、说话人重叠和低延迟约束的挑战，但目前缺乏开放的基准数据集。

Method: 基于部署的AI助手收集数据，采用流式分割和说话人归属流程，在角色拼接音频上微调Whisper-small模型进行长上下文识别。

Result: 构建了包含同步用户和混合声道视图、时间标注和角色标签的数据集，提供了WER、CER和HC-WER等多维度评估指标。

Conclusion: MMedFD为医疗领域流式ASR和端到端双工代理的基准测试建立了可复现的框架，相关资源已公开。

Abstract: Automatic speech recognition (ASR) in clinical dialogue demands robustness to
full-duplex interaction, speaker overlap, and low-latency constraints, yet open
benchmarks remain scarce. We present MMedFD, the first real-world Chinese
healthcare ASR corpus designed for multi-turn, full-duplex settings. Captured
from a deployed AI assistant, the dataset comprises 5,805 annotated sessions
with synchronized user and mixed-channel views, RTTM/CTM timing, and role
labels. We introduce a model-agnostic pipeline for streaming segmentation,
speaker attribution, and dialogue memory, and fine-tune Whisper-small on
role-concatenated audio for long-context recognition. ASR evaluation includes
WER, CER, and HC-WER, which measures concept-level accuracy across healthcare
settings. LLM-generated responses are assessed using rubric-based and pairwise
protocols. MMedFD establishes a reproducible framework for benchmarking
streaming ASR and end-to-end duplex agents in healthcare deployment. The
dataset and related resources are publicly available at
https://github.com/Kinetics-JOJO/MMedFD

</details>


### [48] [SCORE: Scaling audio generation using Standardized COmposite REwards](https://arxiv.org/abs/2509.19831)
*Jaemin Jung,Jaehun Kim,Inkyu Shin,Joon Son Chung*

Main category: eess.AS

TL;DR: 本文提出了一种训练自由的推理时间缩放方法，通过多奖励引导机制提升文本到音频生成的感知质量和文本对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频生成模型在感知质量和文本对齐之间难以达到可靠平衡，需要更有效的推理优化方法。

Method: 采用推理时间缩放技术，提出多奖励引导机制，将各感知组件归一化到统一尺度并通过加权求和实现稳定引导和显式控制。

Result: 实验表明该方法在语义对齐和感知质量方面均有显著提升，优于朴素生成和现有奖励引导技术。

Conclusion: 该方法为文本到音频生成提供了有效的推理优化方案，通过多奖励引导实现了更好的质量与对齐平衡。

Abstract: The goal of this paper is to enhance Text-to-Audio generation at inference,
focusing on generating realistic audio that precisely aligns with text prompts.
Despite the rapid advancements, existing models often fail to achieve a
reliable balance between perceptual quality and textual alignment. To address
this, we adopt Inference-Time Scaling, a training-free method that improves
performance by increasing inference computation. We establish its unexplored
application to audio generation and propose a novel multi-reward guidance that
equally signifies each component essential in perception. By normalizing each
reward value into a common scale and combining them with a weighted summation,
the method not only enforces stable guidance but also enables explicit control
to reach desired aspects. Moreover, we introduce a new audio-text alignment
metric using an audio language model for more robust evaluation. Empirically,
our method improves both semantic alignment and perceptual quality,
significantly outperforming naive generation and existing reward guidance
techniques. Synthesized samples are available on our demo page:
https://mm.kaist.ac.kr/projects/score

</details>


### [49] [Weakly Supervised Phonological Features for Pathological Speech Analysis](https://arxiv.org/abs/2509.19879)
*Jenthe Thienpondt,Geoffroy Vanderreydt,Abdessalem Hammami,Kris Demuynck*

Main category: eess.AS

TL;DR: 本文提出了一种弱监督训练方法，利用音素的已知声学特性，通过训练带有可解释帧级音系特征瓶颈层的ASR模型，用于语音病理分析中的可懂度预测和病理分类。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏描述副语言特性的标记语音数据集，特别是帧级标记数据，自动建模这些特性很困难。

Method: 训练带有可解释帧级音系特征瓶颈层的ASR模型，利用音素的声学特性，开发用于可懂度预测和语音病理分类的模型。

Result: 使用提出的音系特征的模型在两项任务上的表现与其他最先进的声学特征相当，分类准确率为75%，语音可懂度预测的RMSE为8.43。

Conclusion: 与现有方法相比，所提出的音系特征与文本无关且高度可解释，为语音治疗师提供了潜在有用的见解。

Abstract: Paralinguistic properties of speech are essential in analyzing and choosing
optimal treatment options for patients with speech disorders. However,
automatic modeling of these characteristics is difficult due to the lack of
labeled speech datasets describing paralinguistic properties, especially at the
frame-level. In this paper, we propose a weakly supervised training method
which exploits the known acoustic properties of phonemes by training an ASR
model with an interpretable frame-level phonological feature bottleneck layer.
Subsequently, we assess the viability of these phonological features in speech
pathology analysis by developing corresponding models for intelligibility
prediction and speech pathology classification. Models using our proposed
phonological features perform similar to other state-of-the-art acoustic
features on both tasks with a classification accuracy of 75% and a 8.43 RMSE on
speech intelligibility prediction. In contrast to others, our phonological
features are text-independent and highly interpretable, providing potentially
useful insights for speech therapists.

</details>


### [50] [MAGE: A Coarse-to-Fine Speech Enhancer with Masked Generative Model](https://arxiv.org/abs/2509.19881)
*The Hieu Pham,Tan Dat Nguyen,Phuong Thanh Tran,Joon Son Chun,Duc Dung Nguyen*

Main category: eess.AS

TL;DR: MAGE是一个基于掩码生成模型的语音增强方法，通过粗到细的掩码策略和轻量级校正模块，在保持高效性的同时实现了最先进的感知质量。


<details>
  <summary>Details</summary>
Motivation: 解决语音增强中效率与感知质量之间的权衡问题，传统方法难以同时兼顾计算效率和音频质量。

Method: 采用稀缺感知的粗到细掩码策略，优先处理高频token，后期细化低频token；引入轻量级校正模块检测低置信度预测并重新掩码优化；基于BigCodec和Qwen2.5-0.5B构建，通过选择性层保留将参数量压缩至200M。

Result: 在DNS Challenge和噪声LibriSpeech数据集上实验表明，MAGE实现了最先进的感知质量，并显著降低了下游语音识别的词错误率，性能优于更大的基线模型。

Conclusion: MAGE通过创新的掩码策略和模型设计，在语音增强任务中成功平衡了效率与质量，为生成式语音增强提供了紧凑而鲁棒的解决方案。

Abstract: Speech enhancement remains challenging due to the trade-off between
efficiency and perceptual quality. In this paper, we introduce MAGE, a Masked
Audio Generative Enhancer that advances generative speech enhancement through a
compact and robust design. Unlike prior masked generative models with random
masking, MAGE employs a scarcity-aware coarse-to-fine masking strategy that
prioritizes frequent tokens in early steps and rare tokens in later
refinements, improving efficiency and generalization. We also propose a
lightweight corrector module that further stabilizes inference by detecting
low-confidence predictions and re-masking them for refinement. Built on
BigCodec and finetuned from Qwen2.5-0.5B, MAGE is reduced to 200M parameters
through selective layer retention. Experiments on DNS Challenge and noisy
LibriSpeech show that MAGE achieves state-of-the-art perceptual quality and
significantly reduces word error rate for downstream recognition, outperforming
larger baselines. Audio examples are available at
https://hieugiaosu.github.io/MAGE/.

</details>


### [51] [Voice Privacy Preservation with Multiple Random Orthogonal Secret Keys: Attack Resistance Analysis](https://arxiv.org/abs/2509.19906)
*Kohei Tanaka,Hitoshi Kiya,Sayaka Shiota*

Main category: eess.AS

TL;DR: 提出一种基于多随机正交矩阵的语音隐私保护方法，增强传统方法的攻击抵抗能力并放宽模型约束，使其适用于更广泛的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 随着语音数据上传到云端深度学习模型的机会增加，对语音隐私（包括说话人身份和语言内容）的保护需求日益增长。现有基于单随机正交矩阵加密的方法攻击抵抗能力有限且模型适用性受限。

Method: 使用多个随机正交矩阵作为密钥来增强传统语音隐私保护技术，同时引入方法放宽模型约束，使加密可应用于更多类型的深度学习模型。

Result: 实验结果表明，即使在Voice Privacy Challenge中未考虑的更强攻击场景下，所提方法仍能保持说话人身份隐藏的隐私保护性能。

Conclusion: 提出的多随机正交矩阵方法有效提升了语音隐私保护的安全性和适用性，为云端语音处理提供了更可靠的隐私保护方案。

Abstract: Recently, opportunities to transmit speech data to deep learning models
executed in the cloud have increased. This has led to growing concerns about
speech privacy, including both speaker-specific information and the linguistic
content of utterances. As an approach to preserving speech privacy, a speech
privacy-preserving method based on encryption using a secret key with a random
orthogonal matrix has been proposed. This method enables cloud-based model
inference while concealing both the speech content and the speaker identity.
However, the method has limited attack resistance and is constrained in terms
of the deep learning models to which the encryption can be applied. In this
work, we propose a method that enhances the attack resistance of the
conventional speech privacy-preserving technique by employing multiple random
orthogonal matrices as secret keys. We also introduce approaches to relax the
model constraints, enabling the application of our method to a broader range of
deep learning models. Furthermore, we investigate the robustness of the
proposed method against attacks using extended attack scenarios based on the
scenarios employed in the Voice Privacy Challenge. Our experimental results
confirmed that the proposed method maintains privacy protection performance for
speaker concealment, even under more powerful attack scenarios not considered
in prior work.

</details>


### [52] [Measuring Prosody Diversity in Zero-Shot TTS: A New Metric, Benchmark, and Exploration](https://arxiv.org/abs/2509.19928)
*Yifan Yang,Bing Han,Hui Wang,Long Zhou,Wei Wang,Mingyu Cui,Xu Tan,Xie Chen*

Main category: eess.AS

TL;DR: ProsodyEval是一个用于评估零样本文本转语音系统韵律多样性的数据集，提出了新的客观评估指标DS-WED，该指标比传统声学指标更能反映人类感知的韵律多样性。


<details>
  <summary>Details</summary>
Motivation: 现有声学指标只能捕捉韵律变化的局部特征，与人类感知相关性差，需要可靠量化韵律多样性的方法。

Method: 构建包含1000个语音样本和2000个人类评分的ProsodyEval数据集，提出基于加权编辑距离的DS-WED指标，使用HuBERT和WavLM进行语音标记化。

Result: DS-WED与人类判断的相关性显著高于现有声学指标，在LibriSpeech和Seed-TTS数据集上对主流TTS系统进行基准测试，发现生成建模范式、时长控制和强化学习等因素影响韵律多样性。

Conclusion: DS-WED是评估韵律多样性的有效指标，当前大型音频语言模型在捕捉韵律变化方面仍有局限。

Abstract: Prosody diversity is essential for achieving naturalness and expressiveness
in zero-shot text-to-speech (TTS). However, frequently used acoustic metrics
capture only partial views of prosodic variation and correlate poorly with
human perception, leaving the problem of reliably quantifying prosody diversity
underexplored. To bridge this gap, we introduce ProsodyEval, a prosody
diversity assessment dataset that provides Prosody Mean Opinion Score (PMOS)
alongside conventional acoustic metrics. ProsodyEval comprises 1000 speech
samples derived from 7 mainstream TTS systems, with 2000 human ratings.
Building on this, we propose the Discretized Speech Weighted Edit Distance
(DS-WED), a new objective diversity metric that quantifies prosodic variation
via weighted edit distance over semantic tokens. Experiments on ProsodyEval
show that DS-WED achieves substantially higher correlation with human judgments
than existing acoustic metrics, while remaining highly robust in speech
tokenization from HuBERT and WavLM. Leveraging DS-WED, we benchmark
state-of-the-art open-source TTS systems on LibriSpeech test-clean and Seed-TTS
test-en, and further explorations uncover several factors that influence
prosody diversity, including generative modeling paradigms, duration control,
and reinforcement learning. Moreover, we find that current large audio language
models (LALMs) remain limited in capturing prosodic variations. Audio samples
are available at https://prosodyeval.github.io.

</details>


### [53] [Evaluating pretrained speech embedding systems for dysarthria detection across heterogenous datasets](https://arxiv.org/abs/2509.19946)
*Lovisa Wihlborg,Jemima Goodall,David Wheatley,Jacob J. Webber,Johnny Tam,Christine Weaver,Suvankar Pal,Siddharthan Chandran,Sohan Seth,Oliver Watts,Cassia Valentini-Botinhao*

Main category: eess.AS

TL;DR: 本文对17种预训练语音嵌入系统在6个不同数据集上进行了全面评估，用于检测构音障碍语音，重点关注数据集偏差和系统泛化能力。


<details>
  <summary>Details</summary>
Motivation: 构音障碍语音数据集通常规模小且存在记录偏差和数据不平衡问题，需要评估现有语音嵌入系统在这种挑战性条件下的表现。

Method: 采用多个相关条件的数据集，使用交叉验证运行来估计机会水平，通过精心设计的零假设比较分数分布来验证结果是否显著高于机会水平。

Result: 发现数据集内结果因数据集而异，跨数据集准确率低于数据集内结果，表明系统泛化存在挑战。

Conclusion: 这些发现对在同一数据集上训练和测试的系统的临床有效性具有重要意义，需要重新考虑基准数据集的选择标准。

Abstract: We present a comprehensive evaluation of pretrained speech embedding systems
for the detection of dysarthric speech using existing accessible data.
Dysarthric speech datasets are often small and can suffer from recording biases
as well as data imbalance. To address these we selected a range of datasets
covering related conditions and adopt the use of several cross-validations runs
to estimate the chance level. To certify that results are above chance, we
compare the distribution of scores across these runs against the distribution
of scores of a carefully crafted null hypothesis. In this manner, we evaluate
17 publicly available speech embedding systems across 6 different datasets,
reporting the cross-validation performance on each. We also report
cross-dataset results derived when training with one particular dataset and
testing with another. We observed that within-dataset results vary considerably
depending on the dataset, regardless of the embedding used, raising questions
about which datasets should be used for benchmarking. We found that
cross-dataset accuracy is, as expected, lower than within-dataset, highlighting
challenges in the generalization of the systems. These findings have important
implications for the clinical validity of systems trained and tested on the
same dataset.

</details>


### [54] [Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens](https://arxiv.org/abs/2509.20060)
*Pin-Jui Ku,He Huang,Jean-Marie Lemercier,Subham Sekhar Sahoo,Zhehuai Chen,Ante Jukić*

Main category: eess.AS

TL;DR: 本文提出了一种用于文本对齐语音标记化和重建的离散扩散模型框架，通过用离散扩散解码器替换自回归语音解码器，显著提升了重建质量、ASR性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 改进现有的文本对齐语音标记化方法，解决自回归解码器在重建质量和推理速度方面的局限性。

Method: 使用离散扩散模型替代自回归语音解码器，系统比较向量量化模块（FSQ vs RVQ），并分析采样器选择、推理步长和长度尺度估计误差的鲁棒性。

Result: 模型仅需10步去噪即可生成语音，支持单步生成且质量下降较小。FSQ相比RVQ在AR模型上实现35%相对WER降低和+0.14 UT-MOS提升，同时增强DDM性能。

Conclusion: 离散扩散模型框架在语音重建任务中表现出优越性能，实现了高质量、快速和鲁棒的语音生成。

Abstract: This paper introduces a discrete diffusion model (DDM) framework for
text-aligned speech tokenization and reconstruction. By replacing the
auto-regressive speech decoder with a discrete diffusion counterpart, our model
achieves significantly better reconstruction quality, stronger ASR performance,
and faster inference. We provide a comprehensive analysis of applying DDMs to
speech reconstruction, examining sampler choices, inference steps, and
robustness to length-scale estimation errors. Furthermore, we improve the
original TASTE by systematically comparing vector quantization modules, showing
that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement
over RVQ for AR models, while also enhancing DDM performance. Our model
generates speech in just 10 denoising steps and even supports single-step
generation with only minor quality degradation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [55] [MusiCRS: Benchmarking Audio-Centric Conversational Recommendation](https://arxiv.org/abs/2509.19469)
*Rohan Surana,Amit Namburi,Gagan Mundada,Abhay Lal,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.SD

TL;DR: MusiCRS是首个音频中心对话推荐基准，连接Reddit真实用户对话与对应音频轨道，包含477个高质量对话和3,589个音乐实体，支持音频、文本和多模态输入评估。


<details>
  <summary>Details</summary>
Motivation: 音乐推荐需要基于音频内容的推理，而现有系统过度依赖文本信号，难以处理细微的音频推理，这暴露了跨模态知识整合的根本局限性。

Method: 构建包含477个高质量对话的基准数据集，涵盖多种音乐流派，通过YouTube链接提供音频基础，支持三种输入模态配置（仅音频、仅查询、音频+查询）的系统评估。

Result: 实验显示当前系统严重依赖文本信号，在细微音频推理方面表现不佳，模型擅长对话语义但无法有效将抽象音乐概念与实际音频内容关联。

Conclusion: MusiCRS基准揭示了音频中心对话推荐的挑战，为促进该领域进展，作者发布了数据集、评估代码和基线模型。

Abstract: Conversational recommendation has advanced rapidly with large language models
(LLMs), yet music remains a uniquely challenging domain where effective
recommendations require reasoning over audio content beyond what text or
metadata can capture. We present MusiCRS, the first benchmark for audio-centric
conversational recommendation that links authentic user conversations from
Reddit with corresponding audio tracks. MusiCRS contains 477 high-quality
conversations spanning diverse genres (classical, hip-hop, electronic, metal,
pop, indie, jazz) with 3,589 unique musical entities and audio grounding via
YouTube links. MusiCRS enables evaluation across three input modality
configurations: audio-only, query-only, and audio+query (multimodal), allowing
systematic comparison of audio-LLMs, retrieval models, and traditional
approaches. Our experiments reveal that current systems rely heavily on textual
signals and struggle with nuanced audio reasoning. This exposes fundamental
limitations in cross-modal knowledge integration where models excel at dialogue
semantics but cannot effectively ground abstract musical concepts in actual
audio content. To facilitate progress, we release the MusiCRS dataset
(https://huggingface.co/datasets/rohan2810/MusiCRS), evaluation code
(https://github.com/rohan2810/musiCRS), and comprehensive baselines.

</details>


### [56] [ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based Speech Enhancement](https://arxiv.org/abs/2509.19495)
*Bhawana Chhaglani,Yang Gao,Julius Richter,Xilin Li,Syavosh Zadissa,Tarun Pruthi,Andrew Lovitt*

Main category: cs.SD

TL;DR: 本文系统研究了基于扩散模型的语音增强中的伪影预测与减少问题，提出通过语音嵌入的方差预测语音错误，并基于语义一致性设计集成推理方法，有效降低WER并平衡延迟与伪影抑制。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的语音增强虽然能生成自然语音且泛化能力强，但存在生成伪影和高推理延迟等关键限制。

Method: 1）利用语音嵌入的方差预测语音错误；2）提出基于多轮扩散语义一致性的集成推理方法；3）分析扩散步数的影响，采用自适应步数平衡性能与延迟。

Result: 在低信噪比条件下，WER降低15%，有效提高了语音准确性和语义合理性。

Conclusion: 语义先验是引导生成式语音增强实现无伪影输出的有力工具。

Abstract: Diffusion-based speech enhancement (SE) achieves natural-sounding speech and
strong generalization, yet suffers from key limitations like generative
artifacts and high inference latency. In this work, we systematically study
artifact prediction and reduction in diffusion-based SE. We show that variance
in speech embeddings can be used to predict phonetic errors during inference.
Building on these findings, we propose an ensemble inference method guided by
semantic consistency across multiple diffusion runs. This technique reduces WER
by 15% in low-SNR conditions, effectively improving phonetic accuracy and
semantic plausibility. Finally, we analyze the effect of the number of
diffusion steps, showing that adaptive diffusion steps balance artifact
suppression and latency. Our findings highlight semantic priors as a powerful
tool to guide generative SE toward artifact-free outputs.

</details>


### [57] [Thinking While Listening: Simple Test Time Scaling For Audio Classification](https://arxiv.org/abs/2509.19676)
*Prateek Verma,Mert Pilanci*

Main category: cs.SD

TL;DR: 提出一个让神经网络在听日常声音时“边听边思考”的框架，通过推理增强音频分类性能，展示了在分类准确率和测试时扩展方面的改进


<details>
  <summary>Details</summary>
Motivation: 受大型语言模型推理能力进步的启发，探索如何将思考机制融入音频分类流程，实现类别空间推理并提升性能

Method: 采用两种方法：在现有音频分类流程中融入思考机制，以及从头设计支持思考和测试时扩展的新架构；使用轻量级方法（仅重训练冻结小模型的嵌入矩阵）

Result: 模型在两种设置下都表现出分类准确率提升；测试时扩展随着采样轨迹增加带来持续增益；轻量级方法性能超过数十亿参数文本推理模型

Conclusion: 思考机制能有效提升音频分类性能，轻量级重训练方法在性能上优于大型文本推理模型，为音频推理任务提供了高效解决方案

Abstract: We propose a framework that enables neural models to "think while listening"
to everyday sounds, thereby enhancing audio classification performance.
Motivated by recent advances in the reasoning capabilities of large language
models, we address two central questions: (i) how can thinking be incorporated
into existing audio classification pipelines to enable reasoning in the
category space and improve performance, and (ii) can a new architecture be
designed from the ground up to support both thinking and test-time scaling? We
demonstrate that in both settings, our models exhibit improved classification
accuracy. Leveraging test-time scaling, we observe consistent gains as the
number of sampled traces increases. Furthermore, we evaluate two open-source
reasoning models, GPT-OSS-20B and Qwen3-14B, showing that while such models are
capable of zero-shot reasoning, a lightweight approach--retraining only the
embedding matrix of a frozen, smaller model like GPT-2--can surpass the
performance of billion-parameter text-based reasoning models.

</details>


### [58] [Can Audio Large Language Models Verify Speaker Identity?](https://arxiv.org/abs/2509.19755)
*Yiming Ren,Xuenan Xu,Baoxiang Li,Shuai Wang,Chao Zhang*

Main category: cs.SD

TL;DR: 本文研究将音频大语言模型（ALLMs）应用于说话人验证任务，通过将SV重新定义为音频问答任务，并采用监督微调和硬对采样策略来提升性能


<details>
  <summary>Details</summary>
Motivation: 探索ALLMs在说话人验证领域的潜力，利用其通用音频理解能力构建统一的鲁棒说话人验证系统

Method: 将说话人验证重新定义为音频问答任务，提出基于规则的硬对采样策略构建挑战性训练对，进行轻量级监督微调，并扩展到文本相关说话人验证

Result: 微调后ALLMs性能显著提升，在文本相关SV中与级联ASR-SV系统竞争力相当，但仍与传统模型存在差距

Conclusion: 通过适当适配，ALLMs具有作为统一说话人验证模型的巨大潜力，同时保持通用音频理解能力

Abstract: This paper investigates adapting Audio Large Language Models (ALLMs) for
speaker verification (SV). We reformulate SV as an audio question-answering
task and conduct comprehensive zero-shot evaluations on public benchmarks,
showing that current ALLMs have limited zero-shot SV capability and often
struggle in diverse acoustic conditions. To address this challenge, we perform
supervised fine-tuning on speaker verification data. A rule-based hard pair
sampling strategy is proposed to construct more challenging training pairs.
Lightweight fine-tuning substantially improves the performance, though there is
still a gap between ALLMs and conventional models. Then, we extend to
text-dependent SV by jointly querying ALLMs to verify speaker identity and
spoken content, yielding results competitive with cascaded ASR-SV systems. Our
findings demonstrate that with proper adaptation, ALLMs hold substantial
potential as a unified model for robust speaker verification systems, while
maintaining the general audio understanding capabilities.

</details>


### [59] [Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation](https://arxiv.org/abs/2509.19812)
*Yang Cui,Peter Pan,Lei He,Sheng Zhao*

Main category: cs.SD

TL;DR: 提出PKDMark，一种基于渐进知识蒸馏的轻量级深度学习语音水印方法，在保持高性能的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 随着语音生成模型的快速发展，未经授权的语音克隆带来严重的隐私和安全风险。现有水印技术要么效率高但易受攻击，要么保护性强但计算成本高

Method: 采用两阶段方法：1）训练基于可逆神经网络的高性能教师模型；2）通过渐进知识蒸馏将教师能力转移到紧凑的学生模型

Result: 计算成本降低93.6%，在高级失真条件下平均检测F1分数达到99.6%，PESQ为4.30

Conclusion: PKDMark实现了高效的语音水印，适用于实时语音合成应用，在计算效率和鲁棒性之间取得了良好平衡

Abstract: With the rapid advancement of speech generative models, unauthorized voice
cloning poses significant privacy and security risks. Speech watermarking
offers a viable solution for tracing sources and preventing misuse. Current
watermarking technologies fall mainly into two categories: DSP-based methods
and deep learning-based methods. DSP-based methods are efficient but vulnerable
to attacks, whereas deep learning-based methods offer robust protection at the
expense of significantly higher computational cost. To improve the
computational efficiency and enhance the robustness, we propose PKDMark, a
lightweight deep learning-based speech watermarking method that leverages
progressive knowledge distillation (PKD). Our approach proceeds in two stages:
(1) training a high-performance teacher model using an invertible neural
network-based architecture, and (2) transferring the teacher's capabilities to
a compact student model through progressive knowledge distillation. This
process reduces computational costs by 93.6% while maintaining high level of
robust performance and imperceptibility. Experimental results demonstrate that
our distilled model achieves an average detection F1 score of 99.6% with a PESQ
of 4.30 in advanced distortions, enabling efficient speech watermarking for
real-time speech synthesis applications.

</details>


### [60] [Eliminating stability hallucinations in llm-based tts models via attention guidance](https://arxiv.org/abs/2509.19852)
*ShiMing Wang,ZhiHao Du,Yang Xiang,TianYu Zhao,Han Zhao,Qian Chen,XianGang Li,HanJie Guo,ZhenHua Ling*

Main category: cs.SD

TL;DR: 本文提出通过改进注意力机制来解决基于LLM的TTS模型中的稳定性幻觉问题，包括提出最优对齐分数(OAS)指标和利用预训练注意力值进行链式思维指导。


<details>
  <summary>Details</summary>
Motivation: 解决LLM-based TTS模型中存在的稳定性幻觉问题，如重复或遗漏语音，通过改进注意力机制来提升文本-语音对齐质量。

Method: 1) 分析文本token和语音token的对齐机制；2) 提出使用Viterbi算法的最优对齐分数(OAS)指标；3) 将OAS集成到CosyVoice2训练中；4) 利用预训练注意力值通过链式思维指导学生模型训练。

Result: 在Seed-TTS-Eval和CV3-Eval测试集上的实验表明，所提方法能有效减少CosyVoice2的稳定性幻觉，且不引入额外负面影响。

Conclusion: 通过改进注意力机制和引入OAS指标，成功解决了LLM-based TTS模型中的稳定性幻觉问题，提升了合成语音的质量和稳定性。

Abstract: This paper focuses on resolving stability hallucinations (e.g., repetitive or
omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and
leveraging the attention mechanism. First, we analyzed the alignment mechanism
between text tokens and speech tokens in LLMs. We then proposed a metric termed
the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to
evaluate text-speech alignment quality. Subsequently, OAS was integrated into
the training of CosyVoice2 to assist LLMs in learning continuous, stable
alignment. Additionally, the pre-trained attention value is employed to guide
the training of the student CosyVoice2 via chain-of-thought (CoT), which
further reduces stability hallucinations in synthesized speech. Experiments on
the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods
can effectively reduce the stability hallucinations of CosyVoice2 without
introducing additional negative effects. The appendix is available at
https://wsmzzz.github.io/llm_attn.

</details>


### [61] [SEA-Spoof: Bridging The Gap in Multilingual Audio Deepfake Detection for South-East Asian](https://arxiv.org/abs/2509.19865)
*Jinyang Wu,Nana Hou,Zihan Pan,Qiquan Zhang,Sailor Hardik Bhupendra,Soumik Mondal*

Main category: cs.SD

TL;DR: SEA-Spoof是首个针对东南亚语言的大规模音频深度伪造检测数据集，包含300+小时的配对真实和伪造语音，涵盖6种东南亚语言，解决了该地区检测模型性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 东南亚数字经济快速发展导致音频深度伪造风险加剧，但现有数据集对东南亚语言覆盖稀疏，导致检测模型在该地区性能严重下降，存在合成质量不匹配、语言特性差异和数据稀缺等问题。

Method: 构建SEA-Spoof数据集，包含300+小时的真实和伪造语音数据，涵盖泰米尔语、印地语、泰语、印尼语、马来语和越南语。伪造样本来自多种开源和商业系统，捕捉了风格和保真度的广泛变异性。

Result: 基准测试显示最先进检测模型存在严重的跨语言性能下降，但在SEA-Spoof上微调后，模型在不同语言和合成源上的性能得到显著恢复。

Conclusion: 研究强调了东南亚语言音频深度伪造检测的紧迫需求，SEA-Spoof为开发鲁棒、跨语言和防欺诈的检测系统奠定了基础。

Abstract: The rapid growth of the digital economy in South-East Asia (SEA) has
amplified the risks of audio deepfakes, yet current datasets cover SEA
languages only sparsely, leaving models poorly equipped to handle this critical
region. This omission is critical: detection models trained on high-resource
languages collapse when applied to SEA, due to mismatches in synthesis quality,
language-specific characteristics, and data scarcity. To close this gap, we
present SEA-Spoof, the first large-scale Audio Deepfake Detection (ADD) dataset
especially for SEA languages. SEA-Spoof spans 300+ hours of paired real and
spoof speech across Tamil, Hindi, Thai, Indonesian, Malay, and Vietnamese.
Spoof samples are generated from a diverse mix of state-of-the-art open-source
and commercial systems, capturing wide variability in style and fidelity.
Benchmarking state-of-the-art detection models reveals severe cross-lingual
degradation, but fine-tuning on SEA-Spoof dramatically restores performance
across languages and synthesis sources. These results highlight the urgent need
for SEA-focused research and establish SEA-Spoof as a foundation for developing
robust, cross-lingual, and fraud-resilient detection systems.

</details>


### [62] [CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance](https://arxiv.org/abs/2509.19883)
*Junchuan Zhao,Wei Zeng,Tianle Lyu,Ye Wang*

Main category: cs.SD

TL;DR: CoMelSinger是一个零样本歌唱语音合成框架，通过离散编解码建模实现结构化旋律控制，解决了提示生成中的韵律泄漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散编解码的语音合成技术难以直接应用于歌唱语音合成，因为需要精确的旋律控制，而提示生成容易导致韵律泄漏（音高信息与音色提示纠缠）。

Method: 基于非自回归MaskGCT架构，用歌词和音高标记替代传统文本输入；提出粗到细对比学习策略抑制韵律泄漏；引入轻量级歌唱语音转录模块提供细粒度监督。

Result: 实验表明CoMelSinger在音高准确性、音色一致性和零样本迁移性方面显著优于竞争基线。

Conclusion: CoMelSinger框架成功实现了零样本歌唱语音合成中的结构化旋律控制，有效解决了韵律泄漏问题。

Abstract: Singing Voice Synthesis (SVS) aims to generate expressive vocal performances
from structured musical inputs such as lyrics and pitch sequences. While recent
progress in discrete codec-based speech synthesis has enabled zero-shot
generation via in-context learning, directly extending these techniques to SVS
remains non-trivial due to the requirement for precise melody control. In
particular, prompt-based generation often introduces prosody leakage, where
pitch information is inadvertently entangled within the timbre prompt,
compromising controllability. We present CoMelSinger, a zero-shot SVS framework
that enables structured and disentangled melody control within a discrete codec
modeling paradigm. Built on the non-autoregressive MaskGCT architecture,
CoMelSinger replaces conventional text inputs with lyric and pitch tokens,
preserving in-context generalization while enhancing melody conditioning. To
suppress prosody leakage, we propose a coarse-to-fine contrastive learning
strategy that explicitly regularizes pitch redundancy between the acoustic
prompt and melody input. Furthermore, we incorporate a lightweight encoder-only
Singing Voice Transcription (SVT) module to align acoustic tokens with pitch
and duration, offering fine-grained frame-level supervision. Experimental
results demonstrate that CoMelSinger achieves notable improvements in pitch
accuracy, timbre consistency, and zero-shot transferability over competitive
baselines.

</details>


### [63] [Enabling Multi-Species Bird Classification on Low-Power Bioacoustic Loggers](https://arxiv.org/abs/2509.20103)
*Stefano Ciapponi,Leonardo Mannini,Jarek Scanferla,Matteo Anderle,Elisabetta Farella*

Main category: cs.SD

TL;DR: WrenNet是一种高效的神经网络，可在低功耗微控制器上实现实时多物种鸟类音频分类，用于可扩展的生物多样性监测。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在低功耗边缘设备上实现连续、多物种声学监测的实用框架，以解决现有方法在资源受限环境下的效率问题。

Method: 提出了一种半可学习的频谱特征提取器，专门适应鸟类发声特征，优于标准的梅尔尺度和完全可学习的替代方案。

Result: 在70个物种的数据集上，WrenNet在声学特征明显的物种上达到90.8%的准确率，整体任务达到70.1%的准确率。在AudioMoth设备上每次推理仅消耗77mJ能量，在树莓派3B+上比Birdnet节能16倍以上。

Conclusion: 这项工作展示了首个在低功耗边缘设备上实现连续多物种声学监测的实用框架，为生物多样性监测提供了高效的解决方案。

Abstract: This paper introduces WrenNet, an efficient neural network enabling real-time
multi-species bird audio classification on low-power microcontrollers for
scalable biodiversity monitoring. We propose a semi-learnable spectral feature
extractor that adapts to avian vocalizations, outperforming standard mel-scale
and fully-learnable alternatives. On an expert-curated 70-species dataset,
WrenNet achieves up to 90.8\% accuracy on acoustically distinctive species and
70.1\% on the full task. When deployed on an AudioMoth device ($\leq$1MB RAM),
it consumes only 77mJ per inference. Moreover, the proposed model is over 16x
more energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+.
This work demonstrates the first practical framework for continuous,
multi-species acoustic monitoring on low-power edge devices.

</details>

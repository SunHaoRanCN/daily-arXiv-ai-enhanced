<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 12]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-Time Markov Modeling for Single-Photon LiDAR: $1000 \times$ Acceleration and Convergence Analysis](https://arxiv.org/abs/2509.20500)
*Weijian Zhang,Hashan K. Weerasooriya,Prateek Chennuri,Stanley H. Chan*

Main category: eess.SP

TL;DR: 本文提出了一种新的非序列马尔可夫建模方法，用于分析具有死区时间的异步单光子激光雷达（SP-LiDAR）的时间戳分布，实现了比现有方法快1000倍的计算加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要构建大型转移矩阵来计算时间戳分布的稳态分布，但这个过程计算成本高昂，因为状态与死区时间之间存在耦合。

Method: 通过重新参数化积分边界并将死区时间效应分离为基矩阵的确定性行置换，实现了状态解耦，从而支持高效的向量化矩阵构建。

Result: 新模型与蒙特卡洛模拟的黄金标准相比，产生了几乎精确的稳态分布，但仅使用了极少的时间。此外，新的理论分析揭示了第二大特征值的幅度和相位对收敛的关键影响。

Conclusion: 该方法为SP-LiDAR时间戳分布建模提供了一种高效且准确的解决方案，显著提升了计算效率，并为理解马尔可夫链收敛性提供了新的理论见解。

Abstract: Asynchronous single-photon LiDAR (SP-LiDAR) is an important imaging modality
for high-quality 3D applications and navigation, but the modeling of the
timestamp distributions of a SP-LiDAR in the presence of dead time remains a
very challenging open problem. Prior works have shown that timestamps form a
discrete-time Markov chain, whose stationary distribution can be computed as
the leading left eigenvector of a large transition matrix. However,
constructing this matrix is known to be computationally expensive because of
the coupling between states and the dead time. This paper presents the first
non-sequential Markov modeling for the timestamp distribution. The key
innovation is an equivalent formulation that reparameterizes the integral
bounds and separates the effect of dead time as a deterministic row permutation
of a base matrix. This decoupling enables efficient vectorized matrix
construction, yielding up to $1000 \times$ acceleration over existing methods.
The new model produces a nearly exact stationary distribution when compared
with the gold standard Monte Carlo simulations, yet using a fraction of the
time. In addition, a new theoretical analysis reveals the impact of the
magnitude and phase of the second-largest eigenvalue, which are overlooked in
the literature but are critical to the convergence.

</details>


### [2] [Wireless Powered MEC Systems via Discrete Pinching Antennas: TDMA versus NOMA](https://arxiv.org/abs/2509.20908)
*Peng Liu,Zesong Fei,Meng Hua,Guangji Chen,Xinyi Wang,Ruiqi Liu*

Main category: eess.SP

TL;DR: 本文研究了一种实用的离散夹持天线辅助无线供电移动边缘计算框架，通过联合优化能量收集和任务卸载来最大化计算性能，比较了TDMA和NOMA方案在不同PA激活灵活性下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设理想的连续PA放置，而实际应用中PA是离散部署的。将PA集成到无线供电MEC系统中可以同时提升能量传输和任务卸载效率。

Method: 提出离散PA辅助无线供电MEC框架，设备先收集PA发射的射频信号能量，然后采用部分卸载模式。开发了两层算法，结合KKT条件的闭式解和基于交叉熵的学习方法来解决混合整数非线性问题。

Result: 数值结果验证了所提设计在能量收集和计算性能方面的优越性。在较粗的PA激活级别下，TDMA和NOMA性能相当；而在更细的激活粒度下，TDMA优于NOMA。

Conclusion: 离散PA部署在实际MEC系统中具有重要应用价值，激活粒度的细化使得TDMA方案在计算性能上超越NOMA方案。

Abstract: Pinching antennas (PAs), a new type of reconfigurable and flexible antenna
structures, have recently attracted significant research interest due to their
ability to create line-of-sight links and mitigate large-scale path loss. Owing
to their potential benefits, integrating PAs into wireless powered mobile edge
computing (MEC) systems is regarded as a viable solution to enhance both energy
transfer and task offloading efficiency. Unlike prior studies that assume ideal
continuous PA placement along waveguides, this paper investigates a practical
discrete PA-assisted wireless powered MEC framework, where devices first
harvest energy from PA-emitted radio-frequency signals and then adopt a partial
offloading mode, allocating part of the harvested energy to local computing and
the remainder to uplink offloading. The uplink phase considers both the
time-division multiple access (TDMA) and non-orthogonal multiple access (NOMA),
each examined under three levels of PA activation flexibility. For each
configuration, we formulate a joint optimization problem to maximize the total
computational bits and conduct a theoretical performance comparison between the
TDMA and NOMA schemes. To address the resulting mixed-integer nonlinear
problems, we develop a two-layer algorithm that combines closed-form solutions
based on Karush-Kuhn-Tucker (KKT) conditions with a cross-entropy-based
learning method. Numerical results validate the superiority of the proposed
design in terms of the harvested energy and computation performance, revealing
that TDMA and NOMA achieve comparable performance under coarser PA activation
levels, whereas finer activation granularity enables TDMA to achieve superior
computation performance over NOMA.

</details>


### [3] [A General Optimization Framework for Movable Antenna Systems via Discrete Sampling](https://arxiv.org/abs/2509.20987)
*Changhao Liu,Weidong Mei,Zhi Chen,Jun Fang,Boyu Ning*

Main category: eess.SP

TL;DR: 提出了一种用于可移动天线系统位置优化的通用低复杂度框架，通过离散化采样和吉布斯采样来避免局部最优，在广播系统中实现接近最优的性能


<details>
  <summary>Details</summary>
Motivation: 可移动天线系统能够通过局部天线移动来重塑无线信道，但天线位置优化面临高度非线性的挑战，现有方法存在高计算复杂度或局部最优问题

Method: 将天线移动区域离散化为采样点集，将连续优化问题转化为离散点选择问题，通过多轮顺序更新每个天线的最优采样点，并在轮次间引入吉布斯采样来探索相邻和随机候选解

Result: 数值结果表明，所提算法实现了接近最优的性能，并显著优于现有基准方法

Conclusion: 该框架为可移动天线系统提供了一种通用且低复杂度的优化解决方案，有效解决了位置优化中的非线性挑战

Abstract: Movable antenna (MA) systems have attracted growing interest in wireless
communications due to their ability to reshape wireless channels via local
antenna movement within a confined region. However, optimizing antenna
positions to enhance communication performance turns out to be challenging due
to the highly nonlinear relationship between wireless channels and antenna
positions. Existing approaches, such as gradient-based and heuristic
algorithms, often suffer from high computational complexity or undesired local
optima. To address the above challenge, this letter proposes a general and
low-complexity optimization framework for MA position optimization.
Specifically, we discretize the antenna movement region into a set of sampling
points, thereby transforming the continuous optimization problem into a
discrete point selection problem. Next, we sequentially update the optimal
sampling point for each MA over multiple rounds. To avoid convergence to poor
local optima, a Gibbs sampling (GS) phase is introduced between rounds to
explore adjacent and randomly generated candidate solutions. As a case study,
we investigate joint precoding and antenna position optimization for an
MA-enhanced broadcast system by applying the proposed framework. Numerical
results demonstrate that the proposed algorithm achieves near-optimal
performance and significantly outperforms existing benchmarks.

</details>


### [4] [Shapley Features for Robust Signal Prediction in Tactile Internet](https://arxiv.org/abs/2509.21032)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 提出了一种结合高斯过程和ResNet神经网络的新型预测框架，通过Shapley特征值优化特征选择，显著提升触觉互联网中信号传输的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网需要超低延迟和可靠的触觉信号传输，但数据包丢失和延迟问题仍未解决，需要开发更有效的预测和恢复方法。

Method: 集成高斯过程作为信号恢复的预言机，结合ResNet神经网络进行预测，并引入Shapley特征值机制进行特征选择优化。

Result: 达到95.72%的准确率，比现有最佳方法LeFo提升11.1%，同时将推理时间分别减少27%（与LeFo结合）和72%（与GP结合）。

Conclusion: GP+SFV框架为触觉互联网提供了高精度和高效率的解决方案，为实现可靠触觉通信铺平了道路。

Abstract: The Tactile Internet (TI) requires ultra-low latency and reliable haptic
signal transmission, yet packet loss and delay remain unresolved challenges. We
present a novel prediction framework that integrates Gaussian Processes (GP)
with a ResNet-based Neural Network, where GP acts as an oracle to recover
signals lost or heavily delayed. To further optimize performance, we introduce
Shapley Feature Values (SFV), a principled feature selection mechanism that
isolates the most informative inputs for prediction. This GP+SFV framework
achieves 95.72% accuracy, surpassing the state-of-the-art LeFo method by 11.1%,
while simultaneously relaxing TI's rigid delay constraints. Beyond accuracy,
SFV operates as a modular accelerator: when paired with LeFo, it reduces
inference time by 27%, and when paired with GP, by 72%. These results establish
GP+SFV as both a high-accuracy and high-efficiency solution, paving the way for
practical and reliable haptic communications in TI systems.

</details>


### [5] [Neural Integrated Sensing and Communication for the MIMO-OFDM Downlink](https://arxiv.org/abs/2509.21118)
*Ziyi Wang,Frederik Zumegen,Christoph Studer*

Main category: eess.SP

TL;DR: 本文提出了一种基于MIMO-OFDM下行链路的神经ISAC信号处理框架，能够在无需修改现有通信链路的情况下实现广义感知功能，通过测量反向散射通信信号生成空间占用地图。


<details>
  <summary>Details</summary>
Motivation: 无线感知与通信应用在频谱和硬件需求上的持续融合推动了下一代网络中的集成感知与通信（ISAC）范式，神经网络驱动的ISAC利用数据驱动学习技术为现有通信基础设施添加感知能力。

Method: 基于MIMO-OFDM下行链路构建神经ISAC管道，测量反向散射通信信号生成离散地图表示，将其建模为多类或多标签分类问题，并设计了专门特征来减轻封闭或杂乱环境中的强反射路径影响。

Result: 使用射线追踪模型进行的广泛仿真表明，该神经ISAC框架能够可靠地重建场景地图，同时不改变MIMO-OFDM通信管道或降低数据速率。

Conclusion: 该框架为神经ISAC系统提供了一种有效的信号处理方法，能够在保持通信性能的同时实现可靠的场景感知功能。

Abstract: The ongoing convergence of spectrum and hardware requirements for wireless
sensing and communication applications has fueled the integrated sensing and
communication (ISAC) paradigm in next-generation networks. Neural-network-based
ISAC leverages data-driven learning techniques to add sensing capabilities to
existing communication infrastructure. This paper presents a novel
signal-processing framework for such neural ISAC systems based on the
multiple-input multiple-output (MIMO) and orthogonal frequency-division
multiplexing (OFDM) downlink. Our approach enables generalized sensing
functionality without modifying the MIMO-OFDM communication link. Specifically,
our neural ISAC pipeline measures the backscattered communication signals to
generate discrete map representations of spatial occupancy, formulated as
multiclass or multilabel classification problems, which can then be utilized by
specialized downstream tasks. To improve sensing performance in closed or
cluttered environments, our neural ISAC pipeline relies on features
specifically designed to mitigate strong reflective paths. Extensive
simulations using ray-tracing models demonstrate that our neural ISAC framework
reliably reconstructs scene maps without altering the MIMO-OFDM communication
pipeline or reducing data rates.

</details>


### [6] [A Secure ISAC Waveform Design Framework via Random Frequency and PRI Agility](https://arxiv.org/abs/2509.21162)
*Ali Khandan Boroujeni,Hyeon Seok Rou,Ghazal Bagheri,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Kuranage Roche Rayan Ranasinghe,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一种用于增强集成感知与通信（ISAC）系统安全性、数据速率和感知性能的新框架，采用随机频率和脉冲重复间隔（PRI）捷变（RFPA）方法进行波形设计，并引入混合信息嵌入方案。


<details>
  <summary>Details</summary>
Motivation: 当前ISAC系统在安全性、数据速率和感知性能方面存在不足，需要一种能够同时提升这些性能的解决方案，以应对被动敌手的侦察威胁并提高系统效率。

Method: 采用RFPA方法进行波形设计，利用共享密钥生成随机序列来混淆关键雷达参数；引入混合信息嵌入方案，结合ASK、PSK、IM和SM调制技术；提出低复杂度稀疏匹配滤波器接收器进行解码。

Result: 通过模糊函数（AF）分析，表明所提波形具有优异的距离-速度分辨率和杂波抑制能力，能有效阻碍无密钥的被动敌手进行侦察。

Conclusion: 所提框架显著提升了ISAC系统的安全性、数据速率和感知性能，为未来ISAC系统的发展提供了有效的解决方案。

Abstract: This paper presents a novel framework for enhancing the security, data rate,
and sensing performance of integrated sensing and communications (ISAC)
systems. We employ a random frequency and pulse repetition interval (PRI)
agility (RFPA) method for the waveform design, where the necessary random
sequences are governed by shared secrets. These secrets, which can be
pre-shared or generated via channel reciprocity, obfuscate critical radar
parameters like Doppler frequency and pulse start times, thereby significantly
impeding the ability to perform reconnaissance from a passive adversary without
the secret key. To further introduce enhanced data throughput, we also
introduce a hybrid information embedding scheme that integrates amplitude shift
keying (ASK), phase shift keying (PSK), index modulation (IM), and spatial
modulation (SM), for which a low-complexity sparse-matched filter receiver is
proposed for accurate decoding with practical complexity. Finally, the
excellent range-velocity resolution and clutter suppression of the proposed
waveform are analyzed via the ambiguity function (AF).

</details>


### [7] [Adversarially Robust MIMO Physical Layer Authentication for Non-Stationary Channels](https://arxiv.org/abs/2509.21171)
*Ali Khandan Boroujeni,Ghazal Bagheri,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一个针对非平稳MIMO无线信道的对抗性鲁棒物理层认证框架，结合了序列贝叶斯决策、对比学习深度特征提取和生成对抗建模来模拟自适应欺骗者。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设信道平稳或观测独立，无法有效处理非平稳MIMO信道中的时空相关性、视距阻塞和动态欺骗策略。

Method: 集成序列贝叶斯决策、对比学习深度特征提取和生成对抗建模，使用2状态和3状态隐马尔可夫模型进行性能分析，并采用移动平均在线自适应。

Result: 提供了对数似然比、检测概率和稳态近似的闭式递归，相比传统序列认证方案展现出显著的鲁棒性提升。

Conclusion: 该框架能够有效应对非平稳MIMO信道环境中的自适应欺骗攻击，提供更强的认证鲁棒性。

Abstract: We propose an adversarially robust physical layer authentication (AR-PLA)
framework tailored for non-stationary multiple-input multiple-output (MIMO)
wireless channels. The framework integrates sequential Bayesian
decision-making, deep feature extraction via contrastive learning, and
generative adversarial modeling to simulate adaptive spoofers. Unlike
conventional methods that assume stationary channels or independent
observations, our approach explicitly accounts for temporal and spatial
correlations, line-of-sight (LoS) blockages, and dynamic spoofing strategies. A
comprehensive analytical characterization of the authentication performance
using both 2-state and 3-state hidden Markov models (HMMs) with moving-average
online adaptation is also provided, with closed-form recursions for
loglikelihood ratios, detection probabilities, and steady-state approximations,
which demonstrate significant robustness improvement over classical sequential
authentication schemes.

</details>


### [8] [An enhanced statistical feature fusion approach using an improved distance evaluation algorithm and weighted K-nearest neighbor for bearing fault diagnosis](https://arxiv.org/abs/2509.21219)
*Amir Eshaghi Chaleshtori,Abdollah Aghaie*

Main category: eess.SP

TL;DR: 本文提出了一种结合改进距离评估算法和加权K近邻分类器的轴承故障诊断方法，通过多域特征提取和特征选择，在噪声环境中有效识别轴承故障。


<details>
  <summary>Details</summary>
Motivation: 轴承是旋转机械中最易发生故障的部件，其状态直接影响系统性能。在噪声环境中从多传感器数据中准确诊断轴承故障需要提取和选择信息丰富的特征。

Method: 1) 从时域、频域和时频域提取振动统计特征并整合；2) 使用改进的距离评估算法为特征分配权重，剔除不敏感特征；3) 使用选定的特征训练加权KNN分类器。

Result: 使用渥太华大学的轴承数据进行验证，结果表明该方法能够准确识别轴承故障。

Conclusion: 提出的方法在轴承故障诊断中表现出有效性，特别是在噪声环境下的多传感器数据场景中。

Abstract: Bearings are among the most failure-prone components in rotating machinery,
and their condition directly impacts overall performance. Therefore, accurately
diagnosing bearing faults is essential for ensuring system stability. However,
detecting such malfunctions in noisy environments, where data is collected from
multiple sensors, necessitates the extraction and selection of informative
features. This paper proposes an improved distance evaluation algorithm
combined with a weighted K-nearest neighbor (KNN) classifier for bearing fault
diagnosis. The process begins with extracting and integrating statistical
features of vibration across the time, frequency, and time-frequency domains.
Next, the improved distance evaluation algorithm assigns weights to the
extracted features, retaining only the most informative ones by eliminating
insensitive features. Finally, the selected features are used to train the
weighted KNN classifier. To validate the proposed method, we employ bearing
data from the University of Ottawa. The results demonstrate the effectiveness
of our approach in accurately identifying bearing faults.

</details>


### [9] [Vision-Intelligence-Enabled Beam Tracking for Cross-Interface Water-Air Optical Wireless Communications](https://arxiv.org/abs/2509.21290)
*Tianqi Mao,Jiayue Liu,Weijie Liu,Dezhi Zheng,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 本文针对水-空光无线通信中因波动海面导致的波束失准问题，提出了一种基于视觉和AI的动态信道预测算法，通过CNN-Bi-LSTM结合注意力机制来提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 海洋应用的快速发展需要实时无线回传大量观测数据，但窄带声学方法难以满足需求。水-空光无线通信面临波动海面导致的动态波束失准挑战，需要实时自适应对准技术。

Method: 建立水-空光无线传输的数学信道模型，提出基于视觉的波束跟踪算法，整合CNN和Bi-LSTM网络，并引入注意力机制来提取视觉数据中的关键时空特征。

Result: 数值仿真表明，该算法在维持接收信号强度和抑制视觉噪声方面优于传统方法，证明其对恶劣水-空光无线通信条件的鲁棒性。

Conclusion: 所提出的AI驱动波束跟踪算法能有效解决水-空光无线通信中的动态对准问题，为海洋应用提供可靠的宽带传输解决方案。

Abstract: The escalating development of oceanic applications like underwater
surveillance and mineral exploration, is motivating real-time wireless backhaul
of the considerable observation data. Such prospects can be hardly realized by
the narrowband acoustic approach. Alternatively, optical wireless communication
(OWC) has emerged as a promising solution for maritime and underwater
applications due to its great potential for broadband underwater transmission.
However, the implementations of water-air OWC can be rather challenging,
especially when penetrating the fluctuating interface, where the direction of
refracted signals changes dynamically, causing severe beam misalignment with
airborne stations. This has necessitated real-time transceiver alignment
adaptable to the sophisticated oceanic environment, which has yet to be
addressed. Against this background, this paper establishes a mathematical
channel model for water-air optical wireless transmission across the
fluctuating sea surface. Based on the model, we propose a vision-based beam
tracking algorithm that leverages artificial intelligence (AI) methods for
dynamic channel prediction. The proposed algorithm integrates a convolutional
neural network (CNN) with bi-directional long short-term memory (Bi-LSTM),
which further incorporates the attention mechanism to effectively extract
critical spatio-temporal features from the vision data. The numerical
simulation results show that the proposed algorithm can outperform its
classical counterparts in maintaining receiving signal strength and supressing
the vision noises, which demonstrates its robustness against the the harsh
conditions of water-air OWC systems.

</details>


### [10] [Efficient Digital Methods to Quantify Sensor Output Uncertainty](https://arxiv.org/abs/2509.21311)
*Orestis Kaparounakis,Phillip Stanley-Marbell*

Main category: eess.SP

TL;DR: 该论文研究了传感器校准参数量化不确定性对最终测量精度的影响，以热电堆传感器为例，展示了不确定性传播机制，并在嵌入式平台上实现了高效的不确定性跟踪系统。


<details>
  <summary>Details</summary>
Motivation: 传感器输出不确定性的准确表征对于可靠的数据解释至关重要，特别是在校准参数精度有限的情况下，需要研究这种不确定性如何影响整体测量精度。

Method: 以热电堆传感器为例，分析传感器校准和转换方程如何传播校准参数量化引起的不确定性，并在两个商用不确定性跟踪硬件平台上进行原型验证。

Result: 实验结果显示，校准相关量的认知不确定性导致传感器输出绝对误差高达5.3°C（相对误差25.7%），在边缘检测应用中可将Canny算子的假阳性边缘降至零，同时在嵌入式平台上实现16.7mW和147.15mW的功耗，分别比蒙特卡洛计算快42.9倍和94.4倍。

Conclusion: 该方法具有实用性，可在实际嵌入式传感器系统中实现实时不确定性跟踪，为实时应用铺平了道路。

Abstract: Accurate characterization of sensor output uncertainty is important for
reliable data interpretation in many applications. Here, we investigate the
impact of transducer-level measurement uncertainty on overall sensor
measurement accuracy due to limited-precision information about sensor
components. We explain our method using thermopile-based sensors as an example
class of sensors. We show how sensor calibration and conversion equations,
which are an essential part of all sensing systems, propagate uncertainties
resulting from the quantization of calibration parameters, to the final,
compensated sensor output. The experimental results show that the epistemic
uncertainty of calibration-related quantities leads to absolute error in the
sensor output as high as 5.3 {\deg}C (and relative error as high as 25.7%) for
one commonly-used thermopile sensor. In one instance of using the epistemic
uncertainty information in edge detection, we show reduction of false-positives
edges to zero for the conventional Canny operator, while maintaining accuracy.
We show these ideas are practical and possible on actual embedded sensor
systems by prototyping them on two commercially-available uncertainty tracking
hardware platforms, one with average power dissipation 16.7 mW and 42.9x
speedup compared to the equal-confidence Monte Carlo computation (the status
quo), and the other with average power dissipation 147.15 mW and 94.4x speedup,
paving the way for use in real time.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling](https://arxiv.org/abs/2509.20396)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: eess.AS

TL;DR: 提出了一种基于音素级不确定性的数据高效个性化方法，通过蒙特卡洛Dropout识别困难音素并进行针对性过采样，显著提升了非规范性语音的ASR准确率。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在处理由脑瘫或结构异常等条件导致的非规范性语音时表现不佳，主要由于高声学变异性和训练数据稀缺。

Method: 使用蒙特卡洛Dropout量化音素级不确定性，识别模型认为最困难的音素，并基于此进行针对性过采样策略。

Result: 在英语和德语数据集上验证，模型不确定性评估与临床专家对语音困难的评估高度相关，显著提高了ASR准确率。

Conclusion: 该方法提供了一个实用的个性化ASR框架，首次成功将模型不确定性与专家临床评估对齐，为包容性语音识别提供了有效解决方案。

Abstract: Automatic speech recognition (ASR) systems struggle with non-normative speech
from individuals with impairments caused by conditions like cerebral palsy or
structural anomalies. The high acoustic variability and scarcity of training
data severely degrade model performance. This work introduces a data-efficient
personalization method that quantifies phoneme-level uncertainty to guide
fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model
finds most difficult and use these estimates for a targeted oversampling
strategy. We validate our method on English and German datasets. Crucially, we
demonstrate that our model-derived uncertainty strongly correlates with
phonemes identified as challenging in an expert clinical logopedic report,
marking, to our knowledge, the first work to successfully align model
uncertainty with expert assessment of speech difficulty. Our results show that
this clinically-validated, uncertainty-guided sampling significantly improves
ASR accuracy, delivering a practical framework for personalized and inclusive
ASR.

</details>


### [12] [Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition](https://arxiv.org/abs/2509.20397)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Shih-Chii Liu,Yingqiang Gao*

Main category: eess.AS

TL;DR: 本文提出了一种基于贝叶斯低秩适应的ASR个性化方法，用于数据高效微调，显著提高了对言语障碍语音的识别准确性。


<details>
  <summary>Details</summary>
Motivation: 先天性障碍（如脑瘫、唐氏综合征）和后天性脑损伤（如中风、创伤）导致的言语障碍给ASR系统带来重大挑战。现有最先进的ASR模型（如Whisper）由于训练数据有限和声学变异性高，在处理非标准语音时表现不佳。收集和标注非标准语音数据既困难又耗时。

Method: 基于贝叶斯低秩适应的ASR个性化方法，支持数据高效微调。在英语UA-Speech数据集和新收集的德语BF-Sprache数据集上进行验证，该数据集来自一名有结构性言语障碍的儿童。

Result: 该方法显著提高了对障碍语音的ASR准确性，同时保持了数据和标注效率。

Conclusion: 该方法为构建包容性ASR系统提供了一条实用路径，能够有效应对低资源设置下言语障碍个体的ASR挑战。

Abstract: Speech impairments resulting from congenital disorders, such as cerebral
palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due
to stroke, traumatic accidents, or tumors, present major challenges to
automatic speech recognition (ASR) systems. Despite recent advancements,
state-of-the-art ASR models like Whisper still struggle with non-normative
speech due to limited training data availability and high acoustic variability.
Moreover, collecting and annotating non-normative speech is burdensome:
speaking is effortful for many affected individuals, while laborious annotation
often requires caregivers familiar with the speaker. This work introduces a
novel ASR personalization method based on Bayesian Low-rank Adaptation for
data-efficient fine-tuning. We validate our method on the English UA-Speech
dataset and a newly collected German speech dataset, BF-Sprache, from a child
with structural speech impairment. The dataset and approach are designed to
reflect the challenges of low-resource settings that include individuals with
speech impairments. Our method significantly improves ASR accuracy for impaired
speech while maintaining data and annotation efficiency, offering a practical
path toward inclusive ASR.

</details>


### [13] [Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction](https://arxiv.org/abs/2509.20410)
*Weijie Wu,Wenhao Guan,Kaidi Wang,Peijie Chen,Zhuanling Zha,Junbo Li,Jun Fang,Lin Li,Qingyang Hong*

Main category: eess.AS

TL;DR: Phoenix-VAD是一个基于LLM的流式语义端点检测模型，通过利用LLM的语义理解能力和滑动窗口训练策略，实现可靠的语义端点检测，支持流式推理。


<details>
  <summary>Details</summary>
Motivation: 现有的口语对话模型缺乏即插即用的全双工预测模块来进行语义端点检测，这阻碍了无缝的音频交互。

Method: 利用LLM的语义理解能力，采用滑动窗口训练策略，实现流式语义端点检测。

Result: 在语义完整和不完整的语音场景实验中，Phoenix-VAD表现出优秀且具有竞争力的性能。

Conclusion: 该设计使全双工预测模块能够独立于对话模型进行优化，为下一代人机交互提供更可靠和灵活的支持。

Abstract: Spoken dialogue models have significantly advanced intelligent
human\textendash computer interaction, yet they lack a plug\textendash
and\textendash play full\textendash duplex prediction module for semantic
endpoint detection, hindering seamless audio interactions. In this paper, we
introduce Phoenix\textendashVAD, an LLM\textendash based model that enables
streaming semantic endpoint detection. Specifically, Phoenix\textendash VAD
leverages the semantic comprehension capability of the LLM and a sliding window
training strategy to achieve reliable semantic endpoint detection while
supporting streaming inference. Experiments on both semantically complete and
incomplete speech scenarios indicate that Phoenix\textendash VAD achieves
excellent and competitive performance. Furthermore, this design enables the
full\textendash duplex prediction module to be optimized independently of the
dialogue model, providing more reliable and flexible support for
next\textendash generation human\textendash computer interaction.

</details>


### [14] [Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens](https://arxiv.org/abs/2509.20485)
*Ismail Rasim Ulgen,Zongyang Du,Junchen Lu,Philipp Koehn,Berrak Sisman*

Main category: eess.AS

TL;DR: TTScore是一个基于条件预测离散语音标记的定向无参考评估框架，用于合成语音的客观评估，包括TTScore-int（可懂度评估）和TTScore-pro（韵律评估）两个组件。


<details>
  <summary>Details</summary>
Motivation: 现有合成语音评估指标在可懂度和韵律评估方面存在局限性，WER仅提供粗粒度的文本可懂度测量，而F0-RMSE等音高指标提供的是狭窄的、依赖参考的韵律视图，与人类感知相关性较弱。

Method: TTScore采用两个基于输入文本的条件序列到序列预测器：TTScore-int通过内容标记测量可懂度，TTScore-pro通过韵律标记评估韵律。对于每个合成语音，预测器计算相应标记序列的似然度，产生可解释的分数。

Result: 在SOMOS、VoiceMOS和TTSArena基准测试上的实验表明，TTScore-int和TTScore-pro提供了可靠的特异性评估，与人类对整体质量的判断相比现有可懂度和韵律指标具有更强的相关性。

Conclusion: TTScore框架解决了现有合成语音评估指标的局限性，提供了更准确、与人类感知更一致的客观评估方法。

Abstract: Objective evaluation of synthesized speech is critical for advancing speech
generation systems, yet existing metrics for intelligibility and prosody remain
limited in scope and weakly correlated with human perception. Word Error Rate
(WER) provides only a coarse text-based measure of intelligibility, while
F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent
view of prosody. To address these limitations, we propose TTScore, a targeted
and reference-free evaluation framework based on conditional prediction of
discrete speech tokens. TTScore employs two sequence-to-sequence predictors
conditioned on input text: TTScore-int, which measures intelligibility through
content tokens, and TTScore-pro, which evaluates prosody through prosody
tokens. For each synthesized utterance, the predictors compute the likelihood
of the corresponding token sequences, yielding interpretable scores that
capture alignment with intended linguistic content and prosodic structure.
Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that
TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and
achieve stronger correlations with human judgments of overall quality than
existing intelligibility and prosody-focused metrics.

</details>


### [15] [Real-Time System for Audio-Visual Target Speech Enhancement](https://arxiv.org/abs/2509.20741)
*T. Aleksandra Ma,Sile Yin,Li-Chia Yang,Shuo Zhang*

Main category: eess.AS

TL;DR: RAVEN是一个实时音频-视觉语音增强系统，能够在CPU上运行，利用唇部运动信息提升语音增强效果，适用于环境噪声、干扰说话者等多种场景。


<details>
  <summary>Details</summary>
Motivation: 传统单通道音频语音增强方法在存在干扰说话者时效果有限，而现有研究虽然探索了视觉线索的应用，但缺乏能够在CPU硬件上实时运行的交互式音频-视觉语音增强系统。

Method: 使用预训练的音频-视觉语音识别模型中的视觉嵌入来编码唇部运动信息，实现实时目标语音增强。

Result: 系统能够泛化处理环境噪声、干扰说话者、瞬态声音甚至歌声，实现了在CPU上的实时音频-视觉语音增强。

Conclusion: RAVEN填补了实时音频-视觉语音增强系统在CPU硬件上运行的空白，通过现场演示展示了其在实际应用中的有效性。

Abstract: We present a live demonstration for RAVEN, a real-time audio-visual speech
enhancement system designed to run entirely on a CPU. In single-channel,
audio-only settings, speech enhancement is traditionally approached as the task
of extracting clean speech from environmental noise. More recent work has
explored the use of visual cues, such as lip movements, to improve robustness,
particularly in the presence of interfering speakers. However, to our
knowledge, no prior work has demonstrated an interactive system for real-time
audio-visual speech enhancement operating on CPU hardware. RAVEN fills this gap
by using pretrained visual embeddings from an audio-visual speech recognition
model to encode lip movement information. The system generalizes across
environmental noise, interfering speakers, transient sounds, and even singing
voices. In this demonstration, attendees will be able to experience live
audio-visual target speech enhancement using a microphone and webcam setup,
with clean speech playback through headphones.

</details>


### [16] [SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS](https://arxiv.org/abs/2509.20802)
*Tan Dat Nguyen,Jaehun Kim,Ji-Hoon Kim,Shukjae Choi,Youshin Lim,Joon Son Chung*

Main category: eess.AS

TL;DR: SPADE框架通过结构化剪枝和自适应蒸馏技术，将大型语言模型文本转语音系统的Transformer层数减半，在保持感知质量的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM-TTS系统参数量大、延迟高的问题，使其更适合实际部署

Method: 结合基于词错误率的层重要性指标进行剪枝，以及多级知识蒸馏来恢复自回归一致性

Result: 在零样本基准测试中，Transformer深度减半，VRAM使用减少20%，实时因子提升1.7倍，仅使用不到5%的原始训练数据

Conclusion: 紧凑型LLM-TTS模型能够在保持自然度和说话人相似度的同时实现实用的实时语音生成

Abstract: The goal of this paper is to introduce SPADE, a framework for Structured
Pruning and Adaptive Distillation for Efficient Large Language Model-based
text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability
and zero-shot generalization, but their large parameter counts and high latency
limit real-world deployment. SPADE addresses this by combining (i) a pruning
step guided by a word-error-rate-based layer importance index to remove
non-essential Transformer layers, with (ii) multi-level knowledge distillation
to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves
near-parity perceptual quality while halving Transformer depth, reducing VRAM
usage by up to 20%, and achieving up to 1.7x faster real-time factor with less
than 5% of the original training data. These results show that compact LLM-TTS
models can maintain naturalness and speaker similarity while enabling practical
real-time speech generation. Audio samples are available at
https://mm.kaist.ac.kr/projects/SPADE/.

</details>


### [17] [PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice Pickup in Hearables](https://arxiv.org/abs/2509.20875)
*Mattes Ohlenbusch,Mikolaj Kegler,Marko Stamenovic*

Main category: eess.AS

TL;DR: 本文比较了两种解决单通道语音增强中目标语音与干扰语音区分问题的策略：个性化语音增强（PSE）和辅助传感器语音增强（AS-SE），并展示了二者结合（PAS-SE）的互补性能优势。


<details>
  <summary>Details</summary>
Motivation: 在可听设备中实现语音拾取时，单通道方法难以在没有额外上下文的情况下区分目标语音和干扰语音，需要解决这种模糊性问题。

Method: 比较PSE（使用注册语音表示目标）和AS-SE（使用耳内麦克风作为额外输入）两种策略，提出训练时增强方法促进AS-SE系统的跨数据集泛化，并研究PSE和AS-SE的结合（PAS-SE）。

Result: PSE和AS-SE结合提供互补性能优势，特别是当注册语音通过耳内麦克风录制时；使用嘈杂耳内注册的PAS-SE相比AS-SE系统仍保持性能优势。

Conclusion: PSE和AS-SE策略的结合能够有效解决单通道语音增强中的目标语音识别模糊问题，具有实际应用价值。

Abstract: Speech enhancement for voice pickup in hearables aims to improve the user's
voice by suppressing noise and interfering talkers, while maintaining own-voice
quality. For single-channel methods, it is particularly challenging to
distinguish the target from interfering talkers without additional context. In
this paper, we compare two strategies to resolve this ambiguity: personalized
speech enhancement (PSE), which uses enrollment utterances to represent the
target, and auxiliary-sensor speech enhancement (AS-SE), which uses in-ear
microphones as additional input. We evaluate the strategies on two public
datasets, employing different auxiliary sensor arrays, to investigate their
cross-dataset generalization. We propose training-time augmentations to
facilitate cross-dataset generalization of AS-SE systems. We also show that
combining PSE and AS-SE (PAS-SE) provides complementary performance benefits,
especially when enrollment speech is recorded with the in-ear microphone. We
further demonstrate that PAS-SE personalized with noisy in-ear enrollments
maintains performance benefits over the AS-SE system.

</details>


### [18] [TF-Restormer: Complex Spectral Prediction for Speech Restoration](https://arxiv.org/abs/2509.21003)
*Ui-Hyeop Shin,Jaehyun Ko,Woocheol Jeong,Hyuing-Min Park*

Main category: eess.AS

TL;DR: TF-Restormer是一种用于语音恢复的编码器-解码器架构，能够在任意输入输出采样率下实现高效通用的语音恢复，无需冗余重采样，支持流式处理，并在信号保真度和感知质量方面优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有语音恢复系统存在信号保真度牺牲、扩散模型不适用于流式处理、固定目标采样率导致冗余计算等问题，需要一种能够处理复合失真且支持任意采样率的高效解决方案。

Method: 采用时频双路径编码器专注于输入带宽分析，通过带有频率扩展查询的轻量解码器重建缺失高频带；引入共享采样频率无关STFT鉴别器支持跨速率对抗训练；使用因果时间模块支持流式处理；通过频谱归纳偏置注入提高极端退化下的鲁棒性；提出缩放对数频谱损失稳定优化。

Result: TF-Restormer作为跨采样率的单一模型，在信号保真度和感知质量方面持续优于现有系统，其流式模式在实时应用中保持竞争力。

Conclusion: TF-Restormer提供了一种高效通用的语音恢复解决方案，能够在任意采样率下实现高质量恢复，支持流式处理，为实际应用提供了实用价值。

Abstract: Speech restoration in real-world conditions is challenging due to compounded
distortions such as clipping, band-pass filtering, digital artifacts, noise,
and reverberation, and low sampling rates. Existing systems, including
vocoder-based approaches, often sacrifice signal fidelity, while diffusion
models remain impractical for streaming. Moreover, most assume a fixed target
sampling rate, requiring external resampling that leads to redundant
computations. We present TF-Restormer, an encoder-decoder architecture that
concentrates analysis on input-bandwidth with a time-frequency dual-path
encoder and reconstructs missing high-frequency bands through a light decoder
with frequency extension queries. It enables efficient and universal
restoration across arbitrary input-output rates without redundant resampling.
To support adversarial training across diverse rates, we introduce a shared
sampling-frequency-independent (SFI) STFT discriminator. TF-Restormer further
supports streaming with a causal time module, and improves robustness under
extreme degradations by injecting spectral inductive bias into the frequency
module. Finally, we propose a scaled log-spectral loss that stabilizes
optimization under severe conditions while emphasizing well-predicted spectral
details. As a single model across sampling rates, TF-Restormer consistently
outperforms prior systems, achieving balanced gains in signal fidelity and
perceptual quality, while its streaming mode maintains competitive
effectiveness for real-time application. Code and demos are available at
https://tf-restormer.github.io/demo.

</details>


### [19] [Measuring Audio's Impact on Correctness: Audio-Contribution-Aware Post-Training of Large Audio Language Models](https://arxiv.org/abs/2509.21060)
*Haolin He,Xingjian Du,Renhe Sun,Zheqi Dai,Yujia Xiao,Mingru Yang,Jiayi Zhou,Xiquan Li,Zhengxi Liu,Zining Liang,Chunyat Wu,Qianhua He,Tan Lee,Xie Chen,Weilong Zheng,Weiqiang Wang,Mark Plumbley,Jian Liu,Qiuqiang Kong*

Main category: eess.AS

TL;DR: 本文提出了AudioMCQ数据集和两种有效的后训练范式，解决了大音频语言模型在多阶段训练中数据分配不足和音频贡献度低的问题，在多个基准测试中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前大音频语言模型的后训练方法中，多阶段训练的数据分配策略尚未充分探索，且缺乏大规模高质量数据集。同时存在模型仅依赖文本信息而忽略音频内容的"零音频贡献"现象。

Method: 1) 构建包含57.1万样本的AudioMCQ音频多选题数据集；2) 提出音频贡献度过滤方法将数据分为弱/强音频贡献子集；3) 开发Weak-to-Strong和Mixed-to-Strong两种后训练范式。

Result: 在DCASE 2025音频问答挑战赛中获第一名，在MMAU-test-mini、MMAU、MMAR和MMSU基准测试中分别达到78.2%、75.6%、67.1%和70.7%的准确率，创造了新的最优性能。

Conclusion: 通过精心设计的数据集和训练策略，有效解决了LALMs的音频贡献度问题，显著提升了模型性能，为大音频语言模型的多阶段训练提供了有效解决方案。

Abstract: Large Audio Language Models (LALMs) represent an important frontier in
multimodal AI, addressing diverse audio tasks. Recently, post-training of LALMs
has received increasing attention due to significant performance improvements
over foundation models. While single-stage post-training such as reinforcement
learning (RL) has demonstrated promising results, multi-stage approaches such
as supervised fine-tuning (SFT) followed by RL remain suboptimal. The
allocation of data across multiple training stages to maximize LALM
capabilities has not been fully explored, and large-scale, high-quality
datasets for such research are also lacking. To address these problems, we
firstly present AudioMCQ, a comprehensive audio multiple-choice question
dataset comprising 571k samples with two kinds of chain-of-thought annotations.
Secondly, we investigate the prevalent zero audio-contribution phenomenon in
LALMs, where models derive correct answers solely from textual information
without processing audio content. We propose Audio-Contribution Filtering to
partition data into weak and strong audio-contribution subsets. Based on these
insights, we develop two effective post-training paradigms: Weak-to-Strong (SFT
on weak audio-contribution data followed by RL on strong audio-contribution
data) and Mixed-to-Strong (SFT on mixed audio-contribution data followed by RL
on strong audio-contribution data). We achieve first place in the DCASE 2025
Audio-Question-Answering challenge by using AudioMCQ. Additionally, leveraging
our dataset with different training strategies, we achieve 78.2\% on
MMAU-test-mini, 75.6\% on MMAU, 67.1\% on MMAR, and 70.7\% on MMSU,
establishing new state-of-the-art performance across these benchmarks.

</details>


### [20] [Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](https://arxiv.org/abs/2509.21087)
*Rostislav Makarov,Lea Schönherr,Timo Gerkmann*

Main category: eess.AS

TL;DR: 本文展示了先进的语音增强模型容易受到对抗性攻击的脆弱性，即精心设计的对抗性噪声可以被注入，使得增强后的语音输出传达完全不同的语义含义。同时发现扩散模型具有固有的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着语音增强模型表达能力越来越强，这种表达能力引入了新的安全漏洞，需要研究对抗性攻击的脆弱性。

Method: 通过精心设计并经过心理声学掩蔽的对抗性噪声注入到输入信号中，验证当代预测性语音增强模型的可操纵性。

Result: 实验证实当代预测性语音增强模型确实可以被这种方式操纵，而扩散模型由于其随机采样器的设计具有固有的对抗性攻击鲁棒性。

Conclusion: 语音增强模型的表达性增强带来了新的安全风险，扩散模型的设计提供了对抗此类攻击的天然防护机制。

Abstract: Machine learning approaches for speech enhancement are becoming increasingly
expressive, enabling ever more powerful modifications of input signals. In this
paper, we demonstrate that this expressiveness introduces a vulnerability:
advanced speech enhancement models can be susceptible to adversarial attacks.
Specifically, we show that adversarial noise, carefully crafted and
psychoacoustically masked by the original input, can be injected such that the
enhanced speech output conveys an entirely different semantic meaning. We
experimentally verify that contemporary predictive speech enhancement models
can indeed be manipulated in this way. Furthermore, we highlight that diffusion
models with stochastic samplers exhibit inherent robustness to such adversarial
attacks by design.

</details>


### [21] [Hybrid Real- And Complex-Valued Neural Network Concept For Low-Complexity Phase-Aware Speech Enhancement](https://arxiv.org/abs/2509.21185)
*Luan Vinícius Fiorio,Alex Young,Ronald M. Aarts*

Main category: eess.AS

TL;DR: 提出混合实值和复值神经网络用于语音增强，通过将实值网络扩展为混合版本，在相同参数数量下性能优于纯实值或纯复值模型，且计算复杂度显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有的实值或复值模型要么效率低下，要么复杂度高，需要一种既能保持性能又能降低复杂度的解决方案。

Method: 设计了一种将实值网络扩展为混合实复值网络的直接方法，基于卷积和卷积循环架构构建了实值、复值和混合版本进行对比。

Result: 混合网络在相同参数数量下始终优于对应模型，且在乘加运算复杂度方面显著低于其他版本。

Conclusion: 混合实复值神经网络为语音增强提供了一种高效且性能优越的解决方案，平衡了模型性能和计算复杂度。

Abstract: In this paper, we propose hybrid real- and complex-valued neural networks for
speech enhancement. Real- or complex-valued models are either inefficient or
present high complexity. We devise a straightforward design method for
extending a real-valued network into its hybrid counterpart. Based on speech
intelligibility and quality metrics, we compare the real, complex, and hybrid
versions of a convolutional and a convolutional-recurrent architecture. The
hybrid network consistently outperforms its counterparts with the same number
of parameters. Additionally, the hybrid models' complexity in terms of
multiply-accumulate operations is substantially lower than that of their
counterparts.

</details>


### [22] [MeanSE: Efficient Generative Speech Enhancement with Mean Flows](https://arxiv.org/abs/2509.21214)
*Jiahe Wang,Hongyu Wang,Wei Wang,Lei Yang,Chenda Li,Wangyou Zhang,Lufen Tan,Yanmin Qian*

Main category: eess.AS

TL;DR: 提出MeanSE模型，使用平均流方法实现高质量的单次函数评估语音增强，显著提升流匹配基线的性能


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的语音增强模型需要多次函数评估才能达到稳定性能，导致计算负载高且单次评估性能差

Method: 使用平均流方法建模平均速度场，实现高效的单次函数评估语音增强

Result: MeanSE在单次函数评估下显著优于流匹配基线，展现出极好的域外泛化能力

Conclusion: MeanSE是一种高效的生成式语音增强模型，能够在单次评估中实现高质量增强

Abstract: Speech enhancement (SE) improves degraded speech's quality, with generative
models like flow matching gaining attention for their outstanding perceptual
quality. However, the flow-based model requires multiple numbers of function
evaluations (NFEs) to achieve stable and satisfactory performance, leading to
high computational load and poor 1-NFE performance. In this paper, we propose
MeanSE, an efficient generative speech enhancement model using mean flows,
which models the average velocity field to achieve high-quality 1-NFE
enhancement. Experimental results demonstrate that our proposed MeanSE
significantly outperforms the flow matching baseline with a single NFE,
exhibiting extremely better out-of-domain generalization capabilities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [23] [QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection](https://arxiv.org/abs/2509.20679)
*Duc-Tuan Truong,Tianchi Liu,Ruijie Tao,Junjie Li,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: QAMO提出了一种质量感知的多中心单类学习方法，用于语音深度伪造检测，通过引入多个质量感知中心来更好地建模真实语音的类内变异性。


<details>
  <summary>Details</summary>
Motivation: 传统的单中心单类学习过度简化了真实语音表示，忽略了语音质量等有用线索。语音质量反映了语音的自然度，可以通过现有语音质量评估模型获得。

Method: QAMO扩展了传统的单类学习，引入多个质量感知中心，每个中心优化为代表不同的语音质量子空间。支持多中心集成评分策略，改善决策阈值设置，减少推理时对质量标签的需求。

Result: 使用两个中心分别代表高质量和低质量语音，QAMO在In-the-Wild数据集上实现了5.09%的等错误率，优于之前的单类和质量感知系统。

Conclusion: QAMO通过质量感知的多中心方法有效提升了语音深度伪造检测性能，更好地建模了真实语音的类内变异性。

Abstract: Recent work shows that one-class learning can detect unseen deepfake attacks
by modeling a compact distribution of bona fide speech around a single
centroid. However, the single-centroid assumption can oversimplify the bona
fide speech representation and overlook useful cues, such as speech quality,
which reflects the naturalness of the speech. Speech quality can be easily
obtained using existing speech quality assessment models that estimate it
through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware
Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends
conventional one-class learning by introducing multiple quality-aware
centroids. In QAMO, each centroid is optimized to represent a distinct speech
quality subspaces, enabling better modeling of intra-class variability in bona
fide speech. In addition, QAMO supports a multi-centroid ensemble scoring
strategy, which improves decision thresholding and reduces the need for quality
labels during inference. With two centroids to represent high- and low-quality
speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild
dataset, outperforming previous one-class and quality-aware systems.

</details>


### [24] [Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection](https://arxiv.org/abs/2509.20682)
*Duc-Tuan Truong,Tianchi Liu,Junjie Li,Ruijie Tao,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: 本文提出了一种双路径数据增强训练框架（DPDA），通过梯度对齐来解决语音深度伪造检测中原始输入与增强输入之间的梯度冲突问题，从而加速收敛并提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 在语音深度伪造检测中，数据增强常用于提高模型泛化能力，但原始输入和增强输入的反向传播梯度可能不一致，导致参数更新冲突，阻碍模型收敛并降低数据增强的效果。

Method: 设计了双路径数据增强训练框架，每个训练语音通过两个输入路径处理：原始语音和增强版本。通过比较和对齐它们的梯度方向来减少优化冲突。

Result: 分析显示使用RawBoost增强时约25%的训练迭代存在梯度冲突。通过梯度对齐，该方法减少了训练轮次，在In-the-Wild数据集上相比基线实现了18.69%的相对等错误率降低。

Conclusion: 梯度对齐能有效解决数据增强训练中的梯度冲突问题，加速模型收敛并显著提升语音深度伪造检测性能。

Abstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used
to improve model generalization across varied speech conditions and spoofing
attacks. However, during training, the backpropagated gradients from original
and augmented inputs may misalign, which can result in conflicting parameter
updates. These conflicts could hinder convergence and push the model toward
suboptimal solutions, thereby reducing the benefits of DA. To investigate and
address this issue, we design a dual-path data-augmented (DPDA) training
framework with gradient alignment for SDD. In our framework, each training
utterance is processed through two input paths: one using the original speech
and the other with its augmented version. This design allows us to compare and
align their backpropagated gradient directions to reduce optimization
conflicts. Our analysis shows that approximately 25% of training iterations
exhibit gradient conflicts between the original inputs and their augmented
counterparts when using RawBoost augmentation. By resolving these conflicts
with gradient alignment, our method accelerates convergence by reducing the
number of training epochs and achieves up to an 18.69% relative reduction in
Equal Error Rate on the In-the-Wild dataset compared to the baseline.

</details>


### [25] [AIBA: Attention-based Instrument Band Alignment for Text-to-Audio Diffusion](https://arxiv.org/abs/2509.20891)
*Junyoung Koh,Soo Yong Kim,Gyu Hyeong Choi,Yongwon Choi*

Main category: cs.SD

TL;DR: AIBA是一种轻量级、无需训练的方法，用于量化文本到音频扩散模型在时频平面上的注意力分布。


<details>
  <summary>Details</summary>
Motivation: 需要一种方法来理解和可视化文本到音频扩散模型在处理不同音频元素时的注意力机制，特别是针对不同乐器在时频平面上的关注模式。

Method: AIBA通过（i）在推理时钩取交叉注意力记录注意力概率而不修改权重；（ii）将注意力投影到固定大小的梅尔网格上，使其可直接与音频能量比较；（iii）使用可解释的指标（时频IoU/AP、频率轮廓相关性和指向游戏）来评估与乐器频带真实值的对齐程度。

Result: 在Slakh2100数据集上使用AudioLDM2骨干网络，AIBA揭示了乐器依赖的注意力趋势（如低音乐器偏好低频带），并实现了高精度和中等召回率。

Conclusion: AIBA提供了一种有效的训练后分析方法，能够量化文本到音频扩散模型的注意力机制，为模型的可解释性提供了重要工具。

Abstract: We present AIBA (Attention-In-Band Alignment), a lightweight, training-free
pipeline to quantify where text-to-audio diffusion models attend on the
time-frequency (T-F) plane. AIBA (i) hooks cross-attention at inference to
record attention probabilities without modifying weights; (ii) projects them to
fixed-size mel grids that are directly comparable to audio energy; and (iii)
scores agreement with instrument-band ground truth via interpretable metrics
(T-F IoU/AP, frequency-profile correlation, and a pointing game). On Slakh2100
with an AudioLDM2 backbone, AIBA reveals consistent instrument-dependent trends
(e.g., bass favoring low bands) and achieves high precision with moderate
recall.

</details>


### [26] [SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement](https://arxiv.org/abs/2509.20969)
*Shaohan Jiang,Junan Zhang,Yunjia Zhang,Jing Yang,Fan Fan,Zhizheng Wu*

Main category: cs.SD

TL;DR: 本文提出了SingVERSE，首个真实世界的歌唱声音增强基准测试，解决了该领域缺乏现实评估数据的问题，并揭示了感知质量与可理解性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 歌唱声音增强的发展受到缺乏现实评估数据的限制，需要建立一个覆盖多样化声学场景并提供配对、工作室质量干净参考的基准测试。

Method: 引入SingVERSE基准测试，对最先进模型进行全面评估，分析感知质量与可理解性的权衡，并验证在领域内歌唱数据上训练的效果。

Result: 发现模型在感知质量和可理解性之间存在一致权衡，同时证明在歌唱数据上训练能显著提升增强性能而不降低语音能力。

Conclusion: 本研究为社区提供了基础性基准测试和关键见解，为这个探索不足的领域指明了简单有效的未来发展路径。

Abstract: This paper presents a benchmark for singing voice enhancement. The
development of singing voice enhancement is limited by the lack of realistic
evaluation data. To address this gap, this paper introduces SingVERSE, the
first real-world benchmark for singing voice enhancement, covering diverse
acoustic scenarios and providing paired, studio-quality clean references.
Leveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art
models and uncover a consistent trade-off between perceptual quality and
intelligibility. Finally, we show that training on in-domain singing data
substantially improves enhancement performance without degrading speech
capabilities, establishing a simple yet effective path forward. This work
offers the community a foundational benchmark together with critical insights
to guide future advances in this underexplored domain. Demopage:
https://singverse.github.io

</details>


### [27] [i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents](https://arxiv.org/abs/2509.20971)
*Anupam Purwar,Aditya Choudhary*

Main category: cs.SD

TL;DR: 论文研究了低延迟端到端语音到语音通信模型的实时优化，重点关注ASR、TTS和对话管理组件的处理时间减少，发现TTS组件对实时因子影响最大，通过优化RVQ迭代次数和码本数量来平衡质量和延迟。


<details>
  <summary>Details</summary>
Motivation: 优化语音到语音系统以实现实时对话应用，减少处理时间同时保持高质量的交互体验。

Method: 分析V-2-V系统关键组件（ASR、TTS、对话管理），实验基于CSM1b架构，通过优化TTS解码器的RVQ迭代次数和码本数量来降低延迟。

Result: TTS组件对实时因子影响最大，减少RVQ迭代次数和码本数量能显著优化处理时间，但会牺牲一定的语音质量。

Conclusion: 对于基于CSM的V-2-V系统，最重要的优化手段是减少RVQ迭代次数和码本使用量，需要在语音质量和实时性能之间找到平衡点。

Abstract: We experiment with a low-latency, end-to-end voice-to-voice communication
model to optimize it for real-time conversational applications. By analyzing
components essential to voice to voice (V-2-V) system viz. automatic speech
recognition (ASR), text-to-speech (TTS), and dialog management, our work
analyzes how to reduce processing time while maintaining high-quality
interactions to identify the levers for optimizing V-2-V system. Our work
identifies that TTS component which generates life-like voice, full of emotions
including natural pauses and exclamations has highest impact on Real time
factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the
capability to understand tone as well as context of conversation by ingesting
both audio and text of prior exchanges to generate contextually accurate
speech. We explored optimization of Residual Vector Quantization (RVQ)
iterations by the TTS decoder which come at a cost of decrease in the quality
of voice generated. Our experimental evaluations also demonstrate that for
V-2-V implementations based on CSM most important optimizations can be brought
by reducing the number of RVQ Iterations along with the codebooks used in Mimi.

</details>


### [28] [SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization](https://arxiv.org/abs/2509.21033)
*Jiehui Luo,Yuguo Yin,Yuxin Xie,Jinghan Ru,Xianwei Zhuang,Minghua He,Aofan Liu,Zihan Xiong,Dongchao Yang*

Main category: cs.SD

TL;DR: 本文提出支持向量正则化(SVR)方法来解决对比学习中负样本垂直分量的双重性问题，通过引入辅助支持向量来控制垂直分量，在利用其丰富信息的同时减轻优化轨迹漂移问题。


<details>
  <summary>Details</summary>
Motivation: 对比语言-音频预训练中，负样本推力的垂直分量既包含丰富的补充信息，又因其无约束特性会导致优化轨迹漂移和训练不稳定，这是一个双刃剑问题。

Method: 提出SVR方法，引入辅助支持向量来控制垂直分量，并探索了两种无监督语义半径建模策略：直接参数化和带有约束的自适应半径预测器模块。

Result: 实验结果表明，SVR在标准音频-文本数据集上的分类、单语言检索和多语言检索任务中，均优于InfoNCE和SigLIP等广泛使用的基线方法。

Conclusion: 理论分析和优化轨迹漂移实验验证了SVR方法的正确性和有效性，为解决对比学习中的优化稳定性问题提供了新思路。

Abstract: Contrastive language-audio pretraining, which aims to unify multimodal
representations in a shared embedding space, serves as a cornerstone for
building a wide range of applications, from cross-modal retrieval to
cutting-edge multimodal large language models. However, we find that the
perpendicular component of the pushing force from negative samples in
contrastive learning is a double-edged sword: it contains rich supplementary
information from negative samples, yet its unconstrained nature causes
optimization trajectory drift and training instability. To address this, we
propose Support Vector Regularization (SVR), a method that introduces an
auxiliary support vector to control this perpendicular component, aiming to
harness its rich information while mitigating the associated trajectory drift.
The efficacy of SVR is critically governed by its semantic radius, for which we
explore two unsupervised modeling strategies: direct parameterization and an
adaptive radius predictor module enhanced with constraints to improve its
predicting accuracy. Extensive experimental results demonstrate that our method
surpasses widely used baselines like InfoNCE and SigLIP loss across
classification, monolingual retrieval, and multilingual retrieval on standard
audio-text datasets. Both the theoretical analysis and the experimental results
on optimizing trajectory drift validate the correctness and effectiveness of
our SVR method.

</details>


### [29] [UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice](https://arxiv.org/abs/2509.21144)
*Sitong Cheng,Weizhen Bian,Xinsheng Wang,Ruibin Yuan,Jianyi Chen,Shunshun Yin,Yike Guo,Wei Xue*

Main category: cs.SD

TL;DR: UniSS是一个单阶段表达性语音到语音翻译框架，通过语音语义和风格建模与文本LLM集成，解决了数据稀缺、多阶段处理复杂和LLM翻译能力迁移有限等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决表达性语音翻译中的三个关键挑战：保留表达风格的配对语音数据稀缺、多阶段处理管道复杂、大型语言模型的翻译能力迁移有限。

Method: 提出UniSS单阶段框架，设计语音语义和风格建模，与文本LLM框架集成构建统一文本-语音语言模型，采用跨模态思维链提示过程进行音频语义与文本对齐。

Result: 构建并发布了44.8k小时的大规模高质量表达性S2ST数据集UniST，实验结果显示UniSS在翻译保真度和语音质量方面显著优于先前方法，同时保持声音、情感和时长一致性。

Conclusion: UniSS为构建下一代表达性语音翻译系统建立了更简单有效的范式，显著提升了翻译性能和风格保持能力。

Abstract: The ultimate goal of expressive speech-to-speech translation (S2ST) is to
accurately translate spoken content while preserving the speaker identity and
emotional style. However, progress in this field is largely hindered by three
key challenges: the scarcity of paired speech data that retains expressive
styles, the complexity of multi-stage processing pipelines, and the limited
transfer of translation capabilities from large language models (LLMs). In this
work, we address these challenges by introducing UniSS, a novel single-stage
framework for expressive S2ST. Our approach features carefully designed speech
semantic and style modeling, enabling seamless integration with existing
text-based LLM frameworks to develop a unified text-speech language model. To
transfer translation capabilities from text to speech, we propose a cross-modal
chain-of-thought prompting process that progressively aligns audio semantics
with text and ensures style preservation in the decoded results. Furthermore,
we construct and release a large-scale, high-quality expressive S2ST dataset,
UniST, comprising 44.8k hours of data. Experimental results show that UniSS
significantly outperforms previous methods in translation fidelity and speech
quality while preserving voice, emotion, and duration consistency. Our work
establishes a simpler and more effective paradigm for building the next
generation of expressive S2ST systems. Audio samples are available at
https://cmots.github.io/uniss-demo.

</details>

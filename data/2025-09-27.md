<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 12]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-Time Markov Modeling for Single-Photon LiDAR: $1000 \times$ Acceleration and Convergence Analysis](https://arxiv.org/abs/2509.20500)
*Weijian Zhang,Hashan K. Weerasooriya,Prateek Chennuri,Stanley H. Chan*

Main category: eess.SP

TL;DR: 本文提出了异步单光子激光雷达（SP-LiDAR）时间戳分布的首个非顺序马尔可夫建模方法，通过重新参数化积分边界并将死时间效应分离为基矩阵的确定性行置换，实现了高达1000倍的加速。


<details>
  <summary>Details</summary>
Motivation: 异步单光子激光雷达在高质量3D应用和导航中很重要，但存在死时间的时间戳分布建模是一个具有挑战性的开放问题。现有方法构建大型转移矩阵计算成本高。

Method: 提出等效公式，重新参数化积分边界，将死时间效应分离为基矩阵的确定性行置换，实现向量化矩阵构建。

Result: 新模型与蒙特卡洛模拟相比产生几乎精确的平稳分布，但仅需一小部分时间，实现了1000倍的加速。

Conclusion: 该方法显著提高了计算效率，同时新理论分析揭示了第二大特征值的幅值和相位对收敛性的关键影响，这是文献中常被忽视的。

Abstract: Asynchronous single-photon LiDAR (SP-LiDAR) is an important imaging modality
for high-quality 3D applications and navigation, but the modeling of the
timestamp distributions of a SP-LiDAR in the presence of dead time remains a
very challenging open problem. Prior works have shown that timestamps form a
discrete-time Markov chain, whose stationary distribution can be computed as
the leading left eigenvector of a large transition matrix. However,
constructing this matrix is known to be computationally expensive because of
the coupling between states and the dead time. This paper presents the first
non-sequential Markov modeling for the timestamp distribution. The key
innovation is an equivalent formulation that reparameterizes the integral
bounds and separates the effect of dead time as a deterministic row permutation
of a base matrix. This decoupling enables efficient vectorized matrix
construction, yielding up to $1000 \times$ acceleration over existing methods.
The new model produces a nearly exact stationary distribution when compared
with the gold standard Monte Carlo simulations, yet using a fraction of the
time. In addition, a new theoretical analysis reveals the impact of the
magnitude and phase of the second-largest eigenvalue, which are overlooked in
the literature but are critical to the convergence.

</details>


### [2] [Wireless Powered MEC Systems via Discrete Pinching Antennas: TDMA versus NOMA](https://arxiv.org/abs/2509.20908)
*Peng Liu,Zesong Fei,Meng Hua,Guangji Chen,Xinyi Wang,Ruiqi Liu*

Main category: eess.SP

TL;DR: 本文研究了一种实用的离散夹持天线辅助无线供电移动边缘计算框架，通过联合优化能量收集和任务卸载策略，最大化总计算比特数，并比较了TDMA和NOMA方案在不同PA激活灵活性下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设理想的连续PA布置，而实际应用中PA是离散布置的。将PA集成到无线供电MEC系统中可以同时提升能量传输和任务卸载效率，但需要研究离散PA布置下的实际性能。

Method: 提出离散PA辅助无线供电MEC框架，设备先收集PA发射的射频信号能量，然后采用部分卸载模式。开发了两层算法，结合KKT条件的闭式解和基于交叉熵的学习方法来解决混合整数非线性优化问题。

Result: 数值结果验证了所提设计在能量收集和计算性能方面的优越性。在较粗的PA激活级别下，TDMA和NOMA性能相当；而在更细的激活粒度下，TDMA的计算性能优于NOMA。

Conclusion: 离散PA布置在实际MEC系统中具有重要应用价值，激活粒度的细化使TDMA方案在计算性能上优于NOMA，为实际系统设计提供了理论指导。

Abstract: Pinching antennas (PAs), a new type of reconfigurable and flexible antenna
structures, have recently attracted significant research interest due to their
ability to create line-of-sight links and mitigate large-scale path loss. Owing
to their potential benefits, integrating PAs into wireless powered mobile edge
computing (MEC) systems is regarded as a viable solution to enhance both energy
transfer and task offloading efficiency. Unlike prior studies that assume ideal
continuous PA placement along waveguides, this paper investigates a practical
discrete PA-assisted wireless powered MEC framework, where devices first
harvest energy from PA-emitted radio-frequency signals and then adopt a partial
offloading mode, allocating part of the harvested energy to local computing and
the remainder to uplink offloading. The uplink phase considers both the
time-division multiple access (TDMA) and non-orthogonal multiple access (NOMA),
each examined under three levels of PA activation flexibility. For each
configuration, we formulate a joint optimization problem to maximize the total
computational bits and conduct a theoretical performance comparison between the
TDMA and NOMA schemes. To address the resulting mixed-integer nonlinear
problems, we develop a two-layer algorithm that combines closed-form solutions
based on Karush-Kuhn-Tucker (KKT) conditions with a cross-entropy-based
learning method. Numerical results validate the superiority of the proposed
design in terms of the harvested energy and computation performance, revealing
that TDMA and NOMA achieve comparable performance under coarser PA activation
levels, whereas finer activation granularity enables TDMA to achieve superior
computation performance over NOMA.

</details>


### [3] [A General Optimization Framework for Movable Antenna Systems via Discrete Sampling](https://arxiv.org/abs/2509.20987)
*Changhao Liu,Weidong Mei,Zhi Chen,Jun Fang,Boyu Ning*

Main category: eess.SP

TL;DR: 提出了一种低复杂度的可移动天线位置优化框架，通过离散化采样点和吉布斯采样来避免局部最优，在MA增强广播系统中实现接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 可移动天线系统能够通过天线位置调整来重塑无线信道，但现有优化方法存在高计算复杂度或局部最优问题。

Method: 将天线移动区域离散化为采样点，将连续优化问题转化为离散点选择问题，通过多轮顺序更新每个天线的最优采样点，并在轮次间引入吉布斯采样来探索候选解。

Result: 数值结果表明，所提算法实现了接近最优的性能，并显著优于现有基准方法。

Conclusion: 该框架为MA位置优化提供了一种通用且低复杂度的解决方案，能够有效提升通信性能。

Abstract: Movable antenna (MA) systems have attracted growing interest in wireless
communications due to their ability to reshape wireless channels via local
antenna movement within a confined region. However, optimizing antenna
positions to enhance communication performance turns out to be challenging due
to the highly nonlinear relationship between wireless channels and antenna
positions. Existing approaches, such as gradient-based and heuristic
algorithms, often suffer from high computational complexity or undesired local
optima. To address the above challenge, this letter proposes a general and
low-complexity optimization framework for MA position optimization.
Specifically, we discretize the antenna movement region into a set of sampling
points, thereby transforming the continuous optimization problem into a
discrete point selection problem. Next, we sequentially update the optimal
sampling point for each MA over multiple rounds. To avoid convergence to poor
local optima, a Gibbs sampling (GS) phase is introduced between rounds to
explore adjacent and randomly generated candidate solutions. As a case study,
we investigate joint precoding and antenna position optimization for an
MA-enhanced broadcast system by applying the proposed framework. Numerical
results demonstrate that the proposed algorithm achieves near-optimal
performance and significantly outperforms existing benchmarks.

</details>


### [4] [Shapley Features for Robust Signal Prediction in Tactile Internet](https://arxiv.org/abs/2509.21032)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 提出了一种结合高斯过程和ResNet神经网络的新型预测框架，用于解决触觉互联网中的信号丢失和延迟问题，通过Shapley特征值优化特征选择，显著提升了预测精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网需要超低延迟和可靠的触觉信号传输，但数据包丢失和延迟问题仍未解决，需要开发更有效的预测方法来保证通信质量。

Method: 集成高斯过程与基于ResNet的神经网络，使用高斯过程作为预言机来恢复丢失或严重延迟的信号，并引入Shapley特征值进行特征选择优化。

Result: GP+SFV框架达到95.72%的准确率，比现有最佳方法LeFo提升11.1%，同时放宽了触觉互联网的严格延迟约束。SFV作为模块化加速器，与LeFo结合减少推理时间27%，与GP结合减少72%。

Conclusion: GP+SFV框架既是高精度又是高效率的解决方案，为触觉互联网系统中实用可靠的触觉通信铺平了道路。

Abstract: The Tactile Internet (TI) requires ultra-low latency and reliable haptic
signal transmission, yet packet loss and delay remain unresolved challenges. We
present a novel prediction framework that integrates Gaussian Processes (GP)
with a ResNet-based Neural Network, where GP acts as an oracle to recover
signals lost or heavily delayed. To further optimize performance, we introduce
Shapley Feature Values (SFV), a principled feature selection mechanism that
isolates the most informative inputs for prediction. This GP+SFV framework
achieves 95.72% accuracy, surpassing the state-of-the-art LeFo method by 11.1%,
while simultaneously relaxing TI's rigid delay constraints. Beyond accuracy,
SFV operates as a modular accelerator: when paired with LeFo, it reduces
inference time by 27%, and when paired with GP, by 72%. These results establish
GP+SFV as both a high-accuracy and high-efficiency solution, paving the way for
practical and reliable haptic communications in TI systems.

</details>


### [5] [Neural Integrated Sensing and Communication for the MIMO-OFDM Downlink](https://arxiv.org/abs/2509.21118)
*Ziyi Wang,Frederik Zumegen,Christoph Studer*

Main category: eess.SP

TL;DR: 本文提出了一种基于MIMO-OFDM下行链路的神经集成感知与通信(ISAC)信号处理框架，能够在无需修改现有通信链路的情况下实现广义感知功能。


<details>
  <summary>Details</summary>
Motivation: 无线感知与通信应用在频谱和硬件需求上的持续融合推动了下一代网络中的ISAC范式。神经网络驱动的ISAC利用数据驱动学习技术为现有通信基础设施增加感知能力。

Method: 提出神经ISAC管道，通过测量反向散射通信信号生成空间占用的离散地图表示，将其表述为多类或多标签分类问题。设计了专门特征来减轻封闭或杂乱环境中的强反射路径影响。

Result: 基于射线追踪模型的广泛仿真表明，该神经ISAC框架能够可靠地重建场景地图，且无需改变MIMO-OFDM通信管道或降低数据速率。

Conclusion: 该框架为神经ISAC系统提供了一种有效的信号处理方法，实现了通信与感知的无缝集成，在保持通信性能的同时增强了环境感知能力。

Abstract: The ongoing convergence of spectrum and hardware requirements for wireless
sensing and communication applications has fueled the integrated sensing and
communication (ISAC) paradigm in next-generation networks. Neural-network-based
ISAC leverages data-driven learning techniques to add sensing capabilities to
existing communication infrastructure. This paper presents a novel
signal-processing framework for such neural ISAC systems based on the
multiple-input multiple-output (MIMO) and orthogonal frequency-division
multiplexing (OFDM) downlink. Our approach enables generalized sensing
functionality without modifying the MIMO-OFDM communication link. Specifically,
our neural ISAC pipeline measures the backscattered communication signals to
generate discrete map representations of spatial occupancy, formulated as
multiclass or multilabel classification problems, which can then be utilized by
specialized downstream tasks. To improve sensing performance in closed or
cluttered environments, our neural ISAC pipeline relies on features
specifically designed to mitigate strong reflective paths. Extensive
simulations using ray-tracing models demonstrate that our neural ISAC framework
reliably reconstructs scene maps without altering the MIMO-OFDM communication
pipeline or reducing data rates.

</details>


### [6] [A Secure ISAC Waveform Design Framework via Random Frequency and PRI Agility](https://arxiv.org/abs/2509.21162)
*Ali Khandan Boroujeni,Hyeon Seok Rou,Ghazal Bagheri,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Kuranage Roche Rayan Ranasinghe,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 本文提出了一种用于增强集成感知与通信系统安全性和性能的新框架，采用随机频率和脉冲重复间隔捷变方法进行波形设计，并结合混合信息嵌入方案提高数据吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的集成感知与通信系统在安全性和数据速率方面存在不足，容易被被动对手进行侦察。需要一种能够同时增强安全性、数据速率和感知性能的解决方案。

Method: 使用随机频率和脉冲重复间隔捷变方法进行波形设计，随机序列由共享密钥控制；提出混合信息嵌入方案，结合ASK、PSK、索引调制和空间调制；设计低复杂度稀疏匹配滤波器接收器进行解码。

Result: 通过模糊函数分析表明，所提波形具有优异的距离-速度分辨率和杂波抑制能力，能够有效阻止无密钥的被动对手进行侦察。

Conclusion: 该框架成功实现了集成感知与通信系统在安全性、数据速率和感知性能方面的协同增强，为未来无线通信系统提供了有效的解决方案。

Abstract: This paper presents a novel framework for enhancing the security, data rate,
and sensing performance of integrated sensing and communications (ISAC)
systems. We employ a random frequency and pulse repetition interval (PRI)
agility (RFPA) method for the waveform design, where the necessary random
sequences are governed by shared secrets. These secrets, which can be
pre-shared or generated via channel reciprocity, obfuscate critical radar
parameters like Doppler frequency and pulse start times, thereby significantly
impeding the ability to perform reconnaissance from a passive adversary without
the secret key. To further introduce enhanced data throughput, we also
introduce a hybrid information embedding scheme that integrates amplitude shift
keying (ASK), phase shift keying (PSK), index modulation (IM), and spatial
modulation (SM), for which a low-complexity sparse-matched filter receiver is
proposed for accurate decoding with practical complexity. Finally, the
excellent range-velocity resolution and clutter suppression of the proposed
waveform are analyzed via the ambiguity function (AF).

</details>


### [7] [Adversarially Robust MIMO Physical Layer Authentication for Non-Stationary Channels](https://arxiv.org/abs/2509.21171)
*Ali Khandan Boroujeni,Ghazal Bagheri,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一种针对非平稳MIMO无线信道的对抗性鲁棒物理层认证框架，该框架结合了序列贝叶斯决策、对比学习深度特征提取和生成对抗建模来模拟自适应欺骗者。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设信道平稳或观测独立，无法有效处理非平稳MIMO信道中的时空相关性、视距阻塞和动态欺骗策略等挑战。

Method: 采用序列贝叶斯决策、对比学习深度特征提取和生成对抗建模，使用2状态和3状态隐马尔可夫模型进行性能分析，并提供了移动平均在线适应的闭式递归算法。

Result: 该方法在认证性能上表现出显著鲁棒性改进，相比传统序列认证方案有更好的性能表现。

Conclusion: 所提出的AR-PLA框架能够有效应对非平稳MIMO信道环境下的认证挑战，为物理层安全提供了新的解决方案。

Abstract: We propose an adversarially robust physical layer authentication (AR-PLA)
framework tailored for non-stationary multiple-input multiple-output (MIMO)
wireless channels. The framework integrates sequential Bayesian
decision-making, deep feature extraction via contrastive learning, and
generative adversarial modeling to simulate adaptive spoofers. Unlike
conventional methods that assume stationary channels or independent
observations, our approach explicitly accounts for temporal and spatial
correlations, line-of-sight (LoS) blockages, and dynamic spoofing strategies. A
comprehensive analytical characterization of the authentication performance
using both 2-state and 3-state hidden Markov models (HMMs) with moving-average
online adaptation is also provided, with closed-form recursions for
loglikelihood ratios, detection probabilities, and steady-state approximations,
which demonstrate significant robustness improvement over classical sequential
authentication schemes.

</details>


### [8] [An enhanced statistical feature fusion approach using an improved distance evaluation algorithm and weighted K-nearest neighbor for bearing fault diagnosis](https://arxiv.org/abs/2509.21219)
*Amir Eshaghi Chaleshtori,Abdollah Aghaie*

Main category: eess.SP

TL;DR: 本文提出了一种改进的距离评估算法结合加权K近邻分类器用于轴承故障诊断，通过多域特征提取和特征选择来提高在噪声环境下的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 轴承是旋转机械中最易发生故障的部件之一，在噪声环境下从多个传感器收集数据时，需要提取和选择信息丰富的特征来准确诊断轴承故障。

Method: 1) 在时域、频域和时频域提取和整合振动统计特征；2) 使用改进的距离评估算法对特征赋权并保留最有信息量的特征；3) 使用选定的特征训练加权KNN分类器。

Result: 使用渥太华大学的轴承数据进行验证，结果表明该方法能够有效准确地识别轴承故障。

Conclusion: 提出的方法在轴承故障诊断中表现出良好的效果，特别是在噪声环境下具有较高的诊断准确性。

Abstract: Bearings are among the most failure-prone components in rotating machinery,
and their condition directly impacts overall performance. Therefore, accurately
diagnosing bearing faults is essential for ensuring system stability. However,
detecting such malfunctions in noisy environments, where data is collected from
multiple sensors, necessitates the extraction and selection of informative
features. This paper proposes an improved distance evaluation algorithm
combined with a weighted K-nearest neighbor (KNN) classifier for bearing fault
diagnosis. The process begins with extracting and integrating statistical
features of vibration across the time, frequency, and time-frequency domains.
Next, the improved distance evaluation algorithm assigns weights to the
extracted features, retaining only the most informative ones by eliminating
insensitive features. Finally, the selected features are used to train the
weighted KNN classifier. To validate the proposed method, we employ bearing
data from the University of Ottawa. The results demonstrate the effectiveness
of our approach in accurately identifying bearing faults.

</details>


### [9] [Vision-Intelligence-Enabled Beam Tracking for Cross-Interface Water-Air Optical Wireless Communications](https://arxiv.org/abs/2509.21290)
*Tianqi Mao,Jiayue Liu,Weijie Liu,Dezhi Zheng,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 本文提出了一种基于视觉的波束跟踪算法，用于解决水-空光无线通信中因波动海面导致的信号失准问题，该算法结合CNN和Bi-LSTM网络，并引入注意力机制来提取关键时空特征。


<details>
  <summary>Details</summary>
Motivation: 随着水下监视和矿产勘探等海洋应用的发展，需要实时无线回传大量观测数据。传统窄带声学方法难以满足需求，而光无线通信虽具潜力，但在穿透波动海面时会出现严重的波束失准问题。

Method: 建立了水-空光无线传输的数学信道模型，提出基于视觉的波束跟踪算法，该算法整合卷积神经网络和双向长短期记忆网络，并加入注意力机制来从视觉数据中提取关键时空特征。

Result: 数值仿真结果表明，所提算法在保持接收信号强度和抑制视觉噪声方面优于传统方法，证明了其在水-空光无线通信系统恶劣条件下的鲁棒性。

Conclusion: 该研究为解决水-空光无线通信中的动态对准问题提供了有效解决方案，展示了AI方法在复杂海洋环境中的适用性。

Abstract: The escalating development of oceanic applications like underwater
surveillance and mineral exploration, is motivating real-time wireless backhaul
of the considerable observation data. Such prospects can be hardly realized by
the narrowband acoustic approach. Alternatively, optical wireless communication
(OWC) has emerged as a promising solution for maritime and underwater
applications due to its great potential for broadband underwater transmission.
However, the implementations of water-air OWC can be rather challenging,
especially when penetrating the fluctuating interface, where the direction of
refracted signals changes dynamically, causing severe beam misalignment with
airborne stations. This has necessitated real-time transceiver alignment
adaptable to the sophisticated oceanic environment, which has yet to be
addressed. Against this background, this paper establishes a mathematical
channel model for water-air optical wireless transmission across the
fluctuating sea surface. Based on the model, we propose a vision-based beam
tracking algorithm that leverages artificial intelligence (AI) methods for
dynamic channel prediction. The proposed algorithm integrates a convolutional
neural network (CNN) with bi-directional long short-term memory (Bi-LSTM),
which further incorporates the attention mechanism to effectively extract
critical spatio-temporal features from the vision data. The numerical
simulation results show that the proposed algorithm can outperform its
classical counterparts in maintaining receiving signal strength and supressing
the vision noises, which demonstrates its robustness against the the harsh
conditions of water-air OWC systems.

</details>


### [10] [Efficient Digital Methods to Quantify Sensor Output Uncertainty](https://arxiv.org/abs/2509.21311)
*Orestis Kaparounakis,Phillip Stanley-Marbell*

Main category: eess.SP

TL;DR: 本文研究传感器校准参数量化引起的不确定性传播对测量精度的影响，以热电堆传感器为例，展示了校准相关量的认知不确定性可导致高达5.3°C的绝对误差，并在嵌入式平台上实现了实时不确定性跟踪。


<details>
  <summary>Details</summary>
Motivation: 准确表征传感器输出不确定性对于可靠的数据解释至关重要，需要研究由于传感器组件有限精度信息导致的测量不确定性对整体传感器测量精度的影响。

Method: 使用热电堆传感器作为示例传感器类别，分析传感器校准和转换方程如何传播由校准参数量化引起的不确定性到最终补偿的传感器输出，并在两个商用不确定性跟踪硬件平台上进行原型验证。

Result: 实验结果显示校准相关量的认知不确定性导致传感器输出绝对误差高达5.3°C（相对误差高达25.7%），在边缘检测应用中可将Canny算子的误报边缘降至零，同时在嵌入式平台上实现16.7mW和147.15mW的低功耗，相比蒙特卡洛计算分别获得42.9倍和94.4倍的加速。

Conclusion: 该方法在实际嵌入式传感器系统中具有可行性，为实时不确定性跟踪应用铺平了道路，能够显著提高传感器测量的可靠性和准确性。

Abstract: Accurate characterization of sensor output uncertainty is important for
reliable data interpretation in many applications. Here, we investigate the
impact of transducer-level measurement uncertainty on overall sensor
measurement accuracy due to limited-precision information about sensor
components. We explain our method using thermopile-based sensors as an example
class of sensors. We show how sensor calibration and conversion equations,
which are an essential part of all sensing systems, propagate uncertainties
resulting from the quantization of calibration parameters, to the final,
compensated sensor output. The experimental results show that the epistemic
uncertainty of calibration-related quantities leads to absolute error in the
sensor output as high as 5.3 {\deg}C (and relative error as high as 25.7%) for
one commonly-used thermopile sensor. In one instance of using the epistemic
uncertainty information in edge detection, we show reduction of false-positives
edges to zero for the conventional Canny operator, while maintaining accuracy.
We show these ideas are practical and possible on actual embedded sensor
systems by prototyping them on two commercially-available uncertainty tracking
hardware platforms, one with average power dissipation 16.7 mW and 42.9x
speedup compared to the equal-confidence Monte Carlo computation (the status
quo), and the other with average power dissipation 147.15 mW and 94.4x speedup,
paving the way for use in real time.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling](https://arxiv.org/abs/2509.20396)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: eess.AS

TL;DR: 提出了一种基于音素级不确定性的数据高效个性化方法，通过蒙特卡洛Dropout识别困难音素并进行针对性过采样，显著提升了非标准语音（如脑瘫患者）的ASR识别准确率。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在处理非标准语音（如脑瘫患者或结构异常导致的语音障碍）时表现不佳，主要由于高声学变异性和训练数据稀缺。

Method: 使用蒙特卡洛Dropout量化音素级不确定性，识别模型最困难的音素，并采用针对性过采样策略进行微调。

Result: 在英语和德语数据集上验证，模型不确定性评估与临床专家对语音难度的评估高度相关，显著提高了ASR准确率。

Conclusion: 该方法提供了一个实用的个性化ASR框架，首次成功将模型不确定性与专家临床评估对齐，为包容性语音识别提供了有效解决方案。

Abstract: Automatic speech recognition (ASR) systems struggle with non-normative speech
from individuals with impairments caused by conditions like cerebral palsy or
structural anomalies. The high acoustic variability and scarcity of training
data severely degrade model performance. This work introduces a data-efficient
personalization method that quantifies phoneme-level uncertainty to guide
fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model
finds most difficult and use these estimates for a targeted oversampling
strategy. We validate our method on English and German datasets. Crucially, we
demonstrate that our model-derived uncertainty strongly correlates with
phonemes identified as challenging in an expert clinical logopedic report,
marking, to our knowledge, the first work to successfully align model
uncertainty with expert assessment of speech difficulty. Our results show that
this clinically-validated, uncertainty-guided sampling significantly improves
ASR accuracy, delivering a practical framework for personalized and inclusive
ASR.

</details>


### [12] [Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition](https://arxiv.org/abs/2509.20397)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Shih-Chii Liu,Yingqiang Gao*

Main category: eess.AS

TL;DR: 本文提出了一种基于贝叶斯低秩适应的数据高效微调方法，用于改善语音障碍患者的自动语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 由于先天性障碍或后天性脑损伤导致的语音障碍给ASR系统带来重大挑战，现有模型如Whisper在非标准语音上表现不佳，且数据收集和标注困难。

Method: 采用贝叶斯低秩适应方法进行数据高效微调，在英语UA-Speech数据集和新收集的德语BF-Sprache数据集上进行验证。

Result: 该方法显著提高了对障碍语音的ASR准确率，同时保持了数据和标注效率。

Conclusion: 该方法为构建包容性ASR系统提供了实用路径，能够有效处理低资源环境下的语音障碍识别问题。

Abstract: Speech impairments resulting from congenital disorders, such as cerebral
palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due
to stroke, traumatic accidents, or tumors, present major challenges to
automatic speech recognition (ASR) systems. Despite recent advancements,
state-of-the-art ASR models like Whisper still struggle with non-normative
speech due to limited training data availability and high acoustic variability.
Moreover, collecting and annotating non-normative speech is burdensome:
speaking is effortful for many affected individuals, while laborious annotation
often requires caregivers familiar with the speaker. This work introduces a
novel ASR personalization method based on Bayesian Low-rank Adaptation for
data-efficient fine-tuning. We validate our method on the English UA-Speech
dataset and a newly collected German speech dataset, BF-Sprache, from a child
with structural speech impairment. The dataset and approach are designed to
reflect the challenges of low-resource settings that include individuals with
speech impairments. Our method significantly improves ASR accuracy for impaired
speech while maintaining data and annotation efficiency, offering a practical
path toward inclusive ASR.

</details>


### [13] [Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction](https://arxiv.org/abs/2509.20410)
*Weijie Wu,Wenhao Guan,Kaidi Wang,Peijie Chen,Zhuanling Zha,Junbo Li,Jun Fang,Lin Li,Qingyang Hong*

Main category: eess.AS

TL;DR: Phoenix-VAD是一个基于大语言模型的流式语义端点检测模型，通过LLM的语义理解能力和滑动窗口训练策略实现可靠的语义端点检测，支持流式推理。


<details>
  <summary>Details</summary>
Motivation: 现有的语音对话模型缺乏即插即用的全双工预测模块来进行语义端点检测，阻碍了无缝的音频交互。

Method: 利用LLM的语义理解能力，采用滑动窗口训练策略，实现流式语义端点检测。

Result: 在语义完整和不完整的语音场景下，Phoenix-VAD都表现出优异且有竞争力的性能。

Conclusion: 该设计使全双工预测模块能够独立于对话模型进行优化，为下一代人机交互提供更可靠和灵活的支持。

Abstract: Spoken dialogue models have significantly advanced intelligent
human\textendash computer interaction, yet they lack a plug\textendash
and\textendash play full\textendash duplex prediction module for semantic
endpoint detection, hindering seamless audio interactions. In this paper, we
introduce Phoenix\textendashVAD, an LLM\textendash based model that enables
streaming semantic endpoint detection. Specifically, Phoenix\textendash VAD
leverages the semantic comprehension capability of the LLM and a sliding window
training strategy to achieve reliable semantic endpoint detection while
supporting streaming inference. Experiments on both semantically complete and
incomplete speech scenarios indicate that Phoenix\textendash VAD achieves
excellent and competitive performance. Furthermore, this design enables the
full\textendash duplex prediction module to be optimized independently of the
dialogue model, providing more reliable and flexible support for
next\textendash generation human\textendash computer interaction.

</details>


### [14] [Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens](https://arxiv.org/abs/2509.20485)
*Ismail Rasim Ulgen,Zongyang Du,Junchen Lu,Philipp Koehn,Berrak Sisman*

Main category: eess.AS

TL;DR: TTScore是一个基于条件预测离散语音token的定向无参考评估框架，包含TTScore-int评估可懂度和TTScore-pro评估韵律，相比现有指标与人类感知有更强的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成评估指标在可懂度和韵律评估方面范围有限且与人类感知相关性弱，WER仅提供粗粒度的文本可懂度测量，F0-RMSE等音高指标对韵律的评估过于狭窄且依赖参考。

Method: 使用两个基于输入文本的条件序列到序列预测器：TTScore-int通过内容token测量可懂度，TTScore-pro通过韵律token评估韵律。对每个合成语音，预测器计算相应token序列的似然度。

Result: 在SOMOS、VoiceMOS和TTSArena基准测试中，TTScore-int和TTScore-pro提供了可靠的特异性评估，在整体质量的人类判断相关性上优于现有可懂度和韵律指标。

Conclusion: TTScore框架能够提供可解释的评分，捕捉与预期语言内容和韵律结构的对齐，为语音合成系统评估提供了更有效的解决方案。

Abstract: Objective evaluation of synthesized speech is critical for advancing speech
generation systems, yet existing metrics for intelligibility and prosody remain
limited in scope and weakly correlated with human perception. Word Error Rate
(WER) provides only a coarse text-based measure of intelligibility, while
F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent
view of prosody. To address these limitations, we propose TTScore, a targeted
and reference-free evaluation framework based on conditional prediction of
discrete speech tokens. TTScore employs two sequence-to-sequence predictors
conditioned on input text: TTScore-int, which measures intelligibility through
content tokens, and TTScore-pro, which evaluates prosody through prosody
tokens. For each synthesized utterance, the predictors compute the likelihood
of the corresponding token sequences, yielding interpretable scores that
capture alignment with intended linguistic content and prosodic structure.
Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that
TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and
achieve stronger correlations with human judgments of overall quality than
existing intelligibility and prosody-focused metrics.

</details>


### [15] [Real-Time System for Audio-Visual Target Speech Enhancement](https://arxiv.org/abs/2509.20741)
*T. Aleksandra Ma,Sile Yin,Li-Chia Yang,Shuo Zhang*

Main category: eess.AS

TL;DR: RAVEN是一个在CPU上运行的实时视听语音增强系统，利用唇部运动信息提升语音增强效果，填补了CPU硬件上实时交互式视听语音增强系统的空白。


<details>
  <summary>Details</summary>
Motivation: 传统单通道音频语音增强主要从环境噪声中提取干净语音，但缺乏视觉线索的辅助。虽然近期研究开始探索使用视觉线索（如唇部运动）来提高鲁棒性，特别是在存在干扰说话者的情况下，但尚未有在CPU硬件上运行的实时交互式视听语音增强系统。

Method: RAVEN使用预训练的视听语音识别模型中的视觉嵌入来编码唇部运动信息，系统能够泛化处理环境噪声、干扰说话者、瞬态声音甚至歌声。

Result: 系统实现了在CPU硬件上的实时运行，参与者可以通过麦克风和网络摄像头设置体验实时视听目标语音增强，并通过耳机播放干净语音。

Conclusion: RAVEN成功填补了CPU硬件上实时交互式视听语音增强系统的技术空白，展示了利用视觉信息显著提升语音增强性能的可行性。

Abstract: We present a live demonstration for RAVEN, a real-time audio-visual speech
enhancement system designed to run entirely on a CPU. In single-channel,
audio-only settings, speech enhancement is traditionally approached as the task
of extracting clean speech from environmental noise. More recent work has
explored the use of visual cues, such as lip movements, to improve robustness,
particularly in the presence of interfering speakers. However, to our
knowledge, no prior work has demonstrated an interactive system for real-time
audio-visual speech enhancement operating on CPU hardware. RAVEN fills this gap
by using pretrained visual embeddings from an audio-visual speech recognition
model to encode lip movement information. The system generalizes across
environmental noise, interfering speakers, transient sounds, and even singing
voices. In this demonstration, attendees will be able to experience live
audio-visual target speech enhancement using a microphone and webcam setup,
with clean speech playback through headphones.

</details>


### [16] [SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS](https://arxiv.org/abs/2509.20802)
*Tan Dat Nguyen,Jaehun Kim,Ji-Hoon Kim,Shukjae Choi,Youshin Lim,Joon Son Chung*

Main category: eess.AS

TL;DR: SPADE框架通过结构化剪枝和自适应蒸馏技术，在保持语音质量的同时将LLM-TTS模型的Transformer层数减半，显著降低内存占用和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-TTS系统虽然控制性和零样本泛化能力强，但参数量大、延迟高，限制了实际部署应用。

Method: 结合基于词错误率的层重要性剪枝和多层次知识蒸馏，移除非必要的Transformer层并恢复自回归一致性。

Result: 在零样本基准测试中，SPADE保持接近原始模型的感知质量，VRAM使用减少20%，实时因子提升1.7倍，仅需不到5%的原始训练数据。

Conclusion: 紧凑型LLM-TTS模型能够在保持自然度和说话人相似度的同时实现实用的实时语音生成。

Abstract: The goal of this paper is to introduce SPADE, a framework for Structured
Pruning and Adaptive Distillation for Efficient Large Language Model-based
text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability
and zero-shot generalization, but their large parameter counts and high latency
limit real-world deployment. SPADE addresses this by combining (i) a pruning
step guided by a word-error-rate-based layer importance index to remove
non-essential Transformer layers, with (ii) multi-level knowledge distillation
to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves
near-parity perceptual quality while halving Transformer depth, reducing VRAM
usage by up to 20%, and achieving up to 1.7x faster real-time factor with less
than 5% of the original training data. These results show that compact LLM-TTS
models can maintain naturalness and speaker similarity while enabling practical
real-time speech generation. Audio samples are available at
https://mm.kaist.ac.kr/projects/SPADE/.

</details>


### [17] [PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice Pickup in Hearables](https://arxiv.org/abs/2509.20875)
*Mattes Ohlenbusch,Mikolaj Kegler,Marko Stamenovic*

Main category: eess.AS

TL;DR: 本文比较了两种解决单通道语音增强中目标说话人歧义问题的策略：个性化语音增强（PSE）和使用辅助传感器的语音增强（AS-SE），并展示了将两者结合（PAS-SE）的互补性能优势。


<details>
  <summary>Details</summary>
Motivation: 在耳机语音拾取中，单通道方法难以区分目标说话人和干扰说话人，需要解决这种歧义问题。

Method: 比较PSE（使用注册语音表示目标）和AS-SE（使用耳内麦克风作为额外输入）两种策略，提出训练时增强以促进AS-SE系统的跨数据集泛化，并研究PSE和AS-SE的结合（PAS-SE）。

Result: PSE和AS-SE结合提供互补性能优势，特别是当注册语音通过耳内麦克风录制时；使用噪声耳内注册的PAS-SE仍保持对AS-SE系统的性能优势。

Conclusion: PSE和AS-SE策略的结合能够有效解决单通道语音增强中的目标歧义问题，具有实际应用价值。

Abstract: Speech enhancement for voice pickup in hearables aims to improve the user's
voice by suppressing noise and interfering talkers, while maintaining own-voice
quality. For single-channel methods, it is particularly challenging to
distinguish the target from interfering talkers without additional context. In
this paper, we compare two strategies to resolve this ambiguity: personalized
speech enhancement (PSE), which uses enrollment utterances to represent the
target, and auxiliary-sensor speech enhancement (AS-SE), which uses in-ear
microphones as additional input. We evaluate the strategies on two public
datasets, employing different auxiliary sensor arrays, to investigate their
cross-dataset generalization. We propose training-time augmentations to
facilitate cross-dataset generalization of AS-SE systems. We also show that
combining PSE and AS-SE (PAS-SE) provides complementary performance benefits,
especially when enrollment speech is recorded with the in-ear microphone. We
further demonstrate that PAS-SE personalized with noisy in-ear enrollments
maintains performance benefits over the AS-SE system.

</details>


### [18] [TF-Restormer: Complex Spectral Prediction for Speech Restoration](https://arxiv.org/abs/2509.21003)
*Ui-Hyeop Shin,Jaehyun Ko,Woocheol Jeong,Hyuing-Min Park*

Main category: eess.AS

TL;DR: TF-Restormer是一种用于语音恢复的编码器-解码器架构，支持任意输入输出采样率，无需冗余重采样，在信号保真度和感知质量方面均优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中语音恢复的挑战，包括多种失真类型和低采样率问题，现有系统在信号保真度、流式处理能力和固定采样率限制方面存在不足。

Method: 采用时频双路径编码器分析输入带宽，通过带频率扩展查询的轻量解码器重建缺失高频带；引入采样频率无关的STFT判别器支持对抗训练；使用因果时间模块支持流式处理；通过频谱归纳偏置提高鲁棒性；提出缩放对数频谱损失优化训练。

Result: TF-Restormer在多个采样率下一致优于现有系统，在信号保真度和感知质量方面取得平衡增益，流式模式在实时应用中保持竞争力。

Conclusion: TF-Restormer作为一种跨采样率的单一模型，实现了高效通用的语音恢复，解决了现有系统的关键限制，为实时语音恢复应用提供了有效解决方案。

Abstract: Speech restoration in real-world conditions is challenging due to compounded
distortions such as clipping, band-pass filtering, digital artifacts, noise,
and reverberation, and low sampling rates. Existing systems, including
vocoder-based approaches, often sacrifice signal fidelity, while diffusion
models remain impractical for streaming. Moreover, most assume a fixed target
sampling rate, requiring external resampling that leads to redundant
computations. We present TF-Restormer, an encoder-decoder architecture that
concentrates analysis on input-bandwidth with a time-frequency dual-path
encoder and reconstructs missing high-frequency bands through a light decoder
with frequency extension queries. It enables efficient and universal
restoration across arbitrary input-output rates without redundant resampling.
To support adversarial training across diverse rates, we introduce a shared
sampling-frequency-independent (SFI) STFT discriminator. TF-Restormer further
supports streaming with a causal time module, and improves robustness under
extreme degradations by injecting spectral inductive bias into the frequency
module. Finally, we propose a scaled log-spectral loss that stabilizes
optimization under severe conditions while emphasizing well-predicted spectral
details. As a single model across sampling rates, TF-Restormer consistently
outperforms prior systems, achieving balanced gains in signal fidelity and
perceptual quality, while its streaming mode maintains competitive
effectiveness for real-time application. Code and demos are available at
https://tf-restormer.github.io/demo.

</details>


### [19] [Measuring Audio's Impact on Correctness: Audio-Contribution-Aware Post-Training of Large Audio Language Models](https://arxiv.org/abs/2509.21060)
*Haolin He,Xingjian Du,Renhe Sun,Zheqi Dai,Yujia Xiao,Mingru Yang,Jiayi Zhou,Xiquan Li,Zhengxi Liu,Zining Liang,Chunyat Wu,Qianhua He,Tan Lee,Xie Chen,Weilong Zheng,Weiqiang Wang,Mark Plumbley,Jian Liu,Qiuqiang Kong*

Main category: eess.AS

TL;DR: 本文提出了AudioMCQ数据集和两种有效的后训练范式，解决了大型音频语言模型在多阶段训练中数据分配不足的问题，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型的后训练方法中，多阶段训练（如SFT后接RL）效果不理想，且缺乏对数据在不同训练阶段分配的深入研究，同时缺少大规模高质量数据集。

Method: 首先创建了包含57.1万个样本的AudioMCQ数据集，提出音频贡献度过滤方法将数据分为弱音频贡献和强音频贡献子集，然后开发了Weak-to-Strong和Mixed-to-Strong两种后训练范式。

Result: 在DCASE 2025音频问答挑战赛中取得第一名，在MMAU-test-mini、MMAU、MMAR和MMSU基准测试中分别达到78.2%、75.6%、67.1%和70.7%的准确率，创造了新的最先进性能。

Conclusion: 通过AudioMCQ数据集和提出的训练策略，有效解决了LALMs中的零音频贡献现象，显著提升了模型性能，为音频语言模型的多阶段训练提供了有效解决方案。

Abstract: Large Audio Language Models (LALMs) represent an important frontier in
multimodal AI, addressing diverse audio tasks. Recently, post-training of LALMs
has received increasing attention due to significant performance improvements
over foundation models. While single-stage post-training such as reinforcement
learning (RL) has demonstrated promising results, multi-stage approaches such
as supervised fine-tuning (SFT) followed by RL remain suboptimal. The
allocation of data across multiple training stages to maximize LALM
capabilities has not been fully explored, and large-scale, high-quality
datasets for such research are also lacking. To address these problems, we
firstly present AudioMCQ, a comprehensive audio multiple-choice question
dataset comprising 571k samples with two kinds of chain-of-thought annotations.
Secondly, we investigate the prevalent zero audio-contribution phenomenon in
LALMs, where models derive correct answers solely from textual information
without processing audio content. We propose Audio-Contribution Filtering to
partition data into weak and strong audio-contribution subsets. Based on these
insights, we develop two effective post-training paradigms: Weak-to-Strong (SFT
on weak audio-contribution data followed by RL on strong audio-contribution
data) and Mixed-to-Strong (SFT on mixed audio-contribution data followed by RL
on strong audio-contribution data). We achieve first place in the DCASE 2025
Audio-Question-Answering challenge by using AudioMCQ. Additionally, leveraging
our dataset with different training strategies, we achieve 78.2\% on
MMAU-test-mini, 75.6\% on MMAU, 67.1\% on MMAR, and 70.7\% on MMSU,
establishing new state-of-the-art performance across these benchmarks.

</details>


### [20] [Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](https://arxiv.org/abs/2509.21087)
*Rostislav Makarov,Lea Schönherr,Timo Gerkmann*

Main category: eess.AS

TL;DR: 论文展示了先进的语音增强模型容易受到对抗性攻击，攻击者可以注入经过精心设计且心理声学掩蔽的噪声，使增强后的语音输出传达完全不同的语义含义。同时发现扩散模型具有固有的对抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着语音增强模型表达能力越来越强，这种表达能力也引入了安全漏洞。论文旨在研究先进语音增强模型是否容易受到对抗性攻击的影响。

Method: 通过精心设计并利用心理声学掩蔽的对抗性噪声注入到输入信号中，实验验证了当代预测性语音增强模型可以被操纵。同时评估了扩散模型对此类攻击的鲁棒性。

Result: 实验证实当代预测性语音增强模型确实可以被对抗性攻击操纵，增强后的语音输出会传达与原始输入完全不同的语义含义。扩散模型由于其随机采样器的设计，展现出固有的对抗攻击鲁棒性。

Conclusion: 语音增强模型的表达能力带来了安全脆弱性，需要关注对抗性攻击的防御。扩散模型的设计特性使其在这方面具有天然优势，为未来安全语音增强系统的发展提供了方向。

Abstract: Machine learning approaches for speech enhancement are becoming increasingly
expressive, enabling ever more powerful modifications of input signals. In this
paper, we demonstrate that this expressiveness introduces a vulnerability:
advanced speech enhancement models can be susceptible to adversarial attacks.
Specifically, we show that adversarial noise, carefully crafted and
psychoacoustically masked by the original input, can be injected such that the
enhanced speech output conveys an entirely different semantic meaning. We
experimentally verify that contemporary predictive speech enhancement models
can indeed be manipulated in this way. Furthermore, we highlight that diffusion
models with stochastic samplers exhibit inherent robustness to such adversarial
attacks by design.

</details>


### [21] [Hybrid Real- And Complex-Valued Neural Network Concept For Low-Complexity Phase-Aware Speech Enhancement](https://arxiv.org/abs/2509.21185)
*Luan Vinícius Fiorio,Alex Young,Ronald M. Aarts*

Main category: eess.AS

TL;DR: 提出混合实值和复值神经网络用于语音增强，相比纯实值或纯复值模型具有更好的性能和更低的计算复杂度


<details>
  <summary>Details</summary>
Motivation: 现有的实值或复值神经网络模型要么效率低下，要么复杂度过高，需要一种更优的解决方案

Method: 设计了一种将实值网络扩展为混合网络的简单方法，在卷积和卷积-循环架构上实现了实值、复值和混合版本的对比

Result: 混合网络在参数数量相同的情况下始终优于其他版本，且乘积累加运算复杂度显著降低

Conclusion: 混合实值和复值神经网络是语音增强任务中更有效的解决方案

Abstract: In this paper, we propose hybrid real- and complex-valued neural networks for
speech enhancement. Real- or complex-valued models are either inefficient or
present high complexity. We devise a straightforward design method for
extending a real-valued network into its hybrid counterpart. Based on speech
intelligibility and quality metrics, we compare the real, complex, and hybrid
versions of a convolutional and a convolutional-recurrent architecture. The
hybrid network consistently outperforms its counterparts with the same number
of parameters. Additionally, the hybrid models' complexity in terms of
multiply-accumulate operations is substantially lower than that of their
counterparts.

</details>


### [22] [MeanSE: Efficient Generative Speech Enhancement with Mean Flows](https://arxiv.org/abs/2509.21214)
*Jiahe Wang,Hongyu Wang,Wei Wang,Lei Yang,Chenda Li,Wangyou Zhang,Lufen Tan,Yanmin Qian*

Main category: eess.AS

TL;DR: 提出MeanSE模型，使用平均流方法实现高效的单步语音增强，显著优于传统流匹配方法


<details>
  <summary>Details</summary>
Motivation: 传统基于流的语音增强模型需要多次函数评估才能达到稳定性能，计算负载高且单步性能差

Method: 使用平均流方法建模平均速度场，实现高质量的单步函数评估语音增强

Result: MeanSE在单步评估下显著优于流匹配基线，展现出极好的域外泛化能力

Conclusion: MeanSE为语音增强提供了一种高效的单步生成模型解决方案

Abstract: Speech enhancement (SE) improves degraded speech's quality, with generative
models like flow matching gaining attention for their outstanding perceptual
quality. However, the flow-based model requires multiple numbers of function
evaluations (NFEs) to achieve stable and satisfactory performance, leading to
high computational load and poor 1-NFE performance. In this paper, we propose
MeanSE, an efficient generative speech enhancement model using mean flows,
which models the average velocity field to achieve high-quality 1-NFE
enhancement. Experimental results demonstrate that our proposed MeanSE
significantly outperforms the flow matching baseline with a single NFE,
exhibiting extremely better out-of-domain generalization capabilities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [23] [QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection](https://arxiv.org/abs/2509.20679)
*Duc-Tuan Truong,Tianchi Liu,Ruijie Tao,Junjie Li,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: QAMO提出了一种质量感知的多中心单类学习方法，通过引入多个质量感知中心来改进语音深度伪造检测，相比传统单中心方法能更好地建模真实语音的类内变异性。


<details>
  <summary>Details</summary>
Motivation: 传统单中心单类学习方法过度简化了真实语音表示，忽略了语音质量等有用线索。语音质量反映了语音的自然度，可以通过现有语音质量评估模型轻松获取。

Method: QAMO扩展了传统单类学习，引入多个质量感知中心，每个中心优化为代表不同的语音质量子空间。支持多中心集成评分策略，改进决策阈值设置，减少推理时对质量标签的需求。

Result: 使用两个中心分别代表高质量和低质量语音，QAMO在In-the-Wild数据集上实现了5.09%的等错误率，优于之前的单类和质量感知系统。

Conclusion: QAMO通过质量感知的多中心方法有效提升了语音深度伪造检测性能，证明了考虑语音质量多样性的重要性。

Abstract: Recent work shows that one-class learning can detect unseen deepfake attacks
by modeling a compact distribution of bona fide speech around a single
centroid. However, the single-centroid assumption can oversimplify the bona
fide speech representation and overlook useful cues, such as speech quality,
which reflects the naturalness of the speech. Speech quality can be easily
obtained using existing speech quality assessment models that estimate it
through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware
Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends
conventional one-class learning by introducing multiple quality-aware
centroids. In QAMO, each centroid is optimized to represent a distinct speech
quality subspaces, enabling better modeling of intra-class variability in bona
fide speech. In addition, QAMO supports a multi-centroid ensemble scoring
strategy, which improves decision thresholding and reduces the need for quality
labels during inference. With two centroids to represent high- and low-quality
speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild
dataset, outperforming previous one-class and quality-aware systems.

</details>


### [24] [Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection](https://arxiv.org/abs/2509.20682)
*Duc-Tuan Truong,Tianchi Liu,Junjie Li,Ruijie Tao,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: 本文提出了一种双路径数据增强（DPDA）训练框架，通过梯度对齐解决语音深度伪造检测中数据增强导致的梯度冲突问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 在语音深度伪造检测中，数据增强常用于提升模型泛化能力，但原始输入和增强输入的反向传播梯度可能存在冲突，导致参数更新不一致，阻碍模型收敛并降低数据增强的效果。

Method: 设计了双路径数据增强训练框架，每个训练语句通过两个输入路径处理：原始语音路径和增强版本路径。通过比较和对齐这两个路径的反向传播梯度方向来减少优化冲突。

Result: 分析显示使用RawBoost增强时约25%的训练迭代存在梯度冲突。通过梯度对齐解决冲突后，方法减少了训练轮次，在In-the-Wild数据集上相比基线实现了18.69%的相对等错误率降低。

Conclusion: 梯度对齐能有效解决数据增强中的优化冲突问题，加速模型收敛并显著提升语音深度伪造检测性能。

Abstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used
to improve model generalization across varied speech conditions and spoofing
attacks. However, during training, the backpropagated gradients from original
and augmented inputs may misalign, which can result in conflicting parameter
updates. These conflicts could hinder convergence and push the model toward
suboptimal solutions, thereby reducing the benefits of DA. To investigate and
address this issue, we design a dual-path data-augmented (DPDA) training
framework with gradient alignment for SDD. In our framework, each training
utterance is processed through two input paths: one using the original speech
and the other with its augmented version. This design allows us to compare and
align their backpropagated gradient directions to reduce optimization
conflicts. Our analysis shows that approximately 25% of training iterations
exhibit gradient conflicts between the original inputs and their augmented
counterparts when using RawBoost augmentation. By resolving these conflicts
with gradient alignment, our method accelerates convergence by reducing the
number of training epochs and achieves up to an 18.69% relative reduction in
Equal Error Rate on the In-the-Wild dataset compared to the baseline.

</details>


### [25] [AIBA: Attention-based Instrument Band Alignment for Text-to-Audio Diffusion](https://arxiv.org/abs/2509.20891)
*Junyoung Koh,Soo Yong Kim,Gyu Hyeong Choi,Yongwon Choi*

Main category: cs.SD

TL;DR: AIBA是一种轻量级、无需训练的管道，用于量化文本到音频扩散模型在时频平面上的注意力分布。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需修改模型权重的方法来理解和评估文本到音频扩散模型在生成过程中对不同频段的注意力分配模式。

Method: AIBA通过（i）在推理时钩取交叉注意力记录注意力概率；（ii）将其投影到固定大小的梅尔网格上；（iii）使用可解释的度量标准（时频IoU/AP、频率轮廓相关性和指向游戏）来评估与乐器频段真实值的对齐程度。

Result: 在Slakh2100数据集上使用AudioLDM2骨干网络，AIBA揭示了乐器依赖的一致趋势（如低音乐器偏好低频段），并实现了高精度和中等召回率。

Conclusion: AIBA提供了一种有效的方法来分析文本到音频扩散模型的注意力机制，揭示了模型在音频生成过程中的频段偏好模式。

Abstract: We present AIBA (Attention-In-Band Alignment), a lightweight, training-free
pipeline to quantify where text-to-audio diffusion models attend on the
time-frequency (T-F) plane. AIBA (i) hooks cross-attention at inference to
record attention probabilities without modifying weights; (ii) projects them to
fixed-size mel grids that are directly comparable to audio energy; and (iii)
scores agreement with instrument-band ground truth via interpretable metrics
(T-F IoU/AP, frequency-profile correlation, and a pointing game). On Slakh2100
with an AudioLDM2 backbone, AIBA reveals consistent instrument-dependent trends
(e.g., bass favoring low bands) and achieves high precision with moderate
recall.

</details>


### [26] [SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement](https://arxiv.org/abs/2509.20969)
*Shaohan Jiang,Junan Zhang,Yunjia Zhang,Jing Yang,Fan Fan,Zhizheng Wu*

Main category: cs.SD

TL;DR: 本文提出了SingVERSE，这是首个真实世界的歌唱声音增强基准测试，解决了该领域缺乏现实评估数据的问题，并揭示了感知质量与可懂度之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 歌唱声音增强技术的发展受到缺乏现实评估数据的限制，现有基准测试无法充分反映真实应用场景的需求。

Method: 构建了SingVERSE基准测试，涵盖多样化的声学场景，并提供配对的录音室质量干净参考。利用该基准对最先进模型进行全面评估。

Result: 发现模型在感知质量和可懂度之间存在一致性的权衡关系；使用领域内歌唱数据进行训练能显著提升增强性能，且不会降低语音处理能力。

Conclusion: SingVERSE为社区提供了基础性基准测试和关键见解，为这个探索不足的领域指明了简单而有效的前进路径。

Abstract: This paper presents a benchmark for singing voice enhancement. The
development of singing voice enhancement is limited by the lack of realistic
evaluation data. To address this gap, this paper introduces SingVERSE, the
first real-world benchmark for singing voice enhancement, covering diverse
acoustic scenarios and providing paired, studio-quality clean references.
Leveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art
models and uncover a consistent trade-off between perceptual quality and
intelligibility. Finally, we show that training on in-domain singing data
substantially improves enhancement performance without degrading speech
capabilities, establishing a simple yet effective path forward. This work
offers the community a foundational benchmark together with critical insights
to guide future advances in this underexplored domain. Demopage:
https://singverse.github.io

</details>


### [27] [i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents](https://arxiv.org/abs/2509.20971)
*Anupam Purwar,Aditya Choudhary*

Main category: cs.SD

TL;DR: 本文研究了低延迟端到端语音到语音通信模型的优化，重点关注自动语音识别、文本到语音和对话管理组件，发现TTS组件对实时因子影响最大，并通过优化RVQ迭代次数和码本数量来提升性能。


<details>
  <summary>Details</summary>
Motivation: 优化实时对话应用中语音到语音系统的处理时间，同时保持高质量的交互体验，识别V-2-V系统中的关键优化杠杆。

Method: 分析V-2-V系统的关键组件（ASR、TTS、对话管理），实验基于CSM1b架构的V-2-V系统，探索TTS解码器中RVQ迭代的优化，减少Mimi中使用的码本数量。

Result: TTS组件对实时因子影响最大，优化RVQ迭代次数和码本数量可以显著提升系统性能，但会牺牲一定的语音生成质量。

Conclusion: 对于基于CSM的V-2-V实现，最重要的优化是通过减少RVQ迭代次数和Mimi中使用的码本数量来实现的，这为实时对话应用提供了有效的优化路径。

Abstract: We experiment with a low-latency, end-to-end voice-to-voice communication
model to optimize it for real-time conversational applications. By analyzing
components essential to voice to voice (V-2-V) system viz. automatic speech
recognition (ASR), text-to-speech (TTS), and dialog management, our work
analyzes how to reduce processing time while maintaining high-quality
interactions to identify the levers for optimizing V-2-V system. Our work
identifies that TTS component which generates life-like voice, full of emotions
including natural pauses and exclamations has highest impact on Real time
factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the
capability to understand tone as well as context of conversation by ingesting
both audio and text of prior exchanges to generate contextually accurate
speech. We explored optimization of Residual Vector Quantization (RVQ)
iterations by the TTS decoder which come at a cost of decrease in the quality
of voice generated. Our experimental evaluations also demonstrate that for
V-2-V implementations based on CSM most important optimizations can be brought
by reducing the number of RVQ Iterations along with the codebooks used in Mimi.

</details>


### [28] [SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization](https://arxiv.org/abs/2509.21033)
*Jiehui Luo,Yuguo Yin,Yuxin Xie,Jinghan Ru,Xianwei Zhuang,Minghua He,Aofan Liu,Zihan Xiong,Dongchao Yang*

Main category: cs.SD

TL;DR: 本文提出支持向量正则化(SVR)方法来解决对比学习中负样本推力垂直分量带来的优化轨迹漂移问题，通过在音频-文本多模态表示学习中引入辅助支持向量来控制垂直分量，从而在利用负样本丰富信息的同时提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比语言-音频预训练在多模态表示学习中具有重要作用，但研究发现对比学习中负样本推力的垂直分量是一把双刃剑：既包含丰富的补充信息，又因其不受约束的特性导致优化轨迹漂移和训练不稳定。

Method: 提出支持向量正则化(SVR)方法，引入辅助支持向量来控制垂直分量；探索了两种无监督建模策略来设定语义半径：直接参数化和带有约束的自适应半径预测器模块。

Result: 实验结果表明，SVR方法在标准音频-文本数据集上的分类、单语检索和多语检索任务中均优于广泛使用的基线方法如InfoNCE和SigLIP损失。

Conclusion: 理论分析和优化轨迹漂移实验验证了SVR方法的正确性和有效性，能够有效利用负样本信息同时保持训练稳定性。

Abstract: Contrastive language-audio pretraining, which aims to unify multimodal
representations in a shared embedding space, serves as a cornerstone for
building a wide range of applications, from cross-modal retrieval to
cutting-edge multimodal large language models. However, we find that the
perpendicular component of the pushing force from negative samples in
contrastive learning is a double-edged sword: it contains rich supplementary
information from negative samples, yet its unconstrained nature causes
optimization trajectory drift and training instability. To address this, we
propose Support Vector Regularization (SVR), a method that introduces an
auxiliary support vector to control this perpendicular component, aiming to
harness its rich information while mitigating the associated trajectory drift.
The efficacy of SVR is critically governed by its semantic radius, for which we
explore two unsupervised modeling strategies: direct parameterization and an
adaptive radius predictor module enhanced with constraints to improve its
predicting accuracy. Extensive experimental results demonstrate that our method
surpasses widely used baselines like InfoNCE and SigLIP loss across
classification, monolingual retrieval, and multilingual retrieval on standard
audio-text datasets. Both the theoretical analysis and the experimental results
on optimizing trajectory drift validate the correctness and effectiveness of
our SVR method.

</details>


### [29] [UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice](https://arxiv.org/abs/2509.21144)
*Sitong Cheng,Weizhen Bian,Xinsheng Wang,Ruibin Yuan,Jianyi Chen,Shunshun Yin,Yike Guo,Wei Xue*

Main category: cs.SD

TL;DR: UniSS是一个新颖的单阶段表达性语音到语音翻译框架，通过精心设计的语音语义和风格建模，结合文本大语言模型，解决了数据稀缺、多阶段处理复杂性和翻译能力迁移有限等挑战。


<details>
  <summary>Details</summary>
Motivation: 表达性语音到语音翻译领域面临三个主要挑战：保留表达风格的配对语音数据稀缺、多阶段处理流程复杂、以及大语言模型的翻译能力难以迁移到语音领域。

Method: 提出UniSS单阶段框架，设计语音语义和风格建模，构建统一的文本-语音语言模型，采用跨模态思维链提示过程将音频语义与文本对齐，并构建了44.8k小时的大规模高质量数据集UniST。

Result: 实验结果显示UniSS在翻译保真度和语音质量方面显著优于先前方法，同时保持了声音、情感和时长的一致性。

Conclusion: 这项工作为构建下一代表达性语音到语音翻译系统建立了一个更简单有效的范式。

Abstract: The ultimate goal of expressive speech-to-speech translation (S2ST) is to
accurately translate spoken content while preserving the speaker identity and
emotional style. However, progress in this field is largely hindered by three
key challenges: the scarcity of paired speech data that retains expressive
styles, the complexity of multi-stage processing pipelines, and the limited
transfer of translation capabilities from large language models (LLMs). In this
work, we address these challenges by introducing UniSS, a novel single-stage
framework for expressive S2ST. Our approach features carefully designed speech
semantic and style modeling, enabling seamless integration with existing
text-based LLM frameworks to develop a unified text-speech language model. To
transfer translation capabilities from text to speech, we propose a cross-modal
chain-of-thought prompting process that progressively aligns audio semantics
with text and ensures style preservation in the decoded results. Furthermore,
we construct and release a large-scale, high-quality expressive S2ST dataset,
UniST, comprising 44.8k hours of data. Experimental results show that UniSS
significantly outperforms previous methods in translation fidelity and speech
quality while preserving voice, emotion, and duration consistency. Our work
establishes a simpler and more effective paradigm for building the next
generation of expressive S2ST systems. Audio samples are available at
https://cmots.github.io/uniss-demo.

</details>

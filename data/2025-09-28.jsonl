{"id": "2509.20396", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20396", "abs": "https://arxiv.org/abs/2509.20396", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "title": "Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling", "comment": null, "summary": "Automatic speech recognition (ASR) systems struggle with non-normative speech\nfrom individuals with impairments caused by conditions like cerebral palsy or\nstructural anomalies. The high acoustic variability and scarcity of training\ndata severely degrade model performance. This work introduces a data-efficient\npersonalization method that quantifies phoneme-level uncertainty to guide\nfine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model\nfinds most difficult and use these estimates for a targeted oversampling\nstrategy. We validate our method on English and German datasets. Crucially, we\ndemonstrate that our model-derived uncertainty strongly correlates with\nphonemes identified as challenging in an expert clinical logopedic report,\nmarking, to our knowledge, the first work to successfully align model\nuncertainty with expert assessment of speech difficulty. Our results show that\nthis clinically-validated, uncertainty-guided sampling significantly improves\nASR accuracy, delivering a practical framework for personalized and inclusive\nASR."}
{"id": "2509.20397", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20397", "abs": "https://arxiv.org/abs/2509.20397", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Shih-Chii Liu", "Yingqiang Gao"], "title": "Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition", "comment": null, "summary": "Speech impairments resulting from congenital disorders, such as cerebral\npalsy, down syndrome, or apert syndrome, as well as acquired brain injuries due\nto stroke, traumatic accidents, or tumors, present major challenges to\nautomatic speech recognition (ASR) systems. Despite recent advancements,\nstate-of-the-art ASR models like Whisper still struggle with non-normative\nspeech due to limited training data availability and high acoustic variability.\nMoreover, collecting and annotating non-normative speech is burdensome:\nspeaking is effortful for many affected individuals, while laborious annotation\noften requires caregivers familiar with the speaker. This work introduces a\nnovel ASR personalization method based on Bayesian Low-rank Adaptation for\ndata-efficient fine-tuning. We validate our method on the English UA-Speech\ndataset and a newly collected German speech dataset, BF-Sprache, from a child\nwith structural speech impairment. The dataset and approach are designed to\nreflect the challenges of low-resource settings that include individuals with\nspeech impairments. Our method significantly improves ASR accuracy for impaired\nspeech while maintaining data and annotation efficiency, offering a practical\npath toward inclusive ASR."}
{"id": "2509.20410", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20410", "abs": "https://arxiv.org/abs/2509.20410", "authors": ["Weijie Wu", "Wenhao Guan", "Kaidi Wang", "Peijie Chen", "Zhuanling Zha", "Junbo Li", "Jun Fang", "Lin Li", "Qingyang Hong"], "title": "Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction", "comment": null, "summary": "Spoken dialogue models have significantly advanced intelligent\nhuman\\textendash computer interaction, yet they lack a plug\\textendash\nand\\textendash play full\\textendash duplex prediction module for semantic\nendpoint detection, hindering seamless audio interactions. In this paper, we\nintroduce Phoenix\\textendashVAD, an LLM\\textendash based model that enables\nstreaming semantic endpoint detection. Specifically, Phoenix\\textendash VAD\nleverages the semantic comprehension capability of the LLM and a sliding window\ntraining strategy to achieve reliable semantic endpoint detection while\nsupporting streaming inference. Experiments on both semantically complete and\nincomplete speech scenarios indicate that Phoenix\\textendash VAD achieves\nexcellent and competitive performance. Furthermore, this design enables the\nfull\\textendash duplex prediction module to be optimized independently of the\ndialogue model, providing more reliable and flexible support for\nnext\\textendash generation human\\textendash computer interaction."}
{"id": "2509.20485", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20485", "abs": "https://arxiv.org/abs/2509.20485", "authors": ["Ismail Rasim Ulgen", "Zongyang Du", "Junchen Lu", "Philipp Koehn", "Berrak Sisman"], "title": "Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens", "comment": "Under review for IEEE OJSP", "summary": "Objective evaluation of synthesized speech is critical for advancing speech\ngeneration systems, yet existing metrics for intelligibility and prosody remain\nlimited in scope and weakly correlated with human perception. Word Error Rate\n(WER) provides only a coarse text-based measure of intelligibility, while\nF0-RMSE and related pitch-based metrics offer a narrow, reference-dependent\nview of prosody. To address these limitations, we propose TTScore, a targeted\nand reference-free evaluation framework based on conditional prediction of\ndiscrete speech tokens. TTScore employs two sequence-to-sequence predictors\nconditioned on input text: TTScore-int, which measures intelligibility through\ncontent tokens, and TTScore-pro, which evaluates prosody through prosody\ntokens. For each synthesized utterance, the predictors compute the likelihood\nof the corresponding token sequences, yielding interpretable scores that\ncapture alignment with intended linguistic content and prosodic structure.\nExperiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that\nTTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and\nachieve stronger correlations with human judgments of overall quality than\nexisting intelligibility and prosody-focused metrics."}
{"id": "2509.20679", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20679", "abs": "https://arxiv.org/abs/2509.20679", "authors": ["Duc-Tuan Truong", "Tianchi Liu", "Ruijie Tao", "Junjie Li", "Kong Aik Lee", "Eng Siong Chng"], "title": "QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection", "comment": "5 pages, 4 figures", "summary": "Recent work shows that one-class learning can detect unseen deepfake attacks\nby modeling a compact distribution of bona fide speech around a single\ncentroid. However, the single-centroid assumption can oversimplify the bona\nfide speech representation and overlook useful cues, such as speech quality,\nwhich reflects the naturalness of the speech. Speech quality can be easily\nobtained using existing speech quality assessment models that estimate it\nthrough Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware\nMulti-Centroid One-Class Learning for speech deepfake detection. QAMO extends\nconventional one-class learning by introducing multiple quality-aware\ncentroids. In QAMO, each centroid is optimized to represent a distinct speech\nquality subspaces, enabling better modeling of intra-class variability in bona\nfide speech. In addition, QAMO supports a multi-centroid ensemble scoring\nstrategy, which improves decision thresholding and reduces the need for quality\nlabels during inference. With two centroids to represent high- and low-quality\nspeech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild\ndataset, outperforming previous one-class and quality-aware systems."}
{"id": "2509.20500", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20500", "abs": "https://arxiv.org/abs/2509.20500", "authors": ["Weijian Zhang", "Hashan K. Weerasooriya", "Prateek Chennuri", "Stanley H. Chan"], "title": "Real-Time Markov Modeling for Single-Photon LiDAR: $1000 \\times$ Acceleration and Convergence Analysis", "comment": null, "summary": "Asynchronous single-photon LiDAR (SP-LiDAR) is an important imaging modality\nfor high-quality 3D applications and navigation, but the modeling of the\ntimestamp distributions of a SP-LiDAR in the presence of dead time remains a\nvery challenging open problem. Prior works have shown that timestamps form a\ndiscrete-time Markov chain, whose stationary distribution can be computed as\nthe leading left eigenvector of a large transition matrix. However,\nconstructing this matrix is known to be computationally expensive because of\nthe coupling between states and the dead time. This paper presents the first\nnon-sequential Markov modeling for the timestamp distribution. The key\ninnovation is an equivalent formulation that reparameterizes the integral\nbounds and separates the effect of dead time as a deterministic row permutation\nof a base matrix. This decoupling enables efficient vectorized matrix\nconstruction, yielding up to $1000 \\times$ acceleration over existing methods.\nThe new model produces a nearly exact stationary distribution when compared\nwith the gold standard Monte Carlo simulations, yet using a fraction of the\ntime. In addition, a new theoretical analysis reveals the impact of the\nmagnitude and phase of the second-largest eigenvalue, which are overlooked in\nthe literature but are critical to the convergence."}
{"id": "2509.20741", "categories": ["eess.AS", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20741", "abs": "https://arxiv.org/abs/2509.20741", "authors": ["T. Aleksandra Ma", "Sile Yin", "Li-Chia Yang", "Shuo Zhang"], "title": "Real-Time System for Audio-Visual Target Speech Enhancement", "comment": "Accepted into WASPAA 2025 demo session", "summary": "We present a live demonstration for RAVEN, a real-time audio-visual speech\nenhancement system designed to run entirely on a CPU. In single-channel,\naudio-only settings, speech enhancement is traditionally approached as the task\nof extracting clean speech from environmental noise. More recent work has\nexplored the use of visual cues, such as lip movements, to improve robustness,\nparticularly in the presence of interfering speakers. However, to our\nknowledge, no prior work has demonstrated an interactive system for real-time\naudio-visual speech enhancement operating on CPU hardware. RAVEN fills this gap\nby using pretrained visual embeddings from an audio-visual speech recognition\nmodel to encode lip movement information. The system generalizes across\nenvironmental noise, interfering speakers, transient sounds, and even singing\nvoices. In this demonstration, attendees will be able to experience live\naudio-visual target speech enhancement using a microphone and webcam setup,\nwith clean speech playback through headphones."}
{"id": "2509.20682", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20682", "abs": "https://arxiv.org/abs/2509.20682", "authors": ["Duc-Tuan Truong", "Tianchi Liu", "Junjie Li", "Ruijie Tao", "Kong Aik Lee", "Eng Siong Chng"], "title": "Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection", "comment": "5 pages, 4 figures", "summary": "In speech deepfake detection (SDD), data augmentation (DA) is commonly used\nto improve model generalization across varied speech conditions and spoofing\nattacks. However, during training, the backpropagated gradients from original\nand augmented inputs may misalign, which can result in conflicting parameter\nupdates. These conflicts could hinder convergence and push the model toward\nsuboptimal solutions, thereby reducing the benefits of DA. To investigate and\naddress this issue, we design a dual-path data-augmented (DPDA) training\nframework with gradient alignment for SDD. In our framework, each training\nutterance is processed through two input paths: one using the original speech\nand the other with its augmented version. This design allows us to compare and\nalign their backpropagated gradient directions to reduce optimization\nconflicts. Our analysis shows that approximately 25% of training iterations\nexhibit gradient conflicts between the original inputs and their augmented\ncounterparts when using RawBoost augmentation. By resolving these conflicts\nwith gradient alignment, our method accelerates convergence by reducing the\nnumber of training epochs and achieves up to an 18.69% relative reduction in\nEqual Error Rate on the In-the-Wild dataset compared to the baseline."}
{"id": "2509.20908", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20908", "abs": "https://arxiv.org/abs/2509.20908", "authors": ["Peng Liu", "Zesong Fei", "Meng Hua", "Guangji Chen", "Xinyi Wang", "Ruiqi Liu"], "title": "Wireless Powered MEC Systems via Discrete Pinching Antennas: TDMA versus NOMA", "comment": "13 pages, 9 figures. Submitted to IEEE journals for possible\n  publication", "summary": "Pinching antennas (PAs), a new type of reconfigurable and flexible antenna\nstructures, have recently attracted significant research interest due to their\nability to create line-of-sight links and mitigate large-scale path loss. Owing\nto their potential benefits, integrating PAs into wireless powered mobile edge\ncomputing (MEC) systems is regarded as a viable solution to enhance both energy\ntransfer and task offloading efficiency. Unlike prior studies that assume ideal\ncontinuous PA placement along waveguides, this paper investigates a practical\ndiscrete PA-assisted wireless powered MEC framework, where devices first\nharvest energy from PA-emitted radio-frequency signals and then adopt a partial\noffloading mode, allocating part of the harvested energy to local computing and\nthe remainder to uplink offloading. The uplink phase considers both the\ntime-division multiple access (TDMA) and non-orthogonal multiple access (NOMA),\neach examined under three levels of PA activation flexibility. For each\nconfiguration, we formulate a joint optimization problem to maximize the total\ncomputational bits and conduct a theoretical performance comparison between the\nTDMA and NOMA schemes. To address the resulting mixed-integer nonlinear\nproblems, we develop a two-layer algorithm that combines closed-form solutions\nbased on Karush-Kuhn-Tucker (KKT) conditions with a cross-entropy-based\nlearning method. Numerical results validate the superiority of the proposed\ndesign in terms of the harvested energy and computation performance, revealing\nthat TDMA and NOMA achieve comparable performance under coarser PA activation\nlevels, whereas finer activation granularity enables TDMA to achieve superior\ncomputation performance over NOMA."}
{"id": "2509.20802", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20802", "abs": "https://arxiv.org/abs/2509.20802", "authors": ["Tan Dat Nguyen", "Jaehun Kim", "Ji-Hoon Kim", "Shukjae Choi", "Youshin Lim", "Joon Son Chung"], "title": "SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS", "comment": "Submitted to ICASSP 2026", "summary": "The goal of this paper is to introduce SPADE, a framework for Structured\nPruning and Adaptive Distillation for Efficient Large Language Model-based\ntext-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability\nand zero-shot generalization, but their large parameter counts and high latency\nlimit real-world deployment. SPADE addresses this by combining (i) a pruning\nstep guided by a word-error-rate-based layer importance index to remove\nnon-essential Transformer layers, with (ii) multi-level knowledge distillation\nto restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves\nnear-parity perceptual quality while halving Transformer depth, reducing VRAM\nusage by up to 20%, and achieving up to 1.7x faster real-time factor with less\nthan 5% of the original training data. These results show that compact LLM-TTS\nmodels can maintain naturalness and speaker similarity while enabling practical\nreal-time speech generation. Audio samples are available at\nhttps://mm.kaist.ac.kr/projects/SPADE/."}
{"id": "2509.20891", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20891", "abs": "https://arxiv.org/abs/2509.20891", "authors": ["Junyoung Koh", "Soo Yong Kim", "Gyu Hyeong Choi", "Yongwon Choi"], "title": "AIBA: Attention-based Instrument Band Alignment for Text-to-Audio Diffusion", "comment": "NeurIPS 2025 AI for Music Workshop", "summary": "We present AIBA (Attention-In-Band Alignment), a lightweight, training-free\npipeline to quantify where text-to-audio diffusion models attend on the\ntime-frequency (T-F) plane. AIBA (i) hooks cross-attention at inference to\nrecord attention probabilities without modifying weights; (ii) projects them to\nfixed-size mel grids that are directly comparable to audio energy; and (iii)\nscores agreement with instrument-band ground truth via interpretable metrics\n(T-F IoU/AP, frequency-profile correlation, and a pointing game). On Slakh2100\nwith an AudioLDM2 backbone, AIBA reveals consistent instrument-dependent trends\n(e.g., bass favoring low bands) and achieves high precision with moderate\nrecall."}
{"id": "2509.20987", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.20987", "abs": "https://arxiv.org/abs/2509.20987", "authors": ["Changhao Liu", "Weidong Mei", "Zhi Chen", "Jun Fang", "Boyu Ning"], "title": "A General Optimization Framework for Movable Antenna Systems via Discrete Sampling", "comment": null, "summary": "Movable antenna (MA) systems have attracted growing interest in wireless\ncommunications due to their ability to reshape wireless channels via local\nantenna movement within a confined region. However, optimizing antenna\npositions to enhance communication performance turns out to be challenging due\nto the highly nonlinear relationship between wireless channels and antenna\npositions. Existing approaches, such as gradient-based and heuristic\nalgorithms, often suffer from high computational complexity or undesired local\noptima. To address the above challenge, this letter proposes a general and\nlow-complexity optimization framework for MA position optimization.\nSpecifically, we discretize the antenna movement region into a set of sampling\npoints, thereby transforming the continuous optimization problem into a\ndiscrete point selection problem. Next, we sequentially update the optimal\nsampling point for each MA over multiple rounds. To avoid convergence to poor\nlocal optima, a Gibbs sampling (GS) phase is introduced between rounds to\nexplore adjacent and randomly generated candidate solutions. As a case study,\nwe investigate joint precoding and antenna position optimization for an\nMA-enhanced broadcast system by applying the proposed framework. Numerical\nresults demonstrate that the proposed algorithm achieves near-optimal\nperformance and significantly outperforms existing benchmarks."}
{"id": "2509.20875", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20875", "abs": "https://arxiv.org/abs/2509.20875", "authors": ["Mattes Ohlenbusch", "Mikolaj Kegler", "Marko Stamenovic"], "title": "PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice Pickup in Hearables", "comment": "Submitted to ICASSP 2026", "summary": "Speech enhancement for voice pickup in hearables aims to improve the user's\nvoice by suppressing noise and interfering talkers, while maintaining own-voice\nquality. For single-channel methods, it is particularly challenging to\ndistinguish the target from interfering talkers without additional context. In\nthis paper, we compare two strategies to resolve this ambiguity: personalized\nspeech enhancement (PSE), which uses enrollment utterances to represent the\ntarget, and auxiliary-sensor speech enhancement (AS-SE), which uses in-ear\nmicrophones as additional input. We evaluate the strategies on two public\ndatasets, employing different auxiliary sensor arrays, to investigate their\ncross-dataset generalization. We propose training-time augmentations to\nfacilitate cross-dataset generalization of AS-SE systems. We also show that\ncombining PSE and AS-SE (PAS-SE) provides complementary performance benefits,\nespecially when enrollment speech is recorded with the in-ear microphone. We\nfurther demonstrate that PAS-SE personalized with noisy in-ear enrollments\nmaintains performance benefits over the AS-SE system."}
{"id": "2509.20969", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20969", "abs": "https://arxiv.org/abs/2509.20969", "authors": ["Shaohan Jiang", "Junan Zhang", "Yunjia Zhang", "Jing Yang", "Fan Fan", "Zhizheng Wu"], "title": "SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement", "comment": "Demopage: https://singverse.github.io, Dataset:\n  https://huggingface.co/datasets/amphion/SingVERSE", "summary": "This paper presents a benchmark for singing voice enhancement. The\ndevelopment of singing voice enhancement is limited by the lack of realistic\nevaluation data. To address this gap, this paper introduces SingVERSE, the\nfirst real-world benchmark for singing voice enhancement, covering diverse\nacoustic scenarios and providing paired, studio-quality clean references.\nLeveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art\nmodels and uncover a consistent trade-off between perceptual quality and\nintelligibility. Finally, we show that training on in-domain singing data\nsubstantially improves enhancement performance without degrading speech\ncapabilities, establishing a simple yet effective path forward. This work\noffers the community a foundational benchmark together with critical insights\nto guide future advances in this underexplored domain. Demopage:\nhttps://singverse.github.io"}
{"id": "2509.21032", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21032", "abs": "https://arxiv.org/abs/2509.21032", "authors": ["Mohammad Ali Vahedifar", "Qi Zhang"], "title": "Shapley Features for Robust Signal Prediction in Tactile Internet", "comment": null, "summary": "The Tactile Internet (TI) requires ultra-low latency and reliable haptic\nsignal transmission, yet packet loss and delay remain unresolved challenges. We\npresent a novel prediction framework that integrates Gaussian Processes (GP)\nwith a ResNet-based Neural Network, where GP acts as an oracle to recover\nsignals lost or heavily delayed. To further optimize performance, we introduce\nShapley Feature Values (SFV), a principled feature selection mechanism that\nisolates the most informative inputs for prediction. This GP+SFV framework\nachieves 95.72% accuracy, surpassing the state-of-the-art LeFo method by 11.1%,\nwhile simultaneously relaxing TI's rigid delay constraints. Beyond accuracy,\nSFV operates as a modular accelerator: when paired with LeFo, it reduces\ninference time by 27%, and when paired with GP, by 72%. These results establish\nGP+SFV as both a high-accuracy and high-efficiency solution, paving the way for\npractical and reliable haptic communications in TI systems."}
{"id": "2509.21003", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21003", "abs": "https://arxiv.org/abs/2509.21003", "authors": ["Ui-Hyeop Shin", "Jaehyun Ko", "Woocheol Jeong", "Hyuing-Min Park"], "title": "TF-Restormer: Complex Spectral Prediction for Speech Restoration", "comment": "Preprint. Under review", "summary": "Speech restoration in real-world conditions is challenging due to compounded\ndistortions such as clipping, band-pass filtering, digital artifacts, noise,\nand reverberation, and low sampling rates. Existing systems, including\nvocoder-based approaches, often sacrifice signal fidelity, while diffusion\nmodels remain impractical for streaming. Moreover, most assume a fixed target\nsampling rate, requiring external resampling that leads to redundant\ncomputations. We present TF-Restormer, an encoder-decoder architecture that\nconcentrates analysis on input-bandwidth with a time-frequency dual-path\nencoder and reconstructs missing high-frequency bands through a light decoder\nwith frequency extension queries. It enables efficient and universal\nrestoration across arbitrary input-output rates without redundant resampling.\nTo support adversarial training across diverse rates, we introduce a shared\nsampling-frequency-independent (SFI) STFT discriminator. TF-Restormer further\nsupports streaming with a causal time module, and improves robustness under\nextreme degradations by injecting spectral inductive bias into the frequency\nmodule. Finally, we propose a scaled log-spectral loss that stabilizes\noptimization under severe conditions while emphasizing well-predicted spectral\ndetails. As a single model across sampling rates, TF-Restormer consistently\noutperforms prior systems, achieving balanced gains in signal fidelity and\nperceptual quality, while its streaming mode maintains competitive\neffectiveness for real-time application. Code and demos are available at\nhttps://tf-restormer.github.io/demo."}
{"id": "2509.20971", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20971", "abs": "https://arxiv.org/abs/2509.20971", "authors": ["Anupam Purwar", "Aditya Choudhary"], "title": "i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents", "comment": "This paper analyzes a low-latency, end-to-end voice-to-voice (V-2-V)\n  architecture, identifying that the Text-to-Speech (TTS) component has the\n  highest impact on real-time performance. By reducing the number of Residual\n  Vector Quantization (RVQ) iterations in the TTS model, latency can be\n  effectively halved, creating a direct trade-off between conversational speed\n  and audio quality", "summary": "We experiment with a low-latency, end-to-end voice-to-voice communication\nmodel to optimize it for real-time conversational applications. By analyzing\ncomponents essential to voice to voice (V-2-V) system viz. automatic speech\nrecognition (ASR), text-to-speech (TTS), and dialog management, our work\nanalyzes how to reduce processing time while maintaining high-quality\ninteractions to identify the levers for optimizing V-2-V system. Our work\nidentifies that TTS component which generates life-like voice, full of emotions\nincluding natural pauses and exclamations has highest impact on Real time\nfactor (RTF). The experimented V-2-V architecture utilizes CSM1b has the\ncapability to understand tone as well as context of conversation by ingesting\nboth audio and text of prior exchanges to generate contextually accurate\nspeech. We explored optimization of Residual Vector Quantization (RVQ)\niterations by the TTS decoder which come at a cost of decrease in the quality\nof voice generated. Our experimental evaluations also demonstrate that for\nV-2-V implementations based on CSM most important optimizations can be brought\nby reducing the number of RVQ Iterations along with the codebooks used in Mimi."}
{"id": "2509.21118", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.21118", "abs": "https://arxiv.org/abs/2509.21118", "authors": ["Ziyi Wang", "Frederik Zumegen", "Christoph Studer"], "title": "Neural Integrated Sensing and Communication for the MIMO-OFDM Downlink", "comment": "To appear in the IEEE Journal on Selected Areas in Communications", "summary": "The ongoing convergence of spectrum and hardware requirements for wireless\nsensing and communication applications has fueled the integrated sensing and\ncommunication (ISAC) paradigm in next-generation networks. Neural-network-based\nISAC leverages data-driven learning techniques to add sensing capabilities to\nexisting communication infrastructure. This paper presents a novel\nsignal-processing framework for such neural ISAC systems based on the\nmultiple-input multiple-output (MIMO) and orthogonal frequency-division\nmultiplexing (OFDM) downlink. Our approach enables generalized sensing\nfunctionality without modifying the MIMO-OFDM communication link. Specifically,\nour neural ISAC pipeline measures the backscattered communication signals to\ngenerate discrete map representations of spatial occupancy, formulated as\nmulticlass or multilabel classification problems, which can then be utilized by\nspecialized downstream tasks. To improve sensing performance in closed or\ncluttered environments, our neural ISAC pipeline relies on features\nspecifically designed to mitigate strong reflective paths. Extensive\nsimulations using ray-tracing models demonstrate that our neural ISAC framework\nreliably reconstructs scene maps without altering the MIMO-OFDM communication\npipeline or reducing data rates."}
{"id": "2509.21060", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21060", "abs": "https://arxiv.org/abs/2509.21060", "authors": ["Haolin He", "Xingjian Du", "Renhe Sun", "Zheqi Dai", "Yujia Xiao", "Mingru Yang", "Jiayi Zhou", "Xiquan Li", "Zhengxi Liu", "Zining Liang", "Chunyat Wu", "Qianhua He", "Tan Lee", "Xie Chen", "Weilong Zheng", "Weiqiang Wang", "Mark Plumbley", "Jian Liu", "Qiuqiang Kong"], "title": "Measuring Audio's Impact on Correctness: Audio-Contribution-Aware Post-Training of Large Audio Language Models", "comment": null, "summary": "Large Audio Language Models (LALMs) represent an important frontier in\nmultimodal AI, addressing diverse audio tasks. Recently, post-training of LALMs\nhas received increasing attention due to significant performance improvements\nover foundation models. While single-stage post-training such as reinforcement\nlearning (RL) has demonstrated promising results, multi-stage approaches such\nas supervised fine-tuning (SFT) followed by RL remain suboptimal. The\nallocation of data across multiple training stages to maximize LALM\ncapabilities has not been fully explored, and large-scale, high-quality\ndatasets for such research are also lacking. To address these problems, we\nfirstly present AudioMCQ, a comprehensive audio multiple-choice question\ndataset comprising 571k samples with two kinds of chain-of-thought annotations.\nSecondly, we investigate the prevalent zero audio-contribution phenomenon in\nLALMs, where models derive correct answers solely from textual information\nwithout processing audio content. We propose Audio-Contribution Filtering to\npartition data into weak and strong audio-contribution subsets. Based on these\ninsights, we develop two effective post-training paradigms: Weak-to-Strong (SFT\non weak audio-contribution data followed by RL on strong audio-contribution\ndata) and Mixed-to-Strong (SFT on mixed audio-contribution data followed by RL\non strong audio-contribution data). We achieve first place in the DCASE 2025\nAudio-Question-Answering challenge by using AudioMCQ. Additionally, leveraging\nour dataset with different training strategies, we achieve 78.2\\% on\nMMAU-test-mini, 75.6\\% on MMAU, 67.1\\% on MMAR, and 70.7\\% on MMSU,\nestablishing new state-of-the-art performance across these benchmarks."}
{"id": "2509.21033", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21033", "abs": "https://arxiv.org/abs/2509.21033", "authors": ["Jiehui Luo", "Yuguo Yin", "Yuxin Xie", "Jinghan Ru", "Xianwei Zhuang", "Minghua He", "Aofan Liu", "Zihan Xiong", "Dongchao Yang"], "title": "SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization", "comment": null, "summary": "Contrastive language-audio pretraining, which aims to unify multimodal\nrepresentations in a shared embedding space, serves as a cornerstone for\nbuilding a wide range of applications, from cross-modal retrieval to\ncutting-edge multimodal large language models. However, we find that the\nperpendicular component of the pushing force from negative samples in\ncontrastive learning is a double-edged sword: it contains rich supplementary\ninformation from negative samples, yet its unconstrained nature causes\noptimization trajectory drift and training instability. To address this, we\npropose Support Vector Regularization (SVR), a method that introduces an\nauxiliary support vector to control this perpendicular component, aiming to\nharness its rich information while mitigating the associated trajectory drift.\nThe efficacy of SVR is critically governed by its semantic radius, for which we\nexplore two unsupervised modeling strategies: direct parameterization and an\nadaptive radius predictor module enhanced with constraints to improve its\npredicting accuracy. Extensive experimental results demonstrate that our method\nsurpasses widely used baselines like InfoNCE and SigLIP loss across\nclassification, monolingual retrieval, and multilingual retrieval on standard\naudio-text datasets. Both the theoretical analysis and the experimental results\non optimizing trajectory drift validate the correctness and effectiveness of\nour SVR method."}
{"id": "2509.21162", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21162", "abs": "https://arxiv.org/abs/2509.21162", "authors": ["Ali Khandan Boroujeni", "Hyeon Seok Rou", "Ghazal Bagheri", "Giuseppe Thadeu Freitas de Abreu", "Stefan Köpsell", "Kuranage Roche Rayan Ranasinghe", "Rafael F. Schaefer"], "title": "A Secure ISAC Waveform Design Framework via Random Frequency and PRI Agility", "comment": "Submitted to an IEEE conference", "summary": "This paper presents a novel framework for enhancing the security, data rate,\nand sensing performance of integrated sensing and communications (ISAC)\nsystems. We employ a random frequency and pulse repetition interval (PRI)\nagility (RFPA) method for the waveform design, where the necessary random\nsequences are governed by shared secrets. These secrets, which can be\npre-shared or generated via channel reciprocity, obfuscate critical radar\nparameters like Doppler frequency and pulse start times, thereby significantly\nimpeding the ability to perform reconnaissance from a passive adversary without\nthe secret key. To further introduce enhanced data throughput, we also\nintroduce a hybrid information embedding scheme that integrates amplitude shift\nkeying (ASK), phase shift keying (PSK), index modulation (IM), and spatial\nmodulation (SM), for which a low-complexity sparse-matched filter receiver is\nproposed for accurate decoding with practical complexity. Finally, the\nexcellent range-velocity resolution and clutter suppression of the proposed\nwaveform are analyzed via the ambiguity function (AF)."}
{"id": "2509.21087", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21087", "abs": "https://arxiv.org/abs/2509.21087", "authors": ["Rostislav Makarov", "Lea Schönherr", "Timo Gerkmann"], "title": "Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?", "comment": "Copyright 2026 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Machine learning approaches for speech enhancement are becoming increasingly\nexpressive, enabling ever more powerful modifications of input signals. In this\npaper, we demonstrate that this expressiveness introduces a vulnerability:\nadvanced speech enhancement models can be susceptible to adversarial attacks.\nSpecifically, we show that adversarial noise, carefully crafted and\npsychoacoustically masked by the original input, can be injected such that the\nenhanced speech output conveys an entirely different semantic meaning. We\nexperimentally verify that contemporary predictive speech enhancement models\ncan indeed be manipulated in this way. Furthermore, we highlight that diffusion\nmodels with stochastic samplers exhibit inherent robustness to such adversarial\nattacks by design."}
{"id": "2509.21144", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21144", "abs": "https://arxiv.org/abs/2509.21144", "authors": ["Sitong Cheng", "Weizhen Bian", "Xinsheng Wang", "Ruibin Yuan", "Jianyi Chen", "Shunshun Yin", "Yike Guo", "Wei Xue"], "title": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice", "comment": null, "summary": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to\naccurately translate spoken content while preserving the speaker identity and\nemotional style. However, progress in this field is largely hindered by three\nkey challenges: the scarcity of paired speech data that retains expressive\nstyles, the complexity of multi-stage processing pipelines, and the limited\ntransfer of translation capabilities from large language models (LLMs). In this\nwork, we address these challenges by introducing UniSS, a novel single-stage\nframework for expressive S2ST. Our approach features carefully designed speech\nsemantic and style modeling, enabling seamless integration with existing\ntext-based LLM frameworks to develop a unified text-speech language model. To\ntransfer translation capabilities from text to speech, we propose a cross-modal\nchain-of-thought prompting process that progressively aligns audio semantics\nwith text and ensures style preservation in the decoded results. Furthermore,\nwe construct and release a large-scale, high-quality expressive S2ST dataset,\nUniST, comprising 44.8k hours of data. Experimental results show that UniSS\nsignificantly outperforms previous methods in translation fidelity and speech\nquality while preserving voice, emotion, and duration consistency. Our work\nestablishes a simpler and more effective paradigm for building the next\ngeneration of expressive S2ST systems. Audio samples are available at\nhttps://cmots.github.io/uniss-demo."}
{"id": "2509.21171", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21171", "abs": "https://arxiv.org/abs/2509.21171", "authors": ["Ali Khandan Boroujeni", "Ghazal Bagheri", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Stefan Köpsell", "Rafael F. Schaefer"], "title": "Adversarially Robust MIMO Physical Layer Authentication for Non-Stationary Channels", "comment": "Submitted to an IEEE journal", "summary": "We propose an adversarially robust physical layer authentication (AR-PLA)\nframework tailored for non-stationary multiple-input multiple-output (MIMO)\nwireless channels. The framework integrates sequential Bayesian\ndecision-making, deep feature extraction via contrastive learning, and\ngenerative adversarial modeling to simulate adaptive spoofers. Unlike\nconventional methods that assume stationary channels or independent\nobservations, our approach explicitly accounts for temporal and spatial\ncorrelations, line-of-sight (LoS) blockages, and dynamic spoofing strategies. A\ncomprehensive analytical characterization of the authentication performance\nusing both 2-state and 3-state hidden Markov models (HMMs) with moving-average\nonline adaptation is also provided, with closed-form recursions for\nloglikelihood ratios, detection probabilities, and steady-state approximations,\nwhich demonstrate significant robustness improvement over classical sequential\nauthentication schemes."}
{"id": "2509.21185", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21185", "abs": "https://arxiv.org/abs/2509.21185", "authors": ["Luan Vinícius Fiorio", "Alex Young", "Ronald M. Aarts"], "title": "Hybrid Real- And Complex-Valued Neural Network Concept For Low-Complexity Phase-Aware Speech Enhancement", "comment": null, "summary": "In this paper, we propose hybrid real- and complex-valued neural networks for\nspeech enhancement. Real- or complex-valued models are either inefficient or\npresent high complexity. We devise a straightforward design method for\nextending a real-valued network into its hybrid counterpart. Based on speech\nintelligibility and quality metrics, we compare the real, complex, and hybrid\nversions of a convolutional and a convolutional-recurrent architecture. The\nhybrid network consistently outperforms its counterparts with the same number\nof parameters. Additionally, the hybrid models' complexity in terms of\nmultiply-accumulate operations is substantially lower than that of their\ncounterparts."}
{"id": "2509.20396", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20396", "abs": "https://arxiv.org/abs/2509.20396", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "title": "Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling", "comment": null, "summary": "Automatic speech recognition (ASR) systems struggle with non-normative speech\nfrom individuals with impairments caused by conditions like cerebral palsy or\nstructural anomalies. The high acoustic variability and scarcity of training\ndata severely degrade model performance. This work introduces a data-efficient\npersonalization method that quantifies phoneme-level uncertainty to guide\nfine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model\nfinds most difficult and use these estimates for a targeted oversampling\nstrategy. We validate our method on English and German datasets. Crucially, we\ndemonstrate that our model-derived uncertainty strongly correlates with\nphonemes identified as challenging in an expert clinical logopedic report,\nmarking, to our knowledge, the first work to successfully align model\nuncertainty with expert assessment of speech difficulty. Our results show that\nthis clinically-validated, uncertainty-guided sampling significantly improves\nASR accuracy, delivering a practical framework for personalized and inclusive\nASR."}
{"id": "2509.21219", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.21219", "abs": "https://arxiv.org/abs/2509.21219", "authors": ["Amir Eshaghi Chaleshtori", "Abdollah Aghaie"], "title": "An enhanced statistical feature fusion approach using an improved distance evaluation algorithm and weighted K-nearest neighbor for bearing fault diagnosis", "comment": null, "summary": "Bearings are among the most failure-prone components in rotating machinery,\nand their condition directly impacts overall performance. Therefore, accurately\ndiagnosing bearing faults is essential for ensuring system stability. However,\ndetecting such malfunctions in noisy environments, where data is collected from\nmultiple sensors, necessitates the extraction and selection of informative\nfeatures. This paper proposes an improved distance evaluation algorithm\ncombined with a weighted K-nearest neighbor (KNN) classifier for bearing fault\ndiagnosis. The process begins with extracting and integrating statistical\nfeatures of vibration across the time, frequency, and time-frequency domains.\nNext, the improved distance evaluation algorithm assigns weights to the\nextracted features, retaining only the most informative ones by eliminating\ninsensitive features. Finally, the selected features are used to train the\nweighted KNN classifier. To validate the proposed method, we employ bearing\ndata from the University of Ottawa. The results demonstrate the effectiveness\nof our approach in accurately identifying bearing faults."}
{"id": "2509.21214", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21214", "abs": "https://arxiv.org/abs/2509.21214", "authors": ["Jiahe Wang", "Hongyu Wang", "Wei Wang", "Lei Yang", "Chenda Li", "Wangyou Zhang", "Lufen Tan", "Yanmin Qian"], "title": "MeanSE: Efficient Generative Speech Enhancement with Mean Flows", "comment": "Submitted to ICASSP 2026", "summary": "Speech enhancement (SE) improves degraded speech's quality, with generative\nmodels like flow matching gaining attention for their outstanding perceptual\nquality. However, the flow-based model requires multiple numbers of function\nevaluations (NFEs) to achieve stable and satisfactory performance, leading to\nhigh computational load and poor 1-NFE performance. In this paper, we propose\nMeanSE, an efficient generative speech enhancement model using mean flows,\nwhich models the average velocity field to achieve high-quality 1-NFE\nenhancement. Experimental results demonstrate that our proposed MeanSE\nsignificantly outperforms the flow matching baseline with a single NFE,\nexhibiting extremely better out-of-domain generalization capabilities."}
{"id": "2509.20410", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20410", "abs": "https://arxiv.org/abs/2509.20410", "authors": ["Weijie Wu", "Wenhao Guan", "Kaidi Wang", "Peijie Chen", "Zhuanling Zha", "Junbo Li", "Jun Fang", "Lin Li", "Qingyang Hong"], "title": "Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction", "comment": null, "summary": "Spoken dialogue models have significantly advanced intelligent\nhuman\\textendash computer interaction, yet they lack a plug\\textendash\nand\\textendash play full\\textendash duplex prediction module for semantic\nendpoint detection, hindering seamless audio interactions. In this paper, we\nintroduce Phoenix\\textendashVAD, an LLM\\textendash based model that enables\nstreaming semantic endpoint detection. Specifically, Phoenix\\textendash VAD\nleverages the semantic comprehension capability of the LLM and a sliding window\ntraining strategy to achieve reliable semantic endpoint detection while\nsupporting streaming inference. Experiments on both semantically complete and\nincomplete speech scenarios indicate that Phoenix\\textendash VAD achieves\nexcellent and competitive performance. Furthermore, this design enables the\nfull\\textendash duplex prediction module to be optimized independently of the\ndialogue model, providing more reliable and flexible support for\nnext\\textendash generation human\\textendash computer interaction."}
{"id": "2509.21290", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21290", "abs": "https://arxiv.org/abs/2509.21290", "authors": ["Tianqi Mao", "Jiayue Liu", "Weijie Liu", "Dezhi Zheng", "Zhaocheng Wang"], "title": "Vision-Intelligence-Enabled Beam Tracking for Cross-Interface Water-Air Optical Wireless Communications", "comment": null, "summary": "The escalating development of oceanic applications like underwater\nsurveillance and mineral exploration, is motivating real-time wireless backhaul\nof the considerable observation data. Such prospects can be hardly realized by\nthe narrowband acoustic approach. Alternatively, optical wireless communication\n(OWC) has emerged as a promising solution for maritime and underwater\napplications due to its great potential for broadband underwater transmission.\nHowever, the implementations of water-air OWC can be rather challenging,\nespecially when penetrating the fluctuating interface, where the direction of\nrefracted signals changes dynamically, causing severe beam misalignment with\nairborne stations. This has necessitated real-time transceiver alignment\nadaptable to the sophisticated oceanic environment, which has yet to be\naddressed. Against this background, this paper establishes a mathematical\nchannel model for water-air optical wireless transmission across the\nfluctuating sea surface. Based on the model, we propose a vision-based beam\ntracking algorithm that leverages artificial intelligence (AI) methods for\ndynamic channel prediction. The proposed algorithm integrates a convolutional\nneural network (CNN) with bi-directional long short-term memory (Bi-LSTM),\nwhich further incorporates the attention mechanism to effectively extract\ncritical spatio-temporal features from the vision data. The numerical\nsimulation results show that the proposed algorithm can outperform its\nclassical counterparts in maintaining receiving signal strength and supressing\nthe vision noises, which demonstrates its robustness against the the harsh\nconditions of water-air OWC systems."}
{"id": "2509.20969", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.20969", "abs": "https://arxiv.org/abs/2509.20969", "authors": ["Shaohan Jiang", "Junan Zhang", "Yunjia Zhang", "Jing Yang", "Fan Fan", "Zhizheng Wu"], "title": "SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement", "comment": "Demopage: https://singverse.github.io, Dataset:\n  https://huggingface.co/datasets/amphion/SingVERSE", "summary": "This paper presents a benchmark for singing voice enhancement. The\ndevelopment of singing voice enhancement is limited by the lack of realistic\nevaluation data. To address this gap, this paper introduces SingVERSE, the\nfirst real-world benchmark for singing voice enhancement, covering diverse\nacoustic scenarios and providing paired, studio-quality clean references.\nLeveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art\nmodels and uncover a consistent trade-off between perceptual quality and\nintelligibility. Finally, we show that training on in-domain singing data\nsubstantially improves enhancement performance without degrading speech\ncapabilities, establishing a simple yet effective path forward. This work\noffers the community a foundational benchmark together with critical insights\nto guide future advances in this underexplored domain. Demopage:\nhttps://singverse.github.io"}
{"id": "2509.20485", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20485", "abs": "https://arxiv.org/abs/2509.20485", "authors": ["Ismail Rasim Ulgen", "Zongyang Du", "Junchen Lu", "Philipp Koehn", "Berrak Sisman"], "title": "Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens", "comment": "Under review for IEEE OJSP", "summary": "Objective evaluation of synthesized speech is critical for advancing speech\ngeneration systems, yet existing metrics for intelligibility and prosody remain\nlimited in scope and weakly correlated with human perception. Word Error Rate\n(WER) provides only a coarse text-based measure of intelligibility, while\nF0-RMSE and related pitch-based metrics offer a narrow, reference-dependent\nview of prosody. To address these limitations, we propose TTScore, a targeted\nand reference-free evaluation framework based on conditional prediction of\ndiscrete speech tokens. TTScore employs two sequence-to-sequence predictors\nconditioned on input text: TTScore-int, which measures intelligibility through\ncontent tokens, and TTScore-pro, which evaluates prosody through prosody\ntokens. For each synthesized utterance, the predictors compute the likelihood\nof the corresponding token sequences, yielding interpretable scores that\ncapture alignment with intended linguistic content and prosodic structure.\nExperiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that\nTTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and\nachieve stronger correlations with human judgments of overall quality than\nexisting intelligibility and prosody-focused metrics."}
{"id": "2509.21311", "categories": ["eess.SP", "I.4.1; G.3; E.4; H.3"], "pdf": "https://arxiv.org/pdf/2509.21311", "abs": "https://arxiv.org/abs/2509.21311", "authors": ["Orestis Kaparounakis", "Phillip Stanley-Marbell"], "title": "Efficient Digital Methods to Quantify Sensor Output Uncertainty", "comment": null, "summary": "Accurate characterization of sensor output uncertainty is important for\nreliable data interpretation in many applications. Here, we investigate the\nimpact of transducer-level measurement uncertainty on overall sensor\nmeasurement accuracy due to limited-precision information about sensor\ncomponents. We explain our method using thermopile-based sensors as an example\nclass of sensors. We show how sensor calibration and conversion equations,\nwhich are an essential part of all sensing systems, propagate uncertainties\nresulting from the quantization of calibration parameters, to the final,\ncompensated sensor output. The experimental results show that the epistemic\nuncertainty of calibration-related quantities leads to absolute error in the\nsensor output as high as 5.3 {\\deg}C (and relative error as high as 25.7%) for\none commonly-used thermopile sensor. In one instance of using the epistemic\nuncertainty information in edge detection, we show reduction of false-positives\nedges to zero for the conventional Canny operator, while maintaining accuracy.\nWe show these ideas are practical and possible on actual embedded sensor\nsystems by prototyping them on two commercially-available uncertainty tracking\nhardware platforms, one with average power dissipation 16.7 mW and 42.9x\nspeedup compared to the equal-confidence Monte Carlo computation (the status\nquo), and the other with average power dissipation 147.15 mW and 94.4x speedup,\npaving the way for use in real time."}
{"id": "2509.20802", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.20802", "abs": "https://arxiv.org/abs/2509.20802", "authors": ["Tan Dat Nguyen", "Jaehun Kim", "Ji-Hoon Kim", "Shukjae Choi", "Youshin Lim", "Joon Son Chung"], "title": "SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS", "comment": "Submitted to ICASSP 2026", "summary": "The goal of this paper is to introduce SPADE, a framework for Structured\nPruning and Adaptive Distillation for Efficient Large Language Model-based\ntext-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability\nand zero-shot generalization, but their large parameter counts and high latency\nlimit real-world deployment. SPADE addresses this by combining (i) a pruning\nstep guided by a word-error-rate-based layer importance index to remove\nnon-essential Transformer layers, with (ii) multi-level knowledge distillation\nto restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves\nnear-parity perceptual quality while halving Transformer depth, reducing VRAM\nusage by up to 20%, and achieving up to 1.7x faster real-time factor with less\nthan 5% of the original training data. These results show that compact LLM-TTS\nmodels can maintain naturalness and speaker similarity while enabling practical\nreal-time speech generation. Audio samples are available at\nhttps://mm.kaist.ac.kr/projects/SPADE/."}
{"id": "2509.21087", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21087", "abs": "https://arxiv.org/abs/2509.21087", "authors": ["Rostislav Makarov", "Lea Schönherr", "Timo Gerkmann"], "title": "Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?", "comment": "Copyright 2026 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Machine learning approaches for speech enhancement are becoming increasingly\nexpressive, enabling ever more powerful modifications of input signals. In this\npaper, we demonstrate that this expressiveness introduces a vulnerability:\nadvanced speech enhancement models can be susceptible to adversarial attacks.\nSpecifically, we show that adversarial noise, carefully crafted and\npsychoacoustically masked by the original input, can be injected such that the\nenhanced speech output conveys an entirely different semantic meaning. We\nexperimentally verify that contemporary predictive speech enhancement models\ncan indeed be manipulated in this way. Furthermore, we highlight that diffusion\nmodels with stochastic samplers exhibit inherent robustness to such adversarial\nattacks by design."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 12]
- [cs.SD](#cs.SD) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Real-Time Markov Modeling for Single-Photon LiDAR: $1000 \times$ Acceleration and Convergence Analysis](https://arxiv.org/abs/2509.20500)
*Weijian Zhang,Hashan K. Weerasooriya,Prateek Chennuri,Stanley H. Chan*

Main category: eess.SP

TL;DR: 本文提出了首个非顺序马尔可夫建模方法，用于分析带死区时间的异步单光子激光雷达时间戳分布，通过重新参数化积分边界将死区时间效应分离为基矩阵的确定性行置换，实现了高达1000倍的加速。


<details>
  <summary>Details</summary>
Motivation: 异步单光子激光雷达在存在死区时间时的时间戳分布建模是一个极具挑战性的开放问题，现有方法构建大型转移矩阵计算成本高昂。

Method: 提出等效公式重新参数化积分边界，将死区时间效应分离为基矩阵的确定性行置换，实现解耦和高效向量化矩阵构建。

Result: 新模型与蒙特卡洛模拟金标准相比产生近乎精确的稳态分布，同时计算时间大幅减少，实现了1000倍的加速。

Conclusion: 新方法显著提升了计算效率，同时新的理论分析揭示了第二大特征值的幅度和相位对收敛性的关键影响，这是文献中常被忽视的重要因素。

Abstract: Asynchronous single-photon LiDAR (SP-LiDAR) is an important imaging modality
for high-quality 3D applications and navigation, but the modeling of the
timestamp distributions of a SP-LiDAR in the presence of dead time remains a
very challenging open problem. Prior works have shown that timestamps form a
discrete-time Markov chain, whose stationary distribution can be computed as
the leading left eigenvector of a large transition matrix. However,
constructing this matrix is known to be computationally expensive because of
the coupling between states and the dead time. This paper presents the first
non-sequential Markov modeling for the timestamp distribution. The key
innovation is an equivalent formulation that reparameterizes the integral
bounds and separates the effect of dead time as a deterministic row permutation
of a base matrix. This decoupling enables efficient vectorized matrix
construction, yielding up to $1000 \times$ acceleration over existing methods.
The new model produces a nearly exact stationary distribution when compared
with the gold standard Monte Carlo simulations, yet using a fraction of the
time. In addition, a new theoretical analysis reveals the impact of the
magnitude and phase of the second-largest eigenvalue, which are overlooked in
the literature but are critical to the convergence.

</details>


### [2] [Wireless Powered MEC Systems via Discrete Pinching Antennas: TDMA versus NOMA](https://arxiv.org/abs/2509.20908)
*Peng Liu,Zesong Fei,Meng Hua,Guangji Chen,Xinyi Wang,Ruiqi Liu*

Main category: eess.SP

TL;DR: 本文研究了一种实用的离散夹持天线辅助无线供电移动边缘计算框架，通过联合优化能量收集和任务卸载来最大化总计算比特数，比较了TDMA和NOMA方案在不同PA激活灵活性下的性能。


<details>
  <summary>Details</summary>
Motivation: 夹持天线能够创建视距链路并减轻大规模路径损耗，将其集成到无线供电MEC系统中可以提升能量传输和任务卸载效率。现有研究假设理想的连续PA放置，而本文关注更实际的离散PA配置。

Method: 提出离散PA辅助无线供电MEC框架，设备先收集PA发射的射频信号能量，然后采用部分卸载模式。考虑TDMA和NOMA两种多址接入方案，每种方案在三种PA激活灵活性下进行研究。开发了两层算法，结合KKT条件的闭式解和基于交叉熵的学习方法。

Result: 数值结果验证了所提设计在能量收集和计算性能方面的优越性。在较粗的PA激活级别下，TDMA和NOMA性能相当；而在更细的激活粒度下，TDMA能够实现优于NOMA的计算性能。

Conclusion: 离散PA辅助无线供电MEC系统能够有效提升性能，TDMA和NOMA在不同PA激活粒度下各有优势，所提出的优化算法能够有效解决混合整数非线性问题。

Abstract: Pinching antennas (PAs), a new type of reconfigurable and flexible antenna
structures, have recently attracted significant research interest due to their
ability to create line-of-sight links and mitigate large-scale path loss. Owing
to their potential benefits, integrating PAs into wireless powered mobile edge
computing (MEC) systems is regarded as a viable solution to enhance both energy
transfer and task offloading efficiency. Unlike prior studies that assume ideal
continuous PA placement along waveguides, this paper investigates a practical
discrete PA-assisted wireless powered MEC framework, where devices first
harvest energy from PA-emitted radio-frequency signals and then adopt a partial
offloading mode, allocating part of the harvested energy to local computing and
the remainder to uplink offloading. The uplink phase considers both the
time-division multiple access (TDMA) and non-orthogonal multiple access (NOMA),
each examined under three levels of PA activation flexibility. For each
configuration, we formulate a joint optimization problem to maximize the total
computational bits and conduct a theoretical performance comparison between the
TDMA and NOMA schemes. To address the resulting mixed-integer nonlinear
problems, we develop a two-layer algorithm that combines closed-form solutions
based on Karush-Kuhn-Tucker (KKT) conditions with a cross-entropy-based
learning method. Numerical results validate the superiority of the proposed
design in terms of the harvested energy and computation performance, revealing
that TDMA and NOMA achieve comparable performance under coarser PA activation
levels, whereas finer activation granularity enables TDMA to achieve superior
computation performance over NOMA.

</details>


### [3] [A General Optimization Framework for Movable Antenna Systems via Discrete Sampling](https://arxiv.org/abs/2509.20987)
*Changhao Liu,Weidong Mei,Zhi Chen,Jun Fang,Boyu Ning*

Main category: eess.SP

TL;DR: 提出了一种用于可移动天线位置优化的通用低复杂度框架，通过离散化天线移动区域并采用多轮更新和Gibbs采样来避免局部最优，在MA增强广播系统中实现了接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 可移动天线系统能够通过局部天线移动重塑无线信道，但天线位置与无线信道之间的高度非线性关系使得优化变得困难，现有方法存在高计算复杂度或陷入局部最优的问题。

Method: 将天线移动区域离散化为采样点集，将连续优化问题转化为离散点选择问题；通过多轮顺序更新每个MA的最优采样点；在轮次间引入Gibbs采样阶段以探索相邻和随机生成的候选解。

Result: 数值结果表明，所提算法实现了接近最优的性能，并显著优于现有基准方法。

Conclusion: 该框架为MA位置优化提供了一种通用且低复杂度的解决方案，能够有效避免局部最优并实现优异的通信性能。

Abstract: Movable antenna (MA) systems have attracted growing interest in wireless
communications due to their ability to reshape wireless channels via local
antenna movement within a confined region. However, optimizing antenna
positions to enhance communication performance turns out to be challenging due
to the highly nonlinear relationship between wireless channels and antenna
positions. Existing approaches, such as gradient-based and heuristic
algorithms, often suffer from high computational complexity or undesired local
optima. To address the above challenge, this letter proposes a general and
low-complexity optimization framework for MA position optimization.
Specifically, we discretize the antenna movement region into a set of sampling
points, thereby transforming the continuous optimization problem into a
discrete point selection problem. Next, we sequentially update the optimal
sampling point for each MA over multiple rounds. To avoid convergence to poor
local optima, a Gibbs sampling (GS) phase is introduced between rounds to
explore adjacent and randomly generated candidate solutions. As a case study,
we investigate joint precoding and antenna position optimization for an
MA-enhanced broadcast system by applying the proposed framework. Numerical
results demonstrate that the proposed algorithm achieves near-optimal
performance and significantly outperforms existing benchmarks.

</details>


### [4] [Shapley Features for Robust Signal Prediction in Tactile Internet](https://arxiv.org/abs/2509.21032)
*Mohammad Ali Vahedifar,Qi Zhang*

Main category: eess.SP

TL;DR: 提出了一种结合高斯过程和ResNet神经网络的新预测框架，通过Shapley特征值进行特征选择，在触觉互联网中实现95.72%的准确率，比现有方法提升11.1%，同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 触觉互联网需要超低延迟和可靠的触觉信号传输，但数据包丢失和延迟问题仍未解决。

Method: 集成高斯过程和ResNet神经网络，其中高斯过程作为预言机来恢复丢失或严重延迟的信号，并引入Shapley特征值进行特征选择。

Result: 达到95.72%的准确率，比最先进的LeFo方法提升11.1%；与LeFo结合时推理时间减少27%，与高斯过程结合时减少72%。

Conclusion: GP+SFV框架既是高精度又是高效率的解决方案，为触觉互联网中实用可靠的触觉通信铺平了道路。

Abstract: The Tactile Internet (TI) requires ultra-low latency and reliable haptic
signal transmission, yet packet loss and delay remain unresolved challenges. We
present a novel prediction framework that integrates Gaussian Processes (GP)
with a ResNet-based Neural Network, where GP acts as an oracle to recover
signals lost or heavily delayed. To further optimize performance, we introduce
Shapley Feature Values (SFV), a principled feature selection mechanism that
isolates the most informative inputs for prediction. This GP+SFV framework
achieves 95.72% accuracy, surpassing the state-of-the-art LeFo method by 11.1%,
while simultaneously relaxing TI's rigid delay constraints. Beyond accuracy,
SFV operates as a modular accelerator: when paired with LeFo, it reduces
inference time by 27%, and when paired with GP, by 72%. These results establish
GP+SFV as both a high-accuracy and high-efficiency solution, paving the way for
practical and reliable haptic communications in TI systems.

</details>


### [5] [Neural Integrated Sensing and Communication for the MIMO-OFDM Downlink](https://arxiv.org/abs/2509.21118)
*Ziyi Wang,Frederik Zumegen,Christoph Studer*

Main category: eess.SP

TL;DR: 提出了一种基于MIMO-OFDM下行链路的神经ISAC信号处理框架，能够在无需修改通信链路的情况下实现广义感知功能，通过测量反向散射信号生成空间占用地图。


<details>
  <summary>Details</summary>
Motivation: 无线感知与通信应用在频谱和硬件需求上的融合推动了ISAC范式的发展，需要在不改变现有通信基础设施的情况下添加感知能力。

Method: 基于MIMO-OFDM下行链路，测量反向散射通信信号，将空间占用表示为多类或多标签分类问题，并设计专门特征来减轻强反射路径的影响。

Result: 通过射线追踪模型的广泛仿真表明，该框架能够可靠地重建场景地图，且不改变MIMO-OFDM通信流程或降低数据速率。

Conclusion: 该神经ISAC框架为下一代网络提供了有效的集成感知与通信解决方案，能够在保持通信性能的同时实现环境感知功能。

Abstract: The ongoing convergence of spectrum and hardware requirements for wireless
sensing and communication applications has fueled the integrated sensing and
communication (ISAC) paradigm in next-generation networks. Neural-network-based
ISAC leverages data-driven learning techniques to add sensing capabilities to
existing communication infrastructure. This paper presents a novel
signal-processing framework for such neural ISAC systems based on the
multiple-input multiple-output (MIMO) and orthogonal frequency-division
multiplexing (OFDM) downlink. Our approach enables generalized sensing
functionality without modifying the MIMO-OFDM communication link. Specifically,
our neural ISAC pipeline measures the backscattered communication signals to
generate discrete map representations of spatial occupancy, formulated as
multiclass or multilabel classification problems, which can then be utilized by
specialized downstream tasks. To improve sensing performance in closed or
cluttered environments, our neural ISAC pipeline relies on features
specifically designed to mitigate strong reflective paths. Extensive
simulations using ray-tracing models demonstrate that our neural ISAC framework
reliably reconstructs scene maps without altering the MIMO-OFDM communication
pipeline or reducing data rates.

</details>


### [6] [A Secure ISAC Waveform Design Framework via Random Frequency and PRI Agility](https://arxiv.org/abs/2509.21162)
*Ali Khandan Boroujeni,Hyeon Seok Rou,Ghazal Bagheri,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Kuranage Roche Rayan Ranasinghe,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一种集成感知与通信(ISAC)系统的新框架，采用随机频率和脉冲重复间隔捷变(RFPA)方法进行波形设计，结合混合信息嵌入方案提高数据吞吐量，并通过模糊函数分析展示了优异的距离-速度分辨率和杂波抑制性能。


<details>
  <summary>Details</summary>
Motivation: 提升ISAC系统的安全性、数据速率和感知性能，通过随机化关键雷达参数来防止被动对手的侦察，同时增强数据传输能力。

Method: 使用RFPA方法进行波形设计，随机序列由共享密钥控制；提出混合信息嵌入方案，整合ASK、PSK、索引调制和空间调制；设计低复杂度稀疏匹配滤波器接收器进行解码。

Result: 该方法能有效模糊多普勒频率和脉冲起始时间等关键参数，显著阻碍无密钥被动对手的侦察能力；通过模糊函数分析验证了优异的距离-速度分辨率和杂波抑制性能。

Conclusion: 所提出的框架在安全性、数据速率和感知性能方面均有显著提升，为ISAC系统提供了一种有效的解决方案。

Abstract: This paper presents a novel framework for enhancing the security, data rate,
and sensing performance of integrated sensing and communications (ISAC)
systems. We employ a random frequency and pulse repetition interval (PRI)
agility (RFPA) method for the waveform design, where the necessary random
sequences are governed by shared secrets. These secrets, which can be
pre-shared or generated via channel reciprocity, obfuscate critical radar
parameters like Doppler frequency and pulse start times, thereby significantly
impeding the ability to perform reconnaissance from a passive adversary without
the secret key. To further introduce enhanced data throughput, we also
introduce a hybrid information embedding scheme that integrates amplitude shift
keying (ASK), phase shift keying (PSK), index modulation (IM), and spatial
modulation (SM), for which a low-complexity sparse-matched filter receiver is
proposed for accurate decoding with practical complexity. Finally, the
excellent range-velocity resolution and clutter suppression of the proposed
waveform are analyzed via the ambiguity function (AF).

</details>


### [7] [Adversarially Robust MIMO Physical Layer Authentication for Non-Stationary Channels](https://arxiv.org/abs/2509.21171)
*Ali Khandan Boroujeni,Ghazal Bagheri,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一个针对非平稳MIMO无线信道的对抗性鲁棒物理层认证框架，结合了序列贝叶斯决策、对比学习深度特征提取和生成对抗建模来模拟自适应欺骗者。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设信道平稳或观测独立，无法有效处理时空相关性、视距阻塞和动态欺骗策略等现实挑战。

Method: 集成序列贝叶斯决策、对比学习深度特征提取和生成对抗建模，使用2状态和3状态隐马尔可夫模型进行性能分析，并提供移动平均在线自适应。

Result: 提供了对数似然比、检测概率和稳态近似的闭式递推，相比经典序列认证方案展现出显著的鲁棒性提升。

Conclusion: 该框架能够有效应对非平稳MIMO信道中的动态欺骗攻击，提供比传统方法更强的认证鲁棒性。

Abstract: We propose an adversarially robust physical layer authentication (AR-PLA)
framework tailored for non-stationary multiple-input multiple-output (MIMO)
wireless channels. The framework integrates sequential Bayesian
decision-making, deep feature extraction via contrastive learning, and
generative adversarial modeling to simulate adaptive spoofers. Unlike
conventional methods that assume stationary channels or independent
observations, our approach explicitly accounts for temporal and spatial
correlations, line-of-sight (LoS) blockages, and dynamic spoofing strategies. A
comprehensive analytical characterization of the authentication performance
using both 2-state and 3-state hidden Markov models (HMMs) with moving-average
online adaptation is also provided, with closed-form recursions for
loglikelihood ratios, detection probabilities, and steady-state approximations,
which demonstrate significant robustness improvement over classical sequential
authentication schemes.

</details>


### [8] [An enhanced statistical feature fusion approach using an improved distance evaluation algorithm and weighted K-nearest neighbor for bearing fault diagnosis](https://arxiv.org/abs/2509.21219)
*Amir Eshaghi Chaleshtori,Abdollah Aghaie*

Main category: eess.SP

TL;DR: 提出了一种改进的距离评估算法结合加权K近邻分类器用于轴承故障诊断，通过多域特征提取和特征选择，在噪声环境中准确识别轴承故障。


<details>
  <summary>Details</summary>
Motivation: 轴承是旋转机械中最易发生故障的部件之一，在噪声环境中从多个传感器收集数据时，需要提取和选择信息丰富的特征来准确诊断轴承故障。

Method: 首先提取时域、频域和时频域的振动统计特征，然后使用改进的距离评估算法为特征分配权重并保留最信息丰富的特征，最后用选定的特征训练加权KNN分类器。

Result: 使用渥太华大学的轴承数据进行验证，结果表明该方法能有效准确地识别轴承故障。

Conclusion: 提出的改进距离评估算法结合加权KNN分类器的方法在轴承故障诊断中表现出良好的性能，能够准确识别故障类型。

Abstract: Bearings are among the most failure-prone components in rotating machinery,
and their condition directly impacts overall performance. Therefore, accurately
diagnosing bearing faults is essential for ensuring system stability. However,
detecting such malfunctions in noisy environments, where data is collected from
multiple sensors, necessitates the extraction and selection of informative
features. This paper proposes an improved distance evaluation algorithm
combined with a weighted K-nearest neighbor (KNN) classifier for bearing fault
diagnosis. The process begins with extracting and integrating statistical
features of vibration across the time, frequency, and time-frequency domains.
Next, the improved distance evaluation algorithm assigns weights to the
extracted features, retaining only the most informative ones by eliminating
insensitive features. Finally, the selected features are used to train the
weighted KNN classifier. To validate the proposed method, we employ bearing
data from the University of Ottawa. The results demonstrate the effectiveness
of our approach in accurately identifying bearing faults.

</details>


### [9] [Vision-Intelligence-Enabled Beam Tracking for Cross-Interface Water-Air Optical Wireless Communications](https://arxiv.org/abs/2509.21290)
*Tianqi Mao,Jiayue Liu,Weijie Liu,Dezhi Zheng,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 本文提出了一种基于视觉和AI的水-空气光无线通信波束跟踪算法，通过CNN-Bi-LSTM结合注意力机制来应对海面波动导致的波束失准问题。


<details>
  <summary>Details</summary>
Motivation: 随着水下监测和矿产勘探等海洋应用的发展，需要实时无线回传大量观测数据。传统窄带声学方法难以满足需求，而水-空气光无线通信面临海面波动导致的动态波束失准挑战。

Method: 建立了水-空气光无线传输的数学信道模型，提出基于视觉的波束跟踪算法，集成CNN和双向LSTM，并加入注意力机制来提取视觉数据中的关键时空特征。

Result: 数值模拟结果显示，该算法在维持接收信号强度和抑制视觉噪声方面优于传统方法，在水-空气光无线通信系统的恶劣条件下表现出良好的鲁棒性。

Conclusion: 所提出的AI驱动波束跟踪算法能够有效应对海面波动带来的挑战，为水-空气光无线通信系统提供了可靠的实时对准解决方案。

Abstract: The escalating development of oceanic applications like underwater
surveillance and mineral exploration, is motivating real-time wireless backhaul
of the considerable observation data. Such prospects can be hardly realized by
the narrowband acoustic approach. Alternatively, optical wireless communication
(OWC) has emerged as a promising solution for maritime and underwater
applications due to its great potential for broadband underwater transmission.
However, the implementations of water-air OWC can be rather challenging,
especially when penetrating the fluctuating interface, where the direction of
refracted signals changes dynamically, causing severe beam misalignment with
airborne stations. This has necessitated real-time transceiver alignment
adaptable to the sophisticated oceanic environment, which has yet to be
addressed. Against this background, this paper establishes a mathematical
channel model for water-air optical wireless transmission across the
fluctuating sea surface. Based on the model, we propose a vision-based beam
tracking algorithm that leverages artificial intelligence (AI) methods for
dynamic channel prediction. The proposed algorithm integrates a convolutional
neural network (CNN) with bi-directional long short-term memory (Bi-LSTM),
which further incorporates the attention mechanism to effectively extract
critical spatio-temporal features from the vision data. The numerical
simulation results show that the proposed algorithm can outperform its
classical counterparts in maintaining receiving signal strength and supressing
the vision noises, which demonstrates its robustness against the the harsh
conditions of water-air OWC systems.

</details>


### [10] [Efficient Digital Methods to Quantify Sensor Output Uncertainty](https://arxiv.org/abs/2509.21311)
*Orestis Kaparounakis,Phillip Stanley-Marbell*

Main category: eess.SP

TL;DR: 研究传感器校准参数量化不确定性对最终测量精度的影响，以热电堆传感器为例，展示了校准相关量的认知不确定性可导致高达5.3°C的绝对误差，并在嵌入式系统中实现了高效的实时不确定性跟踪。


<details>
  <summary>Details</summary>
Motivation: 准确表征传感器输出不确定性对于可靠的数据解释至关重要，需要研究传感器组件有限精度信息对整体测量精度的影响。

Method: 以热电堆传感器为例，分析传感器校准和转换方程如何传播校准参数量化产生的不确定性到最终补偿输出，并在两个商用不确定性跟踪硬件平台上进行原型实现。

Result: 校准相关量的认知不确定性导致传感器输出绝对误差高达5.3°C（相对误差25.7%），在边缘检测应用中可将Canny算子的假阳性边缘降至零，同时保持准确性。硬件原型分别实现16.7mW功耗和42.9倍加速、147.15mW功耗和94.4倍加速。

Conclusion: 传感器校准参数的不确定性会显著影响测量精度，提出的不确定性跟踪方法在实际嵌入式系统中可行，为实时应用铺平了道路。

Abstract: Accurate characterization of sensor output uncertainty is important for
reliable data interpretation in many applications. Here, we investigate the
impact of transducer-level measurement uncertainty on overall sensor
measurement accuracy due to limited-precision information about sensor
components. We explain our method using thermopile-based sensors as an example
class of sensors. We show how sensor calibration and conversion equations,
which are an essential part of all sensing systems, propagate uncertainties
resulting from the quantization of calibration parameters, to the final,
compensated sensor output. The experimental results show that the epistemic
uncertainty of calibration-related quantities leads to absolute error in the
sensor output as high as 5.3 {\deg}C (and relative error as high as 25.7%) for
one commonly-used thermopile sensor. In one instance of using the epistemic
uncertainty information in edge detection, we show reduction of false-positives
edges to zero for the conventional Canny operator, while maintaining accuracy.
We show these ideas are practical and possible on actual embedded sensor
systems by prototyping them on two commercially-available uncertainty tracking
hardware platforms, one with average power dissipation 16.7 mW and 42.9x
speedup compared to the equal-confidence Monte Carlo computation (the status
quo), and the other with average power dissipation 147.15 mW and 94.4x speedup,
paving the way for use in real time.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Data-Efficient ASR Personalization for Non-Normative Speech Using an Uncertainty-Based Phoneme Difficulty Score for Guided Sampling](https://arxiv.org/abs/2509.20396)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Yingqiang Gao*

Main category: eess.AS

TL;DR: 提出了一种基于音素级不确定性的数据高效个性化方法，通过蒙特卡洛Dropout识别困难音素进行针对性过采样，显著提升非规范性语音的ASR准确率。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在处理由脑瘫或结构异常等条件引起的非规范性语音时表现不佳，主要由于高声学变异性和训练数据稀缺。

Method: 使用蒙特卡洛Dropout量化音素级不确定性，识别模型认为最困难的音素，并基于此进行针对性过采样策略。

Result: 在英语和德语数据集上验证，模型不确定性分析与专家临床语言病理学报告中识别的困难音素高度相关，显著提升了ASR准确率。

Conclusion: 该方法提供了一个实用的个性化包容性ASR框架，首次成功将模型不确定性与专家语音难度评估对齐。

Abstract: Automatic speech recognition (ASR) systems struggle with non-normative speech
from individuals with impairments caused by conditions like cerebral palsy or
structural anomalies. The high acoustic variability and scarcity of training
data severely degrade model performance. This work introduces a data-efficient
personalization method that quantifies phoneme-level uncertainty to guide
fine-tuning. We leverage Monte Carlo Dropout to estimate which phonemes a model
finds most difficult and use these estimates for a targeted oversampling
strategy. We validate our method on English and German datasets. Crucially, we
demonstrate that our model-derived uncertainty strongly correlates with
phonemes identified as challenging in an expert clinical logopedic report,
marking, to our knowledge, the first work to successfully align model
uncertainty with expert assessment of speech difficulty. Our results show that
this clinically-validated, uncertainty-guided sampling significantly improves
ASR accuracy, delivering a practical framework for personalized and inclusive
ASR.

</details>


### [12] [Variational Low-Rank Adaptation for Personalized Impaired Speech Recognition](https://arxiv.org/abs/2509.20397)
*Niclas Pokel,Pehuén Moure,Roman Boehringer,Shih-Chii Liu,Yingqiang Gao*

Main category: eess.AS

TL;DR: 提出了一种基于贝叶斯低秩适应的ASR个性化方法，用于数据高效微调，显著提高了对语音障碍患者的语音识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统对非标准语音（如脑瘫、唐氏综合征、中风等导致的语音障碍）识别效果不佳，且收集和标注此类语音数据困难。

Method: 基于贝叶斯低秩适应的ASR个性化方法，在数据稀缺情况下进行高效微调。

Result: 在英语UA-Speech数据集和新的德语BF-Sprache数据集上验证，方法显著提高了对障碍语音的识别准确率。

Conclusion: 该方法在保持数据和标注效率的同时，为构建包容性ASR系统提供了实用路径。

Abstract: Speech impairments resulting from congenital disorders, such as cerebral
palsy, down syndrome, or apert syndrome, as well as acquired brain injuries due
to stroke, traumatic accidents, or tumors, present major challenges to
automatic speech recognition (ASR) systems. Despite recent advancements,
state-of-the-art ASR models like Whisper still struggle with non-normative
speech due to limited training data availability and high acoustic variability.
Moreover, collecting and annotating non-normative speech is burdensome:
speaking is effortful for many affected individuals, while laborious annotation
often requires caregivers familiar with the speaker. This work introduces a
novel ASR personalization method based on Bayesian Low-rank Adaptation for
data-efficient fine-tuning. We validate our method on the English UA-Speech
dataset and a newly collected German speech dataset, BF-Sprache, from a child
with structural speech impairment. The dataset and approach are designed to
reflect the challenges of low-resource settings that include individuals with
speech impairments. Our method significantly improves ASR accuracy for impaired
speech while maintaining data and annotation efficiency, offering a practical
path toward inclusive ASR.

</details>


### [13] [Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction](https://arxiv.org/abs/2509.20410)
*Weijie Wu,Wenhao Guan,Kaidi Wang,Peijie Chen,Zhuanling Zha,Junbo Li,Jun Fang,Lin Li,Qingyang Hong*

Main category: eess.AS

TL;DR: Phoenix-VAD是一个基于LLM的流式语义端点检测模型，通过利用LLM的语义理解能力和滑动窗口训练策略，实现可靠的语义端点检测，支持流式推理。


<details>
  <summary>Details</summary>
Motivation: 现有的口语对话模型缺乏即插即用的全双工语义端点检测模块，阻碍了无缝的音频交互体验。

Method: 利用LLM的语义理解能力，采用滑动窗口训练策略，实现流式语义端点检测。

Result: 在语义完整和不完整的语音场景实验中，Phoenix-VAD都表现出优异且有竞争力的性能。

Conclusion: 该设计使全双工预测模块能够独立于对话模型进行优化，为下一代人机交互提供更可靠和灵活的支持。

Abstract: Spoken dialogue models have significantly advanced intelligent
human\textendash computer interaction, yet they lack a plug\textendash
and\textendash play full\textendash duplex prediction module for semantic
endpoint detection, hindering seamless audio interactions. In this paper, we
introduce Phoenix\textendashVAD, an LLM\textendash based model that enables
streaming semantic endpoint detection. Specifically, Phoenix\textendash VAD
leverages the semantic comprehension capability of the LLM and a sliding window
training strategy to achieve reliable semantic endpoint detection while
supporting streaming inference. Experiments on both semantically complete and
incomplete speech scenarios indicate that Phoenix\textendash VAD achieves
excellent and competitive performance. Furthermore, this design enables the
full\textendash duplex prediction module to be optimized independently of the
dialogue model, providing more reliable and flexible support for
next\textendash generation human\textendash computer interaction.

</details>


### [14] [Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens](https://arxiv.org/abs/2509.20485)
*Ismail Rasim Ulgen,Zongyang Du,Junchen Lu,Philipp Koehn,Berrak Sisman*

Main category: eess.AS

TL;DR: TTScore是一个基于条件预测离散语音token的无参考评估框架，包含TTScore-int评估可懂度和TTScore-pro评估韵律，在多个基准测试中比现有指标与人类评价有更强的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成评估指标在可懂度和韵律评估方面范围有限且与人类感知相关性较弱，WER只提供粗粒度的文本可懂度测量，F0-RMSE等音高指标提供的是狭窄的、依赖参考的韵律视图。

Method: 提出TTScore框架，使用两个基于输入文本的条件序列到序列预测器：TTScore-int通过内容token测量可懂度，TTScore-pro通过韵律token评估韵律。为每个合成语音计算相应token序列的似然度，得到可解释的分数。

Result: 在SOMOS、VoiceMOS和TTSArena基准测试中，TTScore-int和TTScore-pro提供了可靠的方面特定评估，与人类对整体质量的判断相比现有可懂度和韵律指标有更强的相关性。

Conclusion: TTScore框架通过条件预测离散语音token，实现了无参考的、目标明确的语音合成评估，在可懂度和韵律评估方面优于现有指标，与人类感知有更好的相关性。

Abstract: Objective evaluation of synthesized speech is critical for advancing speech
generation systems, yet existing metrics for intelligibility and prosody remain
limited in scope and weakly correlated with human perception. Word Error Rate
(WER) provides only a coarse text-based measure of intelligibility, while
F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent
view of prosody. To address these limitations, we propose TTScore, a targeted
and reference-free evaluation framework based on conditional prediction of
discrete speech tokens. TTScore employs two sequence-to-sequence predictors
conditioned on input text: TTScore-int, which measures intelligibility through
content tokens, and TTScore-pro, which evaluates prosody through prosody
tokens. For each synthesized utterance, the predictors compute the likelihood
of the corresponding token sequences, yielding interpretable scores that
capture alignment with intended linguistic content and prosodic structure.
Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that
TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and
achieve stronger correlations with human judgments of overall quality than
existing intelligibility and prosody-focused metrics.

</details>


### [15] [Real-Time System for Audio-Visual Target Speech Enhancement](https://arxiv.org/abs/2509.20741)
*T. Aleksandra Ma,Sile Yin,Li-Chia Yang,Shuo Zhang*

Main category: eess.AS

TL;DR: RAVEN是一个在CPU上实时运行的音频-视觉语音增强系统，使用预训练的视觉嵌入来编码唇部运动信息，能够泛化处理环境噪声、干扰说话者、瞬态声音甚至歌声。


<details>
  <summary>Details</summary>
Motivation: 填补在CPU硬件上运行的实时交互式音频-视觉语音增强系统的空白，传统音频增强方法在干扰说话者存在时效果有限，而视觉线索（如唇部运动）可以提高鲁棒性。

Method: 使用来自音频-视觉语音识别模型的预训练视觉嵌入来编码唇部运动信息，构建完全在CPU上运行的实时系统。

Result: 系统能够泛化处理各种干扰，包括环境噪声、干扰说话者、瞬态声音和歌声，实现了实时的目标语音增强。

Conclusion: RAVEN成功展示了在CPU硬件上运行的实时音频-视觉语音增强系统的可行性，为实际应用提供了新的可能性。

Abstract: We present a live demonstration for RAVEN, a real-time audio-visual speech
enhancement system designed to run entirely on a CPU. In single-channel,
audio-only settings, speech enhancement is traditionally approached as the task
of extracting clean speech from environmental noise. More recent work has
explored the use of visual cues, such as lip movements, to improve robustness,
particularly in the presence of interfering speakers. However, to our
knowledge, no prior work has demonstrated an interactive system for real-time
audio-visual speech enhancement operating on CPU hardware. RAVEN fills this gap
by using pretrained visual embeddings from an audio-visual speech recognition
model to encode lip movement information. The system generalizes across
environmental noise, interfering speakers, transient sounds, and even singing
voices. In this demonstration, attendees will be able to experience live
audio-visual target speech enhancement using a microphone and webcam setup,
with clean speech playback through headphones.

</details>


### [16] [SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS](https://arxiv.org/abs/2509.20802)
*Tan Dat Nguyen,Jaehun Kim,Ji-Hoon Kim,Shukjae Choi,Youshin Lim,Joon Son Chung*

Main category: eess.AS

TL;DR: SPADE框架通过结构化剪枝和自适应蒸馏来压缩LLM-TTS模型，在保持感知质量的同时显著减少模型大小和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-TTS系统虽然可控性和零样本泛化能力强，但参数量大、延迟高，限制了实际部署。

Method: 结合基于词错误率的层重要性指数指导的剪枝步骤，以及多级知识蒸馏来恢复自回归连贯性。

Result: 在零样本基准测试中，SPADE保持接近的感知质量，同时将Transformer深度减半，VRAM使用减少20%，实时因子提升1.7倍，仅使用不到5%的原始训练数据。

Conclusion: 紧凑的LLM-TTS模型可以在保持自然度和说话人相似度的同时，实现实用的实时语音生成。

Abstract: The goal of this paper is to introduce SPADE, a framework for Structured
Pruning and Adaptive Distillation for Efficient Large Language Model-based
text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability
and zero-shot generalization, but their large parameter counts and high latency
limit real-world deployment. SPADE addresses this by combining (i) a pruning
step guided by a word-error-rate-based layer importance index to remove
non-essential Transformer layers, with (ii) multi-level knowledge distillation
to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves
near-parity perceptual quality while halving Transformer depth, reducing VRAM
usage by up to 20%, and achieving up to 1.7x faster real-time factor with less
than 5% of the original training data. These results show that compact LLM-TTS
models can maintain naturalness and speaker similarity while enabling practical
real-time speech generation. Audio samples are available at
https://mm.kaist.ac.kr/projects/SPADE/.

</details>


### [17] [PAS-SE: Personalized Auxiliary-Sensor Speech Enhancement for Voice Pickup in Hearables](https://arxiv.org/abs/2509.20875)
*Mattes Ohlenbusch,Mikolaj Kegler,Marko Stamenovic*

Main category: eess.AS

TL;DR: 该论文比较了两种解决单通道语音增强中目标说话人歧义问题的策略：个性化语音增强（PSE）和辅助传感器语音增强（AS-SE），并展示了它们的组合（PAS-SE）能提供互补的性能优势。


<details>
  <summary>Details</summary>
Motivation: 在可听设备中进行语音拾取时，单通道方法难以区分目标说话人和干扰说话人，需要解决这种歧义问题。

Method: 比较了两种策略：PSE使用注册语音表示目标说话人，AS-SE使用耳内麦克风作为额外输入。提出了训练时增强方法促进AS-SE系统的跨数据集泛化，并组合两种策略形成PAS-SE。

Result: PAS-SE提供了互补的性能优势，特别是当注册语音通过耳内麦克风录制时。即使使用嘈杂的耳内注册语音，PAS-SE仍能保持对AS-SE系统的性能优势。

Conclusion: 结合个性化语音增强和辅助传感器语音增强的策略能有效解决单通道语音增强中的目标说话人歧义问题，并在跨数据集场景中表现出良好的泛化能力。

Abstract: Speech enhancement for voice pickup in hearables aims to improve the user's
voice by suppressing noise and interfering talkers, while maintaining own-voice
quality. For single-channel methods, it is particularly challenging to
distinguish the target from interfering talkers without additional context. In
this paper, we compare two strategies to resolve this ambiguity: personalized
speech enhancement (PSE), which uses enrollment utterances to represent the
target, and auxiliary-sensor speech enhancement (AS-SE), which uses in-ear
microphones as additional input. We evaluate the strategies on two public
datasets, employing different auxiliary sensor arrays, to investigate their
cross-dataset generalization. We propose training-time augmentations to
facilitate cross-dataset generalization of AS-SE systems. We also show that
combining PSE and AS-SE (PAS-SE) provides complementary performance benefits,
especially when enrollment speech is recorded with the in-ear microphone. We
further demonstrate that PAS-SE personalized with noisy in-ear enrollments
maintains performance benefits over the AS-SE system.

</details>


### [18] [TF-Restormer: Complex Spectral Prediction for Speech Restoration](https://arxiv.org/abs/2509.21003)
*Ui-Hyeop Shin,Jaehyun Ko,Woocheol Jeong,Hyuing-Min Park*

Main category: eess.AS

TL;DR: TF-Restormer是一个编码器-解码器架构，通过时频双路径编码器专注于输入带宽分析，使用频率扩展查询的轻量解码器重建缺失高频带，支持任意输入输出采样率的高效通用语音恢复，无需冗余重采样。


<details>
  <summary>Details</summary>
Motivation: 现实世界语音恢复面临多种失真（削波、带通滤波、数字伪影、噪声、混响）和低采样率的挑战，现有系统要么牺牲信号保真度，要么无法流式处理，且大多假设固定目标采样率需要外部重采样导致冗余计算。

Method: 采用时频双路径编码器分析输入带宽，轻量解码器通过频率扩展查询重建高频；引入共享采样频率无关STFT判别器支持多速率对抗训练；因果时间模块支持流式处理；频率模块注入频谱归纳偏置提升极端退化下的鲁棒性；提出缩放对数频谱损失稳定优化。

Result: 作为跨采样率的单一模型，TF-Restormer在信号保真度和感知质量上持续优于先前系统，流式模式在实时应用中保持竞争力。

Conclusion: TF-Restormer实现了高效通用的语音恢复，支持任意输入输出采样率，在信号保真度和感知质量间取得平衡，流式模式适合实时应用。

Abstract: Speech restoration in real-world conditions is challenging due to compounded
distortions such as clipping, band-pass filtering, digital artifacts, noise,
and reverberation, and low sampling rates. Existing systems, including
vocoder-based approaches, often sacrifice signal fidelity, while diffusion
models remain impractical for streaming. Moreover, most assume a fixed target
sampling rate, requiring external resampling that leads to redundant
computations. We present TF-Restormer, an encoder-decoder architecture that
concentrates analysis on input-bandwidth with a time-frequency dual-path
encoder and reconstructs missing high-frequency bands through a light decoder
with frequency extension queries. It enables efficient and universal
restoration across arbitrary input-output rates without redundant resampling.
To support adversarial training across diverse rates, we introduce a shared
sampling-frequency-independent (SFI) STFT discriminator. TF-Restormer further
supports streaming with a causal time module, and improves robustness under
extreme degradations by injecting spectral inductive bias into the frequency
module. Finally, we propose a scaled log-spectral loss that stabilizes
optimization under severe conditions while emphasizing well-predicted spectral
details. As a single model across sampling rates, TF-Restormer consistently
outperforms prior systems, achieving balanced gains in signal fidelity and
perceptual quality, while its streaming mode maintains competitive
effectiveness for real-time application. Code and demos are available at
https://tf-restormer.github.io/demo.

</details>


### [19] [Measuring Audio's Impact on Correctness: Audio-Contribution-Aware Post-Training of Large Audio Language Models](https://arxiv.org/abs/2509.21060)
*Haolin He,Xingjian Du,Renhe Sun,Zheqi Dai,Yujia Xiao,Mingru Yang,Jiayi Zhou,Xiquan Li,Zhengxi Liu,Zining Liang,Chunyat Wu,Qianhua He,Tan Lee,Xie Chen,Weilong Zheng,Weiqiang Wang,Mark Plumbley,Jian Liu,Qiuqiang Kong*

Main category: eess.AS

TL;DR: 本文提出了AudioMCQ音频多选问答数据集，研究了LALMs中的零音频贡献现象，并开发了两种有效的后训练范式（Weak-to-Strong和Mixed-to-Strong），在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型音频语言模型（LALMs）多阶段后训练中数据分配优化问题，以及缺乏大规模高质量数据集的问题。

Method: 1. 构建AudioMCQ数据集（57.1万样本，带两种思维链标注）；2. 提出音频贡献过滤方法将数据分为弱/强音频贡献子集；3. 开发两种后训练范式：Weak-to-Strong（SFT+RL）和Mixed-to-Strong（SFT+RL）。

Result: 在DCASE 2025音频问答挑战赛中获第一名；在MMAU-test-mini（78.2%）、MMAU（75.6%）、MMAR（67.1%）和MMSU（70.7%）等基准测试中创造了新的最先进性能。

Conclusion: 通过AudioMCQ数据集和提出的训练策略，有效解决了LALMs中的零音频贡献问题，显著提升了模型在音频任务上的性能。

Abstract: Large Audio Language Models (LALMs) represent an important frontier in
multimodal AI, addressing diverse audio tasks. Recently, post-training of LALMs
has received increasing attention due to significant performance improvements
over foundation models. While single-stage post-training such as reinforcement
learning (RL) has demonstrated promising results, multi-stage approaches such
as supervised fine-tuning (SFT) followed by RL remain suboptimal. The
allocation of data across multiple training stages to maximize LALM
capabilities has not been fully explored, and large-scale, high-quality
datasets for such research are also lacking. To address these problems, we
firstly present AudioMCQ, a comprehensive audio multiple-choice question
dataset comprising 571k samples with two kinds of chain-of-thought annotations.
Secondly, we investigate the prevalent zero audio-contribution phenomenon in
LALMs, where models derive correct answers solely from textual information
without processing audio content. We propose Audio-Contribution Filtering to
partition data into weak and strong audio-contribution subsets. Based on these
insights, we develop two effective post-training paradigms: Weak-to-Strong (SFT
on weak audio-contribution data followed by RL on strong audio-contribution
data) and Mixed-to-Strong (SFT on mixed audio-contribution data followed by RL
on strong audio-contribution data). We achieve first place in the DCASE 2025
Audio-Question-Answering challenge by using AudioMCQ. Additionally, leveraging
our dataset with different training strategies, we achieve 78.2\% on
MMAU-test-mini, 75.6\% on MMAU, 67.1\% on MMAR, and 70.7\% on MMSU,
establishing new state-of-the-art performance across these benchmarks.

</details>


### [20] [Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?](https://arxiv.org/abs/2509.21087)
*Rostislav Makarov,Lea Schönherr,Timo Gerkmann*

Main category: eess.AS

TL;DR: 语音增强模型存在对抗攻击漏洞，精心设计的对抗性噪声可被注入，使增强后的语音输出传达完全不同的语义含义。扩散模型因其随机采样器而具有内在鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着语音增强模型表达能力不断增强，这种表达能力也引入了安全漏洞，需要研究对抗攻击的脆弱性。

Method: 通过精心设计并经过心理声学掩蔽的对抗性噪声注入输入信号，测试当代预测性语音增强模型的脆弱性。

Result: 实验验证当代预测性语音增强模型确实可以被这种方式操纵，而扩散模型由于随机采样器设计具有内在鲁棒性。

Conclusion: 语音增强模型的表达能力带来了对抗攻击的安全风险，扩散模型架构提供了对抗此类攻击的天然防御机制。

Abstract: Machine learning approaches for speech enhancement are becoming increasingly
expressive, enabling ever more powerful modifications of input signals. In this
paper, we demonstrate that this expressiveness introduces a vulnerability:
advanced speech enhancement models can be susceptible to adversarial attacks.
Specifically, we show that adversarial noise, carefully crafted and
psychoacoustically masked by the original input, can be injected such that the
enhanced speech output conveys an entirely different semantic meaning. We
experimentally verify that contemporary predictive speech enhancement models
can indeed be manipulated in this way. Furthermore, we highlight that diffusion
models with stochastic samplers exhibit inherent robustness to such adversarial
attacks by design.

</details>


### [21] [Hybrid Real- And Complex-Valued Neural Network Concept For Low-Complexity Phase-Aware Speech Enhancement](https://arxiv.org/abs/2509.21185)
*Luan Vinícius Fiorio,Alex Young,Ronald M. Aarts*

Main category: eess.AS

TL;DR: 提出混合实值和复值神经网络用于语音增强，在相同参数数量下性能优于纯实值或纯复值模型，且计算复杂度显著降低。


<details>
  <summary>Details</summary>
Motivation: 纯实值或纯复值模型在语音增强任务中要么效率低下，要么复杂度过高，需要一种更优的解决方案。

Method: 设计了一种将实值网络扩展为混合网络的方法，比较了卷积和卷积循环架构的实值、复值和混合版本。

Result: 混合网络在相同参数数量下始终优于对应模型，且在乘加运算方面的复杂度显著低于对应模型。

Conclusion: 混合实值和复值神经网络在语音增强任务中提供了更好的性能-复杂度权衡。

Abstract: In this paper, we propose hybrid real- and complex-valued neural networks for
speech enhancement. Real- or complex-valued models are either inefficient or
present high complexity. We devise a straightforward design method for
extending a real-valued network into its hybrid counterpart. Based on speech
intelligibility and quality metrics, we compare the real, complex, and hybrid
versions of a convolutional and a convolutional-recurrent architecture. The
hybrid network consistently outperforms its counterparts with the same number
of parameters. Additionally, the hybrid models' complexity in terms of
multiply-accumulate operations is substantially lower than that of their
counterparts.

</details>


### [22] [MeanSE: Efficient Generative Speech Enhancement with Mean Flows](https://arxiv.org/abs/2509.21214)
*Jiahe Wang,Hongyu Wang,Wei Wang,Lei Yang,Chenda Li,Wangyou Zhang,Lufen Tan,Yanmin Qian*

Main category: eess.AS

TL;DR: 提出MeanSE，一种使用均值流的高效生成式语音增强模型，通过建模平均速度场实现高质量的单次函数评估增强


<details>
  <summary>Details</summary>
Motivation: 基于流的模型需要多次函数评估才能获得稳定满意的性能，导致计算负载高且单次评估性能差

Method: 使用均值流建模平均速度场，实现高质量的单次函数评估语音增强

Result: MeanSE在单次函数评估下显著优于流匹配基线，展现出极好的域外泛化能力

Conclusion: MeanSE是一种高效的生成式语音增强模型，能够在单次函数评估下实现高质量增强并具有良好的泛化性能

Abstract: Speech enhancement (SE) improves degraded speech's quality, with generative
models like flow matching gaining attention for their outstanding perceptual
quality. However, the flow-based model requires multiple numbers of function
evaluations (NFEs) to achieve stable and satisfactory performance, leading to
high computational load and poor 1-NFE performance. In this paper, we propose
MeanSE, an efficient generative speech enhancement model using mean flows,
which models the average velocity field to achieve high-quality 1-NFE
enhancement. Experimental results demonstrate that our proposed MeanSE
significantly outperforms the flow matching baseline with a single NFE,
exhibiting extremely better out-of-domain generalization capabilities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [23] [QAMO: Quality-aware Multi-centroid One-class Learning For Speech Deepfake Detection](https://arxiv.org/abs/2509.20679)
*Duc-Tuan Truong,Tianchi Liu,Ruijie Tao,Junjie Li,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: QAMO是一种质量感知的多中心单类学习方法，通过引入多个质量感知中心来改进语音深度伪造检测，相比传统单中心方法能更好地建模真实语音的类内变异性。


<details>
  <summary>Details</summary>
Motivation: 传统单中心单类学习方法过度简化了真实语音表示，忽略了语音质量等有用线索。语音质量反映了语音的自然度，可以通过现有语音质量评估模型获得。

Method: QAMO扩展了传统单类学习，引入多个质量感知中心，每个中心优化以表示不同的语音质量子空间，并支持多中心集成评分策略。

Result: 使用两个中心分别表示高质量和低质量语音，在In-the-Wild数据集上实现了5.09%的等错误率，优于之前的单类和质量感知系统。

Conclusion: QAMO通过质量感知的多中心方法有效提升了语音深度伪造检测性能，减少了推理时对质量标签的需求。

Abstract: Recent work shows that one-class learning can detect unseen deepfake attacks
by modeling a compact distribution of bona fide speech around a single
centroid. However, the single-centroid assumption can oversimplify the bona
fide speech representation and overlook useful cues, such as speech quality,
which reflects the naturalness of the speech. Speech quality can be easily
obtained using existing speech quality assessment models that estimate it
through Mean Opinion Score. In this paper, we propose QAMO: Quality-Aware
Multi-Centroid One-Class Learning for speech deepfake detection. QAMO extends
conventional one-class learning by introducing multiple quality-aware
centroids. In QAMO, each centroid is optimized to represent a distinct speech
quality subspaces, enabling better modeling of intra-class variability in bona
fide speech. In addition, QAMO supports a multi-centroid ensemble scoring
strategy, which improves decision thresholding and reduces the need for quality
labels during inference. With two centroids to represent high- and low-quality
speech, our proposed QAMO achieves an equal error rate of 5.09% in In-the-Wild
dataset, outperforming previous one-class and quality-aware systems.

</details>


### [24] [Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection](https://arxiv.org/abs/2509.20682)
*Duc-Tuan Truong,Tianchi Liu,Junjie Li,Ruijie Tao,Kong Aik Lee,Eng Siong Chng*

Main category: cs.SD

TL;DR: 提出双路径数据增强训练框架，通过梯度对齐解决原始语音与增强语音在训练中的梯度冲突问题，提升语音深度伪造检测性能


<details>
  <summary>Details</summary>
Motivation: 数据增强在语音深度伪造检测中常用，但原始输入和增强输入的梯度可能不一致，导致参数更新冲突，影响模型收敛和性能

Method: 设计双路径数据增强训练框架，每条训练语音通过两个输入路径处理：原始语音和增强版本，比较并对齐它们的反向传播梯度方向

Result: 分析显示约25%训练迭代存在梯度冲突，通过梯度对齐方法加速收敛，在In-the-Wild数据集上相比基线相对降低18.69%等错误率

Conclusion: 梯度对齐能有效解决数据增强中的梯度冲突问题，提升语音深度伪造检测模型的训练效率和性能

Abstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used
to improve model generalization across varied speech conditions and spoofing
attacks. However, during training, the backpropagated gradients from original
and augmented inputs may misalign, which can result in conflicting parameter
updates. These conflicts could hinder convergence and push the model toward
suboptimal solutions, thereby reducing the benefits of DA. To investigate and
address this issue, we design a dual-path data-augmented (DPDA) training
framework with gradient alignment for SDD. In our framework, each training
utterance is processed through two input paths: one using the original speech
and the other with its augmented version. This design allows us to compare and
align their backpropagated gradient directions to reduce optimization
conflicts. Our analysis shows that approximately 25% of training iterations
exhibit gradient conflicts between the original inputs and their augmented
counterparts when using RawBoost augmentation. By resolving these conflicts
with gradient alignment, our method accelerates convergence by reducing the
number of training epochs and achieves up to an 18.69% relative reduction in
Equal Error Rate on the In-the-Wild dataset compared to the baseline.

</details>


### [25] [AIBA: Attention-based Instrument Band Alignment for Text-to-Audio Diffusion](https://arxiv.org/abs/2509.20891)
*Junyoung Koh,Soo Yong Kim,Gyu Hyeong Choi,Yongwon Choi*

Main category: cs.SD

TL;DR: AIBA是一个轻量级、无需训练的方法，用于量化文本到音频扩散模型在时频平面上的注意力分布，通过记录交叉注意力概率并与音频能量进行比较来评估模型表现。


<details>
  <summary>Details</summary>
Motivation: 需要量化文本到音频扩散模型在时频平面上的注意力分布，以理解模型如何将文本描述映射到音频特征。

Method: 在推理时钩取交叉注意力概率而不修改权重；将注意力概率投影到固定大小的梅尔网格上；使用可解释的指标（时频IoU/AP、频率轮廓相关性和指向游戏）评估与乐器频段真实值的匹配度。

Result: 在Slakh2100数据集和AudioLDM2骨干网络上，AIBA揭示了乐器依赖的一致趋势（如低音偏好低频段），并实现了高精度和中等召回率。

Conclusion: AIBA提供了一种有效的方法来可视化和量化文本到音频扩散模型的注意力机制，揭示了模型在时频平面上的注意力模式与乐器特性的一致性。

Abstract: We present AIBA (Attention-In-Band Alignment), a lightweight, training-free
pipeline to quantify where text-to-audio diffusion models attend on the
time-frequency (T-F) plane. AIBA (i) hooks cross-attention at inference to
record attention probabilities without modifying weights; (ii) projects them to
fixed-size mel grids that are directly comparable to audio energy; and (iii)
scores agreement with instrument-band ground truth via interpretable metrics
(T-F IoU/AP, frequency-profile correlation, and a pointing game). On Slakh2100
with an AudioLDM2 backbone, AIBA reveals consistent instrument-dependent trends
(e.g., bass favoring low bands) and achieves high precision with moderate
recall.

</details>


### [26] [SingVERSE: A Diverse, Real-World Benchmark for Singing Voice Enhancement](https://arxiv.org/abs/2509.20969)
*Shaohan Jiang,Junan Zhang,Yunjia Zhang,Jing Yang,Fan Fan,Zhizheng Wu*

Main category: cs.SD

TL;DR: 本文介绍了SingVERSE，这是首个用于歌声增强的真实世界基准测试，涵盖了多样化的声学场景，并提供了配对的录音室质量干净参考。通过该基准，作者评估了最先进的模型，揭示了感知质量和清晰度之间的权衡，并展示了使用领域内歌唱数据进行训练可以显著提升增强性能。


<details>
  <summary>Details</summary>
Motivation: 歌声增强的发展受到缺乏现实评估数据的限制，为了解决这一差距，需要建立一个真实世界的基准测试来推动该领域的研究。

Method: 作者开发了SingVERSE基准测试，包含多样化的声学场景和配对的干净参考，并利用该基准对最先进的模型进行了全面评估。

Result: 评估揭示了感知质量和清晰度之间的一致权衡，并证明使用领域内歌唱数据进行训练可以显著改善增强性能，同时不降低语音能力。

Conclusion: 这项工作为社区提供了一个基础基准测试和关键见解，以指导这个未充分探索领域的未来发展。

Abstract: This paper presents a benchmark for singing voice enhancement. The
development of singing voice enhancement is limited by the lack of realistic
evaluation data. To address this gap, this paper introduces SingVERSE, the
first real-world benchmark for singing voice enhancement, covering diverse
acoustic scenarios and providing paired, studio-quality clean references.
Leveraging SingVERSE, we conduct a comprehensive evaluation of state-of-the-art
models and uncover a consistent trade-off between perceptual quality and
intelligibility. Finally, we show that training on in-domain singing data
substantially improves enhancement performance without degrading speech
capabilities, establishing a simple yet effective path forward. This work
offers the community a foundational benchmark together with critical insights
to guide future advances in this underexplored domain. Demopage:
https://singverse.github.io

</details>


### [27] [i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for Agents](https://arxiv.org/abs/2509.20971)
*Anupam Purwar,Aditya Choudhary*

Main category: cs.SD

TL;DR: 该论文研究了如何优化端到端语音到语音通信系统的实时性能，重点关注TTS组件对实时因子的影响，通过减少RVQ迭代次数和码本数量来降低延迟。


<details>
  <summary>Details</summary>
Motivation: 优化语音到语音通信系统的实时性能，使其更适合实时对话应用，同时保持高质量的交互体验。

Method: 分析ASR、TTS和对话管理等V-2-V系统组件，实验使用CSM1b架构，通过优化TTS解码器中的RVQ迭代次数和码本数量来减少处理时间。

Result: 发现TTS组件对实时因子影响最大，减少RVQ迭代次数和码本数量能显著优化系统性能，但会牺牲一定的语音质量。

Conclusion: 基于CSM的V-2-V系统最重要的优化手段是减少RVQ迭代次数和码本数量，这能有效降低延迟并提高实时性能。

Abstract: We experiment with a low-latency, end-to-end voice-to-voice communication
model to optimize it for real-time conversational applications. By analyzing
components essential to voice to voice (V-2-V) system viz. automatic speech
recognition (ASR), text-to-speech (TTS), and dialog management, our work
analyzes how to reduce processing time while maintaining high-quality
interactions to identify the levers for optimizing V-2-V system. Our work
identifies that TTS component which generates life-like voice, full of emotions
including natural pauses and exclamations has highest impact on Real time
factor (RTF). The experimented V-2-V architecture utilizes CSM1b has the
capability to understand tone as well as context of conversation by ingesting
both audio and text of prior exchanges to generate contextually accurate
speech. We explored optimization of Residual Vector Quantization (RVQ)
iterations by the TTS decoder which come at a cost of decrease in the quality
of voice generated. Our experimental evaluations also demonstrate that for
V-2-V implementations based on CSM most important optimizations can be brought
by reducing the number of RVQ Iterations along with the codebooks used in Mimi.

</details>


### [28] [SupCLAP: Controlling Optimization Trajectory Drift in Audio-Text Contrastive Learning with Support Vector Regularization](https://arxiv.org/abs/2509.21033)
*Jiehui Luo,Yuguo Yin,Yuxin Xie,Jinghan Ru,Xianwei Zhuang,Minghua He,Aofan Liu,Zihan Xiong,Dongchao Yang*

Main category: cs.SD

TL;DR: 本文提出支持向量正则化(SVR)方法，通过引入辅助支持向量来控制对比学习中负样本推力垂直分量的影响，既利用其丰富信息又缓解优化轨迹漂移问题。


<details>
  <summary>Details</summary>
Motivation: 对比语言-音频预训练中，负样本推力的垂直分量虽然包含丰富补充信息，但其不受约束的特性会导致优化轨迹漂移和训练不稳定性。

Method: 提出SVR方法，引入辅助支持向量控制垂直分量；探索两种无监督语义半径建模策略：直接参数化和带约束的自适应半径预测器模块。

Result: 实验结果表明，SVR在音频-文本数据集上的分类、单语检索和多语检索任务中，超越了InfoNCE和SigLIP等基线方法。

Conclusion: 理论分析和优化轨迹漂移实验验证了SVR方法的正确性和有效性。

Abstract: Contrastive language-audio pretraining, which aims to unify multimodal
representations in a shared embedding space, serves as a cornerstone for
building a wide range of applications, from cross-modal retrieval to
cutting-edge multimodal large language models. However, we find that the
perpendicular component of the pushing force from negative samples in
contrastive learning is a double-edged sword: it contains rich supplementary
information from negative samples, yet its unconstrained nature causes
optimization trajectory drift and training instability. To address this, we
propose Support Vector Regularization (SVR), a method that introduces an
auxiliary support vector to control this perpendicular component, aiming to
harness its rich information while mitigating the associated trajectory drift.
The efficacy of SVR is critically governed by its semantic radius, for which we
explore two unsupervised modeling strategies: direct parameterization and an
adaptive radius predictor module enhanced with constraints to improve its
predicting accuracy. Extensive experimental results demonstrate that our method
surpasses widely used baselines like InfoNCE and SigLIP loss across
classification, monolingual retrieval, and multilingual retrieval on standard
audio-text datasets. Both the theoretical analysis and the experimental results
on optimizing trajectory drift validate the correctness and effectiveness of
our SVR method.

</details>


### [29] [UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice](https://arxiv.org/abs/2509.21144)
*Sitong Cheng,Weizhen Bian,Xinsheng Wang,Ruibin Yuan,Jianyi Chen,Shunshun Yin,Yike Guo,Wei Xue*

Main category: cs.SD

TL;DR: UniSS是一个单阶段表达性语音到语音翻译框架，通过精心设计的语音语义和风格建模，结合文本大语言模型，解决了表达性S2ST的数据稀缺、处理流程复杂和翻译能力迁移困难等挑战。


<details>
  <summary>Details</summary>
Motivation: 表达性语音到语音翻译面临三个主要挑战：保留表达风格的配对语音数据稀缺、多阶段处理流程复杂、以及大语言模型的翻译能力难以迁移到语音领域。

Method: 提出单阶段框架UniSS，设计了语音语义和风格建模，与文本LLM框架集成构建统一文本-语音语言模型。采用跨模态思维链提示过程，逐步对齐音频语义与文本，确保解码结果中的风格保留。

Result: 构建并发布了大规模高质量表达性S2ST数据集UniST（44.8k小时数据）。实验结果显示UniSS在翻译保真度和语音质量方面显著优于先前方法，同时保持了声音、情感和时长一致性。

Conclusion: UniSS为构建下一代表达性S2ST系统建立了更简单有效的范式，显著提升了翻译性能和风格保持能力。

Abstract: The ultimate goal of expressive speech-to-speech translation (S2ST) is to
accurately translate spoken content while preserving the speaker identity and
emotional style. However, progress in this field is largely hindered by three
key challenges: the scarcity of paired speech data that retains expressive
styles, the complexity of multi-stage processing pipelines, and the limited
transfer of translation capabilities from large language models (LLMs). In this
work, we address these challenges by introducing UniSS, a novel single-stage
framework for expressive S2ST. Our approach features carefully designed speech
semantic and style modeling, enabling seamless integration with existing
text-based LLM frameworks to develop a unified text-speech language model. To
transfer translation capabilities from text to speech, we propose a cross-modal
chain-of-thought prompting process that progressively aligns audio semantics
with text and ensures style preservation in the decoded results. Furthermore,
we construct and release a large-scale, high-quality expressive S2ST dataset,
UniST, comprising 44.8k hours of data. Experimental results show that UniSS
significantly outperforms previous methods in translation fidelity and speech
quality while preserving voice, emotion, and duration consistency. Our work
establishes a simpler and more effective paradigm for building the next
generation of expressive S2ST systems. Audio samples are available at
https://cmots.github.io/uniss-demo.

</details>

{"id": "2509.21872", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.21872", "abs": "https://arxiv.org/abs/2509.21872", "authors": ["Jan C Olivier", "Etienne Barnard"], "title": "Hidden Markov Model Decoding for LDPC Codes", "comment": "11 pages, and 9 figures", "summary": "The paper proposes an iterative Hidden Markov Model (HMM) for decoding a Low\nDensity Parity Check (LDPC) code. It is demonstrated that a first-order HMM\nprovides a natural framework for the decoder. The HMM is time-homogeneous with\na fixed transition matrix and is based on a random walk through the encoded\nframe bits. Each hidden state contains a pair of two encoded bits, and parity\nchecks are naturally incorporated into the observation model. The paper shows\nthat by implementing a forward-backward smoothing estimator for the hidden\nstates, decoding is efficient and requires only a small number of iterations in\nmost cases. The results show that the LDPC decoding threshold is significantly\nimproved compared to belief propagation (BP) on a Tanner graph. Numerical\nresults are presented showing that LDPC codes under the proposed decoder yield\na frame error rate (FER) and decoding threshold comparable to that of a Polar\ncode where Successive Cancellation List (SCL) - Cyclic Redundancy Check (CRC)\ndecoding is deployed. This is shown to be achieved even if the frame length is\nshort (on the order of $512$ bits or less) and a regular LDPC code is used. 1"}
{"id": "2509.22181", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.22181", "abs": "https://arxiv.org/abs/2509.22181", "authors": ["Haochen Li", "Ruikang Zhong", "Jiayi Lei", "Pan Zhiwen", "Yuanwei Liu"], "title": "CRB minimization for PASS Assisted ISAC", "comment": "6 pages, 5 figures, conference", "summary": "A multiple waveguide PASS assisted integrated sensing and communication\n(ISAC) system is proposed, where the base station (BS) is equipped with\ntransmitting pinching antennas (PAs) and receiving uniform linear array (ULA)\nantennas. The PASS-transmitting-ULA-receiving (PTUR) BS transmits the\ncommunication and sensing signals through the stretched PAs on waveguides and\ncollects the echo sensing signals with the mounted ULA. Based on this\nconfiguration, a target sensing Cramer Rao Bound (CRB) minimization problem is\nformulated under communication quality-of-service (QoS) constraints, power\nbudget constraints, and PA deployment constraints. An alternating optimization\n(AO) method is employed to address the formulated non-convex optimization\nproblem. Simulation results demonstrate that the proposed PASS assisted ISAC\nframework achieves superior performance over benchmark schemes."}
{"id": "2509.22204", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.22204", "abs": "https://arxiv.org/abs/2509.22204", "authors": ["Mohammadhossein Karimi", "Yuanzhe Gong", "Tho Le-Ngoc"], "title": "A Deep Neural Network Codebook Approach for Near-Field Nulling Control Beam Focusing", "comment": "6 pages, 5 figures, Submitted to IEEE International Conference on\n  Communications 2026", "summary": "This paper proposes a deep neural network (DNN) codebook approach for\nmulti-user interference (MUI) mitigation in extremely large multiple-input\nmultiple-output (XL-MIMO) systems operating in the near-field region. Unlike\nexisting DNN-based nulling control beamforming (NCBF) methods that face\nscalability and complexity challenges, the proposed framework partitions the\nFresnel region using correlation-based sampling and assigns a lightweight fully\nconnected DNN model to each subsection. Each model is trained on beamforming\nweights generated using the linearly constrained minimum variance (LCMV)\nmethod, enabling accurate prediction of nulling control beam-focusing weights\nthat simultaneously optimize the desired signal strength and suppress potential\ninterference for both collinear and non-collinear user configurations.\nSimulation results show that the trained models achieve average phase and\nmagnitude prediction errors of 0.085 radians and 0.52 dB, respectively, across\n75 sample subsections. Full-wave simulations in Ansys HFSS further demonstrate\nthat the proposed DNN codebook achieves interference suppression better than\n31.64 dB, with a performance gap within 2 dB of the LCMV method, thereby\nvalidating its effectiveness in mitigating MUI while reducing computational\ncomplexity."}
{"id": "2509.22327", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.22327", "abs": "https://arxiv.org/abs/2509.22327", "authors": ["Zheao Li", "Jiancheng An", "Chau Yuen"], "title": "Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM Communications", "comment": null, "summary": "Leveraging the multilayer realization of programmable metasurfaces, stacked\nintelligent metasurfaces (SIM) enable fine-grained wave-domain control.\nHowever, their wideband deployment is impeded by two structural factors: (i) a\nsingle, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)\nmultiuser scheduling changes the subcarrier activation pattern frame by frame,\nrequiring rapid reconfiguration. To address both challenges, we develop a\nSIM-enhanced wideband multiuser transceiver built on orthogonal\nfrequency-division multiplexing with index modulation (OFDM-IM). The sparse\nactivation of OFDM-IM confines high-fidelity equalization to the active tones,\neffectively widening the usable bandwidth. To make the design\nreliability-aware, we directly target the worst-link bit-error rate (BER) and\nadopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a\nprincipled surrogate, turning the reliability optimization tractable. For\nframe-rate inference and interpretability, we propose an unfolded\nprojected-gradient-descent network (UPGD-Net) that double-unrolls across the\nSIM's layers and algorithmic iterations: each cell computes the analytic\ngradient from the cascaded precoder with a learnable per-iteration step size.\nSimulations on wideband multiuser downlinks show fast, monotone convergence, an\nevident layer-depth sweet spot, and consistent gains in worst-link BER and sum\nrate. By combining structural sparsity with a BER-driven, deep-unfolded\noptimization backbone, the proposed framework directly addresses the key\nwideband deficiencies of SIM."}
{"id": "2509.21381", "categories": ["eess.AS", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.21381", "abs": "https://arxiv.org/abs/2509.21381", "authors": ["Guandong Pan", "Yaqian Yang", "Shi Chen", "Xin Wang", "Longzhao Liu", "Hongwei Zheng", "Shaoting Tang"], "title": "Toward a Realistic Encoding Model of Auditory Affective Understanding in the Brain", "comment": null, "summary": "In affective neuroscience and emotion-aware AI, understanding how complex\nauditory stimuli drive emotion arousal dynamics remains unresolved. This study\nintroduces a computational framework to model the brain's encoding of\nnaturalistic auditory inputs into dynamic behavioral/neural responses across\nthree datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological\nprinciples of parallel auditory hierarchy, we decompose audio into multilevel\nauditory features (through classical algorithms and wav2vec 2.0/Hubert) from\nthe original and isolated human voice/background soundtrack elements, mapping\nthem to emotion-related responses via cross-dataset analyses. Our analysis\nreveals that high-level semantic representations (derived from the final layer\nof wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming\nlow-level acoustic features with significantly stronger mappings to behavioral\nannotations and dynamic neural synchrony across most brain regions ($p <\n0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing\nacoustic-semantic information) surpass the final layers in emotion induction\nacross datasets. Moreover, human voices and soundtracks show dataset-dependent\nemotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS\nfavors soundtracks due to higher background energy), with neural analyses\nindicating voices dominate prefrontal/temporal activity while soundtracks excel\nin limbic regions. By integrating affective computing and neuroscience, this\nwork uncovers hierarchical mechanisms of auditory-emotion encoding, providing a\nfoundation for adaptive emotion-aware systems and cross-disciplinary\nexplorations of audio-affective interactions."}
{"id": "2509.21428", "categories": ["cs.SD", "eess.AS", "00A65"], "pdf": "https://arxiv.org/pdf/2509.21428", "abs": "https://arxiv.org/abs/2509.21428", "authors": ["Yusuke Imai"], "title": "Golden Tonnetz", "comment": "15 pages, 11 figures", "summary": "Musical concepts have been represented by geometry with tones. For example,\nin the chromatic circle, the twelve tones are represented by twelve points on a\ncircle, and in Tonnetz, the relationships among harmonies are represented by a\ntriangular lattice. Recently, we have shown that several arrangements of tones\non the regular icosahedron can be associated with chromatic scales, whole-tone\nscales, major tones, and minor tones through the golden ratio. Here, we\ninvestigate another type of connection between music and the golden ratio. We\nshow that there exists an arrangement of 7 tones on a golden triangle that can\nrepresent a given major/minor scale and its tonic, dominant, and subdominant\nchords by golden triangles. By applying this finding, we propose \"golden\nTonnetz\" which represents all the major/minor scales and triads by the golden\ntriangles or gnomons and also represents relative, parallel, and leading-tone\nexchange transformations in Neo-Riemannian theory by transformations among the\ngolden triangles and gnomons."}
{"id": "2509.22396", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.22396", "abs": "https://arxiv.org/abs/2509.22396", "authors": ["Yuhao Chen", "Boxiang He", "Shilian Wang", "Jing Lei"], "title": "Specific multi-emitter identification via multi-label learning", "comment": null, "summary": "Specific emitter identification leverages hardware-induced impairments to\nuniquely determine a specific transmitter. However, existing approaches fail to\naddress scenarios where signals from multiple emitters overlap. In this paper,\nwe propose a specific multi-emitter identification (SMEI) method via\nmulti-label learning to determine multiple transmitters. Specifically, the\nmulti-emitter fingerprint extractor is designed to mitigate the mutual\ninterference among overlapping signals. Then, the multi-emitter decision maker\nis proposed to assign the all emitter identification using the previous\nextracted fingerprint. Experimental results demonstrate that, compared with\nbaseline approach, the proposed SMEI scheme achieves comparable identification\naccuracy under various overlapping conditions, while operating at significantly\nlower complexity. The significance of this paper is to identify multiple\nemitters from overlapped signal with a low complexity."}
{"id": "2509.21382", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21382", "abs": "https://arxiv.org/abs/2509.21382", "authors": ["Farnaz Jazaeri", "Homayoun Kamkar-Parsi", "François Grondin", "Martin Bouchard"], "title": "Multi-Speaker DOA Estimation in Binaural Hearing Aids using Deep Learning and Speaker Count Fusion", "comment": "5 pages, 2 figures, submitted to IEEE ICASSP 2026", "summary": "For extracting a target speaker voice, direction-of-arrival (DOA) estimation\nis crucial for binaural hearing aids operating in noisy, multi-speaker\nenvironments. Among the solutions developed for this task, a deep learning\nconvolutional recurrent neural network (CRNN) model leveraging spectral phase\ndifferences and magnitude ratios between microphone signals is a popular\noption. In this paper, we explore adding source-count information for\nmulti-sources DOA estimation. The use of dual-task training with joint\nmulti-sources DOA estimation and source counting is first considered. We then\nconsider using the source count as an auxiliary feature in a standalone DOA\nestimation system, where the number of active sources (0, 1, or 2+) is\nintegrated into the CRNN architecture through early, mid, and late fusion\nstrategies. Experiments using real binaural recordings are performed. Results\nshow that the dual-task training does not improve DOA estimation performance,\nalthough it benefits source-count prediction. However, a ground-truth (oracle)\nsource count used as an auxiliary feature significantly enhances standalone DOA\nestimation performance, with late fusion yielding up to 14% higher average\nF1-scores over the baseline CRNN. This highlights the potential of using\nsource-count estimation for robust DOA estimation in binaural hearing aids."}
{"id": "2509.21522", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21522", "abs": "https://arxiv.org/abs/2509.21522", "authors": ["Naisong Zhou", "Saisamarth Rajesh Phaye", "Milos Cernak", "Tijana Stojkovic", "Andy Pearce", "Andrea Cavallaro", "Andy Harper"], "title": "Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training", "comment": "5 pages, 2 figures, submitted to ICASSP2026", "summary": "Diffusion-based generative models have achieved state-of-the-art performance\nfor perceptual quality in speech enhancement (SE). However, their iterative\nnature requires numerous Neural Function Evaluations (NFEs), posing a challenge\nfor real-time applications. On the contrary, flow matching offers a more\nefficient alternative by learning a direct vector field, enabling high-quality\nsynthesis in just a few steps using deterministic ordinary differential\nequation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech\nEnhancement (SFMSE), a novel approach that trains a single, step-invariant\nmodel. By conditioning the velocity field on the target time step during a\none-stage training process, SFMSE can perform single, few, or multi-step\ndenoising without any architectural changes or fine-tuning. Our results\ndemonstrate that a single-step SFMSE inference achieves a real-time factor\n(RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable\nto a strong diffusion baseline requiring 60 NFEs. This work also provides an\nempirical analysis of the role of stochasticity in training and inference,\nbridging the gap between high-quality generative SE and low-latency\nconstraints."}
{"id": "2509.22423", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.22423", "abs": "https://arxiv.org/abs/2509.22423", "authors": ["Marcin Wachowiak", "André Bourdoux", "Sofie Pollin"], "title": "Approximation of the Range Ambiguity Function in Near-field Sensing Systems", "comment": "Submitted to IEEE Transactions on Radar Systems", "summary": "This paper investigates the range ambiguity function of near-field systems\nwhere bandwidth and near-field beamfocusing jointly determine the resolution.\nFirst, the general matched filter ambiguity function is derived and the\nnear-field array factors of different antenna array geometries are introduced.\nNext, the near-field ambiguity function is approximated as a product of the\nrange-dependent near-field array factor and the ambiguity function due to the\nutilized bandwidth and waveform. An approximation criterion based on the\naperture-bandwidth product is formulated, and its accuracy is examined.\nFinally, the improvements to the ambiguity function offered by the near-field\nbeamfocusing, as compared to the far-field case, are presented. The performance\ngains are evaluated in terms of resolution improvement offered by beamfocusing,\npeak-to-sidelobe and integrated-sidelobe level improvement. The gains offered\nby the near-field regime are shown to be range-dependent and substantial only\nin close proximity to the array."}
{"id": "2509.21447", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21447", "abs": "https://arxiv.org/abs/2509.21447", "authors": ["Jihwan Lee", "Sean Foley", "Thanathai Lertpetchpun", "Kevin Huang", "Yoonjeong Lee", "Tiantian Feng", "Louis Goldstein", "Dani Byrd", "Shrikanth Narayanan"], "title": "ARTI-6: Towards Six-dimensional Articulatory Speech Encoding", "comment": null, "summary": "We propose ARTI-6, a compact six-dimensional articulatory speech encoding\nframework derived from real-time MRI data that captures crucial vocal tract\nregions including the velum, tongue root, and larynx. ARTI-6 consists of three\ncomponents: (1) a six-dimensional articulatory feature set representing key\nregions of the vocal tract; (2) an articulatory inversion model, which predicts\narticulatory features from speech acoustics leveraging speech foundation\nmodels, achieving a prediction correlation of 0.87; and (3) an articulatory\nsynthesis model, which reconstructs intelligible speech directly from\narticulatory features, showing that even a low-dimensional representation can\ngenerate natural-sounding speech. Together, ARTI-6 provides an interpretable,\ncomputationally efficient, and physiologically grounded framework for advancing\narticulatory inversion, synthesis, and broader speech technology applications.\nThe source code and speech samples are publicly available."}
{"id": "2509.21544", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21544", "abs": "https://arxiv.org/abs/2509.21544", "authors": ["Jeremy Hyrkas"], "title": "Real-time implementation of vibrato transfer as an audio effect", "comment": "4 pages, 4 figures, ICMC 2025", "summary": "An algorithm for deriving delay functions based on real examples of vibrato\nwas recently introduced and can be used to perform a vibrato transfer, in which\nthe vibrato pattern of a target signal is imparted onto an incoming sound using\na delay line. The algorithm contains methods that computationally restrict a\nreal-time implementation. Here, a real-time approximation is presented that\nincorporates an efficient fundamental frequency estimation algorithm and\ntime-domain polyphase IIR filters that approximate an analytic signal. The\nvibrato transfer algorithm is further supplemented with a proposed method to\ntransfer the amplitude modulation of the target sound, moving this method\nbeyond the capabilities of typical delay-based vibrato effects. Modifications\nto the original algorithm for real-time use are detailed here and available as\nsource code for an implementation as a VST plugin. This algorithm has\napplications as an audio effect in sound design, sound morphing, and real-time\nvibrato control of synthesized sounds."}
{"id": "2509.21463", "categories": ["eess.AS", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21463", "abs": "https://arxiv.org/abs/2509.21463", "authors": ["Vishnu Raj", "Gouthaman KV", "Shiv Gehlot", "Lars Villemoes", "Arijit Biswas"], "title": "Enhanced Generative Machine Listener", "comment": null, "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."}
{"id": "2509.21560", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21560", "abs": "https://arxiv.org/abs/2509.21560", "authors": ["Jeremy Hyrkas", "Pablo Dodero Carrillo", "Teresa Díaz de Cossio Sánchez"], "title": "Preserving Russek's \"Summermood\" Using Reality Check and a DeltaLab DL-4 Approximation", "comment": "6 pages, 10 figures, Pure Data Max Conference 2025", "summary": "As a contribution towards ongoing efforts to maintain electroacoustic\ncompositions for live performance, we present a collection of Pure Data patches\nto preserve and perform Antonio Russek's piece \"Summermood\" for bass flute and\nlive electronics. The piece, originally written for the DeltaLab DL-4 delay\nrack unit, contains score markings specific to the DL-4. Here, we approximate\nthe sound and unique functionality of the DL-4 in Pure Data, then refine our\nimplementation to better match the unit on which the piece was performed by\ncomparing settings from the score to two official recordings of the piece. The\nDL-4 emulation is integrated into a patch for live performance based on the\nNull Piece, and regression tested using the Reality Check framework for Pure\nData. Using this library of patches, Summermood can be brought back into live\nrotation without the use of the now discontinued DL-4. The patches will be\ncontinuously tested to ensure that the piece is playable across computer\nenvironments and as the Pure Data programming language is updated."}
{"id": "2509.21597", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21597", "abs": "https://arxiv.org/abs/2509.21597", "authors": ["Yi Zhu", "Heitor R. Guimarães", "Arthur Pimentel", "Tiago Falk"], "title": "AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit", "comment": null, "summary": "With the prevalence of artificial intelligence (AI)-generated content, such\nas audio deepfakes, a large body of recent work has focused on developing\ndeepfake detection techniques. However, most models are evaluated on a narrow\nset of datasets, leaving their generalization to real-world conditions\nuncertain. In this paper, we systematically review 28 existing audio deepfake\ndatasets and present an open-source benchmarking toolkit called AUDDT\n(https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate\nthe evaluation of pretrained detectors across these 28 datasets, giving users\ndirect feedback on the advantages and shortcomings of their deepfake detectors.\nWe start by showcasing the usage of the developed toolkit, the composition of\nour benchmark, and the breakdown of different deepfake subgroups. Next, using a\nwidely adopted pretrained deepfake detector, we present in- and out-of-domain\ndetection results, revealing notable differences across conditions and audio\nmanipulation types. Lastly, we also analyze the limitations of these existing\ndatasets and their gap relative to practical deployment scenarios."}
{"id": "2509.21625", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21625", "abs": "https://arxiv.org/abs/2509.21625", "authors": ["Zitong Lan", "Yiduo Hao", "Mingmin Zhao"], "title": "Guiding Audio Editing with Audio Language Model", "comment": null, "summary": "Audio editing plays a central role in VR/AR immersion, virtual conferencing,\nsound design, and other interactive media. However, recent generative audio\nediting models depend on template-like instruction formats and are restricted\nto mono-channel audio. These models fail to deal with declarative audio\nediting, where the user declares what the desired outcome should be, while\nleaving the details of editing operations to the system. We introduce SmartDJ,\na novel framework for stereo audio editing that combines the reasoning\ncapability of audio language models with the generative power of latent\ndiffusion. Given a high-level instruction, SmartDJ decomposes it into a\nsequence of atomic edit operations, such as adding, removing, or spatially\nrelocating events. These operations are then executed by a diffusion model\ntrained to manipulate stereo audio. To support this, we design a data synthesis\npipeline that produces paired examples of high-level instructions, atomic edit\noperations, and audios before and after each edit operation. Experiments\ndemonstrate that SmartDJ achieves superior perceptual quality, spatial realism,\nand semantic alignment compared to prior audio editing methods. Demos are\navailable at https://zitonglan.github.io/project/smartdj/smartdj.html."}
{"id": "2509.21676", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21676", "abs": "https://arxiv.org/abs/2509.21676", "authors": ["Aurosweta Mahapatra", "Ismail Rasim Ulgen", "Berrak Sisman"], "title": "HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for Expressive and Emotional Synthetic Speech", "comment": "Submitted to IEEE Transactions on Affective Computing", "summary": "Current anti-spoofing systems remain vulnerable to expressive and emotional\nsynthetic speech, since they rarely leverage prosody as a discriminative cue.\nProsody is central to human expressiveness and emotion, and humans\ninstinctively use prosodic cues such as F0 patterns and voiced/unvoiced\nstructure to distinguish natural from synthetic speech. In this paper, we\npropose HuLA, a two-stage prosody-aware multi-task learning framework for spoof\ndetection. In Stage 1, a self-supervised learning (SSL) backbone is trained on\nreal speech with auxiliary tasks of F0 prediction and voiced/unvoiced\nclassification, enhancing its ability to capture natural prosodic variation\nsimilar to human perceptual learning. In Stage 2, the model is jointly\noptimized for spoof detection and prosody tasks on both real and synthetic\ndata, leveraging prosodic awareness to detect mismatches between natural and\nexpressive synthetic speech. Experiments show that HuLA consistently\noutperforms strong baselines on challenging out-of-domain dataset, including\nexpressive, emotional, and cross-lingual attacks. These results demonstrate\nthat explicit prosodic supervision, combined with SSL embeddings, substantially\nimproves robustness against advanced synthetic speech attacks."}
{"id": "2509.21714", "categories": ["cs.SD", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.21714", "abs": "https://arxiv.org/abs/2509.21714", "authors": ["Xuanchen Wang", "Heng Wang", "Weidong Cai"], "title": "MusicWeaver: Coherent Long-Range and Editable Music Generation from a Beat-Aligned Structural Plan", "comment": "5 pages, 1 figure. demo page: https://musicweaver.github.io/", "summary": "Current music generators capture local textures but often fail to model\nlong-range structure, leading to off-beat outputs, weak section transitions,\nand limited editing capability. We present MusicWeaver, a music generation\nmodel conditioned on a beat-aligned structural plan. This plan serves as an\neditable intermediate between the input prompt and the generated music,\npreserving global form and enabling professional, localized edits. MusicWeaver\nconsists of a planner, which translates prompts into a structural plan encoding\nmusical form and compositional cues, and a diffusion-based generator, which\nsynthesizes music under the plan's guidance. To assess generation and editing\nquality, we introduce two metrics: the Structure Coherence Score (SCS) for\nevaluating long-range form and timing, and the Edit Fidelity Score (EFS) for\nmeasuring the accuracy of realizing plan edits. Experiments demonstrate that\nMusicWeaver achieves state-of-the-art fidelity and controllability, producing\nmusic closer to human-composed works. Music results can be found on our project\npage: https://musicweaver.github.io/."}
{"id": "2509.21867", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21867", "abs": "https://arxiv.org/abs/2509.21867", "authors": ["Sunghwan Ahn", "Jinmo Han", "Beom Jun Woo", "Nam Soo Kim"], "title": "FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement", "comment": null, "summary": "Streaming speech enhancement is a crucial task for real-time applications\nsuch as online meetings, smart home appliances, and hearing aids. Deep neural\nnetwork-based approaches achieve exceptional performance while demanding\nsubstantial computational resources. Although recent neural speech enhancement\nmodels have succeeded in reducing the number of parameters and\nmultiply-accumulate operations, their sophisticated architectures often\nintroduce significant processing latency on common hardware. In this work, we\npropose FastEnhancer, a streaming neural speech enhancement model designed\nexplicitly to minimize real-world latency. It features a simple encoder-decoder\nstructure with efficient RNNFormer blocks. Evaluations on various objective\nmetrics show that FastEnhancer achieves state-of-the-art speech quality and\nintelligibility while simultaneously demonstrating the fastest processing speed\non a single CPU thread. Code and pre-trained weights are publicly available\n(https://github.com/aask1357/fastenhancer)."}
{"id": "2509.21728", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21728", "abs": "https://arxiv.org/abs/2509.21728", "authors": ["Xuechen Liu", "Xin Wang", "Junichi Yamagishi"], "title": "Frustratingly Easy Zero-Day Audio DeepFake Detection via Retrieval Augmentation and Profile Matching", "comment": null, "summary": "Modern audio deepfake detectors using foundation models and large training\ndatasets have achieved promising detection performance. However, they struggle\nwith zero-day attacks, where the audio samples are generated by novel synthesis\nmethods that models have not seen from reigning training data. Conventional\napproaches against such attacks require fine-tuning the detectors, which can be\nproblematic when prompt response is required. This study introduces a\ntraining-free framework for zero-day audio deepfake detection based on\nknowledge representations, retrieval augmentation, and voice profile matching.\nBased on the framework, we propose simple yet effective knowledge retrieval and\nensemble methods that achieve performance comparable to fine-tuned models on\nDeepFake-Eval-2024, without any additional model-wise training. We also conduct\nablation studies on retrieval pool size and voice profile attributes,\nvalidating their relevance to the system efficacy."}
{"id": "2509.21900", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21900", "abs": "https://arxiv.org/abs/2509.21900", "authors": ["Yabo Wang", "Bing Yang", "Xiaofei Li"], "title": "IPDnet2: an efficient and improved inter-channel phase difference estimation network for sound source localization", "comment": null, "summary": "IPDnet is our recently proposed real-time sound source localization network.\nIt employs alternating full-band and narrow-band (B)LSTMs to learn the\nfull-band correlation and narrow-band extraction of DP-IPD, respectively, which\nachieves superior performance. However, processing narrow-band independently\nincurs high computational complexity and the limited scalability of LSTM layers\nconstrains the localization accuracy. In this work, we extend IPDnet to\nIPDnet2, improving both localization accuracy and efficiency. IPDnet2 adapts\nthe oSpatialNet as the backbone to enhance spatial cues extraction and provide\nsuperior scalability. Additionally, a simple yet effective frequency-time\npooling mechanism is proposed to compress frequency and time resolutions and\nthus reduce computational cost, and meanwhile not losing localization\ncapability. Experimental results show that IPDnet2 achieves comparable\nlocalization performance with IPDnet while only requiring less than 2\\% of its\ncomputation cost. Moreover, the proposed network achieves state-of-the-art SSL\nperformance by scaling up the model size while still maintaining relatively low\ncomplexity."}
{"id": "2509.21739", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21739", "abs": "https://arxiv.org/abs/2509.21739", "authors": ["Michael Yeung", "Keisuke Toyama", "Toya Teramoto", "Shusuke Takahashi", "Tamaki Kojima"], "title": "Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription", "comment": null, "summary": "Automatic drum transcription (ADT) is traditionally formulated as a\ndiscriminative task to predict drum events from audio spectrograms. In this\nwork, we redefine ADT as a conditional generative task and introduce\nNoise-to-Notes (N2N), a framework leveraging diffusion modeling to transform\naudio-conditioned Gaussian noise into drum events with associated velocities.\nThis generative diffusion approach offers distinct advantages, including a\nflexible speed-accuracy trade-off and strong inpainting capabilities. However,\nthe generation of binary onset and continuous velocity values presents a\nchallenge for diffusion models, and to overcome this, we introduce an Annealed\nPseudo-Huber loss to facilitate effective joint optimization. Finally, to\naugment low-level spectrogram features, we propose incorporating features\nextracted from music foundation models (MFMs), which capture high-level\nsemantic information and enhance robustness to out-of-domain drum audio.\nExperimental results demonstrate that including MFM features significantly\nimproves robustness and N2N establishes a new state-of-the-art performance\nacross multiple ADT benchmarks."}
{"id": "2509.21968", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21968", "abs": "https://arxiv.org/abs/2509.21968", "authors": ["Yushen Chen", "Kai Hu", "Long Zhou", "Shulin Feng", "Xusheng Yang", "Hangting Chen", "Xie Chen"], "title": "AUV: Teaching Audio Universal Vector Quantization with Single Nested Codebook", "comment": "Submitted to ICASSP 2026", "summary": "We propose AUV, a unified neural audio codec with a single codebook, which\nenables a favourable reconstruction of speech and further extends to general\naudio, including vocal, music, and sound. AUV is capable of tackling any 16 kHz\nmixed-domain audio segment at bit rates around 700 bps. To accomplish this, we\nguide the matryoshka codebook with nested domain-specific partitions, assigned\nwith corresponding teacher models to perform distillation, all in a\nsingle-stage training. A conformer-style encoder-decoder architecture with STFT\nfeatures as audio representation is employed, yielding better audio quality.\nComprehensive evaluations demonstrate that AUV exhibits comparable audio\nreconstruction ability to state-of-the-art domain-specific single-layer\nquantizer codecs, showcasing the potential of audio universal vector\nquantization with a single codebook. The pre-trained model and demo samples are\navailable at https://swivid.github.io/AUV/."}
{"id": "2509.21833", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21833", "abs": "https://arxiv.org/abs/2509.21833", "authors": ["Siyi Zhao", "Wei Wang", "Yanmin Qian"], "title": "Lightweight Front-end Enhancement for Robust ASR via Frame Resampling and Sub-Band Pruning", "comment": "Proceedings of Interspeech", "summary": "Recent advancements in automatic speech recognition (ASR) have achieved\nnotable progress, whereas robustness in noisy environments remains challenging.\nWhile speech enhancement (SE) front-ends are widely used to mitigate noise as a\npreprocessing step for ASR, they often introduce computational non-negligible\noverhead. This paper proposes optimizations to reduce SE computational costs\nwithout compromising ASR performance. Our approach integrates layer-wise frame\nresampling and progressive sub-band pruning. Frame resampling downsamples\ninputs within layers, utilizing residual connections to mitigate information\nloss. Simultaneously, sub-band pruning progressively excludes less informative\nfrequency bands, further reducing computational demands. Extensive experiments\non synthetic and real-world noisy datasets demonstrate that our system reduces\nSE computational overhead over 66 compared to the standard BSRNN, while\nmaintaining strong ASR performance."}
{"id": "2509.22061", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22061", "abs": "https://arxiv.org/abs/2509.22061", "authors": ["Shree Harsha Bokkahalli Satish", "Harm Lameris", "Olivier Perrotin", "Gustav Eje Henter", "Éva Székely"], "title": "Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias", "comment": "6 pages, 1 figure, Submitted to IEEE ICASSP 2026", "summary": "Speech Continuation (SC) is the task of generating a coherent extension of a\nspoken prompt while preserving both semantic context and speaker identity.\nBecause SC is constrained to a single audio stream, it offers a more direct\nsetting for probing biases in speech foundation models than dialogue does. In\nthis work we present the first systematic evaluation of bias in SC,\ninvestigating how gender and phonation type (breathy, creaky, end-creak) affect\ncontinuation behaviour. We evaluate three recent models: SpiritLM (base and\nexpressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality\npreservation, and text-based bias metrics. Results show that while both speaker\nsimilarity and coherence remain a challenge, textual evaluations reveal\nsignificant model and gender interactions: once coherence is sufficiently high\n(for VAE-GSLM), gender effects emerge on text-metrics such as agency and\nsentence polarity. In addition, continuations revert toward modal phonation\nmore strongly for female prompts than for male ones, revealing a systematic\nvoice-quality bias. These findings highlight SC as a controlled probe of\nsocially relevant representational biases in speech foundation models, and\nsuggest that it will become an increasingly informative diagnostic as\ncontinuation quality improves."}
{"id": "2509.21919", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21919", "abs": "https://arxiv.org/abs/2509.21919", "authors": ["Yunyi Liu", "Shaofan Yang", "Kai Li", "Xu Li"], "title": "Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment", "comment": null, "summary": "Human auditory perception is shaped by moving sound sources in 3D space, yet\nprior work in generative sound modelling has largely been restricted to mono\nsignals or static spatial audio. In this work, we introduce a framework for\ngenerating moving sounds given text prompts in a controllable fashion. To\nenable training, we construct a synthetic dataset that records moving sounds in\nbinaural format, their spatial trajectories, and text captions about the sound\nevent and spatial motion. Using this dataset, we train a text-to-trajectory\nprediction model that outputs the three-dimensional trajectory of a moving\nsound source given text prompts. To generate spatial audio, we first fine-tune\na pre-trained text-to-audio generative model to output temporally aligned mono\nsound with the trajectory. The spatial audio is then simulated using the\npredicted temporally-aligned trajectory. Experimental evaluation demonstrates\nreasonable spatial understanding of the text-to-trajectory model. This approach\ncould be easily integrated into existing text-to-audio generative workflow and\nextended to moving sound generation in other spatial audio formats."}
{"id": "2509.22148", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22148", "abs": "https://arxiv.org/abs/2509.22148", "authors": ["Ziyun Cui", "Sike Jia", "Yang Lin", "Yinan Duan", "Diyang Qu", "Runsen Chen", "Chao Zhang", "Chang Lei", "Wen Wu"], "title": "Speaker Anonymisation for Speech-based Suicide Risk Detection", "comment": null, "summary": "Adolescent suicide is a critical global health issue, and speech provides a\ncost-effective modality for automatic suicide risk detection. Given the\nvulnerable population, protecting speaker identity is particularly important,\nas speech itself can reveal personally identifiable information if the data is\nleaked or maliciously exploited. This work presents the first systematic study\nof speaker anonymisation for speech-based suicide risk detection. A broad range\nof anonymisation methods are investigated, including techniques based on\ntraditional signal processing, neural voice conversion, and speech synthesis. A\ncomprehensive evaluation framework is built to assess the trade-off between\nprotecting speaker identity and preserving information essential for suicide\nrisk detection. Results show that combining anonymisation methods that retain\ncomplementary information yields detection performance comparable to that of\noriginal speech, while achieving protection of speaker identity for vulnerable\npopulations."}
{"id": "2509.22060", "categories": ["cs.SD", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22060", "abs": "https://arxiv.org/abs/2509.22060", "authors": ["Aravindhan G", "Yuvaraj Govindarajulu", "Parin Shah"], "title": "Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks", "comment": null, "summary": "Recent studies have demonstrated the vulnerability of Automatic Speech\nRecognition systems to adversarial examples, which can deceive these systems\ninto misinterpreting input speech commands. While previous research has\nprimarily focused on white-box attacks with constrained optimizations, and\ntransferability based black-box attacks against commercial Automatic Speech\nRecognition devices, this paper explores cost efficient white-box attack and\nnon transferability black-box adversarial attacks on Automatic Speech\nRecognition systems, drawing insights from approaches such as Fast Gradient\nSign Method and Zeroth-Order Optimization. Further, the novelty of the paper\nincludes how poisoning attack can degrade the performances of state-of-the-art\nmodels leading to misinterpretation of audio signals. Through experimentation\nand analysis, we illustrate how hybrid models can generate subtle yet impactful\nadversarial examples with very little perturbation having Signal Noise Ratio of\n35dB that can be generated within a minute. These vulnerabilities of\nstate-of-the-art open source model have practical security implications, and\nemphasize the need for adversarial security."}
{"id": "2509.22153", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22153", "abs": "https://arxiv.org/abs/2509.22153", "authors": ["Jialun Li", "Weitao Jiang", "Ziyun Cui", "Yinan Duan", "Diyang Qu", "Chao Zhang", "Runsen Chen", "Chang Lei", "Wen Wu"], "title": "Towards Cross-Task Suicide Risk Detection via Speech LLM", "comment": null, "summary": "Suicide risk among adolescents remains a critical public health concern, and\nspeech provides a non-invasive and scalable approach for its detection.\nExisting approaches, however, typically focus on one single speech assessment\ntask at a time. This paper, for the first time, investigates cross-task\napproaches that unify diverse speech suicide risk assessment tasks within a\nsingle model. Specifically, we leverage a speech large language model as the\nbackbone and incorporate a mixture of DoRA experts (MoDE) approach to capture\ncomplementary cues across diverse assessments dynamically. The proposed\napproach was tested on 1,223 participants across ten spontaneous speech tasks.\nResults demonstrate that MoDE not only achieves higher detection accuracy than\nboth single-task specialised models and conventional joint-tuning approaches,\nbut also provides better confidence calibration, which is especially important\nfor medical detection tasks."}
{"id": "2509.22062", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22062", "abs": "https://arxiv.org/abs/2509.22062", "authors": ["Junjie Cao", "Yichen Han", "Ruonan Zhang", "Xiaoyang Hao", "Hongxiang Li", "Shuaijiang Zhao", "Yue Liu", "Xiao-Ping Zhng"], "title": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling", "comment": "conference paper about TTS", "summary": "Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech\n(TTS) systems, while achieving state-of-the-art quality, still face critical\nchallenges. The foundation of this LLM-based paradigm is the discretization of\nthe continuous speech waveform into a sequence of discrete tokens by neural\naudio codec. However, single codebook modeling is well suited to text LLMs, but\nsuffers from significant information loss; hierarchical acoustic tokens,\ntypically generated via Residual Vector Quantization (RVQ), often lack explicit\nsemantic structure, placing a heavy learning burden on the model. Furthermore,\nthe autoregressive process is inherently susceptible to error accumulation,\nwhich can degrade generation stability. To address these limitations, we\npropose CaT-TTS, a novel framework for robust and semantically-grounded\nzero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that\ninjects explicit linguistic features into its primary codebook via semantic\ndistillation from a state-of-the-art ASR model, providing a structured\nrepresentation that simplifies the learning task. Second, we propose an\n``Understand-then-Generate'' dual-Transformer architecture that decouples\ncomprehension from rendering. An initial ``Understanding'' Transformer models\nthe cross-modal relationship between text and the audio's semantic tokens to\nform a high-level utterance plan. A subsequent ``Generation'' Transformer then\nexecutes this plan, autoregressively synthesizing hierarchical acoustic tokens.\nFinally, to enhance generation stability, we introduce Masked Audio Parallel\nInference (MAPI), a nearly parameter-free inference strategy that dynamically\nguides the decoding process to mitigate local errors."}
{"id": "2509.22167", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22167", "abs": "https://arxiv.org/abs/2509.22167", "authors": ["Zhikang Niu", "Shujie Hu", "Jeongsoo Choi", "Yushen Chen", "Peining Chen", "Pengcheng Zhu", "Yunting Yang", "Bowen Zhang", "Jian Zhao", "Chunhui Wang", "Xie Chen"], "title": "Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis", "comment": "Submitted to ICASSP2026", "summary": "While mel-spectrograms have been widely utilized as intermediate\nrepresentations in zero-shot text-to-speech (TTS), their inherent redundancy\nleads to inefficiency in learning text-speech alignment. Compact VAE-based\nlatent representations have recently emerged as a stronger alternative, but\nthey also face a fundamental optimization dilemma: higher-dimensional latent\nspaces improve reconstruction quality and speaker similarity, but degrade\nintelligibility, while lower-dimensional spaces improve intelligibility at the\nexpense of reconstruction fidelity. To overcome this dilemma, we propose\nSemantic-VAE, a novel VAE framework that utilizes semantic alignment\nregularization in the latent space. This design alleviates the\nreconstruction-generation trade-off by capturing semantic structure in\nhigh-dimensional latent representations. Extensive experiments demonstrate that\nSemantic-VAE significantly improves synthesis quality and training efficiency.\nWhen integrated into F5-TTS, our method achieves 2.10% WER and 0.64 speaker\nsimilarity on LibriSpeech-PC, outperforming mel-based systems (2.23%, 0.60) and\nvanilla acoustic VAE baselines (2.65%, 0.59). We also release the code and\nmodels to facilitate further research."}
{"id": "2509.22317", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22317", "abs": "https://arxiv.org/abs/2509.22317", "authors": ["Jiani Ding", "Qiyang Sun", "Alican Akman", "Björn W. Schuller"], "title": "Cross-Dialect Bird Species Recognition with Dialect-Calibrated Augmentation", "comment": null, "summary": "Dialect variation hampers automatic recognition of bird calls collected by\npassive acoustic monitoring. We address the problem on DB3V, a three-region,\nten-species corpus of 8-s clips, and propose a deployable framework built on\nTime-Delay Neural Networks (TDNNs). Frequency-sensitive normalisation (Instance\nFrequency Normalisation and a gated Relaxed-IFN) is paired with\ngradient-reversal adversarial training to learn region-invariant embeddings. A\nmulti-level augmentation scheme combines waveform perturbations, Mixup for rare\nclasses, and CycleGAN transfer that synthesises Region 2 (Interior\nPlains)-style audio, , with Dialect-Calibrated Augmentation (DCA) softly\ndown-weighting synthetic samples to limit artifacts. The complete system lifts\ncross-dialect accuracy by up to twenty percentage points over baseline TDNNs\nwhile preserving in-region performance. Grad-CAM and LIME analyses show that\nrobust models concentrate on stable harmonic bands, providing ecologically\nmeaningful explanations. The study demonstrates that lightweight, transparent,\nand dialect-resilient bird-sound recognition is attainable."}
{"id": "2509.21428", "categories": ["cs.SD", "eess.AS", "00A65"], "pdf": "https://arxiv.org/pdf/2509.21428", "abs": "https://arxiv.org/abs/2509.21428", "authors": ["Yusuke Imai"], "title": "Golden Tonnetz", "comment": "15 pages, 11 figures", "summary": "Musical concepts have been represented by geometry with tones. For example,\nin the chromatic circle, the twelve tones are represented by twelve points on a\ncircle, and in Tonnetz, the relationships among harmonies are represented by a\ntriangular lattice. Recently, we have shown that several arrangements of tones\non the regular icosahedron can be associated with chromatic scales, whole-tone\nscales, major tones, and minor tones through the golden ratio. Here, we\ninvestigate another type of connection between music and the golden ratio. We\nshow that there exists an arrangement of 7 tones on a golden triangle that can\nrepresent a given major/minor scale and its tonic, dominant, and subdominant\nchords by golden triangles. By applying this finding, we propose \"golden\nTonnetz\" which represents all the major/minor scales and triads by the golden\ntriangles or gnomons and also represents relative, parallel, and leading-tone\nexchange transformations in Neo-Riemannian theory by transformations among the\ngolden triangles and gnomons."}
{"id": "2509.22378", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22378", "abs": "https://arxiv.org/abs/2509.22378", "authors": ["Zijian Zhao", "Dian Jin", "Zijing Zhou"], "title": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach", "comment": null, "summary": "Recently, Image-to-Music (I2M) generation has garnered significant attention,\nwith potential applications in fields such as gaming, advertising, and\nmulti-modal art creation. However, due to the ambiguous and subjective nature\nof I2M tasks, most end-to-end methods lack interpretability, leaving users\npuzzled about the generation results. Even methods based on emotion mapping\nface controversy, as emotion represents only a singular aspect of art.\nAdditionally, most learning-based methods require substantial computational\nresources and large datasets for training, hindering accessibility for common\nusers. To address these challenges, we propose the first Vision Language Model\n(VLM)-based I2M framework that offers high interpretability and low\ncomputational cost. Specifically, we utilize ABC notation to bridge the text\nand music modalities, enabling the VLM to generate music using natural\nlanguage. We then apply multi-modal Retrieval-Augmented Generation (RAG) and\nself-refinement techniques to allow the VLM to produce high-quality music\nwithout external training. Furthermore, we leverage the generated motivations\nin text and the attention maps from the VLM to provide explanations for the\ngenerated results in both text and image modalities. To validate our method, we\nconduct both human studies and machine evaluations, where our method\noutperforms others in terms of music quality and music-image consistency,\nindicating promising results. Our code is available at\nhttps://github.com/RS2002/Image2Music ."}
{"id": "2509.21544", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21544", "abs": "https://arxiv.org/abs/2509.21544", "authors": ["Jeremy Hyrkas"], "title": "Real-time implementation of vibrato transfer as an audio effect", "comment": "4 pages, 4 figures, ICMC 2025", "summary": "An algorithm for deriving delay functions based on real examples of vibrato\nwas recently introduced and can be used to perform a vibrato transfer, in which\nthe vibrato pattern of a target signal is imparted onto an incoming sound using\na delay line. The algorithm contains methods that computationally restrict a\nreal-time implementation. Here, a real-time approximation is presented that\nincorporates an efficient fundamental frequency estimation algorithm and\ntime-domain polyphase IIR filters that approximate an analytic signal. The\nvibrato transfer algorithm is further supplemented with a proposed method to\ntransfer the amplitude modulation of the target sound, moving this method\nbeyond the capabilities of typical delay-based vibrato effects. Modifications\nto the original algorithm for real-time use are detailed here and available as\nsource code for an implementation as a VST plugin. This algorithm has\napplications as an audio effect in sound design, sound morphing, and real-time\nvibrato control of synthesized sounds."}
{"id": "2509.22425", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22425", "abs": "https://arxiv.org/abs/2509.22425", "authors": ["Ke Xue", "Rongfei Fan", "Lixin", "Dawei Zhao", "Chao Zhu", "Han Hu"], "title": "From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for Speech Separation", "comment": null, "summary": "Audio-visual speech separation aims to isolate each speaker's clean voice\nfrom mixtures by leveraging visual cues such as lip movements and facial\nfeatures. While visual information provides complementary semantic guidance,\nexisting methods often underexploit its potential by relying on static visual\nrepresentations. In this paper, we propose CSFNet, a Coarse-to-Separate-Fine\nNetwork that introduces a recursive semantic enhancement paradigm for more\neffective separation. CSFNet operates in two stages: (1) Coarse Separation,\nwhere a first-pass estimation reconstructs a coarse audio waveform from the\nmixture and visual input; and (2) Fine Separation, where the coarse audio is\nfed back into an audio-visual speech recognition (AVSR) model together with the\nvisual stream. This recursive process produces more discriminative semantic\nrepresentations, which are then used to extract refined audio. To further\nexploit these semantics, we design a speaker-aware perceptual fusion block to\nencode speaker identity across modalities, and a multi-range spectro-temporal\nseparation network to capture both local and global time-frequency patterns.\nExtensive experiments on three benchmark datasets and two noisy datasets show\nthat CSFNet achieves state-of-the-art (SOTA) performance, with substantial\ncoarse-to-fine improvements, validating the necessity and effectiveness of our\nrecursive semantic enhancement framework."}
{"id": "2509.21625", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21625", "abs": "https://arxiv.org/abs/2509.21625", "authors": ["Zitong Lan", "Yiduo Hao", "Mingmin Zhao"], "title": "Guiding Audio Editing with Audio Language Model", "comment": null, "summary": "Audio editing plays a central role in VR/AR immersion, virtual conferencing,\nsound design, and other interactive media. However, recent generative audio\nediting models depend on template-like instruction formats and are restricted\nto mono-channel audio. These models fail to deal with declarative audio\nediting, where the user declares what the desired outcome should be, while\nleaving the details of editing operations to the system. We introduce SmartDJ,\na novel framework for stereo audio editing that combines the reasoning\ncapability of audio language models with the generative power of latent\ndiffusion. Given a high-level instruction, SmartDJ decomposes it into a\nsequence of atomic edit operations, such as adding, removing, or spatially\nrelocating events. These operations are then executed by a diffusion model\ntrained to manipulate stereo audio. To support this, we design a data synthesis\npipeline that produces paired examples of high-level instructions, atomic edit\noperations, and audios before and after each edit operation. Experiments\ndemonstrate that SmartDJ achieves superior perceptual quality, spatial realism,\nand semantic alignment compared to prior audio editing methods. Demos are\navailable at https://zitonglan.github.io/project/smartdj/smartdj.html."}
{"id": "2509.22461", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22461", "abs": "https://arxiv.org/abs/2509.22461", "authors": ["Hui Li", "Changhao Jiang", "Hongyu Wang", "Ming Zhang", "Jiajun Sun", "Zhixiong Yang", "Yifei Cao", "Shihan Dou", "Xiaoran Fan", "Baoyu Fan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark", "comment": "25 pages, 7 figures", "summary": "The ability to reason from audio, including speech, paralinguistic cues,\nenvironmental sounds, and music, is essential for AI agents to interact\neffectively in real-world scenarios. Existing benchmarks mainly focus on static\nor single-scene settings and do not fully capture scenarios where multiple\nspeakers, unfolding events, and heterogeneous audio sources interact. To\naddress these challenges, we introduce MDAR, a benchmark for evaluating models\non complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR\ncomprises 3,000 carefully curated question-answer pairs linked to diverse audio\nclips, covering five categories of complex reasoning and spanning three\nquestion types. We benchmark 26 state-of-the-art audio language models on MDAR\nand observe that they exhibit limitations in complex reasoning tasks. On\nsingle-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,\nwhereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio\nsubstantially outperforms Qwen2.5-Omni on the more challenging multiple-choice\nand open-ended tasks. Across all three question types, no model achieves 80%\nperformance. These findings underscore the unique challenges posed by MDAR and\nits value as a benchmark for advancing audio reasoning research.Code and\nbenchmark can be found at https://github.com/luckyerr/MDAR."}
{"id": "2509.21739", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21739", "abs": "https://arxiv.org/abs/2509.21739", "authors": ["Michael Yeung", "Keisuke Toyama", "Toya Teramoto", "Shusuke Takahashi", "Tamaki Kojima"], "title": "Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription", "comment": null, "summary": "Automatic drum transcription (ADT) is traditionally formulated as a\ndiscriminative task to predict drum events from audio spectrograms. In this\nwork, we redefine ADT as a conditional generative task and introduce\nNoise-to-Notes (N2N), a framework leveraging diffusion modeling to transform\naudio-conditioned Gaussian noise into drum events with associated velocities.\nThis generative diffusion approach offers distinct advantages, including a\nflexible speed-accuracy trade-off and strong inpainting capabilities. However,\nthe generation of binary onset and continuous velocity values presents a\nchallenge for diffusion models, and to overcome this, we introduce an Annealed\nPseudo-Huber loss to facilitate effective joint optimization. Finally, to\naugment low-level spectrogram features, we propose incorporating features\nextracted from music foundation models (MFMs), which capture high-level\nsemantic information and enhance robustness to out-of-domain drum audio.\nExperimental results demonstrate that including MFM features significantly\nimproves robustness and N2N establishes a new state-of-the-art performance\nacross multiple ADT benchmarks."}
{"id": "2509.21382", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21382", "abs": "https://arxiv.org/abs/2509.21382", "authors": ["Farnaz Jazaeri", "Homayoun Kamkar-Parsi", "François Grondin", "Martin Bouchard"], "title": "Multi-Speaker DOA Estimation in Binaural Hearing Aids using Deep Learning and Speaker Count Fusion", "comment": "5 pages, 2 figures, submitted to IEEE ICASSP 2026", "summary": "For extracting a target speaker voice, direction-of-arrival (DOA) estimation\nis crucial for binaural hearing aids operating in noisy, multi-speaker\nenvironments. Among the solutions developed for this task, a deep learning\nconvolutional recurrent neural network (CRNN) model leveraging spectral phase\ndifferences and magnitude ratios between microphone signals is a popular\noption. In this paper, we explore adding source-count information for\nmulti-sources DOA estimation. The use of dual-task training with joint\nmulti-sources DOA estimation and source counting is first considered. We then\nconsider using the source count as an auxiliary feature in a standalone DOA\nestimation system, where the number of active sources (0, 1, or 2+) is\nintegrated into the CRNN architecture through early, mid, and late fusion\nstrategies. Experiments using real binaural recordings are performed. Results\nshow that the dual-task training does not improve DOA estimation performance,\nalthough it benefits source-count prediction. However, a ground-truth (oracle)\nsource count used as an auxiliary feature significantly enhances standalone DOA\nestimation performance, with late fusion yielding up to 14% higher average\nF1-scores over the baseline CRNN. This highlights the potential of using\nsource-count estimation for robust DOA estimation in binaural hearing aids."}
{"id": "2509.21919", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.21919", "abs": "https://arxiv.org/abs/2509.21919", "authors": ["Yunyi Liu", "Shaofan Yang", "Kai Li", "Xu Li"], "title": "Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment", "comment": null, "summary": "Human auditory perception is shaped by moving sound sources in 3D space, yet\nprior work in generative sound modelling has largely been restricted to mono\nsignals or static spatial audio. In this work, we introduce a framework for\ngenerating moving sounds given text prompts in a controllable fashion. To\nenable training, we construct a synthetic dataset that records moving sounds in\nbinaural format, their spatial trajectories, and text captions about the sound\nevent and spatial motion. Using this dataset, we train a text-to-trajectory\nprediction model that outputs the three-dimensional trajectory of a moving\nsound source given text prompts. To generate spatial audio, we first fine-tune\na pre-trained text-to-audio generative model to output temporally aligned mono\nsound with the trajectory. The spatial audio is then simulated using the\npredicted temporally-aligned trajectory. Experimental evaluation demonstrates\nreasonable spatial understanding of the text-to-trajectory model. This approach\ncould be easily integrated into existing text-to-audio generative workflow and\nextended to moving sound generation in other spatial audio formats."}
{"id": "2509.21597", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21597", "abs": "https://arxiv.org/abs/2509.21597", "authors": ["Yi Zhu", "Heitor R. Guimarães", "Arthur Pimentel", "Tiago Falk"], "title": "AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit", "comment": null, "summary": "With the prevalence of artificial intelligence (AI)-generated content, such\nas audio deepfakes, a large body of recent work has focused on developing\ndeepfake detection techniques. However, most models are evaluated on a narrow\nset of datasets, leaving their generalization to real-world conditions\nuncertain. In this paper, we systematically review 28 existing audio deepfake\ndatasets and present an open-source benchmarking toolkit called AUDDT\n(https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate\nthe evaluation of pretrained detectors across these 28 datasets, giving users\ndirect feedback on the advantages and shortcomings of their deepfake detectors.\nWe start by showcasing the usage of the developed toolkit, the composition of\nour benchmark, and the breakdown of different deepfake subgroups. Next, using a\nwidely adopted pretrained deepfake detector, we present in- and out-of-domain\ndetection results, revealing notable differences across conditions and audio\nmanipulation types. Lastly, we also analyze the limitations of these existing\ndatasets and their gap relative to practical deployment scenarios."}
{"id": "2509.22062", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22062", "abs": "https://arxiv.org/abs/2509.22062", "authors": ["Junjie Cao", "Yichen Han", "Ruonan Zhang", "Xiaoyang Hao", "Hongxiang Li", "Shuaijiang Zhao", "Yue Liu", "Xiao-Ping Zhng"], "title": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling", "comment": "conference paper about TTS", "summary": "Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech\n(TTS) systems, while achieving state-of-the-art quality, still face critical\nchallenges. The foundation of this LLM-based paradigm is the discretization of\nthe continuous speech waveform into a sequence of discrete tokens by neural\naudio codec. However, single codebook modeling is well suited to text LLMs, but\nsuffers from significant information loss; hierarchical acoustic tokens,\ntypically generated via Residual Vector Quantization (RVQ), often lack explicit\nsemantic structure, placing a heavy learning burden on the model. Furthermore,\nthe autoregressive process is inherently susceptible to error accumulation,\nwhich can degrade generation stability. To address these limitations, we\npropose CaT-TTS, a novel framework for robust and semantically-grounded\nzero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that\ninjects explicit linguistic features into its primary codebook via semantic\ndistillation from a state-of-the-art ASR model, providing a structured\nrepresentation that simplifies the learning task. Second, we propose an\n``Understand-then-Generate'' dual-Transformer architecture that decouples\ncomprehension from rendering. An initial ``Understanding'' Transformer models\nthe cross-modal relationship between text and the audio's semantic tokens to\nform a high-level utterance plan. A subsequent ``Generation'' Transformer then\nexecutes this plan, autoregressively synthesizing hierarchical acoustic tokens.\nFinally, to enhance generation stability, we introduce Masked Audio Parallel\nInference (MAPI), a nearly parameter-free inference strategy that dynamically\nguides the decoding process to mitigate local errors."}
{"id": "2509.21676", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21676", "abs": "https://arxiv.org/abs/2509.21676", "authors": ["Aurosweta Mahapatra", "Ismail Rasim Ulgen", "Berrak Sisman"], "title": "HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for Expressive and Emotional Synthetic Speech", "comment": "Submitted to IEEE Transactions on Affective Computing", "summary": "Current anti-spoofing systems remain vulnerable to expressive and emotional\nsynthetic speech, since they rarely leverage prosody as a discriminative cue.\nProsody is central to human expressiveness and emotion, and humans\ninstinctively use prosodic cues such as F0 patterns and voiced/unvoiced\nstructure to distinguish natural from synthetic speech. In this paper, we\npropose HuLA, a two-stage prosody-aware multi-task learning framework for spoof\ndetection. In Stage 1, a self-supervised learning (SSL) backbone is trained on\nreal speech with auxiliary tasks of F0 prediction and voiced/unvoiced\nclassification, enhancing its ability to capture natural prosodic variation\nsimilar to human perceptual learning. In Stage 2, the model is jointly\noptimized for spoof detection and prosody tasks on both real and synthetic\ndata, leveraging prosodic awareness to detect mismatches between natural and\nexpressive synthetic speech. Experiments show that HuLA consistently\noutperforms strong baselines on challenging out-of-domain dataset, including\nexpressive, emotional, and cross-lingual attacks. These results demonstrate\nthat explicit prosodic supervision, combined with SSL embeddings, substantially\nimproves robustness against advanced synthetic speech attacks."}
{"id": "2509.22317", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22317", "abs": "https://arxiv.org/abs/2509.22317", "authors": ["Jiani Ding", "Qiyang Sun", "Alican Akman", "Björn W. Schuller"], "title": "Cross-Dialect Bird Species Recognition with Dialect-Calibrated Augmentation", "comment": null, "summary": "Dialect variation hampers automatic recognition of bird calls collected by\npassive acoustic monitoring. We address the problem on DB3V, a three-region,\nten-species corpus of 8-s clips, and propose a deployable framework built on\nTime-Delay Neural Networks (TDNNs). Frequency-sensitive normalisation (Instance\nFrequency Normalisation and a gated Relaxed-IFN) is paired with\ngradient-reversal adversarial training to learn region-invariant embeddings. A\nmulti-level augmentation scheme combines waveform perturbations, Mixup for rare\nclasses, and CycleGAN transfer that synthesises Region 2 (Interior\nPlains)-style audio, , with Dialect-Calibrated Augmentation (DCA) softly\ndown-weighting synthetic samples to limit artifacts. The complete system lifts\ncross-dialect accuracy by up to twenty percentage points over baseline TDNNs\nwhile preserving in-region performance. Grad-CAM and LIME analyses show that\nrobust models concentrate on stable harmonic bands, providing ecologically\nmeaningful explanations. The study demonstrates that lightweight, transparent,\nand dialect-resilient bird-sound recognition is attainable."}
{"id": "2509.21968", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.21968", "abs": "https://arxiv.org/abs/2509.21968", "authors": ["Yushen Chen", "Kai Hu", "Long Zhou", "Shulin Feng", "Xusheng Yang", "Hangting Chen", "Xie Chen"], "title": "AUV: Teaching Audio Universal Vector Quantization with Single Nested Codebook", "comment": "Submitted to ICASSP 2026", "summary": "We propose AUV, a unified neural audio codec with a single codebook, which\nenables a favourable reconstruction of speech and further extends to general\naudio, including vocal, music, and sound. AUV is capable of tackling any 16 kHz\nmixed-domain audio segment at bit rates around 700 bps. To accomplish this, we\nguide the matryoshka codebook with nested domain-specific partitions, assigned\nwith corresponding teacher models to perform distillation, all in a\nsingle-stage training. A conformer-style encoder-decoder architecture with STFT\nfeatures as audio representation is employed, yielding better audio quality.\nComprehensive evaluations demonstrate that AUV exhibits comparable audio\nreconstruction ability to state-of-the-art domain-specific single-layer\nquantizer codecs, showcasing the potential of audio universal vector\nquantization with a single codebook. The pre-trained model and demo samples are\navailable at https://swivid.github.io/AUV/."}
{"id": "2509.22378", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22378", "abs": "https://arxiv.org/abs/2509.22378", "authors": ["Zijian Zhao", "Dian Jin", "Zijing Zhou"], "title": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach", "comment": null, "summary": "Recently, Image-to-Music (I2M) generation has garnered significant attention,\nwith potential applications in fields such as gaming, advertising, and\nmulti-modal art creation. However, due to the ambiguous and subjective nature\nof I2M tasks, most end-to-end methods lack interpretability, leaving users\npuzzled about the generation results. Even methods based on emotion mapping\nface controversy, as emotion represents only a singular aspect of art.\nAdditionally, most learning-based methods require substantial computational\nresources and large datasets for training, hindering accessibility for common\nusers. To address these challenges, we propose the first Vision Language Model\n(VLM)-based I2M framework that offers high interpretability and low\ncomputational cost. Specifically, we utilize ABC notation to bridge the text\nand music modalities, enabling the VLM to generate music using natural\nlanguage. We then apply multi-modal Retrieval-Augmented Generation (RAG) and\nself-refinement techniques to allow the VLM to produce high-quality music\nwithout external training. Furthermore, we leverage the generated motivations\nin text and the attention maps from the VLM to provide explanations for the\ngenerated results in both text and image modalities. To validate our method, we\nconduct both human studies and machine evaluations, where our method\noutperforms others in terms of music quality and music-image consistency,\nindicating promising results. Our code is available at\nhttps://github.com/RS2002/Image2Music ."}
{"id": "2509.22061", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22061", "abs": "https://arxiv.org/abs/2509.22061", "authors": ["Shree Harsha Bokkahalli Satish", "Harm Lameris", "Olivier Perrotin", "Gustav Eje Henter", "Éva Székely"], "title": "Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias", "comment": "6 pages, 1 figure, Submitted to IEEE ICASSP 2026", "summary": "Speech Continuation (SC) is the task of generating a coherent extension of a\nspoken prompt while preserving both semantic context and speaker identity.\nBecause SC is constrained to a single audio stream, it offers a more direct\nsetting for probing biases in speech foundation models than dialogue does. In\nthis work we present the first systematic evaluation of bias in SC,\ninvestigating how gender and phonation type (breathy, creaky, end-creak) affect\ncontinuation behaviour. We evaluate three recent models: SpiritLM (base and\nexpressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality\npreservation, and text-based bias metrics. Results show that while both speaker\nsimilarity and coherence remain a challenge, textual evaluations reveal\nsignificant model and gender interactions: once coherence is sufficiently high\n(for VAE-GSLM), gender effects emerge on text-metrics such as agency and\nsentence polarity. In addition, continuations revert toward modal phonation\nmore strongly for female prompts than for male ones, revealing a systematic\nvoice-quality bias. These findings highlight SC as a controlled probe of\nsocially relevant representational biases in speech foundation models, and\nsuggest that it will become an increasingly informative diagnostic as\ncontinuation quality improves."}
{"id": "2509.22461", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22461", "abs": "https://arxiv.org/abs/2509.22461", "authors": ["Hui Li", "Changhao Jiang", "Hongyu Wang", "Ming Zhang", "Jiajun Sun", "Zhixiong Yang", "Yifei Cao", "Shihan Dou", "Xiaoran Fan", "Baoyu Fan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark", "comment": "25 pages, 7 figures", "summary": "The ability to reason from audio, including speech, paralinguistic cues,\nenvironmental sounds, and music, is essential for AI agents to interact\neffectively in real-world scenarios. Existing benchmarks mainly focus on static\nor single-scene settings and do not fully capture scenarios where multiple\nspeakers, unfolding events, and heterogeneous audio sources interact. To\naddress these challenges, we introduce MDAR, a benchmark for evaluating models\non complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR\ncomprises 3,000 carefully curated question-answer pairs linked to diverse audio\nclips, covering five categories of complex reasoning and spanning three\nquestion types. We benchmark 26 state-of-the-art audio language models on MDAR\nand observe that they exhibit limitations in complex reasoning tasks. On\nsingle-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,\nwhereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio\nsubstantially outperforms Qwen2.5-Omni on the more challenging multiple-choice\nand open-ended tasks. Across all three question types, no model achieves 80%\nperformance. These findings underscore the unique challenges posed by MDAR and\nits value as a benchmark for advancing audio reasoning research.Code and\nbenchmark can be found at https://github.com/luckyerr/MDAR."}
{"id": "2509.22148", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22148", "abs": "https://arxiv.org/abs/2509.22148", "authors": ["Ziyun Cui", "Sike Jia", "Yang Lin", "Yinan Duan", "Diyang Qu", "Runsen Chen", "Chao Zhang", "Chang Lei", "Wen Wu"], "title": "Speaker Anonymisation for Speech-based Suicide Risk Detection", "comment": null, "summary": "Adolescent suicide is a critical global health issue, and speech provides a\ncost-effective modality for automatic suicide risk detection. Given the\nvulnerable population, protecting speaker identity is particularly important,\nas speech itself can reveal personally identifiable information if the data is\nleaked or maliciously exploited. This work presents the first systematic study\nof speaker anonymisation for speech-based suicide risk detection. A broad range\nof anonymisation methods are investigated, including techniques based on\ntraditional signal processing, neural voice conversion, and speech synthesis. A\ncomprehensive evaluation framework is built to assess the trade-off between\nprotecting speaker identity and preserving information essential for suicide\nrisk detection. Results show that combining anonymisation methods that retain\ncomplementary information yields detection performance comparable to that of\noriginal speech, while achieving protection of speaker identity for vulnerable\npopulations."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 6]
- [eess.AS](#eess.AS) [Total: 13]
- [cs.SD](#cs.SD) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Hidden Markov Model Decoding for LDPC Codes](https://arxiv.org/abs/2509.21872)
*Jan C Olivier,Etienne Barnard*

Main category: eess.SP

TL;DR: 提出了一种基于迭代隐马尔可夫模型（HMM）的LDPC码解码方法，通过前向-后向平滑估计器实现高效解码，在短帧长度下性能优于传统置信传播算法。


<details>
  <summary>Details</summary>
Motivation: 传统LDPC码解码方法如置信传播（BP）在短帧长度下性能受限，需要开发更高效的解码算法来提升解码阈值和误帧率性能。

Method: 使用一阶时间齐次HMM框架，隐藏状态包含两个编码比特对，通过随机游走遍历编码帧比特，将奇偶校验自然融入观测模型，采用前向-后向平滑估计器进行状态估计。

Result: 相比Tanner图上的BP算法，LDPC解码阈值显著提升，在短帧长度（512比特或更短）下，误帧率和解码阈值性能与采用SCL-CRC解码的Polar码相当。

Conclusion: 基于HMM的迭代解码方法为LDPC码提供了一种高效解码方案，在短帧场景下具有优越性能，为实际通信系统提供了有前景的解码选择。

Abstract: The paper proposes an iterative Hidden Markov Model (HMM) for decoding a Low
Density Parity Check (LDPC) code. It is demonstrated that a first-order HMM
provides a natural framework for the decoder. The HMM is time-homogeneous with
a fixed transition matrix and is based on a random walk through the encoded
frame bits. Each hidden state contains a pair of two encoded bits, and parity
checks are naturally incorporated into the observation model. The paper shows
that by implementing a forward-backward smoothing estimator for the hidden
states, decoding is efficient and requires only a small number of iterations in
most cases. The results show that the LDPC decoding threshold is significantly
improved compared to belief propagation (BP) on a Tanner graph. Numerical
results are presented showing that LDPC codes under the proposed decoder yield
a frame error rate (FER) and decoding threshold comparable to that of a Polar
code where Successive Cancellation List (SCL) - Cyclic Redundancy Check (CRC)
decoding is deployed. This is shown to be achieved even if the frame length is
short (on the order of $512$ bits or less) and a regular LDPC code is used. 1

</details>


### [2] [CRB minimization for PASS Assisted ISAC](https://arxiv.org/abs/2509.22181)
*Haochen Li,Ruikang Zhong,Jiayi Lei,Pan Zhiwen,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了一种基于多波导PASS的集成感知与通信系统，通过交替优化方法最小化目标感知的克拉美罗下界，在保证通信质量的同时提升感知性能。


<details>
  <summary>Details</summary>
Motivation: 为了在集成感知与通信系统中同时优化通信和感知性能，需要解决传统架构中感知精度受限的问题，特别是在复杂部署约束下的优化挑战。

Method: 采用PASS发射-ULA接收的基站架构，通过交替优化方法解决非凸优化问题，在通信服务质量、功率预算和天线部署约束下最小化目标感知的克拉美罗下界。

Result: 仿真结果表明，所提出的PASS辅助ISAC框架在性能上优于基准方案，实现了优越的感知与通信综合性能。

Conclusion: 该研究证明了PASS辅助的ISAC系统在满足通信需求的同时，能够有效提升目标感知精度，为未来集成感知通信系统设计提供了有效解决方案。

Abstract: A multiple waveguide PASS assisted integrated sensing and communication
(ISAC) system is proposed, where the base station (BS) is equipped with
transmitting pinching antennas (PAs) and receiving uniform linear array (ULA)
antennas. The PASS-transmitting-ULA-receiving (PTUR) BS transmits the
communication and sensing signals through the stretched PAs on waveguides and
collects the echo sensing signals with the mounted ULA. Based on this
configuration, a target sensing Cramer Rao Bound (CRB) minimization problem is
formulated under communication quality-of-service (QoS) constraints, power
budget constraints, and PA deployment constraints. An alternating optimization
(AO) method is employed to address the formulated non-convex optimization
problem. Simulation results demonstrate that the proposed PASS assisted ISAC
framework achieves superior performance over benchmark schemes.

</details>


### [3] [A Deep Neural Network Codebook Approach for Near-Field Nulling Control Beam Focusing](https://arxiv.org/abs/2509.22204)
*Mohammadhossein Karimi,Yuanzhe Gong,Tho Le-Ngoc*

Main category: eess.SP

TL;DR: 提出一种基于深度神经网络码本的近场XL-MIMO系统多用户干扰抑制方法，通过分区训练轻量级DNN模型来预测波束赋形权重，在降低复杂度的同时实现高效干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于DNN的零陷控制波束赋形方法面临可扩展性和复杂度挑战，特别是在超大规模MIMO系统的近场区域，需要更高效的干扰抑制方案。

Method: 使用基于相关性的采样对菲涅尔区域进行分区，为每个子区域分配轻量级全连接DNN模型，基于LCMV方法生成的波束赋形权重进行训练，预测同时优化信号强度和抑制干扰的零陷控制波束聚焦权重。

Result: 训练模型在75个样本子区域上实现平均相位误差0.085弧度、幅度误差0.52 dB。全波仿真显示干扰抑制优于31.64 dB，与LCMV方法性能差距在2 dB以内。

Conclusion: 所提出的DNN码本方法在显著降低计算复杂度的同时，有效缓解了多用户干扰，验证了其在近场XL-MIMO系统中的实用价值。

Abstract: This paper proposes a deep neural network (DNN) codebook approach for
multi-user interference (MUI) mitigation in extremely large multiple-input
multiple-output (XL-MIMO) systems operating in the near-field region. Unlike
existing DNN-based nulling control beamforming (NCBF) methods that face
scalability and complexity challenges, the proposed framework partitions the
Fresnel region using correlation-based sampling and assigns a lightweight fully
connected DNN model to each subsection. Each model is trained on beamforming
weights generated using the linearly constrained minimum variance (LCMV)
method, enabling accurate prediction of nulling control beam-focusing weights
that simultaneously optimize the desired signal strength and suppress potential
interference for both collinear and non-collinear user configurations.
Simulation results show that the trained models achieve average phase and
magnitude prediction errors of 0.085 radians and 0.52 dB, respectively, across
75 sample subsections. Full-wave simulations in Ansys HFSS further demonstrate
that the proposed DNN codebook achieves interference suppression better than
31.64 dB, with a performance gap within 2 dB of the LCMV method, thereby
validating its effectiveness in mitigating MUI while reducing computational
complexity.

</details>


### [4] [Stacked Intelligent Metasurface-Enhanced Wideband Multiuser MIMO OFDM-IM Communications](https://arxiv.org/abs/2509.22327)
*Zheao Li,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: 本文提出了一种基于OFDM-IM的SIM增强宽带多用户收发器，通过稀疏激活限制高保真均衡到活跃子载波，有效扩展可用带宽。采用最差链路BER优化和max-min SINR准则，并提出了UPGD-Net进行快速帧率推断。


<details>
  <summary>Details</summary>
Motivation: 解决可编程超表面在宽带部署中的两个结构性问题：(i)单一准静态SIM相位张量需要适应所有子载波；(ii)多用户调度每帧改变子载波激活模式，需要快速重新配置。

Method: 开发基于OFDM-IM的SIM增强宽带多用户收发器，采用稀疏激活策略。提出UPGD-Net网络，在SIM层和算法迭代上进行双重展开，每个单元从级联预编码器计算解析梯度并学习迭代步长。

Result: 宽带多用户下行链路仿真显示快速单调收敛、明显的层深最佳点，以及在最差链路BER和和速率方面的一致增益。

Conclusion: 通过将结构稀疏性与BER驱动的深度展开优化框架相结合，该方案直接解决了SIM在宽带部署中的关键缺陷。

Abstract: Leveraging the multilayer realization of programmable metasurfaces, stacked
intelligent metasurfaces (SIM) enable fine-grained wave-domain control.
However, their wideband deployment is impeded by two structural factors: (i) a
single, quasi-static SIM phase tensor must adapt to all subcarriers, and (ii)
multiuser scheduling changes the subcarrier activation pattern frame by frame,
requiring rapid reconfiguration. To address both challenges, we develop a
SIM-enhanced wideband multiuser transceiver built on orthogonal
frequency-division multiplexing with index modulation (OFDM-IM). The sparse
activation of OFDM-IM confines high-fidelity equalization to the active tones,
effectively widening the usable bandwidth. To make the design
reliability-aware, we directly target the worst-link bit-error rate (BER) and
adopt a max-min per-tone signal-to-interference-plus-noise ratio (SINR) as a
principled surrogate, turning the reliability optimization tractable. For
frame-rate inference and interpretability, we propose an unfolded
projected-gradient-descent network (UPGD-Net) that double-unrolls across the
SIM's layers and algorithmic iterations: each cell computes the analytic
gradient from the cascaded precoder with a learnable per-iteration step size.
Simulations on wideband multiuser downlinks show fast, monotone convergence, an
evident layer-depth sweet spot, and consistent gains in worst-link BER and sum
rate. By combining structural sparsity with a BER-driven, deep-unfolded
optimization backbone, the proposed framework directly addresses the key
wideband deficiencies of SIM.

</details>


### [5] [Specific multi-emitter identification via multi-label learning](https://arxiv.org/abs/2509.22396)
*Yuhao Chen,Boxiang He,Shilian Wang,Jing Lei*

Main category: eess.SP

TL;DR: 提出了一种基于多标签学习的特定多发射器识别方法，用于从重叠信号中识别多个发射器。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法处理多个发射器信号重叠的场景，需要解决多发射器识别问题。

Method: 设计了多发射器指纹提取器来减轻重叠信号间的相互干扰，并提出多发射器决策器来分配所有发射器识别。

Result: 实验结果表明，与基线方法相比，所提出的SMEI方案在各种重叠条件下实现了相当的识别精度，同时复杂度显著降低。

Conclusion: 本文的意义在于以低复杂度从重叠信号中识别多个发射器。

Abstract: Specific emitter identification leverages hardware-induced impairments to
uniquely determine a specific transmitter. However, existing approaches fail to
address scenarios where signals from multiple emitters overlap. In this paper,
we propose a specific multi-emitter identification (SMEI) method via
multi-label learning to determine multiple transmitters. Specifically, the
multi-emitter fingerprint extractor is designed to mitigate the mutual
interference among overlapping signals. Then, the multi-emitter decision maker
is proposed to assign the all emitter identification using the previous
extracted fingerprint. Experimental results demonstrate that, compared with
baseline approach, the proposed SMEI scheme achieves comparable identification
accuracy under various overlapping conditions, while operating at significantly
lower complexity. The significance of this paper is to identify multiple
emitters from overlapped signal with a low complexity.

</details>


### [6] [Approximation of the Range Ambiguity Function in Near-field Sensing Systems](https://arxiv.org/abs/2509.22423)
*Marcin Wachowiak,André Bourdoux,Sofie Pollin*

Main category: eess.SP

TL;DR: 本文研究了近场系统的距离模糊函数，分析了带宽和近场波束聚焦如何共同决定分辨率。推导了匹配滤波器模糊函数，提出了基于孔径-带宽乘积的近似准则，并展示了近场波束聚焦相比远场的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究近场系统中带宽和波束聚焦对分辨率的联合影响，探索近场区域相比远场在距离分辨率方面的潜在优势。

Method: 推导了通用的匹配滤波器模糊函数，引入了不同天线阵列几何结构的近场阵列因子，将近场模糊函数近似为距离相关的近场阵列因子与带宽波形模糊函数的乘积。

Result: 建立了基于孔径-带宽乘积的近似准则，验证了其准确性。近场波束聚焦在分辨率、峰值旁瓣比和积分旁瓣水平方面均有改善，但这些增益具有距离依赖性，仅在阵列附近显著。

Conclusion: 近场波束聚焦相比远场能提供更好的性能，但增益范围有限，主要适用于靠近阵列的区域。孔径-带宽乘积是评估近场模糊函数准确性的有效准则。

Abstract: This paper investigates the range ambiguity function of near-field systems
where bandwidth and near-field beamfocusing jointly determine the resolution.
First, the general matched filter ambiguity function is derived and the
near-field array factors of different antenna array geometries are introduced.
Next, the near-field ambiguity function is approximated as a product of the
range-dependent near-field array factor and the ambiguity function due to the
utilized bandwidth and waveform. An approximation criterion based on the
aperture-bandwidth product is formulated, and its accuracy is examined.
Finally, the improvements to the ambiguity function offered by the near-field
beamfocusing, as compared to the far-field case, are presented. The performance
gains are evaluated in terms of resolution improvement offered by beamfocusing,
peak-to-sidelobe and integrated-sidelobe level improvement. The gains offered
by the near-field regime are shown to be range-dependent and substantial only
in close proximity to the array.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [7] [Toward a Realistic Encoding Model of Auditory Affective Understanding in the Brain](https://arxiv.org/abs/2509.21381)
*Guandong Pan,Yaqian Yang,Shi Chen,Xin Wang,Longzhao Liu,Hongwei Zheng,Shaoting Tang*

Main category: eess.AS

TL;DR: 该研究提出了一个计算框架，通过多层级听觉特征分析自然音频对情绪唤醒的动态影响，发现高层次语义表征在情绪编码中起主导作用，且人声和背景音乐在不同数据集中有不同的情绪诱发偏好。


<details>
  <summary>Details</summary>
Motivation: 在情感神经科学和情感感知AI中，理解复杂听觉刺激如何驱动情绪唤醒动态仍是一个未解决的问题，需要建立计算模型来解析大脑对自然听觉输入的编码机制。

Method: 基于听觉层级结构的神经生物学原理，将音频分解为多层级听觉特征（使用传统算法和wav2vec 2.0/Hubert），从原始音频和分离的人声/背景音乐元素中提取特征，通过跨数据集分析将其映射到情绪相关反应。

Result: 高层次语义表征（来自wav2vec 2.0/Hubert的最终层）在情绪编码中起主导作用，显著优于低层次声学特征；中间层（平衡声学-语义信息）在跨数据集情绪诱导中表现最佳；人声和背景音乐的情绪诱发偏好与刺激能量分布相关。

Conclusion: 通过整合情感计算和神经科学，本研究揭示了听觉-情绪编码的层级机制，为自适应情感感知系统和跨学科音频-情感交互探索提供了基础。

Abstract: In affective neuroscience and emotion-aware AI, understanding how complex
auditory stimuli drive emotion arousal dynamics remains unresolved. This study
introduces a computational framework to model the brain's encoding of
naturalistic auditory inputs into dynamic behavioral/neural responses across
three datasets (SEED, LIRIS, self-collected BAVE). Guided by neurobiological
principles of parallel auditory hierarchy, we decompose audio into multilevel
auditory features (through classical algorithms and wav2vec 2.0/Hubert) from
the original and isolated human voice/background soundtrack elements, mapping
them to emotion-related responses via cross-dataset analyses. Our analysis
reveals that high-level semantic representations (derived from the final layer
of wav2vec 2.0/Hubert) exert a dominant role in emotion encoding, outperforming
low-level acoustic features with significantly stronger mappings to behavioral
annotations and dynamic neural synchrony across most brain regions ($p <
0.05$). Notably, middle layers of wav2vec 2.0/hubert (balancing
acoustic-semantic information) surpass the final layers in emotion induction
across datasets. Moreover, human voices and soundtracks show dataset-dependent
emotion-evoking biases aligned with stimulus energy distribution (e.g., LIRIS
favors soundtracks due to higher background energy), with neural analyses
indicating voices dominate prefrontal/temporal activity while soundtracks excel
in limbic regions. By integrating affective computing and neuroscience, this
work uncovers hierarchical mechanisms of auditory-emotion encoding, providing a
foundation for adaptive emotion-aware systems and cross-disciplinary
explorations of audio-affective interactions.

</details>


### [8] [Multi-Speaker DOA Estimation in Binaural Hearing Aids using Deep Learning and Speaker Count Fusion](https://arxiv.org/abs/2509.21382)
*Farnaz Jazaeri,Homayoun Kamkar-Parsi,François Grondin,Martin Bouchard*

Main category: eess.AS

TL;DR: 该论文研究在双耳助听器的多声源DOA估计中加入声源数量信息，通过双任务训练和辅助特征融合策略，发现使用真实声源数量作为辅助特征能显著提升DOA估计性能。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂多说话人环境中，声源到达方向(DOA)估计对双耳助听器提取目标说话人语音至关重要。现有基于深度学习的CRNN模型主要利用麦克风信号的频谱相位差和幅度比，但未充分利用声源数量信息。

Method: 1）采用双任务训练，联合进行多声源DOA估计和声源计数；2）将声源数量（0、1或2+）作为辅助特征，通过早期、中期和晚期融合策略集成到CRNN架构中。

Result: 双任务训练未能改善DOA估计性能，但有利于声源计数预测。而使用真实声源数量作为辅助特征显著提升了独立DOA估计性能，晚期融合策略相比基线CRNN实现了高达14%的平均F1分数提升。

Conclusion: 声源计数估计在双耳助听器的鲁棒DOA估计中具有重要潜力，特别是将声源数量作为辅助特征时能显著提升性能。

Abstract: For extracting a target speaker voice, direction-of-arrival (DOA) estimation
is crucial for binaural hearing aids operating in noisy, multi-speaker
environments. Among the solutions developed for this task, a deep learning
convolutional recurrent neural network (CRNN) model leveraging spectral phase
differences and magnitude ratios between microphone signals is a popular
option. In this paper, we explore adding source-count information for
multi-sources DOA estimation. The use of dual-task training with joint
multi-sources DOA estimation and source counting is first considered. We then
consider using the source count as an auxiliary feature in a standalone DOA
estimation system, where the number of active sources (0, 1, or 2+) is
integrated into the CRNN architecture through early, mid, and late fusion
strategies. Experiments using real binaural recordings are performed. Results
show that the dual-task training does not improve DOA estimation performance,
although it benefits source-count prediction. However, a ground-truth (oracle)
source count used as an auxiliary feature significantly enhances standalone DOA
estimation performance, with late fusion yielding up to 14% higher average
F1-scores over the baseline CRNN. This highlights the potential of using
source-count estimation for robust DOA estimation in binaural hearing aids.

</details>


### [9] [ARTI-6: Towards Six-dimensional Articulatory Speech Encoding](https://arxiv.org/abs/2509.21447)
*Jihwan Lee,Sean Foley,Thanathai Lertpetchpun,Kevin Huang,Yoonjeong Lee,Tiantian Feng,Louis Goldstein,Dani Byrd,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: ARTI-6是一个紧凑的六维发音语音编码框架，基于实时MRI数据，捕捉关键声道区域。包含发音特征集、从声学预测发音特征的逆模型（预测相关性0.87）和从发音特征重建语音的合成模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个可解释、计算高效且基于生理学的框架，用于推进发音逆问题、合成和更广泛的语音技术应用。

Method: 使用实时MRI数据提取六维发音特征集，构建发音逆模型（利用语音基础模型从声学预测发音特征）和发音合成模型（从发音特征直接重建语音）。

Result: 发音逆模型实现了0.87的预测相关性，发音合成模型能够从低维表示生成自然语音，证明紧凑表示的有效性。

Conclusion: ARTI-6提供了一个可解释、计算高效且生理学基础的框架，可用于发音逆问题、合成和语音技术应用，源代码和语音样本已公开。

Abstract: We propose ARTI-6, a compact six-dimensional articulatory speech encoding
framework derived from real-time MRI data that captures crucial vocal tract
regions including the velum, tongue root, and larynx. ARTI-6 consists of three
components: (1) a six-dimensional articulatory feature set representing key
regions of the vocal tract; (2) an articulatory inversion model, which predicts
articulatory features from speech acoustics leveraging speech foundation
models, achieving a prediction correlation of 0.87; and (3) an articulatory
synthesis model, which reconstructs intelligible speech directly from
articulatory features, showing that even a low-dimensional representation can
generate natural-sounding speech. Together, ARTI-6 provides an interpretable,
computationally efficient, and physiologically grounded framework for advancing
articulatory inversion, synthesis, and broader speech technology applications.
The source code and speech samples are publicly available.

</details>


### [10] [Enhanced Generative Machine Listener](https://arxiv.org/abs/2509.21463)
*Vishnu Raj,Gouthaman KV,Shiv Gehlot,Lars Villemoes,Arijit Biswas*

Main category: eess.AS

TL;DR: GMLv2是一个基于参考的音频质量预测模型，使用Beta分布损失函数建模听众评分，在MUSHRA评分预测中优于PEAQ和ViSQOL等传统指标。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够准确预测主观音频质量（MUSHRA评分）的自动化评估框架，以加速现代音频编码技术的研究和开发。

Method: 引入基于Beta分布的损失函数来建模听众评分，并整合额外的神经音频编码主观数据集以提升模型的泛化能力和适用性。

Result: 在多样化测试集上的广泛评估表明，GMLv2在主观评分相关性方面持续优于广泛使用的PEAQ和ViSQOL指标，并能可靠预测不同内容类型和编解码配置的音频质量。

Conclusion: GMLv2提供了一个可扩展且自动化的感知音频质量评估框架，有望加速现代音频编码技术的研究与发展。

Abstract: We present GMLv2, a reference-based model designed for the prediction of
subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta
distribution-based loss to model the listener ratings and incorporates
additional neural audio coding (NAC) subjective datasets to extend its
generalization and applicability. Extensive evaluations on diverse testset
demonstrate that proposed GMLv2 consistently outperforms widely used metrics,
such as PEAQ and ViSQOL, both in terms of correlation with subjective scores
and in reliably predicting these scores across diverse content types and codec
configurations. Consequently, GMLv2 offers a scalable and automated framework
for perceptual audio quality evaluation, poised to accelerate research and
development in modern audio coding technologies.

</details>


### [11] [AUDDT: Audio Unified Deepfake Detection Benchmark Toolkit](https://arxiv.org/abs/2509.21597)
*Yi Zhu,Heitor R. Guimarães,Arthur Pimentel,Tiago Falk*

Main category: eess.AS

TL;DR: 本文系统回顾了28个音频深度伪造数据集，开发了开源基准测试工具包AUDDT，用于自动化评估预训练检测器在不同数据集上的性能，揭示了检测模型在跨域和不同伪造类型下的显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容（如音频深度伪造）的普及，现有检测模型大多在有限数据集上评估，其在实际场景中的泛化能力存在不确定性，需要系统性的跨数据集评估工具。

Method: 开发了AUDDT开源基准测试工具包，自动化评估预训练检测器在28个音频深度伪造数据集上的性能，分析不同伪造子组的分类，并进行域内和跨域检测性能对比。

Result: 使用广泛采用的预训练深度伪造检测器进行测试，发现在不同条件和音频操作类型下检测性能存在显著差异，揭示了现有检测模型的局限性。

Conclusion: 现有音频深度伪造数据集存在局限性，与实际部署场景存在差距，需要更全面的评估框架来提升检测模型的实用性和泛化能力。

Abstract: With the prevalence of artificial intelligence (AI)-generated content, such
as audio deepfakes, a large body of recent work has focused on developing
deepfake detection techniques. However, most models are evaluated on a narrow
set of datasets, leaving their generalization to real-world conditions
uncertain. In this paper, we systematically review 28 existing audio deepfake
datasets and present an open-source benchmarking toolkit called AUDDT
(https://github.com/MuSAELab/AUDDT). The goal of this toolkit is to automate
the evaluation of pretrained detectors across these 28 datasets, giving users
direct feedback on the advantages and shortcomings of their deepfake detectors.
We start by showcasing the usage of the developed toolkit, the composition of
our benchmark, and the breakdown of different deepfake subgroups. Next, using a
widely adopted pretrained deepfake detector, we present in- and out-of-domain
detection results, revealing notable differences across conditions and audio
manipulation types. Lastly, we also analyze the limitations of these existing
datasets and their gap relative to practical deployment scenarios.

</details>


### [12] [HuLA: Prosody-Aware Anti-Spoofing with Multi-Task Learning for Expressive and Emotional Synthetic Speech](https://arxiv.org/abs/2509.21676)
*Aurosweta Mahapatra,Ismail Rasim Ulgen,Berrak Sisman*

Main category: eess.AS

TL;DR: HuLA是一个两阶段的多任务学习框架，通过显式利用韵律特征来检测合成语音欺骗攻击。第一阶段通过自监督学习训练骨干网络，第二阶段联合优化欺骗检测和韵律任务，显著提升了对抗表达性、情感性合成语音攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的反欺骗系统对表达性和情感性合成语音仍然脆弱，因为它们很少利用韵律作为判别性线索。人类本能地使用韵律线索（如F0模式和清浊音结构）来区分自然语音和合成语音。

Method: 两阶段多任务学习框架：第一阶段在真实语音上训练自监督学习骨干网络，包含F0预测和清浊音分类的辅助任务；第二阶段在真实和合成数据上联合优化欺骗检测和韵律任务。

Result: 实验表明HuLA在具有挑战性的域外数据集上持续优于强基线，包括表达性、情感性和跨语言攻击场景。

Conclusion: 显式的韵律监督与自监督学习嵌入相结合，显著提高了对抗先进合成语音攻击的鲁棒性。

Abstract: Current anti-spoofing systems remain vulnerable to expressive and emotional
synthetic speech, since they rarely leverage prosody as a discriminative cue.
Prosody is central to human expressiveness and emotion, and humans
instinctively use prosodic cues such as F0 patterns and voiced/unvoiced
structure to distinguish natural from synthetic speech. In this paper, we
propose HuLA, a two-stage prosody-aware multi-task learning framework for spoof
detection. In Stage 1, a self-supervised learning (SSL) backbone is trained on
real speech with auxiliary tasks of F0 prediction and voiced/unvoiced
classification, enhancing its ability to capture natural prosodic variation
similar to human perceptual learning. In Stage 2, the model is jointly
optimized for spoof detection and prosody tasks on both real and synthetic
data, leveraging prosodic awareness to detect mismatches between natural and
expressive synthetic speech. Experiments show that HuLA consistently
outperforms strong baselines on challenging out-of-domain dataset, including
expressive, emotional, and cross-lingual attacks. These results demonstrate
that explicit prosodic supervision, combined with SSL embeddings, substantially
improves robustness against advanced synthetic speech attacks.

</details>


### [13] [FastEnhancer: Speed-Optimized Streaming Neural Speech Enhancement](https://arxiv.org/abs/2509.21867)
*Sunghwan Ahn,Jinmo Han,Beom Jun Woo,Nam Soo Kim*

Main category: eess.AS

TL;DR: 提出FastEnhancer，一种专为最小化实时延迟设计的流式神经语音增强模型，采用简单的编码器-解码器结构和高效的RNNFormer模块，在单CPU线程上实现最快的处理速度。


<details>
  <summary>Details</summary>
Motivation: 虽然深度神经网络在语音增强方面表现出色，但计算资源需求大，现有模型虽然减少了参数和计算量，但复杂架构在普通硬件上引入了显著的处理延迟，不适用于实时应用。

Method: 采用简单的编码器-解码器结构，结合高效的RNNFormer模块，专门优化以最小化实时延迟。

Result: 在各种客观指标评估中，FastEnhancer实现了最先进的语音质量和可懂度，同时在单CPU线程上展示了最快的处理速度。

Conclusion: FastEnhancer在保持高质量语音增强的同时，显著降低了处理延迟，适用于在线会议、智能家居设备和助听器等实时应用场景。

Abstract: Streaming speech enhancement is a crucial task for real-time applications
such as online meetings, smart home appliances, and hearing aids. Deep neural
network-based approaches achieve exceptional performance while demanding
substantial computational resources. Although recent neural speech enhancement
models have succeeded in reducing the number of parameters and
multiply-accumulate operations, their sophisticated architectures often
introduce significant processing latency on common hardware. In this work, we
propose FastEnhancer, a streaming neural speech enhancement model designed
explicitly to minimize real-world latency. It features a simple encoder-decoder
structure with efficient RNNFormer blocks. Evaluations on various objective
metrics show that FastEnhancer achieves state-of-the-art speech quality and
intelligibility while simultaneously demonstrating the fastest processing speed
on a single CPU thread. Code and pre-trained weights are publicly available
(https://github.com/aask1357/fastenhancer).

</details>


### [14] [IPDnet2: an efficient and improved inter-channel phase difference estimation network for sound source localization](https://arxiv.org/abs/2509.21900)
*Yabo Wang,Bing Yang,Xiaofei Li*

Main category: eess.AS

TL;DR: IPDnet2是对IPDnet的改进版本，通过采用oSpatialNet作为骨干网络和频率-时间池化机制，在保持定位性能的同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 原始IPDnet在处理窄带时计算复杂度高，且LSTM层的有限可扩展性限制了定位精度。

Method: 使用oSpatialNet作为骨干网络增强空间线索提取，并提出频率-时间池化机制压缩频率和时间分辨率。

Result: IPDnet2在仅需不到2%计算成本的情况下达到与IPDnet相当的定位性能，并通过扩大模型规模实现了最先进的SSL性能。

Conclusion: IPDnet2在定位精度和效率方面均有显著提升，同时保持相对较低的复杂度。

Abstract: IPDnet is our recently proposed real-time sound source localization network.
It employs alternating full-band and narrow-band (B)LSTMs to learn the
full-band correlation and narrow-band extraction of DP-IPD, respectively, which
achieves superior performance. However, processing narrow-band independently
incurs high computational complexity and the limited scalability of LSTM layers
constrains the localization accuracy. In this work, we extend IPDnet to
IPDnet2, improving both localization accuracy and efficiency. IPDnet2 adapts
the oSpatialNet as the backbone to enhance spatial cues extraction and provide
superior scalability. Additionally, a simple yet effective frequency-time
pooling mechanism is proposed to compress frequency and time resolutions and
thus reduce computational cost, and meanwhile not losing localization
capability. Experimental results show that IPDnet2 achieves comparable
localization performance with IPDnet while only requiring less than 2\% of its
computation cost. Moreover, the proposed network achieves state-of-the-art SSL
performance by scaling up the model size while still maintaining relatively low
complexity.

</details>


### [15] [AUV: Teaching Audio Universal Vector Quantization with Single Nested Codebook](https://arxiv.org/abs/2509.21968)
*Yushen Chen,Kai Hu,Long Zhou,Shulin Feng,Xusheng Yang,Hangting Chen,Xie Chen*

Main category: eess.AS

TL;DR: AUV是一个统一的神经音频编解码器，使用单一码本，能够在约700 bps的比特率下重建语音和通用音频（包括人声、音乐和声音）。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理混合域音频的统一编解码器，避免为不同音频类型使用专门的编解码器。

Method: 使用嵌套域特定分区的matryoshka码本，配合相应的教师模型进行蒸馏，采用单阶段训练。编码器-解码器架构基于conformer风格，使用STFT特征作为音频表示。

Result: AUV在音频重建能力上与最先进的域特定单层量化器编解码器相当，展示了单一码本音频通用向量量化的潜力。

Conclusion: AUV证明了使用单一码本处理混合域音频的可行性，为音频编解码提供了统一的解决方案。

Abstract: We propose AUV, a unified neural audio codec with a single codebook, which
enables a favourable reconstruction of speech and further extends to general
audio, including vocal, music, and sound. AUV is capable of tackling any 16 kHz
mixed-domain audio segment at bit rates around 700 bps. To accomplish this, we
guide the matryoshka codebook with nested domain-specific partitions, assigned
with corresponding teacher models to perform distillation, all in a
single-stage training. A conformer-style encoder-decoder architecture with STFT
features as audio representation is employed, yielding better audio quality.
Comprehensive evaluations demonstrate that AUV exhibits comparable audio
reconstruction ability to state-of-the-art domain-specific single-layer
quantizer codecs, showcasing the potential of audio universal vector
quantization with a single codebook. The pre-trained model and demo samples are
available at https://swivid.github.io/AUV/.

</details>


### [16] [Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias](https://arxiv.org/abs/2509.22061)
*Shree Harsha Bokkahalli Satish,Harm Lameris,Olivier Perrotin,Gustav Eje Henter,Éva Székely*

Main category: eess.AS

TL;DR: 本文首次系统评估语音延续任务中的偏见，研究性别和发声类型对模型延续行为的影响，发现存在显著的性别偏见和发声质量偏见。


<details>
  <summary>Details</summary>
Motivation: 语音延续任务提供了比对话更直接的设置来探测语音基础模型中的偏见，因为其约束在单一音频流中。

Method: 评估三个最新模型：SpiritLM（基础和表达版本）、VAE-GSLM和SpeechGPT，使用说话人相似性、声音质量保持和基于文本的偏见指标。

Result: 结果显示，虽然说话人相似性和连贯性仍是挑战，但文本评估揭示了显著的模型和性别交互作用：当连贯性足够高时，性别效应在文本指标上显现，如能动性和句子极性。此外，女性提示的发声质量更强烈地回归到正常发声模式。

Conclusion: 语音延续可作为语音基础模型中社会相关表征偏见的受控探针，随着延续质量的提高，它将成为一个越来越有信息量的诊断工具。

Abstract: Speech Continuation (SC) is the task of generating a coherent extension of a
spoken prompt while preserving both semantic context and speaker identity.
Because SC is constrained to a single audio stream, it offers a more direct
setting for probing biases in speech foundation models than dialogue does. In
this work we present the first systematic evaluation of bias in SC,
investigating how gender and phonation type (breathy, creaky, end-creak) affect
continuation behaviour. We evaluate three recent models: SpiritLM (base and
expressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality
preservation, and text-based bias metrics. Results show that while both speaker
similarity and coherence remain a challenge, textual evaluations reveal
significant model and gender interactions: once coherence is sufficiently high
(for VAE-GSLM), gender effects emerge on text-metrics such as agency and
sentence polarity. In addition, continuations revert toward modal phonation
more strongly for female prompts than for male ones, revealing a systematic
voice-quality bias. These findings highlight SC as a controlled probe of
socially relevant representational biases in speech foundation models, and
suggest that it will become an increasingly informative diagnostic as
continuation quality improves.

</details>


### [17] [Speaker Anonymisation for Speech-based Suicide Risk Detection](https://arxiv.org/abs/2509.22148)
*Ziyun Cui,Sike Jia,Yang Lin,Yinan Duan,Diyang Qu,Runsen Chen,Chao Zhang,Chang Lei,Wen Wu*

Main category: eess.AS

TL;DR: 该研究首次系统性地研究了基于语音的自杀风险检测中的说话人匿名化方法，评估了多种匿名化技术在保护说话人身份和保留自杀风险检测信息之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 青少年自杀是全球性健康问题，语音为自杀风险检测提供了经济有效的模态。由于涉及脆弱人群，保护说话人身份尤为重要，因为语音本身可能泄露个人身份信息。

Method: 研究了广泛的匿名化方法，包括基于传统信号处理、神经语音转换和语音合成的技术，并构建了综合评估框架来评估保护说话人身份与保留自杀风险检测信息之间的权衡。

Result: 结果显示，结合保留互补信息的匿名化方法能够获得与原始语音相当的检测性能，同时为脆弱人群提供说话人身份保护。

Conclusion: 通过组合不同的匿名化方法，可以在保护说话人身份的同时保持自杀风险检测的有效性，为脆弱人群提供安全的语音分析解决方案。

Abstract: Adolescent suicide is a critical global health issue, and speech provides a
cost-effective modality for automatic suicide risk detection. Given the
vulnerable population, protecting speaker identity is particularly important,
as speech itself can reveal personally identifiable information if the data is
leaked or maliciously exploited. This work presents the first systematic study
of speaker anonymisation for speech-based suicide risk detection. A broad range
of anonymisation methods are investigated, including techniques based on
traditional signal processing, neural voice conversion, and speech synthesis. A
comprehensive evaluation framework is built to assess the trade-off between
protecting speaker identity and preserving information essential for suicide
risk detection. Results show that combining anonymisation methods that retain
complementary information yields detection performance comparable to that of
original speech, while achieving protection of speaker identity for vulnerable
populations.

</details>


### [18] [Towards Cross-Task Suicide Risk Detection via Speech LLM](https://arxiv.org/abs/2509.22153)
*Jialun Li,Weitao Jiang,Ziyun Cui,Yinan Duan,Diyang Qu,Chao Zhang,Runsen Chen,Chang Lei,Wen Wu*

Main category: eess.AS

TL;DR: 该论文首次研究了跨任务方法，将多种语音自杀风险评估任务统一到单一模型中，使用语音大语言模型作为骨干，并采用混合DoRA专家(MoDE)方法来动态捕捉不同评估中的互补线索。


<details>
  <summary>Details</summary>
Motivation: 青少年自杀风险是严重的公共卫生问题，语音检测提供了一种非侵入性且可扩展的方法。现有方法通常一次只关注单一语音评估任务，需要更有效的跨任务统一方法。

Method: 利用语音大语言模型作为骨干，结合混合DoRA专家(MoDE)方法，动态捕捉不同评估任务中的互补线索。在1,223名参与者的十个自发语音任务上进行测试。

Result: MoDE不仅比单任务专门模型和传统联合调优方法获得更高的检测准确率，还提供了更好的置信度校准，这对医学检测任务尤为重要。

Conclusion: 跨任务的统一模型方法在语音自杀风险评估中表现出优越性能，特别是在准确性和置信度校准方面，为医疗检测任务提供了更可靠的解决方案。

Abstract: Suicide risk among adolescents remains a critical public health concern, and
speech provides a non-invasive and scalable approach for its detection.
Existing approaches, however, typically focus on one single speech assessment
task at a time. This paper, for the first time, investigates cross-task
approaches that unify diverse speech suicide risk assessment tasks within a
single model. Specifically, we leverage a speech large language model as the
backbone and incorporate a mixture of DoRA experts (MoDE) approach to capture
complementary cues across diverse assessments dynamically. The proposed
approach was tested on 1,223 participants across ten spontaneous speech tasks.
Results demonstrate that MoDE not only achieves higher detection accuracy than
both single-task specialised models and conventional joint-tuning approaches,
but also provides better confidence calibration, which is especially important
for medical detection tasks.

</details>


### [19] [Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis](https://arxiv.org/abs/2509.22167)
*Zhikang Niu,Shujie Hu,Jeongsoo Choi,Yushen Chen,Peining Chen,Pengcheng Zhu,Yunting Yang,Bowen Zhang,Jian Zhao,Chunhui Wang,Xie Chen*

Main category: eess.AS

TL;DR: 提出Semantic-VAE框架，通过语义对齐正则化解决VAE在零样本TTS中的重建-生成权衡问题，显著提升合成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统mel-spectrogram在零样本文本转语音中存在冗余问题，而VAE潜在表示面临维度困境：高维提升重建质量但降低可懂度，低维提升可懂度但牺牲重建保真度。

Method: 提出Semantic-VAE框架，在潜在空间中引入语义对齐正则化，在高维潜在表示中捕获语义结构，缓解重建-生成权衡。

Result: 在F5-TTS中集成后，在LibriSpeech-PC上达到2.10% WER和0.64说话人相似度，优于mel-based系统(2.23%, 0.60)和基础VAE(2.65%, 0.59)。

Conclusion: Semantic-VAE有效克服了VAE在零样本TTS中的优化困境，显著提升了合成质量和训练效率。

Abstract: While mel-spectrograms have been widely utilized as intermediate
representations in zero-shot text-to-speech (TTS), their inherent redundancy
leads to inefficiency in learning text-speech alignment. Compact VAE-based
latent representations have recently emerged as a stronger alternative, but
they also face a fundamental optimization dilemma: higher-dimensional latent
spaces improve reconstruction quality and speaker similarity, but degrade
intelligibility, while lower-dimensional spaces improve intelligibility at the
expense of reconstruction fidelity. To overcome this dilemma, we propose
Semantic-VAE, a novel VAE framework that utilizes semantic alignment
regularization in the latent space. This design alleviates the
reconstruction-generation trade-off by capturing semantic structure in
high-dimensional latent representations. Extensive experiments demonstrate that
Semantic-VAE significantly improves synthesis quality and training efficiency.
When integrated into F5-TTS, our method achieves 2.10% WER and 0.64 speaker
similarity on LibriSpeech-PC, outperforming mel-based systems (2.23%, 0.60) and
vanilla acoustic VAE baselines (2.65%, 0.59). We also release the code and
models to facilitate further research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [20] [Golden Tonnetz](https://arxiv.org/abs/2509.21428)
*Yusuke Imai*

Main category: cs.SD

TL;DR: 本文提出了一种基于黄金比例的几何音乐表示方法，通过在黄金三角形上排列7个音调来表示大调/小调音阶及其主和弦、属和弦和下属和弦，并构建了"黄金Tonnetz"来表示所有大调/小调音阶和三和弦。


<details>
  <summary>Details</summary>
Motivation: 探索音乐与黄金比例之间的新联系，将音乐概念通过几何形状进行可视化表示，特别是利用黄金三角形的特性来表示音阶与和弦关系。

Method: 在黄金三角形上排列7个音调，使其能够表示给定大调/小调音阶及其主和弦、属和弦和下属和弦，并构建黄金Tonnetz来表示所有音阶和三和弦。

Result: 成功实现了通过黄金三角形和黄金Tonnetz来表示音乐音阶、和弦关系，以及新黎曼理论中的相对、平行和导音交换变换。

Conclusion: 黄金比例与音乐之间存在深刻的几何联系，黄金Tonnetz为音乐理论提供了一种新颖的几何表示框架，能够统一表示音阶、和弦及其变换关系。

Abstract: Musical concepts have been represented by geometry with tones. For example,
in the chromatic circle, the twelve tones are represented by twelve points on a
circle, and in Tonnetz, the relationships among harmonies are represented by a
triangular lattice. Recently, we have shown that several arrangements of tones
on the regular icosahedron can be associated with chromatic scales, whole-tone
scales, major tones, and minor tones through the golden ratio. Here, we
investigate another type of connection between music and the golden ratio. We
show that there exists an arrangement of 7 tones on a golden triangle that can
represent a given major/minor scale and its tonic, dominant, and subdominant
chords by golden triangles. By applying this finding, we propose "golden
Tonnetz" which represents all the major/minor scales and triads by the golden
triangles or gnomons and also represents relative, parallel, and leading-tone
exchange transformations in Neo-Riemannian theory by transformations among the
golden triangles and gnomons.

</details>


### [21] [Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training](https://arxiv.org/abs/2509.21522)
*Naisong Zhou,Saisamarth Rajesh Phaye,Milos Cernak,Tijana Stojkovic,Andy Pearce,Andrea Cavallaro,Andy Harper*

Main category: cs.SD

TL;DR: 提出SFMSE方法，通过流匹配技术实现高效语音增强，单步推理即可达到接近扩散模型60步的效果，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语音增强中表现优异但计算成本高，流匹配技术能提供更高效的替代方案，需要解决实时应用的计算效率问题。

Method: 使用流匹配学习直接向量场，训练单一、步长不变的模型，通过目标时间步条件化速度场，实现无需架构修改或微调的单步、少步或多步去噪。

Result: 单步SFMSE推理在消费级GPU上实现0.013的实时因子，感知质量与需要60步神经函数评估的强扩散基线相当。

Conclusion: SFMSE成功弥合了高质量生成式语音增强与低延迟约束之间的差距，为实时应用提供了可行的解决方案。

Abstract: Diffusion-based generative models have achieved state-of-the-art performance
for perceptual quality in speech enhancement (SE). However, their iterative
nature requires numerous Neural Function Evaluations (NFEs), posing a challenge
for real-time applications. On the contrary, flow matching offers a more
efficient alternative by learning a direct vector field, enabling high-quality
synthesis in just a few steps using deterministic ordinary differential
equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech
Enhancement (SFMSE), a novel approach that trains a single, step-invariant
model. By conditioning the velocity field on the target time step during a
one-stage training process, SFMSE can perform single, few, or multi-step
denoising without any architectural changes or fine-tuning. Our results
demonstrate that a single-step SFMSE inference achieves a real-time factor
(RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable
to a strong diffusion baseline requiring 60 NFEs. This work also provides an
empirical analysis of the role of stochasticity in training and inference,
bridging the gap between high-quality generative SE and low-latency
constraints.

</details>


### [22] [Real-time implementation of vibrato transfer as an audio effect](https://arxiv.org/abs/2509.21544)
*Jeremy Hyrkas*

Main category: cs.SD

TL;DR: 提出了一种实时颤音转移算法的近似实现，通过高效基频估计算法和时域多相IIR滤波器，将目标信号的颤音模式应用到输入声音上，并增加了振幅调制转移功能。


<details>
  <summary>Details</summary>
Motivation: 现有的颤音转移算法在计算上限制了实时实现，需要开发实时近似方法来克服这些限制。

Method: 使用高效基频估计算法和时域多相IIR滤波器来近似解析信号，并提出了振幅调制转移方法，扩展了传统基于延迟的颤音效果功能。

Result: 开发了实时颤音转移算法，可作为VST插件实现，适用于声音设计、声音变形和合成声音的实时颤音控制。

Conclusion: 该算法成功实现了实时颤音转移，超越了传统延迟型颤音效果的能力，在音频效果处理领域具有应用价值。

Abstract: An algorithm for deriving delay functions based on real examples of vibrato
was recently introduced and can be used to perform a vibrato transfer, in which
the vibrato pattern of a target signal is imparted onto an incoming sound using
a delay line. The algorithm contains methods that computationally restrict a
real-time implementation. Here, a real-time approximation is presented that
incorporates an efficient fundamental frequency estimation algorithm and
time-domain polyphase IIR filters that approximate an analytic signal. The
vibrato transfer algorithm is further supplemented with a proposed method to
transfer the amplitude modulation of the target sound, moving this method
beyond the capabilities of typical delay-based vibrato effects. Modifications
to the original algorithm for real-time use are detailed here and available as
source code for an implementation as a VST plugin. This algorithm has
applications as an audio effect in sound design, sound morphing, and real-time
vibrato control of synthesized sounds.

</details>


### [23] [Preserving Russek's "Summermood" Using Reality Check and a DeltaLab DL-4 Approximation](https://arxiv.org/abs/2509.21560)
*Jeremy Hyrkas,Pablo Dodero Carrillo,Teresa Díaz de Cossio Sánchez*

Main category: cs.SD

TL;DR: 开发Pure Data补丁来模拟已停产的DeltaLab DL-4延迟设备，以保存和现场演奏Antonio Russek的电子音乐作品《Summermood》


<details>
  <summary>Details</summary>
Motivation: 为持续维护现场电子音乐作品的演奏能力，特别是针对使用已停产硬件设备（DeltaLab DL-4延迟机架）的作品进行保存和再现

Method: 在Pure Data中近似模拟DL-4的声音和独特功能，通过比较乐谱设置与官方录音来优化实现，集成到基于Null Piece的现场演奏补丁中，并使用Reality Check框架进行回归测试

Result: 成功创建了能够替代DL-4的Pure Data补丁库，使《Summermood》能够在没有原始硬件的情况下重新进入现场演奏曲目

Conclusion: 该方法为保存依赖特定硬件的电子音乐作品提供了可行方案，通过软件模拟和持续测试确保作品的可演奏性和跨平台兼容性

Abstract: As a contribution towards ongoing efforts to maintain electroacoustic
compositions for live performance, we present a collection of Pure Data patches
to preserve and perform Antonio Russek's piece "Summermood" for bass flute and
live electronics. The piece, originally written for the DeltaLab DL-4 delay
rack unit, contains score markings specific to the DL-4. Here, we approximate
the sound and unique functionality of the DL-4 in Pure Data, then refine our
implementation to better match the unit on which the piece was performed by
comparing settings from the score to two official recordings of the piece. The
DL-4 emulation is integrated into a patch for live performance based on the
Null Piece, and regression tested using the Reality Check framework for Pure
Data. Using this library of patches, Summermood can be brought back into live
rotation without the use of the now discontinued DL-4. The patches will be
continuously tested to ensure that the piece is playable across computer
environments and as the Pure Data programming language is updated.

</details>


### [24] [Guiding Audio Editing with Audio Language Model](https://arxiv.org/abs/2509.21625)
*Zitong Lan,Yiduo Hao,Mingmin Zhao*

Main category: cs.SD

TL;DR: SmartDJ是一个创新的立体声音频编辑框架，结合音频语言模型的推理能力和潜在扩散的生成能力，能够根据高级指令自动分解并执行原子编辑操作。


<details>
  <summary>Details</summary>
Motivation: 当前生成音频编辑模型依赖模板化指令格式且仅限于单声道音频，无法处理声明式音频编辑，即用户只声明期望结果而将编辑操作细节留给系统处理。

Method: SmartDJ将高级指令分解为原子编辑操作序列（如添加、移除或空间重定位事件），然后通过训练好的扩散模型执行这些操作。设计了数据合成管道来生成配对示例。

Result: 实验表明SmartDJ在感知质量、空间真实性和语义对齐方面优于现有音频编辑方法。

Conclusion: SmartDJ框架成功实现了声明式立体声音频编辑，为VR/AR沉浸、虚拟会议和声音设计等应用提供了更强大的音频编辑能力。

Abstract: Audio editing plays a central role in VR/AR immersion, virtual conferencing,
sound design, and other interactive media. However, recent generative audio
editing models depend on template-like instruction formats and are restricted
to mono-channel audio. These models fail to deal with declarative audio
editing, where the user declares what the desired outcome should be, while
leaving the details of editing operations to the system. We introduce SmartDJ,
a novel framework for stereo audio editing that combines the reasoning
capability of audio language models with the generative power of latent
diffusion. Given a high-level instruction, SmartDJ decomposes it into a
sequence of atomic edit operations, such as adding, removing, or spatially
relocating events. These operations are then executed by a diffusion model
trained to manipulate stereo audio. To support this, we design a data synthesis
pipeline that produces paired examples of high-level instructions, atomic edit
operations, and audios before and after each edit operation. Experiments
demonstrate that SmartDJ achieves superior perceptual quality, spatial realism,
and semantic alignment compared to prior audio editing methods. Demos are
available at https://zitonglan.github.io/project/smartdj/smartdj.html.

</details>


### [25] [MusicWeaver: Coherent Long-Range and Editable Music Generation from a Beat-Aligned Structural Plan](https://arxiv.org/abs/2509.21714)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.SD

TL;DR: MusicWeaver是一个基于节拍对齐结构计划的音乐生成模型，通过可编辑的中间表示解决现有模型在长程结构和编辑能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前音乐生成器能捕捉局部纹理但难以建模长程结构，导致节拍不准、段落过渡弱、编辑能力有限的问题。

Method: 包含规划器（将提示转换为编码音乐形式和作曲线索的结构计划）和基于扩散的生成器（在计划指导下合成音乐）。

Result: 实验表明MusicWeaver在保真度和可控性方面达到最先进水平，生成的音乐更接近人类作曲作品。

Conclusion: MusicWeaver通过结构计划作为可编辑中间表示，有效提升了音乐生成的长程结构连贯性和编辑能力。

Abstract: Current music generators capture local textures but often fail to model
long-range structure, leading to off-beat outputs, weak section transitions,
and limited editing capability. We present MusicWeaver, a music generation
model conditioned on a beat-aligned structural plan. This plan serves as an
editable intermediate between the input prompt and the generated music,
preserving global form and enabling professional, localized edits. MusicWeaver
consists of a planner, which translates prompts into a structural plan encoding
musical form and compositional cues, and a diffusion-based generator, which
synthesizes music under the plan's guidance. To assess generation and editing
quality, we introduce two metrics: the Structure Coherence Score (SCS) for
evaluating long-range form and timing, and the Edit Fidelity Score (EFS) for
measuring the accuracy of realizing plan edits. Experiments demonstrate that
MusicWeaver achieves state-of-the-art fidelity and controllability, producing
music closer to human-composed works. Music results can be found on our project
page: https://musicweaver.github.io/.

</details>


### [26] [Frustratingly Easy Zero-Day Audio DeepFake Detection via Retrieval Augmentation and Profile Matching](https://arxiv.org/abs/2509.21728)
*Xuechen Liu,Xin Wang,Junichi Yamagishi*

Main category: cs.SD

TL;DR: 提出无需训练的零日音频深度伪造检测框架，通过知识表示、检索增强和语音特征匹配，在DeepFake-Eval-2024上达到与微调模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于基础模型和大规模训练数据的音频深度伪造检测器在零日攻击（使用新型合成方法生成的音频）上表现不佳，而传统微调方法在需要快速响应时存在问题。

Method: 基于知识表示、检索增强和语音特征匹配的训练免费框架，提出简单的知识检索和集成方法，无需额外模型训练。

Result: 在DeepFake-Eval-2024上达到与微调模型相当的性能，通过消融研究验证了检索池大小和语音特征属性的相关性。

Conclusion: 提出的训练免费框架能有效应对零日音频深度伪造攻击，无需模型微调即可获得良好检测性能。

Abstract: Modern audio deepfake detectors using foundation models and large training
datasets have achieved promising detection performance. However, they struggle
with zero-day attacks, where the audio samples are generated by novel synthesis
methods that models have not seen from reigning training data. Conventional
approaches against such attacks require fine-tuning the detectors, which can be
problematic when prompt response is required. This study introduces a
training-free framework for zero-day audio deepfake detection based on
knowledge representations, retrieval augmentation, and voice profile matching.
Based on the framework, we propose simple yet effective knowledge retrieval and
ensemble methods that achieve performance comparable to fine-tuned models on
DeepFake-Eval-2024, without any additional model-wise training. We also conduct
ablation studies on retrieval pool size and voice profile attributes,
validating their relevance to the system efficacy.

</details>


### [27] [Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription](https://arxiv.org/abs/2509.21739)
*Michael Yeung,Keisuke Toyama,Toya Teramoto,Shusuke Takahashi,Tamaki Kojima*

Main category: cs.SD

TL;DR: 本文提出Noise-to-Notes框架，将自动鼓声转录重新定义为条件生成任务，利用扩散模型将音频条件的高斯噪声转换为带速度的鼓声事件，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动鼓声转录被定义为判别任务，本文将其重新定义为条件生成任务，以获得灵活的速度-精度权衡和强大的修复能力。

Method: 使用扩散模型框架，引入退火伪Huber损失来联合优化二进制起始点和连续速度值，并整合音乐基础模型特征以增强低层频谱图特征。

Result: 实验结果表明，包含音乐基础模型特征显著提高了鲁棒性，Noise-to-Notes在多个自动鼓声转录基准测试中建立了新的最先进性能。

Conclusion: 将自动鼓声转录重新定义为生成任务，结合扩散模型和音乐基础模型特征，能够有效处理二进制起始点和连续速度值的联合优化，并在鲁棒性和性能方面取得显著提升。

Abstract: Automatic drum transcription (ADT) is traditionally formulated as a
discriminative task to predict drum events from audio spectrograms. In this
work, we redefine ADT as a conditional generative task and introduce
Noise-to-Notes (N2N), a framework leveraging diffusion modeling to transform
audio-conditioned Gaussian noise into drum events with associated velocities.
This generative diffusion approach offers distinct advantages, including a
flexible speed-accuracy trade-off and strong inpainting capabilities. However,
the generation of binary onset and continuous velocity values presents a
challenge for diffusion models, and to overcome this, we introduce an Annealed
Pseudo-Huber loss to facilitate effective joint optimization. Finally, to
augment low-level spectrogram features, we propose incorporating features
extracted from music foundation models (MFMs), which capture high-level
semantic information and enhance robustness to out-of-domain drum audio.
Experimental results demonstrate that including MFM features significantly
improves robustness and N2N establishes a new state-of-the-art performance
across multiple ADT benchmarks.

</details>


### [28] [Lightweight Front-end Enhancement for Robust ASR via Frame Resampling and Sub-Band Pruning](https://arxiv.org/abs/2509.21833)
*Siyi Zhao,Wei Wang,Yanmin Qian*

Main category: cs.SD

TL;DR: 提出了一种优化方法，通过层间帧重采样和渐进子带剪枝来降低语音增强的计算成本，同时保持ASR性能。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别在噪声环境中的鲁棒性仍然具有挑战性，而语音增强前端作为预处理步骤虽然能缓解噪声，但通常引入显著的计算开销。

Method: 集成层间帧重采样和渐进子带剪枝。帧重采样在层内对输入进行下采样，利用残差连接减轻信息损失；子带剪枝逐步排除信息量较少的频带。

Result: 在合成和真实世界噪声数据集上的广泛实验表明，该系统相比标准BSRNN减少了超过66%的SE计算开销，同时保持了强大的ASR性能。

Conclusion: 所提出的优化方法能显著降低语音增强的计算成本，而不会影响ASR性能，为噪声环境下的高效语音识别提供了可行方案。

Abstract: Recent advancements in automatic speech recognition (ASR) have achieved
notable progress, whereas robustness in noisy environments remains challenging.
While speech enhancement (SE) front-ends are widely used to mitigate noise as a
preprocessing step for ASR, they often introduce computational non-negligible
overhead. This paper proposes optimizations to reduce SE computational costs
without compromising ASR performance. Our approach integrates layer-wise frame
resampling and progressive sub-band pruning. Frame resampling downsamples
inputs within layers, utilizing residual connections to mitigate information
loss. Simultaneously, sub-band pruning progressively excludes less informative
frequency bands, further reducing computational demands. Extensive experiments
on synthetic and real-world noisy datasets demonstrate that our system reduces
SE computational overhead over 66 compared to the standard BSRNN, while
maintaining strong ASR performance.

</details>


### [29] [Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment](https://arxiv.org/abs/2509.21919)
*Yunyi Liu,Shaofan Yang,Kai Li,Xu Li*

Main category: cs.SD

TL;DR: 提出一个可控的文本到移动声音生成框架，通过文本提示生成3D空间中移动的声音源


<details>
  <summary>Details</summary>
Motivation: 人类听觉感知受3D空间中移动声源影响，但现有生成声音模型主要局限于单声道信号或静态空间音频

Method: 构建合成数据集记录双耳格式的移动声音、空间轨迹和文本描述；训练文本到轨迹预测模型；微调预训练文本到音频模型生成时间对齐的单声道声音；使用预测轨迹模拟空间音频

Result: 实验评估显示文本到轨迹模型具有合理的空间理解能力

Conclusion: 该方法可轻松集成到现有文本到音频生成流程，并可扩展到其他空间音频格式的移动声音生成

Abstract: Human auditory perception is shaped by moving sound sources in 3D space, yet
prior work in generative sound modelling has largely been restricted to mono
signals or static spatial audio. In this work, we introduce a framework for
generating moving sounds given text prompts in a controllable fashion. To
enable training, we construct a synthetic dataset that records moving sounds in
binaural format, their spatial trajectories, and text captions about the sound
event and spatial motion. Using this dataset, we train a text-to-trajectory
prediction model that outputs the three-dimensional trajectory of a moving
sound source given text prompts. To generate spatial audio, we first fine-tune
a pre-trained text-to-audio generative model to output temporally aligned mono
sound with the trajectory. The spatial audio is then simulated using the
predicted temporally-aligned trajectory. Experimental evaluation demonstrates
reasonable spatial understanding of the text-to-trajectory model. This approach
could be easily integrated into existing text-to-audio generative workflow and
extended to moving sound generation in other spatial audio formats.

</details>


### [30] [Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks](https://arxiv.org/abs/2509.22060)
*Aravindhan G,Yuvaraj Govindarajulu,Parin Shah*

Main category: cs.SD

TL;DR: 本文研究语音识别系统的对抗攻击，开发了成本效益高的白盒攻击和非可迁移性黑盒攻击方法，通过混合模型生成微小但有效的对抗样本（信噪比35dB），揭示了开源模型的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注受限优化的白盒攻击和基于可迁移性的黑盒攻击，本文旨在探索更高效的白盒攻击和非可迁移性黑盒攻击方法，并研究投毒攻击如何导致最先进模型性能下降。

Method: 借鉴快速梯度符号方法和零阶优化方法，开发混合模型生成对抗样本，通过投毒攻击来降低模型性能，实验中使用信噪比35dB的微小扰动。

Result: 成功生成了在1分钟内可创建的微小但有效的对抗样本，这些样本能够欺骗最先进的开源语音识别模型，证明了模型存在实际安全漏洞。

Conclusion: 语音识别系统存在严重的安全漏洞，对抗样本攻击具有实际安全影响，强调了加强对抗安全防护的必要性。

Abstract: Recent studies have demonstrated the vulnerability of Automatic Speech
Recognition systems to adversarial examples, which can deceive these systems
into misinterpreting input speech commands. While previous research has
primarily focused on white-box attacks with constrained optimizations, and
transferability based black-box attacks against commercial Automatic Speech
Recognition devices, this paper explores cost efficient white-box attack and
non transferability black-box adversarial attacks on Automatic Speech
Recognition systems, drawing insights from approaches such as Fast Gradient
Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper
includes how poisoning attack can degrade the performances of state-of-the-art
models leading to misinterpretation of audio signals. Through experimentation
and analysis, we illustrate how hybrid models can generate subtle yet impactful
adversarial examples with very little perturbation having Signal Noise Ratio of
35dB that can be generated within a minute. These vulnerabilities of
state-of-the-art open source model have practical security implications, and
emphasize the need for adversarial security.

</details>


### [31] [Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling](https://arxiv.org/abs/2509.22062)
*Junjie Cao,Yichen Han,Ruonan Zhang,Xiaoyang Hao,Hongxiang Li,Shuaijiang Zhao,Yue Liu,Xiao-Ping Zhng*

Main category: cs.SD

TL;DR: CaT-TTS是一个新颖的零样本语音合成框架，通过语义增强的编解码器和理解-生成双Transformer架构，解决了现有LLM语音合成系统的信息丢失、语义结构缺乏和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的自回归语音合成系统的关键挑战：神经音频编解码器离散化过程中的信息损失、分层声学令牌缺乏明确语义结构，以及自回归过程容易产生错误累积的问题。

Method: 1. 提出S3Codec分割RVQ编解码器，通过从最先进ASR模型进行语义蒸馏，将明确的语言特征注入主要码本；2. 设计理解-生成双Transformer架构，将理解与渲染解耦；3. 引入掩码音频并行推理策略增强生成稳定性。

Result: 该方法实现了稳健且语义基础的零样本语音合成，简化了学习任务并提高了生成稳定性。

Conclusion: CaT-TTS通过语义增强的编解码器、解耦的理解-生成架构和动态引导的解码策略，有效解决了现有LLM语音合成系统的局限性，为稳健的零样本语音合成提供了新思路。

Abstract: Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech
(TTS) systems, while achieving state-of-the-art quality, still face critical
challenges. The foundation of this LLM-based paradigm is the discretization of
the continuous speech waveform into a sequence of discrete tokens by neural
audio codec. However, single codebook modeling is well suited to text LLMs, but
suffers from significant information loss; hierarchical acoustic tokens,
typically generated via Residual Vector Quantization (RVQ), often lack explicit
semantic structure, placing a heavy learning burden on the model. Furthermore,
the autoregressive process is inherently susceptible to error accumulation,
which can degrade generation stability. To address these limitations, we
propose CaT-TTS, a novel framework for robust and semantically-grounded
zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that
injects explicit linguistic features into its primary codebook via semantic
distillation from a state-of-the-art ASR model, providing a structured
representation that simplifies the learning task. Second, we propose an
``Understand-then-Generate'' dual-Transformer architecture that decouples
comprehension from rendering. An initial ``Understanding'' Transformer models
the cross-modal relationship between text and the audio's semantic tokens to
form a high-level utterance plan. A subsequent ``Generation'' Transformer then
executes this plan, autoregressively synthesizing hierarchical acoustic tokens.
Finally, to enhance generation stability, we introduce Masked Audio Parallel
Inference (MAPI), a nearly parameter-free inference strategy that dynamically
guides the decoding process to mitigate local errors.

</details>


### [32] [Cross-Dialect Bird Species Recognition with Dialect-Calibrated Augmentation](https://arxiv.org/abs/2509.22317)
*Jiani Ding,Qiyang Sun,Alican Akman,Björn W. Schuller*

Main category: cs.SD

TL;DR: 提出了一种基于TDNN的轻量级框架，通过频率敏感归一化、对抗训练和多级数据增强来解决鸟类声音方言变异问题，显著提升了跨方言识别准确率。


<details>
  <summary>Details</summary>
Motivation: 方言变异阻碍了被动声学监测中鸟类叫声的自动识别，需要开发能够处理不同地区方言差异的鲁棒识别系统。

Method: 使用TDNN架构，结合实例频率归一化、梯度反转对抗训练学习区域不变嵌入，以及包含波形扰动、Mixup和CycleGAN转移的多级数据增强方案。

Result: 系统相比基线TDNN将跨方言准确率提升了多达20个百分点，同时保持区域内性能，Grad-CAM和LIME分析显示模型关注稳定的谐波频带。

Conclusion: 研究表明轻量级、透明且方言鲁棒的鸟类声音识别是可以实现的，为生态学研究提供了有意义的解释。

Abstract: Dialect variation hampers automatic recognition of bird calls collected by
passive acoustic monitoring. We address the problem on DB3V, a three-region,
ten-species corpus of 8-s clips, and propose a deployable framework built on
Time-Delay Neural Networks (TDNNs). Frequency-sensitive normalisation (Instance
Frequency Normalisation and a gated Relaxed-IFN) is paired with
gradient-reversal adversarial training to learn region-invariant embeddings. A
multi-level augmentation scheme combines waveform perturbations, Mixup for rare
classes, and CycleGAN transfer that synthesises Region 2 (Interior
Plains)-style audio, , with Dialect-Calibrated Augmentation (DCA) softly
down-weighting synthetic samples to limit artifacts. The complete system lifts
cross-dialect accuracy by up to twenty percentage points over baseline TDNNs
while preserving in-region performance. Grad-CAM and LIME analyses show that
robust models concentrate on stable harmonic bands, providing ecologically
meaningful explanations. The study demonstrates that lightweight, transparent,
and dialect-resilient bird-sound recognition is attainable.

</details>


### [33] [Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach](https://arxiv.org/abs/2509.22378)
*Zijian Zhao,Dian Jin,Zijing Zhou*

Main category: cs.SD

TL;DR: 提出了首个基于视觉语言模型的图像到音乐生成框架，具有高可解释性和低计算成本，通过ABC记谱法连接文本和音乐模态，使用多模态检索增强生成和自优化技术生成高质量音乐。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像到音乐生成方法缺乏可解释性、依赖大量计算资源和数据集的问题，以及基于情感映射方法的局限性。

Method: 使用视觉语言模型，通过ABC记谱法将文本和音乐模态连接，应用多模态检索增强生成和自优化技术，无需外部训练即可生成高质量音乐。

Result: 在人类研究和机器评估中，该方法在音乐质量和音乐-图像一致性方面优于其他方法，显示出有希望的结果。

Conclusion: 提出的VLM-based I2M框架在保持高可解释性的同时降低了计算成本，为图像到音乐生成提供了有效的解决方案。

Abstract: Recently, Image-to-Music (I2M) generation has garnered significant attention,
with potential applications in fields such as gaming, advertising, and
multi-modal art creation. However, due to the ambiguous and subjective nature
of I2M tasks, most end-to-end methods lack interpretability, leaving users
puzzled about the generation results. Even methods based on emotion mapping
face controversy, as emotion represents only a singular aspect of art.
Additionally, most learning-based methods require substantial computational
resources and large datasets for training, hindering accessibility for common
users. To address these challenges, we propose the first Vision Language Model
(VLM)-based I2M framework that offers high interpretability and low
computational cost. Specifically, we utilize ABC notation to bridge the text
and music modalities, enabling the VLM to generate music using natural
language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and
self-refinement techniques to allow the VLM to produce high-quality music
without external training. Furthermore, we leverage the generated motivations
in text and the attention maps from the VLM to provide explanations for the
generated results in both text and image modalities. To validate our method, we
conduct both human studies and machine evaluations, where our method
outperforms others in terms of music quality and music-image consistency,
indicating promising results. Our code is available at
https://github.com/RS2002/Image2Music .

</details>


### [34] [From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for Speech Separation](https://arxiv.org/abs/2509.22425)
*Ke Xue,Rongfei Fan,Lixin,Dawei Zhao,Chao Zhu,Han Hu*

Main category: cs.SD

TL;DR: CSFNet提出了一种从粗到细的递归语义增强网络，通过两阶段分离和语义增强机制，显著提升了音视频语音分离的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音视频语音分离方法往往依赖静态视觉表示，未能充分利用视觉信息的语义指导潜力。

Method: 采用两阶段分离：粗分离阶段生成初步音频估计，细分离阶段将粗音频与视觉流输入AVSR模型进行递归语义增强，并设计了说话人感知融合模块和多范围谱时分离网络。

Result: 在三个基准数据集和两个噪声数据集上的实验表明，CSFNet达到了最先进的性能，粗到细改进显著。

Conclusion: 递归语义增强框架的必要性和有效性得到了验证，为音视频语音分离提供了新的思路。

Abstract: Audio-visual speech separation aims to isolate each speaker's clean voice
from mixtures by leveraging visual cues such as lip movements and facial
features. While visual information provides complementary semantic guidance,
existing methods often underexploit its potential by relying on static visual
representations. In this paper, we propose CSFNet, a Coarse-to-Separate-Fine
Network that introduces a recursive semantic enhancement paradigm for more
effective separation. CSFNet operates in two stages: (1) Coarse Separation,
where a first-pass estimation reconstructs a coarse audio waveform from the
mixture and visual input; and (2) Fine Separation, where the coarse audio is
fed back into an audio-visual speech recognition (AVSR) model together with the
visual stream. This recursive process produces more discriminative semantic
representations, which are then used to extract refined audio. To further
exploit these semantics, we design a speaker-aware perceptual fusion block to
encode speaker identity across modalities, and a multi-range spectro-temporal
separation network to capture both local and global time-frequency patterns.
Extensive experiments on three benchmark datasets and two noisy datasets show
that CSFNet achieves state-of-the-art (SOTA) performance, with substantial
coarse-to-fine improvements, validating the necessity and effectiveness of our
recursive semantic enhancement framework.

</details>


### [35] [MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark](https://arxiv.org/abs/2509.22461)
*Hui Li,Changhao Jiang,Hongyu Wang,Ming Zhang,Jiajun Sun,Zhixiong Yang,Yifei Cao,Shihan Dou,Xiaoran Fan,Baoyu Fan,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: MDAR是一个用于评估复杂、多场景、动态演化的音频推理任务的基准测试，包含3000个精心策划的问答对，涵盖五种复杂推理类别和三种问题类型。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注静态或单场景设置，无法充分捕捉多个说话者、展开事件和异构音频源交互的场景，因此需要开发能够评估复杂音频推理能力的基准。

Method: 引入MDAR基准，包含3000个与多样化音频片段链接的问答对，涵盖五种复杂推理类别（多说话者、事件演化、异构音频等）和三种问题类型（单选、多选、开放式）。

Result: 对26个最先进的音频语言模型进行基准测试，发现它们在复杂推理任务中存在局限性。Qwen2.5-Omni在单选题上达到76.67%准确率，GPT-4o Audio为68.47%；但在更具挑战性的多选题和开放式任务中，GPT-4o Audio显著优于Qwen2.5-Omni。所有模型在三种问题类型上均未达到80%性能。

Conclusion: MDAR基准揭示了音频推理面临的独特挑战，为推进音频推理研究提供了有价值的评估工具。

Abstract: The ability to reason from audio, including speech, paralinguistic cues,
environmental sounds, and music, is essential for AI agents to interact
effectively in real-world scenarios. Existing benchmarks mainly focus on static
or single-scene settings and do not fully capture scenarios where multiple
speakers, unfolding events, and heterogeneous audio sources interact. To
address these challenges, we introduce MDAR, a benchmark for evaluating models
on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR
comprises 3,000 carefully curated question-answer pairs linked to diverse audio
clips, covering five categories of complex reasoning and spanning three
question types. We benchmark 26 state-of-the-art audio language models on MDAR
and observe that they exhibit limitations in complex reasoning tasks. On
single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,
whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio
substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice
and open-ended tasks. Across all three question types, no model achieves 80%
performance. These findings underscore the unique challenges posed by MDAR and
its value as a benchmark for advancing audio reasoning research.Code and
benchmark can be found at https://github.com/luckyerr/MDAR.

</details>

{"id": "2509.22655", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22655", "abs": "https://arxiv.org/abs/2509.22655", "authors": ["Jackson Loth", "Pedro Sarmento", "Saurjya Sarkar", "Zixun Guo", "Mathieu Barthet", "Mark Sandler"], "title": "GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures", "comment": "To be published in Proceedings of the International Society for Music\n  Information Retrieval Conference (ISMIR), 2025", "summary": "In recent years, the guitar has received increased attention from the music\ninformation retrieval (MIR) community driven by the challenges posed by its\ndiverse playing techniques and sonic characteristics. Mainly fueled by deep\nlearning approaches, progress has been limited by the scarcity and limited\nannotations of datasets. To address this, we present the Guitar On Audio and\nTablatures (GOAT) dataset, comprising 5.9 hours of unique high-quality direct\ninput audio recordings of electric guitars from a variety of different guitars\nand players. We also present an effective data augmentation strategy using\nguitar amplifiers which delivers near-unlimited tonal variety, of which we\nprovide a starting 29.5 hours of audio. Each recording is annotated using\nguitar tablatures, a guitar-specific symbolic format supporting string and fret\nnumbers, as well as numerous playing techniques. For this we utilise both the\nGuitar Pro format, a software for tablature playback and editing, and a\ntext-like token encoding. Furthermore, we present competitive results using\nGOAT for MIDI transcription and preliminary results for a novel approach to\nautomatic guitar tablature transcription. We hope that GOAT opens up the\npossibilities to train novel models on a wide variety of guitar-related MIR\ntasks, from synthesis to transcription to playing technique detection."}
{"id": "2509.22727", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22727", "abs": "https://arxiv.org/abs/2509.22727", "authors": ["Ziqi Chen", "Gongyu Chen", "Yihua Wang", "Chaofan Ding", "Zihao chen", "Wei-Qiang Zhang"], "title": "DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation", "comment": "5 pages, 2 figures", "summary": "Dialect speech embodies rich cultural and linguistic diversity, yet building\ntext-to-speech (TTS) systems for dialects remains challenging due to scarce\ndata, inconsistent orthographies, and complex phonetic variation. To address\nthese issues, we present DiaMoE-TTS, a unified IPA-based framework that\nstandardizes phonetic representations and resolves grapheme-to-phoneme\nambiguities. Built upon the F5-TTS architecture, the system introduces a\ndialect-aware Mixture-of-Experts (MoE) to model phonological differences and\nemploys parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and\nConditioning Adapters for rapid transfer to new dialects. Unlike approaches\ndependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable,\nopen-data-driven synthesis. Experiments demonstrate natural and expressive\nspeech generation, achieving zero-shot performance on unseen dialects and\nspecialized domains such as Peking Opera with only a few hours of data."}
{"id": "2509.22728", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22728", "abs": "https://arxiv.org/abs/2509.22728", "authors": ["Xuanhao Zhang", "Chang Li"], "title": "Prompt-aware classifier free guidance for diffusion models", "comment": "5 pages, 3 figures", "summary": "Diffusion models have achieved remarkable progress in image and audio\ngeneration, largely due to Classifier-Free Guidance. However, the choice of\nguidance scale remains underexplored: a fixed scale often fails to generalize\nacross prompts of varying complexity, leading to oversaturation or weak\nalignment. We address this gap by introducing a prompt-aware framework that\npredicts scale-dependent quality and selects the optimal guidance at inference.\nSpecifically, we construct a large synthetic dataset by generating samples\nunder multiple scales and scoring them with reliable evaluation metrics. A\nlightweight predictor, conditioned on semantic embeddings and linguistic\ncomplexity, estimates multi-metric quality curves and determines the best scale\nvia a utility function with regularization. Experiments on MSCOCO~2014 and\nAudioCaps show consistent improvements over vanilla CFG, enhancing fidelity,\nalignment, and perceptual preference. This work demonstrates that prompt-aware\nscale selection provides an effective, training-free enhancement for pretrained\ndiffusion backbones."}
{"id": "2509.22838", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22838", "abs": "https://arxiv.org/abs/2509.22838", "authors": ["Elliot Q C Garcia", "Nicéias Silva Vilela", "Kátia Pires Nascimento do Sacramento", "Tiago A. E. Ferreira"], "title": "Text-Independent Speaker Identification Using Audio Looping With Margin Based Loss Functions", "comment": "18 pages, 6 figures", "summary": "Speaker identification has become a crucial component in various\napplications, including security systems, virtual assistants, and personalized\nuser experiences. In this paper, we investigate the effectiveness of CosFace\nLoss and ArcFace Loss for text-independent speaker identification using a\nConvolutional Neural Network architecture based on the VGG16 model, modified to\naccommodate mel spectrogram inputs of variable sizes generated from the\nVoxceleb1 dataset. Our approach involves implementing both loss functions to\nanalyze their effects on model accuracy and robustness, where the Softmax loss\nfunction was employed as a comparative baseline. Additionally, we examine how\nthe sizes of mel spectrograms and their varying time lengths influence model\nperformance. The experimental results demonstrate superior identification\naccuracy compared to traditional Softmax loss methods. Furthermore, we discuss\nthe implications of these findings for future research."}
{"id": "2509.22795", "categories": ["eess.SP", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22795", "abs": "https://arxiv.org/abs/2509.22795", "authors": ["Yi Hu", "Zheyuan Cheng"], "title": "Generative Modeling and Decision Fusion for Unknown Event Detection and Classification Using Synchrophasor Data", "comment": "10 pages", "summary": "Reliable detection and classification of power system events are critical for\nmaintaining grid stability and situational awareness. Existing approaches often\ndepend on limited labeled datasets, which restricts their ability to generalize\nto rare or unseen disturbances. This paper proposes a novel framework that\nintegrates generative modeling, sliding-window temporal processing, and\ndecision fusion to achieve robust event detection and classification using\nsynchrophasor data. A variational autoencoder-generative adversarial network is\nemployed to model normal operating conditions, where both reconstruction error\nand discriminator error are extracted as anomaly indicators. Two complementary\ndecision strategies are developed: a threshold-based rule for computational\nefficiency and a convex hull-based method for robustness under complex error\ndistributions. These features are organized into spatiotemporal detection and\nclassification matrices through a sliding-window mechanism, and an\nidentification and decision fusion stage integrates the outputs across PMUs.\nThis design enables the framework to identify known events while systematically\nclassifying previously unseen disturbances into a new category, addressing a\nkey limitation of supervised classifiers. Experimental results demonstrate\nstate-of-the-art accuracy, surpassing machine learning, deep learning, and\nenvelope-based baselines. The ability to recognize unknown events further\nhighlights the adaptability and practical value of the proposed approach for\nwide-area event analysis in modern power systems."}
{"id": "2509.22718", "categories": ["eess.AS", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22718", "abs": "https://arxiv.org/abs/2509.22718", "authors": ["Ke Gu", "Zhicong Wu", "Peng Bai", "Sitong Qiao", "Zhiqi Jiang", "Junchen Lu", "Xiaodong Shi", "Xinyuan Qian"], "title": "PerformSinger: Multimodal Singing Voice Synthesis Leveraging Synchronized Lip Cues from Singing Performance Videos", "comment": null, "summary": "Existing singing voice synthesis (SVS) models largely rely on fine-grained,\nphoneme-level durations, which limits their practical application. These\nmethods overlook the complementary role of visual information in duration\nprediction.To address these issues, we propose PerformSinger, a pioneering\nmultimodal SVS framework, which incorporates lip cues from video as a visual\nmodality, enabling high-quality \"duration-free\" singing voice synthesis.\nPerformSinger comprises parallel multi-branch multimodal encoders, a feature\nfusion module, a duration and variational prediction network, a mel-spectrogram\ndecoder and a vocoder. The fusion module, composed of adapter and fusion\nblocks, employs a progressive fusion strategy within an aligned semantic space\nto produce high-quality multimodal feature representations, thereby enabling\naccurate duration prediction and high-fidelity audio synthesis. To facilitate\nthe research, we design, collect and annotate a novel SVS dataset involving\nsynchronized video streams and precise phoneme-level manual annotations.\nExtensive experiments demonstrate the state-of-the-art performance of our\nproposal in both subjective and objective evaluations. The code and dataset\nwill be publicly available."}
{"id": "2509.23238", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23238", "abs": "https://arxiv.org/abs/2509.23238", "authors": ["Goksenin Yuksel", "Pierre Guetschel", "Michael Tangermann", "Marcel van Gerven", "Kiki van der Heijden"], "title": "WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms", "comment": "Still under review", "summary": "Learning audio representations from raw waveforms overcomes key limitations\nof spectrogram-based audio representation learning, such as the long latency of\nspectrogram computation and the loss of phase information. Yet, while\nself-supervised speech representation learning from raw waveforms has been\nremarkably successful, these approaches have not achieved similar feats for\ngeneral-purpose audio representation learning from waveforms. Here, we propose\nWavJEPA, a waveform-based version of the Joint-Embedding Predictive\nArchitecture. WavJEPA leverages high-level semantic representation learning to\ntackle the shortcomings of representation learning at the speech unit or token\nlevel. We show that this approach substantially outperforms state-of-the-art\ntime-domain audio foundation models across a wide variety of downstream\nbenchmark tasks, while requiring considerably fewer computational resources.\nAdditionally, to overcome the performance drop that time-domain models\ntypically exhibit in noisy and reverberant real-world acoustic environments, we\npresent WavJEPA-Nat. WavJEPA-Nat is a multi-channel extension of the WavJEPA\narchitecture trained on simulated naturalistic scenes. We find that WavJEPA-Nat\nis highly robust to reverberation and noise. These results highlight the\nfeasibility and computational efficiency of general-purpose audio\nrepresentation learning from raw waveforms, showcasing the potential for\nlow-latency, robust time-domain audio foundation models for real-world\napplications."}
{"id": "2509.22810", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22810", "abs": "https://arxiv.org/abs/2509.22810", "authors": ["Jianheng Zhou", "Chenyu Liu", "Jinan Zhou", "Yi Ding", "Yang Liu", "Haoran Luo", "Ziyu Jia", "Xinliang Zhou"], "title": "Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-Purpose Model", "comment": null, "summary": "Sleep staging is essential for diagnosing sleep disorders and assessing\nneurological health. Existing automatic methods typically extract features from\ncomplex polysomnography (PSG) signals and train domain-specific models, which\noften lack intuitiveness and require large, specialized datasets. To overcome\nthese limitations, we introduce a new paradigm for sleep staging that leverages\nlarge multimodal general-purpose models to emulate clinical diagnostic\npractices. Specifically, we convert raw one-dimensional PSG time-series into\nintuitive two-dimensional waveform images and then fine-tune a multimodal large\nmodel to learn from these representations. Experiments on three public datasets\n(ISRUC, MASS, SHHS) demonstrate that our approach enables general-purpose\nmodels, without prior exposure to sleep data, to acquire robust staging\ncapabilities. Moreover, explanation analysis reveals our model learned to mimic\nthe visual diagnostic workflow of human experts for sleep staging by PSG\nimages. The proposed method consistently outperforms state-of-the-art baselines\nin accuracy and robustness, highlighting its efficiency and practical value for\nmedical applications. The code for the signal-to-image pipeline and the PSG\nimage dataset will be released."}
{"id": "2509.22740", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22740", "abs": "https://arxiv.org/abs/2509.22740", "authors": ["Jinbae Seo", "Hyeongjun Kwon", "Kwonyoung Kim", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation", "comment": null, "summary": "Audiovisual instance segmentation (AVIS) requires accurately localizing and\ntracking sounding objects throughout video sequences. Existing methods suffer\nfrom visual bias stemming from two fundamental issues: uniform additive fusion\nprevents queries from specializing to different sound sources, while\nvisual-only training objectives allow queries to converge to arbitrary salient\nobjects. We propose Audio-Centric Query Generation using cross-attention,\nenabling each query to selectively attend to distinct sound sources and carry\nsound-specific priors into visual decoding. Additionally, we introduce\nSound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding\nobject numbers through ordinal regression with monotonic consistency\nconstraints, preventing visual-only convergence during training. Experiments on\nAVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and\n+2.06 FSLA, validating that query specialization and explicit counting\nsupervision are crucial for accurate audiovisual instance segmentation."}
{"id": "2509.23299", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23299", "abs": "https://arxiv.org/abs/2509.23299", "authors": ["Yike Zhu", "Boyi Kang", "Ziqian Wang", "Xingchen Li", "Zihan Zhang", "Wenjie Li", "Longshuai Xiao", "Wei Xue", "Lei Xie"], "title": "MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow", "comment": "Submitted to ICASSP 2026", "summary": "Speech enhancement (SE) recovers clean speech from noisy signals and is vital\nfor applications such as telecommunications and automatic speech recognition\n(ASR). While generative approaches achieve strong perceptual quality, they\noften rely on multi-step sampling (diffusion/flow-matching) or large language\nmodels, limiting real-time deployment. To mitigate these constraints, we\npresent MeanFlowSE, a one-step generative SE framework. It adopts MeanFlow to\npredict an average-velocity field for one-step latent refinement and conditions\nthe model on self-supervised learning (SSL) representations rather than VAE\nlatents. This design accelerates inference and provides robust\nacoustic-semantic guidance during training. In the Interspeech 2020 DNS\nChallenge blind test set and simulated test set, MeanFlowSE attains\nstate-of-the-art (SOTA) level perceptual quality and competitive\nintelligibility while significantly lowering both real-time factor (RTF) and\nmodel size compared with recent generative competitors, making it suitable for\npractical use. The code will be released upon publication at\nhttps://github.com/Hello3orld/MeanFlowSE."}
{"id": "2509.22869", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22869", "abs": "https://arxiv.org/abs/2509.22869", "authors": ["Abdulkadir Bilge", "Erdem Ergen", "Burak Soner", "Sinem Coleri"], "title": "Scalable Wi-Fi RSS-Based Indoor Localization via Automatic Vision-Assisted Calibration", "comment": "Presented at the ICAT 2025 conference, Sarajevo, September 2025. See\n  https://icat.etf.unsa.ba/2025/", "summary": "Wi-Fi-based positioning promises a scalable and privacy-preserving solution\nfor location-based services in indoor environments such as malls, airports, and\ncampuses. RSS-based methods are widely deployable as RSS data is available on\nall Wi-Fi-capable devices, but RSS is highly sensitive to multipath, channel\nvariations, and receiver characteristics. While supervised learning methods\noffer improved robustness, they require large amounts of labeled data, which is\noften costly to obtain. We introduce a lightweight framework that solves this\nby automating high-resolution synchronized RSS-location data collection using a\nshort, camera-assisted calibration phase. An overhead camera is calibrated only\nonce with ArUco markers and then tracks a device collecting RSS data from\nbroadcast packets of nearby access points across Wi-Fi channels. The resulting\n(x, y, RSS) dataset is used to automatically train mobile-deployable\nlocalization algorithms, avoiding the privacy concerns of continuous video\nmonitoring. We quantify the accuracy limits of such vision-assisted RSS data\ncollection under key factors such as tracking precision and label\nsynchronization. Using the collected experimental data, we benchmark\ntraditional and supervised learning approaches under varying signal conditions\nand device types, demonstrating improved accuracy and generalization,\nvalidating the utility of the proposed framework for practical use. All code,\ntools, and datasets are released as open source."}
{"id": "2509.22744", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22744", "abs": "https://arxiv.org/abs/2509.22744", "authors": ["Jinming Chen", "Lu Wang", "Zheshu Song", "Wei Deng"], "title": "Index-MSR: A high-efficiency multimodal fusion framework for speech recognition", "comment": "Submit to icassp 2026", "summary": "Driven by large scale datasets and LLM based architectures, automatic speech\nrecognition (ASR) systems have achieved remarkable improvements in accuracy.\nHowever, challenges persist for domain-specific terminology, and short\nutterances lacking semantic coherence, where recognition performance often\ndegrades significantly. In this work, we present Index-MSR, an efficient\nmultimodal speech recognition framework. At its core is a novel Multimodal\nFusion Decoder (MFD), which effectively incorporates text-related information\nfrom videos (e.g., subtitles and presentation slides) into the speech\nrecognition. This cross-modal integration not only enhances overall ASR\naccuracy but also yields substantial reductions in substitution errors.\nExtensive evaluations on both an in-house subtitle dataset and a public AVSR\ndataset demonstrate that Index-MSR achieves sota accuracy, with substitution\nerrors reduced by 20,50%. These results demonstrate that our approach\nefficiently exploits text-related cues from video to improve speech recognition\naccuracy, showing strong potential in applications requiring strict audio text\nsynchronization, such as audio translation."}
{"id": "2509.23350", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23350", "abs": "https://arxiv.org/abs/2509.23350", "authors": ["Jiahao Zhao", "Yunjia Li", "Wei Li", "Kazuyoshi Yoshii"], "title": "ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following", "comment": null, "summary": "As large language models continue to develop, the feasibility and\nsignificance of text-based symbolic music tasks have become increasingly\nprominent. While symbolic music has been widely used in generation tasks, LLM\ncapabilities in understanding and reasoning about symbolic music remain largely\nunderexplored. To address this gap, we propose ABC-Eval, the first open-source\nbenchmark dedicated to the understanding and instruction-following capabilities\nin text-based ABC notation scores. It comprises 1,086 test samples spanning 10\nsub-tasks, covering scenarios from basic musical syntax comprehension to\ncomplex sequence-level reasoning. Such a diverse scope poses substantial\nchallenges to models' ability to handle symbolic music tasks. We evaluated\nseven state-of-the-art LLMs on ABC-Eval, and the results reveal notable\nlimitations in existing models' symbolic music processing capabilities.\nFurthermore, the consistent performance of individual baselines across\ndifferent sub-tasks supports the reliability of our benchmark."}
{"id": "2509.22891", "categories": ["eess.SP", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2509.22891", "abs": "https://arxiv.org/abs/2509.22891", "authors": ["Ashwini Kulkarni", "Santosh Nannuru"], "title": "Time-Frequency Analysis of Non-Uniformly Sampled Signals via Sample Density Adaptation", "comment": null, "summary": "The analysis of non-stationary signals in non-uniformly sampled data is a\nchallenging task. Time-integrated methods, such as the generalised Lomb-Scargle\n(GLS) periodogram, provide a robust statistical assessment of persistent\nperiodicities but are insensitive to transient events. Conversely, existing\ntime-frequency methods often rely on fixed-duration windows or interpolation,\nwhich can be suboptimal for non-uniform data. We introduce the non-uniform\nStockwell-transform (NUST), a time-frequency framework that applies a localized\ndensity adaptive spectral analysis directly to non-uniformly sampled data. NUST\nemploys a doubly adaptive window that adjusts its width based on both frequency\nand local data density, providing detailed time-frequency information for both\ntransient and persistent signals. We validate the NUST on numerous\nnon-uniformly sampled synthetic signals, demonstrating its superior\ntime-localization performance compared to GLS. Furthermore, we apply NUST to\nHARPS radial velocity data of the multi-planetary system HD 10180, successfully\ndistinguishing coherent planetary signals from stellar activity."}
{"id": "2509.22942", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22942", "abs": "https://arxiv.org/abs/2509.22942", "authors": ["Dominik Klement", "Matthew Maciejewski", "Sanjeev Khudanpur", "Jan Černocký", "Lukáš Burget"], "title": "Unsupervised Speech Enhancement using Data-defined Priors", "comment": "Submitted to ICASSP 2026", "summary": "The majority of deep learning-based speech enhancement methods require paired\nclean-noisy speech data. Collecting such data at scale in real-world conditions\nis infeasible, which has led the community to rely on synthetically generated\nnoisy speech. However, this introduces a gap between the training and testing\nphases. In this work, we propose a novel dual-branch encoder-decoder\narchitecture for unsupervised speech enhancement that separates the input into\nclean speech and residual noise. Adversarial training is employed to impose\npriors on each branch, defined by unpaired datasets of clean speech and,\noptionally, noise. Experimental results show that our method achieves\nperformance comparable to leading unsupervised speech enhancement approaches.\nFurthermore, we demonstrate the critical impact of clean speech data selection\non enhancement performance. In particular, our findings reveal that performance\nmay appear overly optimistic when in-domain clean speech data are used for\nprior definition -- a practice adopted in previous unsupervised speech\nenhancement studies."}
{"id": "2509.23358", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23358", "abs": "https://arxiv.org/abs/2509.23358", "authors": ["Chaohao Lin", "Xu Zheng", "Kaida Wu", "Peihao Xiang", "Ou Bai"], "title": "Emotional Styles Hide in Deep Speaker Embeddings: Disentangle Deep Speaker Embeddings for Speaker Clustering", "comment": "6 pages, 4 figures", "summary": "Speaker clustering is the task of identifying the unique speakers in a set of\naudio recordings (each belonging to exactly one speaker) without knowing who\nand how many speakers are present in the entire data, which is essential for\nspeaker diarization processes. Recently, off-the-shelf deep speaker embedding\nmodels have been leveraged to capture speaker characteristics. However,\nspeeches containing emotional expressions pose significant challenges, often\naffecting the accuracy of speaker embeddings and leading to a decline in\nspeaker clustering performance. To tackle this problem, we propose DTG-VAE, a\nnovel disentanglement method that enhances clustering within a Variational\nAutoencoder (VAE) framework. This study reveals a direct link between emotional\nstates and the effectiveness of deep speaker embeddings. As demonstrated in our\nexperiments, DTG-VAE extracts more robust speaker embeddings and significantly\nenhances speaker clustering performance."}
{"id": "2509.23065", "categories": ["eess.SP", "math.CV"], "pdf": "https://arxiv.org/pdf/2509.23065", "abs": "https://arxiv.org/abs/2509.23065", "authors": ["Mohammad Amin Saeidi", "Hina Tabassum"], "title": "Resource Allocation in Cooperative Mid-band/THz Networks in the Presence of Mobility", "comment": "This paper has been accepted for publication in IEEE journals", "summary": "This paper develops a comprehensive framework to investigate and optimize the\ndownlink performance of cooperative multi-band networks (MBNs) operating on\nupper mid-band (UMB) and terahertz (THz) frequencies, where base stations (BSs)\nin each band cooperatively serve users. The framework captures sophisticated\nfeatures such as near-field channel modeling, fully and partially connected\nantenna architectures, and users' mobility. First, we consider joint user\nassociation and hybrid beamforming optimization to maximize the system\nsum-rate, subject to power constraints, maximum cluster size of cooperating\nBSs, and users' quality-of-service (QoS) constraints. By leveraging fractional\nprogramming FP and majorization-minimization techniques, an iterative algorithm\nis proposed to solve the non-convex optimization problem. We then consider\nhandover (HO)-aware resource allocation for moving users in a cooperative\nUMB/THz MBN. Two HO-aware resource allocation methods are proposed. The first\nmethod focuses on maximizing the HO-aware system sum-rate subject to HO-aware\nQoS constraints. Using Jensen's inequality and properties of logarithmic\nfunctions, the non-convex optimization problem is tightly approximated with a\nconvex one and solved. The second method addresses a multi-objective\noptimization problem to maximize the system sum-rate, while minimizing the\ntotal number of HOs. Numerical results demonstrate the efficacy of the proposed\nalgorithms, cooperative UMB/THz MBN over stand-alone THz networks, as well as\nthe critical importance of accurate near-field modeling in extremely large\nantenna arrays. Moreover, the proposed HO-aware resource allocation methods\neffectively mitigate the impact of HOs, enhancing performance in the considered\nsystem."}
{"id": "2509.23147", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23147", "abs": "https://arxiv.org/abs/2509.23147", "authors": ["Abdul Rehman", "Jingyao Cai", "Jian-Jun Zhang", "Xiaosong Yang"], "title": "BFA: Real-time Multilingual Text-to-speech Forced Alignment", "comment": "Under review", "summary": "We present Bournemouth Forced Aligner (BFA), a system that combines a\nContextless Universal Phoneme Encoder (CUPE) with a connectionist temporal\nclassification (CTC)based decoder. BFA introduces explicit modelling of\ninter-phoneme gaps and silences and hierarchical decoding strategies, enabling\nfine-grained boundary prediction. Evaluations on TIMIT and Buckeye corpora show\nthat BFA achieves competitive recall relative to Montreal Forced Aligner at\nrelaxed tolerance levels, while predicting both onset and offset boundaries for\nricher temporal structure. BFA processes speech up to 240x faster than MFA,\nenabling faster than real-time alignment. This combination of speed and\nsilence-aware alignment opens opportunities for interactive speech applications\npreviously constrained by slow aligners."}
{"id": "2509.23435", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23435", "abs": "https://arxiv.org/abs/2509.23435", "authors": ["Wenyu Li", "Xiaoqi Jiao", "Yi Chang", "Guangyan Zhang", "Yiwen Guo"], "title": "AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models", "comment": null, "summary": "The creation of high-quality multimodal datasets remains fundamental for\nadvancing role-playing capabilities in large language models (LLMs). While\nexisting works predominantly focus on text-based persona simulation, Audio\nRole-Playing (ARP) presents unique challenges due to the need for synchronized\nalignment of semantic content and vocal characteristics. To address this gap,\nwe propose AudioRole, a meticulously curated dataset from 13 TV series spanning\n1K+ hours with 1M+ character-grounded dialogues, providing synchronized\naudio-text pairs annotated with speaker identities and contextual metadata. In\naddition, to demonstrate the effectiveness of the dataset, we introduced\nARP-Eval, a dual-aspect evaluation framework that assesses both response\nquality and role fidelity. Empirical validation showing GLM-4-Voice trained on\nAudioRole (which we called ARP-Model) achieve an average Acoustic\nPersonalization score of 0.31, significantly outperforming the original\nGLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically\nsupports role-playing in one-shot scenarios. The ARP-Model also achieves a\nContent Personalization score of 0.36, surpassing the untrained original model\nby about 38% and maintaining the same level as MiniCPM-O-2.6.\n  AudioRole features dialogues from over 115 main characters, 6 trained\nARP-Models that role-play different characters, and evaluation protocols.\nTogether, they provide an essential resource for advancing audio-grounded\nrole-playing research."}
{"id": "2509.23302", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23302", "abs": "https://arxiv.org/abs/2509.23302", "authors": ["Wilson de Souza Junior", "Taufik Abrao", "Amine Mezghani", "Ekram Hossain"], "title": "Dual-Function Beam Pattern Design for Multi-Target ISAC Systems: A Decoupled Approach", "comment": "13 pages; 7 figures; tables; 3 tables; manuscript submitted to IEEE\n  journal", "summary": "We investigate the beampattern design problem for mono-static multi-user (MU)\nmulti-point-target integrated sensing and communication (ISAC) systems, where a\ndual-function multiple-input multiple-output (DF-MIMO) base station (BS)\nperforms downlink communication and radar sensing simultaneously. In ISAC\nsystems, sensing and communication inherently compete for resources. As\ncommunication demand increases, the beam pattern is reshaped, which might\ndegrade the direction of arrival (DoA) sensing accuracy, measured in terms of\nmean-squared error (MSE) and lower-bounded by the Cramer-Rao lower bound\n(CRLB). Since conventional joint formulations of the sensing-based problem\noften overlook this trade-off, our work addresses it by decomposing the\nsensing-based problem into two subproblems (SPs). This decomposition enables a\nmore effective exploitation of the beam pattern's physical properties, which we\nrefer to as the Sensing-Guided Communication Dual-Function (SGCDF) beam pattern\ndesign. We further develop a low-complexity extension using the Riemannian\nManifold Optimization (RMO) and convex closed-set projection. Simulation\nresults confirm that the proposed method improves multi-target estimation\naccuracy, compared to traditional joint optimization strategies, by preserving\nthe beam pattern, while the low-complexity version offers an excellent\nperformance-complexity tradeoff, maintaining high accuracy with significantly\nreduced computational cost."}
{"id": "2509.23364", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23364", "abs": "https://arxiv.org/abs/2509.23364", "authors": ["Francesca Ronchini", "Luca Comanducci", "Simone Marcucci", "Fabio Antonacci"], "title": "AI-Assisted Music Production: A User Study on Text-to-Music Models", "comment": "Accepted at 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR 25)", "summary": "Text-to-music models have revolutionized the creative landscape, offering new\npossibilities for music creation. Yet their integration into musicians\nworkflows remains underexplored. This paper presents a case study on how TTM\nmodels impact music production, based on a user study of their effect on\nproducers creative workflows. Participants produce tracks using a custom tool\ncombining TTM and source separation models. Semi-structured interviews and\nthematic analysis reveal key challenges, opportunities, and ethical\nconsiderations. The findings offer insights into the transformative potential\nof TTMs in music production, as well as challenges in their real-world\nintegration."}
{"id": "2509.23610", "categories": ["cs.SD", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23610", "abs": "https://arxiv.org/abs/2509.23610", "authors": ["Kai Li", "Kejun Gao", "Xiaolin Hu"], "title": "Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention", "comment": "Technical Report", "summary": "Audio-visual speech separation (AVSS) methods leverage visual cues to extract\ntarget speech and have demonstrated strong separation quality in noisy acoustic\nenvironments. However, these methods usually involve a large number of\nparameters and require high computational cost, which is unacceptable in many\napplications where speech separation serves as only a preprocessing step for\nfurther speech processing. To address this issue, we propose an efficient AVSS\nmethod, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a\ndual-path lightweight video encoder that transforms lip-motion into discrete\naudio-aligned semantic tokens. For audio separation, we construct a lightweight\nencoder-decoder separator, in which each layer incorporates a global-local\nattention (GLA) block to efficiently capture multi-scale dependencies.\nExperiments on three benchmark datasets showed that Dolphin not only surpassed\nthe current state-of-the-art (SOTA) model in separation quality but also\nachieved remarkable improvements in efficiency: over 50% fewer parameters, more\nthan 2.4x reduction in MACs, and over 6x faster GPU inference speed. These\nresults indicate that Dolphin offers a practical and deployable solution for\nhigh-performance AVSS in real-world scenarios. Our code and demo page are\npublicly available at http://cslikai.cn/Dolphin/."}
{"id": "2509.23444", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23444", "abs": "https://arxiv.org/abs/2509.23444", "authors": ["Lorenzo Italiano", "Alireza Pourafzal", "Hui Chen", "Mattia Brambilla", "Gonzalo Seco-Granados", "Monica Nicoli", "Henk Wymeersch"], "title": "HoloTrace: a Location Privacy Preservation Solution for mmWave MIMO-OFDM Systems", "comment": "submitted to IEEE Journal on Selected Areas in Communications", "summary": "The technological innovation towards 6G cellular networks introduces\nunprecedented capabilities for user equipment (UE) localization, but it also\nraises serious concerns about physical layer location privacy. This paper\nintroduces HoloTrace, a signal-level privacy preservation framework that relies\non user-side spoofing of localization-relevant features to prevent the\nextraction of precise location information from the signals received by a base\nstation (BS) in a mmWave MIMO-OFDM system. Spoofing is performed by the user on\nlocation parameters such as angle of arrival (AoA), angle of departure (AoD),\nand time difference of arrival (TDoA). Without requiring any protocol\nmodification nor network-side support, our method strategically perturbs pilot\ntransmissions to prevent a BS from performing non-consensual UE localization.\nThe methodology allows the UE to spoof its position, keeping the precoder\nunchanged. We formulate spoofing as a unified rank-constrained projection\nproblem, and provide closed-form solutions under varying levels of channel\nstate information (CSI) at the UE, including scenarios with and without CSI\nknowledge. Simulation results confirm that the proposed approach enables the UE\nto deceive the BS, inducing significant localization errors, while the impact\non link capacity varies depending on the spoofed position. Our findings\nestablish HoloTrace as a practical and robust privacy-preserving solution for\nfuture 6G networks."}
{"id": "2509.23454", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23454", "abs": "https://arxiv.org/abs/2509.23454", "authors": ["Md. Saiful Bari Siddiqui", "Utsab Saha"], "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification", "comment": "Submitted to ICASSP 2026. This preprint includes some additional\n  details beyond the conference submission", "summary": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training."}
{"id": "2509.23618", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23618", "abs": "https://arxiv.org/abs/2509.23618", "authors": ["Pu Huang", "Shouguang Wang", "Siya Yao", "Mengchu Zhou"], "title": "Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment", "comment": null, "summary": "Neural speech synthesis techniques have enabled highly realistic speech\ndeepfakes, posing major security risks. Speech deepfake detection is\nchallenging due to distribution shifts across spoofing methods and variability\nin speakers, channels, and recording conditions. We explore learning shared\ndiscriminative features as a path to robust detection and propose Information\nBottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN).\nConfidence-guided adversarial alignment adaptively suppresses attack-specific\nartifacts without erasing discriminative cues, while the information bottleneck\nremoves nuisance variability to preserve transferable features. Experiments on\nASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN\nconsistently outperforms baseline and achieves state-of-the-art performance on\nmany benchmarks."}
{"id": "2509.23520", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23520", "abs": "https://arxiv.org/abs/2509.23520", "authors": ["Kalpesh Jaykar", "Prasanth Velvaluri", "Nian X. Sun", "Richard D. James"], "title": "Theoretical framework of passive ME antenna arrays enabling in-vivo monitoring: A pathway to smart implants", "comment": null, "summary": "A new brain-computer interface (BCI) technology, deployed through minimally\ninvasive surgery, is changing the way we think about treating severe\nneurological conditions. The central idea is to place a device called Stentrode\nin the brain's vasculature, which enables neuromodulation and helps patients\nregain the ability to communicate. However, in such devices, the battery and\nelectronics are wired and could introduce damage or implant malfunction. In\nthese cases, a Stentrode integrated with magnetoelectric (ME) antennas could be\nof great interest. ME antennas offer significant advantages over traditional\nantennas, leveraging acoustic resonance rather than electromagnetic resonance\nto achieve a size reduction of up to five orders of magnitude. In addition to\ntheir compactness and immunity to ground-plane interference, ME antennas could\nbe adopted for use in vascular implants, such as coronary stents, potentially\nenabling minimally invasive monitoring and communication. Despite these\nadvantages, a single antenna embedded in the implant may be constrained by the\nlimited volume of magnetostrictive material, which could result in low output\ngain. To address this gain limitation, we propose using antenna arrays designed\nto produce constructive interference at a designated far-field point, ideally\nlocated outside the patient, to enhance signal transmission and receiving\ncapabilities. We develop a mathematical model to represent the antennas and\noptimize their spatial arrangement and phase synchronization. Simulations based\non this model demonstrate promising high-gain performance at the prescribed\nfar-field location through phase manipulation."}
{"id": "2509.23832", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23832", "abs": "https://arxiv.org/abs/2509.23832", "authors": ["Junyu Wang", "Zizhen Lin", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "title": "LORT: Locally Refined Convolution and Taylor Transformer for Monaural Speech Enhancement", "comment": "Speech Communication", "summary": "Achieving superior enhancement performance while maintaining a low parameter\ncount and computational complexity remains a challenge in the field of speech\nenhancement. In this paper, we introduce LORT, a novel architecture that\nintegrates spatial-channel enhanced Taylor Transformer and locally refined\nconvolution for efficient and robust speech enhancement. We propose a Taylor\nmulti-head self-attention (T-MSA) module enhanced with spatial-channel\nenhancement attention (SCEA), designed to facilitate inter-channel information\nexchange and alleviate the spatial attention limitations inherent in\nTaylor-based Transformers. To complement global modeling, we further present a\nlocally refined convolution (LRC) block that integrates convolutional\nfeed-forward layers, time-frequency dense local convolutions, and gated units\nto capture fine-grained local details. Built upon a U-Net-like encoder-decoder\nstructure with only 16 output channels in the encoder, LORT processes noisy\ninputs through multi-resolution T-MSA modules using alternating downsampling\nand upsampling operations. The enhanced magnitude and phase spectra are decoded\nindependently and optimized through a composite loss function that jointly\nconsiders magnitude, complex, phase, discriminator, and consistency objectives.\nExperimental results on the VCTK+DEMAND and DNS Challenge datasets demonstrate\nthat LORT achieves competitive or superior performance to state-of-the-art\n(SOTA) models with only 0.96M parameters, highlighting its effectiveness for\nreal-world speech enhancement applications with limited computational\nresources."}
{"id": "2509.23727", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23727", "abs": "https://arxiv.org/abs/2509.23727", "authors": ["Junyou Wang", "Zehua Chen", "Binjie Yuan", "Kaiwen Zheng", "Chang Li", "Yuxuan Jiang", "Jun Zhu"], "title": "AudioMoG: Guiding Audio Generation with Mixture-of-Guidance", "comment": null, "summary": "Guidance methods have demonstrated significant improvements in cross-modal\naudio generation, including text-to-audio (T2A) and video-to-audio (V2A)\ngeneration. The popularly adopted method, classifier-free guidance (CFG),\nsteers generation by emphasizing condition alignment, enhancing fidelity but\noften at the cost of diversity. Recently, autoguidance (AG) has been explored\nfor audio generation, encouraging the sampling to faithfully reconstruct the\ntarget distribution and showing increased diversity. Despite these advances,\nthey usually rely on a single guiding principle, e.g., condition alignment in\nCFG or score accuracy in AG, leaving the full potential of guidance for audio\ngeneration untapped. In this work, we explore enriching the composition of the\nguidance method and present a mixture-of-guidance framework, AudioMoG. Within\nthe design space, AudioMoG can exploit the complementary advantages of\ndistinctive guiding principles by fulfilling their cumulative benefits. With a\nreduced form, AudioMoG can consider parallel complements or recover a single\nguiding principle, without sacrificing generality. We experimentally show that,\ngiven the same inference speed, AudioMoG approach consistently outperforms\nsingle guidance in T2A generation across sampling steps, concurrently showing\nadvantages in V2A, text-to-music, and image generation. These results highlight\na \"free lunch\" in current cross-modal audio generation systems: higher quality\ncan be achieved through mixed guiding principles at the sampling stage without\nsacrificing inference efficiency. Demo samples are available at:\nhttps://audio-mog.github.io."}
{"id": "2509.23644", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23644", "abs": "https://arxiv.org/abs/2509.23644", "authors": ["Omkar Nitsure", "Sampath Kumar Dondapati", "Satish Mulleti"], "title": "Learnable Kernels for FRI -- Joint Kernel Encoder Optimization and Hardware Validation", "comment": "10 pages", "summary": "Finite Rate of Innovation (FRI) sampling techniques provide efficient\nframeworks for reconstructing signals with inherent sparsity at rates below\nNyquist. However, traditional FRI reconstruction methods rely heavily on\npre-defined kernels, often limiting hardware implementation and reconstruction\naccuracy under noisy conditions. In this paper, we propose a robust, flexible,\nand practically implementable framework for FRI reconstruction by introducing\nnovel learnable kernel strategies. First, we demonstrate effective\nreconstruction using known, fixed kernels such as truncated Gaussian and\nGaussian pair kernels, which mitigate the requirement that the samples should\nhave a sum-of-exponentials (SoE) form. Next, we extend this concept by jointly\noptimizing both the sampling kernel and reconstruction encoder through a\nunified learning approach, yielding adaptive kernels that significantly\noutperform traditional methods in resolution and noise robustness, with reduced\nsampling rates. Furthermore, we propose a practical hardware realization by\nrepresenting kernels as sums of two exponential decay signals with jointly\noptimized poles, facilitating compact, efficient analog implementations. Our\napproach is validated experimentally through hardware implementations using a\nunity-gain Sallen-Key analog filter, achieving accurate real-world signal\nrecovery. The developed convolutional neural network-based encoder\nsubstantially reduces computational complexity, demonstrating competitive\nperformance with fewer parameters, making our method particularly suitable for\nresource-constrained, edge-based deployments."}
{"id": "2509.23833", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23833", "abs": "https://arxiv.org/abs/2509.23833", "authors": ["Cancan Li", "Fei Su", "Juan Liu", "Hui Bu", "Yulong Wan", "Hongbin Suo", "Ming Li"], "title": "AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset with Speech Recognition Baselines", "comment": null, "summary": "Whisper speech recognition is crucial not only for ensuring privacy in\nsensitive communications but also for providing a critical communication bridge\nfor patients under vocal restraint and enabling discrete interaction in\nnoise-sensitive environments. The development of Chinese mandarin audio-visual\nwhisper speech recognition is hindered by the lack of large-scale datasets. We\npresent AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech\ndataset, featuring 30 hours each of whisper speech and parallel normal speech,\nwith synchronized frontal facial videos. Moreover, we propose an audio-visual\nspeech recognition (AVSR) baseline based on the Whisper-Flamingo framework,\nwhich integrates a parallel training strategy to align embeddings across speech\ntypes, and employs a projection layer to adapt to whisper speech's spectral\nproperties. The model achieves a Character Error Rate (CER) of 4.13% for\nwhisper speech and 1.11% for normal speech in the test set of our dataset, and\nestablishes new state-of-the-art results on the wTIMIT benchmark. The dataset\nand the AVSR baseline codes are open-sourced at\nhttps://zutm.github.io/AISHELL6-Whisper."}
{"id": "2509.23759", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23759", "abs": "https://arxiv.org/abs/2509.23759", "authors": ["Ting-Kang Wang", "Yueh-Po Peng", "Li Su", "Vincent K. M. Cheung"], "title": "VioPTT: Violin Technique-Aware Transcription from Synthetic Data Augmentation", "comment": null, "summary": "While automatic music transcription is well-established in music information\nretrieval, most models are limited to transcribing pitch and timing information\nfrom audio, and thus omit crucial expressive and instrument-specific nuances.\nOne example is playing technique on the violin, which affords its distinct\npalette of timbres for maximal emotional impact. Here, we propose\n\\textbf{VioPTT} (Violin Playing Technique-aware Transcription), a lightweight,\nend-to-end model that directly transcribes violin playing technique in addition\nto pitch onset and offset. Furthermore, we release \\textbf{MOSA-VPT}, a novel,\nhigh-quality synthetic violin playing technique dataset to circumvent the need\nfor manually labeled annotations. Leveraging this dataset, our model\ndemonstrated strong generalization to real-world note-level violin technique\nrecordings in addition to achieving state-of-the-art transcription performance.\nTo our knowledge, VioPTT is the first to jointly combine violin transcription\nand playing technique prediction within a unified framework."}
{"id": "2509.23687", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23687", "abs": "https://arxiv.org/abs/2509.23687", "authors": ["Runze Dong", "Buhong Wang", "Cunqian Feng", "Jiang Weng", "Chen Han", "Jiwei Tian"], "title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks", "comment": null, "summary": "Integrated sensing and communication (ISAC) emerges as a key enabler for\nnext-generation applications such as smart cities and autonomous systems. Its\nintegration with unmanned aerial vehicles (UAVs) unlocks new potentials for\nreliable communication and precise sensing in dynamic aerial environments.\nHowever, existing research predominantly treats UAVs as aerial base stations,\noverlooking their role as ISAC users, and fails to leverage large-scale antenna\narrays at terrestrial base stations to enhance security and spectral\nefficiency. This paper propose a secure and spectral efficient ISAC framework\nfor multi-UAV networks, and a two-stage optimization approach is developed to\njointly design hybrid beamforming (HBF), artificial noise (AN) injection, and\nUAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage\nemploys Proximal Policy Optimization (PPO) to optimize digital beamformers and\ntrajectories, and the second stage decomposes the digital solution into analog\nand digital components via low-complexity matrix factorization. Simulation\nresults demonstrate the effectiveness of the proposed framework compared to\nbenchmark schemes."}
{"id": "2509.24187", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24187", "abs": "https://arxiv.org/abs/2509.24187", "authors": ["Bo-Hao Su", "Hui-Ying Shih", "Jinchuan Tian", "Jiatong Shi", "Chi-Chun Lee", "Carlos Busso", "Shinji Watanabe"], "title": "Reasoning Beyond Majority Vote: An Explainable SpeechLM Framework for Speech Emotion Recognition", "comment": null, "summary": "Speech Emotion Recognition (SER) is typically trained and evaluated on\nmajority-voted labels, which simplifies benchmarking but masks subjectivity and\nprovides little transparency into why predictions are made. This neglects valid\nminority annotations and limits interpretability. We propose an explainable\nSpeech Language Model (SpeechLM) framework that frames SER as a generative\nreasoning task. Given an utterance, the model first produces a transcript, then\noutputs both an emotion label and a concise natural-language rationale grounded\nin lexical and acoustic cues. Rationales are generated by a reasoning-capable\nteacher LLM and used as intermediate supervision, combined with majority labels\nduring fine-tuning. Unlike prior work primarily focused on boosting\nclassification accuracy, we aim to enhance explainability while preserving\ncompetitive performance. To this end, we complement majority-label metrics with\nannotator-aware scoring that credits matches with any annotator label. On\nMSP-Podcast v1.12, our model maintains improvements over zero-shot SpeechLM\nbaselines, and produces rationales that human evaluators find plausible and\nwell grounded. This demonstrates that incorporating rationale supervision\noffers a practical path toward interpretable SER without sacrificing predictive\nquality."}
{"id": "2509.23795", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23795", "abs": "https://arxiv.org/abs/2509.23795", "authors": ["Haoyu Song", "Ian McLoughlin", "Qing Gu", "Nan Jiang", "Yan Song"], "title": "An Efficient Transfer Learning Method Based on Adapter with Local Attributes for Speech Emotion Recognition", "comment": null, "summary": "Existing speech emotion recognition (SER) methods commonly suffer from the\nlack of high-quality large-scale corpus, partly due to the complex,\npsychological nature of emotion which makes accurate labeling difficult and\ntime consuming. Recently, transfer learning based methods that exploit the\nencoders pretrained on large-scale speech corpus (e.g., Wav2Vec2.0 and HuBERT)\nhave shown strong potential for downstream SER tasks. However, task-specific\nfine-tuning remains necessary for various conversational scenarios of different\ntopics, speakers and languages to achieve satisfactory performance. It\ngenerally requires costly encoder retraining for individual SER tasks. To\naddress this issue, we propose to train an adapter with local attributes for\nefficient transfer learning. Specifically, a weighted average\npooling-Transformer (WAP-Transformer) is proposed as a lightweight backbone to\nenrich the frame-level representation. An adapter with teacher-student branches\nis exploited for task-agnostic transfer learning, where the student branch is\njointly optimized via mask prediction and self-distillation objectives, and the\nteacher branch is obtained online from the student via exponential moving\naverage (EMA). Meanwhile, local attributes are learned from the teacher branch\nvia unsupervised clustering, which aims to act as a universal model that\nprovides additional semantic-rich supervisions. A statistical attentive pooling\n(SAP) module is proposed to obtain utterance representation for fine-tuning. To\nevaluate the effectiveness of the proposed adapter with local attributes,\nextensive experiments have been conducted on IEMOCAP. Superior performance has\nbeen reported, compared to the previous state-of-the-art methods in similar\nsettings."}
{"id": "2509.23792", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23792", "abs": "https://arxiv.org/abs/2509.23792", "authors": ["Kabuto Arai", "Takumi Yoshida", "Takumi Takahashi", "Koji Ishibashi"], "title": "Expectation Propagation-Based Signal Detection for Highly Correlated MIMO Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Large-scale multiple-input-multiple-output (MIMO) systems typically operate\nin dense array deployments with limited scattering environments, leading to\nhighly correlated and ill-conditioned channel matrices that severely degrade\nthe performance of message-passing-based detectors. To tackle this issue, this\npaper proposes an expectation propagation (EP)-based detector, termed\noverlapping block partitioning EP (OvEP). In OvEP, the large-scale measurement\nvector is partitioned into partially overlapping blocks. For each block and its\noverlapping part, a low-complexity linear minimum mean square error\n(LMMSE)-based filter is designed according to the partitioned structure. The\nresulting LMMSE outputs are then combined to generate the input to the\ndenoiser. In this combining process, subtracting the overlapping-part outputs\nfrom the block outputs effectively mitigates the adverse effects of inter-block\ncorrelation induced by high spatial correlation. The proposed algorithm is\nconsistently derived within the EP framework, and its fixed point is\ntheoretically proven to coincide with the stationary point of a relaxed\nKullback- Leibler (KL) minimization problem. The mechanisms underlying the\ntheoretically predicted performance improvement are further clarified through\nnumerical simulations. The proposed algorithm achieves performance close to\nconventional LMMSE-EP with lower computational complexity."}
{"id": "2509.24286", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24286", "abs": "https://arxiv.org/abs/2509.24286", "authors": ["Jeng-Yue Liu", "Ting-Chao Hsu", "Yen-Tung Yeh", "Li Su", "Yi-Hsuan Yang"], "title": "SynthCloner: Synthesizer Preset Conversion via Factorized Codec with ADSR Envelope Control", "comment": "Submitted to ICASSP26", "summary": "Electronic synthesizer sounds are controlled by presets, parameters settings\nthat yield complex timbral characteristics and ADSR envelopes, making preset\nconversion particularly challenging. Recent approaches to timbre transfer often\nrely on spectral objectives or implicit style matching, offering limited\ncontrol over envelope shaping. Moreover, public synthesizer datasets rarely\nprovide diverse coverage of timbres and ADSR envelopes. To address these gaps,\nwe present SynthCloner, a factorized codec model that disentangles audio into\nthree attributes: ADSR envelope, timbre, and content. This separation enables\nexpressive synthesizer preset conversion with independent control over these\nthree attributes. Additionally, we introduce SynthCAT, a new synthesizer\ndataset with a task-specific rendering pipeline covering 250 timbres, 120 ADSR\nenvelopes, and 100 MIDI sequences. Experiments show that SynthCloner\noutperforms baselines on both objective and subjective metrics, while enabling\nindependent attribute control. The code, model checkpoint, and audio examples\nare available at https://buffett0323.github.io/synthcloner/."}
{"id": "2509.23878", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23878", "abs": "https://arxiv.org/abs/2509.23878", "authors": ["Wei Zeng", "Junchuan Zhao", "Ye Wang"], "title": "Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription", "comment": "30 pages, 13 figures", "summary": "Expressive performance rendering (EPR) and automatic piano transcription\n(APT) are fundamental yet inverse tasks in music information retrieval: EPR\ngenerates expressive performances from symbolic scores, while APT recovers\nscores from performances. Despite their dual nature, prior work has addressed\nthem independently. In this paper we propose a unified framework that jointly\nmodels EPR and APT by disentangling note-level score content and global\nperformance style representations from both paired and unpaired data. Our\nframework is built on a transformer-based sequence-to-sequence architecture and\nis trained using only sequence-aligned data, without requiring fine-grained\nnote-level alignment. To automate the rendering process while ensuring\nstylistic compatibility with the score, we introduce an independent\ndiffusion-based performance style recommendation module that generates style\nembeddings directly from score content. This modular component supports both\nstyle transfer and flexible rendering across a range of expressive styles.\nExperimental results from both objective and subjective evaluations demonstrate\nthat our framework achieves competitive performance on EPR and APT tasks, while\nenabling effective content-style disentanglement, reliable style transfer, and\nstylistically appropriate rendering. Demos are available at\nhttps://jointpianist.github.io/epr-apt/"}
{"id": "2509.23807", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23807", "abs": "https://arxiv.org/abs/2509.23807", "authors": ["Hongyu Wang", "Wenjia Xu", "Guangzuo Li", "Siyuan Wan", "Yaohua Sun", "Jiuniu Wang", "Mugen Peng"], "title": "Online Specific Emitter Identification via Collision-Alleviated Signal Hash", "comment": "This paper has been accepted by IEEE Transactions on Vehicular\n  Technology", "summary": "Specific Emitter Identification (SEI) has been widely studied, aiming to\ndistinguish signals from different emitters given training samples from those\nemitters. However, real-world scenarios often require identifying signals from\nnovel emitters previously unseen. Since these novel emitters only have a few or\nno prior samples, existing models struggle to identify signals from novel\nemitters online and tend to bias toward the distribution of seen emitters. To\naddress these challenges, we propose the Online Specific Emitter Identification\n(OSEI) task, comprising both online \\revise{few-shot and generalized zero-shot}\nlearning tasks. It requires constructing models using signal samples from seen\nemitters and then identifying new samples from seen and novel emitters online\nduring inference. We propose a novel hash-based model, Collision-Alleviated\nSignal Hash (CASH), providing a unified approach for addressing the OSEI task.\nThe CASH operates in two steps: in the seen emitters identifying step, a signal\nencoder and a seen emitters identifier determine whether the signal sample is\nfrom seen emitters, mitigating the model from biasing toward seen emitters\ndistribution. In the signal hash coding step, an online signal hasher assigns a\nhash code to each signal sample, identifying its specific emitter. Experimental\nresults on real-world signal datasets (i.e., ADSB and ORACLE) demonstrate that\nour method accurately identifies signals from both seen and novel emitters\nonline. This model outperforms existing methods by a minimum of 6.08\\% and\n8.55\\% in accuracy for the few-shot and \\revise{generalized zero-shot learning\n}tasks, respectively. The code will be open-sourced at\n\\href{https://github.com/IntelliSensing/OSEI-CASH}{https://github.com/IntelliSensing/OSEI-CASH}."}
{"id": "2509.24310", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24310", "abs": "https://arxiv.org/abs/2509.24310", "authors": ["Hexin Liu", "Haoyang Zhang", "Qiquan Zhang", "Xiangyu Zhang", "Dongyuan Shi", "Eng Siong Chng", "Haizhou Li"], "title": "Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives", "comment": "11 pages, 3 figures, 9 tables, submitted to IEEE TASLP", "summary": "Code-switching automatic speech recognition (CS-ASR) presents unique\nchallenges due to language confusion introduced by spontaneous intra-sentence\nswitching and accent bias that blurs the phonetic boundaries. Although the\nconstituent languages may be individually high-resource, the scarcity of\nannotated code-switching data further compounds these challenges. In this\npaper, we systematically analyze CS-ASR from both model-centric and\ndata-centric perspectives. By comparing state-of-the-art algorithmic methods,\nincluding language-specific processing and auxiliary language-aware multi-task\nlearning, we discuss their varying effectiveness across datasets with different\nlinguistic characteristics. On the data side, we first investigate TTS as a\ndata augmentation method. By varying the textual characteristics and speaker\naccents, we analyze the impact of language confusion and accent bias on CS-ASR.\nTo further mitigate data scarcity and enhance textual diversity, we propose a\nprompting strategy by simplifying the equivalence constraint theory (SECT) to\nguide large language models (LLMs) in generating linguistically valid\ncode-switching text. The proposed SECT outperforms existing methods in ASR\nperformance and linguistic quality assessments, generating code-switching text\nthat more closely resembles real-world code-switching text. When used to\ngenerate speech-text pairs via TTS, SECT proves effective in improving CS-ASR\nperformance. Our analysis of both model- and data-centric methods underscores\nthat effective CS-ASR requires strategies to be carefully aligned with the\nspecific linguistic characteristics of the code-switching data."}
{"id": "2509.24391", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24391", "abs": "https://arxiv.org/abs/2509.24391", "authors": ["Xuenan Xu", "Jiahao Mei", "Zihao Zheng", "Ye Tao", "Zeyu Xie", "Yaoyun Zhang", "Haohe Liu", "Yuning Wu", "Ming Yan", "Wen Wu", "Chao Zhang", "Mengyue Wu"], "title": "UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities", "comment": "Project page: https://wsntxxn.github.io/uniflow_audio", "summary": "Audio generation, including speech, music and sound effects, has advanced\nrapidly in recent years. These tasks can be divided into two categories:\ntime-aligned (TA) tasks, where each input unit corresponds to a specific\nsegment of the output audio (e.g., phonemes aligned with frames in speech\nsynthesis); and non-time-aligned (NTA) tasks, where such alignment is not\navailable. Since modeling paradigms for the two types are typically different,\nresearch on different audio generation tasks has traditionally followed\nseparate trajectories. However, audio is not inherently divided into such\ncategories, making a unified model a natural and necessary goal for general\naudio generation. Previous unified audio generation works have adopted\nautoregressive architectures, while unified non-autoregressive approaches\nremain largely unexplored. In this work, we propose UniFlow-Audio, a universal\naudio generation framework based on flow matching. We propose a dual-fusion\nmechanism that temporally aligns audio latents with TA features and integrates\nNTA features via cross-attention in each model block. Task-balanced data\nsampling is employed to maintain strong performance across both TA and NTA\ntasks. UniFlow-Audio supports omni-modalities, including text, audio, and\nvideo. By leveraging the advantage of multi-task learning and the generative\nmodeling capabilities of flow matching, UniFlow-Audio achieves strong results\nacross 7 tasks using fewer than 8K hours of public training data and under 1B\ntrainable parameters. Even the small variant with only ~200M trainable\nparameters shows competitive performance, highlighting UniFlow-Audio as a\npotential non-auto-regressive foundation model for audio generation. Code and\nmodels will be available at https://wsntxxn.github.io/uniflow_audio."}
{"id": "2509.23920", "categories": ["eess.SP", "math.PR", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23920", "abs": "https://arxiv.org/abs/2509.23920", "authors": ["Masahiro Kurisaki"], "title": "Asymptotic Expansion for Nonlinear Filtering in the Small System Noise Regime", "comment": "This paper is a self-contained exposition of the methodological part\n  of Section 4 in arXiv:2501.16333", "summary": "We propose a new asymptotic expansion method for nonlinear filtering, based\non a small parameter in the system noise. The conditional expectation is\nexpanded as a power series in the noise level, with each coefficient computed\nby solving a system of ordinary differential equations. This approach mitigates\nthe trade-off between computational efficiency and accuracy inherent in\nexisting methods such as Gaussian approximations and particle filters.\nMoreover, by incorporating an Edgeworth-type expansion, our method captures\ncomplex features of the conditional distribution, such as multimodality, with\nsignificantly lower computational cost than conventional filtering algorithms."}
{"id": "2509.24395", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24395", "abs": "https://arxiv.org/abs/2509.24395", "authors": ["Runwu Shi", "Kai Li", "Chang Li", "Jiang Wang", "Sihan Tan", "Kazuhiro Nakadai"], "title": "Unsupervised Single-Channel Speech Separation with a Diffusion Prior under Speaker-Embedding Guidance", "comment": "5 pages, 2 figures, submitted to ICASSP 2026", "summary": "Speech separation is a fundamental task in audio processing, typically\naddressed with fully supervised systems trained on paired mixtures. While\neffective, such systems typically rely on synthetic data pipelines, which may\nnot reflect real-world conditions. Instead, we revisit the source-model\nparadigm, training a diffusion generative model solely on anechoic speech and\nformulating separation as a diffusion inverse problem. However, unconditional\ndiffusion models lack speaker-level conditioning, they can capture local\nacoustic structure but produce temporally inconsistent speaker identities in\nseparated sources. To address this limitation, we propose Speaker-Embedding\nguidance that, during the reverse diffusion process, maintains speaker\ncoherence within each separated track while driving embeddings of different\nspeakers further apart. In addition, we propose a new separation-oriented\nsolver tailored for speech separation, and both strategies effectively enhance\nperformance on the challenging task of unsupervised source-model-based speech\nseparation, as confirmed by extensive experimental results. Audio samples and\ncode are available at https://runwushi.github.io/UnSepDiff_demo."}
{"id": "2509.24404", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24404", "abs": "https://arxiv.org/abs/2509.24404", "authors": ["Song-Ze Yu"], "title": "From Sound to Setting: AI-Based Equalizer Parameter Prediction for Piano Tone Replication", "comment": "Undergraduate project technical preprint. 4 pages, 6 figures. Code &\n  data: https://github.com/vaclisinc/Vaclis_Tone_Replication Primary: cs.SD,\n  Secondary: cs.LG", "summary": "This project presents an AI-based system for tone replication in music\nproduction, focusing on predicting EQ parameter settings directly from audio\nfeatures. Unlike traditional audio-to-audio methods, our approach outputs\ninterpretable parameter values (e.g., EQ band gains) that musicians can further\nadjust in their workflow. Using a dataset of piano recordings with\nsystematically varied EQ settings, we evaluate both regression and neural\nnetwork models. The neural network achieves a mean squared error of 0.0216 on\nmulti-band tasks. The system enables practical, flexible, and automated tone\nmatching for music producers and lays the foundation for extensions to more\ncomplex audio effects."}
{"id": "2509.24097", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24097", "abs": "https://arxiv.org/abs/2509.24097", "authors": ["Henglin Pu", "Zhu Han", "Athina P. Petropulu", "Husheng Li"], "title": "Wideband Integrated Sensing and Communications: Spectral Efficiency and Signaling Design", "comment": null, "summary": "In integrated sensing and communications (ISAC), a distinguishing feature of\n6G wireless networks, the main challenge lies in integrating the two distinct\nfunctions of sensing and communication within the same waveform. In this paper,\nthe ISAC waveform synthesis is studied in the wideband regime, since a large\nbandwidth can simplify the analysis and is justified by the employment of\nmillimeter wave or higher frequency band. Standard orthogonal frequency\ndivision multiplexing (OFDM) signaling is assumed, and the wideband analysis of\nsensing is a counterpart of the existing studies on wideband communications. It\nis proposed that the phase over such OFDM subcarriers is for modulating\ncommunication messages while the power spectral density (PSD) is shaped for the\nsensing performance. Beyond OFDM, we further reveal a duality between the\nproposed PSD-shaping rule and the orthogonal time frequency space (OTFS)\nwaveform. Flattening the OTFS delay-axis PSD produces the same integrated\nsidelobe level (ISL) reduction effect in the delay-Doppler domain as PSD\ncontrol achieves for OFDM in the frequency domain. To balance communication and\nsensing performance over frequency-selective channels, we propose a\nlow-complexity, water-filling-like allocator with an explicit PSD-flatness\n(variance) constraint. The performance of the proposed wideband ISAC scheme is\ndemonstrated using both numerical simulations and hardware experiments."}
{"id": "2509.24457", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24457", "abs": "https://arxiv.org/abs/2509.24457", "authors": ["Wolfgang Mack", "Nezih Topaloglu", "Laura Lechler", "Ivana Balić", "Alexandra Craciun", "Mansur Yesilbursa", "Kamil Wojcicki"], "title": "Assessing speech quality metrics for evaluation of neural audio codecs under clean speech conditions", "comment": null, "summary": "Objective speech-quality metrics are widely used to assess codec performance.\nHowever, for neural codecs, it is often unclear which metrics provide reliable\nquality estimates. To address this, we evaluated 45 objective metrics by\ncorrelating their scores with subjective listening scores for clean speech\nacross 17 codec conditions. Neural-based metrics such as scoreq and utmos\nachieved the highest Pearson correlations with subjective scores. Further\nanalysis across different subjective quality ranges revealed that non-intrusive\nmetrics tend to saturate at high subjective quality levels."}
{"id": "2509.24463", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24463", "abs": "https://arxiv.org/abs/2509.24463", "authors": ["Nia D'Souza Ganapathy", "Arul Selvamani Shaja"], "title": "An Agent-Based Framework for Automated Higher-Voice Harmony Generation", "comment": null, "summary": "The generation of musically coherent and aesthetically pleasing harmony\nremains a significant challenge in the field of algorithmic composition. This\npaper introduces an innovative Agentic AI-enabled Higher Harmony Music\nGenerator, a multi-agent system designed to create harmony in a collaborative\nand modular fashion. Our framework comprises four specialized agents: a\nMusic-Ingestion Agent for parsing and standardizing input musical scores; a\nChord-Knowledge Agent, powered by a Chord-Former (Transformer model), to\ninterpret and provide the constituent notes of complex chord symbols; a\nHarmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN)\nto compose a melodically and rhythmically complementary harmony line; and an\nAudio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer\nto render the final symbolic output into high-fidelity audio. By delegating\nspecific tasks to specialized agents, our system effectively mimics the\ncollaborative process of human musicians. This modular, agent-based approach\nallows for robust data processing, deep theoretical understanding, creative\ncomposition, and realistic audio synthesis, culminating in a system capable of\ngenerating sophisticated and contextually appropriate higher-voice harmonies\nfor given melodies."}
{"id": "2509.24178", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24178", "abs": "https://arxiv.org/abs/2509.24178", "authors": ["Chengwei Zhou", "Steve Majerus", "Gourav Datta"], "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State Monitoring", "comment": "Under review", "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."}
{"id": "2509.24570", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24570", "abs": "https://arxiv.org/abs/2509.24570", "authors": ["Yun Chen", "Qi Chen", "Zheqi Dai", "Arshdeep Singh", "Philip J. B. Jackson", "Mark D. Plumbley"], "title": "ISSE: An Instruction-Guided Speech Style Editing Dataset And Benchmark", "comment": null, "summary": "Speech style editing refers to modifying the stylistic properties of speech\nwhile preserving its linguistic content and speaker identity. However, most\nexisting approaches depend on explicit labels or reference audio, which limits\nboth flexibility and scalability. More recent attempts to use natural language\ndescriptions remain constrained by oversimplified instructions and coarse style\ncontrol. To address these limitations, we introduce an Instruction-guided\nSpeech Style Editing Dataset (ISSE). The dataset comprises nearly 400 hours of\nspeech and over 100,000 source-target pairs, each aligned with diverse and\ndetailed textual editing instructions. We also build a systematic instructed\nspeech data generation pipeline leveraging large language model, expressive\ntext-to-speech and voice conversion technologies to construct high-quality\npaired samples. Furthermore, we train an instruction-guided autoregressive\nspeech model on ISSE and evaluate it in terms of instruction adherence, timbre\npreservation, and content consistency. Experimental results demonstrate that\nISSE enables accurate, controllable, and generalizable speech style editing\ncompared to other datasets. The project page of ISSE is available at\nhttps://ychenn1.github.io/ISSE/."}
{"id": "2509.24482", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24482", "abs": "https://arxiv.org/abs/2509.24482", "authors": ["Roman B. Gebhardt", "Arne Kuhle", "Eylül Bektur"], "title": "Beyond Genre: Diagnosing Bias in Music Embeddings Using Concept Activation Vectors", "comment": "ISMIR 2025", "summary": "Music representation models are widely used for tasks such as tagging,\nretrieval, and music understanding. Yet, their potential to encode cultural\nbias remains underexplored. In this paper, we apply Concept Activation Vectors\n(CAVs) to investigate whether non-musical singer attributes - such as gender\nand language - influence genre representations in unintended ways. We analyze\nfour state-of-the-art models (MERT, Whisper, MuQ, MuQ-MuLan) using the STraDa\ndataset, carefully balancing training sets to control for genre confounds. Our\nresults reveal significant model-specific biases, aligning with disparities\nreported in MIR and music sociology. Furthermore, we propose a post-hoc\ndebiasing strategy using concept vector manipulation, demonstrating its\neffectiveness in mitigating these biases. These findings highlight the need for\nbias-aware model design and show that conceptualized interpretability methods\noffer practical tools for diagnosing and mitigating representational bias in\nMIR."}
{"id": "2509.24222", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24222", "abs": "https://arxiv.org/abs/2509.24222", "authors": ["Zhisheng Chen", "Yingwei Zhang", "Qizhen Lan", "Tianyu Liu", "Huacan Wang", "Yi Ding", "Ziyu Jia", "Ronghao Chen", "Kun Wang", "Xinliang Zhou"], "title": "Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning", "comment": null, "summary": "Foundation models pretrained on various and unlabeled data have demonstrated\nsignificant success in natural language and vision, but their application to\nelectroencephalography (EEG) remains challenged due to the signal's unique\nproperties. Existing brain foundation models that inherit architectures\ndesigned for text or images lead to three limitations in pre-training: 1)\nconflating time-domain waveform patterns with frequency-domain rhythmic\nfeatures in a single processing stream, 2) ignoring the critical spatial\ntopology of electrodes with different standards, and 3) reliance on the\ninflexible, dense network to process functionally distinct EEG patterns. To\naddress these challenges, we introduce the Unified Neural Topological\nFoundation Model (Uni-NTFM), which is designed based on neuroscience principles\nto produce universal and interpretable representations. Uni-NTFM integrates\nthree core innovations: 1) a decoupled architecture parallelly encodes time,\nfrequency, and raw signal representations before performing cross-domain\nfeature integration; 2) a topological embedding mechanism to unify electrodes\nfrom different international standards and generate structured input sequences\nfor brain regions; and 3) a Mixture-of-Experts neural Transformer that\nefficiently scales model capacity by routing signal patterns to specialized\nsubnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B\nparameters and was pretrained on over 28,000 hours of diverse EEG data via a\ndual-domain masked reconstruction objective. Uni-NTFM significantly outperforms\nexisting task-specific methods and foundation models across nine distinct\ndownstream tasks under both linear probing and fine-tuning settings,\ndemonstrating a superior ability to learn universal representations of brain\nactivity."}
{"id": "2509.24629", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24629", "abs": "https://arxiv.org/abs/2509.24629", "authors": ["Tianrui Wang", "Haoyu Wang", "Meng Ge", "Cheng Gong", "Chunyu Qiang", "Ziyang Ma", "Zikang Huang", "Guanrou Yang", "Xiaobao Wang", "Eng Siong Chng", "Xie Chen", "Longbiao Wang", "Jianwu Dang"], "title": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis", "comment": null, "summary": "While emotional text-to-speech (TTS) has made significant progress, most\nexisting research remains limited to utterance-level emotional expression and\nfails to support word-level control. Achieving word-level expressive control\nposes fundamental challenges, primarily due to the complexity of modeling\nmulti-emotion transitions and the scarcity of annotated datasets that capture\nintra-sentence emotional and prosodic variation. In this paper, we propose\nWeSCon, the first self-training framework that enables word-level control of\nboth emotion and speaking rate in a pretrained zero-shot TTS model, without\nrelying on datasets containing intra-sentence emotion or speed transitions. Our\nmethod introduces a transition-smoothing strategy and a dynamic speed control\nmechanism to guide the pretrained TTS model in performing word-level expressive\nsynthesis through a multi-round inference process. To further simplify the\ninference, we incorporate a dynamic emotional attention bias mechanism and\nfine-tune the model via self-training, thereby activating its ability for\nword-level expressive control in an end-to-end manner. Experimental results\nshow that WeSCon effectively overcomes data scarcity, achieving\nstate-of-the-art performance in word-level emotional expression control while\npreserving the strong zero-shot synthesis capabilities of the original TTS\nmodel."}
{"id": "2509.24603", "categories": ["cs.SD", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24603", "abs": "https://arxiv.org/abs/2509.24603", "authors": ["Tianle Wang", "Sirui Zhang", "Xinyi Tong", "Peiyang Yu", "Jishang Chen", "Liangke Zhao", "Xinpu Gao", "Yves Zhu", "Tiezheng Ge", "Bo Zheng", "Duo Xu", "Yang Liu", "Xin Jin", "Feng Yu", "Songchun Zhu"], "title": "Discovering \"Words\" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music", "comment": null, "summary": "This paper presents an unsupervised machine learning algorithm that\nidentifies recurring patterns -- referred to as ``music-words'' -- from\nsymbolic music data. These patterns are fundamental to musical structure and\nreflect the cognitive processes involved in composition. However, extracting\nthese patterns remains challenging because of the inherent semantic ambiguity\nin musical interpretation. We formulate the task of music-word discovery as a\nstatistical optimization problem and propose a two-stage\nExpectation-Maximization (EM)-based learning framework: 1. Developing a\nmusic-word dictionary; 2. Reconstructing the music data. When evaluated against\nhuman expert annotations, the algorithm achieved an Intersection over Union\n(IoU) score of 0.61. Our findings indicate that minimizing code length\neffectively addresses semantic ambiguity, suggesting that human optimization of\nencoding systems shapes musical semantics. This approach enables computers to\nextract ``basic building blocks'' from music data, facilitating structural\nanalysis and sparse encoding. The method has two primary applications. First,\nin AI music, it supports downstream tasks such as music generation,\nclassification, style transfer, and improvisation. Second, in musicology, it\nprovides a tool for analyzing compositional patterns and offers insights into\nthe principle of minimal encoding across diverse musical styles and composers."}
{"id": "2509.24355", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24355", "abs": "https://arxiv.org/abs/2509.24355", "authors": ["Sefa Kayraklık", "Recep Baş", "Hasan Oğuzhan Çalışkan", "Samed Şahinoğlu", "Sercan Erdoğan", "İlhami Ünal", "İbrahim Hökelek", "Kıvanç Nurdan", "Ali Görçin"], "title": "N78 Frequency Band Modular RIS Design and Implementation", "comment": "Presented at EuMC2025, Copyright EuMA", "summary": "Reconfigurable intelligent surface (RIS), capable of dynamically controlling\nwireless propagation characteristics using reflecting antenna elements, is a\npromising technology for enhancing signal coverage and improving end-user\nconnectivity in next-generation wireless networks. This paper presents a\ncomplete design flow of a modular RIS prototype operating at the n78 frequency\nband, starting from the simulations to the prototype development and testing.\nAn RIS prototype includes one master and up to sixteen slave blocks, each of\nwhich has an identical hardware structure with $8\\times 8$ reflecting surface\nelements and a controller board. The phase shift response of each unit element\nis controlled with a PIN diode to form a $180^\\circ$ phase difference between\nthe ON and OFF states. The measurement experiment using two RIS blocks, horn\nantennas, and a vector network analyzer showed that the improvement of the\nreceived signal power is more than $15$ dB across the n78 frequency band for a\ngiven placement."}
{"id": "2509.24674", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24674", "abs": "https://arxiv.org/abs/2509.24674", "authors": ["Manasi Chhibber", "Jagabandhu Mishra", "Tomi H. Kinnunen"], "title": "Advancing Zero-Shot Open-Set Speech Deepfake Source Tracing", "comment": null, "summary": "We propose a novel zero-shot source tracing framework inspired by advances in\nspeaker verification. Specifically, we adapt the SSL-AASIST system for attack\nclassification, ensuring that the attacks used for training are disjoint from\nthose used to form fingerprint-trial pairs. For backend scoring in attack\nverification, we explore both zero-shot approaches (cosine similarity and\nSiamese) and few-shot approaches (MLP and Siamese). Experiments on our recently\nintroduced STOPA dataset suggest that few-shot learning provides advantages in\nthe closed-set scenario, while zero-shot approaches perform better in the\nopen-set scenario. In closed-set trials, few-shot Siamese and MLP achieve equal\nerror rates (EER) of 18.44% and 15.11%, compared to 27.14% for zero-shot cosine\nscoring. Conversely, in open-set trials, zero-shot cosine scoring reaches\n21.70%, outperforming few-shot Siamese and MLP at 27.40% and 22.65%,\nrespectively."}
{"id": "2509.24635", "categories": ["cs.SD", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2509.24635", "abs": "https://arxiv.org/abs/2509.24635", "authors": ["Zeyu Xie", "Chenxing Li", "Xuenan Xu", "Mengyue Wu", "Wenfu Wang", "Ruibo Fu", "Meng Yu", "Dong Yu", "Yuexian Zou"], "title": "When Audio Generators Become Good Listeners: Generative Features for Understanding Tasks", "comment": null, "summary": "This work pioneers the utilization of generative features in enhancing audio\nunderstanding. Unlike conventional discriminative features that directly\noptimize posterior and thus emphasize semantic abstraction while losing fine\ngrained details, audio generation models inherently encode both spatiotemporal\nperception (capturing local acoustic texture across time and frequency) and\nsemantic prior (knowing what to generate). It motivates us to explore the\nbridge of these complementary strengths. We provide a systematic investigation\nof their differences and complementary relationships, and ultimately propose an\neffective fusion strategy. Experiments across multiple tasks, including sound\nevent classification, tagging, and particularly the fine grained task of audio\ncaptioning, demonstrate consistent performance gains. Beyond empirical\nimprovements, this work more importantly introduces a new perspective on audio\nrepresentation learning, highlighting that generative discriminative\ncomplementarity can provide both detailed perception and semantic awareness for\naudio understanding."}
{"id": "2509.24428", "categories": ["eess.SP", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.24428", "abs": "https://arxiv.org/abs/2509.24428", "authors": ["Santos Michelena", "Maxime Ferreira Da Costa", "José Picheral"], "title": "Strong Basin of Attraction for Unmixing Kernels With the Variable Projection Method", "comment": "5 pages, 4 figures. Submitted to the 2026 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP)", "summary": "The problem of recovering a mixture of spike signals convolved with distinct\npoint spread functions (PSFs) lying on a parametric manifold, under the\nassumption that the spike locations are known, is studied. The PSF unmixing\nproblem is formulated as a projected non-linear least squares estimator. A\nlower bound on the radius of the region of strong convexity is established in\nthe presence of noise as a function of the manifold coherence and Lipschitz\nproperties, guaranteeing convergence and stability of the optimization program.\nNumerical experiments highlight the speed of decay of the PSF class in the\nproblem's conditioning and confirm theoretical findings. Finally, the proposed\nestimator is deployed on real-world spectroscopic data from laser-induced\nbreakdown spectroscopy (LIBS), removing the need for manual calibration and\nvalidating the method's practical relevance."}
{"id": "2509.24708", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24708", "abs": "https://arxiv.org/abs/2509.24708", "authors": ["Xingchen Li", "Hanke Xie", "Ziqian Wang", "Zihan Zhang", "Longshuai Xiao", "Lei Xie"], "title": "SenSE: Semantic-Aware High-Fidelity Universal Speech Enhancement", "comment": "Under review", "summary": "Generative universal speech enhancement (USE) methods aim to leverage\ngenerative models to improve speech quality under various types of distortions.\nDiffusion- or flow-based generative models are capable of producing enhanced\nspeech with high quality and fidelity. However, they typically achieve speech\nenhancement by learning an acoustic feature mapping from degraded speech to\nclean speech, while lacking awareness of high-level semantic information. This\ndeficiency tends to cause semantic ambiguity and acoustic discontinuities in\nthe enhanced speech. In contrast, humans can often comprehend heavily corrupted\nspeech by relying on semantic priors, suggesting that semantics play a crucial\nrole in speech enhancement. Therefore, in this paper, we propose SenSE, which\nleverages a language model to capture the semantic information of distorted\nspeech and effectively integrates it into a flow-matching-based speech\nenhancement framework. Specifically, we introduce a semantic-aware speech\nlanguage model to capture the semantics of degraded speech and generate\nsemantic tokens. We then design a semantic guidance mechanism that incorporates\nsemantic information into the flow-matching-based speech enhancement process,\neffectively mitigating semantic ambiguity. In addition, we propose a prompt\nguidance mechanism, which leverages a short reference utterance to alleviate\nthe loss of speaker similarity under severe distortion conditions. The results\nof several benchmark data sets demonstrate that SenSE not only ensures high\nperceptual quality but also substantially improves speech fidelity while\nmaintaining strong robustness under severe distortions. Codes and demos are\navailable."}
{"id": "2509.24650", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24650", "abs": "https://arxiv.org/abs/2509.24650", "authors": ["Yixuan Zhou", "Guoyang Zeng", "Xin Liu", "Xiang Li", "Renjie Yu", "Ziyang Wang", "Runchuan Ye", "Weiyue Sun", "Jiancheng Gui", "Kehan Li", "Zhiyong Wu", "Zhiyuan Liu"], "title": "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning", "comment": "Technical Report", "summary": "Generative models for speech synthesis face a fundamental trade-off: discrete\ntokens ensure stability but sacrifice expressivity, while continuous signals\nretain acoustic richness but suffer from error accumulation due to task\nentanglement. This challenge has driven the field towards multi-stage pipelines\nthat rely on pre-trained speech tokenizers, but these create a\nsemantic-acoustic divide, limiting holistic and expressive speech generation.\nWe resolve these dilemma through hierarchical semantic-acoustic modeling with\nsemi-discrete residual representations and present a novel tokenizer-free TTS\nmodel VoxCPM. Our framework introduces a differentiable quantization bottleneck\nthat induces natural specialization: a Text-Semantic Language Model (TSLM)\ngenerates semantic-prosodic plans, while a Residual Acoustic Model (RALM)\nrecovers fine-grained acoustic details. This hierarchical semantic-acoustic\nrepresentation guides a local diffusion-based decoder to generate high-fidelity\nspeech latents. Critically, the entire architecture is trained end-to-end under\na simple diffusion objective, eliminating dependency on external speech\ntokenizers. Trained on a massive 1.8 million hours of bilingual corpus, our\nVoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among\nopen-source systems, demonstrating that our approach delivers expressive and\nstable synthesis. Besides, VoxCPM shows the capability to comprehend text to\ninfer and generate appropriate prosody and style, delivering speech with\ncontext-aware expressiveness and natural flow. To facilitate community-driven\nresearch and development, VoxCPM is publicly accessible under Apache 2.0."}
{"id": "2509.24537", "categories": ["eess.SP", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.24537", "abs": "https://arxiv.org/abs/2509.24537", "authors": ["Philipp del Hougne"], "title": "Low-Complexity Wireless Multi-Port Sensing by Multiplexed De-Embedding of an Over-the-Air Fixture", "comment": "9 pages including 5 figures", "summary": "Wireless multi-port sensing remotely retrieves the scattering matrix of a\nmulti-port device under test (DUT) connected to a set of\nnot-directly-accessible (NDA) antennas that couple over-the-air (OTA) to a set\nof accessible antennas. If (i) the OTA fixture characteristics are known, and\n(ii) the number of independent measurements at the accessible antennas is\nsufficient, the OTA fixture can be de-embedded to recover the DUT\ncharacteristics. In recent prior work, we solved (i) by connecting the NDA\nantennas to a specific known tunable load network (TLN). Here, we tackle (ii)\nby additionally using the TLN to provide measurement diversity. The connection\nbetween OTA fixture and TLN constitutes a programmable fixture (PF). When the\nDUT characteristics cannot be identified based on a single PF realization, we\nadd measurement diversity with multiple PF realizations. The underlying\n\"multiplexed de-embedding\" achieves the joint de-embedding of an ensemble of PF\nrealizations when a single PF realization cannot be de-embedded. We\nexperimentally demonstrate our concept by remotely estimating the scattering\nmatrix of a reciprocal, non-unitary 4-port DUT (10 complex-valued unknowns) via\na rich-scattering OTA fixture purely based on measurements of a single\ntransmission coefficient between two accessible antennas across 30 different PF\nrealizations. We systematically study the trade-off between the number of\nindependent measurements at the accessible antennas and the number of PF\nrealizations. Multiplexed de-embedding of the OTA fixture paves the path to\nimplementing wireless multi-port sensing with low hardware complexity in areas\nlike RFID and wireless bioelectronics."}
{"id": "2509.24769", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24769", "abs": "https://arxiv.org/abs/2509.24769", "authors": ["Imran Muhammad", "Gerald Schuller"], "title": "Deep Learning-Based Prediction of Energy Decay Curves from Room Geometry and Material Properties", "comment": "Submitted to ICASSP 2026. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component", "summary": "Accurate prediction of energy decay curves (EDCs) enables robust analysis of\nroom acoustics and reliable estimation of key parameters. We present a deep\nlearning framework that predicts EDCs directly from room geometry and surface\nabsorption. A dataset of 6000 shoebox rooms with realistic dimensions,\nsource-receiver placements, and frequency-dependent wall absorptions was\nsynthesized. For each configuration we simulate room impulse responses (RIRs)\nusing Pyroomacoustics and compute target EDCs. Normalized room features are\nprovided to a long short-term memory (LSTM) network that maps configuration to\nEDC. Performance is evaluated with mean absolute error (MAE) and root mean\nsquare error (RMSE) over time. We further derive early decay time (EDT),\nreverberation time (T20), and clarity index (C50) from predicted and target\nEDCs; close agreement is observed (e.g., EDT MAE 0.017 s, T20 MAE 0.021 s). The\napproach generalizes across diverse rooms and supports efficient room-acoustics\nmodeling for early-stage design and real-time applications."}
{"id": "2509.24793", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24793", "abs": "https://arxiv.org/abs/2509.24793", "authors": ["Théo Mariotte", "Martin Lebourdais", "Antonio Almudévar", "Marie Tahon", "Alfonso Ortega", "Nicolas Dugué"], "title": "Sparse Autoencoders Make Audio Foundation Models more Explainable", "comment": "5 pages, 5 figures, 1 table, submitted to ICASSP 2026", "summary": "Audio pretrained models are widely employed to solve various tasks in speech\nprocessing, sound event detection, or music information retrieval. However, the\nrepresentations learned by these models are unclear, and their analysis mainly\nrestricts to linear probing of the hidden representations. In this work, we\nexplore the use of Sparse Autoencoders (SAEs) to analyze the hidden\nrepresentations of pretrained models, focusing on a case study in singing\ntechnique classification. We first demonstrate that SAEs retain both\ninformation about the original representations and class labels, enabling their\ninternal structure to provide insights into self-supervised learning systems.\nFurthermore, we show that SAEs enhance the disentanglement of vocal attributes,\nestablishing them as an effective tool for identifying the underlying factors\nencoded in the representations."}
{"id": "2509.24588", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24588", "abs": "https://arxiv.org/abs/2509.24588", "authors": ["Luis F. Abanto-Leon", "Muhammad Salman", "Lismer Andres Caceres-Najarro"], "title": "BARProp: Fast-Converging and Memory-Efficient RSS-Based Localization Algorithm for IoT", "comment": "9 pages, 8 figures, and 4 tables", "summary": "Leveraging received signal strength (RSS) measurements for indoor\nlocalization is highly attractive due to their inherent availability in\nubiquitous wireless protocols. However, prevailing RSS-based methods often\ndepend on complex computational algorithms or specialized hardware, rendering\nthem impractical for low-cost access points. To address these challenges, this\npaper introduces buffer-aided RMSProp (BARProp), a fast and memory-efficient\nlocalization algorithm specifically designed for RSS-based tasks. The key\ninnovation of BARProp lies in a novel mechanism that dynamically adapts the\ndecay factor by monitoring the energy variations of recent gradients stored in\na buffer, thereby achieving both accelerated convergence and enhanced\nstability. Furthermore, BARProp requires less than 15% of the memory used by\nstate-of-the-art methods. Extensive evaluations with real-world data\ndemonstrate that BARProp not only achieves higher localization accuracy but\nalso delivers at least a fourfold improvement in convergence speed compared to\nexisting benchmarks."}
{"id": "2509.24773", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24773", "abs": "https://arxiv.org/abs/2509.24773", "authors": ["Xin Cheng", "Yuyue Wang", "Xihua Wang", "Yihan Wu", "Kaisi Guan", "Yijing Chen", "Peng Zhang", "Xiaojiang Liu", "Meng Cao", "Ruihua Song"], "title": "VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning", "comment": "Paper Under Review", "summary": "Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed\nas separate tasks, with limited exploration to unify them within a signle\nframework. Recent attempts to unify V2S and VisualTTS face challenges in\nhandling distinct condition types (e.g., heterogeneous video and transcript\nconditions) and require complex training stages. Unifying these two tasks\nremains an open problem. To bridge this gap, we present VSSFlow, which\nseamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching\nframework. VSSFlow uses a novel condition aggregation mechanism to handle\ndistinct input signals. We find that cross-attention and self-attention layer\nexhibit different inductive biases in the process of introducing condition.\nTherefore, VSSFlow leverages these inductive biases to effectively handle\ndifferent representations: cross-attention for ambiguous video conditions and\nself-attention for more deterministic speech transcripts. Furthermore, contrary\nto the prevailing belief that joint training on the two tasks requires complex\ntraining strategies and may degrade performance, we find that VSSFlow benefits\nfrom the end-to-end joint learning process for sound and speech generation\nwithout extra designs on training stages. Detailed analysis attributes it to\nthe learned general audio prior shared between tasks, which accelerates\nconvergence, enhances conditional generation, and stabilizes the\nclassifier-free guidance process. Extensive experiments demonstrate that\nVSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S\nand VisualTTS benchmarks, underscoring the critical potential of unified\ngenerative models."}
{"id": "2509.24853", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24853", "abs": "https://arxiv.org/abs/2509.24853", "authors": ["Xavier Riley", "Simon Dixon"], "title": "Enhanced Automatic Drum Transcription via Drum Stem Source Separation", "comment": null, "summary": "Automatic Drum Transcription (ADT) remains a challenging task in MIR but\nrecent advances allow accurate transcription of drum kits with up 5 classes -\nkick, snare, hi-hats, toms and cymbals - via the ADTOF package. In addition,\nseveral drum kit \\emph{stem} separation models in the open source community\nsupport separation for more than 6 stem classes, including distinct crash and\nride cymbals. In this work we explore the benefits of combining these tools to\nimprove the realism of drum transcriptions. We describe a simple\npost-processing step which expands the transcription output from five to seven\nclasses and furthermore, we are able to estimate MIDI velocity values based on\nthe separated stems. Our solution achieves strong performance when assessed\nagainst a baseline of 8-class drum transcription and produces realistic MIDI\ntranscriptions suitable for MIR or music production tasks."}
{"id": "2509.24683", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24683", "abs": "https://arxiv.org/abs/2509.24683", "authors": ["Johan Arbustini", "Eric Elzenheimer", "Elizaveta Spetzler", "Pablo Mendoza", "Daniel Fernández", "Jordi Madrenas", "Jeffrey McCord", "Michael Höft", "Robert Rieger", "Andreas Bahr"], "title": "Impedance Modeling of Magnetometers: A Path Toward Low-Noise Readout Circuits", "comment": "4 pages, 3 figures, BMT2025 conference paper", "summary": "Optimizing sensor readout schemes and integrated circuit designs for both\nopen-loop and closed-loop implementations requires precise modeling and\nsimulation strategies. This study introduces a novel two-port impedance model\nto estimate the behavior of a converse Magnetoelectric (cME) sensor. This model\nprovides a possible framework for calculating transfer functions and simulating\nmagnetometer behavior in both continuous- and discrete-time simulation\nenvironments, and it is also possibly transferable to other magnetometer types.\nCommon S-parameters were measured experimentally using an impedance analyzer\nand converted to Z-parameters to create a transfer function for system-level\nsimulations. The model was validated through an analysis of output-related\nnoise using MATLAB and LTSpice simulations to optimize the noise of the analog\ncircuit parts of the system. The simulation results were compared with\nexperimental measurements using a Zurich Instruments lock-in amplifier and the\ncustom-designed low-noise printed circuit board (PCB) under model\nconsiderations. The proposed methodology derives noise considerations and the\ntransfer function of a magnetometer. These are essential for readout schemes\nfor mixed-signal circuit design. This allows low-noise electronics to be\ndesigned and extended to other sensor interface electronics, broadening their\napplicability in high-performance magnetic sensing."}
{"id": "2509.24834", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24834", "abs": "https://arxiv.org/abs/2509.24834", "authors": ["Imran Muhammad", "Gerald Schuller"], "title": "Room Impulse Response Prediction with Neural Networks: From Energy Decay Curves to Perceptual Validation", "comment": "Submitted to ICASSP 2026. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Prediction of room impulse responses (RIRs) is essential for room acoustics,\nspatial audio, and immersive applications, yet conventional simulations and\nmeasurements remain computationally expensive and time-consuming. This work\nproposes a neural network framework that predicts energy decay curves (EDCs)\nfrom room dimensions, material absorption coefficients, and source-receiver\npositions, and reconstructs corresponding RIRs via reverse-differentiation. A\nlarge training dataset was generated using room acoustic simulations with\nrealistic geometries, frequency-dependent absorption, and diverse\nsource-receiver configurations. Objective evaluation employed root mean squared\nerror (RMSE) and a custom loss for EDCs, as well as correlation, mean squared\nerror (MSE), spectral similarity for reconstructed RIRs. Perceptual validation\nthrough a MUSHRA listening test confirmed no significant perceptual differences\nbetween predicted and reference RIRs. The results demonstrate that the proposed\nframework provides accurate and perceptually reliable RIR predictions, offering\na scalable solution for practical acoustic modeling and audio rendering\napplications."}
{"id": "2509.24901", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24901", "abs": "https://arxiv.org/abs/2509.24901", "authors": ["Lukas Rauch", "René Heinrich", "Houtan Ghaffari", "Lukas Miklautz", "Ilyass Moummad", "Bernhard Sick", "Christoph Scholz"], "title": "Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio Classification", "comment": "Currently under review @ICLR2026", "summary": "Although probing frozen models has become a standard evaluation paradigm,\nself-supervised learning in audio defaults to fine-tuning. A key reason is that\nglobal pooling creates an information bottleneck causing linear probes to\nmisrepresent the embedding quality: The $\\texttt{cls}$-token discards crucial\ntoken information about dispersed, localized events in multi-label audio. This\nweakness is rooted in the mismatch between the pretraining objective (operating\nglobally) and the downstream task (localized events). Across a comprehensive\nbenchmark of 13 datasets and 6 spectrogram-based encoders, we first investigate\nthe global pooling bottleneck. We then introduce binarized prototypical probes:\na lightweight and simple pooling method that learns prototypes to perform\nclass-wise information aggregation. Despite its simplicity, our method notably\noutperforms linear and attentive probing. Our work establishes probing as a\ncompetitive and efficient paradigm for evaluating audio SSL models, challenging\nthe reliance on costly fine-tuning."}
{"id": "2509.24805", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.24805", "abs": "https://arxiv.org/abs/2509.24805", "authors": ["Andriy Enttsel", "Alex Marchioni", "Andrea Zanellini", "Mauro Mangia", "Gianluca Setti", "Riccardo Rovatti"], "title": "RDD: Pareto Analysis of the Rate-Distortion-Distinguishability Trade-off", "comment": "12 pages, 11 figures", "summary": "Extensive monitoring systems generate data that is usually compressed for\nnetwork transmission. This compressed data might then be processed in the cloud\nfor tasks such as anomaly detection. However, compression can potentially\nimpair the detector's ability to distinguish between regular and irregular\npatterns due to information loss. Here we extend the information-theoretic\nframework introduced in [1] to simultaneously address the trade-off between the\nthree features on which the effectiveness of the system depends: the\neffectiveness of compression, the amount of distortion it introduces, and the\ndistinguishability between compressed normal signals and compressed anomalous\nsignals. We leverage a Gaussian assumption to draw curves showing how moving on\na Pareto surface helps administer such a trade-off better than simply relying\non optimal rate-distortion compression and hoping that compressed signals can\nbe distinguished from each other."}
{"id": "2509.24924", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24924", "abs": "https://arxiv.org/abs/2509.24924", "authors": ["Jaekwon Im", "Juhan Nam"], "title": "SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution", "comment": "5 pages, 3 figures", "summary": "Versatile audio super-resolution (SR) aims to predict high-frequency\ncomponents from low-resolution audio across diverse domains such as speech,\nmusic, and sound effects. Existing diffusion-based SR methods often fail to\nproduce semantically aligned outputs and struggle with consistent\nhigh-frequency reconstruction. In this paper, we propose SAGA-SR, a versatile\naudio SR model that combines semantic and acoustic guidance. Based on a DiT\nbackbone trained with a flow matching objective, SAGA-SR is conditioned on text\nand spectral roll-off embeddings. Due to the effective guidance provided by its\nconditioning, SAGA-SR robustly upsamples audio from arbitrary input sampling\nrates between 4 kHz and 32 kHz to 44.1 kHz. Both objective and subjective\nevaluations show that SAGA-SR achieves state-of-the-art performance across all\ntest cases. Sound examples and code for the proposed model are available\nonline."}
{"id": "2509.25028", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.25028", "abs": "https://arxiv.org/abs/2509.25028", "authors": ["Eric Browne"], "title": "The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools", "comment": "12 Pages, 2 figures, 1 table, The AI Music Creativity Conference\n  (AIMC), 2025", "summary": "Randomness plays a pivotal yet paradoxical role in computational music\ncreativity: it can spark novelty, but unchecked chance risks incoherence. This\npaper presents a thematic review of contemporary AI music systems, examining\nhow designers incorporate randomness and uncertainty into creative practice. I\ndraw on the concept of structured uncertainty to analyse how stochastic\nprocesses are constrained within musical and interactive frameworks. Through a\ncomparative analysis of six systems - Musika (Pasini and Schl\\\"uter, 2022),\nMIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and\nEsling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) -\nwe identify recurring design patterns that support musical coherence, user\ncontrol, and co-creativity. To my knowledge, this is the first thematic review\nexamining randomness in AI music through structured uncertainty, offering\npractical insights for designers and artists aiming to support expressive,\ncollaborative, or improvisational interactions."}
{"id": "2509.24819", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24819", "abs": "https://arxiv.org/abs/2509.24819", "authors": ["Kunyu Wu", "Qiushi Zhao", "Zihan Feng", "Yunxi Mu", "Hao Qin", "Xinyu Zhang", "Xingqi Zhang"], "title": "Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning", "comment": null, "summary": "Urban railway systems increasingly rely on communication based train control\n(CBTC) systems, where optimal deployment of access points (APs) in tunnels is\ncritical for robust wireless coverage. Traditional methods, such as empirical\nmodel-based optimization algorithms, are hindered by excessive measurement\nrequirements and suboptimal solutions, while machine learning (ML) approaches\noften struggle with complex tunnel environments. This paper proposes a deep\nreinforcement learning (DRL) driven framework that integrates parabolic wave\nequation (PWE) channel modeling, conditional generative adversarial network\n(cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for\nAP placement optimization. The PWE method generates high-fidelity path loss\ndistributions for a subset of AP positions, which are then expanded by the cGAN\nto create high resolution path loss maps for all candidate positions,\nsignificantly reducing simulation costs while maintaining physical accuracy. In\nthe DRL framework, the state space captures AP positions and coverage, the\naction space defines AP adjustments, and the reward function encourages signal\nimprovement while penalizing deployment costs. The dueling DQN enhances\nconvergence speed and exploration exploitation balance, increasing the\nlikelihood of reaching optimal configurations. Comparative experiments show\nthat the proposed method outperforms a conventional Hooke Jeeves optimizer and\ntraditional DQN, delivering AP configurations with higher average received\npower, better worst-case coverage, and improved computational efficiency. This\nwork integrates high-fidelity electromagnetic simulation, generative modeling,\nand AI-driven optimization, offering a scalable and data-efficient solution for\nnext-generation CBTC systems in complex tunnel environments."}
{"id": "2509.19812", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.19812", "abs": "https://arxiv.org/abs/2509.19812", "authors": ["Yang Cui", "Peter Pan", "Lei He", "Sheng Zhao"], "title": "Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation", "comment": "6 pages of main text, 1 page of references, 2 figures, 2 tables,\n  accepted at ASRU 2025", "summary": "With the rapid advancement of speech generative models, unauthorized voice\ncloning poses significant privacy and security risks. Speech watermarking\noffers a viable solution for tracing sources and preventing misuse. Current\nwatermarking technologies fall mainly into two categories: DSP-based methods\nand deep learning-based methods. DSP-based methods are efficient but vulnerable\nto attacks, whereas deep learning-based methods offer robust protection at the\nexpense of significantly higher computational cost. To improve the\ncomputational efficiency and enhance the robustness, we propose PKDMark, a\nlightweight deep learning-based speech watermarking method that leverages\nprogressive knowledge distillation (PKD). Our approach proceeds in two stages:\n(1) training a high-performance teacher model using an invertible neural\nnetwork-based architecture, and (2) transferring the teacher's capabilities to\na compact student model through progressive knowledge distillation. This\nprocess reduces computational costs by 93.6% while maintaining high level of\nrobust performance and imperceptibility. Experimental results demonstrate that\nour distilled model achieves an average detection F1 score of 99.6% with a PESQ\nof 4.30 in advanced distortions, enabling efficient speech watermarking for\nreal-time speech synthesis applications."}
{"id": "2509.25131", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.25131", "abs": "https://arxiv.org/abs/2509.25131", "authors": ["Chengyao Wang", "Zhisheng Zhong", "Bohao Peng", "Senqiao Yang", "Yuqi Liu", "Haokun Gui", "Bin Xia", "Jingyao Li", "Bei Yu", "Jiaya Jia"], "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech", "comment": "Code is available at https://github.com/dvlab-research/MGM-Omni", "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation."}
{"id": "2509.24941", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24941", "abs": "https://arxiv.org/abs/2509.24941", "authors": ["Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Emil Björnson"], "title": "Low-Complexity Receiver Design for Multicarrier CAPA-based Systems in Doubly-Dispersive Channels", "comment": "Submitted to an IEEE conference", "summary": "We propose a novel low-complexity receiver design for multicarrier continuous\naperture array (CAPA) systems operating over doubly-dispersive (DD) channels.\nThe receiver leverages a Gaussian Belief Propagation (GaBP)-based framework\nthat hinges only on element-wise scalar operations for the detection of the\ntransmitted symbols. Simulation results for the orthogonal frequency division\nmultiplexing (OFDM), orthogonal time frequency space (OTFS), and affine\nfrequency division multiplexing (AFDM) waveforms demonstrate significant\nperformance improvements in terms of uncoded bit error rate (BER) compared to\nconventional discrete antenna array systems, while maintaining very low\ncomputational complexity."}
{"id": "2509.22655", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22655", "abs": "https://arxiv.org/abs/2509.22655", "authors": ["Jackson Loth", "Pedro Sarmento", "Saurjya Sarkar", "Zixun Guo", "Mathieu Barthet", "Mark Sandler"], "title": "GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures", "comment": "To be published in Proceedings of the International Society for Music\n  Information Retrieval Conference (ISMIR), 2025", "summary": "In recent years, the guitar has received increased attention from the music\ninformation retrieval (MIR) community driven by the challenges posed by its\ndiverse playing techniques and sonic characteristics. Mainly fueled by deep\nlearning approaches, progress has been limited by the scarcity and limited\nannotations of datasets. To address this, we present the Guitar On Audio and\nTablatures (GOAT) dataset, comprising 5.9 hours of unique high-quality direct\ninput audio recordings of electric guitars from a variety of different guitars\nand players. We also present an effective data augmentation strategy using\nguitar amplifiers which delivers near-unlimited tonal variety, of which we\nprovide a starting 29.5 hours of audio. Each recording is annotated using\nguitar tablatures, a guitar-specific symbolic format supporting string and fret\nnumbers, as well as numerous playing techniques. For this we utilise both the\nGuitar Pro format, a software for tablature playback and editing, and a\ntext-like token encoding. Furthermore, we present competitive results using\nGOAT for MIDI transcription and preliminary results for a novel approach to\nautomatic guitar tablature transcription. We hope that GOAT opens up the\npossibilities to train novel models on a wide variety of guitar-related MIR\ntasks, from synthesis to transcription to playing technique detection."}
{"id": "2509.22718", "categories": ["eess.AS", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22718", "abs": "https://arxiv.org/abs/2509.22718", "authors": ["Ke Gu", "Zhicong Wu", "Peng Bai", "Sitong Qiao", "Zhiqi Jiang", "Junchen Lu", "Xiaodong Shi", "Xinyuan Qian"], "title": "PerformSinger: Multimodal Singing Voice Synthesis Leveraging Synchronized Lip Cues from Singing Performance Videos", "comment": null, "summary": "Existing singing voice synthesis (SVS) models largely rely on fine-grained,\nphoneme-level durations, which limits their practical application. These\nmethods overlook the complementary role of visual information in duration\nprediction.To address these issues, we propose PerformSinger, a pioneering\nmultimodal SVS framework, which incorporates lip cues from video as a visual\nmodality, enabling high-quality \"duration-free\" singing voice synthesis.\nPerformSinger comprises parallel multi-branch multimodal encoders, a feature\nfusion module, a duration and variational prediction network, a mel-spectrogram\ndecoder and a vocoder. The fusion module, composed of adapter and fusion\nblocks, employs a progressive fusion strategy within an aligned semantic space\nto produce high-quality multimodal feature representations, thereby enabling\naccurate duration prediction and high-fidelity audio synthesis. To facilitate\nthe research, we design, collect and annotate a novel SVS dataset involving\nsynchronized video streams and precise phoneme-level manual annotations.\nExtensive experiments demonstrate the state-of-the-art performance of our\nproposal in both subjective and objective evaluations. The code and dataset\nwill be publicly available."}
{"id": "2509.25095", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25095", "abs": "https://arxiv.org/abs/2509.25095", "authors": ["M A Al-Masud", "Juan Miguel Lopez Alcaraz", "Nils Strodthoff"], "title": "Benchmarking ECG Foundational Models: A Reality Check Across Clinical Tasks", "comment": "26 pages, 3 figures source code under\n  https://github.com/AI4HealthUOL/ecg-fm-benchmarking", "summary": "The 12-lead electrocardiogram (ECG) is a long-standing diagnostic tool. Yet\nmachine learning for ECG interpretation remains fragmented, often limited to\nnarrow tasks or datasets. Foundation models promise broader adaptability, but\ntheir generalization across diverse ECG tasks is not well understood. We\nbenchmarked eight ECG foundation models on 26 clinically relevant tasks using\n12 public datasets comprising 1,650 regression and classification targets.\nModels were evaluated under fine-tuning and frozen settings, with scaling\nanalyses across dataset sizes. Results show heterogeneous performance across\ndomains: in the most widely studied domain, adult ECG interpretation, three\nfoundation models consistently outperformed strong supervised baselines. In\ncontrast, ECG-CPC, a compact structured state-space model pretrained on HEEDB,\ndominated other categories where most foundation models failed to surpass\nsupervised learning. Foundation models also displayed distinct scaling\nbehaviors with dataset size, which are critical for small-scale clinical\napplications. Overall, while foundation models show promise for adult ECG\nanalysis, substantial gaps remain in cardiac structure, outcome prediction, and\npatient characterization. Notably, ECG-CPC's strong performance despite being\norders of magnitude smaller and consuming minimal computational resources\nhighlights untapped opportunities for advancing ECG foundation models."}
{"id": "2509.22727", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22727", "abs": "https://arxiv.org/abs/2509.22727", "authors": ["Ziqi Chen", "Gongyu Chen", "Yihua Wang", "Chaofan Ding", "Zihao chen", "Wei-Qiang Zhang"], "title": "DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation", "comment": "5 pages, 2 figures", "summary": "Dialect speech embodies rich cultural and linguistic diversity, yet building\ntext-to-speech (TTS) systems for dialects remains challenging due to scarce\ndata, inconsistent orthographies, and complex phonetic variation. To address\nthese issues, we present DiaMoE-TTS, a unified IPA-based framework that\nstandardizes phonetic representations and resolves grapheme-to-phoneme\nambiguities. Built upon the F5-TTS architecture, the system introduces a\ndialect-aware Mixture-of-Experts (MoE) to model phonological differences and\nemploys parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and\nConditioning Adapters for rapid transfer to new dialects. Unlike approaches\ndependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable,\nopen-data-driven synthesis. Experiments demonstrate natural and expressive\nspeech generation, achieving zero-shot performance on unseen dialects and\nspecialized domains such as Peking Opera with only a few hours of data."}
{"id": "2509.22740", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22740", "abs": "https://arxiv.org/abs/2509.22740", "authors": ["Jinbae Seo", "Hyeongjun Kwon", "Kwonyoung Kim", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation", "comment": null, "summary": "Audiovisual instance segmentation (AVIS) requires accurately localizing and\ntracking sounding objects throughout video sequences. Existing methods suffer\nfrom visual bias stemming from two fundamental issues: uniform additive fusion\nprevents queries from specializing to different sound sources, while\nvisual-only training objectives allow queries to converge to arbitrary salient\nobjects. We propose Audio-Centric Query Generation using cross-attention,\nenabling each query to selectively attend to distinct sound sources and carry\nsound-specific priors into visual decoding. Additionally, we introduce\nSound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding\nobject numbers through ordinal regression with monotonic consistency\nconstraints, preventing visual-only convergence during training. Experiments on\nAVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and\n+2.06 FSLA, validating that query specialization and explicit counting\nsupervision are crucial for accurate audiovisual instance segmentation."}
{"id": "2509.23364", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23364", "abs": "https://arxiv.org/abs/2509.23364", "authors": ["Francesca Ronchini", "Luca Comanducci", "Simone Marcucci", "Fabio Antonacci"], "title": "AI-Assisted Music Production: A User Study on Text-to-Music Models", "comment": "Accepted at 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR 25)", "summary": "Text-to-music models have revolutionized the creative landscape, offering new\npossibilities for music creation. Yet their integration into musicians\nworkflows remains underexplored. This paper presents a case study on how TTM\nmodels impact music production, based on a user study of their effect on\nproducers creative workflows. Participants produce tracks using a custom tool\ncombining TTM and source separation models. Semi-structured interviews and\nthematic analysis reveal key challenges, opportunities, and ethical\nconsiderations. The findings offer insights into the transformative potential\nof TTMs in music production, as well as challenges in their real-world\nintegration."}
{"id": "2509.22728", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22728", "abs": "https://arxiv.org/abs/2509.22728", "authors": ["Xuanhao Zhang", "Chang Li"], "title": "Prompt-aware classifier free guidance for diffusion models", "comment": "5 pages, 3 figures", "summary": "Diffusion models have achieved remarkable progress in image and audio\ngeneration, largely due to Classifier-Free Guidance. However, the choice of\nguidance scale remains underexplored: a fixed scale often fails to generalize\nacross prompts of varying complexity, leading to oversaturation or weak\nalignment. We address this gap by introducing a prompt-aware framework that\npredicts scale-dependent quality and selects the optimal guidance at inference.\nSpecifically, we construct a large synthetic dataset by generating samples\nunder multiple scales and scoring them with reliable evaluation metrics. A\nlightweight predictor, conditioned on semantic embeddings and linguistic\ncomplexity, estimates multi-metric quality curves and determines the best scale\nvia a utility function with regularization. Experiments on MSCOCO~2014 and\nAudioCaps show consistent improvements over vanilla CFG, enhancing fidelity,\nalignment, and perceptual preference. This work demonstrates that prompt-aware\nscale selection provides an effective, training-free enhancement for pretrained\ndiffusion backbones."}
{"id": "2509.22744", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22744", "abs": "https://arxiv.org/abs/2509.22744", "authors": ["Jinming Chen", "Lu Wang", "Zheshu Song", "Wei Deng"], "title": "Index-MSR: A high-efficiency multimodal fusion framework for speech recognition", "comment": "Submit to icassp 2026", "summary": "Driven by large scale datasets and LLM based architectures, automatic speech\nrecognition (ASR) systems have achieved remarkable improvements in accuracy.\nHowever, challenges persist for domain-specific terminology, and short\nutterances lacking semantic coherence, where recognition performance often\ndegrades significantly. In this work, we present Index-MSR, an efficient\nmultimodal speech recognition framework. At its core is a novel Multimodal\nFusion Decoder (MFD), which effectively incorporates text-related information\nfrom videos (e.g., subtitles and presentation slides) into the speech\nrecognition. This cross-modal integration not only enhances overall ASR\naccuracy but also yields substantial reductions in substitution errors.\nExtensive evaluations on both an in-house subtitle dataset and a public AVSR\ndataset demonstrate that Index-MSR achieves sota accuracy, with substitution\nerrors reduced by 20,50%. These results demonstrate that our approach\nefficiently exploits text-related cues from video to improve speech recognition\naccuracy, showing strong potential in applications requiring strict audio text\nsynchronization, such as audio translation."}
{"id": "2509.23454", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23454", "abs": "https://arxiv.org/abs/2509.23454", "authors": ["Md. Saiful Bari Siddiqui", "Utsab Saha"], "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification", "comment": "Submitted to ICASSP 2026. This preprint includes some additional\n  details beyond the conference submission", "summary": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training."}
{"id": "2509.22838", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.22838", "abs": "https://arxiv.org/abs/2509.22838", "authors": ["Elliot Q C Garcia", "Nicéias Silva Vilela", "Kátia Pires Nascimento do Sacramento", "Tiago A. E. Ferreira"], "title": "Text-Independent Speaker Identification Using Audio Looping With Margin Based Loss Functions", "comment": "18 pages, 6 figures", "summary": "Speaker identification has become a crucial component in various\napplications, including security systems, virtual assistants, and personalized\nuser experiences. In this paper, we investigate the effectiveness of CosFace\nLoss and ArcFace Loss for text-independent speaker identification using a\nConvolutional Neural Network architecture based on the VGG16 model, modified to\naccommodate mel spectrogram inputs of variable sizes generated from the\nVoxceleb1 dataset. Our approach involves implementing both loss functions to\nanalyze their effects on model accuracy and robustness, where the Softmax loss\nfunction was employed as a comparative baseline. Additionally, we examine how\nthe sizes of mel spectrograms and their varying time lengths influence model\nperformance. The experimental results demonstrate superior identification\naccuracy compared to traditional Softmax loss methods. Furthermore, we discuss\nthe implications of these findings for future research."}
{"id": "2509.22942", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.22942", "abs": "https://arxiv.org/abs/2509.22942", "authors": ["Dominik Klement", "Matthew Maciejewski", "Sanjeev Khudanpur", "Jan Černocký", "Lukáš Burget"], "title": "Unsupervised Speech Enhancement using Data-defined Priors", "comment": "Submitted to ICASSP 2026", "summary": "The majority of deep learning-based speech enhancement methods require paired\nclean-noisy speech data. Collecting such data at scale in real-world conditions\nis infeasible, which has led the community to rely on synthetically generated\nnoisy speech. However, this introduces a gap between the training and testing\nphases. In this work, we propose a novel dual-branch encoder-decoder\narchitecture for unsupervised speech enhancement that separates the input into\nclean speech and residual noise. Adversarial training is employed to impose\npriors on each branch, defined by unpaired datasets of clean speech and,\noptionally, noise. Experimental results show that our method achieves\nperformance comparable to leading unsupervised speech enhancement approaches.\nFurthermore, we demonstrate the critical impact of clean speech data selection\non enhancement performance. In particular, our findings reveal that performance\nmay appear overly optimistic when in-domain clean speech data are used for\nprior definition -- a practice adopted in previous unsupervised speech\nenhancement studies."}
{"id": "2509.23238", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23238", "abs": "https://arxiv.org/abs/2509.23238", "authors": ["Goksenin Yuksel", "Pierre Guetschel", "Michael Tangermann", "Marcel van Gerven", "Kiki van der Heijden"], "title": "WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms", "comment": "Still under review", "summary": "Learning audio representations from raw waveforms overcomes key limitations\nof spectrogram-based audio representation learning, such as the long latency of\nspectrogram computation and the loss of phase information. Yet, while\nself-supervised speech representation learning from raw waveforms has been\nremarkably successful, these approaches have not achieved similar feats for\ngeneral-purpose audio representation learning from waveforms. Here, we propose\nWavJEPA, a waveform-based version of the Joint-Embedding Predictive\nArchitecture. WavJEPA leverages high-level semantic representation learning to\ntackle the shortcomings of representation learning at the speech unit or token\nlevel. We show that this approach substantially outperforms state-of-the-art\ntime-domain audio foundation models across a wide variety of downstream\nbenchmark tasks, while requiring considerably fewer computational resources.\nAdditionally, to overcome the performance drop that time-domain models\ntypically exhibit in noisy and reverberant real-world acoustic environments, we\npresent WavJEPA-Nat. WavJEPA-Nat is a multi-channel extension of the WavJEPA\narchitecture trained on simulated naturalistic scenes. We find that WavJEPA-Nat\nis highly robust to reverberation and noise. These results highlight the\nfeasibility and computational efficiency of general-purpose audio\nrepresentation learning from raw waveforms, showcasing the potential for\nlow-latency, robust time-domain audio foundation models for real-world\napplications."}
{"id": "2509.23147", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23147", "abs": "https://arxiv.org/abs/2509.23147", "authors": ["Abdul Rehman", "Jingyao Cai", "Jian-Jun Zhang", "Xiaosong Yang"], "title": "BFA: Real-time Multilingual Text-to-speech Forced Alignment", "comment": "Under review", "summary": "We present Bournemouth Forced Aligner (BFA), a system that combines a\nContextless Universal Phoneme Encoder (CUPE) with a connectionist temporal\nclassification (CTC)based decoder. BFA introduces explicit modelling of\ninter-phoneme gaps and silences and hierarchical decoding strategies, enabling\nfine-grained boundary prediction. Evaluations on TIMIT and Buckeye corpora show\nthat BFA achieves competitive recall relative to Montreal Forced Aligner at\nrelaxed tolerance levels, while predicting both onset and offset boundaries for\nricher temporal structure. BFA processes speech up to 240x faster than MFA,\nenabling faster than real-time alignment. This combination of speed and\nsilence-aware alignment opens opportunities for interactive speech applications\npreviously constrained by slow aligners."}
{"id": "2509.23299", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23299", "abs": "https://arxiv.org/abs/2509.23299", "authors": ["Yike Zhu", "Boyi Kang", "Ziqian Wang", "Xingchen Li", "Zihan Zhang", "Wenjie Li", "Longshuai Xiao", "Wei Xue", "Lei Xie"], "title": "MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow", "comment": "Submitted to ICASSP 2026", "summary": "Speech enhancement (SE) recovers clean speech from noisy signals and is vital\nfor applications such as telecommunications and automatic speech recognition\n(ASR). While generative approaches achieve strong perceptual quality, they\noften rely on multi-step sampling (diffusion/flow-matching) or large language\nmodels, limiting real-time deployment. To mitigate these constraints, we\npresent MeanFlowSE, a one-step generative SE framework. It adopts MeanFlow to\npredict an average-velocity field for one-step latent refinement and conditions\nthe model on self-supervised learning (SSL) representations rather than VAE\nlatents. This design accelerates inference and provides robust\nacoustic-semantic guidance during training. In the Interspeech 2020 DNS\nChallenge blind test set and simulated test set, MeanFlowSE attains\nstate-of-the-art (SOTA) level perceptual quality and competitive\nintelligibility while significantly lowering both real-time factor (RTF) and\nmodel size compared with recent generative competitors, making it suitable for\npractical use. The code will be released upon publication at\nhttps://github.com/Hello3orld/MeanFlowSE."}
{"id": "2509.23364", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23364", "abs": "https://arxiv.org/abs/2509.23364", "authors": ["Francesca Ronchini", "Luca Comanducci", "Simone Marcucci", "Fabio Antonacci"], "title": "AI-Assisted Music Production: A User Study on Text-to-Music Models", "comment": "Accepted at 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR 25)", "summary": "Text-to-music models have revolutionized the creative landscape, offering new\npossibilities for music creation. Yet their integration into musicians\nworkflows remains underexplored. This paper presents a case study on how TTM\nmodels impact music production, based on a user study of their effect on\nproducers creative workflows. Participants produce tracks using a custom tool\ncombining TTM and source separation models. Semi-structured interviews and\nthematic analysis reveal key challenges, opportunities, and ethical\nconsiderations. The findings offer insights into the transformative potential\nof TTMs in music production, as well as challenges in their real-world\nintegration."}
{"id": "2509.23358", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23358", "abs": "https://arxiv.org/abs/2509.23358", "authors": ["Chaohao Lin", "Xu Zheng", "Kaida Wu", "Peihao Xiang", "Ou Bai"], "title": "Emotional Styles Hide in Deep Speaker Embeddings: Disentangle Deep Speaker Embeddings for Speaker Clustering", "comment": "6 pages, 4 figures", "summary": "Speaker clustering is the task of identifying the unique speakers in a set of\naudio recordings (each belonging to exactly one speaker) without knowing who\nand how many speakers are present in the entire data, which is essential for\nspeaker diarization processes. Recently, off-the-shelf deep speaker embedding\nmodels have been leveraged to capture speaker characteristics. However,\nspeeches containing emotional expressions pose significant challenges, often\naffecting the accuracy of speaker embeddings and leading to a decline in\nspeaker clustering performance. To tackle this problem, we propose DTG-VAE, a\nnovel disentanglement method that enhances clustering within a Variational\nAutoencoder (VAE) framework. This study reveals a direct link between emotional\nstates and the effectiveness of deep speaker embeddings. As demonstrated in our\nexperiments, DTG-VAE extracts more robust speaker embeddings and significantly\nenhances speaker clustering performance."}
{"id": "2509.23454", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.23454", "abs": "https://arxiv.org/abs/2509.23454", "authors": ["Md. Saiful Bari Siddiqui", "Utsab Saha"], "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification", "comment": "Submitted to ICASSP 2026. This preprint includes some additional\n  details beyond the conference submission", "summary": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training."}
{"id": "2509.23435", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23435", "abs": "https://arxiv.org/abs/2509.23435", "authors": ["Wenyu Li", "Xiaoqi Jiao", "Yi Chang", "Guangyan Zhang", "Yiwen Guo"], "title": "AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models", "comment": null, "summary": "The creation of high-quality multimodal datasets remains fundamental for\nadvancing role-playing capabilities in large language models (LLMs). While\nexisting works predominantly focus on text-based persona simulation, Audio\nRole-Playing (ARP) presents unique challenges due to the need for synchronized\nalignment of semantic content and vocal characteristics. To address this gap,\nwe propose AudioRole, a meticulously curated dataset from 13 TV series spanning\n1K+ hours with 1M+ character-grounded dialogues, providing synchronized\naudio-text pairs annotated with speaker identities and contextual metadata. In\naddition, to demonstrate the effectiveness of the dataset, we introduced\nARP-Eval, a dual-aspect evaluation framework that assesses both response\nquality and role fidelity. Empirical validation showing GLM-4-Voice trained on\nAudioRole (which we called ARP-Model) achieve an average Acoustic\nPersonalization score of 0.31, significantly outperforming the original\nGLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically\nsupports role-playing in one-shot scenarios. The ARP-Model also achieves a\nContent Personalization score of 0.36, surpassing the untrained original model\nby about 38% and maintaining the same level as MiniCPM-O-2.6.\n  AudioRole features dialogues from over 115 main characters, 6 trained\nARP-Models that role-play different characters, and evaluation protocols.\nTogether, they provide an essential resource for advancing audio-grounded\nrole-playing research."}
{"id": "2509.23832", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23832", "abs": "https://arxiv.org/abs/2509.23832", "authors": ["Junyu Wang", "Zizhen Lin", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "title": "LORT: Locally Refined Convolution and Taylor Transformer for Monaural Speech Enhancement", "comment": "Speech Communication", "summary": "Achieving superior enhancement performance while maintaining a low parameter\ncount and computational complexity remains a challenge in the field of speech\nenhancement. In this paper, we introduce LORT, a novel architecture that\nintegrates spatial-channel enhanced Taylor Transformer and locally refined\nconvolution for efficient and robust speech enhancement. We propose a Taylor\nmulti-head self-attention (T-MSA) module enhanced with spatial-channel\nenhancement attention (SCEA), designed to facilitate inter-channel information\nexchange and alleviate the spatial attention limitations inherent in\nTaylor-based Transformers. To complement global modeling, we further present a\nlocally refined convolution (LRC) block that integrates convolutional\nfeed-forward layers, time-frequency dense local convolutions, and gated units\nto capture fine-grained local details. Built upon a U-Net-like encoder-decoder\nstructure with only 16 output channels in the encoder, LORT processes noisy\ninputs through multi-resolution T-MSA modules using alternating downsampling\nand upsampling operations. The enhanced magnitude and phase spectra are decoded\nindependently and optimized through a composite loss function that jointly\nconsiders magnitude, complex, phase, discriminator, and consistency objectives.\nExperimental results on the VCTK+DEMAND and DNS Challenge datasets demonstrate\nthat LORT achieves competitive or superior performance to state-of-the-art\n(SOTA) models with only 0.96M parameters, highlighting its effectiveness for\nreal-world speech enhancement applications with limited computational\nresources."}
{"id": "2509.23878", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.23878", "abs": "https://arxiv.org/abs/2509.23878", "authors": ["Wei Zeng", "Junchuan Zhao", "Ye Wang"], "title": "Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription", "comment": "30 pages, 13 figures", "summary": "Expressive performance rendering (EPR) and automatic piano transcription\n(APT) are fundamental yet inverse tasks in music information retrieval: EPR\ngenerates expressive performances from symbolic scores, while APT recovers\nscores from performances. Despite their dual nature, prior work has addressed\nthem independently. In this paper we propose a unified framework that jointly\nmodels EPR and APT by disentangling note-level score content and global\nperformance style representations from both paired and unpaired data. Our\nframework is built on a transformer-based sequence-to-sequence architecture and\nis trained using only sequence-aligned data, without requiring fine-grained\nnote-level alignment. To automate the rendering process while ensuring\nstylistic compatibility with the score, we introduce an independent\ndiffusion-based performance style recommendation module that generates style\nembeddings directly from score content. This modular component supports both\nstyle transfer and flexible rendering across a range of expressive styles.\nExperimental results from both objective and subjective evaluations demonstrate\nthat our framework achieves competitive performance on EPR and APT tasks, while\nenabling effective content-style disentanglement, reliable style transfer, and\nstylistically appropriate rendering. Demos are available at\nhttps://jointpianist.github.io/epr-apt/"}
{"id": "2509.23833", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.23833", "abs": "https://arxiv.org/abs/2509.23833", "authors": ["Cancan Li", "Fei Su", "Juan Liu", "Hui Bu", "Yulong Wan", "Hongbin Suo", "Ming Li"], "title": "AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset with Speech Recognition Baselines", "comment": null, "summary": "Whisper speech recognition is crucial not only for ensuring privacy in\nsensitive communications but also for providing a critical communication bridge\nfor patients under vocal restraint and enabling discrete interaction in\nnoise-sensitive environments. The development of Chinese mandarin audio-visual\nwhisper speech recognition is hindered by the lack of large-scale datasets. We\npresent AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech\ndataset, featuring 30 hours each of whisper speech and parallel normal speech,\nwith synchronized frontal facial videos. Moreover, we propose an audio-visual\nspeech recognition (AVSR) baseline based on the Whisper-Flamingo framework,\nwhich integrates a parallel training strategy to align embeddings across speech\ntypes, and employs a projection layer to adapt to whisper speech's spectral\nproperties. The model achieves a Character Error Rate (CER) of 4.13% for\nwhisper speech and 1.11% for normal speech in the test set of our dataset, and\nestablishes new state-of-the-art results on the wTIMIT benchmark. The dataset\nand the AVSR baseline codes are open-sourced at\nhttps://zutm.github.io/AISHELL6-Whisper."}
{"id": "2509.24404", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24404", "abs": "https://arxiv.org/abs/2509.24404", "authors": ["Song-Ze Yu"], "title": "From Sound to Setting: AI-Based Equalizer Parameter Prediction for Piano Tone Replication", "comment": "Undergraduate project technical preprint. 4 pages, 6 figures. Code &\n  data: https://github.com/vaclisinc/Vaclis_Tone_Replication Primary: cs.SD,\n  Secondary: cs.LG", "summary": "This project presents an AI-based system for tone replication in music\nproduction, focusing on predicting EQ parameter settings directly from audio\nfeatures. Unlike traditional audio-to-audio methods, our approach outputs\ninterpretable parameter values (e.g., EQ band gains) that musicians can further\nadjust in their workflow. Using a dataset of piano recordings with\nsystematically varied EQ settings, we evaluate both regression and neural\nnetwork models. The neural network achieves a mean squared error of 0.0216 on\nmulti-band tasks. The system enables practical, flexible, and automated tone\nmatching for music producers and lays the foundation for extensions to more\ncomplex audio effects."}
{"id": "2509.24286", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24286", "abs": "https://arxiv.org/abs/2509.24286", "authors": ["Jeng-Yue Liu", "Ting-Chao Hsu", "Yen-Tung Yeh", "Li Su", "Yi-Hsuan Yang"], "title": "SynthCloner: Synthesizer Preset Conversion via Factorized Codec with ADSR Envelope Control", "comment": "Submitted to ICASSP26", "summary": "Electronic synthesizer sounds are controlled by presets, parameters settings\nthat yield complex timbral characteristics and ADSR envelopes, making preset\nconversion particularly challenging. Recent approaches to timbre transfer often\nrely on spectral objectives or implicit style matching, offering limited\ncontrol over envelope shaping. Moreover, public synthesizer datasets rarely\nprovide diverse coverage of timbres and ADSR envelopes. To address these gaps,\nwe present SynthCloner, a factorized codec model that disentangles audio into\nthree attributes: ADSR envelope, timbre, and content. This separation enables\nexpressive synthesizer preset conversion with independent control over these\nthree attributes. Additionally, we introduce SynthCAT, a new synthesizer\ndataset with a task-specific rendering pipeline covering 250 timbres, 120 ADSR\nenvelopes, and 100 MIDI sequences. Experiments show that SynthCloner\noutperforms baselines on both objective and subjective metrics, while enabling\nindependent attribute control. The code, model checkpoint, and audio examples\nare available at https://buffett0323.github.io/synthcloner/."}
{"id": "2509.24482", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24482", "abs": "https://arxiv.org/abs/2509.24482", "authors": ["Roman B. Gebhardt", "Arne Kuhle", "Eylül Bektur"], "title": "Beyond Genre: Diagnosing Bias in Music Embeddings Using Concept Activation Vectors", "comment": "ISMIR 2025", "summary": "Music representation models are widely used for tasks such as tagging,\nretrieval, and music understanding. Yet, their potential to encode cultural\nbias remains underexplored. In this paper, we apply Concept Activation Vectors\n(CAVs) to investigate whether non-musical singer attributes - such as gender\nand language - influence genre representations in unintended ways. We analyze\nfour state-of-the-art models (MERT, Whisper, MuQ, MuQ-MuLan) using the STraDa\ndataset, carefully balancing training sets to control for genre confounds. Our\nresults reveal significant model-specific biases, aligning with disparities\nreported in MIR and music sociology. Furthermore, we propose a post-hoc\ndebiasing strategy using concept vector manipulation, demonstrating its\neffectiveness in mitigating these biases. These findings highlight the need for\nbias-aware model design and show that conceptualized interpretability methods\noffer practical tools for diagnosing and mitigating representational bias in\nMIR."}
{"id": "2509.24395", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24395", "abs": "https://arxiv.org/abs/2509.24395", "authors": ["Runwu Shi", "Kai Li", "Chang Li", "Jiang Wang", "Sihan Tan", "Kazuhiro Nakadai"], "title": "Unsupervised Single-Channel Speech Separation with a Diffusion Prior under Speaker-Embedding Guidance", "comment": "5 pages, 2 figures, submitted to ICASSP 2026", "summary": "Speech separation is a fundamental task in audio processing, typically\naddressed with fully supervised systems trained on paired mixtures. While\neffective, such systems typically rely on synthetic data pipelines, which may\nnot reflect real-world conditions. Instead, we revisit the source-model\nparadigm, training a diffusion generative model solely on anechoic speech and\nformulating separation as a diffusion inverse problem. However, unconditional\ndiffusion models lack speaker-level conditioning, they can capture local\nacoustic structure but produce temporally inconsistent speaker identities in\nseparated sources. To address this limitation, we propose Speaker-Embedding\nguidance that, during the reverse diffusion process, maintains speaker\ncoherence within each separated track while driving embeddings of different\nspeakers further apart. In addition, we propose a new separation-oriented\nsolver tailored for speech separation, and both strategies effectively enhance\nperformance on the challenging task of unsupervised source-model-based speech\nseparation, as confirmed by extensive experimental results. Audio samples and\ncode are available at https://runwushi.github.io/UnSepDiff_demo."}
{"id": "2509.24793", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24793", "abs": "https://arxiv.org/abs/2509.24793", "authors": ["Théo Mariotte", "Martin Lebourdais", "Antonio Almudévar", "Marie Tahon", "Alfonso Ortega", "Nicolas Dugué"], "title": "Sparse Autoencoders Make Audio Foundation Models more Explainable", "comment": "5 pages, 5 figures, 1 table, submitted to ICASSP 2026", "summary": "Audio pretrained models are widely employed to solve various tasks in speech\nprocessing, sound event detection, or music information retrieval. However, the\nrepresentations learned by these models are unclear, and their analysis mainly\nrestricts to linear probing of the hidden representations. In this work, we\nexplore the use of Sparse Autoencoders (SAEs) to analyze the hidden\nrepresentations of pretrained models, focusing on a case study in singing\ntechnique classification. We first demonstrate that SAEs retain both\ninformation about the original representations and class labels, enabling their\ninternal structure to provide insights into self-supervised learning systems.\nFurthermore, we show that SAEs enhance the disentanglement of vocal attributes,\nestablishing them as an effective tool for identifying the underlying factors\nencoded in the representations."}
{"id": "2509.24629", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24629", "abs": "https://arxiv.org/abs/2509.24629", "authors": ["Tianrui Wang", "Haoyu Wang", "Meng Ge", "Cheng Gong", "Chunyu Qiang", "Ziyang Ma", "Zikang Huang", "Guanrou Yang", "Xiaobao Wang", "Eng Siong Chng", "Xie Chen", "Longbiao Wang", "Jianwu Dang"], "title": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis", "comment": null, "summary": "While emotional text-to-speech (TTS) has made significant progress, most\nexisting research remains limited to utterance-level emotional expression and\nfails to support word-level control. Achieving word-level expressive control\nposes fundamental challenges, primarily due to the complexity of modeling\nmulti-emotion transitions and the scarcity of annotated datasets that capture\nintra-sentence emotional and prosodic variation. In this paper, we propose\nWeSCon, the first self-training framework that enables word-level control of\nboth emotion and speaking rate in a pretrained zero-shot TTS model, without\nrelying on datasets containing intra-sentence emotion or speed transitions. Our\nmethod introduces a transition-smoothing strategy and a dynamic speed control\nmechanism to guide the pretrained TTS model in performing word-level expressive\nsynthesis through a multi-round inference process. To further simplify the\ninference, we incorporate a dynamic emotional attention bias mechanism and\nfine-tune the model via self-training, thereby activating its ability for\nword-level expressive control in an end-to-end manner. Experimental results\nshow that WeSCon effectively overcomes data scarcity, achieving\nstate-of-the-art performance in word-level emotional expression control while\npreserving the strong zero-shot synthesis capabilities of the original TTS\nmodel."}
{"id": "2509.24853", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24853", "abs": "https://arxiv.org/abs/2509.24853", "authors": ["Xavier Riley", "Simon Dixon"], "title": "Enhanced Automatic Drum Transcription via Drum Stem Source Separation", "comment": null, "summary": "Automatic Drum Transcription (ADT) remains a challenging task in MIR but\nrecent advances allow accurate transcription of drum kits with up 5 classes -\nkick, snare, hi-hats, toms and cymbals - via the ADTOF package. In addition,\nseveral drum kit \\emph{stem} separation models in the open source community\nsupport separation for more than 6 stem classes, including distinct crash and\nride cymbals. In this work we explore the benefits of combining these tools to\nimprove the realism of drum transcriptions. We describe a simple\npost-processing step which expands the transcription output from five to seven\nclasses and furthermore, we are able to estimate MIDI velocity values based on\nthe separated stems. Our solution achieves strong performance when assessed\nagainst a baseline of 8-class drum transcription and produces realistic MIDI\ntranscriptions suitable for MIR or music production tasks."}
{"id": "2509.24773", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24773", "abs": "https://arxiv.org/abs/2509.24773", "authors": ["Xin Cheng", "Yuyue Wang", "Xihua Wang", "Yihan Wu", "Kaisi Guan", "Yijing Chen", "Peng Zhang", "Xiaojiang Liu", "Meng Cao", "Ruihua Song"], "title": "VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning", "comment": "Paper Under Review", "summary": "Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed\nas separate tasks, with limited exploration to unify them within a signle\nframework. Recent attempts to unify V2S and VisualTTS face challenges in\nhandling distinct condition types (e.g., heterogeneous video and transcript\nconditions) and require complex training stages. Unifying these two tasks\nremains an open problem. To bridge this gap, we present VSSFlow, which\nseamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching\nframework. VSSFlow uses a novel condition aggregation mechanism to handle\ndistinct input signals. We find that cross-attention and self-attention layer\nexhibit different inductive biases in the process of introducing condition.\nTherefore, VSSFlow leverages these inductive biases to effectively handle\ndifferent representations: cross-attention for ambiguous video conditions and\nself-attention for more deterministic speech transcripts. Furthermore, contrary\nto the prevailing belief that joint training on the two tasks requires complex\ntraining strategies and may degrade performance, we find that VSSFlow benefits\nfrom the end-to-end joint learning process for sound and speech generation\nwithout extra designs on training stages. Detailed analysis attributes it to\nthe learned general audio prior shared between tasks, which accelerates\nconvergence, enhances conditional generation, and stabilizes the\nclassifier-free guidance process. Extensive experiments demonstrate that\nVSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S\nand VisualTTS benchmarks, underscoring the critical potential of unified\ngenerative models."}

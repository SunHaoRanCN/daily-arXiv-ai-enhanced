<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 25]
- [eess.AS](#eess.AS) [Total: 22]
- [cs.SD](#cs.SD) [Total: 28]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Generative Modeling and Decision Fusion for Unknown Event Detection and Classification Using Synchrophasor Data](https://arxiv.org/abs/2509.22795)
*Yi Hu,Zheyuan Cheng*

Main category: eess.SP

TL;DR: 提出了一种结合生成建模、滑动窗口处理和决策融合的电力系统事件检测与分类框架，能够识别已知事件并将未知扰动分类为新类别，解决了监督分类器的关键限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的标记数据集，难以泛化到罕见或未见过的电力系统扰动，需要更鲁棒的检测和分类方法。

Method: 使用变分自编码器-生成对抗网络建模正常运行状态，提取重构误差和判别器误差作为异常指标；开发基于阈值的规则和基于凸包的方法两种决策策略；通过滑动窗口机制组织时空检测和分类矩阵；集成多个PMU输出的识别和决策融合阶段。

Result: 实验结果显示达到了最先进的准确率，超越了机器学习、深度学习和基于包络线的基线方法，能够识别未知事件。

Conclusion: 该框架具有适应性和实用价值，适用于现代电力系统的广域事件分析。

Abstract: Reliable detection and classification of power system events are critical for
maintaining grid stability and situational awareness. Existing approaches often
depend on limited labeled datasets, which restricts their ability to generalize
to rare or unseen disturbances. This paper proposes a novel framework that
integrates generative modeling, sliding-window temporal processing, and
decision fusion to achieve robust event detection and classification using
synchrophasor data. A variational autoencoder-generative adversarial network is
employed to model normal operating conditions, where both reconstruction error
and discriminator error are extracted as anomaly indicators. Two complementary
decision strategies are developed: a threshold-based rule for computational
efficiency and a convex hull-based method for robustness under complex error
distributions. These features are organized into spatiotemporal detection and
classification matrices through a sliding-window mechanism, and an
identification and decision fusion stage integrates the outputs across PMUs.
This design enables the framework to identify known events while systematically
classifying previously unseen disturbances into a new category, addressing a
key limitation of supervised classifiers. Experimental results demonstrate
state-of-the-art accuracy, surpassing machine learning, deep learning, and
envelope-based baselines. The ability to recognize unknown events further
highlights the adaptability and practical value of the proposed approach for
wide-area event analysis in modern power systems.

</details>


### [2] [Introducing Multimodal Paradigm for Learning Sleep Staging PSG via General-Purpose Model](https://arxiv.org/abs/2509.22810)
*Jianheng Zhou,Chenyu Liu,Jinan Zhou,Yi Ding,Yang Liu,Haoran Luo,Ziyu Jia,Xinliang Zhou*

Main category: eess.SP

TL;DR: 提出了一种新的睡眠分期方法，将一维PSG信号转换为二维波形图像，利用多模态大模型模拟临床诊断实践，在三个公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动睡眠分期方法通常从复杂PSG信号中提取特征并训练领域特定模型，缺乏直观性且需要大量专业数据集。

Method: 将原始一维PSG时间序列转换为直观的二维波形图像，然后微调多模态大模型从这些表示中学习。

Result: 在ISRUC、MASS、SHHS三个数据集上的实验表明，该方法使通用模型能够获得稳健的分期能力，且通过解释分析发现模型学会了模仿人类专家基于PSG图像的视觉诊断工作流程。

Conclusion: 该方法在准确性和鲁棒性方面持续优于最先进的基线方法，突显了其在医疗应用中的效率和实用价值。

Abstract: Sleep staging is essential for diagnosing sleep disorders and assessing
neurological health. Existing automatic methods typically extract features from
complex polysomnography (PSG) signals and train domain-specific models, which
often lack intuitiveness and require large, specialized datasets. To overcome
these limitations, we introduce a new paradigm for sleep staging that leverages
large multimodal general-purpose models to emulate clinical diagnostic
practices. Specifically, we convert raw one-dimensional PSG time-series into
intuitive two-dimensional waveform images and then fine-tune a multimodal large
model to learn from these representations. Experiments on three public datasets
(ISRUC, MASS, SHHS) demonstrate that our approach enables general-purpose
models, without prior exposure to sleep data, to acquire robust staging
capabilities. Moreover, explanation analysis reveals our model learned to mimic
the visual diagnostic workflow of human experts for sleep staging by PSG
images. The proposed method consistently outperforms state-of-the-art baselines
in accuracy and robustness, highlighting its efficiency and practical value for
medical applications. The code for the signal-to-image pipeline and the PSG
image dataset will be released.

</details>


### [3] [Scalable Wi-Fi RSS-Based Indoor Localization via Automatic Vision-Assisted Calibration](https://arxiv.org/abs/2509.22869)
*Abdulkadir Bilge,Erdem Ergen,Burak Soner,Sinem Coleri*

Main category: eess.SP

TL;DR: 提出了一种轻量级框架，通过摄像头辅助的短时校准阶段自动收集高分辨率同步的RSS-位置数据，用于训练可移动部署的Wi-Fi定位算法，解决了传统方法需要大量标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi定位在室内环境中具有广泛应用前景，但RSS方法易受多径、信道变化和接收器特性影响。监督学习方法虽然更鲁棒，但需要大量标注数据，收集成本高昂。

Method: 使用一次性校准的顶置摄像头跟踪设备，收集附近接入点广播包的RSS数据，自动生成(x, y, RSS)数据集，用于训练移动可部署的定位算法。

Result: 量化了视觉辅助RSS数据收集在跟踪精度和标签同步等关键因素下的精度限制，通过实验数据验证了传统和监督学习方法在不同信号条件和设备类型下的性能，展示了改进的准确性和泛化能力。

Conclusion: 该框架为实际应用提供了实用工具，所有代码、工具和数据集均已开源发布。

Abstract: Wi-Fi-based positioning promises a scalable and privacy-preserving solution
for location-based services in indoor environments such as malls, airports, and
campuses. RSS-based methods are widely deployable as RSS data is available on
all Wi-Fi-capable devices, but RSS is highly sensitive to multipath, channel
variations, and receiver characteristics. While supervised learning methods
offer improved robustness, they require large amounts of labeled data, which is
often costly to obtain. We introduce a lightweight framework that solves this
by automating high-resolution synchronized RSS-location data collection using a
short, camera-assisted calibration phase. An overhead camera is calibrated only
once with ArUco markers and then tracks a device collecting RSS data from
broadcast packets of nearby access points across Wi-Fi channels. The resulting
(x, y, RSS) dataset is used to automatically train mobile-deployable
localization algorithms, avoiding the privacy concerns of continuous video
monitoring. We quantify the accuracy limits of such vision-assisted RSS data
collection under key factors such as tracking precision and label
synchronization. Using the collected experimental data, we benchmark
traditional and supervised learning approaches under varying signal conditions
and device types, demonstrating improved accuracy and generalization,
validating the utility of the proposed framework for practical use. All code,
tools, and datasets are released as open source.

</details>


### [4] [Time-Frequency Analysis of Non-Uniformly Sampled Signals via Sample Density Adaptation](https://arxiv.org/abs/2509.22891)
*Ashwini Kulkarni,Santosh Nannuru*

Main category: eess.SP

TL;DR: 提出了一种针对非均匀采样数据的时频分析方法——非均匀Stockwell变换(NUST)，该方法使用双重自适应窗口，在频率和局部数据密度上调整窗口宽度，能够同时检测瞬态和持续信号。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理非均匀采样数据时存在局限：时间积分方法(如GLS周期图)对瞬态事件不敏感，而时频方法通常依赖固定窗口或插值，对非均匀数据不理想。

Method: 开发了NUST框架，直接对非均匀采样数据应用局部密度自适应谱分析，采用基于频率和局部数据密度的双重自适应窗口。

Result: 在多个非均匀采样合成信号上验证了NUST，显示其相比GLS具有更优的时间定位性能；应用于HD 10180多行星系统的HARPS径向速度数据，成功区分了行星信号和恒星活动。

Conclusion: NUST为分析非均匀采样数据中的非平稳信号提供了有效的时频框架，能够同时处理瞬态和持续信号，在行星探测等应用中表现出色。

Abstract: The analysis of non-stationary signals in non-uniformly sampled data is a
challenging task. Time-integrated methods, such as the generalised Lomb-Scargle
(GLS) periodogram, provide a robust statistical assessment of persistent
periodicities but are insensitive to transient events. Conversely, existing
time-frequency methods often rely on fixed-duration windows or interpolation,
which can be suboptimal for non-uniform data. We introduce the non-uniform
Stockwell-transform (NUST), a time-frequency framework that applies a localized
density adaptive spectral analysis directly to non-uniformly sampled data. NUST
employs a doubly adaptive window that adjusts its width based on both frequency
and local data density, providing detailed time-frequency information for both
transient and persistent signals. We validate the NUST on numerous
non-uniformly sampled synthetic signals, demonstrating its superior
time-localization performance compared to GLS. Furthermore, we apply NUST to
HARPS radial velocity data of the multi-planetary system HD 10180, successfully
distinguishing coherent planetary signals from stellar activity.

</details>


### [5] [Resource Allocation in Cooperative Mid-band/THz Networks in the Presence of Mobility](https://arxiv.org/abs/2509.23065)
*Mohammad Amin Saeidi,Hina Tabassum*

Main category: eess.SP

TL;DR: 该论文开发了一个综合框架来优化多频段协作网络的下行性能，涵盖上中频和太赫兹频段，考虑了近场信道建模、天线架构和用户移动性，提出了联合用户关联与混合波束成形优化算法以及切换感知的资源分配方法。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信向更高频段发展，多频段协作网络需要解决复杂的优化问题，包括近场效应、天线架构限制、用户移动性带来的切换问题，以及如何在不同频段间实现高效协作。

Method: 使用分数规划和主化-最小化技术解决非凸优化问题；提出两种切换感知资源分配方法：第一种通过Jensen不等式和对数函数性质将非凸问题转化为凸问题，第二种处理多目标优化同时最大化系统总速率和最小化切换次数。

Result: 数值结果表明所提算法有效，协作UMB/THz网络优于独立THz网络，准确近场建模对超大规模天线阵列至关重要，切换感知资源分配方法能有效减轻切换影响。

Conclusion: 该框架成功解决了多频段协作网络中的关键挑战，提出的优化算法和切换感知方法显著提升了系统性能，为未来高频段无线通信系统设计提供了重要参考。

Abstract: This paper develops a comprehensive framework to investigate and optimize the
downlink performance of cooperative multi-band networks (MBNs) operating on
upper mid-band (UMB) and terahertz (THz) frequencies, where base stations (BSs)
in each band cooperatively serve users. The framework captures sophisticated
features such as near-field channel modeling, fully and partially connected
antenna architectures, and users' mobility. First, we consider joint user
association and hybrid beamforming optimization to maximize the system
sum-rate, subject to power constraints, maximum cluster size of cooperating
BSs, and users' quality-of-service (QoS) constraints. By leveraging fractional
programming FP and majorization-minimization techniques, an iterative algorithm
is proposed to solve the non-convex optimization problem. We then consider
handover (HO)-aware resource allocation for moving users in a cooperative
UMB/THz MBN. Two HO-aware resource allocation methods are proposed. The first
method focuses on maximizing the HO-aware system sum-rate subject to HO-aware
QoS constraints. Using Jensen's inequality and properties of logarithmic
functions, the non-convex optimization problem is tightly approximated with a
convex one and solved. The second method addresses a multi-objective
optimization problem to maximize the system sum-rate, while minimizing the
total number of HOs. Numerical results demonstrate the efficacy of the proposed
algorithms, cooperative UMB/THz MBN over stand-alone THz networks, as well as
the critical importance of accurate near-field modeling in extremely large
antenna arrays. Moreover, the proposed HO-aware resource allocation methods
effectively mitigate the impact of HOs, enhancing performance in the considered
system.

</details>


### [6] [Dual-Function Beam Pattern Design for Multi-Target ISAC Systems: A Decoupled Approach](https://arxiv.org/abs/2509.23302)
*Wilson de Souza Junior,Taufik Abrao,Amine Mezghani,Ekram Hossain*

Main category: eess.SP

TL;DR: 提出了一种用于多用户多目标ISAC系统的感知引导通信双功能波束成形设计方法，通过问题分解和黎曼流形优化，在保证通信性能的同时提升多目标估计精度。


<details>
  <summary>Details</summary>
Motivation: 在ISAC系统中，感知和通信竞争资源，传统联合优化方法往往忽视这种权衡，当通信需求增加时会导致波束模式改变，进而降低DoA感知精度。

Method: 将感知问题分解为两个子问题，利用波束模式的物理特性，开发了基于黎曼流形优化和凸闭集投影的低复杂度扩展方法。

Result: 仿真结果表明，相比传统联合优化策略，所提方法通过保持波束模式提高了多目标估计精度，低复杂度版本在保持高精度的同时显著降低了计算成本。

Conclusion: SGCDF波束成形设计能有效平衡ISAC系统中感知与通信的性能权衡，低复杂度版本提供了优异的性能-复杂度权衡。

Abstract: We investigate the beampattern design problem for mono-static multi-user (MU)
multi-point-target integrated sensing and communication (ISAC) systems, where a
dual-function multiple-input multiple-output (DF-MIMO) base station (BS)
performs downlink communication and radar sensing simultaneously. In ISAC
systems, sensing and communication inherently compete for resources. As
communication demand increases, the beam pattern is reshaped, which might
degrade the direction of arrival (DoA) sensing accuracy, measured in terms of
mean-squared error (MSE) and lower-bounded by the Cramer-Rao lower bound
(CRLB). Since conventional joint formulations of the sensing-based problem
often overlook this trade-off, our work addresses it by decomposing the
sensing-based problem into two subproblems (SPs). This decomposition enables a
more effective exploitation of the beam pattern's physical properties, which we
refer to as the Sensing-Guided Communication Dual-Function (SGCDF) beam pattern
design. We further develop a low-complexity extension using the Riemannian
Manifold Optimization (RMO) and convex closed-set projection. Simulation
results confirm that the proposed method improves multi-target estimation
accuracy, compared to traditional joint optimization strategies, by preserving
the beam pattern, while the low-complexity version offers an excellent
performance-complexity tradeoff, maintaining high accuracy with significantly
reduced computational cost.

</details>


### [7] [HoloTrace: a Location Privacy Preservation Solution for mmWave MIMO-OFDM Systems](https://arxiv.org/abs/2509.23444)
*Lorenzo Italiano,Alireza Pourafzal,Hui Chen,Mattia Brambilla,Gonzalo Seco-Granados,Monica Nicoli,Henk Wymeersch*

Main category: eess.SP

TL;DR: HoloTrace是一个用户侧信号级隐私保护框架，通过在毫米波MIMO-OFDM系统中欺骗定位相关参数来防止基站非授权定位。


<details>
  <summary>Details</summary>
Motivation: 6G网络增强了用户设备定位能力，但也带来了物理层位置隐私的严重担忧，需要防止基站提取精确位置信息。

Method: 用户侧通过策略性地扰动导频传输来欺骗到达角、离开角和时间到达差等定位参数，无需协议修改或网络侧支持，保持预编码器不变。

Result: 仿真结果表明该方法能有效欺骗基站，导致显著定位误差，对链路容量的影响取决于欺骗位置。

Conclusion: HoloTrace是未来6G网络中实用且鲁棒的隐私保护解决方案。

Abstract: The technological innovation towards 6G cellular networks introduces
unprecedented capabilities for user equipment (UE) localization, but it also
raises serious concerns about physical layer location privacy. This paper
introduces HoloTrace, a signal-level privacy preservation framework that relies
on user-side spoofing of localization-relevant features to prevent the
extraction of precise location information from the signals received by a base
station (BS) in a mmWave MIMO-OFDM system. Spoofing is performed by the user on
location parameters such as angle of arrival (AoA), angle of departure (AoD),
and time difference of arrival (TDoA). Without requiring any protocol
modification nor network-side support, our method strategically perturbs pilot
transmissions to prevent a BS from performing non-consensual UE localization.
The methodology allows the UE to spoof its position, keeping the precoder
unchanged. We formulate spoofing as a unified rank-constrained projection
problem, and provide closed-form solutions under varying levels of channel
state information (CSI) at the UE, including scenarios with and without CSI
knowledge. Simulation results confirm that the proposed approach enables the UE
to deceive the BS, inducing significant localization errors, while the impact
on link capacity varies depending on the spoofed position. Our findings
establish HoloTrace as a practical and robust privacy-preserving solution for
future 6G networks.

</details>


### [8] [Theoretical framework of passive ME antenna arrays enabling in-vivo monitoring: A pathway to smart implants](https://arxiv.org/abs/2509.23520)
*Kalpesh Jaykar,Prasanth Velvaluri,Nian X. Sun,Richard D. James*

Main category: eess.SP

TL;DR: 提出了一种基于磁电天线阵列的血管内脑机接口技术，通过天线阵列的相位同步和空间优化，解决了单天线增益不足的问题，实现了高增益信号传输。


<details>
  <summary>Details</summary>
Motivation: 传统Stentrode脑机接口设备存在电池和电子线路可能导致的损伤或故障风险，而磁电天线具有尺寸小、抗干扰等优势，但单天线增益有限。

Method: 采用磁电天线阵列设计，通过数学建模优化天线空间排列和相位同步，在指定远场点产生相长干涉来增强信号传输能力。

Result: 基于模型的仿真显示，通过相位操控在指定远场位置实现了有前景的高增益性能。

Conclusion: 磁电天线阵列技术为血管内植入物提供了有效的信号增强方案，有望改善脑机接口的通信性能。

Abstract: A new brain-computer interface (BCI) technology, deployed through minimally
invasive surgery, is changing the way we think about treating severe
neurological conditions. The central idea is to place a device called Stentrode
in the brain's vasculature, which enables neuromodulation and helps patients
regain the ability to communicate. However, in such devices, the battery and
electronics are wired and could introduce damage or implant malfunction. In
these cases, a Stentrode integrated with magnetoelectric (ME) antennas could be
of great interest. ME antennas offer significant advantages over traditional
antennas, leveraging acoustic resonance rather than electromagnetic resonance
to achieve a size reduction of up to five orders of magnitude. In addition to
their compactness and immunity to ground-plane interference, ME antennas could
be adopted for use in vascular implants, such as coronary stents, potentially
enabling minimally invasive monitoring and communication. Despite these
advantages, a single antenna embedded in the implant may be constrained by the
limited volume of magnetostrictive material, which could result in low output
gain. To address this gain limitation, we propose using antenna arrays designed
to produce constructive interference at a designated far-field point, ideally
located outside the patient, to enhance signal transmission and receiving
capabilities. We develop a mathematical model to represent the antennas and
optimize their spatial arrangement and phase synchronization. Simulations based
on this model demonstrate promising high-gain performance at the prescribed
far-field location through phase manipulation.

</details>


### [9] [Learnable Kernels for FRI -- Joint Kernel Encoder Optimization and Hardware Validation](https://arxiv.org/abs/2509.23644)
*Omkar Nitsure,Sampath Kumar Dondapati,Satish Mulleti*

Main category: eess.SP

TL;DR: 提出了一种基于可学习核的FRI重构框架，通过联合优化采样核和重构编码器，实现了比传统方法更高的分辨率和噪声鲁棒性，并提供了实用的硬件实现方案。


<details>
  <summary>Details</summary>
Motivation: 传统FRI重构方法严重依赖预定义核，限制了硬件实现和在噪声条件下的重构精度。

Method: 使用可截断高斯和高斯对核进行有效重构，然后通过统一学习方法联合优化采样核和重构编码器，提出将核表示为两个指数衰减信号之和的硬件实现方案。

Result: 实验验证通过Sallen-Key模拟滤波器实现硬件实现，实现了准确的实际信号恢复，基于CNN的编码器显著降低了计算复杂度。

Conclusion: 该方法在分辨率和噪声鲁棒性方面显著优于传统方法，采样率更低，特别适合资源受限的边缘部署场景。

Abstract: Finite Rate of Innovation (FRI) sampling techniques provide efficient
frameworks for reconstructing signals with inherent sparsity at rates below
Nyquist. However, traditional FRI reconstruction methods rely heavily on
pre-defined kernels, often limiting hardware implementation and reconstruction
accuracy under noisy conditions. In this paper, we propose a robust, flexible,
and practically implementable framework for FRI reconstruction by introducing
novel learnable kernel strategies. First, we demonstrate effective
reconstruction using known, fixed kernels such as truncated Gaussian and
Gaussian pair kernels, which mitigate the requirement that the samples should
have a sum-of-exponentials (SoE) form. Next, we extend this concept by jointly
optimizing both the sampling kernel and reconstruction encoder through a
unified learning approach, yielding adaptive kernels that significantly
outperform traditional methods in resolution and noise robustness, with reduced
sampling rates. Furthermore, we propose a practical hardware realization by
representing kernels as sums of two exponential decay signals with jointly
optimized poles, facilitating compact, efficient analog implementations. Our
approach is validated experimentally through hardware implementations using a
unity-gain Sallen-Key analog filter, achieving accurate real-world signal
recovery. The developed convolutional neural network-based encoder
substantially reduces computational complexity, demonstrating competitive
performance with fewer parameters, making our method particularly suitable for
resource-constrained, edge-based deployments.

</details>


### [10] [Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks](https://arxiv.org/abs/2509.23687)
*Runze Dong,Buhong Wang,Cunqian Feng,Jiang Weng,Chen Han,Jiwei Tian*

Main category: eess.SP

TL;DR: 提出了一种用于多无人机网络的安全且频谱高效的集成感知与通信框架，采用两阶段优化方法联合设计混合波束成形、人工噪声注入和无人机轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要将无人机视为空中基站，忽视了其作为ISAC用户的角色，且未能利用地面基站的大规模天线阵列来增强安全性和频谱效率。

Method: 采用两阶段优化方法：第一阶段使用近端策略优化算法优化数字波束成形器和轨迹；第二阶段通过低复杂度矩阵分解将数字解分解为模拟和数字分量。

Result: 仿真结果表明，所提出的框架相比基准方案具有更好的性能。

Conclusion: 该框架为多无人机网络提供了安全且频谱高效的ISAC解决方案，能够有效提升系统性能。

Abstract: Integrated sensing and communication (ISAC) emerges as a key enabler for
next-generation applications such as smart cities and autonomous systems. Its
integration with unmanned aerial vehicles (UAVs) unlocks new potentials for
reliable communication and precise sensing in dynamic aerial environments.
However, existing research predominantly treats UAVs as aerial base stations,
overlooking their role as ISAC users, and fails to leverage large-scale antenna
arrays at terrestrial base stations to enhance security and spectral
efficiency. This paper propose a secure and spectral efficient ISAC framework
for multi-UAV networks, and a two-stage optimization approach is developed to
jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and
UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage
employs Proximal Policy Optimization (PPO) to optimize digital beamformers and
trajectories, and the second stage decomposes the digital solution into analog
and digital components via low-complexity matrix factorization. Simulation
results demonstrate the effectiveness of the proposed framework compared to
benchmark schemes.

</details>


### [11] [Expectation Propagation-Based Signal Detection for Highly Correlated MIMO Systems](https://arxiv.org/abs/2509.23792)
*Kabuto Arai,Takumi Yoshida,Takumi Takahashi,Koji Ishibashi*

Main category: eess.SP

TL;DR: 提出了一种基于期望传播的检测器OvEP，通过重叠块分区处理大规模MIMO系统中的高度相关信道矩阵，在降低计算复杂度的同时保持接近传统LMMSE-EP的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统在密集阵列部署和有限散射环境下会产生高度相关且病态的信道矩阵，这会严重降低基于消息传递的检测器性能。

Method: 将大规模测量向量划分为部分重叠的块，为每个块及其重叠部分设计低复杂度LMMSE滤波器，通过减去重叠部分输出来减轻块间相关性影响。

Result: 该算法在保持接近传统LMMSE-EP性能的同时显著降低了计算复杂度，理论分析表明其固定点与松弛KL最小化问题的驻点一致。

Conclusion: OvEP算法有效解决了大规模MIMO系统中信道相关性问题，提供了一种计算效率高且性能优越的检测方案。

Abstract: Large-scale multiple-input-multiple-output (MIMO) systems typically operate
in dense array deployments with limited scattering environments, leading to
highly correlated and ill-conditioned channel matrices that severely degrade
the performance of message-passing-based detectors. To tackle this issue, this
paper proposes an expectation propagation (EP)-based detector, termed
overlapping block partitioning EP (OvEP). In OvEP, the large-scale measurement
vector is partitioned into partially overlapping blocks. For each block and its
overlapping part, a low-complexity linear minimum mean square error
(LMMSE)-based filter is designed according to the partitioned structure. The
resulting LMMSE outputs are then combined to generate the input to the
denoiser. In this combining process, subtracting the overlapping-part outputs
from the block outputs effectively mitigates the adverse effects of inter-block
correlation induced by high spatial correlation. The proposed algorithm is
consistently derived within the EP framework, and its fixed point is
theoretically proven to coincide with the stationary point of a relaxed
Kullback- Leibler (KL) minimization problem. The mechanisms underlying the
theoretically predicted performance improvement are further clarified through
numerical simulations. The proposed algorithm achieves performance close to
conventional LMMSE-EP with lower computational complexity.

</details>


### [12] [Online Specific Emitter Identification via Collision-Alleviated Signal Hash](https://arxiv.org/abs/2509.23807)
*Hongyu Wang,Wenjia Xu,Guangzuo Li,Siyuan Wan,Yaohua Sun,Jiuniu Wang,Mugen Peng*

Main category: eess.SP

TL;DR: 本文提出了在线特定发射器识别（OSEI）任务，解决传统SEI无法识别未见发射器的问题。作者开发了基于哈希的CASH模型，通过两阶段处理来同时识别已知和未知发射器，在真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中需要识别来自未见发射器的信号，而现有模型只能识别训练时见过的发射器，且容易偏向已知发射器分布。

Method: 提出CASH模型：1）已知发射器识别步骤，判断信号是否来自已知发射器；2）信号哈希编码步骤，为每个信号样本分配哈希码来识别具体发射器。

Result: 在ADSB和ORACLE真实信号数据集上，CASH在少样本学习任务中准确率比现有方法至少提高6.08%，在广义零样本学习任务中至少提高8.55%。

Conclusion: CASH模型能够有效在线识别来自已知和未知发射器的信号，解决了传统SEI模型的局限性，在真实场景中表现出色。

Abstract: Specific Emitter Identification (SEI) has been widely studied, aiming to
distinguish signals from different emitters given training samples from those
emitters. However, real-world scenarios often require identifying signals from
novel emitters previously unseen. Since these novel emitters only have a few or
no prior samples, existing models struggle to identify signals from novel
emitters online and tend to bias toward the distribution of seen emitters. To
address these challenges, we propose the Online Specific Emitter Identification
(OSEI) task, comprising both online \revise{few-shot and generalized zero-shot}
learning tasks. It requires constructing models using signal samples from seen
emitters and then identifying new samples from seen and novel emitters online
during inference. We propose a novel hash-based model, Collision-Alleviated
Signal Hash (CASH), providing a unified approach for addressing the OSEI task.
The CASH operates in two steps: in the seen emitters identifying step, a signal
encoder and a seen emitters identifier determine whether the signal sample is
from seen emitters, mitigating the model from biasing toward seen emitters
distribution. In the signal hash coding step, an online signal hasher assigns a
hash code to each signal sample, identifying its specific emitter. Experimental
results on real-world signal datasets (i.e., ADSB and ORACLE) demonstrate that
our method accurately identifies signals from both seen and novel emitters
online. This model outperforms existing methods by a minimum of 6.08\% and
8.55\% in accuracy for the few-shot and \revise{generalized zero-shot learning
}tasks, respectively. The code will be open-sourced at
\href{https://github.com/IntelliSensing/OSEI-CASH}{https://github.com/IntelliSensing/OSEI-CASH}.

</details>


### [13] [Asymptotic Expansion for Nonlinear Filtering in the Small System Noise Regime](https://arxiv.org/abs/2509.23920)
*Masahiro Kurisaki*

Main category: eess.SP

TL;DR: 提出基于系统噪声小参数的非线性滤波渐近展开方法，将条件期望展开为噪声水平的幂级数，通过求解常微分方程组计算系数，平衡计算效率与精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有高斯近似和粒子滤波等方法在计算效率与精度之间的权衡问题，以更低的计算成本捕捉条件分布的复杂特征。

Method: 基于系统噪声小参数进行渐近展开，将条件期望展开为噪声水平的幂级数，每个系数通过求解常微分方程组获得，并引入Edgeworth型展开。

Result: 该方法能够捕捉条件分布的多峰性等复杂特征，相比传统滤波算法显著降低计算成本。

Conclusion: 提出的渐近展开方法在非线性滤波中有效平衡了计算效率与精度，能够以较低计算成本处理复杂分布特征。

Abstract: We propose a new asymptotic expansion method for nonlinear filtering, based
on a small parameter in the system noise. The conditional expectation is
expanded as a power series in the noise level, with each coefficient computed
by solving a system of ordinary differential equations. This approach mitigates
the trade-off between computational efficiency and accuracy inherent in
existing methods such as Gaussian approximations and particle filters.
Moreover, by incorporating an Edgeworth-type expansion, our method captures
complex features of the conditional distribution, such as multimodality, with
significantly lower computational cost than conventional filtering algorithms.

</details>


### [14] [Wideband Integrated Sensing and Communications: Spectral Efficiency and Signaling Design](https://arxiv.org/abs/2509.24097)
*Henglin Pu,Zhu Han,Athina P. Petropulu,Husheng Li*

Main category: eess.SP

TL;DR: 该论文研究了6G网络中集成感知与通信(ISAC)的波形合成问题，提出了在OFDM系统中通过控制功率谱密度(PSD)来优化感知性能，同时保持相位用于通信调制的方法，并揭示了与OTFS波形的对偶关系。


<details>
  <summary>Details</summary>
Motivation: 6G网络中集成感知与通信的主要挑战在于如何在同一个波形中整合感知和通信两种不同功能，特别是在宽带环境下需要平衡两种性能。

Method: 采用OFDM信号，通过控制功率谱密度(PSD)来优化感知性能，同时保持子载波相位用于通信调制；提出了一种低复杂度的类注水分配器，并建立了与OTFS波形的对偶关系。

Result: 提出的宽带ISAC方案通过数值仿真和硬件实验验证了性能，PSD整形规则能够有效降低延迟-多普勒域中的积分旁瓣水平(ISL)。

Conclusion: 在宽带ISAC系统中，通过PSD整形可以同时优化感知和通信性能，提出的方法在OFDM和OTFS波形中都具有良好的效果，为6G网络中的集成感知与通信提供了有效的解决方案。

Abstract: In integrated sensing and communications (ISAC), a distinguishing feature of
6G wireless networks, the main challenge lies in integrating the two distinct
functions of sensing and communication within the same waveform. In this paper,
the ISAC waveform synthesis is studied in the wideband regime, since a large
bandwidth can simplify the analysis and is justified by the employment of
millimeter wave or higher frequency band. Standard orthogonal frequency
division multiplexing (OFDM) signaling is assumed, and the wideband analysis of
sensing is a counterpart of the existing studies on wideband communications. It
is proposed that the phase over such OFDM subcarriers is for modulating
communication messages while the power spectral density (PSD) is shaped for the
sensing performance. Beyond OFDM, we further reveal a duality between the
proposed PSD-shaping rule and the orthogonal time frequency space (OTFS)
waveform. Flattening the OTFS delay-axis PSD produces the same integrated
sidelobe level (ISL) reduction effect in the delay-Doppler domain as PSD
control achieves for OFDM in the frequency domain. To balance communication and
sensing performance over frequency-selective channels, we propose a
low-complexity, water-filling-like allocator with an explicit PSD-flatness
(variance) constraint. The performance of the proposed wideband ISAC scheme is
demonstrated using both numerical simulations and hardware experiments.

</details>


### [15] [BladderFormer: A Streaming Transformer for Real-Time Urological State Monitoring](https://arxiv.org/abs/2509.24178)
*Chengwei Zhou,Steve Majerus,Gourav Datta*

Main category: eess.SP

TL;DR: 提出一种单层流式Transformer模型，用于膀胱压力状态的实时分类，基于小波变换的时间序列数据，具有高效在线推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有膀胱压力监测系统依赖手工特征和浅层分类器，难以适应复杂信号动态变化。

Method: 使用单层流式Transformer模型，结合时间多头自注意力和状态缓存，在小波变换后的时间序列数据上进行实时分类。

Result: 在91名患者数据集上测试，每个患者有2万-8万个样本，模型在准确性、能效和延迟效率方面均有提升。

Conclusion: 该方法适用于边缘设备部署，在低功耗硬件上具有实际应用潜力。

Abstract: Bladder pressure monitoring systems are increasingly vital in diagnosing and
managing urinary tract dysfunction. Existing solutions rely heavily on
hand-crafted features and shallow classifiers, limiting their adaptability to
complex signal dynamics. We propose a one-layer streaming transformer model for
real-time classification of bladder pressure states, operating on
wavelet-transformed representations of raw time-series data. Our model
incorporates temporal multi-head self-attention and state caching, enabling
efficient online inference with high adaptability. Trained on a dataset of 91
patients with 20,000-80,000 samples each, our method demonstrates improved
accuracy, higher energy- and latency-efficiency. Implementation considerations
for edge deployment on low-power hardware, such as edge graphical processing
units (GPU) and micro-controllers, are also discussed.

</details>


### [16] [Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning](https://arxiv.org/abs/2509.24222)
*Zhisheng Chen,Yingwei Zhang,Qizhen Lan,Tianyu Liu,Huacan Wang,Yi Ding,Ziyu Jia,Ronghao Chen,Kun Wang,Xinliang Zhou*

Main category: eess.SP

TL;DR: Uni-NTFM是一个统一的神经拓扑基础模型，专门为脑电图(EEG)设计，通过解耦架构、拓扑嵌入机制和专家混合Transformer，解决了现有方法在时间-频率特征混淆、电极空间拓扑忽略和网络灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本或图像架构的大脑基础模型在处理EEG信号时存在三个主要问题：时间域波形模式与频率域节律特征的混淆、电极空间拓扑的忽略，以及使用不灵活的密集网络处理功能不同的EEG模式。

Method: Uni-NTFM采用三个核心创新：1) 解耦架构并行编码时间、频率和原始信号表示；2) 拓扑嵌入机制统一不同国际标准的电极；3) 专家混合神经Transformer通过路由信号模式到专门子网络来扩展模型容量。

Result: 最大的Uni-NTFM_large模型拥有19亿参数，在28,000小时多样化EEG数据上预训练。在9个不同下游任务中，在线性探测和微调设置下均显著优于现有任务特定方法和基础模型。

Conclusion: Uni-NTFM展示了学习大脑活动通用表示的卓越能力，为EEG信号处理提供了更有效的解决方案。

Abstract: Foundation models pretrained on various and unlabeled data have demonstrated
significant success in natural language and vision, but their application to
electroencephalography (EEG) remains challenged due to the signal's unique
properties. Existing brain foundation models that inherit architectures
designed for text or images lead to three limitations in pre-training: 1)
conflating time-domain waveform patterns with frequency-domain rhythmic
features in a single processing stream, 2) ignoring the critical spatial
topology of electrodes with different standards, and 3) reliance on the
inflexible, dense network to process functionally distinct EEG patterns. To
address these challenges, we introduce the Unified Neural Topological
Foundation Model (Uni-NTFM), which is designed based on neuroscience principles
to produce universal and interpretable representations. Uni-NTFM integrates
three core innovations: 1) a decoupled architecture parallelly encodes time,
frequency, and raw signal representations before performing cross-domain
feature integration; 2) a topological embedding mechanism to unify electrodes
from different international standards and generate structured input sequences
for brain regions; and 3) a Mixture-of-Experts neural Transformer that
efficiently scales model capacity by routing signal patterns to specialized
subnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B
parameters and was pretrained on over 28,000 hours of diverse EEG data via a
dual-domain masked reconstruction objective. Uni-NTFM significantly outperforms
existing task-specific methods and foundation models across nine distinct
downstream tasks under both linear probing and fine-tuning settings,
demonstrating a superior ability to learn universal representations of brain
activity.

</details>


### [17] [N78 Frequency Band Modular RIS Design and Implementation](https://arxiv.org/abs/2509.24355)
*Sefa Kayraklık,Recep Baş,Hasan Oğuzhan Çalışkan,Samed Şahinoğlu,Sercan Erdoğan,İlhami Ünal,İbrahim Hökelek,Kıvanç Nurdan,Ali Görçin*

Main category: eess.SP

TL;DR: 本文提出了一种模块化可重构智能表面(RIS)原型设计流程，该原型工作在n78频段，包含主从模块结构，每个模块有8×8反射单元，通过PIN二极管控制180°相位差，实验显示在n78频段接收信号功率提升超过15dB。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面(RIS)能够动态控制无线传播特性，是增强下一代无线网络信号覆盖和终端用户连接性的有前景技术。

Method: 采用模块化设计，包含一个主模块和最多16个从模块，每个模块具有8×8反射表面单元和控制器板，通过PIN二极管控制每个单元产生180°相位差。

Result: 使用两个RIS模块、喇叭天线和矢量网络分析仪进行的测量实验显示，在n78频段特定放置条件下，接收信号功率提升超过15dB。

Conclusion: 该模块化RIS原型设计在n78频段成功实现了显著的信号功率增强，验证了RIS技术在改善无线连接性能方面的有效性。

Abstract: Reconfigurable intelligent surface (RIS), capable of dynamically controlling
wireless propagation characteristics using reflecting antenna elements, is a
promising technology for enhancing signal coverage and improving end-user
connectivity in next-generation wireless networks. This paper presents a
complete design flow of a modular RIS prototype operating at the n78 frequency
band, starting from the simulations to the prototype development and testing.
An RIS prototype includes one master and up to sixteen slave blocks, each of
which has an identical hardware structure with $8\times 8$ reflecting surface
elements and a controller board. The phase shift response of each unit element
is controlled with a PIN diode to form a $180^\circ$ phase difference between
the ON and OFF states. The measurement experiment using two RIS blocks, horn
antennas, and a vector network analyzer showed that the improvement of the
received signal power is more than $15$ dB across the n78 frequency band for a
given placement.

</details>


### [18] [Strong Basin of Attraction for Unmixing Kernels With the Variable Projection Method](https://arxiv.org/abs/2509.24428)
*Santos Michelena,Maxime Ferreira Da Costa,José Picheral*

Main category: eess.SP

TL;DR: 研究在已知尖峰位置的情况下，从参数流形上不同点扩散函数卷积的尖峰信号混合中恢复信号的问题。提出了投影非线性最小二乘估计器，建立了强凸性区域半径的下界，并在真实LIBS数据上验证了方法的实用性。


<details>
  <summary>Details</summary>
Motivation: 解决在参数流形上不同点扩散函数卷积的尖峰信号混合恢复问题，消除手动校准需求，提高实际应用的可行性。

Method: 使用投影非线性最小二乘估计器，分析流形相干性和Lipschitz性质对优化程序收敛性和稳定性的影响。

Result: 建立了噪声存在下强凸性区域半径的下界，数值实验验证了PSF类在问题条件数中的快速衰减，并在真实LIBS数据上成功应用。

Conclusion: 提出的估计器在理论和实践中都表现出良好性能，能够有效解决PSF解混问题，具有实际应用价值。

Abstract: The problem of recovering a mixture of spike signals convolved with distinct
point spread functions (PSFs) lying on a parametric manifold, under the
assumption that the spike locations are known, is studied. The PSF unmixing
problem is formulated as a projected non-linear least squares estimator. A
lower bound on the radius of the region of strong convexity is established in
the presence of noise as a function of the manifold coherence and Lipschitz
properties, guaranteeing convergence and stability of the optimization program.
Numerical experiments highlight the speed of decay of the PSF class in the
problem's conditioning and confirm theoretical findings. Finally, the proposed
estimator is deployed on real-world spectroscopic data from laser-induced
breakdown spectroscopy (LIBS), removing the need for manual calibration and
validating the method's practical relevance.

</details>


### [19] [Low-Complexity Wireless Multi-Port Sensing by Multiplexed De-Embedding of an Over-the-Air Fixture](https://arxiv.org/abs/2509.24537)
*Philipp del Hougne*

Main category: eess.SP

TL;DR: 提出了一种无线多端口传感方法，通过使用可调负载网络提供测量多样性，解决了在可接入天线测量数量不足时无法解嵌入OTA夹具的问题。


<details>
  <summary>Details</summary>
Motivation: 解决无线多端口传感中当可接入天线的独立测量数量不足时，无法解嵌入OTA夹具以恢复DUT特性的问题。

Method: 使用可调负载网络创建可编程夹具，通过多个夹具实现提供测量多样性，实现多路解嵌入。

Result: 实验成功通过30个不同夹具实现远程估计了一个互易非酉4端口DUT的散射矩阵，仅使用两个可接入天线间的单一传输系数测量。

Conclusion: 多路解嵌入方法为在RFID和无线生物电子学等领域实现低硬件复杂度的无线多端口传感铺平了道路。

Abstract: Wireless multi-port sensing remotely retrieves the scattering matrix of a
multi-port device under test (DUT) connected to a set of
not-directly-accessible (NDA) antennas that couple over-the-air (OTA) to a set
of accessible antennas. If (i) the OTA fixture characteristics are known, and
(ii) the number of independent measurements at the accessible antennas is
sufficient, the OTA fixture can be de-embedded to recover the DUT
characteristics. In recent prior work, we solved (i) by connecting the NDA
antennas to a specific known tunable load network (TLN). Here, we tackle (ii)
by additionally using the TLN to provide measurement diversity. The connection
between OTA fixture and TLN constitutes a programmable fixture (PF). When the
DUT characteristics cannot be identified based on a single PF realization, we
add measurement diversity with multiple PF realizations. The underlying
"multiplexed de-embedding" achieves the joint de-embedding of an ensemble of PF
realizations when a single PF realization cannot be de-embedded. We
experimentally demonstrate our concept by remotely estimating the scattering
matrix of a reciprocal, non-unitary 4-port DUT (10 complex-valued unknowns) via
a rich-scattering OTA fixture purely based on measurements of a single
transmission coefficient between two accessible antennas across 30 different PF
realizations. We systematically study the trade-off between the number of
independent measurements at the accessible antennas and the number of PF
realizations. Multiplexed de-embedding of the OTA fixture paves the path to
implementing wireless multi-port sensing with low hardware complexity in areas
like RFID and wireless bioelectronics.

</details>


### [20] [BARProp: Fast-Converging and Memory-Efficient RSS-Based Localization Algorithm for IoT](https://arxiv.org/abs/2509.24588)
*Luis F. Abanto-Leon,Muhammad Salman,Lismer Andres Caceres-Najarro*

Main category: eess.SP

TL;DR: 提出了BARProp算法，一种快速且内存高效的RSS室内定位方法，通过动态调整衰减因子来加速收敛并提高稳定性，内存使用减少85%，收敛速度提升4倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有RSS定位方法依赖复杂算法或专用硬件，不适用于低成本接入点，需要开发更高效实用的解决方案。

Method: 引入buffer-aided RMSProp算法，通过监控缓冲区中梯度能量变化来动态调整衰减因子，实现快速收敛和稳定性。

Result: 真实数据评估显示，BARProp不仅定位精度更高，而且收敛速度比现有基准方法快至少4倍，内存使用减少85%。

Conclusion: BARProp为RSS室内定位提供了一种高效实用的解决方案，在精度、速度和内存效率方面均优于现有方法。

Abstract: Leveraging received signal strength (RSS) measurements for indoor
localization is highly attractive due to their inherent availability in
ubiquitous wireless protocols. However, prevailing RSS-based methods often
depend on complex computational algorithms or specialized hardware, rendering
them impractical for low-cost access points. To address these challenges, this
paper introduces buffer-aided RMSProp (BARProp), a fast and memory-efficient
localization algorithm specifically designed for RSS-based tasks. The key
innovation of BARProp lies in a novel mechanism that dynamically adapts the
decay factor by monitoring the energy variations of recent gradients stored in
a buffer, thereby achieving both accelerated convergence and enhanced
stability. Furthermore, BARProp requires less than 15% of the memory used by
state-of-the-art methods. Extensive evaluations with real-world data
demonstrate that BARProp not only achieves higher localization accuracy but
also delivers at least a fourfold improvement in convergence speed compared to
existing benchmarks.

</details>


### [21] [Impedance Modeling of Magnetometers: A Path Toward Low-Noise Readout Circuits](https://arxiv.org/abs/2509.24683)
*Johan Arbustini,Eric Elzenheimer,Elizaveta Spetzler,Pablo Mendoza,Daniel Fernández,Jordi Madrenas,Jeffrey McCord,Michael Höft,Robert Rieger,Andreas Bahr*

Main category: eess.SP

TL;DR: 提出了一种新型双端口阻抗模型来估计逆磁电传感器行为，通过实验测量S参数并转换为Z参数创建传递函数，用于系统级仿真和噪声优化。


<details>
  <summary>Details</summary>
Motivation: 优化开环和闭环实现的传感器读出方案和集成电路设计需要精确的建模和仿真策略。

Method: 使用阻抗分析仪测量S参数并转换为Z参数，创建传递函数用于MATLAB和LTSpice仿真，通过噪声分析验证模型。

Result: 仿真结果与实验测量一致，能够优化模拟电路部分的噪声性能。

Conclusion: 该方法为磁强计提供了噪声考虑和传递函数，对于混合信号电路设计的读出方案至关重要，可扩展到其他传感器接口电子设备。

Abstract: Optimizing sensor readout schemes and integrated circuit designs for both
open-loop and closed-loop implementations requires precise modeling and
simulation strategies. This study introduces a novel two-port impedance model
to estimate the behavior of a converse Magnetoelectric (cME) sensor. This model
provides a possible framework for calculating transfer functions and simulating
magnetometer behavior in both continuous- and discrete-time simulation
environments, and it is also possibly transferable to other magnetometer types.
Common S-parameters were measured experimentally using an impedance analyzer
and converted to Z-parameters to create a transfer function for system-level
simulations. The model was validated through an analysis of output-related
noise using MATLAB and LTSpice simulations to optimize the noise of the analog
circuit parts of the system. The simulation results were compared with
experimental measurements using a Zurich Instruments lock-in amplifier and the
custom-designed low-noise printed circuit board (PCB) under model
considerations. The proposed methodology derives noise considerations and the
transfer function of a magnetometer. These are essential for readout schemes
for mixed-signal circuit design. This allows low-noise electronics to be
designed and extended to other sensor interface electronics, broadening their
applicability in high-performance magnetic sensing.

</details>


### [22] [RDD: Pareto Analysis of the Rate-Distortion-Distinguishability Trade-off](https://arxiv.org/abs/2509.24805)
*Andriy Enttsel,Alex Marchioni,Andrea Zanellini,Mauro Mangia,Gianluca Setti,Riccardo Rovatti*

Main category: eess.SP

TL;DR: 本文扩展了信息论框架，研究压缩监控数据时在压缩效率、失真度和异常检测区分度之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 监控系统产生的压缩数据在云端进行异常检测时，压缩可能导致信息损失，影响检测器区分正常与异常模式的能力。

Method: 基于高斯假设扩展信息论框架，通过Pareto曲面分析压缩效率、失真度和信号区分度之间的权衡关系。

Result: 研究表明，在Pareto曲面上进行权衡管理比单纯依赖最优率失真压缩能更好地平衡系统性能。

Conclusion: 同时考虑压缩效率、失真度和异常检测区分度的权衡策略优于仅关注最优压缩的方法。

Abstract: Extensive monitoring systems generate data that is usually compressed for
network transmission. This compressed data might then be processed in the cloud
for tasks such as anomaly detection. However, compression can potentially
impair the detector's ability to distinguish between regular and irregular
patterns due to information loss. Here we extend the information-theoretic
framework introduced in [1] to simultaneously address the trade-off between the
three features on which the effectiveness of the system depends: the
effectiveness of compression, the amount of distortion it introduces, and the
distinguishability between compressed normal signals and compressed anomalous
signals. We leverage a Gaussian assumption to draw curves showing how moving on
a Pareto surface helps administer such a trade-off better than simply relying
on optimal rate-distortion compression and hoping that compressed signals can
be distinguished from each other.

</details>


### [23] [Intelligent Optimization of Wireless Access Point Deployment for Communication-Based Train Control Systems Using Deep Reinforcement Learning](https://arxiv.org/abs/2509.24819)
*Kunyu Wu,Qiushi Zhao,Zihan Feng,Yunxi Mu,Hao Qin,Xinyu Zhang,Xingqi Zhang*

Main category: eess.SP

TL;DR: 提出了一种融合抛物波方程信道建模、条件生成对抗网络数据增强和决斗深度Q网络的深度强化学习框架，用于优化隧道中接入点的部署，提高无线覆盖质量并降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于经验的优化算法需要大量测量且效果不佳，而机器学习方法在复杂隧道环境中表现有限，需要一种更高效、准确的接入点部署优化方案。

Method: 使用抛物波方程生成部分接入点位置的高保真路径损耗分布，通过条件生成对抗网络扩展到所有候选位置，然后采用决斗深度Q网络进行接入点位置优化。

Result: 相比传统Hooke Jeeves优化器和标准DQN，该方法能提供更高的平均接收功率、更好的最差情况覆盖和更高的计算效率。

Conclusion: 该工作将高保真电磁仿真、生成建模和AI驱动优化相结合，为复杂隧道环境中的下一代CBTC系统提供了可扩展且数据高效的解决方案。

Abstract: Urban railway systems increasingly rely on communication based train control
(CBTC) systems, where optimal deployment of access points (APs) in tunnels is
critical for robust wireless coverage. Traditional methods, such as empirical
model-based optimization algorithms, are hindered by excessive measurement
requirements and suboptimal solutions, while machine learning (ML) approaches
often struggle with complex tunnel environments. This paper proposes a deep
reinforcement learning (DRL) driven framework that integrates parabolic wave
equation (PWE) channel modeling, conditional generative adversarial network
(cGAN) based data augmentation, and a dueling deep Q network (Dueling DQN) for
AP placement optimization. The PWE method generates high-fidelity path loss
distributions for a subset of AP positions, which are then expanded by the cGAN
to create high resolution path loss maps for all candidate positions,
significantly reducing simulation costs while maintaining physical accuracy. In
the DRL framework, the state space captures AP positions and coverage, the
action space defines AP adjustments, and the reward function encourages signal
improvement while penalizing deployment costs. The dueling DQN enhances
convergence speed and exploration exploitation balance, increasing the
likelihood of reaching optimal configurations. Comparative experiments show
that the proposed method outperforms a conventional Hooke Jeeves optimizer and
traditional DQN, delivering AP configurations with higher average received
power, better worst-case coverage, and improved computational efficiency. This
work integrates high-fidelity electromagnetic simulation, generative modeling,
and AI-driven optimization, offering a scalable and data-efficient solution for
next-generation CBTC systems in complex tunnel environments.

</details>


### [24] [Low-Complexity Receiver Design for Multicarrier CAPA-based Systems in Doubly-Dispersive Channels](https://arxiv.org/abs/2509.24941)
*Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Emil Björnson*

Main category: eess.SP

TL;DR: 提出了一种用于多载波连续孔径阵列系统的低复杂度接收机设计，基于高斯置信传播框架，在双弥散信道下显著提升性能并保持低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 针对多载波连续孔径阵列系统在双弥散信道下的检测问题，需要设计低复杂度且高性能的接收机方案。

Method: 采用基于高斯置信传播的框架，仅需元素级标量运算来检测传输符号，适用于OFDM、OTFS和AFDM等多种波形。

Result: 仿真结果显示，与传统离散天线阵列系统相比，在未编码误码率方面获得显著性能提升，同时保持极低计算复杂度。

Conclusion: 所提出的GaBP接收机设计为多载波连续孔径阵列系统提供了一种高效且低复杂度的解决方案，在双弥散信道下具有优越性能。

Abstract: We propose a novel low-complexity receiver design for multicarrier continuous
aperture array (CAPA) systems operating over doubly-dispersive (DD) channels.
The receiver leverages a Gaussian Belief Propagation (GaBP)-based framework
that hinges only on element-wise scalar operations for the detection of the
transmitted symbols. Simulation results for the orthogonal frequency division
multiplexing (OFDM), orthogonal time frequency space (OTFS), and affine
frequency division multiplexing (AFDM) waveforms demonstrate significant
performance improvements in terms of uncoded bit error rate (BER) compared to
conventional discrete antenna array systems, while maintaining very low
computational complexity.

</details>


### [25] [Benchmarking ECG Foundational Models: A Reality Check Across Clinical Tasks](https://arxiv.org/abs/2509.25095)
*M A Al-Masud,Juan Miguel Lopez Alcaraz,Nils Strodthoff*

Main category: eess.SP

TL;DR: 对8个ECG基础模型在26个临床任务上的系统性评估显示，在成人ECG解读领域，三个基础模型优于监督基线，但ECG-CPC在其他类别中表现突出，尽管模型规模小且计算资源消耗少。


<details>
  <summary>Details</summary>
Motivation: ECG机器学习应用分散，基础模型的泛化能力未得到充分理解，需要系统评估其在多样化ECG任务上的表现。

Method: 使用12个公共数据集（包含1,650个回归和分类目标），在微调和冻结设置下评估8个ECG基础模型，并进行数据集规模的缩放分析。

Result: 基础模型在不同领域表现不均：成人ECG解读中三个模型优于基线；ECG-CPC在其他类别中表现最佳；模型随数据集大小呈现不同的缩放行为。

Conclusion: 基础模型在成人ECG分析中表现良好，但在心脏结构、结果预测和患者特征方面仍有差距；ECG-CPC的小规模和高效性能表明ECG基础模型有进一步发展的潜力。

Abstract: The 12-lead electrocardiogram (ECG) is a long-standing diagnostic tool. Yet
machine learning for ECG interpretation remains fragmented, often limited to
narrow tasks or datasets. Foundation models promise broader adaptability, but
their generalization across diverse ECG tasks is not well understood. We
benchmarked eight ECG foundation models on 26 clinically relevant tasks using
12 public datasets comprising 1,650 regression and classification targets.
Models were evaluated under fine-tuning and frozen settings, with scaling
analyses across dataset sizes. Results show heterogeneous performance across
domains: in the most widely studied domain, adult ECG interpretation, three
foundation models consistently outperformed strong supervised baselines. In
contrast, ECG-CPC, a compact structured state-space model pretrained on HEEDB,
dominated other categories where most foundation models failed to surpass
supervised learning. Foundation models also displayed distinct scaling
behaviors with dataset size, which are critical for small-scale clinical
applications. Overall, while foundation models show promise for adult ECG
analysis, substantial gaps remain in cardiac structure, outcome prediction, and
patient characterization. Notably, ECG-CPC's strong performance despite being
orders of magnitude smaller and consuming minimal computational resources
highlights untapped opportunities for advancing ECG foundation models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [26] [PerformSinger: Multimodal Singing Voice Synthesis Leveraging Synchronized Lip Cues from Singing Performance Videos](https://arxiv.org/abs/2509.22718)
*Ke Gu,Zhicong Wu,Peng Bai,Sitong Qiao,Zhiqi Jiang,Junchen Lu,Xiaodong Shi,Xinyuan Qian*

Main category: eess.AS

TL;DR: 提出PerformSinger，一种多模态歌唱声音合成框架，利用视频中的嘴唇线索实现无需音素时长的高质量歌唱合成。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱声音合成模型过度依赖细粒度的音素级时长信息，限制了实际应用，且忽视了视觉信息在时长预测中的补充作用。

Method: 使用并行多分支多模态编码器、特征融合模块、时长和变分预测网络、梅尔频谱解码器和声码器。融合模块采用渐进式融合策略在语义对齐空间中生成高质量多模态特征表示。

Result: 在主观和客观评估中均表现出最先进的性能。

Conclusion: PerformSinger通过整合视觉信息实现了高质量的"无时长"歌唱声音合成，并发布了新的数据集以促进相关研究。

Abstract: Existing singing voice synthesis (SVS) models largely rely on fine-grained,
phoneme-level durations, which limits their practical application. These
methods overlook the complementary role of visual information in duration
prediction.To address these issues, we propose PerformSinger, a pioneering
multimodal SVS framework, which incorporates lip cues from video as a visual
modality, enabling high-quality "duration-free" singing voice synthesis.
PerformSinger comprises parallel multi-branch multimodal encoders, a feature
fusion module, a duration and variational prediction network, a mel-spectrogram
decoder and a vocoder. The fusion module, composed of adapter and fusion
blocks, employs a progressive fusion strategy within an aligned semantic space
to produce high-quality multimodal feature representations, thereby enabling
accurate duration prediction and high-fidelity audio synthesis. To facilitate
the research, we design, collect and annotate a novel SVS dataset involving
synchronized video streams and precise phoneme-level manual annotations.
Extensive experiments demonstrate the state-of-the-art performance of our
proposal in both subjective and objective evaluations. The code and dataset
will be publicly available.

</details>


### [27] [Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation](https://arxiv.org/abs/2509.22740)
*Jinbae Seo,Hyeongjun Kwon,Kwonyoung Kim,Jiyoung Lee,Kwanghoon Sohn*

Main category: eess.AS

TL;DR: 提出音频中心查询生成和声音感知序数计数损失，解决视听实例分割中的视觉偏见问题，显著提升分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在视觉偏见问题：统一加法融合阻止查询专门化到不同声源，而仅视觉训练目标允许查询收敛到任意显著物体

Method: 音频中心查询生成使用交叉注意力使每个查询选择性关注不同声源，并引入声音感知序数计数损失通过序数回归显式监督发声物体数量

Result: 在AVISeg基准测试上取得一致改进：+1.64 mAP、+0.6 HOTA和+2.06 FSLA

Conclusion: 查询专门化和显式计数监督对于准确的视听实例分割至关重要

Abstract: Audiovisual instance segmentation (AVIS) requires accurately localizing and
tracking sounding objects throughout video sequences. Existing methods suffer
from visual bias stemming from two fundamental issues: uniform additive fusion
prevents queries from specializing to different sound sources, while
visual-only training objectives allow queries to converge to arbitrary salient
objects. We propose Audio-Centric Query Generation using cross-attention,
enabling each query to selectively attend to distinct sound sources and carry
sound-specific priors into visual decoding. Additionally, we introduce
Sound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding
object numbers through ordinal regression with monotonic consistency
constraints, preventing visual-only convergence during training. Experiments on
AVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and
+2.06 FSLA, validating that query specialization and explicit counting
supervision are crucial for accurate audiovisual instance segmentation.

</details>


### [28] [Index-MSR: A high-efficiency multimodal fusion framework for speech recognition](https://arxiv.org/abs/2509.22744)
*Jinming Chen,Lu Wang,Zheshu Song,Wei Deng*

Main category: eess.AS

TL;DR: Index-MSR是一个高效的多模态语音识别框架，通过融合视频中的文本信息（如字幕和演示文稿）来提升语音识别准确率，特别是在专业术语和短语音场景下。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统虽然在准确性上有显著提升，但在处理领域特定术语和缺乏语义连贯性的短语音时性能仍然较差。

Method: 提出了一种新颖的多模态融合解码器（MFD），将视频中的文本相关信息有效整合到语音识别过程中。

Result: 在内部字幕数据集和公共AVSR数据集上的评估显示，Index-MSR达到了最先进的准确率，替换错误减少了20-50%。

Conclusion: 该方法能有效利用视频中的文本线索来提升语音识别准确率，在需要严格音频文本同步的应用中具有强大潜力。

Abstract: Driven by large scale datasets and LLM based architectures, automatic speech
recognition (ASR) systems have achieved remarkable improvements in accuracy.
However, challenges persist for domain-specific terminology, and short
utterances lacking semantic coherence, where recognition performance often
degrades significantly. In this work, we present Index-MSR, an efficient
multimodal speech recognition framework. At its core is a novel Multimodal
Fusion Decoder (MFD), which effectively incorporates text-related information
from videos (e.g., subtitles and presentation slides) into the speech
recognition. This cross-modal integration not only enhances overall ASR
accuracy but also yields substantial reductions in substitution errors.
Extensive evaluations on both an in-house subtitle dataset and a public AVSR
dataset demonstrate that Index-MSR achieves sota accuracy, with substitution
errors reduced by 20,50%. These results demonstrate that our approach
efficiently exploits text-related cues from video to improve speech recognition
accuracy, showing strong potential in applications requiring strict audio text
synchronization, such as audio translation.

</details>


### [29] [Unsupervised Speech Enhancement using Data-defined Priors](https://arxiv.org/abs/2509.22942)
*Dominik Klement,Matthew Maciejewski,Sanjeev Khudanpur,Jan Černocký,Lukáš Burget*

Main category: eess.AS

TL;DR: 提出了一种新颖的双分支编码器-解码器架构用于无监督语音增强，通过对抗训练在未配对的干净语音和噪声数据集上施加先验，性能与领先的无监督方法相当，并揭示了使用领域内干净语音数据会导致性能评估过于乐观的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数基于深度学习的语音增强方法需要配对的干净-噪声语音数据，但在真实条件下大规模收集此类数据不可行，导致训练和测试阶段存在差距。

Method: 采用双分支编码器-解码器架构，将输入分离为干净语音和残余噪声，使用对抗训练在未配对的干净语音和噪声数据集上施加先验。

Result: 实验结果显示该方法性能与领先的无监督语音增强方法相当，并发现使用领域内干净语音数据会导致性能评估过于乐观。

Conclusion: 提出的无监督语音增强方法有效，但需要谨慎选择干净语音数据，避免使用领域内数据导致评估偏差。

Abstract: The majority of deep learning-based speech enhancement methods require paired
clean-noisy speech data. Collecting such data at scale in real-world conditions
is infeasible, which has led the community to rely on synthetically generated
noisy speech. However, this introduces a gap between the training and testing
phases. In this work, we propose a novel dual-branch encoder-decoder
architecture for unsupervised speech enhancement that separates the input into
clean speech and residual noise. Adversarial training is employed to impose
priors on each branch, defined by unpaired datasets of clean speech and,
optionally, noise. Experimental results show that our method achieves
performance comparable to leading unsupervised speech enhancement approaches.
Furthermore, we demonstrate the critical impact of clean speech data selection
on enhancement performance. In particular, our findings reveal that performance
may appear overly optimistic when in-domain clean speech data are used for
prior definition -- a practice adopted in previous unsupervised speech
enhancement studies.

</details>


### [30] [BFA: Real-time Multilingual Text-to-speech Forced Alignment](https://arxiv.org/abs/2509.23147)
*Abdul Rehman,Jingyao Cai,Jian-Jun Zhang,Xiaosong Yang*

Main category: eess.AS

TL;DR: BFA是一个结合无上下文通用音素编码器和CTC解码器的语音对齐系统，通过显式建模音素间间隙和静音，实现细粒度边界预测，速度比MFA快240倍。


<details>
  <summary>Details</summary>
Motivation: 解决传统语音对齐器速度慢的问题，为交互式语音应用提供快速、静音感知的对齐能力。

Method: 使用无上下文通用音素编码器(CUPE)和基于CTC的解码器，引入音素间间隙和静音的显式建模，采用分层解码策略。

Result: 在TIMIT和Buckeye语料库上评估，在宽松容差水平下与MFA相比具有竞争力的召回率，能预测起始和结束边界，处理速度比MFA快240倍。

Conclusion: BFA的速度和静音感知对齐能力为之前受限于慢速对齐器的交互式语音应用开辟了新机会。

Abstract: We present Bournemouth Forced Aligner (BFA), a system that combines a
Contextless Universal Phoneme Encoder (CUPE) with a connectionist temporal
classification (CTC)based decoder. BFA introduces explicit modelling of
inter-phoneme gaps and silences and hierarchical decoding strategies, enabling
fine-grained boundary prediction. Evaluations on TIMIT and Buckeye corpora show
that BFA achieves competitive recall relative to Montreal Forced Aligner at
relaxed tolerance levels, while predicting both onset and offset boundaries for
richer temporal structure. BFA processes speech up to 240x faster than MFA,
enabling faster than real-time alignment. This combination of speed and
silence-aware alignment opens opportunities for interactive speech applications
previously constrained by slow aligners.

</details>


### [31] [AI-Assisted Music Production: A User Study on Text-to-Music Models](https://arxiv.org/abs/2509.23364)
*Francesca Ronchini,Luca Comanducci,Simone Marcucci,Fabio Antonacci*

Main category: eess.AS

TL;DR: 本文通过用户研究探讨文本到音乐模型对音乐制作人创作流程的影响，发现TTM模型具有变革潜力但也面临现实集成挑战。


<details>
  <summary>Details</summary>
Motivation: 文本到音乐模型虽然革新了创作领域，但其在音乐人工作流程中的整合仍未被充分探索，需要研究这些模型如何影响音乐制作过程。

Method: 采用案例研究方法，让参与者使用结合TTM和源分离模型的自定义工具制作曲目，通过半结构化访谈和主题分析收集数据。

Result: 研究揭示了TTM模型在音乐制作中的关键挑战、机遇和伦理考量，展示了其变革潜力。

Conclusion: TTM模型在音乐制作中具有重要变革潜力，但在现实世界集成中面临挑战，需要进一步解决这些集成问题。

Abstract: Text-to-music models have revolutionized the creative landscape, offering new
possibilities for music creation. Yet their integration into musicians
workflows remains underexplored. This paper presents a case study on how TTM
models impact music production, based on a user study of their effect on
producers creative workflows. Participants produce tracks using a custom tool
combining TTM and source separation models. Semi-structured interviews and
thematic analysis reveal key challenges, opportunities, and ethical
considerations. The findings offer insights into the transformative potential
of TTMs in music production, as well as challenges in their real-world
integration.

</details>


### [32] [AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification](https://arxiv.org/abs/2509.23454)
*Md. Saiful Bari Siddiqui,Utsab Saha*

Main category: eess.AS

TL;DR: AudioFuse融合了频谱图和原始波形两种互补表示来分类心音图，通过宽浅的ViT处理频谱图和浅层1D CNN处理波形，在PhysioNet 2016数据集上达到0.8608 ROC-AUC，并在PASCAL数据集上表现出更强的域适应性。


<details>
  <summary>Details</summary>
Motivation: 生物医学音频信号（如心音图）具有节律性，包含频谱和时域的诊断信息。标准2D频谱图提供丰富的频谱特征但损失了相位信息和时间精度，需要同时利用两种互补表示。

Method: 提出AudioFuse架构，集成自定义的宽浅Vision Transformer处理频谱图和浅层1D CNN处理原始波形，以减轻融合模型的过拟合风险。

Result: 在PhysioNet 2016数据集上达到0.8608 ROC-AUC，优于单独的频谱图（0.8066）和波形（0.8223）基线。在PASCAL数据集上保持0.7181 ROC-AUC，而频谱图基线崩溃至0.4873。

Conclusion: 融合互补表示提供了强归纳偏置，能够创建高效、可泛化的分类器，无需大规模预训练。

Abstract: Biomedical audio signals, such as phonocardiograms (PCG), are inherently
rhythmic and contain diagnostic information in both their spectral (tonal) and
temporal domains. Standard 2D spectrograms provide rich spectral features but
compromise the phase information and temporal precision of the 1D waveform. We
propose AudioFuse, an architecture that simultaneously learns from both
complementary representations to classify PCGs. To mitigate the overfitting
risk common in fusion models, we integrate a custom, wide-and-shallow Vision
Transformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On
the PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive
ROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram
(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior
robustness to domain shift on the challenging PASCAL dataset, maintaining an
ROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing
complementary representations thus provides a strong inductive bias, enabling
the creation of efficient, generalizable classifiers without requiring
large-scale pre-training.

</details>


### [33] [LORT: Locally Refined Convolution and Taylor Transformer for Monaural Speech Enhancement](https://arxiv.org/abs/2509.23832)
*Junyu Wang,Zizhen Lin,Tianrui Wang,Meng Ge,Longbiao Wang,Jianwu Dang*

Main category: eess.AS

TL;DR: 提出LORT架构，结合空间通道增强的泰勒变换器和局部细化卷积，在仅0.96M参数下实现高效鲁棒的语音增强，性能优于或媲美SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 在保持低参数数量和计算复杂度的同时实现优越的语音增强性能仍然是一个挑战。

Method: 提出泰勒多头自注意力模块（T-MSA）和空间通道增强注意力（SCEA），以及局部细化卷积块（LRC）。采用U-Net编码器-解码器结构，通过多分辨率T-MSA模块处理噪声输入，独立解码增强的幅度和相位谱，并使用复合损失函数优化。

Result: 在VCTK+DEMAND和DNS Challenge数据集上的实验结果表明，LORT仅用0.96M参数就达到了与最先进模型竞争或更优的性能。

Conclusion: LORT在有限计算资源下对实际语音增强应用具有高效性，展示了其有效性。

Abstract: Achieving superior enhancement performance while maintaining a low parameter
count and computational complexity remains a challenge in the field of speech
enhancement. In this paper, we introduce LORT, a novel architecture that
integrates spatial-channel enhanced Taylor Transformer and locally refined
convolution for efficient and robust speech enhancement. We propose a Taylor
multi-head self-attention (T-MSA) module enhanced with spatial-channel
enhancement attention (SCEA), designed to facilitate inter-channel information
exchange and alleviate the spatial attention limitations inherent in
Taylor-based Transformers. To complement global modeling, we further present a
locally refined convolution (LRC) block that integrates convolutional
feed-forward layers, time-frequency dense local convolutions, and gated units
to capture fine-grained local details. Built upon a U-Net-like encoder-decoder
structure with only 16 output channels in the encoder, LORT processes noisy
inputs through multi-resolution T-MSA modules using alternating downsampling
and upsampling operations. The enhanced magnitude and phase spectra are decoded
independently and optimized through a composite loss function that jointly
considers magnitude, complex, phase, discriminator, and consistency objectives.
Experimental results on the VCTK+DEMAND and DNS Challenge datasets demonstrate
that LORT achieves competitive or superior performance to state-of-the-art
(SOTA) models with only 0.96M parameters, highlighting its effectiveness for
real-world speech enhancement applications with limited computational
resources.

</details>


### [34] [AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset with Speech Recognition Baselines](https://arxiv.org/abs/2509.23833)
*Cancan Li,Fei Su,Juan Liu,Hui Bu,Yulong Wan,Hongbin Suo,Ming Li*

Main category: eess.AS

TL;DR: 提出了AISHELL6-Whisper数据集和基于Whisper-Flamingo框架的视听语音识别基线模型，用于中文普通话耳语语音识别，在测试集上耳语语音字符错误率为4.13%，正常语音为1.11%。


<details>
  <summary>Details</summary>
Motivation: 中文普通话视听耳语语音识别因缺乏大规模数据集而发展受限，耳语语音识别在隐私保护、患者沟通和噪声敏感环境中具有重要应用价值。

Method: 构建包含30小时耳语语音和并行正常语音的大规模开源视听数据集AISHELL6-Whisper，并提出基于Whisper-Flamingo框架的AVSR基线模型，采用并行训练策略对齐不同语音类型的嵌入，使用投影层适应耳语语音的频谱特性。

Result: 模型在数据集测试集上耳语语音字符错误率为4.13%，正常语音为1.11%，在wTIMIT基准测试中创造了新的最先进结果。

Conclusion: AISHELL6-Whisper数据集和提出的AVSR基线模型有效推动了中文耳语语音识别的发展，相关资源已开源。

Abstract: Whisper speech recognition is crucial not only for ensuring privacy in
sensitive communications but also for providing a critical communication bridge
for patients under vocal restraint and enabling discrete interaction in
noise-sensitive environments. The development of Chinese mandarin audio-visual
whisper speech recognition is hindered by the lack of large-scale datasets. We
present AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech
dataset, featuring 30 hours each of whisper speech and parallel normal speech,
with synchronized frontal facial videos. Moreover, we propose an audio-visual
speech recognition (AVSR) baseline based on the Whisper-Flamingo framework,
which integrates a parallel training strategy to align embeddings across speech
types, and employs a projection layer to adapt to whisper speech's spectral
properties. The model achieves a Character Error Rate (CER) of 4.13% for
whisper speech and 1.11% for normal speech in the test set of our dataset, and
establishes new state-of-the-art results on the wTIMIT benchmark. The dataset
and the AVSR baseline codes are open-sourced at
https://zutm.github.io/AISHELL6-Whisper.

</details>


### [35] [Reasoning Beyond Majority Vote: An Explainable SpeechLM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2509.24187)
*Bo-Hao Su,Hui-Ying Shih,Jinchuan Tian,Jiatong Shi,Chi-Chun Lee,Carlos Busso,Shinji Watanabe*

Main category: eess.AS

TL;DR: 提出了一种可解释的语音语言模型框架，将语音情感识别重构为生成式推理任务，通过生成情感标签和基于词汇及声学线索的自然语言理由来增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别使用多数投票标签，掩盖了主观性，缺乏预测透明度，且忽略了有效的少数标注，限制了可解释性。

Method: 构建可解释的语音语言模型框架，首先生成转录文本，然后输出情感标签和基于词汇及声学线索的简洁自然语言理由。使用推理能力强的教师LLM生成理由作为中间监督，与多数标签结合进行微调。

Result: 在MSP-Podcast v1.12数据集上，模型保持了相对于零样本SpeechLM基线的改进，生成的理据被人类评估者认为是合理且基础牢固的。

Conclusion: 引入理据监督为可解释的语音情感识别提供了一条实用路径，且不牺牲预测质量。

Abstract: Speech Emotion Recognition (SER) is typically trained and evaluated on
majority-voted labels, which simplifies benchmarking but masks subjectivity and
provides little transparency into why predictions are made. This neglects valid
minority annotations and limits interpretability. We propose an explainable
Speech Language Model (SpeechLM) framework that frames SER as a generative
reasoning task. Given an utterance, the model first produces a transcript, then
outputs both an emotion label and a concise natural-language rationale grounded
in lexical and acoustic cues. Rationales are generated by a reasoning-capable
teacher LLM and used as intermediate supervision, combined with majority labels
during fine-tuning. Unlike prior work primarily focused on boosting
classification accuracy, we aim to enhance explainability while preserving
competitive performance. To this end, we complement majority-label metrics with
annotator-aware scoring that credits matches with any annotator label. On
MSP-Podcast v1.12, our model maintains improvements over zero-shot SpeechLM
baselines, and produces rationales that human evaluators find plausible and
well grounded. This demonstrates that incorporating rationale supervision
offers a practical path toward interpretable SER without sacrificing predictive
quality.

</details>


### [36] [SynthCloner: Synthesizer Preset Conversion via Factorized Codec with ADSR Envelope Control](https://arxiv.org/abs/2509.24286)
*Jeng-Yue Liu,Ting-Chao Hsu,Yen-Tung Yeh,Li Su,Yi-Hsuan Yang*

Main category: eess.AS

TL;DR: SynthCloner是一个分解式编解码器模型，能够将音频解耦为ADSR包络、音色和内容三个属性，实现合成器预设转换的独立属性控制。


<details>
  <summary>Details</summary>
Motivation: 现有音色转换方法依赖频谱目标或隐式风格匹配，对包络塑形控制有限，且公开合成器数据集缺乏对音色和ADSR包络的多样化覆盖。

Method: 提出SynthCloner分解式编解码器模型，将音频分离为ADSR包络、音色和内容三个属性；同时构建SynthCAT数据集，包含250种音色、120种ADSR包络和100个MIDI序列。

Result: 实验表明SynthCloner在客观和主观指标上均优于基线方法，并实现了独立的属性控制。

Conclusion: SynthCloner通过属性解耦实现了表达性合成器预设转换，为合成器声音控制提供了新的解决方案。

Abstract: Electronic synthesizer sounds are controlled by presets, parameters settings
that yield complex timbral characteristics and ADSR envelopes, making preset
conversion particularly challenging. Recent approaches to timbre transfer often
rely on spectral objectives or implicit style matching, offering limited
control over envelope shaping. Moreover, public synthesizer datasets rarely
provide diverse coverage of timbres and ADSR envelopes. To address these gaps,
we present SynthCloner, a factorized codec model that disentangles audio into
three attributes: ADSR envelope, timbre, and content. This separation enables
expressive synthesizer preset conversion with independent control over these
three attributes. Additionally, we introduce SynthCAT, a new synthesizer
dataset with a task-specific rendering pipeline covering 250 timbres, 120 ADSR
envelopes, and 100 MIDI sequences. Experiments show that SynthCloner
outperforms baselines on both objective and subjective metrics, while enabling
independent attribute control. The code, model checkpoint, and audio examples
are available at https://buffett0323.github.io/synthcloner/.

</details>


### [37] [Code-switching Speech Recognition Under the Lens: Model- and Data-Centric Perspectives](https://arxiv.org/abs/2509.24310)
*Hexin Liu,Haoyang Zhang,Qiquan Zhang,Xiangyu Zhang,Dongyuan Shi,Eng Siong Chng,Haizhou Li*

Main category: eess.AS

TL;DR: 本文系统分析了代码切换语音识别(CS-ASR)的挑战，从模型和数据两个角度比较了不同方法，并提出基于简化等价约束理论(SECT)的提示策略来生成有效的代码切换文本，通过TTS生成语音-文本对提升CS-ASR性能。


<details>
  <summary>Details</summary>
Motivation: 代码切换语音识别面临语言混淆、口音偏见和标注数据稀缺等挑战，尽管组成语言可能是高资源语言，但代码切换数据的稀缺性加剧了这些困难。

Method: 从模型角度比较了语言特定处理和辅助语言感知多任务学习等方法；从数据角度研究了TTS数据增强，并提出基于简化等价约束理论(SECT)的提示策略来指导大语言模型生成有效的代码切换文本。

Result: 提出的SECT方法在ASR性能和语言质量评估方面优于现有方法，生成的代码切换文本更接近真实世界的代码切换文本，通过TTS生成语音-文本对能有效提升CS-ASR性能。

Conclusion: 有效的CS-ASR需要策略与代码切换数据的特定语言特征仔细对齐，模型和数据方法的综合分析强调了这一点。

Abstract: Code-switching automatic speech recognition (CS-ASR) presents unique
challenges due to language confusion introduced by spontaneous intra-sentence
switching and accent bias that blurs the phonetic boundaries. Although the
constituent languages may be individually high-resource, the scarcity of
annotated code-switching data further compounds these challenges. In this
paper, we systematically analyze CS-ASR from both model-centric and
data-centric perspectives. By comparing state-of-the-art algorithmic methods,
including language-specific processing and auxiliary language-aware multi-task
learning, we discuss their varying effectiveness across datasets with different
linguistic characteristics. On the data side, we first investigate TTS as a
data augmentation method. By varying the textual characteristics and speaker
accents, we analyze the impact of language confusion and accent bias on CS-ASR.
To further mitigate data scarcity and enhance textual diversity, we propose a
prompting strategy by simplifying the equivalence constraint theory (SECT) to
guide large language models (LLMs) in generating linguistically valid
code-switching text. The proposed SECT outperforms existing methods in ASR
performance and linguistic quality assessments, generating code-switching text
that more closely resembles real-world code-switching text. When used to
generate speech-text pairs via TTS, SECT proves effective in improving CS-ASR
performance. Our analysis of both model- and data-centric methods underscores
that effective CS-ASR requires strategies to be carefully aligned with the
specific linguistic characteristics of the code-switching data.

</details>


### [38] [Unsupervised Single-Channel Speech Separation with a Diffusion Prior under Speaker-Embedding Guidance](https://arxiv.org/abs/2509.24395)
*Runwu Shi,Kai Li,Chang Li,Jiang Wang,Sihan Tan,Kazuhiro Nakadai*

Main category: eess.AS

TL;DR: 该论文提出了一种基于扩散模型的语音分离方法，通过说话人嵌入引导和分离导向求解器来解决无监督语音分离中的说话人一致性难题。


<details>
  <summary>Details</summary>
Motivation: 传统语音分离系统依赖合成数据，可能无法反映真实条件。扩散生成模型虽然能捕捉局部声学结构，但缺乏说话人级别的条件约束，导致分离结果中说话人身份在时间上不一致。

Method: 采用源模型范式，在无混响语音上训练扩散生成模型，将分离视为扩散逆问题。提出说话人嵌入引导策略，在反向扩散过程中保持每个分离轨道内的说话人一致性，同时拉远不同说话人嵌入的距离。还设计了专门针对语音分离的分离导向求解器。

Result: 实验结果表明，所提出的策略有效提升了无监督基于源模型的语音分离性能。

Conclusion: 说话人嵌入引导和分离导向求解器能够显著改善基于扩散模型的语音分离效果，特别是在保持说话人一致性方面表现优异。

Abstract: Speech separation is a fundamental task in audio processing, typically
addressed with fully supervised systems trained on paired mixtures. While
effective, such systems typically rely on synthetic data pipelines, which may
not reflect real-world conditions. Instead, we revisit the source-model
paradigm, training a diffusion generative model solely on anechoic speech and
formulating separation as a diffusion inverse problem. However, unconditional
diffusion models lack speaker-level conditioning, they can capture local
acoustic structure but produce temporally inconsistent speaker identities in
separated sources. To address this limitation, we propose Speaker-Embedding
guidance that, during the reverse diffusion process, maintains speaker
coherence within each separated track while driving embeddings of different
speakers further apart. In addition, we propose a new separation-oriented
solver tailored for speech separation, and both strategies effectively enhance
performance on the challenging task of unsupervised source-model-based speech
separation, as confirmed by extensive experimental results. Audio samples and
code are available at https://runwushi.github.io/UnSepDiff_demo.

</details>


### [39] [Assessing speech quality metrics for evaluation of neural audio codecs under clean speech conditions](https://arxiv.org/abs/2509.24457)
*Wolfgang Mack,Nezih Topaloglu,Laura Lechler,Ivana Balić,Alexandra Craciun,Mansur Yesilbursa,Kamil Wojcicki*

Main category: eess.AS

TL;DR: 评估了45种客观语音质量指标与主观听音分数的相关性，发现基于神经网络的指标scoreq和utmos与主观评分相关性最高，非侵入式指标在高质量水平下趋于饱和。


<details>
  <summary>Details</summary>
Motivation: 神经编解码器性能评估中，不清楚哪些客观指标能提供可靠的质量估计，需要系统评估不同指标与主观评分的一致性。

Method: 通过将45种客观指标得分与17种编解码条件下的主观听音分数进行相关性分析，评估指标性能。

Result: 基于神经网络的指标scoreq和utmos获得了最高的皮尔逊相关系数，非侵入式指标在高质量水平下表现饱和。

Conclusion: 神经网络指标在评估神经编解码器语音质量方面表现最佳，但非侵入式指标在高质量区域存在局限性。

Abstract: Objective speech-quality metrics are widely used to assess codec performance.
However, for neural codecs, it is often unclear which metrics provide reliable
quality estimates. To address this, we evaluated 45 objective metrics by
correlating their scores with subjective listening scores for clean speech
across 17 codec conditions. Neural-based metrics such as scoreq and utmos
achieved the highest Pearson correlations with subjective scores. Further
analysis across different subjective quality ranges revealed that non-intrusive
metrics tend to saturate at high subjective quality levels.

</details>


### [40] [ISSE: An Instruction-Guided Speech Style Editing Dataset And Benchmark](https://arxiv.org/abs/2509.24570)
*Yun Chen,Qi Chen,Zheqi Dai,Arshdeep Singh,Philip J. B. Jackson,Mark D. Plumbley*

Main category: eess.AS

TL;DR: 提出了ISSE数据集，包含400小时语音和10万对源-目标语音对，每对都有详细的文本编辑指令。构建了基于大语言模型、TTS和语音转换的数据生成流程，训练了指令引导的自回归语音模型，实现了准确可控的语音风格编辑。


<details>
  <summary>Details</summary>
Motivation: 现有语音风格编辑方法依赖显式标签或参考音频，限制了灵活性和可扩展性。使用自然语言描述的方法受限于过于简化的指令和粗糙的风格控制。

Method: 构建ISSE数据集，使用大语言模型、表达性文本转语音和语音转换技术构建高质量配对样本。训练指令引导的自回归语音模型。

Result: 实验结果表明，ISSE相比其他数据集能够实现更准确、可控和可泛化的语音风格编辑。

Conclusion: ISSE数据集解决了现有语音风格编辑方法的局限性，通过详细的文本指令实现了更灵活和精细的风格控制。

Abstract: Speech style editing refers to modifying the stylistic properties of speech
while preserving its linguistic content and speaker identity. However, most
existing approaches depend on explicit labels or reference audio, which limits
both flexibility and scalability. More recent attempts to use natural language
descriptions remain constrained by oversimplified instructions and coarse style
control. To address these limitations, we introduce an Instruction-guided
Speech Style Editing Dataset (ISSE). The dataset comprises nearly 400 hours of
speech and over 100,000 source-target pairs, each aligned with diverse and
detailed textual editing instructions. We also build a systematic instructed
speech data generation pipeline leveraging large language model, expressive
text-to-speech and voice conversion technologies to construct high-quality
paired samples. Furthermore, we train an instruction-guided autoregressive
speech model on ISSE and evaluate it in terms of instruction adherence, timbre
preservation, and content consistency. Experimental results demonstrate that
ISSE enables accurate, controllable, and generalizable speech style editing
compared to other datasets. The project page of ISSE is available at
https://ychenn1.github.io/ISSE/.

</details>


### [41] [Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis](https://arxiv.org/abs/2509.24629)
*Tianrui Wang,Haoyu Wang,Meng Ge,Cheng Gong,Chunyu Qiang,Ziyang Ma,Zikang Huang,Guanrou Yang,Xiaobao Wang,Eng Siong Chng,Xie Chen,Longbiao Wang,Jianwu Dang*

Main category: eess.AS

TL;DR: WeSCon是一个自训练框架，能够在预训练的零样本TTS模型中实现词级情感和语速控制，无需依赖包含句内情感或语速转换的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有情感TTS研究大多局限于语句级情感表达，无法支持词级控制，主要挑战在于建模多情感转换的复杂性以及缺乏捕捉句内情感和韵律变化的标注数据集。

Method: 提出过渡平滑策略和动态语速控制机制，通过多轮推理过程指导预训练TTS模型进行词级表达合成；引入动态情感注意力偏置机制，通过自训练端到端微调模型。

Result: 实验结果表明WeSCon有效克服数据稀缺问题，在词级情感表达控制方面达到最先进性能，同时保持原始TTS模型的强零样本合成能力。

Conclusion: WeSCon是首个实现词级情感和语速控制的框架，无需专门数据集，成功激活了预训练TTS模型的词级表达控制能力。

Abstract: While emotional text-to-speech (TTS) has made significant progress, most
existing research remains limited to utterance-level emotional expression and
fails to support word-level control. Achieving word-level expressive control
poses fundamental challenges, primarily due to the complexity of modeling
multi-emotion transitions and the scarcity of annotated datasets that capture
intra-sentence emotional and prosodic variation. In this paper, we propose
WeSCon, the first self-training framework that enables word-level control of
both emotion and speaking rate in a pretrained zero-shot TTS model, without
relying on datasets containing intra-sentence emotion or speed transitions. Our
method introduces a transition-smoothing strategy and a dynamic speed control
mechanism to guide the pretrained TTS model in performing word-level expressive
synthesis through a multi-round inference process. To further simplify the
inference, we incorporate a dynamic emotional attention bias mechanism and
fine-tune the model via self-training, thereby activating its ability for
word-level expressive control in an end-to-end manner. Experimental results
show that WeSCon effectively overcomes data scarcity, achieving
state-of-the-art performance in word-level emotional expression control while
preserving the strong zero-shot synthesis capabilities of the original TTS
model.

</details>


### [42] [Advancing Zero-Shot Open-Set Speech Deepfake Source Tracing](https://arxiv.org/abs/2509.24674)
*Manasi Chhibber,Jagabandhu Mishra,Tomi H. Kinnunen*

Main category: eess.AS

TL;DR: 提出基于说话人验证的零样本溯源框架，在封闭集场景下少样本学习表现更好，在开放集场景下零样本方法更优。


<details>
  <summary>Details</summary>
Motivation: 受说话人验证进展启发，开发零样本攻击源追踪方法，解决训练攻击与指纹-试验对攻击不重叠的问题。

Method: 采用SSL-AASIST系统进行攻击分类，探索零样本（余弦相似度和Siamese）和少样本（MLP和Siamese）后端评分方法。

Result: 在封闭集试验中，少样本Siamese和MLP的EER分别为18.44%和15.11%，优于零样本余弦评分的27.14%；在开放集试验中，零样本余弦评分EER为21.70%，优于少样本方法。

Conclusion: 少样本学习在封闭集场景有优势，而零样本方法在开放集场景表现更好，为攻击源追踪提供了有效解决方案。

Abstract: We propose a novel zero-shot source tracing framework inspired by advances in
speaker verification. Specifically, we adapt the SSL-AASIST system for attack
classification, ensuring that the attacks used for training are disjoint from
those used to form fingerprint-trial pairs. For backend scoring in attack
verification, we explore both zero-shot approaches (cosine similarity and
Siamese) and few-shot approaches (MLP and Siamese). Experiments on our recently
introduced STOPA dataset suggest that few-shot learning provides advantages in
the closed-set scenario, while zero-shot approaches perform better in the
open-set scenario. In closed-set trials, few-shot Siamese and MLP achieve equal
error rates (EER) of 18.44% and 15.11%, compared to 27.14% for zero-shot cosine
scoring. Conversely, in open-set trials, zero-shot cosine scoring reaches
21.70%, outperforming few-shot Siamese and MLP at 27.40% and 22.65%,
respectively.

</details>


### [43] [SenSE: Semantic-Aware High-Fidelity Universal Speech Enhancement](https://arxiv.org/abs/2509.24708)
*Xingchen Li,Hanke Xie,Ziqian Wang,Zihan Zhang,Longshuai Xiao,Lei Xie*

Main category: eess.AS

TL;DR: 提出SenSE方法，通过语言模型捕捉失真语音的语义信息，并将其集成到基于流匹配的语音增强框架中，解决传统生成式语音增强方法缺乏语义意识的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散或流生成模型虽然能产生高质量的增强语音，但缺乏对高层语义信息的感知，容易导致语义模糊和声学不连续。人类能够通过语义先验理解严重失真的语音，表明语义在语音增强中起着关键作用。

Method: 1. 引入语义感知的语音语言模型捕捉失真语音的语义并生成语义标记；2. 设计语义引导机制将语义信息整合到流匹配语音增强过程中；3. 提出提示引导机制，利用短参考话语缓解严重失真条件下的说话人相似性损失。

Result: 在多个基准数据集上的结果表明，SenSE不仅确保了高感知质量，还显著提高了语音保真度，同时在严重失真条件下保持强鲁棒性。

Conclusion: SenSE通过有效整合语义信息到语音增强框架中，解决了传统方法的语义模糊问题，在保持高感知质量的同时显著提升了语音保真度和鲁棒性。

Abstract: Generative universal speech enhancement (USE) methods aim to leverage
generative models to improve speech quality under various types of distortions.
Diffusion- or flow-based generative models are capable of producing enhanced
speech with high quality and fidelity. However, they typically achieve speech
enhancement by learning an acoustic feature mapping from degraded speech to
clean speech, while lacking awareness of high-level semantic information. This
deficiency tends to cause semantic ambiguity and acoustic discontinuities in
the enhanced speech. In contrast, humans can often comprehend heavily corrupted
speech by relying on semantic priors, suggesting that semantics play a crucial
role in speech enhancement. Therefore, in this paper, we propose SenSE, which
leverages a language model to capture the semantic information of distorted
speech and effectively integrates it into a flow-matching-based speech
enhancement framework. Specifically, we introduce a semantic-aware speech
language model to capture the semantics of degraded speech and generate
semantic tokens. We then design a semantic guidance mechanism that incorporates
semantic information into the flow-matching-based speech enhancement process,
effectively mitigating semantic ambiguity. In addition, we propose a prompt
guidance mechanism, which leverages a short reference utterance to alleviate
the loss of speaker similarity under severe distortion conditions. The results
of several benchmark data sets demonstrate that SenSE not only ensures high
perceptual quality but also substantially improves speech fidelity while
maintaining strong robustness under severe distortions. Codes and demos are
available.

</details>


### [44] [Deep Learning-Based Prediction of Energy Decay Curves from Room Geometry and Material Properties](https://arxiv.org/abs/2509.24769)
*Imran Muhammad,Gerald Schuller*

Main category: eess.AS

TL;DR: 提出基于深度学习的框架，直接从房间几何形状和表面吸声特性预测能量衰减曲线（EDC），支持高效的房间声学建模。


<details>
  <summary>Details</summary>
Motivation: 准确预测能量衰减曲线对于房间声学分析和关键参数估计至关重要，需要一种能够直接从房间配置预测EDC的可靠方法。

Method: 使用6000个鞋盒形房间数据集，通过Pyroomacoustics模拟房间脉冲响应并计算目标EDC，采用LSTM网络将归一化的房间特征映射到EDC。

Result: 预测EDC与目标EDC在MAE和RMSE上表现良好，从预测EDC推导的早期衰减时间、混响时间和清晰度指数与目标值高度一致（如EDT MAE 0.017秒，T20 MAE 0.021秒）。

Conclusion: 该方法在不同房间中具有良好的泛化能力，支持早期设计阶段和实时应用的房间声学建模。

Abstract: Accurate prediction of energy decay curves (EDCs) enables robust analysis of
room acoustics and reliable estimation of key parameters. We present a deep
learning framework that predicts EDCs directly from room geometry and surface
absorption. A dataset of 6000 shoebox rooms with realistic dimensions,
source-receiver placements, and frequency-dependent wall absorptions was
synthesized. For each configuration we simulate room impulse responses (RIRs)
using Pyroomacoustics and compute target EDCs. Normalized room features are
provided to a long short-term memory (LSTM) network that maps configuration to
EDC. Performance is evaluated with mean absolute error (MAE) and root mean
square error (RMSE) over time. We further derive early decay time (EDT),
reverberation time (T20), and clarity index (C50) from predicted and target
EDCs; close agreement is observed (e.g., EDT MAE 0.017 s, T20 MAE 0.021 s). The
approach generalizes across diverse rooms and supports efficient room-acoustics
modeling for early-stage design and real-time applications.

</details>


### [45] [VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning](https://arxiv.org/abs/2509.24773)
*Xin Cheng,Yuyue Wang,Xihua Wang,Yihan Wu,Kaisi Guan,Yijing Chen,Peng Zhang,Xiaojiang Liu,Meng Cao,Ruihua Song*

Main category: eess.AS

TL;DR: VSSFlow是一个统一的流匹配框架，将视频到声音(V2S)和视觉文本到语音(VisualTTS)任务集成在一起，通过创新的条件聚合机制处理不同输入信号，在两种任务上都超越了现有领域专用基线。


<details>
  <summary>Details</summary>
Motivation: 目前视频条件声音和语音生成任务(V2S和VisualTTS)通常作为独立任务处理，缺乏统一框架。现有统一尝试面临处理不同条件类型和复杂训练阶段的挑战。

Method: VSSFlow使用流匹配框架，通过交叉注意力和自注意力的不同归纳偏置处理视频条件和语音转录：交叉注意力处理模糊视频条件，自注意力处理确定性语音转录。采用端到端联合学习，无需复杂训练策略。

Result: VSSFlow在V2S和VisualTTS基准测试中都超越了最先进的领域专用基线，证明了统一生成模型的潜力。

Conclusion: VSSFlow成功统一了V2S和VisualTTS任务，通过共享音频先验知识加速收敛、增强条件生成并稳定分类器自由引导过程，展示了统一生成模型的关键潜力。

Abstract: Video-conditioned sound and speech generation, encompassing video-to-sound
(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed
as separate tasks, with limited exploration to unify them within a signle
framework. Recent attempts to unify V2S and VisualTTS face challenges in
handling distinct condition types (e.g., heterogeneous video and transcript
conditions) and require complex training stages. Unifying these two tasks
remains an open problem. To bridge this gap, we present VSSFlow, which
seamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching
framework. VSSFlow uses a novel condition aggregation mechanism to handle
distinct input signals. We find that cross-attention and self-attention layer
exhibit different inductive biases in the process of introducing condition.
Therefore, VSSFlow leverages these inductive biases to effectively handle
different representations: cross-attention for ambiguous video conditions and
self-attention for more deterministic speech transcripts. Furthermore, contrary
to the prevailing belief that joint training on the two tasks requires complex
training strategies and may degrade performance, we find that VSSFlow benefits
from the end-to-end joint learning process for sound and speech generation
without extra designs on training stages. Detailed analysis attributes it to
the learned general audio prior shared between tasks, which accelerates
convergence, enhances conditional generation, and stabilizes the
classifier-free guidance process. Extensive experiments demonstrate that
VSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S
and VisualTTS benchmarks, underscoring the critical potential of unified
generative models.

</details>


### [46] [Room Impulse Response Prediction with Neural Networks: From Energy Decay Curves to Perceptual Validation](https://arxiv.org/abs/2509.24834)
*Imran Muhammad,Gerald Schuller*

Main category: eess.AS

TL;DR: 提出神经网络框架预测房间脉冲响应，通过能量衰减曲线重建RIR，在感知测试中与参考RIR无显著差异


<details>
  <summary>Details</summary>
Motivation: 传统房间脉冲响应模拟和测量计算成本高、耗时，需要更高效的预测方法

Method: 使用神经网络从房间尺寸、材料吸收系数和源-接收器位置预测能量衰减曲线，通过反向微分重建RIR

Result: 客观评估显示低误差，MUSHRA听力测试确认预测RIR与参考RIR在感知上无显著差异

Conclusion: 该框架提供准确且感知可靠的RIR预测，为实际声学建模和音频渲染应用提供可扩展解决方案

Abstract: Prediction of room impulse responses (RIRs) is essential for room acoustics,
spatial audio, and immersive applications, yet conventional simulations and
measurements remain computationally expensive and time-consuming. This work
proposes a neural network framework that predicts energy decay curves (EDCs)
from room dimensions, material absorption coefficients, and source-receiver
positions, and reconstructs corresponding RIRs via reverse-differentiation. A
large training dataset was generated using room acoustic simulations with
realistic geometries, frequency-dependent absorption, and diverse
source-receiver configurations. Objective evaluation employed root mean squared
error (RMSE) and a custom loss for EDCs, as well as correlation, mean squared
error (MSE), spectral similarity for reconstructed RIRs. Perceptual validation
through a MUSHRA listening test confirmed no significant perceptual differences
between predicted and reference RIRs. The results demonstrate that the proposed
framework provides accurate and perceptually reliable RIR predictions, offering
a scalable solution for practical acoustic modeling and audio rendering
applications.

</details>


### [47] [SAGA-SR: Semantically and Acoustically Guided Audio Super-Resolution](https://arxiv.org/abs/2509.24924)
*Jaekwon Im,Juhan Nam*

Main category: eess.AS

TL;DR: SAGA-SR是一种多功能音频超分辨率模型，通过结合语义和声学指导，能够将4kHz-32kHz的音频上采样到44.1kHz，在客观和主观评估中都达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的音频超分辨率方法在语义对齐和高频重建一致性方面存在不足，需要开发更有效的指导机制。

Method: 基于DiT主干网络和流匹配目标训练，结合文本和频谱滚降嵌入作为条件，实现语义和声学双重指导。

Result: SAGA-SR能够稳健地将任意4kHz-32kHz输入音频上采样到44.1kHz，在所有测试案例中均达到最先进的性能。

Conclusion: SAGA-SR通过有效的语义和声学指导机制，成功解决了音频超分辨率中的语义对齐和高频重建一致性问题，实现了卓越的性能。

Abstract: Versatile audio super-resolution (SR) aims to predict high-frequency
components from low-resolution audio across diverse domains such as speech,
music, and sound effects. Existing diffusion-based SR methods often fail to
produce semantically aligned outputs and struggle with consistent
high-frequency reconstruction. In this paper, we propose SAGA-SR, a versatile
audio SR model that combines semantic and acoustic guidance. Based on a DiT
backbone trained with a flow matching objective, SAGA-SR is conditioned on text
and spectral roll-off embeddings. Due to the effective guidance provided by its
conditioning, SAGA-SR robustly upsamples audio from arbitrary input sampling
rates between 4 kHz and 32 kHz to 44.1 kHz. Both objective and subjective
evaluations show that SAGA-SR achieves state-of-the-art performance across all
test cases. Sound examples and code for the proposed model are available
online.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [48] [GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures](https://arxiv.org/abs/2509.22655)
*Jackson Loth,Pedro Sarmento,Saurjya Sarkar,Zixun Guo,Mathieu Barthet,Mark Sandler*

Main category: cs.SD

TL;DR: 提出了GOAT数据集，包含5.9小时高质量电吉他音频和29.5小时增强数据，使用吉他六线谱标注，支持多种演奏技术，在MIDI转录和自动六线谱转录方面取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 解决吉他音乐信息检索领域因数据集稀缺和标注有限而进展受限的问题，为吉他相关任务提供高质量数据支持。

Method: 收集高质量直接输入电吉他音频，使用吉他放大器进行数据增强，采用Guitar Pro格式和文本标记编码进行六线谱标注。

Result: 构建了包含5.9小时原始音频和29.5小时增强音频的数据集，在MIDI转录任务上取得竞争性结果，并在自动六线谱转录方面获得初步成果。

Conclusion: GOAT数据集为吉他相关MIR任务（从合成到转录再到演奏技术检测）提供了训练新模型的可能性。

Abstract: In recent years, the guitar has received increased attention from the music
information retrieval (MIR) community driven by the challenges posed by its
diverse playing techniques and sonic characteristics. Mainly fueled by deep
learning approaches, progress has been limited by the scarcity and limited
annotations of datasets. To address this, we present the Guitar On Audio and
Tablatures (GOAT) dataset, comprising 5.9 hours of unique high-quality direct
input audio recordings of electric guitars from a variety of different guitars
and players. We also present an effective data augmentation strategy using
guitar amplifiers which delivers near-unlimited tonal variety, of which we
provide a starting 29.5 hours of audio. Each recording is annotated using
guitar tablatures, a guitar-specific symbolic format supporting string and fret
numbers, as well as numerous playing techniques. For this we utilise both the
Guitar Pro format, a software for tablature playback and editing, and a
text-like token encoding. Furthermore, we present competitive results using
GOAT for MIDI transcription and preliminary results for a novel approach to
automatic guitar tablature transcription. We hope that GOAT opens up the
possibilities to train novel models on a wide variety of guitar-related MIR
tasks, from synthesis to transcription to playing technique detection.

</details>


### [49] [DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation](https://arxiv.org/abs/2509.22727)
*Ziqi Chen,Gongyu Chen,Yihua Wang,Chaofan Ding,Zihao chen,Wei-Qiang Zhang*

Main category: cs.SD

TL;DR: DiaMoE-TTS是一个基于IPA的统一方言语音合成框架，通过标准化的音标表示和方言感知的混合专家模型，解决了方言数据稀缺、拼写不一致和复杂音变问题。


<details>
  <summary>Details</summary>
Motivation: 方言语音蕴含丰富的文化和语言多样性，但构建方言TTS系统面临数据稀缺、拼写不一致和复杂音变等挑战。

Method: 基于F5-TTS架构，引入方言感知的混合专家模型来建模音系差异，并使用LoRA和条件适配器进行参数高效适配，实现快速迁移到新方言。

Result: 实验表明系统能够生成自然且富有表现力的语音，在未见过的方言和专门领域（如京剧）上实现零样本性能，仅需几小时数据。

Conclusion: DiaMoE-TTS实现了可扩展的、基于开放数据的语音合成，不依赖大规模或专有资源。

Abstract: Dialect speech embodies rich cultural and linguistic diversity, yet building
text-to-speech (TTS) systems for dialects remains challenging due to scarce
data, inconsistent orthographies, and complex phonetic variation. To address
these issues, we present DiaMoE-TTS, a unified IPA-based framework that
standardizes phonetic representations and resolves grapheme-to-phoneme
ambiguities. Built upon the F5-TTS architecture, the system introduces a
dialect-aware Mixture-of-Experts (MoE) to model phonological differences and
employs parameter-efficient adaptation with Low-Rank Adaptors (LoRA) and
Conditioning Adapters for rapid transfer to new dialects. Unlike approaches
dependent on large-scale or proprietary resources, DiaMoE-TTS enables scalable,
open-data-driven synthesis. Experiments demonstrate natural and expressive
speech generation, achieving zero-shot performance on unseen dialects and
specialized domains such as Peking Opera with only a few hours of data.

</details>


### [50] [Prompt-aware classifier free guidance for diffusion models](https://arxiv.org/abs/2509.22728)
*Xuanhao Zhang,Chang Li*

Main category: cs.SD

TL;DR: 提出了一种提示感知的指导尺度选择框架，通过预测质量曲线来自动选择最优的CFG尺度，解决了固定尺度在不同复杂度提示下泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型中的无分类器指导(CFG)虽然取得了显著进展，但固定的指导尺度无法适应不同复杂度的提示，导致过饱和或对齐不足的问题。

Method: 构建大型合成数据集，在多个尺度下生成样本并用可靠评估指标评分；训练轻量级预测器，基于语义嵌入和语言复杂度估计多指标质量曲线，通过带正则化的效用函数选择最优尺度。

Result: 在MSCOCO 2014和AudioCaps数据集上的实验表明，相比原始CFG方法，该方法在保真度、对齐度和感知偏好方面均获得一致提升。

Conclusion: 提示感知的尺度选择为预训练扩散主干提供了有效的免训练增强方法，能够自适应不同复杂度的提示。

Abstract: Diffusion models have achieved remarkable progress in image and audio
generation, largely due to Classifier-Free Guidance. However, the choice of
guidance scale remains underexplored: a fixed scale often fails to generalize
across prompts of varying complexity, leading to oversaturation or weak
alignment. We address this gap by introducing a prompt-aware framework that
predicts scale-dependent quality and selects the optimal guidance at inference.
Specifically, we construct a large synthetic dataset by generating samples
under multiple scales and scoring them with reliable evaluation metrics. A
lightweight predictor, conditioned on semantic embeddings and linguistic
complexity, estimates multi-metric quality curves and determines the best scale
via a utility function with regularization. Experiments on MSCOCO~2014 and
AudioCaps show consistent improvements over vanilla CFG, enhancing fidelity,
alignment, and perceptual preference. This work demonstrates that prompt-aware
scale selection provides an effective, training-free enhancement for pretrained
diffusion backbones.

</details>


### [51] [Text-Independent Speaker Identification Using Audio Looping With Margin Based Loss Functions](https://arxiv.org/abs/2509.22838)
*Elliot Q C Garcia,Nicéias Silva Vilela,Kátia Pires Nascimento do Sacramento,Tiago A. E. Ferreira*

Main category: cs.SD

TL;DR: 本文研究了CosFace Loss和ArcFace Loss在基于VGG16的CNN架构中进行文本无关说话人识别的有效性，使用Voxceleb1数据集的mel频谱图作为输入，结果显示这两种损失函数相比传统Softmax损失具有更高的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 说话人识别在安全系统、虚拟助手和个性化用户体验中变得至关重要，需要研究更有效的损失函数来提高识别准确性和鲁棒性。

Method: 使用基于VGG16模型的卷积神经网络架构，适应可变大小的mel频谱图输入，实现CosFace Loss和ArcFace Loss两种损失函数，并以Softmax损失作为比较基准，同时研究mel频谱图大小和时长对模型性能的影响。

Result: 实验结果表明，CosFace Loss和ArcFace Loss在说话人识别准确率上优于传统的Softmax损失方法。

Conclusion: 这些发现为未来研究提供了重要启示，表明基于角度的损失函数在说话人识别任务中具有显著优势。

Abstract: Speaker identification has become a crucial component in various
applications, including security systems, virtual assistants, and personalized
user experiences. In this paper, we investigate the effectiveness of CosFace
Loss and ArcFace Loss for text-independent speaker identification using a
Convolutional Neural Network architecture based on the VGG16 model, modified to
accommodate mel spectrogram inputs of variable sizes generated from the
Voxceleb1 dataset. Our approach involves implementing both loss functions to
analyze their effects on model accuracy and robustness, where the Softmax loss
function was employed as a comparative baseline. Additionally, we examine how
the sizes of mel spectrograms and their varying time lengths influence model
performance. The experimental results demonstrate superior identification
accuracy compared to traditional Softmax loss methods. Furthermore, we discuss
the implications of these findings for future research.

</details>


### [52] [WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms](https://arxiv.org/abs/2509.23238)
*Goksenin Yuksel,Pierre Guetschel,Michael Tangermann,Marcel van Gerven,Kiki van der Heijden*

Main category: cs.SD

TL;DR: WavJEPA是一种基于原始波形的联合嵌入预测架构，通过高层语义表示学习克服了传统频谱图方法的局限性，在多种下游任务中表现优异且计算资源需求更少。


<details>
  <summary>Details</summary>
Motivation: 从原始波形学习音频表示可以克服基于频谱图方法的局限性，如频谱图计算的长延迟和相位信息丢失。虽然自监督语音表示学习在波形上已很成功，但通用音频表示学习尚未达到类似成就。

Method: 提出WavJEPA，一种基于波形的联合嵌入预测架构，利用高层语义表示学习来弥补语音单元或标记级别表示学习的不足。还提出WavJEPA-Nat，一个在模拟自然场景上训练的多通道扩展版本。

Result: WavJEPA在各种下游基准任务中显著优于最先进的时域音频基础模型，同时需要更少的计算资源。WavJEPA-Nat对混响和噪声具有高度鲁棒性。

Conclusion: 这些结果突显了从原始波形进行通用音频表示学习的可行性和计算效率，展示了低延迟、鲁棒的时域音频基础模型在现实应用中的潜力。

Abstract: Learning audio representations from raw waveforms overcomes key limitations
of spectrogram-based audio representation learning, such as the long latency of
spectrogram computation and the loss of phase information. Yet, while
self-supervised speech representation learning from raw waveforms has been
remarkably successful, these approaches have not achieved similar feats for
general-purpose audio representation learning from waveforms. Here, we propose
WavJEPA, a waveform-based version of the Joint-Embedding Predictive
Architecture. WavJEPA leverages high-level semantic representation learning to
tackle the shortcomings of representation learning at the speech unit or token
level. We show that this approach substantially outperforms state-of-the-art
time-domain audio foundation models across a wide variety of downstream
benchmark tasks, while requiring considerably fewer computational resources.
Additionally, to overcome the performance drop that time-domain models
typically exhibit in noisy and reverberant real-world acoustic environments, we
present WavJEPA-Nat. WavJEPA-Nat is a multi-channel extension of the WavJEPA
architecture trained on simulated naturalistic scenes. We find that WavJEPA-Nat
is highly robust to reverberation and noise. These results highlight the
feasibility and computational efficiency of general-purpose audio
representation learning from raw waveforms, showcasing the potential for
low-latency, robust time-domain audio foundation models for real-world
applications.

</details>


### [53] [MeanFlowSE: One-Step Generative Speech Enhancement via MeanFlow](https://arxiv.org/abs/2509.23299)
*Yike Zhu,Boyi Kang,Ziqian Wang,Xingchen Li,Zihan Zhang,Wenjie Li,Longshuai Xiao,Wei Xue,Lei Xie*

Main category: cs.SD

TL;DR: MeanFlowSE是一个一步生成式语音增强框架，通过预测平均速度场进行一步潜在精炼，并使用自监督学习表示而非VAE潜在变量，在保持SOTA感知质量的同时显著降低推理时间和模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决生成式语音增强方法依赖多步采样或大语言模型导致实时部署受限的问题，开发高效的一步生成框架。

Method: 采用MeanFlow预测平均速度场进行一步潜在精炼，基于自监督学习表示而非VAE潜在变量，提供鲁棒的声学语义指导。

Result: 在Interspeech 2020 DNS挑战赛盲测集和模拟测试集上达到SOTA级感知质量和竞争力的可懂度，同时显著降低实时因子和模型大小。

Conclusion: MeanFlowSE在保持高质量的同时实现了高效的实时部署，适合实际应用。

Abstract: Speech enhancement (SE) recovers clean speech from noisy signals and is vital
for applications such as telecommunications and automatic speech recognition
(ASR). While generative approaches achieve strong perceptual quality, they
often rely on multi-step sampling (diffusion/flow-matching) or large language
models, limiting real-time deployment. To mitigate these constraints, we
present MeanFlowSE, a one-step generative SE framework. It adopts MeanFlow to
predict an average-velocity field for one-step latent refinement and conditions
the model on self-supervised learning (SSL) representations rather than VAE
latents. This design accelerates inference and provides robust
acoustic-semantic guidance during training. In the Interspeech 2020 DNS
Challenge blind test set and simulated test set, MeanFlowSE attains
state-of-the-art (SOTA) level perceptual quality and competitive
intelligibility while significantly lowering both real-time factor (RTF) and
model size compared with recent generative competitors, making it suitable for
practical use. The code will be released upon publication at
https://github.com/Hello3orld/MeanFlowSE.

</details>


### [54] [ABC-Eval: Benchmarking Large Language Models on Symbolic Music Understanding and Instruction Following](https://arxiv.org/abs/2509.23350)
*Jiahao Zhao,Yunjia Li,Wei Li,Kazuyoshi Yoshii*

Main category: cs.SD

TL;DR: ABC-Eval是首个专注于文本ABC记谱法理解和指令跟随能力的开源基准测试，包含1,086个测试样本和10个子任务，评估显示现有LLM在符号音乐处理方面存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，基于文本的符号音乐任务的可行性和重要性日益突出。虽然符号音乐在生成任务中已被广泛使用，但LLM在理解和推理符号音乐方面的能力仍未得到充分探索。

Method: 提出ABC-Eval基准测试，涵盖10个子任务和1,086个测试样本，从基础音乐语法理解到复杂序列级推理，评估7个最先进的LLM。

Result: 评估结果显示现有模型在符号音乐处理能力方面存在显著局限性，同时各个基线模型在不同子任务中的一致表现支持了基准测试的可靠性。

Conclusion: ABC-Eval基准测试揭示了LLM在符号音乐理解和推理方面的不足，为未来研究提供了重要的评估工具。

Abstract: As large language models continue to develop, the feasibility and
significance of text-based symbolic music tasks have become increasingly
prominent. While symbolic music has been widely used in generation tasks, LLM
capabilities in understanding and reasoning about symbolic music remain largely
underexplored. To address this gap, we propose ABC-Eval, the first open-source
benchmark dedicated to the understanding and instruction-following capabilities
in text-based ABC notation scores. It comprises 1,086 test samples spanning 10
sub-tasks, covering scenarios from basic musical syntax comprehension to
complex sequence-level reasoning. Such a diverse scope poses substantial
challenges to models' ability to handle symbolic music tasks. We evaluated
seven state-of-the-art LLMs on ABC-Eval, and the results reveal notable
limitations in existing models' symbolic music processing capabilities.
Furthermore, the consistent performance of individual baselines across
different sub-tasks supports the reliability of our benchmark.

</details>


### [55] [Emotional Styles Hide in Deep Speaker Embeddings: Disentangle Deep Speaker Embeddings for Speaker Clustering](https://arxiv.org/abs/2509.23358)
*Chaohao Lin,Xu Zheng,Kaida Wu,Peihao Xiang,Ou Bai*

Main category: cs.SD

TL;DR: 提出DTG-VAE方法，通过解耦情感因素来增强说话人聚类性能，特别是在包含情感表达的语音中。


<details>
  <summary>Details</summary>
Motivation: 现有深度说话人嵌入模型在处理包含情感表达的语音时性能下降，因为情感状态会影响说话人嵌入的有效性。

Method: 在变分自编码器框架中提出DTG-VAE解耦方法，提取更鲁棒的说话人嵌入。

Result: DTG-VAE能够提取更鲁棒的说话人嵌入，显著提升说话人聚类性能。

Conclusion: 情感状态与深度说话人嵌入的有效性存在直接关联，DTG-VAE方法能够有效解决情感表达对说话人聚类的影响。

Abstract: Speaker clustering is the task of identifying the unique speakers in a set of
audio recordings (each belonging to exactly one speaker) without knowing who
and how many speakers are present in the entire data, which is essential for
speaker diarization processes. Recently, off-the-shelf deep speaker embedding
models have been leveraged to capture speaker characteristics. However,
speeches containing emotional expressions pose significant challenges, often
affecting the accuracy of speaker embeddings and leading to a decline in
speaker clustering performance. To tackle this problem, we propose DTG-VAE, a
novel disentanglement method that enhances clustering within a Variational
Autoencoder (VAE) framework. This study reveals a direct link between emotional
states and the effectiveness of deep speaker embeddings. As demonstrated in our
experiments, DTG-VAE extracts more robust speaker embeddings and significantly
enhances speaker clustering performance.

</details>


### [56] [AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models](https://arxiv.org/abs/2509.23435)
*Wenyu Li,Xiaoqi Jiao,Yi Chang,Guangyan Zhang,Yiwen Guo*

Main category: cs.SD

TL;DR: 提出了AudioRole数据集和ARP-Eval评估框架，用于提升大语言模型的音频角色扮演能力，通过同步对齐语义内容和声音特征来解决现有文本角色扮演的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注基于文本的角色模拟，而音频角色扮演需要同步对齐语义内容和声音特征，这带来了独特挑战。高质量多模态数据集对于提升大语言模型的角色扮演能力至关重要。

Method: 从13部电视剧中精心策划了AudioRole数据集，包含1000+小时、100万+角色对话，提供同步的音频-文本对，并标注说话者身份和上下文元数据。同时提出了ARP-Eval双方面评估框架来评估响应质量和角色保真度。

Result: 在AudioRole上训练的GLM-4-Voice（称为ARP-Model）实现了平均声学个性化得分0.31，显著优于原始GLM-4-voice和更强大的MiniCPM-O-2.6模型。内容个性化得分达到0.36，比未训练原始模型提升约38%，与MiniCPM-O-2.6保持相同水平。

Conclusion: AudioRole数据集、6个训练好的ARP-Model和评估协议共同为推进基于音频的角色扮演研究提供了重要资源。

Abstract: The creation of high-quality multimodal datasets remains fundamental for
advancing role-playing capabilities in large language models (LLMs). While
existing works predominantly focus on text-based persona simulation, Audio
Role-Playing (ARP) presents unique challenges due to the need for synchronized
alignment of semantic content and vocal characteristics. To address this gap,
we propose AudioRole, a meticulously curated dataset from 13 TV series spanning
1K+ hours with 1M+ character-grounded dialogues, providing synchronized
audio-text pairs annotated with speaker identities and contextual metadata. In
addition, to demonstrate the effectiveness of the dataset, we introduced
ARP-Eval, a dual-aspect evaluation framework that assesses both response
quality and role fidelity. Empirical validation showing GLM-4-Voice trained on
AudioRole (which we called ARP-Model) achieve an average Acoustic
Personalization score of 0.31, significantly outperforming the original
GLM-4-voice and the more powerful model MiniCPM-O-2.6, which specifically
supports role-playing in one-shot scenarios. The ARP-Model also achieves a
Content Personalization score of 0.36, surpassing the untrained original model
by about 38% and maintaining the same level as MiniCPM-O-2.6.
  AudioRole features dialogues from over 115 main characters, 6 trained
ARP-Models that role-play different characters, and evaluation protocols.
Together, they provide an essential resource for advancing audio-grounded
role-playing research.

</details>


### [57] [Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention](https://arxiv.org/abs/2509.23610)
*Kai Li,Kejun Gao,Xiaolin Hu*

Main category: cs.SD

TL;DR: 提出了一种高效的视听语音分离方法Dolphin，通过轻量级视觉编码器和音频分离器，在保持高质量分离的同时大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 现有视听语音分离方法参数多、计算成本高，在作为预处理步骤的应用中不可接受，需要开发更高效的解决方案

Method: 使用双路径轻量级视频编码器DP-LipCoder将唇部运动转换为离散音频对齐语义令牌，构建包含全局-局部注意力块的轻量级编码器-解码器分离器

Result: 在三个基准数据集上超越当前SOTA模型，参数减少50%以上，MACs降低2.4倍以上，GPU推理速度提升6倍以上

Conclusion: Dolphin为现实场景中的高性能视听语音分离提供了实用且可部署的解决方案

Abstract: Audio-visual speech separation (AVSS) methods leverage visual cues to extract
target speech and have demonstrated strong separation quality in noisy acoustic
environments. However, these methods usually involve a large number of
parameters and require high computational cost, which is unacceptable in many
applications where speech separation serves as only a preprocessing step for
further speech processing. To address this issue, we propose an efficient AVSS
method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a
dual-path lightweight video encoder that transforms lip-motion into discrete
audio-aligned semantic tokens. For audio separation, we construct a lightweight
encoder-decoder separator, in which each layer incorporates a global-local
attention (GLA) block to efficiently capture multi-scale dependencies.
Experiments on three benchmark datasets showed that Dolphin not only surpassed
the current state-of-the-art (SOTA) model in separation quality but also
achieved remarkable improvements in efficiency: over 50% fewer parameters, more
than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These
results indicate that Dolphin offers a practical and deployable solution for
high-performance AVSS in real-world scenarios. Our code and demo page are
publicly available at http://cslikai.cn/Dolphin/.

</details>


### [58] [Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment](https://arxiv.org/abs/2509.23618)
*Pu Huang,Shouguang Wang,Siya Yao,Mengchu Zhou*

Main category: cs.SD

TL;DR: 提出IB-CAAN方法用于语音深度伪造检测，通过置信度引导对抗对齐和信息瓶颈技术，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 神经语音合成技术产生了高度逼真的语音深度伪造，带来重大安全风险。检测面临分布偏移和多种变异性挑战

Method: IB-CAAN方法结合置信度引导对抗对齐和信息瓶颈技术，前者抑制攻击特定伪影，后者去除干扰变异性

Result: 在ASVspoof 2019/2021、ASVspoof 5和In-the-Wild等基准测试中，IB-CAAN始终优于基线方法

Conclusion: 该方法通过共享判别特征学习，为语音深度伪造检测提供了鲁棒解决方案

Abstract: Neural speech synthesis techniques have enabled highly realistic speech
deepfakes, posing major security risks. Speech deepfake detection is
challenging due to distribution shifts across spoofing methods and variability
in speakers, channels, and recording conditions. We explore learning shared
discriminative features as a path to robust detection and propose Information
Bottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN).
Confidence-guided adversarial alignment adaptively suppresses attack-specific
artifacts without erasing discriminative cues, while the information bottleneck
removes nuisance variability to preserve transferable features. Experiments on
ASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN
consistently outperforms baseline and achieves state-of-the-art performance on
many benchmarks.

</details>


### [59] [AudioMoG: Guiding Audio Generation with Mixture-of-Guidance](https://arxiv.org/abs/2509.23727)
*Junyou Wang,Zehua Chen,Binjie Yuan,Kaiwen Zheng,Chang Li,Yuxuan Jiang,Jun Zhu*

Main category: cs.SD

TL;DR: 本文提出了AudioMoG框架，通过混合多种引导原则来提升跨模态音频生成的质量，在保持推理效率的同时实现更高的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的音频生成引导方法通常只依赖单一原则（如CFG的条件对齐或AG的分数准确性），未能充分利用不同引导原则的互补优势。

Method: 提出了混合引导框架AudioMoG，通过组合不同引导原则的累积效益来丰富引导方法的构成，既能考虑平行互补，也能恢复单一原则。

Result: 实验表明AudioMoG在T2A生成中始终优于单一引导方法，同时在V2A、文本到音乐和图像生成中也显示出优势，且不牺牲推理速度。

Conclusion: AudioMoG展示了在采样阶段通过混合引导原则可以实现更高的生成质量，这是当前跨模态音频生成系统的"免费午餐"。

Abstract: Guidance methods have demonstrated significant improvements in cross-modal
audio generation, including text-to-audio (T2A) and video-to-audio (V2A)
generation. The popularly adopted method, classifier-free guidance (CFG),
steers generation by emphasizing condition alignment, enhancing fidelity but
often at the cost of diversity. Recently, autoguidance (AG) has been explored
for audio generation, encouraging the sampling to faithfully reconstruct the
target distribution and showing increased diversity. Despite these advances,
they usually rely on a single guiding principle, e.g., condition alignment in
CFG or score accuracy in AG, leaving the full potential of guidance for audio
generation untapped. In this work, we explore enriching the composition of the
guidance method and present a mixture-of-guidance framework, AudioMoG. Within
the design space, AudioMoG can exploit the complementary advantages of
distinctive guiding principles by fulfilling their cumulative benefits. With a
reduced form, AudioMoG can consider parallel complements or recover a single
guiding principle, without sacrificing generality. We experimentally show that,
given the same inference speed, AudioMoG approach consistently outperforms
single guidance in T2A generation across sampling steps, concurrently showing
advantages in V2A, text-to-music, and image generation. These results highlight
a "free lunch" in current cross-modal audio generation systems: higher quality
can be achieved through mixed guiding principles at the sampling stage without
sacrificing inference efficiency. Demo samples are available at:
https://audio-mog.github.io.

</details>


### [60] [VioPTT: Violin Technique-Aware Transcription from Synthetic Data Augmentation](https://arxiv.org/abs/2509.23759)
*Ting-Kang Wang,Yueh-Po Peng,Li Su,Vincent K. M. Cheung*

Main category: cs.SD

TL;DR: VioPTT是一个轻量级端到端模型，能够同时转录小提琴演奏技巧、音高起始和结束时间，并发布了新的合成数据集MOSA-VPT。


<details>
  <summary>Details</summary>
Motivation: 现有自动音乐转录模型主要关注音高和时序信息，但忽略了表达性和乐器特定的细微差别，特别是小提琴演奏技巧带来的独特音色变化。

Method: 提出了VioPTT模型，采用端到端架构，结合新发布的MOSA-VPT合成数据集进行训练，避免了对人工标注的依赖。

Result: 模型在真实世界的小提琴演奏技巧录音上表现出强大的泛化能力，并达到了最先进的转录性能。

Conclusion: VioPTT是首个在小提琴转录和演奏技巧预测方面实现统一框架的模型，填补了现有转录系统的空白。

Abstract: While automatic music transcription is well-established in music information
retrieval, most models are limited to transcribing pitch and timing information
from audio, and thus omit crucial expressive and instrument-specific nuances.
One example is playing technique on the violin, which affords its distinct
palette of timbres for maximal emotional impact. Here, we propose
\textbf{VioPTT} (Violin Playing Technique-aware Transcription), a lightweight,
end-to-end model that directly transcribes violin playing technique in addition
to pitch onset and offset. Furthermore, we release \textbf{MOSA-VPT}, a novel,
high-quality synthetic violin playing technique dataset to circumvent the need
for manually labeled annotations. Leveraging this dataset, our model
demonstrated strong generalization to real-world note-level violin technique
recordings in addition to achieving state-of-the-art transcription performance.
To our knowledge, VioPTT is the first to jointly combine violin transcription
and playing technique prediction within a unified framework.

</details>


### [61] [An Efficient Transfer Learning Method Based on Adapter with Local Attributes for Speech Emotion Recognition](https://arxiv.org/abs/2509.23795)
*Haoyu Song,Ian McLoughlin,Qing Gu,Nan Jiang,Yan Song*

Main category: cs.SD

TL;DR: 提出一种带有局部属性的适配器方法，用于语音情感识别的高效迁移学习，通过轻量级WAP-Transformer骨干网络和教师-学生分支适配器实现任务无关的迁移学习。


<details>
  <summary>Details</summary>
Motivation: 解决语音情感识别中高质量大规模语料库缺乏的问题，以及传统方法需要针对不同场景进行昂贵编码器重新训练的问题。

Method: 使用加权平均池化-Transformer作为轻量级骨干网络，设计具有教师-学生分支的适配器，通过掩码预测和自蒸馏目标联合优化学生分支，教师分支通过指数移动平均获得，并通过无监督聚类学习局部属性。

Result: 在IEMOCAP数据集上的广泛实验表明，该方法在相似设置下相比之前的最先进方法取得了优越性能。

Conclusion: 提出的带有局部属性的适配器方法能够有效实现语音情感识别的高效迁移学习，减少对大规模标注数据的依赖和编码器重新训练的成本。

Abstract: Existing speech emotion recognition (SER) methods commonly suffer from the
lack of high-quality large-scale corpus, partly due to the complex,
psychological nature of emotion which makes accurate labeling difficult and
time consuming. Recently, transfer learning based methods that exploit the
encoders pretrained on large-scale speech corpus (e.g., Wav2Vec2.0 and HuBERT)
have shown strong potential for downstream SER tasks. However, task-specific
fine-tuning remains necessary for various conversational scenarios of different
topics, speakers and languages to achieve satisfactory performance. It
generally requires costly encoder retraining for individual SER tasks. To
address this issue, we propose to train an adapter with local attributes for
efficient transfer learning. Specifically, a weighted average
pooling-Transformer (WAP-Transformer) is proposed as a lightweight backbone to
enrich the frame-level representation. An adapter with teacher-student branches
is exploited for task-agnostic transfer learning, where the student branch is
jointly optimized via mask prediction and self-distillation objectives, and the
teacher branch is obtained online from the student via exponential moving
average (EMA). Meanwhile, local attributes are learned from the teacher branch
via unsupervised clustering, which aims to act as a universal model that
provides additional semantic-rich supervisions. A statistical attentive pooling
(SAP) module is proposed to obtain utterance representation for fine-tuning. To
evaluate the effectiveness of the proposed adapter with local attributes,
extensive experiments have been conducted on IEMOCAP. Superior performance has
been reported, compared to the previous state-of-the-art methods in similar
settings.

</details>


### [62] [Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription](https://arxiv.org/abs/2509.23878)
*Wei Zeng,Junchuan Zhao,Ye Wang*

Main category: cs.SD

TL;DR: 提出了一个统一框架，联合建模表达性演奏渲染（EPR）和自动钢琴转录（APT），通过解耦音符级乐谱内容和全局演奏风格表示，支持风格迁移和灵活渲染。


<details>
  <summary>Details</summary>
Motivation: EPR和APT是音乐信息检索中的基本但互为逆任务的任务，但之前的工作都是独立处理的。本文旨在建立一个统一框架来联合建模这两个任务。

Method: 基于transformer的序列到序列架构，使用仅序列对齐的数据训练，无需细粒度音符级对齐。引入独立的基于扩散的演奏风格推荐模块，直接从乐谱内容生成风格嵌入。

Result: 客观和主观评估结果表明，该框架在EPR和APT任务上实现了竞争性性能，同时实现了有效的内容-风格解耦、可靠的风格迁移和风格合适的渲染。

Conclusion: 提出的统一框架成功联合建模了EPR和APT任务，实现了内容与风格的有效解耦，支持灵活的演奏风格迁移和渲染。

Abstract: Expressive performance rendering (EPR) and automatic piano transcription
(APT) are fundamental yet inverse tasks in music information retrieval: EPR
generates expressive performances from symbolic scores, while APT recovers
scores from performances. Despite their dual nature, prior work has addressed
them independently. In this paper we propose a unified framework that jointly
models EPR and APT by disentangling note-level score content and global
performance style representations from both paired and unpaired data. Our
framework is built on a transformer-based sequence-to-sequence architecture and
is trained using only sequence-aligned data, without requiring fine-grained
note-level alignment. To automate the rendering process while ensuring
stylistic compatibility with the score, we introduce an independent
diffusion-based performance style recommendation module that generates style
embeddings directly from score content. This modular component supports both
style transfer and flexible rendering across a range of expressive styles.
Experimental results from both objective and subjective evaluations demonstrate
that our framework achieves competitive performance on EPR and APT tasks, while
enabling effective content-style disentanglement, reliable style transfer, and
stylistically appropriate rendering. Demos are available at
https://jointpianist.github.io/epr-apt/

</details>


### [63] [UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities](https://arxiv.org/abs/2509.24391)
*Xuenan Xu,Jiahao Mei,Zihao Zheng,Ye Tao,Zeyu Xie,Yaoyun Zhang,Haohe Liu,Yuning Wu,Ming Yan,Wen Wu,Chao Zhang,Mengyue Wu*

Main category: cs.SD

TL;DR: UniFlow-Audio是一个基于流匹配的统一音频生成框架，通过双融合机制处理时间对齐和非时间对齐任务，在7个音频任务上取得优异性能，参数量少于10亿。


<details>
  <summary>Details</summary>
Motivation: 传统音频生成研究按时间对齐和非时间对齐任务分别发展，但音频本身并无此分类，需要一个统一模型来实现通用音频生成。现有统一方法主要采用自回归架构，非自回归方法尚未充分探索。

Method: 提出基于流匹配的UniFlow-Audio框架，采用双融合机制：时间对齐特征与音频潜在表示对齐，非时间对齐特征通过跨注意力集成。使用任务平衡数据采样，支持文本、音频、视频等多模态输入。

Result: 在7个音频任务上取得强劲结果，训练数据少于8000小时，可训练参数少于10亿。即使是仅约2亿参数的小型变体也表现出竞争力。

Conclusion: UniFlow-Audio展示了作为音频生成非自回归基础模型的潜力，通过多任务学习和流匹配的生成能力实现了统一音频生成。

Abstract: Audio generation, including speech, music and sound effects, has advanced
rapidly in recent years. These tasks can be divided into two categories:
time-aligned (TA) tasks, where each input unit corresponds to a specific
segment of the output audio (e.g., phonemes aligned with frames in speech
synthesis); and non-time-aligned (NTA) tasks, where such alignment is not
available. Since modeling paradigms for the two types are typically different,
research on different audio generation tasks has traditionally followed
separate trajectories. However, audio is not inherently divided into such
categories, making a unified model a natural and necessary goal for general
audio generation. Previous unified audio generation works have adopted
autoregressive architectures, while unified non-autoregressive approaches
remain largely unexplored. In this work, we propose UniFlow-Audio, a universal
audio generation framework based on flow matching. We propose a dual-fusion
mechanism that temporally aligns audio latents with TA features and integrates
NTA features via cross-attention in each model block. Task-balanced data
sampling is employed to maintain strong performance across both TA and NTA
tasks. UniFlow-Audio supports omni-modalities, including text, audio, and
video. By leveraging the advantage of multi-task learning and the generative
modeling capabilities of flow matching, UniFlow-Audio achieves strong results
across 7 tasks using fewer than 8K hours of public training data and under 1B
trainable parameters. Even the small variant with only ~200M trainable
parameters shows competitive performance, highlighting UniFlow-Audio as a
potential non-auto-regressive foundation model for audio generation. Code and
models will be available at https://wsntxxn.github.io/uniflow_audio.

</details>


### [64] [From Sound to Setting: AI-Based Equalizer Parameter Prediction for Piano Tone Replication](https://arxiv.org/abs/2509.24404)
*Song-Ze Yu*

Main category: cs.SD

TL;DR: 提出基于AI的音色复制系统，从音频特征直接预测EQ参数设置，为音乐制作提供可解释的参数输出。


<details>
  <summary>Details</summary>
Motivation: 传统音频到音频方法缺乏可解释性，需要开发能够输出可调整EQ参数的系统，让音乐制作人能够在工作流程中进一步调整。

Method: 使用钢琴录音数据集，系统性地变化EQ设置，评估回归模型和神经网络模型，直接预测EQ频段增益等参数值。

Result: 神经网络在多频段任务上达到0.0216的均方误差，表现优于传统回归方法。

Conclusion: 该系统为音乐制作人提供了实用、灵活且自动化的音色匹配工具，并为扩展到更复杂音频效果奠定了基础。

Abstract: This project presents an AI-based system for tone replication in music
production, focusing on predicting EQ parameter settings directly from audio
features. Unlike traditional audio-to-audio methods, our approach outputs
interpretable parameter values (e.g., EQ band gains) that musicians can further
adjust in their workflow. Using a dataset of piano recordings with
systematically varied EQ settings, we evaluate both regression and neural
network models. The neural network achieves a mean squared error of 0.0216 on
multi-band tasks. The system enables practical, flexible, and automated tone
matching for music producers and lays the foundation for extensions to more
complex audio effects.

</details>


### [65] [An Agent-Based Framework for Automated Higher-Voice Harmony Generation](https://arxiv.org/abs/2509.24463)
*Nia D'Souza Ganapathy,Arul Selvamani Shaja*

Main category: cs.SD

TL;DR: 提出了一种基于多智能体系统的AI和声生成器，通过四个专门化智能体协作生成音乐和声


<details>
  <summary>Details</summary>
Motivation: 解决算法作曲中生成音乐连贯且美学愉悦的和声这一重要挑战

Method: 使用四个专门化智能体：音乐输入智能体、和弦知识智能体（基于Transformer）、和声生成智能体（使用Harmony-GPT和RNN）、音频制作智能体（基于GAN的符号到音频合成器）

Result: 系统能够为给定旋律生成复杂且上下文适当的高音部和声

Conclusion: 这种模块化、基于智能体的方法能够有效模仿人类音乐家的协作过程，实现稳健的数据处理、深度理论理解、创造性作曲和逼真的音频合成

Abstract: The generation of musically coherent and aesthetically pleasing harmony
remains a significant challenge in the field of algorithmic composition. This
paper introduces an innovative Agentic AI-enabled Higher Harmony Music
Generator, a multi-agent system designed to create harmony in a collaborative
and modular fashion. Our framework comprises four specialized agents: a
Music-Ingestion Agent for parsing and standardizing input musical scores; a
Chord-Knowledge Agent, powered by a Chord-Former (Transformer model), to
interpret and provide the constituent notes of complex chord symbols; a
Harmony-Generation Agent, which utilizes a Harmony-GPT and a Rhythm-Net (RNN)
to compose a melodically and rhythmically complementary harmony line; and an
Audio-Production Agent that employs a GAN-based Symbolic-to-Audio Synthesizer
to render the final symbolic output into high-fidelity audio. By delegating
specific tasks to specialized agents, our system effectively mimics the
collaborative process of human musicians. This modular, agent-based approach
allows for robust data processing, deep theoretical understanding, creative
composition, and realistic audio synthesis, culminating in a system capable of
generating sophisticated and contextually appropriate higher-voice harmonies
for given melodies.

</details>


### [66] [Beyond Genre: Diagnosing Bias in Music Embeddings Using Concept Activation Vectors](https://arxiv.org/abs/2509.24482)
*Roman B. Gebhardt,Arne Kuhle,Eylül Bektur*

Main category: cs.SD

TL;DR: 该论文研究了音乐表示模型中存在的文化偏见问题，使用概念激活向量(CAVs)分析歌手性别和语言等非音乐属性对流派表示的影响，并提出了一种基于概念向量操作的后处理去偏策略。


<details>
  <summary>Details</summary>
Motivation: 音乐表示模型广泛应用于标签、检索和理解任务，但其编码文化偏见的潜力尚未充分探索。研究者希望了解歌手性别和语言等非音乐属性是否会对流派表示产生意外影响。

Method: 使用概念激活向量(CAVs)分析四个最先进模型(MERT、Whisper、MuQ、MuQ-MuLan)，采用STraDa数据集并精心平衡训练集以控制流派混杂因素，提出基于概念向量操作的后处理去偏策略。

Result: 研究结果显示模型存在显著的特定偏见，与MIR和音乐社会学中报道的差异一致。提出的去偏策略在减轻这些偏见方面表现出有效性。

Conclusion: 这些发现强调了需要设计偏见感知的模型，并表明概念化可解释性方法为诊断和减轻MIR中的表示偏见提供了实用工具。

Abstract: Music representation models are widely used for tasks such as tagging,
retrieval, and music understanding. Yet, their potential to encode cultural
bias remains underexplored. In this paper, we apply Concept Activation Vectors
(CAVs) to investigate whether non-musical singer attributes - such as gender
and language - influence genre representations in unintended ways. We analyze
four state-of-the-art models (MERT, Whisper, MuQ, MuQ-MuLan) using the STraDa
dataset, carefully balancing training sets to control for genre confounds. Our
results reveal significant model-specific biases, aligning with disparities
reported in MIR and music sociology. Furthermore, we propose a post-hoc
debiasing strategy using concept vector manipulation, demonstrating its
effectiveness in mitigating these biases. These findings highlight the need for
bias-aware model design and show that conceptualized interpretability methods
offer practical tools for diagnosing and mitigating representational bias in
MIR.

</details>


### [67] [Discovering "Words" in Music: Unsupervised Learning of Compositional Sparse Code for Symbolic Music](https://arxiv.org/abs/2509.24603)
*Tianle Wang,Sirui Zhang,Xinyi Tong,Peiyang Yu,Jishang Chen,Liangke Zhao,Xinpu Gao,Yves Zhu,Tiezheng Ge,Bo Zheng,Duo Xu,Yang Liu,Xin Jin,Feng Yu,Songchun Zhu*

Main category: cs.SD

TL;DR: 提出一种无监督机器学习算法，从符号音乐数据中发现重复出现的音乐模式（称为"音乐词汇"），通过两阶段EM学习框架解决音乐语义模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 音乐中的重复模式对音乐结构和创作认知过程至关重要，但由于音乐解释的语义模糊性，提取这些模式具有挑战性。

Method: 将音乐词汇发现建模为统计优化问题，采用两阶段EM学习框架：1. 开发音乐词汇词典；2. 重建音乐数据。通过最小化编码长度来解决语义模糊性。

Result: 与人类专家标注相比，算法获得了0.61的IoU分数，表明最小化编码长度能有效处理语义模糊性。

Conclusion: 该方法使计算机能够从音乐数据中提取"基本构建块"，支持AI音乐任务（生成、分类、风格转换等）和音乐学分析，揭示了人类编码系统优化如何塑造音乐语义。

Abstract: This paper presents an unsupervised machine learning algorithm that
identifies recurring patterns -- referred to as ``music-words'' -- from
symbolic music data. These patterns are fundamental to musical structure and
reflect the cognitive processes involved in composition. However, extracting
these patterns remains challenging because of the inherent semantic ambiguity
in musical interpretation. We formulate the task of music-word discovery as a
statistical optimization problem and propose a two-stage
Expectation-Maximization (EM)-based learning framework: 1. Developing a
music-word dictionary; 2. Reconstructing the music data. When evaluated against
human expert annotations, the algorithm achieved an Intersection over Union
(IoU) score of 0.61. Our findings indicate that minimizing code length
effectively addresses semantic ambiguity, suggesting that human optimization of
encoding systems shapes musical semantics. This approach enables computers to
extract ``basic building blocks'' from music data, facilitating structural
analysis and sparse encoding. The method has two primary applications. First,
in AI music, it supports downstream tasks such as music generation,
classification, style transfer, and improvisation. Second, in musicology, it
provides a tool for analyzing compositional patterns and offers insights into
the principle of minimal encoding across diverse musical styles and composers.

</details>


### [68] [When Audio Generators Become Good Listeners: Generative Features for Understanding Tasks](https://arxiv.org/abs/2509.24635)
*Zeyu Xie,Chenxing Li,Xuenan Xu,Mengyue Wu,Wenfu Wang,Ruibo Fu,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: 该论文开创性地利用生成式特征增强音频理解，通过融合生成模型的空间时间感知和语义先验，在多个音频任务中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统判别式特征直接优化后验分布，强调语义抽象但丢失细粒度细节。生成式音频模型天然编码了空间时间感知（捕捉跨时间和频率的局部声学纹理）和语义先验（知道生成什么），这促使研究者探索这两种互补优势的结合。

Method: 系统研究了生成式和判别式特征的差异和互补关系，提出有效的融合策略。

Result: 在多个任务上的实验，包括声音事件分类、标记以及细粒度的音频字幕生成任务，都显示出一致的性能提升。

Conclusion: 这项工作为音频表示学习引入了新视角，强调生成-判别互补性可以为音频理解提供详细的感知和语义意识。

Abstract: This work pioneers the utilization of generative features in enhancing audio
understanding. Unlike conventional discriminative features that directly
optimize posterior and thus emphasize semantic abstraction while losing fine
grained details, audio generation models inherently encode both spatiotemporal
perception (capturing local acoustic texture across time and frequency) and
semantic prior (knowing what to generate). It motivates us to explore the
bridge of these complementary strengths. We provide a systematic investigation
of their differences and complementary relationships, and ultimately propose an
effective fusion strategy. Experiments across multiple tasks, including sound
event classification, tagging, and particularly the fine grained task of audio
captioning, demonstrate consistent performance gains. Beyond empirical
improvements, this work more importantly introduces a new perspective on audio
representation learning, highlighting that generative discriminative
complementarity can provide both detailed perception and semantic awareness for
audio understanding.

</details>


### [69] [VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning](https://arxiv.org/abs/2509.24650)
*Yixuan Zhou,Guoyang Zeng,Xin Liu,Xiang Li,Renjie Yu,Ziyang Wang,Runchuan Ye,Weiyue Sun,Jiancheng Gui,Kehan Li,Zhiyong Wu,Zhiyuan Liu*

Main category: cs.SD

TL;DR: VoxCPM提出了一种无需外部语音分词器的端到端TTS模型，通过分层语义-声学建模和半离散残差表示解决了离散标记与连续信号之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统语音合成模型面临离散标记稳定性但表达能力有限，连续信号丰富但存在误差累积的困境。多阶段流水线依赖预训练语音分词器，造成了语义-声学分离，限制了整体表达性语音生成。

Method: 引入可微分量化瓶颈实现自然专业化：文本语义语言模型生成语义-韵律规划，残差声学模型恢复细粒度声学细节。分层语义-声学表示指导基于局部扩散的解码器生成高质量语音潜在表示。整个架构在简单扩散目标下端到端训练。

Result: 在180万小时双语语料上训练的VoxCPM-0.5B模型在零样本TTS任务中达到开源系统的最先进性能，能够理解文本以推断和生成适当的韵律和风格，提供上下文感知的表达性和自然流畅的语音。

Conclusion: 该方法实现了表达性和稳定的语音合成，消除了对外部语音分词器的依赖，VoxCPM在Apache 2.0许可下公开可用，促进社区驱动的研究和开发。

Abstract: Generative models for speech synthesis face a fundamental trade-off: discrete
tokens ensure stability but sacrifice expressivity, while continuous signals
retain acoustic richness but suffer from error accumulation due to task
entanglement. This challenge has driven the field towards multi-stage pipelines
that rely on pre-trained speech tokenizers, but these create a
semantic-acoustic divide, limiting holistic and expressive speech generation.
We resolve these dilemma through hierarchical semantic-acoustic modeling with
semi-discrete residual representations and present a novel tokenizer-free TTS
model VoxCPM. Our framework introduces a differentiable quantization bottleneck
that induces natural specialization: a Text-Semantic Language Model (TSLM)
generates semantic-prosodic plans, while a Residual Acoustic Model (RALM)
recovers fine-grained acoustic details. This hierarchical semantic-acoustic
representation guides a local diffusion-based decoder to generate high-fidelity
speech latents. Critically, the entire architecture is trained end-to-end under
a simple diffusion objective, eliminating dependency on external speech
tokenizers. Trained on a massive 1.8 million hours of bilingual corpus, our
VoxCPM-0.5B model achieves state-of-the-art zero-shot TTS performance among
open-source systems, demonstrating that our approach delivers expressive and
stable synthesis. Besides, VoxCPM shows the capability to comprehend text to
infer and generate appropriate prosody and style, delivering speech with
context-aware expressiveness and natural flow. To facilitate community-driven
research and development, VoxCPM is publicly accessible under Apache 2.0.

</details>


### [70] [Sparse Autoencoders Make Audio Foundation Models more Explainable](https://arxiv.org/abs/2509.24793)
*Théo Mariotte,Martin Lebourdais,Antonio Almudévar,Marie Tahon,Alfonso Ortega,Nicolas Dugué*

Main category: cs.SD

TL;DR: 使用稀疏自编码器分析音频预训练模型的隐藏表示，聚焦于歌唱技术分类案例研究，发现SAEs能保留原始表示和类别信息，增强声乐属性的解耦。


<details>
  <summary>Details</summary>
Motivation: 音频预训练模型的表示学习机制不明确，现有分析主要局限于隐藏表示的线性探测，需要更深入的分析工具来理解自监督学习系统。

Method: 采用稀疏自编码器分析预训练模型的隐藏表示，以歌唱技术分类为案例进行研究。

Result: SAEs能够保留原始表示和类别标签信息，其内部结构为自监督学习系统提供了洞察；SAEs增强了声乐属性的解耦能力。

Conclusion: 稀疏自编码器是识别表示中编码的底层因素的有效工具，为分析预训练模型提供了新的视角。

Abstract: Audio pretrained models are widely employed to solve various tasks in speech
processing, sound event detection, or music information retrieval. However, the
representations learned by these models are unclear, and their analysis mainly
restricts to linear probing of the hidden representations. In this work, we
explore the use of Sparse Autoencoders (SAEs) to analyze the hidden
representations of pretrained models, focusing on a case study in singing
technique classification. We first demonstrate that SAEs retain both
information about the original representations and class labels, enabling their
internal structure to provide insights into self-supervised learning systems.
Furthermore, we show that SAEs enhance the disentanglement of vocal attributes,
establishing them as an effective tool for identifying the underlying factors
encoded in the representations.

</details>


### [71] [Enhanced Automatic Drum Transcription via Drum Stem Source Separation](https://arxiv.org/abs/2509.24853)
*Xavier Riley,Simon Dixon*

Main category: cs.SD

TL;DR: 提出了一种结合自动鼓转录和鼓声部分离的简单后处理方法，将转录类别从5个扩展到7个，并能估计MIDI力度值，提高了鼓转录的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动鼓转录技术最多只能处理5个鼓类，而鼓声部分离模型可以分离6个以上的鼓类。结合这两种工具可以改进鼓转录的真实性。

Method: 通过简单的后处理步骤，将5类转录输出扩展到7类，并基于分离的鼓声部估计MIDI力度值。

Result: 与8类鼓转录基线相比表现出色，生成的MIDI转录适合音乐信息检索和音乐制作任务。

Conclusion: 结合自动鼓转录和鼓声部分离工具可以有效提高鼓转录的真实性和实用性。

Abstract: Automatic Drum Transcription (ADT) remains a challenging task in MIR but
recent advances allow accurate transcription of drum kits with up 5 classes -
kick, snare, hi-hats, toms and cymbals - via the ADTOF package. In addition,
several drum kit \emph{stem} separation models in the open source community
support separation for more than 6 stem classes, including distinct crash and
ride cymbals. In this work we explore the benefits of combining these tools to
improve the realism of drum transcriptions. We describe a simple
post-processing step which expands the transcription output from five to seven
classes and furthermore, we are able to estimate MIDI velocity values based on
the separated stems. Our solution achieves strong performance when assessed
against a baseline of 8-class drum transcription and produces realistic MIDI
transcriptions suitable for MIR or music production tasks.

</details>


### [72] [Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio Classification](https://arxiv.org/abs/2509.24901)
*Lukas Rauch,René Heinrich,Houtan Ghaffari,Lukas Miklautz,Ilyass Moummad,Bernhard Sick,Christoph Scholz*

Main category: cs.SD

TL;DR: 论文提出了一种二值化原型探针方法，解决了音频自监督学习中全局池化造成的信息瓶颈问题，使探针评估成为比微调更高效的竞争性范式。


<details>
  <summary>Details</summary>
Motivation: 当前音频自监督学习默认使用微调评估，但全局池化（如CLS-token）会丢弃分散的局部事件信息，导致线性探针无法准确表征嵌入质量，存在预训练目标（全局操作）与下游任务（局部事件）之间的不匹配问题。

Method: 提出了二值化原型探针：一种轻量级简单的池化方法，通过学习原型执行类别级信息聚合，克服全局池化的信息瓶颈。

Result: 在13个数据集和6个基于频谱图的编码器的综合基准测试中，该方法显著优于线性和注意力探针，证明了探针作为音频SSL模型评估的竞争性范式。

Conclusion: 该工作确立了探针作为评估音频自监督学习模型的竞争性和高效范式，挑战了对昂贵微调的依赖。

Abstract: Although probing frozen models has become a standard evaluation paradigm,
self-supervised learning in audio defaults to fine-tuning. A key reason is that
global pooling creates an information bottleneck causing linear probes to
misrepresent the embedding quality: The $\texttt{cls}$-token discards crucial
token information about dispersed, localized events in multi-label audio. This
weakness is rooted in the mismatch between the pretraining objective (operating
globally) and the downstream task (localized events). Across a comprehensive
benchmark of 13 datasets and 6 spectrogram-based encoders, we first investigate
the global pooling bottleneck. We then introduce binarized prototypical probes:
a lightweight and simple pooling method that learns prototypes to perform
class-wise information aggregation. Despite its simplicity, our method notably
outperforms linear and attentive probing. Our work establishes probing as a
competitive and efficient paradigm for evaluating audio SSL models, challenging
the reliance on costly fine-tuning.

</details>


### [73] [The Shape of Surprise: Structured Uncertainty and Co-Creativity in AI Music Tools](https://arxiv.org/abs/2509.25028)
*Eric Browne*

Main category: cs.SD

TL;DR: 本文通过结构化不确定性的概念，对当代AI音乐系统中随机性的应用进行了主题性回顾，分析了六个系统如何约束随机过程以保持音乐连贯性和用户控制。


<details>
  <summary>Details</summary>
Motivation: 随机性在计算音乐创作中具有矛盾作用：既能激发新颖性，又可能导致不连贯。本文旨在探讨AI音乐系统如何通过结构化不确定性来平衡随机性和音乐连贯性。

Method: 采用比较分析方法，研究六个AI音乐系统（Musika、MIDI-DDSP、Melody RNN、RAVE、Wekinator、Somax 2）中随机性和不确定性的设计模式。

Result: 识别出支持音乐连贯性、用户控制和共同创作的设计模式，为设计师和艺术家提供了实用见解。

Conclusion: 这是首个通过结构化不确定性视角审视AI音乐中随机性的主题性回顾，为支持表达性、协作性或即兴交互提供了实践指导。

Abstract: Randomness plays a pivotal yet paradoxical role in computational music
creativity: it can spark novelty, but unchecked chance risks incoherence. This
paper presents a thematic review of contemporary AI music systems, examining
how designers incorporate randomness and uncertainty into creative practice. I
draw on the concept of structured uncertainty to analyse how stochastic
processes are constrained within musical and interactive frameworks. Through a
comparative analysis of six systems - Musika (Pasini and Schl\"uter, 2022),
MIDI-DDSP (Wu et al., 2021), Melody RNN (Magenta Project), RAVE (Caillon and
Esling, 2021), Wekinator (Fiebrink and Cook, 2010), and Somax 2 (Borg, 2019) -
we identify recurring design patterns that support musical coherence, user
control, and co-creativity. To my knowledge, this is the first thematic review
examining randomness in AI music through structured uncertainty, offering
practical insights for designers and artists aiming to support expressive,
collaborative, or improvisational interactions.

</details>


### [74] [Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation](https://arxiv.org/abs/2509.19812)
*Yang Cui,Peter Pan,Lei He,Sheng Zhao*

Main category: cs.SD

TL;DR: 提出PKDMark方法，通过渐进式知识蒸馏将高性能教师模型的能力转移到轻量级学生模型，在保持高鲁棒性和不可感知性的同时，将计算成本降低93.6%。


<details>
  <summary>Details</summary>
Motivation: 随着语音生成模型的快速发展，未经授权的语音克隆带来了严重的隐私和安全风险。现有语音水印技术中，DSP方法效率高但易受攻击，深度学习方法鲁棒性好但计算成本高。

Method: 采用两阶段方法：首先训练基于可逆神经网络的高性能教师模型，然后通过渐进式知识蒸馏将教师能力转移到紧凑的学生模型中。

Result: 蒸馏后的模型在高级失真条件下平均检测F1分数达到99.6%，PESQ为4.30，计算成本降低93.6%。

Conclusion: PKDMark实现了高效语音水印，适用于实时语音合成应用，在保持高性能的同时显著降低了计算开销。

Abstract: With the rapid advancement of speech generative models, unauthorized voice
cloning poses significant privacy and security risks. Speech watermarking
offers a viable solution for tracing sources and preventing misuse. Current
watermarking technologies fall mainly into two categories: DSP-based methods
and deep learning-based methods. DSP-based methods are efficient but vulnerable
to attacks, whereas deep learning-based methods offer robust protection at the
expense of significantly higher computational cost. To improve the
computational efficiency and enhance the robustness, we propose PKDMark, a
lightweight deep learning-based speech watermarking method that leverages
progressive knowledge distillation (PKD). Our approach proceeds in two stages:
(1) training a high-performance teacher model using an invertible neural
network-based architecture, and (2) transferring the teacher's capabilities to
a compact student model through progressive knowledge distillation. This
process reduces computational costs by 93.6% while maintaining high level of
robust performance and imperceptibility. Experimental results demonstrate that
our distilled model achieves an average detection F1 score of 99.6% with a PESQ
of 4.30 in advanced distortions, enabling efficient speech watermarking for
real-time speech synthesis applications.

</details>


### [75] [MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech](https://arxiv.org/abs/2509.25131)
*Chengyao Wang,Zhisheng Zhong,Bohao Peng,Senqiao Yang,Yuqi Liu,Haokun Gui,Bin Xia,Jingyao Li,Bei Yu,Jiaya Jia*

Main category: cs.SD

TL;DR: MGM-Omni是一个统一的多模态大语言模型，采用"大脑-嘴巴"双轨架构，实现多模态理解和长时程语音生成，支持流式语音生成和零样本语音克隆。


<details>
  <summary>Details</summary>
Motivation: 解决传统级联管道中语音合成孤立的问题，实现高效跨模态交互和低延迟流式语音生成，同时支持长时程音频感知和个性化语音生成。

Method: 采用双轨令牌架构，分离多模态推理和实时语音生成；使用统一训练策略和双音频编码器设计；采用基于块的并行解码方案缩小文本-语音令牌率差距。

Result: 在保持音色一致性、生成自然上下文感知语音、长时程音频和多模态理解方面优于现有开源模型，训练数据效率显著更高。

Conclusion: MGM-Omni建立了一个高效端到端的范式，用于多模态理解和可控个性化长时程语音生成。

Abstract: We present MGM-Omni, a unified Omni LLM for omni-modal understanding and
expressive, long-horizon speech generation. Unlike cascaded pipelines that
isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a
dual-track, token-based architecture that cleanly decouples multimodal
reasoning from real-time speech generation. This design enables efficient
cross-modal interaction and low-latency, streaming speech generation. For
understanding, a unified training strategy coupled with a dual audio encoder
design enables long-form audio perception across diverse acoustic conditions.
For generation, a chunk-based parallel decoding scheme narrows the text speech
token-rate gap, accelerating inference and supporting streaming zero-shot voice
cloning with stable timbre over extended durations. Compared to concurrent
work, MGM-Omni achieves these capabilities with markedly data-efficient
training. Extensive experiments demonstrate that MGM-Omni outperforms existing
open source models in preserving timbre identity across extended sequences,
producing natural and context-aware speech, and achieving superior long-form
audio and omnimodal understanding. MGM-Omni establishes an efficient,
end-to-end paradigm for omnimodal understanding and controllable, personalised
long-horizon speech generation.

</details>

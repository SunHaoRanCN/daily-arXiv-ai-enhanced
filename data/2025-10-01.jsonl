{"id": "2509.25275", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.25275", "abs": "https://arxiv.org/abs/2509.25275", "authors": ["Chi Zhang", "Zehua Chen", "Kaiwen Zheng", "Jun Zhu"], "title": "VoiceBridge: Designing Latent Bridge Models for General Speech Restoration at Scale", "comment": null, "summary": "Bridge models have recently been explored for speech enhancement tasks such\nas denoising, dereverberation, and super-resolution, while these efforts are\ntypically confined to a single task or small-scale datasets, with constrained\ngeneral speech restoration (GSR) capability at scale. In this work, we\nintroduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs),\ncapable of reconstructing high-fidelity speech at full-band (\\textit{i.e.,}\n48~kHz) from various distortions. By compressing speech waveform into\ncontinuous latent representations, VoiceBridge models the~\\textit{diverse\nLQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\\textit{a\nsingle latent-to-latent generative process} backed by a scalable transformer\narchitecture. To better inherit the advantages of bridge models from the data\ndomain to the latent space, we present an energy-preserving variational\nautoencoder, enhancing the alignment between the waveform and latent space over\nvarying energy levels. Furthermore, to address the difficulty of HQ\nreconstruction from distinctively different LQ priors, we propose a joint\nneural prior, uniformly alleviating the reconstruction burden of LBM. At last,\nconsidering the key requirement of GSR systems, human perceptual quality, a\nperceptually aware fine-tuning stage is designed to mitigate the cascading\nmismatch in generation while improving perceptual alignment. Extensive\nvalidation across in-domain and out-of-domain tasks and datasets\n(\\textit{e.g.}, refining recent zero-shot speech and podcast generation\nresults) demonstrates the superior performance of VoiceBridge. Demo samples can\nbe visited at: https://VoiceBridge-demo.github.io/."}
{"id": "2509.25296", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.25296", "abs": "https://arxiv.org/abs/2509.25296", "authors": ["Balthazar Bujard", "Jérôme Nika", "Fédéric Bevilacqua", "Nicolas Obin"], "title": "Learning Relationships Between Separate Audio Tracks for Creative Applications", "comment": null, "summary": "This paper presents the first step in a research project situated within the\nfield of musical agents. The objective is to achieve, through training, the\ntuning of the desired musical relationship between a live musical input and a\nreal-time generated musical output, through the curation of a database of\nseparated tracks. We propose an architecture integrating a symbolic decision\nmodule capable of learning and exploiting musical relationships from such\nmusical corpus. We detail an offline implementation of this architecture\nemploying Transformers as the decision module, associated with a perception\nmodule based on Wav2Vec 2.0, and concatenative synthesis as audio renderer. We\npresent a quantitative evaluation of the decision module's ability to reproduce\nlearned relationships extracted during training. We demonstrate that our\ndecision module can predict a coherent track B when conditioned by its\ncorresponding ''guide'' track A, based on a corpus of paired tracks (A, B)."}
{"id": "2509.25495", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25495", "abs": "https://arxiv.org/abs/2509.25495", "authors": ["Jiacheng Shi", "Hongfei Du", "Y. Alicia Hong", "Ye Gao"], "title": "EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for Speech Emotion Recognition", "comment": null, "summary": "Speech emotion recognition (SER) with audio-language models (ALMs) remains\nvulnerable to distribution shifts at test time, leading to performance\ndegradation in out-of-domain scenarios. Test-time adaptation (TTA) provides a\npromising solution but often relies on gradient-based updates or prompt tuning,\nlimiting flexibility and practicality. We propose Emo-TTA, a lightweight,\ntraining-free adaptation framework that incrementally updates class-conditional\nstatistics via an Expectation-Maximization procedure for explicit test-time\ndistribution estimation, using ALM predictions as priors. Emo-TTA operates on\nindividual test samples without modifying model weights. Experiments on six\nout-of-domain SER benchmarks show consistent accuracy improvements over prior\nTTA baselines, demonstrating the effectiveness of statistical adaptation in\naligning model predictions with evolving test distributions."}
{"id": "2509.25670", "categories": ["cs.SD", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25670", "abs": "https://arxiv.org/abs/2509.25670", "authors": ["Kang Yang", "Yifan Liang", "Fangkun Liu", "Zhenping Xie", "Chengshi Zheng"], "title": "LTA-L2S: Lexical Tone-Aware Lip-to-Speech Synthesis for Mandarin with Cross-Lingual Transfer Learning", "comment": "Submitted to ICASSP 2026", "summary": "Lip-to-speech (L2S) synthesis for Mandarin is a significant challenge,\nhindered by complex viseme-to-phoneme mappings and the critical role of lexical\ntones in intelligibility. To address this issue, we propose Lexical Tone-Aware\nLip-to-Speech (LTA-L2S). To tackle viseme-to-phoneme complexity, our model\nadapts an English pre-trained audio-visual self-supervised learning (SSL) model\nvia a cross-lingual transfer learning strategy. This strategy not only\ntransfers universal knowledge learned from extensive English data to the\nMandarin domain but also circumvents the prohibitive cost of training such a\nmodel from scratch. To specifically model lexical tones and enhance\nintelligibility, we further employ a flow-matching model to generate the F0\ncontour. This generation process is guided by ASR-fine-tuned SSL speech units,\nwhich contain crucial suprasegmental information. The overall speech quality is\nthen elevated through a two-stage training paradigm, where a flow-matching\npostnet refines the coarse spectrogram from the first stage. Extensive\nexperiments demonstrate that LTA-L2S significantly outperforms existing methods\nin both speech intelligibility and tonal accuracy."}
{"id": "2509.25982", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.25982", "abs": "https://arxiv.org/abs/2509.25982", "authors": ["Alina Mannanova", "Jakob Kienegger", "Timo Gerkmann"], "title": "An Analysis of Joint Nonlinear Spatial Filtering for Spatial Aliasing Reduction", "comment": "Submitted to ICASSP 2026. This work has been submitted to the IEEE\n  for possible publication", "summary": "The performance of traditional linear spatial filters for speech enhancement\nis constrained by the physical size and number of channels of microphone\narrays. For instance, for large microphone distances and high frequencies,\nspatial aliasing may occur, leading to unwanted enhancement of signals from\nnon-target directions. Recently, it has been proposed to replace linear\nbeamformers by nonlinear deep neural networks for joint spatial-spectral\nprocessing. While it has been shown that such approaches result in higher\nperformance in terms of instrumental quality metrics, in this work we highlight\ntheir ability to efficiently handle spatial aliasing. In particular, we show\nthat joint spatial and tempo-spectral processing is more robust to spatial\naliasing than traditional approaches that perform spatial processing alone or\nseparately with tempo-spectral filtering. The results provide another strong\nmotivation for using deep nonlinear networks in multichannel speech\nenhancement, beyond their known benefits in managing non-Gaussian noise and\nmultiple speakers, especially when microphone arrays with rather large\nmicrophone distances are used."}
{"id": "2509.25277", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25277", "abs": "https://arxiv.org/abs/2509.25277", "authors": ["Kumar Sai Bondada", "Hiten Kothari", "Yibin Liang", "Daniel J. Jakubisin", "R. Michael Buehrer"], "title": "Experimental Demonstration of Robust Distributed Wireless Clock Synchronization", "comment": null, "summary": "Distributed wireless clock synchronization is essential for aligning the\nclocks of distributed transceivers in support of joint transmission and\nreception techniques. One recently explored method involves synchronizing\ndistributed transceivers using a two-tone waveform, where the tones are\nseparated in frequency by a clock (frequency) reference signal. Prior research\nhas demonstrated frequency accuracy better than 1 Hz; however, this approach\nremains vulnerable to both intentional and unintentional interference. In this\ndemonstration, we present a robust, frequency-hopped two-tone waveform that\nenables transceivers to extract the reference signal without prior knowledge of\nthe exact frequency at which the tones are transmitted."}
{"id": "2509.25694", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25694", "abs": "https://arxiv.org/abs/2509.25694", "authors": ["Hung-Ying Chu", "Shao-Yu Wei", "Guan-Wei Chen", "Tzu-Wei Hung", "ChengYang Tsai", "Yu-Cheng Lin"], "title": "HNote: Extending YNote with Hexadecimal Encoding for Fine-Tuning LLMs in Music Modeling", "comment": null, "summary": "Recent advances in large language models (LLMs) have created new\nopportunities for symbolic music generation. However, existing formats such as\nMIDI, ABC, and MusicXML are either overly complex or structurally inconsistent,\nlimiting their suitability for token-based learning architectures. To address\nthese challenges, we propose HNote, a novel hexadecimal-based notation system\nextended from YNote, which encodes both pitch and duration within a fixed\n32-unit measure framework. This design ensures alignment, reduces ambiguity,\nand is directly compatible with LLM architectures. We converted 12,300\nJiangnan-style songs generated from traditional folk pieces from YNote into\nHNote, and fine-tuned LLaMA-3.1(8B) using parameter-efficient LoRA.\nExperimental results show that HNote achieves a syntactic correctness rate of\n82.5%, and BLEU and ROUGE evaluations demonstrate strong symbolic and\nstructural similarity, producing stylistically coherent compositions. This\nstudy establishes HNote as an effective framework for integrating LLMs with\ncultural music modeling."}
{"id": "2509.26133", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.26133", "abs": "https://arxiv.org/abs/2509.26133", "authors": ["Jyrki Alakuijala", "Martin Bruse", "Sami Boukortt", "Jozef Marus Coldenhoff", "Milos Cernak"], "title": "Zimtohrli: An Efficient Psychoacoustic Audio Similarity Metric", "comment": "pip install zimtohrli", "summary": "This paper introduces Zimtohrli, a novel, full-reference audio similarity\nmetric designed for efficient and perceptually accurate quality assessment. In\nan era dominated by computationally intensive deep learning models and\nproprietary legacy standards, there is a pressing need for an interpretable,\npsychoacoustically-grounded metric that balances performance with practicality.\nZimtohrli addresses this gap by combining a 128-bin gammatone filterbank\nfront-end, which models the frequency resolution of the cochlea, with a unique\nnon-linear resonator model that mimics the human eardrum's response to acoustic\nstimuli. Similarity is computed by comparing perceptually-mapped spectrograms\nusing modified Dynamic Time Warping (DTW) and Neurogram Similarity Index\nMeasure (NSIM) algorithms, which incorporate novel non-linearities to better\nalign with human judgment. Zimtohrli achieves superior performance to the\nbaseline open-source ViSQOL metric, and significantly narrows the performance\ngap with the latest commercial POLQA metric. It offers a compelling balance of\nperceptual relevance and computational efficiency, positioning it as a strong\nalternative for modern audio engineering applications, from codec development\nto the evaluation of generative audio systems."}
{"id": "2509.25385", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25385", "abs": "https://arxiv.org/abs/2509.25385", "authors": ["Yanan Du", "Sai Xu", "Jagmohan Chauhan"], "title": "A Graph-based Hybrid Beamforming Framework for MIMO Cell-Free ISAC Networks", "comment": null, "summary": "This paper develops a graph-based hybrid beamforming framework for\nmultiple-input multiple-output (MIMO) cell-free integrated sensing and\ncommunication (ISAC) networks. Specifically, we construct a novel MIMO\ncell-free ISAC network model. In this model, multiple dual-function base\nstation (BS) transmitters employ distributed hybrid beamforming to enable\nsimultaneous communication and sensing, while maintaining physical separation\nbetween the transmitters and the radar receiver. Building on this model, we\nformulate a multi-objective optimization problem under a power constraint to\njointly improve communication and sensing performance. To solve it, the problem\nis first reformulated as a single-objective optimization problem. Then, a\ngraph-based method composed of multiple graph neural networks (GNNs) is\ndeveloped to realize hybrid beamforming with either perfect or imperfect\nchannel state information. Once trained, the neural network model can be\ndeployed distributively across BSs, enabling fast and efficient inference. To\nfurther reduce inference latency, a custom field-programmable gate array\n(FPGA)-based accelerator is developed. Numerical simulations validate the\ncommunication and sensing capabilities of the proposed optimization approach,\nwhile experimental evaluations demonstrate remarkable performance gains of\nFPGA-based acceleration in GNN inference."}
{"id": "2509.26007", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2509.26007", "abs": "https://arxiv.org/abs/2509.26007", "authors": ["Eleonora Ristori", "Luca Bindini", "Paolo Frasconi"], "title": "MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms", "comment": null, "summary": "Research on audio generation has progressively shifted from waveform-based\napproaches to spectrogram-based methods, which more naturally capture harmonic\nand temporal structures. At the same time, advances in image synthesis have\nshown that autoregression across scales, rather than tokens, improves coherence\nand detail. Building on these ideas, we introduce MARS (Multi-channel\nAutoRegression on Spectrograms), a framework that treats spectrograms as\nmulti-channel images and employs channel multiplexing (CMX), a reshaping\ntechnique that lowers height and width without discarding information. A shared\ntokenizer provides consistent discrete representations across scales, enabling\na transformer-based autoregressor to refine spectrograms from coarse to fine\nresolutions efficiently. Experiments on a large-scale dataset demonstrate that\nMARS performs comparably or better than state-of-the-art baselines across\nmultiple evaluation metrics, establishing an efficient and scalable paradigm\nfor high-fidelity audio generation."}
{"id": "2509.26329", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.26329", "abs": "https://arxiv.org/abs/2509.26329", "authors": ["Yi-Cheng Lin", "Yu-Hua Chen", "Jia-Kai Dong", "Yueh-Hsuan Huang", "Szu-Chi Chen", "Yu-Chen Chen", "Chih-Yao Chen", "Yu-Jung Lin", "Yu-Ling Chen", "Zih-Yu Chen", "I-Ning Tsai", "Hsiu-Hsuan Wang", "Ho-Lam Chung", "Ke-Han Lu", "Hung-yi Lee"], "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics", "comment": "5 pages; submitted to ICASSP 2026", "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream."}
{"id": "2509.25497", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25497", "abs": "https://arxiv.org/abs/2509.25497", "authors": ["Duc Tung Bui", "Le-Nam Tran"], "title": "A closed-loop $2\\times4$ downlink MIMO Framework for 5G New Radio using OpenAirInterface", "comment": "The paper has been accepted by IEEE VTC2025Spring", "summary": "We present the first-of-a-kind closed-loop $2\\times4$ MIMO implementation for\nthe downlink of 5G Open RAN using OpenAirInterface (OAI), which is capable of\ntransmitting up to two transmission layers. Our implementation is a fully\nfunctional 5G New Radio (5G NR) system, including the 5G Core Network (5G CN),\n5G Radio Access Network (5G RAN), as well as 5G NR User Equipment (UEs). This\nserves as a foundational framework for further advancements in the context of\nemerging Open RAN (O-RAN) development. A key feature of our implementation is\nthe enhanced Channel State Information (CSI) reporting procedure at the UE,\nwhich includes Rank Indicator (RI), Precoding Matrix Indicator (PMI), and\nChannel Quality Indicator (CQI). It is adjusted for the extended configuration\nto maximize data rates. To demonstrate the performance of our implementation,\nwe measure the downlink data rates using $\\textit{iperf3}$ in two scenarios:\n(i) fixed channels to assess two-layer data transmission and (ii)\n$\\textit{Rice1}$ channels for general transmission analysis. The obtained\nsimulation results demonstrate that, compared to the existing $2\\times2$ MIMO\nconfiguration in the OAI, our implementation improves the data rates in almost\nall scenarios, especially at the high Signal-to-Noise-Ratios (SNRs)."}
{"id": "2509.26140", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26140", "abs": "https://arxiv.org/abs/2509.26140", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models", "comment": null, "summary": "Spatial reasoning is fundamental to auditory perception, yet current audio\nlarge language models (ALLMs) largely rely on unstructured binaural cues and\nsingle step inference. This limits both perceptual accuracy in direction and\ndistance estimation and the capacity for interpretable reasoning. Recent work\nsuch as BAT demonstrates spatial QA with binaural audio, but its reliance on\ncoarse categorical labels (left, right, up, down) and the absence of explicit\ngeometric supervision constrain resolution and robustness. We introduce the\n$\\textbf{Spatial-Acoustic Geometry Encoder (SAGE}$), a geometry-aware audio\nencoder that aligns binaural acoustic features with 3D spatial structure using\npanoramic depth images and room-impulse responses at training time, while\nrequiring only audio at inference. Building on this representation, we present\n$\\textbf{OWL}$, an ALLM that integrates $\\textbf{SAGE}$ with a spatially\ngrounded chain-of-thought to rationalize over direction-of-arrivals (DoA) and\ndistance estimates. Through curriculum learning from perceptual QA to\nmulti-step reasoning, $\\textbf{OWL}$ supports o'clock-level azimuth and DoA\nestimation. To enable large-scale training and evaluation, we construct and\nrelease $\\textbf{BiDepth}$, a dataset of over one million QA pairs combining\nbinaural audio with panoramic depth images and room impulse responses across\nboth in-room and out-of-room scenarios. Across two benchmark datasets, our new\n$\\textbf{BiDepth}$ and the public SpatialSoundQA, $\\textbf{OWL}$ reduces mean\nDoA error by $\\textbf{11$^{\\circ}$}$ through $\\textbf{SAGE}$ and improves\nspatial reasoning QA accuracy by up to $\\textbf{25}$\\% over BAT."}
{"id": "2509.26388", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26388", "abs": "https://arxiv.org/abs/2509.26388", "authors": ["Kai-Wei Chang", "En-Pei Hu", "Chun-Yi Kuan", "Wenze Ren", "Wei-Chih Chen", "Guan-Ting Lin", "Yu Tsao", "Shao-Hua Sun", "Hung-yi Lee", "James Glass"], "title": "Game-Time: Evaluating Temporal Dynamics in Spoken Language Models", "comment": "submitted to ICASSP 2026", "summary": "Conversational Spoken Language Models (SLMs) are emerging as a promising\nparadigm for real-time speech interaction. However, their capacity of temporal\ndynamics, including the ability to manage timing, tempo and simultaneous\nspeaking, remains a critical and unevaluated challenge for conversational\nfluency. To address this gap, we introduce the Game-Time Benchmark, a framework\nto systematically assess these temporal capabilities. Inspired by how humans\nlearn a language through language activities, Game-Time consists of basic\ninstruction-following tasks and advanced tasks with temporal constraints, such\nas tempo adherence and synchronized responses. Our evaluation of diverse SLM\narchitectures reveals a clear performance disparity: while state-of-the-art\nmodels handle basic tasks well, many contemporary systems still struggle with\nfundamental instruction-following. More critically, nearly all models degrade\nsubstantially under temporal constraints, exposing persistent weaknesses in\ntime awareness and full-duplex interaction. The Game-Time Benchmark provides a\nfoundation for guiding future research toward more temporally-aware\nconversational AI. Demos and datasets are available on our project website\nhttps://ga642381.github.io/Game-Time."}
{"id": "2509.25512", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25512", "abs": "https://arxiv.org/abs/2509.25512", "authors": ["Duc Tung Bui", "Le-Nam Tran"], "title": "An Implementation of Multi-User MIMO Downlink for O-RAN 5G New Radio using OpenAirInterface", "comment": "The paper has been accepted by IEEE NFV-SDN2025", "summary": "We present the first implementation of a Multi-User Multiple-Input\nMultiple-Output (MU-MIMO) transmission scheme on the Physical Downlink Shared\nChannel (PDSCH) for 5G Open Radio Access Network (O-RAN) based on\nOpenAirInterface (OAI). Our implementation features a fully functional\nO-RAN-compliant 5G New Radio (5G NR) system, including a 5G Core Network (5G\nCN), a refined 5G RAN, which is split into a Centre Unit (CU) and an\nDistributed Unit (DU), and 5G NR User Equipment (UEs). This implementation\ndemonstrates MU-MIMO performance in the downlink while showcasing the\ndisaggregation capabilities of O-RAN. Specifically, the Base Station (i.e. gNB)\nin our setup is capable of serving two UEs simultaneously over the same\ndownlink Resource Block (RBs). User scheduling is performed based on the\nPrecoding Matrix Indicators (PMIs) reported by the UEs according to the NR\nChannel State Information (CSI) reporting procedure. The system throughput\nperformance is evaluated using $\\textit{iperf}$. The obtained results via\nsimulation and testbed experiments demonstrate that the MU-MIMO scheme achieves\nsignificant downlink throughput gains, particularly in the high\nSignal-to-Noise-Ratio (SNR) regime, while keeping the Block Error Rate (BLER)\nbelow the required threshold of $10^{-1}$ for both UEs."}
{"id": "2509.26177", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26177", "abs": "https://arxiv.org/abs/2509.26177", "authors": ["Luca A. Lanzendörfer", "Florian Grötschla", "Cesare Blaser", "Roger Wattenhofer"], "title": "Benchmarking Diarization Models", "comment": null, "summary": "Speaker diarization is the task of partitioning audio into segments according\nto speaker identity, answering the question of \"who spoke when\" in\nmulti-speaker conversation recordings. While diarization is an essential task\nfor many downstream applications, it remains an unsolved problem. Errors in\ndiarization propagate to downstream systems and cause wide-ranging failures. To\nthis end, we examine exact failure modes by evaluating five state-of-the-art\ndiarization models, across four diarization datasets spanning multiple\nlanguages and acoustic conditions. The evaluation datasets consist of 196.6\nhours of multilingual audio, including English, Mandarin, German, Japanese, and\nSpanish. Overall, we find that PyannoteAI achieves the best performance at\n11.2% DER, while DiariZen provides a competitive open-source alternative at\n13.3% DER. When analyzing failure cases, we find that the primary cause of\ndiarization errors stem from missed speech segments followed by speaker\nconfusion, especially in high-speaker count settings."}
{"id": "2509.26409", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2509.26409", "abs": "https://arxiv.org/abs/2509.26409", "authors": ["Sunghwa Lee", "Jaewon Yu"], "title": "IR-UWB Radar-Based Contactless Silent Speech Recognition with Attention-Enhanced Temporal Convolutional Networks", "comment": "Submitted to IEEE ICCE-Asia 2025", "summary": "Silent speech recognition (SSR) is a technology that recognizes speech\ncontent from non-acoustic speech-related biosignals. This paper utilizes an\nattention-enhanced temporal convolutional network architecture for contactless\nIR-UWB radar-based SSR, leveraging deep learning to learn discriminative\nrepresentations directly from minimally processed radar signals. The\narchitecture integrates temporal convolutions with self-attention and\nsqueeze-and-excitation mechanisms to capture articulatory patterns. Evaluated\non a 50-word recognition task using leave-one-session-out cross-validation, our\napproach achieves an average test accuracy of 91.1\\% compared to 74.0\\% for the\nconventional hand-crafted feature method, demonstrating significant improvement\nthrough end-to-end learning."}
{"id": "2509.25557", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25557", "abs": "https://arxiv.org/abs/2509.25557", "authors": ["Soroush Mesforush", "Murat Bayraktar", "Nuria González-Prelcic"], "title": "Joint UE positioning and distributed sensing in the upper mid-band exploiting virtual apertures", "comment": null, "summary": "Networks exploiting distributed integrated sensing and communication (DISAC)\nnodes can provide enhanced localization and sensing performance, further\nemphasized when operating with large arrays and bandwidths available in the\nupper mid-band (also known as FR3). In this paper, we consider a DISAC system\noperating at FR3 where a single base station (BS) acts as the transmitter and\nseveral vehicular user equipments (UEs) act as the receivers. We tackle the\ndesign of the signal processing chain at the UE side to enable joint UE\npositioning and target localization. The system model exploits a\nmultiple-input-multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) waveform, and incorporates practical effects such as inter-node\ntiming offsets (TOs), extended targets, dense multipath, and realistic uniform\nplanar arrays (UPAs) at both ends. The proposed design includes a multipath\nestimation stage at each UE, clutter removal, a novel clustering and\nassociation scheme, and a final joint estimator of UE positions and target\nlocations. The estimator solves a weighted least squares (WLS) problem to\njointly compute clock offsets and localize UEs and targets. Numerical results\nconsidering two UEs and two targets show that for 80\\% of the cases the target\nlocalization error is below 32cm, while the UE positioning error is below 44cm."}
{"id": "2509.26207", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26207", "abs": "https://arxiv.org/abs/2509.26207", "authors": ["Andrea Diecidue", "Carlo Alberto Barbano", "Piero Fraternali", "Mathieu Fontaine", "Enzo Tartaglione"], "title": "The silence of the weights: an investigation of structural pruning strategies for attention-based audio signal architectures", "comment": null, "summary": "Transformer-based models have become the state of the art across multiple\ndomains, from natural language processing to machine listening, thanks to\nattention mechanisms. However, the attention layers require a large number of\nparameters and high-end hardware for both training and inference. We propose a\nnovel pruning technique targeted explicitly at the attention mechanism, where\nwe decouple the pruning of the four layers in the attention block, namely:\nquery, keys, values and outputs' projection matrices. We also investigate\npruning strategies to prune along the head and channel dimensions, and compare\nthe performance of the Audio Spectrogram Transformer (AST) model under\ndifferent pruning scenarios. Our results show that even by pruning 50\\% of the\nattention parameters we incur in performance degradation of less than 1\\%"}
{"id": "2509.26471", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26471", "abs": "https://arxiv.org/abs/2509.26471", "authors": ["Héctor Delgado", "Giorgio Ramondetti", "Emanuele Dalmasso", "Gennady Karvitsky", "Daniele Colibro", "Haydar Talib"], "title": "On Deepfake Voice Detection -- It's All in the Presentation", "comment": "Submitted to IEEE ICASSP 2026. Paper resources available at\n  https://github.com/CavoloFrattale/deepfake-detection-test-protocol", "summary": "While the technologies empowering malicious audio deepfakes have dramatically\nevolved in recent years due to generative AI advances, the same cannot be said\nof global research into spoofing (deepfake) countermeasures. This paper\nhighlights how current deepfake datasets and research methodologies led to\nsystems that failed to generalize to real world application. The main reason is\ndue to the difference between raw deepfake audio, and deepfake audio that has\nbeen presented through a communication channel, e.g. by phone. We propose a new\nframework for data creation and research methodology, allowing for the\ndevelopment of spoofing countermeasures that would be more effective in\nreal-world scenarios. By following the guidelines outlined here we improved\ndeepfake detection accuracy by 39% in more robust and realistic lab setups, and\nby 57% on a real-world benchmark. We also demonstrate how improvement in\ndatasets would have a bigger impact on deepfake detection accuracy than the\nchoice of larger SOTA models would over smaller models; that is, it would be\nmore important for the scientific community to make greater investment on\ncomprehensive data collection programs than to simply train larger models with\nhigher computational demands."}
{"id": "2509.25656", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25656", "abs": "https://arxiv.org/abs/2509.25656", "authors": ["Yanhua Tan", "Beixiong Zheng", "Yi Fang", "Derrick Wing Kwan Ng", "Rui Zhang", "Jie Xu"], "title": "Rotatable Antenna-Enabled Spectrum Sharing in Cognitive Radio Systems", "comment": "5 pages, 4 figures. Submitted to an lEEE journal for possible\n  publication on September 24, 2025", "summary": "Rotatable antenna (RA) technology has recently drawn significant attention in\nwireless systems owing to its unique ability to exploit additional spatial\ndegrees-of-freedom (DoFs) by dynamically adjusting the three-dimensional (3D)\nboresight direction of each antenna. In this letter, we propose a new\nRA-assisted cognitive radio (CR) system designed to achieve efficient spectrum\nsharing while mitigating interference between primary and secondary\ncommunication links. Specifically, we formulate an optimization problem for the\njoint design of the transmit beamforming and the boresight directions of RAs at\nthe secondary transmitter (ST), aimed at maximizing the received\nsignal-to-interference-plus-noise ratio (SINR) at the secondary receiver (SR),\nwhile satisfying both interference constraint at the primary receiver (PR) and\nthe maximum transmit power constraint at the ST. Although the formulated\nproblem is challenging to solve due to its non-convexity and coupled variables,\nwe develop an efficient algorithm by leveraging alternating optimization (AO)\nand successive convex approximation (SCA) techniques to acquire high-quality\nsolutions. Numerical results demonstrate that the proposed RA-assisted system\nsubstantially outperforms conventional benchmark schemes in spectrum-sharing CR\nsystems, validating RA's capability to simultaneously enhance the communication\nquality at the SR and mitigate interference at the PR."}
{"id": "2509.26291", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26291", "abs": "https://arxiv.org/abs/2509.26291", "authors": ["Alvaro Gonzalez-Jimenez", "Fabian Gröger", "Linda Wermelinger", "Andrin Bürli", "Iason Kastanis", "Simone Lionetti", "Marc Pouly"], "title": "Representation-Based Data Quality Audits for Audio", "comment": null, "summary": "Data quality issues such as off-topic samples, near duplicates, and label\nerrors often limit the performance of audio-based systems. This paper addresses\nthese issues by adapting SelfClean, a representation-to-rank data auditing\nframework, from the image to the audio domain. This approach leverages\nself-supervised audio representations to identify common data quality issues,\ncreating ranked review lists that surface distinct issues within a single,\nunified process. The method is benchmarked on the ESC-50, GTZAN, and a\nproprietary industrial dataset, using both synthetic and naturally occurring\ncorruptions. The results demonstrate that this framework achieves\nstate-of-the-art ranking performance, often outperforming issue-specific\nbaselines and enabling significant annotation savings by efficiently guiding\nhuman review."}
{"id": "2509.26542", "categories": ["eess.AS", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.26542", "abs": "https://arxiv.org/abs/2509.26542", "authors": ["Yueqian Lin", "Zhengmian Hu", "Qinsi Wang", "Yudong Liu", "Hengfan Zhang", "Jayakumar Subramanian", "Nikos Vlassis", "Hai Helen Li", "Yiran Chen"], "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap", "comment": "Code and data available at https://github.com/linyueqian/VERA", "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned."}
{"id": "2509.25675", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25675", "abs": "https://arxiv.org/abs/2509.25675", "authors": ["Haobo Geng", "Yaoyao Li", "Weiping Tong", "Youwei Meng", "Houpu Xiao", "Yicong Liu"], "title": "A Novel Statistical Analysis Method for Radiation Source Classification", "comment": null, "summary": "With the rapid advancement of electronic information technology, the number\nand variety of unknown radiation sources have increased significantly. Some of\nthese sources share common characteristics, which offers the potential to\neffectively address the challenge of identifying unknown radiation sources.\nHowever, research on the classification of radiation sources remains relatively\nlimited. This paper proposes a big data analysis method that combines linear\ndiscriminant analysis (LDA) with a rough neighborhood set (NRS) for radiation\nsource classification, and its effectiveness is validated on the RadioML 2018\ndataset. The results indicate that, under certain constraints, all modulation\ntypes can be categorized into four distinct classes, laying a foundation for\nfurther research on cognitive interference signal cancellation."}
{"id": "2509.26521", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26521", "abs": "https://arxiv.org/abs/2509.26521", "authors": ["Baptiste Hilaire", "Emmanouil Karystinaios", "Gerhard Widmer"], "title": "MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models", "comment": "Accepted at the 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR) 2025", "summary": "Interpretability is essential for deploying deep learning models in symbolic\nmusic analysis, yet most research emphasizes model performance over\nexplanation. To address this, we introduce MUSE-Explainer, a new method that\nhelps reveal how music Graph Neural Network models make decisions by providing\nclear, human-friendly explanations. Our approach generates counterfactual\nexplanations by making small, meaningful changes to musical score graphs that\nalter a model's prediction while ensuring the results remain musically\ncoherent. Unlike existing methods, MUSE-Explainer tailors its explanations to\nthe structure of musical data and avoids unrealistic or confusing outputs. We\nevaluate our method on a music analysis task and show it offers intuitive\ninsights that can be visualized with standard music tools such as Verovio."}
{"id": "2509.25275", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.25275", "abs": "https://arxiv.org/abs/2509.25275", "authors": ["Chi Zhang", "Zehua Chen", "Kaiwen Zheng", "Jun Zhu"], "title": "VoiceBridge: Designing Latent Bridge Models for General Speech Restoration at Scale", "comment": null, "summary": "Bridge models have recently been explored for speech enhancement tasks such\nas denoising, dereverberation, and super-resolution, while these efforts are\ntypically confined to a single task or small-scale datasets, with constrained\ngeneral speech restoration (GSR) capability at scale. In this work, we\nintroduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs),\ncapable of reconstructing high-fidelity speech at full-band (\\textit{i.e.,}\n48~kHz) from various distortions. By compressing speech waveform into\ncontinuous latent representations, VoiceBridge models the~\\textit{diverse\nLQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\\textit{a\nsingle latent-to-latent generative process} backed by a scalable transformer\narchitecture. To better inherit the advantages of bridge models from the data\ndomain to the latent space, we present an energy-preserving variational\nautoencoder, enhancing the alignment between the waveform and latent space over\nvarying energy levels. Furthermore, to address the difficulty of HQ\nreconstruction from distinctively different LQ priors, we propose a joint\nneural prior, uniformly alleviating the reconstruction burden of LBM. At last,\nconsidering the key requirement of GSR systems, human perceptual quality, a\nperceptually aware fine-tuning stage is designed to mitigate the cascading\nmismatch in generation while improving perceptual alignment. Extensive\nvalidation across in-domain and out-of-domain tasks and datasets\n(\\textit{e.g.}, refining recent zero-shot speech and podcast generation\nresults) demonstrates the superior performance of VoiceBridge. Demo samples can\nbe visited at: https://VoiceBridge-demo.github.io/."}
{"id": "2509.25722", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25722", "abs": "https://arxiv.org/abs/2509.25722", "authors": ["Ruibin Chen", "Haozhe Lei", "Hao Guo", "Marco Mezzavilla", "Hitesh Poddar", "Tomoki Yoshimura", "Sundeep Rangan"], "title": "Transformer-Based Rate Prediction for Multi-Band Cellular Handsets", "comment": null, "summary": "Cellular wireless systems are witnessing the proliferation of frequency bands\nover a wide spectrum, particularly with the expansion of new bands in FR3.\nThese bands must be supported in user equipment (UE) handsets with multiple\nantennas in a constrained form factor. Rapid variations in channel quality\nacross the bands from motion and hand blockage, limited field-of-view of\nantennas, and hardware and power-constrained measurement sparsity pose\nsignificant challenges to reliable multi-band channel tracking. This paper\nformulates the problem of predicting achievable rates across multiple antenna\narrays and bands with sparse historical measurements. We propose a\ntransformer-based neural architecture that takes asynchronous rate histories as\ninput and outputs per-array rate predictions. Evaluated on ray-traced\nsimulations in a dense urban micro-cellular setting with FR1 and FR3 arrays,\nour method demonstrates superior performance over baseline predictors, enabling\nmore informed band selection under realistic mobility and hardware constraints."}
{"id": "2509.26580", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26580", "abs": "https://arxiv.org/abs/2509.26580", "authors": ["Luca A. Lanzendörfer", "Constantin Pinkl", "Florian Grötschla"], "title": "Source Separation for A Cappella Music", "comment": null, "summary": "In this work, we study the task of multi-singer separation in a cappella\nmusic, where the number of active singers varies across mixtures. To address\nthis, we use a power set-based data augmentation strategy that expands limited\nmulti-singer datasets into exponentially more training samples. To separate\nsingers, we introduce SepACap, an adaptation of SepReformer, a state-of-the-art\nspeaker separation model architecture. We adapt the model with periodic\nactivations and a composite loss function that remains effective when stems are\nsilent, enabling robust detection and separation. Experiments on the JaCappella\ndataset demonstrate that our approach achieves state-of-the-art performance in\nboth full-ensemble and subset singer separation scenarios, outperforming\nspectrogram-based baselines while generalizing to realistic mixtures with\nvarying numbers of singers."}
{"id": "2509.25296", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.25296", "abs": "https://arxiv.org/abs/2509.25296", "authors": ["Balthazar Bujard", "Jérôme Nika", "Fédéric Bevilacqua", "Nicolas Obin"], "title": "Learning Relationships Between Separate Audio Tracks for Creative Applications", "comment": null, "summary": "This paper presents the first step in a research project situated within the\nfield of musical agents. The objective is to achieve, through training, the\ntuning of the desired musical relationship between a live musical input and a\nreal-time generated musical output, through the curation of a database of\nseparated tracks. We propose an architecture integrating a symbolic decision\nmodule capable of learning and exploiting musical relationships from such\nmusical corpus. We detail an offline implementation of this architecture\nemploying Transformers as the decision module, associated with a perception\nmodule based on Wav2Vec 2.0, and concatenative synthesis as audio renderer. We\npresent a quantitative evaluation of the decision module's ability to reproduce\nlearned relationships extracted during training. We demonstrate that our\ndecision module can predict a coherent track B when conditioned by its\ncorresponding ''guide'' track A, based on a corpus of paired tracks (A, B)."}
{"id": "2509.25732", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25732", "abs": "https://arxiv.org/abs/2509.25732", "authors": ["Chenqing Ji", "Qionghui Liu", "Jiahong Liu", "Chao Yu", "Yifei Sun", "Rui Wang", "Fan Liu"], "title": "Doppler-Based Multistatic Drone Tracking via Cellular Downlink Signals", "comment": null, "summary": "In this paper, a multistatic Doppler sensing system is proposed for the drone\ntracking via downlink Long-Term Evolution (LTE) signals. Specifically, the LTE\nbase stations (BSs) are exploited as signal illuminators, and three passive\nsensing receivers are deployed at different locations to detect the bistatic\nDoppler frequencies of a target drone from received downlink signals. It is\nshown that even without the measurements of BS-drone-receiver range and angle,\nthe Doppler measurements could provide sufficient information for trajectory\ntracking. Particularly, the trajectory of the target drone, consisting of the\ninitial position and velocities of all the time slots, can be reconstructed by\nsolving a minimum mean-squared error problem according to the above Doppler\nmeasurements. It is demonstrated by experiment that although the target drone\nand all the sensing receivers are around 200 meters away from the illuminating\nBSs, the complicated trajectories can be tracked with 90% errors below 90\ncentimeters. Since this accuracy is notably higher than the typical range\nresolution of LTE signals, the demonstration shows that drone trajectory\ntracking with a high accuracy could be feasible solely according to Doppler\ndetection, as long as the deployment density of receivers is sufficiently high."}
{"id": "2509.26329", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.26329", "abs": "https://arxiv.org/abs/2509.26329", "authors": ["Yi-Cheng Lin", "Yu-Hua Chen", "Jia-Kai Dong", "Yueh-Hsuan Huang", "Szu-Chi Chen", "Yu-Chen Chen", "Chih-Yao Chen", "Yu-Jung Lin", "Yu-Ling Chen", "Zih-Yu Chen", "I-Ning Tsai", "Hsiu-Hsuan Wang", "Ho-Lam Chung", "Ke-Han Lu", "Hung-yi Lee"], "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics", "comment": "5 pages; submitted to ICASSP 2026", "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream."}
{"id": "2509.25793", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25793", "abs": "https://arxiv.org/abs/2509.25793", "authors": ["Hao Luo", "Shuaifeng Jiang", "Saeed R. Khosravirad", "Ahmed Alkhateeb"], "title": "Digital Twin Aided Massive MIMO CSI Feedback: Exploring the Impact of Twinning Fidelity", "comment": "16 pages, 15 figures", "summary": "Deep learning (DL) techniques have demonstrated strong performance in\ncompressing and reconstructing channel state information (CSI) while reducing\nfeedback overhead in massive MIMO systems. A key challenge, however, is their\nreliance on extensive site-specific training data, whose real-world collection\nincurs significant overhead and limits scalability across deployment sites. To\naddress this, we propose leveraging site-specific digital twins to assist the\ntraining of DL-based CSI compression models. The digital twin integrates an\nelectromagnetic (EM) 3D model of the environment, a hardware model, and ray\ntracing to produce site-specific synthetic CSI data, allowing DL models to be\ntrained without the need for extensive real-world measurements. We further\ndevelop a fidelity analysis framework that decomposes digital twin quality into\nfour key aspects: 3D geometry, material properties, ray tracing, and hardware\nmodeling. We explore how these factors influence the reliability of the data\nand model performance. To enhance the adaptability to real-world environments,\nwe propose a refinement strategy that incorporates a limited amount of\nreal-world data to fine-tune the DL model pre-trained on the digital twin\ndataset. Evaluation results show that models trained on site-specific digital\ntwins outperform those trained on generic datasets, with the proposed\nrefinement method effectively enhancing performance using limited real-world\ndata. The simulations also highlight the importance of digital twin fidelity,\nespecially in 3D geometry, ray tracing, and hardware modeling, for improving\nCSI reconstruction quality. This analysis framework offers valuable insights\ninto the critical fidelity aspects, and facilitates more efficient digital twin\ndevelopment and deployment strategies for various wireless communication tasks."}
{"id": "2509.26542", "categories": ["eess.AS", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.26542", "abs": "https://arxiv.org/abs/2509.26542", "authors": ["Yueqian Lin", "Zhengmian Hu", "Qinsi Wang", "Yudong Liu", "Hengfan Zhang", "Jayakumar Subramanian", "Nikos Vlassis", "Hai Helen Li", "Yiran Chen"], "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap", "comment": "Code and data available at https://github.com/linyueqian/VERA", "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned."}
{"id": "2509.25854", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25854", "abs": "https://arxiv.org/abs/2509.25854", "authors": ["Hao Zhou", "Yiyan Ma", "Dan Fei", "Weirong Liu", "Zhengyu Zhang", "Mi Yang", "Guoyu Ma", "Yunlong Lu", "Ruisi He", "Guoyu Wang", "Cheng Li", "Zhaohui Song", "Bo Ai"], "title": "Delay-Doppler Domain Channel Measurements and Modeling in High-Speed Railways", "comment": "13 pages, 11 figures", "summary": "As next-generation wireless communication systems need to be able to operate\nin high-frequency bands and high-mobility scenarios, delay-Doppler (DD) domain\nmulticarrier (DDMC) modulation schemes, such as orthogonal time frequency space\n(OTFS), demonstrate superior reliability over orthogonal frequency division\nmultiplexing (OFDM). Accurate DD domain channel modeling is essential for DDMC\nsystem design. However, since traditional channel modeling approaches are\nmainly confined to time, frequency, and space domains, the principles of DD\ndomain channel modeling remain poorly studied. To address this issue, we\npropose a systematic DD domain channel measurement and modeling methodology in\nhigh-speed railway (HSR) scenarios. First, we design a DD domain channel\nmeasurement method based on the long-term evolution for railway (LTE-R) system.\nSecond, for DD domain channel modeling, we investigate quasi-stationary\ninterval, statistical power modeling of multipath components, and particularly,\nthe quasi-invariant intervals of DD domain channel fading coefficients. Third,\nvia LTE-R measurements at 371 km/h, taking the quasi-stationary interval as the\ndecision criterion, we establish DD domain channel models under different\nchannel time-varying conditions in HSR scenarios. Fourth, the accuracy of\nproposed DD domain channel models is validated via bit error rate comparison of\nOTFS transmission. In addition, simulation verifies that in HSR scenario, the\nquasi-invariant interval of DD domain channel fading coefficient is on\nmillisecond (ms) order of magnitude, which is much smaller than the\nquasi-stationary interval length on $100$ ms order of magnitude. This study\ncould provide theoretical guidance for DD domain modeling in high-mobility\nenvironments, supporting future DDMC and integrated sensing and communication\ndesigns for 6G and beyond."}
{"id": "2509.25931", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25931", "abs": "https://arxiv.org/abs/2509.25931", "authors": ["Oksana Moryakova", "Håkan Johansson"], "title": "Closed-Form Least-Squares Design of Fast-Convolution Based Variable-Bandwidth FIR Filters", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a closed-form least-squares (LS) design approach for\nfast-convolution (FC) based variable-bandwidth (VBW) finite-impulse-response\n(FIR) filters. The proposed LS design utilizes frequency sampling and the VBW\nfilter frequency-domain implementation using the overlap-save (OLS) method,\nthat together offer significant savings in implementation and online bandwidth\nreconfiguration complexities. Since combining frequency-domain design and OLS\nimplementation leads to a linear periodic time-varying (LPTV) behavior of the\nVBW filter, a set of the corresponding time-invariant impulse responses is\nconsidered in the proposed design. Through numerical examples, it is\ndemonstrated that the proposed approach enables not only closed-form design of\nFC-based VBW filters with substantial complexity reductions compared to\nexisting solutions for a given performance, but also allows the variable\nbandwidth range to be extended without any increase in complexity. Moreover, a\nway of reducing the maximum approximation error energy over the whole set of\nthe time-invariant filters of the LPTV system is shown by introducing\nappropriate weighting functions in the design."}
{"id": "2509.25937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25937", "abs": "https://arxiv.org/abs/2509.25937", "authors": ["Chandan Kumar Sheemar", "Jorge Querol", "Wali Ullah Khan", "Prabhu Thiruvasagam", "Sourabh Solanki", "Idir Edjekouane", "Alejandro Gonzalez-Garrido", "Mohammed Al-Ansi", "Carla E. Garcia", "Symeon Chatzinotas"], "title": "Joint Communications, Sensing, and Positioning in 6G Multi-Functional Satellite Systems: Survey and Open Challenges", "comment": null, "summary": "Satellite systems are expected to be a cornerstone of sixth-generation (6G)\nnetworks, providing ubiquitous coverage and supporting a wide range of services\nacross communications, sensing, and positioning, navigation, and timing (PNT).\nMeeting these demands with current function-specific payload architectures is\nchallenging in terms of cost, spectral use, and sustainability. This survey\nintroduces the framework of multi-functional satellite systems (MFSS), which\nintegrate two or more of these core services into a single payload, enabling\nresource sharing and functional synergy. A unified taxonomy is proposed,\ncovering joint communications and sensing (JCAS), joint communications and PNT\n(JCAP), joint sensing and PNT (JSAP), and fully integrated joint\ncommunications, sensing, and PNT (JCSAP) systems. The paper reviews the\nstate-of-the-art in each domain, examines existing payload architectures, and\noutlines cooperative, integrated, and joint design strategies. Key challenges,\nincluding waveform co-design, synchronization, interference mitigation, and\nresource management, are discussed, along with potential solutions and future\nresearch directions. By unifying diverse satellite capabilities within a single\nplatform, MFSS can achieve higher spectral efficiency, reduced launch mass and\ncost, improved energy use, and enhanced service versatility, contributing to\nthe development of sustainable and intelligent non-terrestrial networks (NTNs)\nfor the 6G and beyond space era."}
{"id": "2509.25959", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25959", "abs": "https://arxiv.org/abs/2509.25959", "authors": ["Minxing Sun", "Li Miao", "Qingyu Shen", "Yao Mao", "Qiliang Bao"], "title": "Neural Network State-Space Estimators", "comment": "12 pages, 24 figures. Code and data available at\n  github.com/ShineMinxing/PaperNNSSE.git", "summary": "Classical state estimation algorithms rely on predefined target's state-space\nmodel, which complicates model derivation and limits adaptability when system\ndynamics change. Neural network based estimators offer a data-driven\nalternative, but rarely fuse classical estimation theory into their structure\nand demand large, pre-computed training sets. To overcome these limitations, we\npropose a unified state-space structure without target's state-space model and\ntreats both the input-layer activations and all network weights as latent\nstates to be estimated online. We instantiate this nonlinear model with three\ncanonical estimators-the extended Kalman estimator, the unscented Kalman\nestimator, and the particle estimator to simulate different neural network and\ndemonstrate its generality. We then benchmark our approach against seven\nleading neural network estimators across three representative scenarios.\nResults show that our neural network state-space estimators not only retain the\nrobust learning capability, but also match or exceed the accuracy of both\nclassical and pre-trained neural network methods. Code, data, and more result:\ngithub.com/ShineMinxing/PaperNNSSE.git"}
{"id": "2509.26018", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26018", "abs": "https://arxiv.org/abs/2509.26018", "authors": ["Jaewon Yu", "Pyo-Woong Son"], "title": "Performance Evaluation of eLoran Spatial ASF Corrections Based on Measured ASF Map", "comment": "Submitted to ICCE-Asia 2025", "summary": "This paper analyzes the effectiveness of spatial ASF correction methods in\nthe Korean eLoran system using measured ASF maps. Three correction scenarios\nwere evaluated under identical simulation settings: no correction (S0), local\ncorrection using true ASF values (S1), and wide-area correction using a single\nreference station value (S2). Simulation results show that S1 consistently\nachieved the lowest positioning errors, while S0 exhibited the largest errors\nwith extensive high-error regions. S2 provided limited improvements near the\nreference station but degraded with increasing distance and ASF spatial\ngradients. The findings highlight that local ASF correction significantly\nimproves eLoran positioning performance, whereas wide-area correction has only\nlocalized benefits."}
{"id": "2509.26020", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26020", "abs": "https://arxiv.org/abs/2509.26020", "authors": ["Junwoo Song", "Pyo-woong Son"], "title": "Path-Based Correlation Analysis of Meteorological Factors and eLoran Signal Delay Variations", "comment": "Submitted to ICCE-Asia 2025", "summary": "Unlike GNSS, which is vulnerable to jamming and spoofing due to its\ninherently weak received power, eLoran exhibits robustness owing to its high\nfield strength. Therefore, the eLoran system can maintain reliable operation\neven in scenarios where GNSS becomes unavailable. However, since eLoran signals\npropagate through ground waves, the propagation delay is susceptible to changes\nin surface conditions, including both terrain and meteorological variations.\nThis study aims to analyze the correlation between the temporal variations in\neLoran signal propagation delay and meteorological factors at various points\nalong the signal path."}
{"id": "2509.26067", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26067", "abs": "https://arxiv.org/abs/2509.26067", "authors": ["S. Fatemeh Bozorgi", "S. Mohammad Razavizadeh", "Mohsen Rezaee"], "title": "Enhancing Connectivity for Emergency Vehicles Through UAV Trajectory and Resource Allocation Optimization", "comment": "This is the author's accepted manuscript. The final version of record\n  is published in Physical Communication, DOI:\n  https://doi.org/10.1016/j.phycom.2025.102826", "summary": "Effective communication for emergency vehicles - such as ambulances and fire\ntrucks - is essential to support their operations in various traffic and\nenvironmental conditions. In this context, this paper investigates a vehicular\ncommunication system assisted by an Unmanned Aerial Vehicle (UAV), which\nadjusts its trajectory and resource allocation according to communication\nneeds. The system classifies vehicles into two groups to address their varying\nservice requirements: emergency vehicles, which require a minimum instantaneous\ndata rate to access critical information timely, and normal vehicles. To\nsupport both categories effectively, this paper proposes a joint optimization\napproach that coordinates UAV trajectory planning and Dynamic Bandwidth\nAllocation (DBA). The objective is to maximize the minimum average data rate\nfor normal vehicles while ensuring that emergency vehicles maintain an\ninstantaneous rate above a predefined threshold. This approach takes into\naccount some system constraints, including UAV propulsion power consumption,\nmobility limitations, and backhaul capacity. To tackle the resulting non-convex\nproblem, an iterative optimization method is employed, where the original\nproblem is decomposed into two subproblems: bandwidth allocation and UAV\ntrajectory design. In each iteration, the trajectory subproblem is solved using\nthe Successive Convex Approximation (SCA) method. Numerical results confirm\nthat the proposed solution achieves superior performance in meeting service\nrequirements compared to baseline methods."}
{"id": "2509.26249", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26249", "abs": "https://arxiv.org/abs/2509.26249", "authors": ["Ali Khandan Boroujeni", "Hyeon Seok Rou", "Ghazal Bagheri", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Stefan Köpsell", "Rafael F. Schaefer"], "title": "Secrecy-Driven Beamforming for Multi-User Integrated Sensing and Communication", "comment": "Submitted to an IEEE conference", "summary": "This paper proposes a secure integrated sensing and communications (ISAC)\nframework for multi-user systems with multiple communication users (CUs) and\nadversarial targets, where the design problem is formulated to maximize secrecy\nrate under joint sensing and communication constraints. An efficient solution\nis presented based on an accelerated fractional programming method using a\nnon-homogeneous complex quadratic transform (QT), which decomposes the problem\ninto tractable subproblems for beamforming and artificial noise (AN)\noptimization. Unlike conventional artificial noise strategies, the proposed\napproach also exploits AN to enhance sensing while avoiding interference with\nlegitimate users. Simulation results show significant gains in secrecy rate,\ncommunication reliability, and sensing accuracy, confirming the effectiveness\nand scalability of the proposed framework."}
{"id": "2509.26286", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26286", "abs": "https://arxiv.org/abs/2509.26286", "authors": ["Jiaming Zhang", "Jiajun He", "Jie Zhang", "Okan Yurduseven"], "title": "FinGAN: An Interpretable RSS Generation Network for Scalable Fingerprint Localization", "comment": null, "summary": "This work introduces FinGAN, a robust received signal strength (RSS) data\ngenerator designed to expand RSS fingerprint datasets. Compared to existing\ngenerative adversarial models that either rely on known reference positions\n(RPs) or depend on predefined priors, FinGAN learns the latent information\nbetween RPs and RSS values by maximizing the mutual information between the\ngenerated RSS data and the RPs, enabling an end-to-end RSS generation directly\nfrom RPs. This allows us to accurately generate RSS data for previously\nunmeasured RPs. Both quantitative and qualitative evaluations demonstrate that\nFinGAN produces synthetic RSS data closely aligned with real RSS sample\ncollected from the on-site experiment, preserving localization performance\ncomparable to that achieved with complete real-world datasets. To further\nvalidate its generalizability, FinGAN is also trained and evaluated on\nopen-source datasets from three typical office environments,and the results\ndemonstrate consistent performance across different scenarios."}
{"id": "2509.26311", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26311", "abs": "https://arxiv.org/abs/2509.26311", "authors": ["Hassaan Hashmi", "Spyridon Pougkakiotis", "Dionysis Kalogerias"], "title": "Ultra-Reliable Risk-Aggregated Sum Rate Maximization via Model-Aided Deep Learning", "comment": null, "summary": "We consider the problem of maximizing weighted sum rate in a multiple-input\nsingle-output (MISO) downlink wireless network with emphasis on user rate\nreliability. We introduce a novel risk-aggregated formulation of the complex\nWSR maximization problem, which utilizes the Conditional Value-at-Risk (CVaR)\nas a functional for enforcing rate (ultra)-reliability over channel fading\nuncertainty/risk. We establish a WMMSE-like equivalence between the proposed\nprecoding problem and a weighted risk-averse MSE problem, enabling us to design\na tailored unfolded graph neural network (GNN) policy function approximation\n(PFA), named {\\alpha}-Robust Graph Neural Network ({\\alpha}RGNN), trained to\nmaximize lower-tail (CVaR) rates resulting from adverse wireless channel\nrealizations (e.g., deep fading, attenuation). We empirically demonstrate that\na trained {\\alpha}RGNN fully eliminates per user deep rate fades, and\nsubstantially and optimally reduces statistical user rate variability while\nretaining adequate ergodic performance."}
{"id": "2509.26333", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26333", "abs": "https://arxiv.org/abs/2509.26333", "authors": ["Kexin Chen", "Yijie Mao", "Wonjae Shin"], "title": "Transmitter-Side Beyond-Diagonal RIS-Enabled Integrated Sensing and Communications", "comment": null, "summary": "Beyond diagonal reconfigurable intelligent surfaces (BD-RIS) have emerged as\na promising technology for 6G wireless networks, offering more advanced control\nover electromagnetic wave propagation than conventional diagonal RIS. This\npaper proposes a novel integrated sensing and communication (ISAC) framework\nthat incorporates BD-RIS at the transmitter. This not only opens the door to\nenhanced sensing and communication performance, but also alleviates the need\nfor large-scale fully digital radio frequency (RF) chains at the transmitter.\nBased on the proposed system model, we formulate a normalized weighted\noptimization problem to jointly design the active beamforming and the BD-RIS\nscattering matrix with the aim of jointly minimizing the trace of the\nCram\\'er-Rao bound (CRB) for sensing targets and maximizing the sum rate (SR)\nfor communication users. To address this highly coupled optimization problem,\nwe propose a novel and low-complexity iterative algorithm that efficiently\nsolves the active beamforming and scattering matrix subproblems by transforming\neach into a series of tractable projection problems with closed-form solutions.\nNumerical results show the appealing capability of the transmitter-side\nBD-RIS-aided ISAC over conventional diagonal RIS-aided ISAC in enhancing both\nsensing and communication performance. Moreover, compared to the classic\niterative algorithm, the proposed algorithm offers enhanced dual-functional\nperformance while significantly reducing the computational complexity."}
{"id": "2509.26356", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26356", "abs": "https://arxiv.org/abs/2509.26356", "authors": ["Yifeng Zhang", "Xiao Liang"], "title": "A Physics-Informed Multi-Source Domain Adaptation Framework for Label-Free Post-Earthquake Damage Assessment", "comment": null, "summary": "Efficient and intelligent assessment of post-earthquake structural damage is\ncritical for rapid disaster response. While data-driven approaches have shown\npromise, traditional supervised learning methods rely on extensive labeled\ndatasets, which are often impractical to obtain for damaged structures. To\naddress this limitation, we propose a physics-informed multi-source domain\nadaptation framework to predict post-earthquake structural damage for a target\nbuilding without requiring damage labels. The multi-source domain integrates\nactual damage data and numerical modeling data from buildings similar to the\ntarget structure. The framework operates through three key steps. First, the\nsimilarity of key physics from each domain are analyzed to form a weight\nmatrix, which enhances domain differentiation. Second, features from the\nmulti-source and target domains are extracted and fed into a classifier and a\ndiscriminator. The classifier ensures that the features are damage-sensitive\nand accurately assign damage states, while the discriminator enforces that the\nfeatures remain domain-invariant. Finally, the key parameters matrix is applied\nas weights during adversarial training to optimize the contribution of features\nfrom each source domain. The proposed framework provides a robust solution for\nassessing structural damage in scenarios where labeled data is scarce,\nsignificantly advancing the capabilities of post-earthquake damage evaluation."}
{"id": "2509.26395", "categories": ["eess.SP", "math.AP", "physics.bio-ph", "physics.class-ph", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.26395", "abs": "https://arxiv.org/abs/2509.26395", "authors": ["Ugo Boscain", "Xiangyu Ma", "Dario Prandi", "Giuseppina Turco"], "title": "A solution to the mystery of the sub-harmonic series and to the combination tone via a linear mathematical model of the cochlea", "comment": null, "summary": "In this paper, we study a simple linear model of the cochlea as a set of\nvibrating strings. We make hypothesis that the information sent to the auditory\ncortex is the energy stored in the strings and consider all oscillation modes\nof the strings. We show the emergence of the sub-harmonic series whose\nexistence was hypothesized in the XVI century to explain the consonance of the\nminor chord. We additionally show how the nonlinearity of the energy can be\nused to study the emergence of the combination tone (Tartini's third sound)\nshedding new light on this long debated subject."}
{"id": "2509.26500", "categories": ["eess.SP", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.26500", "abs": "https://arxiv.org/abs/2509.26500", "authors": ["Hossein Nasiri", "Muhammad Iqbal Rochman", "Monisha Ghosh"], "title": "Indoor/Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers", "comment": "To be published in the proceedings of IEEE Military Communications\n  Conference (MILCOM) 2025", "summary": "The desirability of the mid-band frequency range (1 - 10 GHz) for federal and\ncommercial applications, combined with the growing applications for commercial\nindoor use-cases, such as factory automation, opens up a new approach to\nspectrum sharing: the same frequency bands used outdoors by federal incumbents\ncan be reused by commercial indoor users. A recent example of such sharing,\nbetween commercial systems, is the 6 GHz band (5.925 - 7.125 GHz) where\nunlicensed, low-power-indoor (LPI) users share the band with outdoor\nincumbents, primarily fixed microwave links. However, to date, there exist no\nreliable, automatic means of determining whether a device is indoors or\noutdoors, necessitating the use of other mechanisms such as mandating indoor\naccess points (APs) to have integrated antennas and not be battery powered, and\nreducing transmit power of client devices which may be outdoors. An accurate\nindoor/outdoor (I/O) classification addresses these challenges, enabling\nautomatic transmit power adjustments without interfering with incumbents. To\nthis end, we leverage the Global Navigation Satellite System (GNSS) signals for\nI/O classification. GNSS signals, designed inherently for outdoor reception and\nhighly susceptible to indoor attenuation and blocking, provide a robust and\ndistinguishing feature for environmental sensing. We develop various\nmethodologies, including threshold-based techniques and machine learning\napproaches and evaluate them using an expanded dataset gathered from diverse\ngeographical locations. Our results demonstrate that GNSS-based methods alone\ncan achieve greater accuracy than approaches relying solely on wireless (Wi-Fi)\ndata, particularly in unfamiliar locations. Furthermore, the integration of\nGNSS data with Wi-Fi information leads to improved classification accuracy,\nshowcasing the significant benefits of multi-modal data fusion."}
{"id": "2509.26508", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26508", "abs": "https://arxiv.org/abs/2509.26508", "authors": ["Charlotte Muth", "Benedikt Geiger", "Daniel Gil Gaviria", "Laurent Schmalen"], "title": "Neural Network-Based Single-Carrier Joint Communication and Sensing: Loss Design, Constellation Shaping and Precoding", "comment": "Published in IEEE Access Source code available at\n  https://github.com/frozenhairdryer/JCAS-loss-shape-precode", "summary": "We investigate the impact of higher-order modulation formats on the sensing\nperformance of single-carrier joint communication and sensing (JCAS) systems.\nSeveral separate components such as a beamformer, a modulator, a target\ndetector, an angle of arrival (AoA) estimator and a communication demapper are\nimplemented as trainable neural networks (NNs). We compare geometrically shaped\nmodulation formats to a classical quadrature amplitude modulation (QAM) scheme.\nWe assess the influence of multi-snapshot sensing and varying signal-to-noise\nratio (SNR) on the overall performance of the autoencoder-based system. To\nimprove the training behavior of the system, we decouple the loss functions\nfrom the respective SNR values and the number of sensing snapshots, using upper\nbounds of the sensing and communication performance, namely the Cram\\'er-Rao\nbound for AoA estimation and the mutual information for communication. The\nNN-based sensing outperforms classical algorithms, such as a Neyman-Pearson\nbased power detector for object detection and ESPRIT for AoA estimation for\nboth the trained constellations and QAM at low SNRs. We show that the gap in\nsensing performance between classical and shaped modulation formats can be\nsignificantly reduced through multi-snapshot sensing. Lastly, we demonstrate\nsystem extension to multi-user multiple-input multiple-output to address the\nimprovement of spatial efficiency when servicing multiple user equipments. Our\ncontribution emphasizes the importance of estimation bounds for training neural\nnetworks, especially when the trained solutions are deployed in varying SNR\nconditions."}
{"id": "2509.26572", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2509.26572", "abs": "https://arxiv.org/abs/2509.26572", "authors": ["Abdelhamid Salem", "Hao Xu", "Kai-Kit Wong", "Chan-Byoung Chae", "Yangyang Zhang"], "title": "Secure ISAC with Fluid Antenna Systems: Joint Precoding and Port Selection", "comment": null, "summary": "This paper presents a novel framework for enhancing physical-layer security\nin integrated sensing and communication (ISAC) systems by leveraging the\nreconfigurability of fluid antenna systems (FAS). We propose a joint precoding\nand port selection (JPPS) strategy that maximizes the sum secrecy rate while\nsimultaneously ensuring reliable radar sensing. The problem is formulated using\nfractional programming (FP) and solved through an iterative algorithm that\nintegrates FP transformations with successive convex approximation (SCA). To\nreduce computational complexity, we further develop low-complexity schemes\nbased on zero-forcing (ZF) precoding, combined with greedy port selection and\ntrace-inverse minimization. Simulation results demonstrate substantial\nimprovements in both secrecy performance and sensing accuracy compared to\nconventional baselines, across a wide range of FAS ports, user loads, and\nsensing targets. These findings highlight the critical importance of FAS\ngeometry optimization in enabling secure and efficient joint\ncommunication-sensing for next-generation wireless networks."}
{"id": "2509.26573", "categories": ["eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.26573", "abs": "https://arxiv.org/abs/2509.26573", "authors": ["Vinay Kulkarni", "V. V. Reddy", "Neha Maheshwari"], "title": "Statistical Inference Framework for Extended Target Detection in mmWave Automotive Radar", "comment": "12 pages, 12 figures", "summary": "Millimeter wave (mmWave) radar systems, owing to their large bandwidth,\nprovide fine range resolution that enables the observation of multiple\nscatterers originating from a single automotive target commonly referred to as\nan extended target. Conventional CFAR-based detection algorithms typically\ntreat these scatterers as independent detections, thereby discarding the\nspatial scattering structure intrinsic to the target. To preserve this\nscattering spread, this paper proposes a Range-Doppler (RD) segment framework\ndesigned to encapsulate the typical scattering profile of an automobile. The\nstatistical characterization of the segment is performed using Maximum\nLikelihood Estimation (MLE) and posterior density modeling facilitated through\nGibbs Markov Chain Monte Carlo (MCMC) sampling. A skewness-based test\nstatistic, derived from the estimated statistical model, is introduced for\nbinary hypothesis classification of extended targets. Additionally, the paper\npresents a detection pipeline that incorporates Intersection over Union (IoU)\nand segment centering based on peak response, optimized to work within a single\ndwell. Extensive evaluations using both simulated and real-world datasets\ndemonstrate the effectiveness of the proposed approach, underscoring its\nsuitability for automotive radar applications through improved detection\naccuracy."}

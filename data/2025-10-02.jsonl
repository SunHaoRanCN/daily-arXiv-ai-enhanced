{"id": "2510.00180", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00180", "abs": "https://arxiv.org/abs/2510.00180", "authors": ["Amit Milstein", "Nir Shlezinger", "Boaz Rafaely"], "title": "DiffAU: Diffusion-Based Ambisonics Upscaling", "comment": null, "summary": "Spatial audio enhances immersion by reproducing 3D sound fields, with\nAmbisonics offering a scalable format for this purpose. While first-order\nAmbisonics (FOA) notably facilitates hardware-efficient acquisition and storage\nof sound fields as compared to high-order Ambisonics (HOA), its low spatial\nresolution limits realism, highlighting the need for Ambisonics upscaling (AU)\nas an approach for increasing the order of Ambisonics signals. In this work we\npropose DiffAU, a cascaded AU method that leverages recent developments in\ndiffusion models combined with novel adaptation to spatial audio to generate\n3rd order Ambisonics from FOA. By learning data distributions, DiffAU provides\na principled approach that rapidly and reliably reproduces HOA in various\nsettings. Experiments in anechoic conditions with multiple speakers, show\nstrong objective and perceptual performance."}
{"id": "2510.00218", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00218", "abs": "https://arxiv.org/abs/2510.00218", "authors": ["Rahul Vijaykumar", "Ajan Ahmed", "John Parker", "Dinesh Pendyala", "Aidan Collins", "Stephanie Schuckers", "Masudul H. Imtiaz"], "title": "Descriptor:: Extended-Length Audio Dataset for Synthetic Voice Detection and Speaker Recognition (ELAD-SVDSR)", "comment": null, "summary": "This paper introduces the Extended Length Audio Dataset for Synthetic Voice\nDetection and Speaker Recognition (ELAD SVDSR), a resource specifically\ndesigned to facilitate the creation of high quality deepfakes and support the\ndevelopment of detection systems trained against them. The dataset comprises 45\nminute audio recordings from 36 participants, each reading various newspaper\narticles recorded under controlled conditions and captured via five microphones\nof differing quality. By focusing on extended duration audio, ELAD SVDSR\ncaptures a richer range of speech attributes such as pitch contours, intonation\npatterns, and nuanced delivery enabling models to generate more realistic and\ncoherent synthetic voices. In turn, this approach allows for the creation of\nrobust deepfakes that can serve as challenging examples in datasets used to\ntrain and evaluate synthetic voice detection methods. As part of this effort,\n20 deepfake voices have already been created and added to the dataset to\nshowcase its potential. Anonymized metadata accompanies the dataset on speaker\ndemographics. ELAD SVDSR is expected to spur significant advancements in audio\nforensics, biometric security, and voice authentication systems."}
{"id": "2510.00238", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00238", "abs": "https://arxiv.org/abs/2510.00238", "authors": ["Armin Gerami", "Ramani Duraiswami"], "title": "Room Impulse Response Synthesis via Differentiable Feedback Delay Networks for Efficient Spatial Audio Rendering", "comment": null, "summary": "We introduce a computationally efficient and tunable feedback delay network\n(FDN) architecture for real-time room impulse response (RIR) rendering that\naddresses the computational and latency challenges inherent in traditional\nconvolution and Fourier transform based methods. Our approach directly\noptimizes FDN parameters to match target RIR acoustic and psychoacoustic\nmetrics such as clarity and definition through novel differentiable\nprogramming-based optimization. Our method enables dynamic, real-time\nadjustments of room impulse responses that accommodates listener and source\nmovement. When combined with previous work on representation of head-related\nimpulse responses via infinite impulse responses, an efficient rendering of\nauditory objects is possible when the HRIR and RIR are known. Our method\nproduces renderings with quality similar to convolution with long binaural room\nimpulse response (BRIR) filters, but at a fraction of the computational cost."}
{"id": "2510.00256", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00256", "abs": "https://arxiv.org/abs/2510.00256", "authors": ["Mattes Ohlenbusch", "Christian Rollwage", "Simon Doclo", "Jan Rennies"], "title": "Subjective quality evaluation of personalized own voice reconstruction systems", "comment": "Submitted to Acta Acustica", "summary": "Own voice pickup technology for hearable devices facilitates communication in\nnoisy environments. Own voice reconstruction (OVR) systems enhance the quality\nand intelligibility of the recorded noisy own voice signals. Since disturbances\naffecting the recorded own voice signals depend on individual factors,\npersonalized OVR systems have the potential to outperform generic OVR systems.\nIn this paper, we propose personalizing OVR systems through data augmentation\nand fine-tuning, comparing them to their generic counterparts. We investigate\nthe influence of personalization on speech quality assessed by objective\nmetrics and conduct a subjective listening test to evaluate quality under\nvarious conditions. In addition, we assess the prediction accuracy of the\nobjective metrics by comparing predicted quality with subjectively measured\nquality. Our findings suggest that personalized OVR provides benefits over\ngeneric OVR for some talkers only. Our results also indicate that performance\ncomparisons between systems are not always accurately predicted by objective\nmetrics. In particular, certain disturbances lead to a consistent\noverestimation of quality compared to actual subjective ratings."}
{"id": "2510.00032", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.00032", "abs": "https://arxiv.org/abs/2510.00032", "authors": ["Ziyi Zeng", "Zhenyang Cai", "Yixi Cai", "Xidong Wang", "Junying Chen", "Rongsheng Wang", "Yipeng Liu", "Siqi Cai", "Benyou Wang", "Zhiguo Zhang", "Haizhou Li"], "title": "WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities", "comment": null, "summary": "Electroencephalography (EEG) interpretation using multimodal large language\nmodels (MLLMs) offers a novel approach for analyzing brain signals. However,\nthe complex nature of brain activity introduces critical challenges: EEG\nsignals simultaneously encode both cognitive processes and intrinsic neural\nstates, creating a mismatch in EEG paired-data modality that hinders effective\ncross-modal representation learning. Through a pivot investigation, we uncover\ncomplementary relationships between these modalities. Leveraging this insight,\nwe propose mapping EEG signals and their corresponding modalities into a\nunified semantic space to achieve generalized interpretation. To fully enable\nconversational capabilities, we further introduce WaveMind-Instruct-338k, the\nfirst cross-task EEG dataset for instruction tuning. The resulting model\ndemonstrates robust classification accuracy while supporting flexible,\nopen-ended conversations across four downstream tasks, thereby offering\nvaluable insights for both neuroscience research and the development of\ngeneral-purpose EEG models."}
{"id": "2510.00006", "categories": ["cs.SD", "cs.CL", "cs.CY", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00006", "abs": "https://arxiv.org/abs/2510.00006", "authors": ["Kajwan Ziaoddini"], "title": "Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches", "comment": null, "summary": "This paper examines how musical symbolism is produced and circulated in\nonline communities by combining content-based music analysis with a lightweight\nnetwork perspective on lyrics. Using a curated corpus of 275 chart-topping\nsongs enriched with audio descriptors (energy, danceability, loudness,\nliveness, valence, acousticness, speechiness, popularity) and full lyric\ntranscripts, we build a reproducible pipeline that (i) quantifies temporal\ntrends in sonic attributes, (ii) models lexical salience and co-occurrence, and\n(iii) profiles mood by genre. We find a decade-long decline in energy (79 ->\n58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and\ndips in 2014-2016 (42) before partially recovering. Correlation analysis shows\nstrong coupling of energy with loudness (r = 0.74) and negative associations\nfor acousticness with both energy (r = -0.54) and loudness (r = -0.51);\ndanceability is largely orthogonal to other features (|r| < 0.20). Lyric\ntokenization (>114k tokens) reveals a pronoun-centric lexicon \"I/you/me/my\" and\na dense co-occurrence structure in which interpersonal address anchors\nmainstream narratives. Mood differs systematically by style: R&B exhibits the\nhighest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70),\nwhereas Latin/Reggaeton is lower (37) despite high danceability. Read through a\nsubcultural identity lens, these patterns suggest the mainstreaming of\npreviously peripheral codes and a commercial preference for relaxed yet\nrhythmically engaging productions that sustain collective participation without\nmaximal intensity. Methodologically, we contribute an integrated\nMIR-plus-network workflow spanning summary statistics, correlation structure,\nlexical co-occurrence matrices, and genre-wise mood profiling that is robust to\nmodality sparsity and suitable for socially aware recommendation or\ncommunity-level diffusion studies."}
{"id": "2510.00313", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00313", "abs": "https://arxiv.org/abs/2510.00313", "authors": ["Tanmay Khandelwal", "Magdalena Fuentes"], "title": "Post-Training Quantization for Audio Diffusion Transformers", "comment": "5 pages, 4 figures, accepted at IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA) 2025", "summary": "Diffusion Transformers (DiTs) enable high-quality audio synthesis but are\noften computationally intensive and require substantial storage, which limits\ntheir practical deployment. In this paper, we present a comprehensive\nevaluation of post-training quantization (PTQ) techniques for audio DiTs,\nanalyzing the trade-offs between static and dynamic quantization schemes. We\nexplore two practical extensions (1) a denoising-timestep-aware smoothing\nmethod that adapts quantization scales per-input-channel and timestep to\nmitigate activation outliers, and (2) a lightweight low-rank adapter\n(LoRA)-based branch derived from singular value decomposition (SVD) to\ncompensate for residual weight errors. Using Stable Audio Open we benchmark\nW8A8 and W4A8 configurations across objective metrics and human perceptual\nratings. Our results show that dynamic quantization preserves fidelity even at\nlower precision, while static methods remain competitive with lower latency.\nOverall, our findings show that low-precision DiTs can retain high-fidelity\ngeneration while reducing memory usage by up to 79%."}
{"id": "2510.00141", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00141", "abs": "https://arxiv.org/abs/2510.00141", "authors": ["Dipankar Shakya", "Naveed A. Abbasi", "Mingjun Ying", "Isha Jariwala", "Jason J. Qin", "Ishaan S. Gupte", "Bridget Meier", "Guanyue Qian", "Daniel Abraham", "Theodore S. Rappaport", "Andreas F. Molisch"], "title": "Standardized Machine-Readable Point-Data Format for Consolidating Wireless Propagation Across Environments, Frequencies, and Institutions", "comment": "6 pages, 4 figures, 4 tables, IEEE MILCOM 2025 conference", "summary": "The necessity of new spectrum for 6G has intensified global interest in radio\npropagation measurements across emerging frequency bands, use cases, and\nantenna types. These measurements are vital for understanding radio channel\nproperties in diverse environments, and involve time-consuming and expensive\ncampaigns. A major challenge for the effective utilization of propagation\nmeasurement data has been the lack of a standardized format for reporting and\narchiving results. Although organizations such as NIST, NGA, and 3GPP have made\ncommendable efforts for data pooling, a unified machine-readable data format\nfor consolidating measurements across different institutions and frequencies\nremains a missing piece in advancing global standardization efforts. This paper\nintroduces a standardized point-data format for radio propagation measurements\nand demonstrates how institutions may merge disparate campaigns into a common\nformat. This data format, alongside an environmental map and a measurement\nsummary metadata table, enables integration of data from disparate sources by\nusing a structured representation of key parameters. Here, we show the efficacy\nof the point-data format standard using data gathered from two independent\nsub-THz urban microcell (UMi) campaigns: 142 GHz measurements at New York\nUniversity (NYU) and 145 GHz measurements at the University of Southern\nCalifornia (USC). A joint path loss analysis using the close-in path loss model\n(1 m ref. distance) yields a refined estimate of the path loss exponent (PLE)\nemploying the proposed standard to pool measurements. Other statistics such as\nRMS delay spread and angular spread are also determined using a joint\npoint-data table. Adopting this simple, unified format will accelerate channel\nmodel development, build multi-institutional datasets, and feed AI/ML\napplications with reliable training data in a common format from many sources."}
{"id": "2510.00030", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00030", "abs": "https://arxiv.org/abs/2510.00030", "authors": ["Chukwuemeka Ugwu", "Oluwafemi Oyeleke"], "title": "Temporal-Aware Iterative Speech Model for Dementia Detection", "comment": null, "summary": "Deep learning systems often struggle with processing long sequences, where\ncomputational complexity can become a bottleneck. Current methods for automated\ndementia detection using speech frequently rely on static, time-agnostic\nfeatures or aggregated linguistic content, lacking the flexibility to model the\nsubtle, progressive deterioration inherent in speech production. These\napproaches often miss the dynamic temporal patterns that are critical early\nindicators of cognitive decline. In this paper, we introduce TAI-Speech, a\nTemporal Aware Iterative framework that dynamically models spontaneous speech\nfor dementia detection. The flexibility of our method is demonstrated through\ntwo key innovations: 1) Optical Flow-inspired Iterative Refinement: By treating\nspectrograms as sequential frames, this component uses a convolutional GRU to\ncapture the fine-grained, frame-to-frame evolution of acoustic features. 2)\nCross-Attention Based Prosodic Alignment: This component dynamically aligns\nspectral features with prosodic patterns, such as pitch and pauses, to create a\nricher representation of speech production deficits linked to functional\ndecline (IADL). TAI-Speech adaptively models the temporal evolution of each\nutterance, enhancing the detection of cognitive markers. Experimental results\non the DementiaBank dataset show that TAI-Speech achieves a strong AUC of 0.839\nand 80.6\\% accuracy, outperforming text-based baselines without relying on ASR.\nOur work provides a more flexible and robust solution for automated cognitive\nassessment, operating directly on the dynamics of raw audio."}
{"id": "2510.00346", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00346", "abs": "https://arxiv.org/abs/2510.00346", "authors": ["Yuanbo Hou", "Zhaoyi Liu", "Xin Shen", "Stephen Roberts"], "title": "Learning Domain-Robust Bioacoustic Representations for Mosquito Species Classification with Contrastive Learning and Distribution Alignment", "comment": null, "summary": "Mosquito Species Classification (MSC) is crucial for vector surveillance and\ndisease control. The collection of mosquito bioacoustic data is often limited\nby mosquito activity seasons and fieldwork. Mosquito recordings across regions,\nhabitats, and laboratories often show non-biological variations from the\nrecording environment, which we refer to as domain features. This study finds\nthat models directly trained on audio recordings with domain features tend to\nrely on domain information rather than the species' acoustic cues for\nidentification, resulting in illusory good performance while actually\nperforming poor cross-domain generalization. To this end, we propose a\nDomain-Robust Bioacoustic Learning (DR-BioL) framework that combines\ncontrastive learning with distribution alignment. Contrastive learning aims to\npromote cohesion within the same species and mitigate inter-domain\ndiscrepancies, and species-conditional distribution alignment further enhances\ncross-domain species representation. Experiments on a multi-domain mosquito\nbioacoustic dataset from diverse environments show that the DR-BioL improves\nthe accuracy and robustness of baselines, highlighting its potential for\nreliable cross-domain MSC in the real world."}
{"id": "2510.00342", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00342", "abs": "https://arxiv.org/abs/2510.00342", "authors": ["Samuel Li", "Ian P. Roberts"], "title": "Site-Specific Beam Learning for Full-Duplex Massive MIMO Wireless Systems", "comment": null, "summary": "Existing beamforming-based full-duplex solutions for multi-antenna wireless\nsystems often rely on explicit estimation of the self-interference channel. The\npilot overhead of such estimation, however, can be prohibitively high in\nmillimeter-wave and massive MIMO systems, thus limiting the practicality of\nexisting solutions, especially in fast-fading conditions. In this work, we\npresent a novel beam learning framework that bypasses explicit\nself-interference channel estimation by designing beam codebooks to efficiently\nobtain implicit channel knowledge that can then be processed by a deep learning\nnetwork to synthesize transmit and receive beams for full-duplex operation.\nSimulation results using ray-tracing illustrate that our proposed technique can\nallow a full-duplex base station to craft serving beams that couple low\nself-interference while delivering high SNR, with 75-97% fewer measurements\nthan would be required for explicit estimation of the self-interference\nchannel."}
{"id": "2510.00052", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00052", "abs": "https://arxiv.org/abs/2510.00052", "authors": ["Anushka Mallick", "Afiya Noorain", "Ashwin Menon", "Ashita Solanki", "Keertan Balaji"], "title": "A Recall-First CNN for Sleep Apnea Screening from Snoring Audio", "comment": null, "summary": "Sleep apnea is a serious sleep-related breathing disorder that is common and\ncan impact health if left untreated. Currently the traditional method for\nscreening and diagnosis is overnight polysomnography. Polysomnography is\nexpensive and takes a lot of time, and is not practical for screening large\ngroups of people. In this paper, we explored a more accessible option, using\nrespiratory audio recordings to spot signs of apnea.We utilized 18 audio\nfiles.The approach involved converting breathing sounds into spectrograms,\nbalancing the dataset by oversampling apnea segments, and applying class\nweights to reduce bias toward the majority class. The model reached a recall of\n90.55 for apnea detection. Intentionally, prioritizing catching apnea events\nover general accuracy. Despite low precision,the high recall suggests potential\nas a low-cost screening tool that could be used at home or in basic clinical\nsetups, potentially helping identify at-risk individuals much earlier."}
{"id": "2510.00771", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00771", "abs": "https://arxiv.org/abs/2510.00771", "authors": ["Woongjib Choi", "Sangmin Lee", "Hyungseob Lim", "Hong-Goo Kang"], "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching", "comment": "Submitted to ICASSP 2026", "summary": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets."}
{"id": "2510.00422", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00422", "abs": "https://arxiv.org/abs/2510.00422", "authors": ["Kleanthis Avramidis", "Myzelle Hughes", "Idan A Blank", "Dani Byrd", "Assal Habibi", "Takfarinas Medani", "Richard M Leahy", "Shrikanth Narayanan"], "title": "A Point Process Model of Skin Conductance Responses in a Stroop Task for Predicting Depression and Suicidal Ideation", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Accurate identification of mental health biomarkers can enable earlier\ndetection and objective assessment of compromised mental well-being. In this\nstudy, we analyze electrodermal activity recorded during an Emotional Stroop\ntask to capture sympathetic arousal dynamics associated with depression and\nsuicidal ideation. We model the timing of skin conductance responses as a point\nprocess whose conditional intensity is modulated by task-based covariates,\nincluding stimulus valence, reaction time, and response accuracy. The resulting\nsubject-specific parameter vector serves as input to a machine learning\nclassifier for distinguishing individuals with and without depression. Our\nresults show that the model parameters encode meaningful physiological\ndifferences associated with depressive symptomatology and yield superior\nclassification performance compared to conventional feature extraction methods."}
{"id": "2510.00264", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00264", "abs": "https://arxiv.org/abs/2510.00264", "authors": ["Yusuf Ziya Isik", "Rafał Łaganowski"], "title": "Low Resource Audio Codec Challenge Baseline Systems", "comment": "Low-Resource Audio Codec Challenge 2025", "summary": "The Low-Resource Audio Codec (LRAC) Challenge aims to advance neural audio\ncoding for deployment in resource-constrained environments. The first edition\nfocuses on low-resource neural speech codecs that must operate reliably under\neveryday noise and reverberation, while satisfying strict constraints on\ncomputational complexity, latency, and bitrate. Track 1 targets transparency\ncodecs, which aim to preserve the perceptual transparency of input speech under\nmild noise and reverberation. Track 2 addresses enhancement codecs, which\ncombine coding and compression with denoising and dereverberation. This paper\npresents the official baseline systems for both tracks in the 2025 LRAC\nChallenge. The baselines are convolutional neural codec models with Residual\nVector Quantization, trained end-to-end using a combination of adversarial and\nreconstruction objectives. We detail the data filtering and augmentation\nstrategies, model architectures, optimization procedures, and checkpoint\nselection criteria."}
{"id": "2510.00914", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00914", "abs": "https://arxiv.org/abs/2510.00914", "authors": ["Sofiane Azzouz", "Pierre-André Vuissoz", "Yves Laprie"], "title": "Reconstruction of the Complete Vocal Tract Contour Through Acoustic to Articulatory Inversion Using Real-Time MRI Data", "comment": null, "summary": "Acoustic to articulatory inversion has often been limited to a small part of\nthe vocal tract because the data are generally EMA (ElectroMagnetic\nArticulography) data requiring sensors to be glued to easily accessible\narticulators. The presented acoustic to articulation model focuses on the\ninversion of the entire vocal tract from the glottis, the complete tongue, the\nvelum, to the lips. It relies on a realtime dynamic MRI database of more than 3\nhours of speech. The data are the denoised speech signal and the automatically\nsegmented articulator contours. Several bidirectional LSTM-based approaches\nhave been used, either inverting each articulator individually or inverting all\narticulators simultaneously. To our knowledge, this is the first complete\ninversion of the vocal tract. The average RMSE precision on the test set is\n1.65 mm to be compared with the pixel size which is 1.62 mm."}
{"id": "2510.00550", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00550", "abs": "https://arxiv.org/abs/2510.00550", "authors": ["Tai Le", "Hau Luu", "Loan Pham-Nguyen", "Hung Viet-Dao", "Duc Nguyen Minh", "Afshan B. Hameed", "Hoang Nguyen", "Liem Thanh Nguyen", "Huy-Dung Han", "Hung Cao"], "title": "Investigation of Using Non-Contact Electrodes for Fetal ECG Monitoring", "comment": null, "summary": "Regular physiological monitoring of maternal and fetal parameters is\nindispensable for ensuring safe outcomes during pregnancy and parturition.\nFetal electrocardiogram (fECG) assessment is crucial to detect fetal distress\nand developmental anomalies. Given challenges of prenatal care due to the lack\nof medical professionals and the limit of accessibility, especially in remote\nand resource-poor areas, we develop a fECG monitoring system using novel\nnon-contact electrodes (NCE) to record the fetal/maternal ECG (f/mECG) signals\nthrough clothes, thereby improving the comfort during measurement. The system\nis designed to be incorporated inside a maternity belt with data acquisition,\ndata transmission module as well as novel NCEs. Thorough characterizations were\ncarried out to evaluate the novel NCE against traditional wet electrodes (i.e.,\nAg/AgCl electrodes), showing comparable performance. A successful {preliminary\npilot feasibility study} conducted with pregnant women (n = 10) between 25 and\n32 weeks of gestation demonstrates the system's performance, usability and\nsafety."}
{"id": "2510.00356", "categories": ["cs.SD", "eess.AS", "I.2.m; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.00356", "abs": "https://arxiv.org/abs/2510.00356", "authors": ["Daniel G. Williams"], "title": "Dereverberation Using Binary Residual Masking with Time-Domain Consistency", "comment": "6 pages, 1 figure", "summary": "Vocal dereverberation remains a challenging task in audio processing,\nparticularly for real-time applications where both accuracy and efficiency are\ncrucial. Traditional deep learning approaches often struggle to suppress\nreverberation without degrading vocal clarity, while recent methods that\njointly predict magnitude and phase have significant computational cost. We\npropose a real-time dereverberation framework based on residual mask prediction\nin the short-time Fourier transform (STFT) domain. A U-Net architecture is\ntrained to estimate a residual reverberation mask that suppresses late\nreflections while preserving direct speech components. A hybrid objective\ncombining binary cross-entropy, residual magnitude reconstruction, and\ntime-domain consistency further encourages both accurate suppression and\nperceptual quality. Together, these components enable low-latency\ndereverberation suitable for real-world speech and singing applications."}
{"id": "2510.00952", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00952", "abs": "https://arxiv.org/abs/2510.00952", "authors": ["Aref Farhadipour", "Shiran Liu", "Masoumeh Chapariniya", "Valeriia Perepelytsia", "Srikanth Madikeri", "Teodora Vukovic", "Volker Dellwo"], "title": "CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation", "comment": "CL-UZH submission for the NIST SRE 2024 Evaluation plan", "summary": "The CL-UZH team submitted one system each for the fixed and open conditions\nof the NIST SRE 2024 challenge. For the closed-set condition, results for the\naudio-only trials were achieved using the X-vector system developed with Kaldi.\nFor the audio-visual results we used only models developed for the visual\nmodality. Two sets of results were submitted for the open-set and closed-set\nconditions, one based on a pretrained model using the VoxBlink2 and VoxCeleb2\ndatasets. An Xvector-based model was trained from scratch using the CTS\nsuperset dataset for the closed set. In addition to the submission of the\nresults of the SRE24 evaluation to the competition website, we talked about the\nperformance of the proposed systems on the SRE24 evaluation in this report."}
{"id": "2510.00562", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00562", "abs": "https://arxiv.org/abs/2510.00562", "authors": ["Shingo Takemoto", "Shunsuke Ono"], "title": "Geometric Spatio-Spectral Total Variation for Hyperspectral Image Denoising and Destriping", "comment": "Submitted to IEEE Open Journal of Signal Processing. The source code\n  is available at\n  https://github.com/MDI-TokyoTech/Geometric-Spatio-Spectral-Total-Variation", "summary": "This article proposes a novel regularization method, named Geometric\nSpatio-Spectral Total Variation (GeoSSTV), for hyperspectral (HS) image\ndenoising and destriping. HS images are inevitably affected by various types of\nnoise due to the measurement equipment and environment. Total Variation\n(TV)-based regularization methods that model the spatio-spectral piecewise\nsmoothness inherent in HS images are promising approaches for HS image\ndenoising and destriping. However, existing TV-based methods are based on\nclassical anisotropic and isotropic TVs, which cause staircase artifacts and\nlack rotation invariance, respectively, making it difficult to accurately\nrecover round structures and oblique edges. To address this issue, GeoSSTV\nintroduces a geometrically consistent formulation of TV that measures\nvariations across all directions in a Euclidean manner. Through this\nformulation, GeoSSTV removes noise while preserving round structures and\noblique edges. Furthermore, we formulate the HS image denoising problem as a\nconstrained convex optimization problem involving GeoSSTV and develop an\nefficient algorithm based on a preconditioned primal-dual splitting method.\nExperimental results on HS images contaminated with mixed noise demonstrate the\nsuperiority of the proposed method over existing approaches."}
{"id": "2510.00395", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00395", "abs": "https://arxiv.org/abs/2510.00395", "authors": ["Jiaye Tan", "Haonan Luo", "Linfeng Song", "Shuaiqi Chen", "Yishan Lyu", "Zian Zhong", "Roujia Wang", "Daniel Jiang", "Haoran Zhang", "Jiaming Bai", "Haoran Cheng", "Q. Vera Liao", "Hao-Wen Dong"], "title": "SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing", "comment": null, "summary": "Low-latency symbolic music generation is essential for real-time\nimprovisation and human-AI co-creation. Existing transformer-based models,\nhowever, face a trade-off between inference speed and musical quality.\nTraditional acceleration techniques such as embedding pooling significantly\ndegrade quality, while recently proposed Byte Pair Encoding (BPE) methods -\nthough effective on single-track piano data - suffer large performance drops in\nmulti-track settings, as revealed by our analysis. We propose\nAttribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's\nstructured symbolic representation, achieving about 30% inference speedup with\nonly a negligible (about 0.4%) quality drop in objective evaluations and slight\nimprovements in subjective listening tests. Our main contributions are (1) the\nfirst systematic study of BPE's generalizability in multi-track symbolic music,\nand (2) the introduction of AS-KVHS for low-latency symbolic music generation.\nBeyond these, we also release SAGE-Music, an open-source benchmark that matches\nor surpasses state-of-the-art models in generation quality."}
{"id": "2510.00982", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00982", "abs": "https://arxiv.org/abs/2510.00982", "authors": ["Emiru Tsunoo", "Hayato Futami", "Yosuke Kashiwagi", "Siddhant Arora", "Shinji Watanabe"], "title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting", "comment": "Accepted for ASRU 2025", "summary": "For streaming speech recognition, a Transformer-based encoder has been widely\nused with block processing. Although many studies addressed improving emission\nlatency of transducers, little work has been explored for improving encoding\nlatency of the block processing. We seek to reduce latency by frequently\nemitting a chunk with a small shift rather than scarce large-chunk emissions,\nresulting in higher computational costs. To efficiently compute with the small\nchunk shift, we propose a new encoder, Spiralformer, tailored for block\nprocessing by combining layer dropping and early exiting. We skip layer\ncomputation in a cyclic manner and shift the computed layer in each block\nspirally, which completes computation for all the layers over the block\nprocessing. Experimentally, we observed that our method achieved 21.6%\nreduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,\ncompared with the baseline with similar computational cost and word error\nrates."}
{"id": "2510.00581", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00581", "abs": "https://arxiv.org/abs/2510.00581", "authors": ["Zhuoran Li", "Zhen Gao", "Boyu Ning", "Zhaocheng Wang"], "title": "Radiation Pattern Reconfigurable FAS-Empowered Interference-Resilient UAV Communication", "comment": "Simulation codes are provided to reproduce the results in this paper:\n  \\href{https://github.com/LiZhuoRan0/2025-JSAC-RadiationPatternReconfigurableAntenna}{https://github.com/LiZhuoRan0}", "summary": "The widespread use of uncrewed aerial vehicles (UAVs) has propelled the\ndevelopment of advanced techniques on countering unauthorized UAV flights.\nHowever, the resistance of legal UAVs to illegal interference remains\nunder-addressed. This paper proposes radiation pattern reconfigurable fluid\nantenna systems (RPR-FAS)-empowered interference-resilient UAV communication\nscheme. This scheme integrates the reconfigurable pixel antenna technology,\nwhich provides each antenna with an adjustable radiation pattern. Therefore,\nRPR-FAS can enhance the angular resolution of a UAV with a limited number of\nantennas, thereby improving spectral efficiency (SE) and interference\nresilience. Specifically, we first design dedicated radiation pattern adapted\nfrom 3GPP-TR-38.901, where the beam direction and half power beamwidth are\ntailored for UAV communications. Furthermore, we propose a low-storage-overhead\northogonal matching pursuit multiple measurement vectors algorithm, which\naccurately estimates the angle-of-arrival (AoA) of the communication link, even\nin the single antenna case. Particularly, by utilizing the Fourier transform to\nthe radiation pattern gain matrix, we design a dimension-reduction technique to\nachieve 1--2 order-of-magnitude reduction in storage requirements. Meanwhile,\nwe propose a maximum likelihood interference AoA estimation method based on the\nlaw of large numbers, so that the SE can be further improved. Finally,\nalternating optimization is employed to obtain the optimal uplink radiation\npattern and combiner, while an exhaustive search is applied to determine the\noptimal downlink pattern, complemented by the water-filling algorithm for\nbeamforming. Comprehensive simulations demonstrate that the proposed schemes\noutperform traditional methods in terms of angular sensing precision and\nspectral efficiency."}
{"id": "2510.00485", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00485", "abs": "https://arxiv.org/abs/2510.00485", "authors": ["Yujia Xiao", "Liumeng Xue", "Lei He", "Xinyi Chen", "Aemon Yat Fei Chiu", "Wenjie Tian", "Shaofei Zhang", "Qiuqiang Kong", "Xinfa Zhu", "Wei Xue", "Tan Lee"], "title": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation", "comment": null, "summary": "Recently, an increasing number of multimodal (text and audio) benchmarks have\nemerged, primarily focusing on evaluating models' understanding capability.\nHowever, exploration into assessing generative capabilities remains limited,\nespecially for open-ended long-form content generation. Significant challenges\nlie in no reference standard answer, no unified evaluation metrics and\nuncontrollable human judgments. In this work, we take podcast-like audio\ngeneration as a starting point and propose PodEval, a comprehensive and\nwell-designed open-source evaluation framework. In this framework: 1) We\nconstruct a real-world podcast dataset spanning diverse topics, serving as a\nreference for human-level creative quality. 2) We introduce a multimodal\nevaluation strategy and decompose the complex task into three dimensions: text,\nspeech and audio, with different evaluation emphasis on \"Content\" and \"Format\".\n3) For each modality, we design corresponding evaluation methods, involving\nboth objective metrics and subjective listening test. We leverage\nrepresentative podcast generation systems (including open-source, close-source,\nand human-made) in our experiments. The results offer in-depth analysis and\ninsights into podcast generation, demonstrating the effectiveness of PodEval in\nevaluating open-ended long-form audio. This project is open-source to\nfacilitate public use: https://github.com/yujxx/PodEval."}
{"id": "2510.01130", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01130", "abs": "https://arxiv.org/abs/2510.01130", "authors": ["Tingting Wang", "Tianrui Wang", "Meng Ge", "Qiquan Zhang", "Xi Shao"], "title": "Learning Time-Graph Frequency Representation for Monaural Speech Enhancement", "comment": "Accepted by IEEE TASLP", "summary": "The Graph Fourier Transform (GFT) has recently demonstrated promising results\nin speech enhancement. However, existing GFT-based speech enhancement\napproaches often employ fixed graph topologies to build the graph Fourier\nbasis, whose the representation lacks the adaptively and flexibility. In\naddition, they suffer from the numerical errors and instability introduced by\nmatrix inversion in GFT based on both Singular Value Decomposition (GFT-SVD)\nand Eigen Vector Decomposition (GFT-EVD). Motivated by these limitations, this\npaper propose a simple yet effective learnable GFT-SVD framework for speech\nenhancement. Specifically, we leverage graph shift operators to construct a\nlearnable graph topology and define a learnable graph Fourier basis by the\nsingular value matrices using 1-D convolution (Conv-1D) neural layer. This\neliminates the need for matrix inversion, thereby avoiding the associated\nnumerical errors and stability problem."}
{"id": "2510.00696", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00696", "abs": "https://arxiv.org/abs/2510.00696", "authors": ["Ferdaous Tarhouni", "Muneer AlZubi", "Mohamed-Slim Alouini"], "title": "Machine Learning-based Path Loss Prediction in Suburban Environment in the Sub-6 GHz Band", "comment": null, "summary": "Accurate path loss (PL) prediction is crucial for successful network\nplanning, antenna design, and performance optimization in wireless\ncommunication systems. Several conventional approaches for PL prediction have\nbeen adopted, but they have been demonstrated to lack flexibility and accuracy.\nIn this work, we investigate the effectiveness of Machine Learning (ML) models\nin predicting PL, particularly for the sub-6 GHz band in a suburban campus of\nKing Abdullah University of Science and Technology (KAUST). For training\npurposes, we generate synthetic datasets using the ray-tracing simulation\ntechnique. The feasibility and accuracy of the ML-based PL models are verified\nand validated using both synthetic and measurement datasets. The random forest\nregression (RFR) and the K-nearest neighbors (KNN) algorithms provide the best\nPL prediction accuracy compared to other ML models. In addition, we compare the\nperformance of the developed ML-based PL models with the traditional\npropagation models, including COST-231 Hata, Longley-Rice, and Close-in models.\nThe results show the superiority of the ML-based PL models compared to\nconventional models. Therefore, the ML approach using the ray-tracing technique\ncan provide a promising and cost-effective solution for predicting and modeling\nradio wave propagation in various scenarios in a flexible manner."}
{"id": "2510.00522", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00522", "abs": "https://arxiv.org/abs/2510.00522", "authors": ["Md. Abdur Rahman", "Selvarajah Thuseethan", "Kheng Cher Yeo", "Reem E. Mohamed", "Sami Azam"], "title": "ARIONet: An Advanced Self-supervised Contrastive Representation Network for Birdsong Classification and Future Frame Prediction", "comment": null, "summary": "Automated birdsong classification is essential for advancing ecological\nmonitoring and biodiversity studies. Despite recent progress, existing methods\noften depend heavily on labeled data, use limited feature representations, and\noverlook temporal dynamics essential for accurate species identification. In\nthis work, we propose a self-supervised contrastive network, ARIONet (Acoustic\nRepresentation for Interframe Objective Network), that jointly optimizes\ncontrastive classification and future frame prediction using augmented audio\nrepresentations. The model simultaneously integrates multiple complementary\naudio features within a transformer-based encoder model. Our framework is\ndesigned with two key objectives: (1) to learn discriminative species-specific\nrepresentations for contrastive learning through maximizing similarity between\naugmented views of the same audio segment while pushing apart different\nsamples, and (2) to model temporal dynamics by predicting future audio frames,\nboth without requiring large-scale annotations. We validate our framework on\nfour diverse birdsong datasets, including the British Birdsong Dataset, Bird\nSong Dataset, and two extended Xeno-Canto subsets (A-M and N-Z). Our method\nconsistently outperforms existing baselines and achieves classification\naccuracies of 98.41%, 93.07%, 91.89%, and 91.58%, and F1-scores of 97.84%,\n94.10%, 91.29%, and 90.94%, respectively. Furthermore, it demonstrates low mean\nabsolute errors and high cosine similarity, up to 95%, in future frame\nprediction tasks. Extensive experiments further confirm the effectiveness of\nour self-supervised learning strategy in capturing complex acoustic patterns\nand temporal dependencies, as well as its potential for real-world\napplicability in ecological conservation and monitoring."}
{"id": "2510.00006", "categories": ["cs.SD", "cs.CL", "cs.CY", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00006", "abs": "https://arxiv.org/abs/2510.00006", "authors": ["Kajwan Ziaoddini"], "title": "Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches", "comment": null, "summary": "This paper examines how musical symbolism is produced and circulated in\nonline communities by combining content-based music analysis with a lightweight\nnetwork perspective on lyrics. Using a curated corpus of 275 chart-topping\nsongs enriched with audio descriptors (energy, danceability, loudness,\nliveness, valence, acousticness, speechiness, popularity) and full lyric\ntranscripts, we build a reproducible pipeline that (i) quantifies temporal\ntrends in sonic attributes, (ii) models lexical salience and co-occurrence, and\n(iii) profiles mood by genre. We find a decade-long decline in energy (79 ->\n58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and\ndips in 2014-2016 (42) before partially recovering. Correlation analysis shows\nstrong coupling of energy with loudness (r = 0.74) and negative associations\nfor acousticness with both energy (r = -0.54) and loudness (r = -0.51);\ndanceability is largely orthogonal to other features (|r| < 0.20). Lyric\ntokenization (>114k tokens) reveals a pronoun-centric lexicon \"I/you/me/my\" and\na dense co-occurrence structure in which interpersonal address anchors\nmainstream narratives. Mood differs systematically by style: R&B exhibits the\nhighest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70),\nwhereas Latin/Reggaeton is lower (37) despite high danceability. Read through a\nsubcultural identity lens, these patterns suggest the mainstreaming of\npreviously peripheral codes and a commercial preference for relaxed yet\nrhythmically engaging productions that sustain collective participation without\nmaximal intensity. Methodologically, we contribute an integrated\nMIR-plus-network workflow spanning summary statistics, correlation structure,\nlexical co-occurrence matrices, and genre-wise mood profiling that is robust to\nmodality sparsity and suitable for socially aware recommendation or\ncommunity-level diffusion studies."}
{"id": "2510.00816", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00816", "abs": "https://arxiv.org/abs/2510.00816", "authors": ["Fernando Moya Caceres", "Akram Al-Hourani", "Saman Atapattu", "Kandeepan Sithamparanathan"], "title": "Null-Shaping for Interference Mitigation in LEO Satellites Under Location Uncertainty", "comment": "6 pages, 8 figures, GLOBECOM 2025", "summary": "Radio frequency interference (RFI) poses a growing challenge to satellite\ncommunications, particularly in uplink channels of Low Earth Orbit (LEO)\nsystems, due to increasing spectrum congestion and uncertainty in the location\nof terrestrial interferers. This paper addresses the impact of RFI source\nposition uncertainty on beamforming-based interference mitigation. First, we\nanalytically characterize how geographic uncertainty in RFI location translates\ninto angular deviation as observed from the satellite. Building on this, we\npropose a robust null-shaping framework to increase resilience in the\ncommunication links by incorporating the probability density function (PDF) of\nthe RFI location uncertainty into the beamforming design via stochastic\noptimization. This allows adaptive shaping of the antenna array's nulling\npattern to enhance interference suppression under uncertainty. Extensive Monte\nCarlo simulations, incorporating realistic satellite orbital dynamics and\nvarious RFI scenarios, demonstrate that the proposed approach achieves\nsignificantly improved mitigation performance compared to conventional\ndeterministic designs."}
{"id": "2510.00626", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00626", "abs": "https://arxiv.org/abs/2510.00626", "authors": ["Chen-An Li", "Tzu-Han Lin", "Hung-yi Lee"], "title": "When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models", "comment": "5 pages; submitted to ICASSP 2026", "summary": "Large audio-language models (LALMs) unify speech and text processing, but\ntheir robustness in noisy real-world settings remains underexplored. We\ninvestigate how irrelevant audio, such as silence, synthetic noise, and\nenvironmental sounds, affects text reasoning tasks where audio is unnecessary.\nAcross three text-based benchmarks, we find that even non-informative audio\nreduces accuracy and increases prediction volatility; the severity of\ninterference scales with longer durations, higher amplitudes, and elevated\ndecoding temperatures. Silence, often assumed neutral, destabilizes outputs as\nstrongly as synthetic noise. While larger models show greater resilience,\nvulnerabilities persist across all evaluated systems. We further test\nmitigation strategies and find that prompting shows limited effectiveness,\nwhereas self-consistency improves stability at the cost of increased\ncomputation. Our results reveal cross-modal interference as a key robustness\nchallenge and highlight the need for efficient fusion strategies that preserve\nreasoning performance in the presence of irrelevant inputs."}
{"id": "2510.00030", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00030", "abs": "https://arxiv.org/abs/2510.00030", "authors": ["Chukwuemeka Ugwu", "Oluwafemi Oyeleke"], "title": "Temporal-Aware Iterative Speech Model for Dementia Detection", "comment": null, "summary": "Deep learning systems often struggle with processing long sequences, where\ncomputational complexity can become a bottleneck. Current methods for automated\ndementia detection using speech frequently rely on static, time-agnostic\nfeatures or aggregated linguistic content, lacking the flexibility to model the\nsubtle, progressive deterioration inherent in speech production. These\napproaches often miss the dynamic temporal patterns that are critical early\nindicators of cognitive decline. In this paper, we introduce TAI-Speech, a\nTemporal Aware Iterative framework that dynamically models spontaneous speech\nfor dementia detection. The flexibility of our method is demonstrated through\ntwo key innovations: 1) Optical Flow-inspired Iterative Refinement: By treating\nspectrograms as sequential frames, this component uses a convolutional GRU to\ncapture the fine-grained, frame-to-frame evolution of acoustic features. 2)\nCross-Attention Based Prosodic Alignment: This component dynamically aligns\nspectral features with prosodic patterns, such as pitch and pauses, to create a\nricher representation of speech production deficits linked to functional\ndecline (IADL). TAI-Speech adaptively models the temporal evolution of each\nutterance, enhancing the detection of cognitive markers. Experimental results\non the DementiaBank dataset show that TAI-Speech achieves a strong AUC of 0.839\nand 80.6\\% accuracy, outperforming text-based baselines without relying on ASR.\nOur work provides a more flexible and robust solution for automated cognitive\nassessment, operating directly on the dynamics of raw audio."}
{"id": "2510.00838", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00838", "abs": "https://arxiv.org/abs/2510.00838", "authors": ["Hasnul Hashim"], "title": "Effectiveness of Reconfigurable Intelligent Surface in Multipath Fading Channel", "comment": "8 pages, 16 figures", "summary": "A method of simulating a single-input single-output reconfigurable\nintelligent surface (RIS) assisted channel is presented using three channel\nblack boxes to represent the direct signal path, the transmit path to the RIS\nand the reflected path from the RIS. The complex coefficients for each channel\nbox is obtained by ray tracing in a scenario with geographic terrain\ninformation that also contains approximate building shapes. The electrical\ncharacteristics of the ground and building walls were also accounted for in the\nray tracing function. Simulations were conducted with reflected rays only and\nreflected rays together with diffracted rays. The received power exhibits\nvariations typical of multipath fading environments. In the best locations, the\nRIS-assisted channel simulation result agrees well with theoretical models, the\nperformance increasing by the RIS size squared as the number of RIS elements is\nincreased. In the simplified theoretical model where the transmitter and\nreceiver are inline and the RIS orthogonal but much closer than the distance\nbetween the former elements, the simulation results also corroborate best\ndeployment close the transmitter or the receiver with a U-shaped drop between\nthem."}
{"id": "2510.00628", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.00628", "abs": "https://arxiv.org/abs/2510.00628", "authors": ["Yu-Xiang Lin", "Chen-An Li", "Sheng-Lun Wei", "Po-Chun Chen", "Hsin-Hsi Chen", "Hung-yi Lee"], "title": "Hearing the Order: Investigating Selection Bias in Large Audio-Language Models", "comment": "The first two authors contributed equally. Submitted to ICASSP 2026", "summary": "Large audio-language models (LALMs) are often used in tasks that involve\nreasoning over ordered options. An open question is whether their predictions\nare influenced by the order of answer choices, which would indicate a form of\nselection bias and undermine their reliability. In this paper, we identify and\nanalyze this problem in LALMs. We demonstrate that no model is immune to this\nbias through extensive experiments on six LALMs across three widely used\nbenchmarks and their spoken counterparts. Shuffling the order of answer options\ncan cause performance fluctuations of up to 24% and even change model rankings,\nraising concerns about the reliability of current evaluation practices. We also\nstudy permutation-based strategies and show that they can mitigate bias in most\ncases. Our work represents the first systematic investigation of this issue in\nLALMs, and we hope it raises awareness and motivates further research in this\ndirection."}
{"id": "2510.00052", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00052", "abs": "https://arxiv.org/abs/2510.00052", "authors": ["Anushka Mallick", "Afiya Noorain", "Ashwin Menon", "Ashita Solanki", "Keertan Balaji"], "title": "A Recall-First CNN for Sleep Apnea Screening from Snoring Audio", "comment": null, "summary": "Sleep apnea is a serious sleep-related breathing disorder that is common and\ncan impact health if left untreated. Currently the traditional method for\nscreening and diagnosis is overnight polysomnography. Polysomnography is\nexpensive and takes a lot of time, and is not practical for screening large\ngroups of people. In this paper, we explored a more accessible option, using\nrespiratory audio recordings to spot signs of apnea.We utilized 18 audio\nfiles.The approach involved converting breathing sounds into spectrograms,\nbalancing the dataset by oversampling apnea segments, and applying class\nweights to reduce bias toward the majority class. The model reached a recall of\n90.55 for apnea detection. Intentionally, prioritizing catching apnea events\nover general accuracy. Despite low precision,the high recall suggests potential\nas a low-cost screening tool that could be used at home or in basic clinical\nsetups, potentially helping identify at-risk individuals much earlier."}
{"id": "2510.00851", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00851", "abs": "https://arxiv.org/abs/2510.00851", "authors": ["Abdelaziz Salama", "Mohammed M. H. Qazzaz", "Zeinab Nezami", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Agentic AI meets Neural Architecture Search: Proactive Traffic Prediction for AI-RAN", "comment": null, "summary": "Next-generation wireless networks require intelligent traffic prediction to\nenable autonomous resource management and handle diverse, dynamic service\ndemands. The Open Radio Access Network (O-RAN) framework provides a promising\nfoundation for embedding machine learning intelligence through its\ndisaggregated architecture and programmable interfaces. This work applies a\nNeural Architecture Search (NAS)-based framework that dynamically selects and\norchestrates efficient Long Short-Term Memory (LSTM) architectures for traffic\nprediction in O-RAN environments. Our approach leverages the O-RAN paradigm by\nseparating architecture optimisation (via non-RT RIC rApps) from real-time\ninference (via near-RT RIC xApps), enabling adaptive model deployment based on\ntraffic conditions and resource constraints. Experimental evaluation across six\nLSTM architectures demonstrates that lightweight models achieve $R^2 \\approx\n0.91$--$0.93$ with high efficiency for regular traffic, while complex models\nreach near-perfect accuracy ($R^2 = 0.989$--$0.996$) during critical scenarios.\nOur NAS-based orchestration achieves a 70-75\\% reduction in computational\ncomplexity compared to static high-performance models, while maintaining high\nprediction accuracy when required, thereby enabling scalable deployment in\nreal-world edge environments."}
{"id": "2510.00639", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00639", "abs": "https://arxiv.org/abs/2510.00639", "authors": ["Bence Mark Halpern", "Tomoki Toda"], "title": "Reference-free automatic speech severity evaluation using acoustic unit language modelling", "comment": "5 pages. Proceedings of the 6th ACM International Conference on\n  Multimedia in Asia Workshops", "summary": "Speech severity evaluation is becoming increasingly important as the economic\nburden of speech disorders grows. Current speech severity models often struggle\nwith generalization, learning dataset-specific acoustic cues rather than\nmeaningful correlates of speech severity. Furthermore, many models require\nreference speech or a transcript, limiting their applicability in ecologically\nvalid scenarios, such as spontaneous speech evaluation. Previous research\nindicated that automatic speech naturalness evaluation scores correlate\nstrongly with severity evaluation scores, leading us to explore a\nreference-free method, SpeechLMScore, which does not rely on pathological\nspeech data. Additionally, we present the NKI-SpeechRT dataset, based on the\nNKI-CCRT dataset, to provide a more comprehensive foundation for speech\nseverity evaluation. This study evaluates whether SpeechLMScore outperforms\ntraditional acoustic feature-based approaches and assesses the performance gap\nbetween reference-free and reference-based models. Moreover, we examine the\nimpact of noise on these models by utilizing subjective noise ratings in the\nNKI-SpeechRT dataset. The results demonstrate that SpeechLMScore is robust to\nnoise and offers superior performance compared to traditional approaches."}
{"id": "2510.00356", "categories": ["cs.SD", "eess.AS", "I.2.m; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.00356", "abs": "https://arxiv.org/abs/2510.00356", "authors": ["Daniel G. Williams"], "title": "Dereverberation Using Binary Residual Masking with Time-Domain Consistency", "comment": "6 pages, 1 figure", "summary": "Vocal dereverberation remains a challenging task in audio processing,\nparticularly for real-time applications where both accuracy and efficiency are\ncrucial. Traditional deep learning approaches often struggle to suppress\nreverberation without degrading vocal clarity, while recent methods that\njointly predict magnitude and phase have significant computational cost. We\npropose a real-time dereverberation framework based on residual mask prediction\nin the short-time Fourier transform (STFT) domain. A U-Net architecture is\ntrained to estimate a residual reverberation mask that suppresses late\nreflections while preserving direct speech components. A hybrid objective\ncombining binary cross-entropy, residual magnitude reconstruction, and\ntime-domain consistency further encourages both accurate suppression and\nperceptual quality. Together, these components enable low-latency\ndereverberation suitable for real-world speech and singing applications."}
{"id": "2510.00896", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00896", "abs": "https://arxiv.org/abs/2510.00896", "authors": ["Romina Garcia Camargo", "Zhiyang Wang", "Alejandro Ribeiro"], "title": "Graph Neural Networks in Large Scale Wireless Communication Networks: Scalability Across Random Geometric Graphs", "comment": null, "summary": "The growing complexity of wireless systems has accelerated the move from\ntraditional methods to learning-based solutions. Graph Neural Networks (GNNs)\nare especially well-suited here, since wireless networks can be naturally\nrepresented as graphs. A key property of GNNs is transferability: models\ntrained on one graph often generalize to much larger graphs with little\nperformance loss. While empirical studies have shown that GNN-based wireless\npolicies transfer effectively, existing theoretical guarantees do not capture\nthis phenomenon. Most works focus on dense graphs where node degrees scale with\nnetwork size, an assumption that fails in wireless systems. In this work, we\nprovide a formal theoretical foundation for transferability on Random Geometric\nGraphs (RGGs), a sparse and widely used model of wireless networks. We further\nvalidate our results through numerical experiments on power allocation, a\nfundamental resource management task."}
{"id": "2510.00657", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00657", "abs": "https://arxiv.org/abs/2510.00657", "authors": ["Bence Mark Halpern", "Thomas B. Tienkamp", "Teja Rebernik", "Rob J. J. H. van Son", "Sebastiaan A. H. J. de Visscher", "Max J. H. Witjes", "Defne Abur", "Tomoki Toda"], "title": "XPPG-PCA: Reference-free automatic speech severity evaluation with principal components", "comment": "14 pages, 4 figures. Author Accepted Manuscript version of the IEEE\n  Selected Topics in Signal Processing with the same title", "summary": "Reliably evaluating the severity of a speech pathology is crucial in\nhealthcare. However, the current reliance on expert evaluations by\nspeech-language pathologists presents several challenges: while their\nassessments are highly skilled, they are also subjective, time-consuming, and\ncostly, which can limit the reproducibility of clinical studies and place a\nstrain on healthcare resources. While automated methods exist, they have\nsignificant drawbacks. Reference-based approaches require transcriptions or\nhealthy speech samples, restricting them to read speech and limiting their\napplicability. Existing reference-free methods are also flawed; supervised\nmodels often learn spurious shortcuts from data, while handcrafted features are\noften unreliable and restricted to specific speech tasks. This paper introduces\nXPPG-PCA (x-vector phonetic posteriorgram principal component analysis), a\nnovel, unsupervised, reference-free method for speech severity evaluation.\nUsing three Dutch oral cancer datasets, we demonstrate that XPPG-PCA performs\ncomparably to, or exceeds established reference-based methods. Our experiments\nconfirm its robustness against data shortcuts and noise, showing its potential\nfor real-world clinical use. Taken together, our results show that XPPG-PCA\nprovides a robust, generalizable solution for the objective assessment of\nspeech pathology, with the potential to significantly improve the efficiency\nand reliability of clinical evaluations across a range of disorders. An\nopen-source implementation is available."}
{"id": "2510.00395", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00395", "abs": "https://arxiv.org/abs/2510.00395", "authors": ["Jiaye Tan", "Haonan Luo", "Linfeng Song", "Shuaiqi Chen", "Yishan Lyu", "Zian Zhong", "Roujia Wang", "Daniel Jiang", "Haoran Zhang", "Jiaming Bai", "Haoran Cheng", "Q. Vera Liao", "Hao-Wen Dong"], "title": "SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing", "comment": null, "summary": "Low-latency symbolic music generation is essential for real-time\nimprovisation and human-AI co-creation. Existing transformer-based models,\nhowever, face a trade-off between inference speed and musical quality.\nTraditional acceleration techniques such as embedding pooling significantly\ndegrade quality, while recently proposed Byte Pair Encoding (BPE) methods -\nthough effective on single-track piano data - suffer large performance drops in\nmulti-track settings, as revealed by our analysis. We propose\nAttribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's\nstructured symbolic representation, achieving about 30% inference speedup with\nonly a negligible (about 0.4%) quality drop in objective evaluations and slight\nimprovements in subjective listening tests. Our main contributions are (1) the\nfirst systematic study of BPE's generalizability in multi-track symbolic music,\nand (2) the introduction of AS-KVHS for low-latency symbolic music generation.\nBeyond these, we also release SAGE-Music, an open-source benchmark that matches\nor surpasses state-of-the-art models in generation quality."}
{"id": "2510.00934", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00934", "abs": "https://arxiv.org/abs/2510.00934", "authors": ["Junwei Ji", "Dongyuan Shi", "Zhengding Luo", "Boxiang Wang", "Ziyi Yang", "Haowen Li", "Woon-Seng Gan"], "title": "A Robust Proactive Communication Strategy for Distributed Active Noise Control Systems", "comment": null, "summary": "Distributed multichannel active noise control (DMCANC) systems assign the\nhigh computational load of conventional centralized algorithms across multiple\nprocessing nodes, leveraging inter-node communication to collaboratively\nsuppress unwanted noise. However, communication overhead can undermine\nalgorithmic stability and degrade overall performance. To address this\nchallenge, we propose a robust communication framework that integrates\nadaptive-fixed-filter switching and the mixed-gradient combination strategy. In\nthis approach, each node independently executes a single-channel filtered\nreference least mean square (FxLMS) algorithm while monitoring real-time noise\nreduction levels. When the current noise reduction performance degrades\ncompared to the previous state, the node halts its adaptive algorithm, switches\nto a fixed filter, and simultaneously initiates a communication request. The\nexchanged information comprises the difference between the current control\nfilter and the filter at the time of the last communication, equivalent to the\naccumulated gradient sum during non-communication intervals. Upon receiving\nneighboring cumulative gradients, the node employs a mixed-gradient combination\nmethod to update its control filter, subsequently reverting to the adaptive\nmode. This proactive communication strategy and adaptive-fixed switching\nmechanism ensure system robustness by mitigating instability risks caused by\ncommunication issues. Simulations demonstrate that the proposed method achieves\nnoise reduction performance comparable to centralized algorithms while\nmaintaining stability under communication constraints, highlighting its\npractical applicability in real-world distributed ANC scenarios."}
{"id": "2510.00743", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00743", "abs": "https://arxiv.org/abs/2510.00743", "authors": ["Yifei Cao", "Changhao Jiang", "Jiabao Zhuang", "Jiajun Sun", "Ming Zhang", "Zhiheng Xi", "Hui Li", "Shihan Dou", "Yuran Wang", "Yunke Zhang", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling", "comment": null, "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment."}
{"id": "2510.00485", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00485", "abs": "https://arxiv.org/abs/2510.00485", "authors": ["Yujia Xiao", "Liumeng Xue", "Lei He", "Xinyi Chen", "Aemon Yat Fei Chiu", "Wenjie Tian", "Shaofei Zhang", "Qiuqiang Kong", "Xinfa Zhu", "Wei Xue", "Tan Lee"], "title": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation", "comment": null, "summary": "Recently, an increasing number of multimodal (text and audio) benchmarks have\nemerged, primarily focusing on evaluating models' understanding capability.\nHowever, exploration into assessing generative capabilities remains limited,\nespecially for open-ended long-form content generation. Significant challenges\nlie in no reference standard answer, no unified evaluation metrics and\nuncontrollable human judgments. In this work, we take podcast-like audio\ngeneration as a starting point and propose PodEval, a comprehensive and\nwell-designed open-source evaluation framework. In this framework: 1) We\nconstruct a real-world podcast dataset spanning diverse topics, serving as a\nreference for human-level creative quality. 2) We introduce a multimodal\nevaluation strategy and decompose the complex task into three dimensions: text,\nspeech and audio, with different evaluation emphasis on \"Content\" and \"Format\".\n3) For each modality, we design corresponding evaluation methods, involving\nboth objective metrics and subjective listening test. We leverage\nrepresentative podcast generation systems (including open-source, close-source,\nand human-made) in our experiments. The results offer in-depth analysis and\ninsights into podcast generation, demonstrating the effectiveness of PodEval in\nevaluating open-ended long-form audio. This project is open-source to\nfacilitate public use: https://github.com/yujxx/PodEval."}
{"id": "2510.00180", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00180", "abs": "https://arxiv.org/abs/2510.00180", "authors": ["Amit Milstein", "Nir Shlezinger", "Boaz Rafaely"], "title": "DiffAU: Diffusion-Based Ambisonics Upscaling", "comment": null, "summary": "Spatial audio enhances immersion by reproducing 3D sound fields, with\nAmbisonics offering a scalable format for this purpose. While first-order\nAmbisonics (FOA) notably facilitates hardware-efficient acquisition and storage\nof sound fields as compared to high-order Ambisonics (HOA), its low spatial\nresolution limits realism, highlighting the need for Ambisonics upscaling (AU)\nas an approach for increasing the order of Ambisonics signals. In this work we\npropose DiffAU, a cascaded AU method that leverages recent developments in\ndiffusion models combined with novel adaptation to spatial audio to generate\n3rd order Ambisonics from FOA. By learning data distributions, DiffAU provides\na principled approach that rapidly and reliably reproduces HOA in various\nsettings. Experiments in anechoic conditions with multiple speakers, show\nstrong objective and perceptual performance."}
{"id": "2510.00981", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00981", "abs": "https://arxiv.org/abs/2510.00981", "authors": ["Jiaqi Li", "Yao Qian", "Yuxuan Hu", "Leying Zhang", "Xiaofei Wang", "Heng Lu", "Manthan Thakker", "Jinyu Li", "Shang Zhao", "Zhizheng Wu"], "title": "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates", "comment": null, "summary": "Neural audio codecs are foundational to speech language models. It is\nexpected to have a low frame rate and decoupled semantic and acoustic\ninformation. A lower frame rate codec can reduce the computational cost of\nspeech language models by shortening the sequence length. Recent studies have\ndeveloped 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs\nremain underexplored. We find that a major challenge for very low frame rate\ntokens is missing semantic information. This paper introduces FlexiCodec to\naddress this limitation. FlexiCodec improves semantic preservation with a\ndynamic frame rate approach and introduces a novel architecture featuring an\nASR feature-assisted dual stream encoding and Transformer bottlenecks. With\ndynamic frame rates, it uses less frames at information-sparse regions through\nadaptively merging semantically similar frames. A dynamic frame rate also\nallows FlexiCodec to support inference-time controllable frame rates between\n3Hz and 12.5Hz. Experiments on 6.25Hz, 8.3Hz and 12.5Hz average frame rates\nconfirm that FlexiCodec excels over baseline systems in semantic information\npreservation and delivers a high audio reconstruction quality. We also validate\nthe effectiveness of FlexiCodec in language model-based TTS. Demos are\navailable at: https://flexicodec.github.io"}
{"id": "2510.00743", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00743", "abs": "https://arxiv.org/abs/2510.00743", "authors": ["Yifei Cao", "Changhao Jiang", "Jiabao Zhuang", "Jiajun Sun", "Ming Zhang", "Zhiheng Xi", "Hui Li", "Shihan Dou", "Yuran Wang", "Yunke Zhang", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling", "comment": null, "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment."}
{"id": "2510.00771", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00771", "abs": "https://arxiv.org/abs/2510.00771", "authors": ["Woongjib Choi", "Sangmin Lee", "Hyungseob Lim", "Hong-Goo Kang"], "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching", "comment": "Submitted to ICASSP 2026", "summary": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets."}
{"id": "2510.01082", "categories": ["cs.SD", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01082", "abs": "https://arxiv.org/abs/2510.01082", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Anomadarshi Barua"], "title": "HVAC-EAR: Eavesdropping Human Speech Using HVAC Systems", "comment": null, "summary": "Pressure sensors are widely integrated into modern Heating, Ventilation and\nAir Conditioning (HVAC) systems. As they are sensitive to acoustic pressure,\nthey can be a source of eavesdropping. This paper introduces HVAC-EAR, which\nreconstructs intelligible speech from low-resolution, noisy pressure data with\ntwo key contributions: (i) We achieve intelligible reconstruction from as low\nas 0.5 kHz sampling rate, surpassing prior work limited to hot word detection,\nby employing a complex-valued conformer with a Complex Unified Attention Block\nto capture phoneme dependencies; (ii) HVAC-EAR mitigates transient HVAC noise\nby reconstructing both magnitude and phase of missing frequencies. For the\nfirst time, evaluations on real-world HVAC deployments show significant\nintelligibility, raising novel privacy concerns."}
{"id": "2510.00934", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.00934", "abs": "https://arxiv.org/abs/2510.00934", "authors": ["Junwei Ji", "Dongyuan Shi", "Zhengding Luo", "Boxiang Wang", "Ziyi Yang", "Haowen Li", "Woon-Seng Gan"], "title": "A Robust Proactive Communication Strategy for Distributed Active Noise Control Systems", "comment": null, "summary": "Distributed multichannel active noise control (DMCANC) systems assign the\nhigh computational load of conventional centralized algorithms across multiple\nprocessing nodes, leveraging inter-node communication to collaboratively\nsuppress unwanted noise. However, communication overhead can undermine\nalgorithmic stability and degrade overall performance. To address this\nchallenge, we propose a robust communication framework that integrates\nadaptive-fixed-filter switching and the mixed-gradient combination strategy. In\nthis approach, each node independently executes a single-channel filtered\nreference least mean square (FxLMS) algorithm while monitoring real-time noise\nreduction levels. When the current noise reduction performance degrades\ncompared to the previous state, the node halts its adaptive algorithm, switches\nto a fixed filter, and simultaneously initiates a communication request. The\nexchanged information comprises the difference between the current control\nfilter and the filter at the time of the last communication, equivalent to the\naccumulated gradient sum during non-communication intervals. Upon receiving\nneighboring cumulative gradients, the node employs a mixed-gradient combination\nmethod to update its control filter, subsequently reverting to the adaptive\nmode. This proactive communication strategy and adaptive-fixed switching\nmechanism ensure system robustness by mitigating instability risks caused by\ncommunication issues. Simulations demonstrate that the proposed method achieves\nnoise reduction performance comparable to centralized algorithms while\nmaintaining stability under communication constraints, highlighting its\npractical applicability in real-world distributed ANC scenarios."}
{"id": "2510.01109", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.01109", "abs": "https://arxiv.org/abs/2510.01109", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "title": "NLDSI-BWE: Non Linear Dynamical Systems-Inspired Multi Resolution Discriminators for Speech Bandwidth Extension", "comment": null, "summary": "In this paper, we design two nonlinear dynamical systems-inspired\ndiscriminators -- the Multi-Scale Recurrence Discriminator (MSRD) and the\nMulti-Resolution Lyapunov Discriminator (MRLD) -- to \\textit{explicitly} model\nthe inherent deterministic chaos of speech. MSRD is designed based on\nRecurrence representations to capture self-similarity dynamics. MRLD is\ndesigned based on Lyapunov exponents to capture nonlinear fluctuations and\nsensitivity to initial conditions. Through extensive design optimization and\nthe use of depthwise-separable convolutions in the discriminators, our\nframework surpasses prior AP-BWE model with a 44x reduction in the\ndiscriminator parameter count \\textbf{($\\sim$ 22M vs $\\sim$ 0.48M)}. To the\nbest of our knowledge, for the first time, this paper demonstrates how BWE can\nbe supervised by the subtle non-linear chaotic physics of voiced sound\nproduction to achieve a significant reduction in the discriminator size."}
{"id": "2510.00180", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00180", "abs": "https://arxiv.org/abs/2510.00180", "authors": ["Amit Milstein", "Nir Shlezinger", "Boaz Rafaely"], "title": "DiffAU: Diffusion-Based Ambisonics Upscaling", "comment": null, "summary": "Spatial audio enhances immersion by reproducing 3D sound fields, with\nAmbisonics offering a scalable format for this purpose. While first-order\nAmbisonics (FOA) notably facilitates hardware-efficient acquisition and storage\nof sound fields as compared to high-order Ambisonics (HOA), its low spatial\nresolution limits realism, highlighting the need for Ambisonics upscaling (AU)\nas an approach for increasing the order of Ambisonics signals. In this work we\npropose DiffAU, a cascaded AU method that leverages recent developments in\ndiffusion models combined with novel adaptation to spatial audio to generate\n3rd order Ambisonics from FOA. By learning data distributions, DiffAU provides\na principled approach that rapidly and reliably reproduces HOA in various\nsettings. Experiments in anechoic conditions with multiple speakers, show\nstrong objective and perceptual performance."}
{"id": "2510.00218", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00218", "abs": "https://arxiv.org/abs/2510.00218", "authors": ["Rahul Vijaykumar", "Ajan Ahmed", "John Parker", "Dinesh Pendyala", "Aidan Collins", "Stephanie Schuckers", "Masudul H. Imtiaz"], "title": "Descriptor:: Extended-Length Audio Dataset for Synthetic Voice Detection and Speaker Recognition (ELAD-SVDSR)", "comment": null, "summary": "This paper introduces the Extended Length Audio Dataset for Synthetic Voice\nDetection and Speaker Recognition (ELAD SVDSR), a resource specifically\ndesigned to facilitate the creation of high quality deepfakes and support the\ndevelopment of detection systems trained against them. The dataset comprises 45\nminute audio recordings from 36 participants, each reading various newspaper\narticles recorded under controlled conditions and captured via five microphones\nof differing quality. By focusing on extended duration audio, ELAD SVDSR\ncaptures a richer range of speech attributes such as pitch contours, intonation\npatterns, and nuanced delivery enabling models to generate more realistic and\ncoherent synthetic voices. In turn, this approach allows for the creation of\nrobust deepfakes that can serve as challenging examples in datasets used to\ntrain and evaluate synthetic voice detection methods. As part of this effort,\n20 deepfake voices have already been created and added to the dataset to\nshowcase its potential. Anonymized metadata accompanies the dataset on speaker\ndemographics. ELAD SVDSR is expected to spur significant advancements in audio\nforensics, biometric security, and voice authentication systems."}
{"id": "2510.00238", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00238", "abs": "https://arxiv.org/abs/2510.00238", "authors": ["Armin Gerami", "Ramani Duraiswami"], "title": "Room Impulse Response Synthesis via Differentiable Feedback Delay Networks for Efficient Spatial Audio Rendering", "comment": null, "summary": "We introduce a computationally efficient and tunable feedback delay network\n(FDN) architecture for real-time room impulse response (RIR) rendering that\naddresses the computational and latency challenges inherent in traditional\nconvolution and Fourier transform based methods. Our approach directly\noptimizes FDN parameters to match target RIR acoustic and psychoacoustic\nmetrics such as clarity and definition through novel differentiable\nprogramming-based optimization. Our method enables dynamic, real-time\nadjustments of room impulse responses that accommodates listener and source\nmovement. When combined with previous work on representation of head-related\nimpulse responses via infinite impulse responses, an efficient rendering of\nauditory objects is possible when the HRIR and RIR are known. Our method\nproduces renderings with quality similar to convolution with long binaural room\nimpulse response (BRIR) filters, but at a fraction of the computational cost."}
{"id": "2510.00256", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00256", "abs": "https://arxiv.org/abs/2510.00256", "authors": ["Mattes Ohlenbusch", "Christian Rollwage", "Simon Doclo", "Jan Rennies"], "title": "Subjective quality evaluation of personalized own voice reconstruction systems", "comment": "Submitted to Acta Acustica", "summary": "Own voice pickup technology for hearable devices facilitates communication in\nnoisy environments. Own voice reconstruction (OVR) systems enhance the quality\nand intelligibility of the recorded noisy own voice signals. Since disturbances\naffecting the recorded own voice signals depend on individual factors,\npersonalized OVR systems have the potential to outperform generic OVR systems.\nIn this paper, we propose personalizing OVR systems through data augmentation\nand fine-tuning, comparing them to their generic counterparts. We investigate\nthe influence of personalization on speech quality assessed by objective\nmetrics and conduct a subjective listening test to evaluate quality under\nvarious conditions. In addition, we assess the prediction accuracy of the\nobjective metrics by comparing predicted quality with subjectively measured\nquality. Our findings suggest that personalized OVR provides benefits over\ngeneric OVR for some talkers only. Our results also indicate that performance\ncomparisons between systems are not always accurately predicted by objective\nmetrics. In particular, certain disturbances lead to a consistent\noverestimation of quality compared to actual subjective ratings."}
{"id": "2510.00313", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00313", "abs": "https://arxiv.org/abs/2510.00313", "authors": ["Tanmay Khandelwal", "Magdalena Fuentes"], "title": "Post-Training Quantization for Audio Diffusion Transformers", "comment": "5 pages, 4 figures, accepted at IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA) 2025", "summary": "Diffusion Transformers (DiTs) enable high-quality audio synthesis but are\noften computationally intensive and require substantial storage, which limits\ntheir practical deployment. In this paper, we present a comprehensive\nevaluation of post-training quantization (PTQ) techniques for audio DiTs,\nanalyzing the trade-offs between static and dynamic quantization schemes. We\nexplore two practical extensions (1) a denoising-timestep-aware smoothing\nmethod that adapts quantization scales per-input-channel and timestep to\nmitigate activation outliers, and (2) a lightweight low-rank adapter\n(LoRA)-based branch derived from singular value decomposition (SVD) to\ncompensate for residual weight errors. Using Stable Audio Open we benchmark\nW8A8 and W4A8 configurations across objective metrics and human perceptual\nratings. Our results show that dynamic quantization preserves fidelity even at\nlower precision, while static methods remain competitive with lower latency.\nOverall, our findings show that low-precision DiTs can retain high-fidelity\ngeneration while reducing memory usage by up to 79%."}
{"id": "2510.00346", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00346", "abs": "https://arxiv.org/abs/2510.00346", "authors": ["Yuanbo Hou", "Zhaoyi Liu", "Xin Shen", "Stephen Roberts"], "title": "Learning Domain-Robust Bioacoustic Representations for Mosquito Species Classification with Contrastive Learning and Distribution Alignment", "comment": null, "summary": "Mosquito Species Classification (MSC) is crucial for vector surveillance and\ndisease control. The collection of mosquito bioacoustic data is often limited\nby mosquito activity seasons and fieldwork. Mosquito recordings across regions,\nhabitats, and laboratories often show non-biological variations from the\nrecording environment, which we refer to as domain features. This study finds\nthat models directly trained on audio recordings with domain features tend to\nrely on domain information rather than the species' acoustic cues for\nidentification, resulting in illusory good performance while actually\nperforming poor cross-domain generalization. To this end, we propose a\nDomain-Robust Bioacoustic Learning (DR-BioL) framework that combines\ncontrastive learning with distribution alignment. Contrastive learning aims to\npromote cohesion within the same species and mitigate inter-domain\ndiscrepancies, and species-conditional distribution alignment further enhances\ncross-domain species representation. Experiments on a multi-domain mosquito\nbioacoustic dataset from diverse environments show that the DR-BioL improves\nthe accuracy and robustness of baselines, highlighting its potential for\nreliable cross-domain MSC in the real world."}
{"id": "2510.00771", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.00771", "abs": "https://arxiv.org/abs/2510.00771", "authors": ["Woongjib Choi", "Sangmin Lee", "Hyungseob Lim", "Hong-Goo Kang"], "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching", "comment": "Submitted to ICASSP 2026", "summary": "In this paper, we present a vocoder-free framework for audio super-resolution\nthat employs a flow matching generative model to capture the conditional\ndistribution of complex-valued spectral coefficients. Unlike conventional\ntwo-stage diffusion-based approaches that predict a mel-spectrogram and then\nrely on a pre-trained neural vocoder to synthesize waveforms, our method\ndirectly reconstructs waveforms via the inverse Short-Time Fourier Transform\n(iSTFT), thereby eliminating the dependence on a separate vocoder. This design\nnot only simplifies end-to-end optimization but also overcomes a critical\nbottleneck of two-stage pipelines, where the final audio quality is\nfundamentally constrained by vocoder performance. Experiments show that our\nmodel consistently produces high-fidelity 48 kHz audio across diverse\nupsampling factors, achieving state-of-the-art performance on both speech and\ngeneral audio datasets."}
{"id": "2510.00952", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00952", "abs": "https://arxiv.org/abs/2510.00952", "authors": ["Aref Farhadipour", "Shiran Liu", "Masoumeh Chapariniya", "Valeriia Perepelytsia", "Srikanth Madikeri", "Teodora Vukovic", "Volker Dellwo"], "title": "CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation", "comment": "CL-UZH submission for the NIST SRE 2024 Evaluation plan", "summary": "The CL-UZH team submitted one system each for the fixed and open conditions\nof the NIST SRE 2024 challenge. For the closed-set condition, results for the\naudio-only trials were achieved using the X-vector system developed with Kaldi.\nFor the audio-visual results we used only models developed for the visual\nmodality. Two sets of results were submitted for the open-set and closed-set\nconditions, one based on a pretrained model using the VoxBlink2 and VoxCeleb2\ndatasets. An Xvector-based model was trained from scratch using the CTS\nsuperset dataset for the closed set. In addition to the submission of the\nresults of the SRE24 evaluation to the competition website, we talked about the\nperformance of the proposed systems on the SRE24 evaluation in this report."}
{"id": "2510.00982", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.00982", "abs": "https://arxiv.org/abs/2510.00982", "authors": ["Emiru Tsunoo", "Hayato Futami", "Yosuke Kashiwagi", "Siddhant Arora", "Shinji Watanabe"], "title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting", "comment": "Accepted for ASRU 2025", "summary": "For streaming speech recognition, a Transformer-based encoder has been widely\nused with block processing. Although many studies addressed improving emission\nlatency of transducers, little work has been explored for improving encoding\nlatency of the block processing. We seek to reduce latency by frequently\nemitting a chunk with a small shift rather than scarce large-chunk emissions,\nresulting in higher computational costs. To efficiently compute with the small\nchunk shift, we propose a new encoder, Spiralformer, tailored for block\nprocessing by combining layer dropping and early exiting. We skip layer\ncomputation in a cyclic manner and shift the computed layer in each block\nspirally, which completes computation for all the layers over the block\nprocessing. Experimentally, we observed that our method achieved 21.6%\nreduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,\ncompared with the baseline with similar computational cost and word error\nrates."}

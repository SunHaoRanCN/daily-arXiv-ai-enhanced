<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [eess.AS](#eess.AS) [Total: 11]
- [cs.SD](#cs.SD) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities](https://arxiv.org/abs/2510.00032)
*Ziyi Zeng,Zhenyang Cai,Yixi Cai,Xidong Wang,Junying Chen,Rongsheng Wang,Yipeng Liu,Siqi Cai,Benyou Wang,Zhiguo Zhang,Haizhou Li*

Main category: eess.SP

TL;DR: 提出了一种使用多模态大语言模型解释脑电图信号的新方法，通过将EEG信号和对应模态映射到统一语义空间来解决跨模态表示学习中的不匹配问题，并引入了首个用于指令调优的跨任务EEG数据集。


<details>
  <summary>Details</summary>
Motivation: 脑电图信号同时编码认知过程和内在神经状态，这种复杂性导致EEG配对数据模态不匹配，阻碍了有效的跨模态表示学习。

Method: 通过枢轴调查发现模态间的互补关系，将EEG信号和对应模态映射到统一语义空间，并引入WaveMind-Instruct-338k数据集进行指令调优。

Result: 所得模型在四个下游任务中展现出稳健的分类准确性，同时支持灵活、开放式的对话能力。

Conclusion: 该方法为神经科学研究和发展通用EEG模型提供了有价值的见解。

Abstract: Electroencephalography (EEG) interpretation using multimodal large language
models (MLLMs) offers a novel approach for analyzing brain signals. However,
the complex nature of brain activity introduces critical challenges: EEG
signals simultaneously encode both cognitive processes and intrinsic neural
states, creating a mismatch in EEG paired-data modality that hinders effective
cross-modal representation learning. Through a pivot investigation, we uncover
complementary relationships between these modalities. Leveraging this insight,
we propose mapping EEG signals and their corresponding modalities into a
unified semantic space to achieve generalized interpretation. To fully enable
conversational capabilities, we further introduce WaveMind-Instruct-338k, the
first cross-task EEG dataset for instruction tuning. The resulting model
demonstrates robust classification accuracy while supporting flexible,
open-ended conversations across four downstream tasks, thereby offering
valuable insights for both neuroscience research and the development of
general-purpose EEG models.

</details>


### [2] [Standardized Machine-Readable Point-Data Format for Consolidating Wireless Propagation Across Environments, Frequencies, and Institutions](https://arxiv.org/abs/2510.00141)
*Dipankar Shakya,Naveed A. Abbasi,Mingjun Ying,Isha Jariwala,Jason J. Qin,Ishaan S. Gupte,Bridget Meier,Guanyue Qian,Daniel Abraham,Theodore S. Rappaport,Andreas F. Molisch*

Main category: eess.SP

TL;DR: 本文提出了一种用于无线电传播测量的标准化点数据格式，旨在解决不同机构和频率测量数据缺乏统一格式的问题，并通过两个独立的亚太赫兹城市微蜂窝测量活动验证了该格式的有效性。


<details>
  <summary>Details</summary>
Motivation: 6G新频谱需求推动了对新兴频段无线电传播测量的研究，但缺乏标准化的数据报告和归档格式阻碍了测量数据的有效利用，尽管NIST、NGA和3GPP等组织已做出努力，但统一的机器可读数据格式仍是全球标准化工作的缺失环节。

Method: 引入标准化的点数据格式，结合环境地图和测量摘要元数据表，使用关键参数的结构化表示来整合来自不同来源的数据，并通过纽约大学142 GHz和南加州大学145 GHz的两个独立亚太赫兹城市微蜂窝测量活动进行验证。

Result: 使用近距离路径损耗模型进行联合路径损耗分析，获得了路径损耗指数的改进估计，同时确定了RMS延迟扩展和角度扩展等统计量，证明了该标准格式在合并不同测量活动数据方面的有效性。

Conclusion: 采用这种简单统一的数据格式将加速信道模型开发，构建多机构数据集，并为AI/ML应用提供来自多个来源的可靠训练数据。

Abstract: The necessity of new spectrum for 6G has intensified global interest in radio
propagation measurements across emerging frequency bands, use cases, and
antenna types. These measurements are vital for understanding radio channel
properties in diverse environments, and involve time-consuming and expensive
campaigns. A major challenge for the effective utilization of propagation
measurement data has been the lack of a standardized format for reporting and
archiving results. Although organizations such as NIST, NGA, and 3GPP have made
commendable efforts for data pooling, a unified machine-readable data format
for consolidating measurements across different institutions and frequencies
remains a missing piece in advancing global standardization efforts. This paper
introduces a standardized point-data format for radio propagation measurements
and demonstrates how institutions may merge disparate campaigns into a common
format. This data format, alongside an environmental map and a measurement
summary metadata table, enables integration of data from disparate sources by
using a structured representation of key parameters. Here, we show the efficacy
of the point-data format standard using data gathered from two independent
sub-THz urban microcell (UMi) campaigns: 142 GHz measurements at New York
University (NYU) and 145 GHz measurements at the University of Southern
California (USC). A joint path loss analysis using the close-in path loss model
(1 m ref. distance) yields a refined estimate of the path loss exponent (PLE)
employing the proposed standard to pool measurements. Other statistics such as
RMS delay spread and angular spread are also determined using a joint
point-data table. Adopting this simple, unified format will accelerate channel
model development, build multi-institutional datasets, and feed AI/ML
applications with reliable training data in a common format from many sources.

</details>


### [3] [Site-Specific Beam Learning for Full-Duplex Massive MIMO Wireless Systems](https://arxiv.org/abs/2510.00342)
*Samuel Li,Ian P. Roberts*

Main category: eess.SP

TL;DR: 提出了一种无需显式自干扰信道估计的波束学习框架，通过设计波束码本获取隐式信道知识，使用深度学习网络合成全双工操作的收发波束。


<details>
  <summary>Details</summary>
Motivation: 现有基于波束成形的全双工解决方案需要显式估计自干扰信道，在毫米波和大规模MIMO系统中导频开销过高，限制了实际应用。

Method: 设计波束码本高效获取隐式信道知识，通过深度学习网络处理这些知识来合成全双工操作的收发波束。

Result: 仿真结果显示，该方法能使全双工基站构建低自干扰高信噪比的服务波束，相比显式自干扰信道估计减少75-97%的测量次数。

Conclusion: 该波束学习框架有效解决了全双工系统中自干扰信道估计的高开销问题，为毫米波和大规模MIMO系统的全双工操作提供了实用解决方案。

Abstract: Existing beamforming-based full-duplex solutions for multi-antenna wireless
systems often rely on explicit estimation of the self-interference channel. The
pilot overhead of such estimation, however, can be prohibitively high in
millimeter-wave and massive MIMO systems, thus limiting the practicality of
existing solutions, especially in fast-fading conditions. In this work, we
present a novel beam learning framework that bypasses explicit
self-interference channel estimation by designing beam codebooks to efficiently
obtain implicit channel knowledge that can then be processed by a deep learning
network to synthesize transmit and receive beams for full-duplex operation.
Simulation results using ray-tracing illustrate that our proposed technique can
allow a full-duplex base station to craft serving beams that couple low
self-interference while delivering high SNR, with 75-97% fewer measurements
than would be required for explicit estimation of the self-interference
channel.

</details>


### [4] [A Point Process Model of Skin Conductance Responses in a Stroop Task for Predicting Depression and Suicidal Ideation](https://arxiv.org/abs/2510.00422)
*Kleanthis Avramidis,Myzelle Hughes,Idan A Blank,Dani Byrd,Assal Habibi,Takfarinas Medani,Richard M Leahy,Shrikanth Narayanan*

Main category: eess.SP

TL;DR: 该研究通过分析情绪Stroop任务中的皮肤电活动，建立点过程模型来识别与抑郁和自杀意念相关的交感神经唤醒动态特征，并用于机器学习分类。


<details>
  <summary>Details</summary>
Motivation: 准确识别心理健康生物标志物可以实现早期检测和客观评估心理健康受损状态，特别是抑郁和自杀意念的识别。

Method: 将皮肤电反应时间建模为点过程，其条件强度受任务相关协变量（刺激效价、反应时间、反应准确性）调节，然后将个体特定参数向量输入机器学习分类器。

Result: 模型参数编码了与抑郁症状相关的有意义的生理差异，相比传统特征提取方法获得了更优的分类性能。

Conclusion: 基于点过程建模的皮肤电活动分析能够有效捕捉抑郁相关的生理特征，为心理健康评估提供了新的客观生物标志物。

Abstract: Accurate identification of mental health biomarkers can enable earlier
detection and objective assessment of compromised mental well-being. In this
study, we analyze electrodermal activity recorded during an Emotional Stroop
task to capture sympathetic arousal dynamics associated with depression and
suicidal ideation. We model the timing of skin conductance responses as a point
process whose conditional intensity is modulated by task-based covariates,
including stimulus valence, reaction time, and response accuracy. The resulting
subject-specific parameter vector serves as input to a machine learning
classifier for distinguishing individuals with and without depression. Our
results show that the model parameters encode meaningful physiological
differences associated with depressive symptomatology and yield superior
classification performance compared to conventional feature extraction methods.

</details>


### [5] [Investigation of Using Non-Contact Electrodes for Fetal ECG Monitoring](https://arxiv.org/abs/2510.00550)
*Tai Le,Hau Luu,Loan Pham-Nguyen,Hung Viet-Dao,Duc Nguyen Minh,Afshan B. Hameed,Hoang Nguyen,Liem Thanh Nguyen,Huy-Dung Han,Hung Cao*

Main category: eess.SP

TL;DR: 开发了一种使用新型非接触电极的胎儿心电图监测系统，可通过衣物记录胎儿/母亲心电信号，提高测量舒适度，并在孕妇中进行了可行性研究。


<details>
  <summary>Details</summary>
Motivation: 解决偏远和资源匮乏地区产前护理的挑战，包括缺乏医疗专业人员和可及性限制，同时提高测量舒适度。

Method: 开发了集成在孕妇腰带内的胎儿心电图监测系统，包含数据采集、数据传输模块和新型非接触电极，并与传统湿电极进行了性能比较。

Result: 新型非接触电极与传统湿电极性能相当，对10名25-32周孕妇的初步可行性研究证明了系统的性能、可用性和安全性。

Conclusion: 该系统为改善产前监测提供了一种舒适有效的解决方案，特别适用于资源有限的环境。

Abstract: Regular physiological monitoring of maternal and fetal parameters is
indispensable for ensuring safe outcomes during pregnancy and parturition.
Fetal electrocardiogram (fECG) assessment is crucial to detect fetal distress
and developmental anomalies. Given challenges of prenatal care due to the lack
of medical professionals and the limit of accessibility, especially in remote
and resource-poor areas, we develop a fECG monitoring system using novel
non-contact electrodes (NCE) to record the fetal/maternal ECG (f/mECG) signals
through clothes, thereby improving the comfort during measurement. The system
is designed to be incorporated inside a maternity belt with data acquisition,
data transmission module as well as novel NCEs. Thorough characterizations were
carried out to evaluate the novel NCE against traditional wet electrodes (i.e.,
Ag/AgCl electrodes), showing comparable performance. A successful {preliminary
pilot feasibility study} conducted with pregnant women (n = 10) between 25 and
32 weeks of gestation demonstrates the system's performance, usability and
safety.

</details>


### [6] [Geometric Spatio-Spectral Total Variation for Hyperspectral Image Denoising and Destriping](https://arxiv.org/abs/2510.00562)
*Shingo Takemoto,Shunsuke Ono*

Main category: eess.SP

TL;DR: 提出了一种新的正则化方法GeoSSTV，用于高光谱图像去噪和去条纹，通过几何一致的TV公式在欧几里得空间中测量所有方向的变分，能更好地保留圆形结构和斜边。


<details>
  <summary>Details</summary>
Motivation: 现有基于TV的方法存在阶梯伪影和缺乏旋转不变性的问题，难以准确恢复圆形结构和斜边，需要一种几何一致的TV公式来解决这些问题。

Method: 引入几何空间谱总变分(GeoSSTV)，通过欧几里得方式测量所有方向的变分，将去噪问题建模为包含GeoSSTV的约束凸优化问题，并基于预条件原始对偶分裂方法开发高效算法。

Result: 在受混合噪声污染的高光谱图像上的实验结果表明，该方法优于现有方法。

Conclusion: GeoSSTV方法能有效去除噪声同时保留圆形结构和斜边，在高光谱图像去噪和去条纹方面表现出优越性能。

Abstract: This article proposes a novel regularization method, named Geometric
Spatio-Spectral Total Variation (GeoSSTV), for hyperspectral (HS) image
denoising and destriping. HS images are inevitably affected by various types of
noise due to the measurement equipment and environment. Total Variation
(TV)-based regularization methods that model the spatio-spectral piecewise
smoothness inherent in HS images are promising approaches for HS image
denoising and destriping. However, existing TV-based methods are based on
classical anisotropic and isotropic TVs, which cause staircase artifacts and
lack rotation invariance, respectively, making it difficult to accurately
recover round structures and oblique edges. To address this issue, GeoSSTV
introduces a geometrically consistent formulation of TV that measures
variations across all directions in a Euclidean manner. Through this
formulation, GeoSSTV removes noise while preserving round structures and
oblique edges. Furthermore, we formulate the HS image denoising problem as a
constrained convex optimization problem involving GeoSSTV and develop an
efficient algorithm based on a preconditioned primal-dual splitting method.
Experimental results on HS images contaminated with mixed noise demonstrate the
superiority of the proposed method over existing approaches.

</details>


### [7] [Radiation Pattern Reconfigurable FAS-Empowered Interference-Resilient UAV Communication](https://arxiv.org/abs/2510.00581)
*Zhuoran Li,Zhen Gao,Boyu Ning,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 提出一种基于辐射模式可重构流体天线系统的抗干扰无人机通信方案，通过可重构像素天线技术增强角度分辨率，提高频谱效率和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 无人机广泛使用推动了反制非法飞行的技术发展，但合法无人机对非法干扰的抵抗能力研究不足。

Method: 设计专用辐射模式，提出低存储开销的正交匹配追踪多测量向量算法进行到达角估计，利用傅里叶变换实现维度缩减，采用交替优化和穷举搜索确定最优辐射模式和组合器。

Result: 综合仿真表明，所提方案在角度感知精度和频谱效率方面优于传统方法。

Conclusion: RPR-FAS方案能有效提升无人机通信的抗干扰能力和频谱效率，为合法无人机提供可靠的通信保障。

Abstract: The widespread use of uncrewed aerial vehicles (UAVs) has propelled the
development of advanced techniques on countering unauthorized UAV flights.
However, the resistance of legal UAVs to illegal interference remains
under-addressed. This paper proposes radiation pattern reconfigurable fluid
antenna systems (RPR-FAS)-empowered interference-resilient UAV communication
scheme. This scheme integrates the reconfigurable pixel antenna technology,
which provides each antenna with an adjustable radiation pattern. Therefore,
RPR-FAS can enhance the angular resolution of a UAV with a limited number of
antennas, thereby improving spectral efficiency (SE) and interference
resilience. Specifically, we first design dedicated radiation pattern adapted
from 3GPP-TR-38.901, where the beam direction and half power beamwidth are
tailored for UAV communications. Furthermore, we propose a low-storage-overhead
orthogonal matching pursuit multiple measurement vectors algorithm, which
accurately estimates the angle-of-arrival (AoA) of the communication link, even
in the single antenna case. Particularly, by utilizing the Fourier transform to
the radiation pattern gain matrix, we design a dimension-reduction technique to
achieve 1--2 order-of-magnitude reduction in storage requirements. Meanwhile,
we propose a maximum likelihood interference AoA estimation method based on the
law of large numbers, so that the SE can be further improved. Finally,
alternating optimization is employed to obtain the optimal uplink radiation
pattern and combiner, while an exhaustive search is applied to determine the
optimal downlink pattern, complemented by the water-filling algorithm for
beamforming. Comprehensive simulations demonstrate that the proposed schemes
outperform traditional methods in terms of angular sensing precision and
spectral efficiency.

</details>


### [8] [Machine Learning-based Path Loss Prediction in Suburban Environment in the Sub-6 GHz Band](https://arxiv.org/abs/2510.00696)
*Ferdaous Tarhouni,Muneer AlZubi,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 该论文研究使用机器学习模型预测路径损耗，在KAUST郊区校园的sub-6 GHz频段中，随机森林回归和K近邻算法表现最佳，优于传统传播模型。


<details>
  <summary>Details</summary>
Motivation: 传统路径损耗预测方法缺乏灵活性和准确性，需要更有效的解决方案来支持无线通信系统的网络规划、天线设计和性能优化。

Method: 使用射线追踪仿真技术生成合成数据集，训练多种机器学习模型，包括随机森林回归和K近邻算法，并与传统传播模型进行对比。

Result: 机器学习模型在路径损耗预测方面优于传统模型（COST-231 Hata、Longley-Rice和Close-in模型），随机森林回归和K近邻算法提供最佳预测精度。

Conclusion: 基于射线追踪技术的机器学习方法为无线电波传播预测和建模提供了有前景且成本效益高的灵活解决方案。

Abstract: Accurate path loss (PL) prediction is crucial for successful network
planning, antenna design, and performance optimization in wireless
communication systems. Several conventional approaches for PL prediction have
been adopted, but they have been demonstrated to lack flexibility and accuracy.
In this work, we investigate the effectiveness of Machine Learning (ML) models
in predicting PL, particularly for the sub-6 GHz band in a suburban campus of
King Abdullah University of Science and Technology (KAUST). For training
purposes, we generate synthetic datasets using the ray-tracing simulation
technique. The feasibility and accuracy of the ML-based PL models are verified
and validated using both synthetic and measurement datasets. The random forest
regression (RFR) and the K-nearest neighbors (KNN) algorithms provide the best
PL prediction accuracy compared to other ML models. In addition, we compare the
performance of the developed ML-based PL models with the traditional
propagation models, including COST-231 Hata, Longley-Rice, and Close-in models.
The results show the superiority of the ML-based PL models compared to
conventional models. Therefore, the ML approach using the ray-tracing technique
can provide a promising and cost-effective solution for predicting and modeling
radio wave propagation in various scenarios in a flexible manner.

</details>


### [9] [Null-Shaping for Interference Mitigation in LEO Satellites Under Location Uncertainty](https://arxiv.org/abs/2510.00816)
*Fernando Moya Caceres,Akram Al-Hourani,Saman Atapattu,Kandeepan Sithamparanathan*

Main category: eess.SP

TL;DR: 提出了一种鲁棒零陷成形框架，通过将RFI位置不确定性的概率密度函数融入波束成形设计，增强卫星通信在干扰源位置不确定情况下的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星通信上行链路面临日益严重的射频干扰挑战，由于频谱拥塞和地面干扰源位置不确定性，传统波束成形方法在干扰源位置不确定时性能下降。

Method: 首先分析RFI位置地理不确定性如何转化为卫星观测的角度偏差，然后提出基于随机优化的鲁棒零陷成形框架，将RFI位置不确定性的概率密度函数融入波束成形设计。

Result: 通过包含实际卫星轨道动力学和各种RFI场景的广泛蒙特卡洛仿真，证明该方法相比传统确定性设计显著提高了干扰抑制性能。

Conclusion: 所提出的鲁棒零陷成形方法能有效应对RFI位置不确定性，显著提升卫星通信链路的抗干扰能力。

Abstract: Radio frequency interference (RFI) poses a growing challenge to satellite
communications, particularly in uplink channels of Low Earth Orbit (LEO)
systems, due to increasing spectrum congestion and uncertainty in the location
of terrestrial interferers. This paper addresses the impact of RFI source
position uncertainty on beamforming-based interference mitigation. First, we
analytically characterize how geographic uncertainty in RFI location translates
into angular deviation as observed from the satellite. Building on this, we
propose a robust null-shaping framework to increase resilience in the
communication links by incorporating the probability density function (PDF) of
the RFI location uncertainty into the beamforming design via stochastic
optimization. This allows adaptive shaping of the antenna array's nulling
pattern to enhance interference suppression under uncertainty. Extensive Monte
Carlo simulations, incorporating realistic satellite orbital dynamics and
various RFI scenarios, demonstrate that the proposed approach achieves
significantly improved mitigation performance compared to conventional
deterministic designs.

</details>


### [10] [Effectiveness of Reconfigurable Intelligent Surface in Multipath Fading Channel](https://arxiv.org/abs/2510.00838)
*Hasnul Hashim*

Main category: eess.SP

TL;DR: 提出了一种使用三个信道黑盒模拟单输入单输出可重构智能表面辅助信道的方法，通过射线追踪获得信道系数，考虑了地形和建筑物信息。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够准确模拟RIS辅助信道的方法，考虑实际环境中的地理地形和建筑物影响，验证RIS在无线通信中的性能提升。

Method: 使用三个信道黑盒分别表示直接信号路径、发射到RIS路径和RIS反射路径，通过射线追踪获取复杂系数，考虑地形、建筑物形状和电气特性，比较仅反射射线和反射加衍射射线两种情况。

Result: 接收功率表现出典型的多径衰落变化，在最佳位置RIS辅助信道仿真结果与理论模型吻合良好，性能随RIS元素数量增加而按平方增长，仿真结果验证了RIS靠近发射机或接收机部署的最佳位置。

Conclusion: 该方法能够有效模拟RIS辅助信道，仿真结果与理论模型一致，证实了RIS在无线通信中的性能提升潜力，并确定了最佳部署位置。

Abstract: A method of simulating a single-input single-output reconfigurable
intelligent surface (RIS) assisted channel is presented using three channel
black boxes to represent the direct signal path, the transmit path to the RIS
and the reflected path from the RIS. The complex coefficients for each channel
box is obtained by ray tracing in a scenario with geographic terrain
information that also contains approximate building shapes. The electrical
characteristics of the ground and building walls were also accounted for in the
ray tracing function. Simulations were conducted with reflected rays only and
reflected rays together with diffracted rays. The received power exhibits
variations typical of multipath fading environments. In the best locations, the
RIS-assisted channel simulation result agrees well with theoretical models, the
performance increasing by the RIS size squared as the number of RIS elements is
increased. In the simplified theoretical model where the transmitter and
receiver are inline and the RIS orthogonal but much closer than the distance
between the former elements, the simulation results also corroborate best
deployment close the transmitter or the receiver with a U-shaped drop between
them.

</details>


### [11] [Agentic AI meets Neural Architecture Search: Proactive Traffic Prediction for AI-RAN](https://arxiv.org/abs/2510.00851)
*Abdelaziz Salama,Mohammed M. H. Qazzaz,Zeinab Nezami,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: eess.SP

TL;DR: 基于神经架构搜索的LSTM动态选择框架，用于O-RAN环境中的流量预测，实现计算复杂度降低70-75%的同时保持高精度


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要智能流量预测来实现自主资源管理，O-RAN框架为嵌入机器学习智能提供了良好基础

Method: 应用神经架构搜索框架，在O-RAN环境中动态选择和编排高效的LSTM架构，通过非实时RIC rApps进行架构优化，通过近实时RIC xApps进行实时推理

Result: 轻量级模型在常规流量下达到R²≈0.91-0.93，复杂模型在关键场景下达到接近完美精度(R²=0.989-0.996)，NAS编排相比静态高性能模型减少70-75%计算复杂度

Conclusion: 该方法能够在保持高预测精度的同时显著降低计算复杂度，实现现实边缘环境中的可扩展部署

Abstract: Next-generation wireless networks require intelligent traffic prediction to
enable autonomous resource management and handle diverse, dynamic service
demands. The Open Radio Access Network (O-RAN) framework provides a promising
foundation for embedding machine learning intelligence through its
disaggregated architecture and programmable interfaces. This work applies a
Neural Architecture Search (NAS)-based framework that dynamically selects and
orchestrates efficient Long Short-Term Memory (LSTM) architectures for traffic
prediction in O-RAN environments. Our approach leverages the O-RAN paradigm by
separating architecture optimisation (via non-RT RIC rApps) from real-time
inference (via near-RT RIC xApps), enabling adaptive model deployment based on
traffic conditions and resource constraints. Experimental evaluation across six
LSTM architectures demonstrates that lightweight models achieve $R^2 \approx
0.91$--$0.93$ with high efficiency for regular traffic, while complex models
reach near-perfect accuracy ($R^2 = 0.989$--$0.996$) during critical scenarios.
Our NAS-based orchestration achieves a 70-75\% reduction in computational
complexity compared to static high-performance models, while maintaining high
prediction accuracy when required, thereby enabling scalable deployment in
real-world edge environments.

</details>


### [12] [Graph Neural Networks in Large Scale Wireless Communication Networks: Scalability Across Random Geometric Graphs](https://arxiv.org/abs/2510.00896)
*Romina Garcia Camargo,Zhiyang Wang,Alejandro Ribeiro*

Main category: eess.SP

TL;DR: 该论文为图神经网络在无线网络中的可迁移性提供了理论保证，特别关注稀疏的随机几何图模型，并通过功率分配任务验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 无线网络日益复杂，推动从传统方法转向基于学习的方法。图神经网络特别适合无线网络，因为无线网络可以自然地表示为图。现有理论保证未能捕捉到GNN在无线系统中的有效迁移现象，因为大多数研究假设密集图，而无线系统是稀疏的。

Method: 为随机几何图上的可迁移性提供正式的理论基础，随机几何图是无线网络广泛使用的稀疏模型。通过功率分配任务的数值实验验证理论结果。

Result: 建立了图神经网络在无线网络稀疏图模型上的理论可迁移性保证，并通过实验验证了理论发现。

Conclusion: 该工作为图神经网络在无线网络中的可迁移性提供了首个正式的理论基础，填补了现有理论无法解释稀疏无线网络场景的空白。

Abstract: The growing complexity of wireless systems has accelerated the move from
traditional methods to learning-based solutions. Graph Neural Networks (GNNs)
are especially well-suited here, since wireless networks can be naturally
represented as graphs. A key property of GNNs is transferability: models
trained on one graph often generalize to much larger graphs with little
performance loss. While empirical studies have shown that GNN-based wireless
policies transfer effectively, existing theoretical guarantees do not capture
this phenomenon. Most works focus on dense graphs where node degrees scale with
network size, an assumption that fails in wireless systems. In this work, we
provide a formal theoretical foundation for transferability on Random Geometric
Graphs (RGGs), a sparse and widely used model of wireless networks. We further
validate our results through numerical experiments on power allocation, a
fundamental resource management task.

</details>


### [13] [A Robust Proactive Communication Strategy for Distributed Active Noise Control Systems](https://arxiv.org/abs/2510.00934)
*Junwei Ji,Dongyuan Shi,Zhengding Luo,Boxiang Wang,Ziyi Yang,Haowen Li,Woon-Seng Gan*

Main category: eess.SP

TL;DR: 提出了一种鲁棒的分布式多通道主动噪声控制通信框架，通过自适应-固定滤波器切换和混合梯度组合策略，在保持性能的同时降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 分布式多通道主动噪声控制系统虽然能分摊计算负载，但通信开销会破坏算法稳定性和性能，需要解决通信效率与系统稳定性之间的矛盾。

Method: 每个节点独立执行单通道FxLMS算法，实时监测降噪性能。当性能下降时，节点停止自适应算法，切换到固定滤波器并启动通信请求。交换信息为当前控制滤波器与上次通信时滤波器的差值（即非通信期间的累积梯度）。采用混合梯度组合方法更新控制滤波器。

Result: 仿真表明，该方法在通信约束下实现了与集中式算法相当的降噪性能，同时保持了系统稳定性。

Conclusion: 所提出的主动通信策略和自适应-固定切换机制确保了系统鲁棒性，在实际分布式ANC场景中具有实用价值。

Abstract: Distributed multichannel active noise control (DMCANC) systems assign the
high computational load of conventional centralized algorithms across multiple
processing nodes, leveraging inter-node communication to collaboratively
suppress unwanted noise. However, communication overhead can undermine
algorithmic stability and degrade overall performance. To address this
challenge, we propose a robust communication framework that integrates
adaptive-fixed-filter switching and the mixed-gradient combination strategy. In
this approach, each node independently executes a single-channel filtered
reference least mean square (FxLMS) algorithm while monitoring real-time noise
reduction levels. When the current noise reduction performance degrades
compared to the previous state, the node halts its adaptive algorithm, switches
to a fixed filter, and simultaneously initiates a communication request. The
exchanged information comprises the difference between the current control
filter and the filter at the time of the last communication, equivalent to the
accumulated gradient sum during non-communication intervals. Upon receiving
neighboring cumulative gradients, the node employs a mixed-gradient combination
method to update its control filter, subsequently reverting to the adaptive
mode. This proactive communication strategy and adaptive-fixed switching
mechanism ensure system robustness by mitigating instability risks caused by
communication issues. Simulations demonstrate that the proposed method achieves
noise reduction performance comparable to centralized algorithms while
maintaining stability under communication constraints, highlighting its
practical applicability in real-world distributed ANC scenarios.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [14] [DiffAU: Diffusion-Based Ambisonics Upscaling](https://arxiv.org/abs/2510.00180)
*Amit Milstein,Nir Shlezinger,Boaz Rafaely*

Main category: eess.AS

TL;DR: DiffAU：一种基于扩散模型的级联Ambisonics升阶方法，可从一阶Ambisonics生成三阶Ambisonics，提升空间音频的空间分辨率


<details>
  <summary>Details</summary>
Motivation: 一阶Ambisonics(FOA)在硬件效率和存储方面具有优势，但其低空间分辨率限制了沉浸感，因此需要将FOA升阶为高阶Ambisonics(HOA)以提升空间音频的真实感

Method: 提出DiffAU方法，利用扩散模型结合空间音频的适应性改进，通过级联方式从FOA生成三阶Ambisonics，通过学习数据分布提供原则性方法

Result: 在无回声条件下对多个扬声器的实验显示，该方法在客观指标和感知性能方面表现优异

Conclusion: DiffAU能够快速可靠地在各种设置中重现高阶Ambisonics，有效提升空间音频质量

Abstract: Spatial audio enhances immersion by reproducing 3D sound fields, with
Ambisonics offering a scalable format for this purpose. While first-order
Ambisonics (FOA) notably facilitates hardware-efficient acquisition and storage
of sound fields as compared to high-order Ambisonics (HOA), its low spatial
resolution limits realism, highlighting the need for Ambisonics upscaling (AU)
as an approach for increasing the order of Ambisonics signals. In this work we
propose DiffAU, a cascaded AU method that leverages recent developments in
diffusion models combined with novel adaptation to spatial audio to generate
3rd order Ambisonics from FOA. By learning data distributions, DiffAU provides
a principled approach that rapidly and reliably reproduces HOA in various
settings. Experiments in anechoic conditions with multiple speakers, show
strong objective and perceptual performance.

</details>


### [15] [Descriptor:: Extended-Length Audio Dataset for Synthetic Voice Detection and Speaker Recognition (ELAD-SVDSR)](https://arxiv.org/abs/2510.00218)
*Rahul Vijaykumar,Ajan Ahmed,John Parker,Dinesh Pendyala,Aidan Collins,Stephanie Schuckers,Masudul H. Imtiaz*

Main category: eess.AS

TL;DR: ELAD SVDSR是一个专门设计的音频数据集，包含36名参与者的45分钟录音，用于生成高质量语音深度伪造和训练检测系统。


<details>
  <summary>Details</summary>
Motivation: 为语音深度伪造检测和说话人识别提供高质量的对抗样本，促进音频取证和生物识别安全技术的发展。

Method: 收集36名参与者在受控条件下朗读报纸文章的45分钟录音，使用5种不同质量的麦克风，并创建了20个深度伪造语音样本。

Result: 创建了包含丰富语音特征的数据集，能够生成更真实连贯的合成语音，为检测系统提供具有挑战性的训练样本。

Conclusion: ELAD SVDSR数据集预计将推动音频取证、生物识别安全和语音认证系统的重大进步。

Abstract: This paper introduces the Extended Length Audio Dataset for Synthetic Voice
Detection and Speaker Recognition (ELAD SVDSR), a resource specifically
designed to facilitate the creation of high quality deepfakes and support the
development of detection systems trained against them. The dataset comprises 45
minute audio recordings from 36 participants, each reading various newspaper
articles recorded under controlled conditions and captured via five microphones
of differing quality. By focusing on extended duration audio, ELAD SVDSR
captures a richer range of speech attributes such as pitch contours, intonation
patterns, and nuanced delivery enabling models to generate more realistic and
coherent synthetic voices. In turn, this approach allows for the creation of
robust deepfakes that can serve as challenging examples in datasets used to
train and evaluate synthetic voice detection methods. As part of this effort,
20 deepfake voices have already been created and added to the dataset to
showcase its potential. Anonymized metadata accompanies the dataset on speaker
demographics. ELAD SVDSR is expected to spur significant advancements in audio
forensics, biometric security, and voice authentication systems.

</details>


### [16] [Room Impulse Response Synthesis via Differentiable Feedback Delay Networks for Efficient Spatial Audio Rendering](https://arxiv.org/abs/2510.00238)
*Armin Gerami,Ramani Duraiswami*

Main category: eess.AS

TL;DR: 提出一种计算高效且可调谐的反馈延迟网络架构，用于实时房间脉冲响应渲染，通过可微分编程优化FDN参数以匹配目标RIR的声学和心理声学指标。


<details>
  <summary>Details</summary>
Motivation: 解决传统卷积和傅里叶变换方法在计算复杂度和延迟方面的挑战，实现动态、实时的房间脉冲响应调整以适应听众和声源移动。

Method: 使用可微分编程直接优化FDN参数，匹配目标RIR的清晰度和定义等声学指标，结合先前HRIR的无限脉冲响应表示工作。

Result: 能够以远低于计算成本的方式产生与长双耳房间脉冲响应滤波器卷积相似质量的渲染效果。

Conclusion: 该方法实现了高效的双耳房间脉冲响应渲染，支持实时动态调整，为听觉对象的高效渲染提供了可行方案。

Abstract: We introduce a computationally efficient and tunable feedback delay network
(FDN) architecture for real-time room impulse response (RIR) rendering that
addresses the computational and latency challenges inherent in traditional
convolution and Fourier transform based methods. Our approach directly
optimizes FDN parameters to match target RIR acoustic and psychoacoustic
metrics such as clarity and definition through novel differentiable
programming-based optimization. Our method enables dynamic, real-time
adjustments of room impulse responses that accommodates listener and source
movement. When combined with previous work on representation of head-related
impulse responses via infinite impulse responses, an efficient rendering of
auditory objects is possible when the HRIR and RIR are known. Our method
produces renderings with quality similar to convolution with long binaural room
impulse response (BRIR) filters, but at a fraction of the computational cost.

</details>


### [17] [Subjective quality evaluation of personalized own voice reconstruction systems](https://arxiv.org/abs/2510.00256)
*Mattes Ohlenbusch,Christian Rollwage,Simon Doclo,Jan Rennies*

Main category: eess.AS

TL;DR: 本文研究了通过数据增强和微调实现个性化自有语音重建系统，发现个性化方法仅对部分说话者有益，且客观指标不能准确预测系统性能比较。


<details>
  <summary>Details</summary>
Motivation: 由于影响自有语音信号的干扰因素具有个体差异性，个性化OVR系统有潜力超越通用OVR系统。

Method: 通过数据增强和微调实现OVR系统的个性化，并与通用系统进行比较，使用客观指标评估语音质量，并进行主观听力测试。

Result: 个性化OVR仅对部分说话者提供优势，客观指标不能准确预测系统性能比较，某些干扰会导致质量的高估。

Conclusion: 个性化OVR系统的效果因人而异，客观指标在评估系统性能时存在局限性，需要结合主观评价。

Abstract: Own voice pickup technology for hearable devices facilitates communication in
noisy environments. Own voice reconstruction (OVR) systems enhance the quality
and intelligibility of the recorded noisy own voice signals. Since disturbances
affecting the recorded own voice signals depend on individual factors,
personalized OVR systems have the potential to outperform generic OVR systems.
In this paper, we propose personalizing OVR systems through data augmentation
and fine-tuning, comparing them to their generic counterparts. We investigate
the influence of personalization on speech quality assessed by objective
metrics and conduct a subjective listening test to evaluate quality under
various conditions. In addition, we assess the prediction accuracy of the
objective metrics by comparing predicted quality with subjectively measured
quality. Our findings suggest that personalized OVR provides benefits over
generic OVR for some talkers only. Our results also indicate that performance
comparisons between systems are not always accurately predicted by objective
metrics. In particular, certain disturbances lead to a consistent
overestimation of quality compared to actual subjective ratings.

</details>


### [18] [Post-Training Quantization for Audio Diffusion Transformers](https://arxiv.org/abs/2510.00313)
*Tanmay Khandelwal,Magdalena Fuentes*

Main category: eess.AS

TL;DR: 本文对音频扩散变换器(DiTs)进行后训练量化评估，提出两种扩展方法：基于去噪时间步的平滑方法和轻量级LoRA分支，在W8A8和W4A8配置下显著减少内存使用达79%同时保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器虽然能实现高质量音频合成，但计算密集且存储需求大，限制了实际部署。

Method: 评估静态和动态量化方案，提出(1)去噪时间步感知平滑方法，按输入通道和时间步调整量化尺度；(2)基于SVD的轻量级LoRA分支补偿权重误差。

Result: 动态量化在低精度下保持保真度，静态方法延迟更低。低精度DiTs可减少内存使用达79%。

Conclusion: 低精度DiTs能够在显著减少内存使用的同时保持高保真度生成，为实际部署提供可行方案。

Abstract: Diffusion Transformers (DiTs) enable high-quality audio synthesis but are
often computationally intensive and require substantial storage, which limits
their practical deployment. In this paper, we present a comprehensive
evaluation of post-training quantization (PTQ) techniques for audio DiTs,
analyzing the trade-offs between static and dynamic quantization schemes. We
explore two practical extensions (1) a denoising-timestep-aware smoothing
method that adapts quantization scales per-input-channel and timestep to
mitigate activation outliers, and (2) a lightweight low-rank adapter
(LoRA)-based branch derived from singular value decomposition (SVD) to
compensate for residual weight errors. Using Stable Audio Open we benchmark
W8A8 and W4A8 configurations across objective metrics and human perceptual
ratings. Our results show that dynamic quantization preserves fidelity even at
lower precision, while static methods remain competitive with lower latency.
Overall, our findings show that low-precision DiTs can retain high-fidelity
generation while reducing memory usage by up to 79%.

</details>


### [19] [Learning Domain-Robust Bioacoustic Representations for Mosquito Species Classification with Contrastive Learning and Distribution Alignment](https://arxiv.org/abs/2510.00346)
*Yuanbo Hou,Zhaoyi Liu,Xin Shen,Stephen Roberts*

Main category: eess.AS

TL;DR: 提出一种域鲁棒生物声学学习框架(DR-BioL)，结合对比学习和分布对齐，解决蚊子物种分类中跨域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 蚊子生物声学数据收集受季节和野外工作限制，不同区域的录音存在非生物性域特征差异。现有模型依赖域信息而非物种声学线索，导致跨域泛化性能差。

Method: 使用对比学习促进同物种内聚性并减轻域间差异，结合物种条件分布对齐增强跨域物种表示。

Result: 在多域蚊子生物声学数据集上的实验表明，DR-BioL提高了基线的准确性和鲁棒性。

Conclusion: 该框架在现实世界中具有可靠的跨域蚊子物种分类潜力。

Abstract: Mosquito Species Classification (MSC) is crucial for vector surveillance and
disease control. The collection of mosquito bioacoustic data is often limited
by mosquito activity seasons and fieldwork. Mosquito recordings across regions,
habitats, and laboratories often show non-biological variations from the
recording environment, which we refer to as domain features. This study finds
that models directly trained on audio recordings with domain features tend to
rely on domain information rather than the species' acoustic cues for
identification, resulting in illusory good performance while actually
performing poor cross-domain generalization. To this end, we propose a
Domain-Robust Bioacoustic Learning (DR-BioL) framework that combines
contrastive learning with distribution alignment. Contrastive learning aims to
promote cohesion within the same species and mitigate inter-domain
discrepancies, and species-conditional distribution alignment further enhances
cross-domain species representation. Experiments on a multi-domain mosquito
bioacoustic dataset from diverse environments show that the DR-BioL improves
the accuracy and robustness of baselines, highlighting its potential for
reliable cross-domain MSC in the real world.

</details>


### [20] [UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching](https://arxiv.org/abs/2510.00771)
*Woongjib Choi,Sangmin Lee,Hyungseob Lim,Hong-Goo Kang*

Main category: eess.AS

TL;DR: 提出了一种无需声码器的音频超分辨率框架，使用流匹配生成模型直接重建波形，避免了传统两阶段方法对声码器的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段扩散方法需要先预测梅尔频谱图，再依赖预训练神经声码器合成波形，最终音频质量受声码器性能限制。

Method: 采用流匹配生成模型捕捉复值频谱系数的条件分布，通过逆短时傅里叶变换直接重建波形，实现端到端优化。

Result: 实验表明该模型在多种上采样因子下能一致生成48kHz高保真音频，在语音和通用音频数据集上达到最先进性能。

Conclusion: 该无声码器框架简化了优化流程，克服了两阶段流水线的关键瓶颈，实现了高质量的音频超分辨率。

Abstract: In this paper, we present a vocoder-free framework for audio super-resolution
that employs a flow matching generative model to capture the conditional
distribution of complex-valued spectral coefficients. Unlike conventional
two-stage diffusion-based approaches that predict a mel-spectrogram and then
rely on a pre-trained neural vocoder to synthesize waveforms, our method
directly reconstructs waveforms via the inverse Short-Time Fourier Transform
(iSTFT), thereby eliminating the dependence on a separate vocoder. This design
not only simplifies end-to-end optimization but also overcomes a critical
bottleneck of two-stage pipelines, where the final audio quality is
fundamentally constrained by vocoder performance. Experiments show that our
model consistently produces high-fidelity 48 kHz audio across diverse
upsampling factors, achieving state-of-the-art performance on both speech and
general audio datasets.

</details>


### [21] [Reconstruction of the Complete Vocal Tract Contour Through Acoustic to Articulatory Inversion Using Real-Time MRI Data](https://arxiv.org/abs/2510.00914)
*Sofiane Azzouz,Pierre-André Vuissoz,Yves Laprie*

Main category: eess.AS

TL;DR: 该论文提出了一种从声学到完整声道（从声门到嘴唇）的发音器官反演模型，使用实时动态MRI数据，实现了首个完整的声道反演。


<details>
  <summary>Details</summary>
Motivation: 传统的声学-发音反演通常仅限于声道的一小部分，因为EMA数据需要在可接触的发音器官上粘贴传感器。本文旨在实现从声门、完整舌头、软腭到嘴唇的整个声道的反演。

Method: 使用超过3小时的实时动态MRI语音数据库，包含去噪语音信号和自动分割的发音器官轮廓。采用多种双向LSTM方法，分别对每个发音器官单独反演或同时反演所有发音器官。

Result: 在测试集上的平均RMSE精度为1.65毫米，与像素尺寸（1.62毫米）相当，表明模型性能接近理论极限。

Conclusion: 这是首个实现完整声道反演的研究，为声学-发音转换提供了更全面的解决方案。

Abstract: Acoustic to articulatory inversion has often been limited to a small part of
the vocal tract because the data are generally EMA (ElectroMagnetic
Articulography) data requiring sensors to be glued to easily accessible
articulators. The presented acoustic to articulation model focuses on the
inversion of the entire vocal tract from the glottis, the complete tongue, the
velum, to the lips. It relies on a realtime dynamic MRI database of more than 3
hours of speech. The data are the denoised speech signal and the automatically
segmented articulator contours. Several bidirectional LSTM-based approaches
have been used, either inverting each articulator individually or inverting all
articulators simultaneously. To our knowledge, this is the first complete
inversion of the vocal tract. The average RMSE precision on the test set is
1.65 mm to be compared with the pixel size which is 1.62 mm.

</details>


### [22] [CL-UZH submission to the NIST SRE 2024 Speaker Recognition Evaluation](https://arxiv.org/abs/2510.00952)
*Aref Farhadipour,Shiran Liu,Masoumeh Chapariniya,Valeriia Perepelytsia,Srikanth Madikeri,Teodora Vukovic,Volker Dellwo*

Main category: eess.AS

TL;DR: CL-UZH团队为NIST SRE 2024挑战赛的固定和开放条件各提交了一个系统。在闭集条件下，音频试验使用Kaldi开发的X-vector系统；音频-视觉结果仅使用视觉模态模型。基于VoxBlink2和VoxCeleb2数据集的预训练模型，以及使用CTS超集数据集从头训练的Xvector模型，分别提交了开放集和闭集条件的结果。


<details>
  <summary>Details</summary>
Motivation: 参与NIST SRE 2024挑战赛，评估不同模态（音频、视觉）和不同训练策略（预训练、从头训练）在说话人识别任务中的性能。

Method: 1. 闭集音频试验：使用Kaldi开发的X-vector系统
2. 音频-视觉结果：仅使用视觉模态模型
3. 开放集和闭集条件：基于VoxBlink2和VoxCeleb2的预训练模型
4. 闭集条件：使用CTS超集数据集从头训练Xvector模型

Result: 提交了SRE24评估结果到竞赛网站，并在报告中讨论了所提系统在SRE24评估中的性能表现。

Conclusion: 该工作展示了多种说话人识别方法在NIST SRE 2024挑战赛中的应用，包括不同模态和训练策略的组合，为说话人识别研究提供了实践经验。

Abstract: The CL-UZH team submitted one system each for the fixed and open conditions
of the NIST SRE 2024 challenge. For the closed-set condition, results for the
audio-only trials were achieved using the X-vector system developed with Kaldi.
For the audio-visual results we used only models developed for the visual
modality. Two sets of results were submitted for the open-set and closed-set
conditions, one based on a pretrained model using the VoxBlink2 and VoxCeleb2
datasets. An Xvector-based model was trained from scratch using the CTS
superset dataset for the closed set. In addition to the submission of the
results of the SRE24 evaluation to the competition website, we talked about the
performance of the proposed systems on the SRE24 evaluation in this report.

</details>


### [23] [Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting](https://arxiv.org/abs/2510.00982)
*Emiru Tsunoo,Hayato Futami,Yosuke Kashiwagi,Siddhant Arora,Shinji Watanabe*

Main category: eess.AS

TL;DR: 提出Spiralformer编码器，通过层丢弃和提前退出机制优化流式语音识别的块处理，减少编码延迟。


<details>
  <summary>Details</summary>
Motivation: 现有流式语音识别中，基于Transformer的编码器广泛使用块处理，但很少研究改进编码延迟。本文旨在通过频繁发射小片段而非稀疏的大片段来降低延迟。

Method: 提出Spiralformer编码器，结合层丢弃和提前退出机制，以螺旋方式跳过层计算并在每个块中移动计算层，在块处理过程中完成所有层的计算。

Result: 在Librispeech上平均token发射延迟减少21.6%，在CSJ上减少7.0%，同时保持相似的计算成本和词错误率。

Conclusion: Spiralformer通过优化块处理有效降低了流式语音识别的编码延迟，且不影响识别精度。

Abstract: For streaming speech recognition, a Transformer-based encoder has been widely
used with block processing. Although many studies addressed improving emission
latency of transducers, little work has been explored for improving encoding
latency of the block processing. We seek to reduce latency by frequently
emitting a chunk with a small shift rather than scarce large-chunk emissions,
resulting in higher computational costs. To efficiently compute with the small
chunk shift, we propose a new encoder, Spiralformer, tailored for block
processing by combining layer dropping and early exiting. We skip layer
computation in a cyclic manner and shift the computed layer in each block
spirally, which completes computation for all the layers over the block
processing. Experimentally, we observed that our method achieved 21.6%
reduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,
compared with the baseline with similar computational cost and word error
rates.

</details>


### [24] [Learning Time-Graph Frequency Representation for Monaural Speech Enhancement](https://arxiv.org/abs/2510.01130)
*Tingting Wang,Tianrui Wang,Meng Ge,Qiquan Zhang,Xi Shao*

Main category: eess.AS

TL;DR: 提出了一种可学习的GFT-SVD框架用于语音增强，通过图移位算子构建可学习的图拓扑，使用1-D卷积神经网络层定义可学习的图傅里叶基，避免了矩阵求逆带来的数值误差和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于GFT的语音增强方法使用固定图拓扑构建图傅里叶基，缺乏自适应性和灵活性，且基于SVD和EVD的GFT方法存在矩阵求逆带来的数值误差和稳定性问题。

Method: 利用图移位算子构建可学习的图拓扑，通过1-D卷积神经网络层使用奇异值矩阵定义可学习的图傅里叶基，消除矩阵求逆需求。

Result: 该方法避免了矩阵求逆带来的数值误差和稳定性问题，提高了语音增强的性能。

Conclusion: 提出的可学习GFT-SVD框架简单有效，解决了传统GFT方法的局限性，为语音增强提供了更灵活和稳定的解决方案。

Abstract: The Graph Fourier Transform (GFT) has recently demonstrated promising results
in speech enhancement. However, existing GFT-based speech enhancement
approaches often employ fixed graph topologies to build the graph Fourier
basis, whose the representation lacks the adaptively and flexibility. In
addition, they suffer from the numerical errors and instability introduced by
matrix inversion in GFT based on both Singular Value Decomposition (GFT-SVD)
and Eigen Vector Decomposition (GFT-EVD). Motivated by these limitations, this
paper propose a simple yet effective learnable GFT-SVD framework for speech
enhancement. Specifically, we leverage graph shift operators to construct a
learnable graph topology and define a learnable graph Fourier basis by the
singular value matrices using 1-D convolution (Conv-1D) neural layer. This
eliminates the need for matrix inversion, thereby avoiding the associated
numerical errors and stability problem.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [25] [Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches](https://arxiv.org/abs/2510.00006)
*Kajwan Ziaoddini*

Main category: cs.SD

TL;DR: 该研究通过结合基于内容的音乐分析和轻量级歌词网络视角，分析了在线社区中音乐符号的生产和传播。研究发现过去十年音乐能量下降、舞蹈性上升，歌词以人称代词为核心，不同风格音乐的情绪特征存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解在线社区中音乐符号的生产和传播机制，通过整合音乐内容和歌词分析来揭示主流音乐的趋势模式和风格特征。

Method: 使用包含275首热门歌曲的语料库，结合音频描述符（能量、舞蹈性、响度等）和完整歌词转录，构建了可复现的分析流程，包括量化声学属性时间趋势、建模词汇显著性和共现、按流派分析情绪特征。

Result: 发现十年间能量从79降至58，舞蹈性从59升至73；情绪值在2013年达到峰值63，2014-2016年降至42后部分恢复；R&B情绪值最高(96)，拉丁/雷鬼顿最低(37)；歌词分析显示以"我/你/我/我的"为核心的人称代词词汇。

Conclusion: 研究揭示了主流音乐中先前边缘化编码的主流化趋势，以及商业偏好放松但节奏吸引人的制作风格。方法上贡献了整合音乐信息检索和网络分析的工作流程，适用于社会感知推荐或社区级传播研究。

Abstract: This paper examines how musical symbolism is produced and circulated in
online communities by combining content-based music analysis with a lightweight
network perspective on lyrics. Using a curated corpus of 275 chart-topping
songs enriched with audio descriptors (energy, danceability, loudness,
liveness, valence, acousticness, speechiness, popularity) and full lyric
transcripts, we build a reproducible pipeline that (i) quantifies temporal
trends in sonic attributes, (ii) models lexical salience and co-occurrence, and
(iii) profiles mood by genre. We find a decade-long decline in energy (79 ->
58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and
dips in 2014-2016 (42) before partially recovering. Correlation analysis shows
strong coupling of energy with loudness (r = 0.74) and negative associations
for acousticness with both energy (r = -0.54) and loudness (r = -0.51);
danceability is largely orthogonal to other features (|r| < 0.20). Lyric
tokenization (>114k tokens) reveals a pronoun-centric lexicon "I/you/me/my" and
a dense co-occurrence structure in which interpersonal address anchors
mainstream narratives. Mood differs systematically by style: R&B exhibits the
highest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70),
whereas Latin/Reggaeton is lower (37) despite high danceability. Read through a
subcultural identity lens, these patterns suggest the mainstreaming of
previously peripheral codes and a commercial preference for relaxed yet
rhythmically engaging productions that sustain collective participation without
maximal intensity. Methodologically, we contribute an integrated
MIR-plus-network workflow spanning summary statistics, correlation structure,
lexical co-occurrence matrices, and genre-wise mood profiling that is robust to
modality sparsity and suitable for socially aware recommendation or
community-level diffusion studies.

</details>


### [26] [Temporal-Aware Iterative Speech Model for Dementia Detection](https://arxiv.org/abs/2510.00030)
*Chukwuemeka Ugwu,Oluwafemi Oyeleke*

Main category: cs.SD

TL;DR: TAI-Speech是一个用于痴呆检测的时序感知迭代框架，通过动态建模自发语音来捕捉认知衰退的早期指标，在DementiaBank数据集上取得了0.839的AUC和80.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于语音的痴呆检测方法通常依赖静态特征或聚合的语言内容，缺乏对语音产生中细微渐进性退化的建模能力，无法捕捉认知衰退的关键动态时序模式。

Method: 1) 光流启发的迭代精炼：将频谱图视为序列帧，使用卷积GRU捕捉声学特征的帧间演化；2) 基于交叉注意力的韵律对齐：动态对齐频谱特征与韵律模式（音高、停顿），创建与功能衰退相关的语音产生缺陷的丰富表示。

Result: 在DementiaBank数据集上，TAI-Speech实现了0.839的AUC和80.6%的准确率，优于基于文本的基线方法，且不依赖ASR。

Conclusion: 该工作为自动化认知评估提供了更灵活和鲁棒的解决方案，直接基于原始音频的动态特性进行操作。

Abstract: Deep learning systems often struggle with processing long sequences, where
computational complexity can become a bottleneck. Current methods for automated
dementia detection using speech frequently rely on static, time-agnostic
features or aggregated linguistic content, lacking the flexibility to model the
subtle, progressive deterioration inherent in speech production. These
approaches often miss the dynamic temporal patterns that are critical early
indicators of cognitive decline. In this paper, we introduce TAI-Speech, a
Temporal Aware Iterative framework that dynamically models spontaneous speech
for dementia detection. The flexibility of our method is demonstrated through
two key innovations: 1) Optical Flow-inspired Iterative Refinement: By treating
spectrograms as sequential frames, this component uses a convolutional GRU to
capture the fine-grained, frame-to-frame evolution of acoustic features. 2)
Cross-Attention Based Prosodic Alignment: This component dynamically aligns
spectral features with prosodic patterns, such as pitch and pauses, to create a
richer representation of speech production deficits linked to functional
decline (IADL). TAI-Speech adaptively models the temporal evolution of each
utterance, enhancing the detection of cognitive markers. Experimental results
on the DementiaBank dataset show that TAI-Speech achieves a strong AUC of 0.839
and 80.6\% accuracy, outperforming text-based baselines without relying on ASR.
Our work provides a more flexible and robust solution for automated cognitive
assessment, operating directly on the dynamics of raw audio.

</details>


### [27] [A Recall-First CNN for Sleep Apnea Screening from Snoring Audio](https://arxiv.org/abs/2510.00052)
*Anushka Mallick,Afiya Noorain,Ashwin Menon,Ashita Solanki,Keertan Balaji*

Main category: cs.SD

TL;DR: 使用呼吸音频记录检测睡眠呼吸暂停的可行性研究，通过将呼吸声音转换为频谱图并应用过采样和类别权重平衡数据集，模型实现了90.55%的召回率，表明其作为低成本筛查工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统多导睡眠图检测睡眠呼吸暂停成本高、耗时长，不适合大规模筛查，需要寻找更便捷的替代方法。

Method: 将呼吸音频转换为频谱图，通过过采样呼吸暂停片段平衡数据集，并应用类别权重减少多数类偏差。

Result: 模型在呼吸暂停检测方面达到90.55%的召回率，虽然精度较低，但成功捕捉了大部分呼吸暂停事件。

Conclusion: 该方法有潜力作为低成本筛查工具，可在家庭或基础临床环境中使用，帮助早期识别高风险个体。

Abstract: Sleep apnea is a serious sleep-related breathing disorder that is common and
can impact health if left untreated. Currently the traditional method for
screening and diagnosis is overnight polysomnography. Polysomnography is
expensive and takes a lot of time, and is not practical for screening large
groups of people. In this paper, we explored a more accessible option, using
respiratory audio recordings to spot signs of apnea.We utilized 18 audio
files.The approach involved converting breathing sounds into spectrograms,
balancing the dataset by oversampling apnea segments, and applying class
weights to reduce bias toward the majority class. The model reached a recall of
90.55 for apnea detection. Intentionally, prioritizing catching apnea events
over general accuracy. Despite low precision,the high recall suggests potential
as a low-cost screening tool that could be used at home or in basic clinical
setups, potentially helping identify at-risk individuals much earlier.

</details>


### [28] [Low Resource Audio Codec Challenge Baseline Systems](https://arxiv.org/abs/2510.00264)
*Yusuf Ziya Isik,Rafał Łaganowski*

Main category: cs.SD

TL;DR: LRAC挑战赛旨在推进资源受限环境下的神经音频编码技术，2025年首届挑战赛聚焦低资源神经语音编解码器，包含透明编解码和增强编解码两个赛道，本文提供了官方基线系统。


<details>
  <summary>Details</summary>
Motivation: 开发能够在资源受限环境下可靠运行的神经音频编码系统，满足计算复杂度、延迟和比特率的严格约束，同时应对日常噪声和混响环境。

Method: 使用带有残差矢量量化的卷积神经编解码模型，通过对抗性和重建目标的组合进行端到端训练，采用数据过滤和增强策略。

Result: 提出了LRAC挑战赛两个赛道的官方基线系统，详细描述了数据处理、模型架构、优化过程和检查点选择标准。

Conclusion: 为低资源神经音频编码研究建立了基准系统，支持在计算受限环境下实现透明或增强的语音编码性能。

Abstract: The Low-Resource Audio Codec (LRAC) Challenge aims to advance neural audio
coding for deployment in resource-constrained environments. The first edition
focuses on low-resource neural speech codecs that must operate reliably under
everyday noise and reverberation, while satisfying strict constraints on
computational complexity, latency, and bitrate. Track 1 targets transparency
codecs, which aim to preserve the perceptual transparency of input speech under
mild noise and reverberation. Track 2 addresses enhancement codecs, which
combine coding and compression with denoising and dereverberation. This paper
presents the official baseline systems for both tracks in the 2025 LRAC
Challenge. The baselines are convolutional neural codec models with Residual
Vector Quantization, trained end-to-end using a combination of adversarial and
reconstruction objectives. We detail the data filtering and augmentation
strategies, model architectures, optimization procedures, and checkpoint
selection criteria.

</details>


### [29] [Dereverberation Using Binary Residual Masking with Time-Domain Consistency](https://arxiv.org/abs/2510.00356)
*Daniel G. Williams*

Main category: cs.SD

TL;DR: 提出了一种基于STFT域残差掩码预测的实时去混响框架，使用U-Net架构估计残差混响掩码，在抑制后期反射的同时保留直接语音成分。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在抑制混响时难以保持语音清晰度，而同时预测幅度和相位的方法计算成本过高，无法满足实时应用的需求。

Method: 使用U-Net架构在STFT域预测残差混响掩码，结合二元交叉熵、残差幅度重建和时域一致性的混合目标函数。

Result: 实现了低延迟的去混响效果，适用于真实世界的语音和歌唱应用。

Conclusion: 该框架在准确性和效率之间取得了良好平衡，能够满足实时去混响应用的需求。

Abstract: Vocal dereverberation remains a challenging task in audio processing,
particularly for real-time applications where both accuracy and efficiency are
crucial. Traditional deep learning approaches often struggle to suppress
reverberation without degrading vocal clarity, while recent methods that
jointly predict magnitude and phase have significant computational cost. We
propose a real-time dereverberation framework based on residual mask prediction
in the short-time Fourier transform (STFT) domain. A U-Net architecture is
trained to estimate a residual reverberation mask that suppresses late
reflections while preserving direct speech components. A hybrid objective
combining binary cross-entropy, residual magnitude reconstruction, and
time-domain consistency further encourages both accurate suppression and
perceptual quality. Together, these components enable low-latency
dereverberation suitable for real-world speech and singing applications.

</details>


### [30] [SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing](https://arxiv.org/abs/2510.00395)
*Jiaye Tan,Haonan Luo,Linfeng Song,Shuaiqi Chen,Yishan Lyu,Zian Zhong,Roujia Wang,Daniel Jiang,Haoran Zhang,Jiaming Bai,Haoran Cheng,Q. Vera Liao,Hao-Wen Dong*

Main category: cs.SD

TL;DR: 提出了AS-KVHS方法，在保持音乐质量的同时实现约30%的推理加速，适用于多轨符号音乐生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的模型在推理速度和音乐质量之间存在权衡，传统加速技术会显著降低质量，而BPE方法在多轨设置下性能下降严重。

Method: 提出了Attribute-Specialized Key-Value Head Sharing (AS-KVHS)方法，适应音乐的结构化符号表示。

Result: 实现了约30%的推理加速，客观评估中仅有约0.4%的质量下降，主观听力测试中略有改善。

Conclusion: AS-KVHS是低延迟符号音乐生成的有效方法，同时发布了开源基准SAGE-Music，在生成质量上达到或超越最先进模型。

Abstract: Low-latency symbolic music generation is essential for real-time
improvisation and human-AI co-creation. Existing transformer-based models,
however, face a trade-off between inference speed and musical quality.
Traditional acceleration techniques such as embedding pooling significantly
degrade quality, while recently proposed Byte Pair Encoding (BPE) methods -
though effective on single-track piano data - suffer large performance drops in
multi-track settings, as revealed by our analysis. We propose
Attribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's
structured symbolic representation, achieving about 30% inference speedup with
only a negligible (about 0.4%) quality drop in objective evaluations and slight
improvements in subjective listening tests. Our main contributions are (1) the
first systematic study of BPE's generalizability in multi-track symbolic music,
and (2) the introduction of AS-KVHS for low-latency symbolic music generation.
Beyond these, we also release SAGE-Music, an open-source benchmark that matches
or surpasses state-of-the-art models in generation quality.

</details>


### [31] [PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation](https://arxiv.org/abs/2510.00485)
*Yujia Xiao,Liumeng Xue,Lei He,Xinyi Chen,Aemon Yat Fei Chiu,Wenjie Tian,Shaofei Zhang,Qiuqiang Kong,Xinfa Zhu,Wei Xue,Tan Lee*

Main category: cs.SD

TL;DR: 提出了PodEval框架，用于评估开放式长音频生成能力，特别是播客类音频生成。通过构建真实播客数据集、多模态评估策略和多种评估方法来解决无参考标准、无统一指标和人工评估不可控的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基准主要关注模型理解能力，对生成能力的评估有限，尤其是在开放式长内容生成方面。存在无参考标准答案、无统一评估指标和人工评估不可控等挑战。

Method: 1) 构建真实世界播客数据集作为参考标准；2) 引入多模态评估策略，将任务分解为文本、语音和音频三个维度，分别关注"内容"和"格式"；3) 为每个模态设计评估方法，包括客观指标和主观听力测试。

Result: 实验使用了代表性播客生成系统（开源、闭源和人工制作），结果提供了对播客生成的深入分析和见解，证明了PodEval在评估开放式长音频方面的有效性。

Conclusion: PodEval是一个全面且设计良好的开源评估框架，能够有效评估开放式长音频生成能力，项目已开源供公众使用。

Abstract: Recently, an increasing number of multimodal (text and audio) benchmarks have
emerged, primarily focusing on evaluating models' understanding capability.
However, exploration into assessing generative capabilities remains limited,
especially for open-ended long-form content generation. Significant challenges
lie in no reference standard answer, no unified evaluation metrics and
uncontrollable human judgments. In this work, we take podcast-like audio
generation as a starting point and propose PodEval, a comprehensive and
well-designed open-source evaluation framework. In this framework: 1) We
construct a real-world podcast dataset spanning diverse topics, serving as a
reference for human-level creative quality. 2) We introduce a multimodal
evaluation strategy and decompose the complex task into three dimensions: text,
speech and audio, with different evaluation emphasis on "Content" and "Format".
3) For each modality, we design corresponding evaluation methods, involving
both objective metrics and subjective listening test. We leverage
representative podcast generation systems (including open-source, close-source,
and human-made) in our experiments. The results offer in-depth analysis and
insights into podcast generation, demonstrating the effectiveness of PodEval in
evaluating open-ended long-form audio. This project is open-source to
facilitate public use: https://github.com/yujxx/PodEval.

</details>


### [32] [ARIONet: An Advanced Self-supervised Contrastive Representation Network for Birdsong Classification and Future Frame Prediction](https://arxiv.org/abs/2510.00522)
*Md. Abdur Rahman,Selvarajah Thuseethan,Kheng Cher Yeo,Reem E. Mohamed,Sami Azam*

Main category: cs.SD

TL;DR: 提出ARIONet自监督对比网络，结合对比分类和未来帧预测，使用增强音频表征进行鸟类鸣声分类，在多个数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有鸟类鸣声分类方法依赖标注数据、特征表示有限且忽略时间动态特性，需要开发不依赖大规模标注的自监督方法。

Method: 使用自监督对比网络ARIONet，联合优化对比分类和未来帧预测，整合多种音频特征，通过Transformer编码器学习物种特异性表示和时间动态。

Result: 在四个鸟类鸣声数据集上表现优异：准确率98.41%、93.07%、91.89%、91.58%，F1分数97.84%、94.10%、91.29%、90.94%，未来帧预测余弦相似度达95%。

Conclusion: 该方法能有效捕捉复杂声学模式和时间依赖关系，在生态保护和监测中具有实际应用潜力。

Abstract: Automated birdsong classification is essential for advancing ecological
monitoring and biodiversity studies. Despite recent progress, existing methods
often depend heavily on labeled data, use limited feature representations, and
overlook temporal dynamics essential for accurate species identification. In
this work, we propose a self-supervised contrastive network, ARIONet (Acoustic
Representation for Interframe Objective Network), that jointly optimizes
contrastive classification and future frame prediction using augmented audio
representations. The model simultaneously integrates multiple complementary
audio features within a transformer-based encoder model. Our framework is
designed with two key objectives: (1) to learn discriminative species-specific
representations for contrastive learning through maximizing similarity between
augmented views of the same audio segment while pushing apart different
samples, and (2) to model temporal dynamics by predicting future audio frames,
both without requiring large-scale annotations. We validate our framework on
four diverse birdsong datasets, including the British Birdsong Dataset, Bird
Song Dataset, and two extended Xeno-Canto subsets (A-M and N-Z). Our method
consistently outperforms existing baselines and achieves classification
accuracies of 98.41%, 93.07%, 91.89%, and 91.58%, and F1-scores of 97.84%,
94.10%, 91.29%, and 90.94%, respectively. Furthermore, it demonstrates low mean
absolute errors and high cosine similarity, up to 95%, in future frame
prediction tasks. Extensive experiments further confirm the effectiveness of
our self-supervised learning strategy in capturing complex acoustic patterns
and temporal dependencies, as well as its potential for real-world
applicability in ecological conservation and monitoring.

</details>


### [33] [When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2510.00626)
*Chen-An Li,Tzu-Han Lin,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现，在大型音频-语言模型中，即使是不相关的音频（如静音、合成噪声和环境声音）也会干扰文本推理任务的性能，降低准确性并增加预测波动性。


<details>
  <summary>Details</summary>
Motivation: 探索大型音频-语言模型在嘈杂现实环境中的鲁棒性，特别是研究不相关音频对文本推理任务的影响。

Method: 在三个文本基准测试上评估不同模型，测试静音、合成噪声和环境声音等不相关音频的干扰效应，并分析持续时间、振幅和解码温度的影响。

Result: 不相关音频会显著降低模型准确性并增加预测波动性，干扰严重程度随音频持续时间、振幅和解码温度增加而加剧。静音的干扰效果与合成噪声相当。

Conclusion: 跨模态干扰是音频-语言模型鲁棒性的关键挑战，需要开发能有效融合策略以在存在不相关输入时保持推理性能。

Abstract: Large audio-language models (LALMs) unify speech and text processing, but
their robustness in noisy real-world settings remains underexplored. We
investigate how irrelevant audio, such as silence, synthetic noise, and
environmental sounds, affects text reasoning tasks where audio is unnecessary.
Across three text-based benchmarks, we find that even non-informative audio
reduces accuracy and increases prediction volatility; the severity of
interference scales with longer durations, higher amplitudes, and elevated
decoding temperatures. Silence, often assumed neutral, destabilizes outputs as
strongly as synthetic noise. While larger models show greater resilience,
vulnerabilities persist across all evaluated systems. We further test
mitigation strategies and find that prompting shows limited effectiveness,
whereas self-consistency improves stability at the cost of increased
computation. Our results reveal cross-modal interference as a key robustness
challenge and highlight the need for efficient fusion strategies that preserve
reasoning performance in the presence of irrelevant inputs.

</details>


### [34] [Hearing the Order: Investigating Selection Bias in Large Audio-Language Models](https://arxiv.org/abs/2510.00628)
*Yu-Xiang Lin,Chen-An Li,Sheng-Lun Wei,Po-Chun Chen,Hsin-Hsi Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 研究发现大型音频语言模型在多项选择任务中存在选项顺序偏见，通过打乱选项顺序可导致性能波动达24%，并影响模型排名。


<details>
  <summary>Details</summary>
Motivation: 探究大型音频语言模型在推理任务中是否受到答案选项顺序的影响，这种选择偏见会削弱模型的可靠性。

Method: 在三个广泛使用的基准测试及其语音版本上对六个大型音频语言模型进行广泛实验，通过打乱答案选项顺序来测试模型偏见。

Result: 所有模型都存在选项顺序偏见，性能波动可达24%，模型排名也会改变。基于排列的策略在大多数情况下可以减轻这种偏见。

Conclusion: 这是对大型音频语言模型中选项顺序偏见的首次系统研究，希望提高认识并推动该方向的进一步研究。

Abstract: Large audio-language models (LALMs) are often used in tasks that involve
reasoning over ordered options. An open question is whether their predictions
are influenced by the order of answer choices, which would indicate a form of
selection bias and undermine their reliability. In this paper, we identify and
analyze this problem in LALMs. We demonstrate that no model is immune to this
bias through extensive experiments on six LALMs across three widely used
benchmarks and their spoken counterparts. Shuffling the order of answer options
can cause performance fluctuations of up to 24% and even change model rankings,
raising concerns about the reliability of current evaluation practices. We also
study permutation-based strategies and show that they can mitigate bias in most
cases. Our work represents the first systematic investigation of this issue in
LALMs, and we hope it raises awareness and motivates further research in this
direction.

</details>


### [35] [Reference-free automatic speech severity evaluation using acoustic unit language modelling](https://arxiv.org/abs/2510.00639)
*Bence Mark Halpern,Tomoki Toda*

Main category: cs.SD

TL;DR: SpeechLMScore是一种无需参考语音的无参考语音严重性评估方法，在噪声环境下表现稳健，优于传统基于声学特征的方法。


<details>
  <summary>Details</summary>
Motivation: 随着语音障碍经济负担增加，当前语音严重性模型存在泛化能力差、依赖参考语音或文本的问题，限制了在真实场景中的应用。

Method: 提出SpeechLMScore无参考方法，不依赖病理语音数据，利用语音自然度评分与严重性评分的强相关性。同时构建NKI-SpeechRT数据集。

Result: SpeechLMScore对噪声具有鲁棒性，性能优于传统方法，并评估了无参考与有参考模型之间的性能差距。

Conclusion: SpeechLMScore为语音严重性评估提供了有效的无参考解决方案，在真实场景中具有更好的适用性。

Abstract: Speech severity evaluation is becoming increasingly important as the economic
burden of speech disorders grows. Current speech severity models often struggle
with generalization, learning dataset-specific acoustic cues rather than
meaningful correlates of speech severity. Furthermore, many models require
reference speech or a transcript, limiting their applicability in ecologically
valid scenarios, such as spontaneous speech evaluation. Previous research
indicated that automatic speech naturalness evaluation scores correlate
strongly with severity evaluation scores, leading us to explore a
reference-free method, SpeechLMScore, which does not rely on pathological
speech data. Additionally, we present the NKI-SpeechRT dataset, based on the
NKI-CCRT dataset, to provide a more comprehensive foundation for speech
severity evaluation. This study evaluates whether SpeechLMScore outperforms
traditional acoustic feature-based approaches and assesses the performance gap
between reference-free and reference-based models. Moreover, we examine the
impact of noise on these models by utilizing subjective noise ratings in the
NKI-SpeechRT dataset. The results demonstrate that SpeechLMScore is robust to
noise and offers superior performance compared to traditional approaches.

</details>


### [36] [XPPG-PCA: Reference-free automatic speech severity evaluation with principal components](https://arxiv.org/abs/2510.00657)
*Bence Mark Halpern,Thomas B. Tienkamp,Teja Rebernik,Rob J. J. H. van Son,Sebastiaan A. H. J. de Visscher,Max J. H. Witjes,Defne Abur,Tomoki Toda*

Main category: cs.SD

TL;DR: XPPG-PCA是一种新颖的无监督、无参考的语音病理严重程度评估方法，在三个荷兰口腔癌数据集上表现优于或与现有基于参考的方法相当，具有临床应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前依赖言语病理学专家评估存在主观性、耗时且成本高的问题，限制了临床研究的可重复性并增加了医疗资源负担。现有自动化方法存在各种缺陷：基于参考的方法需要转录或健康语音样本，限制了应用范围；现有的无参考方法存在学习数据伪捷径或特征不可靠的问题。

Method: 提出了XPPG-PCA方法，这是一种无监督、无参考的语音严重程度评估方法，基于x-vector语音后验图主成分分析技术。

Result: 在三个荷兰口腔癌数据集上的实验表明，XPPG-PCA的表现与现有基于参考的方法相当或更优，且对数据捷径和噪声具有鲁棒性。

Conclusion: XPPG-PCA为语音病理的客观评估提供了一个鲁棒、可泛化的解决方案，有潜力显著提高临床评估的效率和可靠性，已提供开源实现。

Abstract: Reliably evaluating the severity of a speech pathology is crucial in
healthcare. However, the current reliance on expert evaluations by
speech-language pathologists presents several challenges: while their
assessments are highly skilled, they are also subjective, time-consuming, and
costly, which can limit the reproducibility of clinical studies and place a
strain on healthcare resources. While automated methods exist, they have
significant drawbacks. Reference-based approaches require transcriptions or
healthy speech samples, restricting them to read speech and limiting their
applicability. Existing reference-free methods are also flawed; supervised
models often learn spurious shortcuts from data, while handcrafted features are
often unreliable and restricted to specific speech tasks. This paper introduces
XPPG-PCA (x-vector phonetic posteriorgram principal component analysis), a
novel, unsupervised, reference-free method for speech severity evaluation.
Using three Dutch oral cancer datasets, we demonstrate that XPPG-PCA performs
comparably to, or exceeds established reference-based methods. Our experiments
confirm its robustness against data shortcuts and noise, showing its potential
for real-world clinical use. Taken together, our results show that XPPG-PCA
provides a robust, generalizable solution for the objective assessment of
speech pathology, with the potential to significantly improve the efficiency
and reliability of clinical evaluations across a range of disorders. An
open-source implementation is available.

</details>


### [37] [From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling](https://arxiv.org/abs/2510.00743)
*Yifei Cao,Changhao Jiang,Jiabao Zhuang,Jiajun Sun,Ming Zhang,Zhiheng Xi,Hui Li,Shihan Dou,Yuran Wang,Yunke Zhang,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: 提出了MOS-RMBench基准，将多种MOS数据集转化为偏好比较设置，系统评估了三种奖励建模范式，发现标量模型性能最佳，并提出MOS感知生成奖励模型来提升细粒度质量判别能力。


<details>
  <summary>Details</summary>
Motivation: 传统语音质量评估依赖人工主观评分（如MOS），存在标注成本高、评分标准不一致、可复现性差等问题，需要自动化的评估方法。

Method: 构建MOS-RMBench基准，系统评估标量奖励模型、半标量奖励模型和生成奖励模型三种范式，并提出MOS感知GRM，通过MOS差异自适应调整奖励函数。

Result: 标量模型整体性能最强（准确率超过74%）；多数模型在合成语音上表现明显差于人类语音；所有模型在MOS差异很小的样本对上表现不佳；MOS感知GRM显著提升了细粒度质量判别能力。

Conclusion: 该工作建立了基准和方法框架，为自动语音质量评估提供了更严谨和可扩展的研究基础。

Abstract: Assessing the perceptual quality of synthetic speech is crucial for guiding
the development and refinement of speech generation models. However, it has
traditionally relied on human subjective ratings such as the Mean Opinion Score
(MOS), which depend on manual annotations and often suffer from inconsistent
rating standards and poor reproducibility. To address these limitations, we
introduce MOS-RMBench, a unified benchmark that reformulates diverse MOS
datasets into a preference-comparison setting, enabling rigorous evaluation
across different datasets. Building on MOS-RMBench, we systematically construct
and evaluate three paradigms for reward modeling: scalar reward models,
semi-scalar reward models, and generative reward models (GRMs). Our experiments
reveal three key findings: (1) scalar models achieve the strongest overall
performance, consistently exceeding 74% accuracy; (2) most models perform
considerably worse on synthetic speech than on human speech; and (3) all models
struggle on pairs with very small MOS differences. To improve performance on
these challenging pairs, we propose a MOS-aware GRM that incorporates an
MOS-difference-based reward function, enabling the model to adaptively scale
rewards according to the difficulty of each sample pair. Experimental results
show that the MOS-aware GRM significantly improves fine-grained quality
discrimination and narrows the gap with scalar models on the most challenging
cases. We hope this work will establish both a benchmark and a methodological
framework to foster more rigorous and scalable research in automatic speech
quality assessment.

</details>


### [38] [FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates](https://arxiv.org/abs/2510.00981)
*Jiaqi Li,Yao Qian,Yuxuan Hu,Leying Zhang,Xiaofei Wang,Heng Lu,Manthan Thakker,Jinyu Li,Shang Zhao,Zhizheng Wu*

Main category: cs.SD

TL;DR: FlexiCodec是一种动态帧率的神经音频编解码器，通过自适应合并语义相似帧来降低帧率，同时保持语义信息完整性，支持3Hz到12.5Hz的可控帧率。


<details>
  <summary>Details</summary>
Motivation: 现有12.5Hz低帧率音频编解码器在进一步降低帧率时会丢失语义信息，需要开发更低帧率且能保持语义完整性的音频编解码器。

Method: 采用动态帧率方法，通过ASR特征辅助的双流编码和Transformer瓶颈结构，自适应合并语义相似帧来降低帧率。

Result: 在6.25Hz、8.3Hz和12.5Hz平均帧率下，FlexiCodec在语义信息保持和音频重建质量方面均优于基线系统。

Conclusion: FlexiCodec成功解决了极低帧率下语义信息丢失的问题，在语言模型TTS中验证了其有效性，为语音语言模型提供了更高效的音频表示。

Abstract: Neural audio codecs are foundational to speech language models. It is
expected to have a low frame rate and decoupled semantic and acoustic
information. A lower frame rate codec can reduce the computational cost of
speech language models by shortening the sequence length. Recent studies have
developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs
remain underexplored. We find that a major challenge for very low frame rate
tokens is missing semantic information. This paper introduces FlexiCodec to
address this limitation. FlexiCodec improves semantic preservation with a
dynamic frame rate approach and introduces a novel architecture featuring an
ASR feature-assisted dual stream encoding and Transformer bottlenecks. With
dynamic frame rates, it uses less frames at information-sparse regions through
adaptively merging semantically similar frames. A dynamic frame rate also
allows FlexiCodec to support inference-time controllable frame rates between
3Hz and 12.5Hz. Experiments on 6.25Hz, 8.3Hz and 12.5Hz average frame rates
confirm that FlexiCodec excels over baseline systems in semantic information
preservation and delivers a high audio reconstruction quality. We also validate
the effectiveness of FlexiCodec in language model-based TTS. Demos are
available at: https://flexicodec.github.io

</details>


### [39] [HVAC-EAR: Eavesdropping Human Speech Using HVAC Systems](https://arxiv.org/abs/2510.01082)
*Tarikul Islam Tamiti,Biraj Joshi,Rida Hasan,Anomadarshi Barua*

Main category: cs.SD

TL;DR: HVAC-EAR系统能够从HVAC系统中低分辨率、嘈杂的压力传感器数据中重建可理解的语音，首次在真实HVAC部署中实现显著可理解性，引发新的隐私担忧。


<details>
  <summary>Details</summary>
Motivation: HVAC系统中的压力传感器对声压敏感，可能成为窃听源，需要研究从这些传感器数据中重建语音的可能性。

Method: 采用复数变换器(complex-valued conformer)和复数统一注意力块(Complex Unified Attention Block)来捕捉音素依赖关系，并重建缺失频率的幅度和相位以缓解瞬态HVAC噪声。

Result: 从低至0.5 kHz采样率的数据中实现可理解的语音重建，超越了仅限于热词检测的先前工作，在真实HVAC部署评估中显示出显著可理解性。

Conclusion: HVAC-EAR首次证明了从HVAC压力传感器数据中重建可理解语音的可行性，这引发了关于HVAC系统隐私安全的新担忧。

Abstract: Pressure sensors are widely integrated into modern Heating, Ventilation and
Air Conditioning (HVAC) systems. As they are sensitive to acoustic pressure,
they can be a source of eavesdropping. This paper introduces HVAC-EAR, which
reconstructs intelligible speech from low-resolution, noisy pressure data with
two key contributions: (i) We achieve intelligible reconstruction from as low
as 0.5 kHz sampling rate, surpassing prior work limited to hot word detection,
by employing a complex-valued conformer with a Complex Unified Attention Block
to capture phoneme dependencies; (ii) HVAC-EAR mitigates transient HVAC noise
by reconstructing both magnitude and phase of missing frequencies. For the
first time, evaluations on real-world HVAC deployments show significant
intelligibility, raising novel privacy concerns.

</details>


### [40] [NLDSI-BWE: Non Linear Dynamical Systems-Inspired Multi Resolution Discriminators for Speech Bandwidth Extension](https://arxiv.org/abs/2510.01109)
*Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.SD

TL;DR: 提出了两种基于非线性动力学的判别器MSRD和MRLD，通过显式建模语音的确定性混沌特性，在音频带宽扩展任务中实现了判别器参数数量44倍的减少。


<details>
  <summary>Details</summary>
Motivation: 传统音频带宽扩展方法未能充分利用语音固有的非线性混沌物理特性，本文旨在通过显式建模语音的确定性混沌来提升性能并大幅减少模型参数。

Method: 设计了两种非线性动力学判别器：基于递归表示的MSRD捕捉自相似动态，基于李雅普诺夫指数的MRLD捕捉非线性波动和初始条件敏感性，使用深度可分离卷积优化设计。

Result: 提出的框架超越了先前的AP-BWE模型，判别器参数从约2200万减少到约48万，实现了44倍的参数减少。

Conclusion: 首次证明可以通过语音产生过程中的非线性混沌物理特性来监督带宽扩展任务，实现了判别器尺寸的显著减小。

Abstract: In this paper, we design two nonlinear dynamical systems-inspired
discriminators -- the Multi-Scale Recurrence Discriminator (MSRD) and the
Multi-Resolution Lyapunov Discriminator (MRLD) -- to \textit{explicitly} model
the inherent deterministic chaos of speech. MSRD is designed based on
Recurrence representations to capture self-similarity dynamics. MRLD is
designed based on Lyapunov exponents to capture nonlinear fluctuations and
sensitivity to initial conditions. Through extensive design optimization and
the use of depthwise-separable convolutions in the discriminators, our
framework surpasses prior AP-BWE model with a 44x reduction in the
discriminator parameter count \textbf{($\sim$ 22M vs $\sim$ 0.48M)}. To the
best of our knowledge, for the first time, this paper demonstrates how BWE can
be supervised by the subtle non-linear chaotic physics of voiced sound
production to achieve a significant reduction in the discriminator size.

</details>

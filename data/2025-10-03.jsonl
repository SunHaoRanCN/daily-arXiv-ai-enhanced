{"id": "2510.01213", "categories": ["eess.SP", "cs.AR", "cs.CV", "cs.HC", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.01213", "abs": "https://arxiv.org/abs/2510.01213", "authors": ["Tao Han", "Ang Li", "Qinyu Chen", "Chang Gao"], "title": "JaneEye: A 12-nm 2K-FPS 18.9-$μ$J/Frame Event-based Eye Tracking Accelerator", "comment": "Accepted to 2026 IEEE 31st Asia and South Pacific Design Automation\n  Conference (ASP-DAC) 2026", "summary": "Eye tracking has become a key technology for gaze-based interactions in\nExtended Reality (XR). However, conventional frame-based eye-tracking systems\noften fall short of XR's stringent requirements for high accuracy, low latency,\nand energy efficiency. Event cameras present a compelling alternative, offering\nultra-high temporal resolution and low power consumption. In this paper, we\npresent JaneEye, an energy-efficient event-based eye-tracking hardware\naccelerator designed specifically for wearable devices, leveraging sparse,\nhigh-temporal-resolution event data. We introduce an ultra-lightweight neural\nnetwork architecture featuring a novel ConvJANET layer, which simplifies the\ntraditional ConvLSTM by retaining only the forget gate, thereby halving\ncomputational complexity without sacrificing temporal modeling capability. Our\nproposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+\ndataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To\nfurther enhance hardware efficiency, we employ custom linear approximations of\nactivation functions (hardsigmoid and hardtanh) and fixed-point quantization.\nThrough software-hardware co-design, our 12-nm ASIC implementation operates at\n400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames\nPer Second (FPS)) at an energy efficiency of 18.9 $\\mu$J/frame. JaneEye sets a\nnew benchmark in low-power, high-performance eye-tracking solutions suitable\nfor integration into next-generation XR wearables."}
{"id": "2510.01408", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01408", "abs": "https://arxiv.org/abs/2510.01408", "authors": ["Jeong Min Kong", "Ian P. Roberts"], "title": "Satellite Assignment Policy Learning for Coexistence in LEO Networks", "comment": null, "summary": "Unlike in terrestrial cellular networks, certain frequency bands for\nlow-earth orbit (LEO) satellite systems have thus far been allocated on a\nnon-exclusive basis. In this context, systems that launch their satellites\nearlier (referred to as primary systems) are given spectrum access priority\nover those that launch later, known as secondary systems. For a secondary\nsystem to function, it is expected to either coordinate with primary systems or\nensure that it does not cause excessive interference to primary ground users.\nReliably meeting this interference constraint requires real-time knowledge of\nthe receive beams of primary users, which in turn depends on the primary\nsatellite-to-primary user associations. However, in practice, primary systems\nhave thus far not publicly disclosed their satellite assignment policies;\ntherefore, it becomes essential for secondary systems to develop methods to\ninfer such policies. Assuming there is limited historical data indicating which\nprimary satellites have served which primary users, we propose an end-to-end\ngraph structure learning-based algorithm for learning highest elevation primary\nsatellite assignment policies, that, upon deployment, can directly map the\nprimary satellite coordinates into assignment decisions for the primary users.\nSimulation results show that our method can outperform the best baseline,\nachieving approximately a 15% improvement in prediction accuracy."}
{"id": "2510.01411", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01411", "abs": "https://arxiv.org/abs/2510.01411", "authors": ["Hibatallah Alwazani", "Omran Abbas", "Loic Markley", "Anas Chaaban"], "title": "Delay-Augmented Stacked Intelligent Surfaces: Potential, Challenges, and Opportunities", "comment": "7 pages, 3 figures", "summary": "Stacked intelligent surfaces (SIS)s have been proposed recently as an\nenabling technology for Holographic Multiple Input Multiple Output (HMIMO) and\nUltra-massive MIMO (umMIMO) technologies. Their utility can extend beyond\nspatial wave-domain processing of signals if they are enhanced with\nstrategically-tuned symbol-duration level delays to enable temporal processing\nas well. In this work, we introduce the idea of a delay-augmented SIS (DA-SIS).\nWe shed light on the feasibility of realizing delay units in an SIS. Then, we\ndiscuss the relevance of the proposed DA-SIS and present a use case that\nillustrates its potential, wherein the DA-SIS serves as an analog equalizer\nthat aids in eliminating multi-path-induced inter-symbol-interference (ISI). We\nshow how the number of elements affect the equalization process using the bit\nerror rate (BER) as a metric, and demonstrate the potential of the DA-SIS in\nequalization via comparing with digital equalizers as a benchmark. Finally, we\npresent opportunities and future research directions that can be undertaken to\nbring this idea to fruition."}
{"id": "2510.01417", "categories": ["eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.01417", "abs": "https://arxiv.org/abs/2510.01417", "authors": ["Alex Paul Hoffmann", "Matthew G. Finley", "Eftyhia Zesta", "Mark B. Moldwin", "Lauro V. Ojeda"], "title": "A Drone-mounted Magnetometer System for Automatic Interference Removal and Landmine Detection", "comment": "13 pages, 5 figures", "summary": "Landmines have been extensively used in conflict zones as an indiscriminate\nweapon to control military movements, often remaining active long after\nhostilities have ended. Their presence poses a persistent danger to civilians,\nhindering post-war recovery efforts, causing injuries or death, and restricting\naccess to essential land for agriculture and infrastructure. Unmanned aerial\nvehicles (UAV) equipped with magnetometers are commonly used to detect remnant\nhidden landmines but come with significant technical challenges due to magnetic\nfield interference from UAV electronics such as motors. We propose the use of a\nframe-mounted UAV-borne two-magnetometer payload to perform a two-step\nautomated interference removal and landmine detection analysis. The first step\nremoves interference via the Wavelet-Adaptive Interference Cancellation for\nUnderdetermined Platform (WAIC-UP) method designed for spaceflight\nmagnetometers. The second method uses the Rapid Unsupervised Detection of\nEvents (RUDE) algorithm to detect landmine signatures. This two-step\nWAIC-UP/RUDE approach with multiple magnetometers achieves high-fidelity\nordinance detection at a low computational cost and simplifies the design of\nmagnetic survey payloads. We validate the method through a Monte Carlo\nsimulation of randomized landmine placements in a 10 x 10 m square grid and\ndrone motor interference. Additionally, we assess the efficacy of the algorithm\nby varying the drone's altitude, examining its performance at different heights\nabove the ground."}
{"id": "2510.01818", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01818", "abs": "https://arxiv.org/abs/2510.01818", "authors": ["Oğuzhan Kurnaz", "Jagabandhu Mishra", "Tomi H. Kinnunen", "Cemal Hanilçi"], "title": "Joint Optimization of Speaker and Spoof Detectors for Spoofing-Robust Automatic Speaker Verification", "comment": null, "summary": "Spoofing-robust speaker verification (SASV) combines the tasks of speaker and\nspoof detection to authenticate speakers under adversarial settings. Many SASV\nsystems rely on fusion of speaker and spoof cues at embedding, score or\ndecision levels, based on independently trained subsystems. In this study, we\nrespect similar modularity of the two subsystems, by integrating their outputs\nusing trainable back-end classifiers. In particular, we explore various\napproaches for directly optimizing the back-end for the recently-proposed SASV\nperformance metric (a-DCF) as a training objective. Our experiments on the\nASVspoof 5 dataset demonstrate two important findings: (i) nonlinear score\nfusion consistently improves a-DCF over linear fusion, and (ii) the combination\nof weighted cosine scoring for speaker detection with SSL-AASIST for spoof\ndetection achieves state-of-the-art performance, reducing min a-DCF to 0.196\nand SPF-EER to 7.6%. These contributions highlight the importance of modular\ndesign, calibrated integration, and task-aligned optimization for advancing\nrobust and interpretable SASV systems."}
{"id": "2510.01462", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01462", "abs": "https://arxiv.org/abs/2510.01462", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carol Espy Wilson"], "title": "RealClass: A Framework for Classroom Speech Simulation with Public Datasets and Game Engines", "comment": "arXiv admin note: substantial text overlap with arXiv:2506.09206", "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Classroom datasets remain\nlimited and not publicly available, and the absence of dedicated classroom\nnoise or Room Impulse Response (RIR) corpora prevents the use of standard data\naugmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise and RIRs using game engines, a versatile framework that can extend to\nother domains beyond the classroom. Building on this methodology, we present\nRealClass, a dataset that combines a synthesized classroom noise corpus with a\nclassroom speech dataset compiled from publicly available corpora. The speech\ndata pairs a children's speech corpus with instructional speech extracted from\nYouTube videos to approximate real classroom interactions in clean conditions.\nExperiments on clean and noisy speech show that RealClass closely approximates\nreal classroom speech, making it a valuable asset in the absence of abundant\nreal classroom speech."}
{"id": "2510.01437", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01437", "abs": "https://arxiv.org/abs/2510.01437", "authors": ["Ali Amhaz", "Shreya Khisa", "Mohamed Elhattab", "Chadi Assi", "Sanaa Sharafeddine"], "title": "Meta-Learning-Driven Resource Optimization in Full-Duplex ISAC with Movable Antennas", "comment": null, "summary": "This paper investigates a full-duplex (FD) scenario where a base station (BS)\nequipped with movable antennas (MAs) simultaneously provides communication\nservices to a set of downlink (DL) and uplink (UL) users while also enabling\nsensing functionalities for target detection, thereby supporting integrated\nsensing and communication (ISAC) technology. Additionally, a receiving BS, also\nequipped with MAs (denoted as BS R), is responsible for capturing the reflected\necho. To optimize this setup, we formulate an optimization problem aimed at\nmaximizing the signal-to-noise and interference ratio (SINR) of the captured\necho. This is achieved by jointly optimizing the transmit beamforming vectors\nat the FD BS, the receiving beamforming vectors at both the FD BS and BS R, the\nUL users' transmit power, and the MAs' positions at both BSs, all while\nsatisfying the quality-of-service (QoS) requirements for both sensing and\ncommunication. Given the non-convex nature of the problem and the high coupling\nbetween the variables, we employ a gradient-based meta-learning (GML) approach\ntailored for large-scale optimization. Numerical results demonstrate the\neffectiveness of the proposed meta-learning approach, achieving results within\n99% of the optimal solution. Furthermore, the MA-based scheme outperforms\nseveral benchmark approaches, highlighting its advantages in practical ISAC\napplications."}
{"id": "2510.01860", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.01860", "abs": "https://arxiv.org/abs/2510.01860", "authors": ["Angelika Ando", "Auguste Crabeil", "Adrien Lesage", "Rachid Riad"], "title": "SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision", "comment": null, "summary": "Speech encodes paralinguistic information such as demographics, voice\nquality, and health. Yet no audio foundation model supports zero-shot or\nout-of-distribution (OOD) generalization to these tasks. We introduce SLAP\n(Speaker contrastive Language-Audio Pretraining), the first model aligning\nspeech with natural language descriptions of speaker and health metadata\nthrough contrastive learning. SLAP combines a Vision Transformer audio encoder\nwith text encoders, trained on more than 3400 hours across 9 datasets with\ndiverse speaker annotations. We evaluated on 38 binary classification tasks\nspanning demographics, voice characteristics, and clinical assessments across\n14 datasets in 7 languages. SLAP achieves 62.9% average F1 in zero-shot\nevaluation, a 48% relative improvement over CLAP (42.4%), while demonstrating\nstrong OOD generalization to unseen languages and clinical populations. When\nfine-tuned with linear probing, SLAP reaches 69.3% F1 overall and achieves\nbest-in-class performance on health tasks (57.9% F1), surpassing larger\nfoundation models."}
{"id": "2510.01722", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01722", "abs": "https://arxiv.org/abs/2510.01722", "authors": ["Jianing Yang", "Sheng Li", "Takahiro Shinozaki", "Yuki Saito", "Hiroshi Saruwatari"], "title": "Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement", "comment": "In Proceedings of the 17th Asia Pacific Signal and Information\n  Processing Association Annual Summit and Conference (APSIPA ASC 2025)", "summary": "Current emotional Text-To-Speech (TTS) and style transfer methods rely on\nreference encoders to control global style or emotion vectors, but do not\ncapture nuanced acoustic details of the reference speech. To this end, we\npropose a novel emotional TTS method that enables fine-grained phoneme-level\nemotion embedding prediction while disentangling intrinsic attributes of the\nreference speech. The proposed method employs a style disentanglement method to\nguide two feature extractors, reducing mutual information between timbre and\nemotion features, and effectively separating distinct style components from the\nreference speech. Experimental results demonstrate that our method outperforms\nbaseline TTS systems in generating natural and emotionally rich speech. This\nwork highlights the potential of disentangled and fine-grained representations\nin advancing the quality and flexibility of emotional TTS systems."}
{"id": "2510.01605", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01605", "abs": "https://arxiv.org/abs/2510.01605", "authors": ["Hao Wu", "Xinyuan Yao", "Rui Ni", "Chen Gong"], "title": "The Analysis and Performance of LODC-OFDM Signal in Nonlinear Rydberg Atomic Sensor", "comment": null, "summary": "Rydberg atomic sensors have been seen as novel radio frequency (RF)\nmeasurements and the high sensitivity to a large range of frequencies makes it\nattractive for communications reception. However, the signal sensing process in\nRydberg system involves sequential transduction from electromagnetic waves to\noptical signals and finally to electrical signals. The unipolar characteristic\nof the optical interface inherently restricts conventional OFDM reception.\nTherefore, adopting unipolar OFDM schemes, inspired by optical communication\nsystems, becomes essential for compatible signal transmission. In this work, we\ninvestigate the amplitude modulation-to-amplitude modulation (AM-AM)\ncharacteristics of Rydberg atomic sensors, establishing an empirical\napproximation function. Building on the direct current-biased optical\northogonal frequency division multiplexing (DCO-OFDM) framework, we propose a\nnovel local oscillator direct current-biased OFDM (LODC-OFDM) scheme\nspecifically optimized for Rydberg-based sensing, effectively addressing the\nbroadband OFDM reception challenge. Then, we adopt Bussgang theorem to analyze\nthe nonlinear distortion of LODC-OFDM signals and the results in closed-form\nsolutions are derived for AM/AM curves approximated by Taylor series expansion\nand for the ideal pre-distortion case. In real experiments, the experimental\nand theoretical results fit well."}
{"id": "2510.01940", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01940", "abs": "https://arxiv.org/abs/2510.01940", "authors": ["Luan Vinícius Fiorio", "Ivana Nikoloska", "Wim van Houtum", "Ronald M. Aarts"], "title": "Clustering of Acoustic Environments with Variational Autoencoders for Hearing Devices", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Particularly in hearing devices, the environmental context is taken into\naccount for audio processing, often through classification. Traditional\nacoustic environment classification relies on classical algorithms, which are\nunable to extract meaningful representations of high-dimensionality data, or on\nsupervised learning, being limited by the availability of labels. Knowing that\nhuman-imposed labels do not always reflect the true structure of acoustic\nscenes, we explore the (unsupervised) clustering of acoustic environments using\nvariational autoencoders (VAEs), presenting a structured latent space suitable\nfor the task. We propose a VAE model for categorical latent clustering\nemploying a Gumbel-Softmax reparameterization with a time-context windowing\nscheme, tailored for real-world hearing device scenarios. Additionally, general\nadaptations on VAE architectures for audio clustering are also proposed. The\napproaches are validated through the clustering of spoken digits, a simpler\ntask where labels are meaningful, and urban soundscapes, which recordings\npresent strong overlap in time and frequency. While all variational methods\nsucceeded when clustering spoken digits, only the proposed model achieved\neffective clustering performance on urban acoustic scenes, given its\ncategorical nature."}
{"id": "2510.01812", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01812", "abs": "https://arxiv.org/abs/2510.01812", "authors": ["Yuxun Tang", "Lan Liu", "Wenhao Feng", "Yiwen Zhao", "Jionghao Han", "Yifeng Yu", "Jiatong Shi", "Qin Jin"], "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment", "comment": "4 pages, 5 figures; submitted to ICASSP 2026", "summary": "Singing voice generation progresses rapidly, yet evaluating singing quality\nremains a critical challenge. Human subjective assessment, typically in the\nform of listening tests, is costly and time consuming, while existing objective\nmetrics capture only limited perceptual aspects. In this work, we introduce\nSingMOS-Pro, a dataset for automatic singing quality assessment. Building on\nour preview version SingMOS, which provides only overall ratings, SingMOS-Pro\nexpands annotations of the additional part to include lyrics, melody, and\noverall quality, offering broader coverage and greater diversity. The dataset\ncontains 7,981 singing clips generated by 41 models across 12 datasets,\nspanning from early systems to recent advances. Each clip receives at least\nfive ratings from professional annotators, ensuring reliability and\nconsistency. Furthermore, we explore how to effectively utilize MOS data\nannotated under different standards and benchmark several widely used\nevaluation methods from related tasks on SingMOS-Pro, establishing strong\nbaselines and practical references for future research. The dataset can be\naccessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro."}
{"id": "2510.01707", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01707", "abs": "https://arxiv.org/abs/2510.01707", "authors": ["Amila Ravinath", "Minhua Ding", "Bikshapathi Gouda", "Italo Atzeni", "Antti Tölli"], "title": "SEP Analysis of 1-Bit Quantized SIMO Systems with QPSK over Fading Channels", "comment": "Accepted to Asilomar Conference on Signals, Systems, and Computers\n  2025", "summary": "The average symbol error probability (SEP) of a 1-bit quantized single-input\nmultiple-output (SIMO) system is analyzed under Rayleigh fading channels and\nquadrature phase-shift keying (QPSK) modulation. Previous studies have\npartially characterized the diversity gain for selection combining (SC). In\nthis paper, leveraging a novel analytical method, an exact analytical SEP\nexpression is derived for a 1-bit quantized SIMO system employing QPSK\nmodulation at the transmitter and maximum ratio combining (MRC) at the\nreceiver. The corresponding diversity and coding gains of a SIMO-MRC system are\nalso determined. Furthermore, the diversity and coding gains of a 1-bit\nquantized SIMO-SC system are quantified for an arbitrary number of receive\nantennas, thereby extending and complementing prior results."}
{"id": "2510.01462", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01462", "abs": "https://arxiv.org/abs/2510.01462", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carol Espy Wilson"], "title": "RealClass: A Framework for Classroom Speech Simulation with Public Datasets and Game Engines", "comment": "arXiv admin note: substantial text overlap with arXiv:2506.09206", "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Classroom datasets remain\nlimited and not publicly available, and the absence of dedicated classroom\nnoise or Room Impulse Response (RIR) corpora prevents the use of standard data\naugmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise and RIRs using game engines, a versatile framework that can extend to\nother domains beyond the classroom. Building on this methodology, we present\nRealClass, a dataset that combines a synthesized classroom noise corpus with a\nclassroom speech dataset compiled from publicly available corpora. The speech\ndata pairs a children's speech corpus with instructional speech extracted from\nYouTube videos to approximate real classroom interactions in clean conditions.\nExperiments on clean and noisy speech show that RealClass closely approximates\nreal classroom speech, making it a valuable asset in the absence of abundant\nreal classroom speech."}
{"id": "2510.01891", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01891", "abs": "https://arxiv.org/abs/2510.01891", "authors": ["Xuyi Hu", "Jian Li", "Shaojie Zhang", "Stefan Goetz", "Lorenzo Picinali", "Ozgur B. Akan", "Aidan O. T. Hogg"], "title": "HRTFformer: A Spatially-Aware Transformer for Personalized HRTF Upsampling in Immersive Audio Rendering", "comment": "10 pages and 5 figures", "summary": "Personalized Head-Related Transfer Functions (HRTFs) are starting to be\nintroduced in many commercial immersive audio applications and are crucial for\nrealistic spatial audio rendering. However, one of the main hesitations\nregarding their introduction is that creating personalized HRTFs is impractical\nat scale due to the complexities of the HRTF measurement process. To mitigate\nthis drawback, HRTF spatial upsampling has been proposed with the aim of\nreducing measurements required. While prior work has seen success with\ndifferent machine learning (ML) approaches, these models often struggle with\nlong-range spatial consistency and generalization at high upsampling factors.\nIn this paper, we propose a novel transformer-based architecture for HRTF\nupsampling, leveraging the attention mechanism to better capture spatial\ncorrelations across the HRTF sphere. Working in the spherical harmonic (SH)\ndomain, our model learns to reconstruct high-resolution HRTFs from sparse input\nmeasurements with significantly improved accuracy. To enhance spatial\ncoherence, we introduce a neighbor dissimilarity loss that promotes magnitude\nsmoothness, yielding more realistic upsampling. We evaluate our method using\nboth perceptual localization models and objective spectral distortion metrics.\nExperiments show that our model surpasses leading methods by a substantial\nmargin in generating realistic, high-fidelity HRTFs."}
{"id": "2510.01748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01748", "abs": "https://arxiv.org/abs/2510.01748", "authors": ["Hadi Zayyani", "Felipe A. P. de Figueiredo", "Mohammad Salman", "Rausley A. A. de Souza"], "title": "3D 8-Ary Noise Modulation Using Bayesian- and Kurtosis-based Detectors", "comment": null, "summary": "This paper presents a novel three-dimensional (3D) 8-ary noise modulation\nscheme that introduces a new dimension: the mixture probability of a Mixture of\nGaussian (MoG) distribution. This proposed approach utilizes the dimensions of\nmean and variance, in addition to the new probability dimension. Within this\nframework, each transmitted symbol carries three bits, each corresponding to a\ndistinct sub-channel. For detection, a combination of specialized detectors is\nemployed: a simple threshold based detector for the first sub-channel bit\n(modulated by the mean), a Maximum-Likelihood (ML) detector for the second\nsub-channel bit (modulated by the variance), a Kurtosis-based, Jarque-Bera (JB)\ntest, and Bayesian Hypothesis (BHT)-based detectors for the third bit\n(modulated by the MoG probability). The Kurtosis- and JB-based detectors\nspecifically distinguish between Gaussian (or near-Gaussian) and non-Gaussian\nMoG distributions by leveraging higher-order statistical measures. The Bit\nError Probabilities (BEPs) are derived for the threshold-, Kurtosis-, and\nBHT-based detectors. The optimum threshold for the Kurtosis-based detector is\nalso derived in a tractable manner. Simulation results demonstrate that a\ncomparably low BEP is achieved for the third sub-channel bit relative to\nexisting two-dimensional (2D) schemes. Simultaneously, the proposed scheme\nincreases the data rate by a factor of 1.5 and 3 compared to the Generalized\nQuadratic noise modulator and the classical binary KLJN noise modulator,\nrespectively. Furthermore, the Kurtosis-based detector offers a low-complexity\nsolution, achieving an acceptable BEP of approximately 0.06."}
{"id": "2510.01722", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01722", "abs": "https://arxiv.org/abs/2510.01722", "authors": ["Jianing Yang", "Sheng Li", "Takahiro Shinozaki", "Yuki Saito", "Hiroshi Saruwatari"], "title": "Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement", "comment": "In Proceedings of the 17th Asia Pacific Signal and Information\n  Processing Association Annual Summit and Conference (APSIPA ASC 2025)", "summary": "Current emotional Text-To-Speech (TTS) and style transfer methods rely on\nreference encoders to control global style or emotion vectors, but do not\ncapture nuanced acoustic details of the reference speech. To this end, we\npropose a novel emotional TTS method that enables fine-grained phoneme-level\nemotion embedding prediction while disentangling intrinsic attributes of the\nreference speech. The proposed method employs a style disentanglement method to\nguide two feature extractors, reducing mutual information between timbre and\nemotion features, and effectively separating distinct style components from the\nreference speech. Experimental results demonstrate that our method outperforms\nbaseline TTS systems in generating natural and emotionally rich speech. This\nwork highlights the potential of disentangled and fine-grained representations\nin advancing the quality and flexibility of emotional TTS systems."}
{"id": "2510.01903", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01903", "abs": "https://arxiv.org/abs/2510.01903", "authors": ["Jingyi Li", "Zhiyuan Zhao", "Yunfei Liu", "Lijian Lin", "Ye Zhu", "Jiahao Wu", "Qiuqiang Kong", "Yu Li"], "title": "MelCap: A Unified Single-Codebook Neural Codec for High-Fidelity Audio Compression", "comment": "9 pages, 4 figures", "summary": "Neural audio codecs have recently emerged as powerful tools for high-quality\nand low-bitrate audio compression, leveraging deep generative models to learn\nlatent representations of audio signals. However, existing approaches either\nrely on a single quantizer that only processes speech domain, or on multiple\nquantizers that are not well suited for downstream tasks. To address this\nissue, we propose MelCap, a unified \"one-codebook-for-all\" neural codec that\neffectively handles speech, music, and general sound. By decomposing audio\nreconstruction into two stages, our method preserves more acoustic details than\nprevious single-codebook approaches, while achieving performance comparable to\nmainstream multi-codebook methods. In the first stage, audio is transformed\ninto mel-spectrograms, which are compressed and quantized into compact single\ntokens using a 2D tokenizer. A perceptual loss is further applied to mitigate\nthe over-smoothing artifacts observed in spectrogram reconstruction. In the\nsecond stage, a Vocoder recovers waveforms from the mel discrete tokens in a\nsingle forward pass, enabling real-time decoding. Both objective and subjective\nevaluations demonstrate that MelCap achieves quality on comparable to\nstate-of-the-art multi-codebook codecs, while retaining the computational\nsimplicity of a single-codebook design, thereby providing an effective\nrepresentation for downstream tasks."}
{"id": "2510.01763", "categories": ["eess.SP", "math.OC", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.01763", "abs": "https://arxiv.org/abs/2510.01763", "authors": ["Xiao Ding", "Enbin Song", "Dunbiao Niu", "Zhujun Cao", "Qingjiang Shi"], "title": "Exactly or Approximately Wasserstein Distributionally Robust Estimation According to Wasserstein Radii Being Small or Large", "comment": null, "summary": "This paper primarily considers the robust estimation problem under\nWasserstein distance constraints on the parameter and noise distributions in\nthe linear measurement model with additive noise, which can be formulated as an\ninfinite-dimensional nonconvex minimax problem. We prove that the existence of\na saddle point for this problem is equivalent to that for a finite-dimensional\nminimax problem, and give a counterexample demonstrating that the saddle point\nmay not exist. Motivated by this observation, we present a verifiable necessary\nand sufficient condition whose parameters can be derived from a convex problem\nand its dual. Additionally, we also introduce a simplified sufficient\ncondition, which intuitively indicates that when the Wasserstein radii are\nsmall enough, the saddle point always exists. In the absence of the saddle\npoint, we solve an finite-dimensional nonconvex minimax problem, obtained by\nrestricting the estimator to be linear. Its optimal value establishes an upper\nbound on the robust estimation problem, while its optimal solution yields a\nrobust linear estimator. Numerical experiments are also provided to validate\nour theoretical results."}
{"id": "2510.01812", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01812", "abs": "https://arxiv.org/abs/2510.01812", "authors": ["Yuxun Tang", "Lan Liu", "Wenhao Feng", "Yiwen Zhao", "Jionghao Han", "Yifeng Yu", "Jiatong Shi", "Qin Jin"], "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment", "comment": "4 pages, 5 figures; submitted to ICASSP 2026", "summary": "Singing voice generation progresses rapidly, yet evaluating singing quality\nremains a critical challenge. Human subjective assessment, typically in the\nform of listening tests, is costly and time consuming, while existing objective\nmetrics capture only limited perceptual aspects. In this work, we introduce\nSingMOS-Pro, a dataset for automatic singing quality assessment. Building on\nour preview version SingMOS, which provides only overall ratings, SingMOS-Pro\nexpands annotations of the additional part to include lyrics, melody, and\noverall quality, offering broader coverage and greater diversity. The dataset\ncontains 7,981 singing clips generated by 41 models across 12 datasets,\nspanning from early systems to recent advances. Each clip receives at least\nfive ratings from professional annotators, ensuring reliability and\nconsistency. Furthermore, we explore how to effectively utilize MOS data\nannotated under different standards and benchmark several widely used\nevaluation methods from related tasks on SingMOS-Pro, establishing strong\nbaselines and practical references for future research. The dataset can be\naccessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro."}
{"id": "2510.01958", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01958", "abs": "https://arxiv.org/abs/2510.01958", "authors": ["Nikolai Lund Kühne", "Jesper Jensen", "Jan Østergaard", "Zheng-Hua Tan"], "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement", "comment": "Submitted to IEEE for possible publication", "summary": "Recent advances in speech enhancement have shown that models combining Mamba\nand attention mechanisms yield superior cross-corpus generalization\nperformance. At the same time, integrating Mamba in a U-Net structure has\nyielded state-of-the-art enhancement performance, while reducing both model\nsize and computational complexity. Inspired by these insights, we propose\nRWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and\nmulti-head attention in a U-Net structure for improved cross-corpus\nperformance. Resolution-wise shared attention (RWSA) refers to layerwise\nattention-sharing across corresponding time- and frequency resolutions. Our\nbest-performing RWSA-MambaUNet model achieves state-of-the-art generalization\nperformance on two out-of-domain test sets. Notably, our smallest model\nsurpasses all baselines on the out-of-domain DNS 2020 test set in terms of\nPESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms\nof SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and\na fraction of the FLOPs."}
{"id": "2510.01776", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01776", "abs": "https://arxiv.org/abs/2510.01776", "authors": ["Hadi Zayyani", "Mohammad Salman", "Felipe A. P. de Figueiredo", "Rausley A. A. de Souza"], "title": "Composite Generalized Quadratic Noise Modulation via Signal Addition: Towards Higher Dimensional Noise Modulations", "comment": null, "summary": "This letter proposes superposing two Generalized Quadratic Noise Modulators\n(GQNM) by simply adding their outputs. It creates a 16-ary noise modulator that\nresembles QAM modulators in classical communication. It modulates the\ninformation bits on four different means and four different variances. It could\nalso be applied to reach higher-order modulations than 16-ary schemes by adding\nthe outputs of more than two modulators, which is not discussed in detail in\nthis letter and left for future work. By selecting the parameters necessary for\nsatisfying the theoretical distinguishability conditions provided in the paper,\nwe can reach better performances in comparison to the Kirchhoff-Law Johnson\nNoise (KLJN) modulator and the GQNM modulator, which is verified by the\nsimulations. The better result in terms of smaller Bit Error Probability (BEP)\nis achieved by increasing the complexity in the modulator, the transmitter, and\nthe detectors in the receiver."}
{"id": "2510.01891", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01891", "abs": "https://arxiv.org/abs/2510.01891", "authors": ["Xuyi Hu", "Jian Li", "Shaojie Zhang", "Stefan Goetz", "Lorenzo Picinali", "Ozgur B. Akan", "Aidan O. T. Hogg"], "title": "HRTFformer: A Spatially-Aware Transformer for Personalized HRTF Upsampling in Immersive Audio Rendering", "comment": "10 pages and 5 figures", "summary": "Personalized Head-Related Transfer Functions (HRTFs) are starting to be\nintroduced in many commercial immersive audio applications and are crucial for\nrealistic spatial audio rendering. However, one of the main hesitations\nregarding their introduction is that creating personalized HRTFs is impractical\nat scale due to the complexities of the HRTF measurement process. To mitigate\nthis drawback, HRTF spatial upsampling has been proposed with the aim of\nreducing measurements required. While prior work has seen success with\ndifferent machine learning (ML) approaches, these models often struggle with\nlong-range spatial consistency and generalization at high upsampling factors.\nIn this paper, we propose a novel transformer-based architecture for HRTF\nupsampling, leveraging the attention mechanism to better capture spatial\ncorrelations across the HRTF sphere. Working in the spherical harmonic (SH)\ndomain, our model learns to reconstruct high-resolution HRTFs from sparse input\nmeasurements with significantly improved accuracy. To enhance spatial\ncoherence, we introduce a neighbor dissimilarity loss that promotes magnitude\nsmoothness, yielding more realistic upsampling. We evaluate our method using\nboth perceptual localization models and objective spectral distortion metrics.\nExperiments show that our model surpasses leading methods by a substantial\nmargin in generating realistic, high-fidelity HRTFs."}
{"id": "2510.01963", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01963", "abs": "https://arxiv.org/abs/2510.01963", "authors": ["Ahmet Solak", "Florian Grötschla", "Luca A. Lanzendörfer", "Roger Wattenhofer"], "title": "Bias beyond Borders: Global Inequalities in AI-Generated Music", "comment": null, "summary": "While recent years have seen remarkable progress in music generation models,\nresearch on their biases across countries, languages, cultures, and musical\ngenres remains underexplored. This gap is compounded by the lack of datasets\nand benchmarks that capture the global diversity of music. To address these\nchallenges, we introduce GlobalDISCO, a large-scale dataset consisting of 73k\nmusic tracks generated by state-of-the-art commercial generative music models,\nalong with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset\nspans 147 languages and includes musical style prompts extracted from\nMusicBrainz and Wikipedia. The dataset is globally balanced, representing\nmusical styles from artists across 79 countries and five continents. Our\nevaluation reveals large disparities in music quality and alignment with\nreference music between high-resource and low-resource regions. Furthermore, we\nfind marked differences in model performance between mainstream and\ngeographically niche genres, including cases where models generate music for\nregional genres that more closely align with the distribution of mainstream\nstyles."}
{"id": "2510.01778", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01778", "abs": "https://arxiv.org/abs/2510.01778", "authors": ["Samaneh Motie", "Hadi Zayyani", "Mohammad Salman", "Hasan Abu Hilal"], "title": "Closed-form Single UAV-aided Emitter Localization and Trajectory Design Using Doppler and TOA Measurements", "comment": null, "summary": "In this paper, a single Unmanned-Aerial-Vehicle (UAV)-aided localization\nalgorithm which uses both Doppler and Time of Arrival (ToA) measurements is\npresented. In contrast to Doppler-based localization algorithms which are based\non non-convex functions, exploiting ToA measurements in a Least-Square (LS)\nDoppler-based cost function, leads to a quadratic convex function whose\nminimizer lies on a line. Utilizing the ToA measurements in addition to the\nlinear equation of minimizer, a closed form solution is obtained for the\nemitter location using a constrained LS optimization. In addition, a trajectory\ndesign of the UAV is provided which has also closed-form solution. Simulation\nexperiments demonstrate the effectiveness of the proposed algorithm in\ncomparison to some others in the literature."}
{"id": "2510.01903", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01903", "abs": "https://arxiv.org/abs/2510.01903", "authors": ["Jingyi Li", "Zhiyuan Zhao", "Yunfei Liu", "Lijian Lin", "Ye Zhu", "Jiahao Wu", "Qiuqiang Kong", "Yu Li"], "title": "MelCap: A Unified Single-Codebook Neural Codec for High-Fidelity Audio Compression", "comment": "9 pages, 4 figures", "summary": "Neural audio codecs have recently emerged as powerful tools for high-quality\nand low-bitrate audio compression, leveraging deep generative models to learn\nlatent representations of audio signals. However, existing approaches either\nrely on a single quantizer that only processes speech domain, or on multiple\nquantizers that are not well suited for downstream tasks. To address this\nissue, we propose MelCap, a unified \"one-codebook-for-all\" neural codec that\neffectively handles speech, music, and general sound. By decomposing audio\nreconstruction into two stages, our method preserves more acoustic details than\nprevious single-codebook approaches, while achieving performance comparable to\nmainstream multi-codebook methods. In the first stage, audio is transformed\ninto mel-spectrograms, which are compressed and quantized into compact single\ntokens using a 2D tokenizer. A perceptual loss is further applied to mitigate\nthe over-smoothing artifacts observed in spectrogram reconstruction. In the\nsecond stage, a Vocoder recovers waveforms from the mel discrete tokens in a\nsingle forward pass, enabling real-time decoding. Both objective and subjective\nevaluations demonstrate that MelCap achieves quality on comparable to\nstate-of-the-art multi-codebook codecs, while retaining the computational\nsimplicity of a single-codebook design, thereby providing an effective\nrepresentation for downstream tasks."}
{"id": "2510.01968", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01968", "abs": "https://arxiv.org/abs/2510.01968", "authors": ["Luca A. Lanzendörfer", "Kyle Fearne", "Florian Grötschla", "Roger Wattenhofer"], "title": "Multi-bit Audio Watermarking", "comment": null, "summary": "We present Timbru, a post-hoc audio watermarking model that achieves\nstate-of-the-art robustness and imperceptibility trade-offs without training an\nembedder-detector model. Given any 44.1 kHz stereo music snippet, our method\nperforms per-audio gradient optimization to add imperceptible perturbations in\nthe latent space of a pretrained audio VAE, guided by a combined message and\nperceptual loss. The watermark can then be extracted using a pretrained CLAP\nmodel. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal,\nWavMark, and SilentCipher across common filtering, noise, compression,\nresampling, cropping, and regeneration attacks. Our approach attains the best\naverage bit error rates, while preserving perceptual quality, demonstrating an\nefficient, dataset-free path to imperceptible audio watermarking."}
{"id": "2510.01789", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.01789", "abs": "https://arxiv.org/abs/2510.01789", "authors": ["Ruixi Feng", "Weidong Mei", "Lele Lu", "Xin Wei", "Zhi Chen", "Zhen Gao", "Boyu Ning"], "title": "Performance Optimization for Movable Antenna Enhanced MISO-OFDM Systems", "comment": "Accepted to IEEE GLOBECOM 2025 Workshop", "summary": "Movable antenna (MA) technology offers a flexible approach to enhancing\nwireless channel conditions by adjusting antenna positions within a designated\nregion. While most existing works focus on narrowband MA systems, this paper\ninvestigates MA position optimization for an MA-enhanced multiple-input\nsingle-output (MISO) orthogonal frequency-division multiplexing (OFDM) system.\nThis problem appears to be particularly challenging due to the frequency-flat\nnature of MA positioning, which should accommodate the channel conditions\nacross different subcarriers. To overcome this challenge, we discretize the\nmovement region into a multitude of sampling points, thereby converting the\ncontinuous position optimization problem into a discrete point selection\nproblem. Although this problem is combinatorial, we develop an efficient\npartial enumeration algorithm to find the optimal solution using a\nbranch-and-bound framework, where a graph-theoretic method is incorporated to\neffectively prune suboptimal solutions. In the low signal-to-noise ratio (SNR)\nregime, a simplified graph-based algorithm is also proposed to obtain the\noptimal MA positions without the need for enumeration. Simulation results\nreveal that the proposed algorithm outperforms conventional fixed-position\nantennas (FPAs), while narrowband-based antenna position optimization can\nachieve near-optimal performance."}
{"id": "2510.01958", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01958", "abs": "https://arxiv.org/abs/2510.01958", "authors": ["Nikolai Lund Kühne", "Jesper Jensen", "Jan Østergaard", "Zheng-Hua Tan"], "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement", "comment": "Submitted to IEEE for possible publication", "summary": "Recent advances in speech enhancement have shown that models combining Mamba\nand attention mechanisms yield superior cross-corpus generalization\nperformance. At the same time, integrating Mamba in a U-Net structure has\nyielded state-of-the-art enhancement performance, while reducing both model\nsize and computational complexity. Inspired by these insights, we propose\nRWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and\nmulti-head attention in a U-Net structure for improved cross-corpus\nperformance. Resolution-wise shared attention (RWSA) refers to layerwise\nattention-sharing across corresponding time- and frequency resolutions. Our\nbest-performing RWSA-MambaUNet model achieves state-of-the-art generalization\nperformance on two out-of-domain test sets. Notably, our smallest model\nsurpasses all baselines on the out-of-domain DNS 2020 test set in terms of\nPESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms\nof SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and\na fraction of the FLOPs."}
{"id": "2510.02110", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02110", "abs": "https://arxiv.org/abs/2510.02110", "authors": ["Koichi Saito", "Julian Tanke", "Christian Simon", "Masato Ishii", "Kazuki Shimada", "Zachary Novack", "Zhi Zhong", "Akio Hayakawa", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "SoundReactor: Frame-level Online Video-to-Audio Generation", "comment": null, "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming\nan entire video sequence or chunks of frames are available beforehand. This\ncritically limits their use in interactive applications such as live content\ncreation and emerging generative world models. To address this gap, we\nintroduce the novel task of frame-level online V2A generation, where a model\nautoregressively generates audio from video without access to future video\nframes. Furthermore, we propose SoundReactor, which, to the best of our\nknowledge, is the first simple yet effective framework explicitly tailored for\nthis task. Our design enforces end-to-end causality and targets low per-frame\nlatency with audio-visual synchronization. Our model's backbone is a\ndecoder-only causal transformer over continuous audio latents. For vision\nconditioning, it leverages grid (patch) features extracted from the smallest\nvariant of the DINOv2 vision encoder, which are aggregated into a single token\nper frame to maintain end-to-end causality and efficiency. The model is trained\nthrough a diffusion pre-training followed by consistency fine-tuning to\naccelerate the diffusion head decoding. On a benchmark of diverse gameplay\nvideos from AAA titles, our model successfully generates semantically and\ntemporally aligned, high-quality full-band stereo audio, validated by both\nobjective and human evaluations. Furthermore, our model achieves low per-frame\nwaveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on\n30FPS, 480p videos using a single H100. Demo samples are available at\nhttps://koichi-saito-sony.github.io/soundreactor/."}
{"id": "2510.01850", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT", "68T07, 94A12, 62M10", "I.2.6; I.5.4; C.2.1"], "pdf": "https://arxiv.org/pdf/2510.01850", "abs": "https://arxiv.org/abs/2510.01850", "authors": ["Ying-Ren Chien", "Po-Heng Chou", "You-Jie Peng", "Chun-Yuan Huang", "Hen-Wai Tsao", "Yu Tsao"], "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications", "comment": "16 pages, 15 figures, 11 tables, and published in IEEE Transactions\n  on Instrumentation and Measurement, Vol. 74, 2025", "summary": "Capturing comprehensive statistics of nonperiodic asynchronous impulsive\nnoise is a critical issue in enhancing impulse noise processing for narrowband\npowerline communication (NB-PLC) transceivers. However, existing mathematical\nnoise generative models capture only some of the characteristics of additive\nnoise. Therefore, we propose a generative adversarial network (GAN), called the\nnoise-generation GAN (NGGAN), that learns the complicated characteristics of\npractically measured noise samples for data augmentation. To closely match the\nstatistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise\nvia the analog coupling and bandpass filtering circuits of a commercial NB-PLC\nmodem to build a realistic dataset. Specifically, the NGGAN design approaches\nbased on the practically measured dataset are as follows: (i) we design the\nlength of input signals that the NGGAN model can fit to facilitate\ncyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss\nfunction to enhance the similarity between the generated noise and the training\ndataset and ensure that the sample diversity is sufficient for various\napplications. (iii) To measure the similarity performance of the GAN-based\nmodels based on mathematical and practically measured datasets, we perform\nquantitative and qualitative analyses. The training datasets include (1) a\npiecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a\nfrequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC\nsystems. Simulation results demonstrate that the proposed NGGAN trained using\nwaveform characteristics is closer to the practically measured dataset in terms\nof the quality of the generated noise."}
{"id": "2510.01968", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.01968", "abs": "https://arxiv.org/abs/2510.01968", "authors": ["Luca A. Lanzendörfer", "Kyle Fearne", "Florian Grötschla", "Roger Wattenhofer"], "title": "Multi-bit Audio Watermarking", "comment": null, "summary": "We present Timbru, a post-hoc audio watermarking model that achieves\nstate-of-the-art robustness and imperceptibility trade-offs without training an\nembedder-detector model. Given any 44.1 kHz stereo music snippet, our method\nperforms per-audio gradient optimization to add imperceptible perturbations in\nthe latent space of a pretrained audio VAE, guided by a combined message and\nperceptual loss. The watermark can then be extracted using a pretrained CLAP\nmodel. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal,\nWavMark, and SilentCipher across common filtering, noise, compression,\nresampling, cropping, and regeneration attacks. Our approach attains the best\naverage bit error rates, while preserving perceptual quality, demonstrating an\nefficient, dataset-free path to imperceptible audio watermarking."}
{"id": "2510.02171", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02171", "abs": "https://arxiv.org/abs/2510.02171", "authors": ["Edmund Dervakos", "Spyridon Kantarelis", "Vassilis Lyberatos", "Jason Liartis", "Giorgos Stamou"], "title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation", "comment": "Accepted at NeurIPS Creative AI Track 2025: Humanity", "summary": "Music performance is a distinctly human activity, intrinsically linked to the\nperformer's ability to convey, evoke, or express emotion. Machines cannot\nperform music in the human sense; they can produce, reproduce, execute, or\nsynthesize music, but they lack the capacity for affective or emotional\nexperience. As such, music performance is an ideal candidate through which to\nexplore aspects of collaboration between humans and machines. In this paper, we\nintroduce the witheFlow system, designed to enhance real-time music performance\nby automatically modulating audio effects based on features extracted from both\nbiosignals and the audio itself. The system, currently in a proof-of-concept\nphase, is designed to be lightweight, able to run locally on a laptop, and is\nopen-source given the availability of a compatible Digital Audio Workstation\nand sensors."}
{"id": "2510.02000", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.02000", "abs": "https://arxiv.org/abs/2510.02000", "authors": ["Giusy Spacone", "Sebastian Frey", "Mattia Orlandi", "Pierangelo Maria Rapa", "Victor Kartsch", "Simone Benatti", "Luca Benini", "Andrea Cossettini"], "title": "Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist Kinematic Tracking", "comment": null, "summary": "Hand gesture recognition based on biosignals has shown strong potential for\ndeveloping intuitive human-machine interaction strategies that closely mimic\nnatural human behavior. In particular, sensor fusion approaches have gained\nattention for combining complementary information and overcoming the\nlimitations of individual sensing modalities, thereby enabling more robust and\nreliable systems. Among them, the fusion of surface electromyography (EMG) and\nA-mode ultrasound (US) is very promising. However, prior solutions rely on\npower-hungry platforms unsuitable for multi-day use and are limited to discrete\ngesture classification. In this work, we present an ultra-low-power (sub-50 mW)\nsystem for concurrent acquisition of 8-channel EMG and 4-channel A-mode US\nsignals, integrating two state-of-the-art platforms into fully wearable,\ndry-contact armbands. We propose a framework for continuous tracking of 23\ndegrees of freedom (DoFs), 20 for the hand and 3 for the wrist, using a\nkinematic glove for ground-truth labeling. Our method employs lightweight\nencoder-decoder architectures with multi-task learning to simultaneously\nestimate hand and wrist joint angles. Experimental results under realistic\nsensor repositioning conditions demonstrate that EMG-US fusion achieves a root\nmean squared error of $10.6^\\circ\\pm2.0^\\circ$, compared to\n$12.0^\\circ\\pm1^\\circ$ for EMG and $13.1^\\circ\\pm2.6^\\circ$ for US, and a R$^2$\nscore of $0.61\\pm0.1$, with $0.54\\pm0.03$ for EMG and $0.38\\pm0.20$ for US."}
{"id": "2510.02110", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02110", "abs": "https://arxiv.org/abs/2510.02110", "authors": ["Koichi Saito", "Julian Tanke", "Christian Simon", "Masato Ishii", "Kazuki Shimada", "Zachary Novack", "Zhi Zhong", "Akio Hayakawa", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "SoundReactor: Frame-level Online Video-to-Audio Generation", "comment": null, "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming\nan entire video sequence or chunks of frames are available beforehand. This\ncritically limits their use in interactive applications such as live content\ncreation and emerging generative world models. To address this gap, we\nintroduce the novel task of frame-level online V2A generation, where a model\nautoregressively generates audio from video without access to future video\nframes. Furthermore, we propose SoundReactor, which, to the best of our\nknowledge, is the first simple yet effective framework explicitly tailored for\nthis task. Our design enforces end-to-end causality and targets low per-frame\nlatency with audio-visual synchronization. Our model's backbone is a\ndecoder-only causal transformer over continuous audio latents. For vision\nconditioning, it leverages grid (patch) features extracted from the smallest\nvariant of the DINOv2 vision encoder, which are aggregated into a single token\nper frame to maintain end-to-end causality and efficiency. The model is trained\nthrough a diffusion pre-training followed by consistency fine-tuning to\naccelerate the diffusion head decoding. On a benchmark of diverse gameplay\nvideos from AAA titles, our model successfully generates semantically and\ntemporally aligned, high-quality full-band stereo audio, validated by both\nobjective and human evaluations. Furthermore, our model achieves low per-frame\nwaveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on\n30FPS, 480p videos using a single H100. Demo samples are available at\nhttps://koichi-saito-sony.github.io/soundreactor/."}
{"id": "2510.02187", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02187", "abs": "https://arxiv.org/abs/2510.02187", "authors": ["Luca A. Lanzendörfer", "Frédéric Berdoz", "Antonis Asonitis", "Roger Wattenhofer"], "title": "High-Fidelity Speech Enhancement via Discrete Audio Tokens", "comment": null, "summary": "Recent autoregressive transformer-based speech enhancement (SE) methods have\nshown promising results by leveraging advanced semantic understanding and\ncontextual modeling of speech. However, these approaches often rely on complex\nmulti-stage pipelines and low sampling rate codecs, limiting them to narrow and\ntask-specific speech enhancement. In this work, we introduce DAC-SE1, a\nsimplified language model-based SE framework leveraging discrete\nhigh-resolution audio representations; DAC-SE1 preserves fine-grained acoustic\ndetails while maintaining semantic coherence. Our experiments show that DAC-SE1\nsurpasses state-of-the-art autoregressive SE methods on both objective\nperceptual metrics and in a MUSHRA human evaluation. We release our codebase\nand model checkpoints to support further research in scalable, unified, and\nhigh-quality speech enhancement."}
{"id": "2510.02012", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02012", "abs": "https://arxiv.org/abs/2510.02012", "authors": ["Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "David González G.", "Carlo Fischione"], "title": "Computing on Dirty Paper: Interference-Free Integrated Communication and Computing", "comment": "Submitted to an IEEE conference", "summary": "Inspired by Costa's pioneering work on dirty paper coding (DPC), this paper\nproposes a novel scheme for integrated communication and computing (ICC), named\nComputing on Dirty Paper, whereby the transmission of discrete data symbols for\ncommunication, and over-the-air computation (AirComp) of nomographic functions\ncan be achieved simultaneously over common multiple-access channels. In\nparticular, the proposed scheme allows for the integration of communication and\ncomputation in a manner that is asymptotically interference-free, by\nprecanceling the computing symbols at the transmitters (TXs) using DPC\nprinciples. A simulation-based assessment of the proposed ICC scheme under a\nsingle-input multiple-output (SIMO) setup is also offered, including the\nevaluation of performance for data detection, and of mean-squared-error (MSE)\nperformance for function computation, over a block of symbols. The results\nvalidate the proposed method and demonstrate its ability to significantly\noutperform state-of-the-art (SotA) ICC schemes in terms of both bit error rate\n(BER) and MSE."}
{"id": "2510.02171", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02171", "abs": "https://arxiv.org/abs/2510.02171", "authors": ["Edmund Dervakos", "Spyridon Kantarelis", "Vassilis Lyberatos", "Jason Liartis", "Giorgos Stamou"], "title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation", "comment": "Accepted at NeurIPS Creative AI Track 2025: Humanity", "summary": "Music performance is a distinctly human activity, intrinsically linked to the\nperformer's ability to convey, evoke, or express emotion. Machines cannot\nperform music in the human sense; they can produce, reproduce, execute, or\nsynthesize music, but they lack the capacity for affective or emotional\nexperience. As such, music performance is an ideal candidate through which to\nexplore aspects of collaboration between humans and machines. In this paper, we\nintroduce the witheFlow system, designed to enhance real-time music performance\nby automatically modulating audio effects based on features extracted from both\nbiosignals and the audio itself. The system, currently in a proof-of-concept\nphase, is designed to be lightweight, able to run locally on a laptop, and is\nopen-source given the availability of a compatible Digital Audio Workstation\nand sensors."}
{"id": "2510.01860", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.01860", "abs": "https://arxiv.org/abs/2510.01860", "authors": ["Angelika Ando", "Auguste Crabeil", "Adrien Lesage", "Rachid Riad"], "title": "SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision", "comment": null, "summary": "Speech encodes paralinguistic information such as demographics, voice\nquality, and health. Yet no audio foundation model supports zero-shot or\nout-of-distribution (OOD) generalization to these tasks. We introduce SLAP\n(Speaker contrastive Language-Audio Pretraining), the first model aligning\nspeech with natural language descriptions of speaker and health metadata\nthrough contrastive learning. SLAP combines a Vision Transformer audio encoder\nwith text encoders, trained on more than 3400 hours across 9 datasets with\ndiverse speaker annotations. We evaluated on 38 binary classification tasks\nspanning demographics, voice characteristics, and clinical assessments across\n14 datasets in 7 languages. SLAP achieves 62.9% average F1 in zero-shot\nevaluation, a 48% relative improvement over CLAP (42.4%), while demonstrating\nstrong OOD generalization to unseen languages and clinical populations. When\nfine-tuned with linear probing, SLAP reaches 69.3% F1 overall and achieves\nbest-in-class performance on health tasks (57.9% F1), surpassing larger\nfoundation models."}
{"id": "2510.02021", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02021", "abs": "https://arxiv.org/abs/2510.02021", "authors": ["Gian Marti", "Christoph Studer"], "title": "Joint Jammer Mitigation and Data Detection", "comment": "This work has not been submitted to the IEEE for possible\n  publication. The copyright remains with the authors, and this version will\n  remain publicly accessible", "summary": "Multi-antenna (or MIMO) processing is a promising solution to the problem of\njammer mitigation. Existing methods mitigate the jammer based on an estimate of\nits spatial signature that is acquired through a dedicated training phase. This\nstrategy has two main drawbacks: (i) it reduces the communication rate since no\ndata can be transmitted during the training phase and (ii) it can be evaded by\nsmart or multi-antenna jammers that do not transmit during the training phase\nor that dynamically change their subspace through time-varying beamforming. To\naddress these drawbacks, we propose Joint jammer Mitigation and data Detection\n(JMD), a novel paradigm for MIMO jammer mitigation. The core idea of JMD is to\nestimate and remove the jammer interference subspace jointly with detecting the\nlegitimate transmit data over multiple time slots. Doing so removes the need\nfor a dedicated and rate-reducing training period while being able to mitigate\nsmart and dynamic multi-antenna jammers. We provide two JMD-type algorithms,\nSANDMAN and MAED, that differ in the way they estimate the channels of the\nlegitimate transmitters and achieve different complexity-performance tradeoffs.\nExtensive simulations demonstrate the efficacy of JMD for jammer mitigation."}
{"id": "2510.02187", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02187", "abs": "https://arxiv.org/abs/2510.02187", "authors": ["Luca A. Lanzendörfer", "Frédéric Berdoz", "Antonis Asonitis", "Roger Wattenhofer"], "title": "High-Fidelity Speech Enhancement via Discrete Audio Tokens", "comment": null, "summary": "Recent autoregressive transformer-based speech enhancement (SE) methods have\nshown promising results by leveraging advanced semantic understanding and\ncontextual modeling of speech. However, these approaches often rely on complex\nmulti-stage pipelines and low sampling rate codecs, limiting them to narrow and\ntask-specific speech enhancement. In this work, we introduce DAC-SE1, a\nsimplified language model-based SE framework leveraging discrete\nhigh-resolution audio representations; DAC-SE1 preserves fine-grained acoustic\ndetails while maintaining semantic coherence. Our experiments show that DAC-SE1\nsurpasses state-of-the-art autoregressive SE methods on both objective\nperceptual metrics and in a MUSHRA human evaluation. We release our codebase\nand model checkpoints to support further research in scalable, unified, and\nhigh-quality speech enhancement."}
{"id": "2510.02023", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02023", "abs": "https://arxiv.org/abs/2510.02023", "authors": ["Ping Wang", "Zulin Wang", "Yuanhan Ni", "Qu Luo", "Yuanfang Ma", "Xiaosi Tian", "Pei Xiao"], "title": "A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has garnered significant\nattention due to its superior performance in high-mobility scenarios, coupled\nwith multiple waveform parameters that provide greater degrees of freedom for\nsystem design. This paper introduces a novel secure affine frequency division\nmultiplexing (SE-AFDM) system, which advances prior designs by dynamically\nvarying an AFDM pre-chirp parameter to enhance physical-layer security. In the\nSE-AFDM system, the pre-chirp parameter is dynamically generated from a\ncodebook controlled by a long-period pseudo-noise (LPPN) sequence. Instead of\napplying spreading in the data domain, our parameter-domain spreading approach\nprovides additional security while maintaining reliability and high spectrum\nefficiency. We also propose a synchronization framework to solve the problem of\nreliably and rapidly synchronizing the time-varying parameter in fast\ntime-varying channels. The theoretical derivations prove that unsynchronized\neavesdroppers cannot eliminate the nonlinear impact of the time-varying\nparameter and further provide useful guidance for codebook design. Simulation\nresults demonstrate the security advantages of the proposed SE-AFDM system in\nhigh-mobility scenarios, while our hardware prototype validates the\neffectiveness of the proposed synchronization framework."}
{"id": "2510.02029", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02029", "abs": "https://arxiv.org/abs/2510.02029", "authors": ["Haonan Si", "Zhaolin Wang", "Xiansheng Guo", "Jin Zhang", "Yuanwei Liu"], "title": "Joint DOA and Attitude Sensing Based on Tri-Polarized Continuous Aperture Array", "comment": "13 pages, 10 figures", "summary": "This paper investigates joint direction-of-arrival (DOA) and attitude sensing\nusing tri-polarized continuous aperture arrays (CAPAs). By employing\nelectromagnetic (EM) information theory, the spatially continuous received\nsignals in tri-polarized CAPA are modeled, thereby enabling accurate DOA and\nattitude estimation. To facilitate subspace decomposition for continuous\noperators, an equivalent continuous-discrete transformation technique is\ndeveloped. Moreover, both self- and cross-covariances of tri-polarized signals\nare exploited to construct a tri-polarized spectrum, significantly enhancing\nDOA estimation performance. Theoretical analyses reveal that the\nidentifiability of attitude information fundamentally depends on the\navailability of prior target snapshots. Accordingly, two attitude estimation\nalgorithms are proposed: one capable of estimating partial attitude information\nwithout prior knowledge, and the other achieving full attitude estimation when\nsuch knowledge is available. Numerical results demonstrate the feasibility and\nsuperiority of the proposed framework."}
{"id": "2510.02103", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02103", "abs": "https://arxiv.org/abs/2510.02103", "authors": ["Kawon Han", "Kaitao Meng", "Christos Masouros"], "title": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing Unauthorized Sensing", "comment": "15 pages, 12 figures, accepted to IEEE Transactions on Wireless\n  Communications", "summary": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance."}
{"id": "2510.02108", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02108", "abs": "https://arxiv.org/abs/2510.02108", "authors": ["Jinshuo Zhang", "Yafei Wang", "Xinping Yi", "Wenjin Wang", "Shi Jin", "Symeon Chatzinotas", "Björn Ottersten"], "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths."}

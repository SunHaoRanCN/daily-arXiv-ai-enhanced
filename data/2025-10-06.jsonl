{"id": "2510.02320", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02320", "abs": "https://arxiv.org/abs/2510.02320", "authors": ["Yongqi Kang", "Yong Zhao"], "title": "WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis", "comment": "5 pages", "summary": "The advancement of computational psychology requires AI tools capable of\ndeeply understanding counseling dialogues. Existing audio language models\n(AudioLLMs) often rely on single speech encoders pre-trained on general data,\nstruggling to capture domain-specific features like complex emotions and\nprofessional techniques. To address this, we propose WEE-Therapy, a multi-task\nAudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This\nsupplements a powerful base encoder with a pool of lightweight, specialized\nencoders. A novel dual-routing strategy combines stable, data-independent\ndomain knowledge with dynamic, data-dependent expert selection. Evaluated on\nemotion recognition, technique classification, risk detection, and\nsummarization, WEE-Therapy achieves significant performance gains across all\ntasks with minimal parameter overhead, demonstrating strong potential for\nAI-assisted clinical analysis."}
{"id": "2510.02322", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02322", "abs": "https://arxiv.org/abs/2510.02322", "authors": ["Lukas Buess", "Jan Geier", "David Bani-Harouni", "Chantal Pellegrini", "Matthias Keicher", "Paula Andrea Perez-Toro", "Nassir Navab", "Andreas Maier", "Tomas Arias-Vergara"], "title": "SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for Voice-Native Multimodal CT Analysis", "comment": "Submitted to ICASSP 2026; under review", "summary": "Spoken communication plays a central role in clinical workflows. In\nradiology, for example, most reports are created through dictation. Yet, nearly\nall medical AI systems rely exclusively on written text. In this work, we\naddress this gap by exploring the feasibility of learning visual-language\nrepresentations directly from spoken radiology reports. Specifically, we\nsynthesize a large-scale dataset (Speech-RATE) of spoken radiology reports and\ntrain SpeechCT-CLIP, a contrastive model that aligns speech and 3D CT volumes\nin a shared representation space. While naive speech-based models underperform\ncompared to text-trained counterparts, we show that knowledge distillation from\na pretrained text-image CLIP model effectively transfers semantic alignment\ncapabilities from text to speech, substantially narrowing this gap. Experiments\ndemonstrate improved zero-shot classification F1 from 0.623 to 0.705,\nrecovering 88% of the performance difference, and strong retrieval results\nwithout requiring text at inference. These findings highlight speech as a\npractical alternative to text in multimodal pretraining and open the door to\nvoice-driven diagnostic support tools in clinical practice."}
{"id": "2510.02398", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02398", "abs": "https://arxiv.org/abs/2510.02398", "authors": ["Shree Harsha Bokkahalli Satish", "Gustav Eje Henter", "Éva Székely"], "title": "When Voice Matters: Evidence of Gender Disparity in Positional Bias of SpeechLLMs", "comment": "16 pages, 5 figures, To Appear in SPECOM 2025", "summary": "The rapid development of SpeechLLM-based conversational AI systems has\ncreated a need for robustly benchmarking these efforts, including aspects of\nfairness and bias. At present, such benchmarks typically rely on multiple\nchoice question answering (MCQA). In this paper, we present the first\ntoken-level probabilistic evaluation and response-based study of several issues\naffecting the use of MCQA in SpeechLLM benchmarking: 1) we examine how model\ntemperature and prompt design affect gender and positional bias on an MCQA\ngender-bias benchmark; 2) we examine how these biases are affected by the\ngender of the input voice; and 3) we study to what extent observed trends carry\nover to a second gender-bias benchmark. Our results show that concerns about\npositional bias from the text domain are equally valid in the speech domain. We\nalso find the effect to be stronger for female voices than for male voices. To\nour knowledge, this is the first study to isolate positional bias effects in\nSpeechLLM-based gender-bias benchmarks. We conclude that current MCQA\nbenchmarks do not account for speech-based bias and alternative strategies are\nneeded to ensure fairness towards all users."}
{"id": "2510.02556", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02556", "abs": "https://arxiv.org/abs/2510.02556", "authors": ["Klaus Brümann", "Simon Doclo"], "title": "Multi-Source Position and Direction-of-Arrival Estimation Based on Euclidean Distance Matrices", "comment": "13 pages, 6 figures, submitted to IEEE Transactions on Audio, Speech\n  and Language Processing (awaiting review)", "summary": "A popular method to estimate the positions or directions-of-arrival (DOAs) of\nmultiple sound sources using an array of microphones is based on\nsteered-response power (SRP) beamforming. For a three-dimensional scenario,\nSRP-based methods need to jointly optimize three continuous variables for\nposition estimation or two continuous variables for DOA estimation, which can\nbe computationally expensive. In this paper, we propose novel methods for\nmulti-source position and DOA estimation by exploiting properties of Euclidean\ndistance matrices (EDMs) and their respective Gram matrices. In the proposed\nmulti-source position estimation method only a single continuous variable,\nrepresenting the distance between each source and a reference microphone, needs\nto be optimized. For each source, the optimal continuous distance variable and\nset of candidate time-difference of arrival (TDOA) estimates are determined by\nminimizing a cost function that is defined using the eigenvalues of the Gram\nmatrix. The estimated relative source positions are then mapped to estimated\nabsolute source positions by solving an orthogonal Procrustes problem for each\nsource. The proposed multi-source DOA estimation method entirely eliminates the\nneed for continuous variable optimization by defining a relative coordinate\nsystem per source such that one of its coordinate axes is aligned with the\nrespective source DOA. The optimal set of candidate TDOA estimates is\ndetermined by minimizing a cost function that is defined using the eigenvalues\nof a rank-reduced Gram matrix. The computational cost of the proposed EDM-based\nmethods is significantly reduced compared to the SRP-based methods.\nExperimental results for different source and microphone configurations show\nthat the proposed EDM-based method consistently outperforms the SRP-based\nmethod in terms of two-source position and DOA estimation accuracy."}
{"id": "2510.02382", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02382", "abs": "https://arxiv.org/abs/2510.02382", "authors": ["Xuemai Xie", "Xianrui Wang", "Liyuan Zhang", "Yichen Yang", "Shoji Makino"], "title": "Accelerated Convolutive Transfer Function-Based Multichannel NMF Using Iterative Source Steering", "comment": null, "summary": "Among numerous blind source separation (BSS) methods, convolutive transfer\nfunction-based multichannel non-negative matrix factorization (CTF-MNMF) has\ndemonstrated strong performance in highly reverberant environments by modeling\nmulti-frame correlations of delayed source signals. However, its practical\ndeployment is hindered by the high computational cost associated with the\niterative projection (IP) update rule, which requires matrix inversion for each\nsource. To address this issue, we propose an efficient variant of CTF-MNMF that\nintegrates iterative source steering (ISS), a matrix inversion-free update rule\nfor separation filters. Experimental results show that the proposed method\nachieves comparable or superior separation performance to the original\nCTF-MNMF, while significantly reducing the computational complexity."}
{"id": "2510.02646", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02646", "abs": "https://arxiv.org/abs/2510.02646", "authors": ["Jinsung Park", "Junyong Shin", "Yongjeong Oh", "Jihun Park", "Yo-Seb Jeon"], "title": "Rate-Adaptive Semantic Communication via Multi-Stage Vector Quantization", "comment": null, "summary": "This paper proposes a novel framework for rate-adaptive semantic\ncommunication based on multi-stage vector quantization (VQ), termed\n\\textit{MSVQ-SC}. Unlike conventional single-stage VQ approaches, which require\nexponentially larger codebooks to achieve higher fidelity, the proposed\nframework decomposes the quantization process into multiple stages and\ndynamically activates both stages and individual VQ modules. This design\nenables fine-grained rate adaptation under varying bit constraints while\nmitigating computational complexity and the codebook collapse problem. To\noptimize performance, we formulate a module selection problem that minimizes\ntask loss subject to a rate constraint and solve it using an incremental\nallocation algorithm. Furthermore, we extend the framework by incorporating\nentropy coding to exploit non-uniform codeword distributions, further reducing\ncommunication overhead. Simulation results on the CIFAR-10 dataset demonstrate\nthat the proposed framework outperforms existing digital semantic communication\nmethods, achieving superior semantic fidelity with lower complexity while\nproviding flexible and fine-grained rate control."}
{"id": "2510.02672", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02672", "abs": "https://arxiv.org/abs/2510.02672", "authors": ["Dyah A. M. G. Wisnu", "Ryandhimas E. Zezario", "Stefano Rini", "Fo-Rui Li", "Yan-Tsung Peng", "Hsin-Min Wang", "Yu Tsao"], "title": "STSM-FiLM: A FiLM-Conditioned Neural Architecture for Time-Scale Modification of Speech", "comment": null, "summary": "Time-Scale Modification (TSM) of speech aims to alter the playback rate of\naudio without changing its pitch. While classical methods like Waveform\nSimilarity-based Overlap-Add (WSOLA) provide strong baselines, they often\nintroduce artifacts under non-stationary or extreme stretching conditions. We\npropose STSM-FILM - a fully neural architecture that incorporates Feature-Wise\nLinear Modulation (FiLM) to condition the model on a continuous speed factor.\nBy supervising the network using WSOLA-generated outputs, STSM-FILM learns to\nmimic alignment and synthesis behaviors while benefiting from representations\nlearned through deep learning. We explore four encoder-decoder variants:\nSTFT-HiFiGAN, WavLM-HiFiGAN, Whisper-HiFiGAN, and EnCodec, and demonstrate that\nSTSM-FILM is capable of producing perceptually consistent outputs across a wide\nrange of time-scaling factors. Overall, our results demonstrate the potential\nof FiLM-based conditioning to improve the generalization and flexibility of\nneural TSM models."}
{"id": "2510.02401", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02401", "abs": "https://arxiv.org/abs/2510.02401", "authors": ["Konrad Szewczyk", "Daniel Gallo Fernández", "James Townsend"], "title": "Linear RNNs for autoregressive generation of long music samples", "comment": null, "summary": "Directly learning to generate audio waveforms in an autoregressive manner is\na challenging task, due to the length of the raw sequences and the existence of\nimportant structure on many different timescales. Traditional approaches based\non recurrent neural networks, as well as causal convolutions and\nself-attention, have only had limited success on this task. However, recent\nwork has shown that deep state space models, also referred to as linear RNNs,\ncan be highly efficient in this context. In this work, we push the boundaries\nof linear RNNs applied to raw audio modeling, investigating the effects of\ndifferent architectural choices and using context-parallelism to enable\ntraining on sequences up to one minute (1M tokens) in length. We present a\nmodel, HarmonicRNN, which attains state of the art log-likelihoods and\nperceptual metrics on small-scale datasets."}
{"id": "2510.02696", "categories": ["eess.SP", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.02696", "abs": "https://arxiv.org/abs/2510.02696", "authors": ["Anish Pradhan", "Lingjia Liu", "Harpreet S. Dhillon"], "title": "Mutual Information-Driven Visualization and Clustering for Core KPI Selection in O-RAN Testing", "comment": null, "summary": "O-RAN testing is becoming increasingly difficult with the exponentially\ngrowing number of performance measurements as the system grows more complex,\nwith additional units, interfaces, applications, and possible implementations\nand configurations. To simplify the testing procedure and improve system design\nfor O-RAN systems, it is important to identify the dependencies among various\nperformance measurements, which are inherently time-series and can be modeled\nas realizations of random processes. While information theory can be utilized\nas a principled foundation for mapping these dependencies, the robust\nestimation of such measures for random processes from real-world data remains\nchallenging. This paper introduces AMIF-MDS, which employs aggregate mutual\nInformation in frequency (AMIF), a practical proxy for directed information\n(DI), to quantify similarity and visualize inter-series dependencies with\nmultidimensional scaling (MDS). The proposed quantile-based AMIF estimator is\napplied to O-RAN time-series testing data to identify dependencies among\nvarious performance measures so that we can focus on a set of ``core''\nperformance measures. Applying density-based spatial clustering of applications\nwith noise (DBSCAN) to the MDS embedding groups mutually informative metrics,\norganically reveals the link-adaptation indicators among other clusters, and\nyields a ``core'' performance measure set for future learning-driven O-RAN\ntesting."}
{"id": "2510.02797", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02797", "abs": "https://arxiv.org/abs/2510.02797", "authors": ["Chunbo Hao", "Ruibin Yuan", "Jixun Yao", "Qixin Deng", "Xinyi Bai", "Wei Xue", "Lei Xie"], "title": "SongFormer: Scaling Music Structure Analysis with Heterogeneous Supervision", "comment": null, "summary": "Music structure analysis (MSA) underpins music understanding and controllable\ngeneration, yet progress has been limited by small, inconsistent corpora. We\npresent SongFormer, a scalable framework that learns from heterogeneous\nsupervision. SongFormer (i) fuses short- and long-window self-supervised audio\nrepresentations to capture both fine-grained and long-range dependencies, and\n(ii) introduces a learned source embedding to enable training with partial,\nnoisy, and schema-mismatched labels. To support scaling and fair evaluation, we\nrelease SongFormDB, the largest MSA corpus to date (over 10k tracks spanning\nlanguages and genres), and SongFormBench, a 300-song expert-verified benchmark.\nOn SongFormBench, SongFormer sets a new state of the art in strict boundary\ndetection (HR.5F) and achieves the highest functional label accuracy, while\nremaining computationally efficient; it surpasses strong baselines and Gemini\n2.5 Pro on these metrics and remains competitive under relaxed tolerance\n(HR3F). Code, datasets, and model are publicly available."}
{"id": "2510.02500", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02500", "abs": "https://arxiv.org/abs/2510.02500", "authors": ["Sivan Sing", "Julia Wilkins", "Magdalena Fuentes", "Juan Pablo Bello"], "title": "Latent Multi-view Learning for Robust Environmental Sound Representations", "comment": "Accepted to DCASE 2025 Workshop. 4+1 pages, 2 figures, 2 tables", "summary": "Self-supervised learning (SSL) approaches, such as contrastive and generative\nmethods, have advanced environmental sound representation learning using\nunlabeled data. However, how these approaches can complement each other within\na unified framework remains relatively underexplored. In this work, we propose\na multi-view learning framework that integrates contrastive principles into a\ngenerative pipeline to capture sound source and device information. Our method\nencodes compressed audio latents into view-specific and view-common subspaces,\nguided by two self-supervised objectives: contrastive learning for targeted\ninformation flow between subspaces, and reconstruction for overall information\npreservation. We evaluate our method on an urban sound sensor network dataset\nfor sound source and sensor classification, demonstrating improved downstream\nperformance over traditional SSL techniques. Additionally, we investigate the\nmodel's potential to disentangle environmental sound attributes within the\nstructured latent space under varied training configurations."}
{"id": "2510.02744", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02744", "abs": "https://arxiv.org/abs/2510.02744", "authors": ["Yupeng Li", "Ruhao Zhang", "Yitong Liu", "Chunju Shao", "Jing Jin", "Shijian Gao"], "title": "Denoising and Augmentation: A Dual Use of Diffusion Model for Enhanced CSI Recovery", "comment": "This paper is formatted for an IEEE conference. It contains 4 figures\n  and 2 tables. The source code is available at\n  https://github.com/fhghwericge/Diffusion-Model-for-Enhanced-CSI-Recovery", "summary": "This letter introduces a dual application of denoising diffusion\nprobabilistic model (DDPM)-based channel estimation algorithm integrating data\ndenoising and augmentation. Denoising addresses the severe noise in raw signals\nat pilot locations, which can impair channel estimation accuracy. An\nunsupervised structure is proposed to clean field data without prior knowledge\nof pure channel information. Data augmentation is crucial due to the\ndata-intensive nature of training deep learning (DL) networks for channel state\ninformation (CSI) estimation. The network generates new channel data by\nadjusting reverse steps, enriching the training dataset. To manage varying\nsignal-to-noise ratios (SNRs) in communication data, a piecewise forward\nstrategy is proposed to enhance the DDPM convergence precision. The link-level\nsimulations indicate that the proposed scheme achieves a superior tradeoff\nbetween precision and computational cost compared to existing benchmarks."}
{"id": "2510.02813", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02813", "abs": "https://arxiv.org/abs/2510.02813", "authors": ["Ludovic Pirard", "Katarina C. Poole", "Lorenzo Picinali"], "title": "Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph Neural Network", "comment": "Accepted for poster presentation at Forum Acusticum Euronoise 2025,\n  Malaga, Spain", "summary": "Traditional Head-Related Transfer Functions (HRTFs) acquisition methods rely\non specialised equipment and acoustic expertise, posing accessibility\nchallenges. Alternatively, high-resolution 3D modelling offers a pathway to\nnumerically synthesise HRTFs using Boundary Elements Methods and others.\nHowever, the high cost and limited availability of advanced 3D scanners\nrestrict their applicability. Photogrammetry has been proposed as a solution\nfor generating 3D head meshes, though its resolution limitations restrict its\napplication for HRTF synthesis. To address these limitations, this study\ninvestigates the feasibility of using Graph Neural Networks (GNN) using neural\nsubdivision techniques for upsampling low-resolution\nPhotogrammetry-Reconstructed (PR) meshes into high-resolution meshes, which can\nthen be employed to synthesise individual HRTFs. Photogrammetry data from the\nSONICOM dataset are processed using Apple Photogrammetry API to reconstruct\nlow-resolution head meshes. The dataset of paired low- and high-resolution\nmeshes is then used to train a GNN to upscale low-resolution inputs to\nhigh-resolution outputs, using a Hausdorff Distance-based loss function. The\nGNN's performance on unseen photogrammetry data is validated geometrically and\nthrough synthesised HRTFs generated via Mesh2HRTF. Synthesised HRTFs are\nevaluated against those computed from high-resolution 3D scans, to acoustically\nmeasured HRTFs, and to the KEMAR HRTF using perceptually-relevant numerical\nanalyses as well as behavioural experiments, including localisation and Spatial\nRelease from Masking (SRM) tasks."}
{"id": "2510.02597", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02597", "abs": "https://arxiv.org/abs/2510.02597", "authors": ["Akshaj Gupta", "Andrea Guzman", "Anagha Badriprasad", "Hwi Joo Park", "Upasana Puranik", "Robin Netzorg", "Jiachen Lian", "Gopala Krishna Anumanchipalli"], "title": "TART: A Comprehensive Tool for Technique-Aware Audio-to-Tab Guitar Transcription", "comment": null, "summary": "Automatic Music Transcription (AMT) has advanced significantly for the piano,\nbut transcription for the guitar remains limited due to several key challenges.\nExisting systems fail to detect and annotate expressive techniques (e.g.,\nslides, bends, percussive hits) and incorrectly map notes to the wrong string\nand fret combination in the generated tablature. Furthermore, prior models are\ntypically trained on small, isolated datasets, limiting their generalizability\nto real-world guitar recordings. To overcome these limitations, we propose a\nfour-stage end-to-end pipeline that produces detailed guitar tablature directly\nfrom audio. Our system consists of (1) Audio-to-MIDI pitch conversion through a\npiano transcription model adapted to guitar datasets; (2) MLP-based expressive\ntechnique classification; (3) Transformer-based string and fret assignment; and\n(4) LSTM-based tablature generation. To the best of our knowledge, this\nframework is the first to generate detailed tablature with accurate fingerings\nand expressive labels from guitar audio."}
{"id": "2510.02785", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02785", "abs": "https://arxiv.org/abs/2510.02785", "authors": ["Shanglin Yang", "Jean-Marie Gorce", "Muhammad Jehangir Khan", "Dinh-Thuy Phan-Huy", "Guillaume Villemaud"], "title": "Neyman Pearson Detector for Multiple Ambient Backscatter Zero-Energy-Devices Beacons using Near-Perfect Code", "comment": "11 pages", "summary": "Recently, a novel ultra-low-power indoor localization system based on\nZero-Energy Devices (ZEDs) has shown promising results in ambient backscatter\ncommunication. In this paper, we study detection of multiple coexisting ZEDs in\nambient backscatter systems under interference and synchronization uncertainty.\nBuilding on a Neyman-Pearson (NP) formulation previously applied to single-tag\ndetection, we introduce a detector tailored to multi-tag scenarios. The core\nidea is to use a Near-Perfect Code (NPC) as the synchronization sequence, which\nsubstantially improves the peak-to-sidelobe (PSL) ratio and thus separability\namong concurrent tags. The proposed scheme replaces dual band-pass filtering\nwith dual correlators, enabling an explicit Bayesian detector and tight control\nof the false-alarm rate; we further incorporate a contrast metric and\nmulti-frequency combining to reveal secondary tags. Experiments on the\nCorteXlab testbed (part of the SLICES-EU infrastructure) confirm robustness at\nlow SNR, with observed PSL improvements from about 11 dB to about 22 dB. These\nresults advance scalable, reliable ambient backscatter localization in\npractical multi-tag environments."}
{"id": "2510.03025", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03025", "abs": "https://arxiv.org/abs/2510.03025", "authors": ["Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "title": "CVSM: Contrastive Vocal Similarity Modeling", "comment": "13 pages, 3 tables, 8 figures. Submitted article at IEEE Trans. on\n  Audio, Speech and Language Proc. (pre-print version)", "summary": "The availability of large, unlabeled datasets across various domains has\ncontributed to the development of a plethora of methods that learn\nrepresentations for multiple target (downstream) tasks through self-supervised\npre-training. In this work, we introduce CVSM (Contrastive Vocal Similarity\nModeling), a contrastive self-supervised procedure for music signal\nrepresentation learning in the audio domain that can be utilized for musical\nand vocal similarity modeling. Our method operates under a contrastive\nframework, maximizing the similarity between vocal excerpts and musical\nmixtures containing the same vocals; we devise both a label-informed protocol,\nleveraging artist identity information to sample the contrastive pairs, and a\nlabel-agnostic scheme, involving artificial mixture creation from randomly\nsampled vocal and accompaniment excerpts, which are paired with vocals from the\nsame audio segment. We evaluate our proposed method in measuring vocal\nsimilarity both objectively, through linear probing on a suite of appropriate\ndownstream tasks, and subjectively, via conducting a user study consisting of\npairwise comparisons between different models in a recommendation-by-query\nsetting. Our results indicate that the representations learned through CVSM are\neffective in musical and vocal similarity modeling, outperforming numerous\nbaselines across both isolated vocals and complete musical mixtures. Moreover,\nwhile the availability of artist identity labels during pre-training leads to\noverall more consistent performance both in the evaluated downstream tasks and\nthe user study, a label-agnostic CVSM variant incorporating hybrid pre-training\nwith real and artificial mixtures achieves comparable performance to the\nlabel-informed one in artist identification and perceived vocal similarity."}
{"id": "2510.02848", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02848", "abs": "https://arxiv.org/abs/2510.02848", "authors": ["Hieu-Nghia Huynh-Nguyen", "Huynh Nguyen Dang", "Ngoc-Son Nguyen", "Van Nguyen"], "title": "Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech", "comment": null, "summary": "Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling\nmodels to synthesize speech from text using short, limited-context prompts.\nThese prompts serve as voice exemplars, allowing the model to mimic speaker\nidentity, prosody, and other traits without extensive speaker-specific data.\nAlthough recent approaches incorporating language models, diffusion, and flow\nmatching have proven their effectiveness in zero-shot TTS, they still encounter\nchallenges such as unreliable synthesis caused by token repetition or\nunexpected content transfer, along with slow inference and substantial\ncomputational overhead. Moreover, temporal diversity-crucial for enhancing the\nnaturalness of synthesized speech-remains largely underexplored. To address\nthese challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that\nemphasizes low computational cost, low latency, and high speech fidelity\nalongside rich temporal diversity. To achieve this, we reformulate the flow\nmatching training paradigm and incorporate both discrete and continuous\nrepresentations corresponding to different attributes of speech. Experimental\nresults demonstrate that Flamed-TTS surpasses state-of-the-art models in terms\nof intelligibility, naturalness, speaker similarity, acoustic characteristics\npreservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4%\ncompared to the leading zero-shot TTS baselines, while maintaining low latency\nin inference and high fidelity in generated speech. Code and audio samples are\navailable at our demo page https://flamed-tts.github.io."}
{"id": "2510.02793", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.02793", "abs": "https://arxiv.org/abs/2510.02793", "authors": ["Jiachen Tian", "Yu Han", "Zhengtao Jin", "Xi Yang", "Jie Yang", "Wankai Tang", "Xiao Li", "Wenjin Wang", "Shi Jin"], "title": "Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and Implementation", "comment": null, "summary": "The mid-band frequency range, combined with extra large-scale multiple-input\nmultiple-output (XL-MIMO), is emerging as a key enabler for future\ncommunication systems. Thanks to the advent of new spectrum resources and\ndegrees of freedom brought by the near-field propagation, the mid-band XL-MIMO\nsystem is expected to significantly enhance throughput and inherently support\nadvanced functionalities such as integrated sensing and communication. Although\ntheoretical studies have highlighted the benefits of mid-band XL-MIMO systems,\nthe promised performance gains have yet to be validated in practical systems,\nposing a major challenge to the standardization. In this paper, preliminaries\nare first discussed, followed by an analysis of key challenges in constructing\na real-time prototype system. Subsequently, the design and implementation of a\nreal-time mid-band XL-MIMO prototype system are presented. Benefiting from the\nnovel architecture, the proposed prototype system supports metrics aligned with\nstandardization, including a bandwidth of 200 MHz, up to 1024 antenna elements,\nand up to 256 transceiver chains. Operating in time-division duplexing (TDD)\nmode, the prototype enables multiuser communication with support for up to 12\nusers, while retaining standard communication procedures. Built on\nsoftware-defined radio (SDR) platforms, the system is programmable and allows\nfor flexible deployment of advanced algorithms. Moreover, the modular\narchitecture ensures high scalability, making the system adaptable to various\nconfigurations, including distributed deployments and decentralized signal\nprocessing. Experimental results with the proposed prototype system demonstrate\nreal-time digital sample processing at 1167.85 Gbps, a peak data throughput of\n15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80\nbit/s/Hz."}
{"id": "2510.03111", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03111", "abs": "https://arxiv.org/abs/2510.03111", "authors": ["Matías Di Bernardo", "Emmanuel Misley", "Ignacio Correa", "Mateo García Iacovelli", "Simón Mellino", "Gala Lucía Gonzalez Barrios"], "title": "Evaluation of preprocessing pipelines in the creation of in-the-wild TTS datasets", "comment": "5 pages, 4 figures, Submitted to ICASSP 2026", "summary": "This work introduces a reproducible, metric-driven methodology to evaluate\npreprocessing pipelines for in-the-wild TTS corpora generation. We apply a\ncustom low-cost pipeline to the first in-the-wild Argentine Spanish collection\nand compare 24 pipeline configurations combining different denoising and\nquality filtering variants. Evaluation relies on complementary objective\nmeasures (PESQ, SI-SDR, SNR), acoustic descriptors (T30, C50), and\nspeech-preservation metrics (F0-STD, MCD). Results expose trade-offs between\ndataset size, signal quality, and voice preservation; where denoising variants\nwith permissive filtering provide the best overall compromise for our testbed.\nThe proposed methodology allows selecting pipeline configurations without\ntraining TTS models for each subset, accelerating and reducing the cost of\npreprocessing development for low-resource settings."}
{"id": "2510.02864", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02864", "abs": "https://arxiv.org/abs/2510.02864", "authors": ["Viola Negroni", "Davide Salvi", "Daniele Ugo Leonzio", "Paolo Bestagini", "Stefano Tubaro"], "title": "Forensic Similarity for Speech Deepfakes", "comment": "Submitted @ IEEE OJSP", "summary": "In this paper, we introduce a digital audio forensics approach called\nForensic Similarity for Speech Deepfakes, which determines whether two audio\nsegments contain the same forensic traces or not. Our work is inspired by prior\nwork in the image domain on forensic similarity, which proved strong\ngeneralization capabilities against unknown forensic traces, without requiring\nprior knowledge of them at training time. To achieve this in the audio setting,\nwe propose a two-part deep-learning system composed of a feature extractor\nbased on a speech deepfake detector backbone and a shallow neural network,\nreferred to as the similarity network. This system maps pairs of audio segments\nto a score indicating whether they contain the same or different forensic\ntraces. We evaluate the system on the emerging task of source verification,\nhighlighting its ability to identify whether two samples originate from the\nsame generative model. Additionally, we assess its applicability to splicing\ndetection as a complementary use case. Experiments show that the method\ngeneralizes to a wide range of forensic traces, including previously unseen\nones, illustrating its flexibility and practical value in digital audio\nforensics."}
{"id": "2510.02939", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02939", "abs": "https://arxiv.org/abs/2510.02939", "authors": ["Xin Tong", "Zhaoyang Zhang", "Yuzhi Yang", "Yu Ge", "Zhaohui Yang", "Henk Wymeersch", "Mérouane Debbah"], "title": "Integrated Sensing, Communication, and Positioning in Cellular Vehicular Networks", "comment": "This paper is accepted by IEEE Transactions on Vehicular Technology", "summary": "In this correspondence, a novel integrated sensing and communication (ISAC)\nframework is proposed to accomplish data communication, vehicle positioning,\nand environment sensing simultaneously in a cellular vehicular network. By\nincorporating the vehicle positioning problem with the existing\ncomputational-imaging-based ISAC models, we formulate a special integrated\nsensing, communication, and positioning problem in which the unknowns are\nhighly coupled. To mitigate the rank deficiency and make it solvable, we\ndiscretize the region of interest (ROI) into sensing and positioning pixels\nrespectively, and exploit both the line-of-sight and non-line-of-sight\npropagation of the vehicles' uplink access signals. The resultant problem is\nshown to be a polynomial bilinear compressed sensing (CS) reconstruction\nproblem, which is then solved by the alternating optimization (AO) algorithm to\niteratively achieve symbol detection, vehicle positioning and environment\nsensing. Performance analysis and numerical results demonstrate the\neffectiveness of the proposed method."}
{"id": "2510.02382", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02382", "abs": "https://arxiv.org/abs/2510.02382", "authors": ["Xuemai Xie", "Xianrui Wang", "Liyuan Zhang", "Yichen Yang", "Shoji Makino"], "title": "Accelerated Convolutive Transfer Function-Based Multichannel NMF Using Iterative Source Steering", "comment": null, "summary": "Among numerous blind source separation (BSS) methods, convolutive transfer\nfunction-based multichannel non-negative matrix factorization (CTF-MNMF) has\ndemonstrated strong performance in highly reverberant environments by modeling\nmulti-frame correlations of delayed source signals. However, its practical\ndeployment is hindered by the high computational cost associated with the\niterative projection (IP) update rule, which requires matrix inversion for each\nsource. To address this issue, we propose an efficient variant of CTF-MNMF that\nintegrates iterative source steering (ISS), a matrix inversion-free update rule\nfor separation filters. Experimental results show that the proposed method\nachieves comparable or superior separation performance to the original\nCTF-MNMF, while significantly reducing the computational complexity."}
{"id": "2510.02915", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02915", "abs": "https://arxiv.org/abs/2510.02915", "authors": ["Wei Fan", "Kejiang Chen", "Xiangkun Wang", "Weiming Zhang", "Nenghai Yu"], "title": "WavInWav: Time-domain Speech Hiding via Invertible Neural Network", "comment": "13 pages, 5 figures, project page:\n  https://cyberrrange.github.io/project/wavinwav", "summary": "Data hiding is essential for secure communication across digital media, and\nrecent advances in Deep Neural Networks (DNNs) provide enhanced methods for\nembedding secret information effectively. However, previous audio hiding\nmethods often result in unsatisfactory quality when recovering secret audio,\ndue to their inherent limitations in the modeling of time-frequency\nrelationships. In this paper, we explore these limitations and introduce a new\nDNN-based approach. We use a flow-based invertible neural network to establish\na direct link between stego audio, cover audio, and secret audio, enhancing the\nreversibility of embedding and extracting messages. To address common issues\nfrom time-frequency transformations that degrade secret audio quality during\nrecovery, we implement a time-frequency loss on the time-domain signal. This\napproach not only retains the benefits of time-frequency constraints but also\nenhances the reversibility of message recovery, which is vital for practical\napplications. We also add an encryption technique to protect the hidden data\nfrom unauthorized access. Experimental results on the VCTK and LibriSpeech\ndatasets demonstrate that our method outperforms previous approaches in terms\nof subjective and objective metrics and exhibits robustness to various types of\nnoise, suggesting its utility in targeted secure communication scenarios."}
{"id": "2510.02979", "categories": ["eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.02979", "abs": "https://arxiv.org/abs/2510.02979", "authors": ["Jonathan Baum", "Chamot-Nonin Manon", "Oppelt Vera", "David Guiraud", "Christine Azevedo Coste", "Thomas Guiho"], "title": "Towards Electrophysiological and Histological Mapping of Upper Limb Nerves in Pigs Using Epineural Stimulation", "comment": "FESWS 2025 - 15th Vienna International Workshop on Functional\n  Electrical Stimulation & 30 years IFESS Anniversary, IFESS, Sep 2025, Vienna,\n  Austria", "summary": "Understanding the relationship between nerve anatomy and the functional\noutcomes of electrical stimulation is critical for optimizing neural interface\ndesign. In this study, we conducted acute experiments on four pigs in which\nepineural cuff electrodes with multiple contacts were placed around upper limb\nnerves. A subset of electrical stimulation configurations -- previously\nidentified via computational study -- was applied, and the resulting evoked\nelectromyographic (EMG) responses were recorded from target muscles. Muscle\nrecruitment curves were extracted and analysed offline to quantify activation\npatterns. Following the electrophysiological experiments, the stimulated nerves\nwere harvested and processed for histological analysis to visualize fascicular\norganization and distribution. This work presents preliminary results from the\ncombined analysis of muscle activation profiles and fascicle anatomy in one\nanimal. Our findings aim to inform the design of stimulation strategies by\nlinking electrode configuration to selective muscle recruitment, ultimately\ncontributing to more effective neuromodulation and neuroprosthetic\napplications."}
{"id": "2510.02401", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02401", "abs": "https://arxiv.org/abs/2510.02401", "authors": ["Konrad Szewczyk", "Daniel Gallo Fernández", "James Townsend"], "title": "Linear RNNs for autoregressive generation of long music samples", "comment": null, "summary": "Directly learning to generate audio waveforms in an autoregressive manner is\na challenging task, due to the length of the raw sequences and the existence of\nimportant structure on many different timescales. Traditional approaches based\non recurrent neural networks, as well as causal convolutions and\nself-attention, have only had limited success on this task. However, recent\nwork has shown that deep state space models, also referred to as linear RNNs,\ncan be highly efficient in this context. In this work, we push the boundaries\nof linear RNNs applied to raw audio modeling, investigating the effects of\ndifferent architectural choices and using context-parallelism to enable\ntraining on sequences up to one minute (1M tokens) in length. We present a\nmodel, HarmonicRNN, which attains state of the art log-likelihoods and\nperceptual metrics on small-scale datasets."}
{"id": "2510.02916", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02916", "abs": "https://arxiv.org/abs/2510.02916", "authors": ["Amir Dellali", "Luca A. Lanzendörfer", "Florian Grötschla", "Roger Wattenhofer"], "title": "SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos", "comment": null, "summary": "We propose SALSA-V, a multimodal video-to-audio generation model capable of\nsynthesizing highly synchronized, high-fidelity long-form audio from silent\nvideo content. Our approach introduces a masked diffusion objective, enabling\naudio-conditioned generation and the seamless synthesis of audio sequences of\nunconstrained length. Additionally, by integrating a shortcut loss into our\ntraining process, we achieve rapid generation of high-quality audio samples in\nas few as eight sampling steps, paving the way for near-real-time applications\nwithout requiring dedicated fine-tuning or retraining. We demonstrate that\nSALSA-V significantly outperforms existing state-of-the-art methods in both\naudiovisual alignment and synchronization with video content in quantitative\nevaluation and a human listening study. Furthermore, our use of random masking\nduring training enables our model to match spectral characteristics of\nreference audio samples, broadening its applicability to professional audio\nsynthesis tasks such as Foley generation and sound design."}
{"id": "2510.03019", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03019", "abs": "https://arxiv.org/abs/2510.03019", "authors": ["Yang Zhou", "Haochang Wu", "Yunxi Mu", "Hao Qin", "Xinyue Zhang", "Xingqi Zhang"], "title": "Physics-Constrained Inc-GAN for Tunnel Propagation Modeling from Sparse Line Measurements", "comment": null, "summary": "High-speed railway tunnel communication systems require reliable radio wave\npropagation prediction to ensure operational safety. However, conventional\nsimulation methods face challenges of high computational complexity and\ninability to effectively process sparse measurement data collected during\nactual railway operations. This letter proposes an inception-enhanced\ngenerative adversarial network (Inc-GAN) that can reconstruct complete electric\nfield distributions across tunnel cross-sections using sparse value lines\nmeasured during actual train operations as input. This directly addresses\npractical railway measurement constraints. Through an inception-based generator\narchitecture and progressive training strategy, the method achieves robust\nreconstruction from single measurement signal lines to complete field\ndistributions. Numerical simulation validation demonstrates that Inc-GAN can\naccurately predict electric fields based on measured data collected during\nactual train operations, with significantly improved computational efficiency\ncompared to traditional methods, providing a novel solution for railway\ncommunication system optimization based on real operational data."}
{"id": "2510.02915", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02915", "abs": "https://arxiv.org/abs/2510.02915", "authors": ["Wei Fan", "Kejiang Chen", "Xiangkun Wang", "Weiming Zhang", "Nenghai Yu"], "title": "WavInWav: Time-domain Speech Hiding via Invertible Neural Network", "comment": "13 pages, 5 figures, project page:\n  https://cyberrrange.github.io/project/wavinwav", "summary": "Data hiding is essential for secure communication across digital media, and\nrecent advances in Deep Neural Networks (DNNs) provide enhanced methods for\nembedding secret information effectively. However, previous audio hiding\nmethods often result in unsatisfactory quality when recovering secret audio,\ndue to their inherent limitations in the modeling of time-frequency\nrelationships. In this paper, we explore these limitations and introduce a new\nDNN-based approach. We use a flow-based invertible neural network to establish\na direct link between stego audio, cover audio, and secret audio, enhancing the\nreversibility of embedding and extracting messages. To address common issues\nfrom time-frequency transformations that degrade secret audio quality during\nrecovery, we implement a time-frequency loss on the time-domain signal. This\napproach not only retains the benefits of time-frequency constraints but also\nenhances the reversibility of message recovery, which is vital for practical\napplications. We also add an encryption technique to protect the hidden data\nfrom unauthorized access. Experimental results on the VCTK and LibriSpeech\ndatasets demonstrate that our method outperforms previous approaches in terms\nof subjective and objective metrics and exhibits robustness to various types of\nnoise, suggesting its utility in targeted secure communication scenarios."}
{"id": "2510.02995", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02995", "abs": "https://arxiv.org/abs/2510.02995", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michel Dumontier"], "title": "AudioToolAgent: An Agentic Framework for Audio-Language Models", "comment": null, "summary": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks\nbut lack multi-step reasoning and tool-calling found in recent Large Language\nModels (LLMs). This paper presents AudioToolAgent, a framework that coordinates\naudio-language models as tools via a central LLM agent that accesses tool\nadapters for audio question answering and speech-to-text. The agent selects\ntools, asks follow-up questions, and compares outputs for verification.\nExperiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to\n74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling\nfor shapley values across 374 configurations identifies effective agent-tool\ncombinations. The modular design allows integration of new tools and eliminates\nthe use of data and training costs. Code and reproduction materials are\navailable at: github.com/GLJS/AudioToolAgent"}
{"id": "2510.03055", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03055", "abs": "https://arxiv.org/abs/2510.03055", "authors": ["Dexin Wang", "Isha Jariwala", "Ahmad Bazzi", "Sundeep Rangan", "Theodore S. Rappaport", "Marwa Chafii"], "title": "Compressed Multiband Sensing in FR3 Using Alternating Direction Method of Multipliers", "comment": "submitted to IEEE Wireless Communications and Networking Conference\n  (WCNC) 2026", "summary": "Joint detection and localization of users and scatterers in multipath-rich\nchannels on multiple bands is critical for integrated sensing and communication\n(ISAC) in 6G. Existing multiband sensing methods are limited by classical\nbeamforming or computationally expensive approaches. This paper introduces\nalternating direction method of multipliers (ADMM)-assisted compressed\nmultiband sensing (CMS), hereafter referred to as ADMM-CMS, which is a novel\nframework for multiband sensing using uplink QAM-modulated pilot symbols. To\nsolve the CMS problem, we develop an adaptive ADMM algorithm that adjusts to\nnoise and ensures automatic stopping if converged. ADMM combines the\ndecomposability of dual ascent with the robustness of augmented Lagrangian\nmethods, making it suitable for large-scale structured optimization.\nSimulations show that ADMM-CMS achieves higher spatial resolution and improved\ndenoising compared to Bartlett-type beamforming, yielding a 34 dB gain in\nper-antenna transmit power for achieving a 0.9 successful recovery probability\n(SRP). Moreover, compared to performing compressed sensing separately on the\nconstituent 7 GHz and 10 GHz sub-bands, ADMM-CMS achieves reductions in delay\nroot mean squared error of 35% and 38.1%, respectively, at -41 dBm per-antenna\ntransmit power, while also yielding improved SRP. Our findings demonstrate\nADMM-CMS as an efficient enabler of ISAC in frequency range 3 (FR3, 7-24 GHz)\nfor 6G systems."}
{"id": "2510.02320", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02320", "abs": "https://arxiv.org/abs/2510.02320", "authors": ["Yongqi Kang", "Yong Zhao"], "title": "WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis", "comment": "5 pages", "summary": "The advancement of computational psychology requires AI tools capable of\ndeeply understanding counseling dialogues. Existing audio language models\n(AudioLLMs) often rely on single speech encoders pre-trained on general data,\nstruggling to capture domain-specific features like complex emotions and\nprofessional techniques. To address this, we propose WEE-Therapy, a multi-task\nAudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This\nsupplements a powerful base encoder with a pool of lightweight, specialized\nencoders. A novel dual-routing strategy combines stable, data-independent\ndomain knowledge with dynamic, data-dependent expert selection. Evaluated on\nemotion recognition, technique classification, risk detection, and\nsummarization, WEE-Therapy achieves significant performance gains across all\ntasks with minimal parameter overhead, demonstrating strong potential for\nAI-assisted clinical analysis."}
{"id": "2510.03069", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03069", "abs": "https://arxiv.org/abs/2510.03069", "authors": ["Rom Hirsch", "Ziv Aharoni", "Henry D. Pfister", "Haim H. Permuter"], "title": "A Study of Neural Polar Decoders for Communication", "comment": null, "summary": "In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for\nend-to-end communication systems. While prior work demonstrated the\neffectiveness of NPDs on synthetic channels, this study extends the NPD to\nreal-world communication systems. The NPD was adapted to complete OFDM and\nsingle-carrier communication systems. To satisfy practical system requirements,\nthe NPD is extended to support any code length via rate matching, higher-order\nmodulations, and robustness across diverse channel conditions. The NPD operates\ndirectly on channels with memory, exploiting their structure to achieve higher\ndata rates without requiring pilots and a cyclic prefix. Although NPD entails\nhigher computational complexity than the standard 5G polar decoder, its neural\nnetwork architecture enables an efficient representation of channel statistics,\nresulting in manageable complexity suitable for practical systems. Experimental\nresults over 5G channels demonstrate that the NPD consistently outperforms the\n5G polar decoder in terms of BER, BLER, and throughput. These improvements are\nparticularly significant for low-rate and short-block configurations, which are\nprevalent in 5G control channels. Furthermore, NPDs applied to single-carrier\nsystems offer performance comparable to OFDM with lower PAPR, enabling\neffective single-carrier transmission over 5G channels. These results position\nthe NPD as a high-performance, pilotless, and robust decoding solution."}
{"id": "2510.02398", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02398", "abs": "https://arxiv.org/abs/2510.02398", "authors": ["Shree Harsha Bokkahalli Satish", "Gustav Eje Henter", "Éva Székely"], "title": "When Voice Matters: Evidence of Gender Disparity in Positional Bias of SpeechLLMs", "comment": "16 pages, 5 figures, To Appear in SPECOM 2025", "summary": "The rapid development of SpeechLLM-based conversational AI systems has\ncreated a need for robustly benchmarking these efforts, including aspects of\nfairness and bias. At present, such benchmarks typically rely on multiple\nchoice question answering (MCQA). In this paper, we present the first\ntoken-level probabilistic evaluation and response-based study of several issues\naffecting the use of MCQA in SpeechLLM benchmarking: 1) we examine how model\ntemperature and prompt design affect gender and positional bias on an MCQA\ngender-bias benchmark; 2) we examine how these biases are affected by the\ngender of the input voice; and 3) we study to what extent observed trends carry\nover to a second gender-bias benchmark. Our results show that concerns about\npositional bias from the text domain are equally valid in the speech domain. We\nalso find the effect to be stronger for female voices than for male voices. To\nour knowledge, this is the first study to isolate positional bias effects in\nSpeechLLM-based gender-bias benchmarks. We conclude that current MCQA\nbenchmarks do not account for speech-based bias and alternative strategies are\nneeded to ensure fairness towards all users."}
{"id": "2510.02556", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.02556", "abs": "https://arxiv.org/abs/2510.02556", "authors": ["Klaus Brümann", "Simon Doclo"], "title": "Multi-Source Position and Direction-of-Arrival Estimation Based on Euclidean Distance Matrices", "comment": "13 pages, 6 figures, submitted to IEEE Transactions on Audio, Speech\n  and Language Processing (awaiting review)", "summary": "A popular method to estimate the positions or directions-of-arrival (DOAs) of\nmultiple sound sources using an array of microphones is based on\nsteered-response power (SRP) beamforming. For a three-dimensional scenario,\nSRP-based methods need to jointly optimize three continuous variables for\nposition estimation or two continuous variables for DOA estimation, which can\nbe computationally expensive. In this paper, we propose novel methods for\nmulti-source position and DOA estimation by exploiting properties of Euclidean\ndistance matrices (EDMs) and their respective Gram matrices. In the proposed\nmulti-source position estimation method only a single continuous variable,\nrepresenting the distance between each source and a reference microphone, needs\nto be optimized. For each source, the optimal continuous distance variable and\nset of candidate time-difference of arrival (TDOA) estimates are determined by\nminimizing a cost function that is defined using the eigenvalues of the Gram\nmatrix. The estimated relative source positions are then mapped to estimated\nabsolute source positions by solving an orthogonal Procrustes problem for each\nsource. The proposed multi-source DOA estimation method entirely eliminates the\nneed for continuous variable optimization by defining a relative coordinate\nsystem per source such that one of its coordinate axes is aligned with the\nrespective source DOA. The optimal set of candidate TDOA estimates is\ndetermined by minimizing a cost function that is defined using the eigenvalues\nof a rank-reduced Gram matrix. The computational cost of the proposed EDM-based\nmethods is significantly reduced compared to the SRP-based methods.\nExperimental results for different source and microphone configurations show\nthat the proposed EDM-based method consistently outperforms the SRP-based\nmethod in terms of two-source position and DOA estimation accuracy."}
{"id": "2510.02672", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.02672", "abs": "https://arxiv.org/abs/2510.02672", "authors": ["Dyah A. M. G. Wisnu", "Ryandhimas E. Zezario", "Stefano Rini", "Fo-Rui Li", "Yan-Tsung Peng", "Hsin-Min Wang", "Yu Tsao"], "title": "STSM-FiLM: A FiLM-Conditioned Neural Architecture for Time-Scale Modification of Speech", "comment": null, "summary": "Time-Scale Modification (TSM) of speech aims to alter the playback rate of\naudio without changing its pitch. While classical methods like Waveform\nSimilarity-based Overlap-Add (WSOLA) provide strong baselines, they often\nintroduce artifacts under non-stationary or extreme stretching conditions. We\npropose STSM-FILM - a fully neural architecture that incorporates Feature-Wise\nLinear Modulation (FiLM) to condition the model on a continuous speed factor.\nBy supervising the network using WSOLA-generated outputs, STSM-FILM learns to\nmimic alignment and synthesis behaviors while benefiting from representations\nlearned through deep learning. We explore four encoder-decoder variants:\nSTFT-HiFiGAN, WavLM-HiFiGAN, Whisper-HiFiGAN, and EnCodec, and demonstrate that\nSTSM-FILM is capable of producing perceptually consistent outputs across a wide\nrange of time-scaling factors. Overall, our results demonstrate the potential\nof FiLM-based conditioning to improve the generalization and flexibility of\nneural TSM models."}
{"id": "2510.03025", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03025", "abs": "https://arxiv.org/abs/2510.03025", "authors": ["Christos Garoufis", "Athanasia Zlatintsi", "Petros Maragos"], "title": "CVSM: Contrastive Vocal Similarity Modeling", "comment": "13 pages, 3 tables, 8 figures. Submitted article at IEEE Trans. on\n  Audio, Speech and Language Proc. (pre-print version)", "summary": "The availability of large, unlabeled datasets across various domains has\ncontributed to the development of a plethora of methods that learn\nrepresentations for multiple target (downstream) tasks through self-supervised\npre-training. In this work, we introduce CVSM (Contrastive Vocal Similarity\nModeling), a contrastive self-supervised procedure for music signal\nrepresentation learning in the audio domain that can be utilized for musical\nand vocal similarity modeling. Our method operates under a contrastive\nframework, maximizing the similarity between vocal excerpts and musical\nmixtures containing the same vocals; we devise both a label-informed protocol,\nleveraging artist identity information to sample the contrastive pairs, and a\nlabel-agnostic scheme, involving artificial mixture creation from randomly\nsampled vocal and accompaniment excerpts, which are paired with vocals from the\nsame audio segment. We evaluate our proposed method in measuring vocal\nsimilarity both objectively, through linear probing on a suite of appropriate\ndownstream tasks, and subjectively, via conducting a user study consisting of\npairwise comparisons between different models in a recommendation-by-query\nsetting. Our results indicate that the representations learned through CVSM are\neffective in musical and vocal similarity modeling, outperforming numerous\nbaselines across both isolated vocals and complete musical mixtures. Moreover,\nwhile the availability of artist identity labels during pre-training leads to\noverall more consistent performance both in the evaluated downstream tasks and\nthe user study, a label-agnostic CVSM variant incorporating hybrid pre-training\nwith real and artificial mixtures achieves comparable performance to the\nlabel-informed one in artist identification and perceived vocal similarity."}

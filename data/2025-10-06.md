<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Rate-Adaptive Semantic Communication via Multi-Stage Vector Quantization](https://arxiv.org/abs/2510.02646)
*Jinsung Park,Junyong Shin,Yongjeong Oh,Jihun Park,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 提出基于多阶段矢量量化的速率自适应语义通信框架MSVQ-SC，通过动态激活量化阶段和模块实现细粒度速率控制，同时降低计算复杂度和码本崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统单阶段VQ方法需要指数级增大码本才能获得更高保真度，这导致计算复杂度和码本崩溃问题，需要更高效的速率自适应语义通信方法。

Method: 将量化过程分解为多阶段，动态激活阶段和VQ模块；提出模块选择问题并采用增量分配算法求解；结合熵编码利用非均匀码字分布。

Result: 在CIFAR-10数据集上的仿真结果表明，该框架优于现有数字语义通信方法，在更低复杂度下实现更优的语义保真度，并提供灵活的细粒度速率控制。

Conclusion: MSVQ-SC框架通过多阶段VQ和动态模块激活，有效解决了语义通信中的速率自适应问题，在保持高性能的同时降低了复杂度。

Abstract: This paper proposes a novel framework for rate-adaptive semantic
communication based on multi-stage vector quantization (VQ), termed
\textit{MSVQ-SC}. Unlike conventional single-stage VQ approaches, which require
exponentially larger codebooks to achieve higher fidelity, the proposed
framework decomposes the quantization process into multiple stages and
dynamically activates both stages and individual VQ modules. This design
enables fine-grained rate adaptation under varying bit constraints while
mitigating computational complexity and the codebook collapse problem. To
optimize performance, we formulate a module selection problem that minimizes
task loss subject to a rate constraint and solve it using an incremental
allocation algorithm. Furthermore, we extend the framework by incorporating
entropy coding to exploit non-uniform codeword distributions, further reducing
communication overhead. Simulation results on the CIFAR-10 dataset demonstrate
that the proposed framework outperforms existing digital semantic communication
methods, achieving superior semantic fidelity with lower complexity while
providing flexible and fine-grained rate control.

</details>


### [2] [Mutual Information-Driven Visualization and Clustering for Core KPI Selection in O-RAN Testing](https://arxiv.org/abs/2510.02696)
*Anish Pradhan,Lingjia Liu,Harpreet S. Dhillon*

Main category: eess.SP

TL;DR: 提出AMIF-MDS方法，使用聚合互信息(AMIF)和多维缩放(MDS)来识别O-RAN系统中性能测量之间的依赖关系，通过DBSCAN聚类找到核心性能指标集。


<details>
  <summary>Details</summary>
Motivation: 随着O-RAN系统复杂性增加，性能测量数量呈指数增长，测试变得困难。需要识别各种性能测量之间的依赖关系来简化测试过程和改进系统设计。

Method: 使用聚合互信息(AMIF)作为定向信息的实用代理，结合多维缩放(MDS)可视化序列间依赖关系，并应用DBSCAN聚类来分组互信息指标。

Result: 成功识别了链路自适应指标等集群，并得到了用于未来学习驱动O-RAN测试的核心性能测量集。

Conclusion: AMIF-MDS方法能有效识别O-RAN时间序列测试数据中的依赖关系，有助于简化测试流程和聚焦核心性能指标。

Abstract: O-RAN testing is becoming increasingly difficult with the exponentially
growing number of performance measurements as the system grows more complex,
with additional units, interfaces, applications, and possible implementations
and configurations. To simplify the testing procedure and improve system design
for O-RAN systems, it is important to identify the dependencies among various
performance measurements, which are inherently time-series and can be modeled
as realizations of random processes. While information theory can be utilized
as a principled foundation for mapping these dependencies, the robust
estimation of such measures for random processes from real-world data remains
challenging. This paper introduces AMIF-MDS, which employs aggregate mutual
Information in frequency (AMIF), a practical proxy for directed information
(DI), to quantify similarity and visualize inter-series dependencies with
multidimensional scaling (MDS). The proposed quantile-based AMIF estimator is
applied to O-RAN time-series testing data to identify dependencies among
various performance measures so that we can focus on a set of ``core''
performance measures. Applying density-based spatial clustering of applications
with noise (DBSCAN) to the MDS embedding groups mutually informative metrics,
organically reveals the link-adaptation indicators among other clusters, and
yields a ``core'' performance measure set for future learning-driven O-RAN
testing.

</details>


### [3] [Denoising and Augmentation: A Dual Use of Diffusion Model for Enhanced CSI Recovery](https://arxiv.org/abs/2510.02744)
*Yupeng Li,Ruhao Zhang,Yitong Liu,Chunju Shao,Jing Jin,Shijian Gao*

Main category: eess.SP

TL;DR: 提出一种基于DDPM的双重应用信道估计算法，结合数据去噪和增强功能，在精度和计算成本之间实现优越权衡


<details>
  <summary>Details</summary>
Motivation: 解决原始信号在导频位置的严重噪声问题，以及深度学习网络训练所需的大量数据需求

Method: 采用无监督结构清理现场数据，通过调整反向步骤生成新信道数据，并提出分段前向策略处理不同信噪比

Result: 链路级仿真表明该方案在精度和计算成本之间达到优于现有基准的权衡

Conclusion: DDPM框架在信道估计中具有双重应用潜力，能有效处理噪声和数据不足问题

Abstract: This letter introduces a dual application of denoising diffusion
probabilistic model (DDPM)-based channel estimation algorithm integrating data
denoising and augmentation. Denoising addresses the severe noise in raw signals
at pilot locations, which can impair channel estimation accuracy. An
unsupervised structure is proposed to clean field data without prior knowledge
of pure channel information. Data augmentation is crucial due to the
data-intensive nature of training deep learning (DL) networks for channel state
information (CSI) estimation. The network generates new channel data by
adjusting reverse steps, enriching the training dataset. To manage varying
signal-to-noise ratios (SNRs) in communication data, a piecewise forward
strategy is proposed to enhance the DDPM convergence precision. The link-level
simulations indicate that the proposed scheme achieves a superior tradeoff
between precision and computational cost compared to existing benchmarks.

</details>


### [4] [Neyman Pearson Detector for Multiple Ambient Backscatter Zero-Energy-Devices Beacons using Near-Perfect Code](https://arxiv.org/abs/2510.02785)
*Shanglin Yang,Jean-Marie Gorce,Muhammad Jehangir Khan,Dinh-Thuy Phan-Huy,Guillaume Villemaud*

Main category: eess.SP

TL;DR: 提出了一种基于近完美码的多标签检测方案，用于零能耗设备的室内定位系统，在低信噪比下显著提高了峰值旁瓣比和标签可分离性。


<details>
  <summary>Details</summary>
Motivation: 现有的单标签检测方法在多标签共存、干扰和同步不确定性的环境中性能受限，需要开发专门的多标签检测方案来提高系统的可扩展性和可靠性。

Method: 使用近完美码作为同步序列，用双相关器替代双带通滤波，结合对比度度量和多频组合来检测次要标签，构建显式贝叶斯检测器。

Result: 在CorteXlab测试平台上验证，峰值旁瓣比从约11dB提升到约22dB，在低信噪比下表现出良好的鲁棒性。

Conclusion: 该方案显著提升了多标签环境下环境反向散射定位系统的可扩展性和可靠性，为实际应用提供了有效解决方案。

Abstract: Recently, a novel ultra-low-power indoor localization system based on
Zero-Energy Devices (ZEDs) has shown promising results in ambient backscatter
communication. In this paper, we study detection of multiple coexisting ZEDs in
ambient backscatter systems under interference and synchronization uncertainty.
Building on a Neyman-Pearson (NP) formulation previously applied to single-tag
detection, we introduce a detector tailored to multi-tag scenarios. The core
idea is to use a Near-Perfect Code (NPC) as the synchronization sequence, which
substantially improves the peak-to-sidelobe (PSL) ratio and thus separability
among concurrent tags. The proposed scheme replaces dual band-pass filtering
with dual correlators, enabling an explicit Bayesian detector and tight control
of the false-alarm rate; we further incorporate a contrast metric and
multi-frequency combining to reveal secondary tags. Experiments on the
CorteXlab testbed (part of the SLICES-EU infrastructure) confirm robustness at
low SNR, with observed PSL improvements from about 11 dB to about 22 dB. These
results advance scalable, reliable ambient backscatter localization in
practical multi-tag environments.

</details>


### [5] [Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and Implementation](https://arxiv.org/abs/2510.02793)
*Jiachen Tian,Yu Han,Zhengtao Jin,Xi Yang,Jie Yang,Wankai Tang,Xiao Li,Wenjin Wang,Shi Jin*

Main category: eess.SP

TL;DR: 本文设计并实现了一个实时中频段超大规模MIMO原型系统，支持200MHz带宽、1024个天线单元和256个收发链，实验结果显示系统实现了15.81Gbps的峰值数据吞吐量和接近80bit/s/Hz的频谱效率。


<details>
  <summary>Details</summary>
Motivation: 虽然理论研究已证明中频段XL-MIMO系统的优势，但实际性能增益尚未在实际系统中得到验证，这对标准化构成了主要挑战。

Method: 基于软件定义无线电平台构建实时原型系统，采用时分双工模式，支持多用户通信和标准通信流程，具有模块化架构确保高可扩展性。

Result: 原型系统实现了1167.85Gbps的实时数字样本处理速率，12用户时峰值数据吞吐量达15.81Gbps，最大频谱效率接近80bit/s/Hz。

Conclusion: 该原型系统成功验证了中频段XL-MIMO的实际性能，为未来通信系统标准化提供了重要参考依据。

Abstract: The mid-band frequency range, combined with extra large-scale multiple-input
multiple-output (XL-MIMO), is emerging as a key enabler for future
communication systems. Thanks to the advent of new spectrum resources and
degrees of freedom brought by the near-field propagation, the mid-band XL-MIMO
system is expected to significantly enhance throughput and inherently support
advanced functionalities such as integrated sensing and communication. Although
theoretical studies have highlighted the benefits of mid-band XL-MIMO systems,
the promised performance gains have yet to be validated in practical systems,
posing a major challenge to the standardization. In this paper, preliminaries
are first discussed, followed by an analysis of key challenges in constructing
a real-time prototype system. Subsequently, the design and implementation of a
real-time mid-band XL-MIMO prototype system are presented. Benefiting from the
novel architecture, the proposed prototype system supports metrics aligned with
standardization, including a bandwidth of 200 MHz, up to 1024 antenna elements,
and up to 256 transceiver chains. Operating in time-division duplexing (TDD)
mode, the prototype enables multiuser communication with support for up to 12
users, while retaining standard communication procedures. Built on
software-defined radio (SDR) platforms, the system is programmable and allows
for flexible deployment of advanced algorithms. Moreover, the modular
architecture ensures high scalability, making the system adaptable to various
configurations, including distributed deployments and decentralized signal
processing. Experimental results with the proposed prototype system demonstrate
real-time digital sample processing at 1167.85 Gbps, a peak data throughput of
15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80
bit/s/Hz.

</details>


### [6] [Integrated Sensing, Communication, and Positioning in Cellular Vehicular Networks](https://arxiv.org/abs/2510.02939)
*Xin Tong,Zhaoyang Zhang,Yuzhi Yang,Yu Ge,Zhaohui Yang,Henk Wymeersch,Mérouane Debbah*

Main category: eess.SP

TL;DR: 提出了一种新的集成感知与通信框架，在蜂窝车联网中同时实现数据通信、车辆定位和环境感知，通过交替优化算法解决高度耦合的未知参数问题。


<details>
  <summary>Details</summary>
Motivation: 现有的计算成像ISAC模型缺乏车辆定位功能，需要开发能够同时处理通信、定位和环境感知的综合解决方案，以充分利用车辆上行信号的视距和非视距传播特性。

Method: 将感兴趣区域离散化为感知和定位像素，利用车辆上行信号的视距和非视距传播，将问题建模为多项式双线性压缩感知重建问题，采用交替优化算法迭代进行符号检测、车辆定位和环境感知。

Result: 性能分析和数值结果表明所提方法的有效性，能够成功解决高度耦合的未知参数问题，实现通信、定位和感知的同步完成。

Conclusion: 该框架成功地将车辆定位集成到ISAC模型中，通过交替优化算法有效解决了参数耦合问题，为蜂窝车联网提供了同时实现通信、定位和环境感知的可行方案。

Abstract: In this correspondence, a novel integrated sensing and communication (ISAC)
framework is proposed to accomplish data communication, vehicle positioning,
and environment sensing simultaneously in a cellular vehicular network. By
incorporating the vehicle positioning problem with the existing
computational-imaging-based ISAC models, we formulate a special integrated
sensing, communication, and positioning problem in which the unknowns are
highly coupled. To mitigate the rank deficiency and make it solvable, we
discretize the region of interest (ROI) into sensing and positioning pixels
respectively, and exploit both the line-of-sight and non-line-of-sight
propagation of the vehicles' uplink access signals. The resultant problem is
shown to be a polynomial bilinear compressed sensing (CS) reconstruction
problem, which is then solved by the alternating optimization (AO) algorithm to
iteratively achieve symbol detection, vehicle positioning and environment
sensing. Performance analysis and numerical results demonstrate the
effectiveness of the proposed method.

</details>


### [7] [Towards Electrophysiological and Histological Mapping of Upper Limb Nerves in Pigs Using Epineural Stimulation](https://arxiv.org/abs/2510.02979)
*Jonathan Baum,Chamot-Nonin Manon,Oppelt Vera,David Guiraud,Christine Azevedo Coste,Thomas Guiho*

Main category: eess.SP

TL;DR: 该研究通过结合电生理实验和神经组织学分析，探索神经解剖结构与电刺激功能结果之间的关系，旨在优化神经接口设计。


<details>
  <summary>Details</summary>
Motivation: 理解神经解剖与电刺激功能结果的关系对于优化神经接口设计至关重要，需要建立电极配置与选择性肌肉募集之间的联系。

Method: 在4头猪上进行急性实验，使用多触点神经外膜袖套电极刺激上肢神经，记录诱发肌电反应，并进行神经组织学分析以可视化神经束组织。

Result: 初步结果显示肌肉激活模式与神经束解剖结构之间存在关联，为选择性肌肉募集提供了实验依据。

Conclusion: 该研究为设计更有效的神经调控和神经假体应用提供了基础，通过结合电生理和神经解剖分析来优化刺激策略。

Abstract: Understanding the relationship between nerve anatomy and the functional
outcomes of electrical stimulation is critical for optimizing neural interface
design. In this study, we conducted acute experiments on four pigs in which
epineural cuff electrodes with multiple contacts were placed around upper limb
nerves. A subset of electrical stimulation configurations -- previously
identified via computational study -- was applied, and the resulting evoked
electromyographic (EMG) responses were recorded from target muscles. Muscle
recruitment curves were extracted and analysed offline to quantify activation
patterns. Following the electrophysiological experiments, the stimulated nerves
were harvested and processed for histological analysis to visualize fascicular
organization and distribution. This work presents preliminary results from the
combined analysis of muscle activation profiles and fascicle anatomy in one
animal. Our findings aim to inform the design of stimulation strategies by
linking electrode configuration to selective muscle recruitment, ultimately
contributing to more effective neuromodulation and neuroprosthetic
applications.

</details>


### [8] [Physics-Constrained Inc-GAN for Tunnel Propagation Modeling from Sparse Line Measurements](https://arxiv.org/abs/2510.03019)
*Yang Zhou,Haochang Wu,Yunxi Mu,Hao Qin,Xinyue Zhang,Xingqi Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于Inception增强生成对抗网络(Inc-GAN)的方法，用于从高铁隧道中稀疏的测量信号线重建完整的电场分布，解决了传统方法计算复杂和处理稀疏数据困难的问题。


<details>
  <summary>Details</summary>
Motivation: 高铁隧道通信系统需要可靠的无线电波传播预测来确保运营安全，但传统仿真方法计算复杂度高，且无法有效处理实际铁路运营中收集的稀疏测量数据。

Method: 使用Inception增强的生成对抗网络(Inc-GAN)，通过基于Inception的生成器架构和渐进式训练策略，从单条测量信号线重建完整的电场分布。

Result: 数值仿真验证表明，Inc-GAN能够基于实际列车运营中收集的测量数据准确预测电场，计算效率相比传统方法显著提升。

Conclusion: 该方法为基于实际运营数据的铁路通信系统优化提供了一种新颖的解决方案。

Abstract: High-speed railway tunnel communication systems require reliable radio wave
propagation prediction to ensure operational safety. However, conventional
simulation methods face challenges of high computational complexity and
inability to effectively process sparse measurement data collected during
actual railway operations. This letter proposes an inception-enhanced
generative adversarial network (Inc-GAN) that can reconstruct complete electric
field distributions across tunnel cross-sections using sparse value lines
measured during actual train operations as input. This directly addresses
practical railway measurement constraints. Through an inception-based generator
architecture and progressive training strategy, the method achieves robust
reconstruction from single measurement signal lines to complete field
distributions. Numerical simulation validation demonstrates that Inc-GAN can
accurately predict electric fields based on measured data collected during
actual train operations, with significantly improved computational efficiency
compared to traditional methods, providing a novel solution for railway
communication system optimization based on real operational data.

</details>


### [9] [Compressed Multiband Sensing in FR3 Using Alternating Direction Method of Multipliers](https://arxiv.org/abs/2510.03055)
*Dexin Wang,Isha Jariwala,Ahmad Bazzi,Sundeep Rangan,Theodore S. Rappaport,Marwa Chafii*

Main category: eess.SP

TL;DR: 提出了一种基于ADMM的压缩多频段感知方法(ADMM-CMS)，用于6G集成感知与通信中的用户和散射体联合检测与定位，相比传统方法具有更高的空间分辨率和噪声抑制能力。


<details>
  <summary>Details</summary>
Motivation: 现有多频段感知方法受限于经典波束成形或计算复杂度高的方法，需要开发更高效的多频段感知框架来支持6G ISAC应用。

Method: 使用ADMM算法解决压缩多频段感知问题，开发自适应ADMM算法以适应噪声并确保自动停止收敛，利用上行链路QAM调制导频符号进行多频段感知。

Result: 相比Bartlett型波束成形，ADMM-CMS实现了34 dB的每天线发射功率增益；相比在7 GHz和10 GHz子频段分别进行压缩感知，延迟均方根误差分别降低了35%和38.1%。

Conclusion: ADMM-CMS是6G系统中FR3频段(7-24 GHz)实现集成感知与通信的高效使能技术。

Abstract: Joint detection and localization of users and scatterers in multipath-rich
channels on multiple bands is critical for integrated sensing and communication
(ISAC) in 6G. Existing multiband sensing methods are limited by classical
beamforming or computationally expensive approaches. This paper introduces
alternating direction method of multipliers (ADMM)-assisted compressed
multiband sensing (CMS), hereafter referred to as ADMM-CMS, which is a novel
framework for multiband sensing using uplink QAM-modulated pilot symbols. To
solve the CMS problem, we develop an adaptive ADMM algorithm that adjusts to
noise and ensures automatic stopping if converged. ADMM combines the
decomposability of dual ascent with the robustness of augmented Lagrangian
methods, making it suitable for large-scale structured optimization.
Simulations show that ADMM-CMS achieves higher spatial resolution and improved
denoising compared to Bartlett-type beamforming, yielding a 34 dB gain in
per-antenna transmit power for achieving a 0.9 successful recovery probability
(SRP). Moreover, compared to performing compressed sensing separately on the
constituent 7 GHz and 10 GHz sub-bands, ADMM-CMS achieves reductions in delay
root mean squared error of 35% and 38.1%, respectively, at -41 dBm per-antenna
transmit power, while also yielding improved SRP. Our findings demonstrate
ADMM-CMS as an efficient enabler of ISAC in frequency range 3 (FR3, 7-24 GHz)
for 6G systems.

</details>


### [10] [A Study of Neural Polar Decoders for Communication](https://arxiv.org/abs/2510.03069)
*Rom Hirsch,Ziv Aharoni,Henry D. Pfister,Haim H. Permuter*

Main category: eess.SP

TL;DR: 本文扩展了神经极化解码器（NPD）到实际通信系统，支持任意码长、高阶调制，在5G信道中相比标准极化解码器在BER、BLER和吞吐量方面表现更优，特别适用于低速率和短块配置。


<details>
  <summary>Details</summary>
Motivation: 将NPD从合成信道扩展到实际通信系统，满足实际系统需求，包括支持任意码长、高阶调制和不同信道条件。

Method: 将NPD适配到完整的OFDM和单载波通信系统，通过速率匹配支持任意码长，利用神经网络架构高效表示信道统计特性，直接处理有记忆信道。

Result: 在5G信道实验中，NPD在BER、BLER和吞吐量方面持续优于5G标准极化解码器，特别是在低速率和短块配置中表现显著。单载波系统中的NPD性能与OFDM相当但具有更低的PAPR。

Conclusion: NPD是一种高性能、无导频且鲁棒的解码解决方案，适用于实际通信系统。

Abstract: In this paper, we adapt and analyze Neural Polar Decoders (NPDs) for
end-to-end communication systems. While prior work demonstrated the
effectiveness of NPDs on synthetic channels, this study extends the NPD to
real-world communication systems. The NPD was adapted to complete OFDM and
single-carrier communication systems. To satisfy practical system requirements,
the NPD is extended to support any code length via rate matching, higher-order
modulations, and robustness across diverse channel conditions. The NPD operates
directly on channels with memory, exploiting their structure to achieve higher
data rates without requiring pilots and a cyclic prefix. Although NPD entails
higher computational complexity than the standard 5G polar decoder, its neural
network architecture enables an efficient representation of channel statistics,
resulting in manageable complexity suitable for practical systems. Experimental
results over 5G channels demonstrate that the NPD consistently outperforms the
5G polar decoder in terms of BER, BLER, and throughput. These improvements are
particularly significant for low-rate and short-block configurations, which are
prevalent in 5G control channels. Furthermore, NPDs applied to single-carrier
systems offer performance comparable to OFDM with lower PAPR, enabling
effective single-carrier transmission over 5G channels. These results position
the NPD as a high-performance, pilotless, and robust decoding solution.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [WEE-Therapy: A Mixture of Weak Encoders Framework for Psychological Counseling Dialogue Analysis](https://arxiv.org/abs/2510.02320)
*Yongqi Kang,Yong Zhao*

Main category: eess.AS

TL;DR: 提出WEE-Therapy模型，通过弱编码器集成机制增强音频语言模型对心理咨询对话的理解能力，在多个任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型依赖单一语音编码器，难以捕捉心理咨询领域特有的复杂情感和专业技巧等特征

Method: 采用弱编码器集成机制，在强大基础编码器基础上添加多个轻量级专业编码器，结合数据无关的稳定领域知识和数据依赖的动态专家选择策略

Result: 在情感识别、技巧分类、风险检测和摘要生成等任务上均取得显著性能提升，且参数开销极小

Conclusion: WEE-Therapy展现了在AI辅助临床分析中的强大潜力，为计算心理学发展提供了有效工具

Abstract: The advancement of computational psychology requires AI tools capable of
deeply understanding counseling dialogues. Existing audio language models
(AudioLLMs) often rely on single speech encoders pre-trained on general data,
struggling to capture domain-specific features like complex emotions and
professional techniques. To address this, we propose WEE-Therapy, a multi-task
AudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This
supplements a powerful base encoder with a pool of lightweight, specialized
encoders. A novel dual-routing strategy combines stable, data-independent
domain knowledge with dynamic, data-dependent expert selection. Evaluated on
emotion recognition, technique classification, risk detection, and
summarization, WEE-Therapy achieves significant performance gains across all
tasks with minimal parameter overhead, demonstrating strong potential for
AI-assisted clinical analysis.

</details>


### [12] [SpeechCT-CLIP: Distilling Text-Image Knowledge to Speech for Voice-Native Multimodal CT Analysis](https://arxiv.org/abs/2510.02322)
*Lukas Buess,Jan Geier,David Bani-Harouni,Chantal Pellegrini,Matthias Keicher,Paula Andrea Perez-Toro,Nassir Navab,Andreas Maier,Tomas Arias-Vergara*

Main category: eess.AS

TL;DR: SpeechCT-CLIP模型通过语音-3D CT对比学习，证明了语音可直接用于医学多模态预训练，无需依赖文本，为临床语音驱动诊断工具开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 临床工作流中语音交流占据核心地位（如放射科报告大多通过口述完成），但现有医学AI系统几乎完全依赖文本，存在语音与AI系统之间的鸿沟。

Method: 合成大规模语音放射报告数据集Speech-RATE，训练SpeechCT-CLIP对比模型对齐语音和3D CT扫描；采用知识蒸馏从预训练文本-图像CLIP模型向语音模型转移语义对齐能力。

Result: 零样本分类F1从0.623提升到0.705，恢复了88%的性能差距；在无需文本推理的情况下获得强检索结果。

Conclusion: 语音可作为文本的实用替代方案用于多模态预训练，为临床实践中的语音驱动诊断支持工具打开了大门。

Abstract: Spoken communication plays a central role in clinical workflows. In
radiology, for example, most reports are created through dictation. Yet, nearly
all medical AI systems rely exclusively on written text. In this work, we
address this gap by exploring the feasibility of learning visual-language
representations directly from spoken radiology reports. Specifically, we
synthesize a large-scale dataset (Speech-RATE) of spoken radiology reports and
train SpeechCT-CLIP, a contrastive model that aligns speech and 3D CT volumes
in a shared representation space. While naive speech-based models underperform
compared to text-trained counterparts, we show that knowledge distillation from
a pretrained text-image CLIP model effectively transfers semantic alignment
capabilities from text to speech, substantially narrowing this gap. Experiments
demonstrate improved zero-shot classification F1 from 0.623 to 0.705,
recovering 88% of the performance difference, and strong retrieval results
without requiring text at inference. These findings highlight speech as a
practical alternative to text in multimodal pretraining and open the door to
voice-driven diagnostic support tools in clinical practice.

</details>


### [13] [When Voice Matters: Evidence of Gender Disparity in Positional Bias of SpeechLLMs](https://arxiv.org/abs/2510.02398)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: eess.AS

TL;DR: 该论文首次对SpeechLLM基准测试中的多项选择题(MCQA)进行了token级概率评估，揭示了温度设置、提示设计和输入语音性别对性别偏见和位置偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 随着SpeechLLM对话AI系统的快速发展，需要对这些系统的公平性和偏见进行稳健的基准测试。目前此类基准测试通常依赖MCQA，但存在未考虑语音相关偏见的问题。

Method: 采用token级概率评估和基于响应的研究方法，分析模型温度、提示设计对性别和位置偏见的影响，以及输入语音性别如何影响这些偏见，并在两个性别偏见基准上进行验证。

Result: 结果显示文本领域的位置偏见问题在语音领域同样存在，且女性语音的影响比男性语音更强。这是首个在SpeechLLM性别偏见基准中分离位置偏见效应的研究。

Conclusion: 当前的MCQA基准测试未能考虑基于语音的偏见，需要替代策略来确保对所有用户的公平性。

Abstract: The rapid development of SpeechLLM-based conversational AI systems has
created a need for robustly benchmarking these efforts, including aspects of
fairness and bias. At present, such benchmarks typically rely on multiple
choice question answering (MCQA). In this paper, we present the first
token-level probabilistic evaluation and response-based study of several issues
affecting the use of MCQA in SpeechLLM benchmarking: 1) we examine how model
temperature and prompt design affect gender and positional bias on an MCQA
gender-bias benchmark; 2) we examine how these biases are affected by the
gender of the input voice; and 3) we study to what extent observed trends carry
over to a second gender-bias benchmark. Our results show that concerns about
positional bias from the text domain are equally valid in the speech domain. We
also find the effect to be stronger for female voices than for male voices. To
our knowledge, this is the first study to isolate positional bias effects in
SpeechLLM-based gender-bias benchmarks. We conclude that current MCQA
benchmarks do not account for speech-based bias and alternative strategies are
needed to ensure fairness towards all users.

</details>


### [14] [Multi-Source Position and Direction-of-Arrival Estimation Based on Euclidean Distance Matrices](https://arxiv.org/abs/2510.02556)
*Klaus Brümann,Simon Doclo*

Main category: eess.AS

TL;DR: 提出基于欧几里得距离矩阵(EDM)的多源位置和DOA估计方法，显著降低计算复杂度，相比传统SRP方法在精度和效率上都有提升。


<details>
  <summary>Details</summary>
Motivation: 传统SRP方法在三维场景中需要联合优化多个连续变量，计算成本高昂。本文旨在开发更高效的多源声源定位方法。

Method: 利用EDM和Gram矩阵特性，位置估计只需优化单个连续变量(源到参考麦克风的距离)，DOA估计完全消除连续变量优化需求。通过最小化基于Gram矩阵特征值的成本函数来确定最优TDOA估计。

Result: 实验结果显示，提出的EDM方法在不同声源和麦克风配置下，在双源位置和DOA估计精度方面始终优于SRP方法。

Conclusion: 基于EDM的方法显著降低了计算成本，在保持甚至提高定位精度的同时，为多源声源定位提供了更高效的解决方案。

Abstract: A popular method to estimate the positions or directions-of-arrival (DOAs) of
multiple sound sources using an array of microphones is based on
steered-response power (SRP) beamforming. For a three-dimensional scenario,
SRP-based methods need to jointly optimize three continuous variables for
position estimation or two continuous variables for DOA estimation, which can
be computationally expensive. In this paper, we propose novel methods for
multi-source position and DOA estimation by exploiting properties of Euclidean
distance matrices (EDMs) and their respective Gram matrices. In the proposed
multi-source position estimation method only a single continuous variable,
representing the distance between each source and a reference microphone, needs
to be optimized. For each source, the optimal continuous distance variable and
set of candidate time-difference of arrival (TDOA) estimates are determined by
minimizing a cost function that is defined using the eigenvalues of the Gram
matrix. The estimated relative source positions are then mapped to estimated
absolute source positions by solving an orthogonal Procrustes problem for each
source. The proposed multi-source DOA estimation method entirely eliminates the
need for continuous variable optimization by defining a relative coordinate
system per source such that one of its coordinate axes is aligned with the
respective source DOA. The optimal set of candidate TDOA estimates is
determined by minimizing a cost function that is defined using the eigenvalues
of a rank-reduced Gram matrix. The computational cost of the proposed EDM-based
methods is significantly reduced compared to the SRP-based methods.
Experimental results for different source and microphone configurations show
that the proposed EDM-based method consistently outperforms the SRP-based
method in terms of two-source position and DOA estimation accuracy.

</details>


### [15] [STSM-FiLM: A FiLM-Conditioned Neural Architecture for Time-Scale Modification of Speech](https://arxiv.org/abs/2510.02672)
*Dyah A. M. G. Wisnu,Ryandhimas E. Zezario,Stefano Rini,Fo-Rui Li,Yan-Tsung Peng,Hsin-Min Wang,Yu Tsao*

Main category: eess.AS

TL;DR: 提出了STSM-FILM，一种基于FiLM的完全神经时间尺度修改架构，通过条件化连续速度因子来改善语音时间尺度修改的质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法如WSOLA在非平稳或极端拉伸条件下会产生伪影，需要更鲁棒的神经方法来提高时间尺度修改的质量。

Method: 使用FiLM（特征级线性调制）来条件化模型对连续速度因子，通过WSOLA生成的输出监督网络，探索了四种编码器-解码器变体：STFT-HiFiGAN、WavLM-HiFiGAN、Whisper-HiFiGAN和EnCodec。

Result: STSM-FILM能够在广泛的时间尺度因子范围内产生感知一致性的输出，展示了FiLM条件化在改善神经TSM模型泛化性和灵活性方面的潜力。

Conclusion: 基于FiLM的条件化方法有潜力提高神经时间尺度修改模型的泛化能力和灵活性，为语音时间尺度修改提供了更高质量的解决方案。

Abstract: Time-Scale Modification (TSM) of speech aims to alter the playback rate of
audio without changing its pitch. While classical methods like Waveform
Similarity-based Overlap-Add (WSOLA) provide strong baselines, they often
introduce artifacts under non-stationary or extreme stretching conditions. We
propose STSM-FILM - a fully neural architecture that incorporates Feature-Wise
Linear Modulation (FiLM) to condition the model on a continuous speed factor.
By supervising the network using WSOLA-generated outputs, STSM-FILM learns to
mimic alignment and synthesis behaviors while benefiting from representations
learned through deep learning. We explore four encoder-decoder variants:
STFT-HiFiGAN, WavLM-HiFiGAN, Whisper-HiFiGAN, and EnCodec, and demonstrate that
STSM-FILM is capable of producing perceptually consistent outputs across a wide
range of time-scaling factors. Overall, our results demonstrate the potential
of FiLM-based conditioning to improve the generalization and flexibility of
neural TSM models.

</details>


### [16] [SongFormer: Scaling Music Structure Analysis with Heterogeneous Supervision](https://arxiv.org/abs/2510.02797)
*Chunbo Hao,Ruibin Yuan,Jixun Yao,Qixin Deng,Xinyi Bai,Wei Xue,Lei Xie*

Main category: eess.AS

TL;DR: SongFormer是一个可扩展的音乐结构分析框架，通过融合短窗口和长窗口自监督音频表示来捕捉细粒度和长程依赖，并使用学习源嵌入来处理部分、噪声和模式不匹配的标签。


<details>
  <summary>Details</summary>
Motivation: 音乐结构分析在音乐理解和可控生成中至关重要，但受限于小型、不一致的数据集。

Method: 融合短窗口和长窗口自监督音频表示，引入学习源嵌入来处理异构监督数据。

Result: 在SongFormBench上，SongFormer在严格边界检测（HR.5F）和功能标签准确性方面达到最先进水平，计算效率高，优于强基线和Gemini 2.5 Pro。

Conclusion: SongFormer为音乐结构分析提供了可扩展的解决方案，并发布了SongFormDB（最大MSA语料库）和SongFormBench（专家验证基准）。

Abstract: Music structure analysis (MSA) underpins music understanding and controllable
generation, yet progress has been limited by small, inconsistent corpora. We
present SongFormer, a scalable framework that learns from heterogeneous
supervision. SongFormer (i) fuses short- and long-window self-supervised audio
representations to capture both fine-grained and long-range dependencies, and
(ii) introduces a learned source embedding to enable training with partial,
noisy, and schema-mismatched labels. To support scaling and fair evaluation, we
release SongFormDB, the largest MSA corpus to date (over 10k tracks spanning
languages and genres), and SongFormBench, a 300-song expert-verified benchmark.
On SongFormBench, SongFormer sets a new state of the art in strict boundary
detection (HR.5F) and achieves the highest functional label accuracy, while
remaining computationally efficient; it surpasses strong baselines and Gemini
2.5 Pro on these metrics and remains competitive under relaxed tolerance
(HR3F). Code, datasets, and model are publicly available.

</details>


### [17] [Enhancing Photogrammetry Reconstruction For HRTF Synthesis Via A Graph Neural Network](https://arxiv.org/abs/2510.02813)
*Ludovic Pirard,Katarina C. Poole,Lorenzo Picinali*

Main category: eess.AS

TL;DR: 本研究使用图神经网络(GNN)和神经细分技术，将低分辨率摄影测量重建的头网格上采样为高分辨率网格，用于合成个性化HRTF，解决了传统HRTF获取方法设备要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统HRTF获取方法依赖专业设备和声学专业知识，可及性差；高分辨率3D建模成本高且设备有限；摄影测量技术分辨率不足，无法直接用于HRTF合成。

Method: 使用苹果摄影测量API处理SONICOM数据集重建低分辨率头网格，训练GNN将低分辨率网格上采样为高分辨率网格，采用Hausdorff距离损失函数，通过Mesh2HRTF合成HRTF。

Result: 在未见过的摄影测量数据上验证GNN性能，几何和声学评估显示合成HRTF与高分辨率3D扫描结果、声学测量HRTF以及KEMAR HRTF相比具有竞争力，并通过定位和空间掩蔽释放行为实验验证。

Conclusion: 基于GNN的摄影测量网格上采样方法能够有效生成高分辨率头网格，用于合成个性化HRTF，为低成本获取个性化HRTF提供了可行方案。

Abstract: Traditional Head-Related Transfer Functions (HRTFs) acquisition methods rely
on specialised equipment and acoustic expertise, posing accessibility
challenges. Alternatively, high-resolution 3D modelling offers a pathway to
numerically synthesise HRTFs using Boundary Elements Methods and others.
However, the high cost and limited availability of advanced 3D scanners
restrict their applicability. Photogrammetry has been proposed as a solution
for generating 3D head meshes, though its resolution limitations restrict its
application for HRTF synthesis. To address these limitations, this study
investigates the feasibility of using Graph Neural Networks (GNN) using neural
subdivision techniques for upsampling low-resolution
Photogrammetry-Reconstructed (PR) meshes into high-resolution meshes, which can
then be employed to synthesise individual HRTFs. Photogrammetry data from the
SONICOM dataset are processed using Apple Photogrammetry API to reconstruct
low-resolution head meshes. The dataset of paired low- and high-resolution
meshes is then used to train a GNN to upscale low-resolution inputs to
high-resolution outputs, using a Hausdorff Distance-based loss function. The
GNN's performance on unseen photogrammetry data is validated geometrically and
through synthesised HRTFs generated via Mesh2HRTF. Synthesised HRTFs are
evaluated against those computed from high-resolution 3D scans, to acoustically
measured HRTFs, and to the KEMAR HRTF using perceptually-relevant numerical
analyses as well as behavioural experiments, including localisation and Spatial
Release from Masking (SRM) tasks.

</details>


### [18] [CVSM: Contrastive Vocal Similarity Modeling](https://arxiv.org/abs/2510.03025)
*Christos Garoufis,Athanasia Zlatintsi,Petros Maragos*

Main category: eess.AS

TL;DR: CVSM是一种用于音乐信号表示学习的对比自监督方法，专注于音乐和声乐相似性建模，通过最大化包含相同人声的声乐片段和音乐混合物的相似性来学习表示。


<details>
  <summary>Details</summary>
Motivation: 利用大型未标记数据集开发自监督预训练方法，为音乐信号表示学习提供有效解决方案，特别关注声乐相似性建模。

Method: 采用对比学习框架，设计了两种方案：标签知情协议（利用艺术家身份信息采样对比对）和标签不可知方案（通过随机采样的声乐和伴奏片段创建人工混合物）。

Result: CVSM在音乐和声乐相似性建模方面表现优异，在多种下游任务和用户研究中优于多个基线方法。标签知情版本性能更一致，但标签不可知版本在艺术家识别和感知声乐相似性方面表现相当。

Conclusion: CVSM学习到的表示在音乐和声乐相似性建模中非常有效，标签知情预训练能获得更一致的性能，但标签不可知方法也能达到可比较的结果。

Abstract: The availability of large, unlabeled datasets across various domains has
contributed to the development of a plethora of methods that learn
representations for multiple target (downstream) tasks through self-supervised
pre-training. In this work, we introduce CVSM (Contrastive Vocal Similarity
Modeling), a contrastive self-supervised procedure for music signal
representation learning in the audio domain that can be utilized for musical
and vocal similarity modeling. Our method operates under a contrastive
framework, maximizing the similarity between vocal excerpts and musical
mixtures containing the same vocals; we devise both a label-informed protocol,
leveraging artist identity information to sample the contrastive pairs, and a
label-agnostic scheme, involving artificial mixture creation from randomly
sampled vocal and accompaniment excerpts, which are paired with vocals from the
same audio segment. We evaluate our proposed method in measuring vocal
similarity both objectively, through linear probing on a suite of appropriate
downstream tasks, and subjectively, via conducting a user study consisting of
pairwise comparisons between different models in a recommendation-by-query
setting. Our results indicate that the representations learned through CVSM are
effective in musical and vocal similarity modeling, outperforming numerous
baselines across both isolated vocals and complete musical mixtures. Moreover,
while the availability of artist identity labels during pre-training leads to
overall more consistent performance both in the evaluated downstream tasks and
the user study, a label-agnostic CVSM variant incorporating hybrid pre-training
with real and artificial mixtures achieves comparable performance to the
label-informed one in artist identification and perceived vocal similarity.

</details>


### [19] [Evaluation of preprocessing pipelines in the creation of in-the-wild TTS datasets](https://arxiv.org/abs/2510.03111)
*Matías Di Bernardo,Emmanuel Misley,Ignacio Correa,Mateo García Iacovelli,Simón Mellino,Gala Lucía Gonzalez Barrios*

Main category: eess.AS

TL;DR: 提出可复现的指标驱动方法，评估野外TTS语料库预处理流程，应用于阿根廷西班牙语数据集，比较24种配置，发现降噪加宽松过滤提供最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 为低资源环境开发快速、低成本的预处理流程评估方法，避免为每个子集训练TTS模型的高昂成本。

Method: 应用定制低成本流程处理阿根廷西班牙语野外数据集，比较24种配置（不同降噪和质量过滤变体），使用客观指标（PESQ、SI-SDR、SNR）、声学描述符（T30、C50）和语音保存指标（F0-STD、MCD）进行评估。

Result: 结果揭示了数据集大小、信号质量和语音保存之间的权衡，降噪变体配合宽松过滤在测试中提供最佳整体折衷方案。

Conclusion: 所提方法允许在不训练TTS模型的情况下选择预处理配置，加速并降低了低资源环境下预处理开发成本。

Abstract: This work introduces a reproducible, metric-driven methodology to evaluate
preprocessing pipelines for in-the-wild TTS corpora generation. We apply a
custom low-cost pipeline to the first in-the-wild Argentine Spanish collection
and compare 24 pipeline configurations combining different denoising and
quality filtering variants. Evaluation relies on complementary objective
measures (PESQ, SI-SDR, SNR), acoustic descriptors (T30, C50), and
speech-preservation metrics (F0-STD, MCD). Results expose trade-offs between
dataset size, signal quality, and voice preservation; where denoising variants
with permissive filtering provide the best overall compromise for our testbed.
The proposed methodology allows selecting pipeline configurations without
training TTS models for each subset, accelerating and reducing the cost of
preprocessing development for low-resource settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [20] [Accelerated Convolutive Transfer Function-Based Multichannel NMF Using Iterative Source Steering](https://arxiv.org/abs/2510.02382)
*Xuemai Xie,Xianrui Wang,Liyuan Zhang,Yichen Yang,Shoji Makino*

Main category: cs.SD

TL;DR: 提出了一种基于迭代源转向(ISS)的高效CTF-MNMF变体，在保持或提升分离性能的同时显著降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 传统CTF-MNMF方法在高度混响环境中表现出色，但其迭代投影更新规则需要为每个源进行矩阵求逆，计算成本高昂，限制了实际部署

Method: 将矩阵求逆自由的迭代源转向(ISS)更新规则集成到CTF-MNMF框架中，替代原有的迭代投影更新规则

Result: 实验结果表明，所提方法在分离性能上与原CTF-MNMF相当或更优，同时显著降低了计算复杂度

Conclusion: ISS-CTF-MNMF是一种高效实用的盲源分离方法，解决了传统方法计算复杂度过高的问题

Abstract: Among numerous blind source separation (BSS) methods, convolutive transfer
function-based multichannel non-negative matrix factorization (CTF-MNMF) has
demonstrated strong performance in highly reverberant environments by modeling
multi-frame correlations of delayed source signals. However, its practical
deployment is hindered by the high computational cost associated with the
iterative projection (IP) update rule, which requires matrix inversion for each
source. To address this issue, we propose an efficient variant of CTF-MNMF that
integrates iterative source steering (ISS), a matrix inversion-free update rule
for separation filters. Experimental results show that the proposed method
achieves comparable or superior separation performance to the original
CTF-MNMF, while significantly reducing the computational complexity.

</details>


### [21] [Linear RNNs for autoregressive generation of long music samples](https://arxiv.org/abs/2510.02401)
*Konrad Szewczyk,Daniel Gallo Fernández,James Townsend*

Main category: cs.SD

TL;DR: 本文提出了HarmonicRNN模型，通过改进线性RNN架构和利用上下文并行性，在原始音频波形建模中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 直接以自回归方式生成音频波形具有挑战性，因为原始序列长度大且存在多时间尺度的重要结构。传统方法如RNN、因果卷积和自注意力机制在此任务上效果有限。

Method: 使用深度状态空间模型（线性RNN），研究不同架构选择的影响，并采用上下文并行性来训练长达1分钟（100万个token）的序列。

Result: HarmonicRNN模型在小规模数据集上获得了最先进的似然度和感知指标。

Conclusion: 线性RNN在原始音频建模中具有显著潜力，通过适当的架构改进和训练技术可以取得优异性能。

Abstract: Directly learning to generate audio waveforms in an autoregressive manner is
a challenging task, due to the length of the raw sequences and the existence of
important structure on many different timescales. Traditional approaches based
on recurrent neural networks, as well as causal convolutions and
self-attention, have only had limited success on this task. However, recent
work has shown that deep state space models, also referred to as linear RNNs,
can be highly efficient in this context. In this work, we push the boundaries
of linear RNNs applied to raw audio modeling, investigating the effects of
different architectural choices and using context-parallelism to enable
training on sequences up to one minute (1M tokens) in length. We present a
model, HarmonicRNN, which attains state of the art log-likelihoods and
perceptual metrics on small-scale datasets.

</details>


### [22] [Latent Multi-view Learning for Robust Environmental Sound Representations](https://arxiv.org/abs/2510.02500)
*Sivan Sing,Julia Wilkins,Magdalena Fuentes,Juan Pablo Bello*

Main category: cs.SD

TL;DR: 提出一个多视图学习框架，将对比学习原则集成到生成式管道中，用于环境声音表示学习，通过编码压缩音频潜在空间到视图特定和视图公共子空间，结合对比学习和重建目标。


<details>
  <summary>Details</summary>
Motivation: 虽然自监督学习方法在环境声音表示学习方面取得了进展，但如何在一个统一框架中让对比学习和生成式方法互补仍然研究不足。

Method: 编码压缩音频潜在空间到视图特定和视图公共子空间，使用两个自监督目标：对比学习用于子空间间的目标信息流，重建用于整体信息保留。

Result: 在城市声音传感器网络数据集上的评估显示，在声音源和传感器分类任务中，相比传统SSL技术有更好的下游性能。

Conclusion: 该方法能够在一个结构化潜在空间中解耦环境声音属性，并在不同训练配置下展现潜力。

Abstract: Self-supervised learning (SSL) approaches, such as contrastive and generative
methods, have advanced environmental sound representation learning using
unlabeled data. However, how these approaches can complement each other within
a unified framework remains relatively underexplored. In this work, we propose
a multi-view learning framework that integrates contrastive principles into a
generative pipeline to capture sound source and device information. Our method
encodes compressed audio latents into view-specific and view-common subspaces,
guided by two self-supervised objectives: contrastive learning for targeted
information flow between subspaces, and reconstruction for overall information
preservation. We evaluate our method on an urban sound sensor network dataset
for sound source and sensor classification, demonstrating improved downstream
performance over traditional SSL techniques. Additionally, we investigate the
model's potential to disentangle environmental sound attributes within the
structured latent space under varied training configurations.

</details>


### [23] [TART: A Comprehensive Tool for Technique-Aware Audio-to-Tab Guitar Transcription](https://arxiv.org/abs/2510.02597)
*Akshaj Gupta,Andrea Guzman,Anagha Badriprasad,Hwi Joo Park,Upasana Puranik,Robin Netzorg,Jiachen Lian,Gopala Krishna Anumanchipalli*

Main category: cs.SD

TL;DR: 提出一个四阶段端到端管道，直接从音频生成详细的吉他指法谱，解决现有吉他自动转录系统在表达技巧检测和指法分配方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 吉他自动转录面临表达技巧（如滑音、弯音、打击音）检测困难，以及音符到弦和品位的错误映射问题，现有系统在真实世界吉他录音上的泛化能力有限。

Method: 四阶段管道：(1) 通过适配吉他数据集的钢琴转录模型进行音频到MIDI音高转换；(2) 基于MLP的表达技巧分类；(3) 基于Transformer的弦和品位分配；(4) 基于LSTM的指法谱生成。

Result: 据我们所知，这是首个能够从吉他音频生成包含准确指法和表达标签的详细指法谱的框架。

Conclusion: 该端到端管道成功解决了吉他转录中的关键挑战，能够生成包含表达技巧和准确指法信息的详细吉他指法谱。

Abstract: Automatic Music Transcription (AMT) has advanced significantly for the piano,
but transcription for the guitar remains limited due to several key challenges.
Existing systems fail to detect and annotate expressive techniques (e.g.,
slides, bends, percussive hits) and incorrectly map notes to the wrong string
and fret combination in the generated tablature. Furthermore, prior models are
typically trained on small, isolated datasets, limiting their generalizability
to real-world guitar recordings. To overcome these limitations, we propose a
four-stage end-to-end pipeline that produces detailed guitar tablature directly
from audio. Our system consists of (1) Audio-to-MIDI pitch conversion through a
piano transcription model adapted to guitar datasets; (2) MLP-based expressive
technique classification; (3) Transformer-based string and fret assignment; and
(4) LSTM-based tablature generation. To the best of our knowledge, this
framework is the first to generate detailed tablature with accurate fingerings
and expressive labels from guitar audio.

</details>


### [24] [Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech](https://arxiv.org/abs/2510.02848)
*Hieu-Nghia Huynh-Nguyen,Huynh Nguyen Dang,Ngoc-Son Nguyen,Van Nguyen*

Main category: cs.SD

TL;DR: 提出Flamed-TTS，一种新颖的零样本文本转语音框架，通过改进流匹配训练范式并结合离散和连续语音表示，实现低计算成本、低延迟和高保真度的语音合成，同时增强时间多样性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本TTS方法存在合成不可靠、推理速度慢、计算开销大等问题，且时间多样性研究不足，影响合成语音的自然度。

Method: 重新制定流匹配训练范式，结合离散和连续语音表示，对应不同的语音属性，以提升合成质量和效率。

Result: 实验显示Flamed-TTS在可懂度、自然度、说话人相似度、声学特征保持和动态节奏方面优于现有最佳模型，WER达4%，同时保持低延迟和高保真度。

Conclusion: Flamed-TTS通过创新的训练方法和表示融合，有效解决了零样本TTS的关键挑战，实现了高质量、高效率的语音合成。

Abstract: Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling
models to synthesize speech from text using short, limited-context prompts.
These prompts serve as voice exemplars, allowing the model to mimic speaker
identity, prosody, and other traits without extensive speaker-specific data.
Although recent approaches incorporating language models, diffusion, and flow
matching have proven their effectiveness in zero-shot TTS, they still encounter
challenges such as unreliable synthesis caused by token repetition or
unexpected content transfer, along with slow inference and substantial
computational overhead. Moreover, temporal diversity-crucial for enhancing the
naturalness of synthesized speech-remains largely underexplored. To address
these challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that
emphasizes low computational cost, low latency, and high speech fidelity
alongside rich temporal diversity. To achieve this, we reformulate the flow
matching training paradigm and incorporate both discrete and continuous
representations corresponding to different attributes of speech. Experimental
results demonstrate that Flamed-TTS surpasses state-of-the-art models in terms
of intelligibility, naturalness, speaker similarity, acoustic characteristics
preservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4%
compared to the leading zero-shot TTS baselines, while maintaining low latency
in inference and high fidelity in generated speech. Code and audio samples are
available at our demo page https://flamed-tts.github.io.

</details>


### [25] [Forensic Similarity for Speech Deepfakes](https://arxiv.org/abs/2510.02864)
*Viola Negroni,Davide Salvi,Daniele Ugo Leonzio,Paolo Bestagini,Stefano Tubaro*

Main category: cs.SD

TL;DR: 提出了一种基于深度学习的语音深度伪造检测方法，通过比较两个音频片段是否包含相同的法证痕迹来判断其来源，无需在训练时了解具体伪造痕迹。


<details>
  <summary>Details</summary>
Motivation: 受到图像域法证相似性工作的启发，旨在开发能够泛化到未知法证痕迹的音频伪造检测方法，解决现有方法对特定伪造痕迹依赖性强的问题。

Method: 采用两阶段深度学习系统：基于语音深度伪造检测器的特征提取器和浅层相似性网络，将音频对映射到相似性分数，判断是否包含相同法证痕迹。

Result: 实验表明该方法能有效进行来源验证，识别两个样本是否来自同一生成模型，并在拼接检测等任务中表现良好，对未见过的法证痕迹具有强泛化能力。

Conclusion: 该方法在数字音频取证中具有灵活性和实用价值，能够泛化到广泛的法证痕迹，包括训练时未见过的类型。

Abstract: In this paper, we introduce a digital audio forensics approach called
Forensic Similarity for Speech Deepfakes, which determines whether two audio
segments contain the same forensic traces or not. Our work is inspired by prior
work in the image domain on forensic similarity, which proved strong
generalization capabilities against unknown forensic traces, without requiring
prior knowledge of them at training time. To achieve this in the audio setting,
we propose a two-part deep-learning system composed of a feature extractor
based on a speech deepfake detector backbone and a shallow neural network,
referred to as the similarity network. This system maps pairs of audio segments
to a score indicating whether they contain the same or different forensic
traces. We evaluate the system on the emerging task of source verification,
highlighting its ability to identify whether two samples originate from the
same generative model. Additionally, we assess its applicability to splicing
detection as a complementary use case. Experiments show that the method
generalizes to a wide range of forensic traces, including previously unseen
ones, illustrating its flexibility and practical value in digital audio
forensics.

</details>


### [26] [WavInWav: Time-domain Speech Hiding via Invertible Neural Network](https://arxiv.org/abs/2510.02915)
*Wei Fan,Kejiang Chen,Xiangkun Wang,Weiming Zhang,Nenghai Yu*

Main category: cs.SD

TL;DR: 提出了一种基于流可逆神经网络的音频隐写方法，通过建立载体音频、隐写音频和秘密音频之间的直接联系，结合时频损失和加密技术，显著提升了秘密音频的恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有音频隐写方法在恢复秘密音频时质量不佳，主要由于对时频关系建模的固有局限性，需要改进恢复过程的可逆性。

Method: 使用基于流的可逆神经网络建立载体音频、隐写音频和秘密音频之间的直接映射，在时域信号上实现时频损失，并加入加密技术保护隐藏数据。

Result: 在VCTK和LibriSpeech数据集上的实验表明，该方法在主客观指标上均优于先前方法，且对各种噪声具有鲁棒性。

Conclusion: 该方法在目标安全通信场景中具有实用价值，通过改进可逆性和时频建模解决了秘密音频恢复质量问题。

Abstract: Data hiding is essential for secure communication across digital media, and
recent advances in Deep Neural Networks (DNNs) provide enhanced methods for
embedding secret information effectively. However, previous audio hiding
methods often result in unsatisfactory quality when recovering secret audio,
due to their inherent limitations in the modeling of time-frequency
relationships. In this paper, we explore these limitations and introduce a new
DNN-based approach. We use a flow-based invertible neural network to establish
a direct link between stego audio, cover audio, and secret audio, enhancing the
reversibility of embedding and extracting messages. To address common issues
from time-frequency transformations that degrade secret audio quality during
recovery, we implement a time-frequency loss on the time-domain signal. This
approach not only retains the benefits of time-frequency constraints but also
enhances the reversibility of message recovery, which is vital for practical
applications. We also add an encryption technique to protect the hidden data
from unauthorized access. Experimental results on the VCTK and LibriSpeech
datasets demonstrate that our method outperforms previous approaches in terms
of subjective and objective metrics and exhibits robustness to various types of
noise, suggesting its utility in targeted secure communication scenarios.

</details>


### [27] [SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos](https://arxiv.org/abs/2510.02916)
*Amir Dellali,Luca A. Lanzendörfer,Florian Grötschla,Roger Wattenhofer*

Main category: cs.SD

TL;DR: SALSA-V是一个多模态视频到音频生成模型，能够从无声视频内容合成高度同步、高保真的长音频。通过掩码扩散目标和快捷损失，实现快速高质量音频生成，在视听对齐方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决从无声视频生成同步高保真长音频的挑战，满足实时应用需求，并扩展专业音频合成任务的应用范围。

Method: 采用掩码扩散目标实现音频条件生成和无约束长度音频序列合成，集成快捷损失实现快速生成（仅需8个采样步骤），使用随机掩码训练匹配参考音频频谱特征。

Result: 在定量评估和人类听力研究中，SALSA-V在视听对齐和与视频内容同步方面显著优于现有最先进方法，能够快速生成高质量音频。

Conclusion: SALSA-V为视频到音频生成提供了高效解决方案，支持长音频合成和快速生成，适用于Foley生成和声音设计等专业音频合成任务。

Abstract: We propose SALSA-V, a multimodal video-to-audio generation model capable of
synthesizing highly synchronized, high-fidelity long-form audio from silent
video content. Our approach introduces a masked diffusion objective, enabling
audio-conditioned generation and the seamless synthesis of audio sequences of
unconstrained length. Additionally, by integrating a shortcut loss into our
training process, we achieve rapid generation of high-quality audio samples in
as few as eight sampling steps, paving the way for near-real-time applications
without requiring dedicated fine-tuning or retraining. We demonstrate that
SALSA-V significantly outperforms existing state-of-the-art methods in both
audiovisual alignment and synchronization with video content in quantitative
evaluation and a human listening study. Furthermore, our use of random masking
during training enables our model to match spectral characteristics of
reference audio samples, broadening its applicability to professional audio
synthesis tasks such as Foley generation and sound design.

</details>


### [28] [AudioToolAgent: An Agentic Framework for Audio-Language Models](https://arxiv.org/abs/2510.02995)
*Gijs Wijngaard,Elia Formisano,Michel Dumontier*

Main category: cs.SD

TL;DR: AudioToolAgent是一个通过中央LLM代理协调音频语言模型的框架，用于音频问答和语音转文本任务，实现了最先进的准确性，且无需数据和训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有的音频语言模型在多步推理和工具调用方面存在不足，需要一种能够协调多个音频工具进行复杂音频理解任务的框架。

Method: 通过中央LLM代理访问工具适配器，选择工具、提出后续问题并比较输出进行验证，采用模块化设计集成新工具。

Result: 在MMAU、MMAR和MMAU-Pro数据集上达到最先进准确率：MMAU 74.10%、MMAR 68.80%、MMAU-Pro 57.96%，通过蒙特卡洛采样识别了有效的代理-工具组合。

Conclusion: AudioToolAgent框架有效解决了音频语言模型在多步推理和工具调用方面的局限性，实现了高性能的音频理解，且具有模块化和无需训练的优势。

Abstract: Large Audio-Language Models (LALMs) perform well on audio understanding tasks
but lack multi-step reasoning and tool-calling found in recent Large Language
Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates
audio-language models as tools via a central LLM agent that accesses tool
adapters for audio question answering and speech-to-text. The agent selects
tools, asks follow-up questions, and compares outputs for verification.
Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to
74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling
for shapley values across 374 configurations identifies effective agent-tool
combinations. The modular design allows integration of new tools and eliminates
the use of data and training costs. Code and reproduction materials are
available at: github.com/GLJS/AudioToolAgent

</details>

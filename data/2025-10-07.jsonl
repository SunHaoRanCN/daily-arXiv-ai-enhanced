{"id": "2510.03336", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03336", "abs": "https://arxiv.org/abs/2510.03336", "authors": ["Adharsha Sam Edwin Sam Devahi", "Sohail Singh Sangha", "Prachee Priyadarshinee", "Jithin Thilakan", "Ivan Fu Xing Tan", "Christopher Johann Clarke", "Sou Ka Lon", "Balamurali B T", "Yow Wei Quin", "Chen Jer-Ming"], "title": "Linguistic and Audio Embedding-Based Machine Learning for Alzheimer's Dementia and Mild Cognitive Impairment Detection: Insights from the PROCESS Challenge", "comment": null, "summary": "Early detection of Alzheimer's Dementia (AD) and Mild Cognitive Impairment\n(MCI) is critical for timely intervention, yet current diagnostic approaches\nremain resource-intensive and invasive. Speech, encompassing both acoustic and\nlinguistic dimensions, offers a promising non-invasive biomarker for cognitive\ndecline. In this study, we present a machine learning framework for the PROCESS\nChallenge, leveraging both audio embeddings and linguistic features derived\nfrom spontaneous speech recordings. Audio representations were extracted using\nWhisper embeddings from the Cookie Theft description task, while linguistic\nfeatures-spanning pronoun usage, syntactic complexity, filler words, and clause\nstructure-were obtained from transcriptions across Semantic Fluency, Phonemic\nFluency, and Cookie Theft picture description. Classification models aimed to\ndistinguish between Healthy Controls (HC), MCI, and AD participants, while\nregression models predicted Mini-Mental State Examination (MMSE) scores.\nResults demonstrated that voted ensemble models trained on concatenated\nlinguistic features achieved the best classification performance (F1 = 0.497),\nwhile Whisper embedding-based ensemble regressors yielded the lowest MMSE\nprediction error (RMSE = 2.843). Comparative evaluation within the PROCESS\nChallenge placed our models among the top submissions in regression task, and\nmid-range for classification, highlighting the complementary strengths of\nlinguistic and audio embeddings. These findings reinforce the potential of\nmultimodal speech-based approaches for scalable, non-invasive cognitive\nassessment and underline the importance of integrating task-specific linguistic\nand acoustic markers in dementia detection."}
{"id": "2510.03387", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03387", "abs": "https://arxiv.org/abs/2510.03387", "authors": ["Kirill Trapeznikov", "Paul Cummer", "Pranay Pherwani", "Jai Aslam", "Michael S. Davinroy", "Peter Bautista", "Laura Cassani", "Matthew Stamm", "Jill Crisman"], "title": "Audio Forensics Evaluation (SAFE) Challenge", "comment": null, "summary": "The increasing realism of synthetic speech generated by advanced\ntext-to-speech (TTS) models, coupled with post-processing and laundering\ntechniques, presents a significant challenge for audio forensic detection. In\nthis paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation)\nChallenge, a fully blind evaluation framework designed to benchmark detection\nmodels across progressively harder scenarios: raw synthetic speech, processed\naudio (e.g., compression, resampling), and laundered audio intended to evade\nforensic analysis. The SAFE challenge consisted of a total of 90 hours of audio\nand 21,000 audio samples split across 21 different real sources and 17\ndifferent TTS models and 3 tasks. We present the challenge, evaluation design\nand tasks, dataset details, and initial insights into the strengths and\nlimitations of current approaches, offering a foundation for advancing\nsynthetic audio detection research. More information is available at\n\\href{https://stresearch.github.io/SAFE/}{https://stresearch.github.io/SAFE/}."}
{"id": "2510.03728", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03728", "abs": "https://arxiv.org/abs/2510.03728", "authors": ["Kuang Yuan", "Yang Gao", "Xilin Li", "Xinhao Mei", "Syavosh Zadissa", "Tarun Pruthi", "Saeed Bagheri Sereshki"], "title": "Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation", "comment": null, "summary": "Acoustic scene classification (ASC) models on edge devices typically operate\nunder fixed class assumptions, lacking the transferability needed for\nreal-world applications that require adaptation to new or refined acoustic\ncategories. We propose ContrastASC, which learns generalizable acoustic scene\nrepresentations by structuring the embedding space to preserve semantic\nrelationships between scenes, enabling adaptation to unseen categories without\nretraining. Our approach combines supervised contrastive fine-tuning of\npre-trained models with contrastive representation distillation to transfer\nthis structured knowledge to compact student models. Our evaluation shows that\nContrastASC demonstrates improved few-shot adaptation to unseen categories\nwhile maintaining strong closed-set performance."}
{"id": "2510.03735", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03735", "abs": "https://arxiv.org/abs/2510.03735", "authors": ["Benoit Ginies", "Xiaoyu Bie", "Olivier Fercoq", "Gaël Richard"], "title": "Soft Disentanglement in Frequency Bands for Neural Audio Codecs", "comment": null, "summary": "In neural-based audio feature extraction, ensuring that representations\ncapture disentangled information is crucial for model interpretability.\nHowever, existing disentanglement methods often rely on assumptions that are\nhighly dependent on data characteristics or specific tasks. In this work, we\nintroduce a generalizable approach for learning disentangled features within a\nneural architecture. Our method applies spectral decomposition to time-domain\nsignals, followed by a multi-branch audio codec that operates on the decomposed\ncomponents. Empirical evaluations demonstrate that our approach achieves better\nreconstruction and perceptual performance compared to a state-of-the-art\nbaseline while also offering potential advantages for inpainting tasks."}
{"id": "2510.03630", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03630", "abs": "https://arxiv.org/abs/2510.03630", "authors": ["Xiluo He", "Alexander Polok", "Jesús Villalba", "Thomas Thebaud", "Matthew Maciejewski"], "title": "Scaling Multi-Talker ASR with Speaker-Agnostic Activity Streams", "comment": null, "summary": "An increasingly common training paradigm for multi-talker automatic speech\nrecognition (ASR) is to use speaker activity signals to adapt single-speaker\nASR models for overlapping speech. Although effective, these systems require\nrunning the ASR model once per speaker, resulting in inference costs that scale\nwith the number of speakers and limiting their practicality. In this work, we\npropose a method that decouples the inference cost of activity-conditioned ASR\nsystems from the number of speakers by converting speaker-specific activity\noutputs into two speaker-agnostic streams. A central challenge is that\nna\\\"ively merging speaker activities into streams significantly degrades\nrecognition, since pretrained ASR models assume contiguous, single-speaker\ninputs. To address this, we design new heuristics aimed at preserving\nconversational continuity and maintaining compatibility with existing systems.\nWe show that our approach is compatible with Diarization-Conditioned Whisper\n(DiCoW) to greatly reduce runtimes on the AMI and ICSI meeting datasets while\nretaining competitive performance."}
{"id": "2510.03516", "categories": ["eess.SP", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03516", "abs": "https://arxiv.org/abs/2510.03516", "authors": ["Boyang Chen", "Mohd Tasleem Khan", "George Goussetis", "Mathini Sellathurai", "Yuan Ding", "João F. C. Mota"], "title": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy."}
{"id": "2510.03741", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.03741", "abs": "https://arxiv.org/abs/2510.03741", "authors": ["Benoît Giniès", "Xiaoyu Bie", "Olivier Fercoq", "Gaël Richard"], "title": "Désentrelacement Fréquentiel Doux pour les Codecs Audio Neuronaux", "comment": "in French language, Groupe de Recherche et d'Etudes du Traitement du\n  Signal et des Images (GRETSI 2025), Aug 2025, Strasbourg, France", "summary": "While neural-based models have led to significant advancements in audio\nfeature extraction, the interpretability of the learned representations remains\na critical challenge. To address this, disentanglement techniques have been\nintegrated into discrete neural audio codecs to impose structure on the\nextracted tokens. However, these approaches often exhibit strong dependencies\non specific datasets or task formulations. In this work, we propose a\ndisentangled neural audio codec that leverages spectral decomposition of\ntime-domain signals to enhance representation interpretability. Experimental\nevaluations demonstrate that our method surpasses a state-of-the-art baseline\nin both reconstruction fidelity and perceptual quality."}
{"id": "2510.03723", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03723", "abs": "https://arxiv.org/abs/2510.03723", "authors": ["Martin Kocour", "Martin Karafiat", "Alexander Polok", "Dominik Klement", "Lukáš Burget", "Jan Černocký"], "title": "Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition", "comment": null, "summary": "We propose a speaker-attributed (SA) Whisper-based model for multi-talker\nspeech recognition that combines target-speaker modeling with serialized output\ntraining (SOT). Our approach leverages a Diarization-Conditioned Whisper\n(DiCoW) encoder to extract target-speaker embeddings, which are concatenated\ninto a single representation and passed to a shared decoder. This enables the\nmodel to transcribe overlapping speech as a serialized output stream with\nspeaker tags and timestamps. In contrast to target-speaker ASR systems such as\nDiCoW, which decode each speaker separately, our approach performs joint\ndecoding, allowing the decoder to condition on the context of all speakers\nsimultaneously. Experiments show that the model outperforms existing SOT-based\napproaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix)."}
{"id": "2510.03594", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03594", "abs": "https://arxiv.org/abs/2510.03594", "authors": ["Tuo Wu", "Kwai-Man Luk", "Jie Tang", "Kai-Kit Wong", "Jianchao Zheng", "Baiyang Liu", "David Morales-Jimenez", "Maged Elkashlan", "Kin-Fai Tong", "Chan-Byoung Chae", "Fumiyuki Adachi", "George K. Karagiannidis"], "title": "Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems", "comment": "13 pages", "summary": "Fluid antenna systems (FAS) are emerging as a transformative enabler for\nsixth-generation (6G) wireless communications, providing unprecedented spatial\ndiversity through dynamic reconfiguration of antenna ports. However, the\ninherent spatial correlation among ports poses significant challenges for\naccurate analysis. Conventional models such as Jakes are analytically\nintractable, while oversimplified constant-correlation models fail to capture\nthe true behavior. In this work, we address these challenges by applying the\nvariable block-correlation model (VBCM) -- originally proposed by\nRam\\'{i}rez-Espinosa \\textit{et al.} in 2024 -- to FAS security analysis, and\nby developing comprehensive optimization methods to enhance analytical\naccuracy. We derive new closed-form expressions for average secrecy capacity\n(ASC) and secrecy outage probability (SOP), demonstrating that the VBCM\nframework achieves simulation-aligned accuracy, with relative errors\nconsistently below $5\\%$ (compared to $10$--$15\\%$ for constant-correlation\nmodels). To maximize ASC, we further design two algorithms: a grid search (GS)\nmethod and a gradient descent (GD) method. Numerical results reveal that the\nVBCM-based approach not only provides reliable insights into FAS security\nperformance, but also yields substantial gains -- ASC improvements exceeding\n$120\\%$ in high-threat scenarios and $18$--$19\\%$ performance enhancements for\ncompact antenna configurations. These findings underscore the practical value\nof integrating VBCM into FAS security analysis and optimization, establishing\nit as a powerful tool for advancing 6G communication systems."}
{"id": "2510.04157", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04157", "abs": "https://arxiv.org/abs/2510.04157", "authors": ["Efrayim Yanir", "David Burshtein", "Sharon Gannot"], "title": "GDiffuSE: Diffusion-based speech enhancement with noise model guidance", "comment": null, "summary": "This paper introduces a novel speech enhancement (SE) approach based on a\ndenoising diffusion probabilistic model (DDPM), termed Guided diffusion for\nspeech enhancement (GDiffuSE). In contrast to conventional methods that\ndirectly map noisy speech to clean speech, our method employs a lightweight\nhelper model to estimate the noise distribution, which is then incorporated\ninto the diffusion denoising process via a guidance mechanism. This design\nimproves robustness by enabling seamless adaptation to unseen noise types and\nby leveraging large-scale DDPMs originally trained for speech generation in the\ncontext of SE. We evaluate our approach on noisy signals obtained by adding\nnoise samples from the BBC sound effects database to LibriSpeech utterances,\nshowing consistent improvements over state-of-the-art baselines under\nmismatched noise conditions. Examples are available at our project webpage."}
{"id": "2510.03825", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03825", "abs": "https://arxiv.org/abs/2510.03825", "authors": ["Pavel Rajmic", "Jiří Schimmel", "Šimon Cieslar"], "title": "A MATLAB toolbox for Computation of Speech Transmission Index (STI)", "comment": null, "summary": "The speech transmission index (STI) is a popular simple metric for the\nprediction of speech intelligibility when speech is passed through a\ntransmission channel. Computation of STI from acoustic measurements is\ndescribed in the IEC 60268-16:2020 standard. Though, reliable implementations\nof STI are not publicly accessible and are frequently limited to the use with a\nproprietary measurement hardware. We present a Matlab STI implementation of\nboth the direct and indirect approaches according to the standard, including\nthe shortened STIPA protocol. The suggested implementation meets prescribed\nrequirements, as evidenced by tests on reference signals. Additionally, we\nconducted a verification measurement in comparison to a commercial measurement\ndevice. Our software comes with open source code."}
{"id": "2510.03626", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03626", "abs": "https://arxiv.org/abs/2510.03626", "authors": ["Jun Tong"], "title": "On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models", "comment": "This paper was presented at 2025 IEEE International Conference on\n  Communications Workshops (ICC Workshops)", "summary": "Significant studies on communications over doubly selective channels have\nutilized on-grid DD channel models, which are previously investigated in\nBello's seminar paper in 1963. The DD grid is typically specified by the\nbandwidth and time duration of the transmission frames. However, the physical\nchannels are determined by the propagation environments and they are typically\noff-grid. Hence, there is often a gap between an actual physical channel and\nthe on-grid model. This paper revisits the on-grid modeling of practical\nphysical channels. We study the associated on-grid DD-domain representations\nfor continuous-time, doubly selective channels with off-grid delay and Doppler\nshifts, accounting for practical time/frequency-domain windowing at the\ntransceivers. The universal models obtained are applicable under the mild\nassumption that the windows have finite supports, and they extend Bello's\nclassical results to account for more general windows. We also discuss the\nfeatures and implications of the equivalent on-grid models."}
{"id": "2510.04251", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04251", "abs": "https://arxiv.org/abs/2510.04251", "authors": ["Zhao Ren", "Rathi Adarshi Rammohan", "Kevin Scheck", "Tanja Schultz"], "title": "Machine Unlearning in Speech Emotion Recognition via Forget Set Alone", "comment": "Submitted to ICASSP 2026", "summary": "Speech emotion recognition aims to identify emotional states from speech\nsignals and has been widely applied in human-computer interaction, education,\nhealthcare, and many other fields. However, since speech data contain rich\nsensitive information, partial data can be required to be deleted by speakers\ndue to privacy concerns. Current machine unlearning approaches largely depend\non data beyond the samples to be forgotten. However, this reliance poses\nchallenges when data redistribution is restricted and demands substantial\ncomputational resources in the context of big data. We propose a novel\nadversarial-attack-based approach that fine-tunes a pre-trained speech emotion\nrecognition model using only the data to be forgotten. The experimental results\ndemonstrate that the proposed approach can effectively remove the knowledge of\nthe data to be forgotten from the model, while preserving high model\nperformance on the test set for emotion recognition."}
{"id": "2510.03986", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03986", "abs": "https://arxiv.org/abs/2510.03986", "authors": ["Ananya Raghu", "Anisha Raghu", "Nithika Vivek", "Sofie Budman", "Omar Mansour"], "title": "A Multilingual Framework for Dysarthria: Detection, Severity Classification, Speech-to-Text, and Clean Speech Generation", "comment": null, "summary": "Dysarthria is a motor speech disorder that results in slow and often\nincomprehensible speech. Speech intelligibility significantly impacts\ncommunication, leading to barriers in social interactions. Dysarthria is often\na characteristic of neurological diseases including Parkinson's and ALS, yet\ncurrent tools lack generalizability across languages and levels of severity. In\nthis study, we present a unified AI-based multilingual framework that addresses\nsix key components: (1) binary dysarthria detection, (2) severity\nclassification, (3) clean speech generation, (4) speech-to-text conversion, (5)\nemotion detection, and (6) voice cloning. We analyze datasets in English,\nRussian, and German, using spectrogram-based visualizations and acoustic\nfeature extraction to inform model training. Our binary detection model\nachieved 97% accuracy across all three languages, demonstrating strong\ngeneralization across languages. The severity classification model also reached\n97% test accuracy, with interpretable results showing model attention focused\non lower harmonics. Our translation pipeline, trained on paired Russian\ndysarthric and clean speech, reconstructed intelligible outputs with low\ntraining (0.03) and test (0.06) L1 losses. Given the limited availability of\nEnglish dysarthric-clean pairs, we fine-tuned the Russian model on English data\nand achieved improved losses of 0.02 (train) and 0.03 (test), highlighting the\npromise of cross-lingual transfer learning for low-resource settings. Our\nspeech-to-text pipeline achieved a Word Error Rate of 0.1367 after three\nepochs, indicating accurate transcription on dysarthric speech and enabling\ndownstream emotion recognition and voice cloning from transcribed speech.\nOverall, the results and products of this study can be used to diagnose\ndysarthria and improve communication and understanding for patients across\ndifferent languages."}
{"id": "2510.03628", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03628", "abs": "https://arxiv.org/abs/2510.03628", "authors": ["Haochen Li"], "title": "Pinching Antenna Systems (PASS) for Cell-Free Communications", "comment": "5 pages, 5 figures", "summary": "A pinching antenna system (PASS) assisted cell-free communication system is\nproposed. A sum rate maximization problem under the BS power budget constraint\nand PA deployment constraint is formulated. To tackle the proposed non-convex\noptimization problem, an alternating optimization (AO) algorithm is developed.\nIn particular, the digital beamforming sub-problem is solved using the weighted\nminimum mean square error (WMMSE) method, whereas the pinching beamforming\nsub-problem is handled via a penalty based approach combined with element-wise\noptimization. Simulation results demonstrate that: 1) the PASS assisted\ncell-free systems achieve superior performance over benchmark schemes; 2)\nincreasing the number of PAs per waveguides can improve the advantage of PASS\nassisted cell-free systems; and 3) the cell-free architecture mitigates the\naverage user rate degradation as the number of users increases."}
{"id": "2510.04339", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04339", "abs": "https://arxiv.org/abs/2510.04339", "authors": ["Christian Limberg", "Fares Schulz", "Zhe Zhang", "Stefan Weinzierl"], "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space", "comment": "8 pages, accepted to the Proceedings of the 28-th Int. Conf. on\n  Digital Audio Effects (DAFx25) - demo: https://pgesam.faresschulz.com", "summary": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com"}
{"id": "2510.04136", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04136", "abs": "https://arxiv.org/abs/2510.04136", "authors": ["Umberto Cappellazzo", "Minsu Kim", "Pingchuan Ma", "Honglie Chen", "Xubo Liu", "Stavros Petridis", "Maja Pantic"], "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition", "comment": "NeurIPS 2025", "summary": "Large language models (LLMs) have recently shown strong potential in\naudio-visual speech recognition (AVSR), but their high computational demands\nand sensitivity to token granularity limit their practicality in\nresource-constrained settings. Token compression methods can reduce inference\ncost, but they require fixing a compression rate in advance and produce a\nsingle fixed-length output, offering no flexibility to balance information\ndensity and efficiency at inference time. Matryoshka representation learning\n(MRL) addresses this by enabling a single model to operate across multiple\ntoken granularities, allowing compression rates to be adjusted dynamically.\nHowever, current MRL-based methods treat each scale independently during\ntraining, limiting cross-scale generalization, robustness at high compression,\nand interpretability. To overcome these limitations, we propose MoME (Mixture\nof Matryoshka Experts), a novel framework that integrates sparse\nMixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen\nLLM with top-k routed and shared experts, allowing dynamic capacity allocation\nacross scales and modalities. A shared router promotes consistent expert\nactivation across granularities, enabling compressed sequences to benefit from\nrepresentations learned at lower compression. Experiments on LRS2 and LRS3\ndemonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,\nand VSR tasks, while requiring significantly fewer parameters and maintaining\nrobustness under noise. MoME unifies the adaptability of MRL with the\nefficiency of MoE, offering a scalable and interpretable solution for\nresource-aware speech recognition."}
{"id": "2510.03749", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03749", "abs": "https://arxiv.org/abs/2510.03749", "authors": ["Fanghao Xia", "Zesong Fei", "Xinyi Wang", "Nanchi Su", "Zhaolin Wang", "Yuanwei Liu", "Jie Xu"], "title": "Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?", "comment": "13 pages, 12 figures", "summary": "In this paper, sensing-assisted secure communication in a multi-user\nmulti-eavesdropper integrated sensing and communication (ISAC) system is\ninvestigated. Confidential communication signals and dedicated sensing signals\nare jointly transmitted by a base station (BS) to simultaneously serve users\nand sense aerial eavesdroppers (AEs). A sum rate maximization problem is\nformulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and\nsensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A\nfractional-programming-based alternating optimization algorithm is developed to\nsolve this problem for fully digital arrays, where successive convex\napproximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle\nnon-convex constraints. Furthermore, the minimum number of dedicated sensing\nbeams is analyzed via a worst-case rank bound, upon which the proposed\nbeamforming design is further extended to the hybrid analog-digital (HAD) array\narchitecture, where the unit-modulus constraint is addressed by manifold\noptimization. Simulation results demonstrate that only a small number of\nsensing beams are sufficient for both sensing and jamming AEs, and the proposed\ndesigns consistently outperform strong baselines while also revealing the\ncommunication-sensing trade-off."}
{"id": "2510.04463", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04463", "abs": "https://arxiv.org/abs/2510.04463", "authors": ["Takashi Maekaku", "Keita Goto", "Jinchuan Tian", "Yusuke Shinohara", "Shinji Watanabe"], "title": "Evaluating Self-Supervised Speech Models via Text-Based LLMS", "comment": "Accepted to ASRU 2025", "summary": "Self-Supervised Learning (SSL) has gained traction for its ability to learn\nrich representations with low labeling costs, applicable across diverse\ndownstream tasks. However, assessing the downstream-task performance remains\nchallenging due to the cost of extra training and evaluation. Existing methods\nfor task-agnostic evaluation also require extra training or hyperparameter\ntuning. We propose a novel evaluation metric using large language models\n(LLMs). By inputting discrete token sequences and minimal domain cues derived\nfrom SSL models into LLMs, we obtain the mean log-likelihood; these cues guide\nin-context learning, rendering the score more reliable without extra training\nor hyperparameter tuning. Experimental results show a correlation between\nLLM-based scores and automatic speech recognition task. Additionally, our\nfindings reveal that LLMs not only functions as an SSL evaluation tools but\nalso provides inference-time embeddings that are useful for speaker\nverification task."}
{"id": "2510.04162", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04162", "abs": "https://arxiv.org/abs/2510.04162", "authors": ["Aviv Navon", "Aviv Shamsian", "Neta Glazer", "Yael Segal-Feldman", "Gill Hetz", "Joseph Keshet", "Ethan Fetaya"], "title": "Drax: Speech Recognition with Discrete Flow Matching", "comment": null, "summary": "Diffusion and flow-based non-autoregressive (NAR) models have shown strong\npromise in large language modeling, however, their potential for automatic\nspeech recognition (ASR) remains largely unexplored. We propose Drax, a\ndiscrete flow matching framework for ASR that enables efficient parallel\ndecoding. To better align training with inference, we construct an\naudio-conditioned probability path that guides the model through trajectories\nresembling likely intermediate inference errors, rather than direct random\nnoise to target transitions. Our theoretical analysis links the generalization\ngap to divergences between training and inference occupancies, controlled by\ncumulative velocity errors, thereby motivating our design choice. Empirical\nevaluation demonstrates that our approach attains recognition accuracy on par\nwith state-of-the-art speech models while offering improved accuracy-efficiency\ntrade-offs, highlighting discrete flow matching as a promising direction for\nadvancing NAR ASR."}
{"id": "2510.03780", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03780", "abs": "https://arxiv.org/abs/2510.03780", "authors": ["Yiqiao Chen"], "title": "A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification", "comment": "8 pages, 5 figures", "summary": "Cardiovascular disease (CVD) is a major pediatric health burden, and early\nscreening is of critical importance. Electrocardiography (ECG), as a\nnoninvasive and accessible tool, is well suited for this purpose. This paper\npresents the first benchmark study of deep learning for multi-label pediatric\nCVD classification on the recently released ZZU-pECG dataset, comprising 3716\nrecordings with 19 CVD categories. We systematically evaluate four\nrepresentative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under\nboth 9-lead and 12-lead configurations. All models achieved strong results,\nwith Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.\nResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and\nTransformer also showed competitive performance. Per-class analysis indicated\nchallenges for rare conditions such as hypertrophic cardiomyopathy in the\n9-lead subset, reflecting the effect of limited positive samples. This\nbenchmark establishes reusable baselines and highlights complementary strengths\nacross paradigms. It further points to the need for larger-scale, multi-center\nvalidation, age-stratified analysis, and broader disease coverage to support\nreal-world pediatric ECG applications."}
{"id": "2510.04577", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04577", "abs": "https://arxiv.org/abs/2510.04577", "authors": ["Juncheng Wang", "Chao Xu", "Cheng Yu", "Zhe Hu", "Haoyu Xie", "Guoqi Yu", "Lei Shang", "Shujun Wang"], "title": "Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers", "comment": "Accepted to EMNLP 2025", "summary": "While language models (LMs) paired with residual vector quantization (RVQ)\ntokenizers have shown promise in text-to-audio (T2A) generation, they still lag\nbehind diffusion-based models by a non-trivial margin. We identify a critical\ndilemma underpinning this gap: incorporating more RVQ layers improves audio\nreconstruction fidelity but exceeds the generation capacity of conventional\nLMs. To address this, we first analyze RVQ dynamics and uncover two key\nlimitations: 1) orthogonality of features across RVQ layers hinders effective\nLMs training, and 2) descending semantic richness in tokens from deeper RVQ\nlayers exacerbates exposure bias during autoregressive decoding. Based on these\ninsights, we propose Siren, a novel LM-based framework that employs multiple\nisolated transformers with causal conditioning and anti-causal alignment via\nreinforcement learning. Extensive experiments demonstrate that Siren\noutperforms both existing LM-based and diffusion-based T2A systems, achieving\nstate-of-the-art results. By bridging the representational strengths of LMs\nwith the fidelity demands of audio synthesis, our approach repositions LMs as\ncompetitive contenders against diffusion models in T2A tasks. Moreover, by\naligning audio representations with linguistic structures, Siren facilitates a\npromising pathway toward unified multi-modal generation frameworks."}
{"id": "2510.04213", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04213", "abs": "https://arxiv.org/abs/2510.04213", "authors": ["Ze Li", "Ming Cheng", "Ming Li"], "title": "Enhancing Speaker Verification with w2v-BERT 2.0 and Knowledge Distillation guided Structured Pruning", "comment": null, "summary": "Large-scale self-supervised Pre-Trained Models (PTMs) have shown significant\nimprovements in the speaker verification (SV) task by providing rich feature\nrepresentations. In this paper, we utilize w2v-BERT 2.0, a model with\napproximately 600 million parameters trained on 450 million hours of unlabeled\ndata across 143 languages, for the SV task. The MFA structure with Layer\nAdapter is employed to process the multi-layer feature outputs from the PTM and\nextract speaker embeddings. Additionally, we incorporate LoRA for efficient\nfine-tuning. Our model achieves state-of-the-art results with 0.12% and 0.55%\nEER on the Vox1-O and Vox1-H test sets, respectively. Furthermore, we apply\nknowledge distillation guided structured pruning, reducing the model size by\n80% while achieving only a 0.04% EER degradation. Source code and models are\nreleased at https://github.com/ZXHY-82/w2v-BERT-2.0_SV."}
{"id": "2510.03787", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03787", "abs": "https://arxiv.org/abs/2510.03787", "authors": ["Jacopo Pegoraro", "Gianmaria Ventura", "Dario Tagliaferri", "Marco Mezzavilla", "Andrea Bedin", "Michele Rossi", "Joerg Widmer"], "title": "Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms", "comment": "19 pages, 14 figures", "summary": "Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G\nwireless networks. The bandwidth availability and diversity of FR3 offer\nunprecedented opportunities for coherent multiband Integrated Sensing and\nCommunications (ISAC), which aggregates the carrier phase information from\nmultiple frequency bands to increase the sensing resolution to the cm-level.\nHowever, the frequency anisotropy of sensing targets over GHz-wide bands and\nthe non-contiguity of the 6G spectrum, pose critical challenges to the\napplication of existing multiband ISAC techniques. We present the first study\non coherent multiband sensing in FR3. We experimentally characterize the\nfrequency anisotropy of targets and propose new phase coherence metrics for\nmultiband processing. Then, we analyze the impact of non-contiguous FR3 bands\nconsidered by 3GPP, and design a new algorithm to mitigate the resulting\nsensing artifacts, outperforming existing techniques. Our results represent a\nfirst step toward fully developing multiband ISAC for FR3."}
{"id": "2510.04688", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04688", "abs": "https://arxiv.org/abs/2510.04688", "authors": ["Joann Ching", "Gerhard Widmer"], "title": "A Study on the Data Distribution Gap in Music Emotion Recognition", "comment": "Accepted at the 17th International Symposium on Computer Music\n  Multidisciplinary Research (CMMR) 2025", "summary": "Music Emotion Recognition (MER) is a task deeply connected to human\nperception, relying heavily on subjective annotations collected from\ncontributors. Prior studies tend to focus on specific musical styles rather\nthan incorporating a diverse range of genres, such as rock and classical,\nwithin a single framework. In this paper, we address the task of recognizing\nemotion from audio content by investigating five datasets with dimensional\nemotion annotations -- EmoMusic, DEAM, PMEmo, WTC, and WCMED -- which span\nvarious musical styles. We demonstrate the problem of out-of-distribution\ngeneralization in a systematic experiment. By closely looking at multiple data\nand feature sets, we provide insight into genre-emotion relationships in\nexisting data and examine potential genre dominance and dataset biases in\ncertain feature representations. Based on these experiments, we arrive at a\nsimple yet effective framework that combines embeddings extracted from the\nJukebox model with chroma features and demonstrate how, alongside a combination\nof several diverse training sets, this permits us to train models with\nsubstantially improved cross-dataset generalization capabilities."}
{"id": "2510.04219", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04219", "abs": "https://arxiv.org/abs/2510.04219", "authors": ["Zhengjun Yue", "Devendra Kayande", "Zoran Cvetkovic", "Erfan Loweimi"], "title": "Probing Whisper for Dysarthric Speech in Detection and Assessment", "comment": "Submitted to ICASSP 2026", "summary": "Large-scale end-to-end models such as Whisper have shown strong performance\non diverse speech tasks, but their internal behavior on pathological speech\nremains poorly understood. Understanding how dysarthric speech is represented\nacross layers is critical for building reliable and explainable clinical\nassessment tools. This study probes the Whisper-Medium model encoder for\ndysarthric speech for detection and assessment (i.e., severity classification).\nWe evaluate layer-wise embeddings with a linear classifier under both\nsingle-task and multi-task settings, and complement these results with\nSilhouette scores and mutual information to provide perspectives on layer\ninformativeness. To examine adaptability, we repeat the analysis after\nfine-tuning Whisper on a dysarthric speech recognition task. Across metrics,\nthe mid-level encoder layers (13-15) emerge as most informative, while\nfine-tuning induces only modest changes. The findings improve the\ninterpretability of Whisper's embeddings and highlight the potential of probing\nanalyses to guide the use of large-scale pretrained models for pathological\nspeech."}
{"id": "2510.03818", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.03818", "abs": "https://arxiv.org/abs/2510.03818", "authors": ["Lulu Song", "Di Zhang", "Tingting Zhang"], "title": "Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime", "comment": null, "summary": "Source polar coding is a potential solution for short blocklength-based\nlow-latency key generation with limited sources, which is a critical aspect of\nsix generation (6G) Internet of things. However, existing source coding schemes\nstill suffer from significant degradation in key generation rate and\nreconciliation reliability in short blocklength regime. To address this issue,\nwe introduce a multilevel source polarization-adjusted convolutional (PAC)\ncoding framework. Furthermore, we propose a novel code construction algorithm\nthat jointly leverages polarization effects and the maximum likelihood (ML)\ndecoding error coefficient. Simulations demonstrate that the multilevel source\nPAC scheme with the proposed code construction achieves superior key generation\nrate under key disagreement constraints compared to conventional and multilevel\nsource polar coding methods even in short blocklength regimes."}
{"id": "2510.04738", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04738", "abs": "https://arxiv.org/abs/2510.04738", "authors": ["Baher Mohammad", "Magauiya Zhussip", "Stamatios Lefkimmiatis"], "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba", "comment": null, "summary": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and\nSynthesis), a novel autoregressive architecture for text-conditioned voice\nediting and high-fidelity text-to-speech (TTS) synthesis, built on a\ncross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in\nspeech editing and very competitive results in zero-shot TTS, while not being\nexplicitly trained on the latter task, outperforming leading autoregressive and\ndiffusion models on diverse, real-world audio. By integrating Mamba for\nefficient audio sequence modeling with cross-attention for precise\ntext-acoustic alignment, MAVE enables context-aware voice editing with\nexceptional naturalness and speaker consistency. In pairwise human evaluations\non a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%\nof listeners rated MAVE - edited speech as perceptually equal to the original,\nwhile 24.8% prefered the original and 18.0% MAVE - demonstrating that in the\nmajority of cases edits are indistinguishable from the source. MAVE compares\nfavorably with VoiceCraft and FluentSpeech both on pairwise comparisons and\nstandalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE\nexceeds VoiceCraft in both speaker similarity and naturalness, without\nrequiring multiple inference runs or post-processing. Remarkably, these quality\ngains come with a significantly lower memory cost and approximately the same\nlatency: MAVE requires ~6x less memory than VoiceCraft during inference on\nutterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch\nsize 1). Our results demonstrate that MAVE establishes a new standard for\nflexible, high-fidelity voice editing and synthesis through the synergistic\nintegration of structured state-space modeling and cross-modal attention."}
{"id": "2510.04459", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04459", "abs": "https://arxiv.org/abs/2510.04459", "authors": ["Samuel A. Verburg", "Efren Fernandez-Grande", "Peter Gerstoft"], "title": "Differentiable physics for sound field reconstruction", "comment": "28 pages plus references, 8 figures, full journal paper", "summary": "Sound field reconstruction involves estimating sound fields from a limited\nnumber of spatially distributed observations. This work introduces a\ndifferentiable physics approach for sound field reconstruction, where the\ninitial conditions of the wave equation are approximated with a neural network,\nand the differential operator is computed with a differentiable numerical\nsolver. The use of a numerical solver enables a stable network training while\nenforcing the physics as a strong constraint, in contrast to conventional\nphysics-informed neural networks, which include the physics as a constraint in\nthe loss function. We introduce an additional sparsity-promoting constraint to\nachieve meaningful solutions even under severe undersampling conditions.\nExperiments demonstrate that the proposed approach can reconstruct sound fields\nunder extreme data scarcity, achieving higher accuracy and better convergence\ncompared to physics-informed neural networks."}
{"id": "2510.03848", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03848", "abs": "https://arxiv.org/abs/2510.03848", "authors": ["Jianyu Wang", "Zhichao Li", "Wenchi Cheng", "Wei Zhang", "Hailin Zhang"], "title": "Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums", "comment": null, "summary": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. However, the propagation mediums in practical MI based underground\nemergency communications are usually diverse and composed randomly due to the\nimpact of disasters, which poses a challenge for MI communication in practical\napplications. In this paper, we formulate a statistical fading channel model,\nwhich reflects the random composition of diverse mediums and is shown to follow\na lognormal distribution. To mitigate the impact of diverse medium fading,\nMulti-frequency Resonating Compensation (MuReC) based coils are used to achieve\nmultiband transmission. Then, we analyze the performance of MuReC based\nmulti-band MI communication with diverse medium fading and derive the\nexpressions of signal-to-noise ratio (SNR) probability density functions,\nergodic capacities, average bit error rates (BERs), and outage probabilities\nfor both multiplexing and diversity cases. Numerical results show that MuReC\nbased multiband transmission schemes can effectively reduce the impact of\ndiverse medium fading and enhance the performance."}
{"id": "2510.03630", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03630", "abs": "https://arxiv.org/abs/2510.03630", "authors": ["Xiluo He", "Alexander Polok", "Jesús Villalba", "Thomas Thebaud", "Matthew Maciejewski"], "title": "Scaling Multi-Talker ASR with Speaker-Agnostic Activity Streams", "comment": null, "summary": "An increasingly common training paradigm for multi-talker automatic speech\nrecognition (ASR) is to use speaker activity signals to adapt single-speaker\nASR models for overlapping speech. Although effective, these systems require\nrunning the ASR model once per speaker, resulting in inference costs that scale\nwith the number of speakers and limiting their practicality. In this work, we\npropose a method that decouples the inference cost of activity-conditioned ASR\nsystems from the number of speakers by converting speaker-specific activity\noutputs into two speaker-agnostic streams. A central challenge is that\nna\\\"ively merging speaker activities into streams significantly degrades\nrecognition, since pretrained ASR models assume contiguous, single-speaker\ninputs. To address this, we design new heuristics aimed at preserving\nconversational continuity and maintaining compatibility with existing systems.\nWe show that our approach is compatible with Diarization-Conditioned Whisper\n(DiCoW) to greatly reduce runtimes on the AMI and ICSI meeting datasets while\nretaining competitive performance."}
{"id": "2510.04593", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04593", "abs": "https://arxiv.org/abs/2510.04593", "authors": ["Wenhao Guan", "Zhikang Niu", "Ziyue Jiang", "Kaidi Wang", "Peijie Chen", "Qingyang Hong", "Lin Li", "Xie Chen"], "title": "UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated promising performance in both\nautomatic speech recognition (ASR) and text-to-speech (TTS) systems, gradually\nbecoming the mainstream approach. However, most current approaches address\nthese tasks separately rather than through a unified framework. This work aims\nto integrate these two tasks into one unified model. Although discrete speech\ntokenization enables joint modeling, its inherent information loss limits\nperformance in both recognition and generation. In this work, we present\nUniVoice, a unified LLM framework through continuous representations that\nseamlessly integrates speech recognition and synthesis within a single model.\nOur approach combines the strengths of autoregressive modeling for speech\nrecognition with flow matching for high-quality generation. To mitigate the\ninherent divergence between autoregressive and flow-matching models, we further\ndesign a dual attention mechanism, which switches between a causal mask for\nrecognition and a bidirectional attention mask for synthesis. Furthermore, the\nproposed text-prefix-conditioned speech infilling method enables high-fidelity\nzero-shot voice cloning. Experimental results demonstrate that our method can\nachieve or exceed current single-task modeling methods in both ASR and\nzero-shot TTS tasks. This work explores new possibilities for end-to-end speech\nunderstanding and generation."}
{"id": "2510.03850", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03850", "abs": "https://arxiv.org/abs/2510.03850", "authors": ["Fernando Darío Almeida García", "Francisco Raimundo Albuquerque Parente", "Michel Daoud Yacoub", "Jose Cândido Silveira Santos Filho"], "title": "On the Exact Sum PDF and CDF of α-μ Variates", "comment": null, "summary": "The sum of random variables (RVs) appears extensively in wireless\ncommunications, at large, both conventional and advanced, and has been subject\nof longstanding research. The statistical characterization of the referred sum\nis crucial to determine the performance of such communications systems.\nAlthough efforts have been undertaken to unveil these sum statistics, e.g.,\nprobability density function (PDF) and cumulative distribution function (CDF),\nno general efficient nor manageable solutions capable of evaluating the exact\nsum PDF and CDF are available to date. The only formulations are given in terms\nof either the multi-fold Brennan's integral or the multivariate Fox H-function.\nUnfortunately, these methods are only feasible up to a certain number of RVs,\nmeaning that when the number of RVs in the sum increases, the computation of\nthe sum PDF and CDF is subject to stability problems, convergence issues, or\ninaccurate results. In this paper, we derive new, simple, exact formulations\nfor the PDF and CDF of the sum of L independent and identically distributed\n{\\alpha}-{\\mu} RVs. Unlike the available solutions, the computational\ncomplexity of our analytical expressions is independent of the number of\nsummands. Capitalizing on our unprecedented findings, we analyze, in exact and\nasymptotic manners, the performance of L-branch pre-detection equal-gain\ncombining and maximal-ratio combining receivers over {\\alpha}-{\\mu} fading\nenvironments. The coding and diversity gains of the system for both receivers\nare analyzed and quantified. Moreover, numerical simulations show that the\ncomputation time reduces drastically when using our expressions, which are\narguably the most efficient and manageable formulations derived so far."}
{"id": "2510.03723", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03723", "abs": "https://arxiv.org/abs/2510.03723", "authors": ["Martin Kocour", "Martin Karafiat", "Alexander Polok", "Dominik Klement", "Lukáš Burget", "Jan Černocký"], "title": "Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition", "comment": null, "summary": "We propose a speaker-attributed (SA) Whisper-based model for multi-talker\nspeech recognition that combines target-speaker modeling with serialized output\ntraining (SOT). Our approach leverages a Diarization-Conditioned Whisper\n(DiCoW) encoder to extract target-speaker embeddings, which are concatenated\ninto a single representation and passed to a shared decoder. This enables the\nmodel to transcribe overlapping speech as a serialized output stream with\nspeaker tags and timestamps. In contrast to target-speaker ASR systems such as\nDiCoW, which decode each speaker separately, our approach performs joint\ndecoding, allowing the decoder to condition on the context of all speakers\nsimultaneously. Experiments show that the model outperforms existing SOT-based\napproaches and surpasses DiCoW on multi-talker mixtures (e.g., LibriMix)."}
{"id": "2510.04934", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04934", "abs": "https://arxiv.org/abs/2510.04934", "authors": ["Satvik Dixit", "Soham Deshmukh", "Bhiksha Raj"], "title": "AURA Score: A Metric For Holistic Audio Question Answering Evaluation", "comment": null, "summary": "Audio Question Answering (AQA) is a key task for evaluating Audio-Language\nModels (ALMs), yet assessing open-ended responses remains challenging. Existing\nmetrics used for AQA such as BLEU, METEOR and BERTScore, mostly adapted from\nNLP and audio captioning, rely on surface similarity and fail to account for\nquestion context, reasoning, and partial correctness. To address the gap in\nliterature, we make three contributions in this work. First, we introduce\nAQEval to enable systematic benchmarking of AQA metrics. It is the first\nbenchmark of its kind, consisting of 10k model responses annotated by multiple\nhumans for their correctness and relevance. Second, we conduct a comprehensive\nanalysis of existing AQA metrics on AQEval, highlighting weak correlation with\nhuman judgment, especially for longer answers. Third, we propose a new metric -\nAURA score, to better evaluate open-ended model responses. On AQEval, AURA\nachieves state-of-the-art correlation with human ratings, significantly\noutperforming all baselines. Through this work, we aim to highlight the\nlimitations of current AQA evaluation methods and motivate better metrics. We\nrelease both the AQEval benchmark and the AURA metric to support future\nresearch in holistic AQA evaluation."}
{"id": "2510.03852", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03852", "abs": "https://arxiv.org/abs/2510.03852", "authors": ["Jianyu Wang", "Tianrui Hou", "Wenchi Cheng", "Hailin Zhang"], "title": "Robust Beamforming for Magnetic Induction Based Underground Emergency Communications", "comment": null, "summary": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. Based on channel state information (CSI), magnetic beamforming can\nsignificantly improve the performance of MI communication. However, in\npost-disaster underground communication, channel estimation may suffer from\nerrors due to factors such as complex environmental interferences. Taking\nchannel estimation error into account, we formulate a beamforming optimization\nproblem for multi-user MI underground emergency communications, which aims to\nminimize the power consumption under the constraints of sum rate and signal to\ninterference plus noise ratio (SINR) of each user. Based on the worst-case\noptimization criterion and the S-procedure, the non-convex optimization problem\nis transformed into convex and solved. Numerical results show that the proposed\nrobust beamforming scheme can effectively enhance communication reliability and\neffective throughput in the presence of channel estimation errors."}
{"id": "2510.03825", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03825", "abs": "https://arxiv.org/abs/2510.03825", "authors": ["Pavel Rajmic", "Jiří Schimmel", "Šimon Cieslar"], "title": "A MATLAB toolbox for Computation of Speech Transmission Index (STI)", "comment": null, "summary": "The speech transmission index (STI) is a popular simple metric for the\nprediction of speech intelligibility when speech is passed through a\ntransmission channel. Computation of STI from acoustic measurements is\ndescribed in the IEC 60268-16:2020 standard. Though, reliable implementations\nof STI are not publicly accessible and are frequently limited to the use with a\nproprietary measurement hardware. We present a Matlab STI implementation of\nboth the direct and indirect approaches according to the standard, including\nthe shortened STIPA protocol. The suggested implementation meets prescribed\nrequirements, as evidenced by tests on reference signals. Additionally, we\nconducted a verification measurement in comparison to a commercial measurement\ndevice. Our software comes with open source code."}
{"id": "2510.04937", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04937", "abs": "https://arxiv.org/abs/2510.04937", "authors": ["Ben Heritage", "Fiona Ryder", "Michael McLoughlin", "Karolina Prawda"], "title": "Perceptual Evaluation of Extrapolated Spatial Room Impulse Responses From a Mono Source", "comment": "Preprint to be presented as a poster at ADC 2025", "summary": "Immersion in virtual and augmented reality solutions is reliant on plausible\nspatial audio. However, plausibly representing a space for immersive audio\noften requires many individual acoustic measurements of source-microphone pairs\nwith specialist spatial microphones, making the procedure time-consuming and\nexpensive. In this study, we evaluate the plausibility of extrapolated and\nspatialised Room Impulse Responses (RIRs) by using a 3-Alternative Forced\nChoice (3AFC) listening test. The stimuli comprised of RIRs from three spaces\nconvolved with speech, orchestral, and instrumental music. When asked to select\nwhich stimuli was artificial out of one extrapolated and two real stimuli, an\noverall accuracy of 38% was achieved from 20 participants (5 percentage points\nabove the expected guessing rate). Given the listening test result, this study\nshows that it is possible to extrapolate plausible spatial RIRs from mono\nmeasurements, decreasing the need for time and specialist equipment in acoustic\nmeasurements."}
{"id": "2510.03901", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03901", "abs": "https://arxiv.org/abs/2510.03901", "authors": ["Vincent Savaux", "Steve Sawadogo", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications", "comment": "9 pages, 5 figures, conference", "summary": "This paper investigates the robustness of affine frequency division\nmultiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation\nschemes against non-white Gaussian noise, which can model various sources of\nadditive disturbances to the received signal. The proposed approach\ndemonstrates that the performance of these waveforms depends on the ability of\nthe demodulation matrix to whiten the noise-a property that is, in turn,\nrelated to the sparsity of the matrix. AFDM is shown to outperform OTFS and\northogonal frequency division multiplexing (OFDM), as its demodulation matrix\nis generally less sparse than those of the other waveforms. Based on this\nanalysis, several application examples and use cases are presented, such as the\nuse of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals.\nFinally, simulation results confirm that AFDM achieves better performance than\nOTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in\nmost application scenarios."}
{"id": "2510.03986", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.03986", "abs": "https://arxiv.org/abs/2510.03986", "authors": ["Ananya Raghu", "Anisha Raghu", "Nithika Vivek", "Sofie Budman", "Omar Mansour"], "title": "A Multilingual Framework for Dysarthria: Detection, Severity Classification, Speech-to-Text, and Clean Speech Generation", "comment": null, "summary": "Dysarthria is a motor speech disorder that results in slow and often\nincomprehensible speech. Speech intelligibility significantly impacts\ncommunication, leading to barriers in social interactions. Dysarthria is often\na characteristic of neurological diseases including Parkinson's and ALS, yet\ncurrent tools lack generalizability across languages and levels of severity. In\nthis study, we present a unified AI-based multilingual framework that addresses\nsix key components: (1) binary dysarthria detection, (2) severity\nclassification, (3) clean speech generation, (4) speech-to-text conversion, (5)\nemotion detection, and (6) voice cloning. We analyze datasets in English,\nRussian, and German, using spectrogram-based visualizations and acoustic\nfeature extraction to inform model training. Our binary detection model\nachieved 97% accuracy across all three languages, demonstrating strong\ngeneralization across languages. The severity classification model also reached\n97% test accuracy, with interpretable results showing model attention focused\non lower harmonics. Our translation pipeline, trained on paired Russian\ndysarthric and clean speech, reconstructed intelligible outputs with low\ntraining (0.03) and test (0.06) L1 losses. Given the limited availability of\nEnglish dysarthric-clean pairs, we fine-tuned the Russian model on English data\nand achieved improved losses of 0.02 (train) and 0.03 (test), highlighting the\npromise of cross-lingual transfer learning for low-resource settings. Our\nspeech-to-text pipeline achieved a Word Error Rate of 0.1367 after three\nepochs, indicating accurate transcription on dysarthric speech and enabling\ndownstream emotion recognition and voice cloning from transcribed speech.\nOverall, the results and products of this study can be used to diagnose\ndysarthria and improve communication and understanding for patients across\ndifferent languages."}
{"id": "2510.04956", "categories": ["eess.AS", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04956", "abs": "https://arxiv.org/abs/2510.04956", "authors": ["Bi-Cheng Yan", "Ming-Kang Tsai", "Berlin Chen"], "title": "MuFFIN: Multifaceted Pronunciation Feedback Model with Interactive Hierarchical Neural Modeling", "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "summary": "Computer-assisted pronunciation training (CAPT) manages to facilitate\nsecond-language (L2) learners to practice pronunciation skills by offering\ntimely and instructive feedback. To examine pronunciation proficiency from\nmultiple facets, existing methods for CAPT broadly fall into two categories:\nmispronunciation detection and diagnosis (MDD) as well as automatic\npronunciation assessment (APA). The former aims to pinpoint phonetic\npronunciation errors and provide diagnostic feedback, while the latter seeks\ninstead to quantify pronunciation proficiency pertaining to various aspects.\nDespite the natural complementarity between MDD and APA, researchers and\npractitioners, however, often treat them as independent tasks with disparate\nmodeling paradigms. In light of this, we in this paper first introduce MuFFIN,\na Multi-Faceted pronunciation Feedback model with an Interactive hierarchical\nNeural architecture, to jointly address the tasks of MDD and APA. To better\ncapture the nuanced distinctions between phonemes in the feature space, a novel\nphoneme-contrastive ordinal regularization mechanism is then put forward to\noptimize the proposed model to generate more phoneme-discriminative features\nwhile factoring in the ordinality of the aspect scores. In addition, to address\nthe intricate data imbalance problem in MDD, we design a simple yet effective\ntraining objective, which is specifically tailored to perturb the outputs of a\nphoneme classifier with the phoneme-specific variations, so as to better render\nthe distribution of predicted phonemes meanwhile considering their\nmispronunciation characteristics. A series of experiments conducted on the\nSpeechocean762 benchmark dataset demonstrates the efficacy of our method in\nrelation to several cutting-edge baselines, showing state-of-the-art\nperformance on both the APA and MDD tasks."}
{"id": "2510.04037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04037", "abs": "https://arxiv.org/abs/2510.04037", "authors": ["Mohammad Salman", "Hadi Zayyani", "Hasan Abu Hilal", "Mostafa Rashdan"], "title": "Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate", "comment": null, "summary": "This letter presents a novel method for estimating the position, velocity,\nand acceleration of a moving target using range-based measurements. Although\nmost existing studies focus on position and velocity estimation, the framework\nof this letter is extended to include acceleration. To achieve this, we propose\nusing the derivative of the range rate, in addition to the range and range rate\nmeasurements. The proposed method estimates the position at first using\nTime-of-Arrival (TOA)-based techniques; then, develops a reformulated least\nsquares (LS) and weighted least squares (WLS) approaches for velocity\nestimation; and finally, employs the derivative of the range rate to estimate\nthe acceleration using previous position and velocity estimates. On the other\nhand, closed-form LS and WLS solutions are derived for both velocity and\nacceleration. The simulation results show that the proposed approach provides\nimproved performance in estimating moving target kinematics compared to\nexisting methods."}
{"id": "2510.04136", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04136", "abs": "https://arxiv.org/abs/2510.04136", "authors": ["Umberto Cappellazzo", "Minsu Kim", "Pingchuan Ma", "Honglie Chen", "Xubo Liu", "Stavros Petridis", "Maja Pantic"], "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition", "comment": "NeurIPS 2025", "summary": "Large language models (LLMs) have recently shown strong potential in\naudio-visual speech recognition (AVSR), but their high computational demands\nand sensitivity to token granularity limit their practicality in\nresource-constrained settings. Token compression methods can reduce inference\ncost, but they require fixing a compression rate in advance and produce a\nsingle fixed-length output, offering no flexibility to balance information\ndensity and efficiency at inference time. Matryoshka representation learning\n(MRL) addresses this by enabling a single model to operate across multiple\ntoken granularities, allowing compression rates to be adjusted dynamically.\nHowever, current MRL-based methods treat each scale independently during\ntraining, limiting cross-scale generalization, robustness at high compression,\nand interpretability. To overcome these limitations, we propose MoME (Mixture\nof Matryoshka Experts), a novel framework that integrates sparse\nMixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen\nLLM with top-k routed and shared experts, allowing dynamic capacity allocation\nacross scales and modalities. A shared router promotes consistent expert\nactivation across granularities, enabling compressed sequences to benefit from\nrepresentations learned at lower compression. Experiments on LRS2 and LRS3\ndemonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,\nand VSR tasks, while requiring significantly fewer parameters and maintaining\nrobustness under noise. MoME unifies the adaptability of MRL with the\nefficiency of MoE, offering a scalable and interpretable solution for\nresource-aware speech recognition."}
{"id": "2510.03387", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03387", "abs": "https://arxiv.org/abs/2510.03387", "authors": ["Kirill Trapeznikov", "Paul Cummer", "Pranay Pherwani", "Jai Aslam", "Michael S. Davinroy", "Peter Bautista", "Laura Cassani", "Matthew Stamm", "Jill Crisman"], "title": "Audio Forensics Evaluation (SAFE) Challenge", "comment": null, "summary": "The increasing realism of synthetic speech generated by advanced\ntext-to-speech (TTS) models, coupled with post-processing and laundering\ntechniques, presents a significant challenge for audio forensic detection. In\nthis paper, we introduce the SAFE (Synthetic Audio Forensics Evaluation)\nChallenge, a fully blind evaluation framework designed to benchmark detection\nmodels across progressively harder scenarios: raw synthetic speech, processed\naudio (e.g., compression, resampling), and laundered audio intended to evade\nforensic analysis. The SAFE challenge consisted of a total of 90 hours of audio\nand 21,000 audio samples split across 21 different real sources and 17\ndifferent TTS models and 3 tasks. We present the challenge, evaluation design\nand tasks, dataset details, and initial insights into the strengths and\nlimitations of current approaches, offering a foundation for advancing\nsynthetic audio detection research. More information is available at\n\\href{https://stresearch.github.io/SAFE/}{https://stresearch.github.io/SAFE/}."}
{"id": "2510.04160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04160", "abs": "https://arxiv.org/abs/2510.04160", "authors": ["Mohammad Kazzazi", "Mohammad Morsali", "Rouhollah Amiri"], "title": "CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization", "comment": "Mohammad Kazzazi and Mohammad Morsali contributed equally to this\n  work", "summary": "This paper presents CLEAR -- a closed-form localization estimator with a\nreduced sensor network. The proposed method is a computationally efficient,\ntwo-stage estimator that fuses time-difference-of-arrival (TDOA) and\nfrequency-difference-of-arrival (FDOA) measurements with a minimal number of\nsensors. CLEAR localizes a moving source in N-dimensional space using only N+1\nsensors, achieving the theoretical minimum sensor count. The first stage\nintroduces auxiliary range and range-rate parameters to construct a set of\npseudo-linear equations, solved via weighted least squares. An algebraic\nelimination using Sylvester's resultant then reduces the problem to a quartic\nequation, yielding closed-form estimates for the nuisance variables. A second,\nlightweight linear refinement stage is applied to mitigate residual bias. Under\nmild Gaussian noise assumptions, the estimator's position and velocity\nestimates are statistically efficient, closely approaching the Cramer-Rao lower\nbound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios\ndemonstrate CRLB-level accuracy and consistent performance gains over\nrepresentative two-stage and iterative baselines, confirming the method's high\nsuitability for power-constrained, distributed Internet of Things (IoT)\napplications such as UAV tracking and smart transportation."}
{"id": "2510.04162", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04162", "abs": "https://arxiv.org/abs/2510.04162", "authors": ["Aviv Navon", "Aviv Shamsian", "Neta Glazer", "Yael Segal-Feldman", "Gill Hetz", "Joseph Keshet", "Ethan Fetaya"], "title": "Drax: Speech Recognition with Discrete Flow Matching", "comment": null, "summary": "Diffusion and flow-based non-autoregressive (NAR) models have shown strong\npromise in large language modeling, however, their potential for automatic\nspeech recognition (ASR) remains largely unexplored. We propose Drax, a\ndiscrete flow matching framework for ASR that enables efficient parallel\ndecoding. To better align training with inference, we construct an\naudio-conditioned probability path that guides the model through trajectories\nresembling likely intermediate inference errors, rather than direct random\nnoise to target transitions. Our theoretical analysis links the generalization\ngap to divergences between training and inference occupancies, controlled by\ncumulative velocity errors, thereby motivating our design choice. Empirical\nevaluation demonstrates that our approach attains recognition accuracy on par\nwith state-of-the-art speech models while offering improved accuracy-efficiency\ntrade-offs, highlighting discrete flow matching as a promising direction for\nadvancing NAR ASR."}
{"id": "2510.03728", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03728", "abs": "https://arxiv.org/abs/2510.03728", "authors": ["Kuang Yuan", "Yang Gao", "Xilin Li", "Xinhao Mei", "Syavosh Zadissa", "Tarun Pruthi", "Saeed Bagheri Sereshki"], "title": "Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation", "comment": null, "summary": "Acoustic scene classification (ASC) models on edge devices typically operate\nunder fixed class assumptions, lacking the transferability needed for\nreal-world applications that require adaptation to new or refined acoustic\ncategories. We propose ContrastASC, which learns generalizable acoustic scene\nrepresentations by structuring the embedding space to preserve semantic\nrelationships between scenes, enabling adaptation to unseen categories without\nretraining. Our approach combines supervised contrastive fine-tuning of\npre-trained models with contrastive representation distillation to transfer\nthis structured knowledge to compact student models. Our evaluation shows that\nContrastASC demonstrates improved few-shot adaptation to unseen categories\nwhile maintaining strong closed-set performance."}
{"id": "2510.04240", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04240", "abs": "https://arxiv.org/abs/2510.04240", "authors": ["Dario Tagliaferri", "Silvia Mura", "Musa Furkan Keskin", "Sauradeep Dey", "Henk Wymeersch"], "title": "Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks", "comment": "13 pages", "summary": "This paper addresses the challenge of integrating multistatic coherent\nimaging functionalities in the downlink (DL) of a phase-coherent distributed\nmultiple input multiple output (D-MIMO) communication network. During DL, the\nD-MIMO access points (APs) jointly precode the transmitted signals to maximize\nthe spectral efficiency (SE) at the users (UEs) locations. However, imaging\nrequires that \\textit{(i)} a fraction of the APs work as receivers for sensing\nand \\textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals\nto illuminate the area to be imaged and allow multistatic operation. In these\nsettings, our contribution is twofold. We propose a novel distributed\nintegrated sensing and communication (D-ISAC) system that superposes a\npurposely designed AP-specific signal for imaging to the legacy UE-specific\ncommunication one, with a tunable trade-off factor. We detail both the imaging\nwaveform design according to the \\textit{extended orthogonality condition} and\nthe space-frequency precoder design. Then, we propose an optimized selection\nstrategy for the receiving APs, in order to maximize imaging performance under\nhalf-duplex constraints. Extensive numerical results prove the feasibility and\nbenefits of our proposal, materializing the potential of joint multistatic\nimaging and communications in practical D-MIMO deployments."}
{"id": "2510.04213", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04213", "abs": "https://arxiv.org/abs/2510.04213", "authors": ["Ze Li", "Ming Cheng", "Ming Li"], "title": "Enhancing Speaker Verification with w2v-BERT 2.0 and Knowledge Distillation guided Structured Pruning", "comment": null, "summary": "Large-scale self-supervised Pre-Trained Models (PTMs) have shown significant\nimprovements in the speaker verification (SV) task by providing rich feature\nrepresentations. In this paper, we utilize w2v-BERT 2.0, a model with\napproximately 600 million parameters trained on 450 million hours of unlabeled\ndata across 143 languages, for the SV task. The MFA structure with Layer\nAdapter is employed to process the multi-layer feature outputs from the PTM and\nextract speaker embeddings. Additionally, we incorporate LoRA for efficient\nfine-tuning. Our model achieves state-of-the-art results with 0.12% and 0.55%\nEER on the Vox1-O and Vox1-H test sets, respectively. Furthermore, we apply\nknowledge distillation guided structured pruning, reducing the model size by\n80% while achieving only a 0.04% EER degradation. Source code and models are\nreleased at https://github.com/ZXHY-82/w2v-BERT-2.0_SV."}
{"id": "2510.03741", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.03741", "abs": "https://arxiv.org/abs/2510.03741", "authors": ["Benoît Giniès", "Xiaoyu Bie", "Olivier Fercoq", "Gaël Richard"], "title": "Désentrelacement Fréquentiel Doux pour les Codecs Audio Neuronaux", "comment": "in French language, Groupe de Recherche et d'Etudes du Traitement du\n  Signal et des Images (GRETSI 2025), Aug 2025, Strasbourg, France", "summary": "While neural-based models have led to significant advancements in audio\nfeature extraction, the interpretability of the learned representations remains\na critical challenge. To address this, disentanglement techniques have been\nintegrated into discrete neural audio codecs to impose structure on the\nextracted tokens. However, these approaches often exhibit strong dependencies\non specific datasets or task formulations. In this work, we propose a\ndisentangled neural audio codec that leverages spectral decomposition of\ntime-domain signals to enhance representation interpretability. Experimental\nevaluations demonstrate that our method surpasses a state-of-the-art baseline\nin both reconstruction fidelity and perceptual quality."}
{"id": "2510.04258", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04258", "abs": "https://arxiv.org/abs/2510.04258", "authors": ["Ziang Zhao", "Weixi Liang", "Kai Hu", "Qun Zhang", "Xiongbin Yu", "Qiang Li"], "title": "Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments", "comment": null, "summary": "Accurate channel modeling is essential for realizing the potential of\nterahertz (THz) communications in 6G indoor networks, where existing models\nstruggle with severe frequency selectivity and multipath effects. We propose a\nphysically grounded Rician fading channel model that jointly incorporates\ndeterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS)\ncomponents, enhanced by frequency-dependent attenuation characterized by\noptimized exponents alpha and beta. Unlike conventional approaches, our model\nintegrates a two-ray reflection framework to capture standing wave phenomena\nand employs wideband spectral averaging to mitigate frequency selectivity over\nbandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning\n0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE)\nas low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and\nreducing RMSE by 73.3% as bandwidth increases. These findings underscore the\nimportance of bandwidth in suppressing oscillatory artifacts and improving\nmodeling accuracy. Our approach provides a robust foundation for THz system\ndesign, supporting reliable indoor wireless personal area networks (WPANs),\ndevice-to-device (D2D) communications, and precise localization in future 6G\napplications."}
{"id": "2510.04219", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04219", "abs": "https://arxiv.org/abs/2510.04219", "authors": ["Zhengjun Yue", "Devendra Kayande", "Zoran Cvetkovic", "Erfan Loweimi"], "title": "Probing Whisper for Dysarthric Speech in Detection and Assessment", "comment": "Submitted to ICASSP 2026", "summary": "Large-scale end-to-end models such as Whisper have shown strong performance\non diverse speech tasks, but their internal behavior on pathological speech\nremains poorly understood. Understanding how dysarthric speech is represented\nacross layers is critical for building reliable and explainable clinical\nassessment tools. This study probes the Whisper-Medium model encoder for\ndysarthric speech for detection and assessment (i.e., severity classification).\nWe evaluate layer-wise embeddings with a linear classifier under both\nsingle-task and multi-task settings, and complement these results with\nSilhouette scores and mutual information to provide perspectives on layer\ninformativeness. To examine adaptability, we repeat the analysis after\nfine-tuning Whisper on a dysarthric speech recognition task. Across metrics,\nthe mid-level encoder layers (13-15) emerge as most informative, while\nfine-tuning induces only modest changes. The findings improve the\ninterpretability of Whisper's embeddings and highlight the potential of probing\nanalyses to guide the use of large-scale pretrained models for pathological\nspeech."}
{"id": "2510.04157", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04157", "abs": "https://arxiv.org/abs/2510.04157", "authors": ["Efrayim Yanir", "David Burshtein", "Sharon Gannot"], "title": "GDiffuSE: Diffusion-based speech enhancement with noise model guidance", "comment": null, "summary": "This paper introduces a novel speech enhancement (SE) approach based on a\ndenoising diffusion probabilistic model (DDPM), termed Guided diffusion for\nspeech enhancement (GDiffuSE). In contrast to conventional methods that\ndirectly map noisy speech to clean speech, our method employs a lightweight\nhelper model to estimate the noise distribution, which is then incorporated\ninto the diffusion denoising process via a guidance mechanism. This design\nimproves robustness by enabling seamless adaptation to unseen noise types and\nby leveraging large-scale DDPMs originally trained for speech generation in the\ncontext of SE. We evaluate our approach on noisy signals obtained by adding\nnoise samples from the BBC sound effects database to LibriSpeech utterances,\nshowing consistent improvements over state-of-the-art baselines under\nmismatched noise conditions. Examples are available at our project webpage."}
{"id": "2510.04359", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04359", "abs": "https://arxiv.org/abs/2510.04359", "authors": ["Minsu Kim", "Walid Saad", "Dour Calin"], "title": "Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data", "comment": "Submitted to IEEE TWC", "summary": "In 6G wireless networks, multi-modal ML models can be leveraged to enable\nsituation-aware network decisions in dynamic environments. However, trained ML\nmodels often fail to generalize under domain shifts when training and test data\ndistributions are different because they often focus on modality-specific\nspurious features. In practical wireless systems, domain shifts occur\nfrequently due to dynamic channel statistics, moving obstacles, or hardware\nconfiguration. Thus, there is a need for learning frameworks that can achieve\nrobust generalization under scarce multi-modal data in wireless networks. In\nthis paper, a novel and data-efficient two-phase learning framework is proposed\nto improve generalization performance in unseen and unfamiliar wireless\nenvironments with minimal amount of multi-modal data. In the first stage, a\nphysics-based loss function is employed to enable each BS to learn the physics\nunderlying its wireless environment captured by multi-modal data. The\ndata-efficiency of the physics-based loss function is analytically\ninvestigated. In the second stage, collaborative domain adaptation is proposed\nto leverage the wireless environment knowledge of multiple BSs to guide\nunder-performing BSs under domain shift. Specifically, domain-similarity-aware\nmodel aggregation is proposed to utilize the knowledge of BSs that experienced\nsimilar domains. To validate the proposed framework, a new dataset generation\nframework is developed by integrating CARLA and MATLAB-based mmWave channel\nmodeling to predict mmWave RSS. Simulation results show that the proposed\nphysics-based training requires only 13% of data samples to achieve the same\nperformance as a state-of-the-art baseline that does not use physics-based\ntraining. Moreover, the proposed collaborative domain adaptation needs only 25%\nof data samples and 20% of FLOPs to achieve the convergence compared to\nbaselines."}
{"id": "2510.04459", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04459", "abs": "https://arxiv.org/abs/2510.04459", "authors": ["Samuel A. Verburg", "Efren Fernandez-Grande", "Peter Gerstoft"], "title": "Differentiable physics for sound field reconstruction", "comment": "28 pages plus references, 8 figures, full journal paper", "summary": "Sound field reconstruction involves estimating sound fields from a limited\nnumber of spatially distributed observations. This work introduces a\ndifferentiable physics approach for sound field reconstruction, where the\ninitial conditions of the wave equation are approximated with a neural network,\nand the differential operator is computed with a differentiable numerical\nsolver. The use of a numerical solver enables a stable network training while\nenforcing the physics as a strong constraint, in contrast to conventional\nphysics-informed neural networks, which include the physics as a constraint in\nthe loss function. We introduce an additional sparsity-promoting constraint to\nachieve meaningful solutions even under severe undersampling conditions.\nExperiments demonstrate that the proposed approach can reconstruct sound fields\nunder extreme data scarcity, achieving higher accuracy and better convergence\ncompared to physics-informed neural networks."}
{"id": "2510.04251", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04251", "abs": "https://arxiv.org/abs/2510.04251", "authors": ["Zhao Ren", "Rathi Adarshi Rammohan", "Kevin Scheck", "Tanja Schultz"], "title": "Machine Unlearning in Speech Emotion Recognition via Forget Set Alone", "comment": "Submitted to ICASSP 2026", "summary": "Speech emotion recognition aims to identify emotional states from speech\nsignals and has been widely applied in human-computer interaction, education,\nhealthcare, and many other fields. However, since speech data contain rich\nsensitive information, partial data can be required to be deleted by speakers\ndue to privacy concerns. Current machine unlearning approaches largely depend\non data beyond the samples to be forgotten. However, this reliance poses\nchallenges when data redistribution is restricted and demands substantial\ncomputational resources in the context of big data. We propose a novel\nadversarial-attack-based approach that fine-tunes a pre-trained speech emotion\nrecognition model using only the data to be forgotten. The experimental results\ndemonstrate that the proposed approach can effectively remove the knowledge of\nthe data to be forgotten from the model, while preserving high model\nperformance on the test set for emotion recognition."}
{"id": "2510.04402", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04402", "abs": "https://arxiv.org/abs/2510.04402", "authors": ["Binyu Lu", "Matthias Frey", "Stark Draper", "Jingge Zhu"], "title": "Low-Rank-Based Approximate Computation with Memristors", "comment": "5 pages, 2 figures, submitted to an IEEE conference for possible\n  publication", "summary": "Memristor crossbars enable vector-matrix multiplication (VMM), and are\npromising for low-power applications. However, it can be difficult to write the\nmemristor conductance values exactly. To improve the accuracy of VMM, we\npropose a scheme based on low-rank matrix approximation. Specifically, singular\nvalue decomposition (SVD) is first applied to obtain a low-rank approximation\nof the target matrix, which is then factored into a pair of smaller matrices.\nSubsequently, a two-step serial VMM is executed, where the stochastic write\nerrors are mitigated through step-wise averaging. To evaluate the performance\nof the proposed scheme, we derive a general expression for the resulting\ncomputation error and provide an asymptotic analysis under a prescribed\nsingular-value profile, which reveals how the error scales with matrix size and\nrank. Both analytical and numerical results confirm the superiority of the\nproposed scheme compared with the benchmark scheme."}
{"id": "2510.04593", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.04593", "abs": "https://arxiv.org/abs/2510.04593", "authors": ["Wenhao Guan", "Zhikang Niu", "Ziyue Jiang", "Kaidi Wang", "Peijie Chen", "Qingyang Hong", "Lin Li", "Xie Chen"], "title": "UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated promising performance in both\nautomatic speech recognition (ASR) and text-to-speech (TTS) systems, gradually\nbecoming the mainstream approach. However, most current approaches address\nthese tasks separately rather than through a unified framework. This work aims\nto integrate these two tasks into one unified model. Although discrete speech\ntokenization enables joint modeling, its inherent information loss limits\nperformance in both recognition and generation. In this work, we present\nUniVoice, a unified LLM framework through continuous representations that\nseamlessly integrates speech recognition and synthesis within a single model.\nOur approach combines the strengths of autoregressive modeling for speech\nrecognition with flow matching for high-quality generation. To mitigate the\ninherent divergence between autoregressive and flow-matching models, we further\ndesign a dual attention mechanism, which switches between a causal mask for\nrecognition and a bidirectional attention mask for synthesis. Furthermore, the\nproposed text-prefix-conditioned speech infilling method enables high-fidelity\nzero-shot voice cloning. Experimental results demonstrate that our method can\nachieve or exceed current single-task modeling methods in both ASR and\nzero-shot TTS tasks. This work explores new possibilities for end-to-end speech\nunderstanding and generation."}
{"id": "2510.04339", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04339", "abs": "https://arxiv.org/abs/2510.04339", "authors": ["Christian Limberg", "Fares Schulz", "Zhe Zhang", "Stefan Weinzierl"], "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space", "comment": "8 pages, accepted to the Proceedings of the 28-th Int. Conf. on\n  Digital Audio Effects (DAFx25) - demo: https://pgesam.faresschulz.com", "summary": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com"}
{"id": "2510.04409", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04409", "abs": "https://arxiv.org/abs/2510.04409", "authors": ["Samyadip Sarkar", "Arunashish Datta", "David Yang", "Mayukh Nath", "Shovan Maity", "Shreyas Sen"], "title": "Effect of nearby Metals on Electro-Quasistatic Human Body Communication", "comment": "18 pages, 25 Figures, 2 Tables, 5 Appendix", "summary": "In recent decades Human Body Communication has emerged as a promising\nalternative to traditional radio wave communication, utilizing the body's\nconductive properties for low-power connectivity among wearables. This method\nharnesses the human body as an energy-efficient channel for data transmission\nwithin the electro-quasistatic frequency range, enabling advancements in\nhuman-machine interaction. While prior work has noted the role of parasitic\nreturn paths in such capacitively coupled systems, the influence of surrounding\nmetallic objects on these paths, which are critical for EQS wireless signaling,\nhas not been fully explored. This paper fills that gap with a structured study\nof how various conducting objects, from non-grounded (floating) metals and\ngrounded metals to enclosed metallic environments such as elevators and cars,\naffect the body-communication channel. We present a theoretical framework\nsupported by finite element method simulations and experiments with wearable\ndevices. Results show that metallic objects within 20 cm of devices can reduce\ntransmission loss by about 10 dB. When a device ground connects to a grounded\nmetallic object, channel gain can increase by at least 20 dB. Contact area\nduring touch-based interactions with grounded metals produces contact-impedance\ndependent high-pass channel characteristics. Proximity to metallic objects\nintroduces variability within a critical distance, with grounded metals\nproducing a larger overall effect than floating metals. These findings improve\nunderstanding of body-centric communication links and inform design for\nhealthcare, consumer electronics, defense, and industrial applications."}
{"id": "2510.04463", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04463", "abs": "https://arxiv.org/abs/2510.04463", "authors": ["Takashi Maekaku", "Keita Goto", "Jinchuan Tian", "Yusuke Shinohara", "Shinji Watanabe"], "title": "Evaluating Self-Supervised Speech Models via Text-Based LLMS", "comment": "Accepted to ASRU 2025", "summary": "Self-Supervised Learning (SSL) has gained traction for its ability to learn\nrich representations with low labeling costs, applicable across diverse\ndownstream tasks. However, assessing the downstream-task performance remains\nchallenging due to the cost of extra training and evaluation. Existing methods\nfor task-agnostic evaluation also require extra training or hyperparameter\ntuning. We propose a novel evaluation metric using large language models\n(LLMs). By inputting discrete token sequences and minimal domain cues derived\nfrom SSL models into LLMs, we obtain the mean log-likelihood; these cues guide\nin-context learning, rendering the score more reliable without extra training\nor hyperparameter tuning. Experimental results show a correlation between\nLLM-based scores and automatic speech recognition task. Additionally, our\nfindings reveal that LLMs not only functions as an SSL evaluation tools but\nalso provides inference-time embeddings that are useful for speaker\nverification task."}
{"id": "2510.04413", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04413", "abs": "https://arxiv.org/abs/2510.04413", "authors": ["Muhammad Umar Farooq Qaisar", "Weijie Yuan", "Onur Günlü", "Taneli Riihonen", "Yuanhao Cui", "Lin Zhang", "Nuria Gonzalez-Prelcic", "Marco Di Renzo", "Zhu Han"], "title": "The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems", "comment": "28 pages, 6 figures, and 5 tables", "summary": "The commencement of the sixth-generation (6G) wireless networks represents a\nfundamental shift in the integration of communication and sensing technologies\nto support next-generation applications. Integrated sensing and communication\n(ISAC) is a key concept in this evolution, enabling end-to-end support for both\ncommunication and sensing within a unified framework. It enhances spectrum\nefficiency, reduces latency, and supports diverse use cases, including smart\ncities, autonomous systems, and perceptive environments. This tutorial provides\na comprehensive overview of ISAC's role in 6G networks, beginning with its\nevolution since 5G and the technical drivers behind its adoption. Core\nprinciples and system variations of ISAC are introduced, followed by an\nin-depth discussion of the enabling technologies that facilitate its practical\ndeployment. The paper further analyzes current research directions to highlight\nkey challenges, open issues, and emerging trends. Design insights and\nrecommendations are also presented to support future development and\nimplementation. This work ultimately try to address three central questions:\nWhy is ISAC essential for 6G? What innovations does it bring? How will it shape\nthe future of wireless communication?"}
{"id": "2510.04577", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04577", "abs": "https://arxiv.org/abs/2510.04577", "authors": ["Juncheng Wang", "Chao Xu", "Cheng Yu", "Zhe Hu", "Haoyu Xie", "Guoqi Yu", "Lei Shang", "Shujun Wang"], "title": "Language Model Based Text-to-Audio Generation: Anti-Causally Aligned Collaborative Residual Transformers", "comment": "Accepted to EMNLP 2025", "summary": "While language models (LMs) paired with residual vector quantization (RVQ)\ntokenizers have shown promise in text-to-audio (T2A) generation, they still lag\nbehind diffusion-based models by a non-trivial margin. We identify a critical\ndilemma underpinning this gap: incorporating more RVQ layers improves audio\nreconstruction fidelity but exceeds the generation capacity of conventional\nLMs. To address this, we first analyze RVQ dynamics and uncover two key\nlimitations: 1) orthogonality of features across RVQ layers hinders effective\nLMs training, and 2) descending semantic richness in tokens from deeper RVQ\nlayers exacerbates exposure bias during autoregressive decoding. Based on these\ninsights, we propose Siren, a novel LM-based framework that employs multiple\nisolated transformers with causal conditioning and anti-causal alignment via\nreinforcement learning. Extensive experiments demonstrate that Siren\noutperforms both existing LM-based and diffusion-based T2A systems, achieving\nstate-of-the-art results. By bridging the representational strengths of LMs\nwith the fidelity demands of audio synthesis, our approach repositions LMs as\ncompetitive contenders against diffusion models in T2A tasks. Moreover, by\naligning audio representations with linguistic structures, Siren facilitates a\npromising pathway toward unified multi-modal generation frameworks."}
{"id": "2510.04492", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04492", "abs": "https://arxiv.org/abs/2510.04492", "authors": ["Zhou Zhang", "Yizhu Wang", "Saman Atapattu", "Sumei Sun"], "title": "Joint Probing and Scheduling for Cache-Aided Hybrid Satellite-Terrestrial Networks", "comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan", "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."}
{"id": "2510.04738", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04738", "abs": "https://arxiv.org/abs/2510.04738", "authors": ["Baher Mohammad", "Magauiya Zhussip", "Stamatios Lefkimmiatis"], "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba", "comment": null, "summary": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and\nSynthesis), a novel autoregressive architecture for text-conditioned voice\nediting and high-fidelity text-to-speech (TTS) synthesis, built on a\ncross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in\nspeech editing and very competitive results in zero-shot TTS, while not being\nexplicitly trained on the latter task, outperforming leading autoregressive and\ndiffusion models on diverse, real-world audio. By integrating Mamba for\nefficient audio sequence modeling with cross-attention for precise\ntext-acoustic alignment, MAVE enables context-aware voice editing with\nexceptional naturalness and speaker consistency. In pairwise human evaluations\non a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%\nof listeners rated MAVE - edited speech as perceptually equal to the original,\nwhile 24.8% prefered the original and 18.0% MAVE - demonstrating that in the\nmajority of cases edits are indistinguishable from the source. MAVE compares\nfavorably with VoiceCraft and FluentSpeech both on pairwise comparisons and\nstandalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE\nexceeds VoiceCraft in both speaker similarity and naturalness, without\nrequiring multiple inference runs or post-processing. Remarkably, these quality\ngains come with a significantly lower memory cost and approximately the same\nlatency: MAVE requires ~6x less memory than VoiceCraft during inference on\nutterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch\nsize 1). Our results demonstrate that MAVE establishes a new standard for\nflexible, high-fidelity voice editing and synthesis through the synergistic\nintegration of structured state-space modeling and cross-modal attention."}
{"id": "2510.04530", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04530", "abs": "https://arxiv.org/abs/2510.04530", "authors": ["Gayathri Shekar", "Saman Atapattu", "Prathapasinghe Dharmawansa", "Kandeepan Sithamparanathan"], "title": "Performance Analysis for Multi-User Holographic MIMO Downlink with Matched Filter Precoding", "comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan", "summary": "Holographic MIMO (HMIMO) has emerged as a promising solution for future\nwireless systems by enabling ultra-dense, spatially continuous antenna\ndeployments. While prior studies have primarily focused on electromagnetic (EM)\nmodeling or simulation-based performance analysis, a rigorous\ncommunication-theoretic framework remains largely unexplored. This paper\npresents the first analytical performance study of a multi-user HMIMO downlink\nsystem with matched filter (MF) precoding - a low-complexity baseline scheme.\nBy incorporating multipath propagation, mutual coupling, and element\nexcitation, we derive a novel closed-form expression for the MF\nsignal-to-interference-plus-noise ratio (SINR) using an equivalent random\nvariable model. Leveraging bivariate gamma distributions, we then develop\ntractable throughput approximations under full, partial, and no channel state\ninformation (CSI) scenarios. Additionally, we formulate a max-min beamforming\nproblem to benchmark optimal user fairness performance. Numerical results\nvalidate the accuracy of the proposed framework and reveal that MF precoding\nachieves competitive performance with strong robustness to low SINR and CSI\nuncertainty."}
{"id": "2510.04600", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04600", "abs": "https://arxiv.org/abs/2510.04600", "authors": ["Meidong Xia", "Zhenyao He", "Wei Xu", "Yongming Huang", "Derrick Wing Kwan Ng", "Naofal Al-Dhahir"], "title": "Coordinated Beamforming for Networked Integrated Communication and Multi-TMT Localization", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Networked integrated sensing and communication (ISAC) has gained significant\nattention as a promising technology for enabling next-generation wireless\nsystems. To further enhance networked ISAC, delegating the reception of sensing\nsignals to dedicated target monitoring terminals (TMTs) instead of base\nstations (BSs) offers significant advantages in terms of sensing capability and\ndeployment flexibility. Despite its potential, the coordinated beamforming\ndesign for networked integrated communication and time-of-arrival (ToA)-based\nmulti-TMT localization remains largely unexplored. In this paper, we present a\ncomprehensive study to fill this gap. Specifically, we first establish signal\nmodels for both communication and localization, and, for the first time, derive\na closed-form Cram\\'er-Rao lower bound (CRLB) to characterize the localization\nperformance. Subsequently, we exploit this CRLB to formulate two optimization\nproblems, focusing on sensing-centric and communication-centric criteria,\nrespectively. For the sensing-centric problem, we develop a globally optimal\nalgorithm based on semidefinite relaxation (SDR) when each BS is equipped with\nmore antennas than the total number of communication users. While for the\ncommunication-centric problem, we design a globally optimal algorithm for the\nsingle-BS case using bisection search. For the general case of both problems,\nwe propose a unified successive convex approximation (SCA)-based algorithm,\nwhich is suboptimal yet efficient, and further extend it from single-target\nscenarios to more practical multi-target scenarios. Finally, simulation results\ndemonstrate the effectiveness of our proposed algorithms, reveal the intrinsic\nperformance trade-offs between communication and localization, and further show\nthat deploying more TMTs is always preferable to deploying more BSs in\nnetworked ISAC systems."}
{"id": "2510.04734", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04734", "abs": "https://arxiv.org/abs/2510.04734", "authors": ["Juan Vidal Alegría"], "title": "Dimensionally-Efficient Transmission and Storage of Unitary Matrices", "comment": "13 pages, 10 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Unitary matrices are the basis of a large number of signal processing\napplications. In many of these applications, finding ways to efficiently store,\nand even transmit these matrices, can significantly reduce memory and\nthroughput requirements. In this work, we study the problem of efficient\ntransmission and storage of unitary matrices. Specifically, we explicitly\nderive a dimensionally-efficient parametrization (DEP) for unitary matrices\nthat allows identifying them with sequences of real numbers, where the\ndimension coincides with the dimension of the unitary group where they lie. We\nalso characterize its inverse map that allows retrieving the original unitary\nmatrices from their DEP. The proposed approach effectively allows halving the\ndimension with respect to naively considering all the entries of each unitary\nmatrix, thus reducing the resources required to store and transmit these\nmatrices. Furthermore, we show that the sequence of real numbers associated to\nthe proposed DEP is bounded, and we delimit the interval where these numbers\nare contained, facilitating the implementation of quantization approaches with\nlimited distortion. On the other hand, we outline ways to further reduce the\ndimension of the DEP when considering more restrictive constraints for matrices\nthat show up in certain applications. The numerical results showcase the\npotential of the proposed approach in general settings, as well as in three\nspecific applications of current interest for wireless communications research."}
{"id": "2510.04744", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04744", "abs": "https://arxiv.org/abs/2510.04744", "authors": ["Wali Ullah Khan", "Chandan Kumar Sheemar", "Eva Lagunas", "Xingwang Li", "Symeon Chatzinotas", "Petar Popovski", "Zhu Han"], "title": "Multilayer Non-Terrestrial Networks with Spectrum Access aided by Beyond-Diagonal RIS", "comment": "13, 10", "summary": "In this work, we study a multi-user NTN in which a satellite serves as the\nprimary network and a high-altitude platform station (HAPS) operates as the\nsecondary network, acting as a cognitive radio. To reduce the cost, complexity,\nand power consumption of conventional antenna arrays, we equip the HAPS with a\ntransmissive BD-RIS antenna front end. We then formulate a joint optimization\nproblem for the BD-RIS phase response and the HAPS transmit power allocation\nunder strict per-user interference temperature constraints. To tackle the\nresulting highly nonconvex problem, we propose an alternating-optimization\nframework: the power-allocation subproblem admits a closed-form,\nwater-filling-type solution derived from the Karush-Kuhn-Tucker (KKT)\nconditions, while the BD-RIS configuration is refined via Riemannian manifold\noptimization. Simulation results show significant gains in data rate and\ninterference suppression over diagonal RIS-assisted benchmarks, establishing\nBD-RIS as a promising enabler for future multilayer NTNs."}
{"id": "2510.04745", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04745", "abs": "https://arxiv.org/abs/2510.04745", "authors": ["Lucas Sempéré", "Yue Bi", "Yue Wu", "Pengwenlong Gu", "Selma Boumerdassi"], "title": "Interference Alignment for Multi-cluster Over-the-Air Computation", "comment": null, "summary": "One of the main challenges facing Internet of Things (IoT) networks is\nmanaging interference caused by the large number of devices communicating\nsimultaneously, particularly in multi-cluster networks where multiple devices\nsimultaneously transmit to their respective receiver. Over-the-Air Computation\n(AirComp) has emerged as a promising solution for efficient real-time data\naggregation, yet its performance suffers in dense, interference-limited\nenvironments. To address this, we propose a novel Interference Alignment (IA)\nscheme tailored for up-link AirComp systems. Unlike previous approaches, the\nproposed method scales to an arbitrary number $\\sf K$ of clusters and enables\neach cluster to exploit half of the available channels, instead of only\n$\\tfrac{1}{\\sf K}$ as in time-sharing. In addition, we develop schemes tailored\nto scenarios where users are shared between adjacent clusters."}
{"id": "2510.04913", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04913", "abs": "https://arxiv.org/abs/2510.04913", "authors": ["Andreas Bathelt", "Benjamin Deutschmann", "Hyeon Seok Rou", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Peter Vouras"], "title": "The IEEE Signal Processing Society's Leading Role in Developing Standards for Computational Imaging and Sensing: Part II", "comment": "Submitted to the IEEE for possible publication", "summary": "In every imaging or sensing application, the physical hardware creates\nconstraints that must be overcome or they limit system performance. Techniques\nthat leverage additional degrees of freedom can effectively extend performance\nbeyond the inherent physical capabilities of the hardware. An example includes\nsynchronizing distributed sensors so as to synthesize a larger aperture for\nremote sensing applications. An additional example is integrating the\ncommunication and sensing functions in a wireless system through the clever\ndesign of waveforms and optimized resource management. As these technologies\nmature beyond the conceptual and prototype phase they will ultimately\ntransition to the commercial market. Here, standards play a critical role in\nensuring success. Standards ensure interoperability between systems\nmanufactured by different vendors and define industry best practices for\nvendors and customers alike. The Signal Processing Society of the Institute for\nElectrical and Electronics Engineers (IEEE) plays a leading role in developing\nhigh-quality standards for computational sensing technologies through the\nworking groups of the Synthetic Aperture Standards Committee (SASC). In this\ncolumn we highlight the standards activities of the P3383 Performance Metrics\nfor Integrated Sensing and Communication (ISAC) Systems Working Group and the\nP3343 Spatio-Temporal Synchronization of a Synthetic Aperture of Distributed\nSensors Working Group."}
{"id": "2510.04924", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04924", "abs": "https://arxiv.org/abs/2510.04924", "authors": ["Ardavan Rahimian"], "title": "Steady-State Spread Bounds for Graph Diffusion via Laplacian Regularisation", "comment": null, "summary": "We study how far a diffusion process on a graph can drift from a designed\nstarting pattern when that pattern is produced using Laplacian regularisation.\nUnder standard stability conditions for undirected, entrywise nonnegative\ngraphs, we give a closed-form, instance-specific upper bound on the\nsteady-state spread, measured as the relative change between the final and\ninitial profiles. The bound separates two effects: (i) an irreducible term\ndetermined by the graph's maximum node degree, and (ii) a design-controlled\nterm that shrinks as the regularisation strength increases (following an\ninverse square-root law). This leads to a simple design rule: given any target\nlimit on spread, one can choose a sufficient regularisation strength in closed\nform. Although one motivating application is array beamforming, where the\ninitial pattern is the squared magnitude of the beamformer weights, the result\napplies to any scenario that first enforces Laplacian smoothness and then\nevolves by linear diffusion on a graph. Overall, the guarantee is\nnon-asymptotic, easy to compute, and certifies how much steady-state deviation\ncan occur."}
{"id": "2510.05000", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.05000", "abs": "https://arxiv.org/abs/2510.05000", "authors": ["Xiang-Gen Xia"], "title": "My First Five Years of Faculty Career at the University of Delaware", "comment": null, "summary": "In this short article, I would like to briefly summarize my research in the\nfirst 5 years in my university academia life in USA. I think that my research\nresults obtained in these 5 years are the best in my career, at least which I\nlike the most by myself. I wish that my experience in my junior academia career\ncould be of some help to young researchers."}
{"id": "2510.03728", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03728", "abs": "https://arxiv.org/abs/2510.03728", "authors": ["Kuang Yuan", "Yang Gao", "Xilin Li", "Xinhao Mei", "Syavosh Zadissa", "Tarun Pruthi", "Saeed Bagheri Sereshki"], "title": "Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation", "comment": null, "summary": "Acoustic scene classification (ASC) models on edge devices typically operate\nunder fixed class assumptions, lacking the transferability needed for\nreal-world applications that require adaptation to new or refined acoustic\ncategories. We propose ContrastASC, which learns generalizable acoustic scene\nrepresentations by structuring the embedding space to preserve semantic\nrelationships between scenes, enabling adaptation to unseen categories without\nretraining. Our approach combines supervised contrastive fine-tuning of\npre-trained models with contrastive representation distillation to transfer\nthis structured knowledge to compact student models. Our evaluation shows that\nContrastASC demonstrates improved few-shot adaptation to unseen categories\nwhile maintaining strong closed-set performance."}
{"id": "2510.04339", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04339", "abs": "https://arxiv.org/abs/2510.04339", "authors": ["Christian Limberg", "Fares Schulz", "Zhe Zhang", "Stefan Weinzierl"], "title": "Pitch-Conditioned Instrument Sound Synthesis From an Interactive Timbre Latent Space", "comment": "8 pages, accepted to the Proceedings of the 28-th Int. Conf. on\n  Digital Audio Effects (DAFx25) - demo: https://pgesam.faresschulz.com", "summary": "This paper presents a novel approach to neural instrument sound synthesis\nusing a two-stage semi-supervised learning framework capable of generating\npitch-accurate, high-quality music samples from an expressive timbre latent\nspace. Existing approaches that achieve sufficient quality for music production\noften rely on high-dimensional latent representations that are difficult to\nnavigate and provide unintuitive user experiences. We address this limitation\nthrough a two-stage training paradigm: first, we train a pitch-timbre\ndisentangled 2D representation of audio samples using a Variational\nAutoencoder; second, we use this representation as conditioning input for a\nTransformer-based generative model. The learned 2D latent space serves as an\nintuitive interface for navigating and exploring the sound landscape. We\ndemonstrate that the proposed method effectively learns a disentangled timbre\nspace, enabling expressive and controllable audio generation with reliable\npitch conditioning. Experimental results show the model's ability to capture\nsubtle variations in timbre while maintaining a high degree of pitch accuracy.\nThe usability of our method is demonstrated in an interactive web application,\nhighlighting its potential as a step towards future music production\nenvironments that are both intuitive and creatively empowering:\nhttps://pgesam.faresschulz.com"}

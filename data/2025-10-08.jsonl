{"id": "2510.05191", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05191", "abs": "https://arxiv.org/abs/2510.05191", "authors": ["Jonathan Svirsky", "Ofir Lindenbaum", "Uri Shaham"], "title": "Provable Speech Attributes Conversion via Latent Independence", "comment": null, "summary": "While signal conversion and disentangled representation learning have shown\npromise for manipulating data attributes across domains such as audio, image,\nand multimodal generation, existing approaches, especially for speech style\nconversion, are largely empirical and lack rigorous theoretical foundations to\nguarantee reliable and interpretable control. In this work, we propose a\ngeneral framework for speech attribute conversion, accompanied by theoretical\nanalysis and guarantees under reasonable assumptions. Our framework builds on a\nnon-probabilistic autoencoder architecture with an independence constraint\nbetween the predicted latent variable and the target controllable variable.\nThis design ensures a consistent signal transformation, conditioned on an\nobserved style variable, while preserving the original content and modifying\nthe desired attribute. We further demonstrate the versatility of our method by\nevaluating it on speech styles, including speaker identity and emotion.\nQuantitative evaluations confirm the effectiveness and generality of the\nproposed approach."}
{"id": "2510.05295", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.05295", "abs": "https://arxiv.org/abs/2510.05295", "authors": ["M. Sajid", "Deepanshu Gupta", "Yash Modi", "Sanskriti Jain", "Harshith Jai Surya Ganji", "A. Rahaman", "Harshvardhan Choudhary", "Nasir Saleem", "Amir Hussain", "M. Tanveer"], "title": "AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement", "comment": null, "summary": "In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation\nExchange Architecture with Cross-Attention and Squeezeformer for Speech\nEnhancement), a progressive bimodal framework tailored for audio-visual speech\nenhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual\ncues by employing a U-Net-based 1D convolutional encoder for audio and a Swin\nTransformer V2 for efficient and expressive visual feature extraction. Central\nto the architecture is a novel bidirectional cross-attention mechanism, which\nfacilitates deep contextual fusion between modalities, enabling rich and\ncomplementary representation learning. To capture temporal dependencies within\nthe fused embeddings, a stack of lightweight Squeezeformer blocks combining\nconvolutional and attention modules is introduced. The enhanced embeddings are\nthen decoded via a U-Net-style decoder for direct waveform reconstruction,\nensuring perceptually consistent and intelligible speech output. Experimental\nevaluations demonstrate the effectiveness of AUREXA-SE, achieving significant\nperformance improvements over noisy baselines, with STOI of 0.516, PESQ of\n1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at\nhttps://github.com/mtanveer1/AVSEC-4-Challenge-2025."}
{"id": "2510.05542", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05542", "abs": "https://arxiv.org/abs/2510.05542", "authors": ["Xilin Jiang", "Hannes Gamper", "Sebastian Braun"], "title": "Sci-Phi: A Large Language Model Spatial Audio Descriptor", "comment": null, "summary": "Acoustic scene perception involves describing the type of sounds, their\ntiming, their direction and distance, as well as their loudness and\nreverberation. While audio language models excel in sound recognition,\nsingle-channel input fundamentally limits spatial understanding. This work\npresents Sci-Phi, a spatial audio large language model with dual spatial and\nspectral encoders that estimates a complete parameter set for all sound sources\nand the surrounding environment. Learning from over 4,000 hours of synthetic\nfirst-order Ambisonics recordings including metadata, Sci-Phi enumerates and\ndescribes up to four directional sound sources in one pass, alongside\nnon-directional background sounds and room characteristics. We evaluate the\nmodel with a permutation-invariant protocol and 15 metrics covering content,\nlocation, timing, loudness, and reverberation, and analyze its robustness\nacross source counts, signal-to-noise ratios, reverberation levels, and\nchallenging mixtures of acoustically, spatially, or temporally similar sources.\nNotably, Sci-Phi generalizes to real room impulse responses with only minor\nperformance degradation. Overall, this work establishes the first audio LLM\ncapable of full spatial-scene description, with strong potential for real-world\ndeployment. Demo: https://sci-phi-audio.github.io/demo"}
{"id": "2510.05696", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05696", "abs": "https://arxiv.org/abs/2510.05696", "authors": ["Antoine Teissier", "Marie Tahon", "Nicolas Dugué", "Aghilas Sini"], "title": "Sparse deepfake detection promotes better disentanglement", "comment": null, "summary": "Due to the rapid progress of speech synthesis, deepfake detection has become\na major concern in the speech processing community. Because it is a critical\ntask, systems must not only be efficient and robust, but also provide\ninterpretable explanations. Among the different approaches for explainability,\nwe focus on the interpretation of latent representations. In such paper, we\nfocus on the last layer of embeddings of AASIST, a deepfake detection\narchitecture. We use a TopK activation inspired by SAEs on this layer to obtain\nsparse representations which are used in the decision process. We demonstrate\nthat sparse deepfake detection can improve detection performance, with an EER\nof 23.36% on ASVSpoof5 test set, with 95% of sparsity. We then show that these\nrepresentations provide better disentanglement, using completeness and\nmodularity metrics based on mutual information. Notably, some attacks are\ndirectly encoded in the latent space."}
{"id": "2510.05438", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05438", "abs": "https://arxiv.org/abs/2510.05438", "authors": ["Alexander James Fernandes", "Ioannis Psaromiligkos"], "title": "Model-based Deep Learning for Joint RIS Phase Shift Compression and WMMSE Beamforming", "comment": "5 pages, 4 figures, submitted to IEEE Communications Letters", "summary": "A model-based deep learning (DL) architecture is proposed for reconfigurable\nintelligent surface (RIS)-assisted multi-user communications to reduce the\noverhead of transmitting phase shift information from the access point (AP) to\nthe RIS controller. The phase shifts are computed at the AP, which has access\nto the channel state information, and then encoded into a compressed binary\ncontrol message that is sent to the RIS controller for element configuration.\nTo help reduce beamformer mismatches due to phase shift compression errors, the\nbeamformer is updated using weighted minimum mean square error (WMMSE) based on\nthe effective channel resulting from the actual (decompressed) RIS reflection\ncoefficients. By unrolling the iterative WMMSE algorithm as part of the\nwireless communication informed DL architecture, joint phase shift compression\nand WMMSE beamforming can be trained end-to-end. Simulations show that\naccounting for phase shift compression errors during beamforming significantly\nimproves the sum-rate performance, even when the number of control bits is\nlower than the number of RIS elements."}
{"id": "2510.05305", "categories": ["eess.AS", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05305", "abs": "https://arxiv.org/abs/2510.05305", "authors": ["Xi Xuan", "Xuechen Liu", "Wenxin Zhang", "Yi-Cheng Lin", "Xiaojian Lin", "Tomi Kinnunen"], "title": "WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection", "comment": "Submitted to ICASSP 2026", "summary": "Modern front-end design for speech deepfake detection relies on full\nfine-tuning of large pre-trained models like XLSR. However, this approach is\nnot parameter-efficient and may lead to suboptimal generalization to realistic,\nin-the-wild data types. To address these limitations, we introduce a new family\nof parameter-efficient front-ends that fuse prompt-tuning with classical signal\nprocessing transforms. These include FourierPT-XLSR, which uses the Fourier\nTransform, and two variants based on the Wavelet Transform: WSPT-XLSR and\nPartial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture\ncombining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based\nback-end. This design injects multi-resolution features into the prompt\nembeddings, which enhances the localization of subtle synthetic artifacts\nwithout altering the frozen XLSR parameters. Experimental results demonstrate\nthat WaveSP-Net outperforms several state-of-the-art models on two new and\nchallenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable\nparameters and notable performance gains. The code and models are available at\nhttps://github.com/xxuan-acoustics/WaveSP-Net."}
{"id": "2510.05749", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.05749", "abs": "https://arxiv.org/abs/2510.05749", "authors": ["Haoxun Li", "Yuqing Sun", "Hanlei Shi", "Yu Liu", "Leyuan Qu", "Taihao Li"], "title": "MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics for Speech Emotion Recognition", "comment": "Under review for ICASSP 2026", "summary": "Continuous dimensional speech emotion recognition captures affective\nvariation along valence, arousal, and dominance, providing finer-grained\nrepresentations than categorical approaches. Yet most multimodal methods rely\nsolely on global transcripts, leading to two limitations: (1) all words are\ntreated equally, overlooking that emphasis on different parts of a sentence can\nshift emotional meaning; (2) only surface lexical content is represented,\nlacking higher-level interpretive cues. To overcome these issues, we propose\nMSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition),\nwhich augments acoustic features with three complementary levels of textual\nsemantics--Local Emphasized Semantics (LES), Global Semantics (GS), and\nExtended Semantics (ES). These are integrated via an intra-modal gated fusion\nand a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE).\nExperiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves\ndimensional prediction, demonstrating the effectiveness of enriched semantic\nfusion for SER."}
{"id": "2510.05559", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05559", "abs": "https://arxiv.org/abs/2510.05559", "authors": ["Md Rakibul Mowla", "Sukhbinder Kumar", "Ariane E. Rhone", "Brian J. Dlouhy", "Christopher K. Kovach"], "title": "Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model", "comment": "6 pages, 6 figures", "summary": "Statistical significance testing of neural coherence is essential for\ndistinguishing genuine cross-signal coupling from spurious correlations. A\nwidely accepted approach uses surrogate-based inference, where null\ndistributions are generated via time-shift or phase-randomization procedures.\nWhile effective, these methods are computationally expensive and yield discrete\np-values that can be unstable near decision thresholds, limiting scalability to\nlarge EEG/iEEG datasets. We introduce and validate a parametric alternative\nbased on a generalized linear model (GLM) applied to complex-valued\ntime--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio\ntest. Using real respiration belt traces as a driver and simulated neural\nsignals contaminated with broadband Gaussian noise, we perform dense sweeps of\nground-truth coherence and compare GLM-based inference against\ntime-shift/phase-randomized surrogate testing under matched conditions. GLM\nachieved comparable or superior sensitivity while producing continuous, stable\np-values and a substantial computational advantage. At 80% detection power, GLM\ndetects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to\nan approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be\nnearly 200x faster than surrogate approaches. These results establish GLM-based\ninference on complex time--frequency coefficients as a robust, scalable\nalternative to surrogate testing, enabling efficient analysis of large EEG/iEEG\ndatasets across channels, frequencies, and participants."}
{"id": "2510.05478", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05478", "abs": "https://arxiv.org/abs/2510.05478", "authors": ["Haoyu Zhang", "Jiaxian Guo", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "AQA-TTRL: Self-Adaptation in Audio Question Answering with Test-Time Reinforcement Learning", "comment": "5 pages, 4 figures, Submitted to ICASSP 2026", "summary": "Large Audio Language Models (LALMs) demonstrate impressive general audio\nunderstanding, but once deployed, they are static and fail to improve with new\nreal-world audio data. As traditional supervised fine-tuning is costly, we\nintroduce a novel framework for test-time audio understanding, AQA-TTRL, where\nan LALM evolves on-the-fly using only unlabeled test data. It first generates\npseudo-labels from the prediction via majority voting, then optimizes the model\nvia reinforcement learning. To handle the inherent noise in these\nself-generated labels, we introduce a confidence-based weighting method to\nadjust training signals. Furthermore, a multiple-attempt sampling operation\nmitigates advantage collapse and stabilizes training. On the MMAU\n(test-mini/test), MMAR, and MMSU benchmarks, AQA-TTRL achieves significant\naverage improvements of 4.42% for the Qwen2.5-Omni 7B model and 11.04% for the\n3B model. Notably, the adapted 3B model consistently outperforms the direct\ninference of the unadapted 7B model, highlighting the effectiveness of\npreviously unexplored test-time adaptations in audio understanding."}
{"id": "2510.05756", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05756", "abs": "https://arxiv.org/abs/2510.05756", "authors": ["Aleksandr Lukoianov", "Anssi Klapuri"], "title": "Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music", "comment": "Accepted to WASPAA 2025", "summary": "Whereas chord transcription has received considerable attention during the\npast couple of decades, far less work has been devoted to transcribing and\nencoding the rhythmic patterns that occur in a song. The topic is especially\nrelevant for instruments such as the rhythm guitar, which is typically played\nby strumming rhythmic patterns that repeat and vary over time. However, in many\ncases one cannot objectively define a single \"right\" rhythmic pattern for a\ngiven song section. To create a dataset with well-defined ground-truth labels,\nwe asked expert musicians to transcribe the rhythmic patterns in 410 popular\nsongs and record cover versions where the guitar tracks followed those\ntranscriptions. To transcribe the strums and their corresponding rhythmic\npatterns, we propose a three-step framework. Firstly, we perform approximate\nstem separation to extract the guitar part from the polyphonic mixture.\nSecondly, we detect individual strums within the separated guitar audio, using\na pre-trained foundation model (MERT) as a backbone. Finally, we carry out a\npattern-decoding process in which the transcribed sequence of guitar strums is\nrepresented by patterns drawn from an expert-curated vocabulary. We show that\nit is possible to transcribe the rhythmic patterns of the guitar track in\npolyphonic music with quite high accuracy, producing a representation that is\nhuman-readable and includes automatically detected bar lines and time signature\nmarkers. We perform ablation studies and error analysis and propose a set of\nevaluation metrics to assess the accuracy and readability of the predicted\nrhythmic pattern sequence."}
{"id": "2510.05826", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05826", "abs": "https://arxiv.org/abs/2510.05826", "authors": ["Pubudu L. Indrasiri", "Bipasha Kashyap", "Pubudu N. Pathirana"], "title": "Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals", "comment": "14pages, 2 figures", "summary": "Biomedical signals provide insights into various conditions affecting the\nhuman body. Beyond diagnostic capabilities, these signals offer a deeper\nunderstanding of how specific organs respond to an individual's emotions and\nfeelings. For instance, ECG data can reveal changes in heart rate variability\nlinked to emotional arousal, stress levels, and autonomic nervous system\nactivity. This data offers a window into the physiological basis of our\nemotional states. Recent advancements in the field diverge from conventional\napproaches by leveraging the power of advanced transformer architectures, which\nsurpass traditional machine learning and deep learning methods. We begin by\nassessing the effectiveness of the Vision Transformer (ViT), a forefront model\nin image classification, for identifying emotions in imaged ECGs. Following\nthis, we present and evaluate an improved version of ViT, integrating both CNN\nand SE blocks, aiming to bolster performance on imaged ECGs associated with\nemotion detection. Our method unfolds in two critical phases: first, we apply\nadvanced preprocessing techniques for signal purification and converting\nsignals into interpretable images using continuous wavelet transform and power\nspectral density analysis; second, we unveil a performance-boosted vision\ntransformer architecture, cleverly enhanced with convolutional neural network\ncomponents, to adeptly tackle the challenges of emotion recognition. Our\nmethodology's robustness and innovation were thoroughly tested using ECG data\nfrom the YAAD and DREAMER datasets, leading to remarkable outcomes. For the\nYAAD dataset, our approach outperformed existing state-of-the-art methods in\nclassifying seven unique emotional states, as well as in valence and arousal\nclassification. Similarly, in the DREAMER dataset, our method excelled in\ndistinguishing between valence, arousal and dominance, surpassing current\nleading techniques."}
{"id": "2510.05619", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05619", "abs": "https://arxiv.org/abs/2510.05619", "authors": ["Akshay Anand", "Chenxu Guo", "Cheol Jun Cho", "Jiachen Lian", "Gopala Anumanchipalli"], "title": "Teaching Machines to Speak Using Articulatory Control", "comment": null, "summary": "Current speech production systems predominantly rely on large transformer\nmodels that operate as black boxes, providing little interpretability or\ngrounding in the physical mechanisms of human speech. We address this\nlimitation by proposing a new framework: speech generation through explicit\narticulatory control. This reframes speech as a motor control task similar to\nrobotic manipulation. Our approach uses reinforcement learning to train a\npolicy that directly controls the movements of vocal tract articulators, such\nas the tongue, lips, and jaw, to produce syllable-level speech. Specifically,\nwe employ the Proximal Policy Optimization algorithm to learn optimal\narticulatory movements based on acoustic feedback provided by our audio\nperceiver, Sylber. The resulting articulatory trajectories are decoded into\naudio using SPARC, a pre-trained articulatory-to-speech decoder. We train this\nframework on six target syllables, and it demonstrates successful convergence,\nwith similarity scores between the policy-generated audio and the target\nsyllables exceeding 0.85. Accurate human transcription of the audio for\nsyllables such as \"please\", \"loot\", and \"cat\" demonstrates the intelligibility\nof this framework."}
{"id": "2510.05758", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.05758", "abs": "https://arxiv.org/abs/2510.05758", "authors": ["Haoxun Li", "Yu Liu", "Yuqing Sun", "Hanlei Shi", "Leyuan Qu", "Taihao Li"], "title": "EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS", "comment": "Under review for ICASSP 2026", "summary": "Recent LLM-based TTS systems achieve strong quality and zero-shot ability,\nbut lack fine-grained emotional control due to their reliance on discrete\nspeech tokens. Existing approaches either limit emotions to categorical labels\nor cannot generalize to LLM-based architectures. We propose EMORL-TTS\n(Fine-grained Emotion-controllable TTS with Reinforcement Learning), a\nframework that unifies global intensity control in the VAD space with local\nemphasis regulation. Our method combines supervised fine-tuning with\nreinforcement learning guided by task-specific rewards for emotion category,\nintensity, and emphasis. Moreover, we further investigate how emphasis\nplacement modulates fine-grained emotion intensity. Experiments show that\nEMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis\nclarity, while preserving synthesis quality comparable to strong LLM-based\nbaselines."}
{"id": "2510.05834", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05834", "abs": "https://arxiv.org/abs/2510.05834", "authors": ["Tony Lindeberg"], "title": "Time-causal and time-recursive wavelets", "comment": "23 pages, 8 figures", "summary": "When to apply wavelet analysis to real-time temporal signals, where the\nfuture cannot be accessed, it is essential to base all the steps in the signal\nprocessing pipeline on computational mechanisms that are truly time-causal.\n  This paper describes how a time-causal wavelet analysis can be performed\nbased on concepts developed in the area of temporal scale-space theory,\noriginating from a complete classification of temporal smoothing kernels that\nguarantee non-creation of new structures from finer to coarser temporal scale\nlevels. By necessity, convolution with truncated exponential kernels in cascade\nconstitutes the only permissable class of kernels, as well as their temporal\nderivatives as a natural complement to fulfil the admissibility conditions of\nwavelet representations. For a particular way of choosing the time constants in\nthe resulting infinite convolution of truncated exponential kernels, to ensure\ntemporal scale covariance and thus self-similarity over temporal scales, we\ndescribe how mother wavelets can be chosen as temporal derivatives of the\nresulting time-causal limit kernel.\n  By developing connections between wavelet theory and scale-space theory, we\ncharacterize and quantify how the continuous scaling properties transfer to the\ndiscrete implementation, demonstrating how the proposed time-causal wavelet\nrepresentation can reflect the duration of locally dominant temporal structures\nin the input signals.\n  We propose that this notion of time-causal wavelet analysis could be a\nvaluable tool for signal processing tasks, where streams of signals are to be\nprocessed in real time, specifically for signals that may contain local\nvariations over a rich span of temporal scales, or more generally for analysing\nphysical or biophysical temporal phenomena, where a fully time-causal analysis\nis called for to be physically realistic."}
{"id": "2510.05718", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05718", "abs": "https://arxiv.org/abs/2510.05718", "authors": ["Rui Wang", "Liping Chen", "Kong Aik Lee", "Zhengpeng Zha", "Zhenhua Ling"], "title": "Investigation of perception inconsistency in speaker embedding for asynchronous voice anonymization", "comment": null, "summary": "Given the speech generation framework that represents the speaker attribute\nwith an embedding vector, asynchronous voice anonymization can be achieved by\nmodifying the speaker embedding derived from the original speech. However, the\ninconsistency between machine and human perceptions of the speaker attribute\nwithin the speaker embedding remains unexplored, limiting its performance in\nasynchronous voice anonymization. To this end, this study investigates this\ninconsistency via modifications to speaker embedding in the speech generation\nprocess. Experiments conducted on the FACodec and Diff-HierVC speech generation\nmodels discover a subspace whose removal alters machine perception while\npreserving its human perception of the speaker attribute in the generated\nspeech. With these findings, an asynchronous voice anonymization is developed,\nachieving 100% human perception preservation rate while obscuring the machine\nperception. Audio samples can be found in\nhttps://voiceprivacy.github.io/speaker-embedding-eigen-decomposition/."}
{"id": "2510.05828", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05828", "abs": "https://arxiv.org/abs/2510.05828", "authors": ["Christian Marinoni", "Riccardo Fosco Gramaccioni", "Kazuki Shimada", "Takashi Shibuya", "Yuki Mitsufuji", "Danilo Comminiello"], "title": "StereoSync: Spatially-Aware Stereo Audio Generation from Video", "comment": "Accepted at IJCNN 2025", "summary": "Although audio generation has been widely studied over recent years,\nvideo-aligned audio generation still remains a relatively unexplored frontier.\nTo address this gap, we introduce StereoSync, a novel and efficient model\ndesigned to generate audio that is both temporally synchronized with a\nreference video and spatially aligned with its visual context. Moreover,\nStereoSync also achieves efficiency by leveraging pretrained foundation models,\nreducing the need for extensive training while maintaining high-quality\nsynthesis. Unlike existing methods that primarily focus on temporal\nsynchronization, StereoSync introduces a significant advancement by\nincorporating spatial awareness into video-aligned audio generation. Indeed,\ngiven an input video, our approach extracts spatial cues from depth maps and\nbounding boxes, using them as cross-attention conditioning in a diffusion-based\naudio generation model. Such an approach allows StereoSync to go beyond simple\nsynchronization, producing stereo audio that dynamically adapts to the spatial\nstructure and movement of a video scene. We evaluate StereoSync on Walking The\nMaps, a curated dataset comprising videos from video games that feature\nanimated characters walking through diverse environments. Experimental results\ndemonstrate the ability of StereoSync to achieve both temporal and spatial\nalignment, advancing the state of the art in video-to-audio generation and\nresulting in a significantly more immersive and realistic audio experience."}
{"id": "2510.06173", "categories": ["eess.SP", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.06173", "abs": "https://arxiv.org/abs/2510.06173", "authors": ["Shuixin Li", "Jiecheng Chen", "Qingtang Jiang", "Lin Li"], "title": "Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves", "comment": null, "summary": "To analyze signals with rapid frequency variations or transient components,\nthe time-reassigned synchrosqueezing transform (TSST) and its variants have\nbeen recently proposed. Unlike the traditional synchrosqueezing transform, TSST\nsqueezes the time-frequency (TF) coefficients along the group delay (GD)\ntrajectories rather than the instantaneous frequency trajectories. Although\nTSST methods perform well in analyzing transient signals, they are\nfundamentally limited in processing multicomponent signals with intersecting GD\ncurves. This limitation compromises the accuracy of both feature extraction and\nsignal component recovery, thereby significantly reducing the interpretability\nof time-frequency representations (TFRs). This is particularly problematic in\nbroadband signal processing systems, where the linearity of the phase response\nis critical and precise measurement of group delay dispersion (GDD) is\nessential.\n  Motivated by the superior capability of frequency-domain signal modeling in\ncharacterizing rapidly frequency-varying signals, this paper proposes a novel\nthree-dimensional time-frequency-group delay dispersion (TF-GDD) representation\nbased on the frequency-domain chirplet transform. A subsequent time-reassigned\nsynchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to\nachieve a sharper TF-GDD distribution and more accurate GD estimation. For mode\nretrieval, a novel frequency-domain group signal separation operation (FGSSO)\nis proposed.The theoretical contributions include a derivation of the\napproximation error for the GD and GDD reference functions and an establishment\nof the error bounds for FGSSO-based mode retrieval. Experimental results\ndemonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and\nretrieve modes--even for modes with intersecting GD trajectories."}
{"id": "2510.05757", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05757", "abs": "https://arxiv.org/abs/2510.05757", "authors": ["Jingqi Sun", "Shulin He", "Ruizhe Pang", "Zhong-Qiu Wang"], "title": "Neural Forward Filtering for Speaker-Image Separation", "comment": "in submission", "summary": "We address monaural multi-speaker-image separation in reverberant conditions,\naiming at separating mixed speakers but preserving the reverberation of each\nspeaker. A straightforward approach for this task is to directly train\nend-to-end DNN systems to predict the reverberant speech of each speaker based\non the input mixture. Although effective, this approach does not explicitly\nexploit the physical constraint that reverberant speech can be reproduced by\nconvolving the direct-path signal with a linear filter. To address this, we\npropose CxNet, a two-DNN system with a neural forward filtering module in\nbetween. The first DNN is trained to jointly predict the direct-path signal and\nreverberant speech. Based on the direct-path estimate, the neural forward\nfiltering module estimates the linear filter, and the estimated filter is then\nconvolved with the direct-path estimate to obtain another estimate of\nreverberant speech, which is utilized as a discriminative feature to help the\nsecond DNN better estimate the reverberant speech. By explicitly modeling the\nlinear filter, CxNet could leverage the physical constraint between the\ndirect-path signal and reverberant speech to capture crucial information about\nreverberation tails. Evaluation results on the SMS-WSJ dataset show the\neffectiveness of the proposed algorithms."}
{"id": "2510.05829", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05829", "abs": "https://arxiv.org/abs/2510.05829", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Eleonora Grassucci", "Giordano Cicchetti", "Aurelio Uncini", "Danilo Comminiello"], "title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders", "comment": "Acepted at IJCNN 2025", "summary": "In this work, we present FoleyGRAM, a novel approach to video-to-audio\ngeneration that emphasizes semantic conditioning through the use of aligned\nmultimodal encoders. Building on prior advancements in video-to-audio\ngeneration, FoleyGRAM leverages the Gramian Representation Alignment Measure\n(GRAM) to align embeddings across video, text, and audio modalities, enabling\nprecise semantic control over the audio generation process. The core of\nFoleyGRAM is a diffusion-based audio synthesis model conditioned on\nGRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness\nand temporal alignment with the corresponding input video. We evaluate\nFoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio\nmodels. Our experiments demonstrate that aligning multimodal encoders using\nGRAM enhances the system's ability to semantically align generated audio with\nvideo content, advancing the state of the art in video-to-audio synthesis."}
{"id": "2510.05305", "categories": ["eess.AS", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05305", "abs": "https://arxiv.org/abs/2510.05305", "authors": ["Xi Xuan", "Xuechen Liu", "Wenxin Zhang", "Yi-Cheng Lin", "Xiaojian Lin", "Tomi Kinnunen"], "title": "WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection", "comment": "Submitted to ICASSP 2026", "summary": "Modern front-end design for speech deepfake detection relies on full\nfine-tuning of large pre-trained models like XLSR. However, this approach is\nnot parameter-efficient and may lead to suboptimal generalization to realistic,\nin-the-wild data types. To address these limitations, we introduce a new family\nof parameter-efficient front-ends that fuse prompt-tuning with classical signal\nprocessing transforms. These include FourierPT-XLSR, which uses the Fourier\nTransform, and two variants based on the Wavelet Transform: WSPT-XLSR and\nPartial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture\ncombining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based\nback-end. This design injects multi-resolution features into the prompt\nembeddings, which enhances the localization of subtle synthetic artifacts\nwithout altering the frozen XLSR parameters. Experimental results demonstrate\nthat WaveSP-Net outperforms several state-of-the-art models on two new and\nchallenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable\nparameters and notable performance gains. The code and models are available at\nhttps://github.com/xxuan-acoustics/WaveSP-Net."}
{"id": "2510.05922", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05922", "abs": "https://arxiv.org/abs/2510.05922", "authors": ["Vitor Magno de O. S. Bezerra", "Gabriel F. A. Bastos", "Jugurta Montalvão"], "title": "Revisiting MFCCs: Evidence for Spectral-Prosodic Coupling", "comment": "5 pages, 3 figures, ISCMI 2025", "summary": "Mel-frequency cepstral coefficients (MFCCs) are an important feature in\nspeech processing. A deeper understanding of their properties can contribute to\nthe work that is being done with both classical and deep learning models. This\nstudy challenges the long-held assumption that MFCCs lack relevant temporal\ninformation by investigating their relationship with speech prosody. Using a\nnull hypothesis significance testing framework, a systematic assessment is made\nabout the statistical independence between MFCCs and the three prosodic\nfeatures: energy, fundamental frequency (F0), and voicing. The results\ndemonstrate that it is statistically implausible that the MFCCs are independent\nof any of these three prosodic features. This finding suggests that MFCCs\ninherently carry valuable prosodic information, which can inform the design of\nfuture models in speech analysis and recognition."}
{"id": "2510.05875", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.05875", "abs": "https://arxiv.org/abs/2510.05875", "authors": ["Jiahao Mei", "Xuenan Xu", "Zeyu Xie", "Zihao Zheng", "Ye Tao", "Yue Ding", "Mengyue Wu"], "title": "LARA-Gen: Enabling Continuous Emotion Control for Music Generation Models via Latent Affective Representation Alignment", "comment": null, "summary": "Recent advances in text-to-music models have enabled coherent music\ngeneration from text prompts, yet fine-grained emotional control remains\nunresolved. We introduce LARA-Gen, a framework for continuous emotion control\nthat aligns the internal hidden states with an external music understanding\nmodel through Latent Affective Representation Alignment (LARA), enabling\neffective training. In addition, we design an emotion control module based on a\ncontinuous valence-arousal space, disentangling emotional attributes from\ntextual content and bypassing the bottlenecks of text-based prompting.\nFurthermore, we establish a benchmark with a curated test set and a robust\nEmotion Predictor, facilitating objective evaluation of emotional\ncontrollability in music generation. Extensive experiments demonstrate that\nLARA-Gen achieves continuous, fine-grained control of emotion and significantly\noutperforms baselines in both emotion adherence and music quality. Generated\nsamples are available at https://nieeim.github.io/LARA-Gen/."}
{"id": "2510.05934", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05934", "abs": "https://arxiv.org/abs/2510.05934", "authors": ["Huang-Cheng Chou", "Chi-Chun Lee"], "title": "Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions", "comment": "PhD Thesis; ACLCLP Doctoral Dissertation Award -- Honorable Mention", "summary": "Over the past two decades, speech emotion recognition (SER) has received\ngrowing attention. To train SER systems, researchers collect emotional speech\ndatabases annotated by crowdsourced or in-house raters who select emotions from\npredefined categories. However, disagreements among raters are common.\nConventional methods treat these disagreements as noise, aggregating labels\ninto a single consensus target. While this simplifies SER as a single-label\ntask, it ignores the inherent subjectivity of human emotion perception. This\ndissertation challenges such assumptions and asks: (1) Should minority\nemotional ratings be discarded? (2) Should SER systems learn from only a few\nindividuals' perceptions? (3) Should SER systems predict only one emotion per\nsample?\n  Psychological studies show that emotion perception is subjective and\nambiguous, with overlapping emotional boundaries. We propose new modeling and\nevaluation perspectives: (1) Retain all emotional ratings and represent them\nwith soft-label distributions. Models trained on individual annotator ratings\nand jointly optimized with standard SER systems improve performance on\nconsensus-labeled tests. (2) Redefine SER evaluation by including all emotional\ndata and allowing co-occurring emotions (e.g., sad and angry). We propose an\n``all-inclusive rule'' that aggregates all ratings to maximize diversity in\nlabel representation. Experiments on four English emotion databases show\nsuperior performance over majority and plurality labeling. (3) Construct a\npenalization matrix to discourage unlikely emotion combinations during\ntraining. Integrating it into loss functions further improves performance.\nOverall, embracing minority ratings, multiple annotators, and multi-emotion\npredictions yields more robust and human-aligned SER systems."}
{"id": "2510.05881", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05881", "abs": "https://arxiv.org/abs/2510.05881", "authors": ["Ping-Yi Chen", "Chih-Pin Tan", "Yi-Hsuan Yang"], "title": "Segment-Factorized Full-Song Generation on Symbolic Piano Music", "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: AI for Music", "summary": "We propose the Segmented Full-Song Model (SFS) for symbolic full-song\ngeneration. The model accepts a user-provided song structure and an optional\nshort seed segment that anchors the main idea around which the song is\ndeveloped. By factorizing a song into segments and generating each one through\nselective attention to related segments, the model achieves higher quality and\nefficiency compared to prior work. To demonstrate its suitability for human-AI\ninteraction, we further wrap SFS into a web application that enables users to\niteratively co-create music on a piano roll with customizable structures and\nflexible ordering."}
{"id": "2510.06201", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.06201", "abs": "https://arxiv.org/abs/2510.06201", "authors": ["Mingxuan Wang", "Satoshi Nakamura"], "title": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling", "comment": "5 pages, 3 figures. Submitted to IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Machine Speech Chain, simulating the human perception-production loop, proves\neffective in jointly improving ASR and TTS. We propose TokenChain, a fully\ndiscrete speech chain coupling semantic-token ASR with a two-stage TTS: an\nautoregressive text-to-semantic model co-trained with ASR and a\nmasked-generative semantic-to-acoustic model for synthesis only. End-to-end\nfeedback across the text interface is enabled with straight-through\nargmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight\naveraging. Ablations examine optimal temperature schedules for in- and\ncross-domain transfer. Evaluation reveals TokenChain surpasses baseline\naccuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with\nstable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by\n31% on TED-LIUM with minimal forgetting, showing that chain learning remains\neffective with token interfaces and models."}
{"id": "2510.05984", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05984", "abs": "https://arxiv.org/abs/2510.05984", "authors": ["Tao Zhu", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning", "comment": "Accepted for publication by Proceedings of the 2025 ACM Multimedia\n  Asia Conference(MMAsia '25)", "summary": "Diffusion models have demonstrated remarkable performance in speech\nsynthesis, but typically require multi-step sampling, resulting in low\ninference efficiency. Recent studies address this issue by distilling diffusion\nmodels into consistency models, enabling efficient one-step generation.\nHowever, these approaches introduce additional training costs and rely heavily\non the performance of pre-trained teacher models. In this paper, we propose\nECTSpeech, a simple and effective one-step speech synthesis framework that, for\nthe first time, incorporates the Easy Consistency Tuning (ECT) strategy into\nspeech synthesis. By progressively tightening consistency constraints on a\npre-trained diffusion model, ECTSpeech achieves high-quality one-step\ngeneration while significantly reducing training complexity. In addition, we\ndesign a multi-scale gate module (MSGate) to enhance the denoiser's ability to\nfuse features at different scales. Experimental results on the LJSpeech dataset\ndemonstrate that ECTSpeech achieves audio quality comparable to\nstate-of-the-art methods under single-step sampling, while substantially\nreducing the model's training cost and complexity."}
{"id": "2510.05542", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05542", "abs": "https://arxiv.org/abs/2510.05542", "authors": ["Xilin Jiang", "Hannes Gamper", "Sebastian Braun"], "title": "Sci-Phi: A Large Language Model Spatial Audio Descriptor", "comment": null, "summary": "Acoustic scene perception involves describing the type of sounds, their\ntiming, their direction and distance, as well as their loudness and\nreverberation. While audio language models excel in sound recognition,\nsingle-channel input fundamentally limits spatial understanding. This work\npresents Sci-Phi, a spatial audio large language model with dual spatial and\nspectral encoders that estimates a complete parameter set for all sound sources\nand the surrounding environment. Learning from over 4,000 hours of synthetic\nfirst-order Ambisonics recordings including metadata, Sci-Phi enumerates and\ndescribes up to four directional sound sources in one pass, alongside\nnon-directional background sounds and room characteristics. We evaluate the\nmodel with a permutation-invariant protocol and 15 metrics covering content,\nlocation, timing, loudness, and reverberation, and analyze its robustness\nacross source counts, signal-to-noise ratios, reverberation levels, and\nchallenging mixtures of acoustically, spatially, or temporally similar sources.\nNotably, Sci-Phi generalizes to real room impulse responses with only minor\nperformance degradation. Overall, this work establishes the first audio LLM\ncapable of full spatial-scene description, with strong potential for real-world\ndeployment. Demo: https://sci-phi-audio.github.io/demo"}
{"id": "2510.06072", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06072", "abs": "https://arxiv.org/abs/2510.06072", "authors": ["Akshay Muppidi", "Martin Radfar"], "title": "EmoHRNet: High-Resolution Neural Network Based Speech Emotion Recognition", "comment": null, "summary": "Speech emotion recognition (SER) is pivotal for enhancing human-machine\ninteractions. This paper introduces \"EmoHRNet\", a novel adaptation of\nHigh-Resolution Networks (HRNet) tailored for SER. The HRNet structure is\ndesigned to maintain high-resolution representations from the initial to the\nfinal layers. By transforming audio samples into spectrograms, EmoHRNet\nleverages the HRNet architecture to extract high-level features. EmoHRNet's\nunique architecture maintains high-resolution representations throughout,\ncapturing both granular and overarching emotional cues from speech signals. The\nmodel outperforms leading models, achieving accuracies of 92.45% on RAVDESS,\n80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new\nbenchmark in the SER domain."}
{"id": "2510.05756", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05756", "abs": "https://arxiv.org/abs/2510.05756", "authors": ["Aleksandr Lukoianov", "Anssi Klapuri"], "title": "Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music", "comment": "Accepted to WASPAA 2025", "summary": "Whereas chord transcription has received considerable attention during the\npast couple of decades, far less work has been devoted to transcribing and\nencoding the rhythmic patterns that occur in a song. The topic is especially\nrelevant for instruments such as the rhythm guitar, which is typically played\nby strumming rhythmic patterns that repeat and vary over time. However, in many\ncases one cannot objectively define a single \"right\" rhythmic pattern for a\ngiven song section. To create a dataset with well-defined ground-truth labels,\nwe asked expert musicians to transcribe the rhythmic patterns in 410 popular\nsongs and record cover versions where the guitar tracks followed those\ntranscriptions. To transcribe the strums and their corresponding rhythmic\npatterns, we propose a three-step framework. Firstly, we perform approximate\nstem separation to extract the guitar part from the polyphonic mixture.\nSecondly, we detect individual strums within the separated guitar audio, using\na pre-trained foundation model (MERT) as a backbone. Finally, we carry out a\npattern-decoding process in which the transcribed sequence of guitar strums is\nrepresented by patterns drawn from an expert-curated vocabulary. We show that\nit is possible to transcribe the rhythmic patterns of the guitar track in\npolyphonic music with quite high accuracy, producing a representation that is\nhuman-readable and includes automatically detected bar lines and time signature\nmarkers. We perform ablation studies and error analysis and propose a set of\nevaluation metrics to assess the accuracy and readability of the predicted\nrhythmic pattern sequence."}
{"id": "2510.06204", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06204", "abs": "https://arxiv.org/abs/2510.06204", "authors": ["Christopher Mitcheltree", "Hao Hao Tan", "Joshua D. Reiss"], "title": "Modulation Discovery with Differentiable Digital Signal Processing", "comment": "Accepted to WASPAA 2025 (best paper award candidate). Code, audio\n  samples, and plugins can be found at\n  https://christhetree.github.io/mod_discovery/", "summary": "Modulations are a critical part of sound design and music production,\nenabling the creation of complex and evolving audio. Modern synthesizers\nprovide envelopes, low frequency oscillators (LFOs), and more parameter\nautomation tools that allow users to modulate the output with ease. However,\ndetermining the modulation signals used to create a sound is difficult, and\nexisting sound-matching / parameter estimation systems are often\nuninterpretable black boxes or predict high-dimensional framewise parameter\nvalues without considering the shape, structure, and routing of the underlying\nmodulation curves. We propose a neural sound-matching approach that leverages\nmodulation extraction, constrained control signal parameterizations, and\ndifferentiable digital signal processing (DDSP) to discover the modulations\npresent in a sound. We demonstrate the effectiveness of our approach on highly\nmodulated synthetic and real audio samples, its applicability to different DDSP\nsynth architectures, and investigate the trade-off it incurs between\ninterpretability and sound-matching accuracy. We make our code and audio\nsamples available and provide the trained DDSP synths in a VST plugin."}
{"id": "2510.05828", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05828", "abs": "https://arxiv.org/abs/2510.05828", "authors": ["Christian Marinoni", "Riccardo Fosco Gramaccioni", "Kazuki Shimada", "Takashi Shibuya", "Yuki Mitsufuji", "Danilo Comminiello"], "title": "StereoSync: Spatially-Aware Stereo Audio Generation from Video", "comment": "Accepted at IJCNN 2025", "summary": "Although audio generation has been widely studied over recent years,\nvideo-aligned audio generation still remains a relatively unexplored frontier.\nTo address this gap, we introduce StereoSync, a novel and efficient model\ndesigned to generate audio that is both temporally synchronized with a\nreference video and spatially aligned with its visual context. Moreover,\nStereoSync also achieves efficiency by leveraging pretrained foundation models,\nreducing the need for extensive training while maintaining high-quality\nsynthesis. Unlike existing methods that primarily focus on temporal\nsynchronization, StereoSync introduces a significant advancement by\nincorporating spatial awareness into video-aligned audio generation. Indeed,\ngiven an input video, our approach extracts spatial cues from depth maps and\nbounding boxes, using them as cross-attention conditioning in a diffusion-based\naudio generation model. Such an approach allows StereoSync to go beyond simple\nsynchronization, producing stereo audio that dynamically adapts to the spatial\nstructure and movement of a video scene. We evaluate StereoSync on Walking The\nMaps, a curated dataset comprising videos from video games that feature\nanimated characters walking through diverse environments. Experimental results\ndemonstrate the ability of StereoSync to achieve both temporal and spatial\nalignment, advancing the state of the art in video-to-audio generation and\nresulting in a significantly more immersive and realistic audio experience."}
{"id": "2510.06201", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.06201", "abs": "https://arxiv.org/abs/2510.06201", "authors": ["Mingxuan Wang", "Satoshi Nakamura"], "title": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling", "comment": "5 pages, 3 figures. Submitted to IEEE International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Machine Speech Chain, simulating the human perception-production loop, proves\neffective in jointly improving ASR and TTS. We propose TokenChain, a fully\ndiscrete speech chain coupling semantic-token ASR with a two-stage TTS: an\nautoregressive text-to-semantic model co-trained with ASR and a\nmasked-generative semantic-to-acoustic model for synthesis only. End-to-end\nfeedback across the text interface is enabled with straight-through\nargmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight\naveraging. Ablations examine optimal temperature schedules for in- and\ncross-domain transfer. Evaluation reveals TokenChain surpasses baseline\naccuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with\nstable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by\n31% on TED-LIUM with minimal forgetting, showing that chain learning remains\neffective with token interfaces and models."}
{"id": "2510.05829", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05829", "abs": "https://arxiv.org/abs/2510.05829", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Eleonora Grassucci", "Giordano Cicchetti", "Aurelio Uncini", "Danilo Comminiello"], "title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders", "comment": "Acepted at IJCNN 2025", "summary": "In this work, we present FoleyGRAM, a novel approach to video-to-audio\ngeneration that emphasizes semantic conditioning through the use of aligned\nmultimodal encoders. Building on prior advancements in video-to-audio\ngeneration, FoleyGRAM leverages the Gramian Representation Alignment Measure\n(GRAM) to align embeddings across video, text, and audio modalities, enabling\nprecise semantic control over the audio generation process. The core of\nFoleyGRAM is a diffusion-based audio synthesis model conditioned on\nGRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness\nand temporal alignment with the corresponding input video. We evaluate\nFoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio\nmodels. Our experiments demonstrate that aligning multimodal encoders using\nGRAM enhances the system's ability to semantically align generated audio with\nvideo content, advancing the state of the art in video-to-audio synthesis."}
{"id": "2510.05881", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05881", "abs": "https://arxiv.org/abs/2510.05881", "authors": ["Ping-Yi Chen", "Chih-Pin Tan", "Yi-Hsuan Yang"], "title": "Segment-Factorized Full-Song Generation on Symbolic Piano Music", "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: AI for Music", "summary": "We propose the Segmented Full-Song Model (SFS) for symbolic full-song\ngeneration. The model accepts a user-provided song structure and an optional\nshort seed segment that anchors the main idea around which the song is\ndeveloped. By factorizing a song into segments and generating each one through\nselective attention to related segments, the model achieves higher quality and\nefficiency compared to prior work. To demonstrate its suitability for human-AI\ninteraction, we further wrap SFS into a web application that enables users to\niteratively co-create music on a piano roll with customizable structures and\nflexible ordering."}
{"id": "2510.05984", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05984", "abs": "https://arxiv.org/abs/2510.05984", "authors": ["Tao Zhu", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning", "comment": "Accepted for publication by Proceedings of the 2025 ACM Multimedia\n  Asia Conference(MMAsia '25)", "summary": "Diffusion models have demonstrated remarkable performance in speech\nsynthesis, but typically require multi-step sampling, resulting in low\ninference efficiency. Recent studies address this issue by distilling diffusion\nmodels into consistency models, enabling efficient one-step generation.\nHowever, these approaches introduce additional training costs and rely heavily\non the performance of pre-trained teacher models. In this paper, we propose\nECTSpeech, a simple and effective one-step speech synthesis framework that, for\nthe first time, incorporates the Easy Consistency Tuning (ECT) strategy into\nspeech synthesis. By progressively tightening consistency constraints on a\npre-trained diffusion model, ECTSpeech achieves high-quality one-step\ngeneration while significantly reducing training complexity. In addition, we\ndesign a multi-scale gate module (MSGate) to enhance the denoiser's ability to\nfuse features at different scales. Experimental results on the LJSpeech dataset\ndemonstrate that ECTSpeech achieves audio quality comparable to\nstate-of-the-art methods under single-step sampling, while substantially\nreducing the model's training cost and complexity."}
{"id": "2510.06204", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06204", "abs": "https://arxiv.org/abs/2510.06204", "authors": ["Christopher Mitcheltree", "Hao Hao Tan", "Joshua D. Reiss"], "title": "Modulation Discovery with Differentiable Digital Signal Processing", "comment": "Accepted to WASPAA 2025 (best paper award candidate). Code, audio\n  samples, and plugins can be found at\n  https://christhetree.github.io/mod_discovery/", "summary": "Modulations are a critical part of sound design and music production,\nenabling the creation of complex and evolving audio. Modern synthesizers\nprovide envelopes, low frequency oscillators (LFOs), and more parameter\nautomation tools that allow users to modulate the output with ease. However,\ndetermining the modulation signals used to create a sound is difficult, and\nexisting sound-matching / parameter estimation systems are often\nuninterpretable black boxes or predict high-dimensional framewise parameter\nvalues without considering the shape, structure, and routing of the underlying\nmodulation curves. We propose a neural sound-matching approach that leverages\nmodulation extraction, constrained control signal parameterizations, and\ndifferentiable digital signal processing (DDSP) to discover the modulations\npresent in a sound. We demonstrate the effectiveness of our approach on highly\nmodulated synthetic and real audio samples, its applicability to different DDSP\nsynth architectures, and investigate the trade-off it incurs between\ninterpretability and sound-matching accuracy. We make our code and audio\nsamples available and provide the trained DDSP synths in a VST plugin."}

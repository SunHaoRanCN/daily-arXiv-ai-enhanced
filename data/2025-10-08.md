<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 5]
- [eess.AS](#eess.AS) [Total: 8]
- [cs.SD](#cs.SD) [Total: 14]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Model-based Deep Learning for Joint RIS Phase Shift Compression and WMMSE Beamforming](https://arxiv.org/abs/2510.05438)
*Alexander James Fernandes,Ioannis Psaromiligkos*

Main category: eess.SP

TL;DR: 提出基于模型的深度学习架构，用于RIS辅助多用户通信，通过压缩相位信息和控制消息传输来减少开销，并利用WMMSE算法更新波束成形器以补偿压缩误差。


<details>
  <summary>Details</summary>
Motivation: 减少从接入点向RIS控制器传输相位信息的开销，解决相位压缩误差导致的波束成形失配问题。

Method: 在AP端计算相位偏移并压缩为二进制控制消息，使用WMMSE算法基于实际RIS反射系数更新波束成形器，将迭代WMMSE算法展开为无线通信感知的深度学习架构进行端到端训练。

Result: 仿真表明，在波束成形过程中考虑相位压缩误差能显著提高和速率性能，即使控制比特数少于RIS单元数时也能保持良好性能。

Conclusion: 该方法通过联合相位压缩和WMMSE波束成形的端到端训练，有效解决了RIS控制开销问题，提高了系统性能。

Abstract: A model-based deep learning (DL) architecture is proposed for reconfigurable
intelligent surface (RIS)-assisted multi-user communications to reduce the
overhead of transmitting phase shift information from the access point (AP) to
the RIS controller. The phase shifts are computed at the AP, which has access
to the channel state information, and then encoded into a compressed binary
control message that is sent to the RIS controller for element configuration.
To help reduce beamformer mismatches due to phase shift compression errors, the
beamformer is updated using weighted minimum mean square error (WMMSE) based on
the effective channel resulting from the actual (decompressed) RIS reflection
coefficients. By unrolling the iterative WMMSE algorithm as part of the
wireless communication informed DL architecture, joint phase shift compression
and WMMSE beamforming can be trained end-to-end. Simulations show that
accounting for phase shift compression errors during beamforming significantly
improves the sum-rate performance, even when the number of control bits is
lower than the number of RIS elements.

</details>


### [2] [Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model](https://arxiv.org/abs/2510.05559)
*Md Rakibul Mowla,Sukhbinder Kumar,Ariane E. Rhone,Brian J. Dlouhy,Christopher K. Kovach*

Main category: eess.SP

TL;DR: 提出了一种基于广义线性模型(GLM)的参数化替代方法，用于神经相干性的统计显著性检验，相比传统的时间平移/相位随机化方法具有更好的计算效率和检测灵敏度。


<details>
  <summary>Details</summary>
Motivation: 传统的基于替代数据的神经相干性统计显著性检验方法计算成本高，产生的p值在决策阈值附近不稳定，限制了在大规模EEG/iEEG数据集上的可扩展性。

Method: 使用广义线性模型(GLM)应用于复数时频系数，采用似然比检验进行统计推断，并与时间平移/相位随机化替代测试进行比较。

Result: GLM方法在80%检测功率下检测阈值为C=0.25，而替代测试需要C=0.49，相当于6-7 dB的信噪比改进，且计算速度比替代方法快近200倍。

Conclusion: 基于复数时频系数的GLM推断是替代测试的稳健、可扩展替代方案，能够高效分析大型EEG/iEEG数据集。

Abstract: Statistical significance testing of neural coherence is essential for
distinguishing genuine cross-signal coupling from spurious correlations. A
widely accepted approach uses surrogate-based inference, where null
distributions are generated via time-shift or phase-randomization procedures.
While effective, these methods are computationally expensive and yield discrete
p-values that can be unstable near decision thresholds, limiting scalability to
large EEG/iEEG datasets. We introduce and validate a parametric alternative
based on a generalized linear model (GLM) applied to complex-valued
time--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio
test. Using real respiration belt traces as a driver and simulated neural
signals contaminated with broadband Gaussian noise, we perform dense sweeps of
ground-truth coherence and compare GLM-based inference against
time-shift/phase-randomized surrogate testing under matched conditions. GLM
achieved comparable or superior sensitivity while producing continuous, stable
p-values and a substantial computational advantage. At 80% detection power, GLM
detects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to
an approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be
nearly 200x faster than surrogate approaches. These results establish GLM-based
inference on complex time--frequency coefficients as a robust, scalable
alternative to surrogate testing, enabling efficient analysis of large EEG/iEEG
datasets across channels, frequencies, and participants.

</details>


### [3] [Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals](https://arxiv.org/abs/2510.05826)
*Pubudu L. Indrasiri,Bipasha Kashyap,Pubudu N. Pathirana*

Main category: eess.SP

TL;DR: 本文提出了一种改进的Vision Transformer架构，结合CNN和SE模块，用于从ECG图像中识别情绪状态，在YAAD和DREAMER数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生物医学信号情绪识别方面存在局限，而先进的transformer架构在图像分类中表现出色，因此探索其在ECG情绪识别中的应用潜力。

Method: 采用两阶段方法：1）使用连续小波变换和功率谱密度分析将ECG信号转换为图像；2）提出结合CNN和SE模块的增强版Vision Transformer架构。

Result: 在YAAD数据集上，该方法在7种情绪状态分类以及效价和唤醒度分类中优于现有技术；在DREAMER数据集上，在效价、唤醒度和支配度分类方面也表现优异。

Conclusion: 结合CNN和SE模块的改进Vision Transformer架构在ECG情绪识别任务中具有显著优势，为生物医学信号的情绪分析提供了有效解决方案。

Abstract: Biomedical signals provide insights into various conditions affecting the
human body. Beyond diagnostic capabilities, these signals offer a deeper
understanding of how specific organs respond to an individual's emotions and
feelings. For instance, ECG data can reveal changes in heart rate variability
linked to emotional arousal, stress levels, and autonomic nervous system
activity. This data offers a window into the physiological basis of our
emotional states. Recent advancements in the field diverge from conventional
approaches by leveraging the power of advanced transformer architectures, which
surpass traditional machine learning and deep learning methods. We begin by
assessing the effectiveness of the Vision Transformer (ViT), a forefront model
in image classification, for identifying emotions in imaged ECGs. Following
this, we present and evaluate an improved version of ViT, integrating both CNN
and SE blocks, aiming to bolster performance on imaged ECGs associated with
emotion detection. Our method unfolds in two critical phases: first, we apply
advanced preprocessing techniques for signal purification and converting
signals into interpretable images using continuous wavelet transform and power
spectral density analysis; second, we unveil a performance-boosted vision
transformer architecture, cleverly enhanced with convolutional neural network
components, to adeptly tackle the challenges of emotion recognition. Our
methodology's robustness and innovation were thoroughly tested using ECG data
from the YAAD and DREAMER datasets, leading to remarkable outcomes. For the
YAAD dataset, our approach outperformed existing state-of-the-art methods in
classifying seven unique emotional states, as well as in valence and arousal
classification. Similarly, in the DREAMER dataset, our method excelled in
distinguishing between valence, arousal and dominance, surpassing current
leading techniques.

</details>


### [4] [Time-causal and time-recursive wavelets](https://arxiv.org/abs/2510.05834)
*Tony Lindeberg*

Main category: eess.SP

TL;DR: 本文提出了一种基于时间尺度空间理论的时间因果小波分析方法，适用于实时时间信号处理，其中所有计算机制都是真正时间因果的。


<details>
  <summary>Details</summary>
Motivation: 在实时时间信号处理中，由于无法访问未来数据，需要确保信号处理流程中的所有步骤都基于真正时间因果的计算机制。

Method: 基于时间尺度空间理论，使用截断指数核的级联卷积及其时间导数作为允许的核函数，选择特定的时间常数确保时间尺度协方差，将母小波选择为时间因果极限核的时间导数。

Result: 建立了小波理论与尺度空间理论之间的联系，量化了连续缩放特性如何转移到离散实现中，展示了所提出的时间因果小波表示如何反映输入信号中局部主导时间结构的持续时间。

Conclusion: 这种时间因果小波分析方法对于需要实时处理信号流的任务是一个有价值的工具，特别是对于包含丰富时间尺度局部变化的信号，或需要物理现实性的物理或生物物理时间现象分析。

Abstract: When to apply wavelet analysis to real-time temporal signals, where the
future cannot be accessed, it is essential to base all the steps in the signal
processing pipeline on computational mechanisms that are truly time-causal.
  This paper describes how a time-causal wavelet analysis can be performed
based on concepts developed in the area of temporal scale-space theory,
originating from a complete classification of temporal smoothing kernels that
guarantee non-creation of new structures from finer to coarser temporal scale
levels. By necessity, convolution with truncated exponential kernels in cascade
constitutes the only permissable class of kernels, as well as their temporal
derivatives as a natural complement to fulfil the admissibility conditions of
wavelet representations. For a particular way of choosing the time constants in
the resulting infinite convolution of truncated exponential kernels, to ensure
temporal scale covariance and thus self-similarity over temporal scales, we
describe how mother wavelets can be chosen as temporal derivatives of the
resulting time-causal limit kernel.
  By developing connections between wavelet theory and scale-space theory, we
characterize and quantify how the continuous scaling properties transfer to the
discrete implementation, demonstrating how the proposed time-causal wavelet
representation can reflect the duration of locally dominant temporal structures
in the input signals.
  We propose that this notion of time-causal wavelet analysis could be a
valuable tool for signal processing tasks, where streams of signals are to be
processed in real time, specifically for signals that may contain local
variations over a rich span of temporal scales, or more generally for analysing
physical or biophysical temporal phenomena, where a fully time-causal analysis
is called for to be physically realistic.

</details>


### [5] [Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves](https://arxiv.org/abs/2510.06173)
*Shuixin Li,Jiecheng Chen,Qingtang Jiang,Lin Li*

Main category: eess.SP

TL;DR: 本文提出了一种基于频域啁啾变换的三维时频-群延迟色散(TF-GDD)表示方法，以及时间重分配同步压缩频域啁啾变换(TSFCT)和频域群信号分离操作(FGSSO)，用于处理具有相交群延迟轨迹的多分量信号。


<details>
  <summary>Details</summary>
Motivation: 传统时间重分配同步压缩变换(TSST)在处理具有相交群延迟曲线的多分量信号时存在根本性限制，这会降低时频表示的可解释性，特别是在宽带信号处理系统中，群延迟色散的精确测量至关重要。

Method: 基于频域信号建模在表征快速频率变化信号方面的优越能力，提出了三维TF-GDD表示，随后引入TSFCT以获得更锐利的TF-GDD分布和更准确的群延迟估计，并提出了FGSSO进行模式恢复。

Result: 实验结果表明，所提出的TSFCT和FGSSO能够有效估计群延迟并恢复模式，即使对于具有相交群延迟轨迹的模式也能有效处理。

Conclusion: 该方法克服了传统TSST在处理相交群延迟轨迹多分量信号时的局限性，为宽带信号处理系统提供了更精确的群延迟色散测量和模式恢复能力。

Abstract: To analyze signals with rapid frequency variations or transient components,
the time-reassigned synchrosqueezing transform (TSST) and its variants have
been recently proposed. Unlike the traditional synchrosqueezing transform, TSST
squeezes the time-frequency (TF) coefficients along the group delay (GD)
trajectories rather than the instantaneous frequency trajectories. Although
TSST methods perform well in analyzing transient signals, they are
fundamentally limited in processing multicomponent signals with intersecting GD
curves. This limitation compromises the accuracy of both feature extraction and
signal component recovery, thereby significantly reducing the interpretability
of time-frequency representations (TFRs). This is particularly problematic in
broadband signal processing systems, where the linearity of the phase response
is critical and precise measurement of group delay dispersion (GDD) is
essential.
  Motivated by the superior capability of frequency-domain signal modeling in
characterizing rapidly frequency-varying signals, this paper proposes a novel
three-dimensional time-frequency-group delay dispersion (TF-GDD) representation
based on the frequency-domain chirplet transform. A subsequent time-reassigned
synchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to
achieve a sharper TF-GDD distribution and more accurate GD estimation. For mode
retrieval, a novel frequency-domain group signal separation operation (FGSSO)
is proposed.The theoretical contributions include a derivation of the
approximation error for the GD and GDD reference functions and an establishment
of the error bounds for FGSSO-based mode retrieval. Experimental results
demonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and
retrieve modes--even for modes with intersecting GD trajectories.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [6] [WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection](https://arxiv.org/abs/2510.05305)
*Xi Xuan,Xuechen Liu,Wenxin Zhang,Yi-Cheng Lin,Xiaojian Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: 提出了一种参数高效的语音深度伪造检测前端，结合提示调优和信号处理变换，并设计了WaveSP-Net架构，在保持低可训练参数的同时显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音深度伪造检测前端设计依赖对大型预训练模型的完全微调，这种方法参数效率低，且对真实场景数据的泛化能力可能不足。

Method: 提出了融合提示调优和经典信号处理变换的参数高效前端，包括FourierPT-XLSR、WSPT-XLSR和Partial-WSPT-XLSR。进一步设计了WaveSP-Net架构，结合Partial-WSPT-XLSR前端和双向Mamba后端，在不改变冻结XLSR参数的情况下注入多分辨率特征。

Result: WaveSP-Net在两个新的挑战性基准测试Deepfake-Eval-2024和SpoofCeleb上优于多个最先进模型，具有低可训练参数和显著性能提升。

Conclusion: 该方法通过参数高效的设计有效提升了语音深度伪造检测性能，特别是在真实场景数据上的泛化能力。

Abstract: Modern front-end design for speech deepfake detection relies on full
fine-tuning of large pre-trained models like XLSR. However, this approach is
not parameter-efficient and may lead to suboptimal generalization to realistic,
in-the-wild data types. To address these limitations, we introduce a new family
of parameter-efficient front-ends that fuse prompt-tuning with classical signal
processing transforms. These include FourierPT-XLSR, which uses the Fourier
Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and
Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture
combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based
back-end. This design injects multi-resolution features into the prompt
embeddings, which enhances the localization of subtle synthetic artifacts
without altering the frozen XLSR parameters. Experimental results demonstrate
that WaveSP-Net outperforms several state-of-the-art models on two new and
challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable
parameters and notable performance gains. The code and models are available at
https://github.com/xxuan-acoustics/WaveSP-Net.

</details>


### [7] [AQA-TTRL: Self-Adaptation in Audio Question Answering with Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.05478)
*Haoyu Zhang,Jiaxian Guo,Yusuke Iwasawa,Yutaka Matsuo*

Main category: eess.AS

TL;DR: 提出了AQA-TTRL框架，让大型音频语言模型能够在测试时使用未标注数据在线进化，通过多数投票生成伪标签，结合置信度加权和多次采样来稳定训练，显著提升了音频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LALM部署后是静态的，无法利用新的真实音频数据来改进性能，而传统的监督微调成本高昂。

Method: 使用测试时的未标注数据，通过多数投票生成伪标签，采用强化学习优化模型，引入置信度加权处理伪标签噪声，使用多次采样避免优势崩溃。

Result: 在MMAU、MMAR和MMSU基准测试上，Qwen2.5-Omni 7B模型平均提升4.42%，3B模型提升11.04%，适应后的3B模型甚至优于未适应的7B模型直接推理。

Conclusion: 测试时适应在音频理解中具有显著效果，能够有效利用未标注数据提升模型性能，且小模型经过适应后可以超越大模型。

Abstract: Large Audio Language Models (LALMs) demonstrate impressive general audio
understanding, but once deployed, they are static and fail to improve with new
real-world audio data. As traditional supervised fine-tuning is costly, we
introduce a novel framework for test-time audio understanding, AQA-TTRL, where
an LALM evolves on-the-fly using only unlabeled test data. It first generates
pseudo-labels from the prediction via majority voting, then optimizes the model
via reinforcement learning. To handle the inherent noise in these
self-generated labels, we introduce a confidence-based weighting method to
adjust training signals. Furthermore, a multiple-attempt sampling operation
mitigates advantage collapse and stabilizes training. On the MMAU
(test-mini/test), MMAR, and MMSU benchmarks, AQA-TTRL achieves significant
average improvements of 4.42% for the Qwen2.5-Omni 7B model and 11.04% for the
3B model. Notably, the adapted 3B model consistently outperforms the direct
inference of the unadapted 7B model, highlighting the effectiveness of
previously unexplored test-time adaptations in audio understanding.

</details>


### [8] [Teaching Machines to Speak Using Articulatory Control](https://arxiv.org/abs/2510.05619)
*Akshay Anand,Chenxu Guo,Cheol Jun Cho,Jiachen Lian,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: 提出了一种通过显式发音控制生成语音的新框架，将语音生成重新定义为类似机器人操作的运动控制任务，使用强化学习直接控制声道发音器官的运动来产生音节级语音。


<details>
  <summary>Details</summary>
Motivation: 当前语音生成系统主要依赖大型Transformer模型，这些模型作为黑箱运行，缺乏可解释性且与人类语音的物理机制脱节。

Method: 使用近端策略优化算法训练策略，基于音频感知器Sylber提供的声学反馈学习最优发音运动，然后通过预训练的发音到语音解码器SPARC将发音轨迹解码为音频。

Result: 在六个目标音节上训练该框架，策略生成音频与目标音节的相似度得分超过0.85，人类对"please"、"loot"、"cat"等音节的准确转录证明了该框架的可懂度。

Conclusion: 该框架成功实现了通过显式发音控制生成可懂语音，为语音生成提供了更可解释和基于物理机制的方法。

Abstract: Current speech production systems predominantly rely on large transformer
models that operate as black boxes, providing little interpretability or
grounding in the physical mechanisms of human speech. We address this
limitation by proposing a new framework: speech generation through explicit
articulatory control. This reframes speech as a motor control task similar to
robotic manipulation. Our approach uses reinforcement learning to train a
policy that directly controls the movements of vocal tract articulators, such
as the tongue, lips, and jaw, to produce syllable-level speech. Specifically,
we employ the Proximal Policy Optimization algorithm to learn optimal
articulatory movements based on acoustic feedback provided by our audio
perceiver, Sylber. The resulting articulatory trajectories are decoded into
audio using SPARC, a pre-trained articulatory-to-speech decoder. We train this
framework on six target syllables, and it demonstrates successful convergence,
with similarity scores between the policy-generated audio and the target
syllables exceeding 0.85. Accurate human transcription of the audio for
syllables such as "please", "loot", and "cat" demonstrates the intelligibility
of this framework.

</details>


### [9] [Investigation of perception inconsistency in speaker embedding for asynchronous voice anonymization](https://arxiv.org/abs/2510.05718)
*Rui Wang,Liping Chen,Kong Aik Lee,Zhengpeng Zha,Zhenhua Ling*

Main category: eess.AS

TL;DR: 该研究通过修改语音生成过程中的说话人嵌入，发现了机器感知与人类感知之间的不一致性，并开发了一种异步语音匿名化方法，在保持人类感知的同时模糊机器感知。


<details>
  <summary>Details</summary>
Motivation: 探索说话人嵌入中机器感知与人类感知之间的不一致性，以提升异步语音匿名化的性能。

Method: 在FACodec和Diff-HierVC语音生成模型上进行实验，通过修改说话人嵌入来发现能改变机器感知但保持人类感知的子空间。

Result: 发现了一个子空间，移除该子空间可以改变机器对说话人属性的感知，同时保持人类感知不变。开发的方法实现了100%的人类感知保持率。

Conclusion: 成功开发了一种异步语音匿名化方法，能有效保护说话人隐私，同时保持语音的自然感知质量。

Abstract: Given the speech generation framework that represents the speaker attribute
with an embedding vector, asynchronous voice anonymization can be achieved by
modifying the speaker embedding derived from the original speech. However, the
inconsistency between machine and human perceptions of the speaker attribute
within the speaker embedding remains unexplored, limiting its performance in
asynchronous voice anonymization. To this end, this study investigates this
inconsistency via modifications to speaker embedding in the speech generation
process. Experiments conducted on the FACodec and Diff-HierVC speech generation
models discover a subspace whose removal alters machine perception while
preserving its human perception of the speaker attribute in the generated
speech. With these findings, an asynchronous voice anonymization is developed,
achieving 100% human perception preservation rate while obscuring the machine
perception. Audio samples can be found in
https://voiceprivacy.github.io/speaker-embedding-eigen-decomposition/.

</details>


### [10] [Neural Forward Filtering for Speaker-Image Separation](https://arxiv.org/abs/2510.05757)
*Jingqi Sun,Shulin He,Ruizhe Pang,Zhong-Qiu Wang*

Main category: eess.AS

TL;DR: 提出CxNet系统，用于单声道多说话人混响分离，通过显式建模线性滤波器来利用直接路径信号与混响语音之间的物理约束，从而更好地保留每个说话人的混响特性。


<details>
  <summary>Details</summary>
Motivation: 解决单声道多说话人混响分离问题，目标是在分离混合说话人的同时保留每个说话人的混响特性。现有端到端DNN方法虽然有效，但未能显式利用混响语音可由直接路径信号与线性滤波器卷积产生的物理约束。

Method: 提出CxNet双DNN系统，包含神经前向滤波模块。第一个DNN联合预测直接路径信号和混响语音，基于直接路径估计，神经前向滤波模块估计线性滤波器，然后将估计的滤波器与直接路径估计卷积得到另一个混响语音估计，作为判别特征帮助第二个DNN更好地估计混响语音。

Result: 在SMS-WSJ数据集上的评估结果显示所提算法的有效性。

Conclusion: 通过显式建模线性滤波器，CxNet能够利用直接路径信号与混响语音之间的物理约束来捕捉混响尾音的关键信息，在多说话人混响分离任务中表现良好。

Abstract: We address monaural multi-speaker-image separation in reverberant conditions,
aiming at separating mixed speakers but preserving the reverberation of each
speaker. A straightforward approach for this task is to directly train
end-to-end DNN systems to predict the reverberant speech of each speaker based
on the input mixture. Although effective, this approach does not explicitly
exploit the physical constraint that reverberant speech can be reproduced by
convolving the direct-path signal with a linear filter. To address this, we
propose CxNet, a two-DNN system with a neural forward filtering module in
between. The first DNN is trained to jointly predict the direct-path signal and
reverberant speech. Based on the direct-path estimate, the neural forward
filtering module estimates the linear filter, and the estimated filter is then
convolved with the direct-path estimate to obtain another estimate of
reverberant speech, which is utilized as a discriminative feature to help the
second DNN better estimate the reverberant speech. By explicitly modeling the
linear filter, CxNet could leverage the physical constraint between the
direct-path signal and reverberant speech to capture crucial information about
reverberation tails. Evaluation results on the SMS-WSJ dataset show the
effectiveness of the proposed algorithms.

</details>


### [11] [Revisiting MFCCs: Evidence for Spectral-Prosodic Coupling](https://arxiv.org/abs/2510.05922)
*Vitor Magno de O. S. Bezerra,Gabriel F. A. Bastos,Jugurta Montalvão*

Main category: eess.AS

TL;DR: 该研究挑战了MFCC缺乏时间信息的传统假设，通过统计测试证明MFCC与语音韵律特征（能量、基频、浊音）存在显著相关性。


<details>
  <summary>Details</summary>
Motivation: 深入理解MFCC的特性对经典和深度学习模型都很重要，本研究旨在验证MFCC是否真的缺乏相关的时间信息。

Method: 使用零假设显著性检验框架，系统评估MFCC与三个韵律特征（能量、基频F0、浊音）之间的统计独立性。

Result: 结果表明，MFCC与这三个韵律特征在统计上不独立，证明MFCC固有地携带了有价值的韵律信息。

Conclusion: MFCC实际上包含了重要的韵律信息，这一发现可为未来语音分析和识别模型的设计提供指导。

Abstract: Mel-frequency cepstral coefficients (MFCCs) are an important feature in
speech processing. A deeper understanding of their properties can contribute to
the work that is being done with both classical and deep learning models. This
study challenges the long-held assumption that MFCCs lack relevant temporal
information by investigating their relationship with speech prosody. Using a
null hypothesis significance testing framework, a systematic assessment is made
about the statistical independence between MFCCs and the three prosodic
features: energy, fundamental frequency (F0), and voicing. The results
demonstrate that it is statistically implausible that the MFCCs are independent
of any of these three prosodic features. This finding suggests that MFCCs
inherently carry valuable prosodic information, which can inform the design of
future models in speech analysis and recognition.

</details>


### [12] [Revisiting Modeling and Evaluation Approaches in Speech Emotion Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions](https://arxiv.org/abs/2510.05934)
*Huang-Cheng Chou,Chi-Chun Lee*

Main category: eess.AS

TL;DR: 该论文挑战传统语音情感识别方法，提出保留所有情感评分、使用软标签分布、允许多重情感预测的新方法，通过实验证明这些方法能提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别方法将标注者之间的分歧视为噪声，通过聚合标签形成单一共识目标，但这忽略了人类情感感知的主观性。论文旨在探索是否应该丢弃少数情感评分、仅学习少数人的感知、以及仅预测单一情感。

Method: 1. 保留所有情感评分并用软标签分布表示；2. 重新定义评估标准，允许情感共现；3. 构建惩罚矩阵在训练中抑制不可能的情感组合；4. 在四个英文情感数据库上进行实验验证。

Result: 实验结果表明，提出的方法在共识标签测试上表现优于多数投票和复数投票方法，能够提高语音情感识别系统的性能。

Conclusion: 接受少数评分、多标注者和多重情感预测能够构建更鲁棒且与人类感知更一致的语音情感识别系统。

Abstract: Over the past two decades, speech emotion recognition (SER) has received
growing attention. To train SER systems, researchers collect emotional speech
databases annotated by crowdsourced or in-house raters who select emotions from
predefined categories. However, disagreements among raters are common.
Conventional methods treat these disagreements as noise, aggregating labels
into a single consensus target. While this simplifies SER as a single-label
task, it ignores the inherent subjectivity of human emotion perception. This
dissertation challenges such assumptions and asks: (1) Should minority
emotional ratings be discarded? (2) Should SER systems learn from only a few
individuals' perceptions? (3) Should SER systems predict only one emotion per
sample?
  Psychological studies show that emotion perception is subjective and
ambiguous, with overlapping emotional boundaries. We propose new modeling and
evaluation perspectives: (1) Retain all emotional ratings and represent them
with soft-label distributions. Models trained on individual annotator ratings
and jointly optimized with standard SER systems improve performance on
consensus-labeled tests. (2) Redefine SER evaluation by including all emotional
data and allowing co-occurring emotions (e.g., sad and angry). We propose an
``all-inclusive rule'' that aggregates all ratings to maximize diversity in
label representation. Experiments on four English emotion databases show
superior performance over majority and plurality labeling. (3) Construct a
penalization matrix to discourage unlikely emotion combinations during
training. Integrating it into loss functions further improves performance.
Overall, embracing minority ratings, multiple annotators, and multi-emotion
predictions yields more robust and human-aligned SER systems.

</details>


### [13] [TokenChain: A Discrete Speech Chain via Semantic Token Modeling](https://arxiv.org/abs/2510.06201)
*Mingxuan Wang,Satoshi Nakamura*

Main category: eess.AS

TL;DR: TokenChain是一个完全离散的语音链，通过语义标记ASR与两阶段TTS耦合，实现了端到端的跨文本接口反馈，在ASR和TTS任务上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 模拟人类感知-生产循环的机器语音链被证明能有效联合改进ASR和TTS性能，但需要探索在标记接口和模型下链式学习的有效性。

Method: 提出TokenChain：使用语义标记ASR与两阶段TTS（自回归文本到语义模型与掩码生成语义到声学模型）耦合，通过直通argmax/Gumbel-Softmax实现端到端反馈，并通过动态权重平均与监督ASR平衡。

Result: 在LibriSpeech上，TokenChain比基线早2-6个epoch达到更高准确率，错误率降低5-13%，T2S性能稳定；在TED-LIUM上，ASR WER相对降低56%，T2S WER相对降低31%，遗忘最小。

Conclusion: 链式学习在标记接口和模型下仍然有效，TokenChain证明了离散语音链在联合改进ASR和TTS任务上的优势。

Abstract: Machine Speech Chain, simulating the human perception-production loop, proves
effective in jointly improving ASR and TTS. We propose TokenChain, a fully
discrete speech chain coupling semantic-token ASR with a two-stage TTS: an
autoregressive text-to-semantic model co-trained with ASR and a
masked-generative semantic-to-acoustic model for synthesis only. End-to-end
feedback across the text interface is enabled with straight-through
argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight
averaging. Ablations examine optimal temperature schedules for in- and
cross-domain transfer. Evaluation reveals TokenChain surpasses baseline
accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with
stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by
31% on TED-LIUM with minimal forgetting, showing that chain learning remains
effective with token interfaces and models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [14] [Provable Speech Attributes Conversion via Latent Independence](https://arxiv.org/abs/2510.05191)
*Jonathan Svirsky,Ofir Lindenbaum,Uri Shaham*

Main category: cs.SD

TL;DR: 提出了一个具有理论保证的语音属性转换通用框架，通过非概率自编码器架构和独立性约束实现可靠可控的语音风格转换。


<details>
  <summary>Details</summary>
Motivation: 现有语音风格转换方法缺乏理论基础，难以保证可靠和可解释的控制效果，需要建立有理论保证的通用框架。

Method: 基于非概率自编码器架构，在预测潜在变量和目标可控变量之间施加独立性约束，确保在保持原始内容的同时修改目标属性。

Result: 在说话人身份和情感等语音风格转换任务上验证了方法的有效性，定量评估证实了方法的通用性和效果。

Conclusion: 该框架为语音属性转换提供了理论基础和实际解决方案，能够实现一致可靠的信号转换。

Abstract: While signal conversion and disentangled representation learning have shown
promise for manipulating data attributes across domains such as audio, image,
and multimodal generation, existing approaches, especially for speech style
conversion, are largely empirical and lack rigorous theoretical foundations to
guarantee reliable and interpretable control. In this work, we propose a
general framework for speech attribute conversion, accompanied by theoretical
analysis and guarantees under reasonable assumptions. Our framework builds on a
non-probabilistic autoencoder architecture with an independence constraint
between the predicted latent variable and the target controllable variable.
This design ensures a consistent signal transformation, conditioned on an
observed style variable, while preserving the original content and modifying
the desired attribute. We further demonstrate the versatility of our method by
evaluating it on speech styles, including speaker identity and emotion.
Quantitative evaluations confirm the effectiveness and generality of the
proposed approach.

</details>


### [15] [AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement](https://arxiv.org/abs/2510.05295)
*M. Sajid,Deepanshu Gupta,Yash Modi,Sanskriti Jain,Harshith Jai Surya Ganji,A. Rahaman,Harshvardhan Choudhary,Nasir Saleem,Amir Hussain,M. Tanveer*

Main category: cs.SD

TL;DR: AUREXA-SE是一个用于音频-视觉语音增强的渐进式双模态框架，通过双向交叉注意力机制融合音频和视觉特征，使用Squeezeformer块捕获时间依赖关系，在语音质量指标上显著优于噪声基线。


<details>
  <summary>Details</summary>
Motivation: 现有的语音增强方法主要依赖音频信号，但在噪声环境下性能受限。利用视觉信息（如唇部运动）可以提供互补的语音线索，提高在复杂噪声场景下的语音增强效果。

Method: 使用U-Net-based 1D卷积编码器处理原始音频波形，Swin Transformer V2提取视觉特征，通过新颖的双向交叉注意力机制进行模态融合，采用Squeezeformer块捕获时间依赖关系，最后通过U-Net风格解码器重建波形。

Result: 在语音增强任务中取得显著性能提升：STOI为0.516，PESQ为1.323，SI-SDR为-4.322 dB，明显优于噪声基线。

Conclusion: AUREXA-SE通过有效的音频-视觉融合和时序建模，在语音增强任务中表现出色，证明了多模态方法在噪声环境下的优势。

Abstract: In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation
Exchange Architecture with Cross-Attention and Squeezeformer for Speech
Enhancement), a progressive bimodal framework tailored for audio-visual speech
enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual
cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin
Transformer V2 for efficient and expressive visual feature extraction. Central
to the architecture is a novel bidirectional cross-attention mechanism, which
facilitates deep contextual fusion between modalities, enabling rich and
complementary representation learning. To capture temporal dependencies within
the fused embeddings, a stack of lightweight Squeezeformer blocks combining
convolutional and attention modules is introduced. The enhanced embeddings are
then decoded via a U-Net-style decoder for direct waveform reconstruction,
ensuring perceptually consistent and intelligible speech output. Experimental
evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant
performance improvements over noisy baselines, with STOI of 0.516, PESQ of
1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at
https://github.com/mtanveer1/AVSEC-4-Challenge-2025.

</details>


### [16] [Sci-Phi: A Large Language Model Spatial Audio Descriptor](https://arxiv.org/abs/2510.05542)
*Xilin Jiang,Hannes Gamper,Sebastian Braun*

Main category: cs.SD

TL;DR: Sci-Phi是一个空间音频大语言模型，通过双空间和频谱编码器，能够估计所有声源和环境的完整参数集，实现完整的空间场景描述。


<details>
  <summary>Details</summary>
Motivation: 虽然音频语言模型在声音识别方面表现出色，但单通道输入从根本上限制了空间理解能力。

Method: 使用双空间和频谱编码器，从超过4000小时的一阶Ambisonics合成录音中学习，能够一次性枚举和描述多达四个定向声源，以及非定向背景声音和房间特性。

Result: 通过15个指标评估模型，涵盖内容、位置、时间、响度和混响，并在不同声源数量、信噪比、混响水平和挑战性混合条件下表现出鲁棒性。模型能够泛化到真实房间脉冲响应，性能仅有轻微下降。

Conclusion: 这项工作建立了第一个能够进行完整空间场景描述的音频大语言模型，具有强大的实际部署潜力。

Abstract: Acoustic scene perception involves describing the type of sounds, their
timing, their direction and distance, as well as their loudness and
reverberation. While audio language models excel in sound recognition,
single-channel input fundamentally limits spatial understanding. This work
presents Sci-Phi, a spatial audio large language model with dual spatial and
spectral encoders that estimates a complete parameter set for all sound sources
and the surrounding environment. Learning from over 4,000 hours of synthetic
first-order Ambisonics recordings including metadata, Sci-Phi enumerates and
describes up to four directional sound sources in one pass, alongside
non-directional background sounds and room characteristics. We evaluate the
model with a permutation-invariant protocol and 15 metrics covering content,
location, timing, loudness, and reverberation, and analyze its robustness
across source counts, signal-to-noise ratios, reverberation levels, and
challenging mixtures of acoustically, spatially, or temporally similar sources.
Notably, Sci-Phi generalizes to real room impulse responses with only minor
performance degradation. Overall, this work establishes the first audio LLM
capable of full spatial-scene description, with strong potential for real-world
deployment. Demo: https://sci-phi-audio.github.io/demo

</details>


### [17] [Sparse deepfake detection promotes better disentanglement](https://arxiv.org/abs/2510.05696)
*Antoine Teissier,Marie Tahon,Nicolas Dugué,Aghilas Sini*

Main category: cs.SD

TL;DR: 提出了一种基于稀疏表示的深度伪造音频检测方法，通过TopK激活机制在AASIST架构的最后一层嵌入中实现95%稀疏度，在ASVSpoof5测试集上达到23.36%的等错误率，并展示了更好的特征解耦能力。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成技术的快速发展，深度伪造检测成为语音处理领域的重要关注点。系统不仅需要高效和鲁棒，还需要提供可解释性，特别是对潜在表示的解释。

Method: 在AASIST深度伪造检测架构的最后一层嵌入上应用TopK激活机制，获得稀疏表示用于决策过程。使用基于互信息的完整性和模块化指标评估特征解耦能力。

Result: 稀疏深度伪造检测方法在ASVSpoof5测试集上达到23.36%的等错误率，具有95%的稀疏度。稀疏表示提供了更好的特征解耦，某些攻击直接在潜在空间中被编码。

Conclusion: 稀疏表示不仅提高了深度伪造检测性能，还增强了系统的可解释性，使某些攻击模式能够在潜在空间中直接识别。

Abstract: Due to the rapid progress of speech synthesis, deepfake detection has become
a major concern in the speech processing community. Because it is a critical
task, systems must not only be efficient and robust, but also provide
interpretable explanations. Among the different approaches for explainability,
we focus on the interpretation of latent representations. In such paper, we
focus on the last layer of embeddings of AASIST, a deepfake detection
architecture. We use a TopK activation inspired by SAEs on this layer to obtain
sparse representations which are used in the decision process. We demonstrate
that sparse deepfake detection can improve detection performance, with an EER
of 23.36% on ASVSpoof5 test set, with 95% of sparsity. We then show that these
representations provide better disentanglement, using completeness and
modularity metrics based on mutual information. Notably, some attacks are
directly encoded in the latent space.

</details>


### [18] [MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics for Speech Emotion Recognition](https://arxiv.org/abs/2510.05749)
*Haoxun Li,Yuqing Sun,Hanlei Shi,Yu Liu,Leyuan Qu,Taihao Li*

Main category: cs.SD

TL;DR: MSF-SER通过融合多粒度语义信息（局部强调语义、全局语义和扩展语义）来增强语音情感识别，使用门控融合和FiLM调制的轻量级专家混合模型，在MSP-Podcast和IEMOCAP数据集上显著提升了维度预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态方法仅依赖全局转录本的局限性：所有词语被平等对待，忽略了句子不同部分的强调会改变情感含义；仅表示表面词汇内容，缺乏高级解释性线索。

Method: 提出MSF-SER模型，将声学特征与三个互补的文本语义层次（局部强调语义LES、全局语义GS、扩展语义ES）相结合，通过模态内门控融合和跨模态FiLM调制的轻量级专家混合模型（FM-MOE）进行集成。

Result: 在MSP-Podcast和IEMOCAP数据集上的实验表明，MSF-SER在维度预测方面持续改进，验证了丰富语义融合对语音情感识别的有效性。

Conclusion: 多粒度语义融合方法能够有效提升连续维度语音情感识别的性能，证明了考虑不同层次语义信息的重要性。

Abstract: Continuous dimensional speech emotion recognition captures affective
variation along valence, arousal, and dominance, providing finer-grained
representations than categorical approaches. Yet most multimodal methods rely
solely on global transcripts, leading to two limitations: (1) all words are
treated equally, overlooking that emphasis on different parts of a sentence can
shift emotional meaning; (2) only surface lexical content is represented,
lacking higher-level interpretive cues. To overcome these issues, we propose
MSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition),
which augments acoustic features with three complementary levels of textual
semantics--Local Emphasized Semantics (LES), Global Semantics (GS), and
Extended Semantics (ES). These are integrated via an intra-modal gated fusion
and a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE).
Experiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves
dimensional prediction, demonstrating the effectiveness of enriched semantic
fusion for SER.

</details>


### [19] [Transcribing Rhythmic Patterns of the Guitar Track in Polyphonic Music](https://arxiv.org/abs/2510.05756)
*Aleksandr Lukoianov,Anssi Klapuri*

Main category: cs.SD

TL;DR: 提出了一种从多音音乐中转录吉他节奏模式的三步框架，包括近似音轨分离、使用MERT模型检测拨弦、以及基于专家词汇的模式解码。


<details>
  <summary>Details</summary>
Motivation: 虽然和弦转录研究较多，但节奏模式转录研究较少，特别是对于节奏吉他这种重复变化的节奏模式。由于节奏模式缺乏客观的"正确"定义，需要创建有明确标注的数据集。

Method: 三步框架：1）近似音轨分离提取吉他部分；2）使用预训练基础模型MERT检测单个拨弦；3）模式解码过程，将拨弦序列表示为专家策划词汇中的模式。

Result: 能够在多音音乐中较高精度地转录吉他节奏模式，生成包含自动检测小节线和拍号标记的人类可读表示。

Conclusion: 该框架能够准确转录吉他节奏模式，并提出了评估指标来评估预测节奏模式序列的准确性和可读性。

Abstract: Whereas chord transcription has received considerable attention during the
past couple of decades, far less work has been devoted to transcribing and
encoding the rhythmic patterns that occur in a song. The topic is especially
relevant for instruments such as the rhythm guitar, which is typically played
by strumming rhythmic patterns that repeat and vary over time. However, in many
cases one cannot objectively define a single "right" rhythmic pattern for a
given song section. To create a dataset with well-defined ground-truth labels,
we asked expert musicians to transcribe the rhythmic patterns in 410 popular
songs and record cover versions where the guitar tracks followed those
transcriptions. To transcribe the strums and their corresponding rhythmic
patterns, we propose a three-step framework. Firstly, we perform approximate
stem separation to extract the guitar part from the polyphonic mixture.
Secondly, we detect individual strums within the separated guitar audio, using
a pre-trained foundation model (MERT) as a backbone. Finally, we carry out a
pattern-decoding process in which the transcribed sequence of guitar strums is
represented by patterns drawn from an expert-curated vocabulary. We show that
it is possible to transcribe the rhythmic patterns of the guitar track in
polyphonic music with quite high accuracy, producing a representation that is
human-readable and includes automatically detected bar lines and time signature
markers. We perform ablation studies and error analysis and propose a set of
evaluation metrics to assess the accuracy and readability of the predicted
rhythmic pattern sequence.

</details>


### [20] [EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS](https://arxiv.org/abs/2510.05758)
*Haoxun Li,Yu Liu,Yuqing Sun,Hanlei Shi,Leyuan Qu,Taihao Li*

Main category: cs.SD

TL;DR: EMORL-TTS是一个基于强化学习的细粒度情感可控TTS框架，通过结合监督微调和任务特定奖励的强化学习，实现VAD空间的全局强度控制和局部重音调节。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-based TTS系统虽然质量高且具备零样本能力，但由于依赖离散语音标记，缺乏细粒度的情感控制能力。现有方法要么将情感限制为分类标签，要么无法推广到LLM-based架构。

Method: 提出EMORL-TTS框架，结合监督微调和强化学习，使用针对情感类别、强度和重音的任务特定奖励。研究重音位置如何调节细粒度情感强度。

Result: 实验表明EMORL-TTS在情感准确性、强度区分和重音清晰度方面均有提升，同时保持与强LLM-based基线相当的合成质量。

Conclusion: EMORL-TTS成功实现了细粒度的情感控制，在保持合成质量的同时显著提升了情感表达的可控性。

Abstract: Recent LLM-based TTS systems achieve strong quality and zero-shot ability,
but lack fine-grained emotional control due to their reliance on discrete
speech tokens. Existing approaches either limit emotions to categorical labels
or cannot generalize to LLM-based architectures. We propose EMORL-TTS
(Fine-grained Emotion-controllable TTS with Reinforcement Learning), a
framework that unifies global intensity control in the VAD space with local
emphasis regulation. Our method combines supervised fine-tuning with
reinforcement learning guided by task-specific rewards for emotion category,
intensity, and emphasis. Moreover, we further investigate how emphasis
placement modulates fine-grained emotion intensity. Experiments show that
EMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis
clarity, while preserving synthesis quality comparable to strong LLM-based
baselines.

</details>


### [21] [StereoSync: Spatially-Aware Stereo Audio Generation from Video](https://arxiv.org/abs/2510.05828)
*Christian Marinoni,Riccardo Fosco Gramaccioni,Kazuki Shimada,Takashi Shibuya,Yuki Mitsufuji,Danilo Comminiello*

Main category: cs.SD

TL;DR: StereoSync是一个新颖高效的视频对齐音频生成模型，不仅能实现时间同步，还能通过深度图和边界框提取空间线索，生成与视频场景空间结构动态适应的立体音频。


<details>
  <summary>Details</summary>
Motivation: 目前音频生成研究广泛，但视频对齐的音频生成仍相对未被探索。现有方法主要关注时间同步，缺乏空间感知能力。

Method: 利用预训练基础模型提取深度图和边界框的空间线索，在基于扩散的音频生成模型中作为交叉注意力条件，实现时空对齐的立体音频生成。

Result: 在Walking The Maps数据集上的实验表明，StereoSync能够同时实现时间和空间对齐，显著提升了视频到音频生成的沉浸感和真实感。

Conclusion: StereoSync通过引入空间感知能力，在视频对齐音频生成领域取得了显著进展，为创造更沉浸式的音频体验提供了有效解决方案。

Abstract: Although audio generation has been widely studied over recent years,
video-aligned audio generation still remains a relatively unexplored frontier.
To address this gap, we introduce StereoSync, a novel and efficient model
designed to generate audio that is both temporally synchronized with a
reference video and spatially aligned with its visual context. Moreover,
StereoSync also achieves efficiency by leveraging pretrained foundation models,
reducing the need for extensive training while maintaining high-quality
synthesis. Unlike existing methods that primarily focus on temporal
synchronization, StereoSync introduces a significant advancement by
incorporating spatial awareness into video-aligned audio generation. Indeed,
given an input video, our approach extracts spatial cues from depth maps and
bounding boxes, using them as cross-attention conditioning in a diffusion-based
audio generation model. Such an approach allows StereoSync to go beyond simple
synchronization, producing stereo audio that dynamically adapts to the spatial
structure and movement of a video scene. We evaluate StereoSync on Walking The
Maps, a curated dataset comprising videos from video games that feature
animated characters walking through diverse environments. Experimental results
demonstrate the ability of StereoSync to achieve both temporal and spatial
alignment, advancing the state of the art in video-to-audio generation and
resulting in a significantly more immersive and realistic audio experience.

</details>


### [22] [FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders](https://arxiv.org/abs/2510.05829)
*Riccardo Fosco Gramaccioni,Christian Marinoni,Eleonora Grassucci,Giordano Cicchetti,Aurelio Uncini,Danilo Comminiello*

Main category: cs.SD

TL;DR: FoleyGRAM是一种基于Gramian表示对齐度量的视频到音频生成方法，通过多模态编码器对齐实现语义控制，在Greatest Hits数据集上取得先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法缺乏精确的语义控制，需要改进多模态表示对齐来提升生成音频与视频内容的语义一致性。

Method: 使用GRAM对齐视频、文本和音频模态的嵌入表示，构建基于扩散的音频合成模型，结合GRAM对齐嵌入和波形包络进行条件生成。

Result: 在Greatest Hits数据集上的实验表明，GRAM对齐多模态编码器能显著提升生成音频与视频内容的语义对齐能力。

Conclusion: FoleyGRAM通过GRAM对齐多模态表示，在视频到音频生成中实现了更好的语义控制，推动了该领域的技术发展。

Abstract: In this work, we present FoleyGRAM, a novel approach to video-to-audio
generation that emphasizes semantic conditioning through the use of aligned
multimodal encoders. Building on prior advancements in video-to-audio
generation, FoleyGRAM leverages the Gramian Representation Alignment Measure
(GRAM) to align embeddings across video, text, and audio modalities, enabling
precise semantic control over the audio generation process. The core of
FoleyGRAM is a diffusion-based audio synthesis model conditioned on
GRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness
and temporal alignment with the corresponding input video. We evaluate
FoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio
models. Our experiments demonstrate that aligning multimodal encoders using
GRAM enhances the system's ability to semantically align generated audio with
video content, advancing the state of the art in video-to-audio synthesis.

</details>


### [23] [LARA-Gen: Enabling Continuous Emotion Control for Music Generation Models via Latent Affective Representation Alignment](https://arxiv.org/abs/2510.05875)
*Jiahao Mei,Xuenan Xu,Zeyu Xie,Zihao Zheng,Ye Tao,Yue Ding,Mengyue Wu*

Main category: cs.SD

TL;DR: LARA-Gen是一个通过潜在情感表示对齐实现连续情感控制的文本到音乐生成框架，在情感一致性和音乐质量上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到音乐模型虽然能够从文本提示生成连贯的音乐，但在细粒度情感控制方面仍存在不足，需要解决情感控制的精细调节问题。

Method: 通过潜在情感表示对齐(LARA)将内部隐藏状态与外部音乐理解模型对齐，设计基于连续效价-唤醒空间的情感控制模块，将情感属性与文本内容解耦。

Result: 实验表明LARA-Gen实现了连续、细粒度的情感控制，在情感一致性和音乐质量方面显著优于基线方法。

Conclusion: LARA-Gen框架成功解决了文本到音乐生成中的细粒度情感控制问题，为音乐生成的情感可控性提供了有效解决方案。

Abstract: Recent advances in text-to-music models have enabled coherent music
generation from text prompts, yet fine-grained emotional control remains
unresolved. We introduce LARA-Gen, a framework for continuous emotion control
that aligns the internal hidden states with an external music understanding
model through Latent Affective Representation Alignment (LARA), enabling
effective training. In addition, we design an emotion control module based on a
continuous valence-arousal space, disentangling emotional attributes from
textual content and bypassing the bottlenecks of text-based prompting.
Furthermore, we establish a benchmark with a curated test set and a robust
Emotion Predictor, facilitating objective evaluation of emotional
controllability in music generation. Extensive experiments demonstrate that
LARA-Gen achieves continuous, fine-grained control of emotion and significantly
outperforms baselines in both emotion adherence and music quality. Generated
samples are available at https://nieeim.github.io/LARA-Gen/.

</details>


### [24] [Segment-Factorized Full-Song Generation on Symbolic Piano Music](https://arxiv.org/abs/2510.05881)
*Ping-Yi Chen,Chih-Pin Tan,Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: 提出Segmented Full-Song Model (SFS)用于符号化全歌曲生成，通过分段生成和选择性注意力机制提高质量和效率，并开发了支持人机交互的网页应用。


<details>
  <summary>Details</summary>
Motivation: 解决符号化全歌曲生成问题，通过用户提供的歌曲结构和可选种子片段来引导歌曲创作，实现更高质量和效率的音乐生成。

Method: 将歌曲分解为多个片段，通过选择性注意力机制生成每个片段，模型接受用户提供的歌曲结构和可选种子片段作为输入。

Result: 相比先前工作，该模型在质量和效率方面表现更优，并成功开发了支持迭代协作创作的网页应用。

Conclusion: SFS模型为符号化全歌曲生成提供了有效的解决方案，通过分段生成和选择性注意力机制实现了高质量的音乐创作，其网页应用支持灵活的人机交互。

Abstract: We propose the Segmented Full-Song Model (SFS) for symbolic full-song
generation. The model accepts a user-provided song structure and an optional
short seed segment that anchors the main idea around which the song is
developed. By factorizing a song into segments and generating each one through
selective attention to related segments, the model achieves higher quality and
efficiency compared to prior work. To demonstrate its suitability for human-AI
interaction, we further wrap SFS into a web application that enables users to
iteratively co-create music on a piano roll with customizable structures and
flexible ordering.

</details>


### [25] [ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning](https://arxiv.org/abs/2510.05984)
*Tao Zhu,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.SD

TL;DR: ECTSpeech是一种简单有效的一步语音合成框架，首次将Easy Consistency Tuning策略应用于语音合成，通过逐步收紧预训练扩散模型的一致性约束，实现高质量一步生成，同时显著降低训练复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语音合成中表现出色，但通常需要多步采样，导致推理效率低。现有的一致性模型方法需要额外训练成本且严重依赖预训练教师模型的性能。

Method: 提出ECTSpeech框架，将Easy Consistency Tuning策略融入语音合成，通过逐步收紧一致性约束实现一步生成。还设计了多尺度门模块(MSGate)来增强去噪器在不同尺度特征融合的能力。

Result: 在LJSpeech数据集上的实验结果表明，ECTSpeech在单步采样下实现了与最先进方法相当的音频质量，同时大幅降低了模型的训练成本和复杂度。

Conclusion: ECTSpeech是一种高效的一步语音合成方法，通过一致性调优策略在保持高质量的同时显著降低了训练复杂度。

Abstract: Diffusion models have demonstrated remarkable performance in speech
synthesis, but typically require multi-step sampling, resulting in low
inference efficiency. Recent studies address this issue by distilling diffusion
models into consistency models, enabling efficient one-step generation.
However, these approaches introduce additional training costs and rely heavily
on the performance of pre-trained teacher models. In this paper, we propose
ECTSpeech, a simple and effective one-step speech synthesis framework that, for
the first time, incorporates the Easy Consistency Tuning (ECT) strategy into
speech synthesis. By progressively tightening consistency constraints on a
pre-trained diffusion model, ECTSpeech achieves high-quality one-step
generation while significantly reducing training complexity. In addition, we
design a multi-scale gate module (MSGate) to enhance the denoiser's ability to
fuse features at different scales. Experimental results on the LJSpeech dataset
demonstrate that ECTSpeech achieves audio quality comparable to
state-of-the-art methods under single-step sampling, while substantially
reducing the model's training cost and complexity.

</details>


### [26] [EmoHRNet: High-Resolution Neural Network Based Speech Emotion Recognition](https://arxiv.org/abs/2510.06072)
*Akshay Muppidi,Martin Radfar*

Main category: cs.SD

TL;DR: EmoHRNet是一种基于高分辨率网络(HRNet)的语音情感识别模型，通过将音频转换为频谱图并保持高分辨率表示，在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互体验，通过改进语音情感识别技术来更好地理解和响应人类情感。

Method: 将音频样本转换为频谱图，采用HRNet架构在整个网络中保持高分辨率表示，提取高级特征来捕捉语音信号中的细粒度和整体情感线索。

Result: 在RAVDESS数据集上达到92.45%准确率，IEMOCAP数据集上80.06%，EMOVO数据集上92.77%，超越了现有领先模型。

Conclusion: EmoHRNet为语音情感识别领域设立了新的基准，证明了高分辨率表示在情感识别中的有效性。

Abstract: Speech emotion recognition (SER) is pivotal for enhancing human-machine
interactions. This paper introduces "EmoHRNet", a novel adaptation of
High-Resolution Networks (HRNet) tailored for SER. The HRNet structure is
designed to maintain high-resolution representations from the initial to the
final layers. By transforming audio samples into spectrograms, EmoHRNet
leverages the HRNet architecture to extract high-level features. EmoHRNet's
unique architecture maintains high-resolution representations throughout,
capturing both granular and overarching emotional cues from speech signals. The
model outperforms leading models, achieving accuracies of 92.45% on RAVDESS,
80.06% on IEMOCAP, and 92.77% on EMOVO. Thus, we show that EmoHRNet sets a new
benchmark in the SER domain.

</details>


### [27] [Modulation Discovery with Differentiable Digital Signal Processing](https://arxiv.org/abs/2510.06204)
*Christopher Mitcheltree,Hao Hao Tan,Joshua D. Reiss*

Main category: cs.SD

TL;DR: 提出了一种基于神经网络的音频匹配方法，通过调制提取、约束控制信号参数化和可微分数字信号处理来发现音频中的调制信号。


<details>
  <summary>Details</summary>
Motivation: 现代合成器提供了丰富的调制工具，但确定用于创建声音的调制信号很困难，现有系统要么是不可解释的黑盒，要么预测高维参数值而不考虑调制曲线的形状、结构和路由。

Method: 使用调制提取、约束控制信号参数化和可微分数字信号处理(DDSP)来发现音频中的调制信号，适用于不同的DDSP合成器架构。

Result: 在高度调制的合成和真实音频样本上展示了方法的有效性，研究了在可解释性和音频匹配精度之间的权衡。

Conclusion: 该方法能够有效发现音频中的调制信号，提供了代码、音频样本和VST插件形式的训练DDSP合成器。

Abstract: Modulations are a critical part of sound design and music production,
enabling the creation of complex and evolving audio. Modern synthesizers
provide envelopes, low frequency oscillators (LFOs), and more parameter
automation tools that allow users to modulate the output with ease. However,
determining the modulation signals used to create a sound is difficult, and
existing sound-matching / parameter estimation systems are often
uninterpretable black boxes or predict high-dimensional framewise parameter
values without considering the shape, structure, and routing of the underlying
modulation curves. We propose a neural sound-matching approach that leverages
modulation extraction, constrained control signal parameterizations, and
differentiable digital signal processing (DDSP) to discover the modulations
present in a sound. We demonstrate the effectiveness of our approach on highly
modulated synthetic and real audio samples, its applicability to different DDSP
synth architectures, and investigate the trade-off it incurs between
interpretability and sound-matching accuracy. We make our code and audio
samples available and provide the trained DDSP synths in a VST plugin.

</details>

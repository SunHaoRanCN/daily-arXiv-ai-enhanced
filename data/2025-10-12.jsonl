{"id": "2510.07592", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07592", "abs": "https://arxiv.org/abs/2510.07592", "authors": ["Sebastian Braun", "Hannes Gamper", "Dimitra Emmanouilidou"], "title": "SALAD-VAE: Semantic Audio Compression with Language-Audio Distillation", "comment": "submitted to ICASSP 2026", "summary": "Modern generative and multimodal models increasingly rely on compact latent\nrepresentations that trade and balance semantic richness with high-fidelity\nreconstruction. We introduce SALAD-VAE, a continuous and highly compact\nsemantic Audio Variational Autoencoder, which operates in the frequency domain\nand achieves state-of-the-art compression with very low latent frame rate (7.8\nHz) while surfacing semantic structure and producing high audio quality. We\nenhance the standard VAE semantic losses and augmentation, specifically\ncontrastive learning and CLAP-based embedding distillation, enabling it to\ngeneralize across diverse audio domains. With a significantly less\ncomputational complex architecture than comparable state-of-the-art VAEs,\nSALAD-VAE matches their reconstruction quality while it consistently\noutperforms them on a wide range of classification benchmarks. Furthermore, the\nproposed additional loss function provides a trained CLAP projection layer,\nwhich can be used zero-shot audio captioning and classification matching\npretrained CLAP audio-text embeddings."}
{"id": "2510.07838", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07838", "abs": "https://arxiv.org/abs/2510.07838", "authors": ["Guan-Ting Lin", "Shih-Yun Shan Kuan", "Jiatong Shi", "Kai-Wei Chang", "Siddhant Arora", "Shinji Watanabe", "Hung-yi Lee"], "title": "Full-Duplex-Bench-v2: A Multi-Turn Evaluation Framework for Duplex Dialogue Systems with an Automated Examiner", "comment": "Work in progress", "summary": "While full-duplex speech agents enable natural, low-latency interaction by\nspeaking and listening simultaneously, their consistency and task performance\nin multi-turn settings remain underexplored. We introduce Full-Duplex-Bench-v2\n(FDB-v2), a streaming framework that integrates with an automated examiner that\nenforces staged goals under two pacing setups (Fast vs. Slow). FDB-v2 covers\nfour task families: daily, correction, entity tracking, and safety. We report\nturn-taking fluency, multi-turn instruction following, and task-specific\ncompetence. The framework is extensible, supporting both commercial APIs and\nopen source models. When we test full-duplex systems with FDB-v2, they often\nget confused when people talk at the same time, struggle to handle corrections\nsmoothly, and sometimes lose track of who or what is being talked about.\nThrough an open-sourced, standardized streaming protocol and a task set, FDB-v2\nmakes it easy to extend to new task families, allowing the community to tailor\nand accelerate evaluation of multi-turn full-duplex systems."}
{"id": "2510.07908", "categories": ["eess.AS", "68T45", "I.2.7; H.5.5"], "pdf": "https://arxiv.org/pdf/2510.07908", "abs": "https://arxiv.org/abs/2510.07908", "authors": ["Kuan-Yu Chen", "Kuan-Lin Chen", "Yu-Chieh Yu", "Jian-Jiun Ding"], "title": "Guitar Tone Morphing by Diffusion-based Model", "comment": "5 pages", "summary": "In Music Information Retrieval (MIR), modeling and transforming the tone of\nmusical instruments, particularly electric guitars, has gained increasing\nattention due to the richness of the instrument tone and the flexibility of\nexpression. Tone morphing enables smooth transitions between different guitar\nsounds, giving musicians greater freedom to explore new textures and\npersonalize their performances. This study explores learning-based approaches\nfor guitar tone morphing, beginning with LoRA fine-tuning to improve the model\nperformance on limited data. Moreover, we introduce a simpler method, named\nspherical interpolation using Music2Latent. It yields significantly better\nresults than the more complex fine-tuning approach. Experiments show that the\nproposed architecture generates smoother and more natural tone transitions,\nmaking it a practical and efficient tool for music production and real-time\naudio effects."}
{"id": "2510.07909", "categories": ["eess.AS", "68T45", "I.2.7; H.5.5"], "pdf": "https://arxiv.org/pdf/2510.07909", "abs": "https://arxiv.org/abs/2510.07909", "authors": ["Kuan-Yu Chen", "Yi-Cheng Lin", "Jeng-Lin Li", "Jian-Jiun Ding"], "title": "Bloodroot: When Watermarking Turns Poisonous For Stealthy Backdoor", "comment": "5 pages, 3 figures", "summary": "Backdoor data poisoning is a crucial technique for ownership protection and\ndefending against malicious attacks. Embedding hidden triggers in training data\ncan manipulate model outputs, enabling provenance verification, and deterring\nunauthorized use. However, current audio backdoor methods are suboptimal, as\npoisoned audio often exhibits degraded perceptual quality, which is noticeable\nto human listeners. This work explores the intrinsic stealthiness and\neffectiveness of audio watermarking in achieving successful poisoning. We\npropose a novel Watermark-as-Trigger concept, integrated into the Bloodroot\nbackdoor framework via adversarial LoRA fine-tuning, which enhances perceptual\nquality while achieving a much higher trigger success rate and clean-sample\naccuracy. Experiments on speech recognition (SR) and speaker identification\n(SID) datasets show that watermark-based poisoning remains effective under\nacoustic filtering and model pruning. The proposed Bloodroot backdoor framework\nnot only secures data-to-model ownership, but also well reveals the risk of\nadversarial misuse."}
{"id": "2510.07415", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.07415", "abs": "https://arxiv.org/abs/2510.07415", "authors": ["Timothy L. Hutcheson", "Anil K. Raj"], "title": "Autoencoding Coordinate Sequences from Psychophysiologic Signals", "comment": "2 pages, 4 figures, 2025 IEEE Research and Applications of Photonics\n  in Defense Conference (RAPID)", "summary": "We present a method for converting 24 channels of psychophysiologic time\nseries data collected from individual participants via electroencephalogram\n(EEG), electrocardiogram (ECG), electrodermal activity (EDA), respiration rate\n(RR) into trackable three dimensional (3D) coordinates sufficient to estimate\nparticipation in specific task and cognitive states."}
{"id": "2510.07442", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.07442", "abs": "https://arxiv.org/abs/2510.07442", "authors": ["Harshvardhan C. Takawale", "Nirupam Roy", "Phil Brown"], "title": "INFER : Learning Implicit Neural Frequency Response Fields for Confined Car Cabin", "comment": null, "summary": "Accurate modeling of spatial acoustics is critical for immersive and\nintelligible audio in confined, resonant environments such as car cabins.\nCurrent tuning methods are manual, hardware-intensive, and static, failing to\naccount for frequency selective behaviors and dynamic changes like passenger\npresence or seat adjustments. To address this issue, we propose INFER: Implicit\nNeural Frequency Response fields, a frequency-domain neural framework that is\njointly conditioned on source and receiver positions, orientations to directly\nlearn complex-valued frequency response fields inside confined, resonant\nenvironments like car cabins. We introduce three key innovations over current\nneural acoustic modeling methods: (1) novel end-to-end frequency-domain forward\nmodel that directly learns the frequency response field and frequency-specific\nattenuation in 3D space; (2) perceptual and hardware-aware spectral supervision\nthat emphasizes critical auditory frequency bands and deemphasizes unstable\ncrossover regions; and (3) a physics-based Kramers-Kronig consistency\nconstraint that regularizes frequency-dependent attenuation and delay. We\nevaluate our method over real-world data collected in multiple car cabins. Our\napproach significantly outperforms time- and hybrid-domain baselines on both\nsimulated and real-world automotive datasets, cutting average magnitude and\nphase reconstruction errors by over 39% and 51%, respectively. INFER sets a new\nstate-of-the-art for neural acoustic modeling in automotive spaces"}
{"id": "2510.08047", "categories": ["eess.AS", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08047", "abs": "https://arxiv.org/abs/2510.08047", "authors": ["Yi-Cheng Lin", "Yu-Hsuan Li Liang", "Hsuan Su", "Tzu-Quan Lin", "Shang-Tse Chen", "Yun-Nung Chen", "Hung-yi Lee"], "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition", "comment": null, "summary": "Robust ASR under domain shift is crucial because real-world systems encounter\nunseen accents and domains with limited labeled data. Although pseudo-labeling\noffers a practical workaround, it often introduces systematic, accent-specific\nerrors that filtering fails to fix. We ask: How can we correct these recurring\nbiases without target ground truth? We propose a simple parameter-space\ncorrection: in a source domain containing both real and pseudo-labeled data,\ntwo ASR models are fine-tuned from the same initialization, one on ground-truth\nlabels and the other on pseudo-labels, and their weight difference forms a\ncorrection vector that captures pseudo-label biases. When applied to a\npseudo-labeled target model, this vector enhances recognition, achieving up to\na 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten\nAfrican accents with the Whisper tiny model."}
{"id": "2510.07466", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.07466", "abs": "https://arxiv.org/abs/2510.07466", "authors": ["Hanwen Hu", "Jiancheng An", "Lu Gan", "Naofal Al-Dhahir"], "title": "Flexible Intelligent Metasurface for Reconfiguring Radio Environments", "comment": "6 pages, 5 figures, published in IEEE TVT", "summary": "Flexible intelligent metasurface (FIM) technology holds immense potential for\nincreasing the spectral efficiency and energy efficiency of wireless networks.\nIn contrast to traditional rigid reconfigurable intelligent surfaces (RIS), an\nFIM consists of an array of elements, each capable of independently tuning\nelectromagnetic signals, while flexibly adjusting its position along the\ndirection perpendicular to the surface. In contrast to traditional rigid\nmetasurfaces, FIM is capable of morphing its surface shape to attain better\nchannel conditions. In this paper, we investigate the single-input\nsingle-output (SISO) and multiple-input single-output (MISO) communication\nsystems aided by a transmissive FIM. In the SISO scenario, we jointly optimize\nthe FIM phase shift matrix and surface shape to maximize the end-to-end channel\ngain. First, we derive the optimal phase-shift matrix for each tentative FIM\nsurface shape to decompose the high-dimensional non-convex optimization problem\ninto multiple one-dimensional subproblems. Then, we utilize the particle swarm\noptimization (PSO) algorithm and the multi-interval gradient descent (MIGD)\nmethod for updating the FIM's surface shape to maximize the channel gain. In\nthe MISO scenario, we jointly optimize the transmit beamforming, the FIM\nsurface shape, and the phase shift matrix to maximize the channel gain. To\ntackle this complex problem with multiple highly coupled variables, an\nefficient alternating optimization algorithm is proposed. Simulation results\ndemonstrate that FIM significantly improves channel gain compared to\ntraditional RIS and exhibits good adaptability to multipath channels."}
{"id": "2510.07840", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07840", "abs": "https://arxiv.org/abs/2510.07840", "authors": ["Ji Yu", "Yang shuo", "Xu Yuetonghui", "Liu Mengmei", "Ji Qiang", "Han Zerui"], "title": "ACMID: Automatic Curation of Musical Instrument Dataset for 7-Stem Music Source Separation", "comment": null, "summary": "Most current music source separation (MSS) methods rely on supervised\nlearning, limited by training data quantity and quality. Though web-crawling\ncan bring abundant data, platform-level track labeling often causes metadata\nmismatches, impeding accurate \"audio-label\" pair acquisition. To address this,\nwe present ACMID: a dataset for MSS generated through web crawling of extensive\nraw data, followed by automatic cleaning via an instrument classifier built on\na pre-trained audio encoder that filters and aggregates clean segments of\ntarget instruments from the crawled tracks, resulting in the refined\nACMID-Cleaned dataset. Leveraging abundant data, we expand the conventional\nclassification from 4-stem (Vocal/Bass/Drums/Others) to 7-stem\n(Piano/Drums/Bass/Acoustic Guitar/Electric Guitar/Strings/Wind-Brass), enabling\nhigh granularity MSS systems. Experiments on SOTA MSS model demonstrates two\nkey results: (i) MSS model trained with ACMID-Cleaned achieved a 2.39dB\nimprovement in SDR performance compared to that with ACMID-Uncleaned,\ndemostrating the effectiveness of our data cleaning procedure; (ii)\nincorporating ACMID-Cleaned to training enhances MSS model's average\nperformance by 1.16dB, confirming the value of our dataset. Our data crawling\ncode, cleaning model code and weights are available at:\nhttps://github.com/scottishfold0621/ACMID."}
{"id": "2510.08373", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08373", "abs": "https://arxiv.org/abs/2510.08373", "authors": ["Hanke Xie", "Dake Guo", "Chengyou Wang", "Yue Li", "Wenjie Tian", "Xinfa Zhu", "Xinsheng Wang", "Xiulin Li", "Guanqiong Miao", "Bo Liu", "Lei Xie"], "title": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching", "comment": null, "summary": "Recent advances in text-to-speech (TTS) synthesis, particularly those\nleveraging large language models (LLMs), have significantly improved\nexpressiveness and naturalness. However, generating human-like, interactive\ndialogue speech remains challenging. Current systems face limitations due to\nthe scarcity of dual-track data and difficulties in achieving naturalness,\ncontextual coherence, and interactional dynamics, such as turn-taking,\noverlapping speech, and speaker consistency, in multi-turn conversations. To\naddress these challenges, we propose DialoSpeech, a dual-track architecture\ncombining a large language model with Chunked Flow Matching for expressive,\nhuman-like dialogue speech synthesis. DialoSpeech generates natural multi-turn\nconversations with coherent speaker turns and natural overlaps, supporting both\nChinese and English and cross-lingual speech synthesis. We introduce a data\nprocessing pipeline to construct dual-track dialogue datasets, facilitating\nscalable training and experimental validation. Experiments show that our model\noutperforms baselines, offering a solution for generating human-like spoken\ndialogues. Audio samples are available at\nhttps://tiamojames.github.io/DialoSpeech"}
{"id": "2510.07503", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07503", "abs": "https://arxiv.org/abs/2510.07503", "authors": ["Marcelo A. Colominas", "Stefan Steinerberger", "Hau-Tieng Wu"], "title": "Time-Frequency Filtering Meets Graph Clustering", "comment": null, "summary": "We show that the problem of identifying different signal components from a\ntime-frequency representation can be equivalently phrased as a graph clustering\nproblem: given a graph $G=(V,E)$ one aims to identify `clusters', subgraphs\nthat are strongly connected and have relatively few connections between them.\nThe graph clustering problem is well studied, we show how these ideas can\nsuggest (many) new ways to identify signal components. Numerical experiments\nillustrate the ideas."}
{"id": "2510.07979", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.07979", "abs": "https://arxiv.org/abs/2510.07979", "authors": ["Wei Wang", "Rong Cao", "Yi Guo", "Zhengyang Chen", "Kuan Chen", "Yuanyuan Huo"], "title": "IntMeanFlow: Few-step Speech Generation with Integral Velocity Distillation", "comment": null, "summary": "Flow-based generative models have greatly improved text-to-speech (TTS)\nsynthesis quality, but inference speed remains limited by the iterative\nsampling process and multiple function evaluations (NFE). The recent MeanFlow\nmodel accelerates generation by modeling average velocity instead of\ninstantaneous velocity. However, its direct application to TTS encounters\nchallenges, including GPU memory overhead from Jacobian-vector products (JVP)\nand training instability due to self-bootstrap processes. To address these\nissues, we introduce IntMeanFlow, a framework for few-step speech generation\nwith integral velocity distillation. By approximating average velocity with the\nteacher's instantaneous velocity over a temporal interval, IntMeanFlow\neliminates the need for JVPs and self-bootstrap, improving stability and\nreducing GPU memory usage. We also propose the Optimal Step Sampling Search\n(O3S) algorithm, which identifies the model-specific optimal sampling steps,\nimproving speech synthesis without additional inference overhead. Experiments\nshow that IntMeanFlow achieves 1-NFE inference for token-to-spectrogram and\n3-NFE for text-to-spectrogram tasks while maintaining high-quality synthesis.\nDemo samples are available at https://vvwangvv.github.io/intmeanflow."}
{"id": "2510.08392", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08392", "abs": "https://arxiv.org/abs/2510.08392", "authors": ["Guobin Ma", "Jixun Yao", "Ziqian Ning", "Yuepeng Jiang", "Lingxin Xiong", "Lei Xie", "Pengcheng Zhu"], "title": "MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean Flows", "comment": null, "summary": "Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker\nto any unseen target speaker while preserving linguistic content. Growing\napplication scenarios demand models with streaming inference capabilities. This\nhas created a pressing need for models that are simultaneously fast,\nlightweight, and high-fidelity. However, existing streaming methods typically\nrely on either autoregressive (AR) or non-autoregressive (NAR) frameworks,\nwhich either require large parameter sizes to achieve strong performance or\nstruggle to generalize to unseen speakers. In this study, we propose MeanVC, a\nlightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion\ntransformer with a chunk-wise autoregressive denoising strategy, combining the\nstrengths of both AR and NAR paradigms for efficient streaming processing. By\nintroducing mean flows, MeanVC regresses the average velocity field during\ntraining, enabling zero-shot VC with superior speech quality and speaker\nsimilarity in a single sampling step by directly mapping from the start to the\nendpoint of the flow trajectory. Additionally, we incorporate diffusion\nadversarial post-training to mitigate over-smoothing and further enhance speech\nquality. Experimental results demonstrate that MeanVC significantly outperforms\nexisting zero-shot streaming VC systems, achieving superior conversion quality\nwith higher efficiency and significantly fewer parameters. Audio demos and code\nare publicly available at https://aslp-lab.github.io/MeanVC."}
{"id": "2510.07668", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.07668", "abs": "https://arxiv.org/abs/2510.07668", "authors": ["Xingtao Yang", "Zhenghe Guo", "Siyun Liang", "Zhaohui Yang", "Chen Zhu", "Zhaoyang Zhang"], "title": "Rate Maximization for UAV-assisted ISAC System with Fluid Antennas", "comment": null, "summary": "This letter investigates the joint sensing problem between unmanned aerial\nvehicles (UAV) and base stations (BS) in integrated sensing and communication\n(ISAC) systems with fluid antennas (FA). In this system, the BS enhances its\nsensing performance through the UAV's perception system. We aim to maximize the\ncommunication rate between the BS and UAV while guaranteeing the joint system's\nsensing capability. By establishing a communication-sensing model with convex\noptimization properties, we decompose the problem and apply convex optimization\nto progressively solve key variables. An iterative algorithm employing an\nalternating optimization approach is subsequently developed to determine the\noptimal solution, significantly reducing the solution complexity. Simulation\nresults validate the algorithm's effectiveness in balancing system performance."}
{"id": "2510.08004", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08004", "abs": "https://arxiv.org/abs/2510.08004", "authors": ["Honghong Wang", "Jing Deng", "Rong Zheng"], "title": "Personality-Enhanced Multimodal Depression Detection in the Elderly", "comment": "6 pages,2 figures,accepted by ACM Multimedia Asia 2025", "summary": "This paper presents our solution to the Multimodal Personality-aware\nDepression Detection (MPDD) challenge at ACM MM 2025. We propose a multimodal\ndepression detection model in the Elderly that incorporates personality\ncharacteristics. We introduce a multi-feature fusion approach based on a\nco-attention mechanism to effectively integrate LLDs, MFCCs, and Wav2Vec\nfeatures in the audio modality. For the video modality, we combine\nrepresentations extracted from OpenFace, ResNet, and DenseNet to construct a\ncomprehensive visual feature set. Recognizing the critical role of personality\nin depression detection, we design an interaction module that captures the\nrelationships between personality traits and multimodal features. Experimental\nresults from the MPDD Elderly Depression Detection track demonstrate that our\nmethod significantly enhances performance, providing valuable insights for\nfuture research in multimodal depression detection among elderly populations."}
{"id": "2510.07840", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07840", "abs": "https://arxiv.org/abs/2510.07840", "authors": ["Ji Yu", "Yang shuo", "Xu Yuetonghui", "Liu Mengmei", "Ji Qiang", "Han Zerui"], "title": "ACMID: Automatic Curation of Musical Instrument Dataset for 7-Stem Music Source Separation", "comment": null, "summary": "Most current music source separation (MSS) methods rely on supervised\nlearning, limited by training data quantity and quality. Though web-crawling\ncan bring abundant data, platform-level track labeling often causes metadata\nmismatches, impeding accurate \"audio-label\" pair acquisition. To address this,\nwe present ACMID: a dataset for MSS generated through web crawling of extensive\nraw data, followed by automatic cleaning via an instrument classifier built on\na pre-trained audio encoder that filters and aggregates clean segments of\ntarget instruments from the crawled tracks, resulting in the refined\nACMID-Cleaned dataset. Leveraging abundant data, we expand the conventional\nclassification from 4-stem (Vocal/Bass/Drums/Others) to 7-stem\n(Piano/Drums/Bass/Acoustic Guitar/Electric Guitar/Strings/Wind-Brass), enabling\nhigh granularity MSS systems. Experiments on SOTA MSS model demonstrates two\nkey results: (i) MSS model trained with ACMID-Cleaned achieved a 2.39dB\nimprovement in SDR performance compared to that with ACMID-Uncleaned,\ndemostrating the effectiveness of our data cleaning procedure; (ii)\nincorporating ACMID-Cleaned to training enhances MSS model's average\nperformance by 1.16dB, confirming the value of our dataset. Our data crawling\ncode, cleaning model code and weights are available at:\nhttps://github.com/scottishfold0621/ACMID."}
{"id": "2510.07814", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.07814", "abs": "https://arxiv.org/abs/2510.07814", "authors": ["Javad Sayyadi", "Mahdi Nangir", "Mahmood Mohassel Feghhi", "Hamid Sayyadi"], "title": "Utilizing Model-Free Reinforcement Learning for Optimizing Secure Multi-Party Computation Protocols", "comment": "The Third National Conference on Energy, Industrial Automation, and\n  Artificial Intelligence Arak University of Technology, Arak, Iran", "summary": "In this manuscript, we explore the application of model-free reinforcement\nlearning in optimizing secure multiparty computation (SMPC) protocols. SMPC is\na crucial tool for performing computations on private data without the need to\ndisclose it, holding significant importance in various domains, including\ninformation security and privacy. However, the efficiency of current protocols\nis often suboptimal due to computational and communicational complexities. Our\nproposed approach leverages model-free reinforcement learning algorithms to\nenhance the performance of these protocols. We have designed a reinforcement\nlearning model capable of dynamically learning and adapting optimal strategies\nfor secure computations. Our experimental results demonstrate that employing\nthis method leads to a substantial reduction in execution time and\ncommunication costs of the protocols. These achievements highlight the high\npotential of reinforcement learning in improving the efficiency of secure\nmultiparty computation protocols, providing an effective solution to the\nexisting challenges in this field."}
{"id": "2510.08062", "categories": ["cs.SD", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08062", "abs": "https://arxiv.org/abs/2510.08062", "authors": ["Fabio Morreale", "Wiebke Hutiri", "Joan Serrà", "Alice Xiang", "Yuki Mitsufuji"], "title": "Attribution-by-design: Ensuring Inference-Time Provenance in Generative Music Systems", "comment": null, "summary": "The rise of AI-generated music is diluting royalty pools and revealing\nstructural flaws in existing remuneration frameworks, challenging the\nwell-established artist compensation systems in the music industry. Existing\ncompensation solutions, such as piecemeal licensing agreements, lack\nscalability and technical rigour, while current data attribution mechanisms\nprovide only uncertain estimates and are rarely implemented in practice. This\npaper introduces a framework for a generative music infrastructure centred on\ndirect attribution, transparent royalty distribution, and granular control for\nartists and rights' holders. We distinguish ontologically between the training\nset and the inference set, which allows us to propose two complementary forms\nof attribution: training-time attribution and inference-time attribution. We\nhere favour inference-time attribution, as it enables direct, verifiable\ncompensation whenever an artist's catalogue is used to condition a generated\noutput. Besides, users benefit from the ability to condition generations on\nspecific songs and receive transparent information about attribution and\npermitted usage. Our approach offers an ethical and practical solution to the\npressing need for robust compensation mechanisms in the era of AI-generated\nmusic, ensuring that provenance and fairness are embedded at the core of\ngenerative systems."}
{"id": "2510.08004", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08004", "abs": "https://arxiv.org/abs/2510.08004", "authors": ["Honghong Wang", "Jing Deng", "Rong Zheng"], "title": "Personality-Enhanced Multimodal Depression Detection in the Elderly", "comment": "6 pages,2 figures,accepted by ACM Multimedia Asia 2025", "summary": "This paper presents our solution to the Multimodal Personality-aware\nDepression Detection (MPDD) challenge at ACM MM 2025. We propose a multimodal\ndepression detection model in the Elderly that incorporates personality\ncharacteristics. We introduce a multi-feature fusion approach based on a\nco-attention mechanism to effectively integrate LLDs, MFCCs, and Wav2Vec\nfeatures in the audio modality. For the video modality, we combine\nrepresentations extracted from OpenFace, ResNet, and DenseNet to construct a\ncomprehensive visual feature set. Recognizing the critical role of personality\nin depression detection, we design an interaction module that captures the\nrelationships between personality traits and multimodal features. Experimental\nresults from the MPDD Elderly Depression Detection track demonstrate that our\nmethod significantly enhances performance, providing valuable insights for\nfuture research in multimodal depression detection among elderly populations."}
{"id": "2510.07827", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.07827", "abs": "https://arxiv.org/abs/2510.07827", "authors": ["Joseph M. Carlson", "Nitish V. Deshpande", "Miguel Rodrigo Castellanos", "Robert W. Heath Jr"], "title": "Wideband dynamic metasurface antenna performance with practical design characteristics", "comment": null, "summary": "Dynamic metasurface antennas (DMA) provide low-power beamforming through\nreconfigurable radiative slots. Each slot has a tunable component that consumes\nlow power compared to typical analog components like phase shifters. This makes\nDMAs a potential candidate to minimize the power consumption of multiple-input\nmultiple-output (MIMO) antenna arrays. In this paper, we investigate the use of\nDMAs in a wideband communication setting with practical DMA design\ncharacteristics. We develop approximations for the DMA beamforming gain that\naccount for the effects of waveguide attenuation, element\nfrequency-selectivity, and limited reconfigurability of the tunable components\nas a function of the signal bandwidth. The approximations allow for key\ninsights into the wideband performance of DMAs in terms of different design\nvariables. We develop a simple successive beamforming algorithm to improve the\nwideband performance of DMAs by sequentially configuring each DMA element.\nSimulation results for a line-of-sight (LOS) wideband system show the accuracy\nof the approximations with the simulated DMA model in terms of spectral\nefficiency. We also find that the proposed successive beamforming algorithm\nincreases the overall spectral efficiency of the DMA-based wideband system\ncompared with a baseline DMA beamforming method."}
{"id": "2510.08078", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08078", "abs": "https://arxiv.org/abs/2510.08078", "authors": ["Liyang Chen", "Hongkai Chen", "Yujun Cai", "Sifan Li", "Qingwen Ye", "Yiwei Wang"], "title": "Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation", "comment": null, "summary": "Video-to-Audio generation has made remarkable strides in automatically\nsynthesizing sound for video. However, existing evaluation metrics, which focus\non semantic and temporal alignment, overlook a critical failure mode: models\noften generate acoustic events, particularly speech and music, that have no\ncorresponding visual source. We term this phenomenon Insertion Hallucination\nand identify it as a systemic risk driven by dataset biases, such as the\nprevalence of off-screen sounds, that remains completely undetected by current\nmetrics. To address this challenge, we first develop a systematic evaluation\nframework that employs a majority-voting ensemble of multiple audio event\ndetectors. We also introduce two novel metrics to quantify the prevalence and\nseverity of this issue: IH@vid (the fraction of videos with hallucinations) and\nIH@dur (the fraction of hallucinated duration). Building on this, we propose\nPosterior Feature Correction, a novel training-free inference-time method that\nmitigates IH. PFC operates in a two-pass process: it first generates an initial\naudio output to detect hallucinated segments, and then regenerates the audio\nafter masking the corresponding video features at those timestamps. Experiments\non several mainstream V2A benchmarks first reveal that state-of-the-art models\nsuffer from severe IH. In contrast, our PFC method reduces both the prevalence\nand duration of hallucinations by over 50\\% on average, without degrading, and\nin some cases even improving, conventional metrics for audio quality and\ntemporal synchronization. Our work is the first to formally define,\nsystematically measure, and effectively mitigate Insertion Hallucination,\npaving the way for more reliable and faithful V2A models."}
{"id": "2510.08176", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08176", "abs": "https://arxiv.org/abs/2510.08176", "authors": ["Eleonora Mancini", "Joan Serrà", "Paolo Torroni", "Yuki Mitsufuji"], "title": "Leveraging Whisper Embeddings for Audio-based Lyrics Matching", "comment": null, "summary": "Audio-based lyrics matching can be an appealing alternative to other\ncontent-based retrieval approaches, but existing methods often suffer from\nlimited reproducibility and inconsistent baselines. In this work, we introduce\nWEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings\nfor lyrics matching tasks. WEALY establishes robust and transparent baselines,\nwhile also exploring multimodal extensions that integrate textual and acoustic\nfeatures. Through extensive experiments on standard datasets, we demonstrate\nthat WEALY achieves a performance comparable to state-of-the-art methods that\nlack reproducibility. In addition, we provide ablation studies and analyses on\nlanguage robustness, loss functions, and embedding strategies. This work\ncontributes a reliable benchmark for future research, and underscores the\npotential of speech technologies for music information retrieval tasks."}
{"id": "2510.07843", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.07843", "abs": "https://arxiv.org/abs/2510.07843", "authors": ["Jaebum Park", "Chan-Byoung Chae", "Robert W. Heath Jr"], "title": "Accelerating vRAN and O-RAN with SIMD: Architectural Perspectives and Performance Evaluation", "comment": "7 pages, 5 figures, submitted to IEEE Communications Magazine", "summary": "The evolution of radio access networks (RANs) toward virtualization and\nopenness creates new opportunities for flexible, cost-effective, and\nhigh-performance deployments. Achieving real-time and energy-efficient baseband\nprocessing on commercial off-the-shelf platforms, however, remains a critical\nchallenge. This article explores how single instruction multiple data (SIMD)\narchitectures can accelerate RAN workloads. We first outline why key\nphysical-layer functions, such as channel estimation, multiple-input\nmultiple-output (MIMO) detection, and forward error correction, are well\naligned with SIMD's data-level parallelism. We then present practical design\nguidelines and prototype results, showing significant improvements in\nthroughput and energy efficiency compared to conventional CPU-only processing,\nwhile retaining programmability and ease of integration. Finally, we discuss\nopen challenges in workload balancing and hardware heterogeneity, and highlight\nthe role of SIMD as an enabling technology for flexible, efficient, and\nsustainable 6G-ready RANs."}
{"id": "2510.08176", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08176", "abs": "https://arxiv.org/abs/2510.08176", "authors": ["Eleonora Mancini", "Joan Serrà", "Paolo Torroni", "Yuki Mitsufuji"], "title": "Leveraging Whisper Embeddings for Audio-based Lyrics Matching", "comment": null, "summary": "Audio-based lyrics matching can be an appealing alternative to other\ncontent-based retrieval approaches, but existing methods often suffer from\nlimited reproducibility and inconsistent baselines. In this work, we introduce\nWEALY, a fully reproducible pipeline that leverages Whisper decoder embeddings\nfor lyrics matching tasks. WEALY establishes robust and transparent baselines,\nwhile also exploring multimodal extensions that integrate textual and acoustic\nfeatures. Through extensive experiments on standard datasets, we demonstrate\nthat WEALY achieves a performance comparable to state-of-the-art methods that\nlack reproducibility. In addition, we provide ablation studies and analyses on\nlanguage robustness, loss functions, and embedding strategies. This work\ncontributes a reliable benchmark for future research, and underscores the\npotential of speech technologies for music information retrieval tasks."}
{"id": "2510.07948", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.07948", "abs": "https://arxiv.org/abs/2510.07948", "authors": ["Mats Viberg", "Daniele Gerosa", "Tomas McKelvey", "Thomas Eriksson"], "title": "Statistical Analysis of Target Parameter Estimation Using Passive Radar", "comment": "Accepted to 2025 IEEE International Workshop on Computational\n  Advances in Multi-Sensor Adaptive Processing (CAMSAP)", "summary": "A passive radar system uses one or more so-called Illuminators of Opportunity\n(IO) to detect and localize targets. In such systems, a reference channel is\noften used at each receiving node to capture the transmitted IO signal, while\ntargets are detected using the main surveillance channel. The purpose of the\npresent contribution is to analyze a method for estimating the target\nparameters in such a system. Specifically, we quantify the additional error\ncontribution due to not knowing the transmitted IO waveform perfectly. A\nsufficient condition for this error to be negligible as compared to errors due\nto clutter and noise in the surveillance channel is then given."}
{"id": "2510.08392", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08392", "abs": "https://arxiv.org/abs/2510.08392", "authors": ["Guobin Ma", "Jixun Yao", "Ziqian Ning", "Yuepeng Jiang", "Lingxin Xiong", "Lei Xie", "Pengcheng Zhu"], "title": "MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean Flows", "comment": null, "summary": "Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker\nto any unseen target speaker while preserving linguistic content. Growing\napplication scenarios demand models with streaming inference capabilities. This\nhas created a pressing need for models that are simultaneously fast,\nlightweight, and high-fidelity. However, existing streaming methods typically\nrely on either autoregressive (AR) or non-autoregressive (NAR) frameworks,\nwhich either require large parameter sizes to achieve strong performance or\nstruggle to generalize to unseen speakers. In this study, we propose MeanVC, a\nlightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion\ntransformer with a chunk-wise autoregressive denoising strategy, combining the\nstrengths of both AR and NAR paradigms for efficient streaming processing. By\nintroducing mean flows, MeanVC regresses the average velocity field during\ntraining, enabling zero-shot VC with superior speech quality and speaker\nsimilarity in a single sampling step by directly mapping from the start to the\nendpoint of the flow trajectory. Additionally, we incorporate diffusion\nadversarial post-training to mitigate over-smoothing and further enhance speech\nquality. Experimental results demonstrate that MeanVC significantly outperforms\nexisting zero-shot streaming VC systems, achieving superior conversion quality\nwith higher efficiency and significantly fewer parameters. Audio demos and code\nare publicly available at https://aslp-lab.github.io/MeanVC."}
{"id": "2510.08011", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.08011", "abs": "https://arxiv.org/abs/2510.08011", "authors": ["Wei Zhang", "Ding Chen", "Bin Zhou"], "title": "Over-The-Air Phase Calibration of Spaceborne Phased Array for LEO Satellite Communications", "comment": "5 pages,3 figures,accepted by IEEE WCL", "summary": "To avoid the unpredictable phase deviations of the spaceborne phased array\n(SPA), this paper considers the over-the-air (OTA) phase calibration of the SPA\nfor the low earth orbit (LEO) satellite communications, where the phase\ndeviations of the SPA and the unknown channel are jointly estimated with\nmultiple transmissions of the pilots. Moreover, the Cramer Rao Bound (CRB) is\nderived, and the optimization of beam patterns is also presented to lower the\nroot mean squared error (RMSE) of the OTA calibration. The simulation results\nverify the effectiveness of the proposed OTA phase calibration algorithm as the\nRMSEs of the phase estimates closely approach the corresponding CRB, and the\nbeam pattern optimization scheme is also validated for more than 4dB gain of\nSNR over the randomly generated beam patterns."}
{"id": "2510.08140", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.08140", "abs": "https://arxiv.org/abs/2510.08140", "authors": ["Yancheng Wang", "Chuan Huang", "Songyang Zhang", "Guanying Chen", "Wei Guo", "Shenglun Lan", "Lexi Xu", "Xinzhou Cheng", "Xiongyan Tang", "Shuguang Cui"], "title": "Towards Precise Channel Knowledge Map: Exploiting Environmental Information from 2D Visuals to 3D Point Clouds", "comment": null, "summary": "The substantial communication resources consumed by conventional pilot-based\nchannel sounding impose an unsustainable overhead, presenting a critical\nscalability challenge for the future 6G networks characterized by massive\nchannel dimensions, ultra-wide bandwidth, and dense user deployments. As a\ngeneralization of radio map, channel knowledge map (CKM) offers a paradigm\nshift, enabling access to location-tagged channel information without\nexhaustive measurements. To fully utilize the power of CKM, this work\nhighlights the necessity of leveraging three-dimensional (3D) environmental\ninformation, beyond conventional two-dimensional (2D) visual representations,\nto construct high-precision CKMs. Specifically, we present a novel framework\nthat integrates 3D point clouds into CKM construction through a hybrid model-\nand data-driven approach, with extensive case studies in real-world scenarios.\nThe experimental results demonstrate the potential for constructing precise\nCKMs based on 3D environments enhanced with semantic understanding, together\nwith their applications in the next-generation wireless communications. We also\nrelease a real-world dataset of measured channel paired with high-resolution 3D\nenvironmental data to support future research and validation."}
{"id": "2510.08144", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.08144", "abs": "https://arxiv.org/abs/2510.08144", "authors": ["Jiawei Zhang", "Shihan Wang", "Jienan Chen", "Fan Wu", "Jiyun Tao", "Zheqi Gu"], "title": "Channel Charting based Fast Beam Tracking Design and Implementation", "comment": null, "summary": "In the beyond fifth-generation (B5G) and upcoming sixth-generation (6G)\nwireless communication systems, millimeter (mmWave) wave technology is a\npromising solution for offering additional bandwidth resources and mitigating\nspectrum congestion. Beam tracking is an essential procedure for providing\nreliable communication services in the mmWave communication system, with the\nchallenge of providing consistent and accurate tracking performance. In this\nstudy, we introduce a low-overhead beam tracking algorithm based on channel\ncharting, which significantly reduces beam scanning times during the tracking\nprocess. By projecting the beam information to the channel chart, the beam\ntracking problem is transformed into the acquisition of the beam cluster in the\nchannel chart. Leveraging contrastive learning, the proposed channel chart\nprojects high-dimensional channel state information into a low-dimensional\nfeature space that preserves spatial proximities. Using a dynamic candidate\nbeam acquisition strategy, the complexity of our beam tracking algorithm is\nsignificantly reduced. The proposed algorithm significantly reduces scanning\ncomplexity while maintaining high prediction accuracy, achieving an accuracy of\n98.27\\% in simulation environments. Compared to existing methods, the proposed\nmethod can reduce beam scanning times by up to 55.9\\%. In addition, we also\nperformed field tests, and the measured results demonstrated excellent\ncommunication quality during mobility."}
{"id": "2510.08161", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.08161", "abs": "https://arxiv.org/abs/2510.08161", "authors": ["Yaakov Libero", "Itzik Klein"], "title": "Attitude and Heading Estimation in Symmetrical Inertial Arrays", "comment": null, "summary": "Attitude and heading reference systems (AHRS) play a central role in\nautonomous navigation systems on land, air and maritime platforms. AHRS utilize\ninertial sensor measurements to estimate platform orientation. In recent years,\nthere has been increasing interest in multiple inertial measurement units\n(MIMU) arrays to improve navigation accuracy and robustness. A particularly\nchallenging MIMU implementation is the gyro-free (GF) configuration, in which\nangular velocity is derived solely from accelerometer measurements. While the\nGF configurations have multiple benefits, including outlier detection and in\nangular acceleration measurements, their main drawbacks are inherent\ninstability and an increased divergence rate. To address these shortcomings, we\nintroduce a novel symmetrical MIMU formulation, in which the IMUs are arranged\nin symmetric diagonal pairs to decouple linear and rotational acceleration\ncomponents. To this end, we derive the theoretical foundations for the\nsymmetrical MIMU formulation of the GF equations, develop a nonlinear least\nsquares estimation process, and integrate statistical hypothesis testing into\nan AHRS error-state extended Kalman filter. We validate our approach using\nreal-world datasets containing 85 minutes of navigation data recorded on both\nairborne and land platforms. Our results demonstrated a 30\\% average reduction\nin attitude estimation errors, rotation detection accuracy exceeding 95\\%\nimprovement, and significantly improved stability compared to a standard GF\nimplementation. These results enable reliable GF navigation in applications\nwhere gyroscopes are unavailable, unreliable, or energy-constrained. Common\nexamples include miniature platforms, computational-constraint platforms, and\nlong-endurance marine platforms."}

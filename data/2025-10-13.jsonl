{"id": "2510.08580", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08580", "abs": "https://arxiv.org/abs/2510.08580", "authors": ["Benjamin Shiue-Hal Chou", "Purvish Jajal", "Nick John Eliopoulos", "James C. Davis", "George K. Thiruvathukal", "Kristen Yeon-Ji Yun", "Yung-Hsiang Lu"], "title": "LadderSym: A Multimodal Interleaved Transformer for Music Practice Error Detection", "comment": "Under Submission", "summary": "Music learners can greatly benefit from tools that accurately detect errors\nin their practice. Existing approaches typically compare audio recordings to\nmusic scores using heuristics or learnable models. This paper introduces\n\\textit{LadderSym}, a novel Transformer-based method for music error detection.\n\\textit{LadderSym} is guided by two key observations about the state-of-the-art\napproaches: (1) late fusion limits inter-stream alignment and cross-modality\ncomparison capability; and (2) reliance on score audio introduces ambiguity in\nthe frequency spectrum, degrading performance in music with concurrent notes.\nTo address these limitations, \\textit{LadderSym} introduces (1) a two-stream\nencoder with inter-stream alignment modules to improve audio comparison\ncapabilities and error detection F1 scores, and (2) a multimodal strategy that\nleverages both audio and symbolic scores by incorporating symbolic\nrepresentations as decoder prompts, reducing ambiguity and improving F1 scores.\nWe evaluate our method on the \\textit{MAESTRO-E} and \\textit{CocoChorales-E}\ndatasets by measuring the F1 score for each note category. Compared to the\nprevious state of the art, \\textit{LadderSym} more than doubles F1 for missed\nnotes on \\textit{MAESTRO-E} (26.8\\% $\\rightarrow$ 56.3\\%) and improves extra\nnote detection by 14.4 points (72.0\\% $\\rightarrow$ 86.4\\%). Similar gains are\nobserved on \\textit{CocoChorales-E}. This work introduces general insights\nabout comparison models that could inform sequence evaluation tasks for\nreinforcement Learning, human skill assessment, and model evaluation."}
{"id": "2510.08581", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08581", "abs": "https://arxiv.org/abs/2510.08581", "authors": ["Hansol Park", "Hoseong Ahn", "Junwon Moon", "Yejin Lee", "Kyuhong Shim"], "title": "Evaluating Hallucinations in Multimodal LLMs with Spoken Queries under Diverse Acoustic Conditions", "comment": null, "summary": "Hallucinations in vision-language models have been extensively studied using\nbenchmarks that probe reliability in image-text settings. In contrast, the\neffect of spoken queries on multimodal hallucinations remains largely\nunexplored, despite the growing role of voice-driven interfaces. In this work,\nwe investigate how spoken input influences hallucinations in multimodal large\nlanguage models. We present RePOPE-Spk, an audio-augmented extension of the\nRePOPE benchmark, where queries are provided as speech under diverse acoustic\nconditions. Using RePOPE-Spk, we systematically evaluate both proprietary and\nopen-source models. Experimental results show that hallucinations escalate when\nqueries are spoken rather than written: error rates increase by 3% under clean\nspeech and by up to 20% with environmental noise. Input order and query length\nfurther affect robustness, while strategies such as many-shot prompting and\nchain-of-thought reasoning offer partial but insufficient mitigation. These\nfindings highlight a critical and underexplored challenge, opening new\ndirections for building reliable voice interface systems."}
{"id": "2510.08587", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08587", "abs": "https://arxiv.org/abs/2510.08587", "authors": ["Tianheng Zhu", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation", "comment": "Main paper (6 pages). Accepted for publication by IEEE International\n  Conference on Systems, Man, and Cybernetics 2025", "summary": "This paper presents EGSTalker, a real-time audio-driven talking head\ngeneration framework based on 3D Gaussian Splatting (3DGS). Designed to enhance\nboth speed and visual fidelity, EGSTalker requires only 3-5 minutes of training\nvideo to synthesize high-quality facial animations. The framework comprises two\nkey stages: static Gaussian initialization and audio-driven deformation. In the\nfirst stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network\n(KAN) are used to extract spatial features and construct a compact 3D Gaussian\nrepresentation. In the second stage, we propose an Efficient Spatial-Audio\nAttention (ESAA) module to fuse audio and spatial cues, while KAN predicts the\ncorresponding Gaussian deformations. Extensive experiments demonstrate that\nEGSTalker achieves rendering quality and lip-sync accuracy comparable to\nstate-of-the-art methods, while significantly outperforming them in inference\nspeed. These results highlight EGSTalker's potential for real-time multimedia\napplications."}
{"id": "2510.08816", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08816", "abs": "https://arxiv.org/abs/2510.08816", "authors": ["Juan José Burred", "Carmine-Emanuele Cella"], "title": "Audible Networks: Deconstructing and Manipulating Sounds with Deep Non-Negative Autoencoders", "comment": null, "summary": "We propose the use of Non-Negative Autoencoders (NAEs) for sound\ndeconstruction and user-guided manipulation of sounds for creative purposes.\nNAEs offer a versatile and scalable extension of traditional Non-Negative\nMatrix Factorization (NMF)-based approaches for interpretable audio\ndecomposition. By enforcing non-negativity constraints through projected\ngradient descent, we obtain decompositions where internal weights and\nactivations can be directly interpreted as spectral shapes and temporal\nenvelopes, and where components can themselves be listened to as individual\nsound events. In particular, multi-layer Deep NAE architectures enable\nhierarchical representations with an adjustable level of granularity, allowing\nsounds to be deconstructed at multiple levels of abstraction: from high-level\nnote envelopes down to fine-grained spectral details. This framework enables a\nwide new range of expressive, controllable, and randomized sound\ntransformations. We introduce novel manipulation operations including\ncross-component and cross-layer synthesis, hierarchical deconstructions, and\nseveral randomization strategies that control timbre and event density. Through\nvisualizations and resynthesis of practical examples, we demonstrate how NAEs\ncan serve as flexible and interpretable tools for object-based sound editing."}
{"id": "2510.08765", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.08765", "abs": "https://arxiv.org/abs/2510.08765", "authors": ["Mojtaba Amiri", "Rouhollah Amiri"], "title": "UAV-Assisted 3-D Localization for IoT Networks Using a Simple and Efficient TDOA-AOA Estimator", "comment": null, "summary": "This letter proposes an algebraic solution for the problem of 3-D source\nlocalization utilizing the minimum number of measurements, i.e., one Time\nDifference of Arrival (TDOA) and one Angle of Arrival (AOA) pair. The proposed\nmethod employs a closed-form weighted least squares estimator and enables the\npositioning using a single ground station and a cooperative UAV relaying the\nsignal. Analytical derivations and simulation results demonstrate effectiveness\nof the proposed approach, achieving near-optimal performance aligned with the\nCram\\'er-Rao Lower Bound (CRLB) under moderate Gaussian noise conditions."}
{"id": "2510.08585", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08585", "abs": "https://arxiv.org/abs/2510.08585", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carol Espy Wilson"], "title": "Articulation-Informed ASR: Integrating Articulatory Features into ASR via Auxiliary Speech Inversion and Cross-Attention Fusion", "comment": null, "summary": "Prior works have investigated the use of articulatory features as\ncomplementary representations for automatic speech recognition (ASR), but their\nuse was largely confined to shallow acoustic models. In this work, we revisit\narticulatory information in the era of deep learning and propose a framework\nthat leverages articulatory representations both as an auxiliary task and as a\npseudo-input to the recognition model. Specifically, we employ speech inversion\nas an auxiliary prediction task, and the predicted articulatory features are\ninjected into the model as a query stream in a cross-attention module with\nacoustic embeddings as keys and values. Experiments on LibriSpeech demonstrate\nthat our approach yields consistent improvements over strong transformer-based\nbaselines, particularly under low-resource conditions. These findings suggest\nthat articulatory features, once sidelined in ASR research, can provide\nmeaningful benefits when reintroduced with modern architectures."}
{"id": "2510.08878", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08878", "abs": "https://arxiv.org/abs/2510.08878", "authors": ["Yuxuan Jiang", "Zehua Chen", "Zeqian Ju", "Yusheng Dai", "Weibei Dou", "Jun Zhu"], "title": "ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling", "comment": "18 pages, 8 tables, 5 figures", "summary": "Text-to-audio (TTA) generation with fine-grained control signals, e.g.,\nprecise timing control or intelligible speech content, has been explored in\nrecent works. However, constrained by data scarcity, their generation\nperformance at scale is still compromised. In this study, we recast\ncontrollable TTA generation as a multi-task learning problem and introduce a\nprogressive diffusion modeling approach, ControlAudio. Our method adeptly fits\ndistributions conditioned on more fine-grained information, including text,\ntiming, and phoneme features, through a step-by-step strategy. First, we\npropose a data construction method spanning both annotation and simulation,\naugmenting condition information in the sequence of text, timing, and phoneme.\nSecond, at the model training stage, we pretrain a diffusion transformer (DiT)\non large-scale text-audio pairs, achieving scalable TTA generation, and then\nincrementally integrate the timing and phoneme features with unified semantic\nrepresentations, expanding controllability. Finally, at the inference stage, we\npropose progressively guided generation, which sequentially emphasizes more\nfine-grained information, aligning inherently with the coarse-to-fine sampling\nnature of DiT. Extensive experiments show that ControlAudio achieves\nstate-of-the-art performance in terms of temporal accuracy and speech clarity,\nsignificantly outperforming existing methods on both objective and subjective\nevaluations. Demo samples are available at:\nhttps://control-audio.github.io/Control-Audio."}
{"id": "2510.09047", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09047", "abs": "https://arxiv.org/abs/2510.09047", "authors": ["Jiaming Liu", "Rui Wang", "JinJiang Li", "Hong Lin", "Jing Zhang", "Kun Qiu"], "title": "Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission", "comment": "Asia Communications and Photonics Conference 2025", "summary": "We propose a transfer learning-enabled Transformer framework to\nsimultaneously realize accurate modeling and Raman pump design in C+L-band\nsystems. The RMSE for modeling and peak-to-peak GSNR variation/deviation is\nwithin 0.22 dB and 0.86/0.1 dB, respectively."}
{"id": "2510.08586", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08586", "abs": "https://arxiv.org/abs/2510.08586", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Dynamic Stress Detection: A Study of Temporal Progression Modelling of Stress in Speech", "comment": "Accepted at IEEE CogMI 2025", "summary": "Detecting psychological stress from speech is critical in high-pressure\nsettings. While prior work has leveraged acoustic features for stress\ndetection, most treat stress as a static label. In this work, we model stress\nas a temporally evolving phenomenon influenced by historical emotional state.\nWe propose a dynamic labelling strategy that derives fine-grained stress\nannotations from emotional labels and introduce cross-attention-based\nsequential models, a Unidirectional LSTM and a Transformer Encoder, to capture\ntemporal stress progression. Our approach achieves notable accuracy gains on\nMuSE (+5%) and StressID (+18%) over existing baselines, and generalises well to\na custom real-world dataset. These results highlight the value of modelling\nstress as a dynamic construct in speech."}
{"id": "2510.08914", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08914", "abs": "https://arxiv.org/abs/2510.08914", "authors": ["Shulin He", "Zhong-Qiu Wang"], "title": "VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR Virtual Microphone Arrays", "comment": null, "summary": "Blind speech separation (BSS) aims to recover multiple speech sources from\nmulti-channel, multi-speaker mixtures under unknown array geometry and room\nimpulse responses. In unsupervised setup where clean target speech is not\navailable for model training, UNSSOR proposes a mixture consistency (MC) loss\nfor training deep neural networks (DNN) on over-determined training mixtures to\nrealize unsupervised speech separation. However, when the number of microphones\nof the training mixtures decreases, the MC constraint weakens and the\nseparation performance falls dramatically. To address this, we propose\nVM-UNSSOR, augmenting the observed training mixture signals recorded by a\nlimited number of microphones with several higher-SNR virtual-microphone (VM)\nsignals, which are obtained by applying linear spatial demixers (such as IVA\nand spatial clustering) to the observed training mixtures. As linear\nprojections of the observed mixtures, the virtual-microphone signals can\ntypically increase the SNR of each source and can be leveraged to compute extra\nMC losses to improve UNSSOR and address the frequency permutation problem in\nUNSSOR. On the SMS-WSJ dataset, in the over-determined six-microphone,\ntwo-speaker separation setup, VM-UNSSOR reaches 17.1 dB SI-SDR, while UNSSOR\nonly obtains 14.7 dB; and in the determined two-microphone, two-speaker case,\nUNSSOR collapses to -2.7 dB SI-SDR, while VM-UNSSOR achieves 10.7 dB."}
{"id": "2510.09137", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09137", "abs": "https://arxiv.org/abs/2510.09137", "authors": ["Hao Jiang", "Chongjun Ouyang", "Zhaolin Wang", "Yuanwei Liu", "Arumugam Nallanathan", "Zhiguo Ding"], "title": "Pinching-Antenna Assisted Sensing: A Bayesian Cramér-Rao Bound Perspective", "comment": "Submit to IEEE", "summary": "The fundamental sensing limit of pinching-antenna systems (PASS) is studied\nfrom a Bayesian Cram\\'er-Rao bound (BCRB) perspective. Compared to conventional\nCRB, BCRB is independent of the exact values of sensing parameters and is not\nrestricted by the unbiasedness of the estimator, thus offering a practical and\ncomprehensive lower bound for evaluating sensing performance. A system where\nmultiple targets transmit uplink pilots to a single-waveguide PASS under a\ntime-division multiple access (TDMA) scheme is analyzed. For the single-target\nscenario, our analysis reveals a unique mismatch between the sensing centroid\n(i.e., the optimal PA position) and the distribution centroid (i.e., the center\nof the target's prior distribution), underscoring the necessity of dynamic PA\nrepositioning. For the multi-target scenario, two target scheduling protocols\nare proposed: 1) pinch switching (PS), which performs separate pinching\nbeamforming for each time slot, and 2) pinch multiplexing (PM), which applies a\nsingle beamforming configuration across all slots. Based on these protocols,\nboth the total power minimization problem under a BCRB threshold and the\nmin-max BCRB problem under a total power constraint are formulated. By\nleveraging Karush-Kuhn-Tucker (KKT) conditions, these problems are equivalently\nconverted into a search over PA positions and solved using an element-wise\nalgorithm. Numerical results show that i)~PASS, endowed with large-scale\nreconfigurability, can significantly enhance the sensing performance compared\nwith conventional fixed-position arrays, and ii)~PS provides more robust\nperformances than PM at the cost of higher computational complexity."}
{"id": "2510.08599", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08599", "abs": "https://arxiv.org/abs/2510.08599", "authors": ["Yaya Sy", "Christophe Cerisara", "Irina Illina"], "title": "BaldWhisper: Faster Whisper with Head Shearing and Layer Merging", "comment": null, "summary": "Pruning large pre-trained transformers for low-resource languages is\nchallenging, as it often requires massive retraining data to recover\nperformance. For instance, Distill-Whisper prunes Whisper by 40% and retrains\non 21,000 hours of speech, far beyond what is available for most languages. Can\nWhisper be made lighter and faster for edge devices in data-scarce settings?\nFocusing on Bambara with only 32h of speech-to-text data, we propose a new\npruning recipe. Instead of vocabulary pruning, which is unsuitable due to\nfrequent code-switching by Bambara speakers, we compress the embeddings with\nlow-rank decomposition and feature distillation. Rather than removing layers,\nwe merge them to limit performance loss. The final model preserves 90% of the\noriginal performance while being 48% smaller and 2.15x faster on a MacBook Air\nM1."}
{"id": "2510.09016", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09016", "abs": "https://arxiv.org/abs/2510.09016", "authors": ["Zongcai Du", "Guilin Deng", "Xiaofeng Guo", "Xin Gao", "Linke Li", "Kaichang Cheng", "Fubo Han", "Siyu Yang", "Peng Liu", "Pan Zhong", "Qiang Fu"], "title": "DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment", "comment": "under review", "summary": "Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates\nstrong expressiveness but remains limited by data scarcity and model\nscalability. We introduce a two-stage pipeline: a compact seed set of\nhuman-sung recordings is constructed by pairing fixed melodies with diverse\nLLM-generated lyrics, and melody-specific models are trained to synthesize over\n500 hours of high-quality Chinese singing data. Building on this corpus, we\npropose DiTSinger, a Diffusion Transformer with RoPE and qk-norm,\nsystematically scaled in depth, width, and resolution for enhanced fidelity.\nFurthermore, we design an implicit alignment mechanism that obviates\nphoneme-level duration labels by constraining phoneme-to-acoustic attention\nwithin character-level spans, thereby improving robustness under noisy or\nuncertain alignments. Extensive experiments validate that our approach enables\nscalable, alignment-free, and high-fidelity SVS."}
{"id": "2510.09139", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09139", "abs": "https://arxiv.org/abs/2510.09139", "authors": ["Stefania Sardellitti", "Breno C. Bispo", "Fernando A. N. Santos", "Juliano B. Lima"], "title": "Topological Signal Processing Over Cell MultiComplexes Via Cross-Laplacian Operators", "comment": "Submitted to IEEE Transactions on Signal Processing, August 2025", "summary": "One of the key challenges in many research fields is uncovering how different\ninterconnected systems interact within complex networks, typically represented\nas multi-layer networks. Capturing the intra- and cross-layer interactions\namong different domains for analysis and processing calls for topological\nalgebraic descriptors capable of localizing the homologies of different\ndomains, at different scales, according to the learning task. Our first\ncontribution in this paper is to introduce the Cell MultiComplexes (CMCs),\nwhich are novel topological spaces that enable the representation of\nhigher-order interactions among interconnected cell complexes. We introduce\ncross-Laplacian operators as powerful algebraic descriptors of CMC spaces able\nto capture different topological invariants, whether global or local, at\ndifferent resolutions. Using the eigenvectors of these operators as bases for\nthe signal representation, we develop topological signal processing tools for\nsignals defined over CMCs. Then, we focus on the signal spectral representation\nand on the filtering of noisy flows observed over the cross-edges between\ndifferent layers of CMCs. We show that a local signal representation based on\ncross-Laplacians yields a better sparsity/accuracy trade-off compared to\nmonocomplex representations, which provide overcomplete representation of local\nsignals. Finally, we illustrate a topology learning strategy designed to infer\nsecond-order cross-cells between layers, with applications to brain networks\nfor encoding inter-module connectivity patterns."}
{"id": "2510.08618", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08618", "abs": "https://arxiv.org/abs/2510.08618", "authors": ["Rui Hu", "Delai Qiu", "Yining Wang", "Shengping Liu", "Jitao Sang"], "title": "Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization", "comment": null, "summary": "Automatic speech recognition (ASR) systems often struggle with\ndomain-specific terminology, especially in specialized settings such as\nacademic lectures. To address this, we define the SlideASR task, which\nleverages the rich visual information from presentation slides to improve\ntranscription accuracy. Existing pipeline methods for this task tend to be\ncomplex and underperform. Although omni-modal large language models (OLLMs)\nprovide a promising end-to-end framework, they frequently fail in practice by\ndegenerating into simple optical character recognition (OCR) systems. To\novercome this, we propose Visually-Anchored Policy Optimization (VAPO), a novel\npost-training method designed to control the model's reasoning process. Drawing\non the Chain-of-Thought reasoning paradigm, VAPO enforces a structured \"Look\nbefore Transcription\" procedure using a <think><answer> format. Specifically,\nthe model first performs OCR on the slide content within the think step, then\ngenerates the transcription by referencing this recognized visual information\nin the answer step. This reasoning process is optimized via reinforcement\nlearning with four distinct rewards targeting format compliance, OCR accuracy,\nASR quality, and visual anchoring consistency. To support further research, we\nconstruct SlideASR-Bench, a new entity-rich benchmark consisting of a synthetic\ndataset for training and testing, and a challenging real-world set for\nevaluation. Extensive experiments demonstrate that VAPO significantly improves\nrecognition of domain-specific terms, establishing an effective end-to-end\nparadigm for SlideASR."}
{"id": "2510.09025", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09025", "abs": "https://arxiv.org/abs/2510.09025", "authors": ["Louis Bahrman", "Mathieu Fontaine", "Gaël Richard"], "title": "Déréverbération non-supervisée de la parole par modèle hybride", "comment": "in French language", "summary": "This paper introduces a new training strategy to improve speech\ndereverberation systems in an unsupervised manner using only reverberant\nspeech. Most existing algorithms rely on paired dry/reverberant data, which is\ndifficult to obtain. Our approach uses limited acoustic information, like the\nreverberation time (RT60), to train a dereverberation system. Experimental\nresults demonstrate that our method achieves more consistent performance across\nvarious objective metrics than the state-of-the-art."}
{"id": "2510.09154", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09154", "abs": "https://arxiv.org/abs/2510.09154", "authors": ["Tanjim Rahman", "Trupti Ranjan Lenka"], "title": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for High-Power Applications", "comment": "13 pages, 13 figures including DC, RF, and breakdown analysis of\n  field-plated AlGaN/GaN HEMT using TCAD simulations", "summary": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications."}
{"id": "2510.09161", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09161", "abs": "https://arxiv.org/abs/2510.09161", "authors": ["Vincent Martin", "Lorenzo Picinali"], "title": "Impact of HRTF individualisation and head movements in a real/virtual localisation task", "comment": null, "summary": "The objective of Audio Augmented Reality (AAR) applications are to seamlessly\nintegrate virtual sound sources within a real environment. It is critical for\nthese applications that virtual sources are localised precisely at the intended\nposition, and that the acoustic environments are accurately matched.\n  One effective method for spatialising sound on headphones is through\nHead-Related Transfer Functions (HRTFs). These characterise how the physical\nfeatures of a listener modify sound waves before they reach the eardrum. This\nstudy examines the influence of using individualised HRTFs on the localisation\nand the perceived realism of virtual sound sources associated with a real\nvisual object.\n  Participants were tasked with localising virtual and real speech sources\npresented via headphones and through a spherical loudspeaker array,\nrespectively. The assessment focussed on perceived realism and sources\nlocation. All sources were associated with one of thirty real visual sources\n(loudspeakers) arranged in a semi-anechoic room.\n  Various sound source renderings were compared, including single loudspeaker\nrendering and binaural rendering with individualised or non-individualised\nHRTFs. Additionally, the impact of head movements was explored: ten\nparticipants completed the same task with and without the possibility to move\ntheir head.\n  The results showed that using individual HRTFs improved perceived realism but\nnot localisation performance in the static scenario. Surprisingly, the opposite\nwas observed when head movements were possible and encouraged."}
{"id": "2510.09061", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09061", "abs": "https://arxiv.org/abs/2510.09061", "authors": ["Huu Tuong Tu", "Huan Vu", "cuong tien nguyen", "Dien Hy Ngo", "Nguyen Thi Thu Trang"], "title": "O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion", "comment": "EMNLP 2025", "summary": "Traditional voice conversion (VC) methods typically attempt to separate\nspeaker identity and linguistic information into distinct representations,\nwhich are then combined to reconstruct the audio. However, effectively\ndisentangling these factors remains challenging, often leading to information\nloss during training. In this paper, we propose a new approach that leverages\nsynthetic speech data generated by a high-quality, pretrained multispeaker\ntext-to-speech (TTS) model. Specifically, synthetic data pairs that share the\nsame linguistic content but differ in speaker identity are used as input-output\npairs to train the voice conversion model. This enables the model to learn a\ndirect mapping between source and target voices, effectively capturing\nspeaker-specific characteristics while preserving linguistic content.\nAdditionally, we introduce a flexible training strategy for any-to-any voice\nconversion that generalizes well to unseen speakers and new languages,\nenhancing adaptability and performance in zero-shot scenarios. Our experiments\nshow that our proposed method achieves a 16.35% relative reduction in word\nerror rate and a 5.91% improvement in speaker cosine similarity, outperforming\nseveral state-of-the-art methods. Voice conversion samples can be accessed at:\nhttps://oovc-emnlp-2025.github.io/"}
{"id": "2510.09199", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09199", "abs": "https://arxiv.org/abs/2510.09199", "authors": ["Andrei Buciulea", "Bishwadeep Das", "Elvin Isufi", "Antonio G. Marques"], "title": "Learning Product Graphs from Two-dimensional Stationary Signals", "comment": null, "summary": "Graph learning aims to infer a network structure directly from observed data,\nenabling the analysis of complex dependencies in irregular domains. Traditional\nmethods focus on scalar signals at each node, ignoring dependencies along\nadditional dimensions such as time, configurations of the observation device,\nor populations. In this work, we propose a graph signal processing framework\nfor learning graphs from two-dimensional signals, modeled as matrix graph\nsignals generated by joint filtering along both dimensions. This formulation\nleverages the concept of graph stationarity across the two dimensions and\nleverages product graph representations to capture structured dependencies.\nBased on this model, we design an optimization problem that can be solved\nefficiently and provably recovers the optimal underlying\nKronecker/Cartesian/strong product graphs. Experiments on synthetic data\ndemonstrate that our approach achieves higher estimation accuracy and reduced\ncomputational cost compared to existing methods."}
{"id": "2510.09225", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09225", "abs": "https://arxiv.org/abs/2510.09225", "authors": ["Danel Adendorff", "Simon Malan", "Herman Kamper"], "title": "Unsupervised lexicon learning from speech is limited by representations rather than clustering", "comment": "Submitted to ICASSP 2026", "summary": "Zero-resource word segmentation and clustering systems aim to tokenise speech\ninto word-like units without access to text labels. Despite progress, the\ninduced lexicons are still far from perfect. In an idealised setting with gold\nword boundaries, we ask whether performance is limited by the representation of\nword segments, or by the clustering methods that group them into word-like\ntypes. We combine a range of self-supervised speech features\n(continuous/discrete, frame/word-level) with different clustering methods\n(K-means, hierarchical, graph-based) on English and Mandarin data. The best\nsystem uses graph clustering with dynamic time warping on continuous features.\nFaster alternatives use graph clustering with cosine distance on averaged\ncontinuous features or edit distance on discrete unit sequences. Through\ncontrolled experiments that isolate either the representations or the\nclustering method, we demonstrate that representation variability across\nsegments of the same word type -- rather than clustering -- is the primary\nfactor limiting performance."}
{"id": "2510.09065", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09065", "abs": "https://arxiv.org/abs/2510.09065", "authors": ["Akira Takahashi", "Shusuke Takahashi", "Yuki Mitsufuji"], "title": "MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation", "comment": "4 pages, 4 figures, 2 tables", "summary": "We introduce MMAudioSep, a generative model for video/text-queried sound\nseparation that is founded on a pretrained video-to-audio model. By leveraging\nknowledge about the relationship between video/text and audio learned through a\npretrained audio generative model, we can train the model more efficiently,\ni.e., the model does not need to be trained from scratch. We evaluate the\nperformance of MMAudioSep by comparing it to existing separation models,\nincluding models based on both deterministic and generative approaches, and\nfind it is superior to the baseline models. Furthermore, we demonstrate that\neven after acquiring functionality for sound separation via fine-tuning, the\nmodel retains the ability for original video-to-audio generation. This\nhighlights the potential of foundational sound generation models to be adopted\nfor sound-related downstream tasks. Our code is available at\nhttps://github.com/sony/mmaudiosep."}
{"id": "2510.09232", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09232", "abs": "https://arxiv.org/abs/2510.09232", "authors": ["Siddarth Marwaha", "Pawel Kryszkiewicz", "Eduard A. Jorswieck"], "title": "Energy-Efficient Power Control in Single-User M-MIMO-OFDM System with PA Nonlinearity", "comment": null, "summary": "Although multiple works have proposed energy-efficient resource allocation\nschemes for Massive Multiple-Input Multiple-Output (M-MIMO) system, most\napproaches overlook the potential of optimizing Power Amplifier (PA)\ntransmission power while accounting for non-linear distortion effects.\nFurthermore, most M-MIMO studies assume narrow-band transmission, neglecting\nsubcarrier intermodulations at the non-linear PA for an Orthogonal Frequency\nDivision Multiplexing (OFDM) system. Therefore, this work investigates the\nenergy-efficient power allocation for a single-user equipment (UE) M-MIMO\ndownlink (DL) system employing OFDM with nonlinear PAs. Unlike prior works, we\nmodel wide-band transmission using a soft-limiter PA model and derive a\nclosed-form expression for the signal-to-distortion-and-noise ratio (SNDR)\nunder Rayleigh fading and Maximal Ratio Transmission (MRT) precoding. Next, the\nEnergy Efficiency (EE) function is defined considering two PA architectures and\na distorted OFDM signal. We then propose a low complexity root-finding\nalgorithm to maximize EE by transmit power adjustment. Simulation results\ndemonstrate significant EE gains over a fixed PA back-off baseline, with over\n$100\\%$ improvement under both low and high path loss. Our findings reveal how\nthe optimal operating point depends on the antenna count, the PA model, and the\npropagation conditions."}
{"id": "2510.09236", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09236", "abs": "https://arxiv.org/abs/2510.09236", "authors": ["Michele Buccoli", "Yu Du", "Jacob Soendergaard", "Simone Shawn Cazzaniga"], "title": "Effects of automotive microphone frequency response characteristics and noise conditions on speech and ASR quality -- an experimental evaluation", "comment": null, "summary": "Upon choosing microphones for automotive hands-free communication or\nAutomatic Speech Recognition (ASR) applications, OEMs typically specify\nwideband, super wideband or even fullband requirements following established\nstandard recommendations (e.g., ITU-P.1110, ITU-P.1120). In practice, it is\noften challenging to achieve the preferred bandwidth for an automotive\nmicrophone when considering limitations and constraints on microphone placement\ninside the cabin, and the automotive grade environmental robustness\nrequirements. On the other hand, there seems to be no consensus or sufficient\ndata on the effect of each microphone characteristic on the actual performance.\nAs an attempt to answer this question, we used noise signals recorded in real\nvehicles and under various driving conditions to experimentally study the\nrelationship between the microphones' characteristics and the final audio\nquality of speech communication and performance of ASR engines. We focus on how\nvariations in microphone bandwidth and amplitude frequency response shapes\naffect the perceptual speech quality. The speech quality results are compared\nby using ETSI TS 103 281 metrics (S-MOS, N-MOS, G-MOS) and ancillary metrics\nsuch as SNR. The ASR results are evaluated with standard metrics such as Word\nError Rate (WER). Findings from this study provide knowledge in the\nunderstanding of what microphone frequency response characteristics are more\nrelevant for audio quality and choice of proper microphone specifications,\nparticularly for automotive applications."}
{"id": "2510.09072", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09072", "abs": "https://arxiv.org/abs/2510.09072", "authors": ["Upasana Tiwari", "Rupayan Chakraborty", "Sunil Kumar Kopparapu"], "title": "Emotion-Disentangled Embedding Alignment for Noise-Robust and Cross-Corpus Speech Emotion Recognition", "comment": "13 pages, 1 figure", "summary": "Effectiveness of speech emotion recognition in real-world scenarios is often\nhindered by noisy environments and variability across datasets. This paper\nintroduces a two-step approach to enhance the robustness and generalization of\nspeech emotion recognition models through improved representation learning.\nFirst, our model employs EDRL (Emotion-Disentangled Representation Learning) to\nextract class-specific discriminative features while preserving shared\nsimilarities across emotion categories. Next, MEA (Multiblock Embedding\nAlignment) refines these representations by projecting them into a joint\ndiscriminative latent subspace that maximizes covariance with the original\nspeech input. The learned EDRL-MEA embeddings are subsequently used to train an\nemotion classifier using clean samples from publicly available datasets, and\nare evaluated on unseen noisy and cross-corpus speech samples. Improved\nperformance under these challenging conditions demonstrates the effectiveness\nof the proposed method."}
{"id": "2510.09238", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09238", "abs": "https://arxiv.org/abs/2510.09238", "authors": ["Siddarth Marwaha", "Pawel Kryszkiewicz", "Eduard Jorswieck"], "title": "Energy-Efficient Resource Allocation for PA Distortion-Aware M-MIMO OFDM System", "comment": null, "summary": "Maintaining high energy efficiency (EE) in wireless networks is crucial,\nparticularly with the adoption of massive MIMO technology. This work introduces\na resource allocation framework that jointly optimizes transmit power assigned\nto each user and the number of active antennas, while explicitly accounting for\na nonlinear Power Amplifier (PA). We consider a downlink MU-MIMO-OFDM\ntransmission with zero forcing (ZF) precoding, Rayleigh fading channels, and\nsoft-limiter PAs, with both ideal and realistic PA architectures. In contrast\nto existing formulations, our optimization framework avoids imposing an\nexplicit transmit power constraint, since the nonlinear distortion inherently\nlimits the feasible operating region. To solve the resulting non-convex\nproblem, an alternating optimization approach is adopted that, by exploiting\nproperties of the EE function, guarantees convergence to a stationary point.\nExtensive simulations demonstrate consistent performance gains over\ndistortion-neglecting and power-only optimized baselines. In a scenario of a 5\nkm radius cell serving 60 randomly distributed users, the median EE gains over\nthe distortion-neglecting allocation reach 40% for ideal PAs and 20% for Class\nB PAs, confirming high impact of the proposed solution."}
{"id": "2510.09307", "categories": ["eess.AS", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09307", "abs": "https://arxiv.org/abs/2510.09307", "authors": ["Natalia Tomashenko", "Junichi Yamagishi", "Xin Wang", "Yun Liu", "Emmanuel Vincent"], "title": "Target speaker anonymization in multi-speaker recordings", "comment": "Submitted to ICASSP 2026", "summary": "Most of the existing speaker anonymization research has focused on\nsingle-speaker audio, leading to the development of techniques and evaluation\nmetrics optimized for such condition. This study addresses the significant\nchallenge of speaker anonymization within multi-speaker conversational audio,\nspecifically when only a single target speaker needs to be anonymized. This\nscenario is highly relevant in contexts like call centers, where customer\nprivacy necessitates anonymizing only the customer's voice in interactions with\noperators. Conventional anonymization methods are often not suitable for this\ntask. Moreover, current evaluation methodology does not allow us to accurately\nassess privacy protection and utility in this complex multi-speaker scenario.\nThis work aims to bridge these gaps by exploring effective strategies for\ntargeted speaker anonymization in conversational audio, highlighting potential\nproblems in their development and proposing corresponding improved evaluation\nmethodologies."}
{"id": "2510.09245", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09245", "abs": "https://arxiv.org/abs/2510.09245", "authors": ["Zhao Guo", "Ziqian Ning", "Guobin Ma", "Lei Xie"], "title": "SynthVC: Leveraging Synthetic Data for End-to-End Low Latency Streaming Voice Conversion", "comment": "Accepted by NCMMSC2025", "summary": "Voice Conversion (VC) aims to modify a speaker's timbre while preserving\nlinguistic content. While recent VC models achieve strong performance, most\nstruggle in real-time streaming scenarios due to high latency, dependence on\nASR modules, or complex speaker disentanglement, which often results in timbre\nleakage or degraded naturalness. We present SynthVC, a streaming end-to-end VC\nframework that directly learns speaker timbre transformation from synthetic\nparallel data generated by a pre-trained zero-shot VC model. This design\neliminates the need for explicit content-speaker separation or recognition\nmodules. Built upon a neural audio codec architecture, SynthVC supports\nlow-latency streaming inference with high output fidelity. Experimental results\nshow that SynthVC outperforms baseline streaming VC systems in both naturalness\nand speaker similarity, achieving an end-to-end latency of just 77.1 ms."}
{"id": "2510.09384", "categories": ["eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2510.09384", "abs": "https://arxiv.org/abs/2510.09384", "authors": ["Takeo Sasai", "Giacomo Borraccini", "Yue-Kai Huang", "Hideki Nishizawa", "Zehao Wang", "Tingjun Chen", "Yoshiaki Sone", "Minami Takahashi", "Tatsuya Matsumura", "Masanori Nakamura", "Etsushi Yamazaki", "Koichi Takasugi", "Ting Wang", "Yoshiaki Kisaka"], "title": "Optical Link Tomography: First Field Trial and 4D Extension", "comment": "12 pages, 7 figures, accepted version for Journal of Lightwave\n  Technology", "summary": "Optical link tomography (OLT) is a rapidly evolving field that allows the\nmulti-span, end-to-end visualization of optical power along fiber links in\nmultiple dimensions from network endpoints, solely by processing signals\nreceived at coherent receivers. This paper has two objectives: (1) to report\nthe first field trial of OLT, using a commercial transponder under standard\nDWDM transmission, and (2) to extend its capability to visualize across 4D\n(distance, time, frequency, and polarization), allowing for locating and\nmeasuring multiple QoT degradation causes, including time-varying power\nanomalies, spectral anomalies, and excessive polarization dependent loss. We\nalso address a critical aspect of OLT, i.e., its need for high fiber launch\npower, by improving power profile signal-to-noise ratio through averaging\nacross all available dimensions. Consequently, multiple loss anomalies in a\nfield-deployed link are observed even at launch power lower than the\nsystem-optimal level. The applications and use cases of OLT from network\ncommissioning to provisioning and operation for current and near-term network\nscenarios are also discussed."}
{"id": "2510.09504", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09504", "abs": "https://arxiv.org/abs/2510.09504", "authors": ["Liping Chen", "Chenyang Guo", "Kong Aik Lee", "Zhen-Hua Ling", "Wu Guo"], "title": "A Study of the Removability of Speaker-Adversarial Perturbations", "comment": null, "summary": "Recent advancements in adversarial attacks have demonstrated their\neffectiveness in misleading speaker recognition models, making wrong\npredictions about speaker identities. On the other hand, defense techniques\nagainst speaker-adversarial attacks focus on reducing the effects of\nspeaker-adversarial perturbations on speaker attribute extraction. These\ntechniques do not seek to fully remove the perturbations and restore the\noriginal speech. To this end, this paper studies the removability of\nspeaker-adversarial perturbations. Specifically, the investigation is conducted\nassuming various degrees of awareness of the perturbation generator across\nthree scenarios: ignorant, semi-informed, and well-informed. Besides, we\nconsider both the optimization-based and feedforward perturbation generation\nmethods. Experiments conducted on the LibriSpeech dataset demonstrated that: 1)\nin the ignorant scenario, speaker-adversarial perturbations cannot be\neliminated, although their impact on speaker attribute extraction is reduced,\n2) in the semi-informed scenario, the speaker-adversarial perturbations cannot\nbe fully removed, while those generated by the feedforward model can be\nconsiderably reduced, and 3) in the well-informed scenario, speaker-adversarial\nperturbations are nearly eliminated, allowing for the restoration of the\noriginal speech. Audio samples can be found in\nhttps://voiceprivacy.github.io/Perturbation-Generation-Removal/."}
{"id": "2510.09344", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09344", "abs": "https://arxiv.org/abs/2510.09344", "authors": ["Hui Wang", "Jiaming Zhou", "Jiabei He", "Haoqin Sun", "Yong Qin"], "title": "WildElder: A Chinese Elderly Speech Dataset from the Wild with Fine-Grained Manual Annotations", "comment": null, "summary": "Elderly speech poses unique challenges for automatic processing due to\nage-related changes such as slower articulation and vocal tremors. Existing\nChinese datasets are mostly recorded in controlled environments, limiting their\ndiversity and real-world applicability. To address this gap, we present\nWildElder, a Mandarin elderly speech corpus collected from online videos and\nenriched with fine-grained manual annotations, including transcription, speaker\nage, gender, and accent strength. Combining the realism of in-the-wild data\nwith expert curation, WildElder enables robust research on automatic speech\nrecognition and speaker profiling. Experimental results reveal both the\ndifficulties of elderly speech recognition and the potential of WildElder as a\nchallenging new benchmark. The dataset and code are available at\nhttps://github.com/NKU-HLT/WildElder."}
{"id": "2510.09539", "categories": ["eess.SP", "68T05, 94A12", "I.2.6; I.5.4; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.09539", "abs": "https://arxiv.org/abs/2510.09539", "authors": ["Patrick Ferreira", "Paula Costa"], "title": "IF-D: A High-Frequency, General-Purpose Inertial Foundation Dataset for Self-Supervised Learning", "comment": "5 pages, 5 figures. Submitted to IEEE ICASSP 2026. Copyright 2026\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses", "summary": "We present IF-D, a large-scale inertial dataset designed to enable\nself-supervised and foundational learning for IMU time series. IF-D comprises\ncontinuous, long-duration multichannel recordings (accelerometer, gyroscope,\nmagnetometer) sampled at 200Hz using a UM7 IMU mounted inside a 3D-printed\nspherical enclosure that promotes diverse, free rotations during vehicle\ntraversal. The collection spans approximately 135 minutes of recording,\nyielding around 1.6 million samples across nine sensor channels. We describe\nthe data acquisition setup, preprocessing, and calibration procedures\n(six-orientation accelerometer calibration, stationary gyroscope bias\nestimation, and ellipsoid fitting for magnetometer hard-/soft-iron correction),\nand provide quantitative calibration results. IF-D is designed to mitigate\nplatform specific motion bias and expose models to both physical dynamics and\ntypical measurement noise, thereby facilitating robust representation learning\nand downstream tasks such as event detection, motion mode recognition, and\ninertial navigation."}
{"id": "2510.09505", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09505", "abs": "https://arxiv.org/abs/2510.09505", "authors": ["Li Li", "Ming Cheng", "Hongyu Zhang", "Juan Liu", "Ming Li"], "title": "Spatially-Augmented Sequence-to-Sequence Neural Diarization for Meetings", "comment": "This paper has submitted to ICASSP 2026", "summary": "This paper proposes a Spatially-Augmented Sequence-to-Sequence Neural\nDiarization (SA-S2SND) framework, which integrates direction-of-arrival (DOA)\ncues estimated by SRP-DNN into the S2SND backbone. A two-stage training\nstrategy is adopted: the model is first trained with single-channel audio and\nDOA features, and then further optimized with multi-channel inputs under DOA\nguidance. In addition, a simulated DOA generation scheme is introduced to\nalleviate dependence on matched multi-channel corpora. On the AliMeeting\ndataset, SA-S2SND consistently outperform the S2SND baseline, achieving a 7.4%\nrelative DER reduction in the offline mode and over 19% improvement when\ncombined with channel attention. These results demonstrate that spatial cues\nare highly complementary to cross-channel modeling, yielding good performance\nin both online and offline settings."}
{"id": "2510.08585", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08585", "abs": "https://arxiv.org/abs/2510.08585", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carol Espy Wilson"], "title": "Articulation-Informed ASR: Integrating Articulatory Features into ASR via Auxiliary Speech Inversion and Cross-Attention Fusion", "comment": null, "summary": "Prior works have investigated the use of articulatory features as\ncomplementary representations for automatic speech recognition (ASR), but their\nuse was largely confined to shallow acoustic models. In this work, we revisit\narticulatory information in the era of deep learning and propose a framework\nthat leverages articulatory representations both as an auxiliary task and as a\npseudo-input to the recognition model. Specifically, we employ speech inversion\nas an auxiliary prediction task, and the predicted articulatory features are\ninjected into the model as a query stream in a cross-attention module with\nacoustic embeddings as keys and values. Experiments on LibriSpeech demonstrate\nthat our approach yields consistent improvements over strong transformer-based\nbaselines, particularly under low-resource conditions. These findings suggest\nthat articulatory features, once sidelined in ASR research, can provide\nmeaningful benefits when reintroduced with modern architectures."}
{"id": "2510.09573", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09573", "abs": "https://arxiv.org/abs/2510.09573", "authors": ["Siphiwe Shandu", "Thabiso Moropa", "Alain R. Ndjiongue"], "title": "Power Cable Radiation: A Novel Approach to Underground Mining Connectivity", "comment": "5 pages", "summary": "This letter investigates contactless power line communications (CPLC) for\nunderground mining by modeling power wires as long-wire antennas. A\nsystem-level framework is developed, comprising a cascade of RF and power line\nchannels. The model accounts for multipath propagation, frequency-dependent\nattenuation, and Rician fading. Simulations from 1-20 GHz reveal that the\nlength of the wire significantly affects radiation, directivity, and input\nimpedance. The findings show that CPLC transmits electromagnetic waves without\ndirect electrical contact, offering a robust, cost-effective solution that\nenhances mobility, reduces maintenance, and ensures compatibility with existing\nmining power infrastructure."}
{"id": "2510.08580", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08580", "abs": "https://arxiv.org/abs/2510.08580", "authors": ["Benjamin Shiue-Hal Chou", "Purvish Jajal", "Nick John Eliopoulos", "James C. Davis", "George K. Thiruvathukal", "Kristen Yeon-Ji Yun", "Yung-Hsiang Lu"], "title": "LadderSym: A Multimodal Interleaved Transformer for Music Practice Error Detection", "comment": "Under Submission", "summary": "Music learners can greatly benefit from tools that accurately detect errors\nin their practice. Existing approaches typically compare audio recordings to\nmusic scores using heuristics or learnable models. This paper introduces\n\\textit{LadderSym}, a novel Transformer-based method for music error detection.\n\\textit{LadderSym} is guided by two key observations about the state-of-the-art\napproaches: (1) late fusion limits inter-stream alignment and cross-modality\ncomparison capability; and (2) reliance on score audio introduces ambiguity in\nthe frequency spectrum, degrading performance in music with concurrent notes.\nTo address these limitations, \\textit{LadderSym} introduces (1) a two-stream\nencoder with inter-stream alignment modules to improve audio comparison\ncapabilities and error detection F1 scores, and (2) a multimodal strategy that\nleverages both audio and symbolic scores by incorporating symbolic\nrepresentations as decoder prompts, reducing ambiguity and improving F1 scores.\nWe evaluate our method on the \\textit{MAESTRO-E} and \\textit{CocoChorales-E}\ndatasets by measuring the F1 score for each note category. Compared to the\nprevious state of the art, \\textit{LadderSym} more than doubles F1 for missed\nnotes on \\textit{MAESTRO-E} (26.8\\% $\\rightarrow$ 56.3\\%) and improves extra\nnote detection by 14.4 points (72.0\\% $\\rightarrow$ 86.4\\%). Similar gains are\nobserved on \\textit{CocoChorales-E}. This work introduces general insights\nabout comparison models that could inform sequence evaluation tasks for\nreinforcement Learning, human skill assessment, and model evaluation."}
{"id": "2510.08586", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08586", "abs": "https://arxiv.org/abs/2510.08586", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Dynamic Stress Detection: A Study of Temporal Progression Modelling of Stress in Speech", "comment": "Accepted at IEEE CogMI 2025", "summary": "Detecting psychological stress from speech is critical in high-pressure\nsettings. While prior work has leveraged acoustic features for stress\ndetection, most treat stress as a static label. In this work, we model stress\nas a temporally evolving phenomenon influenced by historical emotional state.\nWe propose a dynamic labelling strategy that derives fine-grained stress\nannotations from emotional labels and introduce cross-attention-based\nsequential models, a Unidirectional LSTM and a Transformer Encoder, to capture\ntemporal stress progression. Our approach achieves notable accuracy gains on\nMuSE (+5%) and StressID (+18%) over existing baselines, and generalises well to\na custom real-world dataset. These results highlight the value of modelling\nstress as a dynamic construct in speech."}
{"id": "2510.08581", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08581", "abs": "https://arxiv.org/abs/2510.08581", "authors": ["Hansol Park", "Hoseong Ahn", "Junwon Moon", "Yejin Lee", "Kyuhong Shim"], "title": "Evaluating Hallucinations in Multimodal LLMs with Spoken Queries under Diverse Acoustic Conditions", "comment": null, "summary": "Hallucinations in vision-language models have been extensively studied using\nbenchmarks that probe reliability in image-text settings. In contrast, the\neffect of spoken queries on multimodal hallucinations remains largely\nunexplored, despite the growing role of voice-driven interfaces. In this work,\nwe investigate how spoken input influences hallucinations in multimodal large\nlanguage models. We present RePOPE-Spk, an audio-augmented extension of the\nRePOPE benchmark, where queries are provided as speech under diverse acoustic\nconditions. Using RePOPE-Spk, we systematically evaluate both proprietary and\nopen-source models. Experimental results show that hallucinations escalate when\nqueries are spoken rather than written: error rates increase by 3% under clean\nspeech and by up to 20% with environmental noise. Input order and query length\nfurther affect robustness, while strategies such as many-shot prompting and\nchain-of-thought reasoning offer partial but insufficient mitigation. These\nfindings highlight a critical and underexplored challenge, opening new\ndirections for building reliable voice interface systems."}
{"id": "2510.08599", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08599", "abs": "https://arxiv.org/abs/2510.08599", "authors": ["Yaya Sy", "Christophe Cerisara", "Irina Illina"], "title": "BaldWhisper: Faster Whisper with Head Shearing and Layer Merging", "comment": null, "summary": "Pruning large pre-trained transformers for low-resource languages is\nchallenging, as it often requires massive retraining data to recover\nperformance. For instance, Distill-Whisper prunes Whisper by 40% and retrains\non 21,000 hours of speech, far beyond what is available for most languages. Can\nWhisper be made lighter and faster for edge devices in data-scarce settings?\nFocusing on Bambara with only 32h of speech-to-text data, we propose a new\npruning recipe. Instead of vocabulary pruning, which is unsuitable due to\nfrequent code-switching by Bambara speakers, we compress the embeddings with\nlow-rank decomposition and feature distillation. Rather than removing layers,\nwe merge them to limit performance loss. The final model preserves 90% of the\noriginal performance while being 48% smaller and 2.15x faster on a MacBook Air\nM1."}
{"id": "2510.08587", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08587", "abs": "https://arxiv.org/abs/2510.08587", "authors": ["Tianheng Zhu", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation", "comment": "Main paper (6 pages). Accepted for publication by IEEE International\n  Conference on Systems, Man, and Cybernetics 2025", "summary": "This paper presents EGSTalker, a real-time audio-driven talking head\ngeneration framework based on 3D Gaussian Splatting (3DGS). Designed to enhance\nboth speed and visual fidelity, EGSTalker requires only 3-5 minutes of training\nvideo to synthesize high-quality facial animations. The framework comprises two\nkey stages: static Gaussian initialization and audio-driven deformation. In the\nfirst stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network\n(KAN) are used to extract spatial features and construct a compact 3D Gaussian\nrepresentation. In the second stage, we propose an Efficient Spatial-Audio\nAttention (ESAA) module to fuse audio and spatial cues, while KAN predicts the\ncorresponding Gaussian deformations. Extensive experiments demonstrate that\nEGSTalker achieves rendering quality and lip-sync accuracy comparable to\nstate-of-the-art methods, while significantly outperforming them in inference\nspeed. These results highlight EGSTalker's potential for real-time multimedia\napplications."}
{"id": "2510.08618", "categories": ["eess.AS", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.08618", "abs": "https://arxiv.org/abs/2510.08618", "authors": ["Rui Hu", "Delai Qiu", "Yining Wang", "Shengping Liu", "Jitao Sang"], "title": "Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization", "comment": null, "summary": "Automatic speech recognition (ASR) systems often struggle with\ndomain-specific terminology, especially in specialized settings such as\nacademic lectures. To address this, we define the SlideASR task, which\nleverages the rich visual information from presentation slides to improve\ntranscription accuracy. Existing pipeline methods for this task tend to be\ncomplex and underperform. Although omni-modal large language models (OLLMs)\nprovide a promising end-to-end framework, they frequently fail in practice by\ndegenerating into simple optical character recognition (OCR) systems. To\novercome this, we propose Visually-Anchored Policy Optimization (VAPO), a novel\npost-training method designed to control the model's reasoning process. Drawing\non the Chain-of-Thought reasoning paradigm, VAPO enforces a structured \"Look\nbefore Transcription\" procedure using a <think><answer> format. Specifically,\nthe model first performs OCR on the slide content within the think step, then\ngenerates the transcription by referencing this recognized visual information\nin the answer step. This reasoning process is optimized via reinforcement\nlearning with four distinct rewards targeting format compliance, OCR accuracy,\nASR quality, and visual anchoring consistency. To support further research, we\nconstruct SlideASR-Bench, a new entity-rich benchmark consisting of a synthetic\ndataset for training and testing, and a challenging real-world set for\nevaluation. Extensive experiments demonstrate that VAPO significantly improves\nrecognition of domain-specific terms, establishing an effective end-to-end\nparadigm for SlideASR."}
{"id": "2510.08816", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08816", "abs": "https://arxiv.org/abs/2510.08816", "authors": ["Juan José Burred", "Carmine-Emanuele Cella"], "title": "Audible Networks: Deconstructing and Manipulating Sounds with Deep Non-Negative Autoencoders", "comment": null, "summary": "We propose the use of Non-Negative Autoencoders (NAEs) for sound\ndeconstruction and user-guided manipulation of sounds for creative purposes.\nNAEs offer a versatile and scalable extension of traditional Non-Negative\nMatrix Factorization (NMF)-based approaches for interpretable audio\ndecomposition. By enforcing non-negativity constraints through projected\ngradient descent, we obtain decompositions where internal weights and\nactivations can be directly interpreted as spectral shapes and temporal\nenvelopes, and where components can themselves be listened to as individual\nsound events. In particular, multi-layer Deep NAE architectures enable\nhierarchical representations with an adjustable level of granularity, allowing\nsounds to be deconstructed at multiple levels of abstraction: from high-level\nnote envelopes down to fine-grained spectral details. This framework enables a\nwide new range of expressive, controllable, and randomized sound\ntransformations. We introduce novel manipulation operations including\ncross-component and cross-layer synthesis, hierarchical deconstructions, and\nseveral randomization strategies that control timbre and event density. Through\nvisualizations and resynthesis of practical examples, we demonstrate how NAEs\ncan serve as flexible and interpretable tools for object-based sound editing."}
{"id": "2510.09225", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09225", "abs": "https://arxiv.org/abs/2510.09225", "authors": ["Danel Adendorff", "Simon Malan", "Herman Kamper"], "title": "Unsupervised lexicon learning from speech is limited by representations rather than clustering", "comment": "Submitted to ICASSP 2026", "summary": "Zero-resource word segmentation and clustering systems aim to tokenise speech\ninto word-like units without access to text labels. Despite progress, the\ninduced lexicons are still far from perfect. In an idealised setting with gold\nword boundaries, we ask whether performance is limited by the representation of\nword segments, or by the clustering methods that group them into word-like\ntypes. We combine a range of self-supervised speech features\n(continuous/discrete, frame/word-level) with different clustering methods\n(K-means, hierarchical, graph-based) on English and Mandarin data. The best\nsystem uses graph clustering with dynamic time warping on continuous features.\nFaster alternatives use graph clustering with cosine distance on averaged\ncontinuous features or edit distance on discrete unit sequences. Through\ncontrolled experiments that isolate either the representations or the\nclustering method, we demonstrate that representation variability across\nsegments of the same word type -- rather than clustering -- is the primary\nfactor limiting performance."}
{"id": "2510.08878", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08878", "abs": "https://arxiv.org/abs/2510.08878", "authors": ["Yuxuan Jiang", "Zehua Chen", "Zeqian Ju", "Yusheng Dai", "Weibei Dou", "Jun Zhu"], "title": "ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling", "comment": "18 pages, 8 tables, 5 figures", "summary": "Text-to-audio (TTA) generation with fine-grained control signals, e.g.,\nprecise timing control or intelligible speech content, has been explored in\nrecent works. However, constrained by data scarcity, their generation\nperformance at scale is still compromised. In this study, we recast\ncontrollable TTA generation as a multi-task learning problem and introduce a\nprogressive diffusion modeling approach, ControlAudio. Our method adeptly fits\ndistributions conditioned on more fine-grained information, including text,\ntiming, and phoneme features, through a step-by-step strategy. First, we\npropose a data construction method spanning both annotation and simulation,\naugmenting condition information in the sequence of text, timing, and phoneme.\nSecond, at the model training stage, we pretrain a diffusion transformer (DiT)\non large-scale text-audio pairs, achieving scalable TTA generation, and then\nincrementally integrate the timing and phoneme features with unified semantic\nrepresentations, expanding controllability. Finally, at the inference stage, we\npropose progressively guided generation, which sequentially emphasizes more\nfine-grained information, aligning inherently with the coarse-to-fine sampling\nnature of DiT. Extensive experiments show that ControlAudio achieves\nstate-of-the-art performance in terms of temporal accuracy and speech clarity,\nsignificantly outperforming existing methods on both objective and subjective\nevaluations. Demo samples are available at:\nhttps://control-audio.github.io/Control-Audio."}
{"id": "2510.09236", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09236", "abs": "https://arxiv.org/abs/2510.09236", "authors": ["Michele Buccoli", "Yu Du", "Jacob Soendergaard", "Simone Shawn Cazzaniga"], "title": "Effects of automotive microphone frequency response characteristics and noise conditions on speech and ASR quality -- an experimental evaluation", "comment": null, "summary": "Upon choosing microphones for automotive hands-free communication or\nAutomatic Speech Recognition (ASR) applications, OEMs typically specify\nwideband, super wideband or even fullband requirements following established\nstandard recommendations (e.g., ITU-P.1110, ITU-P.1120). In practice, it is\noften challenging to achieve the preferred bandwidth for an automotive\nmicrophone when considering limitations and constraints on microphone placement\ninside the cabin, and the automotive grade environmental robustness\nrequirements. On the other hand, there seems to be no consensus or sufficient\ndata on the effect of each microphone characteristic on the actual performance.\nAs an attempt to answer this question, we used noise signals recorded in real\nvehicles and under various driving conditions to experimentally study the\nrelationship between the microphones' characteristics and the final audio\nquality of speech communication and performance of ASR engines. We focus on how\nvariations in microphone bandwidth and amplitude frequency response shapes\naffect the perceptual speech quality. The speech quality results are compared\nby using ETSI TS 103 281 metrics (S-MOS, N-MOS, G-MOS) and ancillary metrics\nsuch as SNR. The ASR results are evaluated with standard metrics such as Word\nError Rate (WER). Findings from this study provide knowledge in the\nunderstanding of what microphone frequency response characteristics are more\nrelevant for audio quality and choice of proper microphone specifications,\nparticularly for automotive applications."}
{"id": "2510.08914", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08914", "abs": "https://arxiv.org/abs/2510.08914", "authors": ["Shulin He", "Zhong-Qiu Wang"], "title": "VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR Virtual Microphone Arrays", "comment": null, "summary": "Blind speech separation (BSS) aims to recover multiple speech sources from\nmulti-channel, multi-speaker mixtures under unknown array geometry and room\nimpulse responses. In unsupervised setup where clean target speech is not\navailable for model training, UNSSOR proposes a mixture consistency (MC) loss\nfor training deep neural networks (DNN) on over-determined training mixtures to\nrealize unsupervised speech separation. However, when the number of microphones\nof the training mixtures decreases, the MC constraint weakens and the\nseparation performance falls dramatically. To address this, we propose\nVM-UNSSOR, augmenting the observed training mixture signals recorded by a\nlimited number of microphones with several higher-SNR virtual-microphone (VM)\nsignals, which are obtained by applying linear spatial demixers (such as IVA\nand spatial clustering) to the observed training mixtures. As linear\nprojections of the observed mixtures, the virtual-microphone signals can\ntypically increase the SNR of each source and can be leveraged to compute extra\nMC losses to improve UNSSOR and address the frequency permutation problem in\nUNSSOR. On the SMS-WSJ dataset, in the over-determined six-microphone,\ntwo-speaker separation setup, VM-UNSSOR reaches 17.1 dB SI-SDR, while UNSSOR\nonly obtains 14.7 dB; and in the determined two-microphone, two-speaker case,\nUNSSOR collapses to -2.7 dB SI-SDR, while VM-UNSSOR achieves 10.7 dB."}
{"id": "2510.09016", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09016", "abs": "https://arxiv.org/abs/2510.09016", "authors": ["Zongcai Du", "Guilin Deng", "Xiaofeng Guo", "Xin Gao", "Linke Li", "Kaichang Cheng", "Fubo Han", "Siyu Yang", "Peng Liu", "Pan Zhong", "Qiang Fu"], "title": "DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment", "comment": "under review", "summary": "Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates\nstrong expressiveness but remains limited by data scarcity and model\nscalability. We introduce a two-stage pipeline: a compact seed set of\nhuman-sung recordings is constructed by pairing fixed melodies with diverse\nLLM-generated lyrics, and melody-specific models are trained to synthesize over\n500 hours of high-quality Chinese singing data. Building on this corpus, we\npropose DiTSinger, a Diffusion Transformer with RoPE and qk-norm,\nsystematically scaled in depth, width, and resolution for enhanced fidelity.\nFurthermore, we design an implicit alignment mechanism that obviates\nphoneme-level duration labels by constraining phoneme-to-acoustic attention\nwithin character-level spans, thereby improving robustness under noisy or\nuncertain alignments. Extensive experiments validate that our approach enables\nscalable, alignment-free, and high-fidelity SVS."}
{"id": "2510.09025", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09025", "abs": "https://arxiv.org/abs/2510.09025", "authors": ["Louis Bahrman", "Mathieu Fontaine", "Gaël Richard"], "title": "Déréverbération non-supervisée de la parole par modèle hybride", "comment": "in French language", "summary": "This paper introduces a new training strategy to improve speech\ndereverberation systems in an unsupervised manner using only reverberant\nspeech. Most existing algorithms rely on paired dry/reverberant data, which is\ndifficult to obtain. Our approach uses limited acoustic information, like the\nreverberation time (RT60), to train a dereverberation system. Experimental\nresults demonstrate that our method achieves more consistent performance across\nvarious objective metrics than the state-of-the-art."}
{"id": "2510.09061", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09061", "abs": "https://arxiv.org/abs/2510.09061", "authors": ["Huu Tuong Tu", "Huan Vu", "cuong tien nguyen", "Dien Hy Ngo", "Nguyen Thi Thu Trang"], "title": "O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion", "comment": "EMNLP 2025", "summary": "Traditional voice conversion (VC) methods typically attempt to separate\nspeaker identity and linguistic information into distinct representations,\nwhich are then combined to reconstruct the audio. However, effectively\ndisentangling these factors remains challenging, often leading to information\nloss during training. In this paper, we propose a new approach that leverages\nsynthetic speech data generated by a high-quality, pretrained multispeaker\ntext-to-speech (TTS) model. Specifically, synthetic data pairs that share the\nsame linguistic content but differ in speaker identity are used as input-output\npairs to train the voice conversion model. This enables the model to learn a\ndirect mapping between source and target voices, effectively capturing\nspeaker-specific characteristics while preserving linguistic content.\nAdditionally, we introduce a flexible training strategy for any-to-any voice\nconversion that generalizes well to unseen speakers and new languages,\nenhancing adaptability and performance in zero-shot scenarios. Our experiments\nshow that our proposed method achieves a 16.35% relative reduction in word\nerror rate and a 5.91% improvement in speaker cosine similarity, outperforming\nseveral state-of-the-art methods. Voice conversion samples can be accessed at:\nhttps://oovc-emnlp-2025.github.io/"}
{"id": "2510.09065", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.09065", "abs": "https://arxiv.org/abs/2510.09065", "authors": ["Akira Takahashi", "Shusuke Takahashi", "Yuki Mitsufuji"], "title": "MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation", "comment": "4 pages, 4 figures, 2 tables", "summary": "We introduce MMAudioSep, a generative model for video/text-queried sound\nseparation that is founded on a pretrained video-to-audio model. By leveraging\nknowledge about the relationship between video/text and audio learned through a\npretrained audio generative model, we can train the model more efficiently,\ni.e., the model does not need to be trained from scratch. We evaluate the\nperformance of MMAudioSep by comparing it to existing separation models,\nincluding models based on both deterministic and generative approaches, and\nfind it is superior to the baseline models. Furthermore, we demonstrate that\neven after acquiring functionality for sound separation via fine-tuning, the\nmodel retains the ability for original video-to-audio generation. This\nhighlights the potential of foundational sound generation models to be adopted\nfor sound-related downstream tasks. Our code is available at\nhttps://github.com/sony/mmaudiosep."}

<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [eess.AS](#eess.AS) [Total: 10]
- [cs.SD](#cs.SD) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [UAV-Assisted 3-D Localization for IoT Networks Using a Simple and Efficient TDOA-AOA Estimator](https://arxiv.org/abs/2510.08765)
*Mojtaba Amiri,Rouhollah Amiri*

Main category: eess.SP

TL;DR: 提出使用最小测量量（1个TDOA和1个AOA）进行3D源定位的代数解，采用闭式加权最小二乘估计器，通过单个地面站和合作无人机实现定位。


<details>
  <summary>Details</summary>
Motivation: 解决3D源定位问题，旨在使用最少的测量量（仅需一个TDOA和一个AOA测量对）来降低系统复杂度和成本。

Method: 采用闭式加权最小二乘估计器，利用单个地面站和合作无人机中继信号进行定位。

Result: 在中等高斯噪声条件下，该方法实现了接近最优的性能，与克拉美-罗下界(CRLB)对齐。

Conclusion: 所提出的方法在最小测量量条件下有效实现了3D源定位，性能接近理论最优值。

Abstract: This letter proposes an algebraic solution for the problem of 3-D source
localization utilizing the minimum number of measurements, i.e., one Time
Difference of Arrival (TDOA) and one Angle of Arrival (AOA) pair. The proposed
method employs a closed-form weighted least squares estimator and enables the
positioning using a single ground station and a cooperative UAV relaying the
signal. Analytical derivations and simulation results demonstrate effectiveness
of the proposed approach, achieving near-optimal performance aligned with the
Cram\'er-Rao Lower Bound (CRLB) under moderate Gaussian noise conditions.

</details>


### [2] [Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission](https://arxiv.org/abs/2510.09047)
*Jiaming Liu,Rui Wang,JinJiang Li,Hong Lin,Jing Zhang,Kun Qiu*

Main category: eess.SP

TL;DR: 提出基于迁移学习的Transformer框架，用于C+L波段系统的精确建模和拉曼泵浦设计


<details>
  <summary>Details</summary>
Motivation: 需要在C+L波段系统中实现精确的建模和拉曼泵浦设计，传统方法可能难以同时满足这两个需求

Method: 采用迁移学习增强的Transformer框架，能够同时处理系统建模和泵浦设计任务

Result: 建模的RMSE在0.22 dB以内，峰值到峰值GSNR变化/偏差分别为0.86/0.1 dB

Conclusion: 该框架在C+L波段系统中实现了高精度的建模和泵浦设计性能

Abstract: We propose a transfer learning-enabled Transformer framework to
simultaneously realize accurate modeling and Raman pump design in C+L-band
systems. The RMSE for modeling and peak-to-peak GSNR variation/deviation is
within 0.22 dB and 0.86/0.1 dB, respectively.

</details>


### [3] [Pinching-Antenna Assisted Sensing: A Bayesian Cramér-Rao Bound Perspective](https://arxiv.org/abs/2510.09137)
*Hao Jiang,Chongjun Ouyang,Zhaolin Wang,Yuanwei Liu,Arumugam Nallanathan,Zhiguo Ding*

Main category: eess.SP

TL;DR: 研究了夹持天线系统(PASS)的基本感知极限，从贝叶斯克拉美-罗界(BCRB)角度分析。提出了两种目标调度协议：夹持切换(PS)和夹持复用(PM)，并通过KKT条件优化功率分配和PA位置。结果表明PASS相比传统固定阵列能显著提升感知性能，PS比PM更稳健但计算复杂度更高。


<details>
  <summary>Details</summary>
Motivation: 传统CRB依赖于具体参数值且受限于估计器的无偏性，而BCRB提供了更实用和全面的感知性能下界评估方法。研究PASS系统的感知极限对于优化多目标场景下的性能具有重要意义。

Method: 分析TDMA方案下多目标向单波导PASS传输上行导频的系统。提出PS和PM两种目标调度协议，基于KKT条件将功率最小化和最小-最大BCRB问题转化为PA位置搜索问题，并使用逐元素算法求解。

Result: 单目标场景发现感知质心与分布质心不匹配，需要动态PA重定位。多目标场景中，PS提供比PM更稳健的性能但计算复杂度更高。PASS相比传统固定阵列能显著提升感知性能。

Conclusion: BCRB为PASS系统提供了实用的感知性能评估框架。动态PA重定位对优化性能至关重要，PS协议在稳健性和性能之间提供了良好权衡，PASS的大规模可重构性使其在感知性能上优于传统固定阵列。

Abstract: The fundamental sensing limit of pinching-antenna systems (PASS) is studied
from a Bayesian Cram\'er-Rao bound (BCRB) perspective. Compared to conventional
CRB, BCRB is independent of the exact values of sensing parameters and is not
restricted by the unbiasedness of the estimator, thus offering a practical and
comprehensive lower bound for evaluating sensing performance. A system where
multiple targets transmit uplink pilots to a single-waveguide PASS under a
time-division multiple access (TDMA) scheme is analyzed. For the single-target
scenario, our analysis reveals a unique mismatch between the sensing centroid
(i.e., the optimal PA position) and the distribution centroid (i.e., the center
of the target's prior distribution), underscoring the necessity of dynamic PA
repositioning. For the multi-target scenario, two target scheduling protocols
are proposed: 1) pinch switching (PS), which performs separate pinching
beamforming for each time slot, and 2) pinch multiplexing (PM), which applies a
single beamforming configuration across all slots. Based on these protocols,
both the total power minimization problem under a BCRB threshold and the
min-max BCRB problem under a total power constraint are formulated. By
leveraging Karush-Kuhn-Tucker (KKT) conditions, these problems are equivalently
converted into a search over PA positions and solved using an element-wise
algorithm. Numerical results show that i)~PASS, endowed with large-scale
reconfigurability, can significantly enhance the sensing performance compared
with conventional fixed-position arrays, and ii)~PS provides more robust
performances than PM at the cost of higher computational complexity.

</details>


### [4] [Topological Signal Processing Over Cell MultiComplexes Via Cross-Laplacian Operators](https://arxiv.org/abs/2510.09139)
*Stefania Sardellitti,Breno C. Bispo,Fernando A. N. Santos,Juliano B. Lima*

Main category: eess.SP

TL;DR: 本文提出了细胞多复形（CMCs）作为表示多层网络中高阶交互的新拓扑空间，并引入了交叉拉普拉斯算子作为代数描述符，开发了基于这些算子的拓扑信号处理工具，用于处理跨层信号并学习层间拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 多层网络中不同系统间的复杂交互需要能够定位不同尺度同调性的拓扑代数描述符，现有的单复形表示方法在处理跨层交互时存在局限性。

Method: 引入细胞多复形（CMCs）表示高阶交互，提出交叉拉普拉斯算子作为代数描述符，利用其特征向量进行信号表示，并开发拓扑信号处理工具和拓扑学习策略。

Result: 基于交叉拉普拉斯的局部信号表示在稀疏性/准确性权衡上优于单复形表示，后者会产生过完备的局部信号表示。

Conclusion: CMCs和交叉拉普拉斯算子为多层网络的拓扑分析和信号处理提供了有效的数学框架，特别适用于脑网络等复杂系统的跨模块连接模式编码。

Abstract: One of the key challenges in many research fields is uncovering how different
interconnected systems interact within complex networks, typically represented
as multi-layer networks. Capturing the intra- and cross-layer interactions
among different domains for analysis and processing calls for topological
algebraic descriptors capable of localizing the homologies of different
domains, at different scales, according to the learning task. Our first
contribution in this paper is to introduce the Cell MultiComplexes (CMCs),
which are novel topological spaces that enable the representation of
higher-order interactions among interconnected cell complexes. We introduce
cross-Laplacian operators as powerful algebraic descriptors of CMC spaces able
to capture different topological invariants, whether global or local, at
different resolutions. Using the eigenvectors of these operators as bases for
the signal representation, we develop topological signal processing tools for
signals defined over CMCs. Then, we focus on the signal spectral representation
and on the filtering of noisy flows observed over the cross-edges between
different layers of CMCs. We show that a local signal representation based on
cross-Laplacians yields a better sparsity/accuracy trade-off compared to
monocomplex representations, which provide overcomplete representation of local
signals. Finally, we illustrate a topology learning strategy designed to infer
second-order cross-cells between layers, with applications to brain networks
for encoding inter-module connectivity patterns.

</details>


### [5] [Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for High-Power Applications](https://arxiv.org/abs/2510.09154)
*Tanjim Rahman,Trupti Ranjan Lenka*

Main category: eess.SP

TL;DR: 提出了一种栅极场板工程的Al0.295GaN/GaN HEMT，实现了1kV击穿电压，适用于恶劣环境下的高压高功率射频应用。


<details>
  <summary>Details</summary>
Motivation: HEMT器件在恶劣环境下具有可靠工作能力，但需要提高击穿电压以确保在高压、高温、辐射和腐蚀性环境中的稳定运行。

Method: 采用Al0.295GaN/GaN异质结构产生高密度二维电子气，通过栅极场板工程优化电场分布，并进行直流和击穿特性仿真。

Result: 器件阈值电压-5.5V，饱和漏电流3000mA，击穿电压1kV，截止频率28GHz，最大振荡频率38GHz，在40GHz以下保持稳定。

Conclusion: 所提出的栅极场板HEMT器件在恶劣环境下具有优异的高压高功率射频性能，适合恶劣环境应用。

Abstract: High Electron Mobility Transistors (HEMTs) are most suitable for harsh
environments as they operate reliably under extreme conditions such as high
voltages, high temperatures, radiation exposure and corrosive atmospheres. In
this article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for
achieving high breakdown voltage to reliably operate in harsh environments. The
Al0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)
density of the order of 1013 cm-2 obtained from the self-consistent solution of
Schr\"odinger and Poisson equations. The device has undergone DC and breakdown
simulations which result in threshold voltage of -5.5 V, drain saturation
current of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows
excellent RF characteristics which include cut-off frequency (ft) of 28 GHz and
maximum frequency of oscillation (fmax) of 38 GHz. The proposed gate
field-plated HEMT is stable up to 40 GHz and suitable for high-voltage and
high-power RF operation during harsh environment applications.

</details>


### [6] [Learning Product Graphs from Two-dimensional Stationary Signals](https://arxiv.org/abs/2510.09199)
*Andrei Buciulea,Bishwadeep Das,Elvin Isufi,Antonio G. Marques*

Main category: eess.SP

TL;DR: 提出了一种从二维信号学习图结构的图信号处理框架，通过联合滤波和乘积图表示来捕获结构化依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统图学习方法只关注节点上的标量信号，忽略了时间、设备配置或群体等额外维度的依赖关系。

Method: 使用矩阵图信号建模二维信号，基于图平稳性概念和乘积图表示，设计可高效求解的优化问题来恢复最优的Kronecker/Cartesian/强乘积图。

Result: 在合成数据上的实验表明，该方法相比现有方法具有更高的估计精度和更低的计算成本。

Conclusion: 该框架能够有效从二维信号中学习图结构，捕获复杂的不规则域中的依赖关系。

Abstract: Graph learning aims to infer a network structure directly from observed data,
enabling the analysis of complex dependencies in irregular domains. Traditional
methods focus on scalar signals at each node, ignoring dependencies along
additional dimensions such as time, configurations of the observation device,
or populations. In this work, we propose a graph signal processing framework
for learning graphs from two-dimensional signals, modeled as matrix graph
signals generated by joint filtering along both dimensions. This formulation
leverages the concept of graph stationarity across the two dimensions and
leverages product graph representations to capture structured dependencies.
Based on this model, we design an optimization problem that can be solved
efficiently and provably recovers the optimal underlying
Kronecker/Cartesian/strong product graphs. Experiments on synthetic data
demonstrate that our approach achieves higher estimation accuracy and reduced
computational cost compared to existing methods.

</details>


### [7] [Energy-Efficient Power Control in Single-User M-MIMO-OFDM System with PA Nonlinearity](https://arxiv.org/abs/2510.09232)
*Siddarth Marwaha,Pawel Kryszkiewicz,Eduard A. Jorswieck*

Main category: eess.SP

TL;DR: 本文研究了单用户M-MIMO下行链路系统中考虑非线性功率放大器效应的能效功率分配问题，提出了基于根查找的低复杂度算法来优化能效，相比固定功率回退基准获得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有M-MIMO系统能效资源分配方案大多忽视了优化功率放大器传输功率及其非线性失真效应，且多数研究假设窄带传输，忽略了OFDM系统中非线性PA产生的子载波互调失真。

Method: 采用软限幅PA模型建模宽带传输，推导了瑞利衰落和最大比传输预编码下的信噪失真比闭式表达式，定义了考虑两种PA架构和失真OFDM信号的能效函数，提出了基于根查找的低复杂度传输功率调整算法。

Result: 仿真结果显示，相比固定PA回退基准，所提算法在低和高路径损耗下均实现了超过100%的能效增益，揭示了最优工作点对天线数量、PA模型和传播条件的依赖性。

Conclusion: 通过优化PA传输功率并考虑非线性失真效应，可以显著提升M-MIMO OFDM系统的能效性能，且最优工作点受系统参数和传播环境的影响。

Abstract: Although multiple works have proposed energy-efficient resource allocation
schemes for Massive Multiple-Input Multiple-Output (M-MIMO) system, most
approaches overlook the potential of optimizing Power Amplifier (PA)
transmission power while accounting for non-linear distortion effects.
Furthermore, most M-MIMO studies assume narrow-band transmission, neglecting
subcarrier intermodulations at the non-linear PA for an Orthogonal Frequency
Division Multiplexing (OFDM) system. Therefore, this work investigates the
energy-efficient power allocation for a single-user equipment (UE) M-MIMO
downlink (DL) system employing OFDM with nonlinear PAs. Unlike prior works, we
model wide-band transmission using a soft-limiter PA model and derive a
closed-form expression for the signal-to-distortion-and-noise ratio (SNDR)
under Rayleigh fading and Maximal Ratio Transmission (MRT) precoding. Next, the
Energy Efficiency (EE) function is defined considering two PA architectures and
a distorted OFDM signal. We then propose a low complexity root-finding
algorithm to maximize EE by transmit power adjustment. Simulation results
demonstrate significant EE gains over a fixed PA back-off baseline, with over
$100\%$ improvement under both low and high path loss. Our findings reveal how
the optimal operating point depends on the antenna count, the PA model, and the
propagation conditions.

</details>


### [8] [Energy-Efficient Resource Allocation for PA Distortion-Aware M-MIMO OFDM System](https://arxiv.org/abs/2510.09238)
*Siddarth Marwaha,Pawel Kryszkiewicz,Eduard Jorswieck*

Main category: eess.SP

TL;DR: 提出一个联合优化发射功率和激活天线数的资源分配框架，显式考虑非线性功率放大器，在MU-MIMO-OFDM系统中实现高能效


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO技术应用中维持高能效至关重要，现有方案往往忽略非线性功率放大器的影响

Method: 采用交替优化方法解决非凸问题，利用能效函数特性保证收敛到驻点，避免显式发射功率约束

Result: 在5公里半径小区服务60个用户的场景中，相比忽略失真的基准方案，理想PA的中位能效增益达40%，B类PA达20%

Conclusion: 所提方案能显著提升能效，证实了显式考虑非线性功率放大器的重要性

Abstract: Maintaining high energy efficiency (EE) in wireless networks is crucial,
particularly with the adoption of massive MIMO technology. This work introduces
a resource allocation framework that jointly optimizes transmit power assigned
to each user and the number of active antennas, while explicitly accounting for
a nonlinear Power Amplifier (PA). We consider a downlink MU-MIMO-OFDM
transmission with zero forcing (ZF) precoding, Rayleigh fading channels, and
soft-limiter PAs, with both ideal and realistic PA architectures. In contrast
to existing formulations, our optimization framework avoids imposing an
explicit transmit power constraint, since the nonlinear distortion inherently
limits the feasible operating region. To solve the resulting non-convex
problem, an alternating optimization approach is adopted that, by exploiting
properties of the EE function, guarantees convergence to a stationary point.
Extensive simulations demonstrate consistent performance gains over
distortion-neglecting and power-only optimized baselines. In a scenario of a 5
km radius cell serving 60 randomly distributed users, the median EE gains over
the distortion-neglecting allocation reach 40% for ideal PAs and 20% for Class
B PAs, confirming high impact of the proposed solution.

</details>


### [9] [Optical Link Tomography: First Field Trial and 4D Extension](https://arxiv.org/abs/2510.09384)
*Takeo Sasai,Giacomo Borraccini,Yue-Kai Huang,Hideki Nishizawa,Zehao Wang,Tingjun Chen,Yoshiaki Sone,Minami Takahashi,Tatsuya Matsumura,Masanori Nakamura,Etsushi Yamazaki,Koichi Takasugi,Ting Wang,Yoshiaki Kisaka*

Main category: eess.SP

TL;DR: 本文报告了首个光链路断层扫描（OLT）现场试验，并扩展了其能力以实现4D可视化（距离、时间、频率和偏振），能够定位和测量多种QoT退化原因。通过跨维度平均提高了功率剖面信噪比，即使在低于系统最优水平的发射功率下也能观察到现场链路中的多个损耗异常。


<details>
  <summary>Details</summary>
Motivation: OLT是一个快速发展的领域，允许从网络端点仅通过处理相干接收器接收的信号来实现光纤链路上光功率的多维端到端可视化。本文旨在报告首个OLT现场试验，并扩展其能力以解决高光纤发射功率需求的关键问题。

Method: 使用商用转发器在标准DWDM传输下进行现场试验，通过跨所有可用维度（距离、时间、频率和偏振）进行平均来提高功率剖面信噪比，从而降低对高发射功率的需求。

Result: 成功在低于系统最优水平的发射功率下观察到现场部署链路中的多个损耗异常，实现了4D可视化能力，能够定位和测量时间变化功率异常、频谱异常和过度偏振相关损耗等多种QoT退化原因。

Conclusion: OLT技术在现场试验中证明有效，通过改进的信号处理方法降低了功率需求，扩展了应用范围，为从网络调试到配置和运营的当前和近期网络场景提供了重要工具。

Abstract: Optical link tomography (OLT) is a rapidly evolving field that allows the
multi-span, end-to-end visualization of optical power along fiber links in
multiple dimensions from network endpoints, solely by processing signals
received at coherent receivers. This paper has two objectives: (1) to report
the first field trial of OLT, using a commercial transponder under standard
DWDM transmission, and (2) to extend its capability to visualize across 4D
(distance, time, frequency, and polarization), allowing for locating and
measuring multiple QoT degradation causes, including time-varying power
anomalies, spectral anomalies, and excessive polarization dependent loss. We
also address a critical aspect of OLT, i.e., its need for high fiber launch
power, by improving power profile signal-to-noise ratio through averaging
across all available dimensions. Consequently, multiple loss anomalies in a
field-deployed link are observed even at launch power lower than the
system-optimal level. The applications and use cases of OLT from network
commissioning to provisioning and operation for current and near-term network
scenarios are also discussed.

</details>


### [10] [IF-D: A High-Frequency, General-Purpose Inertial Foundation Dataset for Self-Supervised Learning](https://arxiv.org/abs/2510.09539)
*Patrick Ferreira,Paula Costa*

Main category: eess.SP

TL;DR: IF-D是一个大规模惯性数据集，包含200Hz采样的加速度计、陀螺仪和磁力计多通道连续记录，用于IMU时间序列的自监督和基础学习。


<details>
  <summary>Details</summary>
Motivation: 设计该数据集是为了减轻平台特定的运动偏差，让模型接触物理动力学和典型测量噪声，从而促进鲁棒表示学习和下游任务。

Method: 使用UM7 IMU安装在3D打印球形外壳内进行数据采集，包含约135分钟记录，约160万样本。采用六向加速度计校准、静止陀螺仪偏置估计和磁力计椭球拟合等预处理程序。

Result: 提供了定量校准结果，数据集包含九个传感器通道的连续长时间记录，促进自由旋转期间的多样化运动。

Conclusion: IF-D数据集能够促进鲁棒表示学习，支持事件检测、运动模式识别和惯性导航等下游任务。

Abstract: We present IF-D, a large-scale inertial dataset designed to enable
self-supervised and foundational learning for IMU time series. IF-D comprises
continuous, long-duration multichannel recordings (accelerometer, gyroscope,
magnetometer) sampled at 200Hz using a UM7 IMU mounted inside a 3D-printed
spherical enclosure that promotes diverse, free rotations during vehicle
traversal. The collection spans approximately 135 minutes of recording,
yielding around 1.6 million samples across nine sensor channels. We describe
the data acquisition setup, preprocessing, and calibration procedures
(six-orientation accelerometer calibration, stationary gyroscope bias
estimation, and ellipsoid fitting for magnetometer hard-/soft-iron correction),
and provide quantitative calibration results. IF-D is designed to mitigate
platform specific motion bias and expose models to both physical dynamics and
typical measurement noise, thereby facilitating robust representation learning
and downstream tasks such as event detection, motion mode recognition, and
inertial navigation.

</details>


### [11] [Power Cable Radiation: A Novel Approach to Underground Mining Connectivity](https://arxiv.org/abs/2510.09573)
*Siphiwe Shandu,Thabiso Moropa,Alain R. Ndjiongue*

Main category: eess.SP

TL;DR: 该论文研究了地下采矿中的非接触式电力线通信，通过将电力线建模为长线天线，开发了一个包含RF和电力线信道级联的系统级框架。


<details>
  <summary>Details</summary>
Motivation: 为地下采矿环境提供一种无需直接电气接触的通信解决方案，增强移动性、减少维护成本，并确保与现有采矿电力基础设施的兼容性。

Method: 将电力线建模为长线天线，开发包含RF和电力线信道级联的系统级框架，考虑多径传播、频率相关衰减和Rician衰落，在1-20 GHz频段进行仿真。

Result: 仿真显示电线长度显著影响辐射、方向性和输入阻抗，CPLC能够在不直接电气接触的情况下传输电磁波。

Conclusion: CPLC提供了一种稳健、经济高效的解决方案，可增强移动性、减少维护，并与现有采矿电力基础设施兼容。

Abstract: This letter investigates contactless power line communications (CPLC) for
underground mining by modeling power wires as long-wire antennas. A
system-level framework is developed, comprising a cascade of RF and power line
channels. The model accounts for multipath propagation, frequency-dependent
attenuation, and Rician fading. Simulations from 1-20 GHz reveal that the
length of the wire significantly affects radiation, directivity, and input
impedance. The findings show that CPLC transmits electromagnetic waves without
direct electrical contact, offering a robust, cost-effective solution that
enhances mobility, reduces maintenance, and ensures compatibility with existing
mining power infrastructure.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [12] [Articulation-Informed ASR: Integrating Articulatory Features into ASR via Auxiliary Speech Inversion and Cross-Attention Fusion](https://arxiv.org/abs/2510.08585)
*Ahmed Adel Attia,Jing Liu,Carol Espy Wilson*

Main category: eess.AS

TL;DR: 提出了一种在深度学习中重新利用发音特征的方法，通过辅助任务和伪输入的方式将发音信息整合到语音识别模型中，在低资源条件下显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 虽然之前的研究探索了使用发音特征作为语音识别的补充表示，但这些应用主要局限于浅层声学模型。本研究旨在在深度学习时代重新审视发音信息的作用。

Method: 使用语音反演作为辅助预测任务，将预测的发音特征通过交叉注意力模块注入到识别模型中，其中发音特征作为查询流，声学嵌入作为键和值。

Result: 在LibriSpeech数据集上的实验表明，该方法在基于transformer的强基线基础上实现了持续改进，特别是在低资源条件下表现突出。

Conclusion: 发音特征曾经在ASR研究中被边缘化，但通过现代架构重新引入后可以提供有意义的性能提升。

Abstract: Prior works have investigated the use of articulatory features as
complementary representations for automatic speech recognition (ASR), but their
use was largely confined to shallow acoustic models. In this work, we revisit
articulatory information in the era of deep learning and propose a framework
that leverages articulatory representations both as an auxiliary task and as a
pseudo-input to the recognition model. Specifically, we employ speech inversion
as an auxiliary prediction task, and the predicted articulatory features are
injected into the model as a query stream in a cross-attention module with
acoustic embeddings as keys and values. Experiments on LibriSpeech demonstrate
that our approach yields consistent improvements over strong transformer-based
baselines, particularly under low-resource conditions. These findings suggest
that articulatory features, once sidelined in ASR research, can provide
meaningful benefits when reintroduced with modern architectures.

</details>


### [13] [Dynamic Stress Detection: A Study of Temporal Progression Modelling of Stress in Speech](https://arxiv.org/abs/2510.08586)
*Vishakha Lall,Yisi Liu*

Main category: eess.AS

TL;DR: 提出了一种将心理压力建模为时间演化现象的方法，通过动态标注策略从情感标签中获取细粒度压力标注，并使用基于交叉注意力的序列模型来捕捉时间压力进展。


<details>
  <summary>Details</summary>
Motivation: 传统方法将压力视为静态标签，但实际上压力是受历史情绪状态影响的动态演化现象，需要更精细的时间建模。

Method: 使用动态标注策略从情感标签推导细粒度压力标注，引入基于交叉注意力的序列模型（单向LSTM和Transformer编码器）来捕捉时间压力进展。

Result: 在MuSE数据集上准确率提升5%，在StressID数据集上提升18%，在自定义真实世界数据集上泛化良好。

Conclusion: 将压力建模为动态构造在语音分析中具有重要价值，能够更准确地捕捉压力随时间的变化过程。

Abstract: Detecting psychological stress from speech is critical in high-pressure
settings. While prior work has leveraged acoustic features for stress
detection, most treat stress as a static label. In this work, we model stress
as a temporally evolving phenomenon influenced by historical emotional state.
We propose a dynamic labelling strategy that derives fine-grained stress
annotations from emotional labels and introduce cross-attention-based
sequential models, a Unidirectional LSTM and a Transformer Encoder, to capture
temporal stress progression. Our approach achieves notable accuracy gains on
MuSE (+5%) and StressID (+18%) over existing baselines, and generalises well to
a custom real-world dataset. These results highlight the value of modelling
stress as a dynamic construct in speech.

</details>


### [14] [BaldWhisper: Faster Whisper with Head Shearing and Layer Merging](https://arxiv.org/abs/2510.08599)
*Yaya Sy,Christophe Cerisara,Irina Illina*

Main category: eess.AS

TL;DR: 提出了一种针对低资源语言Bambara的Whisper模型剪枝方法，通过低秩分解和特征蒸馏压缩嵌入，合并层而非删除层，最终模型在仅32小时语音数据下保持90%性能，体积减小48%，速度提升2.15倍。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言（如Bambara仅有32小时语音数据）场景下，传统剪枝方法需要大量重训练数据（如Distill-Whisper需要21,000小时），这在实际应用中不可行。需要开发适用于数据稀缺环境的轻量化方法。

Method: 1. 避免词汇表剪枝（因Bambara使用者频繁语码转换而不适用）
2. 使用低秩分解和特征蒸馏压缩嵌入
3. 合并层而非删除层以减少性能损失

Result: 最终模型在MacBook Air M1上：
- 保持原始模型90%的性能
- 体积减小48%
- 推理速度提升2.15倍

Conclusion: 该方法证明了在极低资源语言环境下，通过适当的压缩策略可以实现高效的模型轻量化，为边缘设备部署提供了可行方案。

Abstract: Pruning large pre-trained transformers for low-resource languages is
challenging, as it often requires massive retraining data to recover
performance. For instance, Distill-Whisper prunes Whisper by 40% and retrains
on 21,000 hours of speech, far beyond what is available for most languages. Can
Whisper be made lighter and faster for edge devices in data-scarce settings?
Focusing on Bambara with only 32h of speech-to-text data, we propose a new
pruning recipe. Instead of vocabulary pruning, which is unsuitable due to
frequent code-switching by Bambara speakers, we compress the embeddings with
low-rank decomposition and feature distillation. Rather than removing layers,
we merge them to limit performance loss. The final model preserves 90% of the
original performance while being 48% smaller and 2.15x faster on a MacBook Air
M1.

</details>


### [15] [Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization](https://arxiv.org/abs/2510.08618)
*Rui Hu,Delai Qiu,Yining Wang,Shengping Liu,Jitao Sang*

Main category: eess.AS

TL;DR: 提出了VAPO方法，通过强化学习优化多模态大语言模型，利用演示文稿的视觉信息提升语音识别准确性，特别是在专业术语识别方面。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在专业领域（如学术讲座）中处理特定术语时表现不佳，现有方法要么复杂低效，要么退化为简单的OCR系统。

Method: 提出视觉锚定策略优化(VAPO)，基于思维链推理范式，强制模型执行"先看后转录"的结构化过程，使用强化学习优化推理过程。

Result: VAPO显著提升了领域特定术语的识别准确性，为SlideASR任务建立了有效的端到端范式。

Conclusion: VAPO方法通过结构化推理过程和强化学习优化，成功解决了多模态大语言模型在SlideASR任务中的退化问题，提高了专业术语识别性能。

Abstract: Automatic speech recognition (ASR) systems often struggle with
domain-specific terminology, especially in specialized settings such as
academic lectures. To address this, we define the SlideASR task, which
leverages the rich visual information from presentation slides to improve
transcription accuracy. Existing pipeline methods for this task tend to be
complex and underperform. Although omni-modal large language models (OLLMs)
provide a promising end-to-end framework, they frequently fail in practice by
degenerating into simple optical character recognition (OCR) systems. To
overcome this, we propose Visually-Anchored Policy Optimization (VAPO), a novel
post-training method designed to control the model's reasoning process. Drawing
on the Chain-of-Thought reasoning paradigm, VAPO enforces a structured "Look
before Transcription" procedure using a <think><answer> format. Specifically,
the model first performs OCR on the slide content within the think step, then
generates the transcription by referencing this recognized visual information
in the answer step. This reasoning process is optimized via reinforcement
learning with four distinct rewards targeting format compliance, OCR accuracy,
ASR quality, and visual anchoring consistency. To support further research, we
construct SlideASR-Bench, a new entity-rich benchmark consisting of a synthetic
dataset for training and testing, and a challenging real-world set for
evaluation. Extensive experiments demonstrate that VAPO significantly improves
recognition of domain-specific terms, establishing an effective end-to-end
paradigm for SlideASR.

</details>


### [16] [Impact of HRTF individualisation and head movements in a real/virtual localisation task](https://arxiv.org/abs/2510.09161)
*Vincent Martin,Lorenzo Picinali*

Main category: eess.AS

TL;DR: 该研究探讨了个性化HRTF在音频增强现实中对虚拟声源定位和感知真实性的影响，发现在静态条件下个性化HRTF能提高感知真实性但不会改善定位性能，而在允许头部运动时情况相反。


<details>
  <summary>Details</summary>
Motivation: 音频增强现实应用需要将虚拟声源无缝集成到真实环境中，要求虚拟声源精确定位且声学环境准确匹配。个性化HRTF可能对虚拟声源的定位和感知真实性产生重要影响。

Method: 参与者通过耳机和球形扬声器阵列分别定位虚拟和真实语音声源，评估感知真实性和声源位置。比较了单扬声器渲染、使用个性化HRTF和非个性化HRTF的双耳渲染，并探索了头部运动的影响。

Result: 在静态场景中，使用个性化HRTF提高了感知真实性但未改善定位性能；而在允许头部运动时，情况相反。

Conclusion: 个性化HRTF对虚拟声源定位和感知真实性的影响取决于是否允许头部运动，在静态和动态条件下表现出不同的效果。

Abstract: The objective of Audio Augmented Reality (AAR) applications are to seamlessly
integrate virtual sound sources within a real environment. It is critical for
these applications that virtual sources are localised precisely at the intended
position, and that the acoustic environments are accurately matched.
  One effective method for spatialising sound on headphones is through
Head-Related Transfer Functions (HRTFs). These characterise how the physical
features of a listener modify sound waves before they reach the eardrum. This
study examines the influence of using individualised HRTFs on the localisation
and the perceived realism of virtual sound sources associated with a real
visual object.
  Participants were tasked with localising virtual and real speech sources
presented via headphones and through a spherical loudspeaker array,
respectively. The assessment focussed on perceived realism and sources
location. All sources were associated with one of thirty real visual sources
(loudspeakers) arranged in a semi-anechoic room.
  Various sound source renderings were compared, including single loudspeaker
rendering and binaural rendering with individualised or non-individualised
HRTFs. Additionally, the impact of head movements was explored: ten
participants completed the same task with and without the possibility to move
their head.
  The results showed that using individual HRTFs improved perceived realism but
not localisation performance in the static scenario. Surprisingly, the opposite
was observed when head movements were possible and encouraged.

</details>


### [17] [Unsupervised lexicon learning from speech is limited by representations rather than clustering](https://arxiv.org/abs/2510.09225)
*Danel Adendorff,Simon Malan,Herman Kamper*

Main category: eess.AS

TL;DR: 该研究在理想化设置下探讨零资源分词和聚类系统的性能限制因素，发现表示变异性而非聚类方法是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管零资源分词和聚类系统已有进展，但生成的词典仍不完美。研究旨在确定在已知词边界的情况下，性能限制主要来自词段表示还是聚类方法。

Method: 结合多种自监督语音特征（连续/离散、帧级/词级）与不同聚类方法（K-means、层次聚类、图聚类），在英语和普通话数据上进行实验。

Result: 最佳系统使用基于动态时间规整的连续特征图聚类。更快的替代方案使用基于余弦距离的平均连续特征图聚类或基于编辑距离的离散单元序列图聚类。

Conclusion: 通过控制实验证明，同一词类型不同词段间的表示变异性是限制性能的主要因素，而非聚类方法。

Abstract: Zero-resource word segmentation and clustering systems aim to tokenise speech
into word-like units without access to text labels. Despite progress, the
induced lexicons are still far from perfect. In an idealised setting with gold
word boundaries, we ask whether performance is limited by the representation of
word segments, or by the clustering methods that group them into word-like
types. We combine a range of self-supervised speech features
(continuous/discrete, frame/word-level) with different clustering methods
(K-means, hierarchical, graph-based) on English and Mandarin data. The best
system uses graph clustering with dynamic time warping on continuous features.
Faster alternatives use graph clustering with cosine distance on averaged
continuous features or edit distance on discrete unit sequences. Through
controlled experiments that isolate either the representations or the
clustering method, we demonstrate that representation variability across
segments of the same word type -- rather than clustering -- is the primary
factor limiting performance.

</details>


### [18] [Effects of automotive microphone frequency response characteristics and noise conditions on speech and ASR quality -- an experimental evaluation](https://arxiv.org/abs/2510.09236)
*Michele Buccoli,Yu Du,Jacob Soendergaard,Simone Shawn Cazzaniga*

Main category: eess.AS

TL;DR: 该研究通过实验分析了汽车麦克风带宽和频率响应特性对语音通信质量和ASR性能的影响，为汽车应用中的麦克风规格选择提供了依据。


<details>
  <summary>Details</summary>
Motivation: 汽车制造商在麦克风选择时遵循标准带宽要求，但在实际应用中受限于车内布局和环境要求，难以达到理想带宽。同时缺乏关于麦克风特性对性能影响的数据和共识。

Method: 使用真实车辆在不同驾驶条件下记录的噪声信号，实验研究麦克风特性与语音通信质量和ASR性能的关系，重点关注带宽和幅度频率响应形状的变化对感知语音质量的影响。

Result: 使用ETSI TS 103 281指标(S-MOS、N-MOS、G-MOS)和SNR等辅助指标评估语音质量，使用WER等标准指标评估ASR性能，获得了关于麦克风频率响应特性重要性的发现。

Conclusion: 研究结果为理解哪些麦克风频率响应特性对音频质量更相关提供了知识，有助于为汽车应用选择合适的麦克风规格。

Abstract: Upon choosing microphones for automotive hands-free communication or
Automatic Speech Recognition (ASR) applications, OEMs typically specify
wideband, super wideband or even fullband requirements following established
standard recommendations (e.g., ITU-P.1110, ITU-P.1120). In practice, it is
often challenging to achieve the preferred bandwidth for an automotive
microphone when considering limitations and constraints on microphone placement
inside the cabin, and the automotive grade environmental robustness
requirements. On the other hand, there seems to be no consensus or sufficient
data on the effect of each microphone characteristic on the actual performance.
As an attempt to answer this question, we used noise signals recorded in real
vehicles and under various driving conditions to experimentally study the
relationship between the microphones' characteristics and the final audio
quality of speech communication and performance of ASR engines. We focus on how
variations in microphone bandwidth and amplitude frequency response shapes
affect the perceptual speech quality. The speech quality results are compared
by using ETSI TS 103 281 metrics (S-MOS, N-MOS, G-MOS) and ancillary metrics
such as SNR. The ASR results are evaluated with standard metrics such as Word
Error Rate (WER). Findings from this study provide knowledge in the
understanding of what microphone frequency response characteristics are more
relevant for audio quality and choice of proper microphone specifications,
particularly for automotive applications.

</details>


### [19] [Target speaker anonymization in multi-speaker recordings](https://arxiv.org/abs/2510.09307)
*Natalia Tomashenko,Junichi Yamagishi,Xin Wang,Yun Liu,Emmanuel Vincent*

Main category: eess.AS

TL;DR: 该研究针对多说话人对话音频中的目标说话人匿名化问题，提出了改进策略和评估方法，特别适用于客服场景中仅需匿名化客户声音的情况。


<details>
  <summary>Details</summary>
Motivation: 现有说话人匿名化研究主要针对单说话人音频，缺乏对多说话人对话场景的支持，特别是在客服等场景中仅需匿名化特定说话人（如客户）的需求。

Method: 探索多说话人对话音频中目标说话人匿名化的有效策略，识别开发过程中的潜在问题，并提出改进的评估方法。

Result: 揭示了传统匿名化方法在多说话人场景中的局限性，以及当前评估方法无法准确衡量隐私保护和实用性的问题。

Conclusion: 该研究填补了多说话人对话音频中目标说话人匿名化的空白，为相关应用场景提供了技术基础和评估框架。

Abstract: Most of the existing speaker anonymization research has focused on
single-speaker audio, leading to the development of techniques and evaluation
metrics optimized for such condition. This study addresses the significant
challenge of speaker anonymization within multi-speaker conversational audio,
specifically when only a single target speaker needs to be anonymized. This
scenario is highly relevant in contexts like call centers, where customer
privacy necessitates anonymizing only the customer's voice in interactions with
operators. Conventional anonymization methods are often not suitable for this
task. Moreover, current evaluation methodology does not allow us to accurately
assess privacy protection and utility in this complex multi-speaker scenario.
This work aims to bridge these gaps by exploring effective strategies for
targeted speaker anonymization in conversational audio, highlighting potential
problems in their development and proposing corresponding improved evaluation
methodologies.

</details>


### [20] [A Study of the Removability of Speaker-Adversarial Perturbations](https://arxiv.org/abs/2510.09504)
*Liping Chen,Chenyang Guo,Kong Aik Lee,Zhen-Hua Ling,Wu Guo*

Main category: eess.AS

TL;DR: 本文研究了说话人对抗扰动的可移除性，在不同对抗攻击者知识水平下（无知、半知情、全知情）进行实验，发现只有全知情场景下才能几乎完全移除扰动并恢复原始语音。


<details>
  <summary>Details</summary>
Motivation: 现有防御技术仅关注减少对抗扰动对说话人属性提取的影响，而不寻求完全移除扰动和恢复原始语音。本文旨在研究说话人对抗扰动的可移除性问题。

Method: 在三种不同对抗攻击者知识水平场景下（无知、半知情、全知情），使用基于优化的前向传播扰动生成方法，在LibriSpeech数据集上进行实验。

Result: 1) 无知场景下无法消除扰动，但能减少其对说话人属性提取的影响；2) 半知情场景下无法完全移除优化生成的扰动，但前向传播模型生成的扰动可显著减少；3) 全知情场景下几乎能完全消除扰动并恢复原始语音。

Conclusion: 说话人对抗扰动的可移除性高度依赖于对扰动生成器的了解程度，只有在全知情场景下才能有效移除扰动并恢复原始语音。

Abstract: Recent advancements in adversarial attacks have demonstrated their
effectiveness in misleading speaker recognition models, making wrong
predictions about speaker identities. On the other hand, defense techniques
against speaker-adversarial attacks focus on reducing the effects of
speaker-adversarial perturbations on speaker attribute extraction. These
techniques do not seek to fully remove the perturbations and restore the
original speech. To this end, this paper studies the removability of
speaker-adversarial perturbations. Specifically, the investigation is conducted
assuming various degrees of awareness of the perturbation generator across
three scenarios: ignorant, semi-informed, and well-informed. Besides, we
consider both the optimization-based and feedforward perturbation generation
methods. Experiments conducted on the LibriSpeech dataset demonstrated that: 1)
in the ignorant scenario, speaker-adversarial perturbations cannot be
eliminated, although their impact on speaker attribute extraction is reduced,
2) in the semi-informed scenario, the speaker-adversarial perturbations cannot
be fully removed, while those generated by the feedforward model can be
considerably reduced, and 3) in the well-informed scenario, speaker-adversarial
perturbations are nearly eliminated, allowing for the restoration of the
original speech. Audio samples can be found in
https://voiceprivacy.github.io/Perturbation-Generation-Removal/.

</details>


### [21] [Spatially-Augmented Sequence-to-Sequence Neural Diarization for Meetings](https://arxiv.org/abs/2510.09505)
*Li Li,Ming Cheng,Hongyu Zhang,Juan Liu,Ming Li*

Main category: eess.AS

TL;DR: 提出SA-S2SND框架，将SRP-DNN估计的DOA信息集成到S2SND骨干网络中，通过两阶段训练策略提升说话人日志性能。


<details>
  <summary>Details</summary>
Motivation: 利用空间线索（DOA）来增强说话人日志性能，解决传统方法对空间信息利用不足的问题。

Method: 采用两阶段训练：先用单通道音频和DOA特征训练模型，再用多通道输入在DOA指导下优化；引入模拟DOA生成方案减少对匹配多通道语料库的依赖。

Result: 在AliMeeting数据集上，SA-S2SND相比S2SND基线在离线模式下相对DER降低7.4%，结合通道注意力时提升超过19%。

Conclusion: 空间线索与跨通道建模高度互补，在在线和离线设置下均能获得良好性能。

Abstract: This paper proposes a Spatially-Augmented Sequence-to-Sequence Neural
Diarization (SA-S2SND) framework, which integrates direction-of-arrival (DOA)
cues estimated by SRP-DNN into the S2SND backbone. A two-stage training
strategy is adopted: the model is first trained with single-channel audio and
DOA features, and then further optimized with multi-channel inputs under DOA
guidance. In addition, a simulated DOA generation scheme is introduced to
alleviate dependence on matched multi-channel corpora. On the AliMeeting
dataset, SA-S2SND consistently outperform the S2SND baseline, achieving a 7.4%
relative DER reduction in the offline mode and over 19% improvement when
combined with channel attention. These results demonstrate that spatial cues
are highly complementary to cross-channel modeling, yielding good performance
in both online and offline settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [22] [LadderSym: A Multimodal Interleaved Transformer for Music Practice Error Detection](https://arxiv.org/abs/2510.08580)
*Benjamin Shiue-Hal Chou,Purvish Jajal,Nick John Eliopoulos,James C. Davis,George K. Thiruvathukal,Kristen Yeon-Ji Yun,Yung-Hsiang Lu*

Main category: cs.SD

TL;DR: LadderSym是一种基于Transformer的音乐错误检测方法，通过双流编码器和跨模态策略显著提升了音符错误检测性能


<details>
  <summary>Details</summary>
Motivation: 现有音乐错误检测方法存在两个主要问题：后期融合限制了跨流对齐和跨模态比较能力；依赖乐谱音频在频谱中引入歧义，影响并发音符的检测性能

Method: 提出LadderSym方法：1) 使用带有跨流对齐模块的双流编码器提升音频比较能力；2) 采用多模态策略，将符号表示作为解码器提示，减少歧义

Result: 在MAESTRO-E数据集上，漏音符检测F1分数从26.8%提升至56.3%，多余音符检测从72.0%提升至86.4%。在CocoChorales-E数据集上也观察到类似提升

Conclusion: 这项工作为比较模型提供了通用见解，可应用于强化学习序列评估、人类技能评估和模型评估任务

Abstract: Music learners can greatly benefit from tools that accurately detect errors
in their practice. Existing approaches typically compare audio recordings to
music scores using heuristics or learnable models. This paper introduces
\textit{LadderSym}, a novel Transformer-based method for music error detection.
\textit{LadderSym} is guided by two key observations about the state-of-the-art
approaches: (1) late fusion limits inter-stream alignment and cross-modality
comparison capability; and (2) reliance on score audio introduces ambiguity in
the frequency spectrum, degrading performance in music with concurrent notes.
To address these limitations, \textit{LadderSym} introduces (1) a two-stream
encoder with inter-stream alignment modules to improve audio comparison
capabilities and error detection F1 scores, and (2) a multimodal strategy that
leverages both audio and symbolic scores by incorporating symbolic
representations as decoder prompts, reducing ambiguity and improving F1 scores.
We evaluate our method on the \textit{MAESTRO-E} and \textit{CocoChorales-E}
datasets by measuring the F1 score for each note category. Compared to the
previous state of the art, \textit{LadderSym} more than doubles F1 for missed
notes on \textit{MAESTRO-E} (26.8\% $\rightarrow$ 56.3\%) and improves extra
note detection by 14.4 points (72.0\% $\rightarrow$ 86.4\%). Similar gains are
observed on \textit{CocoChorales-E}. This work introduces general insights
about comparison models that could inform sequence evaluation tasks for
reinforcement Learning, human skill assessment, and model evaluation.

</details>


### [23] [Evaluating Hallucinations in Multimodal LLMs with Spoken Queries under Diverse Acoustic Conditions](https://arxiv.org/abs/2510.08581)
*Hansol Park,Hoseong Ahn,Junwon Moon,Yejin Lee,Kyuhong Shim*

Main category: cs.SD

TL;DR: 本文研究了语音输入对多模态大语言模型幻觉的影响，发现语音查询比文本查询更容易引发幻觉，特别是在有环境噪声的情况下错误率显著增加。


<details>
  <summary>Details</summary>
Motivation: 随着语音驱动接口的日益普及，语音输入对多模态幻觉的影响尚未得到充分研究，而现有的基准测试主要关注图像-文本设置下的可靠性。

Method: 提出了RePOPE-Spk基准测试，这是一个音频增强的扩展版本，在多种声学条件下提供语音查询，并系统评估了专有和开源模型。

Result: 实验结果显示，语音查询比文本查询更容易引发幻觉：在清晰语音下错误率增加3%，在环境噪声下错误率最高增加20%。输入顺序和查询长度也会影响鲁棒性。

Conclusion: 这些发现突出了一个关键且未被充分探索的挑战，为构建可靠的语音接口系统开辟了新的研究方向。

Abstract: Hallucinations in vision-language models have been extensively studied using
benchmarks that probe reliability in image-text settings. In contrast, the
effect of spoken queries on multimodal hallucinations remains largely
unexplored, despite the growing role of voice-driven interfaces. In this work,
we investigate how spoken input influences hallucinations in multimodal large
language models. We present RePOPE-Spk, an audio-augmented extension of the
RePOPE benchmark, where queries are provided as speech under diverse acoustic
conditions. Using RePOPE-Spk, we systematically evaluate both proprietary and
open-source models. Experimental results show that hallucinations escalate when
queries are spoken rather than written: error rates increase by 3% under clean
speech and by up to 20% with environmental noise. Input order and query length
further affect robustness, while strategies such as many-shot prompting and
chain-of-thought reasoning offer partial but insufficient mitigation. These
findings highlight a critical and underexplored challenge, opening new
directions for building reliable voice interface systems.

</details>


### [24] [EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation](https://arxiv.org/abs/2510.08587)
*Tianheng Zhu,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.SD

TL;DR: EGSTalker是一个基于3D高斯溅射的实时音频驱动说话头生成框架，只需3-5分钟训练视频即可合成高质量面部动画，在保持渲染质量和唇形同步精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 旨在提升音频驱动说话头生成的速度和视觉保真度，满足实时多媒体应用的需求。

Method: 采用两阶段框架：静态高斯初始化和音频驱动变形。第一阶段使用多分辨率哈希三平面和KAN网络构建紧凑3D高斯表示；第二阶段提出高效空间-音频注意力模块融合音频和空间线索，KAN预测高斯变形。

Result: 实验表明EGSTalker在渲染质量和唇形同步精度上与最先进方法相当，但在推理速度上显著优于现有方法。

Conclusion: EGSTalker展示了在实时多媒体应用中的巨大潜力，实现了高质量面部动画的快速生成。

Abstract: This paper presents EGSTalker, a real-time audio-driven talking head
generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance
both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training
video to synthesize high-quality facial animations. The framework comprises two
key stages: static Gaussian initialization and audio-driven deformation. In the
first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network
(KAN) are used to extract spatial features and construct a compact 3D Gaussian
representation. In the second stage, we propose an Efficient Spatial-Audio
Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the
corresponding Gaussian deformations. Extensive experiments demonstrate that
EGSTalker achieves rendering quality and lip-sync accuracy comparable to
state-of-the-art methods, while significantly outperforming them in inference
speed. These results highlight EGSTalker's potential for real-time multimedia
applications.

</details>


### [25] [Audible Networks: Deconstructing and Manipulating Sounds with Deep Non-Negative Autoencoders](https://arxiv.org/abs/2510.08816)
*Juan José Burred,Carmine-Emanuele Cella*

Main category: cs.SD

TL;DR: 提出使用非负自编码器进行声音解构和用户引导的声音操作，作为可解释音频分解的扩展方法


<details>
  <summary>Details</summary>
Motivation: 传统非负矩阵分解方法在可解释音频分解方面存在局限性，需要更灵活和可扩展的方法来支持创意声音处理

Method: 使用非负自编码器，通过投影梯度下降强制非负约束，获得可直接解释为频谱形状和时间包络的分解，支持多层深度架构实现分层表示

Result: 实现了分层声音解构，从高级音符包络到细粒度频谱细节，支持跨组件和跨层合成、分层解构以及随机化策略等多种操作

Conclusion: 非负自编码器可作为灵活且可解释的对象化声音编辑工具，支持广泛的声音变换操作

Abstract: We propose the use of Non-Negative Autoencoders (NAEs) for sound
deconstruction and user-guided manipulation of sounds for creative purposes.
NAEs offer a versatile and scalable extension of traditional Non-Negative
Matrix Factorization (NMF)-based approaches for interpretable audio
decomposition. By enforcing non-negativity constraints through projected
gradient descent, we obtain decompositions where internal weights and
activations can be directly interpreted as spectral shapes and temporal
envelopes, and where components can themselves be listened to as individual
sound events. In particular, multi-layer Deep NAE architectures enable
hierarchical representations with an adjustable level of granularity, allowing
sounds to be deconstructed at multiple levels of abstraction: from high-level
note envelopes down to fine-grained spectral details. This framework enables a
wide new range of expressive, controllable, and randomized sound
transformations. We introduce novel manipulation operations including
cross-component and cross-layer synthesis, hierarchical deconstructions, and
several randomization strategies that control timbre and event density. Through
visualizations and resynthesis of practical examples, we demonstrate how NAEs
can serve as flexible and interpretable tools for object-based sound editing.

</details>


### [26] [ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling](https://arxiv.org/abs/2510.08878)
*Yuxuan Jiang,Zehua Chen,Zeqian Ju,Yusheng Dai,Weibei Dou,Jun Zhu*

Main category: cs.SD

TL;DR: ControlAudio是一个渐进式扩散模型，通过多任务学习实现细粒度可控的文本到音频生成，在时序精度和语音清晰度方面达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的可控文本到音频生成方法受限于数据稀缺，在大规模生成性能上仍有不足，需要更好的方法来处理细粒度控制信号如时序和音素特征。

Method: 采用渐进式扩散建模方法：1) 通过标注和模拟构建包含文本、时序和音素条件的数据；2) 在大规模文本-音频对上预训练扩散变换器，然后逐步集成时序和音素特征；3) 推理阶段采用渐进引导生成策略。

Result: 在客观和主观评估中显著优于现有方法，在时序精度和语音清晰度方面达到最先进性能。

Conclusion: ControlAudio通过渐进式多任务学习成功实现了细粒度可控的文本到音频生成，证明了该方法在提升生成质量和控制精度方面的有效性。

Abstract: Text-to-audio (TTA) generation with fine-grained control signals, e.g.,
precise timing control or intelligible speech content, has been explored in
recent works. However, constrained by data scarcity, their generation
performance at scale is still compromised. In this study, we recast
controllable TTA generation as a multi-task learning problem and introduce a
progressive diffusion modeling approach, ControlAudio. Our method adeptly fits
distributions conditioned on more fine-grained information, including text,
timing, and phoneme features, through a step-by-step strategy. First, we
propose a data construction method spanning both annotation and simulation,
augmenting condition information in the sequence of text, timing, and phoneme.
Second, at the model training stage, we pretrain a diffusion transformer (DiT)
on large-scale text-audio pairs, achieving scalable TTA generation, and then
incrementally integrate the timing and phoneme features with unified semantic
representations, expanding controllability. Finally, at the inference stage, we
propose progressively guided generation, which sequentially emphasizes more
fine-grained information, aligning inherently with the coarse-to-fine sampling
nature of DiT. Extensive experiments show that ControlAudio achieves
state-of-the-art performance in terms of temporal accuracy and speech clarity,
significantly outperforming existing methods on both objective and subjective
evaluations. Demo samples are available at:
https://control-audio.github.io/Control-Audio.

</details>


### [27] [VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR Virtual Microphone Arrays](https://arxiv.org/abs/2510.08914)
*Shulin He,Zhong-Qiu Wang*

Main category: cs.SD

TL;DR: VM-UNSSOR通过添加虚拟麦克风信号增强UNSSOR的无监督语音分离性能，解决了麦克风数量减少时性能急剧下降的问题。


<details>
  <summary>Details</summary>
Motivation: 在无监督语音分离中，当训练混合信号的麦克风数量减少时，混合一致性约束变弱，导致分离性能显著下降。

Method: 通过应用线性空间分离器（如IVA和空间聚类）到观察到的训练混合信号，生成更高信噪比的虚拟麦克风信号，并利用这些信号计算额外的混合一致性损失。

Result: 在六麦克风双说话人分离中，VM-UNSSOR达到17.1 dB SI-SDR，而UNSSOR仅为14.7 dB；在双麦克风双说话人情况下，UNSSOR崩溃至-2.7 dB，而VM-UNSSOR仍达到10.7 dB。

Conclusion: 虚拟麦克风信号能有效增强混合一致性约束，显著提升无监督语音分离在麦克风数量有限时的性能。

Abstract: Blind speech separation (BSS) aims to recover multiple speech sources from
multi-channel, multi-speaker mixtures under unknown array geometry and room
impulse responses. In unsupervised setup where clean target speech is not
available for model training, UNSSOR proposes a mixture consistency (MC) loss
for training deep neural networks (DNN) on over-determined training mixtures to
realize unsupervised speech separation. However, when the number of microphones
of the training mixtures decreases, the MC constraint weakens and the
separation performance falls dramatically. To address this, we propose
VM-UNSSOR, augmenting the observed training mixture signals recorded by a
limited number of microphones with several higher-SNR virtual-microphone (VM)
signals, which are obtained by applying linear spatial demixers (such as IVA
and spatial clustering) to the observed training mixtures. As linear
projections of the observed mixtures, the virtual-microphone signals can
typically increase the SNR of each source and can be leveraged to compute extra
MC losses to improve UNSSOR and address the frequency permutation problem in
UNSSOR. On the SMS-WSJ dataset, in the over-determined six-microphone,
two-speaker separation setup, VM-UNSSOR reaches 17.1 dB SI-SDR, while UNSSOR
only obtains 14.7 dB; and in the determined two-microphone, two-speaker case,
UNSSOR collapses to -2.7 dB SI-SDR, while VM-UNSSOR achieves 10.7 dB.

</details>


### [28] [DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment](https://arxiv.org/abs/2510.09016)
*Zongcai Du,Guilin Deng,Xiaofeng Guo,Xin Gao,Linke Li,Kaichang Cheng,Fubo Han,Siyu Yang,Peng Liu,Pan Zhong,Qiang Fu*

Main category: cs.SD

TL;DR: 提出了DiTSinger，一个基于扩散变换器的歌唱语音合成系统，通过两阶段流程生成大规模高质量中文歌唱数据，并引入隐式对齐机制消除对音素级时长标签的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散的歌唱语音合成面临的数据稀缺和模型可扩展性限制问题。

Method: 采用两阶段流程：首先构建包含固定旋律和多样化LLM生成歌词的种子数据集，训练旋律特定模型生成500+小时高质量中文歌唱数据；然后提出DiTSinger，使用RoPE和qk-norm的扩散变换器，在深度、宽度和分辨率上进行系统扩展；设计隐式对齐机制，通过约束音素到声学注意力在字符级跨度内，消除对音素级时长标签的需求。

Result: 实验验证该方法能够实现可扩展、无需对齐和高保真的歌唱语音合成。

Conclusion: 该方法为歌唱语音合成提供了可扩展、对齐自由和高保真的解决方案。

Abstract: Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates
strong expressiveness but remains limited by data scarcity and model
scalability. We introduce a two-stage pipeline: a compact seed set of
human-sung recordings is constructed by pairing fixed melodies with diverse
LLM-generated lyrics, and melody-specific models are trained to synthesize over
500 hours of high-quality Chinese singing data. Building on this corpus, we
propose DiTSinger, a Diffusion Transformer with RoPE and qk-norm,
systematically scaled in depth, width, and resolution for enhanced fidelity.
Furthermore, we design an implicit alignment mechanism that obviates
phoneme-level duration labels by constraining phoneme-to-acoustic attention
within character-level spans, thereby improving robustness under noisy or
uncertain alignments. Extensive experiments validate that our approach enables
scalable, alignment-free, and high-fidelity SVS.

</details>


### [29] [Déréverbération non-supervisée de la parole par modèle hybride](https://arxiv.org/abs/2510.09025)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: 提出一种仅使用混响语音的无监督训练策略来改进语音去混响系统，无需成对的干/混响数据。


<details>
  <summary>Details</summary>
Motivation: 现有算法大多依赖成对的干/混响数据，但这种数据难以获取，因此需要开发仅使用混响语音的无监督方法。

Method: 利用有限的声学信息（如混响时间RT60）来训练去混响系统，采用无监督学习策略。

Result: 实验结果表明，该方法在各种客观指标上比现有最先进方法表现更一致。

Conclusion: 所提出的无监督训练策略能够有效改进语音去混响系统，在缺乏成对数据的情况下实现良好性能。

Abstract: This paper introduces a new training strategy to improve speech
dereverberation systems in an unsupervised manner using only reverberant
speech. Most existing algorithms rely on paired dry/reverberant data, which is
difficult to obtain. Our approach uses limited acoustic information, like the
reverberation time (RT60), to train a dereverberation system. Experimental
results demonstrate that our method achieves more consistent performance across
various objective metrics than the state-of-the-art.

</details>


### [30] [O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion](https://arxiv.org/abs/2510.09061)
*Huu Tuong Tu,Huan Vu,cuong tien nguyen,Dien Hy Ngo,Nguyen Thi Thu Trang*

Main category: cs.SD

TL;DR: 提出一种利用预训练多说话人TTS模型生成合成语音数据来训练语音转换模型的新方法，通过共享相同语言内容但说话人不同的数据对，直接学习源说话人到目标说话人的映射。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换方法在分离说话人身份和语言信息时存在信息损失问题，难以有效解耦这些因素。

Method: 使用预训练多说话人TTS模型生成合成语音数据对，这些数据对共享语言内容但说话人不同，作为输入-输出对训练语音转换模型，实现任意到任意语音转换的灵活训练策略。

Result: 实验显示该方法在词错误率上相对降低16.35%，在说话人余弦相似度上提升5.91%，优于多个最先进方法。

Conclusion: 提出的方法能够有效捕捉说话人特定特征同时保持语言内容，在零样本场景下具有更好的适应性和性能。

Abstract: Traditional voice conversion (VC) methods typically attempt to separate
speaker identity and linguistic information into distinct representations,
which are then combined to reconstruct the audio. However, effectively
disentangling these factors remains challenging, often leading to information
loss during training. In this paper, we propose a new approach that leverages
synthetic speech data generated by a high-quality, pretrained multispeaker
text-to-speech (TTS) model. Specifically, synthetic data pairs that share the
same linguistic content but differ in speaker identity are used as input-output
pairs to train the voice conversion model. This enables the model to learn a
direct mapping between source and target voices, effectively capturing
speaker-specific characteristics while preserving linguistic content.
Additionally, we introduce a flexible training strategy for any-to-any voice
conversion that generalizes well to unseen speakers and new languages,
enhancing adaptability and performance in zero-shot scenarios. Our experiments
show that our proposed method achieves a 16.35% relative reduction in word
error rate and a 5.91% improvement in speaker cosine similarity, outperforming
several state-of-the-art methods. Voice conversion samples can be accessed at:
https://oovc-emnlp-2025.github.io/

</details>


### [31] [MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation](https://arxiv.org/abs/2510.09065)
*Akira Takahashi,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: MMAudioSep是基于预训练视频到音频模型的生成式音频分离模型，通过利用预训练知识实现高效训练，在音频分离任务上优于现有方法，同时保留原始视频到音频生成能力。


<details>
  <summary>Details</summary>
Motivation: 探索利用预训练音频生成模型的知识来更高效地训练音频分离模型，避免从零开始训练，并验证基础音频生成模型在下游任务中的潜力。

Method: 基于预训练的视频到音频模型构建生成式音频分离模型，通过微调方式训练，利用预训练模型学到的视频/文本与音频关系知识。

Result: MMAudioSep在音频分离性能上优于现有的确定性和生成式基线模型，且微调后仍保留原始视频到音频生成能力。

Conclusion: 基础音频生成模型具有被适配用于音频相关下游任务的潜力，MMAudioSep展示了这种方法的有效性。

Abstract: We introduce MMAudioSep, a generative model for video/text-queried sound
separation that is founded on a pretrained video-to-audio model. By leveraging
knowledge about the relationship between video/text and audio learned through a
pretrained audio generative model, we can train the model more efficiently,
i.e., the model does not need to be trained from scratch. We evaluate the
performance of MMAudioSep by comparing it to existing separation models,
including models based on both deterministic and generative approaches, and
find it is superior to the baseline models. Furthermore, we demonstrate that
even after acquiring functionality for sound separation via fine-tuning, the
model retains the ability for original video-to-audio generation. This
highlights the potential of foundational sound generation models to be adopted
for sound-related downstream tasks. Our code is available at
https://github.com/sony/mmaudiosep.

</details>


### [32] [Emotion-Disentangled Embedding Alignment for Noise-Robust and Cross-Corpus Speech Emotion Recognition](https://arxiv.org/abs/2510.09072)
*Upasana Tiwari,Rupayan Chakraborty,Sunil Kumar Kopparapu*

Main category: cs.SD

TL;DR: 提出一种两阶段方法EDRL-MEA来增强语音情感识别的鲁棒性和泛化能力，通过情感解耦表示学习和多块嵌入对齐来提取判别性特征


<details>
  <summary>Details</summary>
Motivation: 现实场景中的语音情感识别常受噪声环境和数据集差异影响，需要提高模型的鲁棒性和跨数据集泛化能力

Method: 使用EDRL进行情感解耦表示学习，提取类别特异性特征并保留跨类别相似性；然后通过MEA将表示投影到联合判别性潜在子空间，最大化与原始语音输入的协方差

Result: 在未见过的噪声和跨语料库语音样本上评估，表现出改进的性能，证明该方法在挑战性条件下的有效性

Conclusion: EDRL-MEA方法通过改进表示学习，有效提升了语音情感识别在现实场景中的鲁棒性和泛化能力

Abstract: Effectiveness of speech emotion recognition in real-world scenarios is often
hindered by noisy environments and variability across datasets. This paper
introduces a two-step approach to enhance the robustness and generalization of
speech emotion recognition models through improved representation learning.
First, our model employs EDRL (Emotion-Disentangled Representation Learning) to
extract class-specific discriminative features while preserving shared
similarities across emotion categories. Next, MEA (Multiblock Embedding
Alignment) refines these representations by projecting them into a joint
discriminative latent subspace that maximizes covariance with the original
speech input. The learned EDRL-MEA embeddings are subsequently used to train an
emotion classifier using clean samples from publicly available datasets, and
are evaluated on unseen noisy and cross-corpus speech samples. Improved
performance under these challenging conditions demonstrates the effectiveness
of the proposed method.

</details>


### [33] [SynthVC: Leveraging Synthetic Data for End-to-End Low Latency Streaming Voice Conversion](https://arxiv.org/abs/2510.09245)
*Zhao Guo,Ziqian Ning,Guobin Ma,Lei Xie*

Main category: cs.SD

TL;DR: SynthVC是一个流式端到端语音转换框架，通过使用预训练零样本VC模型生成的合成并行数据直接学习说话人音色转换，无需显式的内容-说话人分离或识别模块，实现了77.1ms的低延迟流式推理。


<details>
  <summary>Details</summary>
Motivation: 现有VC模型在实时流式场景中表现不佳，存在高延迟、依赖ASR模块或复杂说话人解耦等问题，导致音色泄漏或自然度下降。

Method: 基于神经音频编解码器架构，直接从预训练零样本VC模型生成的合成并行数据中学习说话人音色转换，无需显式的内容-说话人分离。

Result: 实验结果显示，SynthVC在自然度和说话人相似度方面均优于基线流式VC系统，端到端延迟仅为77.1ms。

Conclusion: SynthVC提供了一种有效的流式端到端语音转换解决方案，在保持高输出保真度的同时实现了低延迟推理。

Abstract: Voice Conversion (VC) aims to modify a speaker's timbre while preserving
linguistic content. While recent VC models achieve strong performance, most
struggle in real-time streaming scenarios due to high latency, dependence on
ASR modules, or complex speaker disentanglement, which often results in timbre
leakage or degraded naturalness. We present SynthVC, a streaming end-to-end VC
framework that directly learns speaker timbre transformation from synthetic
parallel data generated by a pre-trained zero-shot VC model. This design
eliminates the need for explicit content-speaker separation or recognition
modules. Built upon a neural audio codec architecture, SynthVC supports
low-latency streaming inference with high output fidelity. Experimental results
show that SynthVC outperforms baseline streaming VC systems in both naturalness
and speaker similarity, achieving an end-to-end latency of just 77.1 ms.

</details>


### [34] [WildElder: A Chinese Elderly Speech Dataset from the Wild with Fine-Grained Manual Annotations](https://arxiv.org/abs/2510.09344)
*Hui Wang,Jiaming Zhou,Jiabei He,Haoqin Sun,Yong Qin*

Main category: cs.SD

TL;DR: WildElder是一个从在线视频收集的中文老年语音语料库，包含精细的人工标注，用于自动语音识别和说话人分析研究。


<details>
  <summary>Details</summary>
Motivation: 现有中文数据集大多在受控环境中录制，缺乏多样性和现实适用性，而老年语音由于年龄相关变化（如语速变慢、声音颤抖）对自动处理提出独特挑战。

Method: 从在线视频收集老年语音数据，并进行精细人工标注，包括转录、说话人年龄、性别和口音强度。

Result: 实验结果显示老年语音识别的困难性，以及WildElder作为具有挑战性新基准的潜力。

Conclusion: WildElder结合了野外数据的真实性和专家标注，为自动语音识别和说话人分析研究提供了有力支持。

Abstract: Elderly speech poses unique challenges for automatic processing due to
age-related changes such as slower articulation and vocal tremors. Existing
Chinese datasets are mostly recorded in controlled environments, limiting their
diversity and real-world applicability. To address this gap, we present
WildElder, a Mandarin elderly speech corpus collected from online videos and
enriched with fine-grained manual annotations, including transcription, speaker
age, gender, and accent strength. Combining the realism of in-the-wild data
with expert curation, WildElder enables robust research on automatic speech
recognition and speaker profiling. Experimental results reveal both the
difficulties of elderly speech recognition and the potential of WildElder as a
challenging new benchmark. The dataset and code are available at
https://github.com/NKU-HLT/WildElder.

</details>

{"id": "2510.09940", "categories": ["eess.SP", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09940", "abs": "https://arxiv.org/abs/2510.09940", "authors": ["Haytham Albousayri", "Bechir Hamdaoui", "Weng-Keen Wong", "Nora Basha"], "title": "Bluetooth Fingerprint Identification Under Domain Shift Through Transient Phase Derivative", "comment": "9 pages, IEEE CNS 2025", "summary": "Deep learning-based radio frequency fingerprinting (RFFP) has become an\nenabling physical-layer security technology, allowing device identification and\nauthentication through received RF signals. This technology, however, faces\nsignificant challenges when it comes to adapting to domain variations, such as\ntime, location, environment, receiver and channel. For Bluetooth Low Energy\n(BLE) devices, addressing these challenges is particularly crucial due to the\nBLE protocol's frequency-hopping nature. In this work, and for the first time,\nwe investigated the frequency hopping effect on RFFP of BLE devices, and\nproposed a novel, low-cost, domain-adaptive feature extraction method. Our\napproach improves the classification accuracy by up to 58\\% across environments\nand up to 80\\% across receivers compared to existing benchmarks."}
{"id": "2510.09949", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.09949", "abs": "https://arxiv.org/abs/2510.09949", "authors": ["Ran Yang", "Zheng Dong", "Peng Cheng", "Lin Zhang", "Wanting Lyu", "Yue Xiu", "Ning Wei", "Chadi Assi"], "title": "Movable Antenna Enhanced Covert Dual-Functional Radar-Communication: Joint Beamforming and Antenna Position Optimization", "comment": null, "summary": "Movable antenna (MA) has emerged as a promising technology to flexibly\nreconfigure wireless channels by adjusting antenna placement. In this paper, we\nstudy a dual-functional radar-communication (DFRC) system enhanced with movable\nantennas. To ensure communication security, we aim to maximize the achievable\nsum rate by jointly optimizing the transmit beamforming vectors, receiving\nfilter, and antenna placement, subject to radar signal-to-noise ratio (SNR)\nperformance and transmission covertness constraints. To tackle this challenging\noptimization problem, we first employ a Lagrangian dual transformation process\nto reformulate it into a more tractable form. Subsequently, the problem is\nsolved by introducing a block coordinate descent (BCD) algorithm, incorporating\nsemidefinite relaxation (SDR), projected gradient descent (PGD), and successive\nconvex approximation (SCA) techniques. Simulation results demonstrate that the\nproposed method can significantly improve the covert sum rate, and achieve a\nsatisfactory balance between the communication and radar performance compared\nwith existing benchmark schemes by leveraging the flexibility of movable\nantennas."}
{"id": "2510.10045", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10045", "abs": "https://arxiv.org/abs/2510.10045", "authors": ["Qiaoyan Peng", "Qingqing Wu", "Guangji Chen", "Wen Chen", "Shaodan Ma"], "title": "Active IRS Assisted Joint Uplink and Downlink Communications", "comment": null, "summary": "In this paper, we investigate an intelligent reflecting surface (IRS) aided\nwireless communication system, where active IRSs (AIRSs) are deployed to assist\ncommunication between a base station (BS) and users of both the uplink (UL) and\ndownlink (DL). We aim to maximize the weighted sum rate (WSR) of UL and DL\ncommunications through joint optimization of BS, AIRS beamforming, and AIRS\nelement allocation. First, we study three deployment schemes, namely\ndistributed AIRSs, BS-side AIRS, and user-side AIRS. For distributed AIRSs,\nboth optimal and near-optimal solutions are derived in closed form. To draw\nuseful insights, we analytically compare the deployment schemes in terms of the\nrate performance under the single-user setup. For the multi-user case, we\nconsider two beamforming setups at the distributed AIRSs to balance performance\nand complexity tradeoffs. Regarding the user-adaptive AIRS beamforming,\ndifferent AIRS beamforming vectors are adopted for each user; while for the\nstatic AIRS beamforming, all users share the same beamforming vectors, with\nidentical phase shifts but different amplitudes for UL and DL. With the\nuser-adaptive AIRS beamforming, we focus on the optimization of element\nallocation for rate maximization. With static AIRS beamforming, we solve the\nrate maximization problem by optimizing the BS transmit/receive beamformers,\nuser beamforming, and AIRS beamforming. Despite its non-convexity, we develop\nan efficient alternating optimization (AO) based algorithm that solves each\nsub-problem optimally. Numerical results validate the practical advantages of\ndistributed AIRSs compared to passive IRS (PIRS), BS-side AIRS, and user-side\nAIRS, and highlight the benefits of dynamic IRS beamforming."}
{"id": "2510.10235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10235", "abs": "https://arxiv.org/abs/2510.10235", "authors": ["Jinpeng Xu", "Shuowen Zhang"], "title": "MIMO Radar Meets Polarization-Reconfigurable Antennas: A BCRB Perspective", "comment": "To appear in Proc. IEEE Global Communications Conference (Globecom)\n  Workshops, 2025", "summary": "In this paper, we investigate a novel multiple-input multiple-output (MIMO)\nradar system aided by phase shifter based polarization-reconfigurable antennas\n(PRAs). Specifically, a base station (BS) equipped with multiple PRAs at both\nthe transmitter and the receiver aims to sense the unknown and random angular\nlocation parameter of a point target via sending wireless signals and\nprocessing the received echo signals reflected by the target, where only prior\ndistribution information about the location parameter is available for\nexploitation. Firstly, we characterize the sensing performance of this novel\nPRA-based MIMO radar system by deriving the Bayesian Cram\\'er-Rao bound (BCRB)\nof the mean-squared error (MSE) in estimating the desired location parameter\nwith prior distribution information. Then, to fully exploit the new design\ndegrees-of-freedom (DoF) empowered by PRAs, we study the joint optimization of\nthe transmit sample covariance matrix as well as the transmit and receive phase\nshift vectors to minimize the sensing BCRB subject to a transmit power\nconstraint. This problem is non-convex and difficult to solve due to the\ncoupling among optimization variables. To resolve this issue, we develop an\nalternating optimization (AO) based algorithm which iteratively obtains the\nclosed-form optimal solution to each variable with the others being fixed at\neach time, thus being guaranteed to converge to at least a stationary point of\nthe joint optimization problem. Numerical results validate the effectiveness of\nthe proposed algorithm."}
{"id": "2510.09974", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.09974", "abs": "https://arxiv.org/abs/2510.09974", "authors": ["Fei Liu", "Yang Ai", "Ye-Xin Lu", "Rui-Chen Zheng", "Hui-Peng Du", "Zhen-Hua Ling"], "title": "Universal Discrete-Domain Speech Enhancement", "comment": null, "summary": "In real-world scenarios, speech signals are inevitably corrupted by various\ntypes of interference, making speech enhancement (SE) a critical task for\nrobust speech processing. However, most existing SE methods only handle a\nlimited range of distortions, such as additive noise, reverberation, or band\nlimitation, while the study of SE under multiple simultaneous distortions\nremains limited. This gap affects the generalization and practical usability of\nSE methods in real-world environments.To address this gap, this paper proposes\na novel Universal Discrete-domain SE model called UDSE.Unlike regression-based\nSE models that directly predict clean speech waveform or continuous features,\nUDSE redefines SE as a discrete-domain classification task, instead predicting\nthe clean discrete tokens quantized by the residual vector quantizer (RVQ) of a\npre-trained neural speech codec.Specifically, UDSE first extracts global\nfeatures from the degraded speech. Guided by these global features, the clean\ntoken prediction for each VQ follows the rules of RVQ, where the prediction of\neach VQ relies on the results of the preceding ones. Finally, the predicted\nclean tokens from all VQs are decoded to reconstruct the clean speech waveform.\nDuring training, the UDSE model employs a teacher-forcing strategy, and is\noptimized with cross-entropy loss. Experimental results confirm that the\nproposed UDSE model can effectively enhance speech degraded by various\nconventional and unconventional distortions, e.g., additive noise,\nreverberation, band limitation, clipping, phase distortion, and compression\ndistortion, as well as their combinations. These results demonstrate the\nsuperior universality and practicality of UDSE compared to advanced\nregression-based SE methods."}
{"id": "2510.10438", "categories": ["eess.SP", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.10438", "abs": "https://arxiv.org/abs/2510.10438", "authors": ["Shuixin Li", "Jiecheng Chen", "Qingtang Jiang", "Jian Lu"], "title": "Synchrosqueezed windowed linear canonical transform: A method for mode retrieval from multicomponent signals with crossing instantaneous frequencies", "comment": null, "summary": "In nature, signals often appear in the form of the superposition of multiple\nnon-stationary signals. The overlap of signal components in the time-frequency\ndomain poses a significant challenge for signal analysis. One approach to\naddressing this problem is to introduce an additional chirprate parameter and\nuse the chirplet transform (CT) to elevate the two-dimensional time-frequency\nrepresentation to a three-dimensional time-frequency-chirprate representation.\nFrom a certain point of view, the CT of a signal can be regarded as a windowed\nspecial linear canonical transform of that signal, undergoing a shift and a\nmodulation.\n  In this paper, we develop this idea to propose a novel windowed linear\ncanonical transform (WLCT), which provides a new time-frequency-chirprate\nrepresentation. We discuss four types of WLCTs. In addition, we use a special\nX-ray transform to further sharpen the time-frequency-chirprate representation.\nFurthermore, we derive the corresponding three-dimensional synchrosqueezed\ntransform, demonstrating that the WLCTs have great potential for\nthree-dimensional signal separation."}
{"id": "2510.10078", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10078", "abs": "https://arxiv.org/abs/2510.10078", "authors": ["Chung-Soo Ahn", "Rajib Rana", "Sunil Sivadas", "Carlos Busso", "Jagath C. Rajapakse"], "title": "Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model", "comment": null, "summary": "Although speech emotion recognition (SER) research has been advanced, thanks\nto deep learning methods, it still suffers from obtaining inputs from large\nquality-labelled training data. Data augmentation methods have been attempted\nto mitigate this issue, generative models have shown success among them\nrecently. We propose a data augmentation framework that is aided by cross-modal\ninformation transfer and mutual information regularization. Mutual information\nbased metric can serve as an indicator for the quality. Furthermore, we expand\nthis data augmentation scope to multimodal inputs, thanks to mutual information\nensureing dependency between modalities. Our framework was tested on three\nbenchmark datasets: IEMOCAP, MSP-IMPROV and MSP-Podcast. The implementation was\ndesigned to generate input features that are fed into last layer for emotion\nclassification. Our framework improved the performance of emotion prediction\nagainst existing works. Also, we discovered that our framework is able to\ngenerate new inputs without any cross-modal information."}
{"id": "2510.10473", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10473", "abs": "https://arxiv.org/abs/2510.10473", "authors": ["Huizhi Wang", "Tierui Gong", "Emil Björnson", "Chau Yuen"], "title": "Multi-Carrier Rydberg Atomic Quantum Receivers with Enhanced Bandwidth Feature for Communication and Sensing", "comment": null, "summary": "Rydberg atomic quantum receivers (RAQRs) have attracted significant attention\nin recent years due to their ultra-high sensitivity. Although capable of\nprecisely detecting the amplitude and phase of weak signals, conventional RAQRs\nface inherent limitations in accurately receiving wideband RF signals, due to\nthe discrete nature of atomic energy levels and their intrinsic instantaneous\nbandwidth constraints. These limitations hinder their direct application to\nmulti-carrier communication and sensing. To address this issue, this paper\nproposes a multi-carrier Rydberg atomic quantum receiver (MC-RAQR) structure\nwith five energy levels. We derive the amplitude and phase of the MC-RAQR and\nextract the baseband electrical signal for signal processing. In terms of\nmulti-carrier communication and sensing, we analyze the channel capacity and\naccuracy of angle of arrival (AoA) and distance parameters, respectively.\nNumerical results validate our proposed model, showing that the MC-RAQR can\nachieve up to a bandwidth of 14 MHz, which is 56-fold larger than the\nconventional RAQRs. As a result, the channel capacity and the resolution for\nmulti-target sensing are improved significantly. Specifically, the channel\ncapacity of MC-RAQR is 22-fold and 3-fold larger than the conventional antennas\nand RAQRs, respectively. For sensing performance, the MSE of AoA estimation for\nMC-RAQR is 0.16% of the conventional RAQR and the MSE of distance estimation is\n0.01% of the CRB of conventional antennas, showing the superior performance of\nthe MC-RAQR. This demonstrates its compatibility with waveforms such as\northogonal frequency-division multiplexing (OFDM) and its significant\nadvantages for multi-carrier signal reception."}
{"id": "2510.10087", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.10087", "abs": "https://arxiv.org/abs/2510.10087", "authors": ["Jiyun Park", "Carlos Cancino-Chacón", "Suhit Chiruthapudi", "Juhan Nam"], "title": "Matchmaker: An Open-source Library for Real-time Piano Score Following and Systematic Evaluation", "comment": "In Proceedings of the 26th International Society for Music\n  Information Retrieval Conference (ISMIR), 2025", "summary": "Real-time music alignment, also known as score following, is a fundamental\nMIR task with a long history and is essential for many interactive\napplications. Despite its importance, there has not been a unified open\nframework for comparing models, largely due to the inherent complexity of\nreal-time processing and the language- or system-dependent implementations. In\naddition, low compatibility with the existing MIR environment has made it\ndifficult to develop benchmarks using large datasets available in recent years.\nWhile new studies based on established methods (e.g., dynamic programming,\nprobabilistic models) have emerged, most evaluations compare models only within\nthe same family or on small sets of test data. This paper introduces\nMatchmaker, an open-source Python library for real-time music alignment that is\neasy to use and compatible with modern MIR libraries. Using this, we\nsystematically compare methods along two dimensions: music representations and\nalignment methods. We evaluated our approach on a large test set of solo piano\nmusic from the (n)ASAP, Batik, and Vienna4x22 datasets with a comprehensive set\nof metrics to ensure robust assessment. Our work aims to establish a benchmark\nframework for score-following research while providing a practical tool that\ndevelopers can easily integrate into their applications."}
{"id": "2510.10883", "categories": ["eess.AS", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.10883", "abs": "https://arxiv.org/abs/2510.10883", "authors": ["Ali Fallah", "Shun Nakamura", "Steven van de Par"], "title": "Perceptual Compensation of Ambisonics Recordings for Reproduction in Room", "comment": "The manuscript was submitted to the JASA and is under review", "summary": "Ambisonics is a method for capturing and rendering a sound field accurately,\nassuming that the acoustics of the playback room does not significantly\ninfluence the sound field. However, in practice, the acoustics of the playback\nroom may lead to a noticeable degradation in sound quality. We propose a\nrecording and rendering method based on Ambisonics that utilizes a\nperceptually-motivated approach to compensate for the reverberation of the\nplayback room. The recorded direct and reverberant sound field components in\nthe spherical harmonics (SHs) domain are spectrally and spatially compensated\nto preserve the relevant auditory cues including the direction of arrival of\nthe direct sound, the spectral energy of the direct and reverberant sound\ncomponents, and the Interaural Coherence (IC) across each auditory band. In\ncontrast to the conventional Ambisonics, a flexible number of Ambisonics\nchannels can be used for audio rendering. Listening test results show that the\nproposed method provides a perceptually accurate rendering of the originally\nrecorded sound field, outperforming both conventional Ambisonics without\ncompensation and even ideal Ambisonics rendering in a simulated anechoic room.\nAdditionally, subjective evaluations of listeners seated at the center of the\nloudspeaker array demonstrate that the method remains robust to head rotation\nand minor displacements."}
{"id": "2510.10512", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10512", "abs": "https://arxiv.org/abs/2510.10512", "authors": ["Xiaopeng Cheng", "Zhichao Zhang"], "title": "Graph Signal Wiener Filtering in the Linear Canonical Domain: Theory and Method Design", "comment": null, "summary": "The graph linear canonical transform (GLCT)-based filtering methods often\noptimize transform parameters and filters separately, which results in high\ncomputational costs and limited stability. To address this issue, this paper\nproposes a trainable joint optimization framework that combines GLCT parameters\nand Wiener filtering into an end-to-end learning process, allowing for\nsynergistic optimization between transform domain construction and filtering\noperations. The proposed method not only eliminates the cumbersome grid search\nrequired by traditional strategies but also significantly enhances the\nflexibility and training stability of the filtering system. Experimental\nresults on real-world graph data show the proposed method outperforms existing\nmethods in denoising tasks, featuring superior denoising performance, higher\nrobustness and lower computational complexity."}
{"id": "2510.10175", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10175", "abs": "https://arxiv.org/abs/2510.10175", "authors": ["Xian He", "Wei Zeng", "Ye Wang"], "title": "Peransformer: Improving Low-informed Expressive Performance Rendering with Score-aware Discriminator", "comment": "6 pages, 3 figures, accepted by APSIPA ASC 2025", "summary": "Highly-informed Expressive Performance Rendering (EPR) systems transform\nmusic scores with rich musical annotations into human-like expressive\nperformance MIDI files. While these systems have achieved promising results,\nthe availability of detailed music scores is limited compared to MIDI files and\nare less flexible to work with using a digital audio workstation (DAW). Recent\nadvancements in low-informed EPR systems offer a more accessible alternative by\ndirectly utilizing score-derived MIDI as input, but these systems often exhibit\nsuboptimal performance. Meanwhile, existing works are evaluated with diverse\nautomatic metrics and data formats, hindering direct objective comparisons\nbetween EPR systems. In this study, we introduce Peransformer, a\ntransformer-based low-informed EPR system designed to bridge the gap between\nlow-informed and highly-informed EPR systems. Our approach incorporates a\nscore-aware discriminator that leverages the underlying score-derived MIDI\nfiles and is trained on a score-to-performance paired, note-to-note aligned\nMIDI dataset. Experimental results demonstrate that Peransformer achieves\nstate-of-the-art performance among low-informed systems, as validated by\nsubjective evaluations. Furthermore, we extend existing automatic evaluation\nmetrics for EPR systems and introduce generalized EPR metrics (GEM), enabling\nmore direct, accurate, and reliable comparisons across EPR systems."}
{"id": "2510.11366", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11366", "abs": "https://arxiv.org/abs/2510.11366", "authors": ["Ruben Johnson Robert Jeremiah", "Peyman Goli", "Steven van de Par"], "title": "Phase Aware Ear-Conditioned Learning for Multi-Channel Binaural Speaker Separation", "comment": null, "summary": "Separating competing speech in reverberant environments requires models that\npreserve spatial cues while maintaining separation efficiency. We present a\nPhase-aware Ear-conditioned speaker Separation network using eight microphones\n(PEASE-8) that consumes complex STFTs and directly introduces a raw-STFT input\nto the early decoder layer, bypassing the entire encoder pathway to improve\nreconstruction. The model is trained end-to-end with an SI-SDR-based objective\nagainst direct-path ear targets, jointly performing separation and\ndereverberation for two speakers in a fixed azimuth, eliminating the need for\npermutation invariant training. On spatialized two-speaker mixtures spanning\nanechoic, reverberant, and noisy conditions, PEASE-8 delivers strong separation\nand intelligibility. In reverberant environments, it achieves 12.37 dB SI-SDR,\n0.87 STOI, and 1.86 PESQ at T60 = 0.6 s, while remaining competitive under\nanechoic conditions."}
{"id": "2510.10532", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10532", "abs": "https://arxiv.org/abs/2510.10532", "authors": ["Guoyun Xie", "Zhichao Zhang"], "title": "SVD-based ugmt-gft on directed product graphs", "comment": null, "summary": "Traditional directed graph signal processing generally depends on fixed\nrepresentation matrices, whose rigid structures limit the model's ability to\nadapt to complex graph topologies. To address this issue, this study employed\nthe unified graph representation matrix (UGRM) to propose a generalized graph\nFourier transform (UGRM-GFT) method based on singular value decomposition (SVD)\nfor signal analysis on directed graphs and Cartesian product graphs. We defined\nUGRM-GFT for general directed graphs by introducing a parameterized UGRM that\nincorporates traditional representations such as the Laplacian matrix and\nadjacency matrix. The SVD is used to construct spectral transform pairs with\nboth left and right singular vectors. We extended this approach to two types of\nUGRM-GFTs applied to directed Cartesian product graphs. UGRM-GFT-I performs SVD\ndirectly on the composite UGRM matrix of the two-dimensional graph structure,\nsuitable for globally coupled graph signals. UGRM-GFT-II separately applies SVD\nto the UGRMs of the two-factor graphs and then combines the results,\nsignificantly reducing computational complexity while preserving spectral\nexpressiveness. Theoretical analysis confirmed the monotonicity of the proposed\nmethod with respect to the parameters alpha and k embedded in the UGRM.\nExperimental results on real-world datasets demonstrated that the proposed\nmethod significantly outperforms traditional fixed-matrix approaches in\ndenoising tasks, with a particular emphasis on signal-to-noise ratio and\nbandwidth efficiency."}
{"id": "2510.10249", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10249", "abs": "https://arxiv.org/abs/2510.10249", "authors": ["Stephen Ni-Hahn", "Chao Péter Yang", "Mingchen Ma", "Cynthia Rudin", "Simon Mak", "Yue Jiang"], "title": "ProGress: Structured Music Generation via Graph Diffusion and Hierarchical Music Analysis", "comment": null, "summary": "Artificial Intelligence (AI) for music generation is undergoing rapid\ndevelopments, with recent symbolic models leveraging sophisticated deep\nlearning and diffusion model algorithms. One drawback with existing models is\nthat they lack structural cohesion, particularly on harmonic-melodic structure.\nFurthermore, such existing models are largely \"black-box\" in nature and are not\nmusically interpretable. This paper addresses these limitations via a novel\ngenerative music framework that incorporates concepts of Schenkerian analysis\n(SchA) in concert with a diffusion modeling framework. This framework, which we\ncall ProGress (Prolongation-enhanced DiGress), adapts state-of-the-art deep\nmodels for discrete diffusion (in particular, the DiGress model of Vignac et\nal., 2023) for interpretable and structured music generation. Concretely, our\ncontributions include 1) novel adaptations of the DiGress model for music\ngeneration, 2) a novel SchA-inspired phrase fusion methodology, and 3) a\nframework allowing users to control various aspects of the generation process\nto create coherent musical compositions. Results from human experiments suggest\nsuperior performance to existing state-of-the-art methods."}
{"id": "2510.11395", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11395", "abs": "https://arxiv.org/abs/2510.11395", "authors": ["Haixin Zhao", "Kaixuan Yang", "Nilesh Madhu"], "title": "Dynamically Slimmable Speech Enhancement Network with Metric-Guided Training", "comment": "Preprint version of a paper under review at ICASSP2026", "summary": "To further reduce the complexity of lightweight speech enhancement models, we\nintroduce a gating-based Dynamically Slimmable Network (DSN). The DSN comprises\nstatic and dynamic components. For architecture-independent applicability, we\nintroduce distinct dynamic structures targeting the commonly used components,\nnamely, grouped recurrent neural network units, multi-head attention,\nconvolutional, and fully connected layers. A policy module adaptively governs\nthe use of dynamic parts at a frame-wise resolution according to the input\nsignal quality, controlling computational load. We further propose\nMetric-Guided Training (MGT) to explicitly guide the policy module in assessing\ninput speech quality. Experimental results demonstrate that the DSN achieves\ncomparable enhancement performance in instrumental metrics to the\nstate-of-the-art lightweight baseline, while using only 73% of its\ncomputational load on average. Evaluations of dynamic component usage ratios\nindicate that the MGT-DSN can appropriately allocate network resources\naccording to the severity of input signal distortion."}
{"id": "2510.10542", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10542", "abs": "https://arxiv.org/abs/2510.10542", "authors": ["Kimitaka Sumi", "Takuya Sakamoto"], "title": "Data Integration Using Multivariate Mode Decomposition for Physiological Sensing with Multiple Millimeter-Wave Radar Systems", "comment": "7 pages, 4 figures, 5 tables. This work is going to be submitted to\n  the IEEE for possible publication", "summary": "This study proposes a multi-radar system for non-contact physiological\nsensing across arbitrary body orientations. In integrating signals obtained\nfrom different radar viewpoints, we adopt a multivariate variational mode\ndecomposition method to extract the common respiratory component. Experiments\nconducted with six subjects under varying distances and orientations\ndemonstrate that, compared with a single-radar setup, the proposed system\nreduced the root mean square error of the respiratory interval by 35.5%,\ndecreased the mean absolute error of the respiratory rate by 30.8%, and\nimproved accuracy by 9.4 percentage points. These results highlight that\ncombining multiple radar viewpoints with signal integration enables stable\nrespiratory measurement regardless of body orientation."}
{"id": "2510.10396", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.10396", "abs": "https://arxiv.org/abs/2510.10396", "authors": ["Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Xintong Hu", "Yu Zhang", "Li Tang", "Rui Yang", "Han Wang", "Zongbao Zhang", "Yuhan Wang", "Yixuan Chen", "Hankun Xu", "Ke Xu", "Pengfei Fan", "Zhetao Chen", "Yanhao Yu", "Qiange Huang", "Fei Wu", "Zhou Zhao"], "title": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations", "comment": "24 pages", "summary": "Humans rely on multisensory integration to perceive spatial environments,\nwhere auditory cues enable sound source localization in three-dimensional\nspace. Despite the critical role of spatial audio in immersive technologies\nsuch as VR/AR, most existing multimodal datasets provide only monaural audio,\nwhich limits the development of spatial audio generation and understanding. To\naddress these challenges, we introduce MRSAudio, a large-scale multimodal\nspatial audio dataset designed to advance research in spatial audio\nunderstanding and generation. MRSAudio spans four distinct components: MRSLife,\nMRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios. The\ndataset includes synchronized binaural and ambisonic audio, exocentric and\negocentric video, motion trajectories, and fine-grained annotations such as\ntranscripts, phoneme boundaries, lyrics, scores, and prompts. To demonstrate\nthe utility and versatility of MRSAudio, we establish five foundational tasks:\naudio spatialization, and spatial text to speech, spatial singing voice\nsynthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a\nbroad range of spatial audio research. Demos and dataset access are available\nat https://mrsaudio.github.io."}
{"id": "2510.11458", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11458", "abs": "https://arxiv.org/abs/2510.11458", "authors": ["Soubhagya Ranjan Hota", "Arka Roy", "Udit Satija"], "title": "ILD-VIT: A Unified Vision Transformer Architecture for Detection of Interstitial Lung Disease from Respiratory Sounds", "comment": null, "summary": "Interstitial lung disease (ILD) represents a group of restrictive chronic\npulmonary diseases that impair oxygen acquisition by causing irreversible\nchanges in the lungs such as fibrosis, scarring of parenchyma, etc. ILD\nconditions are often diagnosed by various clinical modalities such as\nspirometry, high-resolution lung imaging techniques, crackling respiratory\nsounds (RSs), etc. In this letter, we develop a novel vision transformer\n(VIT)-based deep learning framework namely, ILD-VIT, to detect the ILD\ncondition using the RS recordings. The proposed framework comprises three major\nstages: pre-processing, mel spectrogram extraction, and classification using\nthe proposed VIT architecture using the mel spectrogram image patches.\nExperimental results using the publicly available BRACETS and KAUH databases\nshow that our proposed ILD-VIT achieves an accuracy, sensitivity, and\nspecificity of 84.86%, 82.67%, and 86.91%, respectively, for\nsubject-independent blind testing. The successful onboard implantation of the\nproposed framework on a Raspberry-pi-4 microcontroller indicates its potential\nas a standalone clinical system for ILD screening in a real clinical scenario."}
{"id": "2510.10561", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10561", "abs": "https://arxiv.org/abs/2510.10561", "authors": ["Zhixiong Chen", "Hyundong Shin", "Arumugam Nallanathan", "Jonathon Chambers"], "title": "Large Language Model-Empowered Channel Prediction and Predictive Beamforming for LEO Satellite Communications", "comment": "14 pages, 13 figures", "summary": "Accurate channel prediction and effective beamforming are essential for low\nEarth orbit (LEO) satellite communications to enhance system capacity and\nenable high-speed connectivity. Most existing channel prediction and predictive\nbeamforming methods are limited by model generalization capabilities and\nstruggle to adapt to time-varying wireless propagation environments. Inspired\nby the remarkable generalization and reasoning capabilities of large language\nmodels (LLMs), this work proposes an LLM-based channel prediction framework,\nnamely CPLLM, to forecast future channel state information (CSI) for LEO\nsatellites based on historical CSI data. In the proposed CPLLM, a dedicated CSI\nencoder is designed to map raw CSI data into the textual embedding space,\neffectively bridging the modality gap and enabling the LLM to perform reliable\nreasoning over CSI data. Additionally, a CSI decoder is introduced to\nsimultaneously predict CSI for multiple future time slots, substantially\nreducing the computational burden and inference latency associated with the\ninherent autoregressive decoding process of LLMs. Then, instead of training the\nLLM from scratch, we adopt a parameter-efficient fine-tuning strategy, i.e.,\nLoRA, for CPLLM, where the pretrained LLM remains frozen and trainable low-rank\nmatrices are injected into each Transformer decoder layer to enable effective\nfine-tuning. Furthermore, we extend CPLLM to directly generate beamforming\nstrategies for future time slots based on historical CSI data, namely BFLLM.\nThis extended framework retains the same architecture as CPLLM, while\nintroducing a dedicated beamforming decoder to output beamforming strategies.\nFinally, extensive simulation results validate the effectiveness of the\nproposed approaches in channel prediction and predictive beamforming for LEO\nsatellite communications."}
{"id": "2510.10401", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.10401", "abs": "https://arxiv.org/abs/2510.10401", "authors": ["Yue Gu", "Zhihao Du", "Ying Shi", "Jiqing Han", "Yongjun He"], "title": "Knowledge-Decoupled Functionally Invariant Path with Synthetic Personal Data for Personalized ASR", "comment": "Accepted for publication in IEEE Signal Processing Letters, 2025", "summary": "Fine-tuning generic ASR models with large-scale synthetic personal data can\nenhance the personalization of ASR models, but it introduces challenges in\nadapting to synthetic personal data without forgetting real knowledge, and in\nadapting to personal data without forgetting generic knowledge. Considering\nthat the functionally invariant path (FIP) framework enables model adaptation\nwhile preserving prior knowledge, in this letter, we introduce FIP into\nsynthetic-data-augmented personalized ASR models. However, the model still\nstruggles to balance the learning of synthetic, personalized, and generic\nknowledge when applying FIP to train the model on all three types of data\nsimultaneously. To decouple this learning process and further address the above\ntwo challenges, we integrate a gated parameter-isolation strategy into FIP and\npropose a knowledge-decoupled functionally invariant path (KDFIP) framework,\nwhich stores generic and personalized knowledge in separate modules and applies\nFIP to them sequentially. Specifically, KDFIP adapts the personalized module to\nsynthetic and real personal data and the generic module to generic data. Both\nmodules are updated along personalization-invariant paths, and their outputs\nare dynamically fused through a gating mechanism. With augmented synthetic\ndata, KDFIP achieves a 29.38% relative character error rate reduction on target\nspeakers and maintains comparable generalization performance to the unadapted\nASR baseline."}
{"id": "2510.10175", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10175", "abs": "https://arxiv.org/abs/2510.10175", "authors": ["Xian He", "Wei Zeng", "Ye Wang"], "title": "Peransformer: Improving Low-informed Expressive Performance Rendering with Score-aware Discriminator", "comment": "6 pages, 3 figures, accepted by APSIPA ASC 2025", "summary": "Highly-informed Expressive Performance Rendering (EPR) systems transform\nmusic scores with rich musical annotations into human-like expressive\nperformance MIDI files. While these systems have achieved promising results,\nthe availability of detailed music scores is limited compared to MIDI files and\nare less flexible to work with using a digital audio workstation (DAW). Recent\nadvancements in low-informed EPR systems offer a more accessible alternative by\ndirectly utilizing score-derived MIDI as input, but these systems often exhibit\nsuboptimal performance. Meanwhile, existing works are evaluated with diverse\nautomatic metrics and data formats, hindering direct objective comparisons\nbetween EPR systems. In this study, we introduce Peransformer, a\ntransformer-based low-informed EPR system designed to bridge the gap between\nlow-informed and highly-informed EPR systems. Our approach incorporates a\nscore-aware discriminator that leverages the underlying score-derived MIDI\nfiles and is trained on a score-to-performance paired, note-to-note aligned\nMIDI dataset. Experimental results demonstrate that Peransformer achieves\nstate-of-the-art performance among low-informed systems, as validated by\nsubjective evaluations. Furthermore, we extend existing automatic evaluation\nmetrics for EPR systems and introduce generalized EPR metrics (GEM), enabling\nmore direct, accurate, and reliable comparisons across EPR systems."}
{"id": "2510.10563", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10563", "abs": "https://arxiv.org/abs/2510.10563", "authors": ["Xuyang Zhao", "Jiangtao Wang", "Xinyu Zhang"], "title": "Covert Waveform Design for Integrated Sensing and Communication System in Clutter Environment", "comment": null, "summary": "This paper proposes an integrated sensing and communication (ISAC) system\ncovert waveform design method for complex clutter environments, with the core\nobjective of maximizing the signal-to-clutter-plus-noise ratio (SCNR). The\ndesign achieves efficient clutter suppression while meeting the covertness\nrequirement through joint optimization of the transmit waveform and receive\nfilter, enabling cooperative radar detection and wireless communication. This\nstudy presents key innovations that explicitly address target Doppler shift\nuncertainty, significantly enhancing system robustness against Doppler effects.\nTo ensure communication reliability, the method incorporates phase difference\nconstraints between communication signal elements in the waveform design, along\nwith energy constraint, covert constraint, and peak-to-average power ratio\n(PAPR) constraint. The original non-convex optimization problem is transformed\ninto a tractable convex optimization form through convex optimization\ntechnique. Simulation results demonstrate that the optimized waveform not only\nsatisfies the covertness requirement in complex clutter environment, but also\nachieves superior target detection performance. It also ensures reliable\ncommunication and confirms the effectiveness of propose method."}
{"id": "2510.10509", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10509", "abs": "https://arxiv.org/abs/2510.10509", "authors": ["Zihan Zhang", "Xize Cheng", "Zhennan Jiang", "Dongjie Fu", "Jingyuan Chen", "Zhou Zhao", "Tao Jin"], "title": "MARS-Sep: Multimodal-Aligned Reinforced Sound Separation", "comment": null, "summary": "Universal sound separation faces a fundamental misalignment: models optimized\nfor low-level signal metrics often produce semantically contaminated outputs,\nfailing to suppress perceptually salient interference from acoustically similar\nsources. To bridge this gap, we introduce MARS-Sep, a reinforcement learning\nframework that reformulates separation as decision making. Instead of simply\nregressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy\nthat is optimized by a clipped trust-region surrogate with entropy\nregularization and group-relative advantage normalization. Concretely, we\nsample masks from a frozen old policy, reconstruct waveforms, and update the\ncurrent policy using clipped importance ratios-yielding substantially more\nstable and sample-efficient learning. Multimodal rewards, derived from an\naudio-text-vision encoder, directly incentivize semantic consistency with query\nprompts. We further propose a progressive alignment scheme to fine-tune this\nencoder, boosting its cross-modal discriminability and improving reward\nfaithfulness. Extensive experiments on multiple benchmarks demonstrate\nconsistent gains in Text-, Audio-, and Image-Queried separation, with notable\nimprovements in signal metrics and semantic quality. Our code is available at\nhttps://anonymous.4open.science/r/MARS-Sep. Sound separation samples are\navailable at https://mars-sep.github.io/."}
{"id": "2510.10249", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10249", "abs": "https://arxiv.org/abs/2510.10249", "authors": ["Stephen Ni-Hahn", "Chao Péter Yang", "Mingchen Ma", "Cynthia Rudin", "Simon Mak", "Yue Jiang"], "title": "ProGress: Structured Music Generation via Graph Diffusion and Hierarchical Music Analysis", "comment": null, "summary": "Artificial Intelligence (AI) for music generation is undergoing rapid\ndevelopments, with recent symbolic models leveraging sophisticated deep\nlearning and diffusion model algorithms. One drawback with existing models is\nthat they lack structural cohesion, particularly on harmonic-melodic structure.\nFurthermore, such existing models are largely \"black-box\" in nature and are not\nmusically interpretable. This paper addresses these limitations via a novel\ngenerative music framework that incorporates concepts of Schenkerian analysis\n(SchA) in concert with a diffusion modeling framework. This framework, which we\ncall ProGress (Prolongation-enhanced DiGress), adapts state-of-the-art deep\nmodels for discrete diffusion (in particular, the DiGress model of Vignac et\nal., 2023) for interpretable and structured music generation. Concretely, our\ncontributions include 1) novel adaptations of the DiGress model for music\ngeneration, 2) a novel SchA-inspired phrase fusion methodology, and 3) a\nframework allowing users to control various aspects of the generation process\nto create coherent musical compositions. Results from human experiments suggest\nsuperior performance to existing state-of-the-art methods."}
{"id": "2510.10647", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10647", "abs": "https://arxiv.org/abs/2510.10647", "authors": ["Emanuele Peschiera", "Sangbu Yun", "Youngjoo Lee", "Liesbet Van der Perre", "François Rottenberg"], "title": "A Parametric Power Model of Upper Mid-Band (FR3) Base Stations for 6G", "comment": null, "summary": "Increasing attention is given to the upper mid-band or Frequency Range 3\n(FR3), from 7 to 24 GHz, in the research towards sixth-generation (6G)\nnetworks. Promises of offering large data rates at favorable propagation\nconditions are leading to novel FR3 base station (BS) architectures, with up to\nthousands of antenna elements and radio-frequency (RF) chains. This work\ninvestigates the power consumption of prospective FR3 BSs and its relation to\nthe delivered data rates. We model the power consumed by digital and analog\nsignal processing, power amplifiers (PAs), and supply and cooling during four\nphases (data, signaling, micro-sleep, and idle) in downlink and uplink. Hybrid\npartially-connected beamforming is compared to fully-digital one. Results show\nthat, for BS arrays with $1024$ antennas at $30\\%$ of load, the PA consumes\nmost of the power when $64$ or less RF chains are utilized, while the digital\nand analog processing consumption takes over when the number of RF chains is\n$512$ or more. The digital plus analog processing consumes $2\\times$ to\n$4\\times$ more than the PA for fully-digital beamforming. Hybrid beamforming\nachieves $1.3$ Gbit/s/user in downlink while improving the energy efficiency by\n$1.4\\times$ compared to fully-digital beamforming."}
{"id": "2510.10619", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10619", "abs": "https://arxiv.org/abs/2510.10619", "authors": ["Maximos Kaliakatsos-Papakostas", "Gregoris Bastas", "Dimos Makris", "Dorien Herremans", "Vassilis Katsouros", "Petros Maragos"], "title": "A Machine Learning Approach for MIDI to Guitar Tablature Conversion", "comment": "Proceedings of the 19th Sound and Music Computing Conference, June\n  5-12th, 2022, Saint-\\'Etienne (France)", "summary": "Guitar tablature transcription consists in deducing the string and the fret\nnumber on which each note should be played to reproduce the actual musical\npart. This assignment should lead to playable string-fret combinations\nthroughout the entire track and, in general, preserve parsimonious motion\nbetween successive combinations. Throughout the history of guitar playing,\nspecific chord fingerings have been developed across different musical styles\nthat facilitate common idiomatic voicing combinations and motion between them.\nThis paper presents a method for assigning guitar tablature notation to a given\nMIDI-based musical part (possibly consisting of multiple polyphonic tracks),\ni.e. no information about guitar-idiomatic expressional characteristics is\ninvolved (e.g. bending etc.) The current strategy is based on machine learning\nand requires a basic assumption about how much fingers can stretch on a\nfretboard; only standard 6-string guitar tuning is examined. The proposed\nmethod also examines the transcription of music pieces that was not meant to be\nplayed or could not possibly be played by a guitar (e.g. potentially a\nsymphonic orchestra part), employing a rudimentary method for augmenting\nmusical information and training/testing the system with artificial data. The\nresults present interesting aspects about what the system can achieve when\ntrained on the initial and augmented dataset, showing that the training with\naugmented data improves the performance even in simple, e.g. monophonic, cases.\nResults also indicate weaknesses and lead to useful conclusions about possible\nimprovements."}
{"id": "2510.10948", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10948", "abs": "https://arxiv.org/abs/2510.10948", "authors": ["Xuyao Deng", "Yanjie Sun", "Yong Dou", "Kele Xu"], "title": "Unify Variables in Neural Scaling Laws for General Audio Representations via Embedding Effective Rank", "comment": null, "summary": "Scaling laws have profoundly shaped our understanding of model performance in\ncomputer vision and natural language processing, yet their application to\ngeneral audio representation learning remains underexplored. A key challenge\nlies in the multifactorial nature of general audio\nrepresentation-representation quality is jointly influenced by variables such\nas audio length, embedding dimensionality, model depth, model architecture,\ndata volume, etc., many of which are difficult to isolate or express\nanalytically. In this work, we present a systematic study of scaling laws for\ngeneral audio representations by utilizing embedding effective rank (RankMe) as\na unifying metric that encapsulates the impact of diverse variables on\nrepresentation quality. RankMe enables a label-free, information-theoretic\nquantification of audio embeddings, allowing us to examine scaling behaviors\nacross a wide hyper-parameter space, including model size, training data\nvolume, computational budget, architectural configurations, etc. Our empirical\nfindings reveal a consistent power-law relationship between RankMe and\nrepresentation quality, suggesting that embedding effective rank serves as a\nreliable proxy for assessing and predicting model performance in audio\nrepresentation learning. This work not only validates the applicability of\nclassical scaling principles to the general audio domain but also offers a\ntheoretically grounded and empirically robust framework for guiding future\nmodel scaling strategies in audio foundation models."}
{"id": "2510.10718", "categories": ["eess.SP", "cs.AI", "cs.AR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.10718", "abs": "https://arxiv.org/abs/2510.10718", "authors": ["Rajat Bhattacharjya", "Woohyeok Park", "Arnab Sarkar", "Hyunwoo Oh", "Mohsen Imani", "Nikil Dutt"], "title": "HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing", "comment": "3 figures, 5 pages. Authors' version posted for personal use and not\n  for redistribution", "summary": "Direction of Arrival (DoA) estimation techniques face a critical trade-off,\nas classical methods often lack accuracy in challenging, low signal-to-noise\nratio (SNR) conditions, while modern deep learning approaches are too\nenergy-intensive and opaque for resource-constrained, safety-critical systems.\nWe introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing\n(HDC). The framework introduces two distinct feature extraction strategies --\nMean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline,\nand then reframes DoA estimation as a pattern recognition problem. This\napproach leverages HDC's inherent robustness to noise and its transparent\nalgebraic operations to bypass the expensive matrix decompositions and\n``black-box'' nature of classical and deep learning methods, respectively. Our\nevaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than\nstate-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it\nalso consumes ~93% less energy than competing neural baselines on an embedded\nNVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and\nefficiency establishes HYPERDOA as a robust and viable solution for\nmission-critical applications on edge devices."}
{"id": "2510.10687", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10687", "abs": "https://arxiv.org/abs/2510.10687", "authors": ["Jun Chen", "Shichao Hu", "Jiuxin Lin", "Wenjie Li", "Zihan Zhang", "Xingchen Li", "JinJiang Liu", "Longshuai Xiao", "Chao Weng", "Lei Xie", "Zhiyong Wu"], "title": "LSZone: A Lightweight Spatial Information Modeling Architecture for Real-time In-car Multi-zone Speech Separation", "comment": "submitted to ICASSP 2026", "summary": "In-car multi-zone speech separation, which captures voices from different\nspeech zones, plays a crucial role in human-vehicle interaction. Although\nprevious SpatialNet has achieved notable results, its high computational cost\nstill hinders real-time applications in vehicles. To this end, this paper\nproposes LSZone, a lightweight spatial information modeling architecture for\nreal-time in-car multi-zone speech separation. We design a spatial information\nextraction-compression (SpaIEC) module that combines Mel spectrogram and\nInteraural Phase Difference (IPD) to reduce computational burden while\nmaintaining performance. Additionally, to efficiently model spatial\ninformation, we introduce an extremely lightweight Conv-GRU\ncrossband-narrowband processing (CNP) module. Experimental results demonstrate\nthat LSZone, with a complexity of 0.56G MACs and a real-time factor (RTF) of\n0.37, delivers impressive performance in complex noise and multi-speaker\nscenarios."}
{"id": "2510.11330", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11330", "abs": "https://arxiv.org/abs/2510.11330", "authors": ["KiHyun Nam", "Jongmin Choi", "Hyeongkeun Lee", "Jungwoo Heo", "Joon Son Chung"], "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap", "comment": "5 pages. Submitted to IEEE ICASSP 2026", "summary": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link"}
{"id": "2510.10796", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10796", "abs": "https://arxiv.org/abs/2510.10796", "authors": ["R. Maydani", "Y. Wang", "J. Sarrazin", "B. Ma"], "title": "Spatially Filtered Sparse Bayesian Learning for Direction-of-Arrival Estimation with Leaky-Wave Antennas", "comment": "Preprint submitted to ICASSP 2026. 4 pages, 3 figures", "summary": "Direction-of-arrival (DoA) estimation with leaky-wave antennas (LWAs) offers\na compact and cost-effective alternative to conventional antenna arrays but\nremains challenging in the presence of coherent sources. To address this issue,\nwe propose a spatially filtered sparse Bayesian learning (SF-SBL) framework.\nFirstly, the field of view (FoV) is divided into angular sectors according to\nthe frequency beam-scanning property of LWAs, and Bayesian inverse problems are\nthen solved within each sector to improve efficiency and reduce computational\ncost. Both on-grid SBL and off-grid SBL formulations are developed. Simulation\nresults show that the proposed approach achieves robust and accurate DoA\nestimation, even with coherent sources."}
{"id": "2510.10719", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10719", "abs": "https://arxiv.org/abs/2510.10719", "authors": ["Ummy Maria Muna", "Md Mehedi Hasan Shawon", "Md Jobayer", "Sumaiya Akter", "Md Rakibul Hasan", "Md. Golam Rabiul Alam"], "title": "SS-DPPN: A self-supervised dual-path foundation model for the generalizable cardiac audio representation", "comment": null, "summary": "The automated analysis of phonocardiograms is vital for the early diagnosis\nof cardiovascular disease, yet supervised deep learning is often constrained by\nthe scarcity of expert-annotated data. In this paper, we propose the\nSelf-Supervised Dual-Path Prototypical Network (SS-DPPN), a foundation model\nfor cardiac audio representation and classification from unlabeled data. The\nframework introduces a dual-path contrastive learning based architecture that\nsimultaneously processes 1D waveforms and 2D spectrograms using a novel hybrid\nloss. For the downstream task, a metric-learning approach using a Prototypical\nNetwork was used that enhances sensitivity and produces well-calibrated and\ntrustworthy predictions. SS-DPPN achieves state-of-the-art performance on four\ncardiac audio benchmarks. The framework demonstrates exceptional data\nefficiency with a fully supervised model on three-fold reduction in labeled\ndata. Finally, the learned representations generalize successfully across lung\nsound classification and heart rate estimation. Our experiments and findings\nvalidate SS-DPPN as a robust, reliable, and scalable foundation model for\nphysiological signals."}
{"id": "2510.11507", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11507", "abs": "https://arxiv.org/abs/2510.11507", "authors": ["Alain Riou", "Joan Serrà", "Yuki Mitsufuji"], "title": "Automatic Music Sample Identification with Multi-Track Contrastive Learning", "comment": null, "summary": "Sampling, the technique of reusing pieces of existing audio tracks to create\nnew music content, is a very common practice in modern music production. In\nthis paper, we tackle the challenging task of automatic sample identification,\nthat is, detecting such sampled content and retrieving the material from which\nit originates. To do so, we adopt a self-supervised learning approach that\nleverages a multi-track dataset to create positive pairs of artificial mixes,\nand design a novel contrastive learning objective. We show that such method\nsignificantly outperforms previous state-of-the-art baselines, that is robust\nto various genres, and that scales well when increasing the number of noise\nsongs in the reference database. In addition, we extensively analyze the\ncontribution of the different components of our training pipeline and\nhighlight, in particular, the need for high-quality separated stems for this\ntask."}
{"id": "2510.10923", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.10923", "abs": "https://arxiv.org/abs/2510.10923", "authors": ["Xuyao Deng", "Yong Dou", "Kele Xu"], "title": "Spatial Signal Focusing and Noise Suppression for Direction-of-Arrival Estimation in Large-Aperture 2D Arrays under Demanding Conditions", "comment": null, "summary": "Direction-of-Arrival (DOA) estimation in sensor arrays faces limitations\nunder demanding conditions, including low signal-to-noise ratio,\nsingle-snapshot scenarios, coherent sources, and unknown source counts.\nConventional beamforming suffers from sidelobe interference, adaptive methods\n(e.g., MVDR) and subspace algorithms (e.g., MUSIC) degrade with limited\nsnapshots or coherent signals, while sparse-recovery approaches (e.g., L1-SVD)\nincur high computational complexity for large arrays. In this article, we\nconstruct the concept of the optimal spatial filter to solve the DOA estimation\nproblem under demanding conditions by utilizing the sparsity of spatial\nsignals. By utilizing the concept of the optimal spatial filter, we have\ntransformed the DOA estimation problem into a solution problem for the optimal\nspatial filter. We propose the Spatial Signal Focusing and Noise Suppression\n(SSFNS) algorithm, which is a novel DOA estimation framework grounded in the\ntheoretical existence of an optimal spatial filter, to solve for the optimal\nspatial filter and obtain DOA. Through experiments, it was found that the\nproposed algorithm is suitable for large aperture two-dimensional arrays and\nexperiments have shown that our proposed algorithm performs better than other\nalgorithms in scenarios with few snapshots or even a single snapshot, low\nsignal-to-noise ratio, coherent signals, and unknown signal numbers in\ntwo-dimensional large aperture arrays."}
{"id": "2510.10738", "categories": ["cs.SD", "cs.AI", "68T07 (Primary), 94A12, 68T05 (Secondary)", "I.5.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10738", "abs": "https://arxiv.org/abs/2510.10738", "authors": ["Ling Sun", "Charlotte Zhu", "Shuju Shi"], "title": "Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR", "comment": "Submitted to ICASSP 2026", "summary": "General-purpose ASR underperforms for atypical speakers, such as L2 learners,\nreinforcing bias and limiting use in education and accessibility. Using the\nCEFR-graded Speak and Improve corpus, we show that naive fine-tuning of Whisper\nreduces average WER but simultaneously widens disparities and\ndisproportionately harms lower-level learners. To address this, we propose two\nstrategies: (i) proficiency-aware multitask learning, jointly optimizing ASR\nwith proficiency classification, and (ii) targeted augmentation, applying\nspectrogram masking to low-proficiency speech to counter imbalance. These\napproaches reduce WER by up to 29.4 percent (relative) and insertion/deletion\nerrors by as much as 58.6 percent (relative). Crucially, despite the severe\nimbalance of the dataset reflecting real-world distributions, both strategies\nconsistently narrow proficiency gaps, advancing equitable ASR for L2 learners."}
{"id": "2510.11044", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11044", "abs": "https://arxiv.org/abs/2510.11044", "authors": ["Yang Lu", "Xinke Xie", "Yanqing Xu", "Bo Ai", "Octavia A. Dobre", "Dusit Niyato"], "title": "Dual-Waveguide Pinching Antennas for PLS: Parallel Placement or Orthogonal Placement?", "comment": null, "summary": "Pinching antennas (PAs), as an emerging flexible-antenna technology, enables\nmovable PAs deployed along waveguides to customize channel conditions over a\nlarge scale. This paper investigates an application of PAs to enable\nphysical-layer security (PLS) by enlarging the channel condition diversity\nbetween legitimate users (LUs) and eavesdroppers (Eves). Particularly, we focus\non the dual-waveguide scenario, where the two waveguides employs multiple PAs\nto serve multiple LUs in the presence of an Eve. Specifically, we consider two\nwaveguide placement strategies, i.e., parallel placement and orthogonal\nplacement. Meanwhile, we incorporate two channel models, i.e., in-waveguide\nphase shifts, and in-waveguide phase shifts and attenuation. We formulate the\nsecure sum rate (SSR) and secure energy efficiency (SEE) maximization problems,\nand propose a two-stage algorithm to solve them. The first stage adopts a\nparticle swarm optimization (PSO) method with an improved feasibility module,\ntermed FeaPSO, for PA placement, and the second stage employs the successive\nconvex approximate (SCA) method to optimize beamforming and artificial noise\nvectors. Furthermore, we conduct numerical comparisons between the two\nplacement strategies in terms of average performance and a special case where\nan Eve is positioned in front of LUs. Numerical results validate the\neffectiveness of the proposed algorithm and demonstrate that PAs can\nsignificantly improve both SSR and SEE. Additionally, the necessity of\northogonal waveguide placement is explicitly verified."}
{"id": "2510.10740", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.10740", "abs": "https://arxiv.org/abs/2510.10740", "authors": ["Zhiqi Ai", "Han Cheng", "Yuxin Wang", "Shiyi Mu", "Shugong Xu", "Yongjin Zhou"], "title": "Dual Data Scaling for Robust Two-Stage User-Defined Keyword Spotting", "comment": "5 pages, 3 figures", "summary": "In this paper, we propose DS-KWS, a two-stage framework for robust\nuser-defined keyword spotting. It combines a CTC-based method with a streaming\nphoneme search module to locate candidate segments, followed by a QbyT-based\nmethod with a phoneme matcher module for verification at both the phoneme and\nutterance levels. To further improve performance, we introduce a dual data\nscaling strategy: (1) expanding the ASR corpus from 460 to 1,460 hours to\nstrengthen the acoustic model; and (2) leveraging over 155k anchor classes to\ntrain the phoneme matcher, significantly enhancing the distinction of\nconfusable words. Experiments on LibriPhrase show that DS-KWS significantly\noutperforms existing methods, achieving 6.13\\% EER and 97.85\\% AUC on the Hard\nsubset. On Hey-Snips, it achieves zero-shot performance comparable to full-shot\ntrained models, reaching 99.13\\% recall at one false alarm per hour."}
{"id": "2510.11097", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11097", "abs": "https://arxiv.org/abs/2510.11097", "authors": ["Shumaila Javaid", "Nasir Saeed"], "title": "The Post-Electromagnetic Era: A Vision for Wireless Communication Beyond 6G", "comment": "Magazine Article", "summary": "Electromagnetic (EM) communication is nearing its physical and thermodynamic\nlimits, where further performance gains through spectrum optimization alone\nhave become increasingly unsustainable. Finite bandwidth, propagation loss at\nhigher frequencies, and the inherent trade-offs between energy and information\nconstrain the scalability of 6G and beyond systems. These limitations drive the\nsearch for alternative mechanisms for information transfer beyond conventional\nEM propagation. This work introduces a state-centric framework for post-6G\ncommunication, in which information is conveyed by manipulating physical,\nbiological, and cognitive states rather than EM waves. It identifies ten\nfoundational paradigms that define potential carriers and interaction\nmechanisms for the post-electromagnetic era and outlines a research roadmap\ntoward self-organizing, cognitively integrated networks. Together, these\ndevelopments envision a new class of communication systems that are\nenergy-aware, adaptive, and capable of uniting matter, life, and intelligence\nwithin a single informational continuum. By establishing the conceptual basis\nfor this transition, the work provides a foundation for future research aimed\nat realizing communication paradigms that transcend the limitations of\nspectrum-bound systems."}
{"id": "2510.10774", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10774", "abs": "https://arxiv.org/abs/2510.10774", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "title": "ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for Text-to-Speech Synthesis", "comment": null, "summary": "Persian Language, despite being spoken by over 100 million people worldwide,\nremains severely underrepresented in high-quality speech corpora, particularly\nfor text-to-speech (TTS) synthesis applications. Existing Persian speech\ndatasets are typically smaller than their English counterparts, which creates a\nkey limitation for developing Persian speech technologies. We address this gap\nby introducing ParsVoice, the largest Persian speech corpus designed\nspecifically for TTS applications. We created an automated pipeline that\ntransforms raw audiobook content into TTS-ready data, incorporating components\nsuch as a BERT-based sentence completion detector, a binary search boundary\noptimization method for precise audio-text alignment, and multi-dimensional\nquality assessment frameworks tailored to Persian. The pipeline processes 2,000\naudiobooks, yielding 3,526 hours of clean speech, which was further filtered\ninto a 1,804-hour high-quality subset suitable for TTS, featuring more than 470\nspeakers. ParsVoice is the largest high-quality Persian speech dataset,\noffering speaker diversity and audio quality comparable to major English\ncorpora. The complete dataset has been made publicly available to accelerate\nthe development of Persian speech technologies and to serve as a template for\nother low-resource languages. The ParsVoice dataset is publicly available at\nParsVoice (https://huggingface.co/datasets/MohammadJRanjbar/ParsVoice)."}
{"id": "2510.11113", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11113", "abs": "https://arxiv.org/abs/2510.11113", "authors": ["Hetong Wang", "Tiejun Lv", "Yashuai Cao", "Weicai Li", "Jie Zeng", "Pingmu Huang", "Muhammad Khurram Khan"], "title": "Navigating the Dual-Use Nature and Security Implications of Reconfigurable Intelligent Surfaces in Next-Generation Wireless Systems", "comment": "This manuscript has been accepted for publication in IEEE\n  Communications Surveys and Tutorials. It was received on January 17, 2025,\n  and revised on July 1 and September 16, 2025. This version was accepted on\n  October 10, 2025", "summary": "Reconfigurable intelligent surface (RIS) technology offers significant\npromise in enhancing wireless communication systems, but its dual-use potential\nalso introduces substantial security risks. This survey explores the security\nimplications of RIS in next-generation wireless networks. We first highlight\nthe dual-use nature of RIS, demonstrating how its communication-enhancing\ncapabilities can be exploited by adversaries to compromise legitimate users. We\nidentify a new class of security vulnerabilities termed ``passive-active hybrid\nattacks,'' where RIS, despite passively handling signals, can be reconfigured\nto actively engage in malicious activities, enabling various RIS-assisted\nattacks, such as eavesdropping, man-in-the-middle (MITM), replay, reflection\njamming, and side-channel attacks. Furthermore, we reveal how adversaries can\nexploit the openness of wireless channels to introduce adversarial\nperturbations in artificial intelligence-driven RIS networks, disrupting\ncommunication terminals and causing misclassifications or errors in RIS\nreflection predictions. Despite these risks, RIS technology also plays a\ncritical role in enhancing security and privacy across radio frequency (RF) and\nvisible light communication (VLC) systems. By synthesizing current insights and\nhighlighting emerging threats, we provide actionable insights into cross-layer\ncollaboration, advanced adversarial defenses, and the balance between security\nand cost. This survey provides a comprehensive overview of RIS technology's\nsecurity landscape and underscores the urgent need for robust security\nframeworks in the development of future wireless systems."}
{"id": "2510.10785", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.10785", "abs": "https://arxiv.org/abs/2510.10785", "authors": ["Yurii Halychanskyi", "Cameron Churchwell", "Yutong Wen", "Volodymyr Kindratenko"], "title": "FAC-FACodec: Controllable Zero-Shot Foreign Accent Conversion with Factorized Speech Codec", "comment": "5 pages, 2 figures", "summary": "Previous accent conversion (AC) methods, including foreign accent conversion\n(FAC), lack explicit control over the degree of modification. Because accent\nmodification can alter the perceived speaker identity, balancing conversion\nstrength and identity preservation is crucial. We present an AC framework that\nprovides an explicit, user-controllable parameter for accent modification. The\nmethod targets pronunciation while preserving suprasegmental cues such as\nintonation and phoneme durations. Results show performance comparable to recent\nAC systems, stronger preservation of speaker identity, and unique support for\ncontrollable accent conversion."}
{"id": "2510.11150", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11150", "abs": "https://arxiv.org/abs/2510.11150", "authors": ["Sai Xu", "Yanan Du"], "title": "WiNPA: Wireless Neural Processing Architecture", "comment": null, "summary": "This article presents a wireless neural processing architecture (WiNPA),\nproviding a novel perspective for accelerating edge inference of deep neural\nnetwork (DNN) workloads via joint optimization of wireless and computing\nresources. WiNPA enables fine-grained integration of wireless communication and\nedge computing, bridging the research gap between wireless and edge\nintelligence and significantly improving DNN inference performance. To fully\nrealize its potential, we explore a set of fundamental research issues,\nincluding mathematical modeling, optimization, and unified hardware--software\nplatforms. Additionally, key research directions are discussed to guide future\ndevelopment and practical implementation. A case study demonstrates WiNPA's\nworkflow and effectiveness in accelerating DNN inference through simulations."}
{"id": "2510.10948", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10948", "abs": "https://arxiv.org/abs/2510.10948", "authors": ["Xuyao Deng", "Yanjie Sun", "Yong Dou", "Kele Xu"], "title": "Unify Variables in Neural Scaling Laws for General Audio Representations via Embedding Effective Rank", "comment": null, "summary": "Scaling laws have profoundly shaped our understanding of model performance in\ncomputer vision and natural language processing, yet their application to\ngeneral audio representation learning remains underexplored. A key challenge\nlies in the multifactorial nature of general audio\nrepresentation-representation quality is jointly influenced by variables such\nas audio length, embedding dimensionality, model depth, model architecture,\ndata volume, etc., many of which are difficult to isolate or express\nanalytically. In this work, we present a systematic study of scaling laws for\ngeneral audio representations by utilizing embedding effective rank (RankMe) as\na unifying metric that encapsulates the impact of diverse variables on\nrepresentation quality. RankMe enables a label-free, information-theoretic\nquantification of audio embeddings, allowing us to examine scaling behaviors\nacross a wide hyper-parameter space, including model size, training data\nvolume, computational budget, architectural configurations, etc. Our empirical\nfindings reveal a consistent power-law relationship between RankMe and\nrepresentation quality, suggesting that embedding effective rank serves as a\nreliable proxy for assessing and predicting model performance in audio\nrepresentation learning. This work not only validates the applicability of\nclassical scaling principles to the general audio domain but also offers a\ntheoretically grounded and empirically robust framework for guiding future\nmodel scaling strategies in audio foundation models."}
{"id": "2510.11214", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11214", "abs": "https://arxiv.org/abs/2510.11214", "authors": ["Mehdi Sattari", "Javad Aliakbari", "Alexandre Graell i Amat", "Tommy Svensson"], "title": "CSI Prediction Using Diffusion Models", "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Acquiring accurate channel state information (CSI) is critical for reliable\nand efficient wireless communication, but challenges such as high pilot\noverhead and channel aging hinder timely and accurate CSI acquisition. CSI\nprediction, which forecasts future CSI from historical observations, offers a\npromising solution. Recent deep learning approaches, including recurrent neural\nnetworks and Transformers, have achieved notable success but typically learn\ndeterministic mappings, limiting their ability to capture the stochastic and\nmultimodal nature of wireless channels. In this paper, we introduce a novel\nprobabilistic framework for CSI prediction based on diffusion models, offering\na flexible design that supports integration of diverse prediction schemes. We\ndecompose the CSI prediction task into two components: a temporal encoder,\nwhich extracts channel dynamics, and a diffusion-based generator, which\nproduces future CSI samples. We investigate two inference\nschemes-autoregressive and sequence-to-sequence- and explore multiple diffusion\nbackbones, including U-Net and Transformer-based architectures. Furthermore, we\nexamine a diffusion-based approach without an explicit temporal encoder and\nutilize the DDIM scheduling to reduce model complexity. Extensive simulations\ndemonstrate that our diffusion-based models significantly outperform\nstate-of-the-art baselines."}
{"id": "2510.10995", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.10995", "abs": "https://arxiv.org/abs/2510.10995", "authors": ["Yongyi Zang", "Jiarui Hai", "Wanying Ge", "Qiuqiang Kong", "Zheqi Dai", "Helin Wang", "Yuki Mitsufuji", "Mark D. Plumbley"], "title": "MSRBench: A Benchmarking Dataset for Music Source Restoration", "comment": null, "summary": "Music Source Restoration (MSR) extends source separation to realistic\nsettings where signals undergo production effects (equalization, compression,\nreverb) and real-world degradations, with the goal of recovering the original\nunprocessed sources. Existing benchmarks cannot measure restoration fidelity:\nsynthetic datasets use unprocessed stems but unrealistic mixtures, while real\nproduction datasets provide only already-processed stems without clean\nreferences. We present MSRBench, the first benchmark explicitly designed for\nMSR evaluation. MSRBench contains raw stem-mixture pairs across eight\ninstrument classes, where mixtures are produced by professional mixing\nengineers. These raw-processed pairs enable direct evaluation of both\nseparation accuracy and restoration fidelity. Beyond controlled studio\nconditions, the mixtures are augmented with twelve real-world degradations\nspanning analog artifacts, acoustic environments, and lossy codecs. Baseline\nexperiments with U-Net and BSRNN achieve SI-SNR of -37.8 dB and -23.4 dB\nrespectively, with perceptual quality (FAD CLAP) around 0.7-0.8, demonstrating\nsubstantial room for improvement and the need for restoration-specific\narchitectures."}
{"id": "2510.11216", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11216", "abs": "https://arxiv.org/abs/2510.11216", "authors": ["Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "Normalized Ambiguity Function Characteristics of OFDM, OTFS, AFDM, and CP-AFDM for ISAC", "comment": "Submitted to the IEEE ICC 2025", "summary": "This paper presents a unified and system-agnostic analysis of the ambiguity\nfunction (AF) characteristics of four representative multicarrier waveforms,\northogonal frequency division multiplexing (OFDM), orthogonal time frequency\nspace (OTFS), affine frequency division multiplexing (AFDM), and chirp-permuted\nAFDM (CP-AFDM), which are considered as key candidates for enabling integrated\nsensing and communications (ISAC) in future sixth generation (6G) networks. The\nAF of each waveform is obtained directly from its discrete-time definition and\nenhanced via ideal fractional interpolation, enabling precise characterization\nof its continuous-time delay-Doppler response. Two signaling modes are\nexamined: a communication-oriented case with random information symbols\nsuitable only for monostatic scenarios, and a sensing-oriented case with fixed\nunimodular symbols suitable for general multi-static scenarios. Furthermore,\nthe AFs and the ambiguity metrics including the 3dB mainlobe width,\npeak-to-sidelobe ratio (PSLR), and integrated sidelobe ratio (ISLR), are\nevaluated in normalized delay-Doppler units, enabling direct translation to any\nphysical system configuration defined by bandwidth, sampling frequency, or\nsymbol duration, while ensuring straightforward and consistent comparison\nacross waveforms. The results establish a consistent benchmark for comparing\nwaveform sensing capabilities in ISAC design, consolidating known behaviors:\nOFDM exhibits excellent delay resolution and sidelobe behavior but poor Doppler\nresponse, whereas advanced waveforms achieve improved balance between delay and\nDoppler resolution with varying sidelobe characteristics. The simulation code\nof the smooth AFs, is openly shared to promote reproducibility and support\nfuture ISAC waveform research."}
{"id": "2510.11098", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11098", "abs": "https://arxiv.org/abs/2510.11098", "authors": ["Jiliang Hu", "Wenfu Wang", "Zuchao Li", "Chenxing Li", "Yiyang Zhao", "Hanzhao Li", "Liqiang Zhang", "Meng Yu", "Dong Yu"], "title": "VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents", "comment": "20 pages, 5 figures", "summary": "Recent advances in large audio language models (LALMs) have greatly enhanced\nmultimodal conversational systems. However, existing benchmarks remain limited\n-- they are mainly English-centric, rely on synthetic speech, and lack\ncomprehensive, discriminative evaluation across multiple dimensions. To address\nthese gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality\nChinese benchmark built entirely on real human speech. VCB Bench evaluates\nLALMs from three complementary perspectives: instruction following (including\nspeech-level control beyond text commands), knowledge understanding (general\nknowledge, reasoning, and daily dialogue), and robustness (stability under\nperturbations in content, environment, and speaker traits). Experiments on\nrepresentative LALMs reveal notable performance gaps and highlight future\ndirections for improvement. VCB Bench provides a reproducible and fine-grained\nevaluation framework, offering standardized methodology and practical insights\nfor advancing Chinese voice conversational models."}
{"id": "2510.11279", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11279", "abs": "https://arxiv.org/abs/2510.11279", "authors": ["Mingzhi Wang", "Zhichao Zhang"], "title": "Two-Dimensional Graph Bi-Fractional Fourier Transform", "comment": null, "summary": "Graph signal processing (GSP) advances spectral analysis on irregular\ndomains. However, existing two-dimensional graph fractional Fourier transform\n(2D-GFRFT) employs a single fractional order for both factor graphs, thereby\nlimiting its adaptability to heterogeneous signals. We proposed the\ntwo-dimensional graph bi-fractional Fourier transform (2D-GBFRFT), which\nassigns independent fractional orders to the factor graphs of a Cartesian\nproduct while preserving separability. We established invertibility, unitarity,\nand index additivity, and developed two filtering schemes: a Wiener-style\ndesign through grid search and a differentiable framework that jointly\noptimizes transform orders and diagonal spectral filters. We further introduced\na hybrid interpolation with the joint time-vertex fractional Fourier transform\n(JFRFT), controlled by a tunable parameter that balances the two methods. In\nthe domains of synthetic Cartesian product graph signals, authentic temporal\ngraph datasets, and dynamic image deblurring, 2D-GBFRFT consistently surpasses\n2D-GFRFT and enhances JFRFT. Experimental results confirmed the versatility and\nsuperior performance of 2D-GBFRFT for filtering in GSP."}
{"id": "2510.11124", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.11124", "abs": "https://arxiv.org/abs/2510.11124", "authors": ["Cheng Gong", "Chunyu Qiang", "Tianrui Wang", "Yu Jiang", "Yuheng Lu", "Ruihao Jing", "Xiaoxiao Miao", "Xiaolei Zhang", "Longbiao Wang", "Jianwu Dang"], "title": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker", "comment": "Submitted to Expert Systems with Applications,11 pages", "summary": "Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one\nlanguage that captures the emotion of a speaker from another language while\nmaintaining the target voice's timbre. This process of cross-lingual emotional\nspeech synthesis presents a complex challenge, necessitating flexible control\nover emotion, timbre, and language. However, emotion and timbre are highly\nentangled in speech signals, making fine-grained control challenging. To\naddress this issue, we propose EMM-TTS, a novel two-stage cross-lingual\nemotional speech synthesis framework based on perturbed self-supervised\nlearning (SSL) representations. In the first stage, the model explicitly and\nimplicitly encodes prosodic cues to capture emotional expressiveness, while the\nsecond stage restores the timbre from perturbed SSL representations. We further\ninvestigate the effect of different speaker perturbation strategies-formant\nshifting and speaker anonymization-on the disentanglement of emotion and\ntimbre. To strengthen speaker preservation and expressive control, we introduce\nSpeaker Consistency Loss (SCL) and Speaker-Emotion Adaptive Layer Normalization\n(SEALN) modules. Additionally, we find that incorporating explicit acoustic\nfeatures (e.g., F0, energy, and duration) alongside pretrained latent features\nimproves voice cloning performance. Comprehensive multi-metric evaluations,\nincluding both subjective and objective measures, demonstrate that EMM-TTS\nachieves superior naturalness, emotion transferability, and timbre consistency\nacross languages."}
{"id": "2510.11294", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11294", "abs": "https://arxiv.org/abs/2510.11294", "authors": ["Run Gu", "Renjie Xie", "Wei Xu", "Zhaohui Yang", "Kaibin Huang"], "title": "Channel-Aware Deep Learning for Superimposed Pilot Power Allocation and Receiver Design", "comment": null, "summary": "Superimposed pilot (SIP) schemes face significant challenges in effectively\nsuperimposing and separating pilot and data signals, especially in multiuser\nmobility scenarios with rapidly varying channels. To address these challenges,\nwe propose a novel channel-aware learning framework for SIP schemes, termed\nCaSIP, that jointly optimizes pilot-data power (PDP) allocation and a receiver\nnetwork for pilot-data interference (PDI) elimination, by leveraging channel\npath gain information, a form of large-scale channel state information (CSI).\nThe proposed framework identifies user-specific, resource element-wise PDP\nfactors and develops a deep neural network-based SIP receiver comprising\nexplicit channel estimation and data detection components. To properly leverage\npath gain data, we devise an embedding generator that projects it into\nembeddings, which are then fused with intermediate feature maps of the channel\nestimation network. Simulation results demonstrate that CaSIP efficiently\noutperforms traditional pilot schemes and state-of-the-art SIP schemes in terms\nof sum throughput and channel estimation accuracy, particularly under\nhigh-mobility and low signal-to-noise ratio (SNR) conditions."}
{"id": "2510.11330", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11330", "abs": "https://arxiv.org/abs/2510.11330", "authors": ["KiHyun Nam", "Jongmin Choi", "Hyeongkeun Lee", "Jungwoo Heo", "Joon Son Chung"], "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap", "comment": "5 pages. Submitted to IEEE ICASSP 2026", "summary": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link"}
{"id": "2510.11353", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11353", "abs": "https://arxiv.org/abs/2510.11353", "authors": ["Woo-Hyun Ko", "Jaewon Kim", "Tzu-Hsiang Lin", "Samin Moosavi", "P. R. Kumar"], "title": "A Dynamic Watermarking Technique for Matching Communication Addresses with Cars in a Visual Field", "comment": null, "summary": "We consider a problem faced by an intelligent roadside unit (RSU) monitoring\na roadway by a video camera. Suppose the RSU notices that a particular car in\nits visual field needs to execute a specific evasive maneuver to avoid danger.\nIt would like to send a packet addressed to that particular car with this\nsuggestion. The problem is that while all the cars are communicating with the\nRSU, the RSU does not know which car in the video is associated with what IP\naddress. So, it does not know which IP address to send the packet to. Indeed,\nthe problem of matching addresses with cars in the visual field is a\nfundamental open problem. We provide an active solution employing dynamic\nwatermarking that was originally developed for the security of cyber-physical\nsystems. This technique calls for a car to superpose a small random excitation\nonto its actuation commands for steering angle or throttle/brake positions. The\ncar sends this random waveform to the RSU in a packet containing its IP\naddress. By signal processing of the video stream of a car at the RSU it can\nverify whether it matches with the waveform in the packet and thereby\nassociates that the IP address of the packet with that car in the visual field.\nThe RSU thereby determines which IP address is associated with which car in its\nvisual field. We present two demonstrations of performance. We demonstrate\nexperimental results on a laboratory transportation automated vehicles, a\nvision system, and a network, as well as on the field with two passenger sedans\nin practice. The results demonstrate that employing the dynamic watermarking\nmethod enables an RSU to distinguish the communication of a target vehicle from\nthat of other IP addresses of nearby vehicles."}
{"id": "2510.11454", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11454", "abs": "https://arxiv.org/abs/2510.11454", "authors": ["Kuan-Yi Lee", "Tsung-En Lin", "Hung-Yi Lee"], "title": "Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented Reasoning", "comment": "9pages", "summary": "Recent advancements in large multimodal models (LMMs) have shown strong\ncapabilities in audio understanding. However, most systems rely solely on\nend-to-end reasoning, limiting interpretability and accuracy for tasks that\nrequire structured knowledge or specialized signal analysis. In this work, we\npresent Audio-Maestro -- a tool-augmented audio reasoning framework that\nenables audio-language models to autonomously call external tools and integrate\ntheir timestamped outputs into the reasoning process. This design allows the\nmodel to analyze, transform, and interpret audio signals through specialized\ntools rather than relying solely on end-to-end inference. Experiments show that\nAudio-Maestro consistently improves general audio reasoning performance:\nGemini-2.5-flash's average accuracy on MMAU-Test rises from 67.4% to 72.1%,\nDeSTA-2.5 from 58.3% to 62.8%, and GPT-4o from 60.8% to 63.9%. To our\nknowledge, Audio-Maestro is the first framework to integrate structured tool\noutput into the large audio language model reasoning process."}
{"id": "2510.11374", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.11374", "abs": "https://arxiv.org/abs/2510.11374", "authors": ["Ruiqi Kong", "He Chen"], "title": "CIRSense: Rethinking WiFi Sensing with Channel Impulse Response", "comment": "16 pages, 15 figures", "summary": "WiFi sensing based on channel state information (CSI) collected from\ncommodity WiFi devices has shown great potential across a wide range of\napplications, including vital sign monitoring and indoor localization. Existing\nWiFi sensing approaches typically estimate motion information directly from\nCSI. However, they often overlook the inherent advantages of channel impulse\nresponse (CIR), a delay-domain representation that enables more intuitive and\nprincipled motion sensing by naturally concentrating motion energy and\nseparating multipath components. Motivated by this, we revisit WiFi sensing and\nintroduce CIRSense, a new framework that enhances the performance and\ninterpretability of WiFi sensing with CIR. CIRSense is built upon a new motion\nmodel that characterizes fractional delay effects, a fundamental challenge in\nCIR-based sensing. This theoretical model underpins technical advances for the\nthree challenges in WiFi sensing: hardware distortion compensation,\nhigh-resolution distance estimation, and subcarrier aggregation for extended\nrange sensing. CIRSense, operating with a 160 MHz channel bandwidth,\ndemonstrates versatile sensing capabilities through its dual-mode design,\nachieving a mean error of approximately 0.25 bpm in respiration monitoring and\n0.09 m in distance estimation. Comprehensive evaluations across residential\nspaces, far-range scenarios, and multi-target settings demonstrate CIRSense's\nsuperior performance over state-of-the-art CSI-based baselines. Notably, at a\nchallenging sensing distance of 20 m, CIRSense achieves at least 3x higher\naverage accuracy with more than 4.5x higher computational efficiency."}
{"id": "2510.11507", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11507", "abs": "https://arxiv.org/abs/2510.11507", "authors": ["Alain Riou", "Joan Serrà", "Yuki Mitsufuji"], "title": "Automatic Music Sample Identification with Multi-Track Contrastive Learning", "comment": null, "summary": "Sampling, the technique of reusing pieces of existing audio tracks to create\nnew music content, is a very common practice in modern music production. In\nthis paper, we tackle the challenging task of automatic sample identification,\nthat is, detecting such sampled content and retrieving the material from which\nit originates. To do so, we adopt a self-supervised learning approach that\nleverages a multi-track dataset to create positive pairs of artificial mixes,\nand design a novel contrastive learning objective. We show that such method\nsignificantly outperforms previous state-of-the-art baselines, that is robust\nto various genres, and that scales well when increasing the number of noise\nsongs in the reference database. In addition, we extensively analyze the\ncontribution of the different components of our training pipeline and\nhighlight, in particular, the need for high-quality separated stems for this\ntask."}
{"id": "2510.11384", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11384", "abs": "https://arxiv.org/abs/2510.11384", "authors": ["Jennie Couchman", "Phillip Stanley-Marbell"], "title": "Uncertainty Propagation in Finite Impulse Response Filters: Evaluating the Gaussian Assumption", "comment": "5 pages, 2 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "A common assumption in signal processing is that underlying data numerically\nconforms to a Gaussian distribution. It is commonly utilized in signal\nprocessing to describe unknown additive noise in a system and is often\njustified by citing the central limit theorem for sums of random variables,\nalthough the central limit theorem applies only to sums of independent\nidentically distributed random variables. However, many linear operations in\nsignal processing take the form of weighted sums, which transforms the random\nvariables such that their distributions are no longer identical. One such\noperation is a finite impulse response (FIR) filter. FIR filters are commonly\nused in signal processing applications as a pre-processing step. FIR output\nnoise is generally assumed to be Gaussian. This article examines the FIR output\nresponse in the presence of uniformly distributed quantization noise. We\nexpress the FIR output uncertainty in terms of the input quantization\nuncertainty and filter coefficients. We show that the output uncertainty cannot\nbe assumed to be Gaussian, but depending on the application a Gaussian\nestimation may still be useful. Then, we show through detailed numerical\nsimulations that the output uncertainty distribution of the filter can be\nestimated through its most dominant coefficients."}
{"id": "2510.11646", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.11646", "abs": "https://arxiv.org/abs/2510.11646", "authors": ["Jingyuan Xing", "Mingru Yang", "Zhipeng Li", "Xiaofen Xing", "Xiangmin Xu"], "title": "BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis", "comment": null, "summary": "Autoregressive (AR) frameworks have recently achieved remarkable progress in\nzero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large\nlanguage model techniques. Despite their success, existing AR-based zero-shot\nTTS systems face two critical limitations: (i) an inherent speed-quality\ntrade-off, as sequential token generation either reduces frame rates at the\ncost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a\ntext-oriented supervision mismatch, as cross-entropy loss penalizes token\nerrors uniformly without considering the fine-grained acoustic similarity among\nadjacent tokens. To address these challenges, we propose BridgeTTS, a novel\nAR-TTS framework built upon the dual speech representation paradigm BridgeCode.\nBridgeTTS reduces AR iterations by predicting sparse tokens while\nreconstructing rich continuous features for high-quality synthesis. Joint\noptimization of token-level and feature-level objectives further enhances\nnaturalness and intelligibility. Experiments demonstrate that BridgeTTS\nachieves competitive quality and speaker similarity while significantly\naccelerating synthesis. Speech demos are available at\nhttps://test1562.github.io/demo/."}
{"id": "2510.11461", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11461", "abs": "https://arxiv.org/abs/2510.11461", "authors": ["Eric Han Wang", "Weijia Yan", "Ruihong Huang"], "title": "Thermal Analysis of 3D GPU-Memory Architectures with Boron Nitride Interposer", "comment": null, "summary": "As artificial intelligence (AI) chips become more powerful, the thermal\nmanagement capabilities of conventional silicon (Si) substrates become\ninsufficient for 3D-stacked designs. This work integrates electrically\ninsulative and thermally conductive hexagonal boron nitride (h-BN) interposers\ninto AI chips for effective thermal management. Using COMSOL Multiphysics, the\neffects of High-Bandwidth Memory (HBM) distributions and thermal interface\nmaterial configurations on heat dissipation and hotspot mitigation were\nstudied. A 20 {\\deg}C reduction in hot spots was achieved using h-BN\ninterposers compared to Si interposers. Such an improvement could reduce AI\nchips' power leakage by 22% and significantly enhance their thermal\nperformance."}
{"id": "2510.11465", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11465", "abs": "https://arxiv.org/abs/2510.11465", "authors": ["Diego Tuzi", "Thomas Delamotte", "Andreas Knopp"], "title": "Control Requirements for Robust Beamforming in Multi-Satellite Systems", "comment": "6 pages, 3 figures, IFAC Workshop on Control Aspects of\n  Multi-Satellite Systems (CAMSAT) 2025", "summary": "This work investigates the impact of position and attitude perturbations on\nthe beamforming performance of multi-satellite systems. The system under\nanalysis is a formation of small satellites equipped with direct radiating\narrays that synthesise a large virtual antenna aperture. The results show that\nperformance is highly sensitive to the considered perturbations. However, by\nincorporating position and attitude information into the beamforming process,\nnominal performance can be effectively restored. These findings support the\ndevelopment of control-aware beamforming strategies that tightly integrate the\nattitude and orbit control system with signal processing to enable robust\nbeamforming and autonomous coordination."}
{"id": "2510.11514", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11514", "abs": "https://arxiv.org/abs/2510.11514", "authors": ["Yinchao Yang", "Yahao Ding", "Zhaohui Yang", "Chongwen Huang", "Zhaoyang Zhang", "Dusit Niyato", "Mohammad Shikh-Bahaei"], "title": "Toward Efficient and Privacy-Aware eHealth Systems: An Integrated Sensing, Computing, and Semantic Communication Approach", "comment": "Accepted by the IEEE Internet of Things Journal", "summary": "Real-time and contactless monitoring of vital signs, such as respiration and\nheartbeat, alongside reliable communication, is essential for modern healthcare\nsystems, especially in remote and privacy-sensitive environments. Traditional\nwireless communication and sensing networks fall short in meeting all the\nstringent demands of eHealth, including accurate sensing, high data efficiency,\nand privacy preservation. To overcome the challenges, we propose a novel\nintegrated sensing, computing, and semantic communication (ISCSC) framework. In\nthe proposed system, a service robot utilises radar to detect patient positions\nand monitor their vital signs, while sending updates to the medical devices.\nInstead of transmitting raw physiological information, the robot computes and\ncommunicates semantically extracted health features to medical devices. This\nsemantic processing improves data throughput and preserves the clinical\nrelevance of the messages, while enhancing data privacy by avoiding the\ntransmission of sensitive data. Leveraging the estimated patient locations, the\nrobot employs an interacting multiple model (IMM) filter to actively track\npatient motion, thereby enabling robust beam steering for continuous and\nreliable monitoring. We then propose a joint optimisation of the beamforming\nmatrices and the semantic extraction ratio, subject to computing capability and\npower budget constraints, with the objective of maximising both the semantic\nsecrecy rate and sensing accuracy. Simulation results validate that the ISCSC\nframework achieves superior sensing accuracy, improved semantic transmission\nefficiency, and enhanced privacy preservation compared to conventional joint\nsensing and communication methods."}
{"id": "2510.11582", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.11582", "abs": "https://arxiv.org/abs/2510.11582", "authors": ["Renato Luis Garrido Cavalcante", "Noor Ul Ain", "Lorenzo Miretti", "Slawomir Stanczak"], "title": "Beyond the Use-and-then-Forget (UatF) Bound: Fixed Point Algorithms for Statistical Max-Min Power Control", "comment": null, "summary": "We introduce mathematical tools and fixed point algorithms for optimal\nstatistical max-min power control in cellular and cell-less massive MIMO\nsystems. Unlike previous studies that rely on the use-and-then-forget (UatF)\nlower bound on Shannon achievable (ergodic) rates, our proposed framework can\ndeal with alternative bounds that explicitly consider perfect or imperfect\nchannel state information (CSI) at the decoder. In doing so, we address\nlimitations of UatF-based algorithms, which inherit the shortcomings of the\nUatF bound. For example, the UatF bound can be overly conservative: in extreme\ncases, under fully statistical (nonadaptive) beamforming in zero-mean channels,\nthe UatF bound produces trivial (zero) rate bounds. It also lacks scale\ninvariance: merely scaling the beamformers can change the bound drastically,\nespecially when simple beamforming strategies are employed. In contrast, our\nframework is compatible with information-theoretic bounds that do not suffer\nfrom the above drawbacks. We illustrate the framework by solving a max-min\npower control problem considering a standard bound that exploits instantaneous\nCSI at the decoder."}
{"id": "2510.11628", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11628", "abs": "https://arxiv.org/abs/2510.11628", "authors": ["Patrick Hödl", "Jakob Möderl", "Erik Leitinger", "Klaus Witrisal"], "title": "Bayesian Self-Calibration and Parametric Channel Estimation for 6G Antenna Arrays", "comment": null, "summary": "Accurate channel estimation is essential for both high-rate communication and\nhigh-precision sensing in 6G wireless systems. However, a major performance\nlimitation arises from calibration mismatches when operating phased-array\nantennas under real-world conditions. To address this issue, we propose to\nintegrate antenna element self-calibration into a variational sparse Bayesian\nlearning (VSBL) algorithm for parametric channel estimation. We model antenna\ngain and phase deviations as latent variables and derive explicit update\nequations to jointly infer these calibration parameters and the channel\nparameters: the model order, complex amplitudes, delays, angles, and the noise\nvariance. The resulting algorithm operates online and adapts in real time to\nhardware-induced mismatches. We assess its performance in terms of the root\nmean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric,\ndemonstrating consistent improvements over conventional VSBL without\ncalibration. Our results demonstrate that embedding self-calibration within\nBayesian inference significantly enhances the robustness of channel estimation."}
{"id": "2510.11666", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11666", "abs": "https://arxiv.org/abs/2510.11666", "authors": ["Natalie Lang", "Atsutse K. Kludze", "Nir Shlezinger", "Yasaman Ghasempour", "Tirza Routtenberg", "George C. Alexandropoulos", "Yonina C. Eldar"], "title": "Leaky Wave Antennas for Next Generation Wireless Applications in sub-THz Frequencies: Current Status and Research Challenges", "comment": null, "summary": "The ever-growing demand for ultra-high data rates, massive connectivity, and\njoint communication-sensing capabilities in future wireless networks is driving\nresearch into sub-terahertz (sub-THz) communications. While these frequency\nbands offer abundant spectrum, they also pose severe propagation and hardware\ndesign challenges, motivating the search for alternative antenna solutions\nbeyond conventional antenna arrays. Leaky-wave antennas (LWAs) have emerged as\na promising candidate for sub-THz systems due to their simple feed structure,\nlow fabrication cost, and inherent angle-frequency coupling, which enables\nfrequency-controlled beamsteering with simple hardware. In this article, we\nreview the fundamentals of the LWA technology, highlight their unique\nproperties, and showcase their potential in multi-user wideband sub-THz\nwireless communications. We present representative studies demonstrating that\nLWAs can simultaneously support high-rate multi-user communications and\naccurate localization using only a single antenna element. Finally, several key\nopen challenges are outlined, spanning algorithm design, signal processing,\ninformation theory, standardization, and hardware implementation, that need to\nbe addressed to fully harness LWAs as a cost-effective and scalable enabler of\nnext generations of wireless systems."}
{"id": "2510.11458", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11458", "abs": "https://arxiv.org/abs/2510.11458", "authors": ["Soubhagya Ranjan Hota", "Arka Roy", "Udit Satija"], "title": "ILD-VIT: A Unified Vision Transformer Architecture for Detection of Interstitial Lung Disease from Respiratory Sounds", "comment": null, "summary": "Interstitial lung disease (ILD) represents a group of restrictive chronic\npulmonary diseases that impair oxygen acquisition by causing irreversible\nchanges in the lungs such as fibrosis, scarring of parenchyma, etc. ILD\nconditions are often diagnosed by various clinical modalities such as\nspirometry, high-resolution lung imaging techniques, crackling respiratory\nsounds (RSs), etc. In this letter, we develop a novel vision transformer\n(VIT)-based deep learning framework namely, ILD-VIT, to detect the ILD\ncondition using the RS recordings. The proposed framework comprises three major\nstages: pre-processing, mel spectrogram extraction, and classification using\nthe proposed VIT architecture using the mel spectrogram image patches.\nExperimental results using the publicly available BRACETS and KAUH databases\nshow that our proposed ILD-VIT achieves an accuracy, sensitivity, and\nspecificity of 84.86%, 82.67%, and 86.91%, respectively, for\nsubject-independent blind testing. The successful onboard implantation of the\nproposed framework on a Raspberry-pi-4 microcontroller indicates its potential\nas a standalone clinical system for ILD screening in a real clinical scenario."}

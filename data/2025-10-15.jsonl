{"id": "2510.12042", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12042", "abs": "https://arxiv.org/abs/2510.12042", "authors": ["Wanying Ge", "Xin Wang", "Junichi Yamagishi"], "title": "FakeMark: Deepfake Speech Attribution With Watermarked Artifacts", "comment": null, "summary": "Deepfake speech attribution remains challenging for existing solutions.\nClassifier-based solutions often fail to generalize to domain-shifted samples,\nand watermarking-based solutions are easily compromised by distortions like\ncodec compression or malicious removal attacks. To address these issues, we\npropose FakeMark, a novel watermarking framework that injects\nartifact-correlated watermarks associated with deepfake systems rather than\npre-assigned bitstring messages. This design allows a detector to attribute the\nsource system by leveraging both injected watermark and intrinsic deepfake\nartifacts, remaining effective even if one of these cues is elusive or removed.\nExperimental results show that FakeMark improves generalization to\ncross-dataset samples where classifier-based solutions struggle and maintains\nhigh accuracy under various distortions where conventional watermarking-based\nsolutions fail."}
{"id": "2510.12210", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12210", "abs": "https://arxiv.org/abs/2510.12210", "authors": ["Yakun Song", "Xiaobin Zhuang", "Jiawei Chen", "Zhikang Niu", "Guanrou Yang", "Chenpeng Du", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation", "comment": null, "summary": "Recent attempts to interleave autoregressive (AR) sketchers with\ndiffusion-based refiners over continuous speech representations have shown\npromise, but they remain brittle under distribution shift and offer limited\nlevers for controllability. We introduce DISTAR, a zero-shot text-to-speech\nframework that operates entirely in a discrete residual vector quantization\n(RVQ) code space and tightly couples an AR language model with a masked\ndiffusion model, without forced alignment or a duration predictor. Concretely,\nDISTAR drafts block-level RVQ tokens with an AR language model and then\nperforms parallel masked-diffusion infilling conditioned on the draft to\ncomplete the next block, yielding long-form synthesis with blockwise\nparallelism while mitigating classic AR exposure bias. The discrete code space\naffords explicit control at inference: DISTAR produces high-quality audio under\nboth greedy and sample-based decoding using classifier-free guidance, supports\ntrade-offs between robustness and diversity, and enables variable bit-rate and\ncontrollable computation via RVQ layer pruning at test time. Extensive\nexperiments and ablations demonstrate that DISTAR surpasses state-of-the-art\nzero-shot TTS systems in robustness, naturalness, and speaker/style\nconsistency, while maintaining rich output diversity. Audio samples are\nprovided on https://anonymous.4open.science/w/DiSTAR_demo."}
{"id": "2510.12326", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12326", "abs": "https://arxiv.org/abs/2510.12326", "authors": ["Guanxin Jiang", "Andreas Brendel", "Pablo M. Delgado", "Jürgen Herre"], "title": "DeePAQ: A Perceptual Audio Quality Metric Based On Foundational Models and Weakly Supervised Learning", "comment": "5 pages, 2 figures", "summary": "This paper presents the Deep learning-based Perceptual Audio Quality metric\n(DeePAQ) for evaluating general audio quality. Our approach leverages metric\nlearning together with the music foundation model MERT, guided by surrogate\nlabels, to construct an embedding space that captures distortion intensity in\ngeneral audio. To the best of our knowledge, DeePAQ is the first in the general\naudio quality domain to leverage weakly supervised labels and metric learning\nfor fine-tuning a music foundation model with Low-Rank Adaptation (LoRA), a\ndirection not yet explored by other state-of-the-art methods. We benchmark the\nproposed model against state-of-the-art objective audio quality metrics across\nlistening tests spanning audio coding and source separation. Results show that\nour method surpasses existing metrics in detecting coding artifacts and\ngeneralizes well to unseen distortions such as source separation, highlighting\nits robustness and versatility."}
{"id": "2510.12377", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12377", "abs": "https://arxiv.org/abs/2510.12377", "authors": ["Klaus Linhard", "Philipp Bulling"], "title": "A Phase Synthesizer for Decorrelation to Improve Acoustic Feedback Cancellation", "comment": null, "summary": "Undesired acoustic feedback is a known issue in communication systems, such\nas speech in-car communication, public address systems, or hearing aids.\nWithout additional precautions, there is a high risk that the adaptive filter -\nintended to cancel the feedback path - also suppresses parts of the desired\nsignal. One solution is to decorrelate the loudspeaker and microphone signals.\nIn this work, we combine the two decorrelation approaches frequency shifting\nand phase modulation in a unified framework: a so-called \\textit{phase\nsynthesizer}, implemented in a discrete Fourier transform (DFT) filter bank.\nFurthermore, we extend the phase modulation technique using variable delay\nlines, as known from vibrato and chorus effects. We demonstrate the benefits of\nthe proposed phase synthesizer using an example from speech in-car\ncommunication, employing an adaptive frequency-domain Kalman filter.\nImprovements in system stability, speech quality measured by perceptual\nevaluation of speech quality (PESQ) are presented."}
{"id": "2510.11867", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11867", "abs": "https://arxiv.org/abs/2510.11867", "authors": ["Zelin Gan", "Henrique Buglia", "Romulo Aparecido", "Mindaugas Jarmolovičius", "Eric Sillekens", "Jiaqian Yang", "Ronit Sohanpal", "Robert I. Killey", "Polina Bayvel"], "title": "A Closed-form Expression of the Gaussian Noise Model Supporting O-Band Transmission", "comment": "13 pages, 10 figures", "summary": "We present a novel closed-form model for nonlinear interference (NLI)\nestimation in low-dispersion O-band transmission systems. The formulation\nincorporates the four-wave mixing (FWM) efficiency term as well as the coherent\ncontributions of self- and cross-phase modulation (SPM/XPM) across multiple\nidentical spans. This extension enables accurate evaluation of the NLI in\nscenarios where conventional closed-form Gaussian Noise (GN) models are\nlimited. The proposed model is validated against split-step Fourier method\n(SSFM) simulations and numerical integration across 41-161 channels, with a 96\nGBaud symbol rate, bandwidths of up to 16.1 THz, and transmission distances\nfrom 80 to 800 km. Results show a mean absolute error of the NLI\nsignal-to-noise ratio (SNR) below 0.22 dB. The proposed closed-form model\noffers an efficient and accurate tool for system optimisation in O-band\ncoherent transmission."}
{"id": "2510.11732", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11732", "abs": "https://arxiv.org/abs/2510.11732", "authors": ["Guojian Li", "Qijie Shao", "Zhixian Zhao", "Shuiyuan Wang", "Zhonghua Fu", "Lei Xie"], "title": "Serial-Parallel Dual-Path Architecture for Speaking Style Recognition", "comment": "Accepted by NCMMSC2025", "summary": "Speaking Style Recognition (SSR) identifies a speaker's speaking style\ncharacteristics from speech. Existing style recognition approaches primarily\nrely on linguistic information, with limited integration of acoustic\ninformation, which restricts recognition accuracy improvements. The fusion of\nacoustic and linguistic modalities offers significant potential to enhance\nrecognition performance. In this paper, we propose a novel serial-parallel\ndual-path architecture for SSR that leverages acoustic-linguistic bimodal\ninformation. The serial path follows the ASR+STYLE serial paradigm, reflecting\na sequential temporal dependency, while the parallel path integrates our\ndesigned Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal\ninteraction with temporal simultaneity. Compared to the existing SSR baseline\n-- the OSUM model, our approach reduces parameter size by 88.4% and achieves a\n30.3% improvement in SSR accuracy for eight styles on the test set."}
{"id": "2510.12485", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12485", "abs": "https://arxiv.org/abs/2510.12485", "authors": ["Jiatong Li", "Simon Doclo"], "title": "I-DCCRN-VAE: An Improved Deep Representation Learning Framework for Complex VAE-based Single-channel Speech Enhancement", "comment": null, "summary": "Recently, a complex variational autoencoder (VAE)-based single-channel speech\nenhancement system based on the DCCRN architecture has been proposed. In this\nsystem, a noise suppression VAE (NSVAE) learns to extract clean speech\nrepresentations from noisy speech using pretrained clean speech and noise VAEs\nwith skip connections. In this paper, we improve DCCRN-VAE by incorporating\nthree key modifications: 1) removing the skip connections in the pretrained\nVAEs to encourage more informative speech and noise latent representations; 2)\nusing $\\beta$-VAE in pretraining to better balance reconstruction and latent\nspace regularization; and 3) a NSVAE generating both speech and noise latent\nrepresentations. Experiments show that the proposed system achieves comparable\nperformance as the DCCRN and DCCRN-VAE baselines on the matched DNS3 dataset\nbut outperforms the baselines on mismatched datasets (WSJ0-QUT,\nVoicebank-DEMEND), demonstrating improved generalization ability. In addition,\nan ablation study shows that a similar performance can be achieved with\nclassical fine-tuning instead of adversarial training, resulting in a simpler\ntraining pipeline."}
{"id": "2510.11891", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11891", "abs": "https://arxiv.org/abs/2510.11891", "authors": ["Haoran He"], "title": "Based on Deep Neural Networks: A Machine Learning-Assisted Channel Estimation Method for MIMO Systems", "comment": "4 pages, 8 figures, ISCIPT 2025", "summary": "This paper proposes a machine learning-assisted channel estimation approach\nfor massive MIMO systems, leveraging DNNs to outperform traditional LS and MMSE\nmethods. In 5G and beyond, accurate channel estimation mitigates pilot\ncontamination and high mobility issues that harm system reliability. The\nproposed DNN architecture includes multi-layer perceptrons with ReLU\nactivation, 3 hidden layers (256, 128, 64 neurons respectively), uses Adam\noptimizer (learning rate 1e-4) and MSE loss function. It learns from pilot\nsignals to predict channel matrices, achieving lower NMSE and BER across\ndifferent SNR levels. Simulations use the COST 2100 public standard dataset (a\nwell-recognized MIMO channel dataset for 5G, not synthetic datasets) with\n10,000 samples of 4x4 MIMO channels under urban macro scenarios. Results show\nthe DNN outperforms LS and MMSE by 3-5 dB in NMSE at medium SNR, with robust\nperformance in high-mobility scenarios. The study evaluates metrics like NMSE\nvs. SNR, BER vs. SNR, and sensitivity to pilot length, antenna configurations,\nand computational complexity. The DNN has 2.3 GFlOPs computational complexity,\n15.6k parameters, and 1.8 ms inference time on Raspberry Pi 4, verifying\ndeployment feasibility. This work advances ML integration in wireless\ncommunications, facilitating efficient resource allocation and improved\nspectral efficiency in next-generation networks. Future work may use more\nreal-world datasets and hybrid architectures for better generalization."}
{"id": "2510.11738", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.11738", "abs": "https://arxiv.org/abs/2510.11738", "authors": ["Simone Carnemolla", "Matteo Pennisi", "Chiara Russo", "Simone Palazzo", "Daniela Giordano", "Concetto Spampinato"], "title": "SeeingSounds: Learning Audio-to-Visual Alignment via Text", "comment": "accepted to ACM Multimedia Asia 2025", "summary": "We introduce SeeingSounds, a lightweight and modular framework for\naudio-to-image generation that leverages the interplay between audio, language,\nand vision-without requiring any paired audio-visual data or training on visual\ngenerative models. Rather than treating audio as a substitute for text or\nrelying solely on audio-to-text mappings, our method performs dual alignment:\naudio is projected into a semantic language space via a frozen language\nencoder, and, contextually grounded into the visual domain using a\nvision-language model. This approach, inspired by cognitive neuroscience,\nreflects the natural cross-modal associations observed in human perception. The\nmodel operates on frozen diffusion backbones and trains only lightweight\nadapters, enabling efficient and scalable learning. Moreover, it supports\nfine-grained and interpretable control through procedural text prompt\ngeneration, where audio transformations (e.g., volume or pitch shifts)\ntranslate into descriptive prompts (e.g., \"a distant thunder\") that guide\nvisual outputs. Extensive experiments across standard benchmarks confirm that\nSeeingSounds outperforms existing methods in both zero-shot and supervised\nsettings, establishing a new state of the art in controllable audio-to-visual\ngeneration."}
{"id": "2510.11732", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.11732", "abs": "https://arxiv.org/abs/2510.11732", "authors": ["Guojian Li", "Qijie Shao", "Zhixian Zhao", "Shuiyuan Wang", "Zhonghua Fu", "Lei Xie"], "title": "Serial-Parallel Dual-Path Architecture for Speaking Style Recognition", "comment": "Accepted by NCMMSC2025", "summary": "Speaking Style Recognition (SSR) identifies a speaker's speaking style\ncharacteristics from speech. Existing style recognition approaches primarily\nrely on linguistic information, with limited integration of acoustic\ninformation, which restricts recognition accuracy improvements. The fusion of\nacoustic and linguistic modalities offers significant potential to enhance\nrecognition performance. In this paper, we propose a novel serial-parallel\ndual-path architecture for SSR that leverages acoustic-linguistic bimodal\ninformation. The serial path follows the ASR+STYLE serial paradigm, reflecting\na sequential temporal dependency, while the parallel path integrates our\ndesigned Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal\ninteraction with temporal simultaneity. Compared to the existing SSR baseline\n-- the OSUM model, our approach reduces parameter size by 88.4% and achieves a\n30.3% improvement in SSR accuracy for eight styles on the test set."}
{"id": "2510.11925", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11925", "abs": "https://arxiv.org/abs/2510.11925", "authors": ["Yanan Du", "Zeyang Sun", "Yilan Zhang", "Sai Xu", "Beiyuan Liu"], "title": "Using STAR-IRS to Secure Indoor Communications Through Symbol-Level Random Phase Modulation", "comment": null, "summary": "This paper proposes a secure indoor communication scheme based on\nsimultaneous transmitting and reflecting intelligent reflecting surface\n(STAR-IRS). Specifically, a transmitter (Alice) sends confidential information\nto its intended user (Bob) indoors, while several eavesdroppers (Eves) lurk\noutside. To safeguard the transmission from eavesdropping, the STAR-IRS is\ndeployed on walls or windows. Upon impinging on the STAR-IRS, the incoming\nelectromagnetic wave is dynamically partitioned into two components, enabling\nboth transmission through and reflection from the surface. The reflected signal\nis controlled to enhance reception at Bob, while the transmitted signal is\nmodulated with symbol-level random phase shifts to degrade the signal quality\nat Eves. Based on such a setting, the secrecy rate maximization problem is\nformulated. To solve it, a graph neural network (GNN)-based scheme is\ndeveloped. Furthermore, a field-programmable gate array (FPGA)-based GNN\naccelerator is designed to reduce computational latency. Simulation results\ndemonstrate that the proposed strategy outperforms both the conventional scheme\nand the reflection-only scheme in terms of secrecy performance. Moreover, the\nGNN-based approach achieves superior results compared to benchmark techniques\nsuch as maximum ratio transmission (MRT), zero forcing (ZF), and minimum mean\nsquare error (MMSE) in solving the optimization problem. Finally, experimental\nevaluations confirm that the FPGA-based accelerator enables low inference\nlatency."}
{"id": "2510.11760", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.11760", "abs": "https://arxiv.org/abs/2510.11760", "authors": ["Yi Wang", "Yinfeng Yu", "Fuchun Sun", "Liejun Wang", "Wendong Zheng"], "title": "Audio-Guided Visual Perception for Audio-Visual Navigation", "comment": "Main paper (6 pages). Accepted for publication by International\n  Conference on Virtual Reality and Visualization 2025 (ICVRV 2025)", "summary": "Audio-Visual Embodied Navigation aims to enable agents to autonomously\nnavigate to sound sources in unknown 3D environments using auditory cues. While\ncurrent AVN methods excel on in-distribution sound sources, they exhibit poor\ncross-source generalization: navigation success rates plummet and search paths\nbecome excessively long when agents encounter unheard sounds or unseen\nenvironments. This limitation stems from the lack of explicit alignment\nmechanisms between auditory signals and corresponding visual regions. Policies\ntend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations\nduring training, leading to blind exploration when exposed to novel sound\nsources. To address this, we propose the AGVP framework, which transforms sound\nfrom policy-memorable acoustic fingerprint cues into spatial guidance. The\nframework first extracts global auditory context via audio self-attention, then\nuses this context as queries to guide visual feature attention, highlighting\nsound-source-related regions at the feature level. Subsequent temporal modeling\nand policy optimization are then performed. This design, centered on\ninterpretable cross-modal alignment and region reweighting, reduces dependency\non specific acoustic fingerprints. Experimental results demonstrate that AGVP\nimproves both navigation efficiency and robustness while achieving superior\ncross-scenario generalization on previously unheard sounds."}
{"id": "2510.12175", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12175", "abs": "https://arxiv.org/abs/2510.12175", "authors": ["Junnuo Wang"], "title": "Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis", "comment": "Accepted for publication in the Journal of Artificial Intelligence\n  Research (JAIR), Vol. 3 No. 2, December 2025", "summary": "Recent advances in diffusion-based generative models have enabled\nhigh-quality text-to-audio synthesis, but fine-grained acoustic control remains\na significant challenge in open-source research. We present Audio Palette, a\ndiffusion transformer (DiT) based model that extends the Stable Audio Open\narchitecture to address this \"control gap\" in controllable audio generation.\nUnlike prior approaches that rely solely on semantic conditioning, Audio\nPalette introduces four time-varying control signals: loudness, pitch, spectral\ncentroid, and timbre, for precise and interpretable manipulation of acoustic\nfeatures. The model is efficiently adapted for the nuanced domain of Foley\nsynthesis using Low-Rank Adaptation (LoRA) on a curated subset of AudioSet,\nrequiring only 0.85 percent of the original parameters to be trained.\nExperiments demonstrate that Audio Palette achieves fine-grained, interpretable\ncontrol of sound attributes. Crucially, it accomplishes this novel\ncontrollability while maintaining high audio quality and strong semantic\nalignment to text prompts, with performance on standard metrics such as Frechet\nAudio Distance (FAD) and LAION-CLAP scores remaining comparable to the original\nbaseline model. We provide a scalable, modular pipeline for audio research,\nemphasizing sequence-based conditioning, memory efficiency, and a three-scale\nclassifier-free guidance mechanism for nuanced inference-time control. This\nwork establishes a robust foundation for controllable sound design and\nperformative audio synthesis in open-source settings, enabling a more\nartist-centric workflow."}
{"id": "2510.11994", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.11994", "abs": "https://arxiv.org/abs/2510.11994", "authors": ["Yinan Wang", "Byeongjin Kim", "Nishanth Ravi", "Kapil Saha", "Supratik Dasgupta", "Vakhtang Chulukhadze", "Eugene Kwon", "Lezli Matto", "Pietro Simeoni", "Omar Barrera", "Ian Anderson", "Tzu-Hsuan Hsu", "Jue Hou", "Matteo Rinaldi", "Mark S. Goorsky", "Ruochen Lu"], "title": "62.6 GHz ScAlN Solidly Mounted Acoustic Resonators", "comment": "6 Pages, 7 Figures, 3 Tables", "summary": "We demonstrate a record-high 62.6 GHz solidly mounted acoustic resonator\n(SMR) incorporating a 67.6 nm scandium aluminum nitride (Sc0.3Al0.7N)\npiezoelectric layer on a 40 nm buried platinum (Pt) bottom electrode,\npositioned above an acoustic Bragg reflector composed of alternating SiO2 (28.2\nnm) and Ta2O5 (24.3 nm) layers in 8.5 pairs. The Bragg reflector and\npiezoelectric stack above are designed to confine a third-order\nthickness-extensional (TE) bulk acoustic wave (BAW) mode, while efficiently\ntransducing with thickness-field excitation. The fabricated SMR exhibits an\nextracted piezoelectric coupling coefficient (k2) of 0.8% and a maximum Bode\nquality factor (Q) of 51 at 63 GHz, representing the highest operating\nfrequency reported for an SMR to date. These results establish a pathway toward\nmmWave SMR devices for filters and resonators in next-generation RF front ends."}
{"id": "2510.12000", "categories": ["cs.SD", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12000", "abs": "https://arxiv.org/abs/2510.12000", "authors": ["Jinchuan Tian", "Sang-gil Lee", "Zhifeng Kong", "Sreyan Ghosh", "Arushi Goel", "Chao-Han Huck Yang", "Wenliang Dai", "Zihan Liu", "Hanrong Ye", "Shinji Watanabe", "Mohammad Shoeybi", "Bryan Catanzaro", "Rafael Valle", "Wei Ping"], "title": "UALM: Unified Audio Language Model for Understanding, Generation and Reasoning", "comment": null, "summary": "Recent advances in the audio language modeling (ALM) domain tackle audio\nunderstanding and text-to-audio generation as separate tasks. Very few studies\nattempt to unify these tasks -- an essential step toward advanced multimodal\nreasoning. This paper introduces U}nified Audio Language Model (UALM), which\naims to unify audio understanding, text-to-audio generation, and multimodal\nreasoning in a single model. To achieve this goal, we first present UALM-Gen, a\ntext-to-audio language model that directly predicts audio tokens and is\ncomparable to state-of-the-art diffusion-based models. We then demonstrate,\nusing proper data blending, training recipes, and inference techniques, that\nour single UALM model matches the quality of state-of-the-art specialized\nmodels in audio understanding, text-to-audio generation, and text reasoning.\nFurthermore, we present UALM-Reason, a multimodal reasoning model that utilizes\nboth text and audio in the intermediate thinking steps to facilitate complex\ngeneration tasks. To our knowledge, this is the first demonstration in audio\nresearch of cross-modal generative reasoning, with its effectiveness confirmed\nby subjective evaluations."}
{"id": "2510.12179", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12179", "abs": "https://arxiv.org/abs/2510.12179", "authors": ["Abdullahi Mohammad", "Bdah Eya", "Bassant Selim"], "title": "A Deep Multi-Task Learning Approach to Impulsive Noise Parameter Estimation", "comment": "6, 5", "summary": "Impulsive noise poses a significant challenge to the reliability of wireless\ncommunication systems, necessitating accurate estimation of its statistical\nparameters for effective mitigation. This paper introduces a multitask learning\n(MTL) framework based on a CNN-LSTM architecture enhanced with an attention\nmechanism for the joint estimation of impulsive noise parameters. The proposed\nmodel leverages a unified weighted-loss function to enable simultaneous\nlearning of multiple parameters within a shared representation space, improving\nlearning efficiency and generalization across related tasks. Experimental\nresults show that the proposed MTL framework achieves stable convergence,\nfaster training, and enhanced scalability with modest computational overhead.\nBenchmarking against conventional single-task learning (STL) models confirms\nits favorable complexity-performance trade-off and significant memory savings,\nindicating the effectiveness of the MTL approach for real-time impulsive noise\nparameter estimation in wireless systems."}
{"id": "2510.12175", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12175", "abs": "https://arxiv.org/abs/2510.12175", "authors": ["Junnuo Wang"], "title": "Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis", "comment": "Accepted for publication in the Journal of Artificial Intelligence\n  Research (JAIR), Vol. 3 No. 2, December 2025", "summary": "Recent advances in diffusion-based generative models have enabled\nhigh-quality text-to-audio synthesis, but fine-grained acoustic control remains\na significant challenge in open-source research. We present Audio Palette, a\ndiffusion transformer (DiT) based model that extends the Stable Audio Open\narchitecture to address this \"control gap\" in controllable audio generation.\nUnlike prior approaches that rely solely on semantic conditioning, Audio\nPalette introduces four time-varying control signals: loudness, pitch, spectral\ncentroid, and timbre, for precise and interpretable manipulation of acoustic\nfeatures. The model is efficiently adapted for the nuanced domain of Foley\nsynthesis using Low-Rank Adaptation (LoRA) on a curated subset of AudioSet,\nrequiring only 0.85 percent of the original parameters to be trained.\nExperiments demonstrate that Audio Palette achieves fine-grained, interpretable\ncontrol of sound attributes. Crucially, it accomplishes this novel\ncontrollability while maintaining high audio quality and strong semantic\nalignment to text prompts, with performance on standard metrics such as Frechet\nAudio Distance (FAD) and LAION-CLAP scores remaining comparable to the original\nbaseline model. We provide a scalable, modular pipeline for audio research,\nemphasizing sequence-based conditioning, memory efficiency, and a three-scale\nclassifier-free guidance mechanism for nuanced inference-time control. This\nwork establishes a robust foundation for controllable sound design and\nperformative audio synthesis in open-source settings, enabling a more\nartist-centric workflow."}
{"id": "2510.12204", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12204", "abs": "https://arxiv.org/abs/2510.12204", "authors": ["Zhen Du", "Jingjing Xu", "Yifeng Xiong", "Jie Wang", "Musa Furkan Keskin", "Henk Wymeersch", "Fan Liu", "Shi Jin"], "title": "Probabilistic Constellation Shaping for OFDM ISAC Signals Under Temporal-Frequency Filtering", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Integrated sensing and communications (ISAC) is considered an innovative\ntechnology in sixth-generation (6G) wireless networks, where utilizing\northogonal frequency division multiplexing (OFDM) communication signals for\nsensing provides a cost-effective solution for implementing ISAC. However, the\nsensing performance of matched and mismatched filtering schemes can be\nsignificantly deteriorated due to the signaling randomness induced by\nfinite-alphabet modulations with nonconstant modulus, such as quadrature\namplitude modulation (QAM) constellations. Therefore, improving sensing\nperformance without significantly compromising communication capability (i.e.,\nmaintaining randomness), remains a challenging task. To that end, we propose a\nunified probabilistic constellation shaping (PCS) framework that is compatible\nwith both matched and mismatched filtering schemes, by maximizing the\ncommunication rate while imposing constraints on mean square error (MSE) of\nsensing channel state information (CSI), power, and probability distribution.\nSpecifically, the MSE of sensing CSI is leveraged to optimize sensing\ncapability, which is illustrated to be a more comprehensive metric compared to\nthe output SNR after filtering (SNRout) and integrated sidelobes ratio (ISLR).\nAdditionally, the internal relationships among these three sensing metrics are\nexplicitly analyzed. Finally, both simulations and field measurements validate\nthe efficiency of proposed PCS approach in achieving a flexible S&C trade-off,\nas well as its credibility in enhancing 6G wireless transmission in real-world\nscenarios."}
{"id": "2510.12275", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12275", "abs": "https://arxiv.org/abs/2510.12275", "authors": ["Youhao Si", "Yuan Liao", "Qiushi Han", "Yuhang Yang", "Rui Dai", "Liya Huang"], "title": "TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction", "comment": "5 pages, 3 figures", "summary": "The rapid development of auditory attention decoding (AAD) based on\nelectroencephalography (EEG) signals offers the possibility EEG-driven target\nspeaker extraction. However, how to effectively utilize the target-speaker\ncommon information between EEG and speech remains an unresolved problem. In\nthis paper, we propose a model for brain-controlled speaker extraction, which\nutilizes the EEG recorded from the listener to extract the target speech. In\norder to effectively extract information from EEG signals, we derive\nmulti-scale time--frequency features and further incorporate cortical\ntopological structures that are selectively engaged during the task. Moreover,\nto effectively exploit the non-Euclidean structure of EEG signals and capture\ntheir global features, the graph convolutional networks and self-attention\nmechanism are used in the EEG encoder. In addition, to make full use of the\nfused EEG and speech feature and preserve global context and capture speech\nrhythm and prosody, we introduce MossFormer2 which combines MossFormer and\nRNN-Free Recurrent as separator. Experimental results on both the public\nCocktail Party and KUL dataset in this paper show that our TFGA-Net model\nsignificantly outper-forms the state-of-the-art method in certain objective\nevaluation metrics. The source code is available at:\nhttps://github.com/LaoDa-X/TFGA-NET."}
{"id": "2510.12279", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12279", "abs": "https://arxiv.org/abs/2510.12279", "authors": ["Benedikt Böck", "Amar Kasibovic", "Wolfgang Utschick"], "title": "Wireless Channel Modeling for Machine Learning -- A Critical View on Standardized Channel Models", "comment": null, "summary": "Standardized (link-level) channel models such as the 3GPP TDL and CDL models\nare frequently used to evaluate machine learning (ML)-based physical-layer\nmethods. However, in this work, we argue that a link-level perspective\nincorporates limiting assumptions, causing unwanted distributional shifts or\nnecessitating impractical online training. An additional drawback is that this\nperspective leads to (near-)Gaussian channel characteristics. Thus, ML-based\nmodels, trained on link-level channel data, do not outperform classical\napproaches for a variety of physical-layer applications. Particularly, we\ndemonstrate the optimality of simple linear methods for channel compression,\nestimation, and modeling, revealing the unsuitability of link-level channel\nmodels for evaluating ML models. On the upside, adopting a scenario-level\nperspective offers a solution to this problem and unlocks the relative gains\nenabled by ML."}
{"id": "2510.12780", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12780", "abs": "https://arxiv.org/abs/2510.12780", "authors": ["Cristina Aggazzotti", "Ashi Garg", "Zexin Cai", "Nicholas Andrews"], "title": "Content Anonymization for Privacy in Long-form Audio", "comment": null, "summary": "Voice anonymization techniques have been found to successfully obscure a\nspeaker's acoustic identity in short, isolated utterances in benchmarks such as\nthe VoicePrivacy Challenge. In practice, however, utterances seldom occur in\nisolation: long-form audio is commonplace in domains such as interviews, phone\ncalls, and meetings. In these cases, many utterances from the same speaker are\navailable, which pose a significantly greater privacy risk: given multiple\nutterances from the same speaker, an attacker could exploit an individual's\nvocabulary, syntax, and turns of phrase to re-identify them, even when their\nvoice is completely disguised. To address this risk, we propose new content\nanonymization approaches. Our approach performs a contextual rewriting of the\ntranscripts in an ASR-TTS pipeline to eliminate speaker-specific style while\npreserving meaning. We present results in a long-form telephone conversation\nsetting demonstrating the effectiveness of a content-based attack on\nvoice-anonymized speech. Then we show how the proposed content-based\nanonymization methods can mitigate this risk while preserving speech utility.\nOverall, we find that paraphrasing is an effective defense against\ncontent-based attacks and recommend that stakeholders adopt this step to ensure\nanonymity in long-form audio."}
{"id": "2510.12315", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12315", "abs": "https://arxiv.org/abs/2510.12315", "authors": ["Piyush Priyanshu", "Sudhan Majhi", "Subhabrata Paul"], "title": "A New Method of Constructing Hadamard Matrices, Circulant Hadamard Matrices, CZCS, GCS, CCC, and CZCSS", "comment": null, "summary": "A Hadamard matrix $H$ is a square matrix of order $n$ with entries $\\pm 1$,\nsuch that $HH^\\top=nI_{n}$, where $I_n$ is an identity matrix of order $n$. A\ncirculant Hadamard matrix $H$ is a Hadamard matrix that has rows of entries in\ncyclic order. There exist only $8$ circulant Hadamard matrices of order 4, and\nhere, we provide a novel construction of all such $8$ circulant Hadamard\nmatrices using a linear operator and generalized Boolean function (GBF). The\nconstructed circulant Hadamard matrices are used recursively to construct a\nbinary cross Z-complementary set (CZCS) of all lengths with an even phase, a\nbinary Golay complementary set (GCS) of all lengths, and Hadamard matrices of\norder $2^{n+2}$, where $n\\geq1$. The construction of a binary CZCS covering all\nlengths was not available before. We also propose an alternative,\nlower-complexity construction of binary GCSs of all lengths and Hadamard\nmatrices of order $2^{a+1}10^b26^c$ using circulant matrices, where $ a,b,c\n\\geq 0$. The proposed binary GCS covers all lengths with a flexible flock size.\nThe constructions of GCS are further extended to form binary complete\ncomplementary code (CCC) of the parameter $(2N,2N,2N)-CCC$ where\n$N=2^a10^b26^c, a,b,c \\geq 0$. The constructed binary CCC provides a flexible\nflock size. The construction of CZCS is further extended to form a binary\noptimal cross-Z complementary sequence set (CZCSS) of the parameter $(2^{n+2},\n2^{n+2}, 2^{n+2}, 2^{n+1})-CZCSS$, where $n\\geq1$. Finally, we provide a\nrelation between Hadamard matrices and GCS, which enables the study of the\nHadamard conjecture in a new direction. We also provided a few properties of\ncirculant matrices over aperiodic cross-correlation (ACCF) and aperiodic\nauto-correlation (AACF), which are used to prove the theorems. All proposed\nconstructions are novel, and their parameters are compared with the existing\nstate-of-the-art."}
{"id": "2510.12366", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.12366", "abs": "https://arxiv.org/abs/2510.12366", "authors": ["Zheyu Wu", "Matteo Nerini", "Bruno Clerckx"], "title": "Beyond-Diagonal RIS Architecture Design and Optimization under Physics-Consistent Models", "comment": "13 pages, 5 figures, submitted for possible publication", "summary": "Reconfigurable intelligent surface (RIS) is a promising technology for future\nwireless communication systems. Conventional RIS is constrained to a diagonal\nscattering matrix, which limits its flexibility. Recently, beyond-diagonal RIS\n(BD-RIS) has been proposed as a more general RIS architecture class that allows\ninter-element connections and shows great potential for performance\nimprovement. Despite extensive progress on BD-RIS, most existing studies rely\non simplified channel models that ignore practical electromagnetic (EM) effects\nsuch as mutual coupling and impedance mismatching. To address this gap, this\npaper investigates the architecture design and optimization of BD-RIS under the\ngeneral physics-consistent model derived with multiport network theory in\nrecent literature. Building on a compact reformulation of this model, we show\nthat band-connected RIS achieves the same channel-shaping capability as\nfully-connected RIS, which extends existing results obtained for conventional\nchannel models. We then develop optimization methods under the general\nphysics-consistent model; specifically, we derive closed-form solutions for\nsingle-input single-output (SISO) systems, propose a globally optimal\nsemidefinite relaxation (SDR)-based algorithm for single-stream multi-input\nmulti-output (MIMO) systems, and design an efficient alternating direction\nmethod of multipliers (ADMM)-based algorithm for multiuser MIMO systems. Using\nthe proposed algorithms, we conduct comprehensive simulations to evaluate the\nimpact of various EM effects and approximations, including mutual coupling\namong RIS antennas and the commonly adopted unilateral approximation, on system\nperformance."}
{"id": "2510.12515", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12515", "abs": "https://arxiv.org/abs/2510.12515", "authors": ["Zhige Chen", "Chengxuan Qin", "Wenlong You", "Rui Liu", "Congying Chu", "Rui Yang", "Kay Chen Tan", "Jibin Wu"], "title": "HEAR: An EEG Foundation Model with Heterogeneous Electrode Adaptive Representation", "comment": null, "summary": "Electroencephalography (EEG) is an essential technique for neuroscience\nresearch and brain-computer interface (BCI) applications. Recently, large-scale\nEEG foundation models have been developed, exhibiting robust generalization\ncapabilities across diverse tasks and subjects. However, the heterogeneity of\nEEG devices not only hinders the widespread adoption of these models but also\nposes significant challenges to their further scaling and development. In this\npaper, we introduce HEAR, the first EEG foundation model explicitly designed to\nsupport heterogeneous EEG devices, accommodating varying electrode layouts and\nelectrode counts. HEAR employs a learnable, coordinate-based spatial embedding\nto map electrodes with diverse layouts and varying counts into a unified\nrepresentational space. This unified spatial representation is then processed\nby a novel spatially-guided transformer, which effectively captures\nspatiotemporal dependencies across electrodes. To support the development of\nHEAR, we construct a large-scale EEG dataset comprising 8,782 hours of data\ncollected from over 150 distinct electrode layouts with up to 1,132 electrodes.\nExperimental results demonstrate that HEAR substantially outperforms existing\nEEG foundation models in supporting heterogeneous EEG devices and generalizing\nacross diverse cognitive tasks and subjects."}
{"id": "2510.12648", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12648", "abs": "https://arxiv.org/abs/2510.12648", "authors": ["Abdelali Arous", "Hamza Haif", "Arman Farhang", "Huseyin Arslan"], "title": "A Unified Framework for Adaptive Waveform Processing in Next Generation Wireless Networks", "comment": null, "summary": "The emergence of alternative multiplexing domains to the time-frequency\ndomains, e.g., the delay-Doppler and chirp domains, offers a promising approach\nfor addressing the challenges posed by complex propagation environments and\nnext-generation applications. Unlike the time and frequency domains, these\ndomains offer unique channel representations which provide additional degrees\nof freedom (DoF) for modeling, characterizing, and exploiting wireless channel\nfeatures. This article provides a comprehensive analysis of channel\ncharacteristics, including delay, Doppler shifts, and channel coefficients\nacross various domains, with an emphasis on their inter-domain relationships,\nshared characteristics, and domain-specific distinctions. We further evaluate\nthe comparative advantages of each domain under specific channel conditions.\nBuilding on this analysis, we propose a generalized and adaptive transform\ndomain framework that leverages the pre- and post-processing of the discrete\nFourier transform (DFT) matrix, to enable dynamic transitions between various\ndomains in response to the channel conditions and system requirements. Finally,\nseveral representative use cases are presented to demonstrate the applicability\nof the proposed cross-domain waveform processing framework in diverse\nscenarios, along with future directions and challenges."}
{"id": "2510.12651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12651", "abs": "https://arxiv.org/abs/2510.12651", "authors": ["Axel Janson", "Joakim Andén"], "title": "Moment-based Posterior Sampling for Multi-reference Alignment", "comment": null, "summary": "We propose a Bayesian approach to the problem of multi-reference alignment --\nthe recovery of signals from noisy, randomly shifted observations. While\nexisting frequentist methods accurately recover the signal at arbitrarily low\nsignal-to-noise ratios, they require a large number of samples to do so. In\ncontrast, our proposed method leverages diffusion models as data-driven\nplug-and-play priors, conditioning these on the sample power spectrum (a\nshift-invariant statistic) enabling both accurate posterior sampling and\nuncertainty quantification. The use of an appropriate prior significantly\nreduces the required number of samples, as illustrated in simulation\nexperiments with comparisons to state-of-the-art methods such as\nexpectation--maximization and bispectrum inversion. These findings establish\nour approach as a promising framework for other orbit recovery problems, such\nas cryogenic electron microscopy (cryo-EM)."}
{"id": "2510.12711", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12711", "abs": "https://arxiv.org/abs/2510.12711", "authors": ["Muhammad Talha", "Besma Smida", "David González G"], "title": "Enhanced Angle-Range Cluster Parameter Estimation in Full-Duplex ISAC Systems", "comment": "8 pages, 5 figures", "summary": "This work studies an integrated sensing and communication (ISAC) framework\nfor targets that are spread both in the angle and range domains. We model each\ntarget using a cluster of rays parameterized by a specific density function,\nand propose a truncated Multiple Signal Classification (MUSIC) spread (TMS)\nalgorithm to accurately estimate the parameters of the density function. Unlike\nthe conventional MUSIC spread (CMS), TMS restricts the signal subspace rank\nbased on the eigen decomposition of the received-signal autocorrelation. We\nalso propose a discrete Fourier transform (DFT) based algorithm for estimating\nthe distance and range spread of each target. Leveraging these estimates, we\nthen develop a dynamic transmit beamforming algorithm that successfully\nilluminates multiple targets while also serving multiple downlink (DL) users.\nSimulation results demonstrate the superiority of our proposed algorithms over\nbaseline schemes in both low and high signal-to-noise ratio (SNR) regimes as\nwell as under a wide angular spread regime."}
{"id": "2510.12763", "categories": ["eess.SP", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.12763", "abs": "https://arxiv.org/abs/2510.12763", "authors": ["Saurabh Sihag", "Gonzalo Mateos", "Alejandro Ribeiro"], "title": "Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective", "comment": "Accepted for publication in IEEE Signal Processing Magazine", "summary": "Neurodegeneration, characterized by the progressive loss of neuronal\nstructure or function, is commonly assessed in clinical practice through\nreductions in cortical thickness or brain volume, as visualized by structural\nMRI. While informative, these conventional approaches lack the statistical\nsophistication required to fully capture the spatially correlated and\nheterogeneous nature of neurodegeneration, which manifests both in healthy\naging and in neurological disorders. To address these limitations, brain age\ngap has emerged as a promising data-driven biomarker of brain health. The brain\nage gap prediction (BAGP) models estimate the difference between a person's\npredicted brain age from neuroimaging data and their chronological age. The\nresulting brain age gap serves as a compact biomarker of brain health, with\nrecent studies demonstrating its predictive utility for disease progression and\nseverity. However, practical adoption of BAGP models is hindered by their\nmethodological obscurities and limited generalizability across diverse clinical\npopulations. This tutorial article provides an overview of BAGP and introduces\na principled framework for this application based on recent advancements in\ngraph signal processing (GSP). In particular, we focus on graph neural networks\n(GNNs) and introduce the coVariance neural network (VNN), which leverages the\nanatomical covariance matrices derived from structural MRI. VNNs offer strong\ntheoretical grounding and operational interpretability, enabling robust\nestimation of brain age gap predictions. By integrating perspectives from GSP,\nmachine learning, and network neuroscience, this work clarifies the path\nforward for reliable and interpretable BAGP models and outlines future research\ndirections in personalized medicine."}

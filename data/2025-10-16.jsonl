{"id": "2510.12827", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12827", "abs": "https://arxiv.org/abs/2510.12827", "authors": ["Md. Nayeem", "Md Shamse Tabrej", "Kabbojit Jit Deb", "Shaonti Goswami", "Md. Azizul Hakim"], "title": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation", "comment": null, "summary": "Automatic Speech Recognition (ASR) has undergone a profound transformation\nover the past decade, driven by advances in deep learning. This survey provides\na comprehensive overview of the modern era of ASR, charting its evolution from\ntraditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models\n(GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant\nend-to-end neural architectures. We systematically review the foundational\nend-to-end paradigms: Connectionist Temporal Classification (CTC),\nattention-based encoder-decoder models, and the Recurrent Neural Network\nTransducer (RNN-T), which established the groundwork for fully integrated\nspeech-to-text systems. We then detail the subsequent architectural shift\ntowards Transformer and Conformer models, which leverage self-attention to\ncapture long-range dependencies with high computational efficiency. A central\ntheme of this survey is the parallel revolution in training paradigms. We\nexamine the progression from fully supervised learning, augmented by techniques\nlike SpecAugment, to the rise of self-supervised learning (SSL) with foundation\nmodels such as wav2vec 2.0, which drastically reduce the reliance on\ntranscribed data. Furthermore, we analyze the impact of largescale, weakly\nsupervised models like Whisper, which achieve unprecedented robustness through\nmassive data diversity. The paper also covers essential ecosystem components,\nincluding key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME),\nstandard evaluation metrics (e.g., Word Error Rate), and critical\nconsiderations for real-world deployment, such as streaming inference,\non-device efficiency, and the ethical imperatives of fairness and robustness.\nWe conclude by outlining open challenges and future research directions."}
{"id": "2510.12947", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12947", "abs": "https://arxiv.org/abs/2510.12947", "authors": ["Mahsa Ghazvini Nejad", "Hamed Jafarzadeh Asl", "Amin Edraki", "Mohammadreza Sadeghi", "Masoud Asgharian", "Yuanhao Yu", "Vahid Partovi Nia"], "title": "HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection", "comment": "Mahsa Ghazvini Nejad and Hamed Jafarzadeh Asl contributed equally to\n  this work", "summary": "Personalized Voice Activity Detection (PVAD) systems activate only in\nresponse to a specific target speaker by incorporating speaker embeddings from\nenrollment utterances. Unlike existing methods that require architectural\nchanges, such as FiLM layers, our approach employs a hypernetwork to modify the\nweights of a few selected layers within a standard voice activity detection\n(VAD) model. This enables speaker conditioning without changing the VAD\narchitecture, allowing the same VAD model to adapt to different speakers by\nupdating only a small subset of the layers. We propose HyWA-PVAD, a\nhypernetwork weight adaptation method, and evaluate it against multiple\nbaseline conditioning techniques. Our comparison shows consistent improvements\nin PVAD performance. HyWA also offers practical advantages for deployment by\npreserving the core VAD architecture. Our new approach improves the current\nconditioning techniques in two ways: i) increases the mean average precision,\nii) simplifies deployment by reusing the same VAD architecture."}
{"id": "2510.12995", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12995", "abs": "https://arxiv.org/abs/2510.12995", "authors": ["Xinlu He", "Swayambhu Nath Ray", "Harish Mallidi", "Jia-Hong Huang", "Ashwin Bellur", "Chander Chandak", "M. Maruf", "Venkatesh Ravichandran"], "title": "Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs", "comment": null, "summary": "Unified architectures in multimodal large language models (MLLM) have shown\npromise in handling diverse tasks within a single framework. In the\ntext-to-speech (TTS) task, current MLLM-based approaches rely on discrete token\nrepresentations, which disregard the inherently continuous nature of speech and\ncan lead to loss of fine-grained acoustic information.In this work, we\ninvestigate the TTS within the MLLM paradigm using continuous speech\nrepresentations. We design a dual-head architecture and implement two\ncomplementary training strategies for a robust model. (1) A diffusion head\ngenerating continuous speech representations is added on the MLLM, which is on\nframe-level and strictly autoregressive. (2) The original language model head\nis retained to preserve multitask capability and to control the start and end\nof speech synthesis. (3) Masked training is employed to address exposure bias\nin autoregressive decoding. (4) To stabilize optimization, we propose a\ntwo-stage scheme where the LM is frozen in the second stage, ensuring the\ndiffusion head learns from a fixed input distribution. Evaluations on\nLibriSpeech(PC) test-clean show that our approach achieves state-of-the-art\nautoregressive performance, with a WER of 1.95%, speaker similarity of 0.54,\nand UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction\nover the one-stage training baseline. These results highlight the effectiveness\nof combining autoregressive modeling with continuous-token diffusion, supported\nby a two-stage training procedure."}
{"id": "2510.13221", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.13221", "abs": "https://arxiv.org/abs/2510.13221", "authors": ["Philipp Grundhuber", "Mhd Modar Halimeh", "Emanuël A. P. Habets"], "title": "Acoustic Teleportation via Disentangled Neural Audio Codec Representations", "comment": null, "summary": "This paper presents an approach for acoustic teleportation by disentangling\nspeech content from acoustic environment characteristics in neural audio codec\nrepresentations. Acoustic teleportation transfers room characteristics between\nspeech recordings while preserving content and speaker identity. We build upon\nprevious work using the EnCodec architecture, achieving substantial objective\nquality improvements with non-intrusive ScoreQ scores of 3.03, compared to 2.44\nfor prior methods. Our training strategy incorporates five tasks: clean\nreconstruction, reverberated reconstruction, dereverberation, and two variants\nof acoustic teleportation. We demonstrate that temporal downsampling of the\nacoustic embedding significantly degrades performance, with even 2x\ndownsampling resulting in a statistically significant reduction in quality. The\nlearned acoustic embeddings exhibit strong correlations with RT60. Effective\ndisentanglement is demonstrated using t-SNE clustering analysis, where acoustic\nembeddings cluster by room while speech embeddings cluster by speaker."}
{"id": "2510.12819", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12819", "abs": "https://arxiv.org/abs/2510.12819", "authors": ["Junyao Huang", "Rumin Situ"], "title": "Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis", "comment": "24 pages, 6 figures, 4 tables. First continuous VA framework for pet\n  vocalization analysis with 42,553 samples", "summary": "Traditional pet emotion recognition from vocalizations, based on discrete\nclassification, struggles with ambiguity and capturing intensity variations. We\npropose a continuous Valence-Arousal (VA) model that represents emotions in a\ntwo-dimensional space. Our method uses an automatic VA label generation\nalgorithm, enabling large-scale annotation of 42,553 pet vocalization samples.\nA multi-task learning framework jointly trains VA regression with auxiliary\ntasks (emotion, body size, gender) to enhance prediction by improving feature\nlearning. Our Audio Transformer model achieves a validation Valence Pearson\ncorrelation of r = 0.9024 and an Arousal r = 0.7155, effectively resolving\nconfusion between discrete categories like \"territorial\" and \"happy.\" This work\nintroduces the first continuous VA framework for pet vocalization analysis,\noffering a more expressive representation for human-pet interaction, veterinary\ndiagnostics, and behavioral training. The approach shows strong potential for\ndeployment in consumer products like AI pet emotion translators."}
{"id": "2510.12910", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12910", "abs": "https://arxiv.org/abs/2510.12910", "authors": ["Neda Abdollahpour", "N. Sertac Artan", "Ian Daly", "Mohammadreza Yazdchi", "Zahra Baharlouei"], "title": "Effective Connectivity-Based Unsupervised Channel Selection Method for EEG", "comment": "(This The paper has been accepted for publication in the Journal of\n  Medical Signals & Sensors and will appear soon", "summary": "Analyzing neural data such as Electroencephalography (EEG) data often\ninvolves dealing with high-dimensional datasets, where not all channels provide\nequally meaningful informa- tion. Selecting the most relevant channels is\ncrucial for improving computational efficiency and ensuring robust insights\ninto neural dynamics. This study introduces the Importance of Channels based on\nEffective Connectivity (ICEC) criterion for quantifying effective connectivity\n(EC) in each channel. Effective connectivity refers to the causal influence one\nneural region exerts over another, providing insights into the directional flow\nof information. Using this criterion, we propose an unsupervised channel\nselection method that accounts for the intensity of interactions among\nchannels. To evaluate the proposed channel selection method, we applied it to\nthree well-known EEG datasets across four categories. The assessment involved\ncalculating the ICEC criterion using five effective connectivity metrics:\npartial directed coherence (PDC), generalized PDC (GPDC), renormalized PDC\n(RPDC), directed transfer function (DTF), and direct DTF (dDTF). To focus on\nthe effect of channel selection, we employed the Common Spatial Pattern (CSP)\nalgorithm for feature extraction and a Support Vector Machine (SVM) for\nclassification across all participants. Results were compared with other\nCSP-based methods. The evaluation included comparing participant- specific\naccuracies with and without the proposed method across five effective\nconnectivity metrics. The results showed consistent performance improvements\nand a significant reduction in the number of selected electrodes for all\nparticipants. Compared to state-of-the-art methods, our approach achieved the\nhighest accuracies: 82% (13 out of 22 channels), 86.01% (29 out of 59\nchannels), and 87.56% (48 out of 118 channels) across three datasets."}
{"id": "2510.13281", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13281", "abs": "https://arxiv.org/abs/2510.13281", "authors": ["Sungnyun Kim", "Kangwook Jang", "Sungwoo Cho", "Joon Son Chung", "Hoirin Kim", "Se-Young Yun"], "title": "Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses", "comment": "Preprint work", "summary": "This paper introduces a new paradigm for generative error correction (GER)\nframework in audio-visual speech recognition (AVSR) that reasons over\nmodality-specific evidences directly in the language space. Our framework,\nDualHyp, empowers a large language model (LLM) to compose independent N-best\nhypotheses from separate automatic speech recognition (ASR) and visual speech\nrecognition (VSR) models. To maximize the effectiveness of DualHyp, we further\nintroduce RelPrompt, a noise-aware guidance mechanism that provides\nmodality-grounded prompts to the LLM. RelPrompt offers the temporal reliability\nof each modality stream, guiding the model to dynamically switch its focus\nbetween ASR and VSR hypotheses for an accurate correction. Under various\ncorruption scenarios, our framework attains up to 57.7% error rate gain on the\nLRS2 benchmark over standard ASR baseline, contrary to single-stream GER\napproaches that achieve only 10% gain. To facilitate research within our\nDualHyp framework, we release the code and the dataset comprising ASR and VSR\nhypotheses at https://github.com/sungnyun/dualhyp."}
{"id": "2510.12823", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12823", "abs": "https://arxiv.org/abs/2510.12823", "authors": ["Timothy Tran", "William Schiesser"], "title": "Production and Manufacturing of 3D Printed Acoustic Guitars", "comment": null, "summary": "This research investigates the feasibility of producing affordable,\nfunctional acoustic guitars using 3D printing, with a focus on producing\nstructural designs with proper tonal performance. Conducted in collaboration\nwith William Schiesser, the study uses a classical guitar model, chosen for its\nlower string tension, to evaluate the tonal characteristics of a 3D-printed\nprototype made from polylactic acid (PLA). Due to the build plate size\nconstraints of the Prusa Mark 4 printer, the guitar body was divided into\nmultiple sections joined with press-fit tolerances and minimal cyanoacrylate\nadhesive. CAD modeling in Fusion 360 ensured dimensional accuracy in press-fit\nconnections and the overall assembly. Following assembly, the guitar was strung\nwith nylon strings and tested using Audacity software to compare recorded\nfrequencies and notes with standard reference values. Results showed large\ndeviations in lower string frequencies, likely caused by the material choice\nutilized in printing. Accurate pitches were reached with all strings despite\nfrequency differences through tuning, demonstrating that PLA and modern\nmanufacturing methods can produce affordable, playable acoustic guitars despite\ninevitable challenges. Further research may investigate alternative plastics\nfor superior frequency matching. This approach holds significant potential for\nexpanding access to quality instruments while reducing reliance on endangered\ntonewoods, thereby encouraging both sustainable instrument production and\nincreased musical participation. This also creates opportunities for\ndisadvantaged communities where access to musical instruments remains a\nchallenge.\n  Keywords: Luthiery, Stereolithography, 3D-Print, Guitar Making"}
{"id": "2510.12912", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12912", "abs": "https://arxiv.org/abs/2510.12912", "authors": ["Abdelali Arous", "Hamza Haif", "Huseyin Arslan"], "title": "Enabling Full Duplex ISAC Leveraging Waveform Domain Separability", "comment": null, "summary": "Integrated sensing and communication (ISAC) in monostatic in-band full-duplex\n(IBFD) systems encounters significant challenges due to self-interference (SI)\nat the radar receiver during concurrent communication and radar operations.\nThis paper proposes a novel waveform-domain self-interference cancellation\n(SIC) technique that leverages the unique properties of orthogonal frequency\ndivision multiplexing (OFDM) and affine frequency division multiplexing (AFDM)\nsignals. The proposed approach designs the integrated dual-functionality frame\nto utilize OFDM for communication and AFDM for radar sensing, both generated\nusing the same modulator block. Then, we establish the conditions under which a\nwide sense stationary (WSS) process in the time domain appears as WSS in the\naffine domain and demonstrate that the interfering OFDM signal behaves as an\nadditive white Gaussian noise (AWGN) in this domain. Exploiting this property,\nthe received signal is projected into the affine domain, where the SI appears\nas AWGN, enabling its subtraction with minimal residual interference. To\nfurther mitigate the residual SI, an iterative low-complexity windowing scheme\nis applied, selectively locking onto the radar signal to reduce the processed\nsignal space. A subsequent time-domain spreading step is applied after\nconverting the SIC-processed signal into the post-coded time domain, wherein\nthe SI diminishes separately across the delay and Doppler axes. The proposed\nmethod demonstrates superior performance in terms of detection probability,\ntarget range and velocity root mean square error (RMSE), while maintaining high\nspectral efficiency and minimal computational complexity."}
{"id": "2510.13308", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.13308", "abs": "https://arxiv.org/abs/2510.13308", "authors": ["Chenxin Yu", "Hao Ma", "Xu Li", "Xiao-Lei Zhang", "Mingjie Shao", "Chi Zhang", "Xuelong Li"], "title": "Towards Multimodal Query-Based Spatial Audio Source Extraction", "comment": "Submitted to ICASSP 2026", "summary": "Query-based audio source extraction seeks to recover a target source from a\nmixture conditioned on a query. Existing approaches are largely confined to\nsingle-channel audio, leaving the spatial information in multi-channel\nrecordings underexploited. We introduce a query-based spatial audio source\nextraction framework for recovering dry target signals from first-order\nambisonics (FOA) mixtures. Our method accepts either an audio prompt or a text\nprompt as condition input, enabling flexible end-to-end extraction. The core of\nour proposed model lies in a tri-axial Transformer that jointly models\ntemporal, frequency, and spatial channel dependencies. The model uses\ncontrastive language-audio pretraining (CLAP) embeddings to enable unified\naudio-text conditioning via feature-wise linear modulation (FiLM). To eliminate\ncostly annotations and improve generalization, we propose a label-free data\npipeline that dynamically generates spatial mixtures and corresponding targets\nfor training. The result of our experiment with high separation quality\ndemonstrates the efficacy of multimodal conditioning and tri-axial modeling.\nThis work establishes a new paradigm for high-fidelity spatial audio separation\nin immersive applications."}
{"id": "2510.12834", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07"], "pdf": "https://arxiv.org/pdf/2510.12834", "abs": "https://arxiv.org/abs/2510.12834", "authors": ["Téo Guichoux", "Théodor Lemerle", "Shivam Mehta", "Jonas Beskow", "Gustave Eje Henter", "Laure Soulier", "Catherine Pelachaud", "Nicolas Obin"], "title": "Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction", "comment": "5 pages", "summary": "Human communication is multimodal, with speech and gestures tightly coupled,\nyet most computational methods for generating speech and gestures synthesize\nthem sequentially, weakening synchrony and prosody alignment. We introduce\nGelina, a unified framework that jointly synthesizes speech and co-speech\ngestures from text using interleaved token sequences in a discrete\nautoregressive backbone, with modality-specific decoders. Gelina supports\nmulti-speaker and multi-style cloning and enables gesture-only synthesis from\nspeech inputs. Subjective and objective evaluations demonstrate competitive\nspeech quality and improved gesture generation over unimodal baselines."}
{"id": "2510.12930", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12930", "abs": "https://arxiv.org/abs/2510.12930", "authors": ["Cory Hilton", "Mohammad Rashid", "Faiz Sherman", "Steven Bush", "Jeffrey A. Nanzer"], "title": "Passive Microwave Tag Classification Using RF Fingerprinting and Machine Learning", "comment": "7 pages,7 figures", "summary": "We present an approach to identifying wireless microwave tags using radio\nfrequency (RF) fingerprinting and machine learning. The tags are designed for\nlow cost and simplicity, consisting of only two antennas and a single nonlinear\nelement (a diode). An interrogating transceiver transmits a signal consisting\nof a set of individual frequency tones that is captured by the tag. The signal\nresponse of the diode is nonlinear, and can be represented by an infinite power\nseries, the coefficients of which are similar but not identical for different\nphysical diodes due to small manufacturing perturbations. The small differences\nin the signal responses manifest in the spectral signal response of the tag,\nwhich is retransmitted back to the interrogating transceiver. Input into\nmachine learning algorithms, the slight differences in the spectral responses\nof the diodes can be used to uniquely identify devices. To demonstrate the\nconcept, we designed 2.0 GHz tags consisting of patch antennas and a single\ndiode, along with a bi-static radar system operating at the 2.0 GHz 802.11\nWi-Fi band transmitting multi-tone continuous wave signals representing common\n802.11 training fields. The received signals were processed using a set of\nalgorithms for comparison purposes. A real-time classification accuracy of 95%\nbetween two tags was achieved."}
{"id": "2510.12819", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12819", "abs": "https://arxiv.org/abs/2510.12819", "authors": ["Junyao Huang", "Rumin Situ"], "title": "Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis", "comment": "24 pages, 6 figures, 4 tables. First continuous VA framework for pet\n  vocalization analysis with 42,553 samples", "summary": "Traditional pet emotion recognition from vocalizations, based on discrete\nclassification, struggles with ambiguity and capturing intensity variations. We\npropose a continuous Valence-Arousal (VA) model that represents emotions in a\ntwo-dimensional space. Our method uses an automatic VA label generation\nalgorithm, enabling large-scale annotation of 42,553 pet vocalization samples.\nA multi-task learning framework jointly trains VA regression with auxiliary\ntasks (emotion, body size, gender) to enhance prediction by improving feature\nlearning. Our Audio Transformer model achieves a validation Valence Pearson\ncorrelation of r = 0.9024 and an Arousal r = 0.7155, effectively resolving\nconfusion between discrete categories like \"territorial\" and \"happy.\" This work\nintroduces the first continuous VA framework for pet vocalization analysis,\noffering a more expressive representation for human-pet interaction, veterinary\ndiagnostics, and behavioral training. The approach shows strong potential for\ndeployment in consumer products like AI pet emotion translators."}
{"id": "2510.12851", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12851", "abs": "https://arxiv.org/abs/2510.12851", "authors": ["Tsung-En Lin", "Kuan-Yi Lee", "Hung-Yi Lee"], "title": "Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models", "comment": "Note: This preprint is a version of the paper submitted to ICASSP\n  2026. The author list here includes contributors who provided additional\n  supervision and guidance. The official ICASSP submission may differ slightly\n  in author composition", "summary": "Large Audio-Language Models and Multi-Modal Large Language Models have\ndemonstrated strong capabilities in tasks such as Audio Question Answering\n(AQA), Audio Captioning, and Automatic Speech Recognition (ASR). However, there\nis growing evidence that these models can hallucinate about the content of the\naudio. To address this issue, we probe the models' internal states and propose\nAdaptive Vector Steering (AVS), a method that better grounds generation in\naudio content. We also identify a strong correlation between output correctness\nand internal representations. Experiments show consistent performance gains\nacross two models and two benchmarks. On the Audio Hallucination QA dataset,\nour method boosts the F1-score of Gemma from 0.550 to 0.619 and Qwen from 0.626\nto 0.632. Furthermore, our method increases the accuracy of Qwen on MMAU from\n0.548 to 0.592, marking an 8% relative increase. To the best of our knowledge,\nthis is the first work to apply vector steering to mitigate hallucination in\naudio."}
{"id": "2510.12941", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12941", "abs": "https://arxiv.org/abs/2510.12941", "authors": ["SaiKrishna Saketh Yellapragada", "Atchutaram K. Kocharlakota", "Mário Costa", "Esa Ollila", "Sergiy A. Vorobyov"], "title": "Computationally Efficient Neural Receivers via Axial Self-Attention", "comment": "Submitted for IEEE International Conference on Communications", "summary": "Deep learning-based neural receivers are redefining physical-layer signal\nprocessing for next-generation wireless systems. We propose an axial\nself-attention transformer neural receiver designed for applicability to 6G and\nbeyond wireless systems, validated through 5G-compliant experimental\nconfigurations, that achieves state-of-the-art block error rate (BLER)\nperformance with significantly improved computational efficiency. By\nfactorizing attention operations along temporal and spectral axes, the proposed\narchitecture reduces the quadratic complexity of conventional multi-head\nself-attention from $O((TF)^2)$ to $O(T^2F+TF^2)$, yielding substantially fewer\ntotal floating-point operations and attention matrix multiplications per\ntransformer block compared to global self-attention. Relative to convolutional\nneural receiver baselines, the axial neural receiver achieves significantly\nlower computational cost with a fraction of the parameters. Experimental\nvalidation under 3GPP Clustered Delay Line (CDL) channels demonstrates\nconsistent performance gains across varying mobility scenarios. Under\nnon-line-of-sight CDL-C conditions, the axial neural receiver consistently\noutperforms all evaluated receiver architectures, including global\nself-attention, convolutional neural receivers, and traditional LS-LMMSE at\n10\\% BLER with reduced computational complexity per inference. At stringent\nreliability targets of 1\\% BLER, the axial receiver maintains robust symbol\ndetection at high user speeds, whereas the traditional LS-LMMSE receiver fails\nto converge, underscoring its suitability for ultra-reliable low-latency\n(URLLC) communication in dynamic 6G environments and beyond. These results\nestablish the axial neural receiver as a structured, scalable, and efficient\nframework for AI-Native 6G RAN systems, enabling deployment in\nresource-constrained edge environments."}
{"id": "2510.12823", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12823", "abs": "https://arxiv.org/abs/2510.12823", "authors": ["Timothy Tran", "William Schiesser"], "title": "Production and Manufacturing of 3D Printed Acoustic Guitars", "comment": null, "summary": "This research investigates the feasibility of producing affordable,\nfunctional acoustic guitars using 3D printing, with a focus on producing\nstructural designs with proper tonal performance. Conducted in collaboration\nwith William Schiesser, the study uses a classical guitar model, chosen for its\nlower string tension, to evaluate the tonal characteristics of a 3D-printed\nprototype made from polylactic acid (PLA). Due to the build plate size\nconstraints of the Prusa Mark 4 printer, the guitar body was divided into\nmultiple sections joined with press-fit tolerances and minimal cyanoacrylate\nadhesive. CAD modeling in Fusion 360 ensured dimensional accuracy in press-fit\nconnections and the overall assembly. Following assembly, the guitar was strung\nwith nylon strings and tested using Audacity software to compare recorded\nfrequencies and notes with standard reference values. Results showed large\ndeviations in lower string frequencies, likely caused by the material choice\nutilized in printing. Accurate pitches were reached with all strings despite\nfrequency differences through tuning, demonstrating that PLA and modern\nmanufacturing methods can produce affordable, playable acoustic guitars despite\ninevitable challenges. Further research may investigate alternative plastics\nfor superior frequency matching. This approach holds significant potential for\nexpanding access to quality instruments while reducing reliance on endangered\ntonewoods, thereby encouraging both sustainable instrument production and\nincreased musical participation. This also creates opportunities for\ndisadvantaged communities where access to musical instruments remains a\nchallenge.\n  Keywords: Luthiery, Stereolithography, 3D-Print, Guitar Making"}
{"id": "2510.12964", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12964", "abs": "https://arxiv.org/abs/2510.12964", "authors": ["Maharnab Saikia"], "title": "VCTR: A Transformer-Based Model for Non-parallel Voice Conversion", "comment": null, "summary": "Non-parallel voice conversion aims to convert voice from a source domain to a\ntarget domain without paired training data. Cycle-Consistent Generative\nAdversarial Networks (CycleGAN) and Variational Autoencoders (VAE) have been\nused for this task, but these models suffer from difficult training and\nunsatisfactory results. Later, Contrastive Voice Conversion (CVC) was\nintroduced, utilizing a contrastive learning-based approach to address these\nissues. However, these methods use CNN-based generators, which can capture\nlocal semantics but lacks the ability to capture long-range dependencies\nnecessary for global semantics. In this paper, we propose VCTR, an efficient\nmethod for non-parallel voice conversion that leverages the Hybrid Perception\nBlock (HPB) and Dual Pruned Self-Attention (DPSA) along with a contrastive\nlearning-based adversarial approach. The code can be found in\nhttps://github.com/Maharnab-Saikia/VCTR."}
{"id": "2510.12968", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.12968", "abs": "https://arxiv.org/abs/2510.12968", "authors": ["Najme Ebrahimi", "Arun Paidmarri", "Alexandra Gallyas-Sanhueza", "Yuan Ma", "Haoling Li", "Basem Abdelaziz Abdelmagid", "Tzu-Yuan Huang", "Hua Wang"], "title": "Towards Spectrally Efficient and Physically Reconfigurable Architectures for Multibeam-Waveform Co-Design in Joint Communication and Sensing", "comment": null, "summary": "Joint Communication and Sensing (JCAS) platforms are emerging as a foundation\nof next-generation mmWave (MMW) and sub-THz systems, enabling both\nhigh-throughput data transfer and angular localization within a shared signal\npath. This paper investigates multibeam architectures for JCAS that\nsimultaneously optimize waveform shaping and beamforming across the time,\nfrequency, code, and direct analog/ radio frequency (RF) domains. The paper\ncompares Orthogonal Frequency-Division Multiplexing (OFDM), Frequency Modulated\nArrays (FMA), Time-Modulated Arrays (TMA), direct RF/MMW modulation, and\nCode-Division Multiple Access (CDMA)-based systems with respect to spectral\nefficiency, beam orthogonality, latency, and Angle-of-Arrival (AoA) estimation\naccuracy. The results highlight architecture-specific tradeoffs among beam\nagility, efficiency, accuracy and resolution, and complexity. It also provides\na framework for selecting JCAS front ends optimized for power, latency,\ninter-beam and multi-user interference, and rapid system reconfiguration"}
{"id": "2510.12834", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07"], "pdf": "https://arxiv.org/pdf/2510.12834", "abs": "https://arxiv.org/abs/2510.12834", "authors": ["Téo Guichoux", "Théodor Lemerle", "Shivam Mehta", "Jonas Beskow", "Gustave Eje Henter", "Laure Soulier", "Catherine Pelachaud", "Nicolas Obin"], "title": "Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction", "comment": "5 pages", "summary": "Human communication is multimodal, with speech and gestures tightly coupled,\nyet most computational methods for generating speech and gestures synthesize\nthem sequentially, weakening synchrony and prosody alignment. We introduce\nGelina, a unified framework that jointly synthesizes speech and co-speech\ngestures from text using interleaved token sequences in a discrete\nautoregressive backbone, with modality-specific decoders. Gelina supports\nmulti-speaker and multi-style cloning and enables gesture-only synthesis from\nspeech inputs. Subjective and objective evaluations demonstrate competitive\nspeech quality and improved gesture generation over unimodal baselines."}
{"id": "2510.13244", "categories": ["cs.SD", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13244", "abs": "https://arxiv.org/abs/2510.13244", "authors": ["Xuanchen Wang", "Heng Wang", "Weidong Cai"], "title": "MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding", "comment": "5 pages, 1 figure. demo page: https://motionbeat2025.github.io/", "summary": "Music is both an auditory and an embodied phenomenon, closely linked to human\nmotion and naturally expressed through dance. However, most existing audio\nrepresentations neglect this embodied dimension, limiting their ability to\ncapture rhythmic and structural cues that drive movement. We propose\nMotionBeat, a framework for motion-aligned music representation learning.\nMotionBeat is trained with two newly proposed objectives: the Embodied\nContrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and\nbeat-jitter negatives to achieve fine-grained rhythmic discrimination, and the\nStructural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by\naligning music accents with corresponding motion events. Architecturally,\nMotionBeat introduces bar-equivariant phase rotations to capture cyclic\nrhythmic patterns and contact-guided attention to emphasize motion events\nsynchronized with musical accents. Experiments show that MotionBeat outperforms\nstate-of-the-art audio encoders in music-to-dance generation and transfers\neffectively to beat tracking, music tagging, genre and instrument\nclassification, emotion recognition, and audio-visual retrieval. Our project\ndemo page: https://motionbeat2025.github.io/."}
{"id": "2510.13101", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.13101", "abs": "https://arxiv.org/abs/2510.13101", "authors": ["Kawon Han", "Kaitao Meng", "Alexandra Chatzicharistou", "Christos Masouros"], "title": "Constellation Design in OFDM-ISAC over Data Payloads: From MSE Analysis to Experimentation", "comment": "6 pages", "summary": "Orthogonal frequency division multiplexing (OFDM) is one of the most widely\nadopted waveforms for integrated sensing and communication (ISAC) systems,\nowing to its high spectral efficiency and compatibility with modern\ncommunication standards. This paper investigates the sensing performance of\nOFDM-based ISAC for multi-target delay (range) estimation under specific radar\nreceiver processing schemes. An estimation-theoretic framework is developed to\ncharacterize sensing performance with random communication payloads. We\nestablish the fundamental limit of delay estimation accuracy by deriving the\nclosed-form expression of the mean-square error (MSE) achieved using matched\nfiltering (MF) and reciprocal filtering (RF) receivers. The results show that,\nin multi-target scenarios, the impact of signal constellations on the delay\nestimation MSE differs across receivers: MF performance depends on the\nfourth-order moment of the zero-mean, unit-power constellation in the presence\nof multiple targets, whereas RF performance depends on its inverse second-order\nmoment, irrespective of the number of targets. Building on this analysis, we\npresent a ISAC constellation design under specific receiver architecture that\nbrings a receiver-dependent flexible trade-off between sensing and\ncommunication in OFDM-ISAC systems. The theoretical findings are validated\nthrough simulations and proof-of-concept experiments, and also the sensing and\ncommunication performance trade-off is experimentally shown with the proposed\nconstellation design."}
{"id": "2510.12851", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.12851", "abs": "https://arxiv.org/abs/2510.12851", "authors": ["Tsung-En Lin", "Kuan-Yi Lee", "Hung-Yi Lee"], "title": "Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models", "comment": "Note: This preprint is a version of the paper submitted to ICASSP\n  2026. The author list here includes contributors who provided additional\n  supervision and guidance. The official ICASSP submission may differ slightly\n  in author composition", "summary": "Large Audio-Language Models and Multi-Modal Large Language Models have\ndemonstrated strong capabilities in tasks such as Audio Question Answering\n(AQA), Audio Captioning, and Automatic Speech Recognition (ASR). However, there\nis growing evidence that these models can hallucinate about the content of the\naudio. To address this issue, we probe the models' internal states and propose\nAdaptive Vector Steering (AVS), a method that better grounds generation in\naudio content. We also identify a strong correlation between output correctness\nand internal representations. Experiments show consistent performance gains\nacross two models and two benchmarks. On the Audio Hallucination QA dataset,\nour method boosts the F1-score of Gemma from 0.550 to 0.619 and Qwen from 0.626\nto 0.632. Furthermore, our method increases the accuracy of Qwen on MMAU from\n0.548 to 0.592, marking an 8% relative increase. To the best of our knowledge,\nthis is the first work to apply vector steering to mitigate hallucination in\naudio."}
{"id": "2510.13344", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13344", "abs": "https://arxiv.org/abs/2510.13344", "authors": ["Zhenyu Liu", "Yunxin Li", "Xuanyu Zhang", "Qixun Teng", "Shenyuan Jiang", "Xinyu Chen", "Haoyuan Shi", "Jinchao Li", "Qi Wang", "Haolan Chen", "Fanbo Meng", "Mingjun Zhao", "Yu Xu", "Yancheng He", "Baotian Hu", "Min Zhang"], "title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE", "comment": null, "summary": "Recent advances in unified multimodal models indicate a clear trend towards\ncomprehensive content generation. However, the auditory domain remains a\nsignificant challenge, with music and speech often developed in isolation,\nhindering progress towards universal audio synthesis. This separation stems\nfrom inherent task conflicts and severe data imbalances, which impede the\ndevelopment of a truly unified audio generation model. To address this\nchallenge, we propose UniMoE-Audio, a unified speech and music generation model\nwithin a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.\nArchitecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic\nexpert number allocation, and a hybrid expert design comprising routed experts\nfor domain-specific knowledge, shared experts for domain-agnostic features, and\nnull experts for adaptive computation skipping. To tackle data imbalance, we\nintroduce a three-stage training curriculum: 1) Independent Specialist Training\nleverages original datasets to instill domain-specific knowledge into each\n\"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates\nthese specialists into the UniMoE-Audio architecture, warming up the gate\nmodule and shared expert using a subset of balanced dataset; and 3) Synergistic\nJoint Training trains the entire model end-to-end on the fully balanced\ndataset, fostering enhanced cross-domain synergy. Extensive experiments show\nthat UniMoE-Audio not only achieves state-of-the-art performance on major\nspeech and music generation benchmarks, but also demonstrates superior\nsynergistic learning, mitigating the performance degradation typically seen in\nnaive joint training. Our findings highlight the substantial potential of\nspecialized MoE architecture and curated training strategies in advancing the\nfield of universal audio generation. Homepage:\nhttps://mukioxun.github.io/Uni-MoE-site/home.html"}
{"id": "2510.13399", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.13399", "abs": "https://arxiv.org/abs/2510.13399", "authors": ["Shivani Ranjan", "Anant Jain", "Robin Badal", "Amit Kumar", "Harshal Shende", "Deepak Joshi", "Pramod Yadav", "Lalan Kumar"], "title": "Working Memory Functional Connectivity Analysis for Dementia Classification using EEG", "comment": null, "summary": "Background: Dementia, particularly Alzheimer's Disease (AD), is a progressive\nneurodegenerative disorder marked by cognitive decline. Early detection,\nespecially at the Mild Cognitive Impairment (MCI) stage, is essential for\ntimely intervention. Working Memory (WM) impairment is a key early indicator of\nneurodegeneration, affecting higher cognitive processes. Electroencephalography\n(EEG), with its high temporal resolution, offers a cost-effective method to\nassess brain dynamics. This study investigates WM-related EEG functional\nconnectivity (FC) to identify brain network alterations across dementia stages.\nMethods: EEG signals were recorded from 24 participants (8 AD, 8 MCI, and 8\nhealthy controls) during WM tasks, including encoding, recall, and retrieval\nstages. Data preprocessing involved noise reduction and feature extraction\nusing Spherical and Head Harmonic Decomposition (SHD, HHD). FC was quantified\nusing Cross-Plot Transition Entropy (CPTE) and Phase Lag Index (PLI). Network\nmetrics such as Degree and Eigenvector Centrality were analyzed using Support\nVector Machine, Random Forest, and XGBoost classifiers. Results: The CPTE-based\nconnectivity metrics outperformed the traditional PLI approach in\ndifferentiating dementia stages, attaining a peak classification accuracy of\n97.53% during the retrieval phase with the Random Forest model. A connectivity\nthreshold of 0.5 was optimal for network discrimination. SHD and HHD features\nalso demonstrated strong discriminative potential. AD subjects exhibited higher\nsynchronization patterns during WM tasks than healthy controls. Conclusions:\nThe integration of WM tasks with EEG-based FC analysis provides a robust\nframework for dementia classification. The proposed CPTE-based approach offers\na robust, scalable, non-invasive, and effective diagnostic tool for early\ndetection and monitoring of neurodegenerative diseases."}
{"id": "2510.13558", "categories": ["cs.SD", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.13558", "abs": "https://arxiv.org/abs/2510.13558", "authors": ["Ruitao Feng", "Bixi Zhang", "Sheng Liang", "Zheng Yuan"], "title": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module", "comment": "5 pages, 1 figures. Code is available at:\n  https://github.com/forfrt/SteerMoE. Submitted to ICASSP 2026", "summary": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a\npromising, parameter-efficient path to building powerful multimodal agents.\nHowever, existing methods often require costly full-model finetuning or rely on\nstatic adapters that may lack expressive power. Drawing inspiration from the\nPlatonic Representation Hypothesis, we introduce SteerMoE, a novel and modular\nframework for audio-language alignment. SteerMoE freezes both the audio encoder\nand the LLM decoder, training only a lightweight steering module integrated\nwithin the encoder's layers. This module uses a Mixture-of-Experts (MoE) router\nto dynamically select and apply learned steering vectors, progressively\ntransforming continuous audio representations into a space comprehensible to\nthe LLM. By operating entirely in the continuous embedding space, our approach\nrequires no modifications to the LLM's vocabulary and preserves its advanced\nreasoning and agentic capabilities. We demonstrate through experiments on ASR,\naudio understanding, and a qualitative function-calling task that SteerMoE\nachieves strong performance while remaining highly modular and computationally\nefficient, offering a robust new paradigm for developing sophisticated\naudio-language systems."}
{"id": "2510.13442", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.13442", "abs": "https://arxiv.org/abs/2510.13442", "authors": ["Lorenz Mohr", "Marc Miranda", "Sebastian Semper", "Julia Beuster", "Carsten Andrich", "Sebastian Giehl", "Christian Schneider", "Reiner S. Thomä"], "title": "Oscillator Drift Compensation by Line-of-Sight Tracking for Distributed Multisensor ISAC", "comment": "6 pages, 4 figures", "summary": "We observed synchronization mismatches in the form of non-smooth phase\nprogressions and drifts within mobile multisensor channel sounding\nmeasurements. However, performing Doppler estimation in a distributed\nmultisensor integrated sensing and communications (ISAC) system requires\ncoherence among the nodes, which implies a continuously differentiable phase\nprogression of the received signals. To correct the sounding data in\npost-processing, we extend traditional geometry-based drift compensation\nalgorithms by utilizing Kalman filtering for line-of-sight (LoS) tracking,\nwhich improves the robustness of the LoS estimate in multipath scenarios. This\napproach smooths the phase progression and enables the correction of\ntime-varying drifts while preserving relative sensor motion. Furthermore, we\npropose using the relative residual power after high-resolution parameter\nestimation (HRPE) as a metric for ground-truth-independent comparison of\npost-processing synchronization methods for recorded channel sounding data.\nResults show that the proposed approach outperforms traditional LoS estimation\nheuristics, reducing the relative residual power by more than 5 dB and the\ndelay-Doppler estimate root mean square errors (RMSEs) by approximately 60 %."}
{"id": "2510.12827", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12827", "abs": "https://arxiv.org/abs/2510.12827", "authors": ["Md. Nayeem", "Md Shamse Tabrej", "Kabbojit Jit Deb", "Shaonti Goswami", "Md. Azizul Hakim"], "title": "Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation", "comment": null, "summary": "Automatic Speech Recognition (ASR) has undergone a profound transformation\nover the past decade, driven by advances in deep learning. This survey provides\na comprehensive overview of the modern era of ASR, charting its evolution from\ntraditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models\n(GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant\nend-to-end neural architectures. We systematically review the foundational\nend-to-end paradigms: Connectionist Temporal Classification (CTC),\nattention-based encoder-decoder models, and the Recurrent Neural Network\nTransducer (RNN-T), which established the groundwork for fully integrated\nspeech-to-text systems. We then detail the subsequent architectural shift\ntowards Transformer and Conformer models, which leverage self-attention to\ncapture long-range dependencies with high computational efficiency. A central\ntheme of this survey is the parallel revolution in training paradigms. We\nexamine the progression from fully supervised learning, augmented by techniques\nlike SpecAugment, to the rise of self-supervised learning (SSL) with foundation\nmodels such as wav2vec 2.0, which drastically reduce the reliance on\ntranscribed data. Furthermore, we analyze the impact of largescale, weakly\nsupervised models like Whisper, which achieve unprecedented robustness through\nmassive data diversity. The paper also covers essential ecosystem components,\nincluding key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME),\nstandard evaluation metrics (e.g., Word Error Rate), and critical\nconsiderations for real-world deployment, such as streaming inference,\non-device efficiency, and the ethical imperatives of fairness and robustness.\nWe conclude by outlining open challenges and future research directions."}
{"id": "2510.13495", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.13495", "abs": "https://arxiv.org/abs/2510.13495", "authors": ["Dexin Kong", "Diana Pamela Moya Osorio", "Erik G. Larsson"], "title": "Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning", "comment": null, "summary": "Recent advancements in polymer microwave fiber (PMF) technology have created\nsignificant opportunities for robust, low-cost, and high-speed sub-terahertz\n(THz) radio-over- fiber communications. Recognizing these potential benefits,\nthis paper explores a novel radio-over-fiber (RoF) structure that interconnects\nmultiple radio units (RUs) in cascade via fiber, envi- sioning its application\nin indoor scenarios. This structure creates a number of research opportunities\nwhen considering cascaded distortion effects introduced by non-linear power\namplifiers (PAs) at the RUs and the propagation channel over the fiber. We\npropose maximum-likelihood and non-linear least-squares algorithms to estimate\nthe propagation distance along the RoF and the time-of-arrival between the RoF\nand the user equipment. For the case of linear PAs, we derive the Cram\\'er-Rao\nlower bound to benchmark the performance of the estimators. Finally, we\ninvestigate the use of the system for uplink positioning. Our simulation\nresults demonstrate that the proposed estimators perform satisfactorily even\nwith the cascaded effects of non- linear PAs, and that the deployment of this\nRoF structure can enable new cost-effective opportunities for high-resolution\npositioning in indoor scenarios. In the numerical evaluation, we also use\nmeasured PMF characteristics for high-density polyethylene fibers."}
{"id": "2510.12947", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12947", "abs": "https://arxiv.org/abs/2510.12947", "authors": ["Mahsa Ghazvini Nejad", "Hamed Jafarzadeh Asl", "Amin Edraki", "Mohammadreza Sadeghi", "Masoud Asgharian", "Yuanhao Yu", "Vahid Partovi Nia"], "title": "HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection", "comment": "Mahsa Ghazvini Nejad and Hamed Jafarzadeh Asl contributed equally to\n  this work", "summary": "Personalized Voice Activity Detection (PVAD) systems activate only in\nresponse to a specific target speaker by incorporating speaker embeddings from\nenrollment utterances. Unlike existing methods that require architectural\nchanges, such as FiLM layers, our approach employs a hypernetwork to modify the\nweights of a few selected layers within a standard voice activity detection\n(VAD) model. This enables speaker conditioning without changing the VAD\narchitecture, allowing the same VAD model to adapt to different speakers by\nupdating only a small subset of the layers. We propose HyWA-PVAD, a\nhypernetwork weight adaptation method, and evaluate it against multiple\nbaseline conditioning techniques. Our comparison shows consistent improvements\nin PVAD performance. HyWA also offers practical advantages for deployment by\npreserving the core VAD architecture. Our new approach improves the current\nconditioning techniques in two ways: i) increases the mean average precision,\nii) simplifies deployment by reusing the same VAD architecture."}
{"id": "2510.13498", "categories": ["eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.13498", "abs": "https://arxiv.org/abs/2510.13498", "authors": ["Mingyu Zhao", "Qingna Li", "Hou-Duo Qi"], "title": "A Robust EDM Optimization Approach for 3D Single-Source Localization with Angle and Range Measurements", "comment": "12 pages, 9 figures", "summary": "For the problem of source localization, three elements usually play a very\nimportant role in accurate localization. They are the range measurements, the\nangle measurements and the least absolute deviation criterion, which is\nregarded as a robust metric for denoising the measurements. Building the three\nelements into a computationally tractable model is challenging. In this paper,\nwe introduce a robust Euclidean Distance Matrix (EDM) optimization model that\nsimultaneously incorporates the three elements. For the first time, we show\nthat for the case of 3D single-source localization (3DSSL), the angle\nmeasurements can be represented as a simple box constraint of distances. It is\nachieved by reducing each of the 3D angle measurements to a two-dimensional\nnonlinear optimization problem, whose global minimum and maximum solutions can\nbe characterized and utilized to get the lower and upper bounds of the\ndistances from the unknown source to the sensors. We further develop an\nefficient algorithm. The high quality of the localization by the new EDM model\nis assessed through extensive numerical experiments in comparison with leading\nsolvers for 3DSSL."}
{"id": "2510.12995", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.12995", "abs": "https://arxiv.org/abs/2510.12995", "authors": ["Xinlu He", "Swayambhu Nath Ray", "Harish Mallidi", "Jia-Hong Huang", "Ashwin Bellur", "Chander Chandak", "M. Maruf", "Venkatesh Ravichandran"], "title": "Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs", "comment": null, "summary": "Unified architectures in multimodal large language models (MLLM) have shown\npromise in handling diverse tasks within a single framework. In the\ntext-to-speech (TTS) task, current MLLM-based approaches rely on discrete token\nrepresentations, which disregard the inherently continuous nature of speech and\ncan lead to loss of fine-grained acoustic information.In this work, we\ninvestigate the TTS within the MLLM paradigm using continuous speech\nrepresentations. We design a dual-head architecture and implement two\ncomplementary training strategies for a robust model. (1) A diffusion head\ngenerating continuous speech representations is added on the MLLM, which is on\nframe-level and strictly autoregressive. (2) The original language model head\nis retained to preserve multitask capability and to control the start and end\nof speech synthesis. (3) Masked training is employed to address exposure bias\nin autoregressive decoding. (4) To stabilize optimization, we propose a\ntwo-stage scheme where the LM is frozen in the second stage, ensuring the\ndiffusion head learns from a fixed input distribution. Evaluations on\nLibriSpeech(PC) test-clean show that our approach achieves state-of-the-art\nautoregressive performance, with a WER of 1.95%, speaker similarity of 0.54,\nand UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction\nover the one-stage training baseline. These results highlight the effectiveness\nof combining autoregressive modeling with continuous-token diffusion, supported\nby a two-stage training procedure."}

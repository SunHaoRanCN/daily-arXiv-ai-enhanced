<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.SD](#cs.SD) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Effective Connectivity-Based Unsupervised Channel Selection Method for EEG](https://arxiv.org/abs/2510.12910)
*Neda Abdollahpour,N. Sertac Artan,Ian Daly,Mohammadreza Yazdchi,Zahra Baharlouei*

Main category: eess.SP

TL;DR: 提出了一种基于有效连接性的无监督脑电通道选择方法ICEC，通过评估通道间的因果影响来选择最相关的通道，在三个EEG数据集上显著减少了通道数量并提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 处理高维脑电数据时，并非所有通道都提供同等有意义的信息，选择最相关通道对于提高计算效率和获得稳健的神经动力学洞察至关重要。

Method: 提出ICEC准则量化每个通道的有效连接性，使用五种有效连接性度量（PDC、GPDC、RPDC、DTF、dDTF），结合CSP算法进行特征提取和SVM分类。

Result: 在所有参与者中实现了性能的一致提升和电极数量的显著减少，在三个数据集上分别达到82%（13/22通道）、86.01%（29/59通道）和87.56%（48/118通道）的最高准确率。

Conclusion: 基于有效连接性的通道选择方法能够有效减少脑电数据维度，同时提高分类性能，优于现有最先进方法。

Abstract: Analyzing neural data such as Electroencephalography (EEG) data often
involves dealing with high-dimensional datasets, where not all channels provide
equally meaningful informa- tion. Selecting the most relevant channels is
crucial for improving computational efficiency and ensuring robust insights
into neural dynamics. This study introduces the Importance of Channels based on
Effective Connectivity (ICEC) criterion for quantifying effective connectivity
(EC) in each channel. Effective connectivity refers to the causal influence one
neural region exerts over another, providing insights into the directional flow
of information. Using this criterion, we propose an unsupervised channel
selection method that accounts for the intensity of interactions among
channels. To evaluate the proposed channel selection method, we applied it to
three well-known EEG datasets across four categories. The assessment involved
calculating the ICEC criterion using five effective connectivity metrics:
partial directed coherence (PDC), generalized PDC (GPDC), renormalized PDC
(RPDC), directed transfer function (DTF), and direct DTF (dDTF). To focus on
the effect of channel selection, we employed the Common Spatial Pattern (CSP)
algorithm for feature extraction and a Support Vector Machine (SVM) for
classification across all participants. Results were compared with other
CSP-based methods. The evaluation included comparing participant- specific
accuracies with and without the proposed method across five effective
connectivity metrics. The results showed consistent performance improvements
and a significant reduction in the number of selected electrodes for all
participants. Compared to state-of-the-art methods, our approach achieved the
highest accuracies: 82% (13 out of 22 channels), 86.01% (29 out of 59
channels), and 87.56% (48 out of 118 channels) across three datasets.

</details>


### [2] [Enabling Full Duplex ISAC Leveraging Waveform Domain Separability](https://arxiv.org/abs/2510.12912)
*Abdelali Arous,Hamza Haif,Huseyin Arslan*

Main category: eess.SP

TL;DR: 提出了一种新颖的波形域自干扰消除技术，利用OFDM和AFDM信号的特性，在单站带内全双工系统中实现集成感知与通信，有效解决雷达接收机的自干扰问题。


<details>
  <summary>Details</summary>
Motivation: 单站带内全双工系统中的集成感知与通信面临雷达接收机自干扰的严重挑战，需要开发有效的干扰消除技术来支持并发通信和雷达操作。

Method: 设计集成双功能帧，使用OFDM进行通信，AFDM进行雷达感知，两者使用相同的调制器块生成。将接收信号投影到仿射域，使自干扰表现为加性高斯白噪声，然后进行减法消除。采用迭代低复杂度加窗方案和时域扩展步骤进一步减少残余干扰。

Result: 所提方法在检测概率、目标距离和速度均方根误差方面表现出优越性能，同时保持高频谱效率和低计算复杂度。

Conclusion: 该波形域自干扰消除技术有效解决了ISAC系统中的自干扰问题，为单站带内全双工系统的集成感知与通信提供了可行的解决方案。

Abstract: Integrated sensing and communication (ISAC) in monostatic in-band full-duplex
(IBFD) systems encounters significant challenges due to self-interference (SI)
at the radar receiver during concurrent communication and radar operations.
This paper proposes a novel waveform-domain self-interference cancellation
(SIC) technique that leverages the unique properties of orthogonal frequency
division multiplexing (OFDM) and affine frequency division multiplexing (AFDM)
signals. The proposed approach designs the integrated dual-functionality frame
to utilize OFDM for communication and AFDM for radar sensing, both generated
using the same modulator block. Then, we establish the conditions under which a
wide sense stationary (WSS) process in the time domain appears as WSS in the
affine domain and demonstrate that the interfering OFDM signal behaves as an
additive white Gaussian noise (AWGN) in this domain. Exploiting this property,
the received signal is projected into the affine domain, where the SI appears
as AWGN, enabling its subtraction with minimal residual interference. To
further mitigate the residual SI, an iterative low-complexity windowing scheme
is applied, selectively locking onto the radar signal to reduce the processed
signal space. A subsequent time-domain spreading step is applied after
converting the SIC-processed signal into the post-coded time domain, wherein
the SI diminishes separately across the delay and Doppler axes. The proposed
method demonstrates superior performance in terms of detection probability,
target range and velocity root mean square error (RMSE), while maintaining high
spectral efficiency and minimal computational complexity.

</details>


### [3] [Passive Microwave Tag Classification Using RF Fingerprinting and Machine Learning](https://arxiv.org/abs/2510.12930)
*Cory Hilton,Mohammad Rashid,Faiz Sherman,Steven Bush,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 提出了一种使用射频指纹和机器学习识别无线微波标签的方法，通过二极管非线性响应产生的微小差异来实现设备唯一识别。


<details>
  <summary>Details</summary>
Motivation: 开发低成本、简单的无线标签识别方案，利用制造过程中二极管非线性特性的微小差异来实现设备认证。

Method: 设计2.0 GHz标签（贴片天线+二极管），使用双基地雷达系统发射多音连续波信号，通过机器学习算法分析标签的频谱响应差异。

Result: 在两个标签之间实现了95%的实时分类准确率。

Conclusion: 该方法证明了利用二极管非线性特性的微小制造差异可以实现有效的设备识别，为低成本无线认证提供了可行方案。

Abstract: We present an approach to identifying wireless microwave tags using radio
frequency (RF) fingerprinting and machine learning. The tags are designed for
low cost and simplicity, consisting of only two antennas and a single nonlinear
element (a diode). An interrogating transceiver transmits a signal consisting
of a set of individual frequency tones that is captured by the tag. The signal
response of the diode is nonlinear, and can be represented by an infinite power
series, the coefficients of which are similar but not identical for different
physical diodes due to small manufacturing perturbations. The small differences
in the signal responses manifest in the spectral signal response of the tag,
which is retransmitted back to the interrogating transceiver. Input into
machine learning algorithms, the slight differences in the spectral responses
of the diodes can be used to uniquely identify devices. To demonstrate the
concept, we designed 2.0 GHz tags consisting of patch antennas and a single
diode, along with a bi-static radar system operating at the 2.0 GHz 802.11
Wi-Fi band transmitting multi-tone continuous wave signals representing common
802.11 training fields. The received signals were processed using a set of
algorithms for comparison purposes. A real-time classification accuracy of 95%
between two tags was achieved.

</details>


### [4] [Computationally Efficient Neural Receivers via Axial Self-Attention](https://arxiv.org/abs/2510.12941)
*SaiKrishna Saketh Yellapragada,Atchutaram K. Kocharlakota,Mário Costa,Esa Ollila,Sergiy A. Vorobyov*

Main category: eess.SP

TL;DR: 提出了轴向自注意力变换器神经接收器，通过沿时间和频谱轴分解注意力操作，将复杂度从O((TF)^2)降低到O(T^2F+TF^2)，在5G兼容实验配置中实现了最先进的误块率性能，计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习神经接收器正在重新定义下一代无线系统的物理层信号处理，需要为6G及以后系统设计可扩展且高效的架构。

Method: 采用轴向自注意力变换器架构，将注意力操作沿时间和频谱轴分解，减少传统多头自注意力的二次复杂度，降低浮点运算和注意力矩阵乘法次数。

Result: 在3GPP CDL信道下验证，在非视距CDL-C条件下始终优于所有评估的接收器架构，包括全局自注意力、卷积神经接收器和传统LS-LMMSE，在10% BLER下计算复杂度更低。在1% BLER的严格可靠性目标下，传统LS-LMMSE接收器无法收敛，而轴向接收器保持稳健的符号检测。

Conclusion: 轴向神经接收器为AI原生6G RAN系统提供了一个结构化、可扩展且高效的框架，可在资源受限的边缘环境中部署，特别适用于动态6G环境中的超可靠低延迟通信。

Abstract: Deep learning-based neural receivers are redefining physical-layer signal
processing for next-generation wireless systems. We propose an axial
self-attention transformer neural receiver designed for applicability to 6G and
beyond wireless systems, validated through 5G-compliant experimental
configurations, that achieves state-of-the-art block error rate (BLER)
performance with significantly improved computational efficiency. By
factorizing attention operations along temporal and spectral axes, the proposed
architecture reduces the quadratic complexity of conventional multi-head
self-attention from $O((TF)^2)$ to $O(T^2F+TF^2)$, yielding substantially fewer
total floating-point operations and attention matrix multiplications per
transformer block compared to global self-attention. Relative to convolutional
neural receiver baselines, the axial neural receiver achieves significantly
lower computational cost with a fraction of the parameters. Experimental
validation under 3GPP Clustered Delay Line (CDL) channels demonstrates
consistent performance gains across varying mobility scenarios. Under
non-line-of-sight CDL-C conditions, the axial neural receiver consistently
outperforms all evaluated receiver architectures, including global
self-attention, convolutional neural receivers, and traditional LS-LMMSE at
10\% BLER with reduced computational complexity per inference. At stringent
reliability targets of 1\% BLER, the axial receiver maintains robust symbol
detection at high user speeds, whereas the traditional LS-LMMSE receiver fails
to converge, underscoring its suitability for ultra-reliable low-latency
(URLLC) communication in dynamic 6G environments and beyond. These results
establish the axial neural receiver as a structured, scalable, and efficient
framework for AI-Native 6G RAN systems, enabling deployment in
resource-constrained edge environments.

</details>


### [5] [Towards Spectrally Efficient and Physically Reconfigurable Architectures for Multibeam-Waveform Co-Design in Joint Communication and Sensing](https://arxiv.org/abs/2510.12968)
*Najme Ebrahimi,Arun Paidmarri,Alexandra Gallyas-Sanhueza,Yuan Ma,Haoling Li,Basem Abdelaziz Abdelmagid,Tzu-Yuan Huang,Hua Wang*

Main category: eess.SP

TL;DR: 该论文研究了用于联合通信与感知(JCAS)的多波束架构，比较了OFDM、FMA、TMA、直接RF/MMW调制和CDMA等系统在频谱效率、波束正交性、延迟和AoA估计精度等方面的性能。


<details>
  <summary>Details</summary>
Motivation: 随着毫米波和亚太赫兹系统的发展，JCAS平台成为下一代系统的基础，需要在共享信号路径中同时实现高速数据传输和角度定位。

Method: 通过分析多波束架构，同时优化时域、频域、码域和直接模拟/RF域的波形整形和波束成形技术。

Result: 研究结果突出了不同架构在波束敏捷性、效率、精度与分辨率以及复杂度之间的特定权衡关系。

Conclusion: 论文提供了一个框架，用于选择针对功率、延迟、波束间和多用户干扰以及快速系统重构优化的JCAS前端。

Abstract: Joint Communication and Sensing (JCAS) platforms are emerging as a foundation
of next-generation mmWave (MMW) and sub-THz systems, enabling both
high-throughput data transfer and angular localization within a shared signal
path. This paper investigates multibeam architectures for JCAS that
simultaneously optimize waveform shaping and beamforming across the time,
frequency, code, and direct analog/ radio frequency (RF) domains. The paper
compares Orthogonal Frequency-Division Multiplexing (OFDM), Frequency Modulated
Arrays (FMA), Time-Modulated Arrays (TMA), direct RF/MMW modulation, and
Code-Division Multiple Access (CDMA)-based systems with respect to spectral
efficiency, beam orthogonality, latency, and Angle-of-Arrival (AoA) estimation
accuracy. The results highlight architecture-specific tradeoffs among beam
agility, efficiency, accuracy and resolution, and complexity. It also provides
a framework for selecting JCAS front ends optimized for power, latency,
inter-beam and multi-user interference, and rapid system reconfiguration

</details>


### [6] [Constellation Design in OFDM-ISAC over Data Payloads: From MSE Analysis to Experimentation](https://arxiv.org/abs/2510.13101)
*Kawon Han,Kaitao Meng,Alexandra Chatzicharistou,Christos Masouros*

Main category: eess.SP

TL;DR: 本文研究了OFDM-ISAC系统中多目标延迟估计的感知性能，建立了估计理论框架，推导了匹配滤波和互易滤波接收器的延迟估计MSE闭式表达式，并提出了接收器相关的ISAC星座设计方案。


<details>
  <summary>Details</summary>
Motivation: OFDM因其高频谱效率和与现代通信标准的兼容性，在ISAC系统中被广泛采用。本文旨在研究OFDM-ISAC在多目标延迟估计场景下的感知性能，并分析不同接收器架构下信号星座对性能的影响。

Method: 开发了估计理论框架来表征随机通信载荷下的感知性能，推导了匹配滤波和互易滤波接收器的延迟估计MSE闭式表达式，并基于分析提出了ISAC星座设计方案。

Result: 研究发现，在多目标场景下，信号星座对延迟估计MSE的影响因接收器而异：MF性能取决于零均值单位功率星座的四阶矩，而RF性能取决于其二阶矩的倒数。通过提出的星座设计实现了感知与通信性能的灵活权衡。

Conclusion: 本文建立了OFDM-ISAC系统中多目标延迟估计的理论框架，揭示了不同接收器架构下星座设计对感知性能的影响机制，并通过实验验证了理论发现和所提出的星座设计方案的有效性。

Abstract: Orthogonal frequency division multiplexing (OFDM) is one of the most widely
adopted waveforms for integrated sensing and communication (ISAC) systems,
owing to its high spectral efficiency and compatibility with modern
communication standards. This paper investigates the sensing performance of
OFDM-based ISAC for multi-target delay (range) estimation under specific radar
receiver processing schemes. An estimation-theoretic framework is developed to
characterize sensing performance with random communication payloads. We
establish the fundamental limit of delay estimation accuracy by deriving the
closed-form expression of the mean-square error (MSE) achieved using matched
filtering (MF) and reciprocal filtering (RF) receivers. The results show that,
in multi-target scenarios, the impact of signal constellations on the delay
estimation MSE differs across receivers: MF performance depends on the
fourth-order moment of the zero-mean, unit-power constellation in the presence
of multiple targets, whereas RF performance depends on its inverse second-order
moment, irrespective of the number of targets. Building on this analysis, we
present a ISAC constellation design under specific receiver architecture that
brings a receiver-dependent flexible trade-off between sensing and
communication in OFDM-ISAC systems. The theoretical findings are validated
through simulations and proof-of-concept experiments, and also the sensing and
communication performance trade-off is experimentally shown with the proposed
constellation design.

</details>


### [7] [Working Memory Functional Connectivity Analysis for Dementia Classification using EEG](https://arxiv.org/abs/2510.13399)
*Shivani Ranjan,Anant Jain,Robin Badal,Amit Kumar,Harshal Shende,Deepak Joshi,Pramod Yadav,Lalan Kumar*

Main category: eess.SP

TL;DR: 该研究通过工作记忆任务中的EEG功能连接分析，使用CPTE方法在痴呆症不同阶段（AD、MCI、健康对照）实现了97.53%的分类准确率，为早期神经退行性疾病检测提供了有效的非侵入性诊断工具。


<details>
  <summary>Details</summary>
Motivation: 早期检测痴呆症特别是阿尔茨海默病至关重要，工作记忆损伤是神经退行性变的早期指标。EEG具有高时间分辨率，可评估脑网络动态变化。

Method: 对24名参与者（8名AD、8名MCI、8名健康对照）在工作记忆任务期间记录EEG信号，使用SHD和HHD进行特征提取，采用CPTE和PLI量化功能连接，使用多种机器学习分类器分析网络指标。

Result: CPTE方法优于传统PLI方法，在检索阶段使用随机森林模型达到97.53%的分类准确率，AD患者在WM任务中表现出更高的同步模式。

Conclusion: 工作记忆任务与EEG功能连接分析的结合为痴呆症分类提供了稳健框架，CPTE方法为神经退行性疾病的早期检测和监测提供了有效诊断工具。

Abstract: Background: Dementia, particularly Alzheimer's Disease (AD), is a progressive
neurodegenerative disorder marked by cognitive decline. Early detection,
especially at the Mild Cognitive Impairment (MCI) stage, is essential for
timely intervention. Working Memory (WM) impairment is a key early indicator of
neurodegeneration, affecting higher cognitive processes. Electroencephalography
(EEG), with its high temporal resolution, offers a cost-effective method to
assess brain dynamics. This study investigates WM-related EEG functional
connectivity (FC) to identify brain network alterations across dementia stages.
Methods: EEG signals were recorded from 24 participants (8 AD, 8 MCI, and 8
healthy controls) during WM tasks, including encoding, recall, and retrieval
stages. Data preprocessing involved noise reduction and feature extraction
using Spherical and Head Harmonic Decomposition (SHD, HHD). FC was quantified
using Cross-Plot Transition Entropy (CPTE) and Phase Lag Index (PLI). Network
metrics such as Degree and Eigenvector Centrality were analyzed using Support
Vector Machine, Random Forest, and XGBoost classifiers. Results: The CPTE-based
connectivity metrics outperformed the traditional PLI approach in
differentiating dementia stages, attaining a peak classification accuracy of
97.53% during the retrieval phase with the Random Forest model. A connectivity
threshold of 0.5 was optimal for network discrimination. SHD and HHD features
also demonstrated strong discriminative potential. AD subjects exhibited higher
synchronization patterns during WM tasks than healthy controls. Conclusions:
The integration of WM tasks with EEG-based FC analysis provides a robust
framework for dementia classification. The proposed CPTE-based approach offers
a robust, scalable, non-invasive, and effective diagnostic tool for early
detection and monitoring of neurodegenerative diseases.

</details>


### [8] [Oscillator Drift Compensation by Line-of-Sight Tracking for Distributed Multisensor ISAC](https://arxiv.org/abs/2510.13442)
*Lorenz Mohr,Marc Miranda,Sebastian Semper,Julia Beuster,Carsten Andrich,Sebastian Giehl,Christian Schneider,Reiner S. Thomä*

Main category: eess.SP

TL;DR: 提出了一种基于卡尔曼滤波的几何漂移补偿算法，用于校正移动多传感器信道探测中的同步失配问题，通过改进LoS跟踪提高了多径场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 移动多传感器信道探测中存在同步失配问题，表现为非平滑相位进展和漂移，而分布式ISAC系统的多普勒估计需要节点间相干性，这要求接收信号具有连续可微的相位进展。

Method: 扩展传统基于几何的漂移补偿算法，利用卡尔曼滤波进行LoS跟踪，平滑相位进展并校正时变漂移，同时保留相对传感器运动。提出使用高分辨率参数估计后的相对剩余功率作为后处理同步方法的评估指标。

Result: 所提方法优于传统LoS估计启发式方法，相对剩余功率降低超过5 dB，延迟-多普勒估计均方根误差降低约60%。

Conclusion: 基于卡尔曼滤波的LoS跟踪方法能有效校正同步失配，提高多径场景下的鲁棒性，为信道探测数据的后处理同步提供了有效的解决方案。

Abstract: We observed synchronization mismatches in the form of non-smooth phase
progressions and drifts within mobile multisensor channel sounding
measurements. However, performing Doppler estimation in a distributed
multisensor integrated sensing and communications (ISAC) system requires
coherence among the nodes, which implies a continuously differentiable phase
progression of the received signals. To correct the sounding data in
post-processing, we extend traditional geometry-based drift compensation
algorithms by utilizing Kalman filtering for line-of-sight (LoS) tracking,
which improves the robustness of the LoS estimate in multipath scenarios. This
approach smooths the phase progression and enables the correction of
time-varying drifts while preserving relative sensor motion. Furthermore, we
propose using the relative residual power after high-resolution parameter
estimation (HRPE) as a metric for ground-truth-independent comparison of
post-processing synchronization methods for recorded channel sounding data.
Results show that the proposed approach outperforms traditional LoS estimation
heuristics, reducing the relative residual power by more than 5 dB and the
delay-Doppler estimate root mean square errors (RMSEs) by approximately 60 %.

</details>


### [9] [Radio over Fiber with Cascaded Structure: Algorithm for Uplink Positioning](https://arxiv.org/abs/2510.13495)
*Dexin Kong,Diana Pamela Moya Osorio,Erik G. Larsson*

Main category: eess.SP

TL;DR: 本文提出了一种新型级联式光纤无线通信结构，用于室内场景的多无线电单元连接，并开发了最大似然和非线性最小二乘算法来估计光纤传播距离和到达时间，同时研究了该系统在上行链路定位中的应用。


<details>
  <summary>Details</summary>
Motivation: 聚合物微波光纤技术的进步为低成本、高速的亚太赫兹光纤无线通信创造了机会，本文旨在探索级联式光纤无线通信结构在室内场景中的应用潜力，特别是在考虑级联失真效应时的研究机会。

Method: 提出最大似然和非线性最小二乘算法来估计光纤传播距离和用户设备之间的到达时间；对于线性功率放大器，推导了克拉美-罗下界作为性能基准；使用实测的高密度聚乙烯光纤特性进行数值评估。

Result: 仿真结果表明，即使存在非线性功率放大器的级联效应，所提出的估计器也能表现良好；这种光纤无线通信结构的部署可以为室内场景的高分辨率定位提供新的成本效益机会。

Conclusion: 所提出的级联式光纤无线通信结构在室内定位应用中具有良好性能，能够有效应对非线性功率放大器和光纤传播信道带来的级联失真效应，为高分辨率室内定位提供了成本效益高的解决方案。

Abstract: Recent advancements in polymer microwave fiber (PMF) technology have created
significant opportunities for robust, low-cost, and high-speed sub-terahertz
(THz) radio-over- fiber communications. Recognizing these potential benefits,
this paper explores a novel radio-over-fiber (RoF) structure that interconnects
multiple radio units (RUs) in cascade via fiber, envi- sioning its application
in indoor scenarios. This structure creates a number of research opportunities
when considering cascaded distortion effects introduced by non-linear power
amplifiers (PAs) at the RUs and the propagation channel over the fiber. We
propose maximum-likelihood and non-linear least-squares algorithms to estimate
the propagation distance along the RoF and the time-of-arrival between the RoF
and the user equipment. For the case of linear PAs, we derive the Cram\'er-Rao
lower bound to benchmark the performance of the estimators. Finally, we
investigate the use of the system for uplink positioning. Our simulation
results demonstrate that the proposed estimators perform satisfactorily even
with the cascaded effects of non- linear PAs, and that the deployment of this
RoF structure can enable new cost-effective opportunities for high-resolution
positioning in indoor scenarios. In the numerical evaluation, we also use
measured PMF characteristics for high-density polyethylene fibers.

</details>


### [10] [A Robust EDM Optimization Approach for 3D Single-Source Localization with Angle and Range Measurements](https://arxiv.org/abs/2510.13498)
*Mingyu Zhao,Qingna Li,Hou-Duo Qi*

Main category: eess.SP

TL;DR: 提出了一种结合距离测量、角度测量和最小绝对偏差准则的鲁棒欧几里得距离矩阵优化模型，用于3D单源定位问题。


<details>
  <summary>Details</summary>
Motivation: 在源定位问题中，距离测量、角度测量和最小绝对偏差准则是三个关键要素，但将它们整合到一个计算可行的模型中具有挑战性。

Method: 将3D角度测量转化为距离的简单盒约束，通过将每个3D角度测量简化为二维非线性优化问题，获得距离的上下界，并开发了高效算法。

Result: 通过广泛的数值实验验证了新EDM模型在3D单源定位中的高质量定位性能，与领先的求解器相比表现出色。

Conclusion: 该研究成功构建了同时整合三个关键要素的计算可行模型，为3D单源定位提供了有效的解决方案。

Abstract: For the problem of source localization, three elements usually play a very
important role in accurate localization. They are the range measurements, the
angle measurements and the least absolute deviation criterion, which is
regarded as a robust metric for denoising the measurements. Building the three
elements into a computationally tractable model is challenging. In this paper,
we introduce a robust Euclidean Distance Matrix (EDM) optimization model that
simultaneously incorporates the three elements. For the first time, we show
that for the case of 3D single-source localization (3DSSL), the angle
measurements can be represented as a simple box constraint of distances. It is
achieved by reducing each of the 3D angle measurements to a two-dimensional
nonlinear optimization problem, whose global minimum and maximum solutions can
be characterized and utilized to get the lower and upper bounds of the
distances from the unknown source to the sensors. We further develop an
efficient algorithm. The high quality of the localization by the new EDM model
is assessed through extensive numerical experiments in comparison with leading
solvers for 3DSSL.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [11] [Automatic Speech Recognition in the Modern Era: Architectures, Training, and Evaluation](https://arxiv.org/abs/2510.12827)
*Md. Nayeem,Md Shamse Tabrej,Kabbojit Jit Deb,Shaonti Goswami,Md. Azizul Hakim*

Main category: eess.AS

TL;DR: 这篇综述系统回顾了自动语音识别(ASR)在过去十年中的发展历程，从传统混合系统到端到端神经架构，再到Transformer/Conformer模型，以及从监督学习到自监督学习的训练范式转变。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，ASR技术经历了深刻变革，需要全面梳理从传统系统到现代端到端架构的演进历程，以及训练范式的革命性变化。

Method: 系统回顾了ASR的核心技术演进：从GMM-HMMs、DNN-HMMs混合系统，到CTC、注意力编码器-解码器、RNN-T等端到端范式，再到Transformer/Conformer架构；同时分析了从监督学习到自监督学习(wav2vec 2.0)和大规模弱监督模型(Whisper)的训练范式转变。

Result: 现代ASR系统通过端到端架构和自监督学习实现了更高的性能和鲁棒性，大幅减少了对标注数据的依赖，并在大规模多样化数据上展现出前所未有的鲁棒性。

Conclusion: ASR技术已进入端到端神经架构和自监督学习时代，但仍面临流式推理、设备端效率、公平性和鲁棒性等实际部署挑战，需要继续研究解决。

Abstract: Automatic Speech Recognition (ASR) has undergone a profound transformation
over the past decade, driven by advances in deep learning. This survey provides
a comprehensive overview of the modern era of ASR, charting its evolution from
traditional hybrid systems, such as Gaussian Mixture Model-Hidden Markov Models
(GMM-HMMs) and Deep Neural Network-HMMs (DNN-HMMs), to the now-dominant
end-to-end neural architectures. We systematically review the foundational
end-to-end paradigms: Connectionist Temporal Classification (CTC),
attention-based encoder-decoder models, and the Recurrent Neural Network
Transducer (RNN-T), which established the groundwork for fully integrated
speech-to-text systems. We then detail the subsequent architectural shift
towards Transformer and Conformer models, which leverage self-attention to
capture long-range dependencies with high computational efficiency. A central
theme of this survey is the parallel revolution in training paradigms. We
examine the progression from fully supervised learning, augmented by techniques
like SpecAugment, to the rise of self-supervised learning (SSL) with foundation
models such as wav2vec 2.0, which drastically reduce the reliance on
transcribed data. Furthermore, we analyze the impact of largescale, weakly
supervised models like Whisper, which achieve unprecedented robustness through
massive data diversity. The paper also covers essential ecosystem components,
including key datasets and benchmarks (e.g., LibriSpeech, Switchboard, CHiME),
standard evaluation metrics (e.g., Word Error Rate), and critical
considerations for real-world deployment, such as streaming inference,
on-device efficiency, and the ethical imperatives of fairness and robustness.
We conclude by outlining open challenges and future research directions.

</details>


### [12] [HyWA: Hypernetwork Weight Adapting Personalized Voice Activity Detection](https://arxiv.org/abs/2510.12947)
*Mahsa Ghazvini Nejad,Hamed Jafarzadeh Asl,Amin Edraki,Mohammadreza Sadeghi,Masoud Asgharian,Yuanhao Yu,Vahid Partovi Nia*

Main category: eess.AS

TL;DR: 提出HyWA-PVAD方法，使用超网络修改标准语音活动检测模型的部分层权重，实现个性化语音活动检测，无需改变VAD架构。


<details>
  <summary>Details</summary>
Motivation: 现有PVAD方法需要架构修改（如FiLM层），而本方法旨在通过超网络实现说话人条件化，保持VAD架构不变，便于部署。

Method: 使用超网络修改标准VAD模型中选定层的权重，使同一VAD模型能适应不同说话人，仅更新少量层。

Result: 相比多种基线条件化技术，HyWA-PVAD在PVAD性能上持续提升，提高了平均精度均值。

Conclusion: HyWA方法在提升PVAD性能的同时，通过重用相同VAD架构简化了部署，具有实际部署优势。

Abstract: Personalized Voice Activity Detection (PVAD) systems activate only in
response to a specific target speaker by incorporating speaker embeddings from
enrollment utterances. Unlike existing methods that require architectural
changes, such as FiLM layers, our approach employs a hypernetwork to modify the
weights of a few selected layers within a standard voice activity detection
(VAD) model. This enables speaker conditioning without changing the VAD
architecture, allowing the same VAD model to adapt to different speakers by
updating only a small subset of the layers. We propose HyWA-PVAD, a
hypernetwork weight adaptation method, and evaluate it against multiple
baseline conditioning techniques. Our comparison shows consistent improvements
in PVAD performance. HyWA also offers practical advantages for deployment by
preserving the core VAD architecture. Our new approach improves the current
conditioning techniques in two ways: i) increases the mean average precision,
ii) simplifies deployment by reusing the same VAD architecture.

</details>


### [13] [Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs](https://arxiv.org/abs/2510.12995)
*Xinlu He,Swayambhu Nath Ray,Harish Mallidi,Jia-Hong Huang,Ashwin Bellur,Chander Chandak,M. Maruf,Venkatesh Ravichandran*

Main category: eess.AS

TL;DR: 提出了一种在MLLM框架下使用连续语音表示的文本转语音方法，通过双头架构和两阶段训练策略，结合自回归建模和连续token扩散，在LibriSpeech测试集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的TTS方法依赖离散token表示，忽略了语音的连续特性，导致细粒度声学信息丢失。本文旨在在MLLM范式下使用连续语音表示来解决这个问题。

Method: 设计了双头架构：扩散头生成连续语音表示（帧级严格自回归），保留语言模型头以维持多任务能力；采用掩码训练解决自回归解码中的暴露偏差；提出两阶段训练方案，第二阶段冻结LM以确保扩散头从固定输入分布学习。

Result: 在LibriSpeech测试集上WER为1.95%，说话人相似度为0.54，UTMOS为4.00，达到最先进的自回归性能；两阶段训练相比单阶段基线相对WER降低46%。

Conclusion: 结合自回归建模与连续token扩散，并辅以两阶段训练程序，在MLLM框架下实现了有效的连续语音表示TTS方法。

Abstract: Unified architectures in multimodal large language models (MLLM) have shown
promise in handling diverse tasks within a single framework. In the
text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token
representations, which disregard the inherently continuous nature of speech and
can lead to loss of fine-grained acoustic information.In this work, we
investigate the TTS within the MLLM paradigm using continuous speech
representations. We design a dual-head architecture and implement two
complementary training strategies for a robust model. (1) A diffusion head
generating continuous speech representations is added on the MLLM, which is on
frame-level and strictly autoregressive. (2) The original language model head
is retained to preserve multitask capability and to control the start and end
of speech synthesis. (3) Masked training is employed to address exposure bias
in autoregressive decoding. (4) To stabilize optimization, we propose a
two-stage scheme where the LM is frozen in the second stage, ensuring the
diffusion head learns from a fixed input distribution. Evaluations on
LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art
autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54,
and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction
over the one-stage training baseline. These results highlight the effectiveness
of combining autoregressive modeling with continuous-token diffusion, supported
by a two-stage training procedure.

</details>


### [14] [Acoustic Teleportation via Disentangled Neural Audio Codec Representations](https://arxiv.org/abs/2510.13221)
*Philipp Grundhuber,Mhd Modar Halimeh,Emanuël A. P. Habets*

Main category: eess.AS

TL;DR: 本文提出了一种通过解耦神经音频编解码器表示中的语音内容和声学环境特征来实现声学传送的方法，在保持内容和说话人身份的同时传输房间特性，相比先前方法在客观质量上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 实现声学传送，即在语音录音之间传输房间声学特性，同时保持语音内容和说话人身份不变。

Method: 基于EnCodec架构，采用包含五个任务的训练策略：干净重建、混响重建、去混响以及两种声学传送变体。研究发现声学嵌入的时间下采样会显著降低性能。

Result: 实现了3.03的非侵入式ScoreQ分数，相比先前方法的2.44有显著提升。声学嵌入与RT60有强相关性，t-SNE聚类分析显示声学嵌入按房间聚类，语音嵌入按说话人聚类，证明了有效的解耦。

Conclusion: 该方法成功实现了声学环境特征的解耦和传输，声学嵌入的时间分辨率对性能至关重要，学习到的表示能够有效区分房间特性和说话人特征。

Abstract: This paper presents an approach for acoustic teleportation by disentangling
speech content from acoustic environment characteristics in neural audio codec
representations. Acoustic teleportation transfers room characteristics between
speech recordings while preserving content and speaker identity. We build upon
previous work using the EnCodec architecture, achieving substantial objective
quality improvements with non-intrusive ScoreQ scores of 3.03, compared to 2.44
for prior methods. Our training strategy incorporates five tasks: clean
reconstruction, reverberated reconstruction, dereverberation, and two variants
of acoustic teleportation. We demonstrate that temporal downsampling of the
acoustic embedding significantly degrades performance, with even 2x
downsampling resulting in a statistically significant reduction in quality. The
learned acoustic embeddings exhibit strong correlations with RT60. Effective
disentanglement is demonstrated using t-SNE clustering analysis, where acoustic
embeddings cluster by room while speech embeddings cluster by speaker.

</details>


### [15] [Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses](https://arxiv.org/abs/2510.13281)
*Sungnyun Kim,Kangwook Jang,Sungwoo Cho,Joon Son Chung,Hoirin Kim,Se-Young Yun*

Main category: eess.AS

TL;DR: DualHyp是一个用于视听语音识别的生成式纠错框架，它让大语言模型能够基于独立的ASR和VSR模型的N-best假设进行推理，并通过RelPrompt机制提供模态感知的引导。


<details>
  <summary>Details</summary>
Motivation: 现有的单流生成式纠错方法在视听语音识别中效果有限，需要一种能够有效利用多模态证据进行纠错的新方法。

Method: 提出DualHyp框架，让LLM基于独立的ASR和VSR假设进行推理；引入RelPrompt机制提供模态可靠性感知的引导，动态调整对ASR和VSR假设的关注度。

Result: 在各种噪声场景下，在LRS2基准上相比标准ASR基线实现了57.7%的错误率提升，而单流GER方法仅实现10%提升。

Conclusion: DualHyp框架通过有效利用多模态证据和可靠性感知的引导机制，显著提升了视听语音识别的纠错性能。

Abstract: This paper introduces a new paradigm for generative error correction (GER)
framework in audio-visual speech recognition (AVSR) that reasons over
modality-specific evidences directly in the language space. Our framework,
DualHyp, empowers a large language model (LLM) to compose independent N-best
hypotheses from separate automatic speech recognition (ASR) and visual speech
recognition (VSR) models. To maximize the effectiveness of DualHyp, we further
introduce RelPrompt, a noise-aware guidance mechanism that provides
modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability
of each modality stream, guiding the model to dynamically switch its focus
between ASR and VSR hypotheses for an accurate correction. Under various
corruption scenarios, our framework attains up to 57.7% error rate gain on the
LRS2 benchmark over standard ASR baseline, contrary to single-stream GER
approaches that achieve only 10% gain. To facilitate research within our
DualHyp framework, we release the code and the dataset comprising ASR and VSR
hypotheses at https://github.com/sungnyun/dualhyp.

</details>


### [16] [Towards Multimodal Query-Based Spatial Audio Source Extraction](https://arxiv.org/abs/2510.13308)
*Chenxin Yu,Hao Ma,Xu Li,Xiao-Lei Zhang,Mingjie Shao,Chi Zhang,Xuelong Li*

Main category: eess.AS

TL;DR: 提出基于查询的空间音频源提取框架，使用三轴Transformer联合建模时域、频域和空间通道依赖关系，支持音频或文本提示作为条件输入，通过无标签数据管道实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的音频源提取方法主要局限于单声道音频，未能充分利用多声道录音中的空间信息。

Method: 使用三轴Transformer建模时域、频域和空间通道依赖关系；采用CLAP嵌入实现统一的音频-文本条件输入；提出无标签数据管道动态生成空间混音和对应目标进行训练。

Result: 实验结果显示高分离质量，证明了多模态条件和三轴建模的有效性。

Conclusion: 这项工作为沉浸式应用中的高保真空间音频分离建立了新范式。

Abstract: Query-based audio source extraction seeks to recover a target source from a
mixture conditioned on a query. Existing approaches are largely confined to
single-channel audio, leaving the spatial information in multi-channel
recordings underexploited. We introduce a query-based spatial audio source
extraction framework for recovering dry target signals from first-order
ambisonics (FOA) mixtures. Our method accepts either an audio prompt or a text
prompt as condition input, enabling flexible end-to-end extraction. The core of
our proposed model lies in a tri-axial Transformer that jointly models
temporal, frequency, and spatial channel dependencies. The model uses
contrastive language-audio pretraining (CLAP) embeddings to enable unified
audio-text conditioning via feature-wise linear modulation (FiLM). To eliminate
costly annotations and improve generalization, we propose a label-free data
pipeline that dynamically generates spatial mixtures and corresponding targets
for training. The result of our experiment with high separation quality
demonstrates the efficacy of multimodal conditioning and tri-axial modeling.
This work establishes a new paradigm for high-fidelity spatial audio separation
in immersive applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [17] [Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis](https://arxiv.org/abs/2510.12819)
*Junyao Huang,Rumin Situ*

Main category: cs.SD

TL;DR: 提出了一种基于连续效价-唤醒度(VA)模型的宠物情感识别方法，替代传统的离散分类，能更好地处理情感模糊性和强度变化。


<details>
  <summary>Details</summary>
Motivation: 传统基于离散分类的宠物情感识别方法难以处理情感模糊性和捕捉强度变化，需要更连续的情感表示方法。

Method: 使用自动VA标签生成算法标注42,553个宠物发声样本，采用多任务学习框架联合训练VA回归和辅助任务（情感、体型、性别），使用音频Transformer模型。

Result: 验证集上效价Pearson相关系数r=0.9024，唤醒度r=0.7155，有效解决了'领地性'和'快乐'等离散类别间的混淆。

Conclusion: 这是首个用于宠物发声分析的连续VA框架，为人类-宠物互动、兽医诊断和行为训练提供了更具表现力的情感表示，在AI宠物情感翻译器等消费产品中具有部署潜力。

Abstract: Traditional pet emotion recognition from vocalizations, based on discrete
classification, struggles with ambiguity and capturing intensity variations. We
propose a continuous Valence-Arousal (VA) model that represents emotions in a
two-dimensional space. Our method uses an automatic VA label generation
algorithm, enabling large-scale annotation of 42,553 pet vocalization samples.
A multi-task learning framework jointly trains VA regression with auxiliary
tasks (emotion, body size, gender) to enhance prediction by improving feature
learning. Our Audio Transformer model achieves a validation Valence Pearson
correlation of r = 0.9024 and an Arousal r = 0.7155, effectively resolving
confusion between discrete categories like "territorial" and "happy." This work
introduces the first continuous VA framework for pet vocalization analysis,
offering a more expressive representation for human-pet interaction, veterinary
diagnostics, and behavioral training. The approach shows strong potential for
deployment in consumer products like AI pet emotion translators.

</details>


### [18] [Production and Manufacturing of 3D Printed Acoustic Guitars](https://arxiv.org/abs/2510.12823)
*Timothy Tran,William Schiesser*

Main category: cs.SD

TL;DR: 本研究探讨了使用3D打印技术制造经济实惠、功能正常的原声吉他的可行性，重点关注具有良好音质性能的结构设计。


<details>
  <summary>Details</summary>
Motivation: 通过3D打印技术降低吉他制造成本，减少对濒危音木的依赖，促进可持续乐器生产和音乐参与，特别是为弱势群体提供乐器获取机会。

Method: 使用Prusa Mark 4 3D打印机和PLA材料制造古典吉他原型，由于打印平台尺寸限制，将琴体分为多个部分，采用压配合公差和少量氰基丙烯酸酯粘合剂连接，使用Fusion 360进行CAD建模确保尺寸精度。

Result: 测试显示低音弦频率存在较大偏差，但通过调音所有弦都能达到准确音高，证明PLA和现代制造方法可以生产经济实惠、可演奏的原声吉他。

Conclusion: 3D打印技术具有显著潜力，可以扩大优质乐器的获取渠道，同时减少对濒危音木的依赖，促进可持续乐器生产和音乐参与。

Abstract: This research investigates the feasibility of producing affordable,
functional acoustic guitars using 3D printing, with a focus on producing
structural designs with proper tonal performance. Conducted in collaboration
with William Schiesser, the study uses a classical guitar model, chosen for its
lower string tension, to evaluate the tonal characteristics of a 3D-printed
prototype made from polylactic acid (PLA). Due to the build plate size
constraints of the Prusa Mark 4 printer, the guitar body was divided into
multiple sections joined with press-fit tolerances and minimal cyanoacrylate
adhesive. CAD modeling in Fusion 360 ensured dimensional accuracy in press-fit
connections and the overall assembly. Following assembly, the guitar was strung
with nylon strings and tested using Audacity software to compare recorded
frequencies and notes with standard reference values. Results showed large
deviations in lower string frequencies, likely caused by the material choice
utilized in printing. Accurate pitches were reached with all strings despite
frequency differences through tuning, demonstrating that PLA and modern
manufacturing methods can produce affordable, playable acoustic guitars despite
inevitable challenges. Further research may investigate alternative plastics
for superior frequency matching. This approach holds significant potential for
expanding access to quality instruments while reducing reliance on endangered
tonewoods, thereby encouraging both sustainable instrument production and
increased musical participation. This also creates opportunities for
disadvantaged communities where access to musical instruments remains a
challenge.
  Keywords: Luthiery, Stereolithography, 3D-Print, Guitar Making

</details>


### [19] [Gelina: Unified Speech and Gesture Synthesis via Interleaved Token Prediction](https://arxiv.org/abs/2510.12834)
*Téo Guichoux,Théodor Lemerle,Shivam Mehta,Jonas Beskow,Gustave Eje Henter,Laure Soulier,Catherine Pelachaud,Nicolas Obin*

Main category: cs.SD

TL;DR: Gelina是一个统一框架，通过离散自回归主干和模态特定解码器，从文本联合合成语音和伴随手势，支持多说话人和多风格克隆。


<details>
  <summary>Details</summary>
Motivation: 人类交流是多模态的，语音和手势紧密耦合，但现有计算方法通常顺序合成语音和手势，削弱了同步性和韵律对齐。

Method: 使用交错标记序列在离散自回归主干中联合合成语音和手势，配备模态特定解码器，支持从语音输入进行仅手势合成。

Result: 主观和客观评估显示具有竞争力的语音质量和优于单模态基线的手势生成效果。

Conclusion: Gelina框架成功实现了语音和手势的联合合成，在多说话人和多风格克隆方面表现出色。

Abstract: Human communication is multimodal, with speech and gestures tightly coupled,
yet most computational methods for generating speech and gestures synthesize
them sequentially, weakening synchrony and prosody alignment. We introduce
Gelina, a unified framework that jointly synthesizes speech and co-speech
gestures from text using interleaved token sequences in a discrete
autoregressive backbone, with modality-specific decoders. Gelina supports
multi-speaker and multi-style cloning and enables gesture-only synthesis from
speech inputs. Subjective and objective evaluations demonstrate competitive
speech quality and improved gesture generation over unimodal baselines.

</details>


### [20] [Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models](https://arxiv.org/abs/2510.12851)
*Tsung-En Lin,Kuan-Yi Lee,Hung-Yi Lee*

Main category: cs.SD

TL;DR: 提出自适应向量转向(AVS)方法，通过探测模型内部状态来减少音频-语言模型中的幻觉问题，在多个基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型音频-语言模型和多模态大语言模型在音频问答等任务中表现出色，但存在对音频内容产生幻觉的问题，需要解决这一挑战。

Method: 提出自适应向量转向(AVS)方法，通过分析模型内部状态，使生成内容更好地基于音频内容进行定位。

Result: 在两个模型和两个基准测试中均获得一致性能提升：在Audio Hallucination QA数据集上，Gemma的F1分数从0.550提升至0.619，Qwen从0.626提升至0.632；在MMAU上，Qwen准确率从0.548提升至0.592，相对提升8%。

Conclusion: 这是首个将向量转向技术应用于缓解音频幻觉问题的工作，实验证明输出正确性与内部表示之间存在强相关性，AVS方法能有效提升模型性能。

Abstract: Large Audio-Language Models and Multi-Modal Large Language Models have
demonstrated strong capabilities in tasks such as Audio Question Answering
(AQA), Audio Captioning, and Automatic Speech Recognition (ASR). However, there
is growing evidence that these models can hallucinate about the content of the
audio. To address this issue, we probe the models' internal states and propose
Adaptive Vector Steering (AVS), a method that better grounds generation in
audio content. We also identify a strong correlation between output correctness
and internal representations. Experiments show consistent performance gains
across two models and two benchmarks. On the Audio Hallucination QA dataset,
our method boosts the F1-score of Gemma from 0.550 to 0.619 and Qwen from 0.626
to 0.632. Furthermore, our method increases the accuracy of Qwen on MMAU from
0.548 to 0.592, marking an 8% relative increase. To the best of our knowledge,
this is the first work to apply vector steering to mitigate hallucination in
audio.

</details>


### [21] [VCTR: A Transformer-Based Model for Non-parallel Voice Conversion](https://arxiv.org/abs/2510.12964)
*Maharnab Saikia*

Main category: cs.SD

TL;DR: VCTR是一种用于非平行语音转换的高效方法，结合了混合感知块(HPB)、双剪枝自注意力(DPSA)和基于对比学习的对抗方法，解决了现有方法难以训练和结果不理想的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的非平行语音转换方法如CycleGAN和VAE存在训练困难、结果不理想的问题，而基于对比学习的CVC方法虽然有所改进，但其CNN生成器只能捕捉局部语义，缺乏捕捉长距离依赖关系的能力，无法处理全局语义。

Method: 提出VCTR方法，采用混合感知块(HPB)和双剪枝自注意力(DPSA)机制，结合基于对比学习的对抗训练方法，以更好地捕捉语音转换中的长距离依赖关系和全局语义。

Result: 该方法在非平行语音转换任务中表现出色，代码已在GitHub上开源。

Conclusion: VCTR通过引入HPB和DPSA机制，有效解决了现有非平行语音转换方法在捕捉长距离依赖关系方面的不足，提供了一种更高效的语音转换解决方案。

Abstract: Non-parallel voice conversion aims to convert voice from a source domain to a
target domain without paired training data. Cycle-Consistent Generative
Adversarial Networks (CycleGAN) and Variational Autoencoders (VAE) have been
used for this task, but these models suffer from difficult training and
unsatisfactory results. Later, Contrastive Voice Conversion (CVC) was
introduced, utilizing a contrastive learning-based approach to address these
issues. However, these methods use CNN-based generators, which can capture
local semantics but lacks the ability to capture long-range dependencies
necessary for global semantics. In this paper, we propose VCTR, an efficient
method for non-parallel voice conversion that leverages the Hybrid Perception
Block (HPB) and Dual Pruned Self-Attention (DPSA) along with a contrastive
learning-based adversarial approach. The code can be found in
https://github.com/Maharnab-Saikia/VCTR.

</details>


### [22] [MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding](https://arxiv.org/abs/2510.13244)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.SD

TL;DR: MotionBeat是一个运动对齐的音乐表示学习框架，通过增强的对比损失和结构节奏对齐损失来捕捉音乐的节奏和结构线索，在音乐到舞蹈生成等任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有音频表示忽略了音乐的具身维度，无法充分捕捉驱动人类运动的节奏和结构线索。

Method: 提出Embodied Contrastive Loss（ECL）和Structural Rhythm Alignment Loss（SRAL）两个新目标，引入bar-equivariant相位旋转和接触引导注意力机制。

Result: MotionBeat在音乐到舞蹈生成任务中优于最先进的音频编码器，并能有效迁移到节拍跟踪、音乐标记、分类和检索等任务。

Conclusion: MotionBeat通过结合音乐的具身维度，实现了更好的节奏感知和运动对齐的音乐表示学习。

Abstract: Music is both an auditory and an embodied phenomenon, closely linked to human
motion and naturally expressed through dance. However, most existing audio
representations neglect this embodied dimension, limiting their ability to
capture rhythmic and structural cues that drive movement. We propose
MotionBeat, a framework for motion-aligned music representation learning.
MotionBeat is trained with two newly proposed objectives: the Embodied
Contrastive Loss (ECL), an enhanced InfoNCE formulation with tempo-aware and
beat-jitter negatives to achieve fine-grained rhythmic discrimination, and the
Structural Rhythm Alignment Loss (SRAL), which ensures rhythm consistency by
aligning music accents with corresponding motion events. Architecturally,
MotionBeat introduces bar-equivariant phase rotations to capture cyclic
rhythmic patterns and contact-guided attention to emphasize motion events
synchronized with musical accents. Experiments show that MotionBeat outperforms
state-of-the-art audio encoders in music-to-dance generation and transfers
effectively to beat tracking, music tagging, genre and instrument
classification, emotion recognition, and audio-visual retrieval. Our project
demo page: https://motionbeat2025.github.io/.

</details>


### [23] [UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE](https://arxiv.org/abs/2510.13344)
*Zhenyu Liu,Yunxin Li,Xuanyu Zhang,Qixun Teng,Shenyuan Jiang,Xinyu Chen,Haoyuan Shi,Jinchao Li,Qi Wang,Haolan Chen,Fanbo Meng,Mingjun Zhao,Yu Xu,Yancheng He,Baotian Hu,Min Zhang*

Main category: cs.SD

TL;DR: UniMoE-Audio是一个统一的语音和音乐生成模型，采用动态容量专家混合框架，通过三阶段训练策略解决数据不平衡问题，在语音和音乐生成基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型在听觉领域面临挑战，音乐和语音通常独立发展，阻碍了通用音频合成的发展。这种分离源于任务冲突和数据不平衡问题。

Method: 提出UniMoE-Audio模型，采用动态容量MoE框架，包含Top-P路由策略和混合专家设计（路由专家、共享专家和空专家）。使用三阶段训练课程：独立专家训练、MoE集成预热、协同联合训练。

Result: 在主要语音和音乐生成基准测试中达到最先进性能，展示了优越的协同学习能力，缓解了朴素联合训练中常见的性能下降问题。

Conclusion: 专业化MoE架构和精心设计的训练策略在推进通用音频生成领域具有巨大潜力。

Abstract: Recent advances in unified multimodal models indicate a clear trend towards
comprehensive content generation. However, the auditory domain remains a
significant challenge, with music and speech often developed in isolation,
hindering progress towards universal audio synthesis. This separation stems
from inherent task conflicts and severe data imbalances, which impede the
development of a truly unified audio generation model. To address this
challenge, we propose UniMoE-Audio, a unified speech and music generation model
within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.
Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic
expert number allocation, and a hybrid expert design comprising routed experts
for domain-specific knowledge, shared experts for domain-agnostic features, and
null experts for adaptive computation skipping. To tackle data imbalance, we
introduce a three-stage training curriculum: 1) Independent Specialist Training
leverages original datasets to instill domain-specific knowledge into each
"proto-expert" without interference; 2) MoE Integration and Warmup incorporates
these specialists into the UniMoE-Audio architecture, warming up the gate
module and shared expert using a subset of balanced dataset; and 3) Synergistic
Joint Training trains the entire model end-to-end on the fully balanced
dataset, fostering enhanced cross-domain synergy. Extensive experiments show
that UniMoE-Audio not only achieves state-of-the-art performance on major
speech and music generation benchmarks, but also demonstrates superior
synergistic learning, mitigating the performance degradation typically seen in
naive joint training. Our findings highlight the substantial potential of
specialized MoE architecture and curated training strategies in advancing the
field of universal audio generation. Homepage:
https://mukioxun.github.io/Uni-MoE-site/home.html

</details>


### [24] [Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module](https://arxiv.org/abs/2510.13558)
*Ruitao Feng,Bixi Zhang,Sheng Liang,Zheng Yuan*

Main category: cs.SD

TL;DR: SteerMoE是一个新颖的模块化框架，通过冻结音频编码器和LLM解码器，仅训练轻量级steering模块来实现音频-语言对齐，使用MoE路由器动态选择和应用学习到的steering向量。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要昂贵的全模型微调或依赖可能缺乏表达能力的静态适配器，需要一种参数高效且强大的多模态代理构建方法。

Method: 在编码器层中集成轻量级steering模块，使用MoE路由器动态选择steering向量，将连续音频表示逐步转换为LLM可理解的空间，无需修改LLM词汇表。

Result: 在ASR、音频理解和定性函数调用任务中表现出强大性能，同时保持高度模块化和计算效率。

Conclusion: SteerMoE为开发复杂音频-语言系统提供了一个稳健的新范式，在保持LLM高级推理和代理能力的同时实现高效对齐。

Abstract: Aligning pretrained audio encoders and Large Language Models (LLMs) offers a
promising, parameter-efficient path to building powerful multimodal agents.
However, existing methods often require costly full-model finetuning or rely on
static adapters that may lack expressive power. Drawing inspiration from the
Platonic Representation Hypothesis, we introduce SteerMoE, a novel and modular
framework for audio-language alignment. SteerMoE freezes both the audio encoder
and the LLM decoder, training only a lightweight steering module integrated
within the encoder's layers. This module uses a Mixture-of-Experts (MoE) router
to dynamically select and apply learned steering vectors, progressively
transforming continuous audio representations into a space comprehensible to
the LLM. By operating entirely in the continuous embedding space, our approach
requires no modifications to the LLM's vocabulary and preserves its advanced
reasoning and agentic capabilities. We demonstrate through experiments on ASR,
audio understanding, and a qualitative function-calling task that SteerMoE
achieves strong performance while remaining highly modular and computationally
efficient, offering a robust new paradigm for developing sophisticated
audio-language systems.

</details>

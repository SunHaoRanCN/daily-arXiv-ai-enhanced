{"id": "2510.14166", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14166", "abs": "https://arxiv.org/abs/2510.14166", "authors": ["Yanqing Xu", "Jingjing Cui", "Yongxu Zhu", "Zhiguo Ding", "Tsung-Hui Chang", "Robert Schober", "Vincent W. S. Wong", "Octavia A. Dobre", "George K. Karagiannidis", "H. Vincent Poor", "Xiaohu You"], "title": "Generalized Pinching-Antenna Systems: A Tutorial on Principles, Design Strategies, and Future Directions", "comment": "31 pages, 13 figures", "summary": "Pinching-antenna systems have emerged as a novel and transformative\nflexible-antenna architecture for next-generation wireless networks. They offer\nunprecedented flexibility and spatial reconfigurability by enabling dynamic\npositioning and activation of radiating elements along a signal-guiding medium\n(e.g., dielectric waveguides), which is not possible with conventional fixed\nantenna systems. In this paper, we introduce the concept of generalized\npinching antenna systems, which retain the core principle of creating localized\nradiation points on demand, but can be physically realized in a variety of\nsettings. These include implementations based on dielectric waveguides, leaky\ncoaxial cables, surface-wave guiding structures, and other types of media,\nemploying different feeding methods and activation mechanisms (e.g.,\nmechanical, electronic, or hybrid). Despite differences in their physical\nrealizations, they all share the same inherent ability to form, reposition, or\ndeactivate radiation sites as needed, enabling user-centric and dynamic\ncoverage. We first describe the underlying physical mechanisms of\nrepresentative generalized pinching-antenna realizations and their associated\nwireless channel models, highlighting their unique propagation and\nreconfigurability characteristics compared with conventional antennas. Then, we\nreview several representative pinching-antenna system architectures, ranging\nfrom single- to multiple-waveguide configurations, and discuss advanced design\nstrategies tailored to these flexible deployments. Furthermore, we examine\ntheir integration with emerging wireless technologies to enable synergistic,\nuser-centric solutions. Finally, we identify key open research challenges and\noutline future directions, charting a pathway toward the practical deployment\nof generalized pinching antennas in next-generation wireless networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u7684\u6982\u5ff5\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u67d4\u6027\u5929\u7ebf\u67b6\u6784\uff0c\u80fd\u591f\u5728\u4fe1\u53f7\u5f15\u5bfc\u4ecb\u8d28\u4e0a\u52a8\u6001\u5b9a\u4f4d\u548c\u6fc0\u6d3b\u8f90\u5c04\u5143\u4ef6\uff0c\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u7a7a\u95f4\u53ef\u91cd\u6784\u6027\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\u65e0\u6cd5\u5b9e\u73b0\u8f90\u5c04\u5143\u4ef6\u7684\u52a8\u6001\u5b9a\u4f4d\u548c\u6fc0\u6d3b\uff0c\u9650\u5236\u4e86\u65e0\u7ebf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u548c\u8986\u76d6\u80fd\u529b\u3002\u5e7f\u4e49\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u7528\u6237\u4e2d\u5fc3\u548c\u52a8\u6001\u8986\u76d6\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u7269\u7406\u5b9e\u73b0\u65b9\u5f0f\uff08\u5982\u4ecb\u8d28\u6ce2\u5bfc\u3001\u6f0f\u6cc4\u540c\u8f74\u7535\u7f06\u3001\u8868\u9762\u6ce2\u5bfc\u7ed3\u6784\u7b49\uff09\u548c\u4e0d\u540c\u7684\u9988\u7535\u65b9\u6cd5\u53ca\u6fc0\u6d3b\u673a\u5236\uff08\u673a\u68b0\u3001\u7535\u5b50\u6216\u6df7\u5408\uff09\uff0c\u5728\u4fe1\u53f7\u5f15\u5bfc\u4ecb\u8d28\u4e0a\u521b\u5efa\u5c40\u90e8\u8f90\u5c04\u70b9\u3002", "result": "\u5f00\u53d1\u4e86\u5177\u6709\u4ee3\u8868\u6027\u7684\u5e7f\u4e49\u5939\u6301\u5929\u7ebf\u5b9e\u73b0\u65b9\u6848\u53ca\u5176\u65e0\u7ebf\u4fe1\u9053\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4e0e\u4f20\u7edf\u5929\u7ebf\u76f8\u6bd4\u72ec\u7279\u7684\u4f20\u64ad\u548c\u53ef\u91cd\u6784\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u5355\u6ce2\u5bfc\u5230\u591a\u6ce2\u5bfc\u914d\u7f6e\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "conclusion": "\u5e7f\u4e49\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u7684\u67d4\u6027\u5929\u7ebf\u67b6\u6784\uff0c\u4f46\u4ecd\u9762\u4e34\u5173\u952e\u7814\u7a76\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5b9e\u9645\u90e8\u7f72\u8def\u5f84\u3002"}}
{"id": "2510.14281", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14281", "abs": "https://arxiv.org/abs/2510.14281", "authors": ["Junyuan Gao", "Weifeng Zhu", "Shuowen Zhang", "Yongpeng Wu", "Jiannong Cao", "Giuseppe Caire", "Liang Liu"], "title": "Integrated Massive Communication and Target Localization in 6G Cell-Free Networks", "comment": "submitted to IEEE TWC", "summary": "This paper presents an initial investigation into the combination of\nintegrated sensing and communication (ISAC) and massive communication, both of\nwhich are largely regarded as key scenarios in sixth-generation (6G) wireless\nnetworks. Specifically, we consider a cell-free network comprising a large\nnumber of users, multiple targets, and distributed base stations (BSs). In each\ntime slot, a random subset of users becomes active, transmitting pilot signals\nthat can be scattered by the targets before reaching the BSs. Unlike\nconventional massive random access schemes, where the primary objectives are\ndevice activity detection and channel estimation, our framework also enables\ntarget localization by leveraging the multipath propagation effects introduced\nby the targets. However, due to the intricate dependency between user channels\nand target locations, characterizing the posterior distribution required for\nminimum mean-square error (MMSE) estimation presents significant computational\nchallenges. To handle this problem, we propose a hybrid message passing-based\nframework that incorporates multiple approximations to mitigate computational\ncomplexity. Numerical results demonstrate that the proposed approach achieves\nhigh-accuracy device activity detection, channel estimation, and target\nlocalization simultaneously, validating the feasibility of embedding\nlocalization functionality into massive communication systems for future 6G\nnetworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u548c\u5927\u89c4\u6a21\u901a\u4fe1\u76f8\u7ed3\u5408\u76846G\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u540c\u65f6\u5b9e\u73b0\u8bbe\u5907\u6d3b\u52a8\u68c0\u6d4b\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u76ee\u6807\u5b9a\u4f4d\u3002", "motivation": "\u7814\u7a76\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u548c\u5927\u89c4\u6a21\u901a\u4fe1\u57286G\u7f51\u7edc\u4e2d\u7684\u878d\u5408\uff0c\u89e3\u51b3\u4f20\u7edf\u5927\u89c4\u6a21\u968f\u673a\u63a5\u5165\u65b9\u6848\u4ec5\u5173\u6ce8\u8bbe\u5907\u6d3b\u52a8\u68c0\u6d4b\u548c\u4fe1\u9053\u4f30\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u589e\u52a0\u76ee\u6807\u5b9a\u4f4d\u529f\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u6d88\u606f\u4f20\u9012\u7684\u6846\u67b6\uff0c\u91c7\u7528\u591a\u79cd\u8fd1\u4f3c\u65b9\u6cd5\u6765\u5904\u7406\u7528\u6237\u4fe1\u9053\u4e0e\u76ee\u6807\u4f4d\u7f6e\u4e4b\u95f4\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5e26\u6765\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u907f\u514d\u76f4\u63a5\u8ba1\u7b97\u540e\u9a8c\u5206\u5e03\u7684\u9ad8\u590d\u6742\u5ea6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u8bbe\u5907\u6d3b\u52a8\u68c0\u6d4b\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u76ee\u6807\u5b9a\u4f4d\uff0c\u9a8c\u8bc1\u4e86\u57286G\u7f51\u7edc\u4e2d\u4e3a\u5927\u89c4\u6a21\u901a\u4fe1\u7cfb\u7edf\u5d4c\u5165\u5b9a\u4f4d\u529f\u80fd\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u57286G\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u548c\u5927\u89c4\u6a21\u901a\u4fe1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u540c\u65f6\u652f\u6301\u901a\u4fe1\u548c\u611f\u77e5\u529f\u80fd\u7684\u65b0\u6846\u67b6\u3002"}}
{"id": "2510.14358", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14358", "abs": "https://arxiv.org/abs/2510.14358", "authors": ["Yuanhao Cui", "Jiali Nie", "Fan Liu", "Weijie Yuan", "Zhiyong Feng", "Xiaojun Jing", "Yulin Liu", "Jie Xu", "Christos Masouros", "Shuguang Cui"], "title": "Integrated Sensing and Communication: Towards Multifunctional Perceptive Network", "comment": null, "summary": "The capacity-maximization design philosophy has driven the growth of wireless\nnetworks for decades. However, with the slowdown in recent data traffic demand,\nthe mobile industry can no longer rely solely on communication services to\nsustain development. In response, Integrated Sensing and Communications (ISAC)\nhas emerged as a transformative solution, embedding sensing capabilities into\ncommunication networks to enable multifunctional wireless systems. This\nparadigm shift expands the role of networks from sole data transmission to\nversatile platforms supporting diverse applications. In this review, we provide\na bird's-eye view of ISAC for new researchers, highlighting key challenges,\nopportunities, and application scenarios to guide future exploration in this\nfield.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5728\u901a\u4fe1\u7f51\u7edc\u4e2d\u5d4c\u5165\u611f\u77e5\u80fd\u529b\uff0c\u4f7f\u65e0\u7ebf\u7cfb\u7edf\u5177\u5907\u591a\u529f\u80fd\u6027\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u6d41\u91cf\u9700\u6c42\u653e\u7f13\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u6570\u636e\u6d41\u91cf\u9700\u6c42\u589e\u957f\u653e\u7f13\uff0c\u79fb\u52a8\u884c\u4e1a\u65e0\u6cd5\u4ec5\u4f9d\u9760\u901a\u4fe1\u670d\u52a1\u7ef4\u6301\u53d1\u5c55\uff0c\u9700\u8981\u65b0\u7684\u589e\u957f\u52a8\u529b\u3002", "method": "\u5728\u901a\u4fe1\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u80fd\u529b\uff0c\u6784\u5efa\u591a\u529f\u80fd\u65e0\u7ebf\u7cfb\u7edf\uff0c\u5c06\u7f51\u7edc\u89d2\u8272\u4ece\u5355\u7eaf\u6570\u636e\u4f20\u8f93\u6269\u5c55\u5230\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u7684\u5e73\u53f0\u3002", "result": "ISAC\u5df2\u6210\u4e3a\u53d8\u9769\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u65e0\u7ebf\u7f51\u7edc\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "conclusion": "ISAC\u4ee3\u8868\u4e86\u65e0\u7ebf\u7f51\u7edc\u8bbe\u8ba1\u7406\u5ff5\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u5173\u952e\u6311\u6218\u3001\u673a\u9047\u548c\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.14507", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.14507", "abs": "https://arxiv.org/abs/2510.14507", "authors": ["Qin Yi", "Zeping Sui", "Zilong Liu"], "title": "Error Rate Analysis and Low-Complexity Receiver Design for Zero-Padded AFDM", "comment": "5 pages, 7 figures, submitted to IEEE TVT", "summary": "This paper studies the error rate performance and low-complexity receiver\ndesign for zero-padded affine frequency division multiplexing (ZP-AFDM)\nsystems. By exploiting the unique ZP-aided lower triangular structure of the\ntime domain (TD) channel matrix, we propose {a novel low-complexity} minimum\nmean square error (MMSE) detector and {a} maximum ratio combining-based TD\n(MRC-TD) detector. Furthermore, the theoretical bit error rate (BER)\n{performance} of both MMSE and maximum likelihood detectors {is} analyzed.\nSimulation results demonstrate {that} the proposed detectors can achieve\nidentical BER performance to that of {the conventional MMSE detector based on\nmatrix inversion} while {enjoying significantly reduced complexity.}", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86ZP-AFDM\u7cfb\u7edf\u7684\u8bef\u7801\u7387\u6027\u80fd\u548c\u4f4e\u590d\u6742\u5ea6\u63a5\u6536\u673a\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u68c0\u6d4b\u5668\u5e76\u5206\u6790\u4e86\u7406\u8bbaBER\u6027\u80fd\u3002", "motivation": "\u5229\u7528ZP\u8f85\u52a9\u7684\u65f6\u57df\u4fe1\u9053\u77e9\u9635\u7684\u4e0b\u4e09\u89d2\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u63a5\u6536\u673a\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f4e\u590d\u6742\u5ea6MMSE\u68c0\u6d4b\u5668\u548c\u57fa\u4e8e\u6700\u5927\u6bd4\u5408\u5e76\u7684\u65f6\u57df\u68c0\u6d4b\u5668\uff0c\u5e76\u5206\u6790\u4e86MMSE\u548c\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u5668\u7684\u7406\u8bbaBER\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u68c0\u6d4b\u5668\u80fd\u591f\u8fbe\u5230\u4e0e\u4f20\u7edf\u57fa\u4e8e\u77e9\u9635\u6c42\u9006\u7684MMSE\u68c0\u6d4b\u5668\u76f8\u540c\u7684BER\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f4e\u590d\u6742\u5ea6\u68c0\u6d4b\u5668\u5728\u4fdd\u6301\u76f8\u540cBER\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6709\u6548\u964d\u4f4e\u4e86ZP-AFDM\u7cfb\u7edf\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.13906", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.13906", "abs": "https://arxiv.org/abs/2510.13906", "authors": ["Amrit Romana", "Jaya Narain", "Tien Dung Tran", "Andrea Davis", "Jason Fong", "Ramya Rasipuram", "Vikramjit Mitra"], "title": "Switchboard-Affect: Emotion Perception Labels from Conversational Speech", "comment": "2025 13th International Conference on Affective Computing and\n  Intelligent Interaction (ACII) https://github.com/apple/ml-switchboard-affect", "summary": "Understanding the nuances of speech emotion dataset curation and labeling is\nessential for assessing speech emotion recognition (SER) model potential in\nreal-world applications. Most training and evaluation datasets contain acted or\npseudo-acted speech (e.g., podcast speech) in which emotion expressions may be\nexaggerated or otherwise intentionally modified. Furthermore, datasets labeled\nbased on crowd perception often lack transparency regarding the guidelines\ngiven to annotators. These factors make it difficult to understand model\nperformance and pinpoint necessary areas for improvement. To address this gap,\nwe identified the Switchboard corpus as a promising source of naturalistic\nconversational speech, and we trained a crowd to label the dataset for\ncategorical emotions (anger, contempt, disgust, fear, sadness, surprise,\nhappiness, tenderness, calmness, and neutral) and dimensional attributes\n(activation, valence, and dominance). We refer to this label set as\nSwitchboard-Affect (SWB-Affect). In this work, we present our approach in\ndetail, including the definitions provided to annotators and an analysis of the\nlexical and paralinguistic cues that may have played a role in their\nperception. In addition, we evaluate state-of-the-art SER models, and we find\nvariable performance across the emotion categories with especially poor\ngeneralization for anger. These findings underscore the importance of\nevaluation with datasets that capture natural affective variations in speech.\nWe release the labels for SWB-Affect to enable further analysis in this domain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86Switchboard-Affect (SWB-Affect)\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4f17\u5305\u6807\u6ce8\u5458\u5bf9\u81ea\u7136\u5bf9\u8bdd\u8bed\u97f3\u8fdb\u884c\u60c5\u611f\u6807\u6ce8\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u60c5\u611f\u6570\u636e\u96c6\u5927\u591a\u5305\u542b\u8868\u6f14\u6216\u4f2a\u8868\u6f14\u8bed\u97f3\uff0c\u60c5\u611f\u8868\u8fbe\u53ef\u80fd\u88ab\u5938\u5927\u6216\u6709\u610f\u4fee\u6539\uff0c\u4e14\u4f17\u5305\u6807\u6ce8\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u9009\u62e9Switchboard\u8bed\u6599\u5e93\u4f5c\u4e3a\u81ea\u7136\u5bf9\u8bdd\u8bed\u97f3\u6765\u6e90\uff0c\u8bad\u7ec3\u4f17\u5305\u6807\u6ce8\u5458\u5bf9\u8bed\u97f3\u8fdb\u884c10\u7c7b\u57fa\u672c\u60c5\u611f\u548c3\u4e2a\u7ef4\u5ea6\u5c5e\u6027\uff08\u6fc0\u6d3b\u5ea6\u3001\u6548\u4ef7\u3001\u652f\u914d\u5ea6\uff09\u7684\u6807\u6ce8\u3002", "result": "\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6a21\u578b\uff0c\u53d1\u73b0\u4e0d\u540c\u60c5\u611f\u7c7b\u522b\u7684\u6027\u80fd\u5dee\u5f02\u5f88\u5927\uff0c\u7279\u522b\u662f\u6124\u6012\u60c5\u611f\u7684\u6cdb\u5316\u80fd\u529b\u5f88\u5dee\u3002", "conclusion": "\u5f3a\u8c03\u4f7f\u7528\u6355\u6349\u81ea\u7136\u8bed\u97f3\u60c5\u611f\u53d8\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u53d1\u5e03\u4e86SWB-Affect\u6807\u7b7e\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.14249", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.14249", "abs": "https://arxiv.org/abs/2510.14249", "authors": ["Qixin Deng", "Bryan Pardo", "Thrasyvoulos N Pappas"], "title": "Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?", "comment": null, "summary": "Understanding and modeling the relationship between language and sound is\ncritical for applications such as music information retrieval,text-guided music\ngeneration, and audio captioning. Central to these tasks is the use of joint\nlanguage-audio embedding spaces, which map textual descriptions and auditory\ncontent into a shared embedding space. While multimodal embedding models such\nas MS-CLAP, LAION-CLAP, and MuQ-MuLan have shown strong performance in aligning\nlanguage and audio, their correspondence to human perception of timbre, a\nmultifaceted attribute encompassing qualities such as brightness, roughness,\nand warmth, remains underexplored. In this paper, we evaluate the above three\njoint language-audio embedding models on their ability to capture perceptual\ndimensions of timbre. Our findings show that LAION-CLAP consistently provides\nthe most reliable alignment with human-perceived timbre semantics across both\ninstrumental sounds and audio effects.", "AI": {"tldr": "\u8bc4\u4f30\u4e09\u79cd\u8bed\u8a00-\u97f3\u9891\u8054\u5408\u5d4c\u5165\u6a21\u578b\uff08MS-CLAP\u3001LAION-CLAP\u3001MuQ-MuLan\uff09\u5728\u6355\u6349\u97f3\u8272\u611f\u77e5\u7ef4\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0LAION-CLAP\u5728\u4e50\u5668\u58f0\u97f3\u548c\u97f3\u9891\u6548\u679c\u4e0a\u5747\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u97f3\u8272\u8bed\u4e49\u6700\u4e00\u81f4\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u548c\u58f0\u97f3\u4e4b\u95f4\u7684\u5173\u7cfb\u5bf9\u4e8e\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u3001\u6587\u672c\u5f15\u5bfc\u97f3\u4e50\u751f\u6210\u548c\u97f3\u9891\u5b57\u5e55\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u4e0e\u4eba\u7c7b\u5bf9\u97f3\u8272\u611f\u77e5\u7684\u5bf9\u5e94\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bc4\u4f30MS-CLAP\u3001LAION-CLAP\u548cMuQ-MuLan\u8fd9\u4e09\u79cd\u8054\u5408\u8bed\u8a00-\u97f3\u9891\u5d4c\u5165\u6a21\u578b\u6355\u6349\u97f3\u8272\u611f\u77e5\u7ef4\u5ea6\u7684\u80fd\u529b\u3002", "result": "LAION-CLAP\u5728\u4e50\u5668\u58f0\u97f3\u548c\u97f3\u9891\u6548\u679c\u4e0a\u5747\u63d0\u4f9b\u4e86\u4e0e\u4eba\u7c7b\u611f\u77e5\u97f3\u8272\u8bed\u4e49\u6700\u53ef\u9760\u7684\u5bf9\u9f50\u3002", "conclusion": "LAION-CLAP\u6a21\u578b\u5728\u6355\u6349\u97f3\u8272\u611f\u77e5\u7ef4\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u8bed\u8a00-\u97f3\u9891\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2510.14530", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14530", "abs": "https://arxiv.org/abs/2510.14530", "authors": ["Jiangong Chen", "Xia Lei", "Yuchen Zhang", "Kaitao Meng", "Christos Masouros"], "title": "Integrated Sensing and Communication with Tri-Hybrid Beamforming Across Electromagnetically Reconfigurable Antennas", "comment": null, "summary": "Beamforming with a sufficient number of antennas is one of the most\nsignificant technologies for both Multi-user (MU) Multiple-input\nMultiple-output (MIMO) communication and MIMO radar sensing in Integrated\nSensing and Communication (ISAC) systems. However, its performance suffers from\nlimited Degrees of Freedom (DoFs) in conventional hybrid beamforming systems.\nTo overcome this, we propose an Electromagnetically Reconfigurable Antenna\n(ERA)-aided ISAC system, where transmit ERAs dynamically adjust their radiation\npatterns to enhance system DoFs and improve overall performance. Specifically,\nwe design a tri-hybrid beamforming optimization framework combining digital,\nanalog, and Electromagnetic (EM) beamforming to jointly maximize communication\nrate and sensing Signal-to-Clutter-plus-Noise Ratio (SCNR). Furthermore, an\nintegrated Fractional Programming (FP) and Manifold Optimization (MO) approach\nis developed to transform the problem into tractable subproblems with\nclosed-form updates. Simulation results verify that the proposed ERA-ISAC\nsystem achieves almost 10 dB Sensing and Communication (S&C) performance gain\ncompared to its conventional hybrid beamforming counterparts with\nOmnidirectional Antenna (OA).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u78c1\u53ef\u91cd\u6784\u5929\u7ebf(ERA)\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8f90\u5c04\u6a21\u5f0f\u6765\u589e\u5f3a\u7cfb\u7edf\u81ea\u7531\u5ea6\uff0c\u63d0\u5347\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd", "motivation": "\u4f20\u7edf\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u7cfb\u7edf\u5b58\u5728\u81ea\u7531\u5ea6\u53d7\u9650\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u7528\u6237MIMO\u901a\u4fe1\u548cMIMO\u96f7\u8fbe\u611f\u77e5\u7684\u6027\u80fd", "method": "\u8bbe\u8ba1\u4e86\u4e09\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u5b57\u3001\u6a21\u62df\u548c\u7535\u78c1\u6ce2\u675f\u6210\u5f62\uff0c\u91c7\u7528\u5206\u6570\u89c4\u5212\u548c\u6d41\u5f62\u4f18\u5316\u65b9\u6cd5\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u5b50\u95ee\u9898", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u5168\u5411\u5929\u7ebf\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u7cfb\u7edf\uff0c\u63d0\u51fa\u7684ERA-ISAC\u7cfb\u7edf\u5728\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\u4e0a\u83b7\u5f97\u4e86\u8fd110dB\u7684\u589e\u76ca", "conclusion": "\u7535\u78c1\u53ef\u91cd\u6784\u5929\u7ebf\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u81ea\u7531\u5ea6\uff0c\u663e\u8457\u6539\u5584\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd"}}
{"id": "2510.14551", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.14551", "abs": "https://arxiv.org/abs/2510.14551", "authors": ["Jiangyu Han", "Ruoyu Wang", "Yoshiki Masuyama", "Marc Delcroix", "Johan Rohdin", "Jun Du", "Lukas Burget"], "title": "Spatially Aware Self-Supervised Models for Multi-Channel Neural Speaker Diarization", "comment": "Submitted to ICASSP 2026", "summary": "Self-supervised models such as WavLM have demonstrated strong performance for\nneural speaker diarization. However, these models are typically pre-trained on\nsingle-channel recordings, limiting their effectiveness in multi-channel\nscenarios. Existing diarization systems built on these models often rely on\nDOVER-Lap to combine outputs from individual channels. Although effective, this\napproach incurs substantial computational overhead and fails to fully exploit\nspatial information. In this work, building on DiariZen, a pipeline that\ncombines WavLM-based local endto-end neural diarization with speaker embedding\nclustering, we introduce a lightweight approach to make pre-trained WavLM\nspatially aware by inserting channel communication modules into the early\nlayers. Our method is agnostic to both the number of microphone channels and\narray topologies, ensuring broad applicability. We further propose to fuse\nmulti-channel speaker embeddings by leveraging spatial attention weights.\nEvaluations on five public datasets show consistent improvements over\nsingle-channel baselines and demonstrate superior performance and efficiency\ncompared with DOVER-Lap. Our source code is publicly available at\nhttps://github.com/BUTSpeechFIT/DiariZen.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u7684WavLM\u65e9\u671f\u5c42\u63d2\u5165\u901a\u9053\u901a\u4fe1\u6a21\u5757\uff0c\u4f7f\u5176\u5177\u5907\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u7528\u4e8e\u591a\u901a\u9053\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u9ea6\u514b\u98ce\u901a\u9053\u6570\u91cf\u548c\u9635\u5217\u62d3\u6251\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u6ce8\u610f\u529b\u6743\u91cd\u878d\u5408\u591a\u901a\u9053\u8bf4\u8bdd\u4eba\u5d4c\u5165\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u6a21\u578b\u5982WavLM\u901a\u5e38\u5728\u5355\u901a\u9053\u5f55\u97f3\u4e0a\u9884\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u901a\u9053\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u73b0\u6709\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7cfb\u7edf\u4f9d\u8d56DOVER-Lap\u7ec4\u5408\u5404\u901a\u9053\u8f93\u51fa\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u7a7a\u95f4\u4fe1\u606f\u3002", "method": "\u57fa\u4e8eDiariZen\u6d41\u6c34\u7ebf\uff0c\u5728\u9884\u8bad\u7ec3WavLM\u7684\u65e9\u671f\u5c42\u63d2\u5165\u901a\u9053\u901a\u4fe1\u6a21\u5757\uff0c\u4f7f\u5176\u5177\u5907\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002\u63d0\u51fa\u5229\u7528\u7a7a\u95f4\u6ce8\u610f\u529b\u6743\u91cd\u878d\u5408\u591a\u901a\u9053\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u5355\u901a\u9053\u57fa\u7ebf\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8eDOVER-Lap\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u901a\u9053\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.14391", "categories": ["cs.SD", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14391", "abs": "https://arxiv.org/abs/2510.14391", "authors": ["Jaehoon Ahn", "Moon-Ryul Jung"], "title": "Beat Detection as Object Detection", "comment": "11 pages, 4 figures, 5 tables", "summary": "Recent beat and downbeat tracking models (e.g., RNNs, TCNs, Transformers)\noutput frame-level activations. We propose reframing this task as object\ndetection, where beats and downbeats are modeled as temporal \"objects.\"\nAdapting the FCOS detector from computer vision to 1D audio, we replace its\noriginal backbone with WaveBeat's temporal feature extractor and add a Feature\nPyramid Network to capture multi-scale temporal patterns. The model predicts\noverlapping beat/downbeat intervals with confidence scores, followed by\nnon-maximum suppression (NMS) to select final predictions. This NMS step serves\na similar role to DBNs in traditional trackers, but is simpler and less\nheuristic. Evaluated on standard music datasets, our approach achieves\ncompetitive results, showing that object detection techniques can effectively\nmodel musical beats with minimal adaptation.", "AI": {"tldr": "\u5c06\u8282\u62cd\u548c\u91cd\u62cd\u8ddf\u8e2a\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528FCOS\u68c0\u6d4b\u5668\u4ece\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8fc1\u79fb\u52301D\u97f3\u9891\u5904\u7406\uff0c\u901a\u8fc7\u975e\u6781\u5927\u503c\u6291\u5236\u9009\u62e9\u6700\u7ec8\u9884\u6d4b\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8282\u62cd\u8ddf\u8e2a\u6a21\u578b\u8f93\u51fa\u5e27\u7ea7\u6fc0\u6d3b\uff0c\u672c\u6587\u63d0\u51fa\u5c06\u5176\u91cd\u65b0\u6784\u5efa\u4e3a\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u5c06\u8282\u62cd\u548c\u91cd\u62cd\u5efa\u6a21\u4e3a\u65f6\u95f4\"\u5bf9\u8c61\"\u3002", "method": "\u5c06FCOS\u68c0\u6d4b\u5668\u4ece\u8ba1\u7b97\u673a\u89c6\u89c9\u8fc1\u79fb\u52301D\u97f3\u9891\u5904\u7406\uff0c\u66ff\u6362\u539f\u59cb\u9aa8\u5e72\u7f51\u7edc\u4e3aWaveBeat\u7684\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u6dfb\u52a0\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u6355\u83b7\u591a\u5c3a\u5ea6\u65f6\u95f4\u6a21\u5f0f\uff0c\u4f7f\u7528\u975e\u6781\u5927\u503c\u6291\u5236\u9009\u62e9\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u6807\u51c6\u97f3\u4e50\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "\u76ee\u6807\u68c0\u6d4b\u6280\u672f\u53ea\u9700\u6700\u5c0f\u5316\u9002\u5e94\u5c31\u80fd\u6709\u6548\u5efa\u6a21\u97f3\u4e50\u8282\u62cd\u3002"}}
{"id": "2510.14604", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14604", "abs": "https://arxiv.org/abs/2510.14604", "authors": ["Thomas Feuillen", "Amirafshar Moshtaghpour"], "title": "Proceedings of the second edition of the International Symposium on Computational Sensing (ISCS25)", "comment": "This is the proceedings of the second edition of ISCS which took\n  place in June 2025 in Clervaux (LU)", "summary": "The International Symposium on Computational Sensing (ISCS) brings together\nresearchers from optical microscopy, electron microscopy, RADAR, astronomical\nimaging, biomedical imaging, remote sensing, and signal processing. With a\nparticular focus on applications and demonstrators, the purpose of this\nsymposium is to be a forum where researchers in computational sensing working\nin seemingly unrelated applications can learn, discover, and exchange on their\nnew findings and challenges. This 3-day symposium in the heart of Europe\nfeatures 6 keynotes speakers and is open to extended abstracts for scientific\npresentations and show-and-tell demonstrations.", "AI": {"tldr": "ISCS\u662f\u4e00\u4e2a\u6c47\u96c6\u8ba1\u7b97\u4f20\u611f\u9886\u57df\u7814\u7a76\u8005\u7684\u56fd\u9645\u7814\u8ba8\u4f1a\uff0c\u91cd\u70b9\u5173\u6ce8\u5e94\u7528\u548c\u6f14\u793a\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u4ea4\u6d41\u3002", "motivation": "\u4e3a\u6765\u81ea\u4e0d\u540c\u5e94\u7528\u9886\u57df\uff08\u5982\u5149\u5b66\u663e\u5fae\u955c\u3001\u7535\u5b50\u663e\u5fae\u955c\u3001\u96f7\u8fbe\u3001\u5929\u6587\u6210\u50cf\u7b49\uff09\u7684\u8ba1\u7b97\u4f20\u611f\u7814\u7a76\u8005\u63d0\u4f9b\u4e00\u4e2a\u4ea4\u6d41\u5e73\u53f0\uff0c\u8ba9\u4ed6\u4eec\u80fd\u591f\u5b66\u4e60\u3001\u53d1\u73b0\u548c\u5206\u4eab\u65b0\u53d1\u73b0\u4e0e\u6311\u6218\u3002", "method": "\u4e3e\u529e\u4e3a\u671f3\u5929\u7684\u7814\u8ba8\u4f1a\uff0c\u9080\u8bf76\u4f4d\u4e3b\u9898\u6f14\u8bb2\u5609\u5bbe\uff0c\u5e76\u63a5\u53d7\u79d1\u5b66\u6f14\u793a\u548c\u5c55\u793a\u6f14\u793a\u7684\u6269\u5c55\u6458\u8981\u3002", "result": "\u8be5\u7814\u8ba8\u4f1a\u4e3a\u8ba1\u7b97\u4f20\u611f\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8de8\u5b66\u79d1\u4ea4\u6d41\u7684\u8bba\u575b\u3002", "conclusion": "ISCS\u901a\u8fc7\u6c47\u96c6\u4e0d\u540c\u5e94\u7528\u9886\u57df\u7684\u7814\u7a76\u8005\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u4f20\u611f\u9886\u57df\u7684\u77e5\u8bc6\u4ea4\u6d41\u548c\u5408\u4f5c\u3002"}}
{"id": "2510.14443", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.14443", "abs": "https://arxiv.org/abs/2510.14443", "authors": ["Mayuri Kate", "Suresh Neethirajan"], "title": "Big Data Approaches to Bovine Bioacoustics: A FAIR-Compliant Dataset and Scalable ML Framework for Precision Livestock Welfare", "comment": "40 pages, 14 figures, 9 Tables", "summary": "The convergence of IoT sensing, edge computing, and machine learning is\ntransforming precision livestock farming. Yet bioacoustic data streams remain\nunderused because of computational complexity and ecological validity\nchallenges. We present one of the most comprehensive bovine vocalization\ndatasets to date, with 569 curated clips covering 48 behavioral classes,\nrecorded across three commercial dairy farms using multiple microphone arrays\nand expanded to 2900 samples through domain informed augmentation. This FAIR\ncompliant resource addresses major Big Data challenges - volume (90 hours of\nrecordings, 65.6 GB), variety (multi farm and multi zone acoustics), velocity\n(real time processing), and veracity (noise robust feature extraction). Our\ndistributed processing framework integrates advanced denoising using iZotope\nRX, multimodal synchronization through audio and video alignment, and\nstandardized feature engineering with 24 acoustic descriptors generated from\nPraat, librosa, and openSMILE. Preliminary benchmarks reveal distinct class\nlevel acoustic patterns for estrus detection, distress classification, and\nmaternal communication. The datasets ecological realism, reflecting authentic\nbarn acoustics rather than controlled settings, ensures readiness for field\ndeployment. This work establishes a foundation for animal centered AI, where\nbioacoustic data enable continuous and non invasive welfare assessment at\nindustrial scale. By releasing standardized pipelines and detailed metadata, we\npromote reproducible research that connects Big Data analytics, sustainable\nagriculture, and precision livestock management. The framework supports UN SDG\n9, showing how data science can turn traditional farming into intelligent,\nwelfare optimized systems that meet global food needs while upholding ethical\nanimal care.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u725b\u53ea\u53d1\u58f0\u6570\u636e\u96c6\u548c\u5206\u5e03\u5f0f\u5904\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u51c6\u755c\u7267\u4e1a\u4e2d\u7684\u751f\u7269\u58f0\u5b66\u5206\u6790\uff0c\u652f\u6301\u975e\u4fb5\u5165\u6027\u52a8\u7269\u798f\u5229\u8bc4\u4f30\u3002", "motivation": "\u751f\u7269\u58f0\u5b66\u6570\u636e\u6d41\u56e0\u8ba1\u7b97\u590d\u6742\u6027\u548c\u751f\u6001\u6709\u6548\u6027\u6311\u6218\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u89e3\u51b3\u5927\u6570\u636e\u6311\u6218\uff08\u4f53\u91cf\u3001\u591a\u6837\u6027\u3001\u901f\u5ea6\u3001\u771f\u5b9e\u6027\uff09\u6765\u63a8\u8fdb\u7cbe\u51c6\u755c\u7267\u4e1a\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b569\u4e2a\u7cbe\u9009\u97f3\u9891\u7247\u6bb5\u3001\u8986\u76d648\u4e2a\u884c\u4e3a\u7c7b\u522b\u7684\u725b\u53ea\u53d1\u58f0\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u589e\u5f3a\u6269\u5c55\u52302900\u4e2a\u6837\u672c\uff1b\u96c6\u6210\u5148\u8fdb\u53bb\u566a\u3001\u591a\u6a21\u6001\u540c\u6b65\u548c\u6807\u51c6\u5316\u7279\u5f81\u5de5\u7a0b\u3002", "result": "\u521d\u6b65\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u53d1\u60c5\u68c0\u6d4b\u3001\u75db\u82e6\u5206\u7c7b\u548c\u6bcd\u6027\u4ea4\u6d41\u7b49\u4e0d\u540c\u7c7b\u522b\u7684\u58f0\u5b66\u6a21\u5f0f\uff0c\u6570\u636e\u96c6\u5177\u6709\u751f\u6001\u771f\u5b9e\u6027\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u73b0\u573a\u90e8\u7f72\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ee5\u52a8\u7269\u4e3a\u4e2d\u5fc3\u7684AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u751f\u7269\u58f0\u5b66\u6570\u636e\u5b9e\u73b0\u5de5\u4e1a\u89c4\u6a21\u7684\u8fde\u7eed\u975e\u4fb5\u5165\u6027\u798f\u5229\u8bc4\u4f30\uff0c\u652f\u6301\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u68079\uff0c\u5c06\u4f20\u7edf\u519c\u4e1a\u8f6c\u53d8\u4e3a\u667a\u80fd\u3001\u798f\u5229\u4f18\u5316\u7684\u7cfb\u7edf\u3002"}}
{"id": "2510.14794", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14794", "abs": "https://arxiv.org/abs/2510.14794", "authors": ["Halvin Yang", "Yizhe Zhao", "Kai-Kit Wong", "Hsiao-Hwa Chen", "Chan-Byoung Chae"], "title": "Bridging Theory and Practice in Reconfigurable Fluid Antenna Systems", "comment": "Accepted into IEEE Communications Magazine", "summary": "Fluid antennas, including those based on liquid, mechanical, and pixel-based\ntechnologies, are poised to significantly enhance next-generation wireless\nsystems by adaptively optimizing their radiation characteristics. Many\ntheoretical analyses assumed near-instant reconfiguration, perfect channel\nknowledge, static or slowly varying propagation environments, and ideal\nmaterial properties that rarely hold in practice. In this article, we dissect\nthese common assumptions and contrast them with the realities of finite\nactuation time, limited and imperfect channel state information, rapidly\nchanging fading conditions, electromagnetic coupling, and mechanical\nconstraints. Through illustrative examples and simulations, we demonstrate how\nignoring these factors can lead to overestimated gains in capacity, coverage,\netc.. We then propose modeling refinements, experimental validation methods,\nand emerging control algorithms that better account for real-world constraints.\nOur findings highlight that, while reconfigurable antennas remain highly\npromising for B5G/6G and Internet of things (IoT) applications, their full\npotential can only be realized by incorporating practical considerations into\nsystem design and performance evaluation.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u5730\u5206\u6790\u4e86\u6d41\u4f53\u5929\u7ebf\u7406\u8bba\u5206\u6790\u4e2d\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u6307\u51fa\u8fd9\u4e9b\u5047\u8bbe\u4e0e\u5b9e\u9645\u60c5\u51b5\u7684\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5c55\u793a\u4e86\u5ffd\u7565\u5b9e\u9645\u56e0\u7d20\u4f1a\u5bfc\u81f4\u5bf9\u6027\u80fd\u589e\u76ca\u7684\u9ad8\u4f30\uff0c\u63d0\u51fa\u4e86\u66f4\u7b26\u5408\u5b9e\u9645\u7ea6\u675f\u7684\u5efa\u6a21\u6539\u8fdb\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6d41\u4f53\u5929\u7ebf\u7406\u8bba\u5206\u6790\u901a\u5e38\u5047\u8bbe\u77ac\u65f6\u91cd\u914d\u7f6e\u3001\u5b8c\u7f8e\u4fe1\u9053\u77e5\u8bc6\u3001\u9759\u6001\u4f20\u64ad\u73af\u5883\u548c\u7406\u60f3\u6750\u6599\u7279\u6027\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u6210\u7acb\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u5047\u8bbe\u4e0e\u5b9e\u9645\u60c5\u51b5\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u66f4\u7b26\u5408\u5b9e\u9645\u7ea6\u675f\u7684\u7cfb\u7edf\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u89e3\u6784\u5e38\u89c1\u5047\u8bbe\uff0c\u5bf9\u6bd4\u6709\u9650\u9a71\u52a8\u65f6\u95f4\u3001\u4e0d\u5b8c\u5584\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u3001\u5feb\u901f\u53d8\u5316\u8870\u843d\u6761\u4ef6\u3001\u7535\u78c1\u8026\u5408\u548c\u673a\u68b0\u7ea6\u675f\u7b49\u5b9e\u9645\u56e0\u7d20\uff0c\u4f7f\u7528\u8bf4\u660e\u6027\u793a\u4f8b\u548c\u6a21\u62df\u6765\u9a8c\u8bc1\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\u5ffd\u7565\u5b9e\u9645\u56e0\u7d20\u4f1a\u5bfc\u81f4\u5bf9\u5bb9\u91cf\u3001\u8986\u76d6\u7b49\u6027\u80fd\u589e\u76ca\u7684\u9ad8\u4f30\uff0c\u63d0\u51fa\u7684\u5efa\u6a21\u6539\u8fdb\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6d41\u4f53\u5929\u7ebf\u6027\u80fd\u3002", "conclusion": "\u867d\u7136\u53ef\u91cd\u6784\u5929\u7ebf\u5728B5G/6G\u548c\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u6781\u5177\u524d\u666f\uff0c\u4f46\u53ea\u6709\u5c06\u5b9e\u9645\u7ea6\u675f\u7eb3\u5165\u7cfb\u7edf\u8bbe\u8ba1\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002"}}
{"id": "2510.14570", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.14570", "abs": "https://arxiv.org/abs/2510.14570", "authors": ["Hui Wang", "Jinghua Zhao", "Cheng Liu", "Yuhang Jia", "Haoqin Sun", "Jiaming Zhou", "Yong Qin"], "title": "AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation", "comment": null, "summary": "Text-to-audio (TTA) is rapidly advancing, with broad potential in virtual\nreality, accessibility, and creative media. However, evaluating TTA quality\nremains difficult: human ratings are costly and limited, while existing\nobjective metrics capture only partial aspects of perceptual quality. To\naddress this gap, we introduce AudioEval, the first large-scale TTA evaluation\ndataset, containing 4,200 audio samples from 24 systems with 126,000 ratings\nacross five perceptual dimensions, annotated by both experts and non-experts.\nBased on this resource, we propose Qwen-DisQA, a multimodal scoring model that\njointly processes text prompts and generated audio to predict human-like\nquality ratings. Experiments show its effectiveness in providing reliable and\nscalable evaluation. The dataset will be made publicly available to accelerate\nfuture research.", "AI": {"tldr": "\u63d0\u51fa\u4e86AudioEval\u6570\u636e\u96c6\u548cQwen-DisQA\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u7cfb\u7edf\u7684\u8bc4\u4f30\u96be\u9898\u3002", "motivation": "\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6280\u672f\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u8d28\u91cf\u56f0\u96be\uff1a\u4eba\u5de5\u8bc4\u5206\u6210\u672c\u9ad8\u4e14\u6709\u9650\uff0c\u73b0\u6709\u5ba2\u89c2\u6307\u6807\u53ea\u80fd\u6355\u6349\u611f\u77e5\u8d28\u91cf\u7684\u90e8\u5206\u65b9\u9762\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b4,200\u4e2a\u97f3\u9891\u6837\u672c\u300124\u4e2a\u7cfb\u7edf\u3001126,000\u4e2a\u8bc4\u5206\u7684AudioEval\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faQwen-DisQA\u591a\u6a21\u6001\u8bc4\u5206\u6a21\u578b\uff0c\u8054\u5408\u5904\u7406\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u97f3\u9891\u6765\u9884\u6d4b\u4eba\u7c7b\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u52a0\u901f\u672a\u6765\u7814\u7a76\uff0c\u89e3\u51b3\u4e86TTA\u8bc4\u4f30\u7684\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.14802", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14802", "abs": "https://arxiv.org/abs/2510.14802", "authors": ["Sanjaya Herath", "Armin Gerami", "Kevin Wagner", "Ramani Duraiswami", "Christopher A. Metzler"], "title": "A Scalable MVDR Beamforming Algorithm That is Linear in the Number of Antennas", "comment": "6 pages, 4 figures, Asilomar 2025", "summary": "The Minimum Variance Distortionless Response (MVDR) beamforming technique is\nwidely applied in array systems to mitigate interference. However, applying\nMVDR to large arrays is computationally challenging; its computational\ncomplexity scales cubically with the number of antenna elements. In this paper,\nwe introduce a scalable MVDR beamforming method tailored for massive arrays.\nOur approach, which is specific to scenarios where the signal of interest is\nbelow the noise floor (e.g.,~GPS), leverages the Sherman-Morrison formula,\nlow-rank Singular Value Decomposition (SVD) approximations, and algebraic\nmanipulation. Using our approach, we reduce the computational complexity from\ncubic to linear in the number of antennas. We evaluate the proposed method\nthrough simulations, comparing its computational efficiency and beamforming\naccuracy with the conventional MVDR approach. Our method significantly reduces\nthe computational load while maintaining high beamforming accuracy for\nlarge-scale arrays. This solution holds promise for real-time applications of\nMVDR beamforming in fields like radar, sonar, and wireless communications,\nwhere massive antenna arrays are proliferating.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u9635\u5217\u7684\u53ef\u6269\u5c55MVDR\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u7acb\u65b9\u7ea7\u964d\u4f4e\u5230\u7ebf\u6027\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfMVDR\u6ce2\u675f\u6210\u5f62\u5728\u5927\u89c4\u6a21\u9635\u5217\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u5448\u7acb\u65b9\u589e\u957f\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u4fe1\u53f7\u4f4e\u4e8e\u566a\u58f0\u5e95\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528Sherman-Morrison\u516c\u5f0f\u3001\u4f4e\u79e9SVD\u8fd1\u4f3c\u548c\u4ee3\u6570\u64cd\u4f5c\uff0c\u4e13\u95e8\u9488\u5bf9\u4fe1\u566a\u6bd4\u4f4e\u7684\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u540c\u65f6\u5728\u5927\u89c4\u6a21\u9635\u5217\u4e2d\u4fdd\u6301\u4e86\u9ad8\u6ce2\u675f\u6210\u5f62\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u4e3a\u96f7\u8fbe\u3001\u58f0\u7eb3\u548c\u65e0\u7ebf\u901a\u4fe1\u7b49\u9886\u57df\u7684\u5b9e\u65f6MVDR\u6ce2\u675f\u6210\u5f62\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.14664", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.14664", "abs": "https://arxiv.org/abs/2510.14664", "authors": ["Hui Wang", "Jinghua Zhao", "Yifan Yang", "Shujie Liu", "Junyang Chen", "Yanzhe Zhang", "Shiwan Zhao", "Jinyu Li", "Jiaming Zhou", "Haoqin Sun", "Yan Lu", "Yong Qin"], "title": "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality Evaluation", "comment": null, "summary": "Generative speech technologies are progressing rapidly, but evaluating the\nperceptual quality of synthetic speech remains a core challenge. Existing\nmethods typically rely on scalar scores or binary decisions, which lack\ninterpretability and generalization across tasks and languages. We present\nSpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs)\nto conduct structured and explanation-based speech quality evaluation. To\nsupport this direction, we introduce SpeechEval, a large-scale dataset\ncontaining 32,207 multilingual speech clips and 128,754 annotations spanning\nfour tasks: quality assessment, pairwise comparison, improvement suggestion,\nand deepfake detection. Based on this resource, we develop SQ-LLM, a\nspeech-quality-aware LLM trained with chain-of-thought reasoning and reward\noptimization to improve capability. Experimental results show that SQ-LLM\ndelivers strong performance across tasks and languages, revealing the potential\nof this paradigm for advancing speech quality evaluation. Relevant resources\nwill be open-sourced.", "AI": {"tldr": "SpeechLLM-as-Judges\uff1a\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7SpeechEval\u6570\u636e\u96c6\u548cSQ-LLM\u6a21\u578b\u5728\u591a\u4efb\u52a1\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5b9e\u73b0\u5f3a\u5927\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u6807\u91cf\u5206\u6570\u6216\u4e8c\u5143\u51b3\u7b56\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u4efb\u52a1\u8de8\u8bed\u8a00\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSpeechLLM-as-Judges\u8303\u5f0f\uff0c\u6784\u5efaSpeechEval\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0832,207\u8bed\u97f3\u7247\u6bb5\uff0c128,754\u6807\u6ce8\uff09\uff0c\u5f00\u53d1SQ-LLM\u6a21\u578b\uff0c\u91c7\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u5956\u52b1\u4f18\u5316\u8bad\u7ec3\u3002", "result": "SQ-LLM\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8be5\u8303\u5f0f\u5728\u63a8\u8fdb\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "SpeechLLM-as-Judges\u4e3a\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u8d44\u6e90\u5c06\u5f00\u6e90\u3002"}}
{"id": "2510.14806", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14806", "abs": "https://arxiv.org/abs/2510.14806", "authors": ["Bowen Li", "Junting Chen", "Nikolaos Pappas"], "title": "Joint Channel and CFO Estimation From Beam-Swept Synchronization Signal Under Strong Inter-Cell Interference", "comment": null, "summary": "Complete awareness of the wireless environment, crucial for future\nintelligent networks, requires sensing all transmitted signals, not just the\nstrongest. A fundamental barrier is estimating the target signal when it is\nburied under strong co-channel interference from other transmitters, a failure\nof which renders the signal unusable. This work proposes a maximum likelihood\n(ML)-based cross-preamble estimation framework that exploits carrier frequency\noffset (CFO) constancy across beam-swept synchronization signals (SS),\ncoherently aggregating information across multiple observations to reinforce\nthe desired signal against overwhelming interference. Cramer-Rao lower bound\n(CRLB) analysis and simulation demonstrate reliable estimation even when the\nsignal is over a thousand times weaker than the interference. A low-altitude\nradio-map case study further verifies the framework's practical effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u7684\u8de8\u524d\u5bfc\u7801\u4f30\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u6ce2\u675f\u626b\u63cf\u540c\u6b65\u4fe1\u53f7\u4e2d\u7684\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u6052\u5b9a\u6027\uff0c\u901a\u8fc7\u591a\u89c2\u6d4b\u4fe1\u606f\u805a\u5408\u6765\u589e\u5f3a\u76ee\u6807\u4fe1\u53f7\uff0c\u5728\u5f3a\u5e72\u6270\u4e0b\u5b9e\u73b0\u53ef\u9760\u4f30\u8ba1\u3002", "motivation": "\u672a\u6765\u667a\u80fd\u7f51\u7edc\u9700\u8981\u611f\u77e5\u6240\u6709\u4f20\u8f93\u4fe1\u53f7\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5f3a\u4fe1\u53f7\u3002\u5173\u952e\u6311\u6218\u662f\u5728\u5f3a\u540c\u4fe1\u9053\u5e72\u6270\u4e0b\u4f30\u8ba1\u88ab\u6df9\u6ca1\u7684\u76ee\u6807\u4fe1\u53f7\uff0c\u5426\u5219\u4fe1\u53f7\u5c06\u65e0\u6cd5\u4f7f\u7528\u3002", "method": "\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u7684\u8de8\u524d\u5bfc\u7801\u4f30\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u6ce2\u675f\u626b\u63cf\u540c\u6b65\u4fe1\u53f7\u4e2d\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u7684\u6052\u5b9a\u6027\uff0c\u901a\u8fc7\u76f8\u5e72\u805a\u5408\u591a\u4e2a\u89c2\u6d4b\u4fe1\u606f\u6765\u589e\u5f3a\u76ee\u6807\u4fe1\u53f7\u5bf9\u6297\u5f3a\u5e72\u6270\u3002", "result": "\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c\u5206\u6790\u548c\u4eff\u771f\u8868\u660e\uff0c\u5373\u4f7f\u4fe1\u53f7\u6bd4\u5e72\u6270\u5f31\u4e00\u5343\u500d\u4ee5\u4e0a\uff0c\u4ecd\u80fd\u5b9e\u73b0\u53ef\u9760\u4f30\u8ba1\u3002\u4f4e\u7a7a\u65e0\u7ebf\u7535\u5730\u56fe\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u6052\u5b9a\u6027\uff0c\u5728\u591a\u89c2\u6d4b\u4fe1\u606f\u805a\u5408\u7684\u57fa\u7840\u4e0a\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3a\u76ee\u6807\u4fe1\u53f7\uff0c\u5728\u6781\u7aef\u5e72\u6270\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u9760\u4f30\u8ba1\uff0c\u4e3a\u667a\u80fd\u7f51\u7edc\u73af\u5883\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.14934", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.14934", "abs": "https://arxiv.org/abs/2510.14934", "authors": ["Ming-Hao Hsu", "Liang-Hsuan Tseng", "Hung-yi Lee", "Zhizheng Wu"], "title": "TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation", "comment": null, "summary": "We propose Text-Aligned Speech Tokens with Multiple Layer-Aggregation\n(TASLA), which is a text-aligned speech tokenization framework that aims to\naddress the problem that under a low-frame-rate and text-aligned regime,\nsingle-source speech tokens may lose acoustic details during reconstruction. On\nthe other hand, this paper further explains how different encoder layers\ncollaborate to capture comprehensive acoustic features for tokenization.\nPrevious work, TASTE, proposed the text-aligned speech tokenization framework,\nwhich is a LM-friendly architecture, but struggles to capture acoustic details.\nWe address this trade-off with two components: Multi-Layer Dynamic Attention\n(MLDA), which lets each text position adaptively mix shallow/deep features from\na frozen speech encoder, and Finite Scalar Quantization (FSQ), a simple\nper-dimension discretization with smooth optimization. At about 2.62 Hz\n(tokens/s), TASLA consistently improves prosody and achieves competitive\nquality over TASTE on in-domain (LibriSpeech) and OOD (EXPRESSO, Voxceleb)\nsets. We further demonstrate that dynamic layer mixing is correlated with\nspectral flux and explains why MLDA preserves prosody under a low frame rate\nwith extreme feature compression.", "AI": {"tldr": "TASLA\u662f\u4e00\u79cd\u6587\u672c\u5bf9\u9f50\u7684\u8bed\u97f3\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u52a8\u6001\u6ce8\u610f\u529b\u548c\u6709\u9650\u6807\u91cf\u91cf\u5316\u89e3\u51b3\u4f4e\u5e27\u7387\u4e0b\u8bed\u97f3\u91cd\u5efa\u4e22\u5931\u58f0\u5b66\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u57282.62Hz\u6807\u8bb0\u7387\u4e0b\u663e\u8457\u6539\u5584\u97f5\u5f8b\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5728\u4f4e\u5e27\u7387\u548c\u6587\u672c\u5bf9\u9f50\u673a\u5236\u4e0b\uff0c\u5355\u6e90\u8bed\u97f3\u6807\u8bb0\u4f1a\u4e22\u5931\u58f0\u5b66\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u540c\u65f6\u89e3\u91ca\u4e0d\u540c\u7f16\u7801\u5668\u5c42\u5982\u4f55\u534f\u4f5c\u6355\u6349\u5168\u9762\u7684\u58f0\u5b66\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u52a8\u6001\u6ce8\u610f\u529b(MLDA)\u8ba9\u6bcf\u4e2a\u6587\u672c\u4f4d\u7f6e\u81ea\u9002\u5e94\u6df7\u5408\u51bb\u7ed3\u8bed\u97f3\u7f16\u7801\u5668\u7684\u6d45\u5c42/\u6df1\u5c42\u7279\u5f81\uff0c\u7ed3\u5408\u6709\u9650\u6807\u91cf\u91cf\u5316(FSQ)\u8fdb\u884c\u9010\u7ef4\u79bb\u6563\u5316\u548c\u5e73\u6ed1\u4f18\u5316\u3002", "result": "\u57282.62Hz\u6807\u8bb0\u7387\u4e0b\uff0cTASLA\u5728\u9886\u57df\u5185(LibriSpeech)\u548c\u9886\u57df\u5916(EXPRESSO, Voxceleb)\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u6539\u5584\u97f5\u5f8b\u5e76\u8fbe\u5230\u4e0eTASTE\u76f8\u5f53\u7684\u8d28\u91cf\u3002", "conclusion": "\u52a8\u6001\u5c42\u6df7\u5408\u4e0e\u9891\u8c31\u901a\u91cf\u76f8\u5173\uff0c\u89e3\u91ca\u4e86MLDA\u5982\u4f55\u5728\u6781\u4f4e\u5e27\u7387\u548c\u7279\u5f81\u538b\u7f29\u4e0b\u4fdd\u6301\u97f5\u5f8b\u7279\u6027\u3002"}}
{"id": "2510.14939", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.14939", "abs": "https://arxiv.org/abs/2510.14939", "authors": ["Ken R. Duffy", "Moritz Grundei", "Jane A. Millward", "Muralidhar Rangaswamy", "Muriel Medard"], "title": "Decoding in the presence of ISI without interleaving ORBGRAND AI", "comment": null, "summary": "Inter symbol interference (ISI), which occurs in a wide variety of channels,\nis a result of time dispersion. It can be mitigated by equalization which\nresults in noise coloring. For such colored noise, we propose a decoder called\nOrdered Reliability Bit Guessing Random Additive Noise Decoding (ORBGRANDAI)\nwhich is inspired by the development of approximate independence in statistical\nphysics. By foregoing interleaving, ORBGRAND-AI can deliver the same, or lower,\nblock error rate (BLER) for the same amount of energy per information bit in an\nISI channel as a state-of-the-art soft input decoder, such as Cyclic Redundancy\nCheck Assisted-Successive Cancellation List (CA-SCL) decoding, with an\ninterleaver. To assess the decoding performance of ORBGRAND-AI, we consider\ndelay tap models and their associated colored noise. In particular, we examine\na two-tap dicode ISI channel as well as an ISI channel derived from data from\nRFView, a physics-informed modeling and simulation tool. We investigate the\ndicode and RFView channel under a variety of imperfect channel state\ninformation assumptions and show that a second order autoregressive model\nadequately represents the RFView channel effect.", "AI": {"tldr": "\u63d0\u51fa\u4e86ORBGRAND-AI\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u653e\u5f03\u4ea4\u7ec7\u6765\u5e94\u5bf9ISI\u4fe1\u9053\u4e2d\u7684\u6709\u8272\u566a\u58f0\uff0c\u5728\u76f8\u540c\u80fd\u91cf\u4e0b\u80fd\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u8f6f\u8f93\u5165\u89e3\u7801\u5668\u76f8\u540c\u6216\u66f4\u4f4e\u7684\u8bef\u5757\u7387\u3002", "motivation": "ISI\u4fe1\u9053\u4e2d\u7684\u65f6\u95f4\u5206\u6563\u5bfc\u81f4\u7b26\u53f7\u95f4\u5e72\u6270\uff0c\u5747\u8861\u5316\u4f1a\u4ea7\u751f\u6709\u8272\u566a\u58f0\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6709\u6548\u5904\u7406\u8fd9\u79cd\u6709\u8272\u566a\u58f0\u7684\u89e3\u7801\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7edf\u8ba1\u7269\u7406\u5b66\u4e2d\u7684\u8fd1\u4f3c\u72ec\u7acb\u6027\u5f00\u53d1ORBGRAND-AI\u89e3\u7801\u5668\uff0c\u4e0d\u91c7\u7528\u4ea4\u7ec7\u6280\u672f\uff0c\u9002\u7528\u4e8e\u5ef6\u8fdf\u62bd\u5934\u6a21\u578b\u548c\u6709\u8272\u566a\u58f0\u73af\u5883\u3002", "result": "\u5728\u53cc\u62bd\u5934dicode ISI\u4fe1\u9053\u548cRFView\u4fe1\u9053\u4e2d\uff0cORBGRAND-AI\u5728\u5404\u79cd\u4e0d\u5b8c\u7f8e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u6761\u4ef6\u4e0b\u90fd\u80fd\u8fbe\u5230\u826f\u597d\u6027\u80fd\uff0c\u4e8c\u9636\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u5145\u5206\u8868\u793aRFView\u4fe1\u9053\u6548\u5e94\u3002", "conclusion": "ORBGRAND-AI\u89e3\u7801\u5668\u80fd\u6709\u6548\u5904\u7406ISI\u4fe1\u9053\u4e2d\u7684\u6709\u8272\u566a\u58f0\uff0c\u5728\u4e0d\u4f7f\u7528\u4ea4\u7ec7\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e0e\u73b0\u6709\u5148\u8fdb\u89e3\u7801\u5668\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002"}}

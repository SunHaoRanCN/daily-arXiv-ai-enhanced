{"id": "2510.15566", "categories": ["cs.SD", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.15566", "abs": "https://arxiv.org/abs/2510.15566", "authors": ["Rachmad Vidya Wicaksana Putra", "Aadithyan Rajesh Nair", "Muhammad Shafique"], "title": "SpikeVox: Towards Energy-Efficient Speech Therapy Framework with Spike-driven Generative Language Models", "comment": "Accepted at the IEEE Biomedical Circuits and Systems Conference\n  (BioCAS) 2025, Abu Dhabi, UAE", "summary": "Speech disorders can significantly affect the patients capability to\ncommunicate, learn, and socialize. However, existing speech therapy solutions\n(e.g., therapist or tools) are still limited and costly, hence such solutions\nremain inadequate for serving millions of patients worldwide. To address this,\nstate-of-the-art methods employ neural network (NN) algorithms to help\naccurately detecting speech disorders. However, these methods do not provide\ntherapy recommendation as feedback, hence providing partial solution for\npatients. Moreover, these methods incur high energy consumption due to their\ncomplex and resource-intensive NN processing, hence hindering their deployments\non low-power/energy platforms (e.g., smartphones). Toward this, we propose\nSpikeVox, a novel framework for enabling energy-efficient speech therapy\nsolutions through spike-driven generative language model. Specifically,\nSpikeVox employs a speech recognition module to perform highly accurate\nspeech-to-text conversion; leverages a spike-driven generative language model\nto efficiently perform pattern analysis for speech disorder detection and\ngenerates suitable exercises for therapy; provides guidance on correct\npronunciation as feedback; as well as utilizes the REST API to enable seamless\ninteraction for users. Experimental results demonstrate that SpikeVox achieves\n88% confidence level on average in speech disorder recognition, while providing\na complete feedback for therapy exercises. Therefore, SpikeVox provides a\ncomprehensive framework for energy-efficient speech therapy solutions, and\npotentially addresses the significant global speech therapy access gap."}
{"id": "2510.15227", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15227", "abs": "https://arxiv.org/abs/2510.15227", "authors": ["Xiaohan Zhao", "Hongyu Xiang", "Shengze Ye", "Song Li", "Zhengkun Tian", "Guanyu Chen", "Ke Ding", "Guanglu Wan"], "title": "LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models", "comment": null, "summary": "This paper presents LongCat-Audio-Codec, an audio tokenizer and detokenizer\nsolution designed for industrial grade end-to-end speech large language models.\nBy leveraging a decoupled model architecture and a multistage training\nstrategy, LongCat-Audio-Codec exhibits robust semantic modeling capabilities,\nflexible acoustic feature extraction capabilities, and low-latency streaming\nsynthesis capabilities. It encodes speech at an ultra-low frame rate of 16.67\nHz, with a minimum bitrate of 0.43 kbps and a maximum bitrate of 0.87 kbps.\nEvaluation results demonstrate that LongCat-Audio-Codec achieves strong speech\nintelligibility and is capable of synthesizing highquality speech at low\nbitrate, thus effectively balancing coding efficiency and decoding quality. The\ninference code and model checkpoints of LongCat-Audio-Codec are available at:\nhttps://github.com/meituan-longcat/LongCat-Audio-Codec."}
{"id": "2510.15383", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15383", "abs": "https://arxiv.org/abs/2510.15383", "authors": ["Chitralekha Gupta", "Soundarya Ramesh", "Praveen Sasikumar", "Kian Peen Yeo", "Suranga Nanayakkara"], "title": "DroneAudioset: An Audio Dataset for Drone-based Search and Rescue", "comment": "Accepted in Neurips (Datasets and Benchmarks Track) 2025. The first\n  two authors are equal contributors", "summary": "Unmanned Aerial Vehicles (UAVs) or drones, are increasingly used in search\nand rescue missions to detect human presence. Existing systems primarily\nleverage vision-based methods which are prone to fail under low-visibility or\nocclusion. Drone-based audio perception offers promise but suffers from extreme\nego-noise that masks sounds indicating human presence. Existing datasets are\neither limited in diversity or synthetic, lacking real acoustic interactions,\nand there are no standardized setups for drone audition. To this end, we\npresent DroneAudioset (The dataset is publicly available at\nhttps://huggingface.co/datasets/ahlab-drone-project/DroneAudioSet/ under the\nMIT license), a comprehensive drone audition dataset featuring 23.5 hours of\nannotated recordings, covering a wide range of signal-to-noise ratios (SNRs)\nfrom -57.2 dB to -2.5 dB, across various drone types, throttles, microphone\nconfigurations as well as environments. The dataset enables development and\nsystematic evaluation of noise suppression and classification methods for\nhuman-presence detection under challenging conditions, while also informing\npractical design considerations for drone audition systems, such as microphone\nplacement trade-offs, and development of drone noise-aware audio processing.\nThis dataset is an important step towards enabling design and deployment of\ndrone-audition systems."}
{"id": "2510.15432", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15432", "abs": "https://arxiv.org/abs/2510.15432", "authors": ["Kevin Wilkinghoff", "Alessia Cornaggia-Urrigshardt", "Zheng-Hua Tan"], "title": "Quantization-Based Score Calibration for Few-Shot Keyword Spotting with Dynamic Time Warping in Noisy Environments", "comment": null, "summary": "Detecting occurrences of keywords with keyword spotting (KWS) systems\nrequires thresholding continuous detection scores. Selecting appropriate\nthresholds is a non-trivial task, typically relying on optimizing the\nperformance on a validation dataset. However, such greedy threshold selection\noften leads to suboptimal performance on unseen data, particularly in varying\nor noisy acoustic environments or few-shot settings. In this work, we\ninvestigate detection threshold estimation for template-based open-set few-shot\nKWS using dynamic time warping on noisy speech data. To mitigate the\nperformance degradation caused by suboptimal thresholds, we propose a score\ncalibration approach consisting of two different steps: quantizing embeddings\nand normalizing detection scores using the quantization error prior to\nthresholding. Experiments on KWS-DailyTalk with simulated high frequency radio\nchannels show that the proposed calibration approach simplifies the choice of\ndetection thresholds and significantly improves the resulting performance."}
{"id": "2510.15227", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15227", "abs": "https://arxiv.org/abs/2510.15227", "authors": ["Xiaohan Zhao", "Hongyu Xiang", "Shengze Ye", "Song Li", "Zhengkun Tian", "Guanyu Chen", "Ke Ding", "Guanglu Wan"], "title": "LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models", "comment": null, "summary": "This paper presents LongCat-Audio-Codec, an audio tokenizer and detokenizer\nsolution designed for industrial grade end-to-end speech large language models.\nBy leveraging a decoupled model architecture and a multistage training\nstrategy, LongCat-Audio-Codec exhibits robust semantic modeling capabilities,\nflexible acoustic feature extraction capabilities, and low-latency streaming\nsynthesis capabilities. It encodes speech at an ultra-low frame rate of 16.67\nHz, with a minimum bitrate of 0.43 kbps and a maximum bitrate of 0.87 kbps.\nEvaluation results demonstrate that LongCat-Audio-Codec achieves strong speech\nintelligibility and is capable of synthesizing highquality speech at low\nbitrate, thus effectively balancing coding efficiency and decoding quality. The\ninference code and model checkpoints of LongCat-Audio-Codec are available at:\nhttps://github.com/meituan-longcat/LongCat-Audio-Codec."}
{"id": "2510.15070", "categories": ["eess.SP", "math.DG", "94A14, 94A12, 14M15, 53C22"], "pdf": "https://arxiv.org/pdf/2510.15070", "abs": "https://arxiv.org/abs/2510.15070", "authors": ["Álvaro Pendás-Recondo", "Enrique Pendás-Recondo"], "title": "A Structured Family of Grassmannian Constellations via Geodesic Mapping for MIMO Noncoherent Communications", "comment": "13 pages, 7 figures", "summary": "This work presents a novel structured family of Grassmannian constellations\nfor multiple-input multiple-output (MIMO) noncoherent communications over\nRayleigh block-fading channels, where neither the transmitter nor the receiver\nhas channel state information (CSI). The proposed constellation design is built\nupon the geodesic curves of the Grassmann manifold, thereby exploiting its\nunderlying geometric structure. The resulting solution is limited in spectral\nefficiency (with a maximum constellation size of $4M^2$ points, where $M$ is\nthe number of transmit antennas), targeting a rate in the range of $0.25$-$1$\nbps/Hz. However, all space-time matrices resulting from this design exhibit the\nremarkable property of having a single nonzero entry per row, meaning that only\none transmit antenna is active per time slot. This property significantly\nreduces hardware complexity and implementation cost, while also lowering power\nconsumption, as only a single power amplifier is required for transmission.\nFurthermore, within the constellation size limits, the proposed design achieves\nerror performance comparable to state-of-the-art optimization-based\nunstructured designs, as validated through symbol error rate (SER) numerical\nresults. It also enables simple yet effective bit labeling, confirmed by\ncomparisons of bit error rate (BER) and SER, and reduces the computational\ncomplexity of the maximum-likelihood (ML) detector for Grassmannian\nconstellations by a factor of $M$."}
{"id": "2510.15364", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.15364", "abs": "https://arxiv.org/abs/2510.15364", "authors": ["Jiawei Jiang", "Linping Xu", "Dejun Zhang", "Qingbo Huang", "Xianjun Xia", "Yijian Xiao"], "title": "LDCodec: A high quality neural audio codec with low-complexity decoder", "comment": null, "summary": "Neural audio coding has been shown to outperform classical audio coding at\nextremely low bitrates. However, the practical application of neural audio\ncodecs is still limited by their elevated complexity. To address this\nchallenge, we have developed a high-quality neural audio codec with a\nlow-complexity decoder, named LDCodec (Low-complexity Decoder Neural Audio\nCodec), specifically designed for on-demand streaming media clients, such as\nsmartphones. Specifically, we introduced a novel residual unit combined with\nLong-term and Short-term Residual Vector Quantization (LSRVQ), subband-fullband\nfrequency discriminators, and perceptual loss functions. This combination\nresults in high-quality audio reconstruction with lower complexity. Both our\nsubjective and objective tests demonstrated that our proposed LDCodec at 6kbps\noutperforms Opus at 12kbps."}
{"id": "2510.15195", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.15195", "abs": "https://arxiv.org/abs/2510.15195", "authors": ["Nishant Mehrotra", "Sandesh Rao Mattu", "Robert Calderbank"], "title": "Pulse Shaping Filter Design for Integrated Sensing & Communication with Zak-OTFS", "comment": "6 pages, 3 figures, to be submitted to IEEE for possible publication", "summary": "Zak-OTFS is an emerging framework for integrated sensing & communication\n(ISAC) in high delay and Doppler spread environments. A critical enabler for\nISAC with Zak-OTFS is the design of pulse shaping filters. For sensing, a\nlocalized pulse shaping filter enables ideal input-output (I/O) relation\nestimates close to the physical scattering channel. For communication,\northogonality of the pulse shape on the information lattice prevents\ninter-symbol interference, and no time and bandwidth expansion enables full\nspectral efficiency. A filter simultaneously meeting all three objectives is\nideal for ISAC. Existing filter designs achieve two of the above objectives,\nbut not all three simultaneously. For instance, the sinc filter is orthogonal\nand bandwidth/time-limited, but is not localized. The Gaussian filter is\nlocalized and bandwidth/time-limited, but not orthogonal. The RRC filter is\nlocalized and orthogonal, but not bandwidth/time-limited. A recently proposed\nhybrid Gaussian-sinc filter is more localized than the sinc filter and\nbandwidth/time-limited, but is not orthogonal. In this work, we design optimal\npulse shaping filters meeting all three objectives via the Isotropic Orthogonal\nTransform Algorithm. The proposed pulse shaping filters offer improved data\ndetection (communication) and I/O relation estimation (sensing) performance\ncompared to existing filter choices in the literature."}
{"id": "2510.15383", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15383", "abs": "https://arxiv.org/abs/2510.15383", "authors": ["Chitralekha Gupta", "Soundarya Ramesh", "Praveen Sasikumar", "Kian Peen Yeo", "Suranga Nanayakkara"], "title": "DroneAudioset: An Audio Dataset for Drone-based Search and Rescue", "comment": "Accepted in Neurips (Datasets and Benchmarks Track) 2025. The first\n  two authors are equal contributors", "summary": "Unmanned Aerial Vehicles (UAVs) or drones, are increasingly used in search\nand rescue missions to detect human presence. Existing systems primarily\nleverage vision-based methods which are prone to fail under low-visibility or\nocclusion. Drone-based audio perception offers promise but suffers from extreme\nego-noise that masks sounds indicating human presence. Existing datasets are\neither limited in diversity or synthetic, lacking real acoustic interactions,\nand there are no standardized setups for drone audition. To this end, we\npresent DroneAudioset (The dataset is publicly available at\nhttps://huggingface.co/datasets/ahlab-drone-project/DroneAudioSet/ under the\nMIT license), a comprehensive drone audition dataset featuring 23.5 hours of\nannotated recordings, covering a wide range of signal-to-noise ratios (SNRs)\nfrom -57.2 dB to -2.5 dB, across various drone types, throttles, microphone\nconfigurations as well as environments. The dataset enables development and\nsystematic evaluation of noise suppression and classification methods for\nhuman-presence detection under challenging conditions, while also informing\npractical design considerations for drone audition systems, such as microphone\nplacement trade-offs, and development of drone noise-aware audio processing.\nThis dataset is an important step towards enabling design and deployment of\ndrone-audition systems."}
{"id": "2510.15278", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15278", "abs": "https://arxiv.org/abs/2510.15278", "authors": ["Heyao Zhu", "Yimeng Zhao", "Zirui Zhang", "Huansheng Yi", "Chenbin Gao", "Canhua Xu", "Jianqi Wang", "Fugui Qi"], "title": "Multidimensional Physiology-Inspired Enhanced Vital Sign Monitoring Using MIMO mmWave Bio-radar", "comment": null, "summary": "With the intensiffcation of population aging and increasing burden of chronic\ndiseases, the demand for vital signs monitoring is becoming increasingly\nurgent. A key challenge facing current non-contact detection technologies using\nmillimeter wave (mmWave) radar is the low efffciency of multi-channel signal\nfusion in array radar systems based on equal weighting. To address this\nchallenge, this paper proposes a vital sign enhancement detection method for\nmultiple input and multiple output (MIMO) bio-radar, driven by multidimensional\nphysiological characteristics, which overcomes traditional limitations through\na two-stage fusion strategy. Stage 1: Enhanced Vital Sign Detection Using\nSingle-Channel Signals Based on Physiological Characteristics. First, a chest\nwall multi-scattering point model is constructed. For single channel\ntime-distance two-dimensional echo signals, effective range bins are selected\nbased on the respiratory/cardiac physiological frequency band energy ratio, and\nthe signal-to-noise ratio (SNR) of respiration/heart signals is enhanced using\nphase-aligned maximal ratio combining (MRC). Stage 2: Multi-Channel Fusion\nBased on Organ Radiation Spatial Distribution Characteristics. The spatial\nradiation characteristics of cardiopulmonary organs are introduced for the\nffrst time as the theoretical foundation for SNR-based channel screening,\nchannel attribute identiffcation, and multi-channel weighted fusion. Then, we\npropose a template matching method to extract respiratory rate (RR) and heart\nrate (HR) by adopting physical models of respiration and cardiac activities.\nThe experimental results demonstrate the existence of the spatial distribution\ncharacteristics of organ radiation. In addition, we analyzed the impact of\ndistance and state on the algorithm from these two aspects."}
{"id": "2510.15409", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.15409", "abs": "https://arxiv.org/abs/2510.15409", "authors": ["Azalea Gui", "Woosung Choi", "Junghyun Koo", "Kazuki Shimada", "Takashi Shibuya", "Joan Serrà", "Wei-Hsiang Liao", "Yuki Mitsufuji"], "title": "Towards Blind Data Cleaning: A Case Study in Music Source Separation", "comment": "Submitted to IEEE ICASSP 2026", "summary": "The performance of deep learning models for music source separation heavily\ndepends on training data quality. However, datasets are often corrupted by\ndifficult-to-detect artifacts such as audio bleeding and label noise. Since the\ntype and extent of contamination are typically unknown, cleaning methods\ntargeting specific corruptions are often impractical. This paper proposes and\nevaluates two distinct, noise-agnostic data cleaning methods to address this\nchallenge. The first approach uses data attribution via unlearning to identify\nand filter out training samples that contribute the least to producing clean\noutputs. The second leverages the Fr\\'echet Audio Distance to measure and\nremove samples that are perceptually dissimilar to a small and trusted clean\nreference set. On a dataset contaminated with a simulated distribution of\nreal-world noise, our unlearning-based methods produced a cleaned dataset and a\ncorresponding model that outperforms both the original contaminated data and\nthe small clean reference set used for cleaning. This result closes\napproximately 66.7\\% of the performance gap between the contaminated baseline\nand a model trained on the same dataset without any contamination. Unlike\nmethods tailored for specific artifacts, our noise-agnostic approaches offer a\nmore generic and broadly applicable solution for curating high-quality training\ndata."}
{"id": "2510.15457", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15457", "abs": "https://arxiv.org/abs/2510.15457", "authors": ["Chunhui Li", "Chengrui Wang", "Zhiqiang Yuan", "Wei Fan"], "title": "Multi-Target Flexible Angular Emulation for ISAC Base Station Testing Using a Conductive Amplitude and Phase Matrix Setup: Framework and Experimental Validation", "comment": null, "summary": "Comprehensive evaluation of the functionalities, algorithms, hardware\ncomponents, and performance characteristics of future integrated sensing and\ncommunication (ISAC) base stations (BSs) under realistic deployment scenarios\nin controlled laboratory environments represents a critical requirement for\nISAC technology advancement. A primary challenge in achieving this objective\ninvolves the emulation of multiple targets with arbitrary radar cross-section\n(RCS), range, angle, and Doppler profiles for ISAC BS equipped with large-scale\nantenna arrays using radar target simulator (RTS) with limited interface ports.\nIn this work, we introduce a simple yet highly effective and practical\nconductive amplitude and phase matrix framework to address this fundamental\nchallenge. The core concept involves introducing a tunable conductive amplitude\nand phase modulation network in the test configuration between the ISAC BS\nunder test and a RTS. Based on this structure, we subsequently investigate the\ncorresponding configurations for different sensing operational modes of ISAC\nBSs, specifically the array duplex transmission and reception (ADTR) mode and\nthe split-array transmission and reception (SATR) mode. For experimental\nvalidation, we design two distinct monostatic sensing scenarios to demonstrate\nthe framework capabilities across both operational modes. The first scenario\ninvolves dynamic multi-drone sensing validation for ADTR mode operation, while\nthe second scenario addresses static single-drone sensing for SATR mode\nvalidation. The experimental results demonstrate that the proposed framework\ncan accurately emulate the joint RCS, range, velocity, and angular\ncharacteristics of multiple sensing targets within the conductive test\nenvironment, highlighting its significant potential for testing applications in\nsub-6 GHz ISAC BS development and validation."}
{"id": "2510.15432", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.15432", "abs": "https://arxiv.org/abs/2510.15432", "authors": ["Kevin Wilkinghoff", "Alessia Cornaggia-Urrigshardt", "Zheng-Hua Tan"], "title": "Quantization-Based Score Calibration for Few-Shot Keyword Spotting with Dynamic Time Warping in Noisy Environments", "comment": null, "summary": "Detecting occurrences of keywords with keyword spotting (KWS) systems\nrequires thresholding continuous detection scores. Selecting appropriate\nthresholds is a non-trivial task, typically relying on optimizing the\nperformance on a validation dataset. However, such greedy threshold selection\noften leads to suboptimal performance on unseen data, particularly in varying\nor noisy acoustic environments or few-shot settings. In this work, we\ninvestigate detection threshold estimation for template-based open-set few-shot\nKWS using dynamic time warping on noisy speech data. To mitigate the\nperformance degradation caused by suboptimal thresholds, we propose a score\ncalibration approach consisting of two different steps: quantizing embeddings\nand normalizing detection scores using the quantization error prior to\nthresholding. Experiments on KWS-DailyTalk with simulated high frequency radio\nchannels show that the proposed calibration approach simplifies the choice of\ndetection thresholds and significantly improves the resulting performance."}
{"id": "2510.15575", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15575", "abs": "https://arxiv.org/abs/2510.15575", "authors": ["Yi Tao", "Zhen Gao", "Zhuoran Li", "Ziwei Wan", "Tuan Li", "Chunli Zhu", "Lei Chen", "Guanghui Wen", "Dezhi Zheng", "Dusit Niyato"], "title": "Pseudo-Random TDM-MIMO FMCW Based Millimeter-Wave Sensing and Communication Integration for UAV Swarm", "comment": null, "summary": "The integrated sensing and communications (ISAC) can achieve the sharing of\nhardware and spectrum resources, enabling efficient data transmission and\nenvironmental sensing. This fusion is particularly important for unmanned\naerial vehicle (UAV) swarms, as it enhances the overall performance,\nflexibility, and efficiency of such systems. To facilitate the collaborative\noperations among UAVs, this paper proposes an ISAC solution based on the\npseudo-random time-division multiplexing (TDM)-multiple input multiple output\n(MIMO) millimeter-wave (mmWave) frequency modulated continuous wave (FMCW).\nSpecifically, a novel ISAC chirp waveform is proposed to modulate data in both\nthe delay domain and complex amplitude, while also possessing high-precision\nsensing capabilities. To address challenges in the TDM-MIMO, we utilize the\npseudo-random antenna selection and compressed sensing algorithms, ensuring\nthat the maximum unambiguous velocity is not compromised. Moreover, by\nemploying a chirp-division multiple access scheme, we propose an\ninterference-free multiple antenna transmission scheme to achieve dynamic\nallocation of time-frequency resources and multi-user transmission. Finally, we\npropose a communication and sensing fusion-based dynamic iterative computation\nscheme, simultaneously achieving data demodulation and sensing parameter\nestimation. Simulation results show that the proposed scheme can achieve ISAC\nunder the dynamic flight scenarios of UAVs. Meanwhile, the scheme outperforms\nthe mmWave-LoRadar in communication and sensing performance, yet its sensing\nperformance is slightly lower than that of the traditional FMCW. Under the\nurban clutter modeling, the scheme still maintains favorable robustness despite\na certain degree of performance degradation."}
{"id": "2510.15437", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.15437", "abs": "https://arxiv.org/abs/2510.15437", "authors": ["Tongtao Ling", "Shulin He", "Pengjie Shen", "Zhong-Qiu Wang"], "title": "MC-LExt: Multi-Channel Target Speaker Extraction with Onset-Prompted Speaker Conditioning Mechanism", "comment": "5 pages, 2 figures", "summary": "Multi-channel target speaker extraction (MC-TSE) aims to extract a target\nspeaker's voice from multi-speaker signals captured by multiple microphones.\nExisting methods often rely on auxiliary clues such as direction-of-arrival\n(DOA) or speaker embeddings. However, DOA-based approaches depend on explicit\ndirection estimation and are sensitive to microphone array geometry, while\nmethods based on speaker embeddings model speaker identity in an implicit\nmanner and may degrade in noisy-reverberant conditions. To address these\nlimitations, we propose multi-channel listen to extract (MC-LExt), a simple but\nhighly-effective framework for MC-TSE. Our key idea is to prepend a short\nenrollment utterance of the target speaker to each channel of the multi-channel\nmixture, providing an onset-prompted conditioning signal that can guide TSE.\nThis design allows the deep neural network (DNN) to learn spatial and speaker\nidentity cues jointly in a fully end-to-end manner. Experiments on\nnoisy-reverberant benchmarks, including WHAMR! and MC-Libri2Mix, demonstrate\nthe effectiveness of MC-TSE."}
{"id": "2510.15689", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15689", "abs": "https://arxiv.org/abs/2510.15689", "authors": ["Gebreslassie atsbha weldegebrial", "hunduma legesse geleta"], "title": "More on Boundary Behavior of Univalent Harmonic Mappings", "comment": "10 pages and 3 figures", "summary": "Many authors have examined various boundary behaviors of injective harmonic\nmappings in the open unit disk. Building on Laugesen's work, Bshouty and others\nexplored the boundary behavior of harmonic mappings under different conditions.\nIn this paper, we extend their work and find out the angular limits of the\narguments and logarithms of analytic functions under various conditions. We\nalso examined the dilatation possesses only a finite set of zeros within any\nstolz angle if the first derivative of harmonic function $f$ at the boundary is\npositive infinity."}
{"id": "2510.15659", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.15659", "abs": "https://arxiv.org/abs/2510.15659", "authors": ["Rongfeng Su", "Mengjie Du", "Xiaokang Liu", "Lan Wang", "Nan Yan"], "title": "Magnitude and Phase-based Feature Fusion Using Co-attention Mechanism for Speaker recognition", "comment": null, "summary": "Phase-based features related to vocal source characteristics can be\nincorporated into magnitude-based speaker recognition systems to improve the\nsystem performance. However, traditional feature-level fusion methods typically\nignore the unique contributions of speaker semantics in the magnitude and phase\ndomains. To address this issue, this paper proposed a feature-level fusion\nframework using the co-attention mechanism for speaker recognition. The\nframework consists of two separate sub-networks for the magnitude and phase\ndomains respectively. Then, the intermediate high-level outputs of both domains\nare fused by the co-attention mechanism before a pooling layer. A correlation\nmatrix from the co-attention module is supposed to re-assign the weights for\ndynamically scaling contributions in the magnitude and phase domains according\nto different pronunciations. Experiments on VoxCeleb showed that the proposed\nfeature-level fusion strategy using the co-attention mechanism gave the Top-1\naccuracy of 97.20%, outperforming the state-of-the-art system with 0.82%\nabsolutely, and obtained EER reduction of 0.45% compared to single feature\nsystem using FBank."}
{"id": "2510.15717", "categories": ["eess.SP", "68T05, 92C55", "I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15717", "abs": "https://arxiv.org/abs/2510.15717", "authors": ["Mehdi Zekriyapanah Gashti", "Mostafa Mohammadpour", "Hassan Eshkiki"], "title": "Detection Seizure Onset Zone Using Circadian Fluctuating Epileptic Biomarkers: A Signal Processing and Machine Learning Approach", "comment": null, "summary": "Epileptic biomarkers play a crucial role in identifying the origin of\nseizures, an essential aspect of pre-surgical planning for epilepsy treatment.\nThese biomarkers can vary significantly over time. By studying these temporal\nfluctuations, we can enhance their effectiveness in guiding surgical planning.\nThis research focuses on examining how circadian rhythms influence epilepsy\nbiomarkers and aims to determine the optimal times for their analysis. To\ninvestigate the relationship between epilepsy biomarkers and circadian rhythm,\nthe sleep/wake states first need to be classified. After the biomarkers are\nidentified, they are compared across these states. A retrospective analysis was\nconducted on intracranial electroencephalography data from patients with focal\nepilepsy. The biomarkers spike, sequence of spikes, high-frequency oscillations\n(HFOs), and pathological HFOs were identified through automatic detection. The\nalpha/delta ratio was also calculated to distinguish between asleep and awake\nstages. Data from 9 patients were analyzed, and the classification of sleep and\nwake states was achieved with an area under the curve of 84%. All biomarker\nrates were higher during the sleep stage compared to the wake stage.\nPathological HFOs and the sequence of spikes proved to be more precise\nindicators regarding distance to seizure onset than spikes or HFOs. Unlike\nprevious studies that relied predominantly on long-term spike biomarker\nanalysis, this study is the first to utilize a comprehensive set of biomarkers,\nincluding HFOs, spike sequences, and pathological HFOs, to enhance seizure\nonset zone prediction. The rates of epilepsy biomarkers during sleep vary\nconsiderably from those seen while awake, making sleep data analysis more\neffective for accurately predicting the seizure onset zone."}
{"id": "2510.15759", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15759", "abs": "https://arxiv.org/abs/2510.15759", "authors": ["Ishan Rangajith Koralege", "Nurul Huda Mahmood", "Arthur Sousa de Sena", "Italo Atzeni"], "title": "On the Impact of Electromagnetic Interference and Inter-RIS Reflections in Indoor Factory Local 6G Networks", "comment": null, "summary": "The Sixth Generation (6G) radio technology is expected to include local 6G\nnetworks as a special use case, extending the capabilities of `generic' 6G\nnetworks towards more demanding performance requirements. Reconfigurable\nintelligent surfaces (RISs) offer a novel paradigm for next-generation wireless\ncommunications, especially in the context of local 6G networks, enabling\nadvanced signal propagation control through intelligent phase-shift\nconfigurations. However, in practical deployments, their performance can be\nadversely affected by electromagnetic interference (EMI) from external sources\nand inter-RIS reflections (IRR) caused by signal reflections between multiple\ncolocated RIS units. This paper presents a comprehensive analysis of the joint\nimpact of EMI and IRR in a multi-RIS multi-cell system deployed within an\nindoor factory environment. A detailed evaluation study is first carried out to\ninvestigate their impact on system performance. System-level simulations\ndemonstrate that the joint impact of EMI and IRR degrades system performance\nmore significantly than their individual effects, particularly as RIS\ndimensions and transmit power increase. To address these adverse effects, an\nalternate optimization algorithm using the Riemannian conjugate gradient method\nis then proposed. The novel algorithm optimizes the phase shifts of the RIS\nelements considering the spatial correlation among their associated channels,\nand is found to provide up to several orders of magnitude gains in terms of the\nsystem sum rate and the outage probability."}
{"id": "2510.15763", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15763", "abs": "https://arxiv.org/abs/2510.15763", "authors": ["Qihao Peng", "Jiuyu Liu", "Qu Luo", "Yi Ma", "Pei Xiao", "Maged Elkashlan", "George K. Karagiannidis"], "title": "RIS-assisted Atomic MIMO Receiver", "comment": "Submitted to IEEE journals", "summary": "In this paper, we propose a novel and low-complexity atomic multiple-input\nmultiple-output (MIMO) receiver architecture assisted by a reconfigurable\nintelligent surface (RIS). By introducing RIS and utilizing pulse amplitude\nmodulation (PAM), the phase of the transmitted signal is effectively aligned\nwith that of the local oscillator (LO), thereby mitigating phase ambiguity and\nsubstantially reducing both signal detection complexity and overall receiver\ncomplexity.To tackle the resulting non-convex optimization problem, we\nreformulate it into a tractable form by minimizing the Frobenius norm of an\nequivalent matrix, which is efficiently solved using an Adam-based gradient\ndescent algorithm."}
{"id": "2510.15773", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15773", "abs": "https://arxiv.org/abs/2510.15773", "authors": ["Qihao Peng Tierui Gong", "Zihang Song", "Qu Luo", "Cunhua Pan", "Pei Xiao", "Chau Yuen"], "title": "Rydberg Atomic Quantum Satellites for Enhanced Ground-to-Space Direct Uplink Access", "comment": "Submitted to IEEE journals", "summary": "This paper investigates the performance advantages of Rydberg atomic quantum\n(RAQ)-based multiple-input multiple-output (MIMO) satellites for enhancing\ndirect ground-to-space uplink access.We analytically evaluate the impact of\nRydberg atoms on channel estimation by deriving closed-form expressions for the\nmean-square error (MSE) and normalized mean-square error (NMSE). Based on the\nestimated channels, we further derive lower bounds on the achievable data rates\nfor maximum ratio combining (MRC) and zero-forcing (ZF) detection schemes.\nRigorous analysis demonstrates that RAQ-MIMO outperforms conventional\nradio-frequency (RF) MIMO under both Rayleigh and satellite channel conditions.\nSpecifically, compared with conventional MIMO, RAQR achieves a ``squaring\" gain\nunder Rayleigh fading, especially in long-distance transmission scenarios with\nstringent power constraints. In contrast, under line-of-sight (LoS)-dominated\nsatellite channels, this gain saturates as channel-estimation benefits\ndiminish, with the remaining improvement primarily arising from the normalized\nnoise background. Monte Carlo simulations validate the analytical results and\nshow that the performance gains of RAQ-MIMO satellites translate into smaller\nantenna apertures, lower transmit power, and longer communication ranges,\nthereby paving the way for next-generation satellite networks."}
{"id": "2510.15784", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.15784", "abs": "https://arxiv.org/abs/2510.15784", "authors": ["Qihao Peng", "Qu Luo", "Zheng Chu", "Neng Ye", "Hong Ren", "Cunhua Pan", "Lixia Xiao", "Pei Xiao"], "title": "From Active to Battery-Free: Rydberg Atomic Quantum Receivers for Self-Sustained SWIPT-MIMO Networks", "comment": "Submitted to IEEE journals", "summary": "In this paper, we proposed a hybrid simultaneous wireless information and\npower transfer (SWIPT)-enabled multiple-input multiple-output (MIMO)\narchitecture, where the base station (BS) uses a conventional RF transmitter\nfor downlink transmission and a Rydberg atomic quantum receiver (RAQR) for\nreceiving uplink signal from Internet of Things (IoT) devices. To fully exploit\nthis integration, we jointly design the transmission scheme and the\npower-splitting strategy to maximize the sum rate, which leads to a non-convex\nproblem. To address this challenge, we first derive closed-form lower bounds on\nthe uplink achievable rates for maximum ratio combining (MRC) and zero-forcing\n(ZF), as well as on the downlink rate and harvested energy for maximum ratio\ntransmission (MRT) and ZF precoding. Building upon these bounds, we propose an\niterative algorithm relying on the best monomial approximation and geometric\nprogramming (GP) to solve the non-convex problem. Finally, simulations validate\nthe tightness of our derived lower bounds and demonstrate the superiority of\nthe proposed algorithm over benchmark schemes. Importantly, by integrating RAQR\nwith SWIPT-enabled MIMO, the BS can reliably detect weak uplink signals from\nIoT devices powered only by harvested energy, enabling battery-free\ncommunication."}
{"id": "2510.15810", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.15810", "abs": "https://arxiv.org/abs/2510.15810", "authors": ["Luis F. Abanto-Leon", "Setareh Maghsudi"], "title": "Resilient Full-Duplex ISAC in the Face of Imperfect SI Cancellation: Globally Optimal Timeslot Allocation and Beam Selection", "comment": null, "summary": "This work addresses the radio resource management (RRM) design in downlink\nfull-duplex integrated sensing and communications (ISAC) systems, jointly\noptimizing timeslot allocation and beam selection under imperfect\nself-interference cancellation. Timeslot allocation governs the distribution of\ndiscrete channel uses between sensing and communication tasks, while beam\nselection determines transmit and receive directions along with adaptive\nbeamwidths. The joint design leads to a semi-infinite, nonconvex mixed-integer\nnonlinear program (MINLP), which is difficult to solve. To overcome this, we\ndevelop a tailored reformulation strategy that transforms the problem into a\ntractable mixed-integer linear program (MILP), enabling globally optimal\nsolutions. Our approach provides insights into the coordinated optimization of\ntimeslot allocation and beam selection, enhancing the efficiency of full-duplex\nISAC systems while ensuring resilience against residual self-interference."}

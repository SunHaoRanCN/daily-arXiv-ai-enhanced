{"id": "2510.16094", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16094", "abs": "https://arxiv.org/abs/2510.16094", "authors": ["Carsten Andrich", "Isabella Varga", "Tobias F. Nowack", "Alexander Ihlow", "Sebastian Giehl", "Michael Schubert", "Reiner S. Thomä", "Matthias A. Hein"], "title": "Wideband Antenna Deconvolution for Bistatic Millimeter Wave Radar Reflectivity Measurements", "comment": "5 pages, 5 figures, submitted to EuCAP'26", "summary": "Bistatic radar measurements offer unique spatial diversity and enhanced\ntarget characterization capabilities, rendering them increasingly vital for\ncontemporary sensing application research. The reliability of such measurements\nis contingent upon precise system and antenna calibration. The prevailing\ntechnique is the substitution method, which involves the use of known reference\nobjects. We propose an over-the-air calibration algorithm for spherical\nbistatic measurement systems. Our method is both significantly simpler and\ntwice as fast as existing algorithms. The application of our technique to\nreflectivity measurements of a metal sphere from 76 to 81 GHz demonstrates a\ndynamic range enhancement of up to 40 dB when compared with uncalibrated data.\nA comparison with simulation data demonstrates a high degree of agreement\nbetween measurement and simulation."}
{"id": "2510.16172", "categories": ["eess.SP", "cs.MS", "51-08", "D.2.2; D.2.8; D.2.13"], "pdf": "https://arxiv.org/pdf/2510.16172", "abs": "https://arxiv.org/abs/2510.16172", "authors": ["Jérome Eertmans", "Sophie Lequeu", "Benoît Legat", "Laurent Jacques", "Claude Oestges"], "title": "Fast, Differentiable, GPU-Accelerated Ray Tracing for Multiple Diffraction and Reflection Paths", "comment": "5 pages, 3 figures, submitted to EuCAP 2026", "summary": "We present a fast, differentiable, GPU-accelerated optimization method for\nray path tracing in environments containing planar reflectors and straight\ndiffraction edges. Based on Fermat's principle, our approach reformulates the\npath-finding problem as the minimization of total path length, enabling\nefficient parallel execution on modern GPU architectures. Unlike existing\nmethods that require separate algorithms for reflections and diffractions, our\nunified formulation maintains consistent problem dimensions across all\ninteraction sequences, making it particularly suitable for vectorized\ncomputation. Through implicit differentiation, we achieve efficient gradient\ncomputation without differentiating through solver iterations, significantly\noutperforming traditional automatic differentiation approaches. Numerical\nsimulations demonstrate convergence rates comparable to specialized Newton\nmethods while providing superior scalability for large-scale applications. The\nmethod integrates seamlessly with differentiable programming libraries such as\nJAX and DrJIT, enabling new possibilities in inverse design and optimization\nfor wireless propagation modeling. The source code is openly available at\nhttps://github.com/jeertmans/fpt-jax."}
{"id": "2510.16200", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16200", "abs": "https://arxiv.org/abs/2510.16200", "authors": ["Lorenz Mohr", "Michael Döbereiner", "Steffen Schieler", "Joerg Robert", "Christian Schneider", "Sebastian Semper", "Reiner S. Thoma"], "title": "Performance Comparison of Joint Delay-Doppler Estimation Algorithms", "comment": "5 pages, 4 figures", "summary": "Integrated sensing and communications (ISAC), radar, and beamforming require\nreal-time, high-resolution estimation algorithms to determine delay-Doppler\nvalues of specular paths within the wireless propagation channel. Our\ncontribution is the measurement-based performance comparison of the\ndelay-Doppler estimation between three different algorithms, comprising maximum\nlikelihood (ML), convolutional neural network (CNN), and constant false alarm\nrate (CFAR) approaches. We apply these algorithms to publicly available channel\ndata which includes two spherical targets with analytically describable\ndelay-Doppler parameters. The comparison of the three algorithms features the\ntarget detection rate, root mean squared errors (RMSEs) of the delay-Doppler\nestimates, and a runtime analysis. Notably, all three algorithms demonstrate\nsimilar parameter estimation capabilities in bi-static scenarios, achieving\ntarget detection probabilities of up to 80%. Conversely, forward and backward\nscattering conditions pose a problem to the estimation due to strong\nline-of-sight (LoS) contribution, reducing the corresponding detection\nprobability down to 0%."}
{"id": "2510.16296", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16296", "abs": "https://arxiv.org/abs/2510.16296", "authors": ["Yuan Ai", "Xidong Mu", "Pengbo Si", "Yuanwei Liu"], "title": "Delay Minimization in Pinching-Antenna-enabled NOMA-MEC Networks", "comment": null, "summary": "This letter proposes a novel pinching antenna systems (PASS) enabled\nnon-orthogonal multiple access (NOMA) multi-access edge computing (MEC)\nframework. An optimization problem is formulated to minimize the maximum task\ndelay by optimizing offloading ratios, transmit powers, and pinching antenna\n(PA) positions, subject to constraints on maximum transmit power, user energy\nbudgets, and minimum PA separation to mitigate coupling effects. To address the\nnon-convex problem, a bisection search-based alternating optimization (AO)\nalgorithm is developed, where each subproblem is iteratively solved for a given\ntask delay. Numerical simulations demonstrate that the proposed framework\nsignificantly reduces the task delay compared to benchmark schemes."}
{"id": "2510.16156", "categories": ["eess.AS", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.16156", "abs": "https://arxiv.org/abs/2510.16156", "authors": ["Yueqian Lin", "Zhengmian Hu", "Jayakumar Subramanian", "Qinsi Wang", "Nikos Vlassis", "Hai \"Helen\" Li", "Yiran Chen"], "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning", "comment": "Accepted to the IEEE ASRU 2025 Demo Track", "summary": "Effective human-AI collaboration on complex reasoning tasks requires that\nusers understand and interact with the model's process, not just receive an\noutput. However, the monolithic text from methods like Chain-of-Thought (CoT)\nprevents this, as current interfaces lack real-time verbalization and robust\nuser barge-in. We present AsyncVoice Agent, a system whose asynchronous\narchitecture decouples a streaming LLM backend from a conversational voice\nfrontend. This design allows narration and inference to run in parallel,\nempowering users to interrupt, query, and steer the model's reasoning process\nat any time. Objective benchmarks show this approach reduces interaction\nlatency by more than 600x compared to monolithic baselines while ensuring high\nfidelity and competitive task accuracy. By enabling a two-way dialogue with a\nmodel's thought process, AsyncVoice Agent offers a new paradigm for building\nmore effective, steerable, and trustworthy human-AI systems for high-stakes\ntasks."}
{"id": "2510.16273", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16273", "abs": "https://arxiv.org/abs/2510.16273", "authors": ["Jingyue Huang", "Zachary Novack", "Phillip Long", "Yupeng Hou", "Ke Chen", "Taylor Berg-Kirkpatrick", "Julian McAuley"], "title": "MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding", "comment": null, "summary": "Discrete representation learning has shown promising results across various\ndomains, including generation and understanding in image, speech and language.\nInspired by these advances, we propose MuseTok, a tokenization method for\nsymbolic music, and investigate its effectiveness in both music generation and\nunderstanding tasks. MuseTok employs the residual vector quantized-variational\nautoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based\nencoder-decoder framework, producing music codes that achieve high-fidelity\nmusic reconstruction and accurate understanding of music theory. For\ncomprehensive evaluation, we apply MuseTok to music generation and semantic\nunderstanding tasks, including melody extraction, chord recognition, and\nemotion recognition. Models incorporating MuseTok outperform previous\nrepresentation learning baselines in semantic understanding while maintaining\ncomparable performance in content generation. Furthermore, qualitative analyses\non MuseTok codes, using ground-truth categories and synthetic datasets, reveal\nthat MuseTok effectively captures underlying musical concepts from large music\ncollections."}
{"id": "2510.16389", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16389", "abs": "https://arxiv.org/abs/2510.16389", "authors": ["Yubin Luo", "Li Yu", "Tao Wu", "Yuxiang Zhang", "Jianhua Zhang"], "title": "A Robust CSI-Based Scatterer Geometric Reconstruction Method for 6G ISAC System", "comment": null, "summary": "Digital twin (DT) is a core enabler of sixth generation (6G) mobile systems.\nAs a prerequisite for DT, scatterer geometric reconstruction (SGR) in\npropagation environments is essential but typically requires extra sensors such\nas cameras and LiDAR. With integrated sensing and communication (ISAC) in 6G,\nwe reinterpret the linear sampling method (LSM) from a wireless channel\nviewpoint and propose a CSI based variant for sensor free SGR: by exploiting\nthe shared channel characteristics of multipath and scattering, in band CSI\nreplaces the scattered field measurements usually required by LSM. However,\naperture limited arrays reduce LSM robustness. To address this, we propose\nmatched filtering enhanced multi frequency LSM (MF MLSM). Multi frequency data\nincreases frequency diversity, and matched filtering coherently aligns inter\nfrequency phases to avoid artifacts, both of which improve robustness.\nExperiments with apertures of 93.6 deg, 144 deg, and 180 deg and SNRs of 27 dB\nand 12 dB demonstrate robust SGR with this approach."}
{"id": "2510.16437", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16437", "abs": "https://arxiv.org/abs/2510.16437", "authors": ["Danielle Yaffe", "Ferdinand Campe", "Prachi Sharma", "Dorothea Kolossa", "Boaz Rafaely"], "title": "Audio-Visual Speech Enhancement for Spatial Audio - Spatial-VisualVoice and the MAVE Database", "comment": null, "summary": "Audio-visual speech enhancement (AVSE) has been found to be particularly\nuseful at low signal-to-noise (SNR) ratios due to the immunity of the visual\nfeatures to acoustic noise. However, a significant gap exists in AVSE methods\ntailored to enhance spatial audio under low-SNR conditions. The latter is of\ngrowing interest with augmented reality applications. To address this gap, we\npresent a multi-channel AVSE framework based on VisualVoice that leverages\nspatial cues from microphone arrays and visual information for enhancing the\ntarget speaker in noisy environments. We also introduce MAVe, a novel database\ncontaining multi-channel audio-visual signals in controlled, reproducible room\nconditions across a wide range of SNR levels. Experiments demonstrate that the\nproposed method consistently achieves significant gains in SI-SDR, STOI, and\nPESQ, particularly in low SNRs. Binaural signal analysis further confirms the\npreservation of spatial cues and intelligibility."}
{"id": "2510.16355", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16355", "abs": "https://arxiv.org/abs/2510.16355", "authors": ["Haocheng Yu", "Krishan K. Ahuja", "Lakshmi N. Sankar", "Spencer H. Bryngelson"], "title": "Transmission of High-Amplitude Sound through Leakages of Ill-fitting Earplugs", "comment": null, "summary": "High sound pressure levels (SPL) pose notable risks in loud environments,\nparticularly due to noise-induced hearing loss. Ill-fitting earplugs often lead\nto sound leakage, a phenomenon this study seeks to investigate. To validate our\nmethodology, we first obtained computational and experimental acoustic\ntransmission data for stand-alone slit resonators and orifices, for which\nextensive published data are readily available for comparison. We then examined\nthe frequency-dependent acoustic power absorption coefficient and transmission\nloss (TL) across various leakage geometries, modeled using different orifice\ndiameters. Experimental approaches spanned a frequency range of 1--5 kHz under\nSPL conditions of 120--150 dB. Key findings reveal that unsealed silicone\nrubber earplugs demonstrate an average TL reduction of approximately 18 dB at\nan overall incident SPL (OISPL) of 120 dB. Direct numerical simulations further\nhighlight SPL-dependent acoustic dissipation mechanisms, showing the conversion\nof acoustic energy into vorticity in ill-fitting earplug models at an OISPL of\n150 dB. These results highlight the role of earplug design for\nhigh-sound-pressure-level environments."}
{"id": "2510.16397", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16397", "abs": "https://arxiv.org/abs/2510.16397", "authors": ["Yiming Xu", "Dongfang Xu", "Shenghui Song", "Dusit Niyato"], "title": "Adaptive Sensing Performance Design for Enhancing Secure Communication in Networked ISAC Systems", "comment": "16 pages", "summary": "The channel state information (CSI) of an eavesdropper is crucial for\nphysical layer security (PLS) design, but it is difficult to obtain due to the\npassive and non-cooperative nature of the eavesdropper. To this end, integrated\nsensing and communication (ISAC) offers a novel solution by estimating the CSI\nof the eavesdropper based on sensing information. However, existing studies\nnormally impose explicit and fixed sensing performance requirement without\nconsidering the varying communication conditions, which hinders the system from\nfully exploiting the synergy between sensing and communication. To address this\nissue, this paper proposes sensing-enhanced secure communication with adaptive\nsensing performance. Specifically, we formulate the sensing performance\nimplicitly in the information leakage rate and adaptively optimize it for the\nminimization of the power consumption, offering enhanced flexibility and\nadaptability in sensing performance. We consider both centralized and\ndecentralized designs to thoroughly investigate the impact of network structure\non system performance and complexity. Specifically, we devise a block\ncoordinate descent (BCD)-based method for centralized design. For decentralized\ndesign, we develop an optimization framework based on consensus alternating\ndirection method of multipliers (ADMM) to reduce complexity and information\nexchange overhead. Experimental results demonstrate the advantage of the\nproposed implicit sensing performance requirement design due to its capability\nto adaptively adjust the sensing performance to enhance the system performance\nfor varying system configurations."}
{"id": "2510.16813", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16813", "abs": "https://arxiv.org/abs/2510.16813", "authors": ["Vojtěch Kovanda", "Pavel Rajmic"], "title": "Audio dequantization using instantaneous frequency", "comment": null, "summary": "We present a dequantization method that employs a phase-aware regularizer,\noriginally successfully applied in an audio inpainting problem. The method\nmaintains the temporal continuity of sinusoidal components in the audio signal\ntime-frequency representation and avoids the energy loss artifacts commonly\nencountered with l1-based regularization approaches. The proposed method is\ncalled the Phase-Aware Audio Dequantizer (PHADQ). The methods are evaluated\nusing the objective metric SDR and PEMO-Q ODG."}
{"id": "2510.16489", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16489", "abs": "https://arxiv.org/abs/2510.16489", "authors": ["Mark Huckvale"], "title": "Interpreting the Dimensions of Speaker Embedding Space", "comment": null, "summary": "Speaker embeddings are widely used in speaker verification systems and other\napplications where it is useful to characterise the voice of a speaker with a\nfixed-length vector. These embeddings tend to be treated as \"black box\"\nencodings, and how they relate to conventional acoustic and phonetic dimensions\nof voices has not been widely studied. In this paper we investigate how\nstate-of-the-art speaker embedding systems represent the acoustic\ncharacteristics of speakers as described by conventional acoustic descriptors,\nage, and gender. Using a large corpus of 10,000 speakers and three embedding\nsystems we show that a small set of 9 acoustic parameters chosen to be\n\"interpretable\" predict embeddings about the same as 7 principal components,\ncorresponding to over 50% of variance in the data. We show that some principal\ndimensions operate differently for male and female speakers, suggesting there\nis implicit gender recognition within the embedding systems. However we show\nthat speaker age is not well captured by embeddings, suggesting opportunities\nexist for improvements in their calculation."}
{"id": "2510.16482", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16482", "abs": "https://arxiv.org/abs/2510.16482", "authors": ["Romulo Aparecido", "Jiaqian Yang", "Ronit Sohanpal", "Zelin Gan", "Eric Sillekens", "John D. Downie", "Lidia Galdino", "Vitaly Mikhailov", "Daniel Elson", "Yuta Wakayama", "David DiGiovanni", "Jiawei Luo", "Robert I. Killey", "Polina Bayvel"], "title": "Single-Step Digital Backpropagation for O-band Coherent Transmission Systems", "comment": "conference, 3 pages, 2 figures", "summary": "We demonstrate digital backpropagation-based compensation of fibre\nnonlinearities in the near-zero dispersion regime of the O-band. Single-step\nDBP effectively mitigates self-phase modulation, achieving SNR gains of up to\n1.6 dB for 50 Gbaud PDM-256QAM transmission over a 2-span 151 km SMF-28 ULL\nfibre link."}
{"id": "2510.16841", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16841", "abs": "https://arxiv.org/abs/2510.16841", "authors": ["Wenxi Chen", "Xinsheng Wang", "Ruiqi Yan", "Yushen Chen", "Zhikang Niu", "Ziyang Ma", "Xiquan Li", "Yuzhe Liang", "Hanlin Wen", "Shunshun Yin", "Ming Tao", "Xie Chen"], "title": "SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization", "comment": null, "summary": "Speech codecs that convert continuous speech signals into discrete tokens\nhave become essential for speech language models (SLMs). However, existing\ncodecs struggle to balance high-quality reconstruction with semantically rich\nrepresentations, limiting their effectiveness in both generative and\nunderstanding tasks. In this work, we propose SAC, a neural speech codec with\nsemantic-acoustic dual-stream quantization. By disentangling semantic and\nacoustic modeling into two dedicated streams, SAC enables each to be optimized\nfor its respective role. Comprehensive evaluations show that SAC achieves\nstrong reconstruction performance across diverse bitrates under both clean and\nnoisy conditions, with particularly high scores on UTMOS and WER, demonstrating\nsuperior perceptual quality and intelligibility. Moreover, SAC substantially\noutperforms state-of-the-art codecs in semantic representation, achieving a\nlevel comparable to that of self-supervised learning (SSL) continuous\nembeddings. Finally, our analysis of speech disentanglement highlights the\neffectiveness of the dual-stream design, offering new potential for\ncontrollable speech applications."}
{"id": "2510.16700", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.16700", "abs": "https://arxiv.org/abs/2510.16700", "authors": ["Shiyao Wang", "Shiwan Zhao", "Jiaming Zhou", "Yong Qin"], "title": "Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios", "comment": "NCMMSC 2025 oral", "summary": "Dysarthric speech recognition (DSR) research has witnessed remarkable\nprogress in recent years, evolving from the basic understanding of individual\nwords to the intricate comprehension of sentence-level expressions, all driven\nby the pressing communication needs of individuals with dysarthria.\nNevertheless, the scarcity of available data remains a substantial hurdle,\nposing a significant challenge to the development of effective sentence-level\nDSR systems. In response to this issue, dysarthric data augmentation (DDA) has\nemerged as a highly promising approach. Generative models are frequently\nemployed to generate training data for automatic speech recognition tasks.\nHowever, their effectiveness hinges on the ability of the synthesized data to\naccurately represent the target domain. The wide-ranging variability in\npronunciation among dysarthric speakers makes it extremely difficult for models\ntrained on data from existing speakers to produce useful augmented data,\nespecially in zero-shot or one-shot learning settings. To address this\nlimitation, we put forward a novel text-coverage strategy specifically designed\nfor text-matching data synthesis. This innovative strategy allows for efficient\nzero/one-shot DDA, leading to substantial enhancements in the performance of\nDSR when dealing with unseen dysarthric speakers. Such improvements are of\ngreat significance in practical applications, including dysarthria\nrehabilitation programs and day-to-day common-sentence communication scenarios."}
{"id": "2510.16495", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16495", "abs": "https://arxiv.org/abs/2510.16495", "authors": ["Muhammad Khalil", "Ke Wang", "Jinho Choi"], "title": "Performance Evaluation of High Power Microwave Systems Against UAVs A Probabilistic Antenna Propagation Framework with Sensitivity Analysis", "comment": "10", "summary": "We develop a probabilistic, antenna- and propagation-centric framework to\nquantify the effectiveness of high-power microwave (HPM) engagements against\nunmanned aerial vehicles (UAVs). The model couples stochastic UAV kinematics, a\nbeam-steering jitter-to-gain mapping, and atmospheric propagation (free-space\nspreading with gaseous and rain loss) to obtain closed-form statistics of the\nreceived pulse energy. From these, we derive analytically evaluable per-pulse\nand cumulative neutralization probabilities using log-normal closures and\nGaussian--Hermite quadrature, and we provide a dwell-time expression under a\nstandard pulse-independence assumption. Analytical predictions closely match\nlarge-scale Monte-Carlo simulations across broad parameter ranges. For a\nrepresentative commercial threshold $E_{\\mathrm{th}} = 10^{-2}\\,\\mathrm{J}$,\nthe model predicts $\\bar{P}_{\\mathrm{kill}} \\gtrsim 0.4$ per pulse and\n$P_{\\mathrm{kill,tot}} > 99\\%$ within about $0.1\\,\\mathrm{s}$ at kHz PRF; for\nhardened platforms with $E_{\\mathrm{th}} = 10^{-1}\\,\\mathrm{J}$,\n$\\bar{P}_{\\mathrm{kill}} < 1\\%$ and $P_{\\mathrm{kill,tot}} < 20\\%$ after\n$1\\,\\mathrm{s}$. A closed-form sensitivity (elasticity) analysis shows\nperformance is dominated by slant range ($S_{\\bar{R}} \\approx -2$), with strong\nsecondary dependence on aperture diameter and transmit power; pointing jitter\nand atmospheric variability are comparatively less influential in the evaluated\nregimes. The framework yields fast, accurate, and physics-faithful performance\npredictions and exposes clear antenna/propagation design levers for HPM system\nsizing and risk-aware mission planning."}
{"id": "2510.16995", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16995", "abs": "https://arxiv.org/abs/2510.16995", "authors": ["Tsun-An Hsieh", "Minje Kim"], "title": "Adaptive Deterministic Flow Matching for Target Speaker Extraction", "comment": null, "summary": "Generative target speaker extraction (TSE) methods often produce more natural\noutputs than predictive models. Recent work based on diffusion or flow matching\n(FM) typically relies on a small, fixed number of reverse steps with a fixed\nstep size. We introduce Adaptive Discriminative Flow Matching TSE (AD-FlowTSE),\nwhich extracts the target speech using an adaptive step size. We formulate TSE\nwithin the FM paradigm but, unlike prior FM-based speech enhancement and TSE\napproaches that transport between the mixture (or a normal prior) and the\nclean-speech distribution, we define the flow between the background and the\nsource, governed by the mixing ratio (MR) of the source and background that\ncreates the mixture. This design enables MR-aware initialization, where the\nmodel starts at an adaptive point along the background-source trajectory rather\nthan applying the same reverse schedule across all noise levels. Experiments\nshow that AD-FlowTSE achieves strong TSE with as few as a single step, and that\nincorporating auxiliary MR estimation further improves target speech accuracy.\nTogether, these results highlight that aligning the transport path with the\nmixture composition and adapting the step size to noise conditions yields\nefficient and accurate TSE."}
{"id": "2510.16718", "categories": ["cs.SD", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16718", "abs": "https://arxiv.org/abs/2510.16718", "authors": ["Xusheng Yang", "Long Zhou", "Wenfu Wang", "Kai Hu", "Shulin Feng", "Chenxing Li", "Meng Yu", "Dong Yu", "Yuexian Zou"], "title": "U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation", "comment": null, "summary": "We propose \\textbf{U-Codec}, an \\textbf{U}ltra low frame-rate neural speech\n\\textbf{Codec} that achieves high-fidelity reconstruction and fast speech\ngeneration at an extremely low frame-rate of 5Hz (5 frames per second). Extreme\ncompression at 5Hz typically leads to severe intelligibility and spectral\ndetail loss, we introduce a Transformer-based inter-frame long-term dependency\nmodule and systematically explore residual vector quantization (RVQ) depth and\ncodebook size to identify optimal configurations. Moreover, we apply U-Codec\ninto a large language model (LLM)-based auto-regressive TTS model, which\nleverages global and local hierarchical architecture to effectively capture\ndependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer\nRVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that\nU-Codec improves LLM-based TTS inference speed by around 3 $\\times$ over\nhigh-frame-rate codecs while maintaining similarity and naturalness. These\nresults validate the feasibility of using highly compressed 5Hz discrete tokens\nfor fast and high-fidelity speech synthesis."}
{"id": "2510.16557", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16557", "abs": "https://arxiv.org/abs/2510.16557", "authors": ["Behrad Mousaei Shir-Mohammad", "Behzad Moshiri", "Abolfazl Yaghmaei"], "title": "Topology-Aware Hybrid Wi-Fi/BLE Fingerprinting via Evidence-Theoretic Fusion and Persistent Homology", "comment": null, "summary": "Indoor localization remains challenging in GNSS-denied environments due to\nmultipath, device heterogeneity, and volatile radio conditions. We propose a\ntopology-aware, hybrid Wi-Fi/BLE fingerprinting framework that (i) applies\nphysically consistent RSS normalization (dBm z-scoring or dBm -> linear mW ->\nz-score), (ii) denoises streams with classical Bayesian filters (KF/UKF/PF),\n(iii) combines complementary regressors (Random Forest and weighted kNN with a\ndiagonal Mahalanobis metric), (iv) performs evidence-theoretic fusion via\nDempster-Shafer theory (DST), and (v) augments each sample with\npersistent-homology (PH) descriptors. The system outputs both (x, y) estimates\nand interpretable belief maps, and is engineered for microcontroller-class\ndeployment with per-update cost O(T log M + log M + Mp + S).\n  We evaluate on two heterogeneous datasets, including a new 1,200-sample ESP32\nsurvey, and report ablations, robustness to test-only noise, and significance\nacross 10 stratified splits. Under 10% synthetic RSS noise, the full pipeline\nattains 3.40 m (Dataset 1) and 2.45 m (Dataset 2) RMSE, improving a strong PF +\nRF baseline by about 37%. Averaged across splits, it yields 4.993 +/- 0.15 m\nversus 6.292 +/- 0.13 m (20.6% relative reduction; p < 0.001). In noise-free\ntests, accuracy tightens to 0.44 m and 0.32 m (up to 56% better). Compared with\nrecent learning-heavy approaches that assume large site-specific datasets and\nGPU inference, our method delivers competitive accuracy with formal uncertainty\nquantification and low computational cost suitable for real-time deployment."}
{"id": "2510.16997", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16997", "abs": "https://arxiv.org/abs/2510.16997", "authors": ["Tsun-An Hsieh", "Sebastian Braun"], "title": "Towards Real-Time Generative Speech Restoration with Flow-Matching", "comment": null, "summary": "Generative models have shown robust performance on speech enhancement and\nrestoration tasks, but most prior approaches operate offline with high latency,\nmaking them unsuitable for streaming applications. In this work, we investigate\nthe feasibility of a low-latency, real-time generative speech restoration\nsystem based on flow-matching (FM). Our method tackles diverse real-world\ntasks, including denoising, dereverberation, and generative restoration. The\nproposed causal architecture without time-downsampling achieves introduces an\ntotal latency of only 20 ms, suitable for real-time communication. In addition,\nwe explore a broad set of architectural variations and sampling strategies to\nensure effective training and efficient inference. Notably, our flow-matching\nmodel maintains high enhancement quality with only 5 number of function\nevaluations (NFEs) during sampling, achieving similar performance as when using\n~20 NFEs under the same conditions. Experimental results indicate that causal\nFM-based models favor few-step reverse sampling, and smaller backbones degrade\nwith longer reverse trajectories. We further show a side-by-side comparison of\nFM to typical adversarial-loss-based training for the same model architecture."}
{"id": "2510.16834", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16834", "abs": "https://arxiv.org/abs/2510.16834", "authors": ["Jing Yang", "Sirui Wang", "Chao Wu", "Fan Fan"], "title": "Schrödinger Bridge Mamba for One-Step Speech Enhancement", "comment": "5 pages, 1 figure", "summary": "We propose Schr\\\"odinger Bridge Mamba (SBM), a new concept of\ntraining-inference framework motivated by the inherent compatibility between\nSchr\\\"odinger Bridge (SB) training paradigm and selective state-space model\nMamba. We exemplify the concept of SBM with an implementation for generative\nspeech enhancement. Experiments on a joint denoising and dereverberation task\nusing four benchmark datasets demonstrate that SBM, with only 1-step inference,\noutperforms strong baselines with 1-step or iterative inference and achieves\nthe best real-time factor (RTF). Beyond speech enhancement, we discuss the\nintegration of SB paradigm and selective state-space model architecture based\non their underlying alignment, which indicates a promising direction for\nexploring new deep generative models potentially applicable to a broad range of\ngenerative tasks. Demo page: https://sbmse.github.io"}
{"id": "2510.16963", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.16963", "abs": "https://arxiv.org/abs/2510.16963", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ismail Guvenc"], "title": "Stochastic Geometry Analysis of Asymmetric Uplink Interference for Urban UAV-RC Networks", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) have emerged as a flexible platform for\nproviding coverage over challenging environments, particularly for public\nsafety and surveillance missions in urban areas. However, deploying the UAVs in\ndense urban areas introduces unique challenges, most notably asymmetric uplink\n(UL, remote controller to UAV) interference due to a higher chance of\nline-of-sight (LoS) interference at the UAV. In this letter, we propose a\nstochastic geometry framework to tractably analyze the large-scale asymmetric\ninterference in urban areas. We incorporate a log-Gaussian Cox process (LGCP)\nmodel to capture the spatial correlation of the interference field in both UL\nand downlink (DL) as a function of the UAV altitude and the two-dimensional\n(2-D) distance between the remote controller and UAV. To quantify the UL and\nthe DL interference asymmetry, we also define the interference asymmetry ratio\ncharacterizing the interference disparity between the UL and the DL. Our\nnumerical results demonstrate that the interference asymmetry ratio increases\nas the UAV altitude and 2-D distance increase, highlighting that the UL\ninterference worsens."}
{"id": "2510.17788", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.17788", "abs": "https://arxiv.org/abs/2510.17788", "authors": ["Kyung Yun Lee", "Nils Meyer-Kahlen", "Karolina Prawda", "Vesa Välimäki", "Sebastian J. Schlecht"], "title": "AnyRIR: Robust Non-intrusive Room Impulse Response Estimation in the Wild", "comment": null, "summary": "We address the problem of estimating room impulse responses (RIRs) in noisy,\nuncontrolled environments where non-stationary sounds such as speech or\nfootsteps corrupt conventional deconvolution. We propose AnyRIR, a\nnon-intrusive method that uses music as the excitation signal instead of a\ndedicated test signal, and formulate RIR estimation as an L1-norm regression in\nthe time-frequency domain. Solved efficiently with Iterative Reweighted Least\nSquares (IRLS) and Least-Squares Minimal Residual (LSMR) methods, this approach\nexploits the sparsity of non-stationary noise to suppress its influence.\nExperiments on simulated and measured data show that AnyRIR outperforms\nL2-based and frequency-domain deconvolution, under in-the-wild noisy scenarios\nand codec mismatch, enabling robust RIR estimation for AR/VR and related\napplications."}
{"id": "2510.16893", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16893", "abs": "https://arxiv.org/abs/2510.16893", "authors": ["Bo-Han Feng", "Chien-Feng Liu", "Yu-Hsuan Li Liang", "Chih-Kai Yang", "Szu-Wei Fu", "Zhehuai Chen", "Ke-Han Lu", "Sung-Feng Huang", "Chao-Han Huck Yang", "Yu-Chiang Frank Wang", "Yun-Nung Chen", "Hung-yi Lee"], "title": "Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations", "comment": "Submitted to ICASSP 2026", "summary": "Large audio-language models (LALMs) extend text-based LLMs with auditory\nunderstanding, offering new opportunities for multimodal applications. While\ntheir perception, reasoning, and task performance have been widely studied,\ntheir safety alignment under paralinguistic variation remains underexplored.\nThis work systematically investigates the role of speaker emotion. We construct\na dataset of malicious speech instructions expressed across multiple emotions\nand intensities, and evaluate several state-of-the-art LALMs. Our results\nreveal substantial safety inconsistencies: different emotions elicit varying\nlevels of unsafe responses, and the effect of intensity is non-monotonic, with\nmedium expressions often posing the greatest risk. These findings highlight an\noverlooked vulnerability in LALMs and call for alignment strategies explicitly\ndesigned to ensure robustness under emotional variation, a prerequisite for\ntrustworthy deployment in real-world settings."}
{"id": "2510.17113", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17113", "abs": "https://arxiv.org/abs/2510.17113", "authors": ["Mengzhen Liu", "Ming Li", "Rang Liu", "Qian Liu", "A. Lee Swindlehurst"], "title": "Reconfigurable Antenna Arrays: Bridging Electromagnetics and Signal Processing", "comment": "7 pages, 5 figures, 1 table", "summary": "Reconfigurable antennas (RAs), capable of dynamically adapting their\nradiation patterns, polarization states, and operating frequencies, have\nemerged as a promising technology to meet the stringent performance\nrequirements of sixth-generation (6G) wireless networks. This article\nsystematically introduces essential hardware implementations of RAs and\ninvestigates advanced array architectures, such as fully-digital and tri-hybrid\ndesigns, emphasizing their capability to synergistically integrate\nelectromagnetic (EM) reconfigurability with analog and digital signal\nprocessing. By facilitating coordinated beamforming across the EM and signal\nprocessing domains, RA arrays offer unprecedented flexibility and adaptability\ncompared to conventional static antenna systems. Representative applications\nempowered by RA arrays, including integrated sensing and communication (ISAC),\nphysical layer security (PLS), and near-field communications, are highlighted.\nA case study illustrates the effectiveness of RA arrays in optimizing beam\nsteering, improving link robustness, and alleviating system power consumption.\nFinally, several open challenges and future research directions are outlined,\nemphasizing the need for advancements in theoretical modeling, hardware\nreliability, channel estimation techniques, intelligent optimization methods,\nand innovative network architectures, to fully realize the transformative\nimpact of RAs in future 6G wireless networks."}
{"id": "2510.16273", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16273", "abs": "https://arxiv.org/abs/2510.16273", "authors": ["Jingyue Huang", "Zachary Novack", "Phillip Long", "Yupeng Hou", "Ke Chen", "Taylor Berg-Kirkpatrick", "Julian McAuley"], "title": "MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding", "comment": null, "summary": "Discrete representation learning has shown promising results across various\ndomains, including generation and understanding in image, speech and language.\nInspired by these advances, we propose MuseTok, a tokenization method for\nsymbolic music, and investigate its effectiveness in both music generation and\nunderstanding tasks. MuseTok employs the residual vector quantized-variational\nautoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based\nencoder-decoder framework, producing music codes that achieve high-fidelity\nmusic reconstruction and accurate understanding of music theory. For\ncomprehensive evaluation, we apply MuseTok to music generation and semantic\nunderstanding tasks, including melody extraction, chord recognition, and\nemotion recognition. Models incorporating MuseTok outperform previous\nrepresentation learning baselines in semantic understanding while maintaining\ncomparable performance in content generation. Furthermore, qualitative analyses\non MuseTok codes, using ground-truth categories and synthetic datasets, reveal\nthat MuseTok effectively captures underlying musical concepts from large music\ncollections."}
{"id": "2510.16917", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16917", "abs": "https://arxiv.org/abs/2510.16917", "authors": ["Chih-Kai Yang", "Yen-Ting Piao", "Tzu-Wen Hsu", "Szu-Wei Fu", "Zhehuai Chen", "Ke-Han Lu", "Sung-Feng Huang", "Chao-Han Huck Yang", "Yu-Chiang Frank Wang", "Yun-Nung Chen", "Hung-yi Lee"], "title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models", "comment": "Work in progress", "summary": "Knowledge editing offers an efficient way to update model knowledge without\nfull retraining, but prior work has concentrated almost exclusively on textual\nor visual modalities. We introduce SAKE, the first benchmark specifically\ndesigned for editing auditory attribute knowledge in Large Audio-Language\nModels (LALMs). Unlike factual updates, SAKE targets several abstract auditory\nattributes, capturing knowledge types that go beyond conventional textual and\nvisual domains. We benchmark seven editing methods on two LALMs along four\ndimensions: reliability, generality, audio/text locality, and portability.\nResults highlight challenges such as preserving intra-attribute knowledge\nunrelated to the edit, generalizing edits to multimodal reasoning, and\nmaintaining edits under sequential updates. SAKE provides a principled\nframework to study how knowledge editing extends to the auditory modalities,\nopening new directions for maintaining and adapting LALMs in more diverse\nreal-world scenarios."}
{"id": "2510.17272", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17272", "abs": "https://arxiv.org/abs/2510.17272", "authors": ["Muhammad Asif", "Asim Ihsan", "Zhu Shoujin", "Ali Ranjha", "Xingwang Li", "Khaled M. Rabie", "Symeon Chatzinotas"], "title": "Robust Beamforming Optimization for STAR-RIS Empowered Multi-User RSMA Under Hardware Imperfections and Channel Uncertainty", "comment": "12 pages, and 11 figures. Submitted to IEEE", "summary": "This study explores the synergy between rate-splitting multiple access (RSMA)\nand simultaneous transmitting and reflecting reconfigurable intelligent surface\n(STAR-RIS) as a unified framework to enable ubiquitous, intelligent, and\nresilient connectivity in future sixth-generation networks, while improving\nspectral and energy efficiency. Specifically, we investigate a\nSTAR-RIS-assisted multi-user RSMA system and develop an intelligent\noptimization strategy that jointly designs the transmitter's active\nbeamforming, the common stream rate allocation, and the passive beamforming\nvectors for the STAR-RIS transmission and reflection regions, considering\ntransceiver hardware impairments and imperfect channel state information (CSI).\nIn addition, system robustness is ensured via a bounded channel estimation\nerror model that captures CSI imperfections and guarantees resilience against\nworst-case errors. To address the highly non-convex problem, we propose an\niterative optimization algorithm that decomposes it into two sub-problems.\nFirstly, active beamforming vectors for the common and private signals are\ndetermined by reformulating the original problem into a convex semi-definite\nprogramming (SDP) form using successive convex approximation (SCA) and\nsemi-definite relaxation (SDR). Secondly, passive beamforming vectors are\noptimized through a convex SDP reformulation by exploiting SCA and SDR\ntechniques. Moreover, when higher-rank solutions arise, Gaussian randomization\nis applied to obtain rank-one solutions. Numerical simulations demonstrate that\nthe proposed strategy achieves significant performance gains over benchmark\nschemes and exhibits fast convergence."}
{"id": "2510.16355", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16355", "abs": "https://arxiv.org/abs/2510.16355", "authors": ["Haocheng Yu", "Krishan K. Ahuja", "Lakshmi N. Sankar", "Spencer H. Bryngelson"], "title": "Transmission of High-Amplitude Sound through Leakages of Ill-fitting Earplugs", "comment": null, "summary": "High sound pressure levels (SPL) pose notable risks in loud environments,\nparticularly due to noise-induced hearing loss. Ill-fitting earplugs often lead\nto sound leakage, a phenomenon this study seeks to investigate. To validate our\nmethodology, we first obtained computational and experimental acoustic\ntransmission data for stand-alone slit resonators and orifices, for which\nextensive published data are readily available for comparison. We then examined\nthe frequency-dependent acoustic power absorption coefficient and transmission\nloss (TL) across various leakage geometries, modeled using different orifice\ndiameters. Experimental approaches spanned a frequency range of 1--5 kHz under\nSPL conditions of 120--150 dB. Key findings reveal that unsealed silicone\nrubber earplugs demonstrate an average TL reduction of approximately 18 dB at\nan overall incident SPL (OISPL) of 120 dB. Direct numerical simulations further\nhighlight SPL-dependent acoustic dissipation mechanisms, showing the conversion\nof acoustic energy into vorticity in ill-fitting earplug models at an OISPL of\n150 dB. These results highlight the role of earplug design for\nhigh-sound-pressure-level environments."}
{"id": "2510.17345", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17345", "abs": "https://arxiv.org/abs/2510.17345", "authors": ["Peihong Zhang", "Yuxuan Liu", "Rui Sang", "Zhixin Li", "Yiqiang Cai", "Yizhou Tan", "Shengchen Li"], "title": "DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift", "comment": "Paper has submitted to ICASSP2026", "summary": "Acoustic scene classification (ASC) suffers from device-induced domain shift,\nespecially when labels are limited. Prior work focuses on curriculum-based\ntraining schedules that structure data presentation by ordering or reweighting\ntraining examples from easy-to-hard to facilitate learning; however, existing\ncurricula are static, fixing the ordering or the weights before training and\nignoring that example difficulty and marginal utility evolve with the learned\nrepresentation. To overcome this limitation, we propose the Dynamic Dual-Signal\nCurriculum (DDSC), a training schedule that adapts the curriculum online by\ncombining two signals computed each epoch: a domain-invariance signal and a\nlearning-progress signal. A time-varying scheduler fuses these signals into\nper-example weights that prioritize domain-invariant examples in early epochs\nand progressively emphasize device-specific cases. DDSC is lightweight,\narchitecture-agnostic, and introduces no additional inference overhead. Under\nthe official DCASE 2024 Task~1 protocol, DDSC consistently improves\ncross-device performance across diverse ASC baselines and label budgets, with\nthe largest gains on unseen-device splits."}
{"id": "2510.17324", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17324", "abs": "https://arxiv.org/abs/2510.17324", "authors": ["Idir Edjekouane", "Alejandro González Garrido", "Jorge Querol", "Symeon Chatzinotas"], "title": "When 5G NTN Meets GNSS: Tracking GNSS Signals under Overlaid 5G Waveforms", "comment": "Submitted to IEEE ICC 2026", "summary": "Global Navigation Satellite Systems (GNSS) provide the backbone of\nPositioning, Navigation, and Timing (PNT) but remain vulnerable to\ninterference. Low Earth Orbit (LEO) constellations within Fifth-Generation (5G)\nNon-Terrestrial Networks (NTN) can enhance resilience by jointly supporting\ncommunication and navigation. This paper presents the first quantitative\nanalysis of GNSS tracking and navigation message demodulation under a hybrid\nwaveform where a low-power Direct-Sequence Spread Spectrum (DSSS) component is\noverlaid on an Orthogonal Frequency-Division Multiplexing (OFDM) 5G downlink.\nWe evaluate a minimally modified GNSS receiver that tracks a legacy Global\nPositioning System (GPS) L1 Coarse/Acquisition (C/A) overlay aligned with 5G\nframes while treating the 5G waveform as structured interference. Using Monte\nCarlo simulations under realistic LEO Doppler dynamics, we analyze the Bit\nError Rate (BER) of GPS L1 C/A navigation bits and the subframe decoding\nprobability versus Signalto- Interference-plus-Noise Ratio (SINR) for multiple\nSignalto- Interference Ratios (SIR) and dynamic classes. Results show reliable\ndemodulation across wide SINR ranges for low and medium dynamics, whereas high\ndynamics impose strict lock limits. These findings confirm the feasibility of\nJoint Communication and Positioning (JCAP) using a near-legacy GNSS chipset\nwith minimal receiver modifications."}
{"id": "2510.16489", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16489", "abs": "https://arxiv.org/abs/2510.16489", "authors": ["Mark Huckvale"], "title": "Interpreting the Dimensions of Speaker Embedding Space", "comment": null, "summary": "Speaker embeddings are widely used in speaker verification systems and other\napplications where it is useful to characterise the voice of a speaker with a\nfixed-length vector. These embeddings tend to be treated as \"black box\"\nencodings, and how they relate to conventional acoustic and phonetic dimensions\nof voices has not been widely studied. In this paper we investigate how\nstate-of-the-art speaker embedding systems represent the acoustic\ncharacteristics of speakers as described by conventional acoustic descriptors,\nage, and gender. Using a large corpus of 10,000 speakers and three embedding\nsystems we show that a small set of 9 acoustic parameters chosen to be\n\"interpretable\" predict embeddings about the same as 7 principal components,\ncorresponding to over 50% of variance in the data. We show that some principal\ndimensions operate differently for male and female speakers, suggesting there\nis implicit gender recognition within the embedding systems. However we show\nthat speaker age is not well captured by embeddings, suggesting opportunities\nexist for improvements in their calculation."}
{"id": "2510.17346", "categories": ["cs.SD", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17346", "abs": "https://arxiv.org/abs/2510.17346", "authors": ["Peihong Zhang", "Zhixin Li", "Yuxuan Liu", "Rui Sang", "Yiqiang Cai", "Yizhou Tan", "Shengchen Li"], "title": "TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation", "comment": "Paper has submitted to ICASSP2026", "summary": "Deep learning approaches for heart-sound (PCG) segmentation built on\ntime--frequency features can be accurate but often rely on large expert-labeled\ndatasets, limiting robustness and deployment. We present TopSeg, a topological\nrepresentation-centric framework that encodes PCG dynamics with multi-scale\ntopological features and decodes them using a lightweight temporal\nconvolutional network (TCN) with an order- and duration-constrained inference\nstep. To evaluate data efficiency and generalization, we train exclusively on\nPhysioNet 2016 dataset with subject-level subsampling and perform external\nvalidation on CirCor dataset. Under matched-capacity decoders, the topological\nfeatures consistently outperform spectrogram and envelope inputs, with the\nlargest margins at low data budgets; as a full system, TopSeg surpasses\nrepresentative end-to-end baselines trained on their native inputs under the\nsame budgets while remaining competitive at full data. Ablations at 10%\ntraining confirm that all scales contribute and that combining H_0 and H_1\nyields more reliable S1/S2 localization and boundary stability. These results\nindicate that topology-aware representations provide a strong inductive bias\nfor data-efficient, cross-dataset PCG segmentation, supporting practical use\nwhen labeled data are limited."}
{"id": "2510.17361", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17361", "abs": "https://arxiv.org/abs/2510.17361", "authors": ["Shiming Liu", "Jianhua Xie", "Yan Wang"], "title": "Efficiency-Enhanced Open Earbud Earphone Antenna Using Dual-Feed Technique", "comment": "5 pages, 10 figures, submitted to IEEE Open Journal of Antennas and\n  Propagation", "summary": "The stringent spatial constraints and the demand for high antenna efficiency\nin modern wireless earphones present significant design challenges. To address\nthese issues, this paper presents and thoroughly investigates a novel earphone\nantenna design specifically tailored for open earbud wireless earphones. In\ncontrast to traditional earphone antennas that rely on a conventional\nsingle-feed configuration, the proposed design introduces a dual-feed\nexcitation technique incorporating a controlled phase difference between the\ntwo feeds. This innovative feeding strategy effectively enlarges the equivalent\nradiating aperture, thereby enhancing the overall radiation efficiency of the\nantenna system. Experimental and simulation results demonstrate that the\ndual-feed approach yields an efficiency improvement exceeding 1 dB when\ncompared with standard single-feed designs. Furthermore, the fabricated\nprototype achieves a -6 dB impedance bandwidth that fully encompasses the 2.4\nGHz ISM band, ensuring stable wireless communication performance. The measured\ntotal efficiencies reach -8.5 dB in free space and -9.5 dB under on-head\nconditions. These results confirm that the proposed antenna successfully\nachieves high efficiency and reliable performance within the extremely limited\nvolume of an earbud device, demonstrating strong potential for integration into\nnext-generation compact wireless earphones."}
{"id": "2510.16834", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16834", "abs": "https://arxiv.org/abs/2510.16834", "authors": ["Jing Yang", "Sirui Wang", "Chao Wu", "Fan Fan"], "title": "Schrödinger Bridge Mamba for One-Step Speech Enhancement", "comment": "5 pages, 1 figure", "summary": "We propose Schr\\\"odinger Bridge Mamba (SBM), a new concept of\ntraining-inference framework motivated by the inherent compatibility between\nSchr\\\"odinger Bridge (SB) training paradigm and selective state-space model\nMamba. We exemplify the concept of SBM with an implementation for generative\nspeech enhancement. Experiments on a joint denoising and dereverberation task\nusing four benchmark datasets demonstrate that SBM, with only 1-step inference,\noutperforms strong baselines with 1-step or iterative inference and achieves\nthe best real-time factor (RTF). Beyond speech enhancement, we discuss the\nintegration of SB paradigm and selective state-space model architecture based\non their underlying alignment, which indicates a promising direction for\nexploring new deep generative models potentially applicable to a broad range of\ngenerative tasks. Demo page: https://sbmse.github.io"}
{"id": "2510.17474", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.17474", "abs": "https://arxiv.org/abs/2510.17474", "authors": ["Davide Salvi", "Hendrik Vincent Koops", "Elio Quinton"], "title": "Not All Deepfakes Are Created Equal: Triaging Audio Forgeries for Robust Deepfake Singer Identification", "comment": "Accepted for presentation at the NeurIPS 2025 Workshop on Generative\n  and Protective AI for Content Creation (non-archival)", "summary": "The proliferation of highly realistic singing voice deepfakes presents a\nsignificant challenge to protecting artist likeness and content authenticity.\nAutomatic singer identification in vocal deepfakes is a promising avenue for\nartists and rights holders to defend against unauthorized use of their voice,\nbut remains an open research problem. Based on the premise that the most\nharmful deepfakes are those of the highest quality, we introduce a two-stage\npipeline to identify a singer's vocal likeness. It first employs a\ndiscriminator model to filter out low-quality forgeries that fail to accurately\nreproduce vocal likeness. A subsequent model, trained exclusively on authentic\nrecordings, identifies the singer in the remaining high-quality deepfakes and\nauthentic audio. Experiments show that this system consistently outperforms\nexisting baselines on both authentic and synthetic content."}
{"id": "2510.17462", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17462", "abs": "https://arxiv.org/abs/2510.17462", "authors": ["Sefa Kayraklik", "Ali Fuat Sahin", "Onur Salan", "Recep A. Tasci", "Recep Vural", "Yusuf Islam Tek", "Ertugrul Basar", "Ibrahim Hokelek", "Ali Gorcin", "Karim Boutiba", "Adlen Ksentini"], "title": "ORIX: Orchestration of RIS with xApps for Smart Wireless Factory Environments", "comment": "Submitted in IEEE", "summary": "The vision of a smart wireless factory (SWF) demands highly flexible,\nlow-latency, and reliable connectivity that goes beyond conventional wireless\nsolutions. Reconfigurable intelligent surface (RIS)-empowered communications,\nwhen integrated with the open radio access network (O-RAN) architectures, have\nemerged as a promising enabler to meet these challenging requirements. This\narticle introduces the methodology for the orchestration of RIS with xApps\n(ORIX), bringing the RIS technology into the O-RAN ecosystem through xApp-based\ncontrol for SWF environments. ORIX features three key components: an\nO-RAN-compliant RIS service model for dynamic configuration, an RIS channel\nsimulator that supports 3GPP indoor factory models with multiple industrial\nscenarios, and practical RIS optimization strategies with finite-resolution\ncontrol. Together, these elements provide a realistic end-to-end emulation\nplatform for evaluating RIS placement, control, and performance in SWF\nenvironments prior to deployment. The presented case study demonstrates how\nORIX enables the evaluation of achievable performance gains, exploration of\ntrade-offs among key RIS design parameters, and identification of deployment\nstrategies that balance system performance with practical implementation\nconstraints. By bridging theoretical advances with industrial feasibility, ORIX\nlays the groundwork for RIS-assisted O-RAN networks to power next-generation\nwireless communication in industrial scenarios."}
{"id": "2510.16893", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16893", "abs": "https://arxiv.org/abs/2510.16893", "authors": ["Bo-Han Feng", "Chien-Feng Liu", "Yu-Hsuan Li Liang", "Chih-Kai Yang", "Szu-Wei Fu", "Zhehuai Chen", "Ke-Han Lu", "Sung-Feng Huang", "Chao-Han Huck Yang", "Yu-Chiang Frank Wang", "Yun-Nung Chen", "Hung-yi Lee"], "title": "Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations", "comment": "Submitted to ICASSP 2026", "summary": "Large audio-language models (LALMs) extend text-based LLMs with auditory\nunderstanding, offering new opportunities for multimodal applications. While\ntheir perception, reasoning, and task performance have been widely studied,\ntheir safety alignment under paralinguistic variation remains underexplored.\nThis work systematically investigates the role of speaker emotion. We construct\na dataset of malicious speech instructions expressed across multiple emotions\nand intensities, and evaluate several state-of-the-art LALMs. Our results\nreveal substantial safety inconsistencies: different emotions elicit varying\nlevels of unsafe responses, and the effect of intensity is non-monotonic, with\nmedium expressions often posing the greatest risk. These findings highlight an\noverlooked vulnerability in LALMs and call for alignment strategies explicitly\ndesigned to ensure robustness under emotional variation, a prerequisite for\ntrustworthy deployment in real-world settings."}
{"id": "2510.17512", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.17512", "abs": "https://arxiv.org/abs/2510.17512", "authors": ["Kosta Pavlović", "Lazar Stanarević", "Petar Nedić", "Slavko Kovačević", "Igor Djurović"], "title": "AWARE: Audio Watermarking with Adversarial Resistance to Edits", "comment": null, "summary": "Prevailing practice in learning-based audio watermarking is to pursue\nrobustness by expanding the set of simulated distortions during training.\nHowever, such surrogates are narrow and prone to overfitting. This paper\npresents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an\nalternative approach that avoids reliance on attack-simulation stacks and\nhandcrafted differentiable distortions. Embedding is obtained via adversarial\noptimization in the time-frequency domain under a level-proportional perceptual\nbudget. Detection employs a time-order-agnostic detector with a Bitwise Readout\nHead (BRH) that aggregates temporal evidence into one score per watermark bit,\nenabling reliable watermark decoding even under desynchronization and temporal\ncuts. Empirically, AWARE attains high audio quality and speech intelligibility\n(PESQ/STOI) and consistently low BER across various audio edits, often\nsurpassing representative state-of-the-art learning-based audio watermarking\nsystems."}
{"id": "2510.17502", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17502", "abs": "https://arxiv.org/abs/2510.17502", "authors": ["Li-Hsiang Shen"], "title": "6D Movable Metasurface (6DMM) in Downlink NOMA Transmissions", "comment": null, "summary": "This letter proposes a novel six-dimensional movable metasurface\n(6DMM)-assisted downlink non-orthogonal multiple access (NOMA) system, in which\na conventional base station (BS) equipped with fixed antennas serves multiple\nusers with the assistance of a reconfigurable intelligent surface (RIS) with\nsix-dimensional spatial configurability. In contrast to traditional RIS with\nstatic surface, the proposed 6DMM architecture allows each element to\ndynamically adjust its position and orient the whole metasurface in\nyaw-pitch-roll axes, enabling both in spatial and electromagnetic controls. We\nformulate a sum-rate maximization problem that jointly optimizes the BS\nNOMA-based beamforming, phase-shifts, element positions, and rotation angles of\nmetasurface under constraints of NOMA power levels, unit-modulus of\nphase-shifts, power budget, inter-element separation and boundaries of element\nposition/orientation. Due to non-convexity and high-dimensionality, we employ a\nprobabilistic cross-entropy optimization (CEO) scheme to iteratively refine the\nsolution distribution based on maximizing likelihood and elite solution\nsampling. Simulation results show that the proposed CEO-based 6DMM-NOMA\narchitecture achieves substantial rate performance gains compared to 6DMM\nsub-structures, conventional static RIS, and other multiple access mechanisms.\nIt also highlights the effectiveness of CEO providing probabilistic\noptimization for solving high-dimensional scalable metasurface."}
{"id": "2510.16917", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16917", "abs": "https://arxiv.org/abs/2510.16917", "authors": ["Chih-Kai Yang", "Yen-Ting Piao", "Tzu-Wen Hsu", "Szu-Wei Fu", "Zhehuai Chen", "Ke-Han Lu", "Sung-Feng Huang", "Chao-Han Huck Yang", "Yu-Chiang Frank Wang", "Yun-Nung Chen", "Hung-yi Lee"], "title": "SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models", "comment": "Work in progress", "summary": "Knowledge editing offers an efficient way to update model knowledge without\nfull retraining, but prior work has concentrated almost exclusively on textual\nor visual modalities. We introduce SAKE, the first benchmark specifically\ndesigned for editing auditory attribute knowledge in Large Audio-Language\nModels (LALMs). Unlike factual updates, SAKE targets several abstract auditory\nattributes, capturing knowledge types that go beyond conventional textual and\nvisual domains. We benchmark seven editing methods on two LALMs along four\ndimensions: reliability, generality, audio/text locality, and portability.\nResults highlight challenges such as preserving intra-attribute knowledge\nunrelated to the edit, generalizing edits to multimodal reasoning, and\nmaintaining edits under sequential updates. SAKE provides a principled\nframework to study how knowledge editing extends to the auditory modalities,\nopening new directions for maintaining and adapting LALMs in more diverse\nreal-world scenarios."}
{"id": "2510.17633", "categories": ["cs.SD", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.17633", "abs": "https://arxiv.org/abs/2510.17633", "authors": ["Weilin Lin", "Jianze Li", "Hui Xiong", "Li Liu"], "title": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering", "comment": null, "summary": "Large Audio-Language Models (LALMs) are becoming essential as a powerful\nmultimodal backbone for real-world applications. However, recent studies show\nthat audio inputs can more easily elicit harmful responses than text, exposing\nnew risks toward deployment. While safety alignment has made initial advances\nin LLMs and Large Vision-Language Models (LVLMs), we find that vanilla\nadaptation of these approaches to LALMs faces two key limitations: 1) LLM-based\nsteering fails under audio input due to the large distributional gap between\nactivations, and 2) prompt-based defenses induce over-refusals on benign-speech\nqueries. To address these challenges, we propose Safe-Ablated Refusal Steering\n(SARSteer), the first inference-time defense framework for LALMs. Specifically,\nSARSteer leverages text-derived refusal steering to enforce rejection without\nmanipulating audio inputs and introduces decomposed safe-space ablation to\nmitigate over-refusal. Extensive experiments demonstrate that SARSteer\nsignificantly improves harmful-query refusal while preserving benign responses,\nestablishing a principled step toward safety alignment in LALMs."}
{"id": "2510.17695", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17695", "abs": "https://arxiv.org/abs/2510.17695", "authors": ["Maximilian H. V. Tillmann", "Ban-Sok Shin", "Dmitriy Shutin", "Armin Dekorsy"], "title": "Semantic Joint Source Channel Coding for Distributed Subsurface Imaging in Multi-Agent Systems", "comment": null, "summary": "Multi-agent systems (MAS) are a promising solution for autonomous exploration\ntasks in hazardous or remote environments, such as planetary surveys. In such\nsettings, communication among agents is essential to ensure collaborative task\nexecution, yet conventional approaches treat exploration and communication as\ndecoupled subsystems. This work presents a novel framework that tightly\nintegrates semantic communication into the MAS exploration process, adapting\ncommunication strategies to the exploration methodology to improve overall task\nperformance. Specifically, we investigate the application of semantic joint\nsource-channel coding (JSCC) with over-the-air computation (AirComp) for\ndistributed function computation for the application of cooperative subsurface\nimaging using the adapt-then-combine full waveform inversion (ATC-FWI)\nalgorithm. Our results demonstrate that semantic JSCC significantly outperforms\nclassical point-to-point and standard JSCC methods, especially in\nhigh-connectivity networks. Furthermore, incorporating side information at the\nreceiving agent enhances communication efficiency and imaging accuracy, a\nfeature previously unexplored in MAS-based exploration. We validate our\napproach through a use case inspired by subsurface anomaly detection, showing\nmeasurable improvements in imaging performance per agent. This work underscores\nthe potential of semantic communication in distributed multi-agent exploration,\noffering a communication-aware exploration paradigm that achieves task-relevant\nperformance gains."}
{"id": "2510.17662", "categories": ["cs.SD", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17662", "abs": "https://arxiv.org/abs/2510.17662", "authors": ["Massa Baali", "Rita Singh", "Bhiksha Raj"], "title": "DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model", "comment": null, "summary": "Self-supervised speech models have achieved remarkable success on\ncontent-driven tasks, yet they remain limited in capturing\nspeaker-discriminative features critical for verification, diarization, and\nprofiling applications. We introduce DELULU, a speaker-aware self-supervised\nfoundational model that addresses this limitation by integrating external\nsupervision into the pseudo-label generation process. DELULU leverages\nframe-level embeddings from ReDimNet, a state-of-the-art speaker verification\nmodel, to guide the k-means clustering step during pre-training, introducing a\nstrong speaker-discriminative inductive bias that aligns representation\nlearning with speaker identity. The model is trained using a dual objective\nthat combines masked prediction and denoising, further enhancing robustness and\ngeneralization. DELULU significantly outperforms prior self-supervised learning\n(SSL) models across a range of speaker-centric tasks, achieving up to 62%\nrelative improvement in equal error rate (EER) for speaker verification and\nconsistent gains on zero-shot profiling tasks such as gender, age, accent, and\nspeaker counting. Our findings demonstrate that DELULU is a strong universal\nencoder for speaker-aware speech processing, enabling superior performance even\nwithout task-specific fine-tuning."}
{"id": "2510.17738", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17738", "abs": "https://arxiv.org/abs/2510.17738", "authors": ["Fabian Jaensch", "Giuseppe Caire", "Begüm Demir"], "title": "Beam Index Map Prediction in Unseen Environments from Geospatial Data", "comment": "5 pages", "summary": "In 5G, beam training consists of the efficient association of users to beams\nfor a given beamforming codebook used at the base station and the given\npropagation environment in the cell. We propose a convolutional neural network\napproach that leverages the position of the base station and geospatial data to\npredict beam distributions for all user locations simultaneously. Our method\ngeneralizes to unseen environments without site-specific training or\nspecialized sensors. The results show that it significantly reduces the number\nof candidate beams considered, thereby improving the efficiency of beam\ntraining."}
{"id": "2510.17741", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.17741", "abs": "https://arxiv.org/abs/2510.17741", "authors": ["Navid Reyhanian", "Reza Ghaderi Zefreh", "Parisa Ramezani", "Emil Bjornson"], "title": "Precoding for Uplink RIS-Assisted Cell-Free MIMO-OFDM Systems with Hardware Impairments", "comment": null, "summary": "This paper studies a reconfigurable intelligent surface (RIS)-assisted\ncell-free massive multiple-input multiple-output (CF-mMIMO) system with\nmultiple RISs. Joint design of transmit precoding, RIS coefficients, and\nreceive combining is investigated for uplink sum-rate maximization under\nin-phase and quadrature phase imbalance (IQI) at user equipments (UEs) and\naccess points (APs). A weighted minimum mean squared error (WMMSE) based block\ncoordinate descent (BCD) approach is proposed, where novel iterative methods\nare developed to efficiently solve the BCD subproblems. The efficiency of\nproposed approaches is demonstrated relative to heuristic methods via extensive\nsimulations."}
{"id": "2510.17775", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17775", "abs": "https://arxiv.org/abs/2510.17775", "authors": ["Kweku Abraham", "Amnon Balanov", "Tamir Bendory", "Carlos Esteve-Yagüe"], "title": "Sample Complexity Analysis of Multi-Target Detection via Markovian and Hard-Core Multi-Reference Alignment", "comment": null, "summary": "Motivated by single-particle cryo-electron microscopy, we study the sample\ncomplexity of the multi-target detection (MTD) problem, in which an unknown\nsignal appears multiple times at unknown locations within a long, noisy\nobservation. We propose a patching scheme that reduces MTD to a non-i.i.d.\nmulti-reference alignment (MRA) model. In the one-dimensional setting, the\nlatent group elements form a Markov chain, and we show that the convergence\nrate of any estimator matches that of the corresponding i.i.d. MRA model, up to\na logarithmic factor in the number of patches. Moreover, for estimators based\non empirical averaging, such as the method of moments, the convergence rates\nare identical in both settings. We further establish an analogous result in two\ndimensions, where the latent structure arises from an exponentially mixing\nrandom field generated by a hard-core placement model. As a consequence, if the\nsignal in the corresponding i.i.d. MRA model is determined by moments up to\norder $n_{\\min}$, then in the low-SNR regime the number of patches required to\nestimate the signal in the MTD model scales as $\\sigma^{2n_{\\min}}$, where\n$\\sigma^2$ denotes the noise variance."}

{"id": "2510.18036", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18036", "abs": "https://arxiv.org/abs/2510.18036", "authors": ["Stavros Mitsis", "Ermos Hadjikyriakos", "Humaid Ibrahim", "Savvas Neofytou", "Shashwat Raman", "James Myles", "Eiman Kanjo"], "title": "Transformer Redesign for Late Fusion of Audio-Text Features on Ultra-Low-Power Edge Hardware", "comment": null, "summary": "Deploying emotion recognition systems in real-world environments where\ndevices must be small, low-power, and private remains a significant challenge.\nThis is especially relevant for applications such as tension monitoring,\nconflict de-escalation, and responsive wearables, where cloud-based solutions\nare impractical. Multimodal emotion recognition has advanced through deep\nlearning, but most systems remain unsuitable for deployment on\nultra-constrained edge devices. Prior work typically relies on powerful\nhardware, lacks real-time performance, or uses unimodal input. This paper\naddresses that gap by presenting a hardware-aware emotion recognition system\nthat combines acoustic and linguistic features using a late-fusion architecture\noptimised for Edge TPU. The design integrates a quantised transformer-based\nacoustic model with frozen keyword embeddings from a DSResNet-SE network,\nenabling real-time inference within a 1.8MB memory budget and 21-23ms latency.\nThe pipeline ensures spectrogram alignment between training and deployment\nusing MicroFrontend and MLTK. Evaluation on re-recorded, segmented IEMOCAP\nsamples captured through the Coral Dev Board Micro microphone shows a 6.3%\nmacro F1 improvement over unimodal baselines. This work demonstrates that\naccurate, real-time multimodal emotion inference is achievable on\nmicrocontroller-class edge platforms through task-specific fusion and\nhardware-guided model design."}
{"id": "2510.18308", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18308", "abs": "https://arxiv.org/abs/2510.18308", "authors": ["Haowei Lou", "Hye-Young Paik", "Wen Hu", "Lina Yao"], "title": "ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation", "comment": null, "summary": "Controlling speaking style in text-to-speech (TTS) systems has become a\ngrowing focus in both academia and industry. While many existing approaches\nrely on reference audio to guide style generation, such methods are often\nimpractical due to privacy concerns and limited accessibility. More recently,\nlarge language models (LLMs) have been used to control speaking style through\nnatural language prompts; however, their high computational cost, lack of\ninterpretability, and sensitivity to prompt phrasing limit their applicability\nin real-time and resource-constrained environments. In this work, we propose\nParaStyleTTS, a lightweight and interpretable TTS framework that enables\nexpressive style control from text prompts alone. ParaStyleTTS features a novel\ntwo-level style adaptation architecture that separates prosodic and\nparalinguistic speech style modeling. It allows fine-grained and robust control\nover factors such as emotion, gender, and age. Unlike LLM-based methods,\nParaStyleTTS maintains consistent style realization across varied prompt\nformulations and is well-suited for real-world applications, including\non-device and low-resource deployment. Experimental results show that\nParaStyleTTS generates high-quality speech with performance comparable to\nstate-of-the-art LLM-based systems while being 30x faster, using 8x fewer\nparameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS\nexhibits superior robustness and controllability over paralinguistic speaking\nstyles, providing a practical and efficient solution for style-controllable\ntext-to-speech generation. Demo can be found at\nhttps://parastyletts.github.io/ParaStyleTTS_Demo/. Code can be found at\nhttps://github.com/haoweilou/ParaStyleTTS."}
{"id": "2510.18416", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18416", "abs": "https://arxiv.org/abs/2510.18416", "authors": ["Pengfei Cai", "Joanna Wang", "Haorui Zheng", "Xu Li", "Zihao Ji", "Teng Ma", "Zhongliang Liu", "Chen Zhang", "Pengfei Wan"], "title": "SegTune: Structured and Fine-Grained Control for Song Generation", "comment": null, "summary": "Recent advancements in song generation have shown promising results in\ngenerating songs from lyrics and/or global text prompts. However, most existing\nsystems lack the ability to model the temporally varying attributes of songs,\nlimiting fine-grained control over musical structure and dynamics. In this\npaper, we propose SegTune, a non-autoregressive framework for structured and\ncontrollable song generation. SegTune enables segment-level control by allowing\nusers or large language models to specify local musical descriptions aligned to\nsong sections.The segmental prompts are injected into the model by temporally\nbroadcasting them to corresponding time windows, while global prompts influence\nthe whole song to ensure stylistic coherence. To obtain accurate segment\ndurations and enable precise lyric-to-music alignment, we introduce an\nLLM-based duration predictor that autoregressively generates sentence-level\ntimestamped lyrics in LRC format. We further construct a large-scale data\npipeline for collecting high-quality songs with aligned lyrics and prompts, and\npropose new evaluation metrics to assess segment-level alignment and vocal\nattribute consistency. Experimental results show that SegTune achieves superior\ncontrollability and musical coherence compared to existing baselines. See\nhttps://cai525.github.io/SegTune_demo for demos of our work."}
{"id": "2510.18530", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18530", "abs": "https://arxiv.org/abs/2510.18530", "authors": ["Bin Gu", "Lipeng Dai", "Huipeng Du", "Haitao Zhao", "Jibo Wei"], "title": "A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification", "comment": null, "summary": "Learning robust speaker representations under noisy conditions presents\nsignificant challenges, which requires careful handling of both discriminative\nand noise-invariant properties. In this work, we proposed an anchor-based\nstage-wise learning strategy for robust speaker representation learning.\nSpecifically, our approach begins by training a base model to establish\ndiscriminative speaker boundaries, and then extract anchor embeddings from this\nmodel as stable references. Finally, a copy of the base model is fine-tuned on\nnoisy inputs, regularized by enforcing proximity to their corresponding fixed\nanchor embeddings to preserve speaker identity under distortion. Experimental\nresults suggest that this strategy offers advantages over conventional joint\noptimization, particularly in maintaining discrimination while improving noise\nrobustness. The proposed method demonstrates consistent improvements across\nvarious noise conditions, potentially due to its ability to handle boundary\nstabilization and variation suppression separately."}
{"id": "2510.18169", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18169", "abs": "https://arxiv.org/abs/2510.18169", "authors": ["Yu-Wen Chen", "William Ho", "Sasha M. Vergez", "Grace Flaherty", "Pallavi Gupta", "Zhihong Zhang", "Maryam Zolnoori", "Margaret V. McDonald", "Maxim Topaz", "Zoran Kostic", "Julia Hirschberg"], "title": "Hearing Health in Home Healthcare: Leveraging LLMs for Illness Scoring and ALMs for Vocal Biomarker Extraction", "comment": "The Second Workshop on GenAI for Health at NeurIPS 2025", "summary": "The growing demand for home healthcare calls for tools that can support care\ndelivery. In this study, we explore automatic health assessment from voice\nusing real-world home care visit data, leveraging the diverse patient\ninformation it contains. First, we utilize Large Language Models (LLMs) to\nintegrate Subjective, Objective, Assessment, and Plan (SOAP) notes derived from\nunstructured audio transcripts and structured vital signs into a holistic\nillness score that reflects a patient's overall health. This compact\nrepresentation facilitates cross-visit health status comparisons and downstream\nanalysis. Next, we design a multi-stage preprocessing pipeline to extract short\nspeech segments from target speakers in home care recordings for acoustic\nanalysis. We then employ an Audio Language Model (ALM) to produce\nplain-language descriptions of vocal biomarkers and examine their association\nwith individuals' health status. Our experimental results benchmark both\ncommercial and open-source LLMs in estimating illness scores, demonstrating\ntheir alignment with actual clinical outcomes, and revealing that SOAP notes\nare substantially more informative than vital signs. Building on the illness\nscores, we provide the first evidence that ALMs can identify health-related\nacoustic patterns from home care recordings and present them in a\nhuman-readable form. Together, these findings highlight the potential of LLMs\nand ALMs to harness heterogeneous in-home visit data for better patient\nmonitoring and care."}
{"id": "2510.17808", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17808", "abs": "https://arxiv.org/abs/2510.17808", "authors": ["Amirhesam Aghanouri", "Mohamed Sabry", "Joshua Cherian Varughese", "Cristina Olaverri-Monreal"], "title": "Machine Learning-Based Performance Evaluation of a Solar-Powered Hydrogen Fuel Cell Hybrid in a Radio-Controlled Electric Vehicle", "comment": null, "summary": "This paper presents an experimental investigation and performance evaluation\nof a hybrid electric radio-controlled car powered by a Nickel-Metal Hydride\nbattery combined with a renewable Proton Exchange Membrane Fuel Cell system.\nThe study evaluates the performance of the system under various load-carrying\nscenarios and varying environmental conditions, simulating real-world operating\nconditions including throttle operation. In order to build a predictive model,\ngather operational insights, and detect anomalies, data-driven analyses using\nsignal processing and modern machine learning techniques were employed.\nSpecifically, machine learning techniques were used to distinguish throttle\nlevels with high precision based on the operational data. Anomaly and change\npoint detection methods enhanced voltage stability, resulting in fewer critical\nfaults in the hybrid system compared to battery-only operation. Temporal\nConvolutional Networks were effectively employed to predict voltage behavior,\ndemonstrating potential for use in planning the locations of fueling or\ncharging stations. Moreover, integration with a solar-powered electrolyzer\nconfirmed the system's potential for off-grid, renewable hydrogen use. The\nresults indicate that integrating a Proton Exchange Membrane Fuel Cell with\nNickel-Metal Hydride batteries significantly improves electrical performance\nand reliability for small electric vehicles, and these findings can be a\npotential baseline for scaling up to larger vehicles."}
{"id": "2510.18533", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18533", "abs": "https://arxiv.org/abs/2510.18533", "authors": ["Bin Gu", "Lipeng Dai", "Huipeng Du", "Haitao Zhao", "Jibo Wei"], "title": "Noise-Conditioned Mixture-of-Experts Framework for Robust Speaker Verification", "comment": null, "summary": "Robust speaker verification under noisy conditions remains an open challenge.\nConventional deep learning methods learn a robust unified speaker\nrepresentation space against diverse background noise and achieve significant\nimprovement. In contrast, this paper presents a noise-conditioned\nmixture-ofexperts framework that decomposes the feature space into specialized\nnoise-aware subspaces for speaker verification. Specifically, we propose a\nnoise-conditioned expert routing mechanism, a universal model based expert\nspecialization strategy, and an SNR-decaying curriculum learning protocol,\ncollectively improving model robustness and generalization under diverse noise\nconditions. The proposed method can automatically route inputs to expert\nnetworks based on noise information derived from the inputs, where each expert\ntargets distinct noise characteristics while preserving speaker identity\ninformation. Comprehensive experiments demonstrate consistent superiority over\nbaselines, confirming that explicit noise-dependent feature modeling\nsignificantly enhances robustness without sacrificing verification accuracy."}
{"id": "2510.18190", "categories": ["eess.AS", "cs.LG", "cs.SD", "H.5.5; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.18190", "abs": "https://arxiv.org/abs/2510.18190", "authors": ["Zhanhong He", "Hanyu Meng", "David Huang", "Roberto Togneri"], "title": "Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network", "comment": "Paper submitted to ICASSP2026", "summary": "Estimating piano dynamic from audio recordings is a fundamental challenge in\ncomputational music analysis. In this paper, we propose an efficient multi-task\nnetwork that jointly predicts dynamic levels, change points, beats, and\ndownbeats from a shared latent representation. These four targets form the\nmetrical structure of dynamics in the music score. Inspired by recent vocal\ndynamic research, we use a multi-scale network as the backbone, which takes\nBark-scale specific loudness as the input feature. Compared to log-Mel as\ninput, this reduces model size from 14.7 M to 0.5 M, enabling long sequential\ninput. We use a 60-second audio length in audio segmentation, which doubled the\nlength of beat tracking commonly used. Evaluated on the public MazurkaBL\ndataset, our model achieves state-of-the-art results across all tasks. This\nwork sets a new benchmark for piano dynamic estimation and delivers a powerful\nand compact tool, paving the way for large-scale, resource-efficient analysis\nof musical expression."}
{"id": "2510.17809", "categories": ["eess.SP", "cs.LG", "I.2"], "pdf": "https://arxiv.org/pdf/2510.17809", "abs": "https://arxiv.org/abs/2510.17809", "authors": ["Massimo Capurso", "Luciano Afferrante"], "title": "In-Process Monitoring of Gear Power Honing Using Vibration Signal Analysis and Machine Learning", "comment": "20 pages, 17 figures, 3 tables, 33 references", "summary": "In modern gear manufacturing, stringent Noise, Vibration, and Harshness (NVH)\nrequirements demand high-precision finishing operations such as power honing.\nConventional quality control strategies rely on post-process inspections and\nStatistical Process Control (SPC), which fail to capture transient machining\nanomalies and cannot ensure real-time defect detection. This study proposes a\nnovel, data-driven framework for in-process monitoring of gear power honing\nusing vibration signal analysis and machine learning. Our proposed methodology\ninvolves continuous data acquisition via accelerometers, followed by\ntime-frequency signal analysis. We investigate and compare the efficacy of\nthree subspace learning methods for features extraction: (1) Principal\nComponent Analysis (PCA) for dimensionality reduction; (2) a two-stage\nframework combining PCA with Linear Discriminant Analysis (LDA) for enhanced\nclass separation; and (3) Uncorrelated Multilinear Discriminant Analysis with\nRegularization (R-UMLDA), adapted for tensor data, which enforces feature\ndecorrelation and includes regularization for small sample sizes. These\nextracted features are then fed into a Support Vector Machine (SVM) classifier\nto predict four distinct gear quality categories, established through rigorous\ngeometrical inspections and test bench results of assembled gearboxes. The\nmodels are trained and validated on an experimental dataset collected in an\nindustrial context during gear power-honing operations, with gears classified\ninto four different quality categories. The proposed framework achieves high\nclassification accuracy (up to 100%) in an industrial setting. The approach\noffers interpretable spectral features that correlate with process dynamics,\nenabling practical integration into real-time monitoring and predictive\nmaintenance systems."}
{"id": "2510.16841", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.16841", "abs": "https://arxiv.org/abs/2510.16841", "authors": ["Wenxi Chen", "Xinsheng Wang", "Ruiqi Yan", "Yushen Chen", "Zhikang Niu", "Ziyang Ma", "Xiquan Li", "Yuzhe Liang", "Hanlin Wen", "Shunshun Yin", "Ming Tao", "Xie Chen"], "title": "SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization", "comment": null, "summary": "Speech codecs that convert continuous speech signals into discrete tokens\nhave become essential for speech language models (SLMs). However, existing\ncodecs struggle to balance high-quality reconstruction with semantically rich\nrepresentations, limiting their effectiveness in both generative and\nunderstanding tasks. In this work, we propose SAC, a neural speech codec with\nsemantic-acoustic dual-stream quantization. By disentangling semantic and\nacoustic modeling into two dedicated streams, SAC enables each to be optimized\nfor its respective role. Comprehensive evaluations show that SAC achieves\nstrong reconstruction performance across diverse bitrates under both clean and\nnoisy conditions, with particularly high scores on UTMOS and WER, demonstrating\nsuperior perceptual quality and intelligibility. Moreover, SAC substantially\noutperforms state-of-the-art codecs in semantic representation, achieving a\nlevel comparable to that of self-supervised learning (SSL) continuous\nembeddings. Finally, our analysis of speech disentanglement highlights the\neffectiveness of the dual-stream design, offering new potential for\ncontrollable speech applications."}
{"id": "2510.18206", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18206", "abs": "https://arxiv.org/abs/2510.18206", "authors": ["Hanyu Meng", "Vidhyasaharan Sethu", "Eliathamby Ambikairajah", "Qiquan Zhang", "Haizhou Li"], "title": "Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing", "comment": "Submitted to ICASSP2026", "summary": "In audio signal processing, learnable front-ends have shown strong\nperformance across diverse tasks by optimizing task-specific representation.\nHowever, their parameters remain fixed once trained, lacking flexibility during\ninference and limiting robustness under dynamic complex acoustic environments.\nIn this paper, we introduce a novel adaptive paradigm for audio front-ends that\nreplaces static parameterization with a closed-loop neural controller.\nSpecifically, we simplify the learnable front-end LEAF architecture and\nintegrate a neural controller for adaptive representation via dynamically\ntuning Per-Channel Energy Normalization. The neural controller leverages both\nthe current and the buffered past subband energies to enable input-dependent\nadaptation during inference. Experimental results on multiple audio\nclassification tasks demonstrate that the proposed adaptive front-end\nconsistently outperforms prior fixed and learnable front-ends under both clean\nand complex acoustic conditions. These results highlight neural adaptability as\na promising direction for the next generation of audio front-ends."}
{"id": "2510.17810", "categories": ["eess.SP", "cs.LG", "nlin.CD", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.17810", "abs": "https://arxiv.org/abs/2510.17810", "authors": ["Camilo Quiceno Quintero", "Sandip Varkey George"], "title": "Exploring Complexity Changes in Diseased ECG Signals for Enhanced Classification", "comment": "Version submitted to NODYCON 2025", "summary": "The complex dynamics of the heart are reflected in its electrical activity,\ncaptured through electrocardiograms (ECGs). In this study we use nonlinear time\nseries analysis to understand how ECG complexity varies with cardiac pathology.\nUsing the large PTB-XL dataset, we extracted nonlinear measures from lead II\nECGs, and cross-channel metrics (leads II, V2, AVL) using Spearman correlations\nand mutual information. Significant differences between diseased and healthy\nindividuals were found in almost all measures between healthy and diseased\nclasses, and between 5 diagnostic superclasses ($p<.001$). Moreover,\nincorporating these complexity quantifiers into machine learning models\nsubstantially improved classification accuracy measured using area under the\nROC curve (AUC) from 0.86 (baseline) to 0.87 (nonlinear measures) and 0.90\n(including cross-time series metrics)."}
{"id": "2510.18169", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18169", "abs": "https://arxiv.org/abs/2510.18169", "authors": ["Yu-Wen Chen", "William Ho", "Sasha M. Vergez", "Grace Flaherty", "Pallavi Gupta", "Zhihong Zhang", "Maryam Zolnoori", "Margaret V. McDonald", "Maxim Topaz", "Zoran Kostic", "Julia Hirschberg"], "title": "Hearing Health in Home Healthcare: Leveraging LLMs for Illness Scoring and ALMs for Vocal Biomarker Extraction", "comment": "The Second Workshop on GenAI for Health at NeurIPS 2025", "summary": "The growing demand for home healthcare calls for tools that can support care\ndelivery. In this study, we explore automatic health assessment from voice\nusing real-world home care visit data, leveraging the diverse patient\ninformation it contains. First, we utilize Large Language Models (LLMs) to\nintegrate Subjective, Objective, Assessment, and Plan (SOAP) notes derived from\nunstructured audio transcripts and structured vital signs into a holistic\nillness score that reflects a patient's overall health. This compact\nrepresentation facilitates cross-visit health status comparisons and downstream\nanalysis. Next, we design a multi-stage preprocessing pipeline to extract short\nspeech segments from target speakers in home care recordings for acoustic\nanalysis. We then employ an Audio Language Model (ALM) to produce\nplain-language descriptions of vocal biomarkers and examine their association\nwith individuals' health status. Our experimental results benchmark both\ncommercial and open-source LLMs in estimating illness scores, demonstrating\ntheir alignment with actual clinical outcomes, and revealing that SOAP notes\nare substantially more informative than vital signs. Building on the illness\nscores, we provide the first evidence that ALMs can identify health-related\nacoustic patterns from home care recordings and present them in a\nhuman-readable form. Together, these findings highlight the potential of LLMs\nand ALMs to harness heterogeneous in-home visit data for better patient\nmonitoring and care."}
{"id": "2510.18391", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18391", "abs": "https://arxiv.org/abs/2510.18391", "authors": ["Giovanni Bologni", "Martin Bo Møller", "Richard Heusdens", "Richard C. Hendriks"], "title": "MVDR Beamforming for Cyclostationary Processes", "comment": "Under review for publication from September 2025", "summary": "Conventional acoustic beamformers assume that noise is stationary within\nshort time frames. This assumption prevents them from exploiting correlations\nbetween frequencies in almost-periodic noise sources such as musical\ninstruments, fans, and engines. These signals exhibit periodically varying\nstatistics and are better modeled as cyclostationary processes. This paper\nintroduces the cyclic MVDR (cMVDR) beamformer, an extension of the conventional\nMVDR that leverages both spatial and spectral correlations to improve noise\nreduction, particularly in low-SNR scenarios. The method builds on\nfrequency-shifted (FRESH) filtering, where shifted versions of the input are\ncombined to attenuate or amplify components that are coherent across frequency.\nTo address inharmonicity, where harmonic partials deviate from exact integer\nmultiples of the fundamental frequency, we propose a data-driven strategy that\nestimates resonant frequencies via periodogram analysis and computes the\nfrequency shifts from their spacing. Analytical and experimental results\ndemonstrate that performance improves with increasing spectral correlation. On\nreal recordings, the cMVDR achieves up to 5 dB gain in scale-invariant\nsignal-to-distortion ratio (SI-SDR) over the MVDR and remains effective even\nwith a single microphone. Code is available at\nhttps://github.com/Screeen/cMVDR."}
{"id": "2510.17811", "categories": ["eess.SP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.17811", "abs": "https://arxiv.org/abs/2510.17811", "authors": ["Zhixing Wang", "Renzhi Yuan", "Haifeng Yao", "Chuang Yang", "Mugen Peng"], "title": "Channel Modeling of Satellite-to-Underwater Laser Communication Links: An Analytical-Monte Carlo Hybrid Approach", "comment": null, "summary": "Channel modeling for satellite-to-underwater laser communication (StULC)\nlinks remains challenging due to long distances and the diversity of the\nchannel constituents. The StULC channel is typically segmented into three\nisolated channels: the atmospheric channel, the air-water interface channel,\nand the underwater channel. Previous studies involving StULC channel modeling\neither focused on separated channels or neglected the combined effects of\nparticles and turbulence on laser propagation. In this paper, we established a\ncomprehensive StULC channel model by an analytical-Monte Carlo hybrid approach,\ntaking into account the effects of both particles and turbulence. We first\nobtained the intensity distribution of the transmitted laser beam after passing\nthrough the turbulent atmosphere based on the extended Huygens-Fresnel\nprinciple. Then we derived a closed-form probability density function of the\nphoton propagating direction after passing through the air-water interface,\nwhich greatly simplified the modeling of StULC links. At last, we employed a\nMonte Carlo method to model the underwater links and obtained the power\ndistribution at the receiving plane. Based on the proposed StULC channel model,\nwe analyzed the bit error rate and the outage probability under different\nenvironmental conditions. Numerical results demonstrated that, the influence of\nunderwater particle concentration on the communication performance is much\npronounced than those of both the atmospheric turbulence and the underwater\nturbulence. Notably, increasing the wind speed at the air-water interface does\nnot significantly worsen the communication performance of the StULC links."}
{"id": "2510.18190", "categories": ["eess.AS", "cs.LG", "cs.SD", "H.5.5; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.18190", "abs": "https://arxiv.org/abs/2510.18190", "authors": ["Zhanhong He", "Hanyu Meng", "David Huang", "Roberto Togneri"], "title": "Joint Estimation of Piano Dynamics and Metrical Structure with a Multi-task Multi-Scale Network", "comment": "Paper submitted to ICASSP2026", "summary": "Estimating piano dynamic from audio recordings is a fundamental challenge in\ncomputational music analysis. In this paper, we propose an efficient multi-task\nnetwork that jointly predicts dynamic levels, change points, beats, and\ndownbeats from a shared latent representation. These four targets form the\nmetrical structure of dynamics in the music score. Inspired by recent vocal\ndynamic research, we use a multi-scale network as the backbone, which takes\nBark-scale specific loudness as the input feature. Compared to log-Mel as\ninput, this reduces model size from 14.7 M to 0.5 M, enabling long sequential\ninput. We use a 60-second audio length in audio segmentation, which doubled the\nlength of beat tracking commonly used. Evaluated on the public MazurkaBL\ndataset, our model achieves state-of-the-art results across all tasks. This\nwork sets a new benchmark for piano dynamic estimation and delivers a powerful\nand compact tool, paving the way for large-scale, resource-efficient analysis\nof musical expression."}
{"id": "2510.18423", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18423", "abs": "https://arxiv.org/abs/2510.18423", "authors": ["Toranosuke Manabe", "Yuchi Ishikawa", "Hokuto Munakata", "Tatsuya Komatsu"], "title": "ProLAP: Probabilistic Language-Audio Pre-Training", "comment": "Under review", "summary": "Language-audio joint representation learning frameworks typically depend on\ndeterministic embeddings, assuming a one-to-one correspondence between audio\nand text. In real-world settings, however, the language-audio relationship is\ninherently many-to-many: one audio segment can be described by multiple\ncaptions and vice versa. To address this, we propose Probabilistic\nLanguage-Audio Pre-training (ProLAP), which models multiplicity as the spread\nof probability distributions in a joint language-audio embedding space. To\ntrain the intra-modal hierarchical relationship effectively, we also introduce\ntwo objectives: (i) hierarchical inclusion loss to promote semantic\nhierarchical understanding of inputs and (ii) mask repulsive loss to improve\nthe efficiency of learning when optimizing the hierarchical inclusion loss.\nWith this training strategy, our model can learn the hierarchical structure\ninherent in the data even from small datasets, in contrast to prior\nprobabilistic approaches that rely on large-scale datasets. In our experiments,\nProLAP outperforms existing deterministic approaches on audio-text retrieval\ntasks. Moreover, through experiments on the audio traversal task introduced in\nthis paper, we demonstrate that ProLAP captures the plausible semantic\nhierarchy."}
{"id": "2510.17816", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17816", "abs": "https://arxiv.org/abs/2510.17816", "authors": ["Xin Li", "Jingzhi Hu", "Yinghui He", "Hongbo Wang", "Jin Gan", "Jun Luo"], "title": "Cross-Domain Multi-Person Human Activity Recognition via Near-Field Wi-Fi Sensing", "comment": null, "summary": "Wi-Fi-based human activity recognition (HAR) provides substantial convenience\nand has emerged as a thriving research field, yet the coarse spatial resolution\ninherent to Wi-Fi significantly hinders its ability to distinguish multiple\nsubjects. By exploiting the near-field domination effect, establishing a\ndedicated sensing link for each subject through their personal Wi-Fi device\noffers a promising solution for multi-person HAR under native traffic. However,\ndue to the subject-specific characteristics and irregular patterns of\nnear-field signals, HAR neural network models require fine-tuning (FT) for\ncross-domain adaptation, which becomes particularly challenging with certain\ncategories unavailable. In this paper, we propose WiAnchor, a novel training\nframework for efficient cross-domain adaptation in the presence of incomplete\nactivity categories. This framework processes Wi-Fi signals embedded with\nirregular time information in three steps: during pre-training, we enlarge\ninter-class feature margins to enhance the separability of activities; in the\nFT stage, we innovate an anchor matching mechanism for cross-domain adaptation,\nfiltering subject-specific interference informed by incomplete activity\ncategories, rather than attempting to extract complete features from them;\nfinally, the recognition of input samples is further improved based on their\nfeature-level similarity with anchors. We construct a comprehensive dataset to\nthoroughly evaluate WiAnchor, achieving over 90% cross-domain accuracy with\nabsent activity categories."}
{"id": "2510.18206", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18206", "abs": "https://arxiv.org/abs/2510.18206", "authors": ["Hanyu Meng", "Vidhyasaharan Sethu", "Eliathamby Ambikairajah", "Qiquan Zhang", "Haizhou Li"], "title": "Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing", "comment": "Submitted to ICASSP2026", "summary": "In audio signal processing, learnable front-ends have shown strong\nperformance across diverse tasks by optimizing task-specific representation.\nHowever, their parameters remain fixed once trained, lacking flexibility during\ninference and limiting robustness under dynamic complex acoustic environments.\nIn this paper, we introduce a novel adaptive paradigm for audio front-ends that\nreplaces static parameterization with a closed-loop neural controller.\nSpecifically, we simplify the learnable front-end LEAF architecture and\nintegrate a neural controller for adaptive representation via dynamically\ntuning Per-Channel Energy Normalization. The neural controller leverages both\nthe current and the buffered past subband energies to enable input-dependent\nadaptation during inference. Experimental results on multiple audio\nclassification tasks demonstrate that the proposed adaptive front-end\nconsistently outperforms prior fixed and learnable front-ends under both clean\nand complex acoustic conditions. These results highlight neural adaptability as\na promising direction for the next generation of audio front-ends."}
{"id": "2510.18744", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18744", "abs": "https://arxiv.org/abs/2510.18744", "authors": ["Bunlong Lay", "Rostislav Makarov", "Simon Welker", "Maris Hillemann", "Timo Gerkmann"], "title": "Diffusion Buffer for Online Generative Speech Enhancement", "comment": null, "summary": "Online Speech Enhancement was mainly reserved for predictive models. A key\nadvantage of these models is that for an incoming signal frame from a stream of\ndata, the model is called only once for enhancement. In contrast, generative\nSpeech Enhancement models often require multiple calls, resulting in a\ncomputational complexity that is too high for many online speech enhancement\napplications. This work presents the Diffusion Buffer, a generative\ndiffusion-based Speech Enhancement model which only requires one neural network\ncall per incoming signal frame from a stream of data and performs enhancement\nin an online fashion on a consumer-grade GPU. The key idea of the Diffusion\nBuffer is to align physical time with Diffusion time-steps. The approach\nprogressively denoises frames through physical time, where past frames have\nmore noise removed. Consequently, an enhanced frame is output to the listener\nwith a delay defined by the Diffusion Buffer, and the output frame has a\ncorresponding look-ahead. In this work, we extend upon our previous work by\ncarefully designing a 2D convolutional UNet architecture that specifically\naligns with the Diffusion Buffer's look-ahead. We observe that the proposed\nUNet improves performance, particularly when the algorithmic latency is low.\nMoreover, we show that using a Data Prediction loss instead of Denoising Score\nMatching loss enables flexible control over the trade-off between algorithmic\nlatency and quality during inference. The extended Diffusion Buffer equipped\nwith a novel NN and loss function drastically reduces the algorithmic latency\nfrom 320 - 960 ms to 32 - 176 ms with an even increased performance. While it\nhas been shown before that offline generative diffusion models outperform\npredictive approaches in unseen noisy speech data, we confirm that the online\nDiffusion Buffer also outperforms its predictive counterpart on unseen noisy\nspeech data."}
{"id": "2510.17818", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17818", "abs": "https://arxiv.org/abs/2510.17818", "authors": ["Salar Nouri"], "title": "Single-Snapshot Gridless 2D-DoA Estimation for UCAs: A Joint Optimization Approach", "comment": null, "summary": "This paper tackles the challenging problem of gridless two-dimensional (2D)\ndirection-of-arrival (DOA) estimation for a uniform circular array (UCA) from a\nsingle snapshot of data. Conventional gridless methods often fail in this\nscenario due to prohibitive computational costs or a lack of robustness. We\npropose a novel framework that overcomes these limitations by jointly\nestimating a manifold transformation matrix and the source azimuth-elevation\npairs within a single, unified optimization problem. This problem is solved\nefficiently using an inexact Augmented Lagrangian Method (iALM), which\ncompletely circumvents the need for semidefinite programming. By unifying the\nobjectives of data fidelity and transformation robustness, our approach is\nuniquely suited for the demanding single-snapshot case. Simulation results\nconfirm that the proposed iALM framework provides robust and high-resolution,\ngridless 2D-DOA estimates, establishing its efficacy for challenging array\nsignal processing applications."}
{"id": "2510.18391", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18391", "abs": "https://arxiv.org/abs/2510.18391", "authors": ["Giovanni Bologni", "Martin Bo Møller", "Richard Heusdens", "Richard C. Hendriks"], "title": "MVDR Beamforming for Cyclostationary Processes", "comment": "Under review for publication from September 2025", "summary": "Conventional acoustic beamformers assume that noise is stationary within\nshort time frames. This assumption prevents them from exploiting correlations\nbetween frequencies in almost-periodic noise sources such as musical\ninstruments, fans, and engines. These signals exhibit periodically varying\nstatistics and are better modeled as cyclostationary processes. This paper\nintroduces the cyclic MVDR (cMVDR) beamformer, an extension of the conventional\nMVDR that leverages both spatial and spectral correlations to improve noise\nreduction, particularly in low-SNR scenarios. The method builds on\nfrequency-shifted (FRESH) filtering, where shifted versions of the input are\ncombined to attenuate or amplify components that are coherent across frequency.\nTo address inharmonicity, where harmonic partials deviate from exact integer\nmultiples of the fundamental frequency, we propose a data-driven strategy that\nestimates resonant frequencies via periodogram analysis and computes the\nfrequency shifts from their spacing. Analytical and experimental results\ndemonstrate that performance improves with increasing spectral correlation. On\nreal recordings, the cMVDR achieves up to 5 dB gain in scale-invariant\nsignal-to-distortion ratio (SI-SDR) over the MVDR and remains effective even\nwith a single microphone. Code is available at\nhttps://github.com/Screeen/cMVDR."}
{"id": "2510.18036", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18036", "abs": "https://arxiv.org/abs/2510.18036", "authors": ["Stavros Mitsis", "Ermos Hadjikyriakos", "Humaid Ibrahim", "Savvas Neofytou", "Shashwat Raman", "James Myles", "Eiman Kanjo"], "title": "Transformer Redesign for Late Fusion of Audio-Text Features on Ultra-Low-Power Edge Hardware", "comment": null, "summary": "Deploying emotion recognition systems in real-world environments where\ndevices must be small, low-power, and private remains a significant challenge.\nThis is especially relevant for applications such as tension monitoring,\nconflict de-escalation, and responsive wearables, where cloud-based solutions\nare impractical. Multimodal emotion recognition has advanced through deep\nlearning, but most systems remain unsuitable for deployment on\nultra-constrained edge devices. Prior work typically relies on powerful\nhardware, lacks real-time performance, or uses unimodal input. This paper\naddresses that gap by presenting a hardware-aware emotion recognition system\nthat combines acoustic and linguistic features using a late-fusion architecture\noptimised for Edge TPU. The design integrates a quantised transformer-based\nacoustic model with frozen keyword embeddings from a DSResNet-SE network,\nenabling real-time inference within a 1.8MB memory budget and 21-23ms latency.\nThe pipeline ensures spectrogram alignment between training and deployment\nusing MicroFrontend and MLTK. Evaluation on re-recorded, segmented IEMOCAP\nsamples captured through the Coral Dev Board Micro microphone shows a 6.3%\nmacro F1 improvement over unimodal baselines. This work demonstrates that\naccurate, real-time multimodal emotion inference is achievable on\nmicrocontroller-class edge platforms through task-specific fusion and\nhardware-guided model design."}
{"id": "2510.17821", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17821", "abs": "https://arxiv.org/abs/2510.17821", "authors": ["Long Lin", "Pablo Peiro-Corbacho", "Pablo Ávila", "Alejandro Carta-Bergaz", "Ángel Arenal", "Gonzalo R. Ríos-Muñoz", "Carlos Sevilla-Salcedo"], "title": "CLARAE: Clarity Preserving Reconstruction AutoEncoder for Denoising and Rhythm Classification of Intracardiac Electrograms", "comment": null, "summary": "Intracavitary atrial electrograms (EGMs) provide high-resolution insights\ninto cardiac electrophysiology but are often contaminated by noise and remain\nhigh-dimensional, limiting real-time analysis. We introduce CLARAE\n(CLArity-preserving Reconstruction AutoEncoder), a one-dimensional\nencoder--decoder designed for atrial EGMs, which achieves both high-fidelity\nreconstruction and a compact 64-dimensional latent representation. CLARAE is\ndesigned to preserve waveform morphology, mitigate reconstruction artifacts,\nand produce interpretable embeddings through three principles: downsampling\nwith pooling, a hybrid interpolation--convolution upsampling path, and a\nbounded latent space.\n  We evaluated CLARAE on 495,731 EGM segments (unipolar and bipolar) from 29\npatients across three rhythm types (AF, SR300, SR600). Performance was\nbenchmarked against six state-of-the-art autoencoders using reconstruction\nmetrics, rhythm classification, and robustness across signal-to-noise ratios\nfrom -5 to 15 dB. In downstream rhythm classification, CLARAE achieved\nF1-scores above 0.97 for all rhythm types, and its latent space showed clear\nclustering by rhythm. In denoising tasks, it consistently ranked among the top\nperformers for both unipolar and bipolar signals.\n  In order to promote reproducibility and enhance accessibility, we offer an\ninteractive web-based application. This platform enables users to explore\npre-trained CLARAE models, visualize the reconstructions, and compute metrics\nin real time. Overall, CLARAE combines robust denoising with compact,\ndiscriminative representations, offering a practical foundation for clinical\nworkflows such as rhythm discrimination, signal quality assessment, and\nreal-time mapping."}
{"id": "2510.18423", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18423", "abs": "https://arxiv.org/abs/2510.18423", "authors": ["Toranosuke Manabe", "Yuchi Ishikawa", "Hokuto Munakata", "Tatsuya Komatsu"], "title": "ProLAP: Probabilistic Language-Audio Pre-Training", "comment": "Under review", "summary": "Language-audio joint representation learning frameworks typically depend on\ndeterministic embeddings, assuming a one-to-one correspondence between audio\nand text. In real-world settings, however, the language-audio relationship is\ninherently many-to-many: one audio segment can be described by multiple\ncaptions and vice versa. To address this, we propose Probabilistic\nLanguage-Audio Pre-training (ProLAP), which models multiplicity as the spread\nof probability distributions in a joint language-audio embedding space. To\ntrain the intra-modal hierarchical relationship effectively, we also introduce\ntwo objectives: (i) hierarchical inclusion loss to promote semantic\nhierarchical understanding of inputs and (ii) mask repulsive loss to improve\nthe efficiency of learning when optimizing the hierarchical inclusion loss.\nWith this training strategy, our model can learn the hierarchical structure\ninherent in the data even from small datasets, in contrast to prior\nprobabilistic approaches that rely on large-scale datasets. In our experiments,\nProLAP outperforms existing deterministic approaches on audio-text retrieval\ntasks. Moreover, through experiments on the audio traversal task introduced in\nthis paper, we demonstrate that ProLAP captures the plausible semantic\nhierarchy."}
{"id": "2510.18308", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18308", "abs": "https://arxiv.org/abs/2510.18308", "authors": ["Haowei Lou", "Hye-Young Paik", "Wen Hu", "Lina Yao"], "title": "ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation", "comment": null, "summary": "Controlling speaking style in text-to-speech (TTS) systems has become a\ngrowing focus in both academia and industry. While many existing approaches\nrely on reference audio to guide style generation, such methods are often\nimpractical due to privacy concerns and limited accessibility. More recently,\nlarge language models (LLMs) have been used to control speaking style through\nnatural language prompts; however, their high computational cost, lack of\ninterpretability, and sensitivity to prompt phrasing limit their applicability\nin real-time and resource-constrained environments. In this work, we propose\nParaStyleTTS, a lightweight and interpretable TTS framework that enables\nexpressive style control from text prompts alone. ParaStyleTTS features a novel\ntwo-level style adaptation architecture that separates prosodic and\nparalinguistic speech style modeling. It allows fine-grained and robust control\nover factors such as emotion, gender, and age. Unlike LLM-based methods,\nParaStyleTTS maintains consistent style realization across varied prompt\nformulations and is well-suited for real-world applications, including\non-device and low-resource deployment. Experimental results show that\nParaStyleTTS generates high-quality speech with performance comparable to\nstate-of-the-art LLM-based systems while being 30x faster, using 8x fewer\nparameters, and requiring 2.5x less CUDA memory. Moreover, ParaStyleTTS\nexhibits superior robustness and controllability over paralinguistic speaking\nstyles, providing a practical and efficient solution for style-controllable\ntext-to-speech generation. Demo can be found at\nhttps://parastyletts.github.io/ParaStyleTTS_Demo/. Code can be found at\nhttps://github.com/haoweilou/ParaStyleTTS."}
{"id": "2510.17823", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.17823", "abs": "https://arxiv.org/abs/2510.17823", "authors": ["Saeed Mohammadzadeh", "Rodrigo C. de Lamare", "Yuriy Zakharov"], "title": "Covariance Matrix Construction with Preprocessing-Based Spatial Sampling for Robust Adaptive Beamforming", "comment": "13 figures, 14 pages", "summary": "This work proposes an efficient, robust adaptive beamforming technique to\ndeal with steering vector (SV) estimation mismatches and data covariance matrix\nreconstruction problems. In particular, the direction-of-arrival(DoA) of\ninterfering sources is estimated with available snapshots in which the angular\nsectors of the interfering signals are computed adaptively. Then, we utilize\nthe well-known general linear combination algorithm to reconstruct the\ninterference-plus-noise covariance (IPNC) matrix using preprocessing-based\nspatial sampling (PPBSS). We demonstrate that the preprocessing matrix can be\nreplaced by the sample covariance matrix (SCM) in the shrinkage method. A power\nspectrum sampling strategy is then devised based on a preprocessing matrix\ncomputed with the estimated angular sectors' information. Moreover, the\ncovariance matrix for the signal is formed for the angular sector of the\nsignal-of-interest (SOI), which allows for calculating an SV for the SOI using\nthe power method. An analysis of the array beampattern in the proposed PPBSS\ntechnique is carried out, and a study of the computational cost of competing\napproaches is conducted. Simulation results show the proposed method's\neffectiveness compared to existing approaches."}
{"id": "2510.18744", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.18744", "abs": "https://arxiv.org/abs/2510.18744", "authors": ["Bunlong Lay", "Rostislav Makarov", "Simon Welker", "Maris Hillemann", "Timo Gerkmann"], "title": "Diffusion Buffer for Online Generative Speech Enhancement", "comment": null, "summary": "Online Speech Enhancement was mainly reserved for predictive models. A key\nadvantage of these models is that for an incoming signal frame from a stream of\ndata, the model is called only once for enhancement. In contrast, generative\nSpeech Enhancement models often require multiple calls, resulting in a\ncomputational complexity that is too high for many online speech enhancement\napplications. This work presents the Diffusion Buffer, a generative\ndiffusion-based Speech Enhancement model which only requires one neural network\ncall per incoming signal frame from a stream of data and performs enhancement\nin an online fashion on a consumer-grade GPU. The key idea of the Diffusion\nBuffer is to align physical time with Diffusion time-steps. The approach\nprogressively denoises frames through physical time, where past frames have\nmore noise removed. Consequently, an enhanced frame is output to the listener\nwith a delay defined by the Diffusion Buffer, and the output frame has a\ncorresponding look-ahead. In this work, we extend upon our previous work by\ncarefully designing a 2D convolutional UNet architecture that specifically\naligns with the Diffusion Buffer's look-ahead. We observe that the proposed\nUNet improves performance, particularly when the algorithmic latency is low.\nMoreover, we show that using a Data Prediction loss instead of Denoising Score\nMatching loss enables flexible control over the trade-off between algorithmic\nlatency and quality during inference. The extended Diffusion Buffer equipped\nwith a novel NN and loss function drastically reduces the algorithmic latency\nfrom 320 - 960 ms to 32 - 176 ms with an even increased performance. While it\nhas been shown before that offline generative diffusion models outperform\npredictive approaches in unseen noisy speech data, we confirm that the online\nDiffusion Buffer also outperforms its predictive counterpart on unseen noisy\nspeech data."}
{"id": "2510.18530", "categories": ["cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18530", "abs": "https://arxiv.org/abs/2510.18530", "authors": ["Bin Gu", "Lipeng Dai", "Huipeng Du", "Haitao Zhao", "Jibo Wei"], "title": "A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification", "comment": null, "summary": "Learning robust speaker representations under noisy conditions presents\nsignificant challenges, which requires careful handling of both discriminative\nand noise-invariant properties. In this work, we proposed an anchor-based\nstage-wise learning strategy for robust speaker representation learning.\nSpecifically, our approach begins by training a base model to establish\ndiscriminative speaker boundaries, and then extract anchor embeddings from this\nmodel as stable references. Finally, a copy of the base model is fine-tuned on\nnoisy inputs, regularized by enforcing proximity to their corresponding fixed\nanchor embeddings to preserve speaker identity under distortion. Experimental\nresults suggest that this strategy offers advantages over conventional joint\noptimization, particularly in maintaining discrimination while improving noise\nrobustness. The proposed method demonstrates consistent improvements across\nvarious noise conditions, potentially due to its ability to handle boundary\nstabilization and variation suppression separately."}
{"id": "2510.17825", "categories": ["eess.SP", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17825", "abs": "https://arxiv.org/abs/2510.17825", "authors": ["Shumaila Javaid", "Nasir Saeed"], "title": "Carbon-Aware Orchestration of Integrated Satellite Aerial Terrestrial Networks via Digital Twin", "comment": null, "summary": "Integrated Satellite Aerial Terrestrial Networks (ISATNs) are envisioned as\nkey enablers of 6G, providing global connectivity for applications such as\nautonomous transportation, Industrial IoT, and disaster response. Their\nlarge-scale deployment, however, risks unsustainable energy use and carbon\nemissions. This work advances prior energy-aware studies by proposing a\ncarbon-aware orchestration framework for ISATNs that leverages Digital Twin\n(DT) technology. The framework adopts grams of CO$_2$-equivalent per bit\n(gCO$_2$/bit) as a primary sustainability metric and implements a multi\ntimescale Plan Do Check Act (PDCA) loop that combines day-ahead forecasting\nwith real-time adaptive optimization. ISATN-specific control knobs, including\ncarbon-aware handovers, UAV duty cycling, and renewable-aware edge placement,\nare exploited to reduce emissions. Simulation results with real carbon\nintensity data show up to 29\\% lower gCO$_2$/bit than QoS-only orchestration,\nwhile improving renewable utilization and resilience under adverse events."}
{"id": "2510.18533", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.18533", "abs": "https://arxiv.org/abs/2510.18533", "authors": ["Bin Gu", "Lipeng Dai", "Huipeng Du", "Haitao Zhao", "Jibo Wei"], "title": "Noise-Conditioned Mixture-of-Experts Framework for Robust Speaker Verification", "comment": null, "summary": "Robust speaker verification under noisy conditions remains an open challenge.\nConventional deep learning methods learn a robust unified speaker\nrepresentation space against diverse background noise and achieve significant\nimprovement. In contrast, this paper presents a noise-conditioned\nmixture-ofexperts framework that decomposes the feature space into specialized\nnoise-aware subspaces for speaker verification. Specifically, we propose a\nnoise-conditioned expert routing mechanism, a universal model based expert\nspecialization strategy, and an SNR-decaying curriculum learning protocol,\ncollectively improving model robustness and generalization under diverse noise\nconditions. The proposed method can automatically route inputs to expert\nnetworks based on noise information derived from the inputs, where each expert\ntargets distinct noise characteristics while preserving speaker identity\ninformation. Comprehensive experiments demonstrate consistent superiority over\nbaselines, confirming that explicit noise-dependent feature modeling\nsignificantly enhances robustness without sacrificing verification accuracy."}
{"id": "2510.17832", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17832", "abs": "https://arxiv.org/abs/2510.17832", "authors": ["Henrique de Lima Alexandre", "Clodoaldo Aparecido de Moraes Lima"], "title": "Synthetic EEG Generation using Diffusion Models for Motor Imagery Tasks", "comment": "15 pages, BRACIS", "summary": "Electroencephalography (EEG) is a widely used, non-invasive method for\ncapturing brain activity, and is particularly relevant for applications in\nBrain-Computer Interfaces (BCI). However, collecting high-quality EEG data\nremains a major challenge due to sensor costs, acquisition time, and\ninter-subject variability. To address these limitations, this study proposes a\nmethodology for generating synthetic EEG signals associated with motor imagery\nbrain tasks using Diffusion Probabilistic Models (DDPM). The approach involves\npreprocessing real EEG data, training a diffusion model to reconstruct EEG\nchannels from noise, and evaluating the quality of the generated signals\nthrough both signal-level and task-level metrics. For validation, we employed\nclassifiers such as K-Nearest Neighbors (KNN), Convolutional Neural Networks\n(CNN), and U-Net to compare the performance of synthetic data against real data\nin classification tasks. The generated data achieved classification accuracies\nabove 95%, with low mean squared error and high correlation with real signals.\n  Our results demonstrate that synthetic EEG signals produced by diffusion\nmodels can effectively complement datasets, improving classification\nperformance in EEG-based BCIs and addressing data scarcity."}
{"id": "2510.17836", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17836", "abs": "https://arxiv.org/abs/2510.17836", "authors": ["G. Messa", "G. Acconciaioco", "S. Ripani", "L. Bozzelli", "A. Simone", "O. Giustolisi"], "title": "Two Phases Leakage Detection Strategy Supported by DMAs", "comment": null, "summary": "The present work proposes a novel two phases model-based strategy for leakage\ndetection. The two phases are: the identification of the district metering area\n(DMA) and the pipe pre-localization into the identified DMA. The strategy is\nbased on detecting and pre-localizing the punctual leakage as anomaly with\nrespect to the normal working conditions. A further novelty is the fact that\nthe pre-localization phase returns the sequence of pipes to inspect, which\nmakes the strategy attractive for water utilities, whose aim is to identify the\nanomaly at DMA level and, successively, to localize it with the minimum\ninspection cost. Furthermore, a random database is useful to test the\nperformance of the strategy with respect to the configuration of DMAs and the\npressure metering system. Consequently, a novel strategy to design the location\nof pressure meters is also proposed. It is demonstrated that the entire\nstrategy limits false positives during the DMA identification phase by using\nthe recently proposed index named Asset Management Support Indicator (AMSI).\nAMSI is invariant with respect to the deterioration, i.e., it is sensitive to\nits increase causing punctual leakage. The strategy is studied and discussed\nusing two real Apulian WDNs managed by Acquedotto Pugliese."}
{"id": "2510.18008", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18008", "abs": "https://arxiv.org/abs/2510.18008", "authors": ["Henrik Hellström", "Jiwon Jeong", "Ayfer Özgür", "Viktoria Fodor", "Carlo Fischione"], "title": "Majority Vote Compressed Sensing", "comment": null, "summary": "We consider the problem of non-coherent over-the-air computation (AirComp),\nwhere $n$ devices carry high-dimensional data vectors\n$\\mathbf{x}_i\\in\\mathbb{R}^d$ of sparsity $\\lVert\\mathbf{x}_i\\rVert_0\\leq k$\nwhose sum has to be computed at a receiver. Previous results on non-coherent\nAirComp require more than $d$ channel uses to compute functions of\n$\\mathbf{x}_i$, where the extra redundancy is used to combat non-coherent\nsignal aggregation. However, if the data vectors are sparse, sparsity can be\nexploited to offer significantly cheaper communication. In this paper, we\npropose to use random transforms to transmit lower-dimensional projections\n$\\mathbf{s}_i\\in\\mathbb{R}^T$ of the data vectors. These projected vectors are\ncommunicated to the receiver using a majority vote (MV)-AirComp scheme, which\nestimates the bit-vector corresponding to the signs of the aggregated\nprojections, i.e., $\\mathbf{y} = \\text{sign}(\\sum_i\\mathbf{s}_i)$. By\nleveraging 1-bit compressed sensing (1bCS) at the receiver, the real-valued and\nhigh-dimensional aggregate $\\sum_i\\mathbf{x}_i$ can be recovered from\n$\\mathbf{y}$. We prove analytically that the proposed MVCS scheme estimates the\naggregated data vector $\\sum_i \\mathbf{x}_i$ with $\\ell_2$-norm error\n$\\epsilon$ in $T=\\mathcal{O}(kn\\log(d)/\\epsilon^2)$ channel uses. Moreover, we\nspecify algorithms that leverage MVCS for histogram estimation and distributed\nmachine learning. Finally, we provide numerical evaluations that reveal the\nadvantage of MVCS compared to the state-of-the-art."}
{"id": "2510.18336", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18336", "abs": "https://arxiv.org/abs/2510.18336", "authors": ["Wangye Jiang", "Haoming Yang", "Xinyu Lu", "Mingyuan Wang", "Huimei Sun", "Jingya Zhang"], "title": "MCANet: A Coherent Multimodal Collaborative Attention Network for Advanced Modulation Recognition in Adverse Noisy Environments", "comment": null, "summary": "As wireless communication systems evolve, automatic modulation recognition\n(AMR) plays a key role in improving spectrum efficiency, especially in\ncognitive radio systems. Traditional AMR methods face challenges in complex,\nnoisy environments, particularly in low signal-to-noise ratio (SNR) conditions.\nThis paper introduces MCANet (Multimodal Collaborative Attention Network), a\nmultimodal deep learning framework designed to address these challenges. MCANet\nemploys refined feature extraction and global modeling to support its fusion\nstrategy.Experimental results across multiple benchmark datasets show that\nMCANet outperforms mainstream AMR models, offering better robustness in low-SNR\nconditions."}
{"id": "2510.18422", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18422", "abs": "https://arxiv.org/abs/2510.18422", "authors": ["Yizhen Jia", "Siyao Xiao", "Wenkai Jia", "Hui Chen", "Wen-Qin Wang"], "title": "AWSPNet: Attention-based Dual-Tree Wavelet Scattering Prototypical Network for MIMO Radar Target Recognition and Jamming Suppression", "comment": "13 pages, 10 figures, The code is available in\n  https://github.com/jiaxuanzhi/AwspNet", "summary": "The increasing of digital radio frequency memory based electronic\ncountermeasures poses a significant threat to the survivability and\neffectiveness of radar systems. These jammers can generate a multitude of\ndeceptive false targets, overwhelming the radar's processing capabilities and\nmasking targets. Consequently, the ability to robustly discriminate between\ntrue targets and complex jamming signals, especially in low signal-to-noise\nratio (SNR) environments, is of importance. This paper introduces the\nattention-based dual-tree wavelet scattering prototypical network (AWSPNet), a\ndeep learning framework designed for simultaneous radar target recognition and\njamming suppression. The core of AWSPNet is the encoder that leverages the\ndual-tree complex wavelet transform to extract features that are inherently\nrobust to noise and signal translations. These features are further refined by\nan attention mechanism and a pre-trained backbone network. To address the\nchallenge of limited labeled data and enhance generalization, we employ a\nsupervised contrastive learning strategy during the training phase. The\nclassification is performed by a prototypical network, which is particularly\neffective in few-shot learning scenarios, enabling rapid adaptation to new\nsignal types. We demonstrate the efficacy of our approach through extensive\nexperiments. The results show that AWSPNet achieves 90.45\\% accuracy at -6 dB\nSNR. Furthermore, we provide a physical interpretation of the network's inner\nworkings through t-SNE visualizations, which analyze the feature separability\nat different stages of the model. Finally, by integrating AWSPNet with a\ntime-domain sliding window approach, we present a complete algorithm capable of\nnot only identifying but also effectively suppressing various types of jamming,\nthereby validating its potential for practical application in complex\nelectromagnetic environments."}
{"id": "2510.18501", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18501", "abs": "https://arxiv.org/abs/2510.18501", "authors": ["Tung-Anh Nguyen", "Van-Phuc Bui", "Shashi Raj Pandey", "Kim Hue Ta", "Nguyen H. Tran", "Petar Popovski"], "title": "Microsecond Federated SVD on Grassmann Manifold for Real-time IoT Intrusion Detection", "comment": null, "summary": "This paper introduces FedSVD, a novel unsupervised federated learning\nframework for real-time anomaly detection in IoT networks. By leveraging\nSingular Value Decomposition (SVD) and optimization on the Grassmann manifolds,\nFedSVD enables accurate detection of both known and unknown intrusions without\nrelying on labeled data or centralized data sharing. Tailored for deployment on\nlow-power devices like the NVIDIA Jetson AGX Orin, the proposed method\nsignificantly reduces communication overhead and computational cost.\nExperimental results show that FedSVD achieves performance comparable to deep\nlearning baselines while reducing inference latency by over 10x, making it\nsuitable for latency-sensitive IoT applications."}
{"id": "2510.18604", "categories": ["eess.SP", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.18604", "abs": "https://arxiv.org/abs/2510.18604", "authors": ["Zian Meng", "Qiang Li", "Wenqian Tang", "Mingdie Yan", "Xiaohu Ge"], "title": "Channel-Aware Vector Quantization for Robust Semantic Communication on Discrete Channels", "comment": "12 pages, 8 figures", "summary": "Deep learning-based semantic communication has largely relied on analog or\nsemi-digital transmission, which limits compatibility with modern digital\ncommunication infrastructures. Recent studies have employed vector quantization\n(VQ) to enable discrete semantic transmission, yet existing methods neglect\nchannel state information during codebook optimization, leading to suboptimal\nrobustness. To bridge this gap, we propose a channel-aware vector quantization\n(CAVQ) algorithm within a joint source-channel coding (JSCC) framework, termed\nVQJSCC, established on a discrete memoryless channel. In this framework,\nsemantic features are discretized and directly mapped to modulation\nconstellation symbols, while CAVQ integrates channel transition probabilities\ninto the quantization process, aligning easily confused symbols with\nsemantically similar codewords. A multi-codebook alignment mechanism is further\nintroduced to handle mismatches between codebook order and modulation order by\ndecomposing the transmission stream into multiple independently optimized\nsubchannels. Experimental results demonstrate that VQJSCC effectively mitigates\nthe digital cliff effect, achieves superior reconstruction quality across\nvarious modulation schemes, and outperforms state-of-the-art digital semantic\ncommunication baselines in both robustness and efficiency."}
{"id": "2510.18646", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18646", "abs": "https://arxiv.org/abs/2510.18646", "authors": ["Anwar Ahmed Khan", "Shama Siddiqui", "Indrakshi Dey"], "title": "Delay Management Using Packet Fragmentation in Wireless Industrial Automation Systems", "comment": "21st Int. Conference on Networking and Services (ICNS 2025). Lisbon,\n  Portugal", "summary": "Managing delay is one of the core requirements of industrial automation\napplications due to the high risk associated for equipment and human lives.\nUsing efficient Media Access Control (MAC) schemes guarantees the timely\ntransmission of critical data, particularly in the industrial environments\nwhere heterogeneous data is inherently expected. This paper compares the\nperformance of Fragmentation based MAC (FROG-MAC) against Fuzzy Priority\nScheduling based MAC (FPS-MAC), both of which have been designed to optimize\nthe performance of heterogenous wireless networks. Contiki has been used as a\nsimulation platform and a single hop star topology has been assumed to resemble\nthe industrial environment. It has been shown that FROG-MAC has the potential\nto outperform FPS-MAC in terms of energy efficiency and delay both, due to its\ninherent feature of interrupting ongoing lower priority transmission on the\nchannel."}
{"id": "2510.18662", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18662", "abs": "https://arxiv.org/abs/2510.18662", "authors": ["Shama Siddiqui", "Anwar Ahmed Khan", "Indrakshi Dey"], "title": "A Comparative Analysis of High-Level vs. Low-Level Simulations for Dynamic MAC Protocols in Wireless Sensor Networks", "comment": "21st Int. Conference on Networking and Services (ICNS 2025). Lisbon,\n  Portugal", "summary": "Simulation studies are conducted at different levels of details for assessing\nthe performance of Media Access Control (MAC) protocols in Wireless Sensor\nNetworks (WSN). In the present-day scenario where hundreds of MAC protocols\nhave been proposed, it is important to assess the quality of performance\nevaluation being conducted for each of the proposed protocols. It therefore\nbecomes crucial to compare the results of high-level theoretical simulations\nwith the detailed implementation results before any network protocol could be\ndeployed for a real-world scenario. In this work, we present a comparison of\nhigh-level theoretical and detailed implementation results for Adaptive and\nDynamic Polling-MAC (ADP-MAC). MATLAB has been used for conducting initial\ntheoretical simulations and TinyOS has been used to develop the detailed\nimplementation of protocol for Mica2 platform. Performance evaluation of\nADP-MAC using the two levels of simulation has been conducted based on energy\nand delay. In the high-level implementation, energy consumption was found to be\ndecreasing whereas delay was found to be increasing for increasing channel\npolling intervals. On the other hand, when detailed implementation was\ndeveloped, it was observed that both energy consumption and delay revealed an\nincreasing trend with the increasing polling intervals. Therefore, it has been\nshown that the trends for high- and low-level simulations for ADP-MAC are\nsignificantly different, due to the lack of realistic assumptions in the\nhigher-level study."}
{"id": "2510.18729", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18729", "abs": "https://arxiv.org/abs/2510.18729", "authors": ["Yhonatan Kvich", "Rotem Arie", "Hana Hasan", "Shaik Basheeruddin Shah", "Yonina C. Eldar"], "title": "mSQUID: Model-Based Leanred Modulo Recovery at Low Sampling Rates", "comment": null, "summary": "Modulo sampling enables acquisition of signals with unlimited dynamic range\nby folding the input into a bounded interval prior to sampling, thus\neliminating the risk of signal clipping and preserving information without\nrequiring highresolution ADCs. While this enables low-cost hardware, the\nnonlinear distortion introduced by folding presents recovery challenges,\nparticularly under noise and quantization. We propose a model-based deep\nunfolding network tailored to this setting, combining the interpretability of\nclassical compress sensing (CS) solvers with the flexibility of learning. A key\ninnovation is a soft-quantization module that encodes the modulo prior by\nguiding the solution toward discrete multiples of the folding range in a\ndifferentiable and learnable way. Our method, modulo soft-quantized unfolded\niterative decoder (mSQUID), achieves superior reconstruction performance at low\nsampling rates under additive Gaussian noise. We further demonstrate its\nutility in a challenging case where signals with vastly different amplitudes\nand disjoint frequency bands are acquired simultaneously and quantized. In this\nscenario, classical sampling often struggles due to weak signal distortion or\nstrong signal clipping, while our approach is able to recover the input\nsignals. Our method also offers significantly reduced runtimes, making it\nsuitable for real-time, resource-limited systems."}
{"id": "2510.18743", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18743", "abs": "https://arxiv.org/abs/2510.18743", "authors": ["Kasun R. Wijewardhana", "Animesh Yadav", "Ming Zeng", "Mohamed Elsayed", "Octavia A. Dobre", "Zhiguo Ding"], "title": "Wireless-Fed Pinching-Antenna Systems (Wi-PASS) for NextG Wireless Networks", "comment": "14 pages, 5 figures, For Potential Publication in the IEEE\n  Communications Magazine", "summary": "Waveguide-based pinching-antenna systems (PASS) have recently emerged as a\npromising solution to mitigate severe propagation losses in millimeter-wave and\nterahertz bands by intelligently and flexibly establishing line-of-sight links.\nHowever, their reliance on wire-based feeding confines deployment to areas near\nthe base station (BS), limiting installation flexibility and making them\ncost-ineffective for serving distant users or regions. To overcome this\nchallenge, this article proposes wireless-fed pinchingantenna systems\n(Wi-PASS), which employ wireless feeding to energize waveguides. Wi-PASS offer\na practical and cost-efficient means to extend coverage beyond the BS vicinity.\nSeveral indoor and outdoor use cases demonstrate Wi-PASS advantages over PASS.\nNumerical results further show that Wi-PASS deliver higher data rates than\nconventional fixed-antenna systems, confirming the superior feasibility and\nperformance of Wi-PASS. Key future research directions are also discussed to\nadvance Wi-PASS deployment."}
{"id": "2510.18760", "categories": ["eess.SP", "cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2510.18760", "abs": "https://arxiv.org/abs/2510.18760", "authors": ["Mouna Gharbi", "Silvia Villa", "Emilie Chouzenoux", "Jean-Christophe Pesquet", "Laurent Duval"], "title": "Analyse comparative d'algorithmes de restauration en architecture dépliée pour des signaux chromatographiques parcimonieux", "comment": "4 pages, in French, GRETSI Symposium on Signal and Image Processing,\n  Strasbourg, France, August 2025", "summary": "Data restoration from degraded observations, of sparsity hypotheses, is an\nactive field of study. Traditional iterative optimization methods are now\ncomplemented by deep learning techniques. The development of unfolded methods\nbenefits from both families. We carry out a comparative study of three\narchitectures on parameterized chromatographic signal databases, highlighting\nthe performance of these approaches, especially when employing metrics adapted\nto physico-chemical peak signal characterization."}
{"id": "2510.18827", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18827", "abs": "https://arxiv.org/abs/2510.18827", "authors": ["Michael Fraiman", "Paulina Hoyos", "Tamir Bendory", "Joe Kileel", "Oscar Mickelin", "Nir Sharon", "Amit Singer"], "title": "SO(3)-invariant PCA with application to molecular data", "comment": null, "summary": "Principal component analysis (PCA) is a fundamental technique for\ndimensionality reduction and denoising; however, its application to\nthree-dimensional data with arbitrary orientations -- common in structural\nbiology -- presents significant challenges. A naive approach requires\naugmenting the dataset with many rotated copies of each sample, incurring\nprohibitive computational costs. In this paper, we extend PCA to 3D volumetric\ndatasets with unknown orientations by developing an efficient and principled\nframework for SO(3)-invariant PCA that implicitly accounts for all rotations\nwithout explicit data augmentation. By exploiting underlying algebraic\nstructure, we demonstrate that the computation involves only the square root of\nthe total number of covariance entries, resulting in a substantial reduction in\ncomplexity. We validate the method on real-world molecular datasets,\ndemonstrating its effectiveness and opening up new possibilities for\nlarge-scale, high-dimensional reconstruction problems."}
{"id": "2510.18206", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18206", "abs": "https://arxiv.org/abs/2510.18206", "authors": ["Hanyu Meng", "Vidhyasaharan Sethu", "Eliathamby Ambikairajah", "Qiquan Zhang", "Haizhou Li"], "title": "Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing", "comment": "Submitted to ICASSP2026", "summary": "In audio signal processing, learnable front-ends have shown strong\nperformance across diverse tasks by optimizing task-specific representation.\nHowever, their parameters remain fixed once trained, lacking flexibility during\ninference and limiting robustness under dynamic complex acoustic environments.\nIn this paper, we introduce a novel adaptive paradigm for audio front-ends that\nreplaces static parameterization with a closed-loop neural controller.\nSpecifically, we simplify the learnable front-end LEAF architecture and\nintegrate a neural controller for adaptive representation via dynamically\ntuning Per-Channel Energy Normalization. The neural controller leverages both\nthe current and the buffered past subband energies to enable input-dependent\nadaptation during inference. Experimental results on multiple audio\nclassification tasks demonstrate that the proposed adaptive front-end\nconsistently outperforms prior fixed and learnable front-ends under both clean\nand complex acoustic conditions. These results highlight neural adaptability as\na promising direction for the next generation of audio front-ends."}

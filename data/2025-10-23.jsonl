{"id": "2510.18917", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18917", "abs": "https://arxiv.org/abs/2510.18917", "authors": ["Mandip Goswami"], "title": "RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling", "comment": "8 pages, 3 figures", "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies."}
{"id": "2510.18938", "categories": ["eess.AS", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18938", "abs": "https://arxiv.org/abs/2510.18938", "authors": ["Qianheng Xu"], "title": "StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction", "comment": "13 pages, 5 figures", "summary": "Over 70 million people worldwide experience stuttering, yet most automatic\nspeech systems misinterpret disfluent utterances or fail to transcribe them\naccurately. Existing methods for stutter correction rely on handcrafted feature\nextraction or multi-stage automatic speech recognition (ASR) and text-to-speech\n(TTS) pipelines, which separate transcription from audio reconstruction and\noften amplify distortions. This work introduces StutterZero and StutterFormer,\nthe first end-to-end waveform-to-waveform models that directly convert\nstuttered speech into fluent speech while jointly predicting its transcription.\nStutterZero employs a convolutional-bidirectional LSTM encoder-decoder with\nattention, whereas StutterFormer integrates a dual-stream Transformer with\nshared acoustic-linguistic representations. Both architectures are trained on\npaired stuttered-fluent data synthesized from the SEP-28K and LibriStutter\ncorpora and evaluated on unseen speakers from the FluencyBank dataset. Across\nall benchmarks, StutterZero had a 24% decrease in Word Error Rate (WER) and a\n31% improvement in semantic similarity (BERTScore) compared to the leading\nWhisper-Medium model. StutterFormer achieved better results, with a 28%\ndecrease in WER and a 34% improvement in BERTScore. The results validate the\nfeasibility of direct end-to-end stutter-to-fluent speech conversion, offering\nnew opportunities for inclusive human-computer interaction, speech therapy, and\naccessibility-oriented AI systems."}
{"id": "2510.19174", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19174", "abs": "https://arxiv.org/abs/2510.19174", "authors": ["Yuanming Zhang", "Zeyan Song", "Jing Lu", "Fei Chen", "Zhibin Lin"], "title": "Auditory Attention Decoding from Ear-EEG Signals: A Dataset with Dynamic Attention Switching and Rigorous Cross-Validation", "comment": null, "summary": "Recent promising results in auditory attention decoding (AAD) using scalp\nelectroencephalography (EEG) have motivated the exploration of cEEGrid, a\nflexible and portable ear-EEG system. While prior cEEGrid-based studies have\nconfirmed the feasibility of AAD, they often neglect the dynamic nature of\nattentional states in real-world contexts. To address this gap, a novel cEEGrid\ndataset featuring three concurrent speakers distributed across three of five\ndistinct spatial locations is introduced. The novel dataset is designed to\nprobe attentional tracking and switching in realistic scenarios. Nested\nleave-one-out validation-an approach more rigorous than conventional\nsingle-loop leave-one-out validation-is employed to reduce biases stemming from\nEEG's intricate temporal dynamics. Four rule-based models are evaluated: Wiener\nfilter (WF), canonical component analysis (CCA), common spatial pattern (CSP)\nand Riemannian Geometry-based classifier (RGC). With a 30-second decision\nwindow, WF and CCA models achieve decoding accuracies of 41.5% and 41.4%,\nrespectively, while CSP and RGC models yield 37.8% and 37.6% accuracies using a\n10-second window. Notably, both WF and CCA successfully track attentional state\nswitches across all experimental tasks. Additionally, higher decoding\naccuracies are observed for electrodes positioned at the upper cEEGrid layout\nand near the listener's right ear. These findings underscore the utility of\ndynamic, ecologically valid paradigms and rigorous validation in advancing AAD\nresearch with cEEGrid."}
{"id": "2510.19354", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.19354", "abs": "https://arxiv.org/abs/2510.19354", "authors": ["Eylon Zohar", "Israel Nelken", "Boaz Rafaely"], "title": "An Efficient Neural Network for Modeling Human Auditory Neurograms for Speech", "comment": null, "summary": "Classical auditory-periphery models, exemplified by Bruce et al., 2018,\nprovide high-fidelity simulations but are stochastic and computationally\ndemanding, limiting large-scale experimentation and low-latency use. Prior\nneural encoders approximate aspects of the periphery; however, few are\nexplicitly trained to reproduce the deterministic, rate-domain neurogram ,\nhindering like-for-like evaluation. We present a compact convolutional encoder\nthat approximates the Bruce mean-rate pathway and maps audio to a\nmulti-frequency neurogram. We deliberately omit stochastic spiking effects and\nfocus on a deterministic mapping (identical outputs for identical inputs).\nUsing a computationally efficient design, the encoder achieves close\ncorrespondence to the reference while significantly reducing computation,\nenabling efficient modeling and front-end processing for auditory neuroscience\nand audio signal processing applications."}
{"id": "2510.18978", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18978", "abs": "https://arxiv.org/abs/2510.18978", "authors": ["Tomer Shaked", "Philipp del Hougne", "George C. Alexandropoulos", "Nir Shlezinger"], "title": "AI-Aided Annealed Langevin Dynamics for Rapid Optimization of Programmable Channels", "comment": "5 pages, 3 figures. Accepted to IEEE Signal Processing and Wireless\n  Communications (SPAWC) 2025 conference", "summary": "Emerging technologies such as Reconfigurable Intelligent Surfaces (RIS) make\nit possible to optimize some parameters of wireless channels. Conventional\napproaches require relating the channel and its programmable parameters via a\nsimple model that supports rapid optimization, e.g., re-tuning the parameters\neach time the users move. However, in practice such models are often crude\napproximations of the channel, and a more faithful description can be obtained\nvia complex simulators, or only by measurements. In this work, we introduce a\nnovel approach for rapid optimization of programmable channels based on\nAI-aided Annealed Langevin Dynamics (ALD), which bypasses the need for explicit\nchannel modeling. By framing the ALD algorithm using the MAP estimate, we\ndesign a deep unfolded ALD algorithm that leverages a Deep Neural Network (DNN)\nto estimate score gradients for optimizing channel parameters. We introduce a\ntraining method that overcomes the need for channel modeling using zero-order\ngradients, combined with active learning to enhance generalization, enabling\noptimization in complex and dynamically changing environments. We evaluate the\nproposed method in RIS-aided scenarios subject to rich-scattering effects. Our\nresults demonstrate that our AI-aided ALD method enables rapid and reliable\nchannel parameter tuning with limited latency."}
{"id": "2510.19368", "categories": ["cs.SD", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19368", "abs": "https://arxiv.org/abs/2510.19368", "authors": ["Weichuang Shao", "Iman Yi Liao", "Tomas Henrique Bode Maul", "Tissa Chandesa"], "title": "AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework Trained from Scratch", "comment": null, "summary": "Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and Audio\nFlamingo, achieve top-tier results across standard audio benchmarks but are\nlimited by fixed input rates and durations, hindering their reusability. This\npaper introduces the Augmentation-driven Multiview Audio Transformer (AMAuT), a\ntraining-from-scratch framework that eliminates the dependency on pre-trained\nweights while supporting arbitrary sample rates and audio lengths. AMAuT\nintegrates four key components: (1) augmentation-driven multiview learning for\nrobustness, (2) a conv1 + conv7 + conv1 one-dimensional CNN bottleneck for\nstable temporal encoding, (3) dual CLS + TAL tokens for bidirectional context\nrepresentation, and (4) test-time adaptation/augmentation (TTA^2) to improve\ninference reliability. Experiments on five public benchmarks, AudioMNIST,\nSpeechCommands V1 & V2, VocalSound, and CochlScene, show that AMAuT achieves\naccuracies up to 99.8% while consuming less than 3% of the GPU hours required\nby comparable pre-trained models. Thus, AMAuT presents a highly efficient and\nflexible alternative to large pre-trained models, making state-of-the-art audio\nclassification accessible in computationally constrained settings."}
{"id": "2510.19414", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.19414", "abs": "https://arxiv.org/abs/2510.19414", "authors": ["Tong Zhang", "Yihuan Huang", "Yanzhen Ren"], "title": "EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection", "comment": null, "summary": "The growing prevalence of speech deepfakes has raised serious concerns,\nparticularly in real-world scenarios such as telephone fraud and identity\ntheft. While many anti-spoofing systems have demonstrated promising performance\non lab-generated synthetic speech, they often fail when confronted with\nphysical replay attacks-a common and low-cost form of attack used in practical\nsettings. Our experiments show that models trained on existing datasets exhibit\nsevere performance degradation, with average accuracy dropping to 59.6% when\nevaluated on replayed audio. To bridge this gap, we present EchoFake, a\ncomprehensive dataset comprising more than 120 hours of audio from over 13,000\nspeakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and\nphysical replay recordings collected under varied devices and real-world\nenvironmental settings. Additionally, we evaluate three baseline detection\nmodels and show that models trained on EchoFake achieve lower average EERs\nacross datasets, indicating better generalization. By introducing more\npractical challenges relevant to real-world deployment, EchoFake offers a more\nrealistic foundation for advancing spoofing detection methods."}
{"id": "2510.19007", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19007", "abs": "https://arxiv.org/abs/2510.19007", "authors": ["Haofan Dong", "Houtianfu Wang", "Hanlin Cai", "Ozgur B. Akan"], "title": "Fundamental Limits of Cooperative Integrated Sensing and Communications over Low-Earth Orbit THz Satellite Channels", "comment": null, "summary": "Terahertz inter-satellite links enable unprecedented sensing precision for\nLow Earth Orbit (LEO) constellations, yet face fundamental bounds from hardware\nimpairments, pointing errors, and network interference. We develop a Network\nCram\\'er-Rao Lower Bound (N-CRLB) framework incorporating dynamic topology,\nhardware quality factor $\\Gamma_{\\text{eff}}$, phase noise $\\sigma^2_\\phi$, and\ncooperative effects through recursive Fisher Information analysis. Our analysis\nreveals three key insights: (i) hardware and phase noise create\npower-independent performance ceilings ($\\sigma_{\\text{ceiling}} \\propto\n\\sqrt{\\Gamma_{\\text{eff}}}$) and floors ($\\sigma_{\\text{floor}} \\propto\n\\sqrt{\\sigma^2_\\phi}/f_c$), with power-only scaling saturating above\n$\\text{SNR}_{\\text{crit}}=1/\\Gamma_{\\text{eff}}$; (ii) interference\ncoefficients $\\alpha_{\\ell m}$ enable opportunistic sensing with demonstrated\ngains of 5.5~dB under specific conditions (65~dB processing gain, 50~dBi\nantennas); (iii) measurement correlations from shared timing references, when\nproperly modeled, do not degrade performance and can provide common-mode\nrejection benefits compared to mismodeled independent-noise baselines.\nSub-millimeter ranging requires co-optimized hardware\n($\\Gamma_{\\text{eff}}<0.01$), oscillators ($\\sigma^2_\\phi<10^{-2}$), and\nappropriate 3D geometry configurations."}
{"id": "2510.19435", "categories": ["cs.SD", "math.AT", "nlin.AO", "physics.data-an", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2510.19435", "abs": "https://arxiv.org/abs/2510.19435", "authors": ["Gakusei Sato", "Hiroya Nakao", "Riccardo Muolo"], "title": "Time delay embeddings to characterize the timbre of musical instruments using Topological Data Analysis: a study on synthetic and real data", "comment": null, "summary": "Timbre allows us to distinguish between sounds even when they share the same\npitch and loudness, playing an important role in music, instrument recognition,\nand speech. Traditional approaches, such as frequency analysis or machine\nlearning, often overlook subtle characteristics of sound. Topological Data\nAnalysis (TDA) can capture complex patterns, but its application to timbre has\nbeen limited, partly because it is unclear how to represent sound effectively\nfor TDA. In this study, we investigate how different time delay embeddings\naffect TDA results. Using both synthetic and real audio signals, we identify\ntime delays that enhance the detection of harmonic structures. Our findings\nshow that specific delays, related to fractions of the fundamental period,\nallow TDA to reveal key harmonic features and distinguish between integer and\nnon-integer harmonics. The method is effective for synthetic and real musical\ninstrument sounds and opens the way for future works, which could extend it to\nmore complex sounds using higher-dimensional embeddings and additional\npersistence statistics."}
{"id": "2510.19439", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.19439", "abs": "https://arxiv.org/abs/2510.19439", "authors": ["Wageesha N. Manamperi", "Thushara D. Abhayapala"], "title": "Relative Transfer Matrix Estimator using Covariance Subtraction", "comment": null, "summary": "The Relative Transfer Matrix (ReTM), recently introduced as a generalization\nof the relative transfer function for multiple receivers and sources, shows\npromising performance when applied to speech enhancement and speaker separation\nin noisy environments. Blindly estimating the ReTM of sound sources by\nexploiting the covariance matrices of multichannel recordings is highly\nbeneficial for practical applications. In this paper, we use covariance\nsubtraction to present a flexible and practically viable method for estimating\nthe ReTM for a select set of independent sound sources. To show the versatility\nof the method, we validated it through a speaker separation application under\nreverberant conditions. Separation performance is evaluated at low\nsignal-to-noise ratio levels in comparison with existing ReTM-based and\nrelative transfer function-based estimators, in both simulated and real-life\nenvironments."}
{"id": "2510.19057", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19057", "abs": "https://arxiv.org/abs/2510.19057", "authors": ["Anna Cetera", "Sima Ghafoori", "Ali Rabiee", "Mohammad Hassan Farhadi", "Yalda Shahriari", "Reza Abiri"], "title": "Macroscopic EEG Reveals Discriminative Low-Frequency Oscillations in Plan-to-Grasp Visuomotor Tasks", "comment": "12 pages, 8 figures, 1 table", "summary": "The vision-based grasping brain network integrates visual perception with\ncognitive and motor processes for visuomotor tasks. While invasive recordings\nhave successfully decoded localized neural activity related to grasp type\nplanning and execution, macroscopic neural activation patterns captured by\nnoninvasive electroencephalography (EEG) remain far less understood. We\nintroduce a novel vision-based grasping platform to investigate\ngrasp-type-specific (precision, power, no-grasp) neural activity across\nlarge-scale brain networks using EEG neuroimaging. The platform isolates\ngrasp-specific planning from its associated execution phases in naturalistic\nvisuomotor tasks, where the Filter-Bank Common Spatial Pattern (FBCSP)\ntechnique was designed to extract discriminative frequency-specific features\nwithin each phase. Support vector machine (SVM) classification discriminated\nbinary (precision vs. power, grasp vs. no-grasp) and multiclass (precision vs.\npower vs. no-grasp) scenarios for each phase, and were compared against\ntraditional Movement-Related Cortical Potential (MRCP) methods. Low-frequency\noscillations (0.5-8 Hz) carry grasp-related information established during\nplanning and maintained throughout execution, with consistent classification\nperformance across both phases (75.3-77.8\\%) for precision vs. power\ndiscrimination, compared to 61.1\\% using MRCP. Higher-frequency activity (12-40\nHz) showed phase-dependent results with 93.3\\% accuracy for grasp vs. no-grasp\nclassification but 61.2\\% for precision vs. power discrimination. Feature\nimportance using SVM coefficients identified discriminative features within\nfrontoparietal networks during planning and motor networks during execution.\nThis work demonstrated the role of low-frequency oscillations in decoding grasp\ntype during planning using noninvasive EEG."}
{"id": "2510.19414", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.19414", "abs": "https://arxiv.org/abs/2510.19414", "authors": ["Tong Zhang", "Yihuan Huang", "Yanzhen Ren"], "title": "EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection", "comment": null, "summary": "The growing prevalence of speech deepfakes has raised serious concerns,\nparticularly in real-world scenarios such as telephone fraud and identity\ntheft. While many anti-spoofing systems have demonstrated promising performance\non lab-generated synthetic speech, they often fail when confronted with\nphysical replay attacks-a common and low-cost form of attack used in practical\nsettings. Our experiments show that models trained on existing datasets exhibit\nsevere performance degradation, with average accuracy dropping to 59.6% when\nevaluated on replayed audio. To bridge this gap, we present EchoFake, a\ncomprehensive dataset comprising more than 120 hours of audio from over 13,000\nspeakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and\nphysical replay recordings collected under varied devices and real-world\nenvironmental settings. Additionally, we evaluate three baseline detection\nmodels and show that models trained on EchoFake achieve lower average EERs\nacross datasets, indicating better generalization. By introducing more\npractical challenges relevant to real-world deployment, EchoFake offers a more\nrealistic foundation for advancing spoofing detection methods."}
{"id": "2510.19572", "categories": ["eess.AS"], "pdf": "https://arxiv.org/pdf/2510.19572", "abs": "https://arxiv.org/abs/2510.19572", "authors": ["Petr Pálka", "Jiangyu Han", "Marc Delcroix", "Naohiro Tawara", "Lukáš Burget"], "title": "VBx for End-to-End Neural and Clustering-based Diarization", "comment": "Submitted to ICASSP 2026", "summary": "We present improvements to speaker diarization in the two-stage end-to-end\nneural diarization with vector clustering (EEND-VC) framework. The first stage\nemploys a Conformer-based EEND model with WavLM features to infer frame-level\nspeaker activity within short windows. The identities and counts of global\nspeakers are then derived in the second stage by clustering speaker embeddings\nacross windows. The focus of this work is to improve the second stage; we\nfilter unreliable embeddings from short segments and reassign them after\nclustering. We also integrate the VBx clustering to improve robustness when the\nnumber of speakers is large and individual speaking durations are limited.\nEvaluation on a compound benchmark spanning multiple domains is conducted\nwithout fine-tuning the EEND model or tuning clustering parameters per dataset.\nDespite this, the system generalizes well and matches or exceeds recent\nstate-of-the-art performance."}
{"id": "2510.19209", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19209", "abs": "https://arxiv.org/abs/2510.19209", "authors": ["Yining Li", "Ziwei Wan", "Chongjia Sun", "Kaijun Feng", "Keke Ying", "Wenyan Ma", "Lipeng Zhu", "Xiaodan Shao", "Zhenyu Xiao", "Zhen Gao"], "title": "AI Signal Processing Paradigm for Movable Antenna: From Geometric Optimization to Electromagnetic Reconfigurability", "comment": null, "summary": "As 6G wireless communication systems evolve toward intelligence and high\nreconfigurability, the limitations of traditional fixed antenna (TFA) has\nbecome increasingly prominent, with geometrically movable antenna (GMA) and\nelectromagnetically reconfigurable antenna (ERA) emerging as key technologies\nto break through this bottleneck. GMA activates spatial degrees of freedom\n(DoF) by dynamically adjusting antenna positions, ERA regulates radiation\ncharacteristics using tunable metamaterials, thereby introducing DoF in the\nelectromagnetic domain. However, the ``geometric-electromagnetic dual\nreconfiguration\" paradigm formed by their integration poses severe challenges\nof high-dimensional hybrid optimization to signal processing. To address this\nissue, we integrate the geometric optimization of GMA and the electromagnetic\nreconfiguration of ERA for the first time, propose a unified modeling framework\nfor movable and reconfigurable antenna (MARA), investigate the channel modeling\nand spectral efficiency (SE) optimization for GMA, ERA, and MARA. Besides, we\nsystematically review artificial intelligence (AI)-based solutions, focusing on\nanalyzing the advantages of AI over traditional algorithms in high-dimensional\nnon-convex optimization computations. This paper fills the gap in existing\nliterature regarding the lack of a comprehensive review on the AI-driven signal\nprocessing paradigm under geometric-electromagnetic dual reconfiguration and\nprovides theoretical support for the design and optimization of 6G wireless\nsystems with high SE and flexibility."}
{"id": "2510.19439", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.19439", "abs": "https://arxiv.org/abs/2510.19439", "authors": ["Wageesha N. Manamperi", "Thushara D. Abhayapala"], "title": "Relative Transfer Matrix Estimator using Covariance Subtraction", "comment": null, "summary": "The Relative Transfer Matrix (ReTM), recently introduced as a generalization\nof the relative transfer function for multiple receivers and sources, shows\npromising performance when applied to speech enhancement and speaker separation\nin noisy environments. Blindly estimating the ReTM of sound sources by\nexploiting the covariance matrices of multichannel recordings is highly\nbeneficial for practical applications. In this paper, we use covariance\nsubtraction to present a flexible and practically viable method for estimating\nthe ReTM for a select set of independent sound sources. To show the versatility\nof the method, we validated it through a speaker separation application under\nreverberant conditions. Separation performance is evaluated at low\nsignal-to-noise ratio levels in comparison with existing ReTM-based and\nrelative transfer function-based estimators, in both simulated and real-life\nenvironments."}
{"id": "2510.19256", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19256", "abs": "https://arxiv.org/abs/2510.19256", "authors": ["Haiquan Zhao", "Bei Xu"], "title": "Generalized Modified Blake-Zisserman Robust Spline Adaptive Filter for Generalized Gaussian Noise", "comment": null, "summary": "The spline adaptive filtering (SAF) algorithm-based information-theoretic\nlearning has exhibited strong convergence performance in nonlinear system\nidentification (NSI), establishing SAF as a promising framework for adaptive\nfiltering. However, existing SAF-based methods suffer from performance\ndegradation under generalized Gaussian noise (GGN) environment and exhibit\nsignificant steady-state misalignment under impulse noise. Moreover, prior\nresearch on SAF algorithms has not effectively addressed the adverse effects\ncaused by outliers. To overcome these challenges, the generalized modified\nBlake-Zisserman robust spline adaptive filtering (SAF-GMBZ) algorithm is\nproposed. Compared to conventional SAF algorithms, SAF-GMBZ exhibits superior\nlearning performance in GGN. Furthermore, the mean convergence ranges of the\nstep-sizes and the steady-state mean-square error (MSE) are calculated by\nintroducing the commonly utilized assumptions. To arrive at good convergence\naccuracy and noise cancellation capability in active noise control (ANC)\napplication, the filter-c GMBZ (FcGMBZ) algorithm is further developed based on\nSAF-GMBZ. Simulation results confirm the accuracy of the theoretical\nsteady-state MSE, and the superiority of the SAF-GMBZ algorithm under GGN\nenvironment in NSI, along with the effectiveness of the FcGMBZ algorithm in ANC\napplication under impulsive noise environment."}
{"id": "2510.19267", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19267", "abs": "https://arxiv.org/abs/2510.19267", "authors": ["Shama Siddiqu", "Indrakshi Dey"], "title": "A Study on Delay Assessment for Heterogenous Traffic in VANET", "comment": "5th International Conference on Computing and Communication Networks\n  (ICCCNet-2025). Manchester, UK", "summary": "Vehicular Ad hoc Networks (VANETs) comprise of multi-priority hetero-genous\nnodes, both stationary and/or mobile. The data generated by these nodes may\ninclude messages relating to information, safety, entertainment, traffic\nmanagement and emergency alerts. The data in the network needs dif-ferentiated\nservice based on the priority/urgency. Media Access Control (MAC) protocols\nhold a significant value for managing the data priority. This paper studies a\ncomparison of 802.11p which is a standard PHY and MAC protocol for VANET with a\nfragmentation-based protocol, FROG-MAC. The major design principle of 802.11-p\nis to allow direct Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I)\ncommunication without associa-tion, using Enhanced Distributed Channel Access\n(EDCA) to prioritize safety-critical messages. However, if non-critical\nmessages already start to transmit, the nodes with critical data have to wait.\nFROG-MAC reduces this delay by transmitting normal packets in fragments with\nshort pauses between them, al-lowing urgent packets to access the channel\nduring these intervals. Simula-tions have been performed to assess the delay\nand throughput for high and low priority data. We report that FROG-MAC improves\nboth the performance parameters due to offering an early channel access to the\nemergency traffic."}
{"id": "2510.19269", "categories": ["eess.SP", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19269", "abs": "https://arxiv.org/abs/2510.19269", "authors": ["Anwar Ahmed Khan", "Shama Siddiqui", "Mehar Ullah", "Indrakshi Dey"], "title": "IoT-Enabled Sleep Monitoring and Cognitive Assessment for Evaluating Teacher Well-Being", "comment": "22nd Int. Conference on Networking, Sensing, and Control (ICNSC),\n  Oulu, Finland", "summary": "Sleep quality is an important indicator of the efficient cognitive function\nfor high school teachers. Due to the high work stress and multi-tasking\nexpectations, the teachers often face issues with their sleep quality and\ncognitive function, which has a clearly negative influence on their teaching\nabilities. In this work, we propose a unique but simple method of deploying\nInternet of Things (IoT) technology to monitor the sleep quality of high school\nteachers at Pakistan. Smart watches embedded with pulse rate and SpO2 sensors\nwere used to collect data and categorize the sleep quality as \"poor\", \"fair\" or\n\"good\". Moreover, we used a psychological tool, Cognitive Assessment\nQuestionnaire (CAQ) for the self-assessment of teachers' cognitive function.\nThe study was conducted over 208 high school teachers from across Pakistan. It\nhas been found that most of the teachers had a poor sleep quality and cognitive\nfunction; The link between these two variables indicate that the workload and\nother factors must be improved for the teachers to ensure their well-being,\nwhich will in turn have a positive impact on their teaching quality."}
{"id": "2510.19309", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19309", "abs": "https://arxiv.org/abs/2510.19309", "authors": ["Shreyan Banerjee", "Aasifa Rounak", "Cathal Hoare", "Denis Dowling", "Vikram Pakrashi"], "title": "Neuromorphic computing for anomaly detection in a laser powder bed fusion process", "comment": "Journal Article", "summary": "This study is the first application of spiking neural networks (SNNs) for\nanomaly detection in the Laser Powder Bed Fusion (LPBF) additive manufacturing\nprocess. The neural networks were used to identify print processing anomalies\ngenerated by dropping of laser energy during the printing of individual layers\nin a Ti-6Al-4V alloy lattice structures. Associated changes in the laser\ngenerated melt pool were observed using an in-process photodiode monitoring\ntechnique. photodiode sensors capturing plasma and infrared radiations\nreflected from the print bed of the metal 3D printer were utilized to detect\nsudden changes caused by anomalies during the printing process. The algorithm\nis first implemented on non-neuromorphic hardware including a central\nprocessing unit (CPU), on Field Programmable Gate Arrays (FPGA) and then on\nneuromorphic Intel's Loihi chip. Improved detection of anomalies is achieved by\nadjusting the spike latency of the neural network, which reduces masking of\ninformation by noise within the monitored temporal signal. The work\ndemonstrates the possibility of using low-power neuromorphic chips within an\nedge framework for anomaly detection in additive manufacturing and creates a\nframework for the process."}
{"id": "2510.19360", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19360", "abs": "https://arxiv.org/abs/2510.19360", "authors": ["Dongwon Kim", "Jiwan Seo", "Joonhyuk Kang"], "title": "Multi-code rate Task-Oriented Communication for Multi-Edge Cooperative Inference", "comment": null, "summary": "The integration of artificial intelligence (AI) with the internet of things\n(IoT) enables task-oriented communication for multi-edge cooperative inference\nsystem, where edge devices transmit extracted features of local sensory data to\nan edge server to perform AI-driven tasks. However, the privacy concerns and\nlimited communication bandwidth pose fundamental challenges, since simultaneous\ntransmission of extracted features with a single fixed compression ratio from\nall devices leads to severe inefficiency in communication resource utilization.\nTo address this challenge, we propose a framework that dynamically adjusts the\ncode rate in feature extraction based on its importance to the downstream\ninference task by adopting a rate-adaptive quantization (RAQ) scheme.\nFurthermore, to select the code rate for each edge device under limited\nbandwidth constraint, a dynamic programming (DP) approach is leveraged to\nallocate the code rate across discrete code rate options. Experiments on\nmulti-view datasets demonstrate that the proposed frameworks significantly\noutperform the frameworks using fixed-rate quantization, achieving a favorable\nbalance between communication efficiency and inference performance under\nlimited bandwidth conditions."}
{"id": "2510.19401", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19401", "abs": "https://arxiv.org/abs/2510.19401", "authors": ["Tao Zhou", "Liying Geng", "Yiqun Liang", "Kaifeng Bao", "Tianyun Feng", "Liu Liu", "Bo Ai"], "title": "Ray-Tracing Based Narrow-Beam Channel Simulation, Characterization and Performance Evaluation for 5G-R Systems", "comment": null, "summary": "This paper investigates narrow-beam channel characterization and performance\nevaluation for 5G for railway (5G-R) systems based on ray-tracing (RT)\nsimulation. Three representative high-speed railway (HSR) scenarios including\nviaduct, cutting, and station are established, and RT-based dynamic narrow-beam\nchannel simulations are conducted using a designed beam tracking scheme that\nensures continuous alignment with the moving train. The channel characteristics\nare analyzed in terms of both large-scale and small-scale fading, as well as\nnon-stationarity, providing statistical insights into path loss, shadow fading,\nfading severity, time-frequency-space dispersion, and stationarity interval.\nThe influence of beamwidth on these channel properties is also examined.\nFurthermore, the performance of 5G-R systems operating in such narrow-beam\nchannels is evaluated using the Vienna 5G simulator, with a focus on block\nerror rate, throughput, and spectral efficiency. A hardware-in-the-loop\nsimulation platform is developed to further assess synchronization signal\nreference signal received power, signal-to-interference-plus-noise ratio, and\nreference signal received quality. The results provide valuable guidance for\nthe design and optimization of 5G-R systems in HSR environments."}
{"id": "2510.19402", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19402", "abs": "https://arxiv.org/abs/2510.19402", "authors": ["Kaifeng Bao", "Tao Zhou", "Chaoyi Li", "Liu Liu", "Bo Ai"], "title": "A Novel Delay-Doppler Domain Channel Sounding Method for 6G High-Mobility Scenarios", "comment": "13 pages, 14 figures", "summary": "Channel measurements are the prerequisite for applying emerging transmission\ntechnologies and designing communication systems. In sixth-generation (6G)\nsystem, conventional time or frequency domain channel sounding methods cannot\ndirectly obtain Doppler information induced by high-mobility scenarios. The\nchannel spreading function (CSF) simultaneously captures delay and Doppler\ninformation, while naturally characterizing the propagation environment in the\ndelay-Doppler (DD) domain. However, DD domain channel sounding methods remain\nunderexplored. This paper presents a novel DD domain channel sounding method\nfor 6G high-mobility scenarios. First, we introduce the waveform design for the\nsounding signal and analyze its sounding capability. Next, the methodology of\nDD domain channel sounding, including synchronization and CSF estimation, is\nthoroughly detailed. Additionally, an algorithm for enhancing measurement\nprecision is proposed. The performance of the proposed method is rigorously\nevaluated. Subsequently, a DD domain channel sounding system competent for 6G\nhigh-mobility scenarios is established. Finally, DD domain channel measurements\nare conducted for a vehicle-to-infrastructure scenario in urban environments.\nMeasurement results, including CSF, power delay profile, Doppler power spectral\ndensity, number of multipath components, and other characteristics, are\nderived, which confirm the effectiveness of the proposed method and offer\nhelpful insights for advancing research on 6G high-mobility communications."}
{"id": "2510.19521", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19521", "abs": "https://arxiv.org/abs/2510.19521", "authors": ["Zexin Fang", "Bin Han", "Zhu Han", "Hans D. Schotten"], "title": "Network-Centric Anomaly Filtering and Spoofer localization for 5G-NR Localization in LAWNs", "comment": null, "summary": "This paper investigates security vulnerabilities and countermeasures for 3rd\nGeneration Partnership Project (3GPP) Fifth Generation New Radio (5G-NR) Time\nDifference of Arrival (TDoA)-based unmanned aerial vehicle (UAV) localization\nin low-altitude urban environments. We first optimize node selection strategies\nunder Air to Ground (A2G) channel conditions, proving that optimal selection\ndepends on UAV altitude and deployment density. We propose lightweight User\nEquipment (UE)-assisted that reduce overhead while enhancing accuracy. We then\nexpose critical security vulnerabilities by introducing merged-peak spoofing\nattacks where rogue UAVs transmit multiple lower-power pulses that merge with\nlegitimate signals, bypassing existing detection methods. Through theoretical\nmodeling and sensitivity analysis, we quantify how synchronization quality and\ngeometric factors determine spoofing success probability, revealing fundamental\nweaknesses in current 3GPP positioning frameworks. To address these\nvulnerabilities, we design a network-centric anomaly detection framework at the\nLocalization Management Function (LMF) using existing 3GPP-specified\nparameters, coupled with a recursive gradient descent-based robust localization\nalgorithm that filters anomaly data while estimating UAV position. Our unified\nframework simultaneously provides robust victim localization and spoofer\nlocalization-capabilities not integrated in existing literature. Extensive\nsimulations validate the effectiveness of both optimization and security\nmechanisms for 3GPP-compliant UAV positioning."}
{"id": "2510.19525", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19525", "abs": "https://arxiv.org/abs/2510.19525", "authors": ["Vincent Savaux", "Hyeon Seok Rou", "Zeping Sui", "Giuseppe Thadeu Freitas de Abreu", "Zilong Liu"], "title": "On the Robustness of AFDM and OTFS Against Passive Eavesdroppers", "comment": "5 pages, 3 figures", "summary": "We investigate the robustness of affine frequency division multiplexing\n(AFDM) and orthogonal time frequency space (OTFS) waveforms against passive\neavesdroppers performing brute-force demodulation to intercepted signals, under\nthe assumption that eavesdroppers have no knowledge of chirp parameters (in\nAFDM) or the delay-Doppler grid configuration (in OTFS), such that they must\nsearch exhaustively over possible demodulation matrices. Analytical results\nshow that the brute-force complexity scales as $\\mathcal{O}(\\sqrt{N})$ for OTFS\nand $\\mathcal{O}(N^2)$ for AFDM, where $N$ is the number of subcarriers,\nindicating that AFDM has superior resilience over OTFS. Bit error rate (BER)\nsimulations confirm the analysis by showing that, with AFDM, the signal remains\nnearly undecodable at the eavesdropper, while OTFS allows partial signal\nrecovery under equivalent conditions."}
{"id": "2510.19636", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19636", "abs": "https://arxiv.org/abs/2510.19636", "authors": ["Sahar Maleki", "Reza Lashgari", "Mahdi Aliyari Shoorehdeli", "Mohammad Komareji"], "title": "Multilayer Perceptron Neural Network Model: A Novel Approach for LFP Contrast Sensitivity Tuning", "comment": null, "summary": "Local field potentials (LFPs) have been demonstrated to be an important\nmeasurement to study the activity of a local population of neurons. The\nresponse tunings of LFPs have been mostly reported as weaker and broader than\nspike tunings. Therefore, selecting optimized tuning methods is essential for\nappropriately evaluating the LFP responses and comparing them with neighboring\nspiking activity. In this paper, new models for tuning of the contrast response\nfunctions (CRFs) are proposed. To this end, luminance contrast-evoked LFP\nresponses recorded in primate primary visual cortex (V1) are first analyzed.\nThen, supersaturating CRFs are distinguished from linear and saturating CRFs by\nusing monotonicity index (MI). The supersaturated recording data are then\nidentified through static identification methods including multilayer\nperceptron (MLP) neural network, radial basis function (RBF) neural network,\nfuzzy model, neuro-fuzzy model, and the local linear model tree (LOLIMOT)\nalgorithm. Our results demonstrate that the MLP neural network, compared to\ntraditional and modified hyperbolic Naka-Rushton functions, exhibits superior\nperformance in tuning the local field potential responses to luminance contrast\nstimuli, resulting in successful tuning of a significantly higher number of\nneural recordings of all three types. These results suggest that the MLP neural\nnetwork model can be used as a novel approach to measure a better fitted\ncontrast sensitivity tuning curve of a population of neurons than other\ncurrently used models."}
{"id": "2510.19639", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19639", "abs": "https://arxiv.org/abs/2510.19639", "authors": ["Chenxing Tan", "Yuguan Hou", "Hao Wang", "Zhonghao Yuan"], "title": "Micro-Doppler Energy-Based Robust Multi-Target Vital Signs Monitoring Using 77-GHz FMCW Radar with Spatiotemporal Adaptive Processing", "comment": null, "summary": "This paper presents a novel micro-Doppler energy-based framework for robust\nmulti-target vital signs monitoring using 77-GHz Frequency-Modulated\nContinuous-Wave (FMCW) radar. Unlike conventional phase-based methods that are\nsusceptible to environmental noise, random body movements, and stringent\ncalibration requirements, our approach exploits the energy variations in radar\nreturns induced by cardiopulmonary activities. The proposed system integrates a\ncomprehensive processing pipeline including space-time adaptive processing\n(STAP) for target detection and tracking, MUSIC algorithm for high-resolution\nangle estimation, and an innovative adaptive spectral filtering technique for\nvital signs extraction. We establish a rigorous mathematical framework that\nformalizes the relationship between micro-Doppler energy variations and\nphysiological activities, enabling robust separation of closely spaced targets.\nThe key innovation lies in the micro-Doppler energy extraction methodology that\nprovides inherent robustness to phase noise and motion artifacts. Experimental\nresults using millimeter-wave radar datasets demonstrate that the system can\naccurately detect and separate vital signs of up to four targets within\n\\SI{5}{\\meter} range, achieving mean absolute errors of \\SI{1.2}beats per\nminute and \\SI{2.3} beats per minute for respiration and heart rates,\nrespectively. The proposed approach demonstrates superior performance compared\nto traditional phase-based methods, particularly in challenging multi-target\nscenarios with environmental noise and subject movement."}
{"id": "2510.19775", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19775", "abs": "https://arxiv.org/abs/2510.19775", "authors": ["Ilija Tanasković", "Ljiljana B. Lazarević", "Goran Knežević", "Nikola Milosavljević", "Olga Dubljević", "Bojana Bjegojević", "Nadica Miljković"], "title": "Interpretable machine learning for cardiogram-based biometrics", "comment": null, "summary": "This study investigates the role of electrocardiogram (ECG) and impedance\ncardiogram (ICG) features in biometric identification, emphasizing their\ndiscriminative capacity and robustness to emotional variability. A total of 29\nfeatures spanning four domains (temporal, amplitude, slope, and morphological)\nare evaluated using random forest (RF) models combined with multiple\ninterpretability methods. Feature importance shows that both ECG- and\nICG-derived features are consistently ranked among the top 10 by Gini\nimportance, permutation importance, and SHAP values, with ECG features,\nparticularly QRS-centric descriptors, occupying the highest positions. In\nparallel, ICG BCX features contribute complementary, however, with lower\ncross-method stability. Correlation analysis reveals substantial\nmulticollinearity, where the RF distributes and diminishes importance across\nhighly correlated pairs, confirming reduced independent contributions.\nStatistical analysis identifies 14 features with significant differences\nbetween baseline and anger, without a clear pattern by domain. Feature\nselection with recursive feature elimination and genetic algorithms converges\non a subset (14 features) that attains accuracy within 1% of the full set\n(99%), improving efficiency in storage and computation. These complementary\nanalyses indicate that the individuality required for reliable identification\nis primarily encoded in QRS-related ECG features across all four domains.\nMeanwhile, BCX-derived ICG features contribute mainly through amplitude,\nproviding supportive but less stable discriminatory cues. The confirmed\nresilience of QRS-centric descriptors to emotional variation, where stable\ninter-individual differences in the QRS complex could be traced to variations\nin ventricular mass, conduction pathways, and thoracic geometry, may indicate\ntheir central role in future models of cardiogram-based identity."}
{"id": "2510.18917", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.18917", "abs": "https://arxiv.org/abs/2510.18917", "authors": ["Mandip Goswami"], "title": "RIR-Mega: a large-scale simulated room impulse response dataset for machine learning and room acoustics modeling", "comment": "8 pages, 3 figures", "summary": "Room impulse responses are a core resource for dereverberation, robust speech\nrecognition, source localization, and room acoustics estimation. We present\nRIR-Mega, a large collection of simulated RIRs described by a compact, machine\nfriendly metadata schema and distributed with simple tools for validation and\nreuse. The dataset ships with a Hugging Face Datasets loader, scripts for\nmetadata checks and checksums, and a reference regression baseline that\npredicts RT60 like targets from waveforms. On a train and validation split of\n36,000 and 4,000 examples, a small Random Forest on lightweight time and\nspectral features reaches a mean absolute error near 0.013 s and a root mean\nsquare error near 0.022 s. We host a subset with 1,000 linear array RIRs and\n3,000 circular array RIRs on Hugging Face for streaming and quick tests, and\npreserve the complete 50,000 RIR archive on Zenodo. The dataset and code are\npublic to support reproducible studies."}
{"id": "2510.19174", "categories": ["eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.19174", "abs": "https://arxiv.org/abs/2510.19174", "authors": ["Yuanming Zhang", "Zeyan Song", "Jing Lu", "Fei Chen", "Zhibin Lin"], "title": "Auditory Attention Decoding from Ear-EEG Signals: A Dataset with Dynamic Attention Switching and Rigorous Cross-Validation", "comment": null, "summary": "Recent promising results in auditory attention decoding (AAD) using scalp\nelectroencephalography (EEG) have motivated the exploration of cEEGrid, a\nflexible and portable ear-EEG system. While prior cEEGrid-based studies have\nconfirmed the feasibility of AAD, they often neglect the dynamic nature of\nattentional states in real-world contexts. To address this gap, a novel cEEGrid\ndataset featuring three concurrent speakers distributed across three of five\ndistinct spatial locations is introduced. The novel dataset is designed to\nprobe attentional tracking and switching in realistic scenarios. Nested\nleave-one-out validation-an approach more rigorous than conventional\nsingle-loop leave-one-out validation-is employed to reduce biases stemming from\nEEG's intricate temporal dynamics. Four rule-based models are evaluated: Wiener\nfilter (WF), canonical component analysis (CCA), common spatial pattern (CSP)\nand Riemannian Geometry-based classifier (RGC). With a 30-second decision\nwindow, WF and CCA models achieve decoding accuracies of 41.5% and 41.4%,\nrespectively, while CSP and RGC models yield 37.8% and 37.6% accuracies using a\n10-second window. Notably, both WF and CCA successfully track attentional state\nswitches across all experimental tasks. Additionally, higher decoding\naccuracies are observed for electrodes positioned at the upper cEEGrid layout\nand near the listener's right ear. These findings underscore the utility of\ndynamic, ecologically valid paradigms and rigorous validation in advancing AAD\nresearch with cEEGrid."}

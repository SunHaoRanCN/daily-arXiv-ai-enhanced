{"id": "2510.21115", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21115", "abs": "https://arxiv.org/abs/2510.21115", "authors": ["Yihan Wu", "Georgios Milis", "Ruibo Chen", "Heng Huang"], "title": "Robust Distortion-Free Watermark for Autoregressive Audio Generation Models", "comment": null, "summary": "The rapid advancement of next-token-prediction models has led to widespread\nadoption across modalities, enabling the creation of realistic synthetic media.\nIn the audio domain, while autoregressive speech models have propelled\nconversational interactions forward, the potential for misuse, such as\nimpersonation in phishing schemes or crafting misleading speech recordings, has\nalso increased. Security measures such as watermarking have thus become\nessential to ensuring the authenticity of digital media. Traditional\nstatistical watermarking methods used for autoregressive language models face\nchallenges when applied to autoregressive audio models, due to the inevitable\n``retokenization mismatch'' - the discrepancy between original and retokenized\ndiscrete audio token sequences. To address this, we introduce Aligned-IS, a\nnovel, distortion-free watermark, specifically crafted for audio generation\nmodels. This technique utilizes a clustering approach that treats tokens within\nthe same cluster equivalently, effectively countering the retokenization\nmismatch issue. Our comprehensive testing on prevalent audio generation\nplatforms demonstrates that Aligned-IS not only preserves the quality of\ngenerated audio but also significantly improves the watermark detectability\ncompared to the state-of-the-art distortion-free watermarking adaptations,\nestablishing a new benchmark in secure audio technology applications."}
{"id": "2510.20850", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.20850", "abs": "https://arxiv.org/abs/2510.20850", "authors": ["Chibuzor Okocha", "Maya Bakri", "Christan Grant"], "title": "Can large audio language models understand child stuttering speech? speech summarization, and source separation", "comment": "7 pages, 1 Figure, 8 tables, Under review ICASSP 2026", "summary": "Child speech differs from adult speech in acoustics, prosody, and language\ndevelopment, and disfluencies (repetitions, prolongations, blocks) further\nchallenge Automatic Speech Recognition (ASR) and downstream Natural Language\nProcessing (NLP). Recent large audio-language models (LALMs) demonstrate strong\ncross-modal audio understanding; however, their behavior in disfluent child\nspeech remains underexplored. We evaluate several state-of-the-art LALMs in two\nsettings: an interview (mixed speakers) and a reading task (single child). The\ntasks are (i) single-channel source separation to isolate the child and (ii)\nchild-only summarization that preserves clinically relevant disfluencies and\navoids adult-speech leakage.\n  Evaluation combines Large Language Model (LLM) as a judge, human expert\nratings, and BERTScore (F1), and we report agreement between models and between\nmodels and humans to assess reliability. Our findings delineate the conditions\nunder which LALMs produce faithful child-only summaries from mixed audio and\nwhere they fail, offering practical guidance for clinical and educational\ndeployments. We provide prompts and evaluation scripts to support replication."}
{"id": "2510.21257", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21257", "abs": "https://arxiv.org/abs/2510.21257", "authors": ["Shivam Saini", "Jürgen Peissig"], "title": "HiFi-HARP: A High-Fidelity 7th-Order Ambisonic Room Impulse Response Dataset", "comment": "Under review for ICASSP 2026", "summary": "We introduce HiFi-HARP, a large-scale dataset of 7th-order Higher-Order\nAmbisonic Room Impulse Responses (HOA-RIRs) consisting of more than 100,000\nRIRs generated via a hybrid acoustic simulation in realistic indoor scenes.\nHiFi-HARP combines geometrically complex, furnished room models from the\n3D-FRONT repository with a hybrid simulation pipeline: low-frequency wave-based\nsimulation (finite-difference time-domain) up to 900 Hz is used, while high\nfrequencies above 900 Hz are simulated using a ray-tracing approach. The\ncombined raw RIRs are encoded into the spherical-harmonic domain (AmbiX ACN)\nfor direct auralization. Our dataset extends prior work by providing 7th-order\nAmbisonic RIRs that combine wave-theoretic accuracy with realistic room\ncontent. We detail the generation pipeline (scene and material selection, array\ndesign, hybrid simulation, ambisonic encoding) and provide dataset statistics\n(room volumes, RT60 distributions, absorption properties). A comparison table\nhighlights the novelty of HiFi-HARP relative to existing RIR collections.\nFinally, we outline potential benchmarks such as FOA-to-HOA upsampling, source\nlocalization, and dereverberation. We discuss machine learning use cases\n(spatial audio rendering, acoustic parameter estimation) and limitations (e.g.,\nsimulation approximations, static scenes). Overall, HiFi-HARP offers a rich\nresource for developing spatial audio and acoustics algorithms in complex\nenvironments."}
{"id": "2510.20853", "categories": ["eess.AS", "cs.CL", "cs.SD", "68T01"], "pdf": "https://arxiv.org/pdf/2510.20853", "abs": "https://arxiv.org/abs/2510.20853", "authors": ["Hyungjun Yoon", "Seungjoo Lee", "Yu Yvonne Wu", "Xiaomeng Chen", "Taiting Lu", "Freddy Yifei Liu", "Taeckyung Lee", "Hyeongheon Cha", "Haochen Zhao", "Gaoteng Zhao", "Sung-Ju Lee", "Cecilia Mascolo", "Dongyao Chen", "Lili Qiu"], "title": "Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization", "comment": "19 pages, 9 figures", "summary": "Electrophysiological (ExG) signals offer valuable insights into human\nphysiology, yet building foundation models that generalize across everyday\ntasks remains challenging due to two key limitations: (i) insufficient data\ndiversity, as most ExG recordings are collected in controlled labs with bulky,\nexpensive devices; and (ii) task-specific model designs that require tailored\nprocessing (i.e., targeted frequency filters) and architectures, which limit\ngeneralization across tasks. To address these challenges, we introduce an\napproach for scalable, task-agnostic ExG monitoring in the wild. We collected\n50 hours of unobtrusive free-living ExG data with an earphone-based hardware\nprototype to narrow the data diversity gap. At the core of our approach is\nPhysiology-informed Multi-band Tokenization (PiMT), which decomposes ExG\nsignals into 12 physiology-informed tokens, followed by a reconstruction task\nto learn robust representations. This enables adaptive feature recognition\nacross the full frequency spectrum while capturing task-relevant information.\nExperiments on our new DailySense dataset-the first to enable ExG-based\nanalysis across five human senses-together with four public ExG benchmarks,\ndemonstrate that PiMT consistently outperforms state-of-the-art methods across\ndiverse tasks."}
{"id": "2510.21485", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21485", "abs": "https://arxiv.org/abs/2510.21485", "authors": ["Yoshiki Masuyama", "Kohei Saijo", "Francesco Paissan", "Jiangyu Han", "Marc Delcroix", "Ryo Aihara", "François G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "FlexIO: Flexible Single- and Multi-Channel Speech Separation and Enhancement", "comment": "Submitted to ICASSP 2026", "summary": "Speech separation and enhancement (SSE) has advanced remarkably and achieved\npromising results in controlled settings, such as a fixed number of speakers\nand a fixed array configuration. Towards a universal SSE system, single-channel\nsystems have been extended to deal with a variable number of speakers (i.e.,\noutputs). Meanwhile, multi-channel systems accommodating various array\nconfigurations (i.e., inputs) have been developed. However, these attempts have\nbeen pursued separately. In this paper, we propose a flexible input and output\nSSE system, named FlexIO. It performs conditional separation using prompt\nvectors, one per speaker as a condition, allowing separation of an arbitrary\nnumber of speakers. Multi-channel mixtures are processed together with the\nprompt vectors via an array-agnostic channel communication mechanism. Our\nexperiments demonstrate that FlexIO successfully covers diverse conditions with\none to five microphones and one to three speakers. We also confirm the\nrobustness of FlexIO on CHiME-4 real data."}
{"id": "2510.20860", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20860", "abs": "https://arxiv.org/abs/2510.20860", "authors": ["Vishaal Udandarao", "Zhiyun Lu", "Xuankai Chang", "Yongqiang Wang", "Violet Z. Yao", "Albin Madapally Jose", "Fartash Faghri", "Josh Gardner", "Chung-Cheng Chiu"], "title": "Data-Centric Lessons To Improve Speech-Language Pretraining", "comment": "Tech Report", "summary": "Spoken Question-Answering (SQA) is a core capability for useful and\ninteractive artificial intelligence systems. Recently, several speech-language\nmodels (SpeechLMs) have been released with a specific focus on improving their\nSQA performance. However, a lack of controlled ablations of pretraining data\nprocessing and curation makes it challenging to understand what factors account\nfor performance, despite substantial gains from similar studies in other data\nmodalities. In this work, we address this gap by conducting a data-centric\nexploration for pretraining SpeechLMs. We focus on three research questions\nfundamental to speech-language pretraining data: (1) how to process raw\nweb-crawled audio content for speech-text pretraining, (2) how to construct\nsynthetic pretraining datasets to augment web-crawled data and (3) how to\ninterleave (text, audio) segments into training sequences. We apply the\ninsights from our controlled data-centric ablations to pretrain a\n3.8B-parameter SpeechLM, called SpeLangy, that outperforms models that are up\nto 3x larger by 10.2% absolute performance. We hope our findings highlight the\nimpact of effective data curation for speech-language pretraining and guide\nfuture data-centric exploration in SpeechLMs."}
{"id": "2510.21659", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21659", "abs": "https://arxiv.org/abs/2510.21659", "authors": ["Yongyi Zang", "Chris Manchester", "David Young", "Ivan Ivanov", "Jeffrey Lufkin", "Martin Vladimirov", "PJ Solomon", "Svetoslav Kepchelev", "Fei Yueh Chen", "Dongting Cai", "Teodor Naydenov", "Randal Leistikow"], "title": "Smule Renaissance Small: Efficient General-Purpose Vocal Restoration", "comment": "Technical Report", "summary": "Vocal recordings on consumer devices commonly suffer from multiple concurrent\ndegradations: noise, reverberation, band-limiting, and clipping. We present\nSmule Renaissance Small (SRS), a compact single-stage model that performs\nend-to-end vocal restoration directly in the complex STFT domain. By\nincorporating phase-aware losses, SRS enables large analysis windows for\nimproved frequency resolution while achieving 10.5x real-time inference on\niPhone 12 CPU at 48 kHz. On the DNS 5 Challenge blind set, despite no speech\ntraining, SRS outperforms a strong GAN baseline and closely matches a\ncomputationally expensive flow-matching system. To enable evaluation under\nrealistic multi-degradation scenarios, we introduce the Extreme Degradation\nBench (EDB): 87 singing and speech recordings captured under severe acoustic\nconditions. On EDB, SRS surpasses all open-source baselines on singing and\nmatches commercial systems, while remaining competitive on speech despite no\nspeech-specific training. We release both SRS and EDB under the MIT License."}
{"id": "2510.21014", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21014", "abs": "https://arxiv.org/abs/2510.21014", "authors": ["Ari Frummer", "Helin Wang", "Tianyu Cao", "Adi Arbel", "Yuval Sieradzki", "Oren Gal", "Jesús Villalba", "Thomas Thebaud", "Najim Dehak"], "title": "refess-qi: reference-free evaluation for speech separation with joint quality and intelligibility scoring", "comment": null, "summary": "Source separation is a crucial pre-processing step for various speech\nprocessing tasks, such as automatic speech recognition (ASR). Traditionally,\nthe evaluation metrics for speech separation rely on the matched reference\naudios and corresponding transcriptions to assess audio quality and\nintelligibility. However, they cannot be used to evaluate real-world mixtures\nfor which no reference exists. This paper introduces a text-free reference-free\nevaluation framework based on self-supervised learning (SSL) representations.\nThe proposed framework utilize the mixture and separated tracks to predict\njointly audio quality, through the Scale Invariant Signal to Noise Ratio\n(SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER)\nmetric. We conducted experiments on the WHAMR! dataset, which shows a WER\nestimation with a mean absolute error (MAE) of 17\\% and a Pearson correlation\ncoefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of\n0.95. We further demonstrate the robustness of our estimator by using various\nSSL representations."}
{"id": "2510.20828", "categories": ["eess.SP", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20828", "abs": "https://arxiv.org/abs/2510.20828", "authors": ["Dixon Vimalajeewa", "Ursula U. Muller", "Brani Vidakovic"], "title": "A Multiscale Approach for Enhancing Weak Signal Detection", "comment": null, "summary": "Stochastic resonance (SR), a phenomenon originally introduced in climate\nmodeling, enhances signal detection by leveraging optimal noise levels within\nnon-linear systems. Traditional SR techniques, mainly based on single-threshold\ndetectors, are limited to signals whose behavior does not depend on time. Often\nlarge amounts of noise are needed to detect weak signals, which can distort\ncomplex signal characteristics. To address these limitations, this study\nexplores multi-threshold systems and the application of SR in multiscale\napplications using wavelet transforms. In the multiscale domain signals can be\nanalyzed at different levels of resolution to better understand the underlying\ndynamics.\n  We propose a double-threshold detection system that integrates two\nsingle-threshold detectors to enhance weak signal detection. We evaluate it\nboth in the original data domain and in the multiscale domain using simulated\nand real-world signals and compare its performance with existing methods.\n  Experimental results demonstrate that, in the original data domain, the\nproposed double-threshold detector significantly improves weak signal detection\ncompared to conventional single-threshold approaches. Its performance is\nfurther improved in the frequency domain, requiring lower noise levels while\noutperforming existing detection systems. This study advances SR-based\ndetection methodologies by introducing a robust approach to weak signal\nidentification, with potential applications in various disciplines."}
{"id": "2510.21667", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21667", "abs": "https://arxiv.org/abs/2510.21667", "authors": ["Qihui Yang", "Randal Leistikow", "Yongyi Zang"], "title": "FlowSynth: Instrument Generation Through Distributional Flow Matching and Test-Time Search", "comment": "Submitted to ICASSP 2026", "summary": "Virtual instrument generation requires maintaining consistent timbre across\ndifferent pitches and velocities, a challenge that existing note-level models\nstruggle to address. We present FlowSynth, which combines distributional flow\nmatching (DFM) with test-time optimization for high-quality instrument\nsynthesis. Unlike standard flow matching that learns deterministic mappings,\nDFM parameterizes the velocity field as a Gaussian distribution and optimizes\nvia negative log-likelihood, enabling the model to express uncertainty in its\npredictions. This probabilistic formulation allows principled test-time search:\nwe sample multiple trajectories weighted by model confidence and select outputs\nthat maximize timbre consistency. FlowSynth outperforms the current\nstate-of-the-art TokenSynth baseline in both single-note quality and cross-note\nconsistency. Our approach demonstrates that modeling predictive uncertainty in\nflow matching, combined with music-specific consistency objectives, provides an\neffective path to professional-quality virtual instruments suitable for\nreal-time performance."}
{"id": "2510.21196", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21196", "abs": "https://arxiv.org/abs/2510.21196", "authors": ["Zixiang Wan", "Haoran Zhao", "Guochang Zhang", "Runqiang Han", "Jianqiang Wei", "Yuexian Zou"], "title": "PhoenixCodec: Taming Neural Speech Coding for Extreme Low-Resource Scenarios", "comment": "5 pages, 1 figure, 4 tables", "summary": "This paper presents PhoenixCodec, a comprehensive neural speech coding and\ndecoding framework designed for extremely low-resource conditions. The proposed\nsystem integrates an optimized asymmetric frequency-time architecture, a\nCyclical Calibration and Refinement (CCR) training strategy, and a\nnoise-invariant fine-tuning procedure. Under stringent constraints -\ncomputation below 700 MFLOPs, latency less than 30 ms, and dual-rate support at\n1 kbps and 6 kbps - existing methods face a trade-off between efficiency and\nquality. PhoenixCodec addresses these challenges by alleviating the resource\nscattering of conventional decoders, employing CCR to escape local optima, and\nenhancing robustness through noisy-sample fine-tuning. In the LRAC 2025\nChallenge Track 1, the proposed system ranked third overall and demonstrated\nthe best performance at 1 kbps in both real-world noise and reverberation and\nintelligibility in clean tests, confirming its effectiveness."}
{"id": "2510.20998", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.20998", "abs": "https://arxiv.org/abs/2510.20998", "authors": ["Martin Andersson", "Anubhab Chowdhury", "Erik G. Larsson"], "title": "Is Repeater-Assisted Massive MIMO Compatible with Dynamic TDD?", "comment": "Submitted to 2026 IEEE ICASSP", "summary": "We present a framework for joint amplification and phase shift optimization\nof the repeater gain in dynamic time-division duplex (TDD) repeater-assisted\nmassive MIMO networks. Repeaters, being active scatterers with amplification\nand phase shift, enhance the received signal strengths for users. However, they\ninevitably also amplify undesired noise and interference signals, which become\nparticularly prominent in dynamic TDD systems due to the concurrent downlink\n(DL) and uplink (UL) transmissions, introducing cross-link interference among\naccess points and users operating in opposite transmit directions. This causes\na non-trivial trade-off between amplification of desired and undesired signals.\nTo underpin the conditions under which such a trade-off can improve\nperformance, we first derive DL and UL spectral efficiencies (SEs), and then\ndevelop a repeater gain optimization algorithm for SE maximization.\nNumerically, we show that our proposed algorithm successfully calibrates the\nrepeater gain to amplify the desired signal while limiting the interference."}
{"id": "2510.21685", "categories": ["cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21685", "abs": "https://arxiv.org/abs/2510.21685", "authors": ["Jingyue Huang", "Qihui Yang", "Fei Yueh Chen", "Julian McAuley", "Randal Leistikow", "Perry R. Cook", "Yongyi Zang"], "title": "StylePitcher: Generating Style-Following and Expressive Pitch Curves for Versatile Singing Tasks", "comment": "Submitted to ICASSP 2026", "summary": "Existing pitch curve generators face two main challenges: they often neglect\nsinger-specific expressiveness, reducing their ability to capture individual\nsinging styles. And they are typically developed as auxiliary modules for\nspecific tasks such as pitch correction, singing voice synthesis, or voice\nconversion, which restricts their generalization capability. We propose\nStylePitcher, a general-purpose pitch curve generator that learns singer style\nfrom reference audio while preserving alignment with the intended melody. Built\nupon a rectified flow matching architecture, StylePitcher flexibly incorporates\nsymbolic music scores and pitch context as conditions for generation, and can\nseamlessly adapt to diverse singing tasks without retraining. Objective and\nsubjective evaluations across various singing tasks demonstrate that\nStylePitcher improves style similarity and audio quality while maintaining\npitch accuracy comparable to task-specific baselines."}
{"id": "2510.21209", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21209", "abs": "https://arxiv.org/abs/2510.21209", "authors": ["Zixiang Wan", "Guochang Zhang", "Yifeng He", "Jianqiang Wei"], "title": "SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum Domain", "comment": "Accepted by Interspeech 2025; 5 pages, 1 figure, 5 tables", "summary": "Neural Audio Codecs (NACs) have gained growing attention in recent years as\ntechnologies for audio compression and audio representation in speech language\nmodels. While mainstream NACs typically require G-level computation and M-level\nparameters, the performance of lightweight and streaming NACs remains\nunderexplored. This paper proposes SpecTokenizer, a lightweight streaming codec\nthat operates in the compressed spectral domain. Composed solely of alternating\nCNN and RNN layers, SpecTokenizer achieves greater efficiency and better\nrepresentational capability through multi-scale modeling in the compressed\nspectrum domain. At 4 kbps, the proposed SpecTokenizer achieves comparable or\nsuperior performance compared to the codec with state-of-the-art lightweight\narchitecture while requiring only 20% of the computation and 10% of the\nparameters. Furthermore, it significantly outperforms the codec when using\nsimilar computational and storage resources."}
{"id": "2510.21137", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21137", "abs": "https://arxiv.org/abs/2510.21137", "authors": ["Zhonglun Wang", "Yizhe Zhao", "Gangming Hu", "Yali Zheng", "Kun Yang"], "title": "6D Movable Holographic Surface Assisted Integrated Data and Energy Transfer: A Sensing Enhanced Approach", "comment": null, "summary": "Reconfigurable holographic surface (RHS) enables cost-effective large-scale\narrays with high spatial gain. However, its amplitude-controlled holographic\nbeamforming suffers from directional fluctuations, making it difficult to fully\nexploit the spatial gain of RHS. Fortunately, the promising 6D movable antenna\n(6DMA) provides a potential solution to this problem. In this paper, we study a\n6D movable holographic surface (6DMHS) integrated data and energy transfer\n(IDET) system, where a three-stage protocol is proposed, consisting of an\nuplink sensing stage, an orientation adjustment stage and a downlink\ntransmission stage, to coordinate the 6DMHS and effectively serve the IDET\nreceivers. Firstly, the holographic-based sensing technology is proposed and\nthe sensing information of the IDET receivers is exploited. Secondly, by fixing\nthe rotations with the sensing information, the orientation optimization\nproblem is formulated for designing the holographic beamforming of the RHS and\nadjusting the translations of the 6DMHS. As a result, the directions with\nmaximum beamforming gain are aligned with each IDET receiver. Thirdly, by\nfixing the orientation of the 6DMHS and the holographic beamforming, the\nequivalent wireless channel is obtained. The IDET performance optimization\nproblem is formulated for obtaining the optimal digital beamforming, power\nsplitting factor and energy harvesting (EH) power. Simulation results\ndemonstrate that the proposed scheme is capable of improving the IDET\nperformance compared to the benchmarks."}
{"id": "2510.20850", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.20850", "abs": "https://arxiv.org/abs/2510.20850", "authors": ["Chibuzor Okocha", "Maya Bakri", "Christan Grant"], "title": "Can large audio language models understand child stuttering speech? speech summarization, and source separation", "comment": "7 pages, 1 Figure, 8 tables, Under review ICASSP 2026", "summary": "Child speech differs from adult speech in acoustics, prosody, and language\ndevelopment, and disfluencies (repetitions, prolongations, blocks) further\nchallenge Automatic Speech Recognition (ASR) and downstream Natural Language\nProcessing (NLP). Recent large audio-language models (LALMs) demonstrate strong\ncross-modal audio understanding; however, their behavior in disfluent child\nspeech remains underexplored. We evaluate several state-of-the-art LALMs in two\nsettings: an interview (mixed speakers) and a reading task (single child). The\ntasks are (i) single-channel source separation to isolate the child and (ii)\nchild-only summarization that preserves clinically relevant disfluencies and\navoids adult-speech leakage.\n  Evaluation combines Large Language Model (LLM) as a judge, human expert\nratings, and BERTScore (F1), and we report agreement between models and between\nmodels and humans to assess reliability. Our findings delineate the conditions\nunder which LALMs produce faithful child-only summaries from mixed audio and\nwhere they fail, offering practical guidance for clinical and educational\ndeployments. We provide prompts and evaluation scripts to support replication."}
{"id": "2510.21280", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.21280", "abs": "https://arxiv.org/abs/2510.21280", "authors": ["Christiaan M. Geldenhuys", "Günther Tonitz", "Thomas R. Niesler"], "title": "WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation", "comment": null, "summary": "While recent sound event detection (SED) systems can identify baleen whale\ncalls in marine audio, challenges related to false positive and minority-class\ndetection persist. We propose the boundary proposal network (BPN), which\nextends an existing lightweight SED system. The BPN is inspired by work in\nimage object detection and aims to reduce the number of false positive\ndetections. It achieves this by using intermediate latent representations\ncomputed within the backbone classification model to gate the final output.\nWhen added to an existing SED system, the BPN achieves a 16.8 % absolute\nincrease in precision, as well as 21.3 % and 9.4 % improvements in the F1-score\nfor minority-class d-calls and bp-calls, respectively. We further consider two\napproaches to the selection of post-processing hyperparameters: a\nforward-search and a backward-search. By separately optimising event-level and\nframe-level hyperparameters, these two approaches lead to considerable\nperformance improvements over parameters selected using empirical methods. The\ncomplete WhaleVAD-BPN system achieves a cross-validated development F1-score of\n0.475, which is a 9.8 % absolute improvement over the baseline."}
{"id": "2510.21278", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.21278", "abs": "https://arxiv.org/abs/2510.21278", "authors": ["Laura M. Wolf", "Vincent Albert Wolff", "Simon Steuernagel", "Kolja Thormann", "Marcus Baum"], "title": "Track-to-Track Association for Collective Perception based on Stochastic Optimization", "comment": null, "summary": "Collective perception is a key aspect for autonomous driving in smart cities\nas it aims to combine the local environment models of multiple intelligent\nvehicles in order to overcome sensor limitations. A crucial part of\nmulti-sensor fusion is track-to-track association. Previous works often suffer\nfrom high computational complexity or are based on heuristics. We propose an\nassociation algorithms based on stochastic optimization, which leverages a\nmultidimensional likelihood incorporating the number of tracks and their\nspatial distribution and furthermore computes several association hypotheses.\nWe demonstrate the effectiveness of our approach in Monte Carlo simulations and\na realistic collective perception scenario computing high-likelihood\nassociations in ambiguous settings."}
{"id": "2510.20853", "categories": ["eess.AS", "cs.CL", "cs.SD", "68T01"], "pdf": "https://arxiv.org/pdf/2510.20853", "abs": "https://arxiv.org/abs/2510.20853", "authors": ["Hyungjun Yoon", "Seungjoo Lee", "Yu Yvonne Wu", "Xiaomeng Chen", "Taiting Lu", "Freddy Yifei Liu", "Taeckyung Lee", "Hyeongheon Cha", "Haochen Zhao", "Gaoteng Zhao", "Sung-Ju Lee", "Cecilia Mascolo", "Dongyao Chen", "Lili Qiu"], "title": "Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization", "comment": "19 pages, 9 figures", "summary": "Electrophysiological (ExG) signals offer valuable insights into human\nphysiology, yet building foundation models that generalize across everyday\ntasks remains challenging due to two key limitations: (i) insufficient data\ndiversity, as most ExG recordings are collected in controlled labs with bulky,\nexpensive devices; and (ii) task-specific model designs that require tailored\nprocessing (i.e., targeted frequency filters) and architectures, which limit\ngeneralization across tasks. To address these challenges, we introduce an\napproach for scalable, task-agnostic ExG monitoring in the wild. We collected\n50 hours of unobtrusive free-living ExG data with an earphone-based hardware\nprototype to narrow the data diversity gap. At the core of our approach is\nPhysiology-informed Multi-band Tokenization (PiMT), which decomposes ExG\nsignals into 12 physiology-informed tokens, followed by a reconstruction task\nto learn robust representations. This enables adaptive feature recognition\nacross the full frequency spectrum while capturing task-relevant information.\nExperiments on our new DailySense dataset-the first to enable ExG-based\nanalysis across five human senses-together with four public ExG benchmarks,\ndemonstrate that PiMT consistently outperforms state-of-the-art methods across\ndiverse tasks."}
{"id": "2510.21317", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21317", "abs": "https://arxiv.org/abs/2510.21317", "authors": ["Danilo de Oliveira", "Tal Peer", "Jonas Rochdi", "Timo Gerkmann"], "title": "Are These Even Words? Quantifying the Gibberishness of Generative Speech Models", "comment": null, "summary": "Significant research efforts are currently being dedicated to non-intrusive\nquality and intelligibility assessment, especially given how it enables\ncuration of large scale datasets of in-the-wild speech data. However, with the\nincreasing capabilities of generative models to synthesize high quality speech,\nnew types of artifacts become relevant, such as generative hallucinations.\nWhile intrusive metrics are able to spot such sort of discrepancies from a\nreference signal, it is not clear how current non-intrusive methods react to\nhigh-quality phoneme confusions or, more extremely, gibberish speech. In this\npaper we explore how to factor in this aspect under a fully unsupervised\nsetting by leveraging language models. Additionally, we publish a dataset of\nhigh-quality synthesized gibberish speech for further development of measures\nto assess implausible sentences in spoken language, alongside code for\ncalculating scores from a variety of speech language models."}
{"id": "2510.21378", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21378", "abs": "https://arxiv.org/abs/2510.21378", "authors": ["Biao Dong", "Bin Cao"], "title": "Optimized Power Control for Multi-User Integrated Sensing and Edge AI", "comment": null, "summary": "This work investigates an integrated sensing and edge artificial intelligence\n(ISEA) system, where multiple devices first transmit probing signals for target\nsensing and then offload locally extracted features to the access point (AP)\nvia analog over-the-air computation (AirComp) for collaborative inference. To\ncharacterize the relationship between AirComp error and inference performance,\ntwo proxies are established: the \\emph{computation-optimal} proxy that\nminimizes the aggregation distortion, and the \\emph{decision-optimal} proxy\nthat maximizes the inter-class separability, respectively. Optimal transceiver\ndesigns in terms of closed-form power allocation are derived for both\ntime-division multiplexing (TDM) and frequency-division multiplexing (FDM)\nsettings, revealing threshold-based and dual-decomposition structures,\nrespectively. Experimental results validate the theoretical findings."}
{"id": "2510.21014", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21014", "abs": "https://arxiv.org/abs/2510.21014", "authors": ["Ari Frummer", "Helin Wang", "Tianyu Cao", "Adi Arbel", "Yuval Sieradzki", "Oren Gal", "Jesús Villalba", "Thomas Thebaud", "Najim Dehak"], "title": "refess-qi: reference-free evaluation for speech separation with joint quality and intelligibility scoring", "comment": null, "summary": "Source separation is a crucial pre-processing step for various speech\nprocessing tasks, such as automatic speech recognition (ASR). Traditionally,\nthe evaluation metrics for speech separation rely on the matched reference\naudios and corresponding transcriptions to assess audio quality and\nintelligibility. However, they cannot be used to evaluate real-world mixtures\nfor which no reference exists. This paper introduces a text-free reference-free\nevaluation framework based on self-supervised learning (SSL) representations.\nThe proposed framework utilize the mixture and separated tracks to predict\njointly audio quality, through the Scale Invariant Signal to Noise Ratio\n(SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER)\nmetric. We conducted experiments on the WHAMR! dataset, which shows a WER\nestimation with a mean absolute error (MAE) of 17\\% and a Pearson correlation\ncoefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of\n0.95. We further demonstrate the robustness of our estimator by using various\nSSL representations."}
{"id": "2510.21388", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21388", "abs": "https://arxiv.org/abs/2510.21388", "authors": ["Arshdeep Singh", "Vinayak Abrol", "Mark D. Plumbley"], "title": "Compressing Quaternion Convolutional Neural Networks for Audio Classification", "comment": "Under review in IEEE TASLPRO", "summary": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition."}
{"id": "2510.21509", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21509", "abs": "https://arxiv.org/abs/2510.21509", "authors": ["Carmen Álvarez Roa", "Yunus Can Gültekin", "Vincent van Vliet", "Menno van den Hout", "Chigo Okonkwo", "Alex Alvarado"], "title": "On Irradiance Distributions for Weakly Turbulent FSO Links: Log-Normal vs. Gamma-Gamma", "comment": null, "summary": "Weak turbulence is commonly modeled using the log-normal distribution. Our\nexperimental results show that this distribution fails to capture irradiance\nfluctuations in this regime. The Gamma-Gamma model is shown to be more\naccurate."}
{"id": "2510.21196", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21196", "abs": "https://arxiv.org/abs/2510.21196", "authors": ["Zixiang Wan", "Haoran Zhao", "Guochang Zhang", "Runqiang Han", "Jianqiang Wei", "Yuexian Zou"], "title": "PhoenixCodec: Taming Neural Speech Coding for Extreme Low-Resource Scenarios", "comment": "5 pages, 1 figure, 4 tables", "summary": "This paper presents PhoenixCodec, a comprehensive neural speech coding and\ndecoding framework designed for extremely low-resource conditions. The proposed\nsystem integrates an optimized asymmetric frequency-time architecture, a\nCyclical Calibration and Refinement (CCR) training strategy, and a\nnoise-invariant fine-tuning procedure. Under stringent constraints -\ncomputation below 700 MFLOPs, latency less than 30 ms, and dual-rate support at\n1 kbps and 6 kbps - existing methods face a trade-off between efficiency and\nquality. PhoenixCodec addresses these challenges by alleviating the resource\nscattering of conventional decoders, employing CCR to escape local optima, and\nenhancing robustness through noisy-sample fine-tuning. In the LRAC 2025\nChallenge Track 1, the proposed system ranked third overall and demonstrated\nthe best performance at 1 kbps in both real-world noise and reverberation and\nintelligibility in clean tests, confirming its effectiveness."}
{"id": "2510.21485", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21485", "abs": "https://arxiv.org/abs/2510.21485", "authors": ["Yoshiki Masuyama", "Kohei Saijo", "Francesco Paissan", "Jiangyu Han", "Marc Delcroix", "Ryo Aihara", "François G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "FlexIO: Flexible Single- and Multi-Channel Speech Separation and Enhancement", "comment": "Submitted to ICASSP 2026", "summary": "Speech separation and enhancement (SSE) has advanced remarkably and achieved\npromising results in controlled settings, such as a fixed number of speakers\nand a fixed array configuration. Towards a universal SSE system, single-channel\nsystems have been extended to deal with a variable number of speakers (i.e.,\noutputs). Meanwhile, multi-channel systems accommodating various array\nconfigurations (i.e., inputs) have been developed. However, these attempts have\nbeen pursued separately. In this paper, we propose a flexible input and output\nSSE system, named FlexIO. It performs conditional separation using prompt\nvectors, one per speaker as a condition, allowing separation of an arbitrary\nnumber of speakers. Multi-channel mixtures are processed together with the\nprompt vectors via an array-agnostic channel communication mechanism. Our\nexperiments demonstrate that FlexIO successfully covers diverse conditions with\none to five microphones and one to three speakers. We also confirm the\nrobustness of FlexIO on CHiME-4 real data."}
{"id": "2510.21388", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21388", "abs": "https://arxiv.org/abs/2510.21388", "authors": ["Arshdeep Singh", "Vinayak Abrol", "Mark D. Plumbley"], "title": "Compressing Quaternion Convolutional Neural Networks for Audio Classification", "comment": "Under review in IEEE TASLPRO", "summary": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition."}
{"id": "2510.21209", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21209", "abs": "https://arxiv.org/abs/2510.21209", "authors": ["Zixiang Wan", "Guochang Zhang", "Yifeng He", "Jianqiang Wei"], "title": "SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum Domain", "comment": "Accepted by Interspeech 2025; 5 pages, 1 figure, 5 tables", "summary": "Neural Audio Codecs (NACs) have gained growing attention in recent years as\ntechnologies for audio compression and audio representation in speech language\nmodels. While mainstream NACs typically require G-level computation and M-level\nparameters, the performance of lightweight and streaming NACs remains\nunderexplored. This paper proposes SpecTokenizer, a lightweight streaming codec\nthat operates in the compressed spectral domain. Composed solely of alternating\nCNN and RNN layers, SpecTokenizer achieves greater efficiency and better\nrepresentational capability through multi-scale modeling in the compressed\nspectrum domain. At 4 kbps, the proposed SpecTokenizer achieves comparable or\nsuperior performance compared to the codec with state-of-the-art lightweight\narchitecture while requiring only 20% of the computation and 10% of the\nparameters. Furthermore, it significantly outperforms the codec when using\nsimilar computational and storage resources."}
{"id": "2510.21485", "categories": ["cs.SD", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21485", "abs": "https://arxiv.org/abs/2510.21485", "authors": ["Yoshiki Masuyama", "Kohei Saijo", "Francesco Paissan", "Jiangyu Han", "Marc Delcroix", "Ryo Aihara", "François G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "title": "FlexIO: Flexible Single- and Multi-Channel Speech Separation and Enhancement", "comment": "Submitted to ICASSP 2026", "summary": "Speech separation and enhancement (SSE) has advanced remarkably and achieved\npromising results in controlled settings, such as a fixed number of speakers\nand a fixed array configuration. Towards a universal SSE system, single-channel\nsystems have been extended to deal with a variable number of speakers (i.e.,\noutputs). Meanwhile, multi-channel systems accommodating various array\nconfigurations (i.e., inputs) have been developed. However, these attempts have\nbeen pursued separately. In this paper, we propose a flexible input and output\nSSE system, named FlexIO. It performs conditional separation using prompt\nvectors, one per speaker as a condition, allowing separation of an arbitrary\nnumber of speakers. Multi-channel mixtures are processed together with the\nprompt vectors via an array-agnostic channel communication mechanism. Our\nexperiments demonstrate that FlexIO successfully covers diverse conditions with\none to five microphones and one to three speakers. We also confirm the\nrobustness of FlexIO on CHiME-4 real data."}
{"id": "2510.21280", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.21280", "abs": "https://arxiv.org/abs/2510.21280", "authors": ["Christiaan M. Geldenhuys", "Günther Tonitz", "Thomas R. Niesler"], "title": "WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation", "comment": null, "summary": "While recent sound event detection (SED) systems can identify baleen whale\ncalls in marine audio, challenges related to false positive and minority-class\ndetection persist. We propose the boundary proposal network (BPN), which\nextends an existing lightweight SED system. The BPN is inspired by work in\nimage object detection and aims to reduce the number of false positive\ndetections. It achieves this by using intermediate latent representations\ncomputed within the backbone classification model to gate the final output.\nWhen added to an existing SED system, the BPN achieves a 16.8 % absolute\nincrease in precision, as well as 21.3 % and 9.4 % improvements in the F1-score\nfor minority-class d-calls and bp-calls, respectively. We further consider two\napproaches to the selection of post-processing hyperparameters: a\nforward-search and a backward-search. By separately optimising event-level and\nframe-level hyperparameters, these two approaches lead to considerable\nperformance improvements over parameters selected using empirical methods. The\ncomplete WhaleVAD-BPN system achieves a cross-validated development F1-score of\n0.475, which is a 9.8 % absolute improvement over the baseline."}
{"id": "2510.21317", "categories": ["eess.AS", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21317", "abs": "https://arxiv.org/abs/2510.21317", "authors": ["Danilo de Oliveira", "Tal Peer", "Jonas Rochdi", "Timo Gerkmann"], "title": "Are These Even Words? Quantifying the Gibberishness of Generative Speech Models", "comment": null, "summary": "Significant research efforts are currently being dedicated to non-intrusive\nquality and intelligibility assessment, especially given how it enables\ncuration of large scale datasets of in-the-wild speech data. However, with the\nincreasing capabilities of generative models to synthesize high quality speech,\nnew types of artifacts become relevant, such as generative hallucinations.\nWhile intrusive metrics are able to spot such sort of discrepancies from a\nreference signal, it is not clear how current non-intrusive methods react to\nhigh-quality phoneme confusions or, more extremely, gibberish speech. In this\npaper we explore how to factor in this aspect under a fully unsupervised\nsetting by leveraging language models. Additionally, we publish a dataset of\nhigh-quality synthesized gibberish speech for further development of measures\nto assess implausible sentences in spoken language, alongside code for\ncalculating scores from a variety of speech language models."}
{"id": "2510.21388", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.21388", "abs": "https://arxiv.org/abs/2510.21388", "authors": ["Arshdeep Singh", "Vinayak Abrol", "Mark D. Plumbley"], "title": "Compressing Quaternion Convolutional Neural Networks for Audio Classification", "comment": "Under review in IEEE TASLPRO", "summary": "Conventional Convolutional Neural Networks (CNNs) in the real domain have\nbeen widely used for audio classification. However, their convolution\noperations process multi-channel inputs independently, limiting the ability to\ncapture correlations among channels. This can lead to suboptimal feature\nlearning, particularly for complex audio patterns such as multi-channel\nspectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)\naddress this limitation by employing quaternion algebra to jointly capture\ninter-channel dependencies, enabling more compact models with fewer learnable\nparameters while better exploiting the multi-dimensional nature of audio\nsignals. However, QCNNs exhibit higher computational complexity due to the\noverhead of quaternion operations, resulting in increased inference latency and\nreduced efficiency compared to conventional CNNs, posing challenges for\ndeployment on resource-constrained platforms. To address this challenge, this\nstudy explores knowledge distillation (KD) and pruning, to reduce the\ncomputational complexity of QCNNs while maintaining performance. Our\nexperiments on audio classification reveal that pruning QCNNs achieves similar\nor superior performance compared to KD while requiring less computational\neffort. Compared to conventional CNNs and Transformer-based architectures,\npruned QCNNs achieve competitive performance with a reduced learnable parameter\ncount and computational complexity. On the AudioSet dataset, pruned QCNNs\nreduce computational cost by 50\\% and parameter count by 80\\%, while\nmaintaining performance comparable to the conventional CNNs. Furthermore,\npruned QCNNs generalize well across multiple audio classification benchmarks,\nincluding GTZAN for music genre recognition, ESC-50 for environmental sound\nclassification and RAVDESS for speech emotion recognition."}

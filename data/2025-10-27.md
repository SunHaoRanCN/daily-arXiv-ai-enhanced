<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 6]
- [eess.AS](#eess.AS) [Total: 9]
- [cs.SD](#cs.SD) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Multiscale Approach for Enhancing Weak Signal Detection](https://arxiv.org/abs/2510.20828)
*Dixon Vimalajeewa,Ursula U. Muller,Brani Vidakovic*

Main category: eess.SP

TL;DR: 提出了一种双阈值检测系统，结合随机共振和小波变换来增强弱信号检测，在原始数据域和频域都显著优于传统单阈值方法


<details>
  <summary>Details</summary>
Motivation: 传统随机共振技术基于单阈值检测器，局限于时间无关信号，且需要大量噪声来检测弱信号，这会扭曲复杂信号特征

Method: 开发双阈值检测系统，整合两个单阈值检测器，并在原始数据域和多尺度域（使用小波变换）进行评估

Result: 在原始数据域中，双阈值检测器显著改善弱信号检测；在频域中性能进一步提升，需要更低噪声水平且优于现有检测系统

Conclusion: 该研究通过引入稳健的弱信号识别方法推进了基于随机共振的检测方法学，在多个学科具有潜在应用价值

Abstract: Stochastic resonance (SR), a phenomenon originally introduced in climate
modeling, enhances signal detection by leveraging optimal noise levels within
non-linear systems. Traditional SR techniques, mainly based on single-threshold
detectors, are limited to signals whose behavior does not depend on time. Often
large amounts of noise are needed to detect weak signals, which can distort
complex signal characteristics. To address these limitations, this study
explores multi-threshold systems and the application of SR in multiscale
applications using wavelet transforms. In the multiscale domain signals can be
analyzed at different levels of resolution to better understand the underlying
dynamics.
  We propose a double-threshold detection system that integrates two
single-threshold detectors to enhance weak signal detection. We evaluate it
both in the original data domain and in the multiscale domain using simulated
and real-world signals and compare its performance with existing methods.
  Experimental results demonstrate that, in the original data domain, the
proposed double-threshold detector significantly improves weak signal detection
compared to conventional single-threshold approaches. Its performance is
further improved in the frequency domain, requiring lower noise levels while
outperforming existing detection systems. This study advances SR-based
detection methodologies by introducing a robust approach to weak signal
identification, with potential applications in various disciplines.

</details>


### [2] [Is Repeater-Assisted Massive MIMO Compatible with Dynamic TDD?](https://arxiv.org/abs/2510.20998)
*Martin Andersson,Anubhab Chowdhury,Erik G. Larsson*

Main category: eess.SP

TL;DR: 提出了一个在动态TDD中继辅助大规模MIMO网络中联合优化中继器增益放大和相移的框架，通过算法校准中继器增益来放大期望信号同时限制干扰。


<details>
  <summary>Details</summary>
Motivation: 中继器作为具有放大和相移功能的有源散射体，能增强用户接收信号强度，但也会放大不需要的噪声和干扰信号，特别是在动态TDD系统中由于上下行并发传输引入交叉链路干扰，导致期望信号和不期望信号放大之间存在权衡。

Method: 首先推导下行和上行频谱效率，然后开发用于频谱效率最大化的中继器增益优化算法。

Result: 数值结果表明，所提算法成功校准中继器增益，在放大期望信号的同时限制干扰。

Conclusion: 该框架能够有效解决动态TDD中继辅助网络中期望信号与干扰信号放大的权衡问题，通过优化中继器增益提升系统性能。

Abstract: We present a framework for joint amplification and phase shift optimization
of the repeater gain in dynamic time-division duplex (TDD) repeater-assisted
massive MIMO networks. Repeaters, being active scatterers with amplification
and phase shift, enhance the received signal strengths for users. However, they
inevitably also amplify undesired noise and interference signals, which become
particularly prominent in dynamic TDD systems due to the concurrent downlink
(DL) and uplink (UL) transmissions, introducing cross-link interference among
access points and users operating in opposite transmit directions. This causes
a non-trivial trade-off between amplification of desired and undesired signals.
To underpin the conditions under which such a trade-off can improve
performance, we first derive DL and UL spectral efficiencies (SEs), and then
develop a repeater gain optimization algorithm for SE maximization.
Numerically, we show that our proposed algorithm successfully calibrates the
repeater gain to amplify the desired signal while limiting the interference.

</details>


### [3] [6D Movable Holographic Surface Assisted Integrated Data and Energy Transfer: A Sensing Enhanced Approach](https://arxiv.org/abs/2510.21137)
*Zhonglun Wang,Yizhe Zhao,Gangming Hu,Yali Zheng,Kun Yang*

Main category: eess.SP

TL;DR: 提出了一种6D可移动全息表面集成数据与能量传输系统，通过三阶段协议协调6DMHS，有效服务IDET接收器，显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 可重构全息表面虽然能实现低成本大规模阵列，但其幅度控制的全息波束成形存在方向波动问题，难以充分利用RHS的空间增益。6D可移动天线为解决这一问题提供了潜在方案。

Method: 采用三阶段协议：上行感知阶段、方向调整阶段和下行传输阶段。首先利用全息感知技术获取接收器信息，然后优化6DMHS的方向和全息波束成形，最后优化数字波束成形、功率分配因子和能量收集功率。

Result: 仿真结果表明，所提方案相比基准方法能够显著提升IDET性能，使波束成形增益最大的方向与每个IDET接收器对齐。

Conclusion: 6D可移动全息表面集成数据与能量传输系统通过协调6DMHS，有效解决了全息波束成形的方向波动问题，实现了性能提升。

Abstract: Reconfigurable holographic surface (RHS) enables cost-effective large-scale
arrays with high spatial gain. However, its amplitude-controlled holographic
beamforming suffers from directional fluctuations, making it difficult to fully
exploit the spatial gain of RHS. Fortunately, the promising 6D movable antenna
(6DMA) provides a potential solution to this problem. In this paper, we study a
6D movable holographic surface (6DMHS) integrated data and energy transfer
(IDET) system, where a three-stage protocol is proposed, consisting of an
uplink sensing stage, an orientation adjustment stage and a downlink
transmission stage, to coordinate the 6DMHS and effectively serve the IDET
receivers. Firstly, the holographic-based sensing technology is proposed and
the sensing information of the IDET receivers is exploited. Secondly, by fixing
the rotations with the sensing information, the orientation optimization
problem is formulated for designing the holographic beamforming of the RHS and
adjusting the translations of the 6DMHS. As a result, the directions with
maximum beamforming gain are aligned with each IDET receiver. Thirdly, by
fixing the orientation of the 6DMHS and the holographic beamforming, the
equivalent wireless channel is obtained. The IDET performance optimization
problem is formulated for obtaining the optimal digital beamforming, power
splitting factor and energy harvesting (EH) power. Simulation results
demonstrate that the proposed scheme is capable of improving the IDET
performance compared to the benchmarks.

</details>


### [4] [Track-to-Track Association for Collective Perception based on Stochastic Optimization](https://arxiv.org/abs/2510.21278)
*Laura M. Wolf,Vincent Albert Wolff,Simon Steuernagel,Kolja Thormann,Marcus Baum*

Main category: eess.SP

TL;DR: 提出一种基于随机优化的多传感器轨迹关联算法，通过多维似然函数和多重关联假设解决智能车辆集体感知中的轨迹关联问题


<details>
  <summary>Details</summary>
Motivation: 解决多传感器融合中轨迹关联的高计算复杂度和启发式方法限制，克服智能车辆传感器局限，实现更有效的集体环境感知

Method: 基于随机优化的关联算法，使用包含轨迹数量和空间分布的多维似然函数，计算多个关联假设

Result: 在蒙特卡洛模拟和真实集体感知场景中验证了算法有效性，能够在模糊设置下计算高似然关联

Conclusion: 所提出的随机优化方法为智能城市中自动驾驶的集体感知提供了有效的轨迹关联解决方案

Abstract: Collective perception is a key aspect for autonomous driving in smart cities
as it aims to combine the local environment models of multiple intelligent
vehicles in order to overcome sensor limitations. A crucial part of
multi-sensor fusion is track-to-track association. Previous works often suffer
from high computational complexity or are based on heuristics. We propose an
association algorithms based on stochastic optimization, which leverages a
multidimensional likelihood incorporating the number of tracks and their
spatial distribution and furthermore computes several association hypotheses.
We demonstrate the effectiveness of our approach in Monte Carlo simulations and
a realistic collective perception scenario computing high-likelihood
associations in ambiguous settings.

</details>


### [5] [Optimized Power Control for Multi-User Integrated Sensing and Edge AI](https://arxiv.org/abs/2510.21378)
*Biao Dong,Bin Cao*

Main category: eess.SP

TL;DR: 本文研究了一个集成感知与边缘AI系统，通过AirComp技术将本地提取的特征传输到接入点进行协同推理，提出了两种优化代理并设计了最优收发器。


<details>
  <summary>Details</summary>
Motivation: 研究集成感知与边缘AI系统，探索AirComp误差与推理性能之间的关系，以优化系统性能。

Method: 建立了两种优化代理：计算最优代理和决策最优代理，分别最小化聚合失真和最大化类间可分性，并为TDM和FDM设置推导了最优收发器设计。

Result: 理论分析揭示了基于阈值的结构和双分解结构，实验结果验证了理论发现。

Conclusion: 提出的优化代理和收发器设计有效提升了集成感知与边缘AI系统的性能，为实际应用提供了理论支持。

Abstract: This work investigates an integrated sensing and edge artificial intelligence
(ISEA) system, where multiple devices first transmit probing signals for target
sensing and then offload locally extracted features to the access point (AP)
via analog over-the-air computation (AirComp) for collaborative inference. To
characterize the relationship between AirComp error and inference performance,
two proxies are established: the \emph{computation-optimal} proxy that
minimizes the aggregation distortion, and the \emph{decision-optimal} proxy
that maximizes the inter-class separability, respectively. Optimal transceiver
designs in terms of closed-form power allocation are derived for both
time-division multiplexing (TDM) and frequency-division multiplexing (FDM)
settings, revealing threshold-based and dual-decomposition structures,
respectively. Experimental results validate the theoretical findings.

</details>


### [6] [On Irradiance Distributions for Weakly Turbulent FSO Links: Log-Normal vs. Gamma-Gamma](https://arxiv.org/abs/2510.21509)
*Carmen Álvarez Roa,Yunus Can Gültekin,Vincent van Vliet,Menno van den Hout,Chigo Okonkwo,Alex Alvarado*

Main category: eess.SP

TL;DR: 实验表明对数正态分布无法准确描述弱湍流中的辐照度波动，Gamma-Gamma模型更为准确


<details>
  <summary>Details</summary>
Motivation: 弱湍流通常使用对数正态分布建模，但该模型在描述辐照度波动方面存在不足

Method: 通过实验研究比较对数正态分布和Gamma-Gamma模型在弱湍流中的表现

Result: 实验结果显示对数正态分布无法捕捉弱湍流中的辐照度波动，而Gamma-Gamma模型表现更准确

Conclusion: Gamma-Gamma模型比对数正态分布更适合用于弱湍流中辐照度波动的建模

Abstract: Weak turbulence is commonly modeled using the log-normal distribution. Our
experimental results show that this distribution fails to capture irradiance
fluctuations in this regime. The Gamma-Gamma model is shown to be more
accurate.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [7] [Can large audio language models understand child stuttering speech? speech summarization, and source separation](https://arxiv.org/abs/2510.20850)
*Chibuzor Okocha,Maya Bakri,Christan Grant*

Main category: eess.AS

TL;DR: 评估大型音频-语言模型在儿童口吃语音中的表现，包括源分离和摘要生成任务，发现模型在某些条件下能生成准确的儿童专用摘要，但在其他情况下会失败。


<details>
  <summary>Details</summary>
Motivation: 儿童语音在声学、韵律和语言发展方面与成人不同，且口吃（重复、延长、阻塞）对自动语音识别和自然语言处理构成挑战。目前大型音频-语言模型在口吃儿童语音中的表现尚未充分探索。

Method: 在两个设置中评估多个最先进的LALM：访谈（混合说话者）和阅读任务（单个儿童）。任务包括单通道源分离以隔离儿童语音，以及保留临床相关口吃并避免成人语音泄漏的儿童专用摘要生成。

Result: 研究发现LALM在某些条件下能够从混合音频中生成忠实的儿童专用摘要，但在其他情况下会失败。评估结合了LLM作为评判、人类专家评分和BERTScore，并报告了模型之间以及模型与人类之间的一致性。

Conclusion: 研究为临床和教育部署提供了实用指导，明确了LALM在混合音频中生成忠实儿童摘要的条件和失败场景，并提供了提示和评估脚本以支持复现。

Abstract: Child speech differs from adult speech in acoustics, prosody, and language
development, and disfluencies (repetitions, prolongations, blocks) further
challenge Automatic Speech Recognition (ASR) and downstream Natural Language
Processing (NLP). Recent large audio-language models (LALMs) demonstrate strong
cross-modal audio understanding; however, their behavior in disfluent child
speech remains underexplored. We evaluate several state-of-the-art LALMs in two
settings: an interview (mixed speakers) and a reading task (single child). The
tasks are (i) single-channel source separation to isolate the child and (ii)
child-only summarization that preserves clinically relevant disfluencies and
avoids adult-speech leakage.
  Evaluation combines Large Language Model (LLM) as a judge, human expert
ratings, and BERTScore (F1), and we report agreement between models and between
models and humans to assess reliability. Our findings delineate the conditions
under which LALMs produce faithful child-only summaries from mixed audio and
where they fail, offering practical guidance for clinical and educational
deployments. We provide prompts and evaluation scripts to support replication.

</details>


### [8] [Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization](https://arxiv.org/abs/2510.20853)
*Hyungjun Yoon,Seungjoo Lee,Yu Yvonne Wu,Xiaomeng Chen,Taiting Lu,Freddy Yifei Liu,Taeckyung Lee,Hyeongheon Cha,Haochen Zhao,Gaoteng Zhao,Sung-Ju Lee,Cecilia Mascolo,Dongyao Chen,Lili Qiu*

Main category: eess.AS

TL;DR: 提出了一种可扩展、任务无关的生理电信号监测方法，通过耳塞式硬件收集50小时自由生活数据，并开发了生理信息多频带标记化(PiMT)技术来学习鲁棒表示。


<details>
  <summary>Details</summary>
Motivation: 解决生理电信号建模的两个关键挑战：数据多样性不足（主要在实验室收集）和任务特定模型设计限制跨任务泛化能力。

Method: 使用耳塞式硬件原型收集自由生活数据，开发PiMT技术将生理电信号分解为12个生理信息标记，通过重建任务学习鲁棒表示。

Result: 在新建的DailySense数据集和四个公共基准测试中，PiMT在多样化任务上持续优于最先进方法。

Conclusion: 该方法实现了在真实环境中可扩展、任务无关的生理电信号监测，显著提升了跨任务泛化性能。

Abstract: Electrophysiological (ExG) signals offer valuable insights into human
physiology, yet building foundation models that generalize across everyday
tasks remains challenging due to two key limitations: (i) insufficient data
diversity, as most ExG recordings are collected in controlled labs with bulky,
expensive devices; and (ii) task-specific model designs that require tailored
processing (i.e., targeted frequency filters) and architectures, which limit
generalization across tasks. To address these challenges, we introduce an
approach for scalable, task-agnostic ExG monitoring in the wild. We collected
50 hours of unobtrusive free-living ExG data with an earphone-based hardware
prototype to narrow the data diversity gap. At the core of our approach is
Physiology-informed Multi-band Tokenization (PiMT), which decomposes ExG
signals into 12 physiology-informed tokens, followed by a reconstruction task
to learn robust representations. This enables adaptive feature recognition
across the full frequency spectrum while capturing task-relevant information.
Experiments on our new DailySense dataset-the first to enable ExG-based
analysis across five human senses-together with four public ExG benchmarks,
demonstrate that PiMT consistently outperforms state-of-the-art methods across
diverse tasks.

</details>


### [9] [Data-Centric Lessons To Improve Speech-Language Pretraining](https://arxiv.org/abs/2510.20860)
*Vishaal Udandarao,Zhiyun Lu,Xuankai Chang,Yongqiang Wang,Violet Z. Yao,Albin Madapally Jose,Fartash Faghri,Josh Gardner,Chung-Cheng Chiu*

Main category: eess.AS

TL;DR: 本文通过数据中心的探索，研究了语音语言模型预训练中的数据处理、合成数据构建和训练序列组织等关键因素，并基于这些发现训练了性能优越的SpeLangy模型。


<details>
  <summary>Details</summary>
Motivation: 当前语音问答系统缺乏对预训练数据处理和整理的受控消融研究，难以理解影响性能的具体因素，尽管其他数据模态的类似研究已取得显著进展。

Method: 进行数据中心探索，重点关注三个研究问题：原始网络爬取音频内容的处理方式、合成预训练数据集的构建方法、以及文本和音频段在训练序列中的交织方式。

Result: 基于研究发现的见解预训练了3.8B参数的SpeechLM模型SpeLangy，其性能比大3倍的模型高出10.2%。

Conclusion: 有效的数据整理对语音语言预训练具有重要影响，为未来SpeechLMs的数据中心探索提供了指导。

Abstract: Spoken Question-Answering (SQA) is a core capability for useful and
interactive artificial intelligence systems. Recently, several speech-language
models (SpeechLMs) have been released with a specific focus on improving their
SQA performance. However, a lack of controlled ablations of pretraining data
processing and curation makes it challenging to understand what factors account
for performance, despite substantial gains from similar studies in other data
modalities. In this work, we address this gap by conducting a data-centric
exploration for pretraining SpeechLMs. We focus on three research questions
fundamental to speech-language pretraining data: (1) how to process raw
web-crawled audio content for speech-text pretraining, (2) how to construct
synthetic pretraining datasets to augment web-crawled data and (3) how to
interleave (text, audio) segments into training sequences. We apply the
insights from our controlled data-centric ablations to pretrain a
3.8B-parameter SpeechLM, called SpeLangy, that outperforms models that are up
to 3x larger by 10.2% absolute performance. We hope our findings highlight the
impact of effective data curation for speech-language pretraining and guide
future data-centric exploration in SpeechLMs.

</details>


### [10] [refess-qi: reference-free evaluation for speech separation with joint quality and intelligibility scoring](https://arxiv.org/abs/2510.21014)
*Ari Frummer,Helin Wang,Tianyu Cao,Adi Arbel,Yuval Sieradzki,Oren Gal,Jesús Villalba,Thomas Thebaud,Najim Dehak*

Main category: eess.AS

TL;DR: 提出了一种基于自监督学习表示的无文本无参考语音分离评估框架，能够预测音频质量和语音可懂度，无需参考音频或转录文本。


<details>
  <summary>Details</summary>
Motivation: 传统语音分离评估指标需要参考音频和对应转录文本，无法评估真实世界中无参考的混合语音。

Method: 利用自监督学习表示，从混合和分离的音频轨道联合预测音频质量（SI-SNR）和语音可懂度（WER）。

Result: 在WHAMR!数据集上，WER估计的MAE为17%，PCC为0.77；SI-SNR估计的MAE为1.38，PCC为0.95。

Conclusion: 该框架在无参考情况下有效评估语音分离性能，且对不同自监督学习表示具有鲁棒性。

Abstract: Source separation is a crucial pre-processing step for various speech
processing tasks, such as automatic speech recognition (ASR). Traditionally,
the evaluation metrics for speech separation rely on the matched reference
audios and corresponding transcriptions to assess audio quality and
intelligibility. However, they cannot be used to evaluate real-world mixtures
for which no reference exists. This paper introduces a text-free reference-free
evaluation framework based on self-supervised learning (SSL) representations.
The proposed framework utilize the mixture and separated tracks to predict
jointly audio quality, through the Scale Invariant Signal to Noise Ratio
(SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER)
metric. We conducted experiments on the WHAMR! dataset, which shows a WER
estimation with a mean absolute error (MAE) of 17\% and a Pearson correlation
coefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of
0.95. We further demonstrate the robustness of our estimator by using various
SSL representations.

</details>


### [11] [PhoenixCodec: Taming Neural Speech Coding for Extreme Low-Resource Scenarios](https://arxiv.org/abs/2510.21196)
*Zixiang Wan,Haoran Zhao,Guochang Zhang,Runqiang Han,Jianqiang Wei,Yuexian Zou*

Main category: eess.AS

TL;DR: PhoenixCodec是一个专为极低资源条件设计的神经语音编解码框架，在700 MFLOPs计算量、30ms延迟和1/6 kbps双码率约束下，在LRAC 2025挑战赛中排名第三，在1 kbps码率下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极低资源条件下（计算量<700 MFLOPs、延迟<30ms、支持1/6 kbps双码率）面临效率与质量的权衡问题，需要解决传统解码器的资源分散问题。

Method: 采用优化的非对称频时架构、周期性校准与精炼(CCR)训练策略和噪声不变性微调程序，通过CCR避免局部最优，通过噪声样本微调增强鲁棒性。

Result: 在LRAC 2025挑战赛Track 1中总体排名第三，在1 kbps码率下在真实噪声和混响环境以及清晰度测试中表现最佳。

Conclusion: PhoenixCodec在极低资源条件下有效解决了效率与质量的权衡问题，证实了其有效性。

Abstract: This paper presents PhoenixCodec, a comprehensive neural speech coding and
decoding framework designed for extremely low-resource conditions. The proposed
system integrates an optimized asymmetric frequency-time architecture, a
Cyclical Calibration and Refinement (CCR) training strategy, and a
noise-invariant fine-tuning procedure. Under stringent constraints -
computation below 700 MFLOPs, latency less than 30 ms, and dual-rate support at
1 kbps and 6 kbps - existing methods face a trade-off between efficiency and
quality. PhoenixCodec addresses these challenges by alleviating the resource
scattering of conventional decoders, employing CCR to escape local optima, and
enhancing robustness through noisy-sample fine-tuning. In the LRAC 2025
Challenge Track 1, the proposed system ranked third overall and demonstrated
the best performance at 1 kbps in both real-world noise and reverberation and
intelligibility in clean tests, confirming its effectiveness.

</details>


### [12] [SpecTokenizer: A Lightweight Streaming Codec in the Compressed Spectrum Domain](https://arxiv.org/abs/2510.21209)
*Zixiang Wan,Guochang Zhang,Yifeng He,Jianqiang Wei*

Main category: eess.AS

TL;DR: 提出SpecTokenizer，一种在压缩频谱域操作的轻量级流式音频编解码器，仅使用CNN和RNN层，在4kbps下性能优于现有轻量级编解码器，同时仅需20%计算量和10%参数。


<details>
  <summary>Details</summary>
Motivation: 当前主流神经音频编解码器需要G级计算和M级参数，而轻量级和流式编解码器的性能研究不足。

Method: 在压缩频谱域进行多尺度建模，仅使用交替的CNN和RNN层构建轻量级流式编解码器。

Result: 在4kbps下，SpecTokenizer性能优于现有轻量级编解码器，计算量仅需20%，参数仅需10%。在相同计算和存储资源下显著优于其他编解码器。

Conclusion: SpecTokenizer证明了在压缩频谱域进行多尺度建模的有效性，为轻量级流式音频编解码提供了高效解决方案。

Abstract: Neural Audio Codecs (NACs) have gained growing attention in recent years as
technologies for audio compression and audio representation in speech language
models. While mainstream NACs typically require G-level computation and M-level
parameters, the performance of lightweight and streaming NACs remains
underexplored. This paper proposes SpecTokenizer, a lightweight streaming codec
that operates in the compressed spectral domain. Composed solely of alternating
CNN and RNN layers, SpecTokenizer achieves greater efficiency and better
representational capability through multi-scale modeling in the compressed
spectrum domain. At 4 kbps, the proposed SpecTokenizer achieves comparable or
superior performance compared to the codec with state-of-the-art lightweight
architecture while requiring only 20% of the computation and 10% of the
parameters. Furthermore, it significantly outperforms the codec when using
similar computational and storage resources.

</details>


### [13] [WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation](https://arxiv.org/abs/2510.21280)
*Christiaan M. Geldenhuys,Günther Tonitz,Thomas R. Niesler*

Main category: eess.AS

TL;DR: 提出了边界提议网络(BPN)来改进鲸鱼声音事件检测系统，通过使用中间潜在表示来减少误报，在精度上获得16.8%的绝对提升，并在少数类检测上显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的鲸鱼声音事件检测系统存在误报和少数类检测困难的问题，需要提高检测精度和减少假阳性。

Method: 扩展现有轻量级SED系统，提出边界提议网络(BPN)，利用分类模型中的中间潜在表示来筛选最终输出，并采用前向搜索和后向搜索两种超参数选择方法。

Result: BPN使精度绝对提升16.8%，少数类d-calls和bp-calls的F1分数分别提高21.3%和9.4%，完整系统F1分数达到0.475，比基线提升9.8%。

Conclusion: BPN能有效减少鲸鱼声音检测中的误报，提升少数类检测性能，结合优化的超参数选择方法可显著改善系统整体性能。

Abstract: While recent sound event detection (SED) systems can identify baleen whale
calls in marine audio, challenges related to false positive and minority-class
detection persist. We propose the boundary proposal network (BPN), which
extends an existing lightweight SED system. The BPN is inspired by work in
image object detection and aims to reduce the number of false positive
detections. It achieves this by using intermediate latent representations
computed within the backbone classification model to gate the final output.
When added to an existing SED system, the BPN achieves a 16.8 % absolute
increase in precision, as well as 21.3 % and 9.4 % improvements in the F1-score
for minority-class d-calls and bp-calls, respectively. We further consider two
approaches to the selection of post-processing hyperparameters: a
forward-search and a backward-search. By separately optimising event-level and
frame-level hyperparameters, these two approaches lead to considerable
performance improvements over parameters selected using empirical methods. The
complete WhaleVAD-BPN system achieves a cross-validated development F1-score of
0.475, which is a 9.8 % absolute improvement over the baseline.

</details>


### [14] [Are These Even Words? Quantifying the Gibberishness of Generative Speech Models](https://arxiv.org/abs/2510.21317)
*Danilo de Oliveira,Tal Peer,Jonas Rochdi,Timo Gerkmann*

Main category: eess.AS

TL;DR: 该论文探索了在无监督设置下使用语言模型来评估合成语音中的生成幻觉，如高质量音素混淆或胡言乱语语音，并发布了高质量合成胡言乱语语音数据集。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型合成高质量语音能力的增强，新的伪影类型如生成幻觉变得相关。当前的非侵入式方法如何应对高质量音素混淆或胡言乱语语音尚不清楚。

Method: 在完全无监督的设置下，利用语言模型来考虑这一方面，并发布了高质量合成胡言乱语语音数据集。

Result: 提供了计算各种语音语言模型得分的代码，并发布了用于评估口语中不合理句子的高质量合成胡言乱语语音数据集。

Conclusion: 该研究为评估合成语音中的生成幻觉提供了新的方法和数据集，有助于开发衡量口语中不合理句子的措施。

Abstract: Significant research efforts are currently being dedicated to non-intrusive
quality and intelligibility assessment, especially given how it enables
curation of large scale datasets of in-the-wild speech data. However, with the
increasing capabilities of generative models to synthesize high quality speech,
new types of artifacts become relevant, such as generative hallucinations.
While intrusive metrics are able to spot such sort of discrepancies from a
reference signal, it is not clear how current non-intrusive methods react to
high-quality phoneme confusions or, more extremely, gibberish speech. In this
paper we explore how to factor in this aspect under a fully unsupervised
setting by leveraging language models. Additionally, we publish a dataset of
high-quality synthesized gibberish speech for further development of measures
to assess implausible sentences in spoken language, alongside code for
calculating scores from a variety of speech language models.

</details>


### [15] [Compressing Quaternion Convolutional Neural Networks for Audio Classification](https://arxiv.org/abs/2510.21388)
*Arshdeep Singh,Vinayak Abrol,Mark D. Plumbley*

Main category: eess.AS

TL;DR: 该研究探索通过知识蒸馏和剪枝来降低四元数卷积神经网络(QCNNs)的计算复杂度，同时保持性能。实验表明剪枝QCNNs在音频分类中比知识蒸馏更有效，能减少50%计算成本和80%参数数量，同时在多个音频分类基准上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理多通道音频时无法有效捕捉通道间相关性，而QCNNs虽然能解决这个问题但计算复杂度较高，限制了在资源受限平台上的部署。

Method: 采用知识蒸馏(KD)和剪枝两种方法来降低QCNNs的计算复杂度，并在多个音频分类数据集上进行实验比较。

Result: 剪枝QCNNs在AudioSet数据集上减少50%计算成本和80%参数数量，同时保持与传统CNN相当的性能，在GTZAN、ESC-50和RAVDESS等基准数据集上表现出良好的泛化能力。

Conclusion: 剪枝是降低QCNNs计算复杂度的有效方法，相比知识蒸馏更高效，能在保持性能的同时显著减少模型大小和计算需求，适合资源受限环境部署。

Abstract: Conventional Convolutional Neural Networks (CNNs) in the real domain have
been widely used for audio classification. However, their convolution
operations process multi-channel inputs independently, limiting the ability to
capture correlations among channels. This can lead to suboptimal feature
learning, particularly for complex audio patterns such as multi-channel
spectrogram representations. Quaternion Convolutional Neural Networks (QCNNs)
address this limitation by employing quaternion algebra to jointly capture
inter-channel dependencies, enabling more compact models with fewer learnable
parameters while better exploiting the multi-dimensional nature of audio
signals. However, QCNNs exhibit higher computational complexity due to the
overhead of quaternion operations, resulting in increased inference latency and
reduced efficiency compared to conventional CNNs, posing challenges for
deployment on resource-constrained platforms. To address this challenge, this
study explores knowledge distillation (KD) and pruning, to reduce the
computational complexity of QCNNs while maintaining performance. Our
experiments on audio classification reveal that pruning QCNNs achieves similar
or superior performance compared to KD while requiring less computational
effort. Compared to conventional CNNs and Transformer-based architectures,
pruned QCNNs achieve competitive performance with a reduced learnable parameter
count and computational complexity. On the AudioSet dataset, pruned QCNNs
reduce computational cost by 50\% and parameter count by 80\%, while
maintaining performance comparable to the conventional CNNs. Furthermore,
pruned QCNNs generalize well across multiple audio classification benchmarks,
including GTZAN for music genre recognition, ESC-50 for environmental sound
classification and RAVDESS for speech emotion recognition.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [16] [Robust Distortion-Free Watermark for Autoregressive Audio Generation Models](https://arxiv.org/abs/2510.21115)
*Yihan Wu,Georgios Milis,Ruibo Chen,Heng Huang*

Main category: cs.SD

TL;DR: 提出Aligned-IS水印技术，解决自回归音频模型中重标记化不匹配问题，提高水印检测能力同时保持音频质量


<details>
  <summary>Details</summary>
Motivation: 随着自回归音频模型的广泛应用，语音伪造等滥用风险增加，需要有效的水印技术确保数字媒体真实性。传统统计水印方法在音频模型中面临重标记化不匹配的挑战

Method: 使用聚类方法将同一簇内的标记视为等价，专门针对音频生成模型设计无失真水印技术Aligned-IS

Result: 在主流音频生成平台上测试显示，Aligned-IS在保持生成音频质量的同时，显著提高了水印检测能力，优于现有无失真水印方法

Conclusion: Aligned-IS为安全音频技术应用设立了新标准，有效解决了自回归音频模型中的水印挑战

Abstract: The rapid advancement of next-token-prediction models has led to widespread
adoption across modalities, enabling the creation of realistic synthetic media.
In the audio domain, while autoregressive speech models have propelled
conversational interactions forward, the potential for misuse, such as
impersonation in phishing schemes or crafting misleading speech recordings, has
also increased. Security measures such as watermarking have thus become
essential to ensuring the authenticity of digital media. Traditional
statistical watermarking methods used for autoregressive language models face
challenges when applied to autoregressive audio models, due to the inevitable
``retokenization mismatch'' - the discrepancy between original and retokenized
discrete audio token sequences. To address this, we introduce Aligned-IS, a
novel, distortion-free watermark, specifically crafted for audio generation
models. This technique utilizes a clustering approach that treats tokens within
the same cluster equivalently, effectively countering the retokenization
mismatch issue. Our comprehensive testing on prevalent audio generation
platforms demonstrates that Aligned-IS not only preserves the quality of
generated audio but also significantly improves the watermark detectability
compared to the state-of-the-art distortion-free watermarking adaptations,
establishing a new benchmark in secure audio technology applications.

</details>


### [17] [HiFi-HARP: A High-Fidelity 7th-Order Ambisonic Room Impulse Response Dataset](https://arxiv.org/abs/2510.21257)
*Shivam Saini,Jürgen Peissig*

Main category: cs.SD

TL;DR: HiFi-HARP是一个大规模7阶高阶Ambisonic房间脉冲响应数据集，包含超过10万个通过混合声学模拟在真实室内场景中生成的RIR。


<details>
  <summary>Details</summary>
Motivation: 为复杂环境中的空间音频和声学算法开发提供高质量资源，结合波动理论精度与真实房间内容，扩展现有RIR数据集。

Method: 使用3D-FRONT存储库中的复杂家具房间模型，采用混合模拟方法：900Hz以下使用基于波的有限差分时域模拟，900Hz以上使用射线追踪方法，最后编码到球谐域。

Result: 生成了包含超过10万个7阶Ambisonic RIR的数据集，提供了详细的房间体积、RT60分布和吸声特性统计，并与其他RIR集合进行了比较。

Conclusion: HiFi-HARP为复杂环境中的空间音频渲染、声学参数估计等机器学习应用提供了丰富资源，尽管存在模拟近似和静态场景等限制。

Abstract: We introduce HiFi-HARP, a large-scale dataset of 7th-order Higher-Order
Ambisonic Room Impulse Responses (HOA-RIRs) consisting of more than 100,000
RIRs generated via a hybrid acoustic simulation in realistic indoor scenes.
HiFi-HARP combines geometrically complex, furnished room models from the
3D-FRONT repository with a hybrid simulation pipeline: low-frequency wave-based
simulation (finite-difference time-domain) up to 900 Hz is used, while high
frequencies above 900 Hz are simulated using a ray-tracing approach. The
combined raw RIRs are encoded into the spherical-harmonic domain (AmbiX ACN)
for direct auralization. Our dataset extends prior work by providing 7th-order
Ambisonic RIRs that combine wave-theoretic accuracy with realistic room
content. We detail the generation pipeline (scene and material selection, array
design, hybrid simulation, ambisonic encoding) and provide dataset statistics
(room volumes, RT60 distributions, absorption properties). A comparison table
highlights the novelty of HiFi-HARP relative to existing RIR collections.
Finally, we outline potential benchmarks such as FOA-to-HOA upsampling, source
localization, and dereverberation. We discuss machine learning use cases
(spatial audio rendering, acoustic parameter estimation) and limitations (e.g.,
simulation approximations, static scenes). Overall, HiFi-HARP offers a rich
resource for developing spatial audio and acoustics algorithms in complex
environments.

</details>


### [18] [FlexIO: Flexible Single- and Multi-Channel Speech Separation and Enhancement](https://arxiv.org/abs/2510.21485)
*Yoshiki Masuyama,Kohei Saijo,Francesco Paissan,Jiangyu Han,Marc Delcroix,Ryo Aihara,François G. Germain,Gordon Wichern,Jonathan Le Roux*

Main category: cs.SD

TL;DR: FlexIO是一个灵活的语音分离和增强系统，能够处理可变数量的麦克风输入和说话人输出，通过提示向量实现条件分离。


<details>
  <summary>Details</summary>
Motivation: 现有语音分离系统通常在固定设置下工作，缺乏同时处理可变输入（麦克风阵列配置）和输出（说话人数量）的灵活性。

Method: 使用每个说话人的提示向量作为条件，结合阵列无关的通道通信机制处理多通道混合信号。

Result: FlexIO成功覆盖了1-5个麦克风和1-3个说话人的多种条件，并在CHiME-4真实数据上表现出鲁棒性。

Conclusion: FlexIO实现了灵活输入输出的语音分离，为通用语音分离系统的发展提供了有效解决方案。

Abstract: Speech separation and enhancement (SSE) has advanced remarkably and achieved
promising results in controlled settings, such as a fixed number of speakers
and a fixed array configuration. Towards a universal SSE system, single-channel
systems have been extended to deal with a variable number of speakers (i.e.,
outputs). Meanwhile, multi-channel systems accommodating various array
configurations (i.e., inputs) have been developed. However, these attempts have
been pursued separately. In this paper, we propose a flexible input and output
SSE system, named FlexIO. It performs conditional separation using prompt
vectors, one per speaker as a condition, allowing separation of an arbitrary
number of speakers. Multi-channel mixtures are processed together with the
prompt vectors via an array-agnostic channel communication mechanism. Our
experiments demonstrate that FlexIO successfully covers diverse conditions with
one to five microphones and one to three speakers. We also confirm the
robustness of FlexIO on CHiME-4 real data.

</details>


### [19] [Smule Renaissance Small: Efficient General-Purpose Vocal Restoration](https://arxiv.org/abs/2510.21659)
*Yongyi Zang,Chris Manchester,David Young,Ivan Ivanov,Jeffrey Lufkin,Martin Vladimirov,PJ Solomon,Svetoslav Kepchelev,Fei Yueh Chen,Dongting Cai,Teodor Naydenov,Randal Leistikow*

Main category: cs.SD

TL;DR: 提出了SRS模型，一种在复杂STFT域进行端到端人声恢复的紧凑单阶段模型，在iPhone 12 CPU上实现10.5倍实时推理速度，并在多退化场景下超越多个基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决消费设备录音中常见的多种并发退化问题，包括噪声、混响、带宽限制和削波，需要一种高效且通用的恢复方法。

Method: 采用在复杂STFT域的单阶段端到端模型，结合相位感知损失，使用大分析窗口提高频率分辨率。

Result: 在DNS 5挑战赛盲测集上超越强GAN基线，接近计算昂贵的流匹配系统；在极端退化基准测试中，在歌唱音频上超越所有开源基线，与商业系统相当，在语音上保持竞争力。

Conclusion: SRS模型在多种退化条件下实现了高效且有效的音频恢复，同时发布了模型和基准数据集促进进一步研究。

Abstract: Vocal recordings on consumer devices commonly suffer from multiple concurrent
degradations: noise, reverberation, band-limiting, and clipping. We present
Smule Renaissance Small (SRS), a compact single-stage model that performs
end-to-end vocal restoration directly in the complex STFT domain. By
incorporating phase-aware losses, SRS enables large analysis windows for
improved frequency resolution while achieving 10.5x real-time inference on
iPhone 12 CPU at 48 kHz. On the DNS 5 Challenge blind set, despite no speech
training, SRS outperforms a strong GAN baseline and closely matches a
computationally expensive flow-matching system. To enable evaluation under
realistic multi-degradation scenarios, we introduce the Extreme Degradation
Bench (EDB): 87 singing and speech recordings captured under severe acoustic
conditions. On EDB, SRS surpasses all open-source baselines on singing and
matches commercial systems, while remaining competitive on speech despite no
speech-specific training. We release both SRS and EDB under the MIT License.

</details>


### [20] [FlowSynth: Instrument Generation Through Distributional Flow Matching and Test-Time Search](https://arxiv.org/abs/2510.21667)
*Qihui Yang,Randal Leistikow,Yongyi Zang*

Main category: cs.SD

TL;DR: FlowSynth结合分布流匹配和测试时优化，通过概率化建模和基于置信度的轨迹选择，解决了虚拟乐器生成中的音色一致性难题。


<details>
  <summary>Details</summary>
Motivation: 现有音符级模型难以在不同音高和力度下保持一致的音色，这是虚拟乐器生成的关键挑战。

Method: 使用分布流匹配将速度场参数化为高斯分布，通过负对数似然优化，并在测试时采样多个轨迹，基于模型置信度选择最大化音色一致性的输出。

Result: FlowSynth在单音符质量和跨音符一致性方面均优于当前最先进的TokenSynth基线。

Conclusion: 在流匹配中建模预测不确定性，结合音乐特定的音色一致性目标，为实时表演提供了专业质量虚拟乐器的有效路径。

Abstract: Virtual instrument generation requires maintaining consistent timbre across
different pitches and velocities, a challenge that existing note-level models
struggle to address. We present FlowSynth, which combines distributional flow
matching (DFM) with test-time optimization for high-quality instrument
synthesis. Unlike standard flow matching that learns deterministic mappings,
DFM parameterizes the velocity field as a Gaussian distribution and optimizes
via negative log-likelihood, enabling the model to express uncertainty in its
predictions. This probabilistic formulation allows principled test-time search:
we sample multiple trajectories weighted by model confidence and select outputs
that maximize timbre consistency. FlowSynth outperforms the current
state-of-the-art TokenSynth baseline in both single-note quality and cross-note
consistency. Our approach demonstrates that modeling predictive uncertainty in
flow matching, combined with music-specific consistency objectives, provides an
effective path to professional-quality virtual instruments suitable for
real-time performance.

</details>


### [21] [StylePitcher: Generating Style-Following and Expressive Pitch Curves for Versatile Singing Tasks](https://arxiv.org/abs/2510.21685)
*Jingyue Huang,Qihui Yang,Fei Yueh Chen,Julian McAuley,Randal Leistikow,Perry R. Cook,Yongyi Zang*

Main category: cs.SD

TL;DR: StylePitcher是一个通用的音高曲线生成器，通过参考音频学习歌手风格，同时保持与目标旋律的对齐，基于整流流匹配架构，可适应多种歌唱任务而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有音高曲线生成器存在两个主要问题：忽略歌手特定的表现力，难以捕捉个体歌唱风格；通常作为特定任务的辅助模块开发，限制了泛化能力。

Method: 基于整流流匹配架构，灵活结合符号音乐乐谱和音高上下文作为生成条件，能够无缝适应多样歌唱任务而无需重新训练。

Result: 在各种歌唱任务的客观和主观评估中，StylePitcher在保持与任务特定基线相当的音高准确性的同时，提高了风格相似性和音频质量。

Conclusion: StylePitcher作为一个通用音高曲线生成器，能够有效学习歌手风格并适应多种歌唱任务，在风格相似性和音频质量方面表现优异。

Abstract: Existing pitch curve generators face two main challenges: they often neglect
singer-specific expressiveness, reducing their ability to capture individual
singing styles. And they are typically developed as auxiliary modules for
specific tasks such as pitch correction, singing voice synthesis, or voice
conversion, which restricts their generalization capability. We propose
StylePitcher, a general-purpose pitch curve generator that learns singer style
from reference audio while preserving alignment with the intended melody. Built
upon a rectified flow matching architecture, StylePitcher flexibly incorporates
symbolic music scores and pitch context as conditions for generation, and can
seamlessly adapt to diverse singing tasks without retraining. Objective and
subjective evaluations across various singing tasks demonstrate that
StylePitcher improves style similarity and audio quality while maintaining
pitch accuracy comparable to task-specific baselines.

</details>
